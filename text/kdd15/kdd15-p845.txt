Extreme States Distribution Decomposition Method for
Search Engine Online Evaluation
Kirill Nikolaev
Yandex , Moscow , Russia kvn@yandex team.ru
Alexander Ulianov Yandex , Moscow , Russia ulyanov@yandex team.ru
Alexey Drutsa
Yandex , Moscow , Russia adrutsa@yandex.ru
Ekaterina Gladkikh Yandex , Moscow , Russia kglad@yandex team.ru
Gleb Gusev
Yandex , Moscow , Russia gleb57@yandex team.ru
Pavel Serdyukov
Yandex , Moscow , Russia pavser@yandex team.ru
ABSTRACT Nowadays , the development of most leading web services is controlled by online experiments that qualify and quantify the steady stream of their updates . The challenging problem is to define an appropriate online metric of user behavior , so called Overall Evaluation Criterion ( OEC ) , which is both interpretable and sensitive . The state of the art approach is to choose a type of entities to observe in the behavior data , to define a key metric for these observations , and to estimate the average value of this metric over the observations in each of the system versions . A significant disadvantage of the OEC obtained in this way is that the average value of the key metric does not necessarily change , even if its distribution changes significantly . The reason is that the difference between the mean values of the key metric over the two variants of the system does not necessarily reflect the character of the change in the distribution .
We develop a novel method of quantifying the change in the distribution of the key metric , which is ( 1 ) interpretable , ( 2 ) is based on the analysis of the two distributions as a whole , and , for this reason , is sensitive to more ways the two distributions may actually differ . We provide a thorough theoretical analysis of our approach and show experimentally that , other things being equal , it produces more sensitive OEC than the average . Categories and Subject Descriptions : H12 [ User/Machine Systems ] : Human information processing ; H52 [ User interface ] : Evaluation/methodology General Terms : Theory , Measurement , Experimentation Keywords : Distribution decomposition ; A/B test ; effect variable
1 .
INTRODUCTION
Online controlled experiments , such as A/B testing , have become the state of the art technique for improving web services based on data driven decisions [ 22 ] . They are utilized by many web companies including search engines such as Bing [ 8 , 22 ] and Google [ 29 ] , social networks such as Facebook [ 1 ] , etc . The largest web services have designed special experimental platforms that allow them to run A/B tests at large scale ( eg , hundreds of concurrent experiments per day ) [ 29 , 21 ] . An A/B test compares two variants of a service at a time , usually its current version ( control ) and a new one ( treatment ) , by exposing them to two groups of users . The aim of controlled experiments is to detect the causal effect of the system updates on its performance relying on an Overall Evaluation Criterion ( OEC ) , a user behavior metric that is assumed to correlate with the quality of the system . The challenging problem is then to choose an appropriate OEC applicable in practice , since it has to meet two crucial requirements , which often conflict .
First , the OEC should provide a quantitative value that allows analysts to make conclusions about the change in the system ’s quality , particularly , about the sign and magnitude of that change . In other words , the value of OEC must have a clear interpretation . As long as it is known [ 20 ] that many user behavior metrics may result in contradictory interpretations and their use in practice may be misleading , the right choice of an appropriate OEC is a difficult task . Second , when the treatment effect exists , the OEC has to detect the difference of the two versions of the system at a high level of statistical significance in order to distinguish the treatment effect from the noise observed when the effect does not exist . This property is referred to as the sensitivity of the metric [ 23 ] . The common problem is the low metric sensitivity in the cases when only a subtle modification is being tested or if only a small amount of traffic is affected by the system change [ 22 ] .
The state of the art approach to online evaluation is based on the estimation of the average value of a key metric over the entities and events of a certain type observed in the behavioral data . These events could be : users , user sessions , user queries , or user transitions ( clicks ) . The exemplary key metrics for a search engine can be , respectively : the number of sessions of the user [ 20 , 28 ] , the number of queries in the session or its duration , the number of user ’s actions after the query [ 9 , 22 , 11 ] , and dwell time of the transition [ 18 ] . The conclusion is based on a statistical significance test , where the null hypothesis is that the mean values of the key metric over the two groups of users coincide . In this way , the common approach is to define OEC as the empirical average of the key metric . A fundamental disadvantage of this criterion is that the mean value of the key metric does not
845 necessarily change , even if its distribution changes significantly . Therefore , the described approach may fail to detect significant changes in the system performance even when they are reflected by changes in the distribution of the key metric .
In this paper , we focus on online evaluation based on the two distributions of the key metric in the two versions of the system as a whole rather than their average values . The standard approaches to examine the equality of two probability distributions , such as the Kolmogorov–Smirnov test [ 17 , 25 ] , can only detect inequality , while their values do not allow to interpret the system change . Therefore , we develop a novel approach based on the intuition that the observed distribution is a mixture of two latent distributions , which represent successes and failures .
Inspired by the basic concepts of quantum physics , we assume that , in each individual online experiment , there is a latent binary variable Y ( a function of the observed events ) , which takes two values 1 and 0 and is responsible for the change in the distribution of the key metric X under the evaluated system update . Intuitively , the two values 1 and 0 may correspond to extreme types of events : absolute successes and absolute failures . We follow the intuition that , if the key metric is able to detect the treatment effect , then this effect is reflected in the change of Y . Therefore , we assume that X does not directly depend on the system version Z , but there is only indirect dependency via Y , ie , the following equality of conditional distributions holds : P ( X | Y , Z ) = P ( X | Y ) . In this way , there is no causal effect of the treatment that is not conditioned by Y .
We assume that , generally , we have no reliable ground truth to estimate the distribution of Y for each event individually . Instead , we describe the latent variable Y by the two distributions F1 and F0 of the key metric X conditioned by the two values of the variable ( 1 and 0 respectively ) . The distribution of the key metric X can be represented as a mixture pF1 + ( 1 − p)F0 , and the mean value p of the variable Y over a given system version has a clear interpretation as the portion of one of the two components in this decomposition . Therefore , we propose to use the mean value p as an OEC . When we know F1 and F0 , we can estimate the change in p between the two versions of the system . We propose a theoretically sound approach to estimate F1 and F0 based on the search for the latent variable Y that had undergone the smallest change in the experiment . Our approach is not restricted to the case of one key metric , and a promising research direction is to investigate its applicability to the problem of handling multiple key metrics .
We evaluated our approach in Yandex ( wwwyandexcom ) , a global search engine used by tens of millions of users throughout the world . In the case of a search engine , there is a variety of different components , and their updates may affect the distribution of a key metric differently . The most important are the components of the search engine result page ( SERP ) , which normally presents data from several sources : organic search results , advertising results , vertical results , and others . If an update affects only the advertising results , then it is difficult to assess the total quality of the whole SERP , because advertising makes less than 10 % in the search engine traffic . Therefore , the problem of low sensitivity of OEC becomes particularly acute in this case . Among other key metrics , we consider dwell time of a click .
In this case , Y may be interpreted as the indicator of the satisfied clicks . Now we summarize our main contributions : • We proposed a novel problem of online evaluation based on the comparison of the two distributions of the key metric rather than their mean values .
• We developed a theoretically sound approach to define an interpretable OEC based on the two distributions as a whole , which is relative and therefore sensitive to a wide range of ways the two distributions may differ . • We described a large set of online experiments with a popular search engine , where the proposed approach demonstrates its advantage over the state of the art .
The rest of the paper is organized as follows . In Section 2 , we present the related work . In Section 3 , we introduce the novel method in a conceptual way and show its theoretical foundations . Section 4 considers several peculiarities of the practical use of our approach . Section 5 describes the conducted A/B experiments with the changes in search and advertising components of the search engine under study ( Yandex ) and presents the application of the proposed method in a particular context .
2 . RELATED WORK
We compare our research with other studies in two aspects . The first one relates to the common methodology of conducting online controlled experiments . The second one concerns evaluation metrics used in web services and , particularly , search engines .
Online controlled experiment studies . The theoretical details of the online controlled experiment methodology were widely described in the existing works [ 26 , 23 ] , and we conduct our experiments in accordance with them . A number of practical lessons learned from the applications of this methodology to different evaluation cases in many web companies was recently described in [ 19 , 29 , 21 ] . These studies concern , inter alia , experiments with different components of a web service ( eg , the user interface [ 19 , 11 ] and ranking algorithms [ 28 , 11] ) , large scale experimental infrastructure [ 29 , 21 ] , different aspects of user behavior and interaction with a web service ( clicks [ 20 , 22 ] , speed [ 22 ] , abandonment [ 22 ] , periodicity [ 11 , 10] ) , and so on . These experiments show that system updates of different types may affect various key metrics differently . In our evaluations , we experiment with a diverse set of system changes and key metrics . Some of the existing works focused on the study of the trustworthiness of the results of an A/B test . Various pitfalls and puzzling outcomes of online controlled experimentation were shared in [ 5 , 20 ] and several “ rules of thumb ” were discussed in [ 22 ] . We were aware of all these experiences and pitfalls during our A/B experiments .
Some other studies focused on sensitivity improvement techniques for an online controlled experiment . The simplest ones [ 23 ] include increasing of the experimental time period or expanding the user population participating in the experiment ; investigation of evaluation metric with lower variance [ 11 ] ; and elimination of users who were not affected by the service change in the treatment group [ 28 ] . More sophisticated sensitivity improvement techniques include the stratification and the usage of control variates that are defined on the basis of pre experiment data [ 9 ] ; the reduction
846 of skewness of the evaluation metric [ 22 ] ( eg , transformation of the metric or capping its values ) ; the sensitivity improvement for two stage online controlled experiments [ 8 ] ; the future user behavior prediction approach [ 12 ] ; and diluted treatment effect estimation for trigger analysis [ 7 ] . In all previous studies so far , the value of OEC is assigned to each system version irrespectively of the distributions of the key metric for the other versions . Our novel approach benefits from the comparison of the two distributions of a key metric for two versions of the system and results in relative OEC that directly evaluates the difference between the two versions of the system . Moreover , our study is complementary to the previous methods , since it is straightforward to combine it with any of them .
Evaluation metrics . User engagement metrics are popular in A/B testing practice of many companies , because user engagement reflects how often the user solves her needs ( eg , to search something ) by means of the considered service ( eg , a search engine ) [ 20 , 21 , 22 ] . Hence , on the one hand , these metrics are measurable in the short term experiment period , and , on the other hand , they are predictive of long term goals of the company [ 20 , 21 , 22 ] . The most well known metrics of loyalty aspect of user engagement are the state of the art number of sessions per user [ 28 ] and the absence time [ 13 ] metrics . There are also several widely used metrics of activity aspect of user engagement like the number of clicks per user , shows per user , queries per user [ 23 , 9 ] , dwell time per click [ 6 ] , etc . All these metrics were utilized to evaluate different changes in search engines [ 4 , 11 , 12 , 10 ] . In our work , we utilize 6 main user engagement metrics ( the same as in [ 28 , 11 , 12 , 10 ] ) to validate our approach against them .
3 . FRAMEWORK
3.1 A/B testing background
In A/B testing , users participated in an experiment , are randomly exposed to one of the two variants of a service ( the control ( A ) and the treatment ( B ) , eg , the current production version of the service and its update ) in order to compare them [ 23 , 20 , 22 ] . The experiment ’s objective is quantitatively measured by an evaluation metric µ ( also known as the online service quality metric , the Overall Evaluation Criterion ( OEC ) , etc . [ 23] ) . In the classical methodology of A/B testing , this metric is usually the average value µ = avgΩX of a scalar measure X(ω ) ( a key metric ) over the events ( entities ) ω ∈ Ω . If the events or experimental units Ω are users ( a popular choice for web services ) , then the metric µ is referred to as a per user one ( eg , sessions per user , clicks per user , etc ) Any other set of independent events could be experimental units as well , and then we come to a non per user metric ( eg , clicks persession , click through rate , etc ) Thus , for each user group G ∈ {A , B} , we have the distribution DG of the measure X over the experimental units ΩG . Then , the average valX , G ∈ {A , B} are used as OEC and their ues µG = avgΩG difference ∆ = µB − µA is calculated .
Nonetheless , the quantity ∆ could not serve itself as an indicator of positive or negative consequences of the evaluated changes of the service . The difference between the evaluation metrics over groups should be controlled by a statistical significance test1 that calculates the probability ( also known as p value ) to observe the difference under the null hypothesis , which assumes that the observed difference is caused by random fluctuations , and the variants of the system are not actually different from the user ’s point of view . If the p value is lower than the threshold pval ≤ pT h val = 0.05 is commonly used [ 23 , 20 , 9 , 28 , 22 , 11 , 12] ) , then the test rejects the null hypothesis , and the difference ∆ is assumed to be statistically significant . val ( pT h
The additional details of the A/B testing framework could be found in the survey and practical guide [ 23 ] . 3.2 Latent effect variable
A fundamental disadvantage of the classical approach described in the previous subsection is that a significant change in the system , which is reflected in a noticeable change of the distribution of the key metric X , does not necessary lead to a significant change in the mean value µ(X ) . Therefore , the described approach may fail to detect significant treatment effects even when they are reflected by the distribution of X . In order to overcome this issue , we state the following hyif the distribution P(X ) of the observed variable pothesis : X is subjected to the treatment effect , then a latent variable Y , which is responsible for this effect , should exist . The variable Y has its own distribution P(Y ) whose mean value might be simpler , more interpretable and sensitive to the treatment effect than the one of the observed variable X . Our approach is inspired by the basic concepts of quantum physics [ 15 ] such as quantum states or mixed states , where , eg , we have a beam of electrons ( regarded as events ) , each electron may have an unobservable mixed state , but we only observe a binary variable responding for orientation of the spin . To this end , in our approach , we restrict Y by the case of a binary variable , which takes 2 values y1 and y0 . In this case , the distribution P(Y ) is determined by only one parameter , ie , the probability of Y = y1 . Hence , if such latent variable exists , then the treatment effect in its distribution P(Y ) will have the simple interpretation : the probability of occurring of the event {Y = y1} changes in one of possible directions ( increases or decreases ) , while the probability of occurring of the opposite event {Y = y0} changes in the contrary direction . Moreover , the difference between the probability P(Y = y1 ) for the variants A and B could be easily derived from the average value of the variable Y if its values are 1 and 0 .
In our approach , we assume that the distribution of X is completely conditioned by Y in the sense that there is no causal effect of the treatment that is not conditioned by Y . In this way , the conditional distributions of X wrt the latent variable Y do not depend on the variant , ie , they are not subjected to the treatment effect . Further , in our theoretical analysis ( see Sec 3.3 ) , we show that such binary latent effect variable Y always exists ( ie , the above stated hypothesis holds ) . Moreover , in the general case , there is an infinite number of latent effect variables that satisfy our restrictions . Therefore , we want to find a latent binary variable Y , which is the best among those that respond to the change .
So , let us consider how the distributions of variables X and Y relate to each other and the variants A and B . Let Si = {Y = yi} , i = 1 , 0 , be the states of the effect variable Y . 1The widely applicable ones are two sample t test ( as in [ 9 , 28 , 8 , 11 , 12 ] ) and Bootstrapping ( [5 , 22] ) .
847 Then the distribution P(X ) of the observed variable X can be represented in the form
P(X ) = P(S1)P(X|S1 ) + P(S0)P(X|S0 ) , which , for the control and treatment variants , transforms to
P(X|A ) = P(S1|A)P(X|S1 , A ) + P(S0|A)P(X|S0 , A ) , P(X|B ) = P(S1|B)P(X|S1 , B ) + P(S0|B)P(X|S0 , B ) .

According to our restrictions on the variable Y , the distributions P(X|Si , A ) = P(X|Si , B ) = P(X|Si ) , i = 1 , 0 , do not depend on the variant that leads to
P(X|A ) = P(S1|A)P(X|S1 ) + P(S0|A)P(X|S0 ) , P(X|B ) = P(S1|B)P(X|S1 ) + P(S0|B)P(X|S0 ) .
We introduce the following notations : DG = P(X|G ) , pG = P(S1|G ) for G = A , B , and Fi = P(X|Si ) , i = 1 , 0 . Then P(S0|G ) = 1 − pG , G = A , B , and our problem of finding an effect variable Y is reduced to decomposition the two distributions DA , DB into the linear combination of two distributions F1 and F0
2 :
DA(x ) = pAF1(x ) + ( 1 − pA)F0(x ) DB(x ) = pBF1(x ) + ( 1 − pB)F0(x )
∀x ∈ X , ∀x ∈ X , where pA , pB ∈ [ 0 , 1 ] and the set X = {x | ∃ω ∈ Ω : x = X(ω)} is the region of values of the variable X ( eg , the positive real numbers R for the dwell time , the natural numbers N for the number of sessions , etc ) 3.3 Distribution decomposition problem over X , ie
Formalization . In this section , we consider our decomposition problem . Let DA and DB be some distributions X DG(x)dx = 1 and DG(x ) ≥ 0,∀x ∈ X for
G = A , B . Then , our problem could be formalized as follows .
Problem 1 . Having DA and DB as described above , find pA , pB , and F1 , F0 : X → R , so that
DA(x ) = pAF1(x ) + ( 1 − pA)F0(x ) ∀x ∈ X , DB(x ) = pBF1(x ) + ( 1 − pB)F0(x ) ∀x ∈ X , pA , pB ∈ [ 0 , 1 ] , Fi(x ) ≥ 0 , X Fi(x)dx = 1 ,
∀x ∈ X , i = 1 , 0 , i = 1 , 0 .
( 1 )
The scalar values pA and pB are referred to as the mixture coefficients and the pair of distributions F1 and F0 is referred to as the decomposition basis .
Preliminary analysis . Let us show that , if the decomposition ( 1 ) exists , then the difference between the distributions F1 and F0 is determined by the difference in the mixture coefficients solely . We denote this difference by α = pB − pA ( the mixture difference ) . Subtracting the first equation in ( 1 ) from the second one , we obtain the following identity , which connects the difference of the decomposition basis ( F1 , F0 ) and the source distributions DA and DB :
DB − DA = α(F1 − F0 ) .
( 2 )
Hence , we infer that , first , the disagreement of the decomposition basis ( ie , its power ) is linearly proportional to the 2This differs from studying of a linear combination of the given distributions DA and DB like in [ 16 ] . difference between DA and DB . And , second , the lower absolute value of the mixture difference |α| , the higher this disagreement . From here on in this subsection , we do not consider the degenerate case α = 0 , where we have DA = DB . In practice , α is usually not equal to 0 due to random fluctuations in real empirical distributions DA and DB .
Solutions of the problem . Now we will find all solutions of Problem 1 . First , one expresses the basis distributions Fi , i = 1 , 0 , in terms of pA , pB , DA , and DB . In order to do so , we subtract the first equation in ( 1 ) from the second one , multiplying them by a suitable scalar ( ie , by 1 − pB and 1 − pA for i = 1 , or by pB and pA for i = 0 , respectively ) . After simple transformations , we obtain :
F1(pA , pB ; DA , DB ) =
[ (1 − pA)DB − ( 1 − pB)DA ] , ( 3 )
1 α
F0(pA , pB ; DA , DB ) =
[ pBDA − pADB ] .
1 α
( 4 )
Thus , F1 and F0 could be uniquely restored for each pair ( pA , pB ) .
Lemma 1 . If α = 0 , then
X DA(x)dx = 1 ,
X Fi(x)dx = 1 for i = 1 , 0 .
X DB(x)dx = 1 , and
Proof . The proof follows from integration of the expres sions ( 3 ) and ( 4 ) over X , using linearity of an integral .
Therefore , one concludes that Problem 1 is equivalent to the following problem of finding the mixture coefficients pA and pB :
Problem 2 . Find pA and pB , such that pA , pB ∈ [ 0 , 1 ] , Fi(pA , pB ; DA , DB ; x ) ≥ 0 , i = 1 , 0 ,
∀x ∈ X .
( 5 )
The solution domain of this problem on the plane R2 is denoted by P . This region possesses several properties that are summarized in the following lemma .
( pA,pB )
Lemma 2 . The following properties hold : 1 . P is not empty and contains at least two trivial solutions ( 1 , 0 ) and ( 0 , 1 ) with the decomposition bases ( DA , DB ) and ( DB , DA ) , respectively ;
2 . P is a centrosymmetric figure wrt the point ( 1/2 , 1/2 )
( ie , the center of the unit square [ 0 , 1]2 ) ;
3 . P does not cross the diagonal pA = pB ( where α = 0 ) ; 4 . P is a closed set ; 5 . Each set P ∩ {pA > pB} and P ∩ {pA < pB} is convex and connected .
Proof . The properties 1 and 3 are obvious . Next , let ( pA , pB ) ∈ P , then , for the point ( 1 − pA , 1 − pB ) , we have Fi(1 − pA , 1 − pB ; DA , DB ; x ) = F1−i(pA , pB ; DA , DB ; x ) ≥ 0 ∀x ∈ X ( due to the expressions ( 3)–(4) ) . Therefore , the property 2 holds . For α > 0 ( α < 0 ) , the inequalities ( 5 ) define a closed convex set for each x ∈ X . The region P is the intersection of all of them , and , hence , P is also closed , convex , and thus connected , ie , the properties 4 and 5 hold .
848 basis distributions . Similarly , the case M = +∞ infers that P = {pB ∈ [ 1 − m , 1 ] , pA = 0} ∪ {pB ∈ [ 0 , m ] , pA = 1} , ie , F1 = DA or F0 = DA . Having M = +∞ ⇔ ∃x ∈ X : DA(x ) = 0 , DB(x ) > 0 , we derive that , if there is a region in X which represents the variant B solely , then the distribution of the variant A ought to be one of the basis distributions . Finally , if both m = 0 , and M = +∞ hold , then the set P degenerates to the points ( 0 , 1 ) and ( 1 , 0 ) . Therefore , if , both for the variant A and for the variant B , there is a region in X , which represents the considered variant solely , the decomposition basis ( F1 , F0 ) ought to coincide with the two source distributions DA and DB . In this case , we conclude that the distribution decomposition could not find more feasible basis of the studied variant comparison ( A vs B ) than the existing distributions 4 . In terms of the effect variable , in the described case , the effect variable Y is the variable which exactly indicates whether the variant is A or B .
Optimal decomposition . So , as we stated above , it is essential that Problem 1 has a continuum of possible solutions ( if it is not a degenerated case with m = 0 and M = +∞ ) . Therefore , to correctly define the effect variable Y , we introduce an additional requirement on the appropriate solution of Problem 1 . Namely , we state that the solution should be optimal in the sense that the absolute value of the mixture difference should be minimum possible , ie , we state the task of finding a solution such that |α| → min . On the one hand , it means that we try to find such decomposition basis ( F1 , F0 ) , in which the difference ( in the sense of mixture coefficient ) between the source distributions is minimal . On the other hand , identity 2 implies that the minimization of the absolute value of the mixture difference |α| infers the maximization of the disagreement of the distributions of the decomposition basis ( ie , its power ) : F1 − F0 = α−1(DB − DA ) . In terms of the effect variable , we try to find such effect variable Y that has the most different basis distributions F1 and F0 . This agrees well with the intuition that F1 and F0 represent two extremely different ( absolute ) states of an event , Y = 1 and Y = 0 .
Theorem 2 . There are only two solutions of Problem 1 with minimum |α| ( see Fig 1 ( b) ) :
α0± = ± ( M − 1)(1 − m )
M − m with the mixture coefficients ( for α0
+ ) :
( 10 )
1 − m M − m
M ( 1 − m ) M − m p0 A = p0 B =
.
,
( 11 ) Proof . The minimization of |α| = |pB −pA| on the plane R2 ( pA,pB ) is equivalent to the minimization of the distance to the line pA = pB . Thus , the desired point ( pA , pB ) will be the closest point of the set P to this line . There are only two closest points in this set : the internal angles of the quadrangles ( see Fig 1 ( b ) ) whose coordinates , ie , ( pA , pB ) , could be found as the intersections of the lines that bound the half planes in the inequalities ( 7 ) and ( 8 ) . For α > 0
4In practice , such a situation may arise due to a lack in the number of observed data . For instance , some values of X may occur in both variants A and B , but with a very low probability . As a result , such values may meet only in one of the sample distributions . We will discuss in the next section , how to overcome this issue .
Figure 1 : R2 ( pA,pB ) . ( b ) The optimal solutions ( p0 optimal differences α0± .
( a ) The solution set P on the plane B ) and the
A , p0
Let us consider the minimum and the maximum ratios of
DB and DA and denote them by3 m = inf x∈X
DB(x ) DA(x )
∈ [ 0 , 1 ) , M = sup x∈X
DB(x ) DA(x )
∈ ( 1 , +∞ ] . ( 6 )
Theorem 1 . The set of all solutions of Problem 2 is a union of two quadrangles that are centrally symmetric to each other wrt the point ( 1/2 , 1/2 ) and are limited in the unit square [ 0 , 1]2 by the union of two pairs of intersected half planes ( see Fig 1 ) : pB ≥ M pA and pB ≥ 1 − m(1 − pA ) , pB ≤ mpA and pB ≤ 1 − M ( 1 − pA ) .
( 7 )
( 8 )
Proof . We will provide the proof for the case α > 0 . For this restriction , the non negativeness of Fi(pA , pB ; DA , DB ; x ) , i = 1 , 0 , x ∈ X , is equivalent to the non negativeness of their numerators in the expressions ( 3)–(4 ) , which are linear functions of ( pA , pB ) . Therefore , for each x ∈ X , they define the half planes : pB ≥ pA
DB(x ) DA(x )
,
( 9 ) whose intersection over all x ∈ X leads to the inequalities ( 7 ) . The analysis of the case α < 0 is similar .
, pB ≥ 1 − ( 1 − pA )
DB(x ) DA(x )
Discussion of found solutions . First , note that , due to Lemma 2 , each solution s = ( pA , pB , F1 , F0 ) has its dual solution s = ( 1 − pA , 1 − pB , F0 , F1 ) . Their mixture differences α and α have the same absolute values , but have opposite signs ( ie , α = −α ) . Hence , the verdict on the sign of the treatment effect ( positive or negative ) must be based on the analysis of both the sign of the mixture difference α and the decomposition basis ( F1 , F0 ) . In terms of the effect variable Y , it means that , if a binary variable Y satisfies our constraints , then the opposite ¬Y also satisfies them . Second , we will consider the special cases of m = 0 and M = +∞ . If m = 0 , then , on the one hand , it means that the set P becomes the pair of two segments ( see the inequalities ( 7)–(8) ) : {pB = 1 , pA ∈ [ 0 , M−1]} and {pB = 0 , pA ∈ [ 1 − M−1 , 1]} ( see Fig 1 ( a) ) . Thus , F1 = DB for pB = 1 and F0 = DB for pA = 0 . On the other hand , m = 0 holds if and only if ∃x ∈ X : DB(x ) = 0 , DA(x ) > 0 . Hence , if there is a region in X which represents the variant A solely , then the distribution of the variant B ought to be one of the 3In the case , when ∃x ∈ X : DA(x ) = 0 , DB(x ) > 0 , we set M = +∞ . Also note that m = 1 ⇔ M = 1 ⇔ DA = DB .
849 ( the case α < 0 is similar ) , we have pB = M pA , pB = 1 − m(1 − pA ) ⇒ M p0 The last equation implies the coordinates ( 11 ) of the intersection point ( p0
B ) and the identity ( 10 ) for α0 + .
A , p0
A = 1 − m(1 − p0
A ) .
Note that the relative difference β = α/pA for the positive + = M − 1 . Also α0 + in Eq ( 10 ) has a very simple form : β0 note that , for fixed F0 and F1 , the right hand side of the Eq ( 10 ) tends to zero as M and m tend to 1 , thus Eq ( 10 ) provides a robust solution .
This theorem shows that , first , the optimal decomposition exists and unique ( up to the transposition of the basis distributions ) . Second , the decomposition parameters could be easily derived from the given source distributions DA and DB , and , moreover , the mixture parameters α0 A , and p0 B depend only on the minimum m and the maximum M of the ratio of these distributions over X . From here on in this paper , the decomposition ( 1 ) described by equations ( 3 ) , ( 4 ) , ( 10 ) , and ( 11 ) is referred to as the Optimal Distribution Decomposition ( ODD ) .
+ , p0
The case of a binary variable . In order to illustrate the Optimal Distribution Decomposition , we will consider a simple special case , when the variable X is binary , ie , X could take only two values on experimental units , for instance , 1 and 0 ( eg , representing , whether a click is satisfied or not ) . Then , the distributions DA and DB are defined by the probabilities of the event X = 1 : let they are qA = P(X = 1 | A ) and qB = P(X = 1 | B ) . Assume qB > qA , then M = qB/qA and m = ( 1 − qB)/(1 − qA ) . From identities ( 3)–(4 ) and ( 10)–(11 ) , we derive
+ = qB − qA , α0 p0 A = qA , p0 B = qB ,
F 0
1 = Ix=1 ,
F 0
0 = Ix=0 .
So , in this simple case , the optimal distribution decomposition is the standard representation form of the binary variable distributions :
DG = qGIx=1 + ( 1 − qG)Ix=0 , G = A , B .
Thus , if the observed variable X is binary , then the optimal effect variable Y is equal either to X , or its opposite ¬X . 3.4 Optimal Distribution Decomposition Ap proach
Based on Theorem 1 , we conclude that the latent effect variable always exists . Theorem 2 infers that the optimal latent effect variable is unique up to the transposition of its vales 0 and 1 . Therefore , the Optimal Distribution Decomposition could be applied to evaluate treatment effects in online controlled experiments .
Details and limitations of the approach usage . Now we discuss how to determine the sign of the effect on the basis of the obtained OEC . As we discussed above , the mixture difference α0 + is the value of the mixture coefficient change . In order to understand the meaning of this change , one should compare the distribution F1 whose portions in DB increased wrt its portion in DA with the distribution F0 . The sign of the change could be determined only after we decide whether F1 or F0 correspond to the better state of an event . This is the first limitation of the ODD approach : the method could be easily applied to any measure X , any service and any A/B experiment , but , in order to derive the sign of the observed changes in the optimal effect variable Y , one should have a clear interpretation of the distributions F1 and F0
Actually , as we previously stated , the optimal effect variable Y depends on the considered A/B experiment . In other words , if we conduct another A/B experiment , we may obtain another optimal effect variable Y . We expect that , for changes of a similar type ( eg , changes in the UI ) , the optimal effect variable Y should be the same . However , we could make the opposite statement : if the changes ( ie , A/B experiments ) have the same optimal effect variable Y , then these changes are of the same type and could be compared on the same scale ( eg , to select the best one to be implemented in priority ) . Otherwise , if the changes have different decomposition bases , they could not be compared in terms of the mixture difference ( it means that the evaluated updates change the system in different aspects ) .
Also , we expect that the ODD approach may give different decomposition bases on different systems , because , even if the basis distributions express the states of the same latent variable Y , this latent variable may have different distributions over different systems ( eg , satisfied clicks could have different distributions wrt dwell time for advertising and for organic search results ) .
Advantages of the approach . First , we regard our technique as a complementary one to other sensitivity improvement approaches ( like in [ 9 , 22 , 7 , 12] ) . Second , the ODD approach could be applied to a series of metrics , or to a multidimensional variable X , while the classical approach ( which evaluates the average values ) deals with onedimensional variable only ( ie , X ⊂ R ) . Third , the ODD approach may be used not only for A/B testing purposes ( when we compare different changes of the same service ) , but also in order to analyze the difference between two services with respect to the distribution ( in this case the variant A is the first service and the variant B is the other ) . Its applicability and effectiveness in these scenarios should be evaluated in future studies .
4 . THE ODD APPROACH IN PRACTICE
In the previous section , we discussed the theoretical aspects of our novel approach of evaluating the treatment in an A/B experiment . However , using the ODD approach in practice has its specificities and potential pitfalls . The first specificity is peculiar to any evaluation criterion in A/B testing and concerns the random fluctuations in the observed data , which always result in some difference between the distributions of X of the variants A and B , even when there is no actual difference in their quality . Thus , we will discuss how we measure the statistical significance of the observed difference .
The second specificity is specific to the ODD approach and comes from the fact that a large number of possible values of X occur in our variants with a very low probability . Since the sample size is usually limited , as only a marginal share of users can be exposed to the treatment variant [ 19 , 29 , 9 ] , some values may occur in one of the variants only . Then , this would lead to a mismatch of the value regions of X for the treatment and control variants . Therefore , a straightforward application of the ODD approach to the source empirical distributions of the variants A and B may cause the degenerated case , when the solution set P consists of 2 points and α0 + = 1 . This would mean that the treatment
850 variant is a completely different system ( web service , etc . ) from the control variant ( see Sec 3.4 ) , while the real ( ideal ) distributions for the variants might differ insignificantly . We overcome this issue by the means of discretization of the observed variable X , a technique widely used in statistics . 4.1 Variable discretization The main idea consists in a decomposition of the domain of values X into a small and finite number of subdomains , referred to as bins , in such a way that each bin will be filled by a sufficient number of observations of X for the considered sample size constraint . We assume the frequent case , when X is the interval [ 0,∞ ) ⊂ R , for simplicity . Let observations X A NA take place for the control variant A and observations X B NB take place for the treatment one B . Then , we apply the equal frequency binning for {X A j=1 . Namely , we divide the range of values [ 0,∞ ) to S bins [ 0,∞ ) = [ t0 , t1 ) [ t1 , t2 ) . . . [ tS−1 , tS ) in such a way that the numbers of values X A falling into each bin j [ tl−1 , tl ) are approximately equal to each other ( here t0 = 0 , tS = ∞ ) . Then , for the samples {X A j } , we obtain their empirical discrete distributions wrt the bins : j } and {X B
2 , . . . , X B
2 , . . . , X A j }NA
1 , X B
1 , X A
( dG
1 , dG
2 , . . . , dG
S ) , dG l =
, G = A , B .
|{j | X G j ∈ [ tl−1 , tl)}| |{X G j }|
Then , in the ODD approach , we utilize either these discrete distributions as DA and DB , or the piecewise constant approximations of the source empirical densities ( ie , these l /(tl − tl−1 ) density functions have the constant values dG over each bin [ tl−1 , tl) ) . Note that , in both cases , the ODD approach works in the same way , because both the solution set P , and the optimal parameters p0 + depend solely on M and m , which are the same for both cases . From here on in this paper , we assume that the variable X has been passed the above mentioned discretization procedure .
B , α0
A , p0
This approach to discretization , on the one hand , is simple to implement , but , on the other hand , has the following constraint : a priori , we assume a high proximity of the distributions of the variants ( in this case the numbers of values X B j falling into each bin are expected to be sufficient to avoid the above described issue ) . Therefore , it is well applicable for evaluation of small changes or updates of the system .
We also note that a discretization procedure adds noise to the evaluation criterion α0 It is the other side of the + . coin of such procedure , which should be additionally studied from different points of view . The reduction of this noise could be stated as a reason to construct novel discretization approaches . Moreover , the basic components of the ODD method , such as M and m , could be replaced by their unbiased estimations , which would reduce the discretization noise . Since it is not the main point of our study , we left it for the future work . 4.2 Statistical significance
The key component of any A/B experiment is a statistical test , which evaluates whether the studied metric ( the overall evaluation criterion ) differs for user groups exposed to the control ( A ) and the treatment ( B ) variants [ 5 ] . The test accepts or rejects the null hypothesis , which assumes that the observed difference in the metric is caused by random fluctuations , and the variants are not actually different . There is a large variety of statistical tests , whose details could be
Table 1 : Steps of our bootstrap adaptation . j } from the samples {XA
1 Obtain bootstrap samples {(cid:102)XA 2 Calculate the empirical distributions DA and DB based on {(cid:102)XA j } and {(cid:102)XB j } and j } and {(cid:102)XB j } j } respectively .
{XB respectively ( see Sec 41 ) tions DA and DB , according to the ODD approach ( see Sec 34 )
+ on the basis of the obtained discrete distribu
3 Calculate the value a1 = α0
4 Obtain a bootstrap sample {(cid:102)XA 5 Repeat steps 2 4 using {(cid:102)XA j } from the sample {XA j } instead of {(cid:102)XB j } . j } everywhere , obtain some value a2 .
6 Calculate a = a1 − a2 . found in textbooks , such as [ 2 ] . different tests of statistical significance5 .
In our work , we use two
Kolmogorov–Smirnov test . First , our approach is based on the hypothesis that the underlying distributions of DA and DB are different . Moreover , from the theoretical analysis of our approach ( see Sec 3.3 ) , we know that a non zero + = 0 ) holds if difference in the evaluation criterion ( ie , α0 and only if DA and DB are different . Hence , it is natural to apply a test which evaluates a statistical significance of the overall difference of these distributions . We utilize the Kolmogorov–Smirnov test [ 17 , 25 ] .
Bootstrap technique . Second , we utilize a statistical + = 0 , as well . Our test to evaluate the null hypothesis of α0 scalar evaluation criterion α0 + has a pitfall : any empirical estimation of this variable has a positive bias , since α0 + is always positive according to the methodology of our approach ( see Sec 33 ) Hence , the commonly used two sample t test [ 23 , 9 , 28 , 8 , 11 , 12 ] could not be applied in our case ( unlike the state of the art difference of the average values , see Sec 31 ) Hence , we pay our attention to the bootstrap technique [ 14 ] , which is also widely used in A/B testing [ 5 , 22 ] . We adapt this technique in order to make it aware of the bias of the α0
+ estimation .
Our bootstrap adaptation is as follows . In order to collect several estimates of α0 + , we apply bootstrapping [ 14 ] to the empirical distributions of observations X A NA for the control variant A and observations X B 2 , . . . , X B NB for the treatment one B . Namely , we proceed the steps from Table 1 to obtain one ( corrected ) estimation a of the coefficient α0 + . We repeat this procedure g times and , finally , apply one sample t test to the obtained values a1 , a2 , . . . , ag .
2 , . . . , X A
1 , X B
1 , X A
5 . EXPERIMENTATION
We apply our ODD approach and the classical one to evaluation of the treatment effect in several A/B experiments conducted on real users of Yandex in order to compare them and validate the novel one . Each experiment ran for two weeks , the user6 samples used in these A/B tests were all uniformly randomly selected , and the control and the treatment groups were almost of the same sizes ( at least , hundreds of thousands of users ) . The first part of experiments consider the relationship between the dwell time of clicks measured on advertising results and the ones measured on organic search results for a search query . We conduct thorough practical analysis of our approach on these experiments . Additionally , since the improvement of the sensitivity of A/B testing has a number of useful implications for practice [ 19 , 29 , 9 ,
5However , experimenters , using the ODD approach in practice , do not have to limit themselves to them . 6In this paper , we use cookie IDs [ 23 ] to identify users as done in other studies [ 13 , 24 , 28 , 11 ] .
851 12 ] ( see Sec 1 and 2 ) , we compared sensitivity levels of our approach and the baseline one wrt 6 popular user engagement measures in the second part of A/B tests . The bootstrapping technique of the ODD approach is applied with 1000 iterations . In our discretization procedure 4.1 , we use 20 bins . We utilize the threshold pT h val = 0.05 , which is commonly used [ 23 , 20 , 9 , 28 , 22 ] , for the p value of all statistical significance tests used in this section . 5.1 Click dwell time evaluation
We start our experimental validation of the ODD approach from consideration of three following A/B experiments :
• The first A/B experiment ( Exp #1 ) evaluates a modification of the design of the advertising , which is displayed together with organic search results on the search engine result page ( SERP ) . In the control variant A , the advertising results were marked out as a special SERP region by a slightly different background color . In the treatment variant B , each ad snippet was marked separately and the background was the same as for the rest of the SERP .
• The second A/B experiment ( Exp #2 ) evaluates a deterioration of the organic search results by ignoring the commercial quality labels , which are described in [ 27 ] . • The third A/B experiment ( Exp #3 ) evaluates a deterioration of the organic search results by swapping the results on the SERP .
Note that the treatment effects of two latter experiments are expected to be negative wrt organic search , while the treatment effect of the first one is expected to be positive wrt advertising . Measurements . We consider a click as our experimental unit ω ∈ Ω and its dwell time as the measure X . For this measure , the mean value approach results in the state ofthe art metric “ dwell time per click ” . Additionally , we used the popular metric “ fraction of long clicks ” [ 18 , 3 , 30 ] , which is based on the binary indicator of whether the click dwell time is more than 30 seconds or not . Then , for the first measure , we apply the ODD approach , obtaining the latent variable Y . Since the second measure is a binary variable , the corresponding latent variable is always equal to the observed one ( see Sec 33 )
We conducted 18 control experiments ( ie , A/A tests ) in order to check the correctness of the experimentation [ 23 , 5 ] . An A/A test , which compares the same versions of the service , should be failed in not more than 5 % of the time for our p value threshold 0.05 [ 23 , 5 ] . The classical approach ( the mean value ) both with the two sample t test and with the Bootstrap test [ 14 ] fails only one A/A experiment , while the Kolmogorov–Smirnov test and the ODD approach with the Bootstrap test do not fail any of A/A tests . These results are acceptable for all considered evaluation criteria .
In each experiment , we applied our evaluation criteria ( both the novel and the baseline ones ) to three sets of the whole click traffic ( the SERP segments , where a user clicks ) : the clicks on the advertising results only ( ADV ) ; the clicks on the organic search results only ( ORG ) ; the clicks on all results ( ALL ) .
A/B tests results . As considered before , for 3 sets of click traffic ( ADV , ORG , and ALL ) , for 3 A/B experiments , we have implemented 4 evaluation criteria :
• the classical approach ( Mean ) is applied to the dwell time measure ( Mean DT ) ;
• the ODD approach is applied to the dwell time mea sure ( ODD DT ) with the bootstrap test ;
• the Kolmogorov–Smirnov test is applied to the dis cretized distributions of the dwell time ( KS DT ) ;
• the classical approach ( Mean ) is applied to the long click indication measure ( Mean LC ) .
The results for all of them are summarized in Table 2 . Let us consider the first A/B experiment in detail . From the results of the classical A/B testing approach for the dwell time measure ( Mean DT ) , we see that the treatment effect is detected both for the advertising traffic ( ADV ) and for the whole traffic ( ALL ) . The dwell time of an average click increases for them . At the same time , the dwell time does not change for the organic search traffic . Therefore , we conclude that , in this experiment , the quality of the clicks on the advertising results increases [ 18 , 3 , 30 ] without changing the quality of the clicks on the organic search results . Therefore , from this evaluation criterion , we conclude that this experiment is positive7 . The same conclusion could be made , based on the results for the fraction of long clicks ( Table 2 , Mean LC ) .
The results of the Kolmogorov–Smirnov test detects the treatment effect in the same click traffics ( ADV and ALL ) as the classical approach , while the ODD approach with the Bootstrap technique detects a significant small difference even in the organic search traffic ( ORG ) . A similar situation is observed for the second and third experiment : the ODD approach with our Bootstrap technique detects treatments in those A/B tests , where it is detected by any other evaluation criterion , and in several A/B tests , which treatment effect is not detected by others . Therefore , we conclude that the novel ODD evaluation criterion outperforms the classical mean evaluation criterion ( both with the two sample t test and with the Bootstrap test ) and the Kolmogorov–Smirnov test both in terms of sensitivity level ( p value ) , and in terms of the number of detected treatment effects .
We summarize the details of applying the ODD approach to the Exp #1 on the advertising traffic in Fig 2 . From Fig 2 ( d ) , we see that the mean value of the distribution F1 is higher than the one of F0 . Hence , we conclude that the extreme state Y = 1 of the latent variable corresponds to more qualitative clicks [ 18 , 3 , 30 ] , and , thus , it is a positive ( success ) state8 . Therefore , the considered A/B test Exp #1 is positive wrt the ODD approach , which agrees with the conclusion based on the classical approach . From Fig 2 ( c ) , we also see that the identity ( 2 ) for the distributions differences DB − DA and F1 − F0 holds in practice . 7Further , this change in the advertisement increased the revenue of the company . 8We remind that , according to our methodology , the difference α0 A is always positive for an existing treatment effect , and the sign of this change should be derived from the analysis of the basis distributions F1 and F0 ( see Sec 34 )
B − p0
+ = p0
852 Table 2 : The results of the ODD and classical evaluation criteria for 3 A/B experiments and 3 click traffics .
Exps
Click traffic
Exp #1 ADV
Advert . ORG
∆
µA 667.4 11.06 0.0163 2.04 6e−08 <0.001 0.64 0.028 0.044 0.0012 <0.001 <0.001 907.8 −0.2 −0.0002 1.2
0.43 0.0009 0.0021 0.0003
0.02 val
0.876
0.88 >0.05 1e−08 <0.001 0.77 0.006 0.0078 0.0005 <0.001 <0.001 pBoot val pBoot val p0 A
α0 + design ALL
865.2
5.5
0.0064
1
Mean DT % ∆ SE pT−test
ODD DT SE
β0 +
KS DT pKS val
µA
∆
0.65
0.014
Mean LC
% ∆ SE pT−test 0.02 0.0004 6e−298 val
0.7491 0.978 0.7257 0.0037 0.005 0.0002 1e−140
0.0002
0
0
Exp #2 ADV
Comm . ORG quality ALL
Exp #3 ADV
Swap ORG
ALL
2.9
0.0053
550.4 5.7 651.4 −6.9 −0.0106 2.7 631.5 −5.9 −0.0093 2.6
0.68
0.65
0.49 0.003
0.006
0.002
0.17
>0.05
0.014
0.019
0.3
0.01
0.03 0.0014 <0.001
<0.05
0.036
0.039
0.37 0.008 0.022 0.0011 <0.001
<0.05
0.5
0.0007
668.1 >0.05 1122.0 −18.9 −0.0168 0.8 1e−127 <0.001 0.58 0.008 0.0014 0.0004 <0.001 <0.001 1104.3 −18.3 −0.0166 0.7 9e−127 <0.001 0.58 0.0077 0.013 0.004 <0.001 <0.001
0.43 0.0034 0.0079 0.0012
0.801
0.796
0.01
3.2
0.013
0.019
0.0011
0.6537 0.24 0.6935 0.0071 0.0102 0.0006 4e−37 0.6856 0.006 0.0088 0.0005 8e−33 0.6834 0.0023 0.0034 0.0005 2e−05 0.7178 0.0015 0.0021 0.0001 8e−46 0.7165 0.0016 0.0022 0.0001 3e−50
Table 3 : The number of failed A/A tests ( out of 18 ) and the number of A/B tests ( out of 23 ) with detected treatment effect for each user engagement measure and for each evaluation criterion .
A/A tests
A/B tests
Figure 2 : The first A/B experiment on the advertising traffic : ( a ) the distributions DA and DB of clicks over the dwell time for the variants ; ( b ) the solution set P of the ODD approach and the opti+ ; ( c ) the differences DB − DA and mal p0 F1 − F0 ; ( d ) the optimal basis F1 , F0 .
B , and α0
A , p0
5.2 User engagement evaluation
Experimental setup .
In this section , we consider 23 A/B experiments conducted on real users of Yandex in order to validate our approach wrt several user engagement metrics . Each experiment ran for two weeks , the user samples used in these A/B tests were all uniformly randomly selected , and the control and the treatment groups were almost of the same sizes ( at least , hundreds of thousands of users ) . Each experiment evaluates a change in one of the main components of the search engine ( the ranking algorithm , computational infrastructure , or the user interface ) . Engagement measures . We study the following 6 popular engagement measures , which represent both loyalty and activity aspects of user engagement : the number of sessions ( S ) , the number of queries ( Q ) , the number of clicks ( C ) , the presence time ( PT ) , the number of clicks per query ( CpQ ) , and the absence time per session ( ATpS ) . They are defined at the user level in the same way as in [ 28 , 11 , 12 ] . A brief analysis of these measures , of the relationships between them and of their persistence across time could be found in [ 12 ] .
A/A tests . First of all , 18 A/A tests ( the same as in the previous section ) are utilized in order to check correctness of our evaluation criteria against the user engagement measures , as well . As in the previous section , an A/A test should be failed not more than in 5 % of the time for our pvalue threshold 0.05 [ 23 , 5 ] . The number of failed A/A tests for each metric and each UE measure is reported in Table 3 in three first columns . We see that the results of all methods for all user engagement measures are acceptable : the classical approach ( the mean value ) both with the two sample t test and with the Bootstrap test [ 14 ] fails one A/A test for the measures C , PT , and CpQ , while the Kolmogorov–Smirnov
Metric
Mean KS ODD Mean KS
ODD
S Q C PT CpQ ATpS
0 0 1 1 1 0
0 0 0 0 0 0
0 0 0 0 0 0
2 1 6 2 10 4
0 0 3 0 7 0
4 ( +3 ) 3 ( +3 ) 7 ( +2 ) 2 ( +2 ) 11 ( +1 ) 2 ( +1 ) test and the ODD approach with the Bootstrap test never fail any of A/A tests .
A/B tests . Table 3 summarizes the number of A/B experiments ( out of 23 ) that are passed ( ie , pval < 0.05 ) wrt each evaluation criterion and each UE measure ( the best result in each row is highlighted in boldface ) . We see that , first , the click measures ( ie , C and CpQ ) demonstrate the highest sensitivity among all UE measures ( both for the classical approach and the ODD one ) . This observation agrees with the findings of the study [ 11 ] ( ie , our experimentation reproduce them ) . Second , we see that the Kolmogorov– Smirnov test is very insensitive , it does not detect most of the treatment effects that were detected by the classical approach . Third , we see that our novel approach with bootstrap technique detects the treatment effect in a higher number of A/B experiments than the classical one for the number of sessions S , the number of queries Q , the number of clicks C , and the number of clicks per queries CpQ . However , it detects changes of the web service in a smaller number of experiments for the absence time per session measure ATpS . Therefore , we conclude that our novel approach is more sensitive than the baseline classical one wrt more than a half of user engagement measures . Finally , we see that our novel approach with bootstrap technique detects the treatment effect in those A/B experiments where the classical one does not detect for all user engagement measures ( the numbers in brackets in Table 3 ) . Therefore , we conclude that our novel evaluation criterion improves the sensitivity of the baseline one wrt all user engagement measures in case of combined utilization .
6 . CONCLUSIONS AND FUTURE WORK In our work , we addressed a novel problem of online evaluation based on the comparison of the two distributions of the key metric rather than their average values , which are commonly used . We developed a novel theoretically sound
853 ODD approach to comparison of the two distributions of the key metric : for treatment and control . This approach produces an Overall Evaluation Criterion , which is relative in the sense that it is assigned to the pair of the compared system variants , but not to each of them . Our OEC is both interpretable and more sensitive as compared to the standard average . We provided a thorough experimental analysis , and concluded that our OEC outperforms the classical average with different types of key metrics , for different components of a system , and for different types of updates in terms of sensitivity . Moreover , we also found that our OEC is also more sensitive than the standard Kolmogorov–Smirnov test , which evaluates only the fact of the difference .
Future work . The investigated approach has a very wide scope of applications , since it is applicable to different key metrics and systems . First , an experimentation with the sample size reduction could be conducted in order to thoroughly understand a practical utility of the sensitivity improvement to run more and faster online experiments . Second , one advantage of our model is its potential to handle multidimensional key metrics , so it will be definitely interesting to evaluate its ability to find and the latent effect variables in the multidimensional cases and use them to interpret the importance of each of the dimensions .
7 . REFERENCES [ 1 ] E . Bakshy and D . Eckles . Uncertainty in online experiments with dependent data : An evaluation of bootstrap methods . In KDD’2013 , pages 1303–1311 , 2013 .
[ 2 ] G . E . Box , J . S . Hunter , and W . G . Hunter . Statistics for experimenters : design , innovation , and discovery . AMC , 10:12 , 2005 .
[ 3 ] G . Buscher , L . van Elst , and A . Dengel . Segment level display time as implicit feedback : a comparison to eye tracking . In SIGIR’2009 , pages 67–74 , 2009 .
[ 4 ] S . Chakraborty , F . Radlinski , M . Shokouhi , and
P . Baecke . On correlation of absence time and search effectiveness . In SIGIR’2014 , pages 1163–1166 , 2014 . [ 5 ] T . Crook , B . Frasca , R . Kohavi , and R . Longbotham .
Seven pitfalls to avoid when running controlled experiments on the web . In KDD’2009 , pages 1105–1114 , 2009 .
[ 6 ] K . Dai . Modeling score distributions for information retrieval . 2012 .
[ 7 ] A . Deng and V . Hu . Diluted treatment effect estimation for trigger analysis in online controlled experiments . In WSDM’2015 , pages 349–358 , 2015 . [ 8 ] A . Deng , T . Li , and Y . Guo . Statistical inference in two stage online controlled experiments with treatment selection and validation . In WWW’2014 , pages 609–618 , 2014 .
[ 9 ] A . Deng , Y . Xu , R . Kohavi , and T . Walker . Improving the sensitivity of online controlled experiments by utilizing pre experiment data . In WSDM’2013 , 2013 .
[ 10 ] A . Drutsa . Sign aware periodicity metrics of user engagement for online search quality evaluation . In SIGIR’2015 , 2015 .
[ 11 ] A . Drutsa , G . Gusev , and P . Serdyukov . Engagement periodicity in search engine usage : Analysis and its application to search quality evaluation . In WSDM’2015 , pages 27–36 , 2015 .
[ 12 ] A . Drutsa , G . Gusev , and P . Serdyukov . Future user engagement prediction and its application to improve the sensitivity of online experiments . In WWW’2015 , pages 256–266 , 2015 .
[ 13 ] G . Dupret and M . Lalmas . Absence time and user engagement : evaluating ranking functions . In WSDM’2013 , pages 173–182 , 2013 .
[ 14 ] B . Efron and R . J . Tibshirani . An introduction to the bootstrap . CRC press , 1994 .
[ 15 ] U . Fano . Description of states in quantum mechanics by density matrix and operator techniques . Reviews of Modern Physics , 29(1):74–93 , 1957 .
[ 16 ] J . Jagarlamudi and P . N . Bennett . Fractional similarity : Cross lingual feature selection for search . In ECIR’2011 , pages 226–237 . 2011 .
[ 17 ] F . James . Statistical methods in experimental physics .
Singapore : World Scientific , 7(4 ) , 2006 .
[ 18 ] Y . Kim , A . Hassan , R . W . White , and I . Zitouni .
Modeling dwell time to predict click level satisfaction . In WSDM’2014 , pages 193–202 , 2014 .
[ 19 ] R . Kohavi , T . Crook , R . Longbotham , B . Frasca ,
R . Henne , J . L . Ferres , and T . Melamed . Online experimentation at microsoft . Data Mining Case Studies , page 11 , 2009 .
[ 20 ] R . Kohavi , A . Deng , B . Frasca , R . Longbotham ,
T . Walker , and Y . Xu . Trustworthy online controlled experiments : Five puzzling outcomes explained . In KDD’2012 , pages 786–794 , 2012 .
[ 21 ] R . Kohavi , A . Deng , B . Frasca , T . Walker , Y . Xu , and
N . Pohlmann . Online controlled experiments at large scale . In KDD’2013 , pages 1168–1176 , 2013 .
[ 22 ] R . Kohavi , A . Deng , R . Longbotham , and Y . Xu .
Seven rules of thumb for web site experimenters . In KDD’2014 , 2014 .
[ 23 ] R . Kohavi , R . Longbotham , D . Sommerfield , and R . M . Henne . Controlled experiments on the web : survey and practical guide . Data Min . Knowl . Discov . , 18(1):140–181 , 2009 .
[ 24 ] J . Lehmann , M . Lalmas , G . Dupret , and
R . Baeza Yates . Online multitasking and user engagement . In CIKM’2013 , pages 519–528 , 2013 .
[ 25 ] R . H . Lopes . Kolmogorov smirnov test . In
International Encyclopedia of Statistical Science , pages 718–720 . Springer , 2011 .
[ 26 ] E . T . Peterson . Web analytics demystified : a marketer ’s guide to understanding how your web site affects your business . Ingram , 2004 .
[ 27 ] A . Shishkin , P . Zhinalieva , and K . Nikolaev .
Quality biased ranking for queries with commercial intent . In WWW’2013 , pages 1145–1148 , 2013 .
[ 28 ] Y . Song , X . Shi , and X . Fu . Evaluating and predicting user engagement change with degraded search relevance . In WWW’2013 , pages 1213–1224 , 2013 . [ 29 ] D . Tang , A . Agarwal , D . O’Brien , and M . Meyer .
Overlapping experiment infrastructure : More , better , faster experimentation . In KDD’2010 , pages 17–26 , 2010 .
[ 30 ] R . W . White and D . Kelly . A study on the effects of personalization and task information on implicit feedback performance . In CIKM’2006 , pages 297–306 , 2006 .
854
