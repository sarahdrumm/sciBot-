On Estimating the Swapping Rate for Categorical Data
Daniel Kifer
Department of Computer Science & Engineering
Penn State University
ABSTRACT it is important to account for all When analyzing data , sources of noise . Public use datasets , such as those provided by the Census Bureau , often undergo additional perturbations designed to protect confidentiality . This source of noise is generally ignored in data analysis because crucial parameters and details about its implementation are withheld . In this paper , we consider the problem of inferring such parameters from the data . Specifically , we target data swapping , a perturbation technique commonly used by the US Census Bureau and which , barring practical breakthroughs in disclosure control , will be used in the foreseeable future . The vanilla version of data swapping selects pairs of records and exchanges some of their attribute values . The number of swapped records is kept secret even though it is needed for data analysis and investigations into the confidentiality protection of individual records . We propose algorithms for estimating the number of swapped records in categorical data , even when the true data distribution is unknown .
Categories and Subject Descriptors G.3 [ Probability and Statistics ] : Contingency table analysis
Keywords data swapping
1 .
INTRODUCTION
Organizations that intend to publicly release data must contend with the possibility of embarrassment [ 25 , 26 , 2 ] and other undesirable consequences [ 4 , 8 ] if their data release reveals confidential information about specific individuals . Thus , they are turning to the release of perturbed data .
These perturbed datasets pose a challenge for statistical analysis since a valid analysis must account for the new types of errors that are introduced . In some cases , the perturbations act globally on the data to produce a sequence of records that are no longer iid In other cases , the full details of the perturbation scheme are not publicly revealed . This “ security by obscurity ” is often used as a stopgap because current advances in privacy technology [ 13 ] have not always caught up to practical requirements [ 5 , 6 , 29 ] . Security by obscurity also poses a challenge to data analysis : how can we analyze data sets if we do not know what was done to them ? .
Of particular importance is data swapping , a workhorse technique used by the US Census Bureau since 1990 [ 5 , 6 , 29 ] ( and for the foreseeable future ) to release microdata while protecting sensitive information . There are countless variations [ 14 , 9 , 24 ] . For concreteness , we consider the following vanilla version : given a file with N records , select ρN pairs of records ( ρ is the data swap rate ) and exchange some attribute values between pairs of records . Although data swapping may adversely affect statistical analysis [ 17 , 15 ] , the swapping rate ρ is kept secret , making it difficult to account for this perturbation when analyzing Census data . If the true data distribution were known , estimating the number of swaps is a fairly easy problem.1 In practice , neither the true data distribution nor the amount of perturbation is known , and so both must be simultaneously estimated from perturbed data . Learning about the data distribution can help assess which records were swapped , and this assessment can help us improve our estimate of the data distribution . However , note that errors can propagate : errors in estimating the true distribution will lead to incorrect assessments about which records were swapped ( and vice versa ) . We tackle this challenge for categorical ( discrete ) datasets . To see some of the difficulties involved , consider two alternative ways of modeling categorical data : an unconstrained multinomial joint distribution vs . a more constrained distribution with fewer parameters ( such as graphical models [ 20] ) . In the unconstrained model , if there are r variables , each with v possible values , then there are vr − 1 parameters to estimate . The curse of dimensionality renders this impractical as we will not have enough data for reliable estimates of the model parameters .
Now consider models , such as graphical models , that use fewer parameters . For classical tasks such as classification , they tend to work well even though they are often unrealistic models of the data [ 12 ] . However , for our purposes , inaccurate models of the data could lead to incorrect estimates of the likelihood of different records being swapped , which can lead to a wildly incorrect estimate of the swap rate .
1For example , it can be handled by Algorithm 1 in Section 4.1 by removing the lines dealing with parameter estimation .
557 D
Original data set
The dataset output by data swapping
D ( ri,si ) A record in D ( note swapping only affects
( ri , si ) A data record in D with attribute vector ri and swap attribute si the swap attribute ) The jthcomponent of ri number of records in D ri[j ] N n0 , n1 Number of records in D with si = 0 , si = 1
ρ α Mθ
Data swapping parameter Group swap count A probabilistic model of the data with pa| ri ) is its rameter vector θ . Mθ(s = si estimate of the probability that s = si for record i .
Table 1 : Table of Notation
As a result of these considerations , we develop a two stage estimation algorithm that first builds simplified models from the perturbed data and then corrects their biased estimate of the data swapping rate . Empirical results on real and synthetic data justify our approach .
To the best of our knowledge , this difficult problem has not been studied before . In Section 2 , we formalize the problem and introduce notation . We discuss related work in Section 3 . We present algorithms for constructing parametric models from swapped data and correcting their biased estimate of the swap rate in Section 4 . Extensive experiments in Section 5 are used to validate our approach . We discuss conclusions and future work in Section 6 .
2 . PROBLEM SETUP AND NOTATION
Let D = {(r1 , s1 ) . . . , ( rN , sN )} be a dataset of N records , where each record has a multidimensional attribute r and another attribute s called the swap attribute . We use r[j ] to refer to its jthcomponent . The notation used in this paper is summarized in Table 1 . Without loss of generality , we assume that s is binary valued ( if not , its possible values can always be partitioned into 2 groups , see Section 413 ) We now define the data swapping variation studied in this paper . Note that the actual Census Bureau data swapping algorithm [ 29 ] adds additional complications ( see Section 3 ) that will be studied in future work .
Definition 2.1
( ρ Data swap ) . Given a parameter ρ ∈ [ 0 , 1/2 ) ( called the data swap rate ) , a ρ data swap on a data set D = {(r1 , s1 ) , . . . , ( rN , sN )} is a procedure that randomly selects ρN pairs of records from D . For each pair of records ( ri , si ) , ( rj , sj ) , it interchanges their s values . The resulting list of records is called the swapped dataset and is repre sented as D = {(r1,s1 ) , . . . , ( rN ,sN )} . This process is illus trated in Figure 1 .
Instead of directly estimating ρ , the main goal is to estimate a more informative property called the group swap count .
Definition 2.2 set D and its swapped version D , the group swap count α is
( Group swap count α ) . Given a data
D
˜r r1 r2 r3 r4 r5 r6 r7 r8 r9 r10 s 0 1 1 0 1 0 1 1 1 1
D
˜r r1 r2 r3 r4 r5 r6 r7 r8 r9 r10 s
1 1 0 0 0 1 1 1 1 1
Figure 1 : Data swapping visualized . The number of swaps is ρN = 3 and the group swap count is α = 2 . The number of records with s = 0 is n0 = 3 and the number of records with s = 1 is n1 = 7 . Note that the swap for records 8 and 10 has no practical effect . the number of records whose s attribute was changed from 0 to 1 by the swapping procedure.2
Lemma 23 For N records , the expected group count E[α ] from a ρ data swap is E[α ] = ρ 2n0n1 N−1 , where n0 ( resp . n1 ) is the number of records with s = 0 ( resp . s = 1 ) . Thus , ρ can be estimated from α as ˆρ = α(N−1 )
.
2n0n1
Note that when pairs of records ( ri , si ) , ( rj , sj ) are swapped ,
( ri,si ) , ( rj,sj ) are only different from these input records there is no noticeable change if si = sj ( eg , records 8 and 10 in Figure 1 ) . The corresponding output records if si = sj ( eg , records 1 and 3 in Figure 1 ) . This observation leads to another interpretation of the ρ data swapping procedure : the group swap count α determines the actual amount of perturbation while ρ induces a probability distribution P ( α | ρ ) on α ; hence the swap procedure is equivalent to sampling α from this distribution and then choosing α pairs of records having differing s values and interchanging them .
The group swap count α , which measures the actual amount of perturbation , is therefore more informative than the data swap rate ρ . For this reason ( and because ρ can be estimated from α via Lemma 2.3 ) , we focus our attention on algorithms for estimating the group swap count α for the rest of the paper .
3 . RELATED WORK
Despite theoretical advances in privacy preserving data publishing , such as differential privacy [ 13 ] , there is still a gap between the data utility that current theory provides and the data utility that applications require . As a result statistical disclosure techniques with weak privacy guarantees ( eg , data swapping ) are in heavy use [ 29 , 14 ] as compared to differential privacy [ 23 ] .
Data swapping parameters and other implementation details are often withheld to nominally increase its security properties . As a result , users of swapped data cannot modify their analyses to account for this perturbation and risk generating invalid statistical conclusions . Consequently , the utility of swapped data is often measured by the quality of a model naively trained on it ( ie without accounting for the added noise ) [ 17 , 15 ] . To the best of our knowledge , no prior work tried to estimate the data swapping parameter . Data swapping techniques are surveyed and critiqued in [ 14 ] . In general , there are three main variations . The first 2By the definition of data swapping , α is also the number of records whose s attribute was changed from 1 to 0
558 variation [ 15 , 28 ] , also used in this paper , swaps predefined attributes between randomly chosen pairs of records . The second variation adds restrictions that certain statistics ( like marginal counts ) must remain constant or change by a small amount [ 14 ] . The third main variation is used by the US Census Bureau [ 29 , 24 ] and is often employed when the swap attributes are continuous or spatial variables ( eg , Census blocks ) . Here , two records can only be paired for a swap if their swap attributes are close to each other ( eg , with a certain distance ) .
The techniques in this paper can also be viewed as attacks against privacy preserving technology . An interesting twist is that most prior work attempts to deduce information about the original records [ 22 , 7 ] while we aim to deduce properties of algorithm that produced the sanitized data .
From a technical standpoint , most of statistics deals with estimation of properties of the data along with properties of the noise . The emphasis in the current work is that the noise ( resulting from data swapping ) is not a modeling convenience but rather a real , empirical quantity that needs to be estimated accurately .
PAC ( probably approximately correct ) learning involves learning a hypothesis H from a concept class C from data where the labels have random [ 1 ] or malicious [ 18 ] noise . In the ( unrealistic ) realizable case , where the true concept belongs to C , the true amount of noise can be estimated accurately ( although that is not their main goal ) .
More modern problems with similar considerations are adversarial learning and crowd sourcing . In crowd sourcing , human workers provide noisy ( possibly malicious ) answers to a set of tasks and one of the goals is to estimate an aggregate or per worker reliability rate [ 16 , 11 , 27 ] . One of the difficulties of applying such methods to swap rate estimation ( each record could be thought of as a worker and swap variable as the worker ’s answer ) is that work on crowd sourcing leverages application specific redundancy – the level of agreement of different workers on the same task . In adversarial learning , an attacker , such as a spammer , manipulates data so that a learning algorithm will produce a classifier favorable to the attacker ( eg , it produces a spam filter that classifies the spammer ’s messages as legitimate ) . In such work , it is typical to use a game theoretic framework to estimate and defend against an attacker ’s best data manipulation strategies [ 10 , 30 ] .
4 . GROUP SWAP COUNT ESTIMATION
In this Section we provide algorithms for estimating the group swap count α . In Section 4.1 , we show how to do this for parametric models . It is worth noting that in cases where real data does not follow a simple parametric model , the estimates of α would be highly biased . To address this issue we present the Consensus algorithm , a method for correcting this bias ( by replacing the parametric assumption with a weaker independence assumption ) , in Section 42 4.1 Parametric Methods
Our approach allows users to plug in a parametric model of the data ( it does not have to be the true model ) . Let Mθ be a probabilistic model for the data D in which records are iid and let PMθ ( ri , si ) denote the probability of record ( ri , si ) under this model . θ is a vector of unknown parameters that are needed to fully specify the model . The estimation procedure , shown in Algorithm 1 , simultaneously trains the model Mθ ( ie , estimates a good value for θ ) while using it to estimate , for each record , the likelihood that the record was perturbed . The rationale and details for this algorithm are below .
Let n0 ( resp . n1 ) be the number of records withsi = 0 ( resp . si = 1).3 To simplify notation , rearrange the records s1 , . . . ,sN of the swapped data D so that 0 =s1 = ··· =sn0 and 1 =sn0+1 = ··· =sN . Conditioning on n0 and n1 , the PMθ ( rj | sj = 1 ) PM θ(rj | sj = 0 ) PMθ ( rj | sj = 1 ) n0 n PMθ ( ri | si = 0 ) α PMθ ( ri | si = 1 ) PMθ ( ri | si = 0 ) likelihood function of θ and the group swap count α is :
×
L(α , θ ) =
1 cα j=n0+1 i=1 i1,,iα j1,,jα
=1 where cα is the number of sets with α pairs of swaps such that every swap is between pairs of records ( i , j ) where i ∈ {1n0} and j ∈ {n0 + 1N} This likelihood function is intractable and is also difficult to optimize because α is an integer . Thus we relax the optimization criterion , as discussed next . 411 Relaxed Optimization Problem For tractability , we will perform maximum likelihood estimation using probabilities given by a relaxed version of the data swap procedure . This relaxed version has the following generative process for D = {(r1,s1 ) , . . . , ( rN ,sN )} : 2 . For each record with si = 0 , setsi = 1 with probability bution and sample ri from PMθ ( ri | si ) . α/n0 ( si = 0 otherwise ) . 3 . For each record with sj = 1 , setsj = 0 with probability α/n1 ( sj = 1 otherwise ) . 4 . Reorder data so that records withs = 0 appear first . is changed from 0 to 1 ( ie si = 0 andsi = 1 ) is not necessar
1 . For i = 1 , . . . , N , sample si from a Bernoulli( n1
In this relaxed process , the number of records whose s value ily α ; instead , it is a multinomial random variable whose expected value is α ( similarly for the number of records where s changes from 1 to 0 ) . Notably , it has removed the pairing between records : the change in a record ’s s value from 0 to 1 is not necessarily accompanied by a corresponding change in another record ’s s value from 1 to 0 ( it is only true in expectation).4
N ) distri
We use the likelihood of this relaxed process ( in place of the true likelihood ) to estimate the group swap count α . This relaxed likelihood ( after simplification ) is : n0 − α α
N n0 × N i=1 j=n0+1
N
L(cid:63)(α , θ ) =
PMθ ( ri| s = 0 ) +
PMθ ( ri| s = 1 )
α N
PMθ ( rj| s = 0 ) + n1 − α
N
PMθ ( rj| s = 1 )
412 Estimation algorithm for parametric models Given a user specified data model Mθ , we maximize this relaxed likelihood L(cid:63 ) using coordinate ascent . We optimize 3Note that n0 ( resp . n1 ) is also equal to the number of records in the original data D with si = 0 ( resp . si = 1 ) . 4One can think of it as records having their s values interchanged with records of individuals outside of our data set .
559 n0
α using Newton ’s method and the parameter vector θ of Mθ using EM . The pseudo code is shown in Algorithm 1 .
The components needed for Newton ’s method ( lines 14 16 in Algorithm 1 ) are provided by the following lemma .
Lemma 41 The natural log of the relaxed likelihood func tion L(cid:63 ) is concave in α and
∂ log L(cid:63 )
∂α
∂2 log L(cid:63 )
∂α2
PMθ ( ri| s = 1 ) − PMθ ( ri| s = 0 )
=
+ i=1 j=n0+1
( n0 − α)PMθ ( ri| s = 0 ) + αPMθ ( ri| s = 1 ) N PMθ ( rj| s = 0 ) − PMθ ( rj| s = 1 )
=− n0 − N
2 αPMθ ( rj| s = 0 ) + ( n1 − α)PMθ ( rj| s = 1 )
( n0 − α)PMθ ( ri| s = 0 ) + αPMθ ( ri| s = 1 ) PMθ ( rj| s = 0 ) − PMθ ( rj| s = 1 )
PMθ ( ri| s = 1 ) − PMθ ( ri| s = 0 )
αPMθ ( rj| s = 0 ) + ( n1 − α)PMθ ( rj| s = 1 ) i=1 j=n0+1 data likelihood is :
To apply the EM framework ( lines 4 13 in Algorithm 1 ) for optimizing θ , we introduce latent variable indicators z = ( z1 , . . . , zN ) so that zi = 0 if the s value of record i was not modified ( ie si =si ) , and zi = 1 otherwise . The complete n0 − α n0 × N
PMθ ( ri| s = 0 ) α zi n1 − α
1−zi α
PMθ ( ri| s = 1 )
PMθ ( rj| s = 1 )
PMθ ( rj| s = 0 ) zi i=1
N
N
1−zi
N j=n0+1
N
2
4
5 6
7
8
9 10
11
12 13
14 15 16 17 end while 18 Return ˆα and θ end while
Algorithm 1 : gsc( ) : //Group swap count α estimation . Input : Mθ : parametric probabilistic model of records
Input : D = {(r1,s1 ) , . . . , ( rN ,sN )} : swapped data
1 ˆα = 0.05 min(n0 , n1 ) 2 t = 0 3 while ˆα , θ have not converged do
// Init α to a small value // Initialize indicator variables
// Estimate θ and t with fixed ˆα while θ has not converged do
// M step : specific to Mθ . P ( s = 0 ) ← n0 θ ← mstep(t ) // E step : re estimate t for i = 1 , . . . , n0 do
// eg , see Example 4.2 ti =
( n0− ˆα)PMθ
( ri| s=1 )
ˆαPMθ ( ri| s=0 ) + ˆαPMθ
( ri| s=1 ) end for for j = n0 + 1 , . . . , N do ˆαPMθ ( rj | s=1 ) + ˆαPMθ
( n1− ˆα)PMθ
( rj| s=0 ) tj =
( rj| s=0 ) end for end while // Estimate ˆα for fixed θ ( Newton ’s method ) . while ˆα has not converged do
ˆα ← ˆα − dL(cid:63)( ˆα ) d ˆα / d2L(cid:63)( ˆα ) d ˆα2
// Using Lemma 4.1
EM algorithms work with the expected values of such indicator variables z since their true values are unknown ; we use t to represent the estimated expected value of z . The M step in the expectation maximization framework depends on the parametric model Mθ that we can choose to plug in . Our experiments use the well known Naive Bayes model , whose M step is described next .
Example 42 Consider the Naive Bayes model Mθ that uses s as the class attribute and where r has m components . If continuous components of r are modeled as Gaussian random variables and discrete components as multinomial random variables , the parameter vector θ consists of : • for components r[j ] with continuous domains , the parameters are µ0,j and σ0,j for modeling the density f ( r[j ] | s = 0 ) , as well as µ1,j and σ1,j for the density f ( r[j ] | s = 1 ) , • for components r[j ] with discrete domains , the parameters are values for P ( r[j ] = γ | s = 0 ) and P ( r[j ] = γ | s = 1 ) , for all possible values of γ .
These quantities can be computed from the vector t ( Lines 8 , 11 in Algorithm 1 ) as follows .
µ0,j← i=n0+1 ti i=n0+1 tiri[j ] i=1(1 − ti ) +N Ψ←n0 n0 i=1(1 − ti)ri[j ] +N n0 i=1(1 − ti)(ri[j ] − µ0,j )2 +N P ( r[j ] = γ | s = 0 ) ← 1 +n0 i=1(1 − ti)111{ri[j]=γ} N
Φ + size of domain of r[j ] i=n0+1 ti111{ri[j]=γ}
σ0,j←
Ψ
Ψ
+
Φ + size of domain of r[j ] i=n0+1 ti(ri[j ] − µ0,j )2
The quantities µ1,j , σ1,j , P ( r[j ] = γ | s = 1 ) are computed analogously .
413 Multi valued Swap Attributes In cases where the swap attribute s can take more than just two values , the estimation procedure in Algorithm 1 can still be applied with some preprocessing as follows . Split the domain of s into two groups A and B . For each record i , if si ∈ A , then set si = 0 and if si ∈ B then set si = 1 , then use this version of the data for group swap count estimation . We can repeat the estimation with a second split ( A , B ) of the domain of s . Concordance between the resulting estimators can be used as an indication of how reliable the estimate of the group swap count α is . 414 Assessing Model Reliability As our experiments show , Algorithm 1 works well when the chosen model Mθ fits the data well . In cases where Mθ is not an appropriate model for the data , the estimation procedure can be severely biased and the Consensus algorithm of Section 4.2 will need to be used to correct the bias .
How can we determine if Mθ is inappropriate if all we have is perturbed data with an unknown amount of perturbation ? A useful diagnostic is the vector t of values produced in Lines 8 and 11 of Algorithm 1 . Each component ti has a specific it is an estimate of the probarole in the EM framework : bility that the value si of record i was altered ( ie si =si ) . Thus n0 whose s value was changed5 from 1 to 0 and N 5Recall that for simplicity , records in the swapped data D were re ordered so that the n0 records withsi = 0 appear i=1 ti is the estimated expected number of records i=n0+1 ti first .
560 is the estimated expected number of records whose s value was changed from 0 to 1 . As seen in our experiments ( Section 5.2 ) , a large difference in these quantities indicates that model Mθ would have been a poor fit for the original data . If this is the case , the Consensus algorithm of Section 4.2 , should be used to improve the estimate of α . 4.2 The Consensus Algorithm
Recall that Algorithm 1 estimates the group swap count by simultaneously training a model Mθ while using it to detect perturbed records . The final result for that procedure is only as good as the model Mθ . If the choice is a poor model of the data , then the estimates can have significant errors . The question we address in this section is whether we can correct such errors without access to the true data or knowledge of the data swapping parameters .
Often , an unrealistic model Mθ for the data is used because a good class of models is not known or requires too many accurate parameter estimates ( for example , a multinomial distribution over binary categorical data with dimension d uses 2d − 1 parameters and hence suffers from the curse of dimensionality ) . Further , we conjecture that the group swap count α cannot be estimated without knowing ( or making some assumptions ) about properties of the data . The Consensus algorithm ( shown in Algorithm 2 ) replaces the assumption that the data distribution has a parametric form Mθ with a simpler independence assumption ( which we describe in this section ) . This allows it to correct biased estimates from Mθ .
First , split the non swap attribute vector r into two components rL and rR . The Consensus algorithm relies on two pluggable models MθL and MθR . MθL only uses the information in rL and let MθL ( a | rL i ) denote its estimated probability P ( si = a | rL i ) ( and similarly for MθR ) . Consider the quantities : c ≡N w ≡N c ≡N w ≡N
EL EL ER ER i=1 MθL ( si | rL i ) i=1 MθL ( 1 − si | rL i ) i=1 MθR ( si | rR i ) i=1 MθR ( 1 − si | rR i )
= N − EL c
= N − ER c
( 1 )
( 2 )
( 3 )
( 4 ) where , informally , EL c is the sum of probabilities MθL assigns to the correct value si of each record ( note that we therefore do not know EL w is the total score it assigns to the wrong value ( ie 1 − si ) for each record , etc . Also consider the joint versions of those quantities : c ) and EL
Ecc ≡N Ecw ≡N Ewc ≡N Eww ≡N Ecc ≡N Ecw ≡N Ewc ≡N Eww ≡N i=1 MθL ( si |rL i=1 MθL ( si |rL i=1 MθL ( 1 − si |rL i=1 MθL ( 1 − si |rL i=1 MθL ( si |rL i=1 MθL ( si |rL i=1 MθL ( 1 −si |rL i=1 MθL ( 1 −si |rL i ) × MθR ( si |rR i ) i ) × MθR ( 1 − si |rR i ) i ) × MθR ( si |rR i ) i ) × MθR ( 1 − si |rR i ) × MθR ( si |rR i ) i ) × MθR ( 1 −si |rR i ) × MθR ( si |rR i ) × MθR ( 1 −si |rR i ) i ) i ) i )
( 5 )
( 6 )
( 7 )
( 8 )
( 9 )
( 10 )
( 11 )
( 12 )
The last 4 quantities are defined over swapped data and hence can be computed . When we take expectations of these quantities with respect to the true unknown distribution of
D ( to get E[Ewc] ) , the following equations hold , by definition of data swapping and the group swap count α.6
1 − 2α N 1 − 2α N
E[Ecw ] +
E[Ecc ] = E[Ecw ] = E[Ewc ] = E[Eww ] =
2α N
2α N
E[Ecw ] +
2α N 1 − 2α N 1 − 2α N
E[Ecc ] +
2α N
E[Eww ]
( 13 )
E[Ewc ]
( 14 )
E[Ewc ]
( 15 )
E[Ecc ] +
E[Eww ]
( 16 )
We would like to solve for E[Ecc ] , E[Ewc ] , E[Ecw ] , E[Eww ] and α in terms of E[Ecc ] , E[Ewc ] , E[Ecw ] , E[Eww ] but we have one more variable than equations . This is where the choice of how to split r into rL and rR comes into play .
Using an idea inspired by co training [ 3 ] , we would like to choose the split of r into rL and rR so that MθR and MθL make uncorrelated predictions :
MθL ( s = a | rL ) × MθR ( s = b | rR ) = E[MθL ( s = a | rL ) ] × E[MθR ( s = b | rR ) ]
( 17 )
E where the expectation is with respect to the ( unknown ) record generating probability distribution . This requires some domain knowledge but technical results in Section 432 and experimental results in Section 5.4 suggest that the failure of the assumption in Equation 17 may be detectable . Applying Equation 17 to the definitions of EccEwc , Eww , Ecw , we get the following equalities . c ]/N
E[Ecc ] = E[EL E[Ecw ] = E[EL E[Ewc ] = E[EL E[Eww ] = E[EL c ] E[ER c ] E[ER w ] E[ER w ] E[ER w ]/N c ]/N w ]/N
( 18 )
( 19 )
( 20 )
( 21 )
Plugging Equations ( 1 ) – ( 4 ) and ( 18 ) – ( 21 ) into Equations
( 13 ) – ( 16 ) , we can solve for α in terms of E[Ecc ] , E[Ecw ] , E[Ewc ] and E[Eww ] .
α =
N 4
1 −
Lemma 43 For Equations ( 13 ) – ( 16 ) and ( 18)–(21 ) , if less than half of the records have been perturbed then the feasible solution for the group swap count α is :
E[Ecc]E[Eww ] − E[Ecw]E[Ewc ] E[Ecc ] + E[Eww ] − E[Ecw ] − E[Ewc ] To apply Lemma 4.3 , we note that E[Ecc ] , E[Ecw ] , E[Ewc ] and E[Eww ] can be estimated from MθL , MθR and the swapped data D using the empirical approximations ( Ecc , Ecw , Ewc , Eww ) . As for the models MθL , MθR , we can train
1 − 4 N them by using Algorithm 1 . The full Consensus algorithm is shown in Algorithm 2 . 4.3 Analysis of the Consensus Algorithm
In this section , we analyze the Consensus algorithm to determine some of its key properties , including its behavior when the models MθL ( · | rL ) and MθR ( · | rR ) are positively correlated .
6There are 2α records changing their s values .
561 Algorithm 2 : The Consensus Algorithm Input : rL and rR : a partition of the components of r i=1
4
AD−BC
1 − 4 into uncorrelated pieces using domain knowledge .
N ,sN )} N ,sN )}
3 θL ← gsc(M L 4 θR ← gsc(M R
Input : Mθ : probabilistic model with trainable param θ
θ , DL ) // train MθL with Algorithm 1 θ , DR ) // train MθR with Algorithm 1
Input : D = {(r1,s1 ) , . . . , ( rN ,sN )} : swapped data 1 DL ← {(rL 1 ,s1 ) , . . . , ( rL 2 DR ← {(rR 1 ,s1 ) , . . . , ( rR 5 A ←N // ≈ E[Ecc ] i=1 MθL ( si |ri ) × MθR ( si |ri ) 6 B ←N // ≈ E[Ecw ] i=1 MθL ( si |ri ) × MθR ( 1 −si |ri ) 7 C ←N // ≈ E[Ewc ] i=1 MθL ( 1 −si |ri ) × MθR ( si |ri ) 8 D ← N MθL ( 1 −si |ri ) × MθR ( 1 −si |ri ) // ≈ E[Eww ] 1 −
9 α ← N 10 Return α 431 Asymptotic Convergence of α/N estimator α/N from Algorithm 2 can be rewritten as
( EccEww − EcwEwc)/N 2 α ( Ecc + Eww − Ecw − Ewc)/N quantities such as Ewc/N ( defined in Equation 11 ) as long fact that Ewc is computed over swapped data D . The rate of convergence of quantities such as Ewc/N can be obtained
It is easy to see that the law of large numbers applies to as α/N → c ( for some constant c ) as N → ∞ and model training is consistent . The dependence on α is due to the
Let α/N denote the group swap rate – the fraction of the tuples that had their s values changed from 0 to 1 . The
A+D−B−C
1 − 4
=
N
1 −
1 4
( 22 )
N using Hoeffding bounds . The details are omitted ; instead , in the next section , we focus on a better understanding of how the estimator behaves in the limit and what happens when the assumptions we used to derive it are violated . The derivation of the consensus estimatora relied on the 432 Analysis of Correlated Models assumption that the models MθL ( · | rL ) and MθR ( · | rR ) made uncorrelated predictions . Let us consider what happens when this assumption fails . Equation 18 would need to be re written as E[Ecc ] = E[EL c ]/N + βN for some constant β > 0 which quantifies the average per record strength of the positive correlation ( recall that Ecc is a sum over N records so β must be multiplied by N ) . With this representation , we can express our estimator α/N with the It follows that the estimator α/N is asymptotically
Lemma 44 Express E[Ecc ] as E[Ecc ] = E[EL following asymptotically equivalent quantity . c ] E[ER c ] E[ER
βN .
γ+βN 2
, where γ =
1 −
1 − 8α(1−2α/N ) c ] − E[ER w ] )
N γ+4βN 2 equivalent to 1 4 c ] − E[EL
( E[EL w])(E[ER c ]/N +
Lemma 4.4 has several important implications . First , if the correlation β between models is 0 , simple algebra shows that the result is equivalent to α/N , so that our estimator is asymptotically unbiased .
N c ]−E[EL w ] )
Second , if β = 0 , its effect on our estimator is mitigated if it is small compared to α and γ/N 2 ( as defined in Lemma 44 ) This quantity γ/N 2 itself has an important interpretation . Note that ( E[EL is , by definition , the average difference in probability scores that model MθL assigns to the actual value of the swap attribute minus the incorrect value . It is thus a measure of how well the model MθL would have performed on the original data . Similarly , ( E[ER is a measure of how well the model MθR would have performed on the original data . The quantity γ/N 2 is simply the product of those two . The end result is that if both models are accurate and have modest correlation , then our estimate of the group swap rate will still be accurate . c ]−E[ER w ] )
N we take the swapped data D and perform additional data
The third important implication of Lemma 4.4 is that it may be possible to diagnose , using only swapped data , whether the unknown correlation between the models affects our estimates of α . Notice that the estimator has a nonlinear dependence on the correlation β . By contrast , if swapping over it ( with a new swap rate ρ ) , the new group swap count will change in a way that is approximately linear in ρ . Thus if the correlation β has little effect , our estimator should also change approximately linearly .
This idea leads to the following procedure . To detect if the correlation between models MθL and MθR are affecting our estimate , we should re swap the data several times and look at how the estimate changes . If the change appears linear , the correlation is having little effect and our estimator should be reliable . If the change is highly nonlinear , the correlation is causing a problem and we should seek a new split rL and rR ( of the non swap attributes r ) with which to build our models .
This effect can be seen in our experimental results in Sec tion 54
5 . EXPERIMENTS
We designed experiments in this paper to test all aspects of our group swap count predictions : what happens when the assumptions behind our algorithms are true , what happens when they are violated , and how can violations be detected . To this end , we used a combination of real , synthetic , and semi synthetic data in our experiments . We used : • The Adult dataset from the UCI repository [ 21 ] . We used the Salary Class attribute ( whose values are ≤ 50k and > 50k ) as the swap attribute . The non swap attributes r consist of Work Class , Relationship , Gender , Education , Marital Status , Occupation , Race ( we did not use the remaining attributes because they contained too many distinct values ) . To use the Consensus algorithm , we split r into rL ( containing the attributes Work Class , Relationship , Gender ) and rR ( containing the attributes Education , Marital Status , Occupation , Race ) . This split was chosen because the attributes in rL were used to build a Naive Bayes model in prior work [ 19 ] for other purposes . Thus there was no tuning for the choice of split . • The Non null Adult dataset . This is a version of the adult dataset in which records that contain an attribute with a missing value are removed . We use the same split rL and rR as in the original adult dataset . The rea
562 Figure 2 : Normalized estimation error ( ie absolute error /N ) of the group swap count from the Gaussian dataset with N = 10k , µ0 = 0 , µ1 = 1 . The estimates are very accurate until α/N ≈ 0.25 , a level at which half of the tuples in D have been perturbed .
Figure 3 : Normalized estimation error for the Gaussian dataset with µ0 = 0 and µ1 = 1 . N varies from 1 , 000 to 35 , 000 while α is fixed to 0.1N . Even before swapping is done , approximately 31 % of the points generated from Gaussian(µ0 ) are closer to µ1 .
θ and M R son that we consider two versions of the adult dataset is that records with null values represent a clear correlation ( data is not missing at random ) . Out of the 1843 records that had missing values for some attribute , 1836 had missing attributes in both rL and rR . Thus the original adult dataset contains correlations between rL and rR ( which thus creates correlations between the models M L θ built over those attributes ) while the Nonnull Adult dataset has this specific correlation removed . • A Semi Synthetic Adult dataset . We generated this dataset by building a Naive Bayes model from the Adult dataset ( with Laplace smoothing ) using Salary Class as the class variable . We then resampled data points from this model to create the semi synthetic dataset . Clearly this data set is modeled extremely well by a Naive Bayes model but the original Adult dataset is not modeled well ( thus we can test the effect of model mis specification ) . • A Gaussian Dataset . For a dataset of size N , the swap attribute s for half of the records is equal to 1 and for the rest s = 0 . The non swap attribute r has just one component – a Gaussian(µ0 , 1 ) random variable if s = 0 and Gaussian(µ1 , 1 ) if s = 1 . This again lets us evaluate how our algorithms behave when the true distribution has a known parametric form .
We first evaluate how well our estimation algorithm works when the data follows a nice parametric distribution in Section 51 In Section 5.2 we discuss how to diagnose the case when a chosen parametric distribution is a poor fit for the original data ( using only swapped data for the diagnosis ) . Then we evaluate our algorithms on real data ( without a nice parametric form ) in Section 53 Finally , in Section 5.4 we experimentally consider what happens when the assumptions behind the Consensus algorithm fail . As discussed in Section 432 ) , the group swap count estimate changes nonlinearly with the number of swaps . 5.1 Estimation and Parametric Distributions We now evaluate Algorithm 1 for inferring the group swap count α from data that follows a known parametric distri bution ( but with unknown parameters ) . The following are a representative sample of our results .
We generated the Gaussian dataset with N =10,000 records and mean parameters µ1 = 1 , µ0 = 0 . We varied α/N , the normalized group swap count , from 0 to 025 When the group swap count is 0.25N , half of the tuples are perturbed ( 0.25 N tuples change their swap attribute from 0 to 1 and 0.25N change their swap attribute from 1 to 0 ) and so at this extreme range we would expect unreliable performance . The results ( averaged over 10 runs ) are shown in Figure 2 – the estimation error is very low except until we reach the extreme ranges ( α/N ≥ 02 ) In practice , we would expect a group swap count much less than 0.2N ( otherwise the published data has too much perturbation and too little utility ) .
The next set of experiments evaluate how the results change with the size N of the data . Again we set µ1 = 1 and µ0 = 0 . We fixed α/N = 0.1 and varied N from 1 , 000 to 35 , 000 . For small data sizes , this is a difficult dataset because a Gaussian(0 , 1 ) random variable has around 31 % chance of being closer to 1 ( ie µ1 ) than 0 . On top of this error , α/N = 0.1 means that 20 % of the tuples have been perturbed . The error in estimating α is shown in Figure 3 ( averaged over 10 runs ) . Even in this difficult case , accuracy reaches acceptable levels at N = 10 , 000 with error/N ≈ 00082 What about parameter estimates ? Algorithm 1 needed to estimate µ0 , µ1 , σ0 , σ1 . The total error in estimating both means is shown in Figure 4 . It is instructive to compare this against a baseline that naively estimates µ0 as the mean r value for all records in the swapped data In the original data , 0.5N records had s = 0 and α ( ie 0.1N ) of those had their s value changed from 0 to 1 ( and similarly for records with s = 1 ) . Thus the naive estimate for µ0 would be 0.2 and the naive estimate for µ1 would be 08 The total parameter estimation error for this naive estimate would be 04 The estimates from Algorithm 1 significantly outperform this baseline . The corresponding experiment with the with s = 0 ( and similarly for µ1 ) .
563 semi synthetic adult data is shown in Figure 5 . The error in estimating the group swap count α is extremely low . 5.2 Diagnosing Inappropriate Model Choices It is important to understand when a parametric model used by Algorithm 1 is a poor choice for the original data so that the Consensus algorithm can be used to correct the estimate of α . In Section 414 , we noted that Algorithm
1 maintains a variable ti , one for each record ( ri,si ) , and si =si ) . Thus let that ti can be interpreted as the algorithm ’s estimate of the probability that record i has had its s value changed ( ie
A0 = ti
A1 = tj i:si=0 j:sj =1
Figure 4 : Total error in estimating µ0 , µ1 for the Gaussian dataset with µ0 = 0 and µ1 = 1 . N varies from 1 , 000 to 35 , 000 while α is fixed to 0.1N . The baseline error is obtained by naive estimation over the swapped data ( ie estimating µ0 as the average attribute value r of records withs = 0 ) .
Intuitively , A0 is the algorithm ’s estimate of the number of records whose s value was changed from 1 to 0 and A1 is the estimated number of records whose s value was changed from 0 to 1 . Let us refer to |A0 − A1|/N as the estimated imbalance . If the parametric model used by Algorithm 1 is good , we expect the estimated imbalance to be very small . If the model is inappropriate , it should be large . Estimated imbalance on synthetic and semi synthetic data are shown in Figures 6 , and 7 . In these cases , the model choice is appropriate since Algorithm 1 used Gaussian models ( but had to estimate parameters ) on Gaussian data ( Figure 6 ) and it used the Naive Bayes model ( and had to estimate its parameters ) on the semi synthetic adult dataset ( Figure 7 ) . The extremely low estimated imbalance indicates that the models used by Algorithm 1 are appropriate for the data .
Figure 5 : Normalized error in estimating α on semisynthetic adult data .
Figure 6 : Estimated imbalance on Gaussian data with the same setup as in Figure 3 .
Figure 7 : Estimated imbalance on semi synthetic adult data .
Now let us consider what happens when the models used by Algorithm 1 are not appropriate ( ie for real data ) . The results are shown in Figures 8 ( Adult data set ) and 9 ( nonnull Adult data set ) . In both cases , Algorithm 1 trained the Naive Bayes model with class Salary Class and features Work Class , Relationship , Gender . This model is a poor choice and is reflected in the estimated imbalance , which hovers at around 0.15 – this means that generally the algorithm did not provide consistent results between the number of s values changing from 0 to 1 and the number changing from 1 to 0 ; the discrepancy between the two is about 15 % of the records in the data .
564 Figure 8 : Estimated imbalance on adult data .
Figure 9 : Estimated imbalance on adult data without records having missing attribute values .
5.3 Estimation on Real data
In Figure 10 we vary the group swap rate α/N and mea sure the estimated error ofα/N on real data ( non null adult dataset ) . We partitioned the record attributes in rL and rR as described at the beginning of Section 5 . We plot the error of the Consensus algorithm . “ Naive Bayes 1 ” refers to the error of Algorithm 1 using the Naive Bayes model on rL and “ Naive Bayes 2 ” refers to the model using rR . As expected , Algorithm 1 performs poorly because Naive Bayes is not appropriate for this dataset , but the Consensus algorithm is able to correct this error . Note that we made no special attempt to optimize the choice of rL and rR – we simply used the attributes used by research on the same dataset but different topic [ 19 ] . 5.4 Diagnosing correlations in models used by
Consensus Algorithm
Finally , we consider what happens when rL and rR have correlations ( so that models built over them are also correlated ) . This is the case in the original Adult data set since the presence of missing values in records is highly correlated between rL and rR ( as discussed in the data set description at the beginning of Section 5 ) . The resulting estimates of
Figure 10 : Error of the Consensus algorithm compared to two Naive Bayes models trained by Algorithm 1 .
α from the Consensus algorithm and Algorithm 1 ( using a Naive Bayes model trained on rL and another trained on rR ) are shown in Figure 11 and the corresponding error is shown in Figure 12 . As anticipated , the correlation between rL and rR causes problems when the group swap count is small but has limited effect when the group swap count gets larger . The effect of correlation on our estimator is clearly nonlinear and can be detected by re swapping the ( swapped ) data and seeing how the estimator changes .
6 . CONCLUSIONS AND FUTURE WORK In this paper we presented algorithms for detecting how much perturbation was added to data via data swapping . The algorithms are accurate on real and synthetic data and provide means for diagnosing situations in which their assumptions fail to hold . Our future work includes simplifying , scaling , and extending these results to data swapping for spatial variables [ 29 ] as well as developing statistical techniques to adjust inferences made on swapped data .
7 . ACKNOWLEDGMENTS
This research was supported by NSF Award # 1054389 .
8 . REFERENCES [ 1 ] D . Angluin and P . Laird . Learning from noisy examples . Machine Learning , 2:343–370 , 1988 .
[ 2 ] M . Barbaro and T . Zeller . A face is exposed for AOL searcher no . 4417749 . New York Times , August 9 2006 .
[ 3 ] A . Blum and T . Mitchell . Combining labeled and unlabeled data with co training . In COLT , 1998 .
[ 4 ] T . Buley . Netflix settles privacy lawsuit , cancels prize sequel . Forbes , 2010 .
[ 5 ] Census 2010 special tabulations program http://wwwcensusgov/population/www/cen2010/ spec tab/stp_confidhtml
[ 6 ] Pums accuracy of the data ( 2010 ) http://wwwcensusgov/acs/www/Downloads/data_ documentation/pums/Accuracy/2010AccuracyPUMS . pdf .
565 Figure 11 : Estimated group swap count on adult dataset . Assumption violations are detected by a nonlinear dependence of the Consensus estimator as the number of swaps changes .
Figure 12 : Corresponding error of the Consensus algorithm and Algorithm 1 on a dataset that violates the assumptions behind the Consensus algorithm .
[ 7 ] B C Chen , D . Kifer , K . LeFevre , and
[ 19 ] D . Kifer . Attacks on privacy and deFinetti ’s theorem .
A . Machanavajjhala . Privacy preserving data publishing . Foundations and Trends in Databases , 2(1 2):1–167 , 2009 .
[ 8 ] G . Church , C . Heeney , N . Hawkins , J . de Vries ,
P . Boddington , J . Kaye , M . Bobrow , B . Weir , and P . Consortium . Public access to genome wide data : Five views on balancing research with privacy and protection . PLoS Genet , 5(10 ) , 10 2009 .
[ 9 ] T . Dalenius and S . P . Reiss . Data swapping : A technique for disclosure control . Journal of statistical planning and inference , 6(1):73–85 , 1982 .
[ 10 ] N . Dalvi , P . Domingos , Mausam , S . Sanghai , and
D . Verma . Adversarial classification . In KDD , 2004 .
[ 11 ] A . Dawid and A . Skene . Maximum likelihood estimation of observer error rates using the EM algorithm . Journal of the Royal Statistical Society . Series C ( Applied Statistics ) , 28(1 ) , 1979 .
[ 12 ] P . Domingos and M . Pazzani . On the optimality of the simple Bayesian classifier under zero one loss . Mach . Learn . , 29(2 3 ) , 1997 .
[ 13 ] C . Dwork , F . McSherry , K . Nissim , and A . Smith .
Calibrating noise to sensitivity in private data analysis . In Theory of Cryptography Conference , 2006 .
[ 14 ] S . E . Fienberg and J . McIntyre . Data swapping :
Variations on a theme by Dalenius and Reiss . In PSD , pages 14–29 , 2004 .
[ 15 ] S . Gomatam , A . Karr , and A . Sanil . Data swapping as a decision problem . Journal of Official Statistics , 21(4 ) , 2005 .
[ 16 ] D . R . Karger , S . Oh , and D . Shah . Iterative learning for reliable crowdsourcing systems . In NIPS , 2011 .
[ 17 ] A . Karr , C . Kohnen , A . Oganian , J . Reiter , and
A . Sanil . A framework for evaluating the utility of data altered to protect confidentiality . American Statistical Association , 60(3 ) , 2006 .
[ 18 ] M . Kearns and M . Li . Learning in the presence of malicious errors . In STOC , 1988 .
In SIGMOD , 2009 .
[ 20 ] S . L . Lauritzen . Graphical Models . Oxford Science
Publications , 1996 .
[ 21 ] M . Lichman . UCI machine learning repository , 2013 . [ 22 ] K . Liu , C . Giannella , and H . Kargupta . A survey of attack techniques on privacy preserving data perturbation methods . In C . Aggarwal and P . Yu , editors , Privacy Preserving Data Mining , pages 359–381 . Springer US , 2008 .
[ 23 ] A . Machanavajjhala , D . Kifer , J . Abowd , J . Gehrke , and L . Vilhuber . Privacy : From theory to practice on the map . In ICDE , 2008 .
[ 24 ] R . A . Moore . Controlled data swapping techniques for masked public use microdata sets . Technical report , US Bureau of the Census , Statistical Research Division , 1996 .
[ 25 ] A . Narayanan and V . Shmatikov . How to break anonymity of the netflix prize dataset . arxiv.org/pdf/cs/0610105 , 2006 .
[ 26 ] V . Pandurangan . On taxis and rainbows lessons from nyc ’s improperly anonymized taxi logs https://medium.com/@vijayp/ of taxis and rainbows f6bc289679a1 .
[ 27 ] J . Whitehill , P . Ruvolo , J . Bergsma , T . Wu , and
J . Movellan . Whose vote should count more : Optimal integration of labels from labelers of unknown expertise . In NIPS , 2009 .
[ 28 ] L . Willenborg and T . de Waal . Elements of Statistical
Disclosure Control . Springer Verlag , 2001 .
[ 29 ] L . Zayatz , J . Lucero , P . Massell , and A . Ramanayake .
Disclosure avoidance for census 2010 and american community survey five year tabular data products . Technical Report Research Report Series : Statistics #2009 10 , US Census Bureau , 2009 .
[ 30 ] Y . Zhou , M . Kantarcioglu , and B . Thuraisingham .
Adversarial support vector machine learning . In KDD , 2012 .
566
