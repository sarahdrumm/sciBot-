ClusType : Effective Entity Recognition and Typing by
Relation Phrase Based Clustering
Xiang Reny Ahmed El Kishkyy Chi Wangz Fangbo Taoy Clare R . Voss ⋆ Heng Ji ♯ Jiawei Hany y University of Illinois at Urbana Champaign , Urbana , IL , USA z Microsoft Research , Redmond , WA , USA
♯ Computer Science Department , Rensselaer Polytechnic Institute , USA
⋆ Information Sciences Directorate , Army Research Laboratory , Adelphi , MD , USA yfxren7 , elkishk2 , ftao2 , hanjg@illinois.edu z chiw@microsoft.com ⋆clarervossciv@mailmil ♯jih@rpi.edu
ABSTRACT Entity recognition is an important but challenging research problem . In reality , many text collections are from specific , dynamic , or emerging domains , which poses significant new challenges for entity recognition with increase in name ambiguity and context sparsity , requiring entity detection without domain restriction . In this paper , we investigate entity recognition ( ER ) with distant supervision and propose a novel relation phrase based ER framework , called ClusType , that runs data driven phrase mining to generate entity mention candidates and relation phrases , and enforces the principle that relation phrases should be softly clustered when propagating type information between their argument entities . Then we predict the type of each entity mention based on the type signatures of its co occurring relation phrases and the type indicators of its surface name , as computed over the corpus . Specifically , we formulate a joint optimization problem for two tasks , type propagation with relation phrases and multi view relation phrase clustering . Our experiments on multiple genres|news , Yelp reviews and tweets|demonstrate the effectiveness and robustness of ClusType , with an average of 37 % improvement in F1 score over the best compared method . Categories and Subject Descriptors H28 [ Database Management ] : Database applications| Data mining Keywords Entity Recognition and Typing ; Relation Phrase Clustering ;
1 .
INTRODUCTION
Entity recognition is an important task in text analysis . Identifying token spans as entity mentions in documents and labeling their types ( eg , people , product or food ) enables effective structured analysis of unstructured text corpus . The extracted entity information can be used in a variety of ways ( eg , to serve as primitives for information extraction [ 20 ] c⃝ 2015 ACM . ISBN 978 1 4503 3664 2/15/08 $1500
DOI : http://dxdoiorg/101145/27832582783362
Figure 1 : An example of distant supervision . and knowledge base ( KB ) population [ 2 ] . Traditional named entity recognition systems [ 18 , 15 ] are usually designed for several major types ( eg , person , organization , location ) and general domains ( eg , news ) , and so require additional steps for adaptation to a new domain and new types .
Entity linking techniques [ 21 ] map from given entity mentions detected in text to entities in KBs like Freebase [ 1 ] , where type information can be collected . But most of such information is manually curated , and thus the set of entities so obtained is of limited coverage and freshness ( eg , over 50 % entities mentioned in Web documents are unlinkable [ 11] ) . The rapid emergence of large , domain specific text corpora ( eg , product reviews ) poses significant challenges to traditional entity recognition and entity linking techniques and calls for methods of recognizing entity mentions of target types with minimal or no human supervision , and with no requirement that entities can be found in a KB . There are broadly two kinds of efforts towards that goal : weak supervision and distant supervision . Weak supervision relies on manually specified seed entity names in applying pattern based bootstrapping methods [ 7 , 9 ] or label propagation methods [ 24 ] to identify more entities of each type . Both methods assume the seed entities are unambiguous and sufficiently frequent in the corpus , which requires careful seed entity selection by human [ 10 ] . Distant supervision is a more recent trend , aiming to reduce expensive human labor by utilizing entity information in KBs [ 16 , 11 ] ( see Fig 1 ) . The typical workflow is : i ) detect entity mentions from a corpus , ii ) map candidate mentions to KB entities of target types , and iii ) use those confidently mapped fmention , typeg pairs as labeled data to infer the types of remaining candidate mentions .
1 , , ?K A J 6ANJ  D=I ? ?AH I MDAJDAH =>K EI = = O B 9=IDE CJ )KIJH= E= >A ? AI = ? IA = O B JDA 7 EJA@ 5J=JAI ! 0A D=I BBE?AI E 9=IDE CJ * IJ = @ 5= .H= ?EI ? " 6DA +=H@E = ME ID=HA JDA JEJ A MEJD += EB H E= EB JDA / @A *A=HI >A=J 9=IDE CJ =JAH 5=JKH@=O # )K>KH M JDA C= A !" & LAH JDA @ABA @E C =JE = ?D= FE I 6ANJ ? HFKI M A@CA >=IA 9=IDE CJ ! 9=IDE CJ " =JE = ?D= FE 7 EJA@ 5J=JAI )KIJH= E=" / @A *A=HI# )K>HK  =>K M JDA C= A !" & LAH>A=J>A ? AI = ? IA = O BEI = = O B/ LAH A J/ LAH A J/ LAH A J5F HJ JA= IF HJ JA= C LAH A J IF HJ JA= IF HJ JA= 995 In this paper , we study the problem of distantly supervised entity recognition in a domain specific corpus : Given a domainspecific corpus and a set of target entity types from a KB , we aim to effectively and efficiently detect entity mentions from that corpus , and categorize each by target types or Not Of Interest ( NOI ) , with distant supervision . Existing distant supervision methods encounter the following limitations when handling a large , domain specific corpus . ffl Domain Restriction : They assume entity mentions are already extracted by existing entity detection tools such as noun phrase chunkers . These tools are usually trained on general domain corpora like news articles ( clean , grammatical ) and make use of various linguistic features , but do not work well on specific , dynamic or emerging domains ( eg , tweets or restaurant reviews ) . ffl Name Ambiguity : Entity names are often ambiguous| multiple entities may share the same surface name . In Fig 1 , for example , the surface name \Washington" can refer to either the US government , a sport team , or the US capital city . However , most existing studies [ 22 , 9 ] simply output a type distribution for each surface name , instead of an exact type for each mention of the entity . ffl Context Sparsity : Previous methods have difficulties in handling entity mentions with sparse context . They leverage a variety of contextual clues to find sources of shared semantics across different entities , including keywords [ 24 ] , Wikipedia concepts [ 22 ] , linguistic patterns [ 16 ] and textual relations [ 11 ] . However , there are often many ways to describe even the same relation between two entities ( eg , \beat" and \won the game 34 28 over " in Fig 1 ) . This poses challenges on typing entity mentions when they are isolated from other entities or only share infrequent ( sparse ) context . We address these challenges with several intuitive ideas .
First , to address the domain restriction , we consider a domainagnostic phrase mining algorithm to extract entity mention candidates with minimal dependence of linguistic assumption ( eg , part of speech ( POS ) tagging requires fewer assumptions of the linguistic characteristics of a domain than semantic parsing ) . Second , to address the name ambiguity , we do not simply merge the entity mention candidates with identical surface names but model each of them based on its surface name and contexts . Third , to address the context sparsity , we mine relation phrases co occurring with the mention candidates , and infer synonymous relation phrases which share similar type signatures ( ie , express similar types of entities as arguments ) . This helps to form connecting bridges among entities that do not share identical context , but share synonymous relation phrases .
To systematically integrate these ideas , we develop a novel solution called ClusType . First , it mines both entity mention candidates and relation phrases by POS constrained phrase segmentation ; this demonstrates great cross domain performance ( Sec 31 ) Second , it constructs a heterogeneous graph to faithfully represent candidate entity mentions , entity surface names , and relation phrases and their relationship types in a unified form ( see Fig 2 ) . The entity mentions are kept as individual objects to be disambiguated , and linked to surface names and relation phrases ( Sec 32 34 ) With the heterogeneous graph , we formulate a graph based semi supervised learning of two tasks jointly : ( 1 ) type propagation on graph , and ( 2 ) relation phrase clustering . By clustering synonymous relation phrases , we can propagate types among entities bridged via these synonymous relation phrases . Conversely , derived entity argument types serve as good features for clustering relation phrases . These two tasks mutually enhance each other and lead to
Figure 2 : The constructed heterogeneous graph . quality recognition of unlinkable entity mentions . In this paper , we present an alternating minimization algorithm to efficiently solve the joint optimization problem , which iterates between type propagation and relation phrase clustering ( Sec 4 ) . To our knowledge , this is the first work to integrate entity recognition with textual relation clustering .
The major novel contributions of this paper are as follows : ( 1 ) we develop an efficient , domain independent phrase mining algorithm for entity mention candidate and relation phrase extraction ; ( 2 ) we propose a relation phrase based entity recognition approach which models the type of each entity mention in a scalable way and softly clusters relation phrases , to resolve name ambiguity and context sparsity issues ; ( 3 ) we formulate a joint optimization problem for clustering integrated type propagation ; and ( 4 ) our experiments on three datasets of different genres|news , Yelp reviews and tweets| demonstrate that the proposed method achieves significant improvement over the state of the art ( eg , 58.3 % enhancement in F1 on the Yelp dataset over the best competitor from existing work ) .
2 . PROBLEM DEFINITION The input to our proposed ER framework is a document collection D , a knowledge base ) with type schema T ) , and a target type set T fl T ) . In this work , we use the type schema of Freebase [ 1 ] and assume T is covered by Freebase .
An entity mention , m , is a token span in the text document which refers to a real world entity e . Let cm denote the surface name of m . In practice , people may use multiple surface names to refer to the same entity ( eg , \black mamba" and \KB " for Kobe Bryant ) . On the other hand , a surface name c could refer to different entities ( eg , \Washington" in Fig 1 ) . Moreover , even though an entity e can have multiple types ( eg , JFK airport is both a location and an organization ) , the type of its specific mention m is usually unambiguous . We use a type indicator vector ym 2 f0 ; 1gT to denote the entity type for each mention m , where T = jT j + 1 , ie , m has type t 2 T or is Not ofInterest ( NOI ) . By estimating ym , one can predict type of m as type ( m ) = argmax1iT ym;i .
Extracting textual relations from documents has been previously studied [ 4 ] and applied to entity typing [ 16 , 11 ] . A relation phrase is a phrase that denotes a unary or binary relation in a sentence [ 4 ] ( see Fig 3 for example ) . We leverage the rich semantics embedded in relation phrases to provide type cues for their entity arguments . Specifically , we define the type signature of a relation phrase p as two indicator vec2 RT . They measure how likely the left/right tors pL ; pR entity arguments of p belong to different types ( T or NOI ) . A large positive value on pL;t ( pR;t ) indicates that the left/right argument of p is likely of type t . Let M = fm1 ; :: : ; mMg denote the set of M candidate entity mentions extracted from D . Suppose a subset of entity
FF F"  ! ? ? ?! JEJO IKHB=?A = A4A =JE FDH=IA JEJO A JE A JE A JE HA =JE A JE = A HA =JE ABJ =HCK A J HA =JE A JE = A =FFE CJANJ ? ??KHHA ?AHECDJ =HCK A J HA =JE " # A JE ? HHA =JE F!IO O KI HA =JE FDH=IAI9=IDE CJ ! 9=IDE CJ %$ 9=IDE CJ EI = = O BD=I BBE?A =J =>K  =>K "# =>K = K ?A@A ? =HA996 EP : entity mention candidate ; RP : relation phrase . Figure 3 : Example output of candidate generation . mentions ML fl M can be confidently mapped to entities in ) . The type of a linked candidate m 2 ML can be obtained based on its mapping entity e(m ) ( see Sec 41 ) This work focuses on predicting the types of unlinkable candidate mentions MU = MnML , where MU may consist of ( 1 ) mentions of the emerging entities which are not in ) ; ( 2 ) new names of the existing entities in ) ; and ( 3 ) invalid entity mentions . Formally , we define the problem of distantly supervised entity recognition as follows ( Problem Definition ) . Given a document collection D , a target type set T and a knowledge base ) , our task aims to : ( 1 ) extract candidate entity mentions M from D ; ( 2 ) generate seed mentions ML with ) ; and ( 3 ) for each unlinkable candidate mention m 2 MU , estimate its type indicator vector ym to predict its type .
Definition 1
In our study , we assume each mention within a sentence is only associated with a single type t 2 T . We also assume the target type set T is given ( It is outside the scope of this study to generate T ) . Finally , while our work is independent of entity linking techniques [ 21 ] , our ER framework output may be useful to entity linking .
Framework Overview . Our overall framework is as follows : 1 . Perform phrase mining on a POS tagged corpus to extract candidate entity mentions and relation phrases , and construct a heterogeneous graph G to represent available information in a unified form , which encodes our insights on modeling the type for each entity mention ( Sec 3 ) . 2 . Collect seed entity mentions ML as labels by linking extracted candidate mentions M to the KB ) ( Sec 41 ) 3 . Estimate type indicator y for unlinkable candidate mention m 2 MU with the proposed type propagation integrated with relation phrase clustering on G ( Sec 4 ) .
3 . CONSTRUCTION OF GRAPHS
We first introduce candidate generation in Sec 3.1 , which leads to three kinds of objects , namely candidate entity mentions M , their surface names C and surrounding relation phrases P . We then build a heterogeneous graph G , which consists of multiple types of objects and multiple types of links , to model their relationship . The basic idea for constructing the graph is that : the more two objects are likely to share the same label ( ie , t 2 T or NOI ) , the larger the weight will be associated with their connecting edge .
Specifically , the constructed graph G unifies three types of links : mention name link which represents the mapping between entity mentions and their surface names , entity name relation phrase link which captures corpus level cooccurrences between entity surface names and relation phrase , and mention mention link which models distributional similarity between entity mentions . This leads to three subgraphs GM;C , GC;P and GM , respectively . We introduce the construction of them in Secs . 32{34 3.1 Candidate Generation
To ensure the extraction of informative and coherent entity mentions and relation phrases , we introduce a scalable , data driven phrase mining method by incorporating both corpus level statistics and syntactic constraints . Our
Table 1 : Performance on entity detection .
Method
NYT
Yelp
Tweet
Our method NP chunker
Prec 0.469 0.220
Recall 0.956 0.609
Prec 0.306 0.296
Recall 0.849 0.247
Prec 0.226 0.287
Recall 0.751 0.181 method adopts a global significance score to guide the filtering of low quality phrases and relies on a set of generic POS patterns to remove phrases with improper syntactic structure [ 4 ] . By extending the methodology used in [ 3 ] , we can partition sentences in the corpus into non overlapping segments which meet a significance threshold and satisfy our syntactic constraints . In doing so , entity candidates and relation phrases can be jointly extracted in an effective way . First , we mine frequent contiguous patterns ( ie , sequences of tokens with no gap ) up to a fixed length and aggregate their counts . A greedy agglomerative merging is then performed to form longer phrases while enforcing our syntactic constraints . Suppose the size of corpus D is N and the frequency of a phrase S is denoted by *(S ) . The phrasemerging step selects the most significant merging , by comparing the frequency of a potential merging of two consecutive phrases , *(S1 ( S2 ) , to the expected frequency assuming independence , N *(S1 ) N . Additionally , we conduct syntactic constraint check on every potential merging by applying an entity check function Ie( ) and a relation check function Ip( ) . Ie(S ) returns one if S is consecutive nouns and zero otherwise ; and Ip(S ) return one if S ( partially ) matches one of the patterns in Table 2 . Similar to Student ’s t test , we define a score function flX ( ) to measure the significance and syntactic correctness of a merging [ 3 ] , where X can be e ( entity mention ) or p ( relation phrase ) .
*(S2 )
N
√ *(S1 ( S2 ) , N *(S1 ) *(S1 ( S2 )
N flX ( S1 ; S2 ) =
*(S2 )
N
IX ( S1 ( S2 ) ( 1 )
At each iteration , the greedy agglomerative algorithm performs the merging which has highest scores ( fle or flp ) , and terminates when the next highest score merging does not meet a pre defined significance threshold . Relation phrases without matched POS patterns are discarded and their valid sub phrases are recovered . Because the significance score can be considered analogous to hypothesis testing , one can use standard rule of thumb values for the threshold ( eg , Z score2 ) [ 3 ] . Overall the threshold setting is not sensitive in our empirical studies . As all merged phrases are frequent , we have fast access to their aggregate counts and thus it is efficient to compute the score of a potential merging .
Fig 3 provides an example output of the candidate generation on New York Times ( NYT ) corpus . We further compare our method with a popular noun phrase chunker1 in terms of entity detection performance , using the extracted entity mentions . Table 1 summarizes the comparison results on three datasets from different domains ( see Sec 5 for details ) . Recall is most critical for this step , since we can recognize false positives in later stages of our framework , but no chance to later detect the misses , ie , false negatives . Table 2 : POS tag patterns for relation phrases .
Pattern
Example disperse ; hit ; struck ; knock ;
V P V P fi VW caused major damage on ; come lately V verb ; P prep ; W fadv | adj | noun | det | prong fi W denotes multiple W ; ( P ) denotes optional . locate in ; come from ; talk to ; in ; at ; of ; from ; to ;
( P )
3.2 Mention Name Subgraph
In practice , directly modeling the type indicator for each candidate mention may be infeasible due to the large num
1TextBlob : http://textblobreadthedocsorg/en/dev/
LAH 42 JDA MAA A @ JDA IOIJA 2 @H FFA@ 42 A=H O E ?DAI B I M E 42 MAIJAH =D = 2 = @ =J 42 ,= =I . HJ 9 HJD 1 JAH =JE = )EHF HJ 2 I AAJ = @ E?A ?=KIA@ 42 DK @HA@I B B ECDJ ?= ?A =JE I 2 = @ @A =OI 1J EI B HA?=IJ 42 J HA=?D 42 HJDAH /A HCE= 2 >O 42 6KAI@=O =BJAH 2 9=IDE CJ 2 = @ AM ; H 2 >O 42 9A@ AI@=O =BJAH 2 AJA H CEIJI 2 I=E@ 42 997 ter space to O(
Figure 4 : Example entity name relation phrase links from Yelp reviews . ber of candidate mentions ( eg , jMj > 1 million in our experiments ) . This results in an intractable size of parameter space , ie , O(jMjT ) . Intuitively , both the entity name and the surrounding relation phrases provide strong cues on the type of a candidate entity mention . In Fig 1 , for example , the relation phrase \beat" suggests \Golden Bears" can mention a person or a sport team , while the surface name \Golden Bears" may refer to a sport team or a company . We propose to model the type indicator of a candidate mention based on the type indicator of its surface name and the type signatures of its associated relation phrases ( see Sec 4 for details ) . By doing so , we can reduce the size of the paramewhere jCj+jPj ≪ jMj ( see Table 3 and Sec 51 ) This enables our method to scale up . Suppose there are n unique surface names C = fc1 ; :: : ; cng in all the extracted candidate mentions M . This leads to a biadjacency matrix ffC 2 f0 ; 1gM.n to represent the subgraph GM;C , where ffC;ij = 1 if the surface name of mj is cj , and 0 otherwise . Each column of ffC is normalized by its ℓ2norm to reduce the impact of popular entity names . We use a T dimensional type indicator vector to measure how likely an entity name is subject to different types ( T or NOI ) and denote the type indicators for C by matrix C 2 Rn.T . Similarly , we denote the type indicators for M by Y 2 RM.T . 3.3 Name Relation Phrase Subgraph
( jCj+jPj)T
)
By exploiting the aggregated co occurrences between entity surface names and their surrounding relation phrases across multiple documents collectively , we weight the importance of different relation phrases for an entity name , and use their connected edge as bridges to propagate type information between different surface names by way of relation phrases . For each mention candidate , we assign it as the left ( right , resp . ) argument to the closest relation phrase appearing on its right ( left , resp . ) in a sentence . The type signature of a relation phrase refers to the two type indicators for its left and right arguments , respectively . The following hypothesis guides the type propagation between surface names and relation phrases .
Hypothesis 1
( Entity Relation Co occurrences ) . If surface name c often appears as the left ( right ) argument of relation phrase p , then c ’s type indicator tends to be similar to the corresponding type indicator in p ’s type signature .
In Fig 4 , for example , if we know \pizza" refers to food and find it frequently co occurs with the relation phrase \serves up" in its right argument position , then another surface name that appears in the right argument position of \serves up" is likely food . This reinforces the type propagation that \cheese steak sandwich" is also food . Formally , suppose there are l different relation phrases P = fp1 ; :: : ; plg extracted from the corpus . We use two biadjacency matrices ffL ; ffR 2 f0 ; 1gM.l to represent the co occurrences between relation phrases and their left and right entity arguments , respectively . We define ffL;ij = 1 ( ffR;ij = 1 ) if mi occurs as the closest entity mention on the left ( right ) of pj in a sentence ; and 0 otherwise . Each
Figure 5 : Example mention mention links for entity surface name \White House" from Tweets . column of ffL and ffR is normalized by its ℓ2 norm to reduce the impact of popular relation phrases . Two bipartite subgraphs GC;P can be further constructed to capture the aggregated co occurrences between relation phrases P and entity names C across the corpus . We use two biadjacency matrices WL ; WR 2 Rn.l to represent the edge weights for the two types of links , and normalize them . WL = ffTC ffL and WR = ffTC ffR ; , 1 ( C ) , 1 2 R WRD
SL = D(C ) ∑ where SL and SR are normalized biadjacency matrices . For left argument relationships , we define the diagonal surface name degree matrix D(C ) 2 Rn.n as D(C ) ∑ l j=1 WL;ij and 2 Rl.l as D(P ) the relation phrase degree matrix D(P ) L;jj = 2 Rn.n and D(P ) i=1 WL;ij . Likewise , we define D(C ) 2 Rl.l based on WR for the right argument relationships . 3.4 Mention Correlation Subgraph
L WLD(P ) and SR = D
( P ) , 1 R
L;ii =
, 1 2
R
R
2
;
L
L
L
2 n
An entity mention candidate may have an ambiguous name as well as associate with ambiguous relation phrases . For example , \White House" mentioned in the first sentence in Fig 5 can refer to either an organization or a facility , while its relation phrase \felt" can have either a person or an organization entity as the left argument . It is observed that other co occurring entity mentions ( eg , \birth certificate" and \rose garden" in Fig 5 ) may provide good hints to the type of an entity mention candidate . We propose to propagate the type information between candidate mentions of each entity name based on the following hypothesis .
Hypothesis 2
( Mention correlation ) . If there exists a strong correlation ( ie , within sentence , common neighbor mentions ) between two candidate mentions that share the same name , then their type indicators tend to be similar . Specifically , for each candidate entity mention mi 2 M , ( jDj=*D(cj ) ) we extract the set of entity surface names which co occur with mi in the same sentence . An n dimensional TF IDF vector f ( i ) 2 Rn is used to represent the importance of these co occurring names for mi where f ( i ) with term frequency in the sentence *s(cj ) and document frequency *D(cj ) in D . We use an affinity subgraph to represent the mention mention link based on k nearest neighbor ( KNN ) graph construction [ 8 ] , denoted by an adjacency matrix WM 2 RM.M . Each mention candidate is linked to its k most similar mention candidates which share the same name in terms of the vectors f . if f ( i ) 2 Nk(f ( j ) ) or f ( j ) 2 Nk(f ( i ) ) and c(mi ) = c(mj ) ; otherwise . j = *s(cj ) log sim(f ( i ) ; f ( j) ) ;
WM;ij =
{
0 ;
( , ∥f ( i ) , f ( j)∥2=t ) where we use the heat kernel function to measure similarity , ie , sim(f ( i ) ; f ( j ) ) = exp with t = 5 [ 8 ] . We use Nk(f ) to denote k nearest neighbors of f and c(m ) to denote the surface name of mention m . Similarly , we , 1 normalize WM into SM = D 2M where the degree matrix DM 2 RM.M is defined by DM;ii =
, 1 2M WMD
∑ j=1 WM;ij .
M
6DEI F =?A 2 IAHLAI KF 42 JDA >AIJ ?DAAIA IJA= I= @ME?D 2 MAIJ B 42 JDA EIIEIIEFFE 2 . KH 2A= I 2 IAHLAI KF 42 I A >AAHI 2 = @ CHA=J A=JI 42 6DAO FH LE@A = @A?A J IA A?JE B 42 >AAHI 2 = @ DECD A @ ME AI 2 6 I B 42 F =?AI E JDA L= AO 2 E O AI 2 IAHLAI KF 42 C @ 21 ) 2 2E = 2 EI LAHO =LAH=CA 42 6DA KII= = 2 EI LAHO =LAH=CA 42 MEJD 42 B =L H 2 >AAHIME AIFE = KII= =IAHLAI KFFH LE@A = @A?A J IA A?JE BEI LAHO =LAH=CA?DAAIA IJA= I= @ME?D4A =JE FDH=IA JEJO IJHE C = A6ANJ + HFKIHECDJ =HCK A J ABJ =HCK A J5=@ J JDE 42 JDA 9DEJA 0 KIA 2 BA J 42 EJ D=H@ J HA A=IA 2 >EHJD ?AHJEBE?=JA 2 6DA 9DEJA 0 KIA 2 ANF =E I 42 JDA @A?EIE 2 J HA A=IA >= = 2 C B H >EHJD ?AHJEBE?=JA 2 +AHA O 2 EI ?=JA@ E 42 9DEJA 0 KIA 2 4 IA /=H@A 2 J D H M E?DA A >= = 2 J MHEJA > => KJ 42 9DEJA 0 KIA 2 H IA C=H@A 2 2HAIE@A J 2 BAJAI 42 5= .H= ?EI ? /E= JI 2 =J 42 JDA H IA C=H@A 2 9DEJA 0 KIA 2 6MAAJ ? A?JE %" 9DEJA 0 KIA !"# 9DEJA 0 KIA$#$& 9DEJA 0 KIA 9DEJA 0 KIA&' %' 9DEJA 0 KIA JEJO IKHB=?A = A 9DEJA 0 KIA >EHJD ?AHJEBE?=JA >= = H IA C=H@A 998 4 . CLUSTERING INTEGRATED TYPE
PROPAGATION ON GRAPHS
This section introduces our unified framework for joint type propagation and relation phrase clustering on graphs . A straightforward solution is to first perform hard clustering on the extracted relation phrases and then conduct type propagation between entity names and relation phrase clusters . Such a solution encounters several problems . One relation phrase may belong to multiple clusters , and the clusters so derived do not incorporate the type information of entity arguments . As such , the type prediction performance may not be best optimized by the mined clusters .
In our solution , we formulate a joint optimization problem to minimize both a graph based semi supervised learning error and a multi view relation phrase clustering objective . 4.1 Seed Mention Generation We first collect type information for the extracted mention candidates M by linking them to the KB . This yields a set of type labeled mentions ML . Our goal is then to type the remaining unlinkable mention candidates MU = M=ML .
We utilize a state of the art entity name disambiguation tool2 to map each candidate mention to Freebase entities . Only the mention candidates which are mapped with high confidence scores ( ie , 0:8 ) are considered as valid output . We denote the mapping entity of a linked mention m as e(m ) , and the set of types of e(m ) in Freebase as T ( m ) . The linked mentions which associate with multiple target types ( ie , jT ( m ) \ T j > 1 ) are discarded to avoid type ambiguity . This finally leads to a set of labeled ( seed ) mentions ML . In our experiments , we found that only a very limited amount of extracted candidate entity mentions can be confidently mapped to Freebase entities ( ie , jMLj=jMj < 7% ) . We define the type indicator ym for a linked mention m 2 ML as ym;t = 1 if T ( m ) \ T = ftg and 0 otherwise , for t 2 T . Meanwhile , ym;NOI is assigned with 1 if T ( m ) \ T = ∅ and 0 otherwise . 4.2 Relation Phrase Clustering
In practice , we observe that many extracted relation phrases have very few occurrences in the corpus . This makes it hard to model their type signature based on the aggregated cooccurrences with entity names ( ie , Hypothesis 1 ) . In our experimental datasets , about 37 % of the relation phrases have less than 3 unique entity surface names ( in right or left arguments ) in GC;P . Intuitively , by softly clustering synonymous relation phrases , the type signatures of frequent relation phrases can help infer the type signatures of infrequent ( sparse ) ones that have similar cluster memberships , based on the following hypothesis .
Hypothesis 3
( Type signature consistency ) . If two relation phrases have similar cluster memberships , the type indicators of their left and right arguments ( type signature ) tend to be similar , respectively .
There has been some studies [ 6 , 14 ] on clustering synonymous relation phrases based on different kinds of signals and clustering methods ( see Sec 6 ) . We propose a general relation phrase clustering method to incorporate different features for clustering , which can be integrated with the graphbased type propagation in a mutually enhancing framework , based on the following hypothesis .
Hypothesis 4
( Relation phrase similarity ) . Two relation phrases tend to have similar cluster memberships , if 2http://spotlightdbpediaorg/ they have similar ( 1 ) strings ; ( 2 ) context words ; and ( 3 ) left and right argument type indicators .
′ nc
In particular , type signatures of relation phrases have proven very useful in clustering of relation phrases which have infrequent or ambiguous strings and contexts [ 6 ] . In contrast to previous approaches , our method leverages the type information derived by the type propagation and thus does not rely strictly on external sources to determine the type information for all the entity arguments . Formally , suppose there are ns ( nc ) unique words fw1 ; :: : ; wns ( fw g ) in all the relation phrase strings ( contexts ) . ′ 1 ; :: : ; w We represent the strings and contexts of the extracted relation phrases P by two feature matrices Fs 2 Rl.ns and Fc 2 Rl.nc , respectively . We set Fs;ij = 1 if pi contains the word wj and 0 otherwise . We use a text window of 10 words to extract the context for a relation phrase from each sentence it appears in , and construct context features Fc based on TF IDF weighting . Let PL ; PR 2 Rl.T denote the type signatures of P . Our solution uses the derived features ( ie , fFs ; Fc ; PL ; PRg ) for multi view clustering of relation phrases based on joint non negative matrix factorization , which will be elaborated in the next section . 4.3 The Joint Optimization Problem Our goal is to infer the label ( type t 2 T or NOI ) for each unlinkable entity mention candidate m 2 MU , ie , estimating Y . We propose an optimization problem to unify two different tasks to achieve this goal : ( i ) type propagation over both the type indicators of entity names C and the type signatures of relation phrases fPL ; PRg on the heterogeneous graph G by way of graph based semi supervised learning , and ( ii ) multi view relation phrase clustering . The seed mentions ML are used as initial labels for the type propagation . We formulate the objective function as follows . Off;fl ; = F(C ; PL ; PR ) + Lff + Ωfl;(Y ; C ; PL ; PR ) :
PL ; PR;fU(v ) ; V(v)g ; U
( 2 ) The first term F follows from Hypothesis 1 to model type propagation between entity names and relation phrases . By extending local and global consistency idea [ 8 ] , it ensures that the type indicator of an entity name is similar to the type indicator of the left ( or right ) argument of a relation phrase , if their corresponding association is strong . F ( C ; PL ; PR ) =
, PZ;j√ flflflflfl Ci√ flflflflfl2
∑ n∑ l∑ fi )
WZ;ij
( i=1
Z;ii j=1
Z;jj
D(C )
Z2fL;Rg
D(P ) ( 3 ) The second term Lff in Eq ( 2 ) follows Hypotheses 3 and 4 to model the multi view relation phrase clustering by joint non negative matrix factorization . In this study , we consider each derived feature as one view in the clustering , ie , fF(0 ) ; F(1 ) ; F(2 ) ; F(3)g = fPL ; PR ; Fs ; Fcg and derive a fourview clustering objective as follows . Lff
( d∑ PL ; PR;fU(v ) ; V(v)g ; U fi ) ( ∥F(v ) , U(v)V(v)T∥2
F + ff∥U(v)Q(v ) , U fi∥2
F fi(v )
)
( 4 )
;
2
: g
= v=0
The first part of Eq ( 4 ) performs matrix factorization on each feature matrix . Suppose there exists K relation phrase clusters . For each view v , we factorize the feature matrix for all
F(v ) into a cluster membership matrix U(v ) 2 Rl.K0 relation phrases P and a type indicator matrix V(v ) 2 RT.K0 for the K derived clusters . The second part of Eq ( 4 ) enforces the consistency between the four derived cluster membership matrices through a consensus matrix U fi 2 Rl.K0 ,
999 T kk =
∑ i=1 V ( v ) which applies Hypothesis 4 to incorporate multiple similarity measures to cluster relation phrases . As in [ 13 ] , we normalize fU(v)g to the same scale ( ie , ∥U(v)Q(v)∥F 1 ) with the diagonal matrices fQ(v)g , where Q(v ) ik =∥F(v)∥F , so that they are comparable under the same consensus matrix . A tuning parameter ff 2 [ 0 ; 1 ] is used to control the degree of consistency between the cluster membership of each view and the consensus matrix . ffi(v)g are used to weight the information among different views , which will be automatically estimated . As the first part of Eq ( 4 ) enforces fU(0 ) ; U(1)g U and the second part of Eq ( 4 ) imposes PL U(0)V(0)T and PR U(1)V(1)T , it can be checked j implies both PL;i PL;j and PR;i PR;j for fi fi that U i any two relation phrases , which captures Hypothesis 3 . The last term Ωfl ; in Eq ( 2 ) models the type indicator for each entity mention candidate , the mention mention link and the supervision from seed mentions .
U fi flflY , f ( ffCC ; ffLPL ; ffRPR ) flfl2 flflflflfl Yi√ , Yj√ D(M ) flflflflfl2
+ ∥Y , Y0∥2 F :
D(M )
F
2 jj ii
( 5 )
Ωfl;(Y ; C ; PL ; PR ) =
M∑ i;j=1
+ fl 2
WM;ij
In the first part of Eq ( 5 ) , the type of each entity mention candidate is modeled by a function f ( ) based on the the type indicator of its surface name as well as the type signatures of its associated relation phrases . Different functions can be used to combine the information from surface names and relation phrases . In this study , we use an equalweight linear combination , ie , f ( X1 ; X2 ; X3 ) = X1+X2+X3 . The second part follows Hypothesis 2 to model the mentionmention correlation by graph regularization , which ensures the consistency between the type indicators of two candidate mentions if they are highly correlated . The third part enforces the estimated Y to be similar to the initial labels from seed mentions , denoted by a matrix Y0 2 RM.T ( see Sec 41 ) Two tuning parameters fl ; 2 [ 0 ; 1 ] are used to control the degree of guidance from mention correlation in GM and the degree of supervision from Y0 , respectively . To derive the exact type of each candidate entity mention , we impose the 0 1 integer constraint Y 2 f0 ; 1gM.T and Y1 = 1 . To model clustering , we further require the cluster membership matrices fU(v)g , the type indicator matrices of the derived clusters fV(v)g and the consensus matrix U to be non negative . With the definition of O , we define the joint optimization problem as follows . fi
Off;fl ;
( 6 ) min fi Y ; C ; PL ; PR ; U fU(v ) ; V(v ) ; fi(v)g Y 2 f0 ; 1gM.T fU(v ) ; V(v)g 0 ; ,fi(v )
∑ s:t :
∑
; Y1 = 1 ; U d v=0 e fi 0 ; ,fi(v )
= 1 ; d v=0 e
= 1 is used for avoiding trivial solution , where ie , solution which completely favors a certain view . 4.4 The ClusType Algorithm
The optimization problem in Eq ( 6 ) is mix integer programming and thus is NP hard . We propose a two step approximate solution : first solve the real valued relaxation of Eq ( 6 ) which is a non convex problem with Y 2 RM.T ; then impose back the constraints to predict the exact type of each candidate mention mi 2 MU by type ( mi ) = argmax Yi .
Directly solving the real valued relaxation of Eq ( 6 ) is not easy because it is non convex . We develop an alternating minimization algorithm to optimize the problem with respect to each variable alternatively , which accomplishes two tasks iteratively : type propagation on the heterogeneous graph , and multi view clustering of relation phrases . First , to learn the type indicators of candidate entity mentions , we take derivative on O with respect to Y while fixing other variables . As links only exist between entity mentions sharing the same surface name in WM , we can efficiently estimate Y with respect to each entity name c 2 C . Let Y(c ) and S(c)M denote the sub matrices of Y and SM , which correspond to the candidate entity mentions with the name ) c , respectively . We have the update rule for Y(c ) as follows : ; 8c 2 C ; ( 7 )
( 1 + fl + )Ic , flS(c)M
.(c ) + Y(c )
],1
Y(c ) =
(
[
0
0 where . = ffCC + ffLPL + ffRPR . Similarly , we denote .(c ) and Y(c ) as sub matrices of . and Y0 which correspond to the candidate mentions with name c , respectively . It can be shown that [ (1+fl+)Ic,flS(c)M ] is positive definite given > 0 and thus is invertible . Eq ( 7 ) can be efficiently computed since the average number of mentions of an entity name is small ( eg , < 10 in our experiments ) . One can further parallelize this step to reduce the computational time .
Second , to learn the type indicators of entity names and the type signatures of relation phrases , we take derivative on
O with respect to C , PL and PR while fixing other variables , ] ]
] [ leading to the following closed form update rules . [ SLPL + SRPR + ffTC ( Y , ffLPL , ffRPR ) [ ,1 0 ,1 1
L(Y , ffCC , ffRPR ) + fi(0)U(0)V(0)T R(Y , ffCC , ffLPL ) + fi(1)U(1)V(1)T
LC + ffT ST ST RC + ffT
PR = X
PL = X
C =
( 8 )
1 2
;
;
;
LffL and ffT
RffR are diagonal matrices . where we define X0 = [ (1 + fi(0))Il + ffT LffL ] and X1 = [ (1 + fi(1))Il +ffT RffR ] respectively . Note that the matrix inversions in Eq ( 8 ) can be efficiently calculated with linear complexity since both ffT Finally , to perform multi view clustering , we first optimize Eq ( 2 ) with respect to fU(v ) ; V(v)g while fixing other variand ffi(v)g by fixing fU(v ) ; V(v)g ables , and then update U and other variables , which follows the procedure in [ 13 ] . We first take the derivative of O with respect to V(v ) and apply Karush Kuhn Tucker complementary condition to impose the non negativity constraint on it , leading to the multiplicative update rules as follows : fi
V ( v ) jk = V ( v ) jk
[ F(v)T U(v)]jk + ff jk + ff i=1 U ( v)2 ik l
∆(v )
( ∑
∑ )(∑ l i=1 U T
) ; fi ikU ( v ) i=1 V ( v ) ik ik
( 9 ) where we define the matrix ∆(v ) = V(v)U(v)T U(v)+F(v ) U(v ) . It is easy to check that fV(v)g remains non negative after each update based on Eq ( 9 ) . We then normalize the column vectors of V(v ) and U(v ) by V(v ) = V(v)Q(v),1 and U(v ) = U(v)Q(v ) . Following similar procedure for updating V(v ) , the update rule for U(v ) can be derived as follows :
,
[ F(v)+
V(v ) + ffU
]ik fi ik
,
: ( 10 )
V(v ) + ffU(v)]ik
[ U(v)V(v)T V(v ) + F(v ) , ik = U ( v ) U ( v ) In particular , we make the decomposition F(v ) = F(v)+ , , ij = ( jAijj , Aij )=2 , F(v ) in order to preserve the non negativity of fU(v)g . The proposed algorithm optimizes fU(v ) ; V(v)g for each view v , by iterating between Eqs . ( 9 ) and ( 10 ) until the following reconstruction error converges . ij = ( jAijj + Aij )=2 and A
, where A+ ffi(v ) = ∥F(v ) , U(v)V(v)T∥2
F + ff∥U(v)Q(v ) , U fi∥2
F
( 11 )
1000 Algorithm 1 The ClusType algorithm biadjacency matrices fffC ; ffL ; ffR ; WL ; WR ; WMg , Input : clustering features fFs ; Fcg , seed labels Y0 , number of clusters K , parameters fff ; fl ; g RY0g , 1 : Initialize fY ; C ; PL ; PRg with fY0 ; ffTC Y0 ; ffT
LY0 ; ffT fU(v ) ; V(v ) ; fi(v)g and U fi with positive values .
2 : repeat 3 : 4 :
Update candidate mention type indicator Y by Eq ( 7 ) Update entity name type indicator C and relation phrase type signature fPL ; PRg by Eq ( 8 ) repeat for v = 0 to 3 do
Update V(v ) with Eq ( 9 ) Normalize U(v ) = U(v)Q(v ) , V(v ) = V(v)Q(v),1 Update U(v ) by Eq ( 10 )
5 : 6 : 7 : 8 : 9 : 10 : 11 : 12 : 13 : until the objective O in Eq ( 6 ) converges 14 : Predict the type of mi 2 MU by type(mi ) = argmax Yi . end for Update consensus matrix U ffi(v)g using Eq ( 12 ) until Eq ( 11 ) converges fi and relative feature weights
With optimized fU(v ) ; V(v)g , we update U and ffi(v)g by taking the derivative on O with respect to each of them while fixing all other variables . This leads to the closed( form update rules as follows :
∑
) fi fi d v=0 fi(v)U(v)Q(v )
U
=
; fi(v ) = , log
∑ ffi(v)∑ d i=0 ffi(i )
:
( 12 ) d v=0 fi(v )
Algorithm 1 summarizes our algorithm . For convergence analysis , ClusType applies block coordinate descent on the real valued relaxation of Eq ( 6 ) . The proof procedure in [ 26 ] ( not included for lack of space ) can be adopted to prove convergence for ClusType ( to the local minimum ) . 4.5 Computational Complexity Analysis Given a corpus D with ND words , the time complexity for our candidate generation and generation of fffC ; ffL ; ffR ; Fs ; Fcg is O(ND ) . For construction of the heterogeneous graph G , the costs for computing GC;P and GM are O(nl ) and O(M MCdC ) , respectively , where MC denotes average number of mentions each name has and dC denotes average size of feature dimensions ( MC < 10 ; dC < 5000 in our experiments ) . It takes O(M T ) and O(M M 2C + l2 ) time to initialize all the variables and pre compute the constants in update rules , respectively . We then study the computational complexity of ClusType in Algorithm 1 with the pre computed matrices . In each it ) eration of the outer loop , ClusType costs O(M MCT ) to upto update tinlK(T + ns + nc ) supposing it stops after tin iterations ( tin < 100 in our experand ffi(v)g takes O(lK ) time . Overall , iments ) . Update of U toutnlT + , supposing that the outer loop stops date Y , O(nlT ) to update C and O( ) fPL ; PRg . The cost for inner loop is O( the computational complexity of ClusType is O( touttinlK(T + ns + nc ) in tout iterations ( tout < 10 in our experiments ) . nT ( K + l )
) fi
5 . EXPERIMENTS 5.1 Data Preparation
Our experiments use three real world datasets3 : ( 1 ) NYT : constructed by crawling 2013 news articles from New York Times . The dataset contains 118,664 articles ( 57M tokens
3Code and datasets used in this paper can be downloaded at : http://webengrillinoisedu/~xren7/clustypezip
Table 3 : Statistics of the heterogeneous graphs . Data sets #Entity mention candidates ( M ) #Entity surface names ( n ) #Relation phrases ( l ) #Links Avg#mentions per string name
NYT 4.88M 1.32M 832k 195k 271k 743k
29.32M 8.64M 3.59M 10.56
703k 67k 57k
Yelp Tweet
6.78
5.86
Table 4 : Target type sets T for the datasets .
NYT Yelp Tweet person , organization , location , time event food , time event , job title , location , organization time event , business consumer product , person , location , organization , business job title , time year of day and 480k unique words ) covering various topics such as Politics , Business and Sports ; ( 2 ) Yelp : We collected 230,610 reviews ( 25M tokens and 418k unique words ) from the 2014 Yelp dataset challenge ; and ( 3 ) Tweet : We randomly selected 10,000 users in Twitter and crawled at most 100 tweets for each user in May 2011 . This yields a collection of 302,875 tweets ( 4.2M tokens and 157k unique words ) . 1 . Heterogeneous Graphs . We first performed lemmatization on the tokens using NLTK WordNet Lemmatizer4 to reduce variant forms of words ( eg , eat , ate , eating ) into their lemma form ( eg , eat ) , and then applied Stanford POS tagger [ 25 ] on the corpus . In candidate generation ( see Sec 3.1 ) , we set maximal pattern length as 5 , minimum support as 30 and significance threshold as 2 , to extract candidate entity mentions and relation phrases from the corpus . We then followed the introduction in Sec 3 to construct the heterogeneous graph for each dataset . We used 5 nearest neighbor graphs when constructing the mention correlation subgraph . Table 3 summarizes the statistics of the constructed heterogeneous graphs for all three datasets . 2 . Clustering Feature Generation . Following the procedure introduced in Sec 4.2 , we used a text window of 10 words to extract the context features for each relation phrase ( 5 words on the left and the right of a relation phrase ) , where stop words are removed . We obtained 56k string terms ( ns ) and 129k context terms ( nc ) for the NYT dataset , 58k string terms and 37k context terms for the Yelp dataset and 18k string terms and 38k context terms for the Tweet dataset , respectively all unique term counts . Each row of the feature matrices were then normalized by its ℓ 2 norm . 3 . Seed and Evaluation Sets . For evaluation purposes , we selected entity types which are popular in the dataset from Freebase , to construct the target type set T . Table 4 shows the target types used in the three datasets . To generate the set of seed mentions ML , we followed the process introduced in Sec 4.1 by setting the confidence score threshold as = 0:8 . To generate the evaluation sets , we randomly selected a subset of documents from each dataset and annotated them using the target type set T ( each entity mention is tagged by one type ) . 1k documents are annotated for the NYT dataset ( 25,451 annotated mentions ) . 2.5k reviews are annotated for the Yelp dataset ( 21,252 annotated mentions ) . 3k tweets are annotated for the Tweet dataset ( 5,192 annotated mentions ) . We removed the mentions from the seed mention sets if they were in the evaluation sets . 5.2 Experimental Settings In our testing of ClusType and its variants , we set the number of clusters K = f4000 ; 1500 ; 300g for NYT , Yelp and Tweet datasets , respectively , based on the analyses in Sec 53 We set fff ; fl ; g = f0:4 ; 0:7 ; 0:5g by five fold cross validation 4 http://wwwnltkorg/
1001 Table 5 : Performance comparisons on three datasets in terms of Precision , Recall and F1 score .
Data sets Method Pattern [ 7 ] FIGER [ 12 ]
SemTagger [ 9 ] APOLLO [ 22 ] NNPLB [ 11 ]
ClusType NoClus ClusType NoWm ClusType TwoStep
ClusType
Precision
0.4576 0.8668 0.8667 0.9257 0.7487 0.9130 0.9244 0.9257 0.9550
NYT Recall 0.2247 0.8964 0.2658 0.6972 0.5538 0.8685 0.9015 0.9033 0.9243
F1
0.3014 0.8814 0.4069 0.7954 0.6367 0.8902 0.9128 0.9143 0.9394
Precision
0.3790 0.5010 0.3769 0.3534 0.4248 0.7629 0.7812 0.8025 0.8333
Yelp Recall 0.1354 0.1237 0.2440 0.2366 0.6397 0.7581 0.7634 0.7629 0.7849
F1
0.1996 0.1983 0.2963 0.2834 0.5106 0.7605 0.7722 0.7821 0.8084
Precision
0.2107 0.7354 0.4225 0.1471 0.3327 0.3466 0.3539 0.3748 0.3956
Tweet Recall 0.2368 0.1951 0.1632 0.2635 0.1951 0.4920 0.5434 0.5230 0.5230
F1
0.2230 0.3084 0.2355 0.1883 0.2459 0.4067 0.4286 0.4367 0.4505
( of classification accuracy ) on the seed mention sets . For convergence criterion , we stop the outer ( inner ) loop in Algorithm 1 if the relative change of O in Eq ( 6 ) reconstruction error in Eq ( 11 )
,4 , respectively .
(
) is smaller than 10
Compared Methods : We compared the proposed method ( ClusType ) with its variants which only model part of the proposed hypotheses . Several state of the art entity recognition approaches were also implemented ( or tested using their published codes ) : ( 1 ) Stanford NER [ 5 ] : a CRF classifier trained on classic corpora for several major entity types ; ( 2 ) Pattern [ 7 ] : a state of the art pattern based bootstrapping method which uses the seed mention sets ML ; ( 3 ) SemTagger [ 9 ] : a bootstrapping method which trains contextual classifiers using the seed mention set ML in a self training manner ; ( 4 ) FIGER [ 12 ] : FIGER trains sequence labeling models using automatically annotated Wikipeida corpora ; ( 5 ) NNPLB [ 11 ] : It uses ReVerb assertions [ 4 ] to construct graphs and performs entity name level label propagation ; and ( 6 ) APOLLO [ 22 ] : APOLLO constructs heterogeneous graphs on entity mentions , Wikipedia concepts and KB entities , and then performs label propagation .
All compared methods were first tuned on our seed mention sets using five fold cross validation . For ClusType , besides the proposed full fledged model , ClusType , we compare ( 1 ) ClusType NoWm : This variant does not consider mention correlation subgraph , ie , set fl = 0 in ClusType ; ( 2 ) ClusType NoClus : It performs only type propagation on the heterogeneous graph , ie , Eq ( 4 ) is removed from O ; and ( 3 ) ClusType TwoStep : It first conducts multi view clustering to assign each relation phrase to a single cluster , and then performs ClusType NoClus between entity names , candidate entity mentions and relation phrase clusters .
Evaluation Metrics : We use F1 score computed from Precision and Recall to evaluate the entity recognition performance . We denote the #system recognized entity men∑ tions as J and the # ground truth annotated mentions in ∑ the evaluation set as A . Precision is calculated by Prec = m = tm)=jJj and Recall is calculated by Rec = ′ m2J\A !(t ′ m = tm)=jAj . Here , tm and t ′ m2J\A !(t m denote the true type and the predicted type for m , respectively . Function !( ) returns 1 if the predicted type is correct and 0 otherwise . Only mentions which have correct boundaries and predicted types are considered correct . For cross validation on the seed mention sets , we use classification accuracy to evaluate the performance .
5.3 Experiments and Performance Study 1 . Comparing ClusType with the other methods on entity recognition . Table 5 summarizes the comparison results on the three datasets . Overall , ClusType and its three variants outperform others on all metrics on NYT and Yelp and achieve superior Recall and F1 scores on Tweet . In particular , ClusType obtains a 46.08 % improvement in F1 score and 168 % improvement in Recall compared to the best baseline
( a ) Yelp
( b ) Tweet
Figure 6 : Performance breakdown by types .
FIGER on the Tweet dataset and improves F1 by 48.94 % compared to the best baseline , NNPLB , on the Yelp dataset . FIGER utilizes a rich set of linguistic features to train sequence labeling models but suffers from low recall moving from a general domain ( eg , NYT ) to a noisy new domain ( eg , Tweet ) where feature generation is not guaranteed to work well ( eg , 65 % drop in F1 score ) . Superior performance of ClusType demonstrates the effectiveness of our candidate generation and of the proposed hypotheses on type propagation over domain specific corpora . NNPLB also utilizes textual relation for type propagation , but it does not consider entity surface name ambiguity . APOLLO propagates type information between entity mentions but encounters severe context sparsity issue when using Wikipedia concepts . ClusType obtains superior performance because it not only uses semantic rich relation phrases as type cues for each entity mention , but also clusters the synonymous relation phrases to tackle the context sparsity issues . 2 . Comparing ClusType with its variants . Comparing with ClusType NoClus and ClusType TwoStep , ClusType gains performance from integrating relation phrase clustering with type propagation in a mutually enhancing way . It always outperforms ClusType NoWm on Precision and F1 on all three datasets . The enhancement mainly comes from modeling the mention correlation links , which helps disambiguate entity mentions sharing the same surface names . 3 . Comparing on different entity types . Fig 6 shows the performance on different types on Yelp and Tweet . ClusType outperforms all the others on each type . It obtains larger gain on organization and person , which have more entities with ambiguous surface names . This indicates that modeling types on entity mention level is critical for name disambiguation . Superior performance on product and food mainly comes from the domain independence of our method because both NNPLB and SemTagger require sophisticated linguistic feature generation which is hard to adapt to new types . 4 . Comparing with trained NER . Table 6 compares ours with a traditional NER method , Stanford NER , trained using classic corpora like ACE corpus , on three major types| person , location and organization . ClusType and its variants outperform Stanford NER on the corpora which are dynamic ( eg , NYT ) or domain specific ( eg , Yelp ) . On the Tweet dataset , ClusType has lower Precision but achieves a 63.59 % improvement in Recall and 7.62 % improvement in F1 score . The superior Recall of ClusType mainly comes from the domain independent candidate generation .
FoodLocationOrganizationF1 score00102030405060708091ClusTypeClusType NoClusSemTaggerNNPLBPersonProductOrganizationF1 score00102030405060708091ClusTypeClusType NoClusSemTaggerNNPLB1002 Table 6 : F1 score comparison with trained NER .
Method
Stanford NER [ 5 ] ClusType NoClus
ClusType
NYT 0.6819 0.9031 0.9419
Yelp 0.2403 0.4522 0.5943
Tweet 0.4383 0.4167 0.4717
( a ) #Clusters
( b ) Seed set size
( c ) Corpus size
Figure 7 : Performance changes in F1 score with #clusters , #seeds and corpus size on Tweets .
5 . Testing on sensitivity over the number of relation phrase clusters , K . Fig 7(a ) , ClusType was less sensitive to K compared with its variants . We found on the Tweet dataset , ClusType achieved the best performance when K=300 while its variants peaked at K=500 , which indicates that better performance can be achieved with fewer clusters if type propagation is integrated with clustering in a mutually enhancing way . On the NYT and the Yelp datasets ( not shown here ) , ClusType peaked at K=4000 and K=1500 , respectively . 6 . Testing on the size of seed mention set . Seed mentions are used as labels ( distant supervision ) for typing other mentions . By randomly selecting a subset of seed mentions as labeled data ( sampling ratio from 0.1 to 1.0 ) , Fig 7(b ) shows ClusType and its variants are not very sensitive to the size of seed mention set . Interestingly , using all the seed mentions does not lead to the best performance , likely caused by the type ambiguity among the mentions . 7 . Testing on the effect of corpus size . Experimenting on the same parameters for candidate generation and graph construction , Fig 7(c ) shows the performance trend when varying the sampling ratio ( subset of documents randomly sampled to form the input corpus ) . ClusType and its variants are not very sensitive to the changes of corpus size , but NNPLB had over 17 % drop in F1 score when sampling ratio changed from 1.0 to 0.1 ( while ClusType had only 55 % ) In particular , they always outperform FIGER , which uses a trained classifier and thus does not depend on corpus size . 5.4 Case Studies 1 . Example output on two Yelp reviews . Table 7 shows the output of ClusType , SemTagger and NNPLB on two Yelp reviews : ClusType extracts more entity mention candidates ( eg , \BBQ" , \ihop" ) and predicts their types with better accuracy ( eg , \baked beans" , \pulled pork sandwich" ) . 2 . Testing on context sparsity . The type indicator of each entity mention candidate is modeled in ClusType based on the type indicator of its surface name and the type signatures of its co occurring relation phrases . To test the handling of different relation phrase sparsity , two groups of 500 mentions are selected from Yelp : mentions in Group A cooccur with frequent relation phrases ( 4.6k occurrences in the corpus ) and those in Group B co occur with sparse relation phrases ( 3.4 occurrences in the corpus ) . Fig 8(a ) compares their F1 scores on the Tweet dataset . In general , all methods obtained better performance when mentions cooccurring with frequent relation phrases than with sparse relation phrases . In particular , we found that ClusType and its variants had comparable performance in Group A but ClusType obtained superior performance in Group B . Also ,
( a ) Context sparsity
( b ) Surface name popularity Figure 8 : Case studies on context sparsity and surface name popularity on the Tweet dataset .
ClusType TwoStep obtained larger performance gain over ClusType NoClus in Group B . This indicates that clustering relation phrases is critical for performance enhancement when dealing with sparse relation phrases , as expected . 3 . Testing on surface name popularity . We generated the mentions in Group A with high frequency surface names ( 2.7k occurrences ) and those in Group B with infrequent surface names ( 15 ) Fig 8(b ) shows the degraded performance of all methods in both cases|likely due to ambiguity in popular mentions and sparsity in infrequent mentions . ClusType outperforms its variants in Group B , showing it handles well mentions with insufficient corpus statistics . 4 . Example relation phrase clusters . Table 8 shows relation phrases along with their corpus frequency from three example relation phrase clusters for the NYT dataset ( K = 4000 ) . We found that not only synonymous relation phrases , but also both sparse and frequent relation phrases can be clustered together effectively ( eg , \want hire by" and \recruited by" ) . This shows that ClusType can boost sparse relation phrases with type information from the frequent relation phrases with similar group memberships .
Table 8 : Example relation phrase clusters and their corpus frequency from the NYT dataset .
ID 1 2
3
Relation phrase recruited by ( 5.1k ) ; employed by ( 3.4k ) ; want hire by ( 264 ) go against ( 2.4k ) ; struggling so much against ( 54 ) ; run for re election against ( 112 ) ; campaigned against ( 1.3k ) looking at ways around ( 105 ) ; pitched around ( 1.9k ) ; echo around ( 844 ) ; present at ( 5.5k ) ;
6 . RELATED WORK 1 . Entity Recognition : Existing work leverages various levels of human supervision to recognize entities , from fully annotated documents ( supervised ) , seed entities ( weakly supervised ) , to knowledge bases ( distantly supervised ) .
Traditional supervised methods [ 18 , 15 ] use fully annotated documents and different linguistic features to train sequence labeling model . To obtain an effective model , the amount of labeled data is significant [ 18 ] , despite of semisupervised sequence labeling [ 19 ] .
Weakly supervised methods utilize a small set of typed entities as seeds and extract more entities of target types , which can largely reduce the amount of required labeled data . Pattern based bootstrapping [ 7 , 23 ] derives patterns from contexts of seed entities and uses them to incrementally extract new entities and new patterns unrestricted by specific domains , but often suffers low recall and semantic drift [ 12 ] . Iterative bootstrapping , such as probabilistic method [ 17 ] and label propagation [ 24 ] softly assign multiple types to an entity name and iteratively update its type distribution , yet cannot decide the exact type for each entity mention based on its local context .
Distantly supervised methods [ 16 , 11 , 12 ] avoid expensive human labels by leveraging type information of entity
#Clusters ( K)100500100015002000300003603804042044046ClusTypeClusType NoWmClusType TwoStepSample ratio0102040608101502025030350404505ClusTypeClusType NoWmClusType NoClusNNPLBSample ratio0102040608101502025030350404505ClusTypeClusType NoWmClusType NoClusNNPLBABF100102030405060708091ClusTypeClusType TwoStepClusType NoClusFIGERABF100102030405060708091ClusTypeClusType TwoStepClusType NoClusFIGER1003 Table 7 : Example output of ClusType and the compared methods on the Yelp dataset .
ClusType
SemTagger
NNPLB tasted best BBQ:Food I’ve
The in Phoenix:LOC ! I had the [ pulled pork sandwich]:Food with coleslaw:Food and [ baked beans]:Food for lunch . I only go to ihop:LOC for pancakes:Food because I don’t really like anything else on the menu . Ordered [ chocolate chip pancakes]:Food and a [ hot chocolate]:Food .
The best BBQ I’ve tasted in Phoenix:LOC ! I had the pulled [ pork sandwich]:LOC with coleslaw:Food and [ baked beans]:LOC for lunch . I only go to ihop for pancakes because I don’t really like anything else on the menu . Ordered [ chocolate chip pancakes]:LOC and a [ hot chocolate]:LOC .
I’ve tasted best BBQ:Loc in The Phoenix:LOC ! I had the pulled pork sandwich:Food with coleslaw and baked beans:Food for lunch:Food . I only go to ihop for pancakes because I don’t really like anything else on the menu . Ordered chocolate chip pancakes and a hot chocolate . mentions which are confidently mapped to entries in KBs . Linked mentions are used to type those unlinkable ones in different ways , including training a contextual classifier [ 16 ] , learning a sequence labeling model [ 12 ] and serving as labels in graph based semi supervised learning [ 11 ] .
Our work is also related to knowledge base population methods [ 22 ] which study entity linking and fine grained categorization of unlinkable mentions in a unified framework , which shares the similar idea of modeling each entity mention individually to resolve name ambiguity . 2 . Open Relation Mining : Extracting textual relation between subjective and objective from text has been extensively studied [ 4 ] and applied to entity typing [ 11 ] . Fader et al . [ 4 ] utilize POS patterns to extract verb phrases between detected noun phrases to form relation assertion . Schmitz et al . [ 20 ] further extend the textual relation by leveraging dependency tree patterns . These methods rely on linguistic parsers that may not generalize across domains . They also do not consider significance of the detected entity mentions in the corpus ( see comparison with NNPLB [ 11] ) .
There have been some studies on clustering and and canonicalizing synonymous relations generated by open information extraction [ 6 ] . These methods either ignore entity type information when resolving relations , or assume types of relation arguments are already given . 7 . CONCLUSIONS
We have studied distantly supervised entity recognition and proposed a novel relation phrase based entity recognition framework . A domain agnostic phrase mining algorithm is developed for generating candidate entity mentions and relation phrases . By integrating relation phrase clustering with type propagation , the proposed method is effective in minimizing name ambiguity and context problems , and thus predicts each mention ’s type based on type distribution of its string name and type signatures of its surrounding relation phrases . We formulate a joint optimization problem to learn object type indicators/signatures and cluster memberships simultaneously . Our performance comparison and case studies show a significant improvement over state ofthe art methods and demonstrate its effectiveness . 8 . ACKNOWLEDGEMENTS
Research was sponsored in part by the US Army Research Lab . under Cooperative Agreement No . W911NF09 2 0053 ( NSCTA ) , National Science Foundation IIS 1017362 , IIS 1320617 , and IIS 1354329 , HDTRA1 10 1 0120 , and grant 1U54GM114838 awarded by NIGMS through funds provided by the trans NIH Big Data to Knowledge ( BD2K ) initiative ( wwwbd2knihgov ) , and MIAS , a DHS IDS Center for Multimodal Information Access and Synthesis at UIUC . References [ 1 ] K . Bollacker , C . Evans , P . Paritosh , T . Sturge , and J . Taylor . Freebase : a collaboratively created graph database for structuring human knowledge . In SIGMOD , 2008 .
[ 2 ] X . L . Dong , T . Strohmann , S . Sun , and W . Zhang . Knowledge vault : A web scale approach to probabilistic knowledge fusion . In SIGKDD , 2014 .
[ 3 ] A . El Kishky , Y . Song , C . Wang , C . R . Voss , and J . Han . Scalable topical phrase mining from text corpora . VLDB , 2015 .
[ 4 ] A . Fader , S . Soderland , and O . Etzioni . Identifying relations for open information extraction . In EMNLP , 2011 .
[ 5 ] J . R . Finkel , T . Grenager , and C . Manning . Incorporating non local information into information extraction systems by gibbs sampling . In ACL , 2005 .
[ 6 ] L . Galarraga , G . Heitz , K . Murphy , and F . M . Suchanek .
Canonicalizing open knowledge bases . In CIKM , 2014 .
[ 7 ] S . Gupta and C . D . Manning . Improved pattern learning for bootstrapped entity extraction . In CONLL , 2014 .
[ 8 ] X . He and P . Niyogi . Locality preserving projections .
In
NIPS , 2004 .
[ 9 ] R . Huang and E . Riloff . Inducing domain specific semantic class taggers from ( almost ) nothing . In ACL , 2010 .
[ 10 ] Z . Kozareva and E . Hovy . Not all seeds are equal : Measuring the quality of text mining seeds . In NAACL , 2010 .
[ 11 ] T . Lin , O . Etzioni , et al . No noun phrase left behind : de tecting and typing unlinkable entities . In EMNLP , 2012 .
[ 12 ] X . Ling and D . S . Weld . Fine grained entity recognition . In
AAAI , 2012 .
[ 13 ] J . Liu , C . Wang , J . Gao , and J . Han . Multi view clustering via joint nonnegative matrix factorization . In SDM , 2013 .
[ 14 ] B . Min , S . Shi , R . Grishman , and C Y Lin . Ensemble semantics for large scale unsupervised relation extraction . In EMNLP , 2012 .
[ 15 ] D . Nadeau and S . Sekine . A survey of named entity recognition and classification . Lingvisticae Investigationes , 30(1):3{ 26 , 2007 .
[ 16 ] N . Nakashole , T . Tylenda , and G . Weikum . Fine grained semantic typing of emerging entities . In ACL , 2013 .
[ 17 ] K . Nigam and R . Ghani . Analyzing the effectiveness and applicability of co training . In CIKM , 2000 .
[ 18 ] L . Ratinov and D . Roth . Design challenges and misconcep tions in named entity recognition . In ACL , 2009 .
[ 19 ] S . Sarawagi and W . W . Cohen . Semi markov conditional random fields for information extraction . In NIPS , 2004 .
[ 20 ] M . Schmitz , R . Bart , S . Soderland , O . Etzioni , et al . Open In EMNLP , language learning for information extraction . 2012 .
[ 21 ] W . Shen , J . Wang , and J . Han . Entity linking with a knowledge base : Issues , techniques , and solutions . TKDE , ( 99):1{ 20 , 2014 .
[ 22 ] W . Shen , J . Wang , P . Luo , and M . Wang . A graph based In approach for ontology population with named entities . CIKM , 2012 .
[ 23 ] S . Shi , H . Zhang , X . Yuan , and J R Wen . Corpus based semantic class mining : distributional vs . pattern based approaches . In COLING , 2010 .
[ 24 ] P . P . Talukdar and F . Pereira . Experiments in graph based semi supervised learning methods for class instance acquisition . In ACL , 2010 .
[ 25 ] K . Toutanova , D . Klein , C . D . Manning , and Y . Singer . Feature rich part of speech tagging with a cyclic dependency network . In HLT NAACL , 2003 .
[ 26 ] P . Tseng . Convergence of a block coordinate descent method for nondifferentiable minimization . JOTA , 109(3):475{494 , 2001 .
1004
