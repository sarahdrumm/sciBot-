Distributed Personalization
Xu Miao
LinkedIn Corporate 605 W Maude Ave
Sunnyvale , USA 94085 xmiao@linkedin.com
Chun Te Chu
Microsoft Corporate
1 Microsoft Way Redmond , USA chch@microsoft.com
Lijun Tang
LinkedIn Corporate 605 W Maude Ave
Sunnyvale , USA 94085 litang@linkedin.com
Yitong Zhou
LinkedIn Corporate 2061 Stierln Court
Mountain View , USA 94043 yizhou@linkedin.com
Joel Young
LinkedIn Corporate 605 W Maude Ave
Sunnyvale , USA 94085 joyoung@linkedin.com
Anmol Bhasin LinkedIn Corporate 605 W Maude Ave
Sunnyvale , USA 94085 abhasin@linkedin.com
ABSTRACT Personalization is a long standing problem in data mining and machine learning . Companies make personalized product recommendations to millions of users every second . In addition to the recommendation problem , with the emerging of personal devices , many conventional problems , eg , recognition , need to be personalized as well . Moreover , as the number of users grows huge , solving personalization becomes quite challenging . In this paper , we formalize the generic personalization problem as an optimization problem . We propose several ADMM algorithms to solve this problem in a distributed way including a new Asynchronous ADMM that removes all synchronous barriers to maximize the training throughput . We provide a mathematical analysis to show that the proposed Asynchronous ADMM algorithm holds a linear convergence rate which is the best to our knowledge . The distributed personalization allows training to be performed in either a cluster or even on a user ’s device . This can improve the privacy protection as no personal data is uploaded , while personal models can still be shared with each other . We apply this approach to two industry problems , Facial Expression Recognition and Job Recommendation . Experiments demonstrate more than 30 % relative error reduction on both problems . Asynchronous ADMM allows faster training for problems with millions of users since it eliminates all network I/O waiting time to maximize the cluster CPU throughput . Experiments demonstrate 4 times faster than original synchronous ADMM algorithm .
Categories and Subject Descriptors H28 [ Database Application ] : Data mining ; H34 [ Systems and Software ] : Distributed systems
General Terms Big Data , Distributed Optimization , Personalization
Keywords Distributed computing , User modeling , ADMMs
1 .
INTRODUCTION
Personalization is a key component in many applications such as advertising [ 9 , 5 , 21 ] , recommendation [ 17 , 18 ] , and personalized medicine [ 23 ] , where an impression , a product and a type of drug is predicted according to the user ’s profile . Sparse Matrix Factorization approaches [ 18 , 7 , 19 ] that seek a shared lowdimensional representation for both users and items have inspired significant amount of research . Both theoretical work and empirical work demonstrated that with the low rank assumption , ie , very few dominating factors for individual users , these approaches have achieved state of the arts accuracies on many datasets [ 7 ] . However , they are restricted by the matrix formulation , and less flexible to incorporate contextual information , therefore , not general enough to address the personalization aspect in broader machine learning problems . For example , in movie recommendation , extra information about movies like the directors and the budgets are not encoded in the rating matrix . In this paper , we address the personalization differently . Each individual is assigned one model with a regularization term to penalize the difference between individual models and a global consensus model . The regularization term allows individual experiences to be shared among the population , while allowing the models to be different from each other . A parameter in the regularization term controls how much of a difference these models are . This formulation has been proposed under Multi task Learning problem [ 15 , 3 ] where each individual model is a specific task . These tasks are related to each other , so solving one task provides information to solve others . Our proposed algorithm can be applied to these problem naturally , especially for tasks distributed among a network , eg , remote sensing and multi agent planning .
Solving this personalization problem in a distributed environment is quite challenging , especially for data from large companies like LinkedIn and Microsoft . In this paper , we study algorithms based on alternating direction method of multipliers ( ADMM ) [ 6 ] . We propose several variants of the original ADMM algorithm including a novel Asynchronous ADMM algorithm that frees up the waiting barriers to speed optimization . As the number of worker nodes grows , the processing speed , computation power , network delay usually vary among different worker nodes . Iterating until all nodes finish is expensive as nodes have to wait for the slowest worker node before the next iteration starts . Asynchronous training
1989 addresses the problem and achieves better optimization throughput . We provide a mathematical analysis to prove the proposed Asynchronous ADMM converges at a comparable rate as the original Synchronus ADMM .
We apply our personalization model to solve two industry problems , facial expression recognition and job recommendation . The experimental results demonstrate a great gain in accuracy , and the proposed Asynchronous ADMM speeds up the training process dramatically .
In summary , our proposed approach enables systematical training of personalized models jointly in a distributed manner . To our knowledge , this is the first solution of large scale personalization problems via asynchronous distributed machine learning . In addition , the proposed asynchronous ADMM is the first algorithm that demonstrates linear convergence both in theory and empirically .
2 . DISTRIBUTED OPTIMIZATION
Before detailing the distributed algorithm , we first formulate our personalization framework as an optimization problem . Equation 1 provides a general formalization where fi(wi ) is a arbitrary machine learning task for the i th user , wi is the personal model for that user , and z is the consensus model . Equation 2 shows an example fi(wi ) where l is a convex loss function , xij is an input feature vector , and yij is the target to predict . γ is a regularizatino term inversely coupling the individual and consensus models . With only a handful of samples per model , we need a relatively large γ to prevent overfitting .
1 2z2 +M fi(wi ) =Ni i=1
,fi(wi ) + γ
2wi − z2
2
( 1 )
( 2 ) j=1 l(xij , yij ; wi ) minz,{w1wM }
We focus on distributed personalization of classification tasks and as such assume that the number of users is proportional to the total number of samples , and finally , that each user ’s data is drawn iid from the distribution Di . These are captured in Assumption 1 where Ni is the number of training samples for user i , N is the total number of training samples , and M is the total number of users .
ASSUMPTION 1 . We make the following assumptions :
1 . Let N =M i Ni where M = O(N ) . 2 . For each user i ∈ [ 1M ] , {xij , yij} ∼ Di . 3 . Network latency is iid with mean τ .
Applying distributed stochastic optimization approaches [ 1 , 10 , 2 , 12 ] to this distributed personalization problem is non trivial . Although the consensus model can be obtained , there aren’t enough samples to reliably estimate individual user models . In addition , tracking the learning rate for each user is difficult .
We experiment with a synchronous ADMM algorithm [ 6 , 25 ] from the literature , a novel versioned adaptation , and propose a faster asynchronous algorithm . A key observation is that alternately optimizing wi and z converges slowly — a low sample count per user drives a large γ large coupling the personal and consensus models which , in turn , leads to histeresis . 2.1 Synchronous ADMM
We first derive a synchronous ADMM to solve Equation 1 . By introducing local consensus variables qi , we rewrite the problem as in Equation 3 . Turning it into the augmented Lagrangian , we have
Equation 4 . min z,q,{wi} subject to
M i=1 fi(wi ) + wi − qi2 +
γ 2 z2
1 2
( 3 ) qi = z,∀i ∈ [ 1M ]
L(z,{qi , wi , λi} ) = fi(wi ) + wi − qi2 +
γ 2
M i=1 ρ 2 qi − z2 + λT i ( qi − z ) + z2(4 )
1 2
An synchronous ADMM solution can be obtained by Equations [ 5,6,7 ] . wt+1 i
, qt+1 i fi(wi ) + wi − qi2 + γ 2 i(qi − zt ) qi − zt2 + λt
= arg min qi,wi ρ 2 zt+1 = arg min z i − z2 + z2 qt+1 ρ 2 i − z ) + 1 i(qt+1 2 i − zt+1 )
λt
λt+1 i
= λt i + ρ(qt+1
( 5 )
( 6 )
( 7 )
In Equation 5 , qi has analytical relation to wi , we obtain the updating rules in Equation [ 9,10,11,8 ] , where µ = λ/ρ . ∀i , µt+1 ∀i , wt+1 ∀i , qt+1 i + qt = arg min i − zt fi(w ) + w − zt + µt+1
( 8 ) 2(9 )
= µt
µt+1
( 10 )
ργ
=
γ w i i i i i
ρ
M
ρ + γ i=1 zt +
,qt+1
ρ + γ i + µt+1 i
2(ρ + γ ) i − ρ wt+1 ρ + γ zt+1 =
( 11 )
M + 1 ρ
From Equation 9 , even when γ is huge , the regularization is upper bounded by ρ 2 that avoids obtaining a w over influenced by z , and leads to faster convergence . Note that we update µi first , and update z at the end of the iteration for a simpler derivation .
In synchronous ADMM , Equations [ 8,9,10 ] are performed in each worker node , while Equation 11 is performed in the master node . The master node waits for all sub problems to be solved and their local models submitted , before merging and broadcasting a new consensus model , see Figure 1 . Waiting for all nodes to finish slows down the distributed computation process , and each user benefits from the latest consensus less frequently . The asynchronous ADMM essentially unblocks the master and allows the consensus model to be updated freely . Therefore , the aggregated knowledge can be shared between individuals more frequently . 2.2 Versioned Synchronous ADMM
Different users have different numbers of samples to optimize , and different workers have different hardware latency . The submitted local models qi , and dual variables µi come to the master out of order . It is easy to see that z can be updated whenever the master is ready as long as it is broadcast at the end of the iteration . On the other hand , we can broadcast z to the workers at any time , and force the remaining workers to restart the sub problem based on the new consensus z , ie , at any moment , there is only one version of consensus . Theorem 2 proves that this Single Versioned ADMM converges at the same rate as the original Synchronous ADMM . The convergence analysis for Synchronous ADMM can be found in literature [ [16],[22] ] .
1990 Figure 1 : Synchronous ADMM updating schedule . Although all personal models proceed in parallel , the master has to wait for all updates come to merge . This induces a great overhead .
Figure 3 : A Multi Versioned ADMM updating schedule . Similar to Single Versioned ADMM , this allows the consensus periodically updated , but no additional barriers are added . updates . For each one of them j ∈ Bk , the sub problem is solved by using the consensus model ˜zh(j ) , where h(j ) ∈ [ 1k − 1 ] is the mapping from the user to the consensus model version . We first update ˜zh(j ) for all j ∈ Bk , then update zk , and broadcast it . The consensus updating rules for this multi versioned ADMM are defined in Equations 12 and 13 .
Figure 2 : A Single Versioned ADMM updating schedule . The master updates the consensus periodically to allow faster sharing , but it adds additional barriers that potentially stalls the process more .
THEOREM 2 . Let Θi = ( wi , qi , µi ) , Lagrangian in Equation 4 can be rewritten as L({Θi} , z ) . Assuming ∀i , fi is a con} ∪ vex function , at any point , update zt+1 {Θt Θ1Θt+1 tion . i} , z ) , then any optimization sequence , ie , k = arg minz L({Θt+1
M , converges at rate o( 1 i+1ΘT
, Θt+1
, zt+1 k i i
PROOF . For any iteration t , Lagrangian Lt({Θt intermediate consensus updates , we can verify that i , qt k , µt+1 i , qt i , zt i , µt+1 i
T ) on expectai} , zt k ) after k
EL( , wt EL( , µt i
)fi ≤ EL( , wt i)fi ≤ EL( , wt i , ie , i , qt i , wt i , wt i , qt i , qt
)fi . )fi . i , zt k , µt+1 i
On the other hand , on expectation , an optimization step wt is equivalent to µt i , qt i , zt k , µt+1 i h(j ) = ˜zt ˜zt+1 h(j ) +
˜zt+1 k
=
1 |Bk|
1
M + 1 ρ
˜zt+1 h(j ) j∈Bk
( qt j + µt j − qt−1 j − µt−1 j
) ( 12 )
( 13 )
REMARK 3 . When ∀j , k , h(j ) = k−1 , multi versioned ADMM reduces to single verioned ADMM .
To prove that multi versioned ADMM converges , we first show that a Multi Block ADMM problem updates using rules in Equations 12 and 13 . Consider the optimization problem in Equation 14 , where K is the total number of consensus updates . In this problem , we have multiple blocks , {qi , wi} , ˜zk and z . According to [ 8 ] , extending the ADMM algorithm directly by alternating optimizations over these blocks achieves the same linear convergence when the equality constraints satisfy the condition in Theorem 4 . min z,{qi,wi},{˜zk} subject to
M i=1 fi(wi ) +
γ 2 qi = ˜zk,∀i ∈ Bk ˜zh(i ) = ˜zk,∀i ∈ Bk ˜zK = z wi − qi2 + z2(14 )
1 2
Combining these two , the optimization sequence for the SingleVersioned ADMM is dominated by the sequence of the original Synchronous ADMM . If fi is convex , both sequences arrive at a saddle point .
If the master and individual workers optimize independently , it is possibly that users’ personal models branch out on different versions of the consensus , and request to merge at different pace . These delayed updates do not necessarily dominate the synchronous ADMM optimization . However , we can show that they dominate an optimization sequence for a multiple block ADMM formulation [ 13 ] . The technique is to create the auxiliary variables for these different versions of consensus models . Assuming for the k th merge , we have a set of users Bk ⊂ [ 1M ] submitted local
THEOREM 4 . [ 8 ] Assuming a constraint optimization has con2 A3 = 0 , a straint A1x1 + A2x2 + A3x3 = 0 , if AT direct extension of ADMM converges in O( 1
1 A2 = 0 or AT
T ) .
In our case , it is easy to see that Ak for ˜zk is canceling each other , and therefore Ak = 0 . The dual variables for constraints ˜zk = z cancel out , so Equation [ 12,13 ] are equivalent to the ADMM updating rules for ˜zk and z . Therefore , for any such consensus updating schedules , we can construct a Multi Block ADMM program that updates correspondingly . The only difference is that MultiVersioned ADMM also updates z immediately after ˜z is updated . Similar to Theorem 2 , optimizing z multiple times within the same iteration dominates the original optimization sequence , so we conclude convergence as stated in Theorem 5 .
1991 Figure 4 : Asynchronous ADMM updating schedule . This algorithm removes the iteration barriers and free up the consensus updates completely .
THEOREM 5 . For any optimization sequence that follows EquaM , it con i+1ΘT h(j ) , Θt+1
, µt+1
, ˜zt+1 tion [ 12,13 ] , Θ1 verges at rate O( 1 2.3 Asynchronous ADMM
1 , Θt+1 T ) . h(j ) , zt+1 k i
For the algorithms described in previous section , the last synchronous property holds that no one proceeds until the end of the current iteration . The master keeps track of the versions of the consensus models , which are the only dependencies to update the new version of the consensus . We can simply remove this iteration lock , and let the whole process runs . This results in the proposed Asynchronous ADMM algorithm as shown in Algorithm 1 .
K is the latest version number ; input : {∆j , h(j)|j ∈ B} for j ∈ B do zh(j ) = zh(j ) + ∆j M + 1 ρ
; end K = K + 1 ; j∈B zh(j ) ; zK = 1 B Broadcast ( zK , K ) ;
Algorithm 1 : Master for ASYNC ADMM zk is the current consensus ; input : x , y , i Add x , y to samples for user i ; Update µi , qi , wi according to Equations [ 8,10,9 ] ; Let µo ∆i = µi + qi − µo Reduce ( ∆i , k ) i be the original values ; i − qo i ; i and qo
Algorithm 2 : Worker for ASYNC ADMM
By assuming that networking latency has an identical distribution with mean τ . We can show that this AsynchADMM converges at a similar rate as synchronous ADMM . Note that the concept of iterations does not exist any more . Here , T refers to the number of operations from the master , and is about M times the number of iterations in Synchronous setting . Therefore , the convergence rate is comparable for both of them .
THEOREM 6 . With assumptions 1 , Asynchronous ADMM Algo rithms [ 1,2 ] converge at O( M γτ T )
One interesting fact about the Theorem 6 is that the personalization strength γ influences the convergence too . Intuitively , if the personalized models are independent from the consensus model ,
Figure 6 : Facial expression of happy emotion on different people ’s faces . ie , γ = 0 , the local model qi can be simply assigned to the consensus z , and the iteration stops . On the other hand , if different persons have very different personal preference , enforcing them to reach the consensus is very difficult . Allowing the models to be diverged effectively improves convergence speed .
3 . FACIAL EXPRESSION
Human emotion study is a critical topic in several fields , including human computer interaction , psychology research , medical diagnosis and treatment , entertainment , education , etc . Facial expression is one of the most useful non verbal information for detecting human emotion . For decades , people have already investigated in automatic facial expression analysis through image , video , or audio [ 20 ] . As the camera becomes prevalent in modern devices , like mobile phone , laptop , desktop , and even wearable device , automatically recognizing the facial expression from image or video has gained much attention . Great amount of research [ 4 ] and industry products 1 2 are proposed to solve this problem . The state of the art techniques [ 4 ] includes three major steps as shown in figure 5 . First of all , human face is detected from the video frame , and the landmark points are located , such as eyes , nose , and mouth . After that , features are extracted from the face image . Popular features are optical flow , Gabor filter , gradient feature , local binary patterns , motion signature , geometry displacement , etc . These features can effectively represent the characteristics of different expressions . Given the feature set extracted from the collected data , classifiers have been trained with various machine learning models such as linear classifier , neural networks , hidden Markov model , random forests . However , due to different personalities , cultures , ages , and appearances , the facial expressions of the same emotion vary greatly . For instance , in figure 6 , three people performs happy face but with different visual appearance . Hence , using a single model is not sufficient enough to represent all the users . Therefore , personalization will be the key to enhance the accuracy and improve the user experience .
To support this , we consider the data set collected for the facial expression recognition app developed by Microsoft in Xbox One Kinect 3 . The following experiments are conducted based on a subset of this data set . The subset includes 151,413 samples collected from 250 users . Conventionally , one model is trained for a classifier of a facial expression . In this way , the individual accuracy is sacrificed for obtaining fair average accuracy . With our Asynchronous ADMM framework , we can train a personalized model for each user which can better represent his/her feature distribution . We follow the state of the art techniques in face and landmarks detection 1FaceReader:http://wwwnolduscom/human behaviorresearch/products/facereader 2Kaggle representation learning facial expression recognition challenge 3http://wwwshacknewscom/article/79301/xbox one kinectreading emotions and heart rate
Challenge:https://wwwkagglecom/c/challenges in
1992 Figure 5 : Flow chart of facial expression recognition . strate such a behavior , we study the accuracies in different γ settings . The larger the γ is , the more the personal model is towards the consensus model . We show the distribution of the ROC curves across different users . We compute the ROC curves for each individual user , and sample the data points along the curve to compute the mean and the variance . Figure 8 represents a mean ROC curve and the standard deviation interval along the curve to show how consistent the model performs across the population . From the figures , we demonstrate that γ = 50 ( chosen by a validation set ) produces the best prediction at the testing set ( with better mean ROC curve and narrower variance range ) , while γ = 10−6 and γ = 106 produce inferior results . At one extreme side , γ = 10−6 corresponds to a model trained completely by using personal data . The overfitted model introduces high variance . On the other extreme side , γ → ∞ , everyone is using nearly the same model which is too generalized to represent different users’ distribution . By tuning the γ , we find the balance between generalization and personalization . In the future , our vision is γ value can be an user controlled setting . Users can freely select how personalized their own models should be . Our general recommendation is : when the user has only few data , it is always intuitive to have the consensus model to be the cold start model . After the user accumulates the personal data , the model can be relaxed a little bit to support divergence . When the user has enough personal data , the model can be personalized completely , and the consensus model has less and less influence .
3.2 Learning Curves
One of the benefits of the distributed personalization is to leverage community knowledge to improve individual user ’s learning curve . In this experiment , we vary the number of training samples to show the progress of the accuracy improvement . Figure 9 and 10 show the testing ROC curves using personalized models trained on different amounts of training samples . The training samples are set as 1 sample per user , 3 samples per user , 5 samples per user , and 10 % of an user ’s data for each curve , respectively . The corresponding area under curve ( AUC ) is shown in Table 1 . From 1 sample to 3 samples , the learning speed is really fast that increases the accuracy a lot which demonstrates that our framework enables the users to share the knowledge with each other efficiently . On the contrary , if we use consensus model , the improvement of the accuracy is slow compare to the increase of training samples . It is true that the consensus model performs better than the personalized model when using only 1 training sample per user . It means 1 sample of the personal data is not sufficient enough to represent the intra user variance for each user . With more training data , it covers the variation within each user , ie , it adapts individual user well , the accuracy of personalized models starts to outperform the consensus model .
This experiment demonstrates that the learning speed of personalized model is fast as the size of training set increases , ie , users only need few samples to improve their personal models . This is relatively important when we consider the applications in
Figure 7 : ROC curves of personalized model and baseline ( consensus ) model in Facial expression data set . γ id set as 50 in this case . and feature extraction . The features we extract are popular features used in computer vision domain , including , spatial filter responses , temporal filter responses , statistical values , and geometry values .
In addition to the accuracy improvement , our framework reduces the privacy concern of the users . People are usually unwilling to share their privacy data to others , especially this kind of visual data from which the subject ’s identity can be easily revealed . In the existing framework , in order to train a facial expression classifier , the personal data must be uploaded to somewhere hence loses the privacy . In our distributed framework , training can be performed locally , such as in a smart device or private cluster . The personal data does not need to be uploaded to other places . Although the personal model is still exposed to others , the privacy of the data itself is preserved which largely mitigates the concerns of the privacy . 3.1 Influence of the Personalization
First , we compare the difference between the results with personalization and without personalization in accuracy . For each user , we use 5 positive samples and 5 negative samples for training . Figure 7 shows the ROC curve of the overall accuracy among all the users . When the personalization is considered , the accuracy is better than the one without personalization(baseline model ) . The baseline model is obtained based on conventional ADMM where all the users train a single consensus model . This result is not surprising since one model does not fit all the users . This result also tells during the real world application , the personalized models would provide better satisfaction to the users than the consensus model .
As we discussed in the previous section , the strength of personalization plays the key role for the generalization ability . To demon
1993 Figure 8 : The distributions of the ROC curves of using personalized models for all the users . For each user , we use 5 positive samples and 5 negative samples for training . Each subfigure corresponds to different personalization strengths ( γ = 10−6 , 10−3 , 1 , 50 , 103 , 106 ) . The accuracy is the highest when γ = 50 and drops when γ is too large or small .
1994 commercial products . In most of the existing personalization products/services , eg , ads , recommendation system , users are usually asked to provide some personal information , answer questions , or do data collection and tagging . However , users sometimes do not have patience to provide so much information . With our framework , even though users only provide little amount of data , the system can make the models adapt quickly . It encourages the users to provide more data since they can feel the improvement of the accuracy . With more data , the models fit individuals better , and the whole community is also benefited from the ability of knowledge sharing in our framework .
Figure 9 : ROC curves of the personalized models trained on different number of samples . γ = 1 in this experiment .
Figure 10 : ROC curves of the baseline ( Consensus ) models trained on different number of samples . γ = 1 in this experiment .
4 . JOBS YOU MIGHT BE INTERESTED IN
( JYMBII )
One of the premium member value propositions of LinkedIn is the jobs recommendation product , Jobs You May Be Interested In ( JYMBII ) , where LinkedIn members can impress active job postings relevant to their professional profiles . It is one of most valuable
Table 1 : Area under curve in figures[9,10 ]
Model
Personalized
Baseline
1 sample 0.7771 0.8277
3 samples
5 samples
10 % of data
0.8812 0.8373
0.9084 0.8383
0.936 0.8623 product contributing to member engagement on LinkedIn . Members can consume JYMBII recommendations in different channels , such as LinkedIn homepage feeds , Email , and LinkedIn Jobs Home ( ie the ’jobs’ tab on LinkedIn home page ) .
As many members maintain an up to date professional profile on LinkedIn , it is very important that JYMBII can recommend highly relevant and personalized jobs for members , whether they are active job seekers or passive ones who just want gain some awareness of the job market . Without personalization , there are circumstances that a bad job recommendation can significantly impair user ’s satisfaction , for example , an executive in a large size company may feel offended by seeing entry level job recommendation , an experienced professional may feel uncomfortable receiving intern level job recommendations . Even though current recommendation engine has already taken into consideration of members’ personal career characteristics , such as their professional skills , current job position , and their next job preference ( either explicit or implicit ) , the model coefficients for these features are global as we train a single model for all of our members , which inevitably return less satisfactory recommendations for certain members . The Asynchronous ADMM model we propose , on the other hand , will train a personalized model for each member with sufficient job seeking history , and still maintain a global consensus model for those without enough historical record . In doing so , it can provide additional personalization in a way such that it greatly improve user satisfaction for member ’s personal job seeking experience .
In order to compare personalization ADMM model against current baseline consensus model trained with synchronized ADMM setting , we have trained our personalization ADMM model with different gammas , which controls the strength of personalization , with larger gamma indicating more personalization . Each example is a ( member , job , label ) tuple , with label being either 1 or 0 indicating whether or not the member has applied the job . The feature set is the same for baseline and treatment , it mainly includes field based content matching features and member preference features . Each content matching feature is a cosine similarity score between the two fields’ tf.idf vectors , with one field from member profile and the other field from job posting , for example , member ’s experience field matching against job ’s description field , member ’s profile summary field matching against job ’s desired skills field , etc . 11 illustrates the field layout of a ( member,job ) pair on LinkedIn . The member preference features include job location preference , industry preference , and company size preference . They can be members’ explicit preference they set on the web UI , or their latent preference mined by a model [ 24 ] . The final model includes one hundred of such features for each example , and the dataset includes about a million samples . Figure 12 illustrates the ROC curves and their AUCs for model comparison .
The personalized model has the best AUC of 0.91 with gamma = 100 , comparing to AUC of 0.79 for the most personalization model ( with gamma = 1 ) , and AUC of 0.86 for the least personalization model ( with gamma = 10000 ) . It suggests that neither personalization nor extreme personalization performs the best , but the optimal strength for personalization is somewhere falls in between .
1995 Figure 12 : ROC of JYMBII baseline model and personalization models
Comparing to the baseline without personalization ( AUC=0.86 ) , it is interesting to see all of the personalization models performs much better in the operation area with low false positive rates ( eg false positive rate less than 0.1 ) , this area is critical as it aligns better with the online system which observes much more negative examples than positive examples , thus it is a necessity to operate with a low false positive rate all the time .
5 . CONVERGENCE EXPERIMENT
Due to the asynchronous nature of Algorithm 1&2 , it is difficult to implement such algorithms in a typical Map/Reduce architecture [ 11 ] , which requires a synchronization step between each map/reduce stage . We develop an asynchronous computation framework based on the Publish/Subscribe message paradigm [ 14 ] to facilitate the parallel computation interfaces . The framework consists of a master node and multiple worker nodes . Each worker holds a subset of users and their data , initializes a personal model and subscribes a message channel for each user . The training events are triggered through user interactions . The corresponding worker fetches the current best known consensus model after receiving a training event , performs a local optimization and sends the updated personal model back to the master . The consensus model then gets updated at the reception of each updating event . In synchronous ADMM ( SYNC ADMM ) , all personal models start training at the same time using the same consensus model , the next round is triggered after all personal models are merged into the consensus . For multi versioned ADMM ( MV ADMM ) , all personal models start training at approximately the same time using their current known consensus model . The consensus model does not block any training event in any single round and merges continuously , but it waits for the completion of all user models before triggering another round . In asynchronous ADMM ( ASYNCADMM ) , each user training event only depends on the completion of the user ’s previous round . The consensus model merges any reported personal model immediately . During the entire experiment process , the total number of training events triggered is more than 10 million . In Figure 13 , for each ADMM model , we divide the
Figure 11 : A Member Profile ( upper ) and A Job Posting ( lower )
1996 cation to efficiently leverage computational resources — a natural extension to the proposed algorithm .
References [ 1 ] A . Agarwal , O . Chapelle , M . Dudík , and J . Langford . A reliable effective terascale linear learning system . J . Mach . Learn . Res . , 15(1):1111–1133 , Jan . 2014 .
[ 2 ] A . Agarwal and J . C . Duchi . Distributed delayed stochastic In Proceedings of the 51th IEEE Conference optimization . on Decision and Control , CDC 2012 , December 10 13 , 2012 , Maui , HI , USA , pages 5451–5452 , 2012 .
[ 3 ] S . Ben David , J . Gehrke , and R . Schuller . A theoretical framework for learning from a pool of disparate data sources . In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD ’02 , pages 443–449 , New York , NY , USA , 2002 . ACM .
[ 4 ] V . Bettadapura . Face expression recognition and analysis : the state of the art . arXiv preprint arXiv:1203.6722 , 2012 .
[ 5 ] M . Bilenko and M . Richardson . Predictive client side profiles for personalized advertising . In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD ’11 , pages 413–421 , New York , NY , USA , 2011 . ACM .
[ 6 ] S . Boyd , N . Parikh , E . Chu , B . Peleato , and J . Eckstein . Distributed optimization and statistical learning via the alternating direction method of multipliers . Found . Trends Mach . Learn . , 3(1):1–122 , Jan . 2011 .
[ 7 ] E . CandÃ´ls and B . Recht . Exact matrix completion via convex optimization . Foundations of Computational Mathematics , 9(6):717–772 , 2009 .
[ 8 ] C . Chen , B . He , Y . Ye , and X . Yuan . The direct extension of admm for multi block convex minimization problems is not necessarily convergent . Mathematical Programming , pages 1–23 , 2014 .
[ 9 ] Y . Chen , D . Pavlov , and J . F . Canny . Large scale behavioral targeting . In Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD ’09 , pages 209–218 , New York , NY , USA , 2009 . ACM .
[ 10 ] A . Cotter , O . Shamir , N . Srebro , and K . Sridharan . Better mini batch algorithms via accelerated gradient methods . In J . Shawe Taylor , R . Zemel , P . Bartlett , F . Pereira , and K . Weinberger , editors , Advances in Neural Information Processing Systems 24 , pages 1647–1655 . Curran Associates , Inc . , 2011 .
[ 11 ] J . Dean and S . Ghemawat . Mapreduce : Simplified data processing on large clusters . Commun . ACM , 51(1):107–113 , Jan . 2008 .
[ 12 ] O . Dekel , R . Gilad Bachrach , O . Shamir , and L . Xiao . Optimal distributed online prediction using mini batches . J . Mach . Learn . Res . , 13:165–202 , Jan . 2012 .
[ 13 ] W . Deng , M J Lai , Z . Peng , and W . Yin . Parallel MultiBlock ADMM with o(1/k ) Convergence . ArXiv e prints , Dec . 2013 .
Figure 13 : Convergence rate of different ADMM models . Note that the objective function is in log scale . time window into 300 buckets and only report the average objective function value observed within each time bucket . We choose the starting time when all user models are trained at least once . The flat blank periods within SYNC ADMM and MV ADMM models are the blocking periods when they wait for all personal models being merged into the consensus . As demonstrated in Figure 13 , ASYNC ADMM converges at a close to identical rate compared to other models , but performs 2∼4 times faster by eliminating all blocking period .
6 . CONCLUSION AND DISCUSSION
In this paper , we propose a distributed personalization framework and present its successful application to two industry problems . Our approach enables distributed personalization not only for traditional recommendation problems , but also for recognition . Provided each individual task can be formulated as a convex optimization , our framework can scale , for both training and personalization , to millions of simultaneous users . One of the major contribution that enables this distributed learning system is a novel Asynchronous ADMM algorithm that removes all network I/O wait barriers to maximize the CPU throughput . One of the interesting features of our publish/subscribe based distributed computation approach allows worker nodes to execute on personal devices . Unlike other popular distributed learning algorithms , we do not rely on a decaying step size to control convergence . Instead , Asynchronous ADMM keeps track of versions of the consensus model , thereby avoiding complicated learning rate adjustments . In the future , we plan to extend our approach to the real online setting . Problems like warm start , online model adaptation , and in session personalization will be studied in depth . Furthermore , we currently employ only one master to update the consensus . This wastes some network bandwidth . Much research has suggested that a Minimum Spanning Tree topology can provide optimal network communi
1997 [ 14 ] G . Eisenhauer , K . Schwan , and F . E . Bustamante . Publishsubscribe for high performance computing . Internet Computing , IEEE , 10(1):40–47 , 2006 .
[ 15 ] T . Evgeniou and M . Pontil . Regularized multi–task learning . In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD ’04 , pages 109–117 , New York , NY , USA , 2004 . ACM .
[ 16 ] M . Hong and Z Q Luo . On the Linear Convergence of the Alternating Direction Method of Multipliers . ArXiv e prints , Aug . 2012 .
[ 17 ] Y . Koren . Factorization meets the neighborhood : A multifaceted collaborative filtering model . In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD ’08 , pages 426–434 , New York , NY , USA , 2008 . ACM .
[ 18 ] Y . Koren , R . Bell , and C . Volinsky . Matrix factorization techniques for recommender systems . Computer , 42(8):30–37 , Aug . 2009 .
[ 19 ] D . D . Lee and H . S . Seung . Algorithms for non negative maIn In NIPS , pages 556–562 . MIT Press , trix factorization . 2000 .
[ 20 ] M . Pantic and L . J . M . Rothkrantz . Automatic analysis of facial expressions : The state of the art . Pattern Analysis and
Machine Intelligence , IEEE Transactions on , 22(12):1424– 1445 , 2000 .
[ 21 ] C . Perlich , B . Dalessandro , T . Raeder , O . Stitelman , and F . Provost . Machine learning for targeted display advertising : Transfer learning in action . Mach . Learn . , 95(1):103–127 , Apr . 2014 .
[ 22 ] W . Shi , Q . Ling , K . Yuan , G . Wu , and W . Yin . On the linear convergence of the admm in decentralized consensus optimization . Signal Processing , IEEE Transactions on , 62:1750– 1761 , February 2014 .
[ 23 ] L . Veer and R . Bernards . Enabling personalized cancer medicine through analysis of gene expression patterns . Nature , 452:564–570 , 2008 .
[ 24 ] J . Wang and D . Hardtke . User latent preference model for better downside management in recommender systems . In International World Wide Web Conference , WWW ’15 , Florence , Italy , May 2015 .
[ 25 ] C . Zhang , H . Lee , and K . G . Shin . Efficient distributed linear classification algorithms via the alternating direction method of multipliers . In Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics , AISTATS 2012 , La Palma , Canary Islands , April 21 23 , 2012 , pages 1398–1406 , 2012 .
1998
