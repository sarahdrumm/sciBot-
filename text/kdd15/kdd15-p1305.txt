Predicting Winning Price in Real Time Bidding with
Censored Data
Wush Chi Hsuan Wu
Dept . of Electrical Engineering
National Taiwan Univ .
Taipei , Taiwan wush978@gmail.com
Mi Yen Yeh
Academia Sinica Taipei , Taiwan miyen@iissinicaedutw mschen@cceentuedutw
Inst . of Information Science
Dept . of Electrical Engineering
Ming Syan Chen
National Taiwan Univ .
Taipei , Taiwan
ABSTRACT In the aspect of a Demand Side Platform ( DSP ) , which is the agent of advertisers , we study how to predict the winning price such that the DSP can win the bid by placing a proper bidding value in the real time bidding ( RTB ) auction . We propose to leverage the machine learning and statistical methods to train the winning price model from the bidding history . A major challenge is that a DSP usually suffers from the censoring of the winning price , especially for those lost bids in the past . To solve it , we utilize the censored regression model , which is widely used in the survival analysis and econometrics , to fit the censored bidding data . Note , however , the assumption of censored regression does not hold on the real RTB data . As a result , we further propose a mixture model , which combines linear regression on bids with observable winning prices and censored regression on bids with the censored winning prices , weighted by the winning rate of the DSP . Experiment results show that the proposed mixture model in general prominently outperforms linear regression in terms of the prediction accuracy .
Categories and Subject Descriptors : H35 [ Information Storage And Retrieval ] : Online Information Services ; I26 [ Artificial Intelligence ] : Learning
Keywords : Demand Side Platform , Real Time Bidding , Display Advertising , Learning with Partial Labels
1 .
INTRODUCTION
In recent years , programmatic advertising has been taking over the online ad industry . To enable automatic selling and purchasing ad impressions between advertisers and publishers through real time auctions , Real Time Bidding ( RTB ) is quickly becoming the leading method according to the white paper by Google Inc . [ 8 ] .
In contrast to the traditional online ad market , where a certain amount of impressions is sold at a fixed rate , RTB allows advertisers to bid each impression individually in real time at a cost based on impression level features . In the
RTB paradigm , publishers manage and sell their inventories of ad impressions via the Supply Side Platform ( SSP ) while advertisers can bid and manage their ads across multiple inventory sources via the Demand Side Platform ( DSP ) . While an impression is loaded in a web browser , information about the user reading this webpage will be passed to the advertisers . DSP , on behalf of the advertisers , will help decide whether to bid the impression and set a bid to the ad exchange . Usually RTB takes the second price auction , where the advertiser with the highest bidder wins and pays the second highest bid . Interested readers are referred to the introduction and tutorials in [ 17 ] and [ 19 ] for more details about RTB .
Since a DSP is the agent of advertisers , it has to know how to compute the bid properly . The common paying models for the advertiser include cost per mille ( CPM ) for each impression , cost per click ( CPC ) for each click , and cost peraction ( CPA ) for each conversion . Therefore , DSPs usually estimate the click through rate ( CTR ) and/or the conversion rate of the impression so that they can set a proper bid for advertisers to maximize their profits or maximize the revenue with a limited budget . From the standpoint of DSP in the RTB exchange , we propose in this paper to predict the winning price of the bid directly . The main reason is that the winning price is usually the same as the cost of winning the bid . There is usually a budget constraint for DSPs to bid the impressions , and the cost is hence an important factor of the bidding decision making as stated by Zhang et al . [ 21 ] . Furthermore , the winning price is an indicator of the importance of an ad impression or the importance of a user in the market . The information of the market value might help DSPs predict the CTR and conversion rate . Our experiment shows that adding the winning price to the feature sets in the prediction of CTR can increase the AUC value from 0.708 to 07331
Apparently , DSPs meet a grand challenge to predict the winning price . Ghosh et al . [ 7 ] first described that the partial observable exchange makes them hard to explore the pattern of winning prices . According to the mechanism of the modern RTB display advertising , the winning price is only observable for the DSP who wins the bid . For the lost bids , DSPs can only observe the lower bound , which is their own bidding price . Moreover , the modern RTB process introducing the soft floor price makes the winning price of some winning bids also unobservable , of which more details
1The data of the experiment is the day 2013 06 07 of the dataset iPinYou session 2 . The details of the dataset and the features are shown in Sec 41
1305 are given in Sec 2 . In such cases , only the upper bound or the lower bound of the winning price , which is the paying price of the bid , is observed . This kind of partially observed winning price is called censoring , and is handled in many fields such as survival analysis [ 12 ] and econometrics [ 15 ] .
To solve the problem of predicting the winning price for a DSP , say Di , we propose to learn a mixture model of winning price from historical bids , where the winning prices are either observable or unseen for Di . Our rationale is as follows . Intuitively , we can train a linear regression model based only on the bids , of which the winning price is observable to Di ( or for simplicity , we call these bids winning bids of Di hereafter ) . Apparently , the model could be overfitting to these limited winning bids . As a result , we further propose to use the censored regression model [ 9 ] to consider also the censored data , which are the bids with unobservable winning prices ( or we call them losing bids of Di hereafter for simplicity ) . However , the real bidding data in fact violate an assumption of the censored regression model : the pattern of winning prices on observed data and that on censored data should be consistent . We will show this violation in Sec 42 To remedy the violation , we propose to mix the result of linear regression model and that of the censored regression model , weighted by the winning rate of Di on a bid . This is because the linear regression model works better on the winning bids in general and the censored regression model takes the censored information into account . Therefore , if the winning rate , which is the chance that Di wins the bid , is higher , then the linear regression model accounts for more percentage in the mixture model , and vice versa .
Our contributions can be summarized as follows . We are the first , to our knowledge , to predict the winning price on the censored data by applying the censored regression model . Furthermore , we propose an approach to predicting the winning rate for a DSP on a bid and leverage it to mix the results of the linear regression model on winning bids and the censored regression model . We conduct a series of experiments on real RTB data sets from two existing DSPs . Experiment results show that our proposed mixture model outperforms the baseline methods in all cases in terms of having the smaller predicting errors . This shows the effectiveness of our proposed model in practice .
The rest of the paper is organized as follows . Section 2 reviews the auction process of RTB and describes the challenge of modeling winning prices . Section 3 presents our methodology to predict the winning price . The experiment results are provided in Section 4 . We give a brief literature survey related to our topic in Section 5 , followed by the conclusion in Section 6 .
2 . PRELIMINARIES
In this section , we describe the mechanism of modern RTB and define the winning price we discussed in this paper . Also , we show the availability of the true winning price in different bidding cases and thus the challenges of modeling it . 2.1 The Censoring of Winning Price
We only focus on the interaction between a DSP and the ad exchange in the RTB process . Suppose we are the DSP and our bidding price is b . The winning price we discuss here refers to the price value that we , as a DSP , have to offer at least to win the bid . In the current RTB process , the publishers can set a soft floor price and the hard floor price for each ad impression . If all the bids from all DSPs are below the hard floor price , the impression will fail to be sold . In this case , the winning price is the hard floor price set by the publisher of the ad impression . On the other hand , if there is at least one bidding price greater than the hard floor price , the winning price is then the highest bidding price from all other DSPs .
Usually RTB runs a second price auction . Suppose the highest bidding value of all other DSPs is b2 and we win the bid , then we only need to pay b2 . Thus the winning price is b2 . In practice , however , the winning price is affected by the soft floor price set by SSPs at the same time . If b is smaller than the soft floor price of the ad impression , RTB will ask the winner to pay b . If b is larger than the soft floor price of the ad impression and b2 is smaller than the soft floor price , RTB will ask the bidder to pay the soft floor price instead of b2 . In this case , the winning price is leftcensored , which means we only know an upper bound of the winning price . If we lose a bid , the winning price is not available and generally right censored , ie , our bidding price is a lower bound of the winning price . The winning price of each bid can be known by us only if the RTB process makes the bidding price public or every DSP makes its own bidding public . In summary , the auction process and the resulting data censoring are shown in Fig 1 . 2.2 Problem Statement
The goal of this paper is to learn the winning price , in the aspect of a DSP , with historical RTB bids and to deal with the problem of various winning price censoring at the same time . We aim to propose a machine learning based model that can help a DSP to predict the winning price and thus decide the bidding price .
3 . METHODOLOGY
In this section , we describe the proposed method to predict winning price . We state the true , but unavailable in practice , model of the winning price and how we approximate it with a statistical model in Sec 31 The modeling of the censored data is introduced in Sec 32 We show how and why to use the estimated winning rate to mitigate the impact of the unrealistic assumption in Sec 33
For ease of presentation , we assume that all winning bids are observed and all losing bids are right censored in the following content . We will also show how to extend our model to left censored data . 3.1 Modeling Winning Price
Suppose there are J DSPs , D1 , , DJ , in the RTB market and without loss of generality the following process is described from the standpoint of DSP D1 . For the ith bid , we denote the true winning price by vi , the observed one by wi , and the bid placed by D1 by bi . If D1 wins the bid , then wi = vi . Otherwise , wi=0 .
In practical RTB , SSPs will deliver several features of the ad impression for sale to all DSPs with the bid request , which are denoted by x1 i here . Of course , DSPs themselves can collect features other than x1 i of the bid on their own and do not make them available to others . We denote the features collected and observed only by D1 as x2 i and features collected by other DSPs and unknown to D1 as x3 i . For example , according to the report of iPinYou [ 21 ] , the feature
1306 Figure 1 : Flow chart of the RTB auction and the resulting winning price censoring . i for iPinYou and is deemed as x3
User Tags is only observed by iPinYou . Hence this feature belongs to x2 i by other DSPs in the market . In the bidding process , each DSP offers a price according to its bidding strategy function fj(· ) , which could be either deterministic or stochastic , for Dj and the available features . i , x3 Therefore , the ad exchange receives bids f1(x1 i ) , , and fJ ( x1 i ) . Suppose the hard floor price of the publisher , which is the webpage owner of the auctioned impression , is denoted as fp(x1 i ) because the publisher is one of the public feature . Then we can model the true winning price of the ith bid for D1 as vi = max{fp(x1 i ) , , fJ ( x1 i ) , f2(x1 i )} . i , x2 i ) , f2(x1 i , x3 i , x3 i , x3
( 1 )
D1 will win the impression if it offers the price higher than vi .
In reality , x3 i , and thus vi in Eq ( 1 ) , is unknown to D1 . We propose to use a linear regression model to approximate the true winning price as follows . vi ≈ βT xi + εi , i , x2
( 2 ) where xi = {x1 i} is the set of all features observed by D1 , βT xi captures the expected value of the winning price , and εi is assumed to be an independent and identically distributed ( iid ) and normally distributed random variable with zero mean and σ2 variance . With Eq ( 2 ) , we can predict the winning price vi based on the features xi and the vector β . Therefore , we call β an estimator in the following content .
Although the linear regression model with iid normal distributed assumption of εi may not be the best fit for predictive analysis , as we can see , this model is generally elegant and effective to capture the censored information , which we will discuss in the next subsection , compared to other sophisticated model .
We use the negative log likelihood function to measure how good Eq ( 2 ) can approximate the observed winning price :
−log(φ( wi − βT σ lmxi
) ) ,
( 3 ) i∈W where W represents all the historical bids that D1 wins and thus it observed the true winning prices . φ is the probability density function ( pdf ) of the standard normal distribution . The vector βlm can be learned from historical bidding data by finding the value that minimizes Eq ( 3 ) .
3.2 Modeling Censored Data
In Eq ( 3 ) , the estimator βlm learns the winning bids of D1 . However , we may lose too much winning price information from those bids that D1 lost . Second , the winning price from the observed data usually is lower than the winning price of the censored data as shown in Sec 4.1 , and the estimator βlm hence usually underestimates the winning price on the censored data . Therefore , in this section we describe how to apply the censored regression model to explore the information from those losing bids .
The linear regression described in Eq ( 2 ) predicts not only the winning price but also the winning rate . That is , the probability that D1 will win the ith bid , can be derived
1307 from Eq ( 2 ) as follow .
P ( vi < bi ) = P ( εi < bi − βT xi )
= Φ(
) ,
( 4 ) bi − βT xi
σ where Φ is the cumulative density function of the standard normal distribution . Note that one can modify Eq ( 4 ) to the left censoring case by changing ” < ” to ” > ” properly . Therefore , the censored regression model can be modified to include the unobserved winning price due to the soft floor price discussed in Sec 2 .
The idea of censored regression is to combine Eq ( 3 ) and Eq ( 4 ) . Eq ( 3 ) measures how well the model in Eq ( 2 ) approximates the observed data and Eq ( 4 ) measures how well the model in Eq ( 2 ) approximates the censored data . Following the principle of maximum likelihood and the Tobit model [ 16 ] , the estimator βclm is learned via minimizing the negative log likelihood function as follows . −log(Φ(
−log(φ( clmxi
) ) + bi − βT clmxi σ
) ) , wi − βT σ i∈L i∈W
( 5 ) where W represents the set of all the winning bids and L represents the set of all the losing bids of D1 , respectively . Apparently , Eq ( 5 ) is a mixture of winning price prediction in Eq ( 2 ) and the winning rate prediction in Eq ( 4 ) . Because the censored regression learns from more data compared to the linear regression , which learns from the winning bids only , the estimator of the censored regression βclm is supposed to have a smaller variance compared to the estimator βlm .
However , there is an important assumption for deriving the likelihood function in Eq ( 5 ) : the pattern of the winning prices on the observed data should be the same as that of the winning prices on censored data . In Eq ( 2 ) , the estimator β is a realization of the pattern of winning price because β describes the changes of winning price after changing the features xi . More specifically , the assumption for deriving the likelihood function in Eq ( 5 ) is that β estimator on the observed data and that on the censored data are the same . If these patterns are different , the estimator βclm becomes a mixture of two different βs . Therefore , the estimator βclm is biased when it is used to predict the winning price on the observed data . Similarly , the estimator βlm is biased when it is used to predict the winning price on the censored data . In practice , the estimator βlm usually underestimates the winning price , and βclm usually provides higher estimation than βlm does because it learns the censored data .
We will further show in Sec 4.2 that the patterns of the winning price on the observed data are different from those on the censored data in practice . Therefore , it is hard to conclude that βclm will be better than βlm or not because the assumption is invalid . To solve this problem , we propose a mixture model in the next subsection . 3.3 Mixture Model
We introduce the censored regression to learn the pattern of winning price from both observed and censored data . However , the patterns on the observed and censored data are inconsistent , which may violate the assumption of the censored regression model . Therefore , in this section , we propose a mixture model to address the effect of mixing different patterns .
The winning rate prediction provides the likelihood of the bid whether it is going to be observed or not . Therefore , we use the predicted winning rate as the weight to mix the estimator βlm and the estimator βclm . That is to say , we predict the winning price according to the following equation . vi = [ P ( vi < bi)βlm + ( 1 − P ( vi < bi))βclm]T xi
= βT mixxi .
( 6 )
Unlike that of the estimator βclm , the performance of the estimator βmix is better than that of the estimator βlm if the winning rate prediction is accurate enough . Our reasons are as follows . If the winning rate prediction is absolutely correct , ie the winning rate is 1 on the observed data and 0 on the censored data , the estimator βlm outperforms on the observed data and the estimator βclm outperforms on the censored data , the performance of the estimator βmix will be the same as that of βlm on the observed data , and will be the same as that of βclm on the censored data . That is to say , the winning rate helps βmix to pick the better estimator . Note that , in practice , the winning rate prediction will not be absolutely correct and the estimator βclm might not outperform βlm on the censored data . Also , the winning price on the censored data is usually higher than the winning price on the observed data because these bids are lost due to the bidding price is not high enough . Therefore , βlm underestimates the winning price at censored data while βclm provides a higher estimation . If βclm underperforms , it suggests that βclm overestimates too much . Interestingly , βmix aggregates the merits of these two estimators and provides a better estimation . βmix gives higher estimation than βlm does because βclm is higher and βmix gives more conservatively lifting of the estimation compared to βclm .
We will compare these estimators βlm , βclm , and βmix on all data , observed data and censored data in Sec 4.3 and Sec 44
To realize the mixture model , however , we understand that Eq ( 4 ) is not directly applicable to predict the winning rate P ( vi < bi ) . First , the accuracy of Eq ( 4 ) depends on the distribution of the random variable vi . Therefore , the accuracy is affected by the accuracy of the winning price prediction . Second , predicting winning rate with Eq ( 4 ) with the estimator βlm or βclm might introduce hidden dependency because we already mix them to derive the estimator βmix . In view of these , we do not use the model in Eq ( 4 ) directly .
Instead , we use the logistic regression to predict the win ning rate used in Eq ( 6 )
P ( vi < bi ) =
1
1 + e−βT lr xi
.
( 7 )
The logistic regression describes the winning rate based on the features directly , so it is not derived from Eq ( 2 ) . Therefore , it is not affected by the accuracy of the winning price and it does not introduce any hidden dependency from any estimator derived from Eq ( 2 ) .
4 . EXPERIMENTS
In this section , we shall address the following questions . ( 1 ) Are the patterns of the winning price on the observed data and those on the censored data different ? ( 2 ) Does the censored regression model have better performance on predicting winning price compared to the linear regression
1308 day
2013 06 06 2013 06 07 2013 06 08 2013 06 09 2013 06 10 2013 06 11 2013 06 12 day
2013 10 19 2013 10 20 2013 10 21 2013 10 22 2013 10 23 2013 10 24 2013 10 25 2013 10 26 2013 10 27
# of bids # of win . bids WR EWR WR AUC Avg . WP Avg . WP on W Avg . WP on L 1,821,479 1,806,062 1,634,967 1,651,630 1,920,576 1,745,905 1,657,578
1,514,416 1,524,314 1,352,038 1,366,097 1,603,798 1,461,085 1,378,728
74.86498 72.31279 81.14319 81.31667 79.83572 79.62260 79.99693
52.46772 51.12051 58.48506 58.95707 58.91341 58.91626 58.80196
185.3269 186.9674 189.4200 188.2934 185.7621 185.8431 184.7920
0.83 0.85 0.83 0.83 0.83 0.86 0.85
0.89 0.90 0.87 0.88 0.91 0.85 0.84
0.83 0.84 0.83 0.83 0.84 0.84 0.83
Table 1 : The statistics for the dataset iPinYou Season 2 .
# of bids # of win . bids WR EWR WR AUC Avg . WP Avg . WP on W Avg . WP on L 228,133 214,295 848,760 681,700 226,791 245,897 318,240 268,380 110,467
50.89386 51.95333 51.40931 52.88855 53.44846 45.58807 49.19676 49.18726 48.43570
86.73686 87.89860 90.08863 91.51073 89.27304 74.18814 82.27562 89.61273 86.00950
193.3647 192.9054 195.9472 200.3125 198.6853 190.2397 194.2589 204.9104 209.7080
170,739 159,646 621,626 503,108 170,850 197,279 245,671 198,709 84,730
0.75 0.74 0.73 0.74 0.75 0.80 0.77 0.74 0.77
0.85 0.87 0.89 0.87 0.83 0.83 0.85 0.89 0.90
0.75 0.74 0.73 0.74 0.75 0.80 0.77 0.74 0.77
Table 2 : The statistics for the dataset iPinYou Season 3 . model trained from only the historical winning bids ? ( 3 ) Does the proposed mixture model successfully mitigate the violation of the censored regression model and leverage the results of both linear and censored models ? 4.1 Settings
For the following experiments , we use two real datasets , the iPinYou RTB data set2 and the Bridgewell dataset3 . Since both the two DSPs can only provide the winning prices of their own won bids , we simulate the bidding results of both winning and losing bids as follows . For each historical bid , we divide the bidding price originally offered by each DSP by a factor as the new bidding price . If not specified , the factor is set to 2 . The new bidding price is compared to the original winning price to produce the simulated winning ( new bidding price > original winning price ) and losing bids . As such , we can always have the ground truth of the winning price of each bid and the pattern of the winning prices will not vary . Note that winning bids and observed data are used interchangeably , and so does losing bids and censored data . Although the winning price is obtained from our simulation results instead of randomly drawn from the winning prices in the real market , we believe the experimental study of the winning price pattern conditioned on the simulated winning and losing bids reveals reasonable trends accordingly . We release the codes for experiments on the iPinYou dataset on https://githubcom/wush978/KDD2015wpp
There are three seasons of RTB records on the iPinYou dataset . We use the data of season 2 and season 3 because the private feature User Tags , which corresponds to x2 i of iPinYou , is not provided in season 1 . All the features we used from the logs of iPinYou are shown in Table 4 . These features are categorical and converted to binary features via hashing trick proposed by Weinberger et al . [ 18 ] . This kind
2http://datacomputational advertisingorg/ 3The dataset is sampled from the real RTB logs from Bridgewell Inc(http://wwwbridgewellcomtw/index html ) , which is one of DSPs in Taiwan . of feature extraction is widely used in the literature such as He et al . [ 10 ] , Zhang et al . [ 21 ] and Chapelle [ 3 ] . The hash size in the experiments is set to 220 .
The proposed mixture model of the winning price requires the winning rate as input which is predicted by the simulated bidding result . According to Zhang et al . [ 20 ] , the expected KPI , which is defined according to different business models and usually is the expected CTR or the expected conversion rate , is an important feature for predicting winning rate . As a result , we put the expected CTR into the feature set x . Because the iPinYou dataset only provides the log of click events instead of the expected CTR directly , we estimate the CTR on our own by the logistic regression as stated by Zhang et al . [ 21 ] . The accuracy of our expected CTR is shown in Table 5 . Compared to the benchmark reported by Zhang et al . [ 21 ] , the total AUC of our model is only 0.0009 lower than theirs . Hence , the expected CTR is good enough for being a part of x .
To simulate the online environment , we predict CTR and the winning rate in an online fashion . After receiving a bid request , the model predicts the corresponding CTR and the winning rate immediately . Then we check whether the impression that occurs ten minutes ago is clicked or not and update the CTR model . Similarly , we check the result of bidding that occurs one minute ago and update the winning rate model . The updated model takes effect immediately . Both the CTR and winning rate models are optimized by FTPRL proposed by McMahan et al . [ 13 ] .
There are 9 day logs in the Bridgewell dataset . These logs record the bidding history for a specific advertiser . Unlike the iPinYou dataset , there are real expected KPI in the dataset , so we estimate the winning rate according to the features and expected KPI directly . The features we used are similar to those in the iPinYou dataset . Because there are some extremely high winning prices in the Bridgewell dataset , we convert the winning prices to a logarithmic scale before using them .
The data are split by the day according to the occurrence time of the bid request . Table 1 , Table 2 and Table 3 show
1309 day
2014 12 01 2014 12 02 2014 12 03 2014 12 04 2014 12 05 2014 12 06 2014 12 07 2014 12 08 2014 12 09
# of bids # of win . bids WR EWR WR AUC Avg . WP Avg . WP on W Avg . WP on L 2,799,327 3,042,964 2,667,145 2,418,780 1,734,819 1,209,382 1,314,122 1,721,061 1,675,202
1,911,988 1,952,962 1,712,442 1,487,997 972,121 646,522 707,196 949,686 933,429
256.8171 253.0018 254.9517 259.4486 265.4780 236.3764 230.5689 251.8090 278.5520
254.3374 253.0107 251.9175 253.1226 258.3856 227.0063 220.1978 244.4737 268.3400
262.1601 252.9859 260.3940 269.5615 274.5178 247.1393 242.6534 260.8399 291.4026
0.68 0.64 0.64 0.62 0.56 0.53 0.54 0.55 0.56
0.68 0.64 0.64 0.61 0.56 0.53 0.54 0.55 0.56
0.77 0.77 0.76 0.74 0.72 0.71 0.71 0.73 0.72
Table 3 : The statistics for the dataset Bridgewell . the basic statistics of the dataset , where each row shows the statistics of a specific day . In these tables , # of bids and # of win . bids are the count of total bids and simulated winning bids respectively . WR is the resulting simulated winning rate which equals # of win . bids / # of bids . EWR is the expected winning rate according to our winning rate model in Eq ( 7 ) . WR AUC is the Area Under Curve ( AUC ) of our winning rate model based on our simulated bidding results . Avg . WP , Avg . WP on W , and Avg . WP on L are the average winning price on all bids , the winning bids , and the censored bids , respectively .
It can be seen that the characteristics of the three datasets are very different . First , the winning rate varies significantly among three tables . Also , the proportions of the winning bids to the total bids are also different . The number of winning bids is almost equal to the number of losing bids on the Bridgewell dataset while the number of winning bids is around 4.88 times as many as the losing ones on the iPinYou Season 2 dataset .
Feature Name IP Region City AdExchange
Example 18318197* 216 236 2 3d68edb4b8f5bba8bea6782e33c8e228 Domain e63cfda49ec2a36a0cafcd646906227b 3844656199 250 250 OtherView Na 7321 6 02 2259 10684,10102,10006
URL AdSlotId AdSlotWidth AdSlotHeight AdSlotVisibility AdSlotFormat CreativeID weekday hour adid usertag
Table 4 : Example features of the iPinYou dataset [ 21 ] .
4.2 The Difference of Winning Price Patterns To answer question ( 1 ) , we observe the patterns of the winning price in two ways . First , we read the average winning price on both winning and losing bids as shown in Table 1 , Table 2 , and Table 3 . We can see that the average winning prices on losing bids , which are the values in column Avg . WP on L , are usually higher than the average winning prices in column Avg . WP on W in Table 3 . This adid 1458 2259 2261 2821 2997 3358 3386 3427 3476 total 2 total 3 total log.loss 0.0068 0.0075 0.0024 0.0051 0.0249 0.0093 0.0081 0.0074 0.0053 0.0073 0.0074 0.0073 auc 0.7350 0.5665 0.5652 0.5411 0.5908 0.7221 0.7086 0.6987 0.6837 0.7153 0.7433 0.7211
Table 5 : The performance of our CTR model of the iPinYou dataset . shows one difference of the patterns of the winning price on the winning and the losing bids .
Furthermore , we use the linear regression model and observe the estimator learning the winning bids only , denoted by βw lm , and the estimator learning the losing bids only , denoted by βl lm , by minimizing Eq ( 3 ) . Because there are too many features , we minimize Eq ( 3 ) numerically via the LBFGS B algorithm proposed by Byrd et al . [ 2 ] with L2 regularization . The parameter of L2 regularization is selected via progressive validation on the winning data . Note that the estimator βl lm is not available in reality because the winning prices on the losing bids are unknown to the DSP . Each estimator learns one day of the data and its performance is tested using the data in the very next day . lm outperforms βl
The Mean Squared Error ( MSE ) between the true winning price and the estimated winning price evaluated on the winning bids of each day by either βw lm learning from the winning bids or βl lm from the losing bids in the previous day are shown in Fig 2 . We can see that βw lm in terms of having smaller MSE . In contrast to the result of Fig 2 , βl lm has smaller MSE on the losing bids compared to lm as shown in Fig 3 . In contrast , βl βw The results here also show that the winning price patterns on the winning and losing bids are different , otherwise the results of the two estimators should be similar . To ameliorate this effect , we consider the interaction [ 1 ] between the estimated winning rate and the features used to train the model . In other words , the model is trained by considering both the original features , ie , Table . 4 , and the features after interaction with the estimated winning rate . In this way , the different patterns of winning and losing bids can lm outperforms .
1310 Figure 2 : Mean Squared Error ( MSE ) of linear regression on the winning price based on the winning bids , βw lm . The result is evaluated on the winning bids . lm , and based on the losing bids , βl
Figure 3 : Mean squared error ( MSE ) of linear regression on the winning price based on the winning bids , βw lm . The result is evaluated on the losing bids . lm , and based on the losing bids , βl thus be differentiated with the aid of the estimated winning rate .
4.3 Evaluation on βlm vs . βclm
To answer question ( 2 ) , we conduct experiments to compare the estimator from the linear regression βlm on the winning bids only , and the estimator from the censored regression model βclm on both the winning and losing bids . Note that we cannot see the true winning price of the losing bids thus they are censored .
It is difficult to solve Eq ( 5 ) with the parameter σ numerically . The σ usually diverges to infinity . In the experiment , we treat σ as a tuning parameter and replace it heuristically with the sample standard deviation of the observed winning price . The optimization function is solved numerically via L BFGS B algorithm with L2 regularization . The parameter of L2 regularization is the same as the one of βlm . Then we calculate the MSE value of them on the losing bids of the same day to see the performance of the estimators at the censored data .
In Fig 4 , we can see that βclm outperforms βlm by having smaller MSE values . To explore the reason , we compare the average estimated winning price and the average true winning price at the censored data .
The average winning prices from different estimators are shown in Fig 5 . ytrue represents the average true winning price , and ylm , yclm , and ymix are the average estimated winning price by βlm , βclm , and βmix , respectively . The censoring in the second price auction is due to high winning price . Hence , the winning price at observed data is usually lower than at censored data as shown in Table 1 , Table 2 , and Table 3 . Consequently , the average winning price from βlm , ie ylm , is lower than the true average winning price , ie ytrue , in all experiments as shown in Fig 5 .
On the other hand , βclm learns from the censored data , and it hence has higher average estimated winning prices compared to βlm as shown in Fig 5 . However , it is hard to tell the relationship between the average estimated winning price from βclm and the average true winning price . Fig 5 suggests that different data sources have different relationships . On the dataset iPinYou Season 2 and season 3 , βclm still underestimates the winning price on the censored data on average . Therefore , it is not surprising that the MSE value of βclm is better than βlm because the latter gives even lower estimation on average . βclm overestimates the winning price on the Bridgewell dataset sometimes . Because the difference between yclm and ytrue is lower than the difference between ylm and ytrue , βclm has smaller MSE than βlm does in Fig 4 .
In summary , the overall MSE value of βclm is lower than that of βlm . This result depends on the bias , ie , the difference between the predicted winning price and the true winning price , of βclm on the censored data . It turns out that the predicted winning price of βlm is lower than the true one and the winning price estimated by βclm is higher than the one predicted by βlm . 4.4 Evaluation on Mixture Model
To answer the question ( 3 ) , we conduct experiments to compare estimators βlm , βclm and βmix at the same time . All of them learn from the data of a specific day as a training day via minimizing the objective function . The detailed optimization is the same as Sec 4.2 and Sec 43
The results are shown in Fig 6 , Fig 7 and Fig 8 . A very interesting point is that the performance of βmix is close to the better one of the other two estimators βlm and βclm . Sometimes , βmix is the best of the three .
Fig 5 shows that the value of estimated winning price by βmix is between that by βlm and that by βclm . In all exper
05000100001500020000Jun 07Jun 08Jun 09Jun 10Jun 11Jun 12daySeason 2 iPinYou0500010000150002000025000Oct 21Oct 23Oct 25Oct 27daySeason 3 iPinYou0200004000060000Dec 03Dec 05Dec 07Dec 09dayBridgewellMSEmodelβlmlβlmw0500010000Jun 07Jun 08Jun 09Jun 10Jun 11Jun 12daySeason 2 iPinYou050001000015000Oct 21Oct 23Oct 25Oct 27daySeason 3 iPinYou01000020000Dec 03Dec 05Dec 07Dec 09dayBridgewellMSEmodelβlmlβlmw1311 Figure 4 : Mean Square Error of the winning prices on the losing bids . The evaluation is conducted on the same day of the training data .
Figure 5 : The average winning price estimated by different models on the losing bids . iments , the bias , which is observed via the difference of the average predicted winning price and average true winning price as shown in Fig 5 , of βmix on the censored data is less than βlm . The bias of the βmix exceeds βlm only if the βclm extremely overestimates the winning price on the censored data . It is not likely to happen in the real case because βclm also learns from the observed data whose winning prices are lower . As shown in our experiments , there is no βclm which extremely overestimates the winning price on censored data . Although the characteristic of the dataset and the performance of the predicted winning rate , which are shown in Table 1 , Table 2 , and Table 3 , are different , the results of our model are still robust compared to the linear regression , ie the model ignoring the censored data . If we change the ratio of the bidding price at the beginning of the simulation , βmix consistently outperforms βlm as shown in Fig 9 .
In summary , our experiments show that βmix outperforms βlm . We therefore conclude that the proposed framework indeed enhance the prediction of the winning price . The main reason is that the βlm underestimates the winning price on the censored data and the proposed model lifts the estimation on the censored data properly . In addition , when compared to the censored regression , our model is much more robust .
5 . RELATED WORK
While many existing reports study good bidding strategies for maximizing the revenues of either DSPs or SSPs [ 11 , 6 , 5 ] , the issue of predicting the winning price of a bid has not been well explored yet . Zhang et al . [ 20 ] proposed a framework to optimize the bidding strategy for DSPs , of which the cost of a bid , usually the same as the winning price , is a required input . As the winning price is not available in many cases , they used the bidding price as the upper bound of the cost instead . Ghosh et al . [ 7 ] proposed adaptive bidding algorithms under a fixed budget with user specified constraints .
They considered both the settings with fully and partially observed winning price information . For the latter case , they simply assumed the winning price was drawn iid from a CDF P , as their main goal is to win more bids when controlling the cost . All the above reports justify that knowing the winning price in RTB is important in practice . In this study we aim to further provide a systematic method based on machine learning and statistics to predict this value .
Cui et al . [ 4 ] studied the prediction of winning price , but on the seller side . Unlike Ghosh et al . [ 7 ] modeling the winning price by iid CDF P , they modeled the winning price with the mixture of log normal distribution on various targeting attributes . Since their study target is for SSPs , they did not address challenges of data censoring . Also , they paid attention to the global distribution of the winning price while we focus on the prediction of the winning price conditioned to the features , ie , the case when only some features are available for DSPs .
Reisinger and Driscoll [ 14 ] studied the publisher advertiser information asymmetry for SSPs . Their future work includes using the censored regression to construct the full bid distribution for SSPs because the floor price can be treated as dynamic left censoring . Our work can be regarded as similar to their future work , but we are studying the bid distribution conditioned on the partially observed features and designing the prediction method for DSPs explicitly .
To our knowledge , we are the first to predict the winning price on the DSP side and with the censored data . We study the censored regression and extend it heuristically for the RTB data . We show that the accuracy of the winning price model can be enhanced prominently by cooperating with the censored information .
6 . CONCLUSION AND FUTURE WORK
In this paper , we studied the prediction of winning price for DSPs in the RTB process when only partial features and only the winning price of historical winning bids were
050001000015000Jun 07Jun 08Jun 09Jun 10Jun 11Jun 12daySeason 2 iPinYou050001000015000Oct 21Oct 23Oct 25Oct 27daySeason 3 iPinYou05000100001500020000Dec 03Dec 05Dec 07Dec 09dayBridgewellMSEmodelβclmβlm100150Jun 06Jun 07Jun 08Jun 09Jun 10Jun 11dayiPinYou Season 250100150200Oct 20Oct 22Oct 24Oct 26dayiPinYou Season 3150200250Dec 02Dec 04Dec 06Dec 08dayBridgewellAvg . Winning Priceytrueylmyclmymixture1312 Figure 6 : Mean squared error of the predicted winning price .
Figure 7 : Mean squared error of the predicted winning price on the winning bids . able to be observed . We showed that the prediction performance was improved by taking the censored information into account . Because the patterns of the winning price on the observed data and unobserved data are different in RTB , which violates the assumption of censored regression , we further proposed a mixture model to combine the power of linear regression on observed data and censored regression on both observed and censored data . Since the linear regression model works better on the winning bids in general and the censored regression model takes the censored information into account , we proposed to weight the two models according to the winning rate , which shows the probability that a DSP wins a bid . If the winning rate was higher , the linear regression model accounted for more percentage in the mixture model , and vice versa . From experiments on three real RTB datasets we showed that the proposed mixture model outperformed the model with observed data only .
For future work , we will seek finer and more sophisticated models that can better fit the censored winning price . Also , we would like to study the effect of introducing winning price prediction on real applications of RTB and online advertising such as designing bidding strategy , exploring the potential of new campaigns , and estimating the CTR and the conversion rate .
7 . ACKNOWLEDGMENTS
This study was supported in part by the Ministry of Science and Technology of Taiwan , ROC , under Contracts MOST103 2221 E 001 006 MY2 and MOST103 3111 Y 001025 . The authors would also like to thank Bridgewell Inc . for providing the RTB logs in this work . All opinions , findings , conclusions and recommendations in this paper are those of the authors and do not necessarily reflect the views of the funding agencies .
8 . REFERENCES [ 1 ] T . Brambor , W . R . Clark , and M . Golder .
Understanding interaction models . improving empirical analyses . Political Analysis , 14:63–82 , 2006 . [ 2 ] R . H . Byrd , P . Lu , J . Nocedal , and C . Zhu . A limited memory algorithm for bound constrained optimization . SIAM J . Sci . Comput . , 16(5):1190–1208 , Sept . 1995 .
[ 3 ] O . Chapelle . Modeling delayed feedback in display advertising . In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD ’14 , pages 1097–1105 , New York , NY , USA , 2014 . ACM .
[ 4 ] Y . Cui , R . Zhang , W . Li , and J . Mao . Bid landscape forecasting in online ad exchange marketplace . In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD ’11 , pages 265–273 , New York , NY , USA , 2011 . ACM .
[ 5 ] J . Feldman , M . Henzinger , N . Korula , V . S . Mirrokni , and C . Stein . Online stochastic packing applied to display ad allocation . In M . de Berg and U . Meyer , editors , ESA ( 1 ) , volume 6346 of Lecture Notes in Computer Science , pages 182–194 . Springer , 2010 .
[ 6 ] J . Feldman , A . Mehta , V . Mirrokni , and
S . Muthukrishnan . Online stochastic matching : Beating 1 1/e . In Proceedings of the 2009 50th Annual IEEE Symposium on Foundations of Computer Science , FOCS ’09 , pages 117–126 , Washington , DC , USA , 2009 . IEEE Computer Society .
[ 7 ] A . Ghosh , B . I . Rubinstein , S . Vassilvitskii , and
M . Zinkevich . Adaptive bidding for display advertising . In Proceedings of the 18th International Conference on World Wide Web , WWW ’09 , pages 251–260 , New York , NY , USA , 2009 . ACM .
[ 8 ] Google . The arrival of real time bidding . 2011 .
0100020003000Jun 07Jun 08Jun 09Jun 10Jun 11Jun 12daySeason 2 iPinYou020004000Oct 21Oct 23Oct 25Oct 27daySeason 3 iPinYou05000100001500020000Dec 03Dec 05Dec 07Dec 09dayBridgewellMSEmodelβclmβlmβmix0250500750Jun 07Jun 08Jun 09Jun 10Jun 11Jun 12daySeason 2 iPinYou05001000150020002500Oct 21Oct 23Oct 25Oct 27daySeason 3 iPinYou05000100001500020000Dec 03Dec 05Dec 07Dec 09dayBridgewellMSEmodelβclmβlmβmix1313 ads at facebook . In Proceedings of the Eighth International Workshop on Data Mining for Online Advertising , ADKDD 2014 , August 24 , 2014 , New York City , New York , USA , pages 1–9 , 2014 .
[ 11 ] R . M . Karp , U . V . Vazirani , and V . V . Vazirani . An optimal algorithm for on line bipartite matching . In Proceedings of the Twenty second Annual ACM Symposium on Theory of Computing , STOC ’90 , pages 352–358 , New York , NY , USA , 1990 . ACM .
[ 12 ] J . Klein and M . Moeschberger . Survival Analysis :
Techniques for Censored and Truncated Data . Statistics for Biology and Health . Springer , 2003 . [ 13 ] H . B . McMahan , G . Holt , D . Sculley , M . Young ,
D . Ebner , J . Grady , L . Nie , T . Phillips , E . Davydov , D . Golovin , S . Chikkerur , D . Liu , M . Wattenberg , A . M . Hrafnkelsson , T . Boulos , and J . Kubica . Ad click prediction : a view from the trenches . In Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining ( KDD ) , 2013 .
[ 14 ] J . Reisinger and M . Driscoll . Pricing externalities in real time bidding markets . In NIPS 2010 Workshop : Machine Learning in Online ADvertising , 2010 .
[ 15 ] W . Schnedler . Likelihood estimation for censored random vectors . Working Papers 0417 , University of Heidelberg , Department of Economics , Feb . 2005 .
[ 16 ] J . Tobin . Estimation of relationships for limited dependent variables . Cowles Foundation Discussion Papers 3R , Cowles Foundation for Research in Economics , Yale University , 1956 .
[ 17 ] J . Wang , S . Shen , and S . Seljan . Real time bidding : A new frontier of computational advertising research . Workshop in CIKM , 11 2013 .
[ 18 ] K . Weinberger , A . Dasgupta , J . Langford , A . Smola , and J . Attenberg . Feature hashing for large scale multitask learning . In Proceedings of the 26th Annual International Conference on Machine Learning , ICML ’09 , pages 1113–1120 , New York , NY , USA , 2009 . ACM .
[ 19 ] S . Yuan , J . Wang , and X . Zhao . Real time bidding for online advertising : Measurement and analysis . CoRR , abs/1306.6542 , 2013 .
[ 20 ] W . Zhang , S . Yuan , and J . Wang . Optimal real time bidding for display advertising . In The 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD ’14 , New York , NY , USA August 24 27 , 2014 , pages 1077–1086 , 2014 .
[ 21 ] W . Zhang , S . Yuan , J . Wang , and X . Shen . Real time bidding benchmarking with ipinyou dataset . arXiv preprint arXiv:1407.7073 , 2014 .
Figure 8 : Mean squared error of the predicted winning price on the losing bids .
Figure 9 : Mean square error of the predicted winning prices with different winning/losing bid ratio on the iPinYou dataset on 2013 10 20 .
[ 9 ] W . H . Greene . Censored data and truncated distributions . Working papers , New York University , Leonard N . Stern School of Business , Department of Economics , 2005 .
[ 10 ] X . He , J . Pan , O . Jin , T . Xu , B . Liu , T . Xu , Y . Shi ,
A . Atallah , R . Herbrich , S . Bowers , and J . Q . Candela . Practical lessons from predicting clicks on
0500010000Jun 07Jun 08Jun 09Jun 10Jun 11Jun 12daySeason 2 iPinYou050001000015000Oct 21Oct 23Oct 25Oct 27daySeason 3 iPinYou01000020000Dec 03Dec 05Dec 07Dec 09dayBridgewellMSEmodelβclmβlmβmix025005000750001670333050006670833bid ratiomodelβclmβlmβmix1314
