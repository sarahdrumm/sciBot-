Model Multiple Heterogeneity via Hierarchical
Multi Latent Space Learning
Pei Yang
Arizona State University Tempe , AZ 85281 , USA cspyang@gmailcom
Jingrui He
Arizona State University Tempe , AZ 85281 , USA jingruihe@asuedu
ABSTRACT In many real world applications such as satellite image analysis , gene function prediction , and insider threat detection , the data collected from heterogeneous sources often exhibit multiple types of heterogeneity , such as task heterogeneity , view heterogeneity , and label heterogeneity . To address this problem , we propose a Hierarchical Multi Latent Space ( HiMLS ) learning approach to jointly model the triple types of heterogeneity . The basic idea is to learn a hierarchical multi latent space by which we can simultaneously leverage the task relatedness , view consistency and the label correlations to improve the learning performance . We first propose a multi latent space framework to model the complex heterogeneity , which is used as a building block to stack up a multi layer structure so as to learn the hierarchical multilatent space . In such a way , we can gradually learn the more abstract concepts in the higher level . Then , a deep learning algorithm is proposed to solve the optimization problem . The experimental results on various data sets show the effectiveness of the proposed approach .
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications— Data Mining ; I52 [ Computing Methodologies ] : Pattern Recognition—Design Methodology
General Terms Theory , algorithm , performance , experiment
Keywords Heterogeneous learning ; multi task learning ; multi view learning ; multi label learning
1 .
INTRODUCTION
In the era of big data , a large amount of information collected from heterogeneous sources are correlated with each other . It is of great importance to mine such hidden correlations in the presence of multiple types of heterogeneity for many real world applications , such as satellite image analysis , gene function prediction , and insider threat detection . In this paper , we focus on triple types of heterogeneity , ie , task heterogeneity , view heterogeneity , and label heterogeneity . For example , for the satellite image analysis problems , task heterogeneity refers to the images collected from different satellites following from different distributions ; view heterogeneity refers to various types of features such as color histogram , edge distribution histogram , and bag of visual words ; label heterogeneity refers to the multiple labels associated with each image such as sea , plane , and yacht .
The major challenge for learning with the triple types of heterogeneity is how to effectively mine the hidden correlations among the heterogeneous data . Such correlations should reflect the key assumptions underlying each type of heterogeneity , including the task relatedness assumption [ 4 ] , the view consistency assumption [ 9 ] , as well as the label correlation assumption [ 23 ] . To the best of our knowledge , we are the first to jointly model the triple types of heterogeneity including the task heterogeneity , view heterogeneity and label heterogeneity .
To address this problem , we propose a Hierarchical MultiLatent Space ( HiMLS ) learning approach to jointly model the triple types of heterogeneity . The basic idea is to learn a hierarchical multi latent space by which we can accommodate multiple types of relationship among the instances , features and labels including the task relatedness , view consistency and the label correlations . We first present the multilatent space framework to model the complex heterogeneity which is formulated as a regularized non negative matrix triple factorization problem . It aims to minimize the reconstruction loss on the instance feature data , classification loss on the instance label data , together with the regularization term . Furthermore , the multi latent space model is used as a building block to stack up a multi layer structure so as to learn the hierarchical multi latent space . In such a way , we can gradually learn the more abstract concepts in a higher layer . Finally , we will introduce an iterative updating algorithm to solve the resulting optimization problem . The proposed algorithm consists of two phases . First , each layer is pre trained in a greedy layer wise way . Then , it fine tunes the weights of all the layer to reduce the total reconstruction loss and the classification loss . It is worth noting that the proposed model is a generalized framework to learn from complex heterogeneity . For example , some popular methods
1375 on learning from a single heterogeneity can be viewed as the special cases of the proposed approach .
The main contributions of this paper can be summarized as follows : triple types of heterogeneity ; latent space from complex heterogeneity ;
• A novel learning problem which simultaneously models • A multi layer architecture to learn the hierarchical multi• A deep learning algorithm to solve the optimization • Generalization of some previous work on learning from • Experimental results on various data sets showing the single heterogeneity ; problem ; effectiveness of the proposed approach .
The rest of the paper is organized as follows . After a brief review of the related work in Section 2 , we present the proposed model and the optimization algorithm in Section 3 , followed by the discussion of some special cases of the proposed approach in Section 4 . Section 5 shows the experimental results . Finally , we conclude in Section 6 .
2 . RELATED WORK
In this section , we review the related work on modeling a single or dual types of heterogeneity .
In multi view learning , the features from multiple sources form natural partitions ( views ) . Co Training [ 3 ] is one of the earliest algorithms for multi view learning . More recent work includes : SVM 2K [ 9 ] which combined KCCA with SVM in an optimization framework ; the informationtheoretic framework for multi view learning [ 20 ] ; the CoMR method [ 19 ] based on a data dependent Reproducing Kernel Hilbert Space ( RKHS ) ; the large margin framework for multi view data based on a latent space Markov network [ 5 ] ; the MSL [ 24 ] model which is a convex multi view subspace learning method that enforced conditional independence among the multiple views while reducing dimensionality .
In multi task learning , the goal is to leverage the small amount of labeled data from multiple related tasks to improve the learner for each task . Among others , alternating structure optimization [ 1 ] decomposed the model into the task specific and task shared feature mapping ; multitask feature learning [ 2 ] assumed that multiple related tasks share a low dimensional representation ; clustered multi task learning [ 34 ] assumed that multiple tasks follow a clustered structure . Some recent multi task learning methods are able to deal with irrelevant tasks by assuming that the model can be decomposed into a shared feature structure that captures task relatedness , and a group sparse structure that detects outliers [ 10 ] .
In multi label learning , each instance is associated with a set of labels [ 23 , 33 ] . The key issue for multi label learning is how to exploit correlations or dependencies among multiple labels . To name a few , first order method such as MLkNN [ 32 ] assumed that labels are independent and transformed the multi label learning into a number of independent binary classification problems ; second order approach such as Rank SVM [ 8 ] transformed the multi label learning into the label ranking problem ; LEAD [ 31 ] employed Bayesian network to encode the conditional dependencies of the labels ; subspace learning approach LS ML [ 14 ] assumed that a common subspace is shared among multiple labels ; graph based method HG [ 21 ] constructed a hypergraph to exploit the correlation information among different labels ; transductive approach TRAM [ 15 ] leveraged the information from unlabeled data to estimate the optimal label concept compositions ; MLLOC [ 13 ] assumed that the label correlation may be shared by only a subset of instances rather than all the instances ; boosting based method MAHR [ 12 ] aimed to discover the label relationship by using a hypothesis reuse mechanism ; a generic empirical risk minimization framework was proposed for large scale multi label learning [ 28 ] .
More recently , researchers begin to study problems with dual types of heterogeneity . For problems with both task and view heterogeneity , a variety of techniques have been proposed to model task relatedness in the presence of multiple views , eg , [ 11 , 30 , 25 , 26 ] . For the problems with both label and view heterogeneity , the L2F method proposed in [ 27 ] modeled both the view consistency and the label correlations in a graph based framework . For the more complex setting with all three types of heterogeneity , these techniques cannot be readily applied without disregarding the useful information from a certain type of heterogeneity .
3 . THE PROPOSED HIMLS MODEL
The basic idea of the proposed HiMLS approach is to learn a hierarchical multi latent space by which we can take advantage of multiple types of heterogeneity to improve the performance of the learning system . We first present the multi latent space framework to model the complex heterogeneity , which is then used as a building block to stack up a multi layer structure so as to learn the hierarchical multilatent space . Finally , we will introduce an iterative updating algorithm to solve the resulting optimization problem . fi
.
Before going into the details , the problem statement and the notations will be introduced . Suppose we are given the multi label data with multiple views in different tasks . Let T be the number of tasks , V the number of views , m the number of labels , respectively . Let X be an instance space , L = {L1,L2,··· ,Lm} a finite set of class labels . An instance x ∈ X is described from V views , and associated with a subset of labels L(x ) ∈ 2 , which is the set of relevant labels of x . In practice , the relevant labels L(x ) can be denoted by a binary label vector y(x ) = [ y1(x ) , y2(x),··· , ym(x) ] , where yk(x ) = 1(1≤ k ≤ m ) if Lk ∈ L(x ) , and 0 otherwise . For the ith task and jth view , denote the number of instances and features by ni and dj , respectively . Let ∈ Rni×dj be the instance feature matrix for the ˜Xij= ith task and jth view , where Xij is the training data and X u ij ∈ Rni×m be the instanceis the test data . Let ˜Yi = label matrix for the ith task , where Yi is for the training data and Y u is for the test data . The goal is to build a multi label classifier f ( x ) = [ f1(x ) , f2(x),··· , fm(x) ] , where i fk(x ) ∈ {1 , 0}(1 ≤ k ≤ m ) by using the data given in multiple tasks and described with multiple views . 3.1 Multi Latent Space Learning
Xij X u ij
Yi Y u i
L
. fi
In order to leverage the triple types of heterogeneity , we propose a multi latent space learning framework to jointly model the task relatedness , view consistency , and label correlations in a principled way .
1376 fi
.
Ri Ru i
Intuitively , we can do multi way clustering on the heterogeneous data which simultaneously clusters the instances , features and labels into the their corresponding clusters , re∈ Rni×p be the instance tospectively . Let ˜Ri = cluster matrix where p is the dimensionality of instance latent space , Ri and Ru i are for training and test data , respectively . Let Cj ∈ Rdj×q be the feature to cluster matrix , CY ∈ Rm×q the label to cluster matrix where q is the dimensionality of feature ( or label ) latent space . Each row in ˜Ri ( or Cj , CY ) represents the coeffecients of the instance ( or feature , label ) associated with the instance ( or feature , label ) clusters . Denote Mij ∈ Rp×q , MiY ∈ Rp×q as the colatent space matrices . Note that Mij models the correlations between the instance clusters and the feature clusters , while MiY models the correlations between the instance clusters and the label clusters .
In the proposed model , we aim to learn the multi latent spaces from the instances and labels of multiple tasks , and the features with multiple views . The multi latent space model is formulated as a regularized non negative matrix triple factorization problem , which simultaneously decomposes the instance feature and instance label matrices , while enforcing the task relatedness , view consistency , and label correlations on the data . The objective is to minimize the reconstruction loss on the instance feature data , classification loss on the instance label data , together with the regularization term :
' ff min Jrec
˜X , R , M , C
+ αJcl ( Y , R , M , C )
( 1 )
+ βΩ ( R , M , C ) where α and β are the non negative coefficients to control the importance of classification loss and regularization , respectively . Ω ( R , M , C ) is the regularization of R , C and M .
Various regularization techniques can be used to encode our prior knowledge about the task relatedness , view consistency , and the label correlations . Since the instances , features , and labels may share the latent semantic concepts , we hope the learned co latent spaces are similar to each other . In specific , we hope that the co latent spaces learned in the feature spaces from multiple views are as similar as possible to the ones learned from label spaces , which acts as a bridge to link the labels with the features from multiple views in the latent spaces . Therefore , Eq 1 can be instantiated as ,
T
V Yi − RiMiY C T
˜Xij − ˜RiMijC T 2 2 V T
+ β j=1 i=1
Y j
F
+
F i=1 j=1 min
{R,M,C}>0
T
α i=1
( 2 )
'Mij − MiY '2
F
The non negative constraints R > 0 andC > 0 allow for the multi way clustering interpretation . The multi latent space model can be interpreted from the perspective of constrained multi way clustering . By constraining the multiway clustering procedures , we model the task relatedness by requiring that the features across different tasks share the same feature clustering coefficients , enhance the view consistency by requiring that the instances share the same instance clustering coefficients across different views , model the label correlations by requiring that the labels share the same label clustering coefficients across different tasks .
Specifically , Eq 2 encodes multiple types of correlations among the heterogeneous data as follows : • Correlations among feature instance label : For the ith task , the decompositions of instance feature data Xij and instance label data Yi share the same instance tocluster matrix Ri .
• Label correlation : The labels share the same label to cluster matrix CY across different tasks .
• View consistency : For the ith task , the decompositions of the instance feature data Xij ( 1 ≤ j ≤ V ) in different views share the same instance to cluster matrix Ri . • Task relatedness : For the jth view , the decompositions of the instance feature data Xij ( 1 ≤ i ≤ T ) in different tasks share the same feature to cluster matrix Cj .
3.2 Hierarchical Multi Latent Space Model
The multi latent space model can be used as a building block to stack up a multi layer architecture so as to learn the hierarchical multi latent space from complex heterogeneity . Take the text data as an example . In each layer , we do multiway clustering on the documents , features and labels . Since all of the documents , features and labels may have hierarchical structures , they can be clustered into sub categories , and further into high level sub categories , until the top categories . In such a way , we can gradually learn the more abstract concepts in a higher layer .
The co latent spaces Mij and MiY can be viewed as the compact representations for the original input data ˜Xij and Yi . Based on the co latent space M ) where l represents the layer , we hope to further learn its own colatent space M
( l−1 ) iY
( l−1 ) ij
( or M
( l ) ij ( or M ( l−1 ) ij ( l−1 ) iY
M
M
( l ) iY ) in a higher level , ie , ≈ R ≈ R
( l ) i M
( l ) ij C
( l)T j
( l ) i M
( l ) iY C
( l)T Y
Suppose L is the number of layers . Similar to Eq 2 , we can obtain the objective function for the lth(2 ≤ l ≤ L ) layer : iY − R ( l−1 )
( l ) i M
( l ) iY C
( l)T Y
( 3 ) min
{R,M,C}>0
( l−1 ) ij
− R
( l ) i M
( l ) ij C
( l)T j
M i=1
T V M T V T j=1 i=1 i=1 j=1
M
+ α
+ β
2
F
( l ) ij − M
( l ) iY
2
F
2
F
Based on the learned co latent spaces M
( L ) iY in the highest layer L , we hope to recover the original input data , ˜Xij and Yi , in the first layer as accurately as possible . Thus , the objective for the multi layer architecture is as follows : and M
( L ) ij min
{R,M,C}>0
( 1:L ) i M
( L ) ij C
( 1:L)T j
˜Xij − ˜R i=1 j=1
T V Yi − R T M V T i=1 i=1 j=1
+ α
+ β
2
F
( L ) ij − M
( L ) iY
2
F
2
F
( 1:L ) i M
( L ) iY C
( 1:L)T Y
( 4 )
1377 The minimum is obtained by setting the derivative to zero :
Then , we get the update rule as follows : ff
G
M , M ( t )
= 0
' fi fi fi j
RT i XiCi + β
KjP T j i i RiM C T RT j i Ci + β
M PjP T j
' ff ff ff
≥ min
M
∂
∂Muv
)ff α fi ' ff ' = G ≥ J
α i
M = M ' ff
'
Since J
M ( t )
M ( t ) , M ( t )
G
M , M ( t )
=
M ( t+1 ) , M ( t )
G is non increasing under the above update rule .
M ( t+1 )
, the objective function J ( M )
The following Theorem 3.1 shows the multiplicative update rules for Eq 2 , and demonstrates its convergence and correctness . The update rules for Eq 3 can be obtained in a similar way .
Theorem 3.1
( Convergence of Pre training ) . The objective function in Eq 2 is non increasing under the updating rules :
Ri = Ri .
.fififififififi'
Vff j=1
Vff j=1
Xij Cj M T ij + αYiCY M T iY
RiMij CT j Cj M T ij + αRiMiY CT
Y CY M T iY t( l=s where A(s:t ) = any matrix A .
A(l ) if s ≤ t , and A(s:t ) = I otherwise for
Prediction : The final prediction is the weighted sum of predictions resulting from each layer . For ith(1 ≤ i ≤ T ) task , the predicted instance label matrix is as follows :
L
L
Fi = wlF
( l ) i = wlR u(1 ) i R
( 2:l ) i M
( l ) iY C
( 1:l)T Y
( 5 ) l=1 l=1 where wl controls the weight for lth layer . 3.3 Optimization Algorithm
Following the tactics successfully used in deep learning , we adopt a two phase procedure to train the multi layer model . We first pre train the weights of each layer in a greedy layer wise manner , then fine tune the weights of all layers to reduce the total reconstruction loss and classification loss . In specific , we pre train the first layer by using Eq 2 , and the following lth(2 ≤ l ≤ L ) layer by using Eq 3 . Then , at the fine tuning phase , we update the weights for all the layers by using Eq 4 .
To derive the multiplicative update rules for pre training and fine tuning , we first derive Lemma 31 The proof of Lemma 3.1 is a key step to prove Theorem 3.1 and Theorem 32
Lemma 31 The objective function ,
J ( M ) = α i
2
F
+β j i
Xi − RiM C T )ff α fi fi
RT
α i is non increasing under the updating rule :
M = M i XiCi + β i RT i RiM C T j i Ci + β
F ( 6 )
'M Pj − Kj'2 fi fi
KjP T j
M PjP T j j
Proof . We make use of auxiliary function approach [ 16 ] to derive the updating rules and prove its convergence and correctness .
Similar to [ 7 ] , we can derive the updating rule for M . The objective function for M is as follows :
Xi − RiM C T i
2
F
+ β j
'M Pj − Kj'2
F ffi
= αtr
M T RT i RiM C T
+ βtr j
M T M PjP T i Ci − 2M T RT ffi j − 2M T KjP T j i XiCi
+ const
J ( M ) = α fl fl i i
Let t be the number of iteration . Next we will show that
.
G
M , M
( t )
' i
= α
' u,v
'
+ β j u,v fi ⎧⎪⎨ ⎪⎩ '
)
⎧⎪⎨ ⎪⎩
RT i RiM ( t ) CT i Ci ( t )
) uv
M
M ( t ) Pj P T j uv
M
( t ) uv
· M 2 uv uv
)
R
− 2
· M 2 uv
)
− 2
Kj P
T j
T i Xi Ci
M uv
( t ) uv
M
( t ) uv uv
⎛ ⎝1 + ln
⎞ ⎠
⎫⎪⎬ ⎪⎭
Muv ( t ) uv
M
⎛ ⎝1 + ln ⎞ ⎠
Muv ( t ) uv
M
⎫⎪⎬ ⎪⎭ is an auxiliary function of J ( M ) due to the facts :
( 7 )
( 8 )
( 9 )
( 10 )
( 11 )
( 12 )
Ru i = Ru i .
Cj = Cj .
CY = CY .
Mij = Mij .
X u ij Cj M T ij
Ru i Mij CT j Cj M T ij
˜X T ij
˜RiMij
Cj M T ij
˜RT i
˜RiMij
Y T i RiMiY
CY M T iY RT i RiMiY
˜Xij Cj +βMiY i ˜RiMij CT j Cj +βMij i YiCY + β
Vff j=1
Mij
Vff j=1
Tff i=1
Tff i=1 i=1 j=1
Tff
Vff
.fififififififi' .fifififififi' .fifififififi' Tff .fifi' ˜RT .fifififi' αRT
αRT
˜RT i i=1
MiY = MiY . ffl
Proof . Please see Appendix A . i RiMiY CT
Y CY + βV MiY
Denote ˜R
˜Ωij = ˜R Φ ( A ) = R
=
( 1 ) i u(1 ) i
( 1:L ) i
R R ( L ) ( 1:L)T ij C j ( 1:l−1)T AR i
( 1:L ) i M
( l+1:L)T i
( 2:L ) i
R
, Ωij = R
( 1:L ) i M
( L ) ij C
( 1:L)T j
,
, ΩiY = R
( 1:L ) i M
( L ) iY C
( 1:L)T Y
, and for any matrix A . and
G
M , M ( t )
≥ J ( M ) .
Theorem 3.2 shows the multiplicative update rules for the objective function Eq 4 , and demonstrates its convergence and correctness .
G ( M , M ) = J ( M )
' ff
1378 Theorem 3.2
( Convergence of Fine tuning ) . The objective function in Eq 4 is non increasing under the updating rules : j M ( L)T
Y M ( L)T i = R(l )
Xij C(1:L )
YiC(1:L ) i .
+ αΦ
R(l ) j=1
Φ iY ij
Φ
Ωij C(1:L ) j M ( L)T ij
+ αΦ
ΩiY C(1:L )
Y M ( L)T iY
V V j=1
V j=1
Ru i = Ru i .
C(l ) j = C(l ) j .
C(l ) Y = C(l )
Y .
.fifififififi' .fifififififi'
Tff i=1
Tff i=1
Tff i=1
Tff i=1
M ( L ) ij = M ( L ) ij .
M ( L ) iY = M ( L ) iY .
V j=1
X u ij C(1:L ) j M ( L)T ij R(2:L)T i
Ru i R(2:L ) i M ( L ) ij C(1:L)T j
C(1:L ) j M ( L)T ij R(2:L)T i
C(1:l−1)T j
˜X T ij
C(1:l−1)T j
˜ΩT ij
˜R(1:L ) i M ( L ) ij C(l+1:L)T j
˜R(1:L ) i M ( L ) ij C(l+1:L)T j
C(1:l−1)T
Y i R(1:L ) Y T i M ( L ) iY C(l+1:L)T
Y iY R(1:L ) ΩT i M ( L ) iY C(l+1:L)T
Y
C(1:l−1)T
Y i
.fififi' ˜R(1:L)T .fifififi' αR(1:L)T
˜R(1:L)T i i j
˜Xij C(1:L ) ˜Ωij C(1:L ) j
YiC(1:L )
Y
+ β
+ βM ( L ) iY + βM ( L ) Vff ij
M ( L ) ij j=1
αR(1:L)T i
ΩiY C(1:L )
Y
+ βV M ( L ) iY
( 13 )
( 14 )
( 15 )
( 16 )
( 17 )
( 18 )
Proof . Please see Appendix B .
After obtaining R ( l )
( l ) ij and iY ( 1 ≤ l < L ) by using the updating rule got in the pre
, we can update M and C
( l ) j
( l ) i
M training phase .
Based on Theorem 3.1 and Theorem 3.2 , we summarize the optimization algorithm for HiMLS in Algorithm 1 . There are two training phases including pre training and fine tuning in Algorithm 1 . As shown in Steps 1 9 , the pre training phase goes forward from the first layer to the highest layer , and each layer is trained in a greedy layer wise manner . In contrast , the fine tuning phase shown in Steps 10 17 moves in an opposite direction , and the weights of all the layers will be updated . The convergence of the HiMLS algorithm is guaranteed by Theorem 3.1 and Theorem 32
Time complexity : Similar to other matrix factorization methods based on multiplicative update rules [ 16 , 7 ] , a nice property of the proposed HiMLS algorithm is that most of the computations are matrix multiplications and can be computed efficiently . Theorem 3.3 shows the complexity of the algorithm . The proof is omitted for brevity .
Theorem 3.3
( Complexity ) . The time complexity for the multiplicative update rules in Theorem 3.1 are as follows : O(Ri ) = O(Ru pq + q2 + djq + mq i ) = O ff niN j=1
'V 'T 'T i=1 i=1 djN mN nip + pq + p2ff nip + pq + p2ff
O(Cj ) = O
O(CY ) = O
3 :
4 : 5 : 6 :
7 :
12 : 13 :
14 :
15 :
Input :
Algorithm 1 HiMLS Algorithm matrices ˜Xij ( 1 ≤ i ≤ T , 1 ≤ j ≤ V ) , instance label matrices for train data Yi ( 1 ≤ i ≤ T ) , α , β , number of layers L . Output : Predicted instance label matrices Fi ( 1 ≤ i ≤ T )
Instance feature for test data .
1 : for l = 1 :L do ( l ) 2 : i
( l ) Y and C
( 1 ≤ i ≤ T ) , C
( 1 ≤ j ≤ V ) labels , and features using
Initialize ˜R by clustering instances , probabilistic latent semantic analysis , respectively ; Initialize M ( l−1 ) iY C
, M RT and C
( l−1 ) C ij RT R
( l)† i M †
−1
( l)† j
=
=
( l ) iY
( l ) j
†
−1
( l ) = R ij ( l)† Y where R . Note that M ij = ˜Xij , andM
( 0 ) iY = Yi ;
= ( 0 )
( l)† R i M C T C C repeat
( l ) i ( l ) j
Update ˜R Update C q . 10 ; Update M by Eq 11 and Eq 12 ;
( l ) ij and M
( 1 ≤ i ≤ T ) by Eq 7 and Eq 8 ; ( 1 ≤ j ≤ V ) andC
( l ) Y by Eq 9 and EiY where 1 ≤ i ≤ T , 1 ≤ j ≤ V
( l ) until converged
8 : 9 : end for ; 10 : repeat 11 : Update M
( L ) ij and M by Eq 17 and Eq 18 ; for l = L : 1 do
( L ) iY where 1 ≤ i ≤ T , 1 ≤ j ≤ V
( l ) i
( 1 ≤ i ≤ T ) andR u ( 1 ≤ j ≤ V ) and C
Update R q . 14 ; Update C q . 16 ; Update M V , l ( = L by Eq 11 and Eq 12 ; end for ;
( l ) ij and M
( l ) j i by Eq 13 and E
( l ) Y by Eq 15 and EiY where 1 ≤ i ≤ T , 1 ≤ j ≤
( l )
16 : 17 : until converged 18 : return Predictions for the test data using Eq 5 .
O(Mij ) = O O(MiY ) = O pN pN nidj + qdj + pq + q2 nim + qm + pq + q2 where 1 ≤ i ≤ T , 1 ≤ j ≤ V and N is the number of iteration until convergence .
Note that the dimensionalities of the latent spaces are usually far smaller than the ones in the original spaces , ie , p ) ni and q ) dj . Theorem 3.3 shows that the multiplicative update rules for pre training are scalable to the problem sizes . Likewise , we can obtain the time complexity of the update rules for fine tuning , which are omitted due to space limit .
4 . THE SPECIAL CASES OF HIMLS
The proposed model is a generalized framework for learning complex heterogeneity . It is widely applicable to multiple types of heterogeneous learning problems .
A special case of HiMLS is to learn the common co latent space M shared among all the tasks , view , and labels , ie , Mij = MiY = M ( 1 ≤ i ≤ T , 1 ≤ j ≤ V ) . And by using the training data only , Eq 2 can be specialized as :
T
V i=1 j=1
Xij − RiM CT j
2
F
Yi − RiM CT
Y
2
F
T i=1
+ α min
R,M,C
( 19 )
1379 It is worth noting that Eq 19 is not a trivial special case . Theorem 4.1 shows that some popular methods for learning from single heterogeneity can be viewed as the special cases of our proposed model , such as the multi view learning method MSL [ 24 ] and the multi label learning method LS CCA [ 22 ] . Both MSL and LS CCA are closely related to canonical correlation analysis ( CCA ) , while MSL is a unsupervised learning method aiming to learn the subspace from multiple views , and LS CCA is a supervised learning method for the multi label problem when one of the views used in CCA is derived from the labels .
Theorem 41 The multi view learning method MSL [ 24 ] learning method LS CCA [ 22 ] can be and the multi label viewed as the special cases of HiMLS .
Proof . Consider two special cases of HiMLS for learning from a single heterogeneity as follows :
1 ) Unsupervised multi view learning : By letting T = 1 ,
V = 2 , and α = 0 , Eq 19 can be rewritten into :
X1 − RM C T
1
2
F
X2 − RM C T
2
+
2
F min R,M,C
( 20 ) where Xj(j = 1 , 2 ) is the instance feature matrix for the jth view .
2 ) Supervised multi label learning : By letting T = 1 , V =
1 , and α = 1 , Eq 19 can be rewritten into :
X − RM C T
1
2
F
Y − RM C T
2
+
2
F min R,M,C
( 21 ) where X and Y are the instance feature matrix and instancelabel matrix , respectively .
Mathematically speaking , both Eq 20 and Eq 21 have the same form as follows : min H,C
[ X , Y ] − HC T
2
F
1 , C T 2 fl Z − HC T
2
F min
H,CT C=I fl
( 22 ) ffi
( 23 ) where H = RM and C T = C T . ized data matrix defined as Z = When imposing the orthogonal constraint C T C = I , Eq 22 can be rewritten into :
. Consider the normal
2 X , ( Y Y T )
( XX T )
− 1
− 1
2 Y
Let f ( H , C ) denote the objective function for Eq 23 , which can be transformed into : f ( H , C ) = tr
Z T Z − 2CH T Z + H T H ffi
When fixing C , we have :
∇H f ( H , C ) = −2ZC + 2H = 0 ⇒ H = ZC
By substituting H = ZC into Eq 23 , we have min
CT C=I
2
F
Z − HC T fl ffi tr fl
= min
CT C=I
= tr
Z T Z
Z T Z − 2CC T Z T Z + C T Z T XC − max
C T Z T ZC fl ffi tr
CT C=I ffi
( 24 )
The optimal solution for C is given by the top k eigenvectors of Z T Z . According to [ 24 ] , Eq 24 has the same optimal solution with CCA which aims to optimize : ff
U T XY T V st U T XX T U = V T Y Y T V = I
' max U,V tr
Therefore , the first special case of HiMLS is equivalent to applying CCA to the instance feature matrices from multiple views , which is equivalent to MSL [ 24 ] . The second special case of HiMLS is equivalent to applying CCA to both the instance feature matrix and instance label matrix , which is equivalent to LS CCA [ 22 ] .
5 . EXPERIMENTS
In this section , we verify the effectiveness of the proposed algorithm on various data sets in comparison with the stateof the art techniques . 5.1 Data sets and Setup
Three online benchmark data sets 1 including two text data sets and one image data set are used for evaluation .
The first data set is the Reuters Corpus Volume I ( RCV1V2 ) data set [ 17 ] , which is a collection of over 800,000 newswire stories . There are three category sets of data : Topics ( ie major subject of a story ) , Industry Codes ( ie type of business discussed ) , and Regions ( i . e . geographic locations ) . Each of these category sets has a hierarchical structures . It is usually common to use several subsets of this data , each containing 6000 data instances on average and with a total number of 101 class labels .
EUR Lex [ 18 ] is a text data set containing European Union official laws in practice , different kinds of treaties and agreements , parliamentary journals . This data set contains nearly 20,000 text documents classified according to three different schemas : i ) subject matter ( eg agriculture ) , ii ) official classification hierarchy called the directory codes ( eg a document belonging to a class also belongs to all its parent classes ) , and iii ) EUROVOC , a multilingual thesaurus maintained by the Office for Official Publications of the European Communities . Each of these category sets forms a hierarchical structures .
NUS WIDE 2 [ 6 ] is the a real world web image data set comprising over 269,000 images with over 5,000 user provided tags , and ground truth of 81 concepts with a hierarchical structures . There are several types of low level visual features such as 64 D color histogram in LAB color space , 144D color correlogram in HSV color space , 73 D edge distribution histogram , and 500 D bag of visual words . We use the light version of NUS WIDE .
Table 1 shows the properties of different data sets . Label cardinality is the average number of labels per instance . Accordingly , label density normalizes label cardinality by the the number of labels . Label diversity is the number of distinct label combinations observed in the data set [ 33 ] .
In these data sets , the label refer to the multiple categories each instance belonging to . For the NUS WIDE data , the view refers to different types of low level visual feature . For either RCV1V2 or EUR Lex data sets , similar to [ 29 ] , the data are described from two views : one corresponds to the TF IDF features ; another corresponds to the latent topics obtained by applying probabilistic latent semantic analysis3 on the term counts . The task refers to classify the instances belonging to different sub categories , which follow different but related distributions [ 11 ] .
1
2
3 http://mulansourceforgenet/datasets mlchtml http://lmscompnusedusg/research/NUS WIDEhtm http://learinrialpesfr/people/verbeek/code
1380 Table 1 : Statistics of Different Data sets .
Data set RCV1V2 1 RCV1V2 2 RCV1V2 3 RCV1V2 4 EUR Lex NUS WIDE
Instances Features Labels Cardinality Density Diversity
6000 6000 6000 6000 19348 55615
47236 47236 47229 47236 5000 708
101 101 101 101 412 81
2.880 2.634 2.614 2.484 1.292 1.869
0.029 0.026 0.026 0.025 0.003 0.023
1028 954 939 816 1615 18430
Table 2 : Comparison among HiMLS Variants on RCV1V2 1 .
Table 4 : Comparison among HiMLS Variants on RCV1V2 3 .
Algorithm HiMLS MLS MLS T MLS V MLS S
F1 score 0895±0010 0868±0023 0864±0008 0857±0009 0847±0018
Accuracy 0860±0012 0829±0028 0822±0009 0813±0010 0805±0020
Hamming loss 0081±0007 0102±0018 0106±0006 0107±0005 0116±0011
Algorithm HiMLS MLS MLS T MLS V MLS S
F1 score 0900±0006 0878±0011 0871±0019 0860±0003 0854±0018
Accuracy 0869±0006 0844±0012 0841±0022 0820±0004 0814±0024
Hamming loss 0076±0003 0096±0007 0091±0012 0103±0001 0106±0013
Table 3 : Comparison among HiMLS Variants on RCV1V2 2 .
Table 5 : Comparison among HiMLS Variants on RCV1V2 4 .
Algorithm HiMLS MLS MLS T MLS V MLS S
F1 score 0903±0006 0880±0006 0872±0003 0856±0026 0842±0011
Accuracy 0872±0006 0846±0008 0828±0005 0818±0030 0814±0014
Hamming loss 0073±0003 0089±0004 0098±0005 0102±0017 0100±0007
Algorithm HiMLS MLS MLS T MLS V MLS S
F1 score 0894±0002 0874±0010 0864±0019 0859±0016 0851±0013
Accuracy 0860±0002 0835±0012 0825±0023 0816±0020 0807±0016
Hamming loss 0082±0002 0097±0008 0103±0014 0106±0013 0113±0011
5.2 Evaluation Metrics
In order to comprehensively investigate the performance of the proposed method , we use F1 score , accuracy and Hamming loss on the test data as the evaluation metrics .
F1 score [ 33 ] is the harmonic mean of precision and recall where precision is the proportion of predicted correct labels to the total number of actual labels , recall is the proportion of predicted correct labels to the total number of predicted labels , averaged over all instances . Note that the larger value of F1 score is indicating the better performance .
Accuracy [ 33 ] for each instance is defined as the proportion of the predicted correct labels to the total number of labels for that instance . Overall accuracy is the average across all instances . Note that the larger value of accuracy is indicating the better performance .
Hamming Loss [ 33 ] reports how many times on average , the relevance of an instance to a class label is incorrectly predicted . Therefore , hamming loss takes into account the prediction error ( an incorrect label is predicted ) and the missing error ( a relevant label not predicted ) , normalized over total number of classes and total number of instances . Note that the smaller the value of Hamming loss , the better the performance of the learning algorithm . 5.3 Effectiveness of HiMLS components
In order to demonstrate the effectiveness of simultaneously modeling the multiple heterogeneity in a multi layer framework , we compare HiMLS with its four variants with only one layer : 1 ) multi task multi view variant MLS ; 2 ) multi task single view variant MLS T ; 3 ) multi view singletask variant MLS V ; 4 ) single task single view variant MLSS .
HiMLS and MLS are input with multi task and multi view data . For the single view setting , the features from all the views are concatenated into one single view . For the singletask setting , the instances in all the tasks are pooled into one single task . For HiMLS , we empirically set the number of layers L = 2 , and the numbers of latent topics [ p , q ] for the instances and features(or labels ) to [ 200 , 100 ] , [ 40 , 20 ] in the first and second layer , respectively . For all the other methods with only one layer , we set [ p , q ] = [ 40 , 20 ] .
The classification performances of HiMLS and its variants on RCV1V2 data sets are shown on Tables 2 5 . Based on these comparison results , we have the following findings :
• Both MLS T and MLS V perform better than MLS S in most cases by incorporating either task relatedness or view consistency . It suggests that simply concatenating the features from different views is not the best way to model the view heterogeneity ; likewise , simply pooling the instances of all tasks into one single task is not the best way to model the task heterogeneity . • MLS perform better than either MLS T or MLS V in most cases . It suggests that jointly modeling multiple types of heterogeneity can gain performance improvement upon single heterogeneity learning .
• HiMLS performs better than MLS . It indicates that the learned hierarchical multi latent space helps build a more robust and discriminative classifier . One possible reason to account for this is that the multi layer structure helps find the local optimum in a higher quality by gradually learning the abstract concepts . In contrast , the single layer methods may suffer from the local optimal solution in lower quality .
1381 Table 6 : Classification performance on RCV1V2 1 .
Algorithm HiMLS L2F ML kNN LS ML TRAM
F1 score 0895±0010 0847±0011 0803±0092 0821±0021 0888±0003
Accuracy 0860±0012 0802±0015 0775±0094 0789±0019 0857±0004
Hamming loss 0081±0007 0110±0009 0102±0038 0109±0005 0082±0003
Table 7 : Classification performance on RCV1V2 2 .
Algorithm HiMLS L2F ML kNN LS ML TRAM
F1 score 0903±0006 0884±0005 0772±0009 0828±0019 0874±0004
Accuracy 0872±0006 0850±0005 0751±0008 0799±0016 0848±0004
Hamming loss 0073±0003 0083±0003 0103±0001 0100±0004 0079±0002
Table 8 : Classification performance on RCV1V2 3 .
Algorithm HiMLS L2F ML kNN LS ML TRAM
F1 score 0900±0006 0837±0008 0764±0005 0816±0010 0873±0006
Accuracy 0869±0006 0788±0010 0738±0006 0785±0006 0846±0005
Hamming loss 0076±0003 0120±0005 0115±0001 0107±0003 0081±0003
Table 9 : Classification performance on RCV1V2 4 .
Algorithm HiMLS L2F ML kNN LS ML TRAM
F1 score 0894±0002 0858±0005 0754±0005 0831±0017 0870±0004
Accuracy 0860±0002 0816±0005 0728±0007 0801±0015 0851±0006
Hamming loss 0082±0002 0106±0005 0118±0004 0104±0004 0075±0004
Table 10 : Classification performance on EUR Lex .
Algorithm HiMLS L2F ML kNN LS ML TRAM
F1 score 0749±0009 0713±0020 0498±0029 0664±0013 0667±0016
Accuracy 0719±0011 0680±0020 0472±0027 0631±0013 0635±0016
Hamming loss 0033±0002 0033±0003 0043±0002 0088±0006 0040±0002
Table 11 : Classification performance on NUS WIDE .
Algorithm HiMLS L2F ML kNN LS ML TRAM
F1 score 0675±0007 0700±0001 0589±0004 0628±0021 0684±0007
Accuracy 0645±0006 0615±0002 0582±0003 0618±0020 0676±0008
Hamming loss 0187±0003 0204±0002 0215±0002 0190±0008 0166±0003
5.4 Performance Comparison
In this work , we focus on improving the performance of multi label learning by leveraging the multiple type of heterogeneity . To the best of our knowledge , there is no previous work for learning from the triple heterogeneity . Therefore , we compare HiMLS with a variety of multi label learning methods which learn from single or dual heterogeneity . The comparison approaches includes : 1 ) multi view multilabel learning methods L2F [ 27 ] ; 2 ) graph based multi label approach ML kNN [ 32 ] ; 3 ) multi label method based on subspace learning LS ML [ 14 ] ; 4 ) transductive multi label learning approach TRAM [ 15 ] .
HiMLS is input with multi task and multi view data . For the other algorithms , the instances of all the tasks are pooled together . L2F method is given the multi view features , whereas the other methods are given the concatenated features from all the views . The parameters are tuned for each algorithm using cross validation on the training data . We repeat the experiments ten times for each data set and report the average performances and the standard deviations . Tables 6 9 show the classification performances of different methods on RCV1V2 . The performances on EUR Lex and NUS WIDE are shown in Tables 10 11 , respectively .
From these results , we can see that HiMLS performs bet ter than the other algorithms in most cases . For ML kNN [ 32 ] , since it ignores the correlation among multiple labels , its performance on these data sets is not comparable with the other methods . In contrast , LS ML [ 14 ] learns a common subspace shared among multiple labels , which helps improve the learning performance for the multi label data . However , since its objective function is non convex , the performance of LS ML may be limited by the local optimum problem . TRAM [ 15 ] is a tranductive multi label learning method which tries to exploit the information from unlabeled data to estimate the optimal label concept compositions . The results show that unlabeled data can provide helpful information to build the multi label classifier . Different from these methods for learning from single heterogeneity , both HiMLS and L2F [ 27 ] model the feature and label heterogeneity and gain performance improvement by enhancing the view consistency . It suggests that treating the features from different views in a discriminative and complementary way is usually better than just concatenating all the features into one view . Likewise , treating the instances in different tasks discriminatively is usually better than just pooling all the instances together . The performance superiority of HiMLS over the comparison methods verifies the effectiveness of the proposed approach to model the complex heterogeneity in a principled framework . Another important competency of HiMLS is that its multi layer structure helps build a robust classifier by gradually finding the more high level concepts in the deep structures .
TRAM performs a little better than HiMLS on NUSWIDE data set . It indicates that NUS WIDE may be consistent with the smoothness assumption , and TRAM is able to effectively leverage this assumption .
5.5 Parameter Sensitivity and Convergence We study the parameter sensitivity on the RCV1V2 1 data set . α and β are tuned on the grid 10[−3:1:3 ] . The results are shown in Figure 1(a b ) . α is used to balance the importance of classification loss . The algorithm performs worse as α approaches 0 . When α = 0 , it means that no label information is used for training . The optimal performance is achieved at α = 1 . Nevertheless , the performance is quite robust over a wide range of values of α . β is used to control the importance of regularization . The result shown in Figure 1(b ) indicates that setting appropriate weight to the regularization term can lead to better performance . As a result , we tune the parameters , α and β , for each data set by cross validation on the training data .
Here , we empirically study the convergence of HiMLS on the RCV1V2 1 data set . The result is shown in Figure 1(c ) . From this figure , we can see that HiMLS converges fast and its performance becomes stable after 50 iterations .
1382 0.95
0.9
0.85
0.8 e r o c s − 1 F
0.95
0.9
0.85
0.8 e r o c s − 1 F
0.9
0.88
0.86
0.84
0.82
0.8 e r o c s − 1 F
0.75
0 0.001 0.01 0.1
1
10 100 1000
α
0.75 0
0.001 0.01 0.1
1
10
100 1000
β
0.78 0
10
20
30
Iteration
40
50
60
Figure 1 : From left to right : a ) F1 score vs . α ( log10 scale ) ; b ) F1 score vs . β ( log10 scale ) ; c ) F1 score vs . iteration .
6 . CONCLUSION
In this paper , we propose a multi layer framework to jointly model triple heterogeneity . In each layer , we learn a multilatent space shared among the instances and labels from multiple tasks , and features from multiple views . Then the multi latent model is used as a building block to stack up a multi layer structure so as to gradually learn the more abstract concepts . A deep learning algorithm is proposed to solve the optimization problem , which first pre trains each layer and then fine tunes the whole multi layer structure by using the multiplicative update rules . The comparison experiments with state of the art methods demonstrate the effectiveness of the proposed model .
Acknowledgment This work is partially supported by the NSF ( No . IIS1017415 ) , the Army Research Laboratory ( No . W911NF 09 2 0053 ) , Region II University Transportation Center ( No . 49997 33 25 ) , DARPA ( No . W911NF 11C 0200 and W911NF 12 C 0028 ) , and NSFC ( No . 61473123 ) .
7 . REFERENCES [ 1 ] R . K . Ando and T . Zhang . A framework for learning predictive structures from multiple tasks and unlabeled data . Journal of Machine Learning Research ( JMLR ) , 6:1817–1853 , 2005 .
[ 2 ] A . Argyriou , T . Evgeniou , and M . Pontil . Multi task feature learning . In NIPS , pages 41–48 , 2006 .
[ 3 ] A . Blum and T . Mitchell . Combining labeled and unlabeled data with co training . In COLT , pages 92–100 , 1998 .
[ 4 ] R . Caruana . Multitask learning . Machine Learning ,
28(1):41–75 , 1997 .
[ 5 ] N . Chen , J . Zhu , and E . P . Xing . Predictive subspace learning for multi view data : a large margin approach . In NIPS , 2010 .
[ 6 ] T . Chua , J . Tang , R . Hong , H . Li , Z . Luo , and Y . Zheng . NUS WIDE : a real world web image database from national university of singapore . In CIVR , 2009 .
[ 7 ] C . H . Q . Ding , T . Li , W . Peng , and H . Park .
Orthogonal nonnegative matrix tri factorizations for clustering . In KDD , pages 126–135 , 2006 .
[ 8 ] A . Elisseeff and J . Weston . A kernel method for multi labelled classification . In NIPS , pages 681–687 , 2001 .
[ 9 ] J . D . R . Farquhar , D . R . Hardoon , H . Meng ,
J . Shawe Taylor , and S . Szedm´ak . Two view learning : Svm 2k , theory and practice . In NIPS , 2005 .
[ 10 ] P . Gong , J . Ye , and C . Zhang . Robust multi task feature learning . In KDD , pages 895–903 , 2012 .
[ 11 ] J . He and R . Lawrence . A graph based framework for multi task multi view learning . In ICML , pages 25–32 , 2011 .
[ 12 ] S J Huang , Y . Yu , and Z H Zhou . Multi label hypothesis reuse . In KDD , pages 525–533 , 2012 .
[ 13 ] S J Huang and Z H Zhou . Multi label learning by exploiting label correlations locally . In AAAI , pages 1–7 , 2012 .
[ 14 ] S . Ji , L . Tang , S . Yu , and J . Ye . Extracting shared subspace for multi label classification . In KDD , pages 381–389 , 2008 .
[ 15 ] X . Kong , M . K . Ng , and Z H Zhou . Transductive multilabel learning via label set propagation . IEEE Trans . Knowl . Data Eng . ( TKDE ) , pages 704–719 , 2013 .
[ 16 ] D . D . Lee and H . S . Seung . Algorithms for non negative matrix factorization . In NIPS , pages 556–562 , 2000 .
[ 17 ] D . D . Lewis , Y . Yang , T . G . Rose , and F . Li . Rcv1 : A new benchmark collection for text categorization research . Journal of Machine Learning Research ( JMLR ) , 5:361–397 , 2004 .
[ 18 ] E . L . Menc´ıa and J . F¨urnkranz . Efficient pairwise multilabel classification for large scale problems in the legal domain . In ECML PKDD , pages 126–135 , 2008 .
[ 19 ] V . Sindhwani and D . S . Rosenberg . An rkhs for multi view learning and manifold co regularization . In ICML , pages 976–983 , 2008 .
[ 20 ] K . Sridharan and S . M . Kakade . An information theoretic framework for multi view learning . In COLT , pages 403–414 , 2008 .
[ 21 ] L . Sun , S . Ji , and J . Ye . Hypergraph spectral learning for multi label classification . In KDD , pages 668–676 , 2008 .
[ 22 ] L . Sun , S . Ji , and J . Ye . Canonical correlation analysis for multilabel classification : A least squares formulation , extensions , and analysis . IEEE Trans . Pattern Anal . Mach . Intell . , 33(1):194–200 , 2011 .
[ 23 ] G . Tsoumakas and I . Katakis . Multi label classification : An overview . International Journal of Data Warehousing and Mining , 3(3):1–13 , 2007 .
[ 24 ] M . White , Y . Yu , X . Zhang , and D . Schuurmans .
Convex multi view subspace learning . In NIPS , pages 1682–1690 , 2012 .
1383 [ 25 ] H . Yang and J . He . Learning with dual heterogeneity :
A nonparametric bayes model . In KDD , pages 582–590 , 2014 .
Based on Lemma 3.1 , we can derive the updating rule for Mij as in Eq 11 .
5 ) Update rule for MiY : The objective function for MiY
[ 26 ] P . Yang , J . He , and J Y Pan . Learning complex rare is as follows : categories with dual heterogeneity . In SDM , 2015 .
[ 27 ] P . Yang , J . He , H . Yang , and H . Fu . Learning from
J ( MiY ) = α
Yi − RiMiY C T
Y
2
F
V
+ β j=1
'MiY − Mij'2
F
Based on Lemma 3.1 , we can derive the updating rule for MiY as in Eq 12 . B . PROOF OF THEOREM 3.2
Proof . Based on Lemma 3.1 , we can derive the updating rules for the objective function in Eq 4 and prove the convergence and correctness . The detailed deduction is as follows :
1 ) Update rule for R
( l ) i
( l ) : The objective function for R i is as follows :
'
J
R
( l ) i
Xij − R ff V Yi − R j=1
=
+ α
( 1:L ) i
· M
( L ) iY · C
( 1:L)T Y
2
F
( 1:L ) i
· M
( L ) ij
· C
( 1:L)T j
2
F
Based on Lemma 3.1 , we can derive the updating rule for R i as in Eq 13 and Eq 14 , respectively .
( l )
: The objective function for C
( l ) j is i and Ru 2 ) Update rule for C
( l ) j as follows :
T
˜Xij − ˜R
J
( l ) j
C
= i=1
( 1:L ) i M
( L ) ij C
( 1:L)T j
Based on Lemma 3.1 , we can derive the updating rule for C
( l ) Y : The objective function for C
( l ) Y is as in Eq 15 .
( l ) j 3 ) Update rule for C as follows :
'
' ff ff
T
Yi − R
J
( l ) Y
C
= α i=1
( 1:L ) i M
( L ) iY C
( 1:L)T Y
Based on Lemma 3.1 , we can derive the updating rule for C
: The objective function for M
( L ) ij as in Eq 16 .
( l ) j 4 ) Update rule for M is as follows :
'
J
M
( L ) ij
+ β
( L ) ij
˜Xij − L l=1
( L ) ij − M
( L ) iY ff M
=
· M
( L ) ij
( l ) i
˜R
2
F
· 1 l=L
( l)T j
C
2
F
Based on Lemma 3.1 , we can derive the updating rule for M
( L ) iY : The objective function for M
( L ) iY as in Eq 17 .
( L ) ij 5 ) Update rule for M is as follows :
'
J ff M
Yi − R 2
( L ) iY
F
( L ) ij − M
= α
M
( L ) iY
V
β j=1
( 1:L ) i
· M
( L ) iY · C
( 1:L)T Y
+
2
F
2
F
2
F label and feature heterogeneity . In ICDM , 2014 .
[ 28 ] H F Yu , P . Jain , P . Kar , and I . S . Dhillon .
Large scale multi label learning with missing labels . In ICML , pages 593–601 , 2014 .
[ 29 ] D . Zhang , J . He , and R . D . Lawrence . Mi2ls : multi instance learning from multiple informationsources . In KDD , pages 149–157 , 2013 .
[ 30 ] J . Zhang and J . Huan . Inductive multi task learning with multiple view data . In KDD , pages 543–551 , 2012 .
[ 31 ] M L Zhang and K . Zhang . Multi label learning by exploiting label dependency . In KDD , pages 999–1008 , 2010 .
[ 32 ] M L Zhang and Z H Zhou . Ml knn : A lazy learning approach to multi label learning . Pattern Recognition , pages 2038–2048 , 2007 .
[ 33 ] M L Zhang and Z H Zhou . A review on multi label learning algorithms . IEEE Transactions on Knowledge and Data Engineering , 26(8):1819–1837 , 2014 .
[ 34 ] J . Zhou , J . Chen , and J . Ye . Clustered multi task learning via alternating structure optimization . In NIPS , pages 702–710 , 2011 .
APPENDIX A . PROOF OF THEOREM 3.1
Proof . Based on Lemma 3.1 , we can derive the updating rules for the objective function in Eq 2 and prove the convergence and correctness . The detailed deduction is as follows :
1 ) Update rule for Ri : The objective function for Ri is as
V
Xij − RiMijC T j
2
F
Yi − RiMiY C T
Y
2
F
+ α follows :
J ( Ri ) = j=1
Based on Lemma 3.1 , we can derive the updating rules for Ri and Ru i as in Eq 7 and Eq 8 , respectively .
2 ) Update rule for Cj : The objective function for Cj is as follows :
T
˜Xij − ˜RiMijC T j
2
F
J ( Cj ) = i=1
Based on Lemma 3.1 , we can derive the updating rule for Cj as Eq 9 .
3 ) Update rule for CY : The objective function for CY is as follows :
T
Yi − RiMiY C T
Y
2
F
J ( CY ) = α i=1
Based on Lemma 3.1 , we can derive the updating rule for CY as in Eq 10 .
4 ) Update rule for Mij : The objective function for Mij is as follows :
J ( Mij ) =
˜Xij − ˜RiMijC T j
2
F
+ β 'Mij − MiY '2
F
Based on Lemma 3.1 , we can derive the updating rule for M
( L ) iY as in Eq 18 .
1384
