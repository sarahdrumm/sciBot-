Probabilistic Graphical Models of Dyslexia
Yair Lakretz
Gal Chechik
Sagol School of Neuroscience
The Gonda Brain Research
Tel Aviv University
69978 , Israel yairlakretz@posttauacil
Center
Bar Ilan University
52900 , Israel galchechik@biuacil
Naama Friedmann
Sagol School of Neuroscience and School of Education
Tel Aviv University
69978 , Israel naamafr@posttauacil
Michal Rosen Zvi
IBM Research Laboratory
Haifa , Israel rosen@ilibmcom
ABSTRACT Reading is a complex cognitive process , errors in which may assume diverse forms . In this study , introducing a novel approach , we use two families of probabilistic graphical models to analyze patterns of reading errors made by dyslexic people : an LDA based model and two Na¨ıve Bayes models which differ by their assumptions about the generation process of reading errors . The models are trained on a large corpus of reading errors . Results show that a Na¨ıve Bayes model achieves highest accuracy compared to labels given by clinicians ( AU C = 0.801 ± 0.05 ) , thus providing the first automated and objective diagnosis tool for dyslexia which is solely based on reading errors data . Results also show that the LDA based model best captures patterns of reading errors and could therefore contribute to the understanding of dyslexia and to future improvement of the diagnostic procedure . Finally , we draw on our results to shed light on a theoretical debate about the definition and heterogeneity of dyslexia . Our results support a model assuming multiple dyslexia subtypes , that of a heterogeneous view of dyslexia .
Categories and Subject Descriptors J.4 [ Computer Applications ] : Social and behavioral sciences
General Terms Psychology
Keywords Probabilistic Graphical Models ; Dyslexia ; Diagnosis ; Latent Dirichlet Allocation ; Na¨ıve Bayes
1 .
INTRODUCTION
Reading is an intricate task involving multiple processes located in different areas in the brain [ 25 ] . Errors in reading can result from dyslexia , a disorder involving a variety of such errors . Dyslexia is a highly common disorder with estimates of prevalence ranging from 5 % to 17.5 % [ 34 ] , pointing to the need for accurate and accessible diagnosis . However , current diagnostic procedures require experts , and are mostly lengthy and expensive . In this study , we aim to provide an accessible , reliable and cheap diagnosis tool for dyslexia . We do this by showing that rich probabilistic graphical models can diagnose dyslexia closely to how experts do , rendering them accurate , reliable and automated tool for diagnosis . Moreover , by providing probability distributions of errors given target words in screening tests , models can be used for future improvement of the diagnosis procedure and understanding of dyslexia . Finally , our results provide a novel and quantitative perspective on an ongoing debate regarding the nature of dyslexia ( eg , [ 9] ) .
A dyslexic person may read the word three as there , as tree or as four . Despite this heterogeneity , many studies in the field of reading disorders focus on measures of speed and accuracy to characterize dyslexia , thus possibly ignoring the rich structure of types of reading errors . These studies often assume a common phonological [ 26 ] , [ 28 ] , [ 36 ] , [ 37 ] or sensorimotor [ 37 ] cause to all types of reading errors . However , an alternative approach views dyslexia as an umbrella term for various deficiencies , or dyslexia subtypes , each characterized by specific error types ( eg , [ 6 ] , [ 11 ] , [ 20] ) . According to this approach , each of the errors above could result from a different subtype of dyslexia . This approach suggests that fundamental aspects of reading disorders lay in the complex structure of reading errors .
We explore the structure of reading errors and the heterogeneity of dyslexia using probabilistic tools . Probabilistic graphical models have several advantages as a framework for modelling reading errors . Probabilistic models allow explicitly capturing a dependency structure among variables in the problem , making learning more efficient . Probabilistic graphical models provide both diagnoses and explicit error distributions per each type of dyslexia and target word , as learned from the corpus data , and can naturally follow an error generation process .
We use two families of graphical models . First , we construct Na¨ıve Bayes models which are well known for hav
1919 ing a good performance as classifiers with relatively low model complexity ( see , eg , [ 18] ) . These models are thus suitable for the classification of reading errors . Next , we construct a Latent Dirichlet Allocation based ( LDA based ) model , which has been proven to well capture generation processes in the context of writing and text [ 1 ] , [ 31 ] . In such a model , the generation of reading errors made by a dyslexic person is a more complex process than that in a Na¨ıve Bayes model , as it assumes that errors are generated according to a mixture of subtypes of dyslexia . An LDA model is therefore a natural candidate for dyslexia analysis .
We compare the models on two different tasks . First , we identify the probabilistic model that best performs on the task of diagnosing dyslexia . We achieve this by comparing the diagnoses of the models to those of experts . Secondly , we explore which model best captures the complex structure of the patterns of reading errors . We achieve this by comparing the models on the task of predicting unseen errors . Finally , the performance of the models is used to explore the question of whether reading errors do result from different subtypes of dyslexia , or rather from one general malfunction .
The rest of the paper is organized as follows : section 2 describes the related literature of probabilistic graphical models and of dyslexia . It also provides a summary of the major views of dyslexia in the current literature of reading disorders . Section 3 describes the structure and size of the data that was used in this study . Section 4 provides a detailed description of the probabilistic graphical models . Finally , section 5 describes the results of this study : the performance of the models on the task of diagnosing dyslexia ( section 5.1 ) , and the performance of the models in predicting reading errors ( section 52 )
2 . RELATED WORK
2.1 Probabilistic graphical models
The LDA model was introduced a decade ago to discover topics in documents , and has been most influential since . Three main algorithms for approximate inference in the model were proposed : Variational Expectation Maximization [ 1 ] , the Expectation Propagation [ 21 ] , and Collapsed Gibbs Sampling [ 15 ] . In this paper we use the latter method . The generative process described by the LDA model commences in Dirichlet priors which are often chosen to be uniform . However , following the demonstration of the importance of using non uniform priors for this model [ 39 ] , we use these and not uniform priors in this study . 2.2 Dyslexia and reading errors
Literature on reading disorders presents various accounts of the causes of developmental dyslexia . One central approach describes developmental dyslexia as a disorder that can be reduced into a single cause , and will be referred to hereby as the single type approach . The most influential example of this approach is the phonological deficit hypothesis . According to this theory , developmental dyslexia originates from a deficit affecting either the representations and processing of speech sounds [ 34 ] , [ 35 ] , [ 36 ] , or as other authors have argued , from the access to these representations [ 27 ] . This hypothesis had influence on much research in neuroscience and brain imaging studies , eg , [ 2 ] , [ 8 ] , [ 10 ] , [ 14 ] , [ 26 ] , [ 28 ] .
Figure 1 : The dual route model ( see , eg , [ 11] ) .
Another central approach describes dyslexia as a heterogeneous disorder , having several different causes , and will be referred to hereby as the subtypes approach . The most influential example of this approach derives from the Dual Route Reading Model [ 5 ] , [ 6 ] , [ 20 ] , describing the process of reading by several processing stages and routes ( figure 1 ) . According to this approach , there are subtypes of dyslexia , each resulting from a deficit or malfunction in a different stage in the process of reading . For example , reading signs as sings may result from a specific malfunction in letter position processing during the early visual orthographic stage , and is in this case ascribed to Letter Position Dyslexia [ 12 ] . However , reading signs while pronouncing the ’g’ , may be caused by Surface Dyslexia , characterized by reading irregular words according to the regular grapheme to phoneme conversion rules [ 7 ] . The deficit leading to Surface Dyslexia occurs in one of two long term memory lexicons , the orthographic or phonological lexicon , or in the information processing between them . Brain imaging studies following this approach aim to identify the neural correlates of different subtypes of dyslexia , eg , [ 16 ] , [ 19 ] .
The single type approach and the subtypes approach use different procedures to diagnose dyslexia . The diagnostic procedure based on the single type approach mainly uses phonological tasks , for example , asking the subject to swap between the first phonemes in the two words fresh bread ( expecting to get bresh fread ) . When otherwise using reading tasks , the degree of dyslexia is assessed by measuring accuracy and speed in such tasks , while disregarding the types of errors which were made . However , the diagnostic procedure based on the subtypes approach mainly uses reading tasks , asking the subject to read aloud a list of words which are prone to erroneous reading . These tests assess the subtype of dyslexia by counting the errors made of each type .
Another group of studies aim to characterize dyslexia subtypes ( eg , [ 23] ) , however the diagnostic procedure in these studies is not based on data of error types in reading tests , but on measures of speed and accuracy in various tasks . Several such studies have proposed probabilistic models to analyze the data ( eg , [ 24] ) . See also [ 30 ] for analysis of reading aloud data , however in contrast to our study , they were not concerned with error types in reading tests but focused on reading of nonwords by normal subjects .
1920 3 . THE DATA
We test our models on reading tests collected from 313 Hebrew speaking individuals with developmental dyslexia , aged 7 to 62 years , all diagnosed in the Language and Brain laboratory in Tel Aviv University . Out of the 313 subjects , 97 were diagnosed with an Attention Deficit Disorder ( ADD ) in addition to dyslexia . This is , to our knowledge , the largest reading errors corpus used so far ( see [ 22 ] , [ 29] ) . The battery of tests has two parts : ( 1 ) screening tests : reading tests given for initial assessment of dyslexia ( TILTAN battery , [ 13] ) , and ( 2 ) varying post tests which are determined according to the results of the screening tests . These are used to further specify the diagnosis . Since different subjects take different post tests , only the corpus of the screening tests was used in this study .
The screening test is composed of 196 target words in Hebrew , which are prone to erroneous readings . The audio responses of subjects are recorded , and then encoded before used in the model . Responses were encoded by experts in dyslexia diagnosis and research , into one of 19 different error types ( appendix B , table 2 ) . Each of the 313 subjects therefore read 196 words in the screening test , resulting in a corpus consisting of a total of 61,348 pairs of target word and response .
Based on the screening tests and on additional post tests , had these been conducted , each subject is diagnosed by an expert by marking the subtypes of dyslexia that he or she has . We represent each diagnosis as a multi label binary vector of length 7 , corresponding to seven dyslexia subtypes : ’Letter position dyslexia’ , ’Attentional dyslexia’ , ’Neglexia’ , ’Surface dyslexia’ , ’Visual dyslexia’ , ’Vowel letter dyslexia’ and ’Normal state’ .
The collected data corpus is in Hebrew , therefore it is important to discuss the applicability of this work to other languages , specifically English . The underspecification of vowels in Hebrew makes the Hebrew orthography neighborhood denser than that of languages such as English . An example of this is that in Hebrew , letter transpositions inside a word are more likely to result in another existing word , than in English . This property makes some dyslexias more easily detectable in Hebrew than in English . However , once the relevant stimuli are selected for an English reading task , the same dyslexias with the same properties are detectable in both languages . And indeed , the dyslexias we describe have all been identified in English ( For a comprehensive survey of this literature of dyslexia types in English , see for example [ 6 ] ; for the specific dyslexias that we describe here see Letter position dyslexia : [ 6 ] ; Attentional dyslexia : [ 32 ] , [ 33 ] ; Neglexia : [ 17 ] ; Surface dyslexia : [ 3 ] , [ 4 ] , [ 5 ] , [ 7] ) . This work can therefore be expanded to English and other languages .
[ 38 ] ; vowel dyslexia :
4 . THE MODELS
Bayesian models are expressive statistical models . Typically , these are used for mathematically modeling assumptions regarding the manner in which data ( in this case , reading errors ) , is generated . We construct three different probabilistic graphical models for this purpose : a Latent Dirichlet Allocation ( LDA ) model , and two Na¨ıve Bayes models .
4.1 Naïve Bayes models
We begin to explore the generation process of reading errors with the class of Na¨ıve Bayes models . Na¨ıve Bayes models have shown much success in classification tasks , and could therefore fit the task of classifying reading error into dyslexia subtypes .
The types of the errors made by the subjects are denoted in the models by ei(i = 1 , . . . , N ) . This variable takes one of E possible values ei ∈ 1 , . . . , E , where E stands for the number of possible error types including a correct response . Another observed variable of the model represents the targetwords . We denote it by wi(i = 1 , . . . , N ) , where N represents the number of target words in the screening test . This variable can have one of W possible values wi ∈ 1 , . . . , W . Since in our case each target word is present only once in the screening test , we have W = N .
In all three models , a dyslexia state defines a hidden variable that stands for a state associated with a specific subtype of dyslexia , and is characterized by particular reading errors . For example , being in a state of low orthographic attention could be associated with Letter Position Dyslexia ( LPD ) , typically leading to letter migration errors . We denote a dyslexia state by di , where di can have one of D di ∈ 1 , . . . , D dyslexia states . We have 7 such states as described above .
In all models , S stands for the total number of subjects , N stands for the number of target words in the screening test , W is the size of the vocabulary of the target words , and D stands for the total number of dyslexia states . e represents an error type , w a target word and d a dyslexia state . 411 Word independent Naïve Bayes Model Typically , clinicians use the screening test to diagnose dyslexic subjects based on the total counts of different errors types they make . This approach disregards the specific target words on which the errors were made , for instance , whether it is a word that many subjects misread , or specific subjects only . This diagnosis procedure can be modeled with a Na¨ıve Bayes model in which the probability of an errortype depends on the dyslexia subtype only . In what follows , we refer to this model as the Na¨ıve Bayes 1 ( NB1 ) model .
Model description .
For test results of a single subject , the joint distribution of the NB1 model factorizes as :
N p(¯e , ¯w , d ) = p(d ) p(ei | d)p(wi ) i=1 where the subscript i runs over all word error pairs in the screening test . Note that each error is independent of the specific target word on which it was made . Figure 2a describes the plate diagram of the model . 412 Word dependent Naïve Bayes Model A more complex model is a Na¨ıve Bayes model in which the probability of an error is dependent on the dyslexia subtype and also on the target word . We examine this model as well and will refer to it in what follows as the Na¨ıve Bayes 2 ( NB2 ) model .
1921 Figure 2 : Plate diagrams for all three models . ( a ) Na¨ıve Bayes 1 ( b ) Na¨ıve Bayes 2 ( c ) LDA like model . S represents the total number of subjects , N the number of target words in the screening test , W the size of the vocabulary of the target words , and D represents the total number of dyslexia states . e represents an error type , w a target word and d a dyslexia state . In the LDA model , θ represents a diagnosis , φ represents a word error dyslexia distribution . α and β are the Dirichlet priors for these distributions respectively .
Model description .
In this model the response of the subject ei is dependent on the ith target word wi and on the dyslexia state di . Similarly to the NB1 model , the joint distribution for the NB2 factorizes as :
N p(¯e , ¯w , d ; β ) = p(d ) p(ei | d , wi ; β)p(wi ) i=1 where the subscript i runs over all word error pairs in the screening test as before . Figure 2b describes the plate diagram of the model . 4.2 The LDA model
We continue our inquiry with a particular family of Bayesian models that has attracted much attention in the past decade , the Latent Dirichlet Allocation ( LDA ) Model [ 1 ] . The LDA model enables inferring unobserved variables given observed data , and has shown much success in capturing generative processes of text documents [ 1 ] , [ 31 ] . 421 Model description The LDA model captures the more complex assumption that a person can suffer from a mixture of different dyslexia subtypes . According to this model , each person has a unique distribution over dyslexia subtypes . Additionally , each dyslexia subtype has a unique distribution of expected errors for a given target word . When a person is reading a targetword , a particular dyslexia subtype is directing the way by which the word is read . The response is then dependent on this dyslexia subtype and the target word . We therefore define two additional hidden variables . A diagnosis θs , which is a distribution over dyslexia states di for a subject . This distribution is learned from the data . A word error dyslexia distribution φwed = p(ei = e | wi = w , di = d ) is a distribution for all subjects in which , given a dyslexia state d and a target word w , each error has a certain probability of occurring . For example , given an LPD state , and the target word three , the erroneous response there would be of high probability , but tree would be of a lower one . Similarly , given a normal state , and the target word three , all error responses would have low probabilities except for the ( correct ) response three .
The input to the model is the word error pairs of all subjects . The hidden variables discovered by the model are : ( 1 ) N dyslexia states for each subject ; ( 2 ) the diagnosis of each subject θs , and ( 3 ) a single word error dyslexia distribution φwed for all subjects . Figure 2c describes the plate diagram of the dyslexia state model . 422 Dirichlet priors The generative process we use to model reading errors uses two Dirichlet priors from which the two distributions θs , φwed are sampled . Dirichlet priors are often chosen to be uniform . However , adding priors that adequately describe the generative process could enhance model performance . As the subtype approach to dyslexia has developed ways to detect typical reading errors for each dyslexia subtype , this prior knowledge can be incorporated into the LDA model . We tested the model both with this prior knowledge coded into it and without it .
The expert knowledge is used for sampling the word errordyslexia distribution and we denote it by βwed . This knowledge tells us about the error expected for each target word given a dyslexia state ( figure 2c ) . For example , given an LPD dyslexia state , the word three is more likely to be read as there than tree or four . We incorporate this into the model by assigning a higher prior value βhigh to the more probable responses of an LPD dyslexia state , in this example there . We also assign a lower prior value βlow to the less probable responses , in this case tree and four . This is repeated for all dyslexia states , including the normal state .
1922 423 Parameter estimation Gibbs sampling is a form of a Markov Chain Monte Carlo ( MCMC ) method which is widely used for parameter estimation in topic models ( [15 ] , [ 31] ) . We use Gibbs sampling to construct a Markov chain between dyslexia states , where the order of the Markov chain is based on the order of targetwords in the reading test . In each step , a dyslexia state is sampled from its posterior distribution conditioned on all other variables in the model ( see appendix A ) : p(di = d | wi = w , ei = e , d−i , w−i , e−i , α , β ) ∝
Cwed,−i + βwed e [ Cwed,−i + βwed ]
Cds,−i + αds d [ Cds,−i + αds ]
( 1 ) where , di = d represents the assignment of the ith word error pair to dyslexia state d . wi = w represents the observation that the ith target word is w , and ei = e represents the observation that the ith error type is e . d−i represents all dyslexia state assignments excluding the current ith instant . w−i represents all target words observations excluding the current ith instant . Similarly , e−i represents all target words observations excluding the current ith instant . Cwed,−i represents the number of times the word error pair ( w , e ) was assigned to the dyslexia state d , excluding the current instance . Cds,−i represents the number of times the dyslexiastate d was assigned to the dyslexic person s . αds , βwed are the Dirichlet prior weights ( see 412 ) At each step of the Markov chain , we update the diagnoses of the subjects {θs}S s=1 and the word error dyslexia distribution φwed according to the dyslexia state assignments ¯ds . These are then used to assess the accuracy and predictive power of the model .
5 . EXPERIMENTS
We conduct two experiments with the data . In the first , described in section 5.1 , we compare three graphical models to find which model most accurately predicts dyslexia diagnosis . In principle , this model can be used to automate dyslexia diagnosis . In the second experiment , described in section 5.2 , we compare the quality by which these three graphical models predict specific reading errors . The best model to predict errors could contribute to the better characterization of dyslexia .
Experimental set up .
We use two different types of cross validation , first leaving out a set of subjects and then leaving out a set of word readings of the test subjects . Specifically , we randomly split the data into a training set with 219 subjects ( 70 % ) and a testset with 94 subjects ( 30% ) , controlled to have the same proportion of subjects which are also diagnosed with Attention Deficit Disorder . See illustration in figure 3 . The test set is further divided into two equal sized sets : a diagnosis set with 98 pairs of target words and subject responses , used to infer the diagnosis of test subjects , and a predictionset also with 98 word response pairs , used for assessing the predictions of the models . 5.1 Probabilistic modeling as a dyslexia diag nosis tool
Automation of the diagnosis process could facilitate the accessibility of the process to many people and increase
Figure 3 : Data is partitioned into 3 sets . The dataset is first divided into a training set which contains reading tests of 70 % of the subjects , and a test set which contains the remaining 30 % of the data . The test set is further divided into two subsets , a diagnosis set containing the first halves of the reading tests , and a prediction set containing the remaining reading tests of the test subjects . awareness to treatment of specific reading deficits . We compare the automatic diagnosis of the three models – NB1 , NB2 and LDA – to those given by experts , to test if the automated tool can indicate the diagnosed dyslexia type with high accordance to diagnoses given by experts . We evaluate the diagnosis quality as follows . The automatic diagnosis is based on the diagnosis set . This set is used to infer the diagnosis distribution θs for each subject . The accuracy of the models is then assessed using the diagnoses given by experts . The prediction set together with the diagnosis distribution θs are later used for the evaluation of the predictive power of the models . The prediction set is therefore not used in the diagnosis process . Diagnosis quality of the two Na¨ıve Bayes models , is estimated by computing p(d | ¯e , ¯w ) ∝ p(d ) p(ei | d , wi ) i=1 where the dyslexia prior p(d ) , the probability of error given a dyslexia subtype p(ei|d ) , and the probability of an error given a dyslexia type and a target word p(ei | d , wi ) are evaluated from the training group .
For the LDA model , we train the model on the trainingset together with the diagnosis set . The model is initialized with random dyslexia state assignments { ¯ds}P s=1 which are then iteratively sampled according to the sampling rule in ( 1 ) . At each step of this process , we update the dyslexia distribution per each subject θs and the word error dyslexia p(d | ¯e ) ∝ p(d )
[ p(ei | d)]Cei
N N i=1
( 2 )
( 3 )
1923 distribution φwed according to the new sampled dyslexiastate .
The reason we train the model on the training set and the diagnosis set together is to avoid a discrepancy between the hidden dyslexia states of the word error dyslexia distribution φwed , learned from the training set , and those of the dyslexia distributions {θs}s∈test−set , learned from the diagnosis set . If the model is trained on the sets separately , the hidden states of these distributions might differ by a permutation relation . All results below are averages over 5 splits of subjects to train and test sets ( allocations of subjects ) , and also over 10 different random initializations of the dyslexia state assignments ( a total of 50 runs ) . 500 iterations were found to suffice for convergence ( figure 5A ) .
We compare the diagnoses given by the three models to those given by experts . The Na¨ıve Bayes models are known to be strong classifiers and were designed by similar assumptions and guidelines to the ones held by diagnosticians , and therefore their diagnoses are expected to strongly correlate with those given by experts . The LDA model is designed to capture the generative process of the patterns of reading errors , ( see section 5.2 ) but may therefore be a worse predictor of dyslexia subtypes . Note also that the diagnoses of the clinicians are given according to the results of the entire screening tests and the post tests results , whereas the diagnoses of the models are based on only halves of the screeningtests . This is expected to lower the maximum accuracy for all models .
Table 1 : Accuracy of the models .
NB1
NB2
LDA
Without priors
0.731 ( 0.018 )
0.792 ( 0.006 )
0.720 ( 0.005 )
With Priors
—
0.801 ( 0.005 )
0.739 ( 0.002 )
AUC values for the three models , standard deviation values in parentheses replicating diagnoses of experts , and therefore can be used as an automated data driven tool for dyslexia diagnosis .
As expected , the LDA model reasonably correlates with expert diagnosis . Yet , it predicts the same diagnosis as experts in fewer cases than NB2 . In the following section we examine whether this is ought to the LDA model being an inaccurate model of dyslexia , or rather the opposite it may contribute to expert knowledge in capturing dyslexic phenomena . 5.2 The predictive power of the models
Which model of the proposed models ( NB1 , NB2 , LDA ) best captures the hidden error patterns in the data , thereby best characterizing dyslexic errors ? We answer this by comparing the predictive power of the three models . We evaluate the predictive power of the models using the prediction set ( figure 3 ) . The perplexity score is commonly used in assessing the predictive power of a language model ( eg , [ 1] ) . Here , it reflects the degree of ” surprise ” of the models to the errors made by the dyslexic person . Formally it is :
−
Ntest s=1
Ntest s=1 log(p( ¯ws , ¯es ) )
| ¯ws|
( 4 )
P erplexity = exp
Figure 4 : Accuracy results for the three graphical models – LDA , NB1 and NB2 , with ( dark bars ) and without ( light bars ) typical errors priors to NB2 and LDA .
Figure 4 shows the comparison between the accuracy of the models , table 1 summarizes the resulting AUC values . The highest accuracy is achieved with the NB2 model when it is provided with typical errors priors βwed . Next are the LDA when it is provided with typical errors priors and the NB1 Models . Results reflect that the NB2 model is best at and it is evaluated on the prediction set , where ¯ws are the observed target words and ¯es are the error types in the | ¯ws| represents the number of prediction set of person p . words in the prediction set ( | ¯ws| = 98 ) , and Ntest is the total number of test subjects ( Ntest = 219 ) .
We also compare the predictive power of NB2 to LDA when provided with priors about the typical errors for each dyslexia subtype . Figure 5 shows that the perplexity of these models is lower when provided with priors ( figure 5 : dark curve in panel A and dark bars in panel B ) . Incorporating expert knowledge into the model therefore increases its predictive power .
The prior value of the diagnosis distribution was set to be uniform with arbitrary value αdp = 005 The lower prior for the word error dyslexia distribution was set to βlow = 0.05 , whereas for the higher prior βhigh , different values were explored . βhigh is then chosen to be the value that achieves lowest perplexity βhigh = 0.7 ( figure 5 ) . The perplexity values for all three models are summarized in figure 5b .
These results show that the best prediction of reading errors is achieved by the LDA model when it is provided with typical errors priors ( figure 5B ) . This result is consistent for different typical error priors when compared to the
1924 Figure 5 : ( A ) Perplexity values for the prediction set during training of the LDA model , with ( dark curve ) and without ( light curve ) typical error priors . ( B ) A comparison between perplexity values calculated for the three models , with ( dark bars ) and without ( light bars ) typical error priors to the NB2 and LDA models .
NB2 model ( figure 6 ) . These results indicate that the LDA model best captures the hidden structure of reading errors , among all three models . 5.3 The Dyslexia Attention model of reading In addition to dyslexia , other factors may contribute to reading errors . For instance , people make reading mistakes due to momentarily lowered attention , which may be caused by an attention deficit or simply because they become tired as the diagnostic procedure progresses . These causes may interact with dyslexia . We hypothesized that we could ” clean out ” incorrect assignments of reading errors to dyslexia , rather than to an unattended state , by disentangling errors caused by dyslexia from errors caused by low attention . We further hypothesized that by disentangling the two , we could classify between participants which are diagnosed with attention deficit disorder in addition to dyslexia . We therefore designed a model that takes into account the two possible causes of reading errors , specific dyslexia types and a generic low attention state , incorporating the manner in which these change during the diagnostic procedure . Specifically , we added to the LDA based graphical model a binary random hidden variable marking the attentional state of the subject , attended or unattended , while reading each word . We set these attentional states as a hidden Markov chain , incorporating the dynamics of reading errors in time . We trained the model in a similar way to the LDA based model , but this time also inferring the hidden attention states and learning the transition probabilities between these states .
When evaluating this model , as in the above experiments , we did not find the Dyslexia Attention model to improve over the dyslexia state LDA model . We hypothesize that two reasons may explain the lack of improvement . First , adding the attention hidden variables requires a significantly larger number of parameters , which might be hard to well optimize with the current dataset . It is possible that training the model with an even larger corpus of data would improve the predictions . A second possible reason is that the attentional processes follow a different time scale than the one captured by the current experiments . The time spent in a state in an hidden Markov model follows an exponential distribution , and this may not be an adequate model for switching between attention states . We encourage and intend to further examine this line of research with more accumulating data .
6 . DISCUSSION
This study is a first demonstration of using probabilistic graphical models as a method for analyzing reading errors made by dyslexic people . The work has two main novel contributions : ( a ) it provides an automatic diagnosis tool of dyslexia , based on data of types of reading errors ; ( b ) it provides probabilistic tools which enable exploring dyslexic phenomena based on reading errors data . By using these tools , the hidden structure of reading errors data , such as the probability of an error type given a dyslexia and a targetword , may be extracted .
We tested three graphical models , a Latent Dirichlet Allocation ( LDA ) model and two Na¨ıve Bayes models , all differing by their complexity . The models were trained on a uniquely large data set of reading tests taken by dyslexic people .
Two experiments were conducted with the data to determine : ( a ) which model best captures the fine structure of the patterns of reading errors , and ( b ) which model best agrees with expert diagnoses .
1925 sues from different research fields , such as reading disorders in the case here .
Our vision is to deploy the algorithm of the NB2 model as a mobile application , providing an accessible , cheap and fast screening test , which allows people to obtain initial screening results , helping them decide whether further diagnosis is needed . This can be achieved by a mobile application which presents a series of words , asks the user to read them aloud , recognizes the spoken words , and uses the diagnosis algorithms described here to provide an initial diagnosis . The main challenge in achieving this is having the speech recognition component recognize utterances that are nonwords . Responses of dyslexic people may differ from existing words , whereas automatic speech recognizers ( ASRs ) are often trained to provide the most likely word elicited . For example , a subject may read an existing word such as cloud , as another existing word could , but also as a nonword such as clud . This kind of distinction is crucial for the diagnosis . This challenge can be addressed by training and assessing ASR tools on data sets of labeled nonwords .
To conclude , this study is a demonstration of the use of probabilistic graphical models as a method of analyzing reading errors data . The progress presented in this paper can also be deployed as a novel , efficient diagnostic tool for dyslexia , based on reading errors only . The Naive Bayes model tested in this research was compared to labels given by experts in diagnosis of dyslexia , and was found to be an accurate and reliable candidate for an automated tool for initial screening of dyslexia , by its subtypes .
APPENDIX A . DERIVATION OF THE UPDATE RULE In this section , we expand on the derivation of the update rule of the Gibbs sampler ( section 42 ) The derivation we use follows similar lines as the ones described in [ 15 ] , with the modification required for our dyslexia model ( figure 2C ) :
The joint distribution of the model is given by : p(¯es | ¯ds , ¯ws)p( ¯ws)p( ¯ds | Θs ) p(¯e , ¯w , ¯d , Θ , Φ ; α , β ) =
S S s=1 p(Φ | β)p(β ) p(Θs | αs)p(αs ) = p(Φ | β)p(β )
N p(ei,s | wi,s , di,s)p(di,s | Θs)p(Θs | αs ) s=1 i=1 p(wi,s)p(αs )
Given that p(Θs | αs ) and p(Φ | β ) are Dirichlet priors , and that p(wi,s ) are constant , rewriting the expression for the joint distribution , we get : p(¯e , ¯w , ¯d , Θ , Φ ; α , β ) ∝
S
W
E
D
φCwed+βwed−1 wed
θCds+αds−1 ds s=1 w=1 e=1 d=1
Figure 6 : Perplexity values calculated for the NB2 and LDA models for varying size of typical error prior value βhigh .
To answer the above questions , multiple models where trained on a large corpus including 313 subjects and 196 target words in each test . Note that the majority of responses on the reading tests are correct responses of the subject , therefore most of the information about the structure of the errors resides in a small proportion of the data ( < 20% ) . We expect accuracy of algorithms to improve as more data is accumulated .
The results show that the LDA model achieves best performance in terms of predicting reading errors , and it is therefore best in capturing the fine structure of the data . This result has both theoretical and practical implications . It may contribute to the understanding of dyslexic phenomena , and it may be used to improve designs of screening tests .
Another result of this study may serve as an application for diagnosis of dyslexia . NB2 was best in replicating diagnoses of experts ( AU C = 0801 ) This is yet another demonstration of Na¨ıve Bayes models being efficient classifiers . Note that in this study all models were trained on the screening tests only , whereas diagnoses of clinicians are usually based on the results of the post tests as well . The data driven models , and particularly NB2 , performed well in detecting the dyslexia subtype without leveraging the posttest information and thus are very effective .
Finally , the predictive power of the Na¨ıve Bayes and LDA models is improved when the models are provided with priors which are set according to knowledge of experts from the subtype approach . If the subtype approach were wrong , adding these priors would damage the predictive power of the model rather than improving it . The accuracy of these models is also improved when providing the models with such priors . Our work therefore provides support to the subtype approach to dyslexia , and exemplifies the ability of probabilistic graphical models to shed light on theoretical is
1926 Table 2 : List of error types and possible responsible dyslexia types . NS Normal State ; LPD Letter Position Dyslexia ; VD Visual Dyslexia ; VLD Vowel Letter Dyslexia ; AD Attentional Dyslexia ; ND Neglect Dyslexia ; Other Other types of dyslexia , such as phonological output buffer dyslexia or deep dyslexia , which are diagnosed based on additional post tests .
Error type
Correct response
Vowel substitution
Consonant substitution
Vowel migration Vowel omission Vowel addition
Consonant migration Consonant omission Consonant addition
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 Attentional migration between words 19
Visual error ( left side ) Function word error
Letter doubling , or double omission
Attentional vowel letter error
Migration of end letters
Attentional omission
Morphological error
Semantic error
Surface error
Integrating out θ and φ , we get : p(¯e , ¯w , ¯d ; α , β ) ∝
Dyslexia type
NS
LPD , VD
VD VD VD
VLD , LPD , VD
VLD , VD VLD , VD VLD , VD
Other Other
SD
ND , VD
Other
LPD , VD
AD
AD , VLD
AD VD dθdφ =
W
E
D w=1 e=1 d=1
θCds+αds−1 ds dθ s=1
S
D Γ(D s=1
φCwed+βwed−1
θCds+αds−1 wed
W w=1
D W d=1 ds
E E D Γ(E e=1 w=1 d=1
φCwed+βwed−1 wed dφ = d=1 Γ(Cds + αds ) d=1(Cds + αds ) ) e=1 Γ(Cwed + βwed ) e=1(Cwed + βwed ) )
The posterior distribution for the ith dyslexic state can now be expressed using the expression for the joint distribution : p(di = d | wi = w , ei = e , d−i , w−i , e−i ; α , β ) ∝ p(¯e , ¯w , ¯d ; α , β ) p(¯e−i , ¯w−i , ¯d−i ; α , β )
The numerator and denominator of the above fraction , differ only by the counts having the ith pair of word error in them , whereas all other terms cancel out . The posterior distribution thus results in the following update rule ( section 423 ) : p(di = d | wi = w , ei = e , d−i , w−i , e−i , α , β ) ∝
Cwed,−i + βwed e [ Cwed,−i + βwed ]
Cds,−i + αds d [ Cds,−i + αds ]
B . LIST OF ERROR TYPES
The list of all error types used to encode the data is shown in table 2 . Possible dyslexia types which may be responsible for each error are listed in the second column .
References [ 1 ] D . M . Blei , A . Y . Ng , and M . I . Jordan . “ Latent dirichlet allocation ” . In : the Journal of machine Learning research 3 ( 2003 ) , pp . 993–1022 .
[ 2 ] B . Boets et al . “ Intact but less accessible phonetic representations in adults with dyslexia ” . In : Science 342.6163 ( 2013 ) , pp . 1251–1254 .
[ 3 ] Y . M . Broom and Doctor A . E . “ Developmental phonological dyslexia : A case study of the efficacy of a remediation program ” . In : Cognitive Neuropsychology 12.7 ( 1995 ) , pp . 725–766 .
[ 4 ] A . Castles , T . Bates , and M . Coltheart . “ John Marshall and the developmental dyslexias ” . In : Aphasiology 20.9 ( 2006 ) , pp . 871–892 .
[ 5 ] A . Castles and M . Coltheart . “ Varieties of developmental dyslexia ” . In : Cognition 47.2 ( 1993 ) , pp . 149–180 .
[ 6 ] M . Coltheart and S . Kohnen . “ Acquired and developmental disorders of reading and spelling ” . In : The handbook of the neuropsychology of language . Ed . by M . Faust . 2012 , pp . 892–920 .
[ 7 ] M . Coltheart et al . “ Surface dyslexia ” . In : Quarterly
Journal of Experimental Psychology 35.3 ( 1983 ) , pp . 469– 495 .
[ 8 ] S . Dehaene . Reading in the brain : The new science of how we read . Penguin , 2009 .
[ 9 ] J . G . Elliott and E . L . Grigorenko . The dyslexia debate .
Cambridge University Press , 2014 .
[ 10 ] M . van Ermingen Marbach et al . “ Distinct neural signatures of cognitive subtypes of dyslexia with and without phonological deficits ” . In : NeuroImage : Clinical 2 ( 2013 ) , pp . 477–490 .
[ 11 ] N . Friedmann and M . Coltheart . “ Types of developmental dyslexia ” . In : Handbook of communication disorders : Theoretical , empirical , and applied linguistics perspectives . Ed . by A . Bar On and D . Ravid . Berlin , Boston : De Gruyter Mouton , in press .
[ 12 ] N . Friedmann and A . Gvion . “ Letter position dyslexia ” .
In : Cognitive Neuropsychology 18 ( 2001 ) , p . 673 .
[ 13 ] N . Friedmann and A . Gvion . TILTAN : Battery for the diagnosis of dyslexias in Hebrew . Tel Aviv : Tel Aviv University , 2003 .
[ 14 ] P . Georgiewa et al . “ Phonological processing in dyslexic children : a study combining functional imaging and event related potentials ” . In : Neuroscience letters 318.1 ( 2002 ) , pp . 5–8 .
[ 15 ] T . L . Griffiths and M . Steyvers . “ Finding scientific topics ” . In : Proceedings of the National Academy of Sciences . 5228 5235 : 101 ( suppl . 1 ) , 2004 .
[ 16 ] G . Jobard , F . Crivello , and N . Tzourio Mazoyer . “ Evaluation of the dual route theory of reading : a metanalysis of 35 neuroimaging studies ” . In : Neuroimage 20.2 ( 2003 ) , pp . 693–712 .
1927 [ 17 ] L . Khentov Kraus and N . Friedmann . “ Dyslexia in vowel
[ 34 ] S . E . Shaywitz and B . A . Shaywitz . “ Dyslexia ( specific letters ( DIVL ) ” . In : Language and Brain 10 ( 2011 ) , pp . 65–106 . reading disability ) ” . In : Biological psychiatry 57.11 ( 2005 ) , pp . 1301–1309 .
[ 35 ] M . J . Snowling . Dyslexia . Blackwell Publishing , 2000 .
[ 36 ] M . J . Snowling . “ Dyslexia as a Phonological Deficit : Evidence and Implication ” . In : Child and Adolescent Mental Health 3 ( 1998 ) , pp . 4–11 .
[ 37 ] J . Stein . “ The magnocellular theory of developmental dyslexia ” . In : Dyslexia 7 ( 2001 ) , pp . 12–36 .
[ 38 ] G . Vallar , C . Burani , and L . S . Arduino . “ Neglect dyslexia : a review of the neuropsychological literature ” . In : Experimental Brain Research 206.2 ( 2010 ) , pp . 219–235 .
[ 39 ] H . M . Wallach , D . Minmo , and A . McCallum . “ Rethinking LDA : Why priors matter ” . In : Neural Information Processing Systems ( 2009 ) , pp . 1973–1981 .
[ 18 ] P . Langley , W . Iba , and K . Thompson . “ An analysis of Bayesian classifiers.AAAI ” . In : 90 ( 1992 ) , pp . 223–228 .
[ 19 ] J . Levy et al . “ Testing for the dual route cascade reading model in the brain : an fMRI effective connectivity account of an efficient reading style ” . In : PLoS One 4.8 ( 2009 ) , p . 6675 .
[ 20 ] J . C . Marshall and F . Newcombe . “ Patterns of paralexia : A psycholinguistic approach ” . In : Journal of psycholinguistic research 2.3 ( 1973 ) , pp . 175–199 .
[ 21 ] T . Minka and J . Lafferty . “ Expectation propagation for the generative aspect model ” . In : In Proceedings of the Eighteenth conference on Uncertainty in artificial intelligence . 2002 , pp . 352–359 .
[ 22 ] J . Pedler . “ Computer correction of real word spelling errors in dyslexic text ” . PhD thesis . Birkbeck , University of London , 2007 .
[ 23 ] B . F . Pennington . “ From single to multiple deficit models of developmental disorders ” . In : Cognition 101.2 ( 2006 ) , pp . 385–413 .
[ 24 ] B . F . Pennington et al . “ Individual prediction of dyslexia by single versus multiple deficit models ” . In : Journal of abnormal psychology 121.1 ( 2012 ) , p . 212 .
[ 25 ] C . J . Price . “ A review and synthesis of the first 20years of PET and fMRI studies of heard speech , spoken language and reading ” . In : Neuroimage 62.2 ( 2012 ) , pp . 816–847 .
[ 26 ] F . Ramus . “ Neuroimaging sheds new light on the phonological deficit in dyslexia ” . In : Trends in cognitive sciences 18.6 ( 2014 ) , pp . 274–275 .
[ 27 ] F . Ramus and G . Szenkovits . “ What phonological deficit ? ”
In : The Quarterly Journal of Experimental Psychology 61.1 ( 2008 ) , pp . 129–141 .
[ 28 ] F . Ramus et al . “ Theories of developmental dyslexia : insights from a multiple case sudty of dyslexic adults ” . In : Brain 126.4 ( 2003 ) , pp . 841–865 .
[ 29 ] L . Rello , R . Baeza Yates , and J . Llisterri DysList . “ An Annotated Resource of Dyslexic Errors ” . In : In Proceedings LREC ( 2014 ) , pp . 26–31 .
[ 30 ] S . Robidoux and S . Pritchard . “ Hierarchical Clustering Analysis of Reading Aloud Data : A New Technique for Evaluating the Performance of Computational Models ” . In : Frontiers in Psychology 5.267 ( 2014 ) .
[ 31 ] M . Rosen Zvi et al . “ The author topic model for authors and documents ” . In : In Proceedings of the 20th conference on Uncertainty in artificial intelligence . 2004 , pp . 487–494 .
[ 32 ] E . M . Saffran and H . B . Coslett . “ Implicit vs . letterby letter reading in pure alexia : A tale of two systems ” . In : Cognitive Neuropsychology 15.1 2 ( 1998 ) , pp . 141– 165 .
[ 33 ] T . Shallice and E . K . Warrington . “ The possible role of selective attention in acquired dyslexia ” . In : Neuropsychologia 15 ( 1977 ) , pp . 31–41 .
1928
