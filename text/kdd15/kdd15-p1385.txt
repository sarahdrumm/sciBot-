Structural Graphical Lasso for Learning Mouse Brain
Connectivity
Sen Yang
IDST at Alibaba Group , USA senyangsy@alibaba inccom
Peter Wonka
King Abdullah University of Science and Technology ,
Saudi Arabia peterwonka@kaustedusa
Qian Sun qsun21@asu.edu Ian Davidson
Shuiwang Ji sji@csoduedu Jieping Ye
Arizona State University , USA
Old Dominion University , USA
University of California , USA davidson@csucdavisedu
University of Michigan , USA jpye@umich.edu
ABSTRACT Investigations into brain connectivity aim to recover networks of brain regions connected by anatomical tracts or by functional associations . The inference of brain networks has recently attracted much interest due to the increasing availability of high resolution brain imaging data . Sparse inverse covariance estimation with lasso and group lasso penalty has been demonstrated to be a powerful approach to discover brain networks . Motivated by the hierarchical structure of the brain networks , we consider the problem of estimating a graphical model with tree structural regularization in this paper . The regularization encourages the graphical model to exhibit a brain like structure . Specifically , in this hierarchical structure , hundreds of thousands of voxels serve as the leaf nodes of the tree . A node in the intermediate layer represents a region formed by voxels in the subtree rooted at that node . The whole brain is considered as the root of the tree . We propose to apply the tree structural regularized graphical model to estimate the mouse brain network . However , the dimensionality of whole brain data , usually on the order of hundreds of thousands , poses significant computational challenges . Efficient algorithms that are capable of estimating networks from high dimensional data are highly desired . To address the computational challenge , we develop a screening rule which can quickly identify many zero blocks in the estimated graphical model , thereby dramatically reducing the computational cost of solving the proposed model . It is based on a novel insight on the relationship between screening and the so called proximal operator that we first establish in this paper . We perform experiments on both synthetic data and real data from the Allen Developing Mouse Brain Atlas ; results demonstrate the effectiveness and efficiency of the proposed approach . c⃝ 2015 ACM . ISBN 978 1 4503 3664 2/15/08 $1500
DOI : http://dxdoiorg/101145/27832582783391
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications| Data Mining
General Terms Algorithms
Keywords Graphical lasso , tree structural regularization , screening , secondorder method , proximal operator , brain networks
1 .
INTRODUCTION
The rich behavior of numerous complex systems is rooted in the underlying networks governing element interactions . For example , cells are best described as networks of molecules connected by chemical reactions ; brains are commonly represented as networks comprising a set of neurons interconnected by their communication pathways ; our society is characterized by a network of individuals connected by social relationships . In reality , it is usually the behaviors of individual elements , rather than their interactions , that are directly measurable . This gives rise to the central problem of how to identify system interaction structures by reasoning backwards from the observed behaviors of the individual elements , a process known as network modeling .
Undirected graphical models explore relationships among a set of random variables through their joint distribution . The estimation of undirected graphical models has applications in many domains , such as biology and medicine [ 10 , 12 , 33 , 27 ] . One instance is the analysis of gene expression data . As shown in many biological studies , genes tend to work in groups based on their biological functions , and there exist some regulatory relationships between genes [ 6 ] . Such biological knowledge can be represented as a graph , where nodes are the genes , and edges describe the regulatory relationships . Graphical models provide a useful tool for modeling these relationships , and can be used to explore gene interactions . One of the most widely used graphical models is the Gaussian graphical model ( GGM ) . In the GGM , the variables are assumed to follow a Gaussian distribution [ 4 , 35 ] . Then the problem of learning a graphical model is equivalent to estimating the inverse of the covariance matrix ( pre
1385 cision matrix ) , since the nonzero off diagonal elements of the precision matrix represent edges in the graph [ 4 , 35 ] .
The main challenge of estimating a sparse precision matrix for problems with a large number of nodes ( variables ) is its intensive computation . Witten et al . [ 30 ] and Mazumder and Hastie [ 21 ] independently derived a necessary and sufficient condition for the solution of a single graphical lasso to be block diagonal ( subject to some rearrangement of variables ) . This can be used as a simple screening test to identify the associated blocks , and the original problem can thus be decomposed into a group of smaller sized but independent problems corresponding to these blocks . When the number of blocks is large , it can achieve massive computational speedup . However , these formulations only assume that the graph is sparse . In many applications , domain structural knowledge exists and can potentially be exploited to improve the learning performance ; in this case , structural regularization can be used to improve the estimation of graphical model . However , due to the complexity of structural regularization , it is challenging to derive screening rules for general structural regularization .
To attack the above central challenge , we derive a screening rule for structural Graphical Lasso in this paper . Specifically , we show that the derivation of the screening rule is critically dependent on the so called \proximal operator" associated with the structural regularization , eg , Lasso , group Lasso , or tree Lasso penalty . In recent years , tremendous efforts have been devoted to the efficient computation of the proximal operators , which plays a central role in structured sparse learning [ 3 , 34 ] . In many cases , the proximal operators , such as Lasso penalty , group lasso penalty , and tree group penalty , have closed form solutions , thereby leading to very efficient computation [ 17 ] . One of our major contributions in this work is to establish a bridge between the computation of the proximal operator associated with a structural regularization and the derivation of a screening rule for structural Graphical Lasso . To the best of our knowledge , our work represents the first attempt to construct screening rules for the Structural Graphical Lasso based on general structural regularization .
The major contributions of this work are summarized as follows : ffl We propose a structural Graphical Lasso formulation based on a general structural regularization imposed on the nodes , eg , a tree structure . The proposed formulation estimates a sparse precision matrix which is encouraged to satisfy certain structures . ffl We derive a screening rule to identify a block diagonal structure of the resulting network . The original large size precision matrix can thus be decomposed into a set of smaller sized blocks . Such decomposition can potentially lead to massive computational speedup . One of our key technical contributions of this work is the establishment of the intrinsic relationship between screening rules and proximal operators . ffl The proposed screening is safe in the sense the screening does not affect the final solution . In addition , the proposed screening only relies on the data and the parameters , thus it can be combined with any existing algorithms to reduce the computational cost . ffl We evaluate the proposed screening rule and the proposed model using both synthetic data and real data from the Allen Developing Mouse Brain Atlas . Results demonstrate the effectiveness and efficiency of the proposed methods .
Notation : In this paper , ℜ stands for the set of all real numbers , ℜn denotes the n dimensional Euclidean space , and the set of all m . n matrices with real entries is denoted by ℜmn All matrices are presented in bold format . The space of symmetric matrices is denoted by S n . If X 2 S n is positive semidefinite ( resp . definite ) , we write X ⪰ 0 ( resp . X ≻ 0 ) . The cone of positive semidefinite matrices in S n is denoted by S n + . Given matrices X and Y in ℜm.n , the standard inner product is defined by ⟨X ; Y⟩ := tr(XYT ) , where tr( ) denotes the trace of a matrix . The determinant of a real symmetric matrix X is denoted by det(X ) . Given a matrix X 2 ℜn.n , diag(X ) denotes the vector formed by the diagonal of X , ie , diag(X)i = Xii for i = 1 ; : : : ; n .
Organization : The rest of this paper is organized as follows . We introduce the structural graphical lasso formulation as well as the screening rule in Section 2 . The experimental results for both synthetic data and real data are shown in Section 3 . Related work is discussed in Section 4 . We conclude the paper in Section 5 .
2 . STRUCTURAL GRAPHICAL LASSO
Suppose we are given a data set X 2 ℜn.p with n samples , and p features ( or variables ) . The n samples are independently and identically distributed with a p variate Gaussian distribution with zero mean and positive definite covariance matrix . Even if all features are correlated , there are usually many conditional independences among these fea,1 is tures . In other words , a sparse precision matrix . = of interest in most cases . This Gaussian graphical model ( GMM ) is also referred to as Gaussian Markov Random Field ( GMRF ) . The negative log likelihood for the data X takes the form of
L( . ) := , log det( . ) + tr(S. ) ;
( 1 )
= S n XT X . where S is the sample covariance matrix given by S = 1 Minimizing ( 1 ) leads to the maximum likelihood estimation ,1 . However , there are some issues with fi ( MLE ) . MLE . In particular , it fails in the high dimensional setting fi ( n < p ) , where MLE . does not exist due to the singularity of S . To handle this issue , regularization is usually employed , resulting in penalized maximum likelihood estimation . For applications with prior domain knowledge , different regularization terms can be employed to encourage the estimated model to satisfy the desired structural property . For example , a common assumption is that the graphical model is sparse . In this case , the ℓ1 regularization has been employed to encourage sparsity [ 8 ] . In this paper , we consider the general structural graphical lasso , which integrates the structural regularization as follows :
, log det( . ) + tr(S . ) + ϕ(. ) ;
( 2 ) min .≻0 where ϕ( . ) is the convex structural regularization . We refer to problem ( 2 ) as structural graphical lasso ( SGL ) . Examples include but are not limited to ffl Sparsity : ϕ( . ) = ∥.∥1
1386 on the data and the parameters , thus it can be combined with any existing algorithms to reduce the computational cost . We discuss two special cases in Section 24 2.1 Tree Guided Graphical Lasso Formulation In this subsection , we present a hierarchical graphical model framework where the features exhibit a hierarchical structure . A motivating example is the estimation of brain networks . The brain is a multi level system , and the brain network has a native hierarchical structure as shown in Figure 2 : hundreds of thousands of voxels form regions , and regions form systems .
We employ the tree structural group regularization to encourage the estimated graph to have a hierarchical structure . Specifically , in this hierarchical structure , hundreds of thousands of voxels serve as the leaf nodes of the tree . A node in the intermediate layer represents a region formed by voxels in the subtree rooted at that node . The whole brain is considered as the root of the tree . Mathematically , we solve the following formulation :
, log det( . ) + tr(S . ) + ϕ( . ) where
ϕ( . ) = min .≻0
0@∑
∑ j i̸=i′ ii′∥.Gj wj i ;Gj i′
∥F + wj ii
∥.Gj i ;Gj i ;of f
( 3 )
1A ;
∥F i ;Gj i ; Gj i′ , and wj ii′ = wj i′i is a positive weight for .Gj
Gj i is the i th group at depth j ( the groups of a tree are defined in Definition 1 below ; see Figure 3 for an illustration ) , .Gj i′ denotes the submatrix of . consisting of features in Gj i′ . i ;Gj .:;:;of f represents the matrix .: ; : excluding the diagonal elements . We do not penalize the diagonal elements of . since . is required to be positive definite . For simplicity of notation , we use .j ii= to repi′i)T , thus we resent .Gj require wj i′i . The regularization ϕ( . ) encourages the estimated precision matrix to be tree structural ( see Figure 4 for an example ) . We formally define a tree structure as follows : ii′ to represent .Gj i ;of f . It is clear that .j i ;Gj ii′ = ( .j i ;Gj ii′ = wj i′ , and .j
Definition 1 . [ 18 ] For an index tree T of depth U , we g contain all the nodes corresponding 1 = f1 ; : : : ; Kg and ni 1 ; i = let Tu = fG1 ; : : : ; Gni to depth u , where n0 = 1 ; G0 1 ; : : : ; U . The nodes satisfy the following conditions : ffl The nodes from the same depth level have non overlapping j \ Gu k = ∅;8u = 1 ; : : : ; U ; j ̸= k ; 1 be the parent node of a non root node Gu j , indices , ie , Gu j ; k ̸= ni ; ffl Let Gu,1 j0 then Gu j Gu,1 j0 .
2.2 Algorithm
We propose to employ the second order method to solve the tree guided graphical lasso problem in ( 3 ) as it has been shown to be quite efficient for solving the Graphical Lasso formulation with ℓ1 regularization [ 11 ] . Let f ( . ) be the smooth function in ( 3 ) such that f ( . ) = , log det( . ) + tr(S. ) :
Figure 1 : Illustration of two precision matrices ( bottom ) whose nodes are in different order correspond to the same graph with two connected components ( top ) . The white color in the precision matrices represents a zero entry .
Figure 2 : Illustration of the brain2 . Yellow : frontal lobe ; green : parietal lobe ; red : temporal lobe ; blue : occipital lobe . Number represents brain regions within lobes .
∑ ffl Group sparsity : ϕ( . ) =
∥.Gi;Gj
∥F . i;j
The penalized log likelihood function with a convex regularizer , ie , problem ( 2 ) , is strictly convex , however , the minimum of problem ( 2 ) may not be achievable . This is usually dependent on the property of the sample covariance matrix S . For example , diag(S ) > 0 is a sufficient condition for problem ( 2 ) to have a unique solution [ 32 ] when the ℓ1 regularization exists . For simplicity of presentation , we assume throughout the paper that the minimum of problem ( 2 ) can be achieved , ie , problem ( 2 ) has a unique solution . The remainder of this section is organized as follows . We introduce a Tree Guided Graphical Lasso formulation in Section 21 In Section 2.2 , we present a second order method to efficiently solve the proposed model . In addition , we derive a sufficient condition for estimating many zero blocks in the graph in Section 23 Based on this property , we propose a simple screening rule which significantly reduces the complexity of the optimization problem , thus improving the computational efficiency . The proposed screening only relies
2 http://wwwumichedu/~cogneuro/jpg/Brodmannhtml c1c2fi1387 1 Initialization : .0 = ( Diag(S ) ) 2 while Not Converged do 3
,1 ;
Algorithm 1 : Tree Guided Graphical Lasso ( TGL ) Input : S;fGj Output : . g;fwj ii′g i
Compute the Newton direction D by solving ( 5 ) and ( 7 ) . Choose .t+1 by performing the Armijo backtracking line search along .t + fiD .
4
Figure 3 : A sample index tree . Root : G0 f1 ; 2 ; 3 ; 4 ; 5 ; 6 ; 7 ; 8g . Depth 1 : G1 1 = f1 ; 2g , G1 1 = f1g , G2 3 = f7 ; 8g . Depth 2 : G2 f3 ; 4 ; 5 ; 6g , G1 6 = f8g . 3 = f3 ; 4 ; 5g , G2 5 = f7g , G2 G2
4 = f6g , G2
1 = 2 = 2 = f2g ,
Illustration of a hierarchical graphical Figure 4 : model . The features exhibit a hierarchical structure specified by tree groups fGj g : The blue blocks represent the nonzero blocks in the precision matrix . i
( 3 ) can be rewritten as min .≻0 f ( . ) + ϕ(. ) :
( 4 )
In the second order method , we solve a \quadratic" model of ( 3 ) at each iteration defined by min .
1 2 tr(WtDWtD ) + tr((S , Wt)D ) + ϕ(. ) ;
( 5 ) and D = . , .t , and t represents the
,1 t where Wt = . t th Newton iteration .
The subproblem ( 5 ) can be solved by non monotone spectral projected gradient ( NSPG ) method [ 31 ] . When applied to ( 5 ) , NSPG needs to solve the proximal subproblem in the form of min .
1 2
∥ . , Gr∥2
F + ffϕ(. ) ;
( 6 ) where
Gr = .r , ff(S , 2Wt + Wt.rWt )
5 end 6 return .t+1 ; and r denotes the r th inner iteration in NSPG . Denote
R = .r , .r,1 and ff = tr(RWtRWt)=∥R∥2 F ; then ff is given by ff = max(ffmin ; min(1= ff ; ffmax) ) ; where [ ffmin ; ffmax ] is a predefined safeguard . fi After obtaining the optimal solution of ( 5 ) .
, the New ton direction D can be computed as fi , .t :
D = .
( 7 )
Once the Newton direction is obtained , we need to find an appropriate step size fi 2 ( 0 ; 1 ] to ensure a sufficient reduction in the objective function in ( 4 ) . Because of the positive definite constraint in ( 4 ) , we need to ensure the next iterate .t+1 = .t + fiD to be positive definite . It is not hard to show that such step size satisfying the above requirements always exits [ 11 ] . Thus , we can adopt the Amrmijo ’s backtracking line search rule to select a step length fi 2 ( 0 ; 1 ] . We use the Cholesky decomposition to check the positive definiteness of .t+1 = .t + fiD . In addition , the log det(.t+1 ) ,1 and . t+1 can be efficiently computed as a byproduct of the Cholesky decomposition of t+1 The algorithm is summarized in Algorithm 1 .
Under the assumption that the subproblem ( 5 ) is solved exactly , the convergence rate of the second order method is locally quadratic when the exact Hessian is used [ 11 , 14 , 28 ] . If the subproblem ( 5 ) is solved inexactly , the convergence rate of the second method is locally superlinear by adopting an adaptive stopping criterion in NSPG [ 14 ] . Due to the use of Cholesky decomposition and the need of computing tr(WtDWtD ) in ( 5 ) , the complexity of Algorithm 1 is O(p3 ) . 2.3 Screening
Due to the existence of the log determination , it is computationally expensive to solve the penalized log likelihood model ( 3 ) by applying Algorithm 1 directly . The screening strategy has commonly been employed to reduce the size of optimization problems so that a massive computational gain can be achieved . In this section , we derive a sufficient condition for the solution of SGL to be block diagonal ( subject to some rearrangement of features ; see Figure 1 for illustration ) , thus significantly reducing the complexity of the problem .
( cid:1833)(cid:1833)(cid:1833)(cid:2870)(cid:1833)(cid:2871)(cid:1833)(cid:2870)(cid:1833)(cid:2870)(cid:2870)(cid:1833)(cid:2871)(cid:2870)(cid:1833)(cid:2872)(cid:2870)(cid:1833)(cid:2873)(cid:2870)(cid:1833)(cid:2874)(cid:2870)G13G23G33G43G53G73G12G22G32G42G11G21G63G331388 Let C1 ; : : : ; CL be a partition of the p features into L non overlapping sets such that
′
′
:
Cl′ ; l ̸= l
Cl \ Cl′ = ∅ ; 8l ̸= l
( subject to some rearrangement of features ) with L known
. Without loss of generality , we assume that a l = 1 ; : : : ; L
We say that the solution b . of SGL ( 2 ) is block diagonal blocks Cl ; l = 1 ; : : : ; L if b.ij = b.ji = 0 for i 2 Cl ; j 2 block diagonal solution b . with L blocks Cl ; 1CCA ; takes the form ofb . = where b.l is the jClj . jClj symmetric submatrix of b . con b.L
0BB@ b.1
. . .
( 8 ) sisting of features in Cl .
Since the elements in off diagonal blocks are zero , the original optimization problem can thus be reduced to a much smaller problem restricted to the elements in the diagonal blocks , resulting in a great computational gain . Our main result is summarized in the following theorem :
Theorem 1 . Suppose Ud+1 = ,S , where d is the depth i at the depth j , of the tree structure . For different groups Gj define U j recursively as follows :
0
∥Uj+1 ii′ ∥F ,wj ii′ ∥Uj+1 ii′ ∥F
∥Uj+1 ∥Uj+1 ii′ ∥F wj ii′ ii′ ∥F > wj ii′
Uj+1 ii′
( 9 )
8< :
Uj ii′ =
A sufficient condition for the solution of SGL to be block diagonal with blocks C1 ; : : : ; CL is that Uj at some layer j has the same block diagonal structure such that ′ ii′ = 0;8Gj Uj and there is no group Gj not exist x1 2 Cl , and x2 2 Cl′ , such that fx1 ; x2g Gj i .
Proof . By the first order optimality condition , b . is the i across two blocks , that is , there do i′ Cl′ ; l ̸= l
Cl ; Gj i optimal solution of problem ( 2 ) if and only if it satisfies
,(b . )
,1 + S + @ϕ(b . ) = 0 :
( 10 )
Suppose that Uj at layer j has the block diagonal struc ture C1 ; : : : ; CL such that
Cl ; Gj ii′ = 0;8Gj Uj and there is no group Gj i across two blocks . According to [ 18 ] , it is not hard to show that Uj is the solution of the following problem : i′ Cl′ ; l ̸= l
′ i where
~ϕ( . ) = min
1 2
X
0@∑ d∑ k=j i̸=i′
∥X + S∥2
F + ~ϕ(X ) ; ii′∥.k wk ii′∥F + wk ii∥.k ii=∥F
( 11 )
1A ;
According to Theorem 1 in [ 18 ] , we have Uk Gj 0 ; : : : ; j , 1 if Uj ii′ = 0 . Thus , we have i ;Gj i′ = 0 ; k = i ;Gj
SGj i′ + @ ~ϕ(0)Gj i′ = 0 ; k = 0 ; : : : ; j , 1 if Uj i′ = 0 : ii′ = 0 , it can be shown , for k = 0 ; : : : ; j , 1 since the minimum
As Uk Gj that 0 2 ϕ(.)k of ∥ ∥F is achieved at 0 . Then , we have i ;Gj i ;Gj i ;Gj
Gj i min
.l;l=1;:::;L of the following problem : i
Gj i ;Gj i ;Gj
SGj i′ = 0 ; i′ + @ϕ(0)Gj i ;Gj since 0 2 ϕ(.)k , for k = 0 ; : : : ; j , 1 . Therefore , the first order optimality condition holds for the elements in off diagonal blocks .
Next we show how to construct a b . which satisfies the first optimality condition . Let b . be a block diagonal matrix with let the elements in the diagonal blocks of b . be the solution blocks Cl ; l = 1 : : : ; L . It is clear that the optimality condition of ( 2 ) for off diagonal elements are satisfied . We can
L∑ ( , log det(.l ) + tr(Sl.l ) ) + ϕ( . ) s:t : .i;j = 0;8i 2 Cl ; j 2 Cl′ ; l ̸= l ′ i′ = 0 , for k = 0 ; : : : ; j,1 if Uj l=1
:
Since Uk Gj optimality condition ( 10 ) holds for b . , thus b . is the optimal ii′ = 0 , the first i ;Gj solution of ( 2 ) . This completes the proof of the theorem .
Theorem 1 can be used as a screening rule to determine the elements in the identified off diagonal blocks to be zero in advance . Assume that there are L blocks of the same size identified by the screening rule , p2(1 , 1 L ) elements do not need to be computed as the optimal values for these elements are determined as 0 by the screening . Recall that the complexity of the proposed second order method is O(p3 ) due to Cholesky decomposition and computation of tr(WtDWtD ) . The complexity of solving the proximal operator ( 11 ) for the tree group structural regularization is O(p2 ) [ 18 ] . By applying the screening rule , the complexity of Cholesky decomposition and computation of tr(WtDWtD ) are reduced to O(p3=L2 ) , and the complexity of solving ( 11 ) is reduced to O(p2=L ) . Therefore , the complexity of the second order method with screening is O(p3=L2 ) since L p . When L is large , application of the screening rule can achieve a great computational gain . 2.4 Discussions
We want to emphasize that Theorem 1 provides a screening rule for a large family of graphical model problems . Several examples in the literature can be reformulated into problem ( 2 ) with specific constraints . In the following , we provide several examples as follows .
ℓ1 regularization : When the ℓ1 regularization is used , SGL degenerates to standard graphical lasso [ 4 , 8 ] given by :
, log det( . ) + tr(S . ) + ∥.∥1 : min .≻0 and U0 is the solution of
The proximal operator in ( 11 ) can be written as min
X
1 2
∥X + S∥2
F + ϕ(X ) : min
X
1 2
∥X + S∥2
F + ∥X∥1 :
( 12 )
1389 According to Theorem 1 , the sufficient condition for the optimal solution ( ie , the solution of graphical lasso based on the ℓ1 regularization ) to have a block structure C1 ; : : : ; CL is that the optimal solution bX of ( 12 ) has the same block di agonal structure , ie , C1 ; : : : ; CL . It is not hard to see that the following first order optimality condition is satisfied
, Sij ; 8i 2 Cl ; j 2 Cl′ ; l ̸= l
′
; which is exactly the same as the screening condition for graphical lasso proposed in [ 21 , 30 ] : jSijj ; 8i 2 Cl ; j 2 Cl′ ; l ̸= l
′
:
Thus , the screening rule in [ 21 , 30 ] is a special case of the proposed rule .
Group regularization : The graphical lasso with group regularization has been studied in [ 13 ] . The formulation of group graphical lasso is given by
, log det( . ) + tr(S . ) + min .≻0
∥.Gi;Gj
∥F ;
∑ i;j where .Gi;Gj is a submatrix of . , and Gi is the i th group of features . Note that [ Gi = f1 ; : : : ; pg and different groups do not overlap . In [ 13 ] , Kolar et al . proposed a sufficient condition for the solution of group graphical lasso to be block diagonal , which is given by
∥SGi;Gj
∥F ; 8 Gi Cl ; Gj Cl′ ; l ̸= l
′
:
( 13 )
It is clear that condition ( 13 ) is the first order optimality condition for the solution of ( 11 ) to have the block diagonal solution C1 ; : : : ; CL . Thus , the screening rule in [ 21 , 30 ] is also a special case of the proposed rule .
Figure 5 : The ontology hierarchy of the Allen Developing Mouse Brain Atlas from level 0 to level 5 . Each brain region is colored using the color code of the Allen Developing Mouse Brain Reference Atlas .
3 . EXPERIMENTAL EVALUATION
In this section , we conduct experiments to demonstrate the effectiveness and efficiency of the proposed screening rule and the proposed tree guided graphical lasso ( TGL ) . We used both synthetic and real mouse brain gene expression data to evaluate our methods . The experiments are performed on a PC with quad core Intel i7 3.4GHz CPU with 16GB of RAM . The TGL formulation is implemented in MATLAB , while the sub routine for solving the subproblem ( 6 ) is implemented in C . We compare TGL with standard graphical lasso ( GLasso ) in the experiments . 3.1 Synthetic Data
We first evaluate our method using synthetic data . We follow [ 32 ] in generating the synthetic covariance matrix . Specifically , we first generate the ground truth precision matrix . with random block nonzero patterns . Each nonzero block has a random sparse structure . Given the precision matrix . , we sample from a Gaussian distribution to compute the sample covariance matrix . The weights for treestructural group regularization take the form of
√ wj ii′ = fl= j.j ii′j ; where fl is a given positive parameter and j.j ii′j is the number of elements in .j ii′ . To make a fair comparison between different methods , we control the regularization parameters of TGL and GLasso to ensure the numbers of edges obtained from both estimations to be the same .
Figure 6 shows the comparison between TGL and GLasso in terms of edge detection . The first column of Figure 6 shows the nonzero patterns ( ie , edges ) of two ground truth precision matrices . In both cases , the same index tree is used , which is given by
8>>>><>>>> : i = fig ; i = 1 ; : : : ; 100 ; G3 i = f20i + 1 : 20(i + 1)g ; i = 0 ; : : : ; 4 ; G2 1 = f1 : 60g ; G1 2 = f61 : 100g : G1
( 14 )
We can observe from Figure 6 that the nonzero patterns of the precision matrices estimated by TGL are more similar to the ground truth than GLasso . These results demonstrate that TGL outperforms GLasso in terms of detecting true edges in the precision matrices .
We conduct experiments to demonstrate the effectiveness of the proposed screening rule . We terminate NSPG using the following stopping criterion : r , .(k ) r,1 ∥.(k ) ∥1 r,1
1e 6 :
∥.(k )
∥1
Additionally , TGL is terminated when the relative error of the objective value is smaller than 1e 5 . The used index tree is given by8>>>><>>>> : i = fig ; i = 1 ; : : : ; p ; G3 i = f ip G2 i = f ip G1
+ 1 :
+ 1 :
2L
2L
( i + 1)p
( i + 1)p
L
L g ; i = 0 ; : : : ; 2L , 1 ; g ; i = 0 ; : : : ; L , 1 :
( 15 )
NP(0)F(1)SP(2)RSP(3)POTel(4)POR(5)POA(5)PPHy(4)PPHyA(5)PPHyB(5)PPHyF(5)CSP(3)Tel(4)TelR(5)TelA(5)PedHy(4)PHyA(5)PHyB(5)PHyF(5)D(2)p3(3)PTh(4)p3R(5)p3A(5)PThTg(4)p3B(5)p3F(5)p2(3)Th(4)p2R(5)p2A(5)ThTg(4)p2B(5)p2F(5)p1(3)Pt(4)p1R(5)p1A(5)PtTg(4)p1B(5)p1F(5)M(1)m1(3)MTt(4)m1R(5)m1A(5)MTg(4)m1B(5)m1F(5)m2(3)PIsTt(4)m2R(5)m2A(5)PIsTg(4)m2B(5)m2F(5)H(1)PPH(2)is(3)isR(5)isA(5)isB(5)isF(5)r1(3)r1R(5)r1A(5)r1B(5)r1F(5)r2(3)r2R(5)r2A(5)r2B(5)r2F(5)PH(2)r3(3)r3R(5)r3A(5)r3B(5)r3F(5)r4(3)r4R(5)r4A(5)r4B(5)r4F(5)PMH(2)r5(3)r5R(5)r5A(5)r5B(5)r5F(5)r6(3)r6R(5)r6A(5)r6B(5)r6F(5)MH(2)r7(3)r7R(5)r7A(5)r7B(5)r7F(5)r8(3)r8R(5)r8A(5)r8B(5)r8F(5)r9(3)r9R(5)r9A(5)r9B(5)r9F(5)r10(3)r10R(5)r10A(5)r10B(5)r10F(5)r11(3)r11R(5)r11A(5)r11B(5)r11F(5)SpC(1)my1(3)my1R(5)my1A(5)my1B(5)my1F(5)1390 Table 1 : Timing comparison of the proposed TGL with and without screening in terms of average computational time ( seconds ) . TGL S denotes TGL with screening . The computational time of TGL S is the summation of screening and TGLs . p stands for the dimension , and L is the number of blocks . ∥.∥0 represents the total number of nonzero entries in the ground truth precision matrix . , and ∥ . fi∥0 is the number of nonzeros in the solution .
Data setting ∥.∥0 11442 23694 11142 23308
L
5
10 p
1000 2000 1000 2000 fi∥0 ∥ . 11914 23854 9782 23862
Computational time ( seconds )
TGL S screening 0.0109 0.0395 0.0105 0.0366
TGLs 0.1715 1.0839 0.2286 0.4257
TGL
2.8219 12.2679
6.481
19.1117 where L is the number of blocks . The time comparison results are given in Table 1 . We can observe that the computational time of screening is negligible compared with that of solving the TGL . Since the complexity of identifying the connected components is O(∥ . fi∥0 ) , the computational time of screening is almost linear with respect to ∥ . fi∥0 . Results in Table 1 demonstrate that the screening rule can achieve very significant computational gain . The larger the L is , the higher the speedup is . These results demonstrate the potential of our method for identifying structured networks for large scale data . 3.2 Allen Developing Mouse Brain Atlas Data We also evaluate our methods using the Allen Developing Mouse Brain Atlas data . The Allen Developing Mouse Brain Atlas contains spatiotemporal in situ hybridization ( ISH ) gene expression data across multiple stages of mouse brain development [ 26 , 1 ] . The primary data consist of 3 D , cellular resolution ISH expression patterns of approximately 2000 genes in sagittal plane across four embryonic ( E11.5 , E13.5 , E15.5 , and E18.5 ) and three early postnatal ages ( P4 , P14 , and P28 ) . The ISH image series are passed through an informatics data processing pipeline by which they are converted to grid level expression summaries in the same coordinate space [ 2 ] .
After the ISH image series are mapped to the reference space , a gridding module is applied to divide the 3 D reference space into regular grids , creating a low resolution 3 D summary of the gene expression . The resolution of the data grids varies with age . For each grid voxel , expression density is the number of expressing pixels divided by the number of image pixels in the voxel ; expression intensity is the averaged inverted ISH gray scale value at expressing pixels within the span of the grid voxel ; expression energy is defined as the product of expression intensity with expression density . Our analysis in this work is also based on the grid level expression energy . In this work , we use data from the first three developmental ages with 7796 , 9963 , and 8258 structural voxels , respectively . We use a data set of 1724 genes .
We apply the proposed TGL method to the voxel level gene expression data to demonstrate the effectiveness of TGL and the proposed screening rule . In the Allen Developing Mouse Brain Atlas , the brain regions are organized into a tree structural hierarchy as shown in Figure 5 . This provides an ideal setting for evaluating our proposed tree structural graphical Lasso formulation . We use such hierarchical structure as the input prior knowledge to our algorithm TGL . We compare TGL with the standard GLasso on this data . Fig ure 7 shows the comparison between the precision matrices estimated by TGL and GLasso . We can observe that , although the data inherently exhibits certain tree structures , the results obtained by GLasso do not recover these structures clearly . In contrast , our proposed TGL method successfully recovers the hierarchical structures . Nevertheless , GLasso recovers some overall structures that are largely consistent with the hierarchy with the corruption of some noises . To demonstrate the power of the proposed screening , we report the running time of the TGL with and without screening . We use the data from the first stage for our evaluation . We stop the computation of the algorithm after we obtain a solution with precision 1e 6 . The computational time of TGL without screening is 57189.6 seconds . With the screening , the total computational time of TGL S including the time for screening is reduced to 2781.5 seconds , demonstrating the superiority of the proposed screening rule .
4 . RELATED WORK
Brain connectivity describes how the brain regions are connected , thereby providing information pathways in the brain . Graphical modeling is a statistical tool to capture the connectivity between multiple random variables . Thus graphical models are natural tools for brain connectivity analysis . However , the dimensionality is usually very large for brain data , and this prohibits the direct application of many existing methods . Therefore , large scale brain network estimation is considered as a big data problem and has raised several challenges and opportunities [ 20 ] .
The task of estimating the whole brain connectivity is important , but also very challenging . There are two major types of connectivity analysis ; namely functional connectivity and effective connectivity . There are a few simple approaches for estimating functional connectivity , ie , these based on pair wise correlations , clustering , and independent component analysis ( ICA ) [ 9 ] . Effective connectivity aims to find directional relationships between brain regions . Popular approaches for effective connectivity include dynamic causal modeling , structural equation models , and Granger causality . These tools are complex in computation and modeling , thus they are usually applicable for a small number ( eg < 100 ) of preselected voxels or regions . Recently , voxel correlations are used to provide more accurate selection of voxels for a certain region of the brain . However , the results may be sensitive to the selection of regions , and the network inference can be biased if the influence from other omitted
1391 Figure 6 : Comparison between TGL and GLasso in terms of edge detection . Left : the ground truth precision matrix ; middle : the precision matrix estimated by GLasso ; right : the precision matrix estimated by TGL . regions is large [ 20 ] . To date , several challenges remain in inferring large scale direct connectivity .
Sparse Gaussian graphical models ( sGGM ) [ 4 , 8 , 21 , 22 , 35 ] are proposed to estimate large scale brain connectivity . This type of models has a solid probabilistic foundation for distinguishing direct connections from indirect connections . Suppose we have a multivariate variable X following a pvariate normal distribution N ( ; ) , and we are given n iid observations . sGGM represents the relationships between the p variables by a network of p nodes , where each node represents a variable and there are connections between nodes . Formally , inference of the connections between the p nodes is reduced to estimating a sparse inverse covariance ,1 , where a nonzero off diagonal entry in . indi . = cates that the corresponding row and column variables are connected . Similarly , a zero entry indicates the absence of connection . The sGGM approach performs well on a simulation study using a small number of regions [ 20 ] .
In recent years , considerable research efforts have been devoted to estimating the precision matrix and the corresponding sGGM [ 11 , 12 , 15 , 16 , 19 , 23 , 24 ] . Numerous methods have been developed for solving this model . For example , Banerjee et al . [ 4 ] and Friedman et al . [ 8 ] proposed block coordinate ascent methods for solving the dual problem . The latter method [ 8 ] is widely referred to as Graphical lasso ( GLasso ) . Yuan [ 36 ] and Scheinberg et al . [ 25 ] applied the alternating direction method of multipliers ( ADMM ) [ 5 ] to this problem . Wang et al . [ 29 ] , Hsieh et al . [ 11 ] , Olsen et al . [ 24 ] , and Dinh et al . [ 7 ] applied the Newton method for solving this model .
The brain network system is complex and structured . For example , brain regions are usually organized into a hierarchy in which a large region includes multiple sub regions . We propose a tree structural graphical model to represent the multi level brain network in this paper . Specifically , voxels are represented as the leaf nodes of the tree . The nodes in the intermediate layer represents the regions . This way , the entire brain is considered as the root of the tree . Our model is different from the model in [ 20 ] in multiple ways , and our proposed model is more general . Specifically , the nodes in [ 20 ] can only connect with each other via the hub nodes , while the nodes can connect in arbitrary ways in our model . In [ 20 ] an alternating update algorithm is proposed to solve the model , and much computational efforts have been devoted to computing the determinant of This prohibits the direct application of graphical models from large scale brain datasets . The contributions of this paper lie in two folds : ( 1 ) we propose a tree structural graphical model to incorporate the multi level brain structure ; and ( 2 ) we develop a sufficient screening rule to dramatically reduce the computational cost for computing the determinant of . in general structural graphical model .
0204060801000102030405060708090100nz = 510Ground Truth0204060801000102030405060708090100nz = 334GLasso0204060801000102030405060708090100nz = 334TGL0204060801000102030405060708090100nz = 536Ground Truth0204060801000102030405060708090100nz = 476GLasso0204060801000102030405060708090100nz = 476TGL1392 Figure 7 : Comparison between TGL and GLasso in terms of edge detection on Allen developing mouse brain atlas data . Upper : the precision matrix estimated by GLasso ; bottom : the precision matrix estimated by TGL . Left to right : Mouse brain networks in the 1st , 2nd , and 3rd development stages . The red and green grids visualize the tree structural groups in two layers .
5 . CONCLUSION AND FUTURE WORK
In this work , we propose a hierarchical graphical model framework known as the tree guided graphical lasso . In order to scale the proposed formulation to large scale network inference , we develop a screening rule to dramatically speedup the computation . Specifically , we employ the secondorder method to solve the proposed formulation . In addition , we derive a sufficient condition for the TGL solution to be block diagonal . Based on this condition , a simple screening rule has been developed to scale our methods to large scale problems . We apply the proposed methods to infer the large scale mouse brain connectivity . Numerical experiments on synthetic and real data demonstrate the efficiency and effectiveness of the proposed method and the proposed screening rule .
This work focuses on the inference of mouse brain networks . On the other hand , human brain networks are more complex and involve more structures . We plan to apply the proposed methods to the human brain networks in the future . This work represents the first attempt to investigate screening rules for structural graphical Lasso , and many theoretical problems in this direction remain unexplored . We plan to derive a necessary and sufficient condition for screening the TGL solution , thereby enhancing the theoretical guarantee of our algorithm . In addition , we plan to explore the convergence properties of the second order method using the inexact Newton direction .
6 . ACKNOWLEDGMENTS
This work was supported in part by research grants from NIH ( R01 LM010730 ) and NSF ( IIS 0953662 , DBI 1147134 , DBI 1350258 , III 1421057 , and III 1421100 ) .
7 . REFERENCES [ 1 ] Allen Developing Mouse Brain Atlas . http://developingmousebrain maporg 2013 .
[ 2 ] Allen Institute for Brain Science . Technical White Paper : Informatics Data Processing for the Allen Developing Mouse Brain Atlas , 2012 .
[ 3 ] Francis Bach , Rodolphe Jenatton , Julien Mairal ,
Guillaume Obozinski , et al . Structured sparsity through convex optimization . Statistical Science , 27(4):450{468 , 2012 .
020004000600080000100020003000400050006000700080009000nz = 660932GLasso1393 [ 4 ] O . Banerjee , L . El Ghaoui , and A . d’Aspremont .
[ 20 ] X . Luo . A hierarchical graphical model for big inverse
Model selection through sparse maximum likelihood estimation for multivariate gaussian or binary data . The Journal of Machine Learning Research , 9:485{516 , 2008 .
[ 5 ] S . Boyd , N . Parikh , E . Chu , B . Peleato , and
J . Eckstein . Distributed optimization and statistical learning via the alternating direction method of multipliers . Foundations and Trends R⃝ in Machine Learning , 3(1):1{122 , 2011 . covariance estimation with an application to fmri . arXiv preprint arXiv:1403.4698 , 2014 .
[ 21 ] R . Mazumder and T . Hastie . Exact covariance thresholding into connected components for large scale graphical lasso . The Journal of Machine Learning Research , 13:781{794 , 2012 .
[ 22 ] R . Mazumder and T . Hastie . The graphical lasso : New insights and alternatives . Electronic Journal of Statistics , 6:2125{2149 , 2012 .
[ 6 ] HY Chuang , E . Lee , YT Liu , D . Lee , and T . Ideker .
[ 23 ] N . Meinshausen and P . B(cid:127)uhlmann . High dimensional
Network based classification of breast cancer metastasis . Molecular systems biology , 3(1 ) , 2007 . [ 7 ] Q . Dinh , A . Kyrillidis , and V . Cevher . A proximal newton framework for composite minimization : Graph learning without cholesky decompositions and matrix inversions . arXiv preprint arXiv:1301.1459 , 2013 . [ 8 ] J . Friedman , T . Hastie , and R . Tibshirani . Sparse inverse covariance estimation with the graphical lasso . Biostatistics , 9(3):432{441 , 2008 .
[ 9 ] Karl J Friston . Functional and effective connectivity : a review . Brain connectivity , 1(1):13{36 , 2011 .
[ 10 ] J . Guo , E . Levina , G . Michailidis , and J . Zhu . Joint estimation of multiple graphical models . Biometrika , 98(1):1{15 , 2011 .
[ 11 ] CJ Hsieh , M . A . Sustik , I . S . Dhillon , and
P . Ravikumar . Sparse inverse covariance matrix estimation using quadratic approximation . Advances in Neural Information Processing Systems ( NIPS ) , 24 , 2011 . graphs and variable selection with the lasso . The Annals of Statistics , 34(3):1436{1462 , 2006 .
[ 24 ] P . A . Olsen , F . Oztoprak , J . Nocedal , and S . J . Rennie . Newton like methods for sparse inverse covariance estimation . In Advances in Neural Information Processing Systems ( NIPS ) , 2012 .
[ 25 ] K . Scheinberg , S . Ma , and D . Goldfarb . Sparse inverse covariance selection via alternating linearization methods . In Advances in Neural Information Processing Systems ( NIPS ) , 2010 .
[ 26 ] Carol L Thompson , Lydia Ng , Vilas Menon , Salvador Martinez , Chang Kyu Lee , Katie Glattfelder , Susan M Sunkin , Alex Henry , Christopher Lau , Chinh Dang , et al . A high resolution spatiotemporal atlas of gene expression of the developing mouse brain . Neuron , 83(2):309{323 , 2014 .
[ 27 ] R . Tibshirani . Regression shrinkage and selection via the lasso . Journal of the Royal Statistical Society . Series B ( Methodological ) , pages 267{288 , 1996 .
[ 12 ] S . Huang , J . Li , L . Sun , J . Liu , T . Wu , K . Chen ,
[ 28 ] P . Tseng and S . Yun . A coordinate gradient descent
A . Fleisher , E . Reiman , and J . Ye . Learning brain connectivity of alzheimer ’s disease from neuroimaging data . In Advances in Neural Information Processing Systems ( NIPS ) , pages 808{816 , 2009 .
[ 13 ] M . Kolar , H . Liu , and E . Xing . Markov network estimation from multi attribute data . In Proceedings of The 30th International Conference on Machine Learning , pages 73{81 , 2013 .
[ 14 ] J . D . Lee , Y . Sun , and M . A . Saunders . Proximal newton type methods for minimizing convex objective functions in composite form . arXiv preprint arXiv:1206.1623 , 2012 .
[ 15 ] L . Li and KC Toh . An inexact interior point method for l 1 regularized sparse covariance selection . Mathematical Programming Computation , 2(3):291{315 , 2010 .
[ 16 ] H . Liu , K . Roeder , and L . Wasserman . Stability approach to regularization selection ( StARS ) for high dimensional graphical models . In Advances in Neural Information Processing Systems ( NIPS ) , 2011 .
[ 17 ] J . Liu , S . Ji , and J . Ye . SLEP : Sparse Learning with Efficient Projections . Arizona State University , 2009 .
[ 18 ] J . Liu and J . Ye . Moreau yosida regularization for grouped tree structure learning . In Advances in Neural Information Processing Systems ( NIPS ) , pages 1459{1467 , 2010 .
[ 19 ] Z . Lu . Smooth optimization approach for sparse covariance selection . SIAM Journal on Optimization , 19(4):1807{1827 , 2009 . method for nonsmooth separable minimization . Mathematical Programming , 117(1):387{423 , 2009 .
[ 29 ] C . Wang , D . Sun , and KC Toh . Solving log determinant optimization problems by a newton cg primal proximal point algorithm . SIAM Journal on Optimization , 20(6):2994{3013 , 2010 .
[ 30 ] D . M . Witten , J . H . Friedman , and N . Simon . New insights and faster computations for the graphical lasso . Journal of Computational and Graphical Statistics , 20(4):892{900 , 2011 .
[ 31 ] S . J . Wright , R . D . Nowak , and MAT Figueiredo . Sparse reconstruction by separable approximation . Signal Processing , IEEE Transactions on , 57(7):2479{2493 , 2009 .
[ 32 ] S . Yang , Z . Lu , X . Shen , P . Wonka , and J . Ye . Fused multiple graphical lasso . arXiv preprint arXiv:1209.2139v2 , 2013 .
[ 33 ] S . Yang , L . Yuan , YC Lai , X . Shen , P . Wonka , and
J . Ye . Feature grouping and selection over an undirected graph . In KDD , pages 922{930 . ACM , 2012 .
[ 34 ] J . Ye and J . Liu . Sparse methods for biomedical data .
ACM SIGKDD Explorations Newsletter , 14(1):4{15 , 2012 .
[ 35 ] M . Yuan and Y . Lin . Model selection and estimation in the gaussian graphical model . Biometrika , 94(1):19{35 , 2007 .
[ 36 ] X . Yuan . Alternating direction method for covariance selection models . Journal of Scientific Computing , 51(2):261{273 , 2012 .
1394
