Learning with Similarity Functions on Graphs using
Matchings of Geometric Embeddings
Fredrik D . Johansson
Chalmers University of Technology
Göteborg , Sweden frejohk@chalmers.se
Devdatt Dubhashi
Chalmers University of Technology
Göteborg , Sweden dubhashi@chalmers.se
ABSTRACT We develop and apply the Balcan–Blum–Srebro ( BBS ) theory of classification via similarity functions ( which are not necessarily kernels ) to the problem of graph classification . First we place the BBS theory into the unifying framework of optimal transport theory . This also opens the way to exploit coupling methods for establishing properties required of a good similarity function as per their definition . Next , we use the approach to the problem of graph classification via geometric embeddings such as the Laplacian , pseudo– inverse Laplacian and the Lov´asz orthogonal labellings . We consider the similarity function given by optimal and near– optimal matchings with respect to Euclidean distance of the corresponding embeddings of the graphs in high dimensions . We use optimal couplings to rigorously establish that this yields a “ good ” similarity measure in the BBS sense for two well known families of graphs . Further , we show that the similarity yields better classification accuracy in practice , on these families , than matchings of other well known graph embeddings . Finally we perform an extensive empirical evaluation on benchmark data sets where we show that classifying graphs using matchings of geometric embeddings outperforms the previous state–of–the–art methods .
Categories and Subject Descriptors G22 [ Discrete Mathematics ] : Graph Theory—Network problems ; I51 [ Pattern Recognition ] : Models—Geometric
General Terms Algorithms
Keywords Matchings , Similarity functions , Graphs , Geometric embeddings , Classification
1 .
INTRODUCTION
Geometric embeddings offer a powerful and flexible approach for learning on graph structured data by capturing the structure of the graph and exploiting the geometry of Euclidean spaces to devise fast algorithms [ 34 , 23 ] .
In particular , graph embeddings offer a very natural approach to defining similarities on graphs , namely , by computing an optimal matching between the corresponding point sets in Euclidean space . Fr¨ohlich et al . [ 13 ] used the idea of optimal assignments to propose a graph kernel , although not with geometric embeddings . Unfortunately , Vert [ 39 ] showed that optimal assignment does not always yield a positive semidefinite kernel , which deals a serious blow to using the theory of kernel methods , in particular kernel support vector machines . While it is possible to use the matching function anyway for useful classification , as Vert remarks , it would be more assuring and satisfying if this was justified by a corresponding theory . We address this issue here .
We take as starting point , the theory of learning using similarity functions developed by Balcan , Blum & Srebro ( BBS ) [ 2 ] . They observe that kernels do not necessarily correspond to natural notions of similarity , while on the other hand , natural notions of similarity do not correspond to positive semidefinite kernels and it might require significant additional work to coerce a similarity into a kernel , possibly also degrading the quality . This is in fact the case with the optimal assignment approach mentioned above . To address this issue , BBS propose a natural general notion of a “ good ” similarity function and prove that it is sufficient for learning in classification problems . However , there are three major lacunae left over in their work ;
• It may not be easy to establish rigorously that a given similarity measure satisfies the properties required of a “ good ” similarity measure in their definition and neither [ 2 ] nor any subsequent papers give any general techniques for how to establish the property for a similarity function of interest .
• Thus , no specific examples of “ good ” similarity functions with rigorous theoretical guarantees , are given in BBS [ 2 ] or subsequent papers [ 43 , 24 , 25 , 3 ] to illustrate their general theory in concrete applications .
• Thirdly , BBS [ 2 ] did not perform an experimental evaluation of the landmarks method . There has been no evaluation of similarity functions tailored to structured data , such as graphs , in subsequent work either .
467 Contributions .
We take the theory of BBS [ 2 ] forward on these fronts . • First , we place the definition and theory of BBS [ 2 ] into the unifying context of optimal transport , a well developed classical theory in analysis [ 40 ] . By connecting to optimal transport , the definition of a good similarity function becomes natural and obviates the need for somewhat ad hoc adjustments with weighting factors needed in [ 2 ] , see Example 31 Besides providing a natural unifying setting , the connection also opens up the opportunity to draw upon the theory and methods of optimal transport , including optimal couplings [ 38 , 27 ] to prove rigorous results , in particular to verify the properties of a “ good ” similarity measure – which , as noted above , can often be quite difficult to show rigorously . The connection to optimal transport also opens the way to further theoretical development of the subject .
• We use this connection to instantiate the theory for the problem of classifying graphs . In Section 3 , we use coupling methods to give the first rigorous results establishing that some natural matching measures are “ good ” similarity functions for classification of two wellknown families of graphs .
• We give the first extensive empirical evaluation of the proposed methods in several graph classification tasks . We evaluate several similarity functions , defined by matchings of seven different geometric embeddings of graphs , see Section 24 The first tasks are synthetic experiments on well known families of random graphs , confirming in fine detail the theoretical results of Section 3 . Second , we classify a set of four benchmark graph datasets . We compare the accuracy of our method to that of state of the art graph kernels , and show that our proposed method achieves the best results on all benchmarks , outperforming existing methods by a good margin .
In addition , there is also a computational issue that is not addressed in all previous works using matching as similarity functions ; computing the optimal matching is a computationally demanding task , requiring an involved algorithm with running time O(mn + n2 log n ) where n is the number of vertices andm the number of edges ( thus on a dense graph , it is O(n3 ) . Thus the approach will not scale to large graphs .
• To address the computational issue , we note that it may not be necessary to compute the optimal matching , but approximate near optimal matchings may be sufficient . This opens up the opportunity to take advantage of recent dramatic advances in algorithms for approximate and near optimal matchings with nearly linear running times [ 9 , 1 ] , some of which are also very easy to implement in practice , Thus by leveraging these approximate matching methods , we can get classification algorithms that scale to large graphs .
) of an unseen graph G . with support from the theory of learning with similarity functions , and motivate using maximum weight matchings of geometric embeddings of graphs for graph classification . 2.1 Classification with graph kernels Given a set of graphs Gt = {G1 , , GN} ⊆ G with discrete class labels .(Gi ) , graph classification is the task of predicting the label ( G . Over the last decade , graph kernels [ 41 ] have become state ofthe art tools for this task , with applications in areas including medicine [ 12 ] , chemo informatics [ 36 ] and information retrieval [ 21 ] . Graph kernels provide a means for general kernel methods , such as support vector machines or kernel principal component analysis [ 32 ] , to be applied to graphs . Formally , a graph kernel is a function k : G×G → R such that there exist a mapping φ(G ) to a Hilbert space H , with the )ffH for any pair of graphs property k(G , G . G , G . ∈ G . Equivalently , the gram matrix K with elements kij = k(Gi , Gj ) for i , j ∈ N must be positive semidefinite . Many existing graph kernels , such as the shortest path kernel [ 4 ] , the graphlet kernel [ 37 ] or the random walk kernel [ 15 ] , share the same structure : they count features of substructures in the graph . Weisfeiler Lehman ( WL ) graph kernels [ 36 ] , based on the isomorphism test with the same name , have seen great success in learning with node attributed graphs . The WL framework defines a sequence of label sets ( L1 , . . . , Lh ) , Li : V → R on the nodes , such that the label Li+1(v ) is determined by Li(v ) and the labellings Li(u ) of neighbor nodesu : ( u , v ) ∈ E . In applications , all of the labellings in the sequence are used to train the classifier . As such , the WL framework provides a natural means of enriching the expressivity of similarity functions on graphs , including graph kernels .
) = 'φ(G ) , φ(G .
While kernel methods have been successfully used in learning with graphs , the positive definiteness condition on the kernel function is sometimes too restrictive . We expand on this below . 2.2 Learning with similarity functions
Balcan , Blum & Srebro [ 2 ] remarked that many natural similarity functions are not kernels . Instead of forcing practitioners to massage these functions into positive definite versions , they propose a theory of learning with similarity functions . Specifically , they define “ good ” similarity functions to be functions that on average deem points of the same class to be more similar than points of different classes . Let x , x . ∈ X be points drawn from a distribution according to a learning problem P and .(x ) ∈ {±1} be the binary label of x .
Definition 2.1
( Balcan , Blum & Srebro [ 2] ) . K is a strongly ( , γ) good similarity function for a learning problem P if there exists a bounded weighting function w over X such that at least 1 − probability mass of examples x satisfy :
Ex.∼P [ w(x . ≥ Ex.∼P [ w(x .
)K(x , x . )K(x , x .
) | .(x ) = ( x ) | .(x ) ( = ( x
) ] ) ] + γ .
( 1 )
2 . GRAPH CLASSIFICATION
We establish the setting of this paper by introducing the graph classification problem and current state of the art methods for solving it – graph kernels . We challenge this paradigm
( , γ) good similarity functions are not kernels in general . However , Balcan , Blum & Srebro [ 2 ] show that indefinite similarity functions can be used with standard learning methods through a two stage algorithm . First , a subset of data points , S , called landmarks , are sampled . These landmarks
468 :→ R
|S| are used to create a similarity map φS in which every data point is represented by its similarity to each of the landmarks . Then , standard methods , such as SVM:s , can be used to classify points in the new space , defined by φS . Specifically , Balcan , Blum & Srebro [ 2 ] gave the following result on learning with such a representation .
Theorem 21 ( Learning with similarity functions Balcan , Blum & Srebro [ 2 ] ) Let K be an ( , γ) good similarity function for a learning problem P . For any δ > 0 , let S = {˜x1 , . . . , ˜xd} be a sample of size d = 8 log(1/δ)/γ2 drawn from P . Consider the the mapping φS , defined . With probability at least 1 − δ over the by φS random sample S , the induced distribution φS has a separator of error at most + δ at margin at least γ/2 .
( P ) in R
:→ R i ( x ) =
K(x,˜xi)√ d d d
Woznica , Kalousis & Hilario [ 44 ] defined graph kernels based on set distances , that are not kernels themselves . Instead , they used an idea similar to landmarks , called a representation set . Indeed , many similarity ( or dissimilarity ) functions are indefinite . We turn now to one of the most natural similarity functions for comparing structured data – maximum weight matchings . 2.3 Matchings and kernels
Graph matching is one of the most well–studied problems in combinatorial optimization . We show below how it is a natural way of comparing structured data . The input of the graph matching problem is a weighted graph G = ( V , E , w ) where n = |V | and m = |E| are the number of vertices and edges , and w is a edge weight function , w : E → R . An unweighted graph is one for which w(e ) = 1 for all e ∈ E . The output , a matching , is a set of vertex–disjoint edges . We use mwm to denote the problem of finding a maximum weight matching , as well as the matching itself . The weight of a matching , denoted w(mwm ) , is the sum of its edge weights . The mwm problem on bipartite graphs G = ( V , V . , E , w ) is often called the assignment problem . The weight of a bipartite matching can be interpreted as the fit , or similarity , between the two node sets V and V . . We note that a bipartite graph has a natural representation as a rectangular |V | × |V .| matrix where wij = w(e ) for e = ( i , j ) : i ∈ V , j ∈ V . . We let mwm(W ) denote the maximum weight matching of the complete bipartite graph determined by the weight matrix W . Consider objects x , y that decompose like x = ( x1 , . . . , xn ) , y = ( y1 , . . . , yk ) , such that xi , yj ∈ X , and a similarity function on the parts , k1 : X × X −→ R . Let Sn be the set of permutations of n elements . Fr¨ohlich et al [ 13 ] introduced a kernel on such objects with the Optimal assignment kernel , k1(xi , yp(i ) ) if|y| ≥ |x| k1(xp(i ) , yi ) otherwise
.
( 2 ) kA(x , y ) =
⎧⎪⎪⎪⎪⎨ ⎪⎪⎪⎪⎩
|x| max p∈S|x| max p∈S|y| i=1
|y| i=1
It is clear that the optimal assignment kernel corresponds to the maximum weight matching in the complete bipartite graph with edge weights specified by k1(·,· ) . Specifically , let W be the |x| × |y| matrix with wij = k1(xi , yj ) . Then if |y| ≥ |x| , kA = wij .
( i,j)∈mwm(W )
Unfortunately , Vert [ 39 ] showed that the optimal assignment kernel is not always positive definite . This means that kA does not enjoy the theoretical aspects associated with kernel methods in general . The optimal assignment kernel is a perfect motivating example for the framework of good similarity functions introduced previously ; it is a very natural similarity function , but not positive definite . We proceed to introduce geometric embeddings of graphs , as a way of representing graphs as compositions , to use with the optimal assignment kernel . 2.4 Geometric embeddings of graphs A geometric embedding of a graph G = ( V , E ) with n = |V | , represents each node i ∈ V , by a point ui in a geometric . We denote such an embedding U = {u1 , . . . ,u n} . space R We consider a number of different geometric embeddings of graphs to use in similarity functions .
D
First , we note that any positive semidefinite kernel on the nodes of a graph has an embedding implicitly associated with it . For example , the graph Laplacian , L has a decomposition L = U L UL for some matrix UL , as L is positive semi definite . The same is true for the ( pseudo )inverse Laplacian , L+ = ( LL ) [ 18 ] . The embeddings associated with L and L+ can be computed using Cholesky decomposition . The Laplacian also has an explicitly defined decomposition known as the incidence matrix , M , such that L = MM . For a graph G = ( V , E ) , M is an |E| × |V | matrix such that for all e = ( i , j ) ∈ E and v ∈ V
−1LT
⎧⎨ ⎩ mev =
1 −1 0 if v = i if v = j otherwise
.
( 3 )
The embedding associated with Lov´asz number , ϑ(G ) [ 28 ] , has rich theoretical properties and has been used successfully in both approximation algorithms for graph theoretic problems [ 16 ] and graph classification [ 23 ] . It is an orthonormal embedding , in that the vectors of any pair of disconnected nodes are orthogonal , and all vectors of the embedding have norm 1 . A related orthonormal embedding is the Uls = A/ρ + I , Luz Schrijver labelling , Uls defined by Uls for ρ ≥ −λmin(A ) [ 29 ] . It has been used to approximate Lov´asz number [ 22 ] .
Last , we consider the set of eigenvectors of the adjacency matrix and the Laplacian matrix respectively . We denote the set of eigenvectors of a matrix A by Eλ(A ) . While not strictly an embedding of the nodes of a graph , Eλ is a set of n points in R and can be used in the optimal assignment kernel , in place of geometric embeddings .
D
Next , we survey algorithms for approximate matching , for use in computing the optimal assignment kernel in general , and for geometric embeddings in particular . 2.5 Fast Approximate Matching
Computing a maximum cardinality matching in a graph √ ( both bipartite and general graphs ) with n vertices and m n ) . For weighted bipartite graphs , edges takes time O(m an optimal matching with n vertices and m edges can be computed , in O(mn+n2 log n ) time , using the classical Hungarian algorithm .
Recently there have been impressive advances in faster methods for computing approximate or near optimal matchings . For c < 1 , the c–mwm algorithm is an algorithm that delivers a matching whose weight is at least c times the weight of the optimal matching . Building on earlier work , Pettie & Sanders [ 31 ] gave an algorithm for a ( 2/3− )–mwm
469 algorithm running in O(m −1 ) time , for any >0 . More recently , Duan & Pettie [ 9 ] gave a ( 1 − )–mwm algorithm running in time ( m −1 log −1 ) .
If the graph is given by a geometric embedding , as in our setting , the geometry of Euclidean space can be exploited to give even faster algorithms . Building on a rich body of work about approximate nearest neighbours in Euclidean spaces , Agarwal & Sharathkumar [ 1 ] very recently showed that for a constant δ > 0 , an O(1/δlog 1/δ ) approximate matching can be computed in O(n1+δ log n ) time .
3 . GOOD SIMILARITY FUNCTIONS AND
OPTIMAL TRANSPORT
In the previous section , we proposed the use of matchings of geometric embeddings as natural similarity functions on graphs . To support this idea , we introduce the topic of optimal transport , connecting it to the theory of good similarity functions [ 2 ] , and state results on the classification margin . Further , we give two results characterizing the goodness of the optimal assignment kernel as a similarity function for classifying graphs .
Optimal transport is a classical subject in analysis [ 40 ] . The abstract setting is a metric space ( Ω , d ) . The transportation cost distance between probability distributions P , Q on Ω with respect tod is defined as d(P , Q ) := min π(X,Y )
Eπ[d(X , Y ) ] ,
( 4 ) where the minimum is over all couplings of P and Q ie over joint distributions π whose marginals are P and Q respectively .
Given a similarity function K , it is often convenient to deal with the associated dissimilarity function ˆK : for example , if the original similarity function K has range [ −1 , +1 ] , a dissimilarity function with range [ 0 , 1 ] is given by ˆK(x , y ) := ( 1 − K(x , y))/2 . As a dissimilarity function we also have ˆK(x , x ) = 0 for all x ∈ Ω . The definition of ˆK extends from points to distributions via coupling , see ( 4 ) . In this case , note that ˆK(P , P ) = 0 for all distributions by considering the trivial coupling that places all mass on points ( x , x ) .
The following definition is motivated by the definition in
Balcan , Blum & Srebro [ 2 ] :
Definition 31 A similarity function K has margin γ with respect to a distribution P , if the transportation cost ˆK(P i , P j is the distribution conditioned on the class with label i .
) ≥ γ for i ( = j , where P i
This definition is somewhat different in detail , and more general than the one in Balcan , Blum & Srebro [ 2 ] , in that it does not need a separate weighting function w(x ) .
Example 31 Consider the example in Figure 1 . For the distribution given there , and K being cosine similarity and the corresponding distance is ˆK = ( 1 − K(x , y))/2 , we have ˆK(P + , P − ) = 5/8 . Thus , K is a good similarity function even without using the weighting w , proposed in [ 2 ] , to eliminate the “ outliers ” . When the two distributions have the measure concentration property [ 6 , 26 , 10 ] , the two definitions are essentially equivalent . For example , with the weighting function used in the same example , the two distributions become concentrated and then ˆK(P + , P −
) = 1/4 .
Figure 1 : Example from [ 2 ] . Positives are split equally among upper left and upper right . Negatives are all in the lower right . For α = 30 , half of the positive examples have higher dot product with negatives , than the other positives .
◦
The main theorem of Balcan , Blum & Srebro [ 2 ] translates in this setting to the following .
Theorem 31 Let K be a similarity function with margin γ for a learning problem P with the masure concentration property . For any δ > 0 , let S = {x1,··· , xd} be a sample of /γ2 drawn independently at random from P . size d = 8 log 1 δ Consider the mapping φS d)i∈[d ] . With probability at least 1− δ,the induced separator has margin at least γ/2 .
: x → ( K(x , xi)/
√
Consider the space of all n vertex graphs and the dissimilarity function given by the minimum cost matching between their corresponding Laplacian embeddings where the cost on an edge is .1–Euclidean distance between the points . We show that this corresponds to a “ good ” similarity function for two pairs of distributions .
Proposition 31 ( Random Graphs with different densities ) Consider the space of all n vertex graphs and the two distributions being the Erd˝os R´enyi random graph distributions G(n , p ) and G(n , p . ) corresponding to different densities p < p . . The minimum cost matching has margin Ω(n3(p . − p) ) . Proof . ( Sketch ) In this case , it is easy to see that the minimal coupling is concentrated on points ( G , G . ) generated as follows : first generate G according to G(n , p ) , then generate G . by taking G and independently adding an edge with probability p . − p between two vertices not adjacent in G . It can be shown that .1–Euclidean distance between ) two corresponding points in the Laplacian embedding of the ( 1 − p)(p . − p ) and the ) graphs is concentrated around ( 1 − p)(p . − p ) . total matching distance is thus n
( ( n 2 n 2
We note that the margin grows linearly with the difference in density . This is confirmed experimentally in Section 51 Planted clique graphs are semi random graphs which contain a fully connected subgraph . Specifically , in G(n , p , k ) graphs , each edge is included with probability p apart from a random subgraph of k nodes that is fully connected .
Proposition 32 ( Random Graphs Versus Planted Clique Graphs ) Consider the space of all n vertex graphs and the two distributions being the Erd˝os R´enyi random graph distributions G(n , p ) and planted clique graphs G(n , p , k ) . Then the minimum cost matching has margin Ω(nk(1 − p) ) .
470 with n and n . nodes respectively
Algorithm 1 Matching similarity , kM ( G , G . 1 : Input : Graphs G = ( V , E ) , G .
= ( V . , E .
)
) wij = ffui − u
2 : Compute embeddings U ( G ) and U ( G . ) 3 : Let W be an n × n . matrix such that jff2 ui ∈ U ( G ) and u j ∈ U ( G . . . 4 : 5 : Compute kM ( G , G . ) = w(mwm(W ) ) , see Section 2.3 6 : Output : , kM ( G , G .
)
)
Figure 2 : Overview of algorithm 1 .
Proof . ( Sketch ) In this case , it is easy to see that the minimal coupling is concentrated on points ( G , G . ) generated as follows : first generate G according to G(n , p ) , then generate G . by planting a clique of size k on a set of k vertices chosen arbitrarily from G . It can be shown that the . 1– Euclidean distance between two corresponding points in the Laplacian embedding of the graphs is concentrated around k(1−p ) and the total matching distance is thus nk(1−p ) .
The optimal transport framework introduced here is a general technique that can be applied to any family of graphs , which we have illustrated here with specific families of random graphs . Of course the specific properties of the graph families will determine the optimal couplings , for instance , and hence the separation bounds .
4 . CLASSIFICATION ALGORITHM AND RUNNING TIME ANALYSIS
We define our algorithm for graph classification based on the landmarks framework of Balcan , Blum & Srebro [ 2 ] and a similarity function based on the maximum weight matching of geometric embeddings . Algorithm 1 , illustrated in figure 2 , implements the matching similarity functions and Algorithm 2 implements the landmarks framework .
The time complexity of our method can be broken down as follows . Consider a set of graphs G = {G1 , . . . , GN} . Natu rally , the time required for computing the geometric embeddings U ( G ) depends on the choice of embedding U , see Section 24 For the inverse Laplacian and Luz Schrijver embeddings , the complexity is dominated by the Cholesky decomposition . These , and the eigenvector embeddings Eλ(A ) and Eλ(L ) all have complexity O(n3 ) . The Lov´asz ϑ embedding has complexity ˜O(n5 ) [ 7 ] . The stand out among the embeddings is the Laplacian for which there is an explicit embedding in the incidence matrix M with complexity O(m ) . The running time of Algorithm 1 depends on the matching algorithm and the choice of embedding . The matching part can be performed in O(nn . 1 /γ2 ) time using the algorithm of Duan & Pettie [ 9 ] or O((n + n . /γ2 ) using the geometric matching algorithm of Agarwal & Sharathkumar [ 1 ] . Algorithm 2 is made up of N d invocations of Algorithm 1 , the time of sampling landmarks and the time of training the classifier . An SVM classifier can be trained for d landmarks , using the Pegasos algorithm to find an accurate solution with probability greater than 1 − δ , in ˜O( d δλ ) time [ 33 ] . Using the incidence matrix embedding , the matching can be done in almost linear time in m and n . We note in passing that Algorithm 1 does not implement an “ alignment ” step to compensate for rotational differences
2 log 1 δ log 1 log 1 δ
)1+δ
Algorithm 2 Classification with landmarks 1 : Input : Set of graphs G = {G1 , . . . , GN} 2 : Sample landmarks S = { ˜G1 , . . . , ˜Gd} from G 3 : for j = 1 , . . . , N , i = 1 , . . . , d do 4 : 5 : end for 6 : Train classifier h using φS 7 : Output : h
φS i ( Gj ) = kM ( Gj , ˜Gi ) where kM is Algorithm 1 . in the embeddings . This could be posed as a procrustean problem [ 17 ] . Preliminary investigations suggested that the accuracy gained by such a step was not sufficiently high to motivate the extra computational cost .
Attributed graphs .
In previous sections , we have only discussed classification of simple , unweighted graphs . In practice however , graphs are often attributed with node or edge labels , and incorporating these can vastly improve the classification accuracy [ 36 ] . A simple modification of our method allows for using node or edge labels . Consider Algorithm 1 . The weight matrix W is defined by the euclidean distance between embeddings of nodes from either graph . Without changing the embeddings , we can incorporate label information by matching instead based on a matrix W . is a matrix of label similarities . In Section 5.2 , we investigate ) = W ◦ W L using the element wise product f ( W , W L where W L ij = 1 if the node labels of i and j are equal , and 0 otherwise . That is , only nodes of the same label are matched . is the binary label similarity with wL
) where W L
= f ( W , W L
General structured data .
We note in passing that , while the presentation of this paper is specialized towards graphs , Algorithms 1 & 2 can be used for many kinds of structured objects , including strings and images . The problem of learning with general structured objects has been addressed before , notably so within kernel methods [ 19 , 14 ] . In many cases however , where objects , such as graphs or strings , are represented by sets of geometric points , a maximum weight matching may be a more natural means of comparison , than a kernel . This motivates further study of matchings of geometric embeddings as good similarity functions .
5 . EXPERIMENTS
We evaluate the use of matchings of geometric embeddings of graphs in graph classification . We apply the classification method of Algorithm 2 with the graph similarity functions of Algorithm 1 , in several settings and with seven
471 Table 1 : Evaluated graph embeddings . See Section 2.4 for details .
Embedding Notation Result label L Laplacian Pseudo inverse laplacian L+ Adjacency eigenvectors Laplacian eigenvectors Incidence matrix Luz Schrijver labelling Lov´asz ϑ labelling
OA L OA L+ OA Eλ(A ) OA Eλ(L ) OA M OA ls OA lo ϑ
Eλ(A ) Eλ(L ) M different graph embeddings , see Table 1 . First , we consider synthetic datasets corresponding to the theoretical results of Section 3 . Second we classify real world graphs often used as benchmarks for evaluating graph classification methods and graph kernels .
In all experiments , we use a standard C SVM1 and 10fold cross validation to estimate the classification accuracy of each method , where the parameter C is fit for each fold . For each fold we sample Nt points for a training set , from which we sample d landmarks uniformly at random . For each training and testing point , the similarities to these landmarks form a new feature vector . The new training set , an Nt × d feature matrix , is used to the train the SVM . The remaining data is then used for testing . For an overview , see Algorithm 2 . 5.1 Classifying synthetic graphs
We evaluate our method in two binary classification tasks , corresponding to the theoretical results of Section 3 . The first task concerns classification of two classes of Erd˝os R´enyi random graphs . This corresponds to the setting of Proposition 31 The goal is to assess to the performance of the various embeddings considered in this work , and to confirm the aforementioned theoretical results . Specifically , we form two classes containing G(n , p ) and G(n , p . ) graphs respectively , with G(n , p ) the graph formed by independently including every edge with probability p . We perform a suite of experiments classifying these graphs on the form pi = 0.5 − i · C for varying values of p and p . i = 0.5 + i · C so that for each i , we classify G(n , pi ) and p . versus G(n , p . i ) . For each i , we generate N = 100 graphs of each class , with n = 100 nodes . Here , we’ve used C = 0.005 and i = 0 , . . . ,10 .
In the second task , the first class comprises random G(n , p ) graphs and the second planted clique graphs G(n , p , k ) , corIn G(n , p , k ) graphs , each responding to Proposition 32 edge is included with probability p apart from a in random subgraph of k nodes that is made into a clique . We let n for t = 0 , 0.5 , , 4 . For each value of p = 1/2 and k = 2t t , we create N graphs of each class with n = 200 for both . The number of landmarks was set to 100 for both tasks . Landmarks were sampled 10 times and the results averaged .
√
100
90
80
70
60
50
OA L OA L+ OA Eλ(A ) OA Eλ(L ) OA LS OA M OA LO ϑ
)
%
( y c a r u c c a n o i t a c i f i s s a C l
40
0
0.01
0.02
0.03
0.04
0.06
0.07
0.08
0.09
0.1
0.05 |p' p|
Figure 3 : Average classification accuracy ( % ) using 10 fold ) for varying |p − p| cross validation on G(n , p ) vs G(n , p .
OA L OA L+ OA LS OA M OA LO ϑ h t w o r g n g r a M i
1.6
1.4
1.2
1
0.8
0.6
0.4
0.2
0
0.2
0.4
0.01
0.02
0.03
0.04
0.06
0.07
0.08
0.09
0.1
0.05
|p' p|
Figure 4 : Margin growth on G(n , p ) vs G(n , p . ) for varying |p−p| The top ( overlapping ) plots are OA L and OA L+ . See Section 5.1 for our definition of margin growth . OA Eλ(A ) and OA Eλ(L ) left out for clarity .
)
%
( y c a r u c c a n o i t a c i f i s s a C l
100
90
80
70
60
50
40
0
0.5
OA L OA L+ OA Eλ(A ) OA Eλ(L ) OA LS OA M OA LO ϑ
3.5
4
1.5
3 1 Planted clique size factor , t
2.5
2
Results .
The results of the G(n , p ) vs G(n , p .
) experiment are presented in Figure 3 . We note three embeddings that stand out : the Laplacian , the inverse Laplacian and the incidence matrix , which is also a decomposition of the Laplacian . These
1http://wwwcsientuedutw/~cjlin/libsvm/
Figure 5 : Average classification accuracy ( % ) using 10 fold cross validation on G(n , p ) vs G(n , p , k ) forn = 200 , p = 1/2 , k = 2t
√ n and varying t .
472 embeddings reach maximum accuracy for smaller differences between the classes than any other embedding . According to Proposition 3.1 , the margin specified by matchings of Laplacian embeddings grows linearly with |p . − p| . To verify this , we have computed the empirical margin , ˆγK = E[K(x , x .
) ] − E[K(x , x .
)|.(x ) = ( x
)|.(x ) ( = ( x
) ] based on the definition of a good similarity function , see ( 1 ) . In Figure 4 , we plot the empirical margin growth for all embeddings , for increasing values of |p . − p| . As the various embeddings produce similarity functions in different ranges , we have normalized ˆγK for each embedding , by dividing it by the mean over the values of|p . − p| , and subtracting the initial value , so that all start at 0 . We see that the theoretical linear growth for the Laplacian and inverse Laplacians occur empirically as well . This explains , in part , the good classification accuracy , seen in Figure 3 .
The results of the G(n , p ) vs G(n , p , k ) experiment are presented in Figure 5 . We note that , as expected , the Lov´asz embedding performs well , as it has been shown to solve the planted clique problem well for small t [ 11 ] . The results for the LS labelling are similar to those of Johansson et al . [ 23 ] , who showed that the margin grows linearly with the size of the planted clique .
Interestingly , the incidence matrix , which is the embedding with lowest computational complexity , separates the classes well in both experiments . In all three figures described above , we see that the Lov´asz labelling , and the three Laplacian embeddings perform the best . In contrast , the spectral embeddings perform poorly , with the adjacency spectrum embedding failing to separate the classes of the second experiment for all values of t . 5.2 Classification of benchmark graphs
We compare our methods to six different graph kernels . These kernels are the shortest path kernel ( SP ) [ 4 ] , the 4graphlet kernel ( GL ) [ 37 ] , the random walk kernel ( RW ) [ 15 ] , the Lov´asz ϑ ( Lo ϑ ) andsvm ϑ ( SVM ϑ ) kernels [ 23 ] , and the fast subtree kernel ( ST ) [ 35 ] .
Here , we investigate two different ways of using matchings of geometric embeddings for graph classification . First , we classify the graphs using Algorithm 2 . The landmarks form new feature vectors , and are used with a linear C SVM . We normalize each feature by making it have zero mean and standard deviation 1 . Here , we let the number of landmarks be the size of the training set . Second , we use the matching similarity matrix KM , where [ KM ]ij = kM ( Gi , Gj ) as defined by Algorithm 1 , in place of a kernel in a kernel SVM . Doing so does not give the theoretical guarantees associated with kernel methods , as KM is not positive semi definite in general .
We classify graphs from four different datasets , commonly used as benchmarks for graph kernels , ptc , mutag , enzyme and nci1s , see Table 2a for details . ptc is a set of graphs representing chemical compounds labeled according to their carcinogenic effects on rats and mice [ 20 ] . Compounds reported as having clear or some evidence are labeled ( +1 ) and those said to have no evidence ( 1 ) . The dataset is split into four groups by male or female rats or mice . A separate classifier was trained for each group and the average best accuracy of all four is reported . mutag [ 8 ] is a dataset of 188 graphs representing compounds labeled according to whether they have a mutagenic effect on the Salmonella ty phimurium bacterium . enzyme is a collection of 600 graphs representing tertiary protein structures collected by [ 5 ] , each labeled with one of the 6 EC top level classes . nci1s is a set of 200 graphs representing a subset of chemical compounds screened for activity against non small cell lung cancer cell lines [ 42 ] . The set is a random subset of the nci1 set [ 42 ] .
, where W is as in Algorithm 1 and W L
In each of the datasets , nodes have discrete labels . We incorporate labels into our similarity function , prior to com , we compute W . puting the matching . For graphs G , G . = W ◦ W L is the ij is 1 if nodes i ∈ V binary matrix where each entry wL and j ∈ V . have the same label , and 0 otherwise . That is , we only match nodes of the same label . We then compute kM ( G , G .
) = w(mwm(W .
) ) .
Results .
The results of the classification experiments are presented in Table 2b . Each column represents a dataset . The rows are divided into three large groups . The first group shows classification results for the methods applied to graphs where node labels have been removed . The second corresponds to regular use of node labels and the third to graphs where each kernel/matching has been used with the Weisfeiler Lehman framework [ 36 ] . Each of these groups are divided into graph kernels and Optimal assignment . GL , Lo ϑ and SVM ϑ kernels appear only in the first group as they do not make use of node labels . For the OA kernels , the numbers in the table represent the best result of the two classification methods , landmarks or indefinite kernels . This choice was made to reduce the size of the table . For a comparison between the two settings , based on the same results , see Figure 6 . The numbers are the average relative accuracy ( within each approach ) as compared to the best overall result ( for all approaches ) , for each dataset and label type . For example , the green bars represent the average accuracy when classifying using landmarks , relative to the best result using any method .
We first note that in each experiment group , at least one of the OA methods perform better than or equal to the best graph kernel . Notably , the OA L+ embedding achieves the highest accuracy on both enzyme and nci1s in all of the experiments , and is competitive with the best graph kernels on the other sets . Furthermore , we see that on all of the datasets , leveraging node labels increases the best accuracy for all OA methods , either with or without WL iterations . Also notable is the results of OA M which is the fastest embedding to compute . It achieves the highest score in 3 of the 12 experiments and is within 2.5 % of the best result in 6 of the remaining 9 .
Perhaps most impressive is the result for OA L+ on enzyme , without node labels , on which the difference between the best and second best method is 8.1 % and the distance to the best graph kernel is 253 % When including labels , the distance is 26.3 % and when using the WL framework 133 % It should be noted that higher accuracy has been achieved when using continuous vector labels on the nodes in enzyme [ 12 ] , something that can be leveraged by our method as well .
As remarked at the end of Section 4 , the time complexity of our similarity function with the incidence matrix embedding , is linear in the number of edges , placing it among the fastest graph kernels in theory . However , the datasets used for evaluation are too small for this to take effect . On mu
473 Unlabeled graphs
Node labels , no WL
Node labels , 8 WL iterations
1
0.8
0.6
0.4
0.2
0
OA Indefinite OA Landmarks Best graph kernel
PTC
MUTAG ENZYMES NCI1S
1
0.8
0.6
0.4
0.2
0
OA Indefinite OA Landmarks Best graph kernel
PTC
MUTAG ENZYMES NCI1S
1
0.8
0.6
0.4
0.2
0
OA Indefinite OA Landmarks Best graph kernel
PTC
MUTAG ENZYMES NCI1S
Figure 6 : Relative accuracy of the different classification methods , compared to the best result in each setting . Both OA methods are developed in this paper , and the best graph kernel is selected from previous methods . tag , for example , OA M is computed in 4.2s , SP in 0.7s and GL in 45s On enzyme , the numbers are 18s , 3.3s and 49s , even though SP has complexity in O(n3 ) .
6 . RELATED WORK
The problem of learning with indefinite similarity functions has been approached from several angles before [ 2 , 43 ] . Balcan , Blum & Srebro [ 2 ] addressed the problem of training a large margin classifier in this setting . Wang , Yang & Feng [ 43 ] proposed learning with with good dissimilarity functions using a boosting algorithm . The methods developed in this paper can easily be carried over to this class of algorithms as well . A common idea in learning with similarities , is that of sampling a set of landmark points and using the similarity to these points to produce feature vectors of the data [ 2 , 24 , 44 , 30 ] .
In contrast to our work , neither of these papers give specific examples of what are “ good ” similarity ( or dissimilarity ) functions , to illustrate their general frameworks . Nor do they give suggestions of how to prove such results . Further , none of them apply their framework to graphs , geometric embeddings or optimal assignment .
Bellet , Habrard & Sebban [ 3 ] considered the problem of learning good , linear similarity functions from data . A related , but different , setting was adopted by Kar & Jain [ 24 ] who proposed learning the definition of goodness . In contrast to our work , this class of methods involve an additional learning step – learning the similarity or goodness criteria , which itself is computationally expensive . Further , the resulting similarity function may not have a natural interpretation .
7 . CONCLUSIONS & FUTURE WORK
We have advanced theory and practice of learning with similarity functions , starting from the theory of Balcan , Blum & Srebro ( BBS ) [ 2 ] . First , we have placed the BBS theory in the more general context of optimal transport theory . This makes the notion of good similarity functions more natural and less ad hoc . More importantly , the connection also opens up the opportunity to draw upon the theory and methods of optimal transport , including optimal couplings , to establish that specific similarity functions are “ good ” in the sense of BBS . We use couplings to show that maximum weight matchings of the Laplacian embedding of graphs is a good similarity function . Further , we perform an extensive empirical evaluation , both confirming the theoretical results on synthetic datasets , and improving on state of the art performance on standard benchmarks for graph classification .
An exciting direction for future work is to establish that matchings based on other geometric embeddings of graphs , are also good similarity functions . As remarked previously , the method of using matchings of geometric embeddings developed in this paper can be applied to other types of structured data . It would be interesting to prove corresponding rigorous results for these . Another aspect to investigate further is the scalability of our method to larger datasets . We noted in the experiments that there was no substantial gain in efficiency when using approximate matching algorithms on the small benchmark datasets commonly used for graph classification .
Acknowledgements This work is supported in part by the Swedish Foundation for Strategic Research ( SSF ) .
8 . REFERENCES
[ 1 ] Pankaj K Agarwal and R Sharathkumar .
Approximation algorithms for bipartite matching with metric and geometric costs . In STOC 14 , 2014 .
[ 2 ] Maria Florina Balcan , Avrim Blum , and Nathan
Srebro . A theory of learning with similarity functions . Machine Learning , 72(1 2):89–112 , 2008 .
[ 3 ] Aurelien Bellet , Amaury Habrard , and Marc Sebban . Similarity learning for provably accurate sparse linear classification . In John Langford and Joelle Pineau , editors , Proceedings of the 29th International Conference on Machine Learning ( ICML 12 ) , ICML ’12 , pages 1871–1878 , New York , NY , USA , July 2012 . Omnipress .
[ 4 ] Karsten M Borgwardt and Hans Peter Kriegel .
Shortest path kernels on graphs . In Proceedings of ICDM , pages 74–81 , 2005 .
[ 5 ] Karsten M Borgwardt , Cheng Soon Ong , Stefan
Sch¨onauer , SVN Vishwanathan , Alex J Smola , and Hans Peter Kriegel . Protein function prediction via graph kernels . Bioinformatics , 21(suppl 1):i47–i56 , 2005 .
[ 6 ] St´ephane Boucheron , G´abor Lugosi , and Pascal
Massart . Concentration inequalities : A nonasymptotic theory of independence . Oxford University Press , 2013 .
[ 7 ] T H Hubert Chan , Kevin L . Chang , and Rajiv
Raman . An sdp primal dual algorithm for approximating the lov´asz theta function . In Proceedings of ISIT , pages 2808–2812 , Piscataway , NJ , USA , 2009 . IEEE Press .
474 Table 2 : Experiment results on benchmark datasets .
( a ) Benchmark dataset statistics . graphs . Same for all six classes .
‡
†
Average for the four types of
Graphs Nodes ( avg . ) Edges ( avg . ) Classes Class dist . ptc †
345 25.7 26.1
2
188 17.9 19.8
2
39/61
33/67 mutag enzyme nci1s 200 32.6 30.2
2
45/55
600 32.6 62.1 ‡
6 .17
( b ) Classification accuracy ( % ) using 10 fold cross validation on benchmark datasets . The result groups are unlabeled graphs , labeled graphs and labeled graphs with the WL framework . All datasets have 2 classes apart from enzyme which has 6 . Bold numbers indicate the best result for each dataset in each result group . Ran out of memory ( Result from [ 36] ) .
Did not finish within 5 days .
†
‡ ptc mutag enzyme nci1s l a m i t p O s l e n r e k
.
G s l e n r e k h p a r G t n e m n g i s s a
87.2 83.5 85.6 86.2 87.8 87.8 85.6 85.1 84.2 87.8 86.1 85.6 86.0
SP GL RW Lo ϑ SVM ϑ ST OA L OA L+ OA Eλ(A ) OA Eλ(L ) OA M OA ls OA lo ϑ
Kernels Without node labels 63.0 63.5 60.6 64.3 63.8 61.0 63.7 63.4 64.5 63.7 63.7 64.1 64.4
30.5 27.3 21.2 26.5 33.5 37.0 54.3 62.3 37.2 52.0 38.5 54.2 41.8 With node labels and no WL iterations 39.0 29.3 39.0 61.3 65.3 47.7 59.5 47.5 59.8 48.3
SP RW ST OA L OA L+ OA Eλ(A ) OA Eλ(L ) OA M OA ls OA lo ϑ
85.6 86.7 85.6 87.8 85.8 85.6 86.0 86.7 85.1 86.7
62.9 62.5 62.9 65.9 66.0 66.2 65.9 66.4 65.1 64.4 t n e m n g i s s a l a m i t p O
With node labels and WL iterations s l e n r e k
.
G t n e m n g i s s a l a m i t p O
WL SP WL RW WL ST OA L OA L+ OA Eλ(A ) OA Eλ(L ) OA M OA ls OA lo ϑ
64.2 † 65.0 64.3 64.8 64.1 63.9 65.5 64.3 65.0
84.6 † 84.6 88.3 87.2 86.2 88.2 89.4 88.8 87.8
50.5 † ‡ 52.2 61.3 65.3 52.8 58.0 53.3 59.3 54.7
64.5 62.0 61.0 63.5 60.0 63.5 66.0 68.5 65.5 62.5 66.0 65.5 57.0
64.0 61.0 64.0 67.0 70.0 65.5 67.5 68.0 65.5 61.0
74.5 † 65.5 76.5 77.0 73.5 76.0 75.0 75.5 76.0
[ 8 ] Asim Kumar Debnath , Rosa L . Lopez de Compadre ,
Gargi Debnath , Alan J . Shusterman , and Corwin Hansch . Structureactivity relationship of mutagenic aromatic and heteroaromatic nitro compounds . Correlation with molecular orbital energies and hydrophobicity . Journal of Medicinal Chemistry , 34:786–797 , 1991 .
[ 9 ] Ran Duan and Seth Pettie . Linear time approximation for maximum weight matching . Journal of the ACM ( JACM ) , 61(1):1 , 2014 .
[ 10 ] Devdatt P Dubhashi and Alessandro Panconesi .
Concentration of measure for the analysis of randomized algorithms . Cambridge University Press , 2009 .
[ 11 ] Uriel Feige and Robert Krauthgamer . Finding and certifying a large hidden clique in a semirandom graph . Random Structures & Algorithms , 16(2):195–208 , 2000 .
[ 12 ] Aasa Feragen , Niklas Kasenburg , Jens Petersen , Marleen de Bruijne , and Karsten M . Borgwardt . Scalable kernels for graphs with continuous attributes . In NIPS , pages 216–224 , 2013 .
[ 13 ] Holger Fr¨ohlich , J¨org K . Wegner , Florian Sieker , and
Andreas Zell . Optimal assignment kernels for attributed molecular graphs . In Luc de Raedt and Stefan Wrobel , editors , Proceedings of the 22nd International Conference on Machine Learning ( ICML 2005 ) , pages 225–232 , Bonn , Germany , August 2005 . ACM Press .
[ 14 ] Thomas G¨artner . A survey of kernels for structured data . SIGKDD Explor . Newsl . , 5(1):49–58 , July 2003 . [ 15 ] Thomas G¨artner , Peter Flach , and Stefan Wrobel . On graph kernels : Hardness results and efficient alternatives . Learning Theory and Kernel Machines , pages 129–143 , 2003 .
[ 16 ] Michel X . Goemans . Semidefinite programming in combinatorial optimization . Math . Program . , 79:143–161 , 1997 .
[ 17 ] John C Gower and Garmt B Dijksterhuis . Procrustes problems , volume 3 . Oxford University Press Oxford , 2004 .
[ 18 ] Xiao W . Gutman , I . Generalized inverse of the laplacian matrix and some applications . Bulletin . Classe des Sciences Math ˜Al’matiques et Naturelles . Sciences Math ˜Al’matiques , 129(29):15–23 , 2004 . [ 19 ] David Haussler . Convolution kernels on discrete structures . Technical report , Technical report , Department of Computer Science , University of California at Santa Cruz , 1999 .
[ 20 ] C . Helma , R . D . King , S . Kramer , and A . Srinivasan .
The predictive toxicology challenge 2000–2001 . Bioinformatics , 17(1):107–108 , 2001 .
[ 21 ] Linus Hermansson , Tommi Kerola , Fredrik Johansson ,
Vinay Jethava , and Devdatt Dubhashi . Entity disambiguation in anonymized graphs using graph kernels . In Proceedings of the International Conference on Information and Knowledge Management ( CIKM ) , pages 1037–1046 . ACM , 2013 .
[ 22 ] Vinay Jethava , Anders Martinsson , Chiranjib
Bhattacharyya , and Devdatt Dubhashi . Lovasz theta function , svms and finding dense subgraphs . Journal of Machine Learning Research , 14:3495–3536 , 2014 .
475 [ 23 ] Fredrik D . Johansson , Vinay Jethava , Devdatt
[ 34 ] Blake Shaw and Tony Jebara . Structure preserving
Dubhashi , and Chiranjib Bhattacharyya . Global graph kernels using geometric embeddings . In Tony Jebara and Eric P . Xing , editors , Proceedings of the 31st International Conference on Machine Learning ( ICML ) , pages 694–702 . JMLR Workshop and Conference Proceedings , 2014 .
[ 24 ] Purushottam Kar and Prateek Jain . Similarity based learning via data driven embeddings . In Advances in neural information processing systems , pages 1998–2006 , 2011 .
[ 25 ] Purushottam Kar and Prateek Jain . Supervised learning with similarity functions . In Advances in neural information processing systems , pages 215–223 , 2012 .
[ 26 ] Michel Ledoux . The concentration of measure phenomenon , volume 89 . American Mathematical Soc . , 2005 .
[ 27 ] Torgny Lindvall . Lectures on the coupling method .
Wiley Series in Probability and Mathematical Statistics : Probability and Mathematical Statistics . John Wiley & Sons , Inc . , New York , 1992 . A Wiley Interscience Publication .
[ 28 ] L´aszl´o Lov´asz . On the shannon capacity of a graph .
IEEE Transactions on Information Theory , 25(1):1–7 , 1979 .
[ 29 ] Carlos J . Luz and Alexander Schrijver . A convex quadratic characterization of the lov´asz theta number . SIAM J . Discrete Math . , 19(2):382–387 , 2005 .
[ 30 ] El˙zbieta P fiekalska and Robert PW Duin . Dissimilarity representations allow for building good classifiers . Pattern Recognition Letters , 23(8):943–956 , 2002 .
[ 31 ] Seth Pettie and Peter Sanders . A simpler linear time
2/3 epsilon approximation for maximum weight matching . Inf . Process . Lett . , 91(6):271–276 , 2004 .
[ 32 ] Bernhard Sch¨olkopf and Alexander J Smola . Learning with kernels : Support vector machines , regularization , optimization , and beyond . MIT press , 2001 .
[ 33 ] Shai Shalev Shwartz , Yoram Singer , Nathan Srebro , and Andrew Cotter . Pegasos : Primal estimated sub gradient solver for svm . Mathematical programming , 127(1):3–30 , 2011 . embedding . In Proceedings of the 26th Annual International Conference on Machine Learning , pages 937–944 . ACM , 2009 .
[ 35 ] Nino Shervashidze and Karsten Borgwardt . Fast subtree kernels on graphs . In Proceedings of NIPS , pages 1660–1668 . 2009 .
[ 36 ] Nino Shervashidze , Pascal Schweitzer , Erik Jan van
Leeuwen , Kurt Mehlhorn , and Karsten M . Borgwardt . Weisfeiler lehman graph kernels . Journal of Machine Learning Research , 12:2539–2561 , 2011 .
[ 37 ] Nino Shervashidze , SVN Vishwanathan , Tobias Petri , Kurt Mehlhorn , and Karsten M Borgwardt . Efficient graphlet kernels for large graph comparison . In Proceedings of AISTATS , 2009 .
[ 38 ] Hermann Thorisson . Coupling , stationarity , and regeneration . Probability and its Applications ( New York ) . Springer Verlag , New York , 2000 .
[ 39 ] Jean Philippe Vert . The optimal assignment kernel is not positive definite . CoRR , abs/0801.4061 , 2008 .
[ 40 ] C´edric Villani . Topics in optimal transportation . Number 58 . American Mathematical Soc . , 2003 . [ 41 ] SVN Vishwanathan , Nicol N Schraudolph , Risi
Kondor , and Karsten M Borgwardt . Graph kernels . Journal of Machine Learning Research , 11:1201–1242 , 2010 .
[ 42 ] Nikil Wale , IanA . Watson , and George Karypis .
Comparison of descriptor spaces for chemical compound retrieval and classification . Knowledge and Information Systems , 14(3):347–375 , 2008 .
[ 43 ] Liwei Wang , Cheng Yang , and Jufu Feng . On learning with dissimilarity functions . In Proceedings of the 24th international conference on Machine learning , pages 991–998 . ACM , 2007 .
[ 44 ] Adam Woznica , Alexandros Kalousis , and Melanie
Hilario . Matching based kernels for labeled graphs . In ECML/PKDD Workshop on Mining and Learning with Graphs , 2006 .
476
