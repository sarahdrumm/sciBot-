Anatomical Annotations for Drosophila Gene Expression
Patterns via Multi Dimensional Visual Descriptors
Integration
University of Texas at Arlington
Shanghai Jiao Tong University
Hongchang Gao Computer Science and
Engineering
Arlington , TX , 76019 hongchanggao@gmail.com
Weidong Cai
School of Information
Technologies
University of Sydney NSW 2006 , Australia tomcai@sydneyeduau
Lin Yan
Department of Electronic
Engineering
Shanghai , China yanlin@sjtueducn
Heng Huang∗
Computer Science and
Engineering
University of Texas at Arlington
Arlington , TX , 76019 heng@uta.edu
ABSTRACT In Drosophila gene expression pattern research , the in situ hybridization ( ISH ) image has become the standard technique to visualize and study the spatial distribution of RNA . To facilitate the search and comparison of Drosophila gene expression patterns during Drosophila embryogenesis , it is highly desirable to annotate the tissue level anatomical ontology terms for ISH images . In ISH image annotations , the image content representation is crucial to achieve satisfactory results . However , existing methods mainly focus on improving the classification algorithms and only using simple visual descriptor . If we integrate the effective local and holistic visual descriptors via proper learning method , we can achieve more accurate image annotation results than using individual visual descriptor .
We propose a novel structured sparsity inducing norms based feature learning model to integrate the multi dimensional visual descriptors for Drosophila gene expression patterns annotations . The new mixed norms are designed to learn the importance of different features from both local and global point of views . We successfully integrate six widely used visual descriptors to annotate the Drosophila gene expression patterns from the lateral , dorsal , and ventral views . The empirical results show that the proposed new method can effectively integrate different visual descriptors , and consistently outperforms related methods using the concatenated visual descriptors .
∗ Corresponding Author . Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . Copyrights for components of this work owned by others than ACM must be honored . Abstracting with credit is permitted . To copy otherwise , or republish , to post on servers or to redistribute to lists , requires prior specific permission and/or a fee . Request permissions from Permissions@acmorg KDD’15 , August 10 13 , 2015 , Sydney , NSW , Australia . cfl 2015 ACM . ISBN 978 1 4503 3664 2/15/08 $1500 DOI : http://dxdoiorg/101145/27832582783384
Categories and Subject Descriptors H28 [ Database Management ] : Database ApplicationsData Mining
General Terms Algorithms
Keywords Multi Dimensional Feature Learning ; Multi View Data Integration ; Drosophila Gene Expression Pattern Annotation
1 .
INTRODUCTION
The mRNA in situ hybridization ( ISH ) is a powerful method for analyzing the expression and localization properties of RNAs at subcellular resolution in intact embryos and tissues . The in situ images can provide the subcellular visualization of the exact transcript localization , which controls the important biological functions such as cell fate determination , cell migration , cell polarity , neuronal processes , morphogenesis , and embryonic axis specification [ 7 , 12 , 6 ] . Thus , the ISH images have been widely used to study the cellular and subcellular distributions of RNAs within cells and tissues of the organisms to infer and understand their stability , translation , and biological functions .
Because a large number of genes in development stage are shared by the Drosophila melanogaster ( fruit fly ) and other species ( including human ) , it is important to study the Drosophila expression patterns for helping the understanding of the fundamental principles of different species development [ 20 , 4 , 23 ] . In Drosophila gene expression pattern research , the ISH image has become the standard technique to visualize and records the spatial distribution of RNA . The Drosophila ISH images are often associated to controlled vocabularies from the biologists’ perspective [ 21 ] and these anatomical ontology terms describe the spatial biological patterns and functions .
339 Figure 1 : Examples of Drosophila embryo ISH images and associated annotation terms for three views .
The comparative analysis of gene expression patterns need to analyze a large number of ISH images of individual embryos . In literature , more than one hundred thousand ISH images of early embryogenesis are available for Drosophila gene expression research . To facilitate the search and comparison of Drosophila gene expression patterns during Drosophila embryogenesis , it is highly desirable to annotate the tissuelevel anatomical ontology terms for ISH images . This annotation is of great importance in the study of comparative and developmental biology , because it provides a direct way to reveal the interactions and biological functions of genes based on gene expressions and enhance gene regulatory networks inference . Due to the rapid increase in the number of ISH images and the inevitable bias annotation by human curators , it is necessary to develop an automatic system to annotate anatomical structure using controlled vocabulary . Many research efforts have been made to automatically annotate Drosophila gene expression patterns [ 11 , 17 ] . The most recent works include local regression ( LR ) [ 10 ] , local shared subspace ( LS ) [ 9 ] , CCA [ 1 ] , etc . All these methods mainly focus on the classification algorithms design with formulating the Drosophila gene expression pattern annotation as a multi label multi class classification problem . However , the variability , ambiguity and the wide range of illumination in Drosophila ISH images increase the difficulty of classifications , Thus the image content representation issue is crucial for researchers to achieve the satisfactory annotation results . The most effective way to tackle such problems is to utilize the low level image features such as color , texture histograms , object shapes , etc . All existing Drosophila gene expression pattern analysis methods either directly use the image pixels or use the Scale Invariant Feature Transform ( SIFT ) [ 13 ] features , which focus on the local information of images .
To address the image content representation problem , many visual descriptors have been proposed and SIFT is only one of them . In general , these visual descriptors fall into two categories : local feature descriptors and holistic descriptors . Besides SIFT , Histogram of Oriented Gradients ( HOG ) [ 3 ] is the other most popularly used visual descriptor to overcome image variability caused by changing viewpoints , occlusions , and varying illumination . Local Binary Patterns ( LBP ) was proposed in [ 15 ] as a powerful texture feature based on occurrence histogram of local binary patterns . GIST [ 16 ] , CENTRIST [ 27 ] and color moment [ 28 ] are three well known holistic descriptors for image representations . Previous work
Figure 2 : Demonstration of Dense SIFT feature extraction . Each Dense SIFT feature is extracted on a patch with 16 × 16 pixels . showed if we integrate different visual descriptors , we could enhance the image recognition performance [ 2 ] .
In this paper , we propose a novel structured sparsityinducing norms based feature learning model to integrate the multi dimensional visual descriptors for Drosophila gene expression patterns annotations . The new mixed norms are designed to learn the importance of different features from both local and global point of views . The optimization algorithm is derived with convergence proof to solve the proposed non smooth objective . We conduct the experiments with integrating six widely used visual descriptors to annotate the Drosophila gene expression patterns from the lateral , dorsal , and ventral views . The empirical results show that the proposed new method can effectively integrate different visual descriptors , and consistently outperforms related methods using the concatenated visual descriptors .
2 . DATA DESCRIPTION
The Drosophila embryos are 3D objects . Each embryo is characterized by 2D images from different views . That is lateral , dorsal , ventral and intermediate view . We only tackle the lateral , dorsal and ventral images as shown in Figure 1 , because the number of the images from other views is too small to provide sufficient description for the data set . In addition , we consider these views separately in ISH image annotations . 2.1 Visual Feature Descriptions
As we know , different features describe different aspects of the visual characteristics . Thus , we extract features using six visual descriptors for each image to characterize it . CENTRIST [ 27 ] is a holistic descriptor to capture the rough geometrical information by gradient derived from intensity . We use it to extract the global shape structure within an image . ColorMoment ( CMT ) [ 28 ] captures local photometrical and spatial information derived from pixel values . It can capture the distribution of the color within an image . GIST [ 16 ] is a low dimensional representation of the image , encoding rough geometry and spatial structures within an image without considering detailed local information . That is to say it can represent the dominant spatial structure of an image . HOG [ 3 ] counts occurrences of the gradient orientation in localized portions within an image to describe the shape information . With it we can extract the local object appearance and shape within an image . LBP [ 15 ] is an efficient texture descriptor by counting the occurrences of local binary patterns . We use it to extract the texture information within the image . Dense SIFT ( DSIFT ) [ 5 ] is an improved SIFT descriptor . It computes SIFT descriptor gapprocephalic ectoderm anlage in statu nascendidorsal ectoderm anlage in statu nascendiLateralDorsalVentraltrunk mesoderm primordium P2head mesoderm primordium P4anterior endoderm anlageposterior endoderm primordium P2foregut anlageubiquitousbrain primordiumhindgut proper primordiumtrunk mesoderm primordiumanterior midgut primordiumposterior midgut primordiumViewImagesAnnotation Terms340 ( a ) Lateral
( b ) CENTRIST
( c ) Color Moment
( d ) GIST
( e ) LBP
( f ) HOG
( g ) Dorsal
( h ) CENTRIST
( i ) Color Moment
( j ) GIST
( k ) LBP
( l ) HOG
( m ) Ventral
( n ) CENTRIST
( o ) ColorMoment
( p ) GIST
( q ) LBP
( r ) HOG
Figure 3 : The visual descriptors CENTRIST , Color Moment , GIST , LBP , and HOG on Drosophila images from three views . The first row is the lateral view , the second is the dorsal view , and the third is the ventral view . over dense grids in the image rather than interest points , which will provide more information .
Figure 3 shows the visual patterns of CENTRIST , CMT , GIST , HOG , and LBP extracted from three samples . All of the images from BDGP database have been aligned and resized to 128 × 320 . For all the above descriptors except DSIFT , we extract the features from the whole image . For DSIFT descriptor , we extract features from the overlapped patches with both radius and spacing as 16 pixels [ 8 ] , as shown in Figure 2 . For each patch , we extract a feature vector with 128 dimensions . In detail , we set the size of neighborhoods as 4 × 4 pixels and the number of bins as 8 . Thus , there are a total of 133 SIFT descriptors within an image . If we use the raw DSIFT descriptor to represent an image , the dimension of the feature vector ( 133×128 ) will be too large , which is cost for computation and suppresses other kinds of features . Therefore , we do not use the raw DSIFT descriptor directly to represent the image . A reasonable way is to build a codebook over all extracted SIFT features , and then encode each image with a new feature vector based on the codebook , such that the dimensionality is much less than the original one . 2.2 Codebook Construction
A codebook stores the codewords , which can be considered as a representative of several similar features . Thus , a reasonable way to produce the representative codebook is to perform clustering algorithms on the collection of the features , and the clustering centers are chosen as the representative codewords of the codebook . The number of the clusters , that is the number of the codewords , is the codebook size . Specially , the dimensionality of the encoded feature vector based on the codebook is equal to the size of the codebook .
In this paper , we perform K means on the collection of SIFT features to do clustering . The produced centroids are chosen as the codewords of the codebook . After that , we can use these centroids to represent each image . In addition , we construct the codebook for three views respectively because different views depict different part of the embryo . Considering the number of images in each view as shown in Table 1 , we set the size of the codebook as 1000 , 500 and 250 for lateral , dorsal and ventral view respectively , which means the dimensionality of the encoded feature vector is 1000 , 500 and 150 for the three views respectively .
Table 1 : The summary of the refined BDGP images with 79 terms
Stage range
4 6
7 8 9 10 11 12 13 16 Total
Size of control term 11 No . of lateral images 1514 812 727 1356 1004 5414 2152 No . of dorsal images 226 324 431 447 No . of ventral images 164 137 81 214 812
724 216
31
12 12
20
79
2.3 Data Representation
After getting the codebook , we can encode each image based on the codebook . Specifically , the dimensionality of the encoded vector for each image is equal to the number of the codewords ( clustering centroids ) . Each element of the encoded vector corresponds to a codeword . For each extracted feature from the small patch of an image , we find the most similar codeword based on the Euclidean distance and add one to the corresponding element of the encoded vector . As a result , each image can be represented by the codebook . For instance , we represent an image taken from the lateral view based on the codebook . Since the size of the lateral codebook is 1000 , we use a feature vector with 1000 dimensions to represent the image . Each element of the feature vector corresponds to a codeword in the codebook . If a codeword is chosen to represent a patch feature , then the corresponding element will be added one , which means the elements of the feature vector correspond to the number of
341 Figure 4 : Demonstration of the proposed three sparse mixed norms on weight matrix W . The elements in matrix with colors have large values . The G1 norm and G1,2 norm place emphasis on the learning of group wise weights ( ie local importance of features ) . The 2,1 norm accentuates the individual weight learning across all/most prediction tasks ( ie global importance of features ) . we proposed to use the G1 norm : c k
( 1 )
||W||G1 =
||wj i||2 .
As a result , the objective becomes : i=1 j=1
||Y − X T W||2
F + γ1||W||G1 . min W
( 3 )
( 4 ) chosen codeword . Thus , for an lateral image can be represented as xl ∈ 1000 , for the dorsal view it is xd ∈ 500 , and for the ventral view it is xv ∈ 250 .
Besides the DSIFT feature , we have extracted the other five features for each image . We use xcen , xcmt , xgist , xhog , xlbp to represent CENTRIST , Color Moment , GIST , HOG , LBP feature respectively . We concatenate all the six features together so that each image can be represented by a multimodal vector . For instance , we can represent a lateral image as x = [ xl ; xcen ; xcmt ; xgist ; xhog ; xlbp ] . Moreover , due to the different magnitude of different kinds of features , we normalize each type of features before concatenating them together .
3 . METHODOLOGY i , , xk
It is challenging to integrate the multi dimensional feaIn this section , we will propose a novel structured tures . sparsity inducing norms based multi dimensional feature learning method . Given n training images {(xi , yi)}n i=1 , where i )T ∈ d is the i th image ’s feature vecxi = ( x1 tor coming from a total of k different modalities described as above . For each modality j , it has dj features where j=1 dj . yi ∈ c is the class label vector corresponding to the i th image xi , where c is the number of annotation terms , which can be viewed as the number of tasks . Let X = [ x1 , x2 , , xn ] ∈ d×n and Y = [ (y1)T , ( y2)T , , ( yn)T ]T ∈ n×c . Our goal is to learn a d × c parameter matrix as : d =k
 w1
wk 1
1 c
··· w1 ··· wk c
 ∈ d×c ,
W = i ∈ dj corresponds to the weights of the features where wj within the j th modality with respect to i th task . Each element in wj i denotes the importance of the corresponding feature to the target . The larger the element is , the more important the feature is to the target .
Each embryo image corresponds to several annotation terms , thus the Drosophila image annotation problem is a multilabel multi class classification task . In this paper , we resort to the multivariate least squares regression model as loss function to predict the annotation terms . Previous research work has shown that the linear regression model can naturally incorporate the label correlations and have excellent performance in multi label classifications .
To integrate the multi dimensional visual features , it is important to design the proper regularization terms to capture the interrelationships of modalities and features . The general objective function is defined as the following :
||Y − X T W||2
F + γR(W ) , min W
( 2 ) where γ is a trade off parameter .
Within the multi dimensional learning task , a sample is represented by features from several modalities . These modalities come from different measurements , showing heterogeneous property . Some modalities are more discriminative to the target , and some are not . If treating all the modalities equally , the performance of the learning task will be weakened . Thus , we should enhance the effect of the discriminative modality , and at the same time suppress the less discriminative ones . Thus , in recent work [ 26 , 24 , 25 ] ,
As shown in Eq ( 3 ) , we use 2 norm within each modality and 1 norm between modalities . As we know , the 1 norm suppresses the weights of non important features to zeros ( or close to zeros ) . Thus , G1 norm can enforce the sparsity between different modalities due to the 1 norm performed between modalities . If one modality is more discriminative for the target , the features of this modality will have large weights , otherwise will get zeros as weights . As a result , the G1 norm can select more features from the discriminative modalities .
Although the G1 norm selects the discriminative feature modalities , the features within these modalities are treated equally , which means they have the same effect on the target . However , some features within a modality are more representative than the others . Thus , we should select these more representative features . On the other hand , the G1norm could over shrink the feature weights in the less discriminative feature modalities . In one less discriminative feature modality , a small number of features could still be useful for classifications . For example , the global shapes of all the embryo are similar to each other , which means that the global shape may be not discriminative for the target . However , a local shape within an embryo usually shows a different properties among different embryos , meaning that it is discriminative for the target .
We expect these feature modalities are suppressed , but not totally to zeros . To achieve this goal , we propose a new
342 G1,2 norm as a regularization term to select these features , such as Eq ( 5 ) : c k
(
||W||2
1,2 =
||wj i||1)2 .
( 5 )
( 6 )
Thus , our objective becomes : i=1 j=1
||Y − X T W||2
F + γ1(||W||G1 + ||W||2
G1,2 ) , min W where we should notice that we use the same trade off parameter for G1 norm and G1,2 norm . Because both of them tackle the same segmentation of the column and perform compatibly . Of course , they can use different parameters , but we don’t want to have too many parameters for tuning . As shown in Eq ( 5 ) , 1 norm is performed within each modality and 2 norm is performed between modalities . Due to the sparsity of 1 norm , the more representative features within a modality will be assigned large weights , and the less ones will be assigned zero weights . Thus , the more discriminative within a modality will be chosen . Meanwhile , the 2 norm on each modality will guarantee at least one feature in this modality has non zero weight ( if not , the 2norm becomes zero ) .
With the G1 norm and G1,2 norm , we enforce structured sparsity between modalities and within modalities . The local importance of all features can be learned to adjust their weights in classification model . As a result , the more discriminative modality and the more representative features within the discriminative modality will be enhanced to help the ISH image annotations .
On the other hand , we should also consider the global importance of features . Although the above G1,2 norm can help the less discriminative modalities to keep a small number of features ( highly representative for the target ) with non zero weights , we may not keep all of them and should select the features which are important to most classification tasks . Thus , we add an additional 2,1 norm to the objective function as following :
F + γ1(||W||G1 + ||W||2 min W
||Y − X T W||2 where ||W||2,1 =d c i=1 j=1 w2 ij .
G1,2 ) + γ2||W||2,1 , ( 7 )
From the multi task learning point of view , the 2,1 norm performs 2 norm between all tasks and 1 norm between all features . Thus , it imposes the sparsity between all features and non sparsity between tasks . Consequently , the features that are discriminative for most tasks will be assigned large weights .
Figure 4 illustrates the properties of the above three norms on matrix W . In this figure , G1 norm imposes sparsity between visual descriptors and makes the important feature modalities with large weights . For example , the k th modality has large weights than the 1st modality on ubiquitous term as shown in the figure . The G1,2 norm imposes the sparsity within each visual descriptor modality , and each modality will keep at least one feature with non zero weight . 2,1 norm will selects the features important to most tasks . For example , although the 1st type of features are not important to foregut anlage and W 1,d1 is suppressed to zero , this feature is important to most of other tasks . As a result , this feature will be selected by 2,1 norm regularizer . Thus , with our proposed method , we can learn the discrimi c native modality and the representative feature to accurately annotate the Drosophila ISH images .
4 . OPTIMIZATION AND ALGORITHM ANAL
YSIS
The proposed objective leads to a non smooth convex optimization problem . We are going to derive an efficient algorithm to solve the objective . 4.1 Optimization Algorithm Taking the derivative of the objective function with respect to the i th column wi(1 ≤ i ≤ c ) and setting it to zero , we get the following : 2XX T wi − 2Xyi + 2γ1Eiwi + 2γ1Fiwi + 2γ2Dwi = 0 . ( 8 )
Then , wi = ( XX T + γ1Ei + γ1Fi + γ2D )
( 9 ) And Ei is a block diagonal matrix with the m th ( 1 ≤ m ≤ k ) diagonal block defined as :
−1Xyi .
Em i =
1 2||wm i ||2
Im ,
( 10 ) where Im is a dm by dm identity matrix , and wm is the mi th segmentation of the wi . Then , we have Ei as a diagonal matrix Diag(E1 Fi is also a block diagonal matrix whose m th ( 1 ≤ m ≤ k ) i ) ∈ d×d . i ,··· , Em diagonal block elements defined as : ||wm i ||1 |wji| i ( j , j ) =
F m
,
( 11 ) where wm i Fi as : Diag(F 1 i ,··· , F m i ) ∈ d×d . is the same segmentation as in Em i . Then we have
D is a diagonal matrix with the i th diagonal element defined as
D(i , i ) =
1
2||wi||2
,
( 12 ) where wi is the i th row of W . As a result , we have D as : Diag(
) ∈ d×d .
,··· ,
1
1
2||w1||2
2||wd||2
Note that Ei , Fi and D are dependent on W and thus are also unknown variables . We propose an iterative algorithm to solve Ei , Fi , D and W alternatively . The algorithm is described in Algorithm 1 . In each iteration , We update each column of W by Eq ( 9 ) in terms of the current Ei , Fi and D , and then Ei , Fi and D are updated based on the new W . 4.2 Algorithm Analysis
Theorem 1 . Our algorithm monotonically decreases the objective function value of Eq ( 7 ) in each iteration till converges .
Proof . In each iteration , we denote the updated W as
( cid:102)W . From Eq ( 9 ) , we know that for each column wi : i Fiwi + γ2wT wT i XX Twi − wiXyi + γ1wiEiwi + γ1wT i Dwi
≤ wT i XX T wi − wiXyi + γ1wiEiwi + γ1wT i Fiwi + γ2wT i Dwi ( 13 )
343 Algorithm 1 Algorithm to solve the problem in Eq ( 7 ) . Input :
X = [ x1 , x2,··· , xn ] ∈ d×n , Y = [ (y1)T , ( y2)T ,··· , ( yn)T ]T ∈ n×c
Output : W ∈ d×c
Initialize all the elements of W as 1 . repeat
1 . Update Ei by Eq ( 10 ) :
Calculate the m th diagonal block of Ei as Em
Im i =
1 2||wm i ||2
2 . Update Fi by Eq ( 11 ) :
Calculate the m th diagonal block of Fi as F m i ( j , j ) = i ||1 ||wm |wji|
3 . Update D by Eq ( 12 ) :
Calculate the i th diagonal element as D(i , i ) =
1
2||wi||2
4 . Update W by Eq ( 9 )
Calculate the i th column of W as wi = ( XX T + γ1Ei + γ1Fi + γ2D)−1Xyi until Converges
Summing all the columns together , we have :
L((cid:102)W ) + γ1
≤ L(W ) + γ1 c c i=1 wiEiwi + γ1 wiEiwi + γ1 c c i=1 wT i Fiwi + γ2 wT i Fiwi + γ2 i=1 i=1 c c i=1 i=1 wT i Dwi wT i Dwi
( 14 ) Based on the definition of Ei and the Lemma1 in [ 14 ] , we have the following inequality : i ||2 − K k=1 i ||2 ||wk i ||2 2||wk i ||2 − wT i Eiwi
||wi||2 − d i=1
||wi||2 2||wi||2
2
( 15 )
( 16 )
2 k=1 i ||2 i ||2 2||wk i ||2 − ||wk ||wk ⇒ K i ||2 − K ||wk ⇒ K ||wk i ||2 − wT c K ||wk c K
⇒ γ1 k=1 k=1 k=1 i=1
2 k=1
≤ ||wk i ||2 i ||2 − ||wk ≤ K i ||2 2||wk ||wk i ||2 ||wk i ||2 2||wk i Eiwi ≤ K c wT i Eiwi c
||wk k=1 i=1 i ||2 − γ1
||wk i ||2 − γ1 wT i Eiwi
≤ γ1 i=1 k=1 i=1 d i=1
2
||wi||2
≤ d ||wi||2 − γ2T r((cid:102)W T D(cid:102)W )
2||wi||2
||wi||2 − d d d
||wi||2 − γ2T r(W T DW ) i=1 i=1 i=1
⇒ γ2
≤ γ2 i=1
Taking the similar schema as Ei for D , we will get :
In addition , according to the Cauchy inequality , we have dk+1−1 j=dk
||wk i ||1
|wji| w2 ji ≥ ||wk i ||2
1
( 17 ) dk+1−1 j=dk
||wk i ||1 |wji| w2 ji
Thus ,
1 − k=1 j=dk
1 − i ||2 i ||2 i ||2 i ||1 i ||1
||wk
||wk ji ≤ ||wk
|wji| w2 dk+1−1 ||wk |wji| w2 ⇒ K 1 − K dk+1−1 ||wk ≤ K dk+1−1 1 − K ||wk i ||1 |wji| w2 c K c wT i Fiwi K c c
1 − γ1
⇒ γ1
||wk
||wk i ||2 i ||2 j=dk j=dk k=1 k=1 k=1 k=1 i=1 i=1 ji
||wk i ||2
1 − γ1 wT i Fiwi
≤ γ1 ji i=1 k=1 i=1
Note that : c i=1 wT i Dwi = T r(W T DW )
( 18 )
( 19 )
Thus , through adding Eqs . ( 14,15,16,18 ) together , we will get
L((cid:102)W ) + γ1 c K ||wk K c k=1 i=1 c K ||wk K c k=1 i=1 i ||2 + γ1 i ||2
1 + γ2 d ||wk||2 d i=1
≤ L(W ) + γ1
||wk i ||2 + γ1
||wk i ||2
1 + γ2
||wk||2 i=1 k=1 i=1 k=1 i=1
( 20 )
As a consequence , Algorithm 1 can decrease the objective value of Eq ( 7 ) in each iteration monotonically . Meanwhile , when the objective stops decreasing , the solution W satisfies KKT conditions and is a stationary point . Thus , our proposed optimization algorithm converges to the global optimum .
5 . EXPERIMENTS AND DISCUSSIONS
In this section , we will apply our proposed algorithm for annotating the dataset and compare it with other state ofart classification methods . 5.1 Data Refinement
We use the embryo images from the BDGP database [ 21 ] , in which the Drosophila embryogenesis has been divided into 16 stages . These stages have been collected into small groups : 1 3 , 4 6 , 7 8 , 9 10 , 11 12 and 13 16 [ 19 ] . In this paper , we only consider the last five stage groups . Because the first stage group ( 1 3 ) contains only 2 annotation terms , it is too small . Thus , we perform our algorithm only on the last five stage groups .
In addition , the total number of the annotation terms is 303 . However , some of them are too common to provide the discriminative information . Thus , we have to refine the dataset at first . In our experiment , we eliminate
344 Table 2 : The summary of different descriptors on three views
Table 5 : The anatomical term annotation performance on ventral views
Method LS CCA Ridge SVM 1NN CENTRIST CMT GIST HOG LBP DSIFT Our
Ma Pre Ma F1 Mi Pre Mi F1 0.1126 0.1588 0.1068 0.1146 0.1624 0.1683 0.1631 0.1217 0.0782 0.1335 0.0726 0.0765 0.1332 0.1845 0.0985 0.1562 0.1476 0.0976 0.1358 0.1254 0.2019 0.1892
0.1498 0.0956 0.1526 0.1324 0.0817 0.0544 0.1320 0.1000 0.0888 0.1015 0.1768
0.1563 0.1550 0.2284 0.1690 0.1352 0.1232 0.2049 0.1710 0.1746 0.1840 0.2362
( a ) Lateral
( b ) Dorsal
( c ) Ventral
Figure 5 : The convergence of our objective on the data from three views : lateral , dorsal and ventral . Our optimization algorithm usually converges fast within 20 iterations .
5.2 Experiment Setup
To construct multi modal representation for each image , we adopt six kinds of image descriptors to extract the features . We extract the features from the whole image for all the descriptors except Dense SIFT . The dimensionality of each kind of descriptor is shown in Table 2 .
Since an image may correspond to more than one annotation items , the annotation task can be considered as a multiclass multi label problem . To evaluate the performance of such problems , precision and F1 score are two commonly used criteria . F1 score is computed based on precision and recall . Precision is the ratio of the correct positive results over all positive ones . Recall is the ratio of the corrective results over the true positive ones . F1 score is the weight average of the precision and recall ( [18] ) . To address the multi label problem , macro and micro average of precision and F1 score are introduced by [ 22 ] to evaluate the performance on multiple labels . In our experiment , we use the Macro Precision , Macro F1 , Micro Precision and Micro F1 to evaluate the performance . 5.3 Drosophila ISH Image Annotation Results To evaluate our proposed algorithm , we compare it with four state of the art multi label classification method . ( 1 ) Local shared subspace ( LS ) [ 9 ] . This method is to discover a common subspace shared among multiple labels to do multilabel classification . ( 2 ) CCA+Ridge . We first apply CCA [ 1 ] for feature fusion from multiple modality of the data , and then running ridge regression on these extracted features . ( 3 ) SVM . We apply support vector machines ( SVM ) separately for each annotation item in the one against rest scheme . ( 4 ) KNN . We adopt the k nearest neighbor method to annotate the unlabeled images . To be specific , we use
Descriptor CENTRIST CMT GIST HOG LBP DSIFT Total lateral dorsal ventral
254 48 512 360 256 1000 2430
254 48 512 360 256 500 1930
254 48 512 360 256 250 1680
Table 3 : The anatomical term annotation performance on lateral views
Method LS CCA Ridge SVM 1NN CENTRIST CMT GIST HOG LBP DSIFT Our
Ma Pre Ma F1 Mi Pre Mi F1 0.1741 0.3423 0.1383 0.1531 0.2147 0.2073 0.1989 0.2859 0.1233 0.1908 0.0857 0.0909 0.1858 0.2893 0.1751 0.2176 0.1659 0.1959 0.2581 0.1666 0.2240 0.3476
0.3010 0.2060 0.2792 0.2830 0.2175 0.1384 0.3163 0.2751 0.2526 0.2628 0.3535
0.2372 0.1094 0.1720 0.2039 0.1147 0.0637 0.1816 0.1461 0.1372 0.1723 0.2437
Table 4 : The anatomical term annotation performance on dorsal views
Method LS CCA Ridge SVM 1NN CENTRIST CMT GIST HOG LBP DSIFT Our
Ma Pre Ma F1 Mi Pre Mi F1 0.1403 0.2570 0.1270 0.1223 0.1929 0.1960 0.2475 0.1862 0.1018 0.1748 0.0764 0.0779 0.1563 0.2630 0.1373 0.2094 0.1831 0.1428 0.1742 0.1508 0.2082 0.3120
0.1714 0.0096 0.1661 0.1883 0.1057 0.0537 0.1630 0.1292 0.1289 0.1293 0.1919
0.2387 0.1714 0.2660 0.2522 0.1839 0.1268 0.2621 0.2410 0.2251 0.2116 0.3161 six common annotation terms , that is , no staining , ubiquitous , strong ubiquitous , faint ubiquitous , maternal , rapidly degraded . These terms provide less discriminative information so that they can be viewed as the outliers to be eliminated . After that , we eliminate the annotation terms whose data sample are less than 50 , because these terms with so small samples are far from representative . As a result , the total number of annotation terms is 79 after refinement .
As we know , the Drosophila embryos are 3D objects , thus the images are taken from different views to characterize the object . In our experiment , we only take consideration into 3 views , that is , lateral , dorsal and ventral view . Each image taken from a certain view corresponds to some annotation terms as shown in Figure 1 . Specifically , we tackle these three views respectively , which means that we annotate the image from a certain view rather than an embryo object . The total number of the images in each view is shown in Table 1 .
051015201180118511901195120012051210121512200510152053053554054555055505101520192193194195196197198199345 In Figure 8 , we illustrate the representative features for a certain annotation term . The sample image corresponds to five annotation terms , that is , cellular blastoderm , dorsal ectoderm anlage in statu nascendi , procephalic ectoderm anlage in statu nascendi , subset and ventral ectoderm anlage in statu nascendi . In Figure 8 , the representative features for different annotation terms have been rendered . For example , the features of Color Moment contribute more to ventral ectoderm anlage in statu nascendi term than the other terms .
6 . CONCLUSION
In this paper , we proposed a novel sparse multi dimensional feature learning method to annotate the Drosophila embryo images automatically . We designed a set of sparse mixed norms as regularization terms to learn the discriminative feature modalities and the representative features in each modality . We derived an efficient optimization algorithm to solve the proposed objective with convergence proof . The experimental results show that our method consistently outperforms other related methods in Drosophila ISH image annotations .
7 . ACKNOWLEDGMENTS
This work was supported in part by Australian Research Council ( ARC ) grants and US NSF IIS 1117965 , NSF IIS 1302675 , NSF IIS 1344152 , NSF DBI 1356628 .
8 . REFERENCES [ 1 ] M . B . Blaschko and C . H . Lampert . Correlational spectral clustering . In Computer Vision and Pattern Recognition , 2008 . CVPR 2008 . IEEE Conference on , pages 1–8 . IEEE , 2008 .
[ 2 ] X . Cai , F . Nie , H . Huang , and F . Kamangar . Heterogeneous image feature integration via multi modal spectral clustering . In Computer Vision and Pattern Recognition ( CVPR ) , 2011 IEEE Conference on , pages 1977–1984 . IEEE , 2011 .
[ 3 ] N . Dalal and B . Triggs . Histograms of oriented gradients for human detection . In Computer Vision and Pattern Recognition , 2005 . CVPR 2005 . IEEE Computer Society Conference on , volume 1 , pages 886–893 . IEEE , 2005 .
[ 4 ] M . Feany and W . Bender . A Drosophila model of
Parkinson ’s disease . Nature , 404:394–398 , 2000 .
[ 5 ] L . Fei Fei and P . Perona . A bayesian hierarchical model for learning natural scene categories . In Computer Vision and Pattern Recognition , 2005 . CVPR 2005 . IEEE Computer Society Conference on , volume 2 , pages 524–531 . IEEE , 2005 .
[ 6 ] C . Fowlkes , C . L . Hendriks , S . Keranen , and et al . A quantitative spatiotemporal atlas of gene expression in the Drosophila blastoderm . Cell , 133:364–374 , 2008 .
[ 7 ] C . L . Hendriks , S . Keranen , C . Fowlkes , and et al .
Three dimensional morphology and gene expression in the Drosophila blastoderm at cellular resolution I : data acquisition pipeline . Genome Biol . , 7:R123 , 2006 .
[ 8 ] S . Ji , Y X Li , Z H Zhou , S . Kumar , and J . Ye . A bag of words approach for drosophila gene expression pattern annotation . BMC bioinformatics , 10(1):119 , 2009 .
Figure 6 : The map of the first 10 columns ( corresponding to the first 10 annotation terms : Malpighian tubule primordium , anlage in statu nascendi , anterior endoderm anlage , anterior endoderm anlage in statu nascendi , anterior endoderm primordium , anterior midgut primordium , apically cleared , atrium , brain primordium , cellular blastoderm ) of weight matrix W from lateral view . The elements restricted the red rectangle are almost zero . k = 1 and denote it as 1NN . Additionally , we run our method without G1 norm and G1,2 norm on single feature . We run our experiments with 5 fold cross validation , and the samples are partitioned randomly . The result in the paper is the average of the five rounds .
The performance on 79 term dataset of all these methods are shown in Tables 3,4,5 . Compared with the above methods , our proposed method has the best performance by all metrics . In addition , we compared the performance of method and the other four multi modal methods on each annotation term in Figure 7 . Due to the limitation of space , we only show the the Micro F1 score and Macro F1 score . We can see our method performs better than the others on almost all the terms . Figure 5 shows the convergence of our proposed method on lateral , dorsal and ventral dataset . Our algorithm needs about 15 20 iterations to converge . It converges reasonably fast . Moreover , for the large scale dataset , we can parallelize Alg . 1 to speed up it .
To identify the discriminative modality and the representative features , we visualize the weight matrix W . Due to the limitation of the space , we only plot the first 10 columns of weight matrix W from lateral view , such as shown in Figure 6 . We can see that a lot of elements approach to zero , contributing to sparsity . Additionally , some elements have large values , such elements correspond to the representative features . Note that the elements restricted in the red rectangle are almost zero , which means the corresponding modality may be less discriminative for the annotation terms . Thus , we visualize each kind of features of the weight matrix W in Figure 9 . We can see that the elements in CENTRIST are almost zero , only a few elements have large values , which means this modality is less discriminative . But there are still some elements with large values , meaning that such features are representative for annotation although its modality is less discriminative . The other five kinds of features all have considerable large values , meaning that they are discriminative for the annotation task .
246810500100015002000−08−06−04−0200204346 ( a ) Micro F1 Score
Figure 7 : Micro F1 score and Macro F1 score of five methods performed on lateral views about each annotation terms
( b ) Macro F1 Score
Figure 8 : The map between annotation terms and the selected multi dimensional features . The selected image has five annotation terms and the features with non zero weights associated to each individual task in all six visual descriptors are plotted . Different annotation terms need different features for classifications . The data integration can enhance the anatomical term annotations .
00102030405060708Malpighianfitubule…anteriorfiendoderm…anteriorfiendoderm…apicallyficlearedbrainfiprimordiumclypeo(cid:882)labral…dorsalfiectoderm…dorsalfiprothoracic…embryonicfianalfipadembryonicficentral…embryonicfiepipharynxembryonicfiforegutembryonicfihead…embryonic…embryonicfimidgutembryonicfisalivary…embryonicfiventral…embryonic/larval…embryonic/larval…embryonic/larval…foregutfianlagegermficellheadfimesoderm…headfimesoderm…hindgutfiproper…mesectoderm…plasmatocytesfianlageposteriorfiendoderm…posteriorfiendoderm…procephalic…procephalic…rapidlyfidegradedsalivaryfigland…trachealfiprimordiumtrunkfimesoderm…ventralfiectoderm…ventralfiepidermis…ventralfinerveficordventralfinerveficord…visceralfimuscle…CCAKNNLSSVMOur001020304050607Malpighianfitubulefi…anteriorfiendodermfi…anteriorfiendodermfi…apicallyficlearedbrainfiprimordiumclypeo(cid:882)labralfi…dorsalfiectodermfi…dorsalfiprothoracicfi…embryonicfianalfipadembryonicficentralfi…embryonicfiepipharynxembryonicfiforegutembryonicfiheadfi…embryonicfi…embryonicfimidgutembryonicfisalivaryfi…embryonicfiventralfi…embryonic/larvalfi…embryonic/larvalfi…embryonic/larvalfi…foregutfianlagegermficellheadfimesodermfi…headfimesodermfi…hindgutfiproperfi…mesectodermfi…plasmatocytesfianlageposteriorfiendodermfi…posteriorfiendodermfi…procephalicfi…procephalicfi…rapidlyfidegradedsalivaryfiglandfi…trachealfiprimordiumtrunkfimesodermfi…ventralfiectodermfi…ventralfiepidermisfi…ventralfinerveficordventralfinerveficordfi…visceralfimusclefi…CCAKNNLSSVMOurCENTRISTColor MomentGISTLBPHOGSIFTEmbryo Imageprocephalic ectoderm anlage in statu nascendisubsetAnnotation Terms:cellular blastodermdorsal ectoderm anlage in statu nascendiventral ectoderm anlage in statu nascendiImage descriptor347 ( a ) CENTRIST
( b ) ColorMoment
( c ) GIST
( d ) LBP
( e ) HOG
( f ) DenseSIFT
Figure 9 : The map of the first 10 columns ( corresponding to the first 10 annotation terms ) of each modality in matrix W from lateral view .
[ 9 ] S . Ji , L . Tang , S . Yu , and J . Ye . Extracting shared
[ 20 ] G . Rubin and E . Lewis . A brief history of Drosophila ’s subspace for multi label classification . In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining , pages 381–389 . ACM , 2008 .
[ 10 ] S . Ji , L . Yuan , Y . Li , Z . Zhou , S . Kumar , and J . Ye . Drosophila gene expression pattern annotation using sparse features and term term interactions . In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining , pages 407–416 , 2009 .
[ 11 ] S . Kumar , K . Jayaraman , S . Panchanathan ,
R . Gurunathan , A . Marti Subirana , and S . Newfeld . BEST : A novel computational approach for comparing gene expression patterns from early stages of Drosophila melanogaster development . Genetics , 162:2037–2047 , 2002 .
[ 12 ] E . L’ecuyer , H . Yoshida , N . Parthasarathy , and et al .
Global analysis of mRNA localization reveals a prominent role in organizing cellular architecture and function . Cell , 131:174–187 , 2007 .
[ 13 ] D . Lowe . Distinctive image features from scale invariant keypoints . International journal of computer vision , 60(2):91–110 , 2004 . contributions to genome research . Science , 287:2216–2218 , 2000 .
[ 21 ] P . Tomancak , A . Beaton , R . Weiszmann , E . Kwan ,
S . Shu , S . E . Lewis , S . Richards , M . Ashburner , V . Hartenstein , S . E . Celniker , et al . Systematic determination of patterns of gene expression during drosophila embryogenesis . Genome Biol , 3(12):0081–0088 , 2002 .
[ 22 ] G . Tsoumakas and I . Vlahavas . Random k labelsets : An ensemble method for multilabel classification . In Machine Learning : ECML 2007 , pages 406–417 . Springer , 2007 .
[ 23 ] H . Wan , A . DiAntonio , R . Fetter , and et al . Highwire regulates synaptic growth in Drosophila . Neuron , 26:313–329 , 2002 .
[ 24 ] H . Wang , F . Nie , and H . Huang . Heterogeneous visual features fusion via sparse multimodal machine . IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ) , pages 3095–3100 , 2013 .
[ 25 ] H . Wang , F . Nie , and H . Huang . Multi view clustering and feature learning via structured sparsity . The 30th International Conference on Machine Learning ( ICML 2013 ) , pages 352–360 , 2013 .
[ 14 ] F . Nie , H . Huang , X . Cai , and C . Ding . Efficient and
[ 26 ] H . Wang , F . Nie , H . Huang , S . L . Risacher , A . J . robust feature selection via joint l2,1 norms minimization . NIPS , 2010 .
[ 15 ] T . Ojala , M . Pietikainen , and T . Maenpaa .
Multiresolution gray scale and rotation invariant texture classification with local binary patterns . Pattern Analysis and Machine Intelligence , IEEE Transactions on , 24(7):971–987 , 2002 .
Saykin , L . Shen , and ADNI . Identifying disease sensitive and quantitative trait relevant biomarkers from multi dimensional heterogeneous imaging genetics data via sparse multi modal multi task learning . Bioinformatics [ 20th Annual International Conference on Intelligent Systems for Molecular Biology ( ISMB) ] , 28(12):i127–i136 , 2012 .
[ 16 ] A . Oliva and A . Torralba . Modeling the shape of the
[ 27 ] J . Wu and J . M . Rehg . Where am i : Place instance scene : A holistic representation of the spatial envelope . International journal of computer vision , 42(3):145–175 , 2001 .
[ 17 ] H . Peng and E . W . Myers . Comparing in situ mRNA expression patterns of drosophila embryos . RECOMB , pages 157–166 , 2004 .
[ 18 ] D . M . Powers . Evaluation : from precision , recall and f measure to roc , informedness , markedness and correlation . 2011 .
[ 19 ] D . B . Roberts et al . Drosophila : a practical approach .
IRL press , 1986 . and category recognition using spatial pact . In Computer Vision and Pattern Recognition , 2008 . CVPR 2008 . IEEE Conference on , pages 1–8 . IEEE , 2008 .
[ 28 ] H . Yu , M . Li , H J Zhang , and J . Feng . Color texture moments for content based image retrieval . In Image Processing . 2002 . Proceedings . 2002 International Conference on , volume 3 , pages 929–932 . IEEE , 2002 .
−08−06−04−0200204 −08−06−04−0200204 −08−06−04−0200204 −08−06−04−0200204 −08−06−04−0200204 −08−06−04−0200204348
