Diversifying Restricted Boltzmann Machine for Document
Modeling
Pengtao Xie
School of Computer Science Carnegie Mellon University
Pittsburgh , PA , 15213 pengtaox@cscmuedu
Yuntian Deng
School of Computer Science Carnegie Mellon University
Pittsburgh , PA , 15213 yuntiand@cscmuedu
Eric P . Xing
School of Computer Science Carnegie Mellon University
Pittsburgh , PA , 15213 epxing@cscmuedu
ABSTRACT Restricted Boltzmann Machine ( RBM ) has shown great effectiveness in document modeling . It utilizes hidden units to discover the latent topics and can learn compact semantic representations for documents which greatly facilitate document retrieval , clustering and classification . The popularity ( or frequency ) of topics in text corpora usually follow a power law distribution where a few dominant topics occur very frequently while most topics ( in the long tail region ) have low probabilities . Due to this imbalance , RBM tends to learn multiple redundant hidden units to best represent dominant topics and ignore those in the long tail region , which renders the learned representations to be redundant and non informative . To solve this problem , we propose Diversified RBM ( DRBM ) which diversifies the hidden units , to make them cover not only the dominant topics , but also those in the long tail region . We define a diversity metric and use it as a regularizer to encourage the hidden units to be diverse . Since the diversity metric is hard to optimize directly , we instead optimize its lower bound and prove that maximizing the lower bound with projected gradient ascent can increase this diversity metric . Experiments on document retrieval and clustering demonstrate that with diversification , the document modeling power of DRBM can be greatly improved .
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications , Data Mining
General Terms Algorithms , Experiments
Keywords Diversified Restricted Boltzmann Machine , Diversity , Powerlaw Distribution , Document Modeling , Topic Modeling
1 .
INTRODUCTION
Learning lower dimensional semantic representations [ 14 , 3 , 13 , 25 , 18 , 26 , 8 , 9 , 11 , 15 , 19 , 7 ] for documents is essential for many tasks such as document retrieval , clustering and classification , to name a few . It is assumed that there exist a set of hidden topics underlying the observed words and each document has a representation vector in the latent topic space . In general , the latent topic space is able to capture more compact and semantically meaningful characteristics of the documents than the observed word space . Performing text mining and natural language understanding tasks on the learned semantic representations can yield better performance than on the words based representations .
Restricted Boltzmann Machine ( RBM ) [ 24 , 13 , 26 ] has been successfully used for document representation learning . RBM is a two layer undirected graphical model where one layer of hidden units are used to capture latent topics and one layer of visible units are used to represent observed words . The correlation between hidden units and visible units are modeled with undirected weighted edges . Each hidden unit is characterized by a weight vector ( to be learned ) where each component in the vector corresponds to a word in the vocabulary . Compared with directed topic models such as Probabilistic Latent Semantic Analysis ( PLSA ) [ 14 ] and Latent Dirichlet Allocation ( LDA ) [ 3 ] , RBM is very efficient in inferring the latent representations of documents , which is crucial for applications having real time requirements such as retrieval . And RBM has been demonstrated to perform more accurately than LDA in document retrieval and classification [ 13 ] .
In most ( if not all ) document collections , the popularity of topics usually follows a power law distribution . A few dominant topics appear with very high frequency while most topics are of low probabilities . For example , in a news corpus , topics like politics , economics , sports occur a lot while topics like garden , furniture , poem are rarely mentioned . Due to this skewness of topic popularity , RBM tends to learn multiple redundant hidden units to cover dominant topics as best as possible and pay little attention to topics in the long tail region . Failing to capture long tail topics incurs significant information loss . Though the probability of each long tail topic is low , the total probability mass of the longtail region is large because the number of topics in this region is large . These long tail topics with such a large total probability mass are of equal importance with , if not more importance than , the few dominant topics . Besides , the latent representations learned by RBM are redundant in that many dimensions are actually of the same or similar meaning
1315 and the resultant high dimensionality incurs computational inefficiency .
In this paper , we aim to address this problem by proposing to enhance the diversity of the hidden units in RBM to make them cover as many topics as possible , not only the dominant ones , but also those in the long tail region . To combat the phenomenon that many redundant hidden units are learned to characterize the dominant topics as best as possible with the price of ignoring long tail topics , we impose a diversity regularizer over these hidden units to reduce their redundancy and improve their coverage of long tail topics . We define a diversity metric to formally measure how diverse a set of hidden units are : given the weight vectors A where each column in A corresponds to one hidden unit , the diversity metric is defined as Ω(A ) = Ψ(A)− Π(A ) , where Ψ(A ) is the mean of the angles between all pairs of weights vectors and Π(A ) is the variance of these angles . A larger Ω(A ) indicates that the weight vectors in A are more diverse . This diversity metric is defined with the rationale that a set of weight vectors possess a larger diversity if the pairwise angles between them have a larger mean Ψ(A ) and a smaller variance Π(A ) . A larger mean implies that these vectors share larger angles on the whole , hence are more different from each other . A smaller variance indicates that these vectors uniformly spread out to different directions and each vector is evenly different from all other vectors . We employ this diversity metric to regularize the learning of RBM to encourage the hidden units to be diverse . Considering that the diversity metric is hard to optimize , we seek a lower bound of it which is much more amenable for optimization . We prove that maximizing the lower bound with projected gradient ascent can increase the diversity metric . Experiments on three datasets demonstrate that with diversification , RBM can learn much more powerful latent document representations that boost the performance of document retrieval and clustering greatly .
The major contributions of this paper are : • We propose the problem of diversifying Restricted Boltzmann Machine to make the learned hidden units not only to cover dominant topics , but also to capture longtail topics effectively .
• We define a diversity metric to measure how diverse the hidden units are and use it to regularize RBM to encourage diversity .
• To make optimization easier , we derive a lower bound of the diversity metric and show that maximizing the lower bound with projected gradient ascent can increase the diversity metric .
• On two tasks — document retrieval and clustering — and three datasets , we empirically demonstrate that through diversification , RBM can learn much more effective document representations .
The rest of the paper is organized as follows . In Section 2 , we review related works . In Section 3 , we propose the Diversified RBM ( DRBM ) . Section 4 gives experimental results and Section 5 concludes the paper .
2 . RELATED WORKS
Document modeling aims to discover the latent semantics underlying document corpora and learn representations for documents in the latent semantic space . Directed topic models such as Probabilistic Latent Semantic Analysis ( PLSA ) [ 14 ] and Latent Dirichlet Allocation ( LDA ) [ 3 ] represent each topic with a multinomial distribution over the words in the vocabulary and assume each document has a multinomial distribution over topics . The observed words are generated from the latent topics in a hierarchical way . In directed topic models , inferring documents’ distributions over latent topics usually involve iterative optimization or sampling , which is time consuming and limits the applicability of these models in real time applications such as document retrieval .
Restricted Boltzmann Machine ( RBM ) [ 13 , 26 ] has shown great success in document modeling . [ 13 ] proposed a Replicated Softmax RBM to model word counts . [ 26 ] introduced Deep Boltzmann Machine to extract distributed semantic representations for documents . Another paradigm of works are based on neural network ( NN ) [ 2 , 25 , 18 , 20 , 8 , 9 , 11 , 15 , 19 , 7 ] , which aim to learn word/document representations that are able to capture the latent semantics . Compared with directed topic models [ 14 , 3 ] , RBM and NN based methods enable fast inference of the latent representations , which is key to real time applications . Our work is built on top of the existing RBM methods , with a special focus on how to capture low frequency topics/semantics . It is worth noting that the techniques developed in this paper can be also applied to NN based approaches .
There have been several works [ 21 , 22 , 27 , 1 ] attempting to capture the power law behavior of topic popularity , using priors that encourage the distributions over topics to be long tailed , such as Pitman Yor Process prior [ 21 , 22 ] , asymmetric Dirichlet prior [ 27 ] and Indian Buffet Process compound Dirichlet Process prior [ 1 ] . These methods have no regularization over topics’ distributions over words . While these priors encourage new topics to be generated , the newly generated topics may be alike to the existing ones used to model dominant topics . In the end , the learned topics are still redundant and unable to capture the long tail topics effectively . Notably , [ 17 ] studied the diversification of Latent Dirichlet Allocation with the goal to learn diverse topics , by imposing a Determinantal Point Process ( DPP ) [ 16 ] prior over the topics word multinomial vectors . The DPP prior allows one to specify a preference for diversity using a positive definite kernel function . In this paper , we study the diversification of RBM and propose an alternative diversity measure that has properties complementary to the DPP prior , such as ( 1 ) invariant to scaling of the weight vectors ; ( 2 ) encouraging the weight vectors to evenly spread out .
3 . DIVERSIFY RESTRICTED BOLTZMANN
MACHINE
In this section , we propose to diversify the hidden units in RBM to reduce their redundancy and improve the coverage of long tail topics . 3.1 Restricted Boltzmann Machine
Figure 1 shows the basic Restricted Boltzmann Machine ( RBM ) . RBM [ 24 ] is an undirected graphical model consisting of a set of hidden units h = {hk}K k=1 and a set of visible units v = {vj}J j=1 . Both hk and vj are assumed to be binary . Each hidden unit is connected with all visible units with undirected edges and there is no connection between two hidden units or two visible units . The energy function
1316 i=1
K
K j=1(θ(ai , aj ) − Ψ(A))2 is the variance of these 1 K2 angles . A larger Ω(A ) indicates that the weight vectors in A are more diverse . Intuitively , the set of weight vectors possess a larger diversity if the pairwise angles between them have a larger mean and a smaller variance . A larger mean implies that these vectors share larger angles on the whole , hence are more different from each other . A smaller variance indicates that these vectors uniformly spread out to different directions and each vector is evenly different from all other vectors . Encouraging the variance to be small can prevent the phenomenon that the vectors fall into several groups where vectors in the same group have small angles and vectors between groups have large angles . Such a phenomenon renders the vectors to be redundant and less diverse , and hence should be prohibited . Throughout the paper , we assume the weight vectors are linearly independent . Later we present a justification of the validity of this assumption .
To diversify the hidden units in RBM , we use the diversity metric described above to regularize the learning of the weight vectors and define a Diversified RBM ( DRBM ) problem as
( P1 ) maxA L({Vn}N n=1 ; A , α , β ) + λΩ(A )
( 4 ) where λ > 0 is a tradeoff parameter between the data likelihood and the diversity regularizer . The term λΩ(A ) in the new objective function encourages the weight vectors in A to be diverse . λ plays an important role in balancing the fitness of A to the data likelihood and its diversity . Under a small λ , A is learned to best maximize the data likelihood and its diversity is ignored . As discussed earlier , such a A has high redundancy and may not be able to cover long tail topics effectively . Under a large λ , A is learned with high diversity , but may not be well fitted to the data likelihood and hence lose the capability to properly model documents . To sum up , a proper λ needs to be chosen to achieve the optimal balance . 3.3 Optimization
Accordingly , ( P1 ) can be reformulated as vector and gi denotes the L2 norm of the ith column of A ,
In this section , we study how to solve the problem ( P1 ) defined in Eq ( 4 ) . For the ease of optimization , we first reformulate this problem . Let A = diag(g)A , where g is a then the L2 norm of each column vector ˜a in A is 1 . Based on the definition of the diversity metric , we have Ω(A ) = Ω(A ) . n=1 ; diag(g)A , α , β ) + λΩ(A ) ( P2 ) maxA,g L({Vn}N ( P2 ) can be solved by alternating between g and A : optimizing g with A fixed and optimizing A with g fixed . With A fixed , the problem defined over g is
∀i = 1,··· , K,˜ai = 1 st
( 5 ) maxg L({Vn}N n=1 ; diag(g)A , α , β )
( 6 ) which can be efficiently solved with contrastive divergence ( CD ) based gradient ascent method . Fixing g , the problem defined over A is non smooth and non convex , which is hard to solve . Now we focus on how to tackle it . As discussed earlier , the data likelihood of RBM is usually maximized with gradient ascent ( GA ) method . To be consistent , it is desirable to optimize the diversity regularizer with projected1
1Projection is needed due to the constraints in ( P2 ) .
Figure 1 : Restricted Boltzmann Machine defined over h and v is
E(h , v ) = − J
αjvj − K
βkhk − J
K
Ajkvjhk
( 1 ) j=1 k=1 j=1 k=1 j=1 and β = {βj}J where α = {αj}J j=1 are the biases associated with the visible and hidden units respectively . A are the weights on the edges connecting two set of units . α , β and A are model parameters . To better model word counts , [ 13 ] proposed Replicated Softmax RBM . Let V be a D × J observed binary matrix of a document containing D tokens . J is the vocabulary size . Row i of V is the 1 of J coding vector of the ith token in this document . Vij = 1 if the ith token is the jth word in the vocabulary . Under this representation , the energy function E(h , V ) is defined as αjVij − D
βkhk − D
− D
AjkVijhk
K
J
K
J
( 2 ) i=1 j=1 k=1 i=1 j=1 k=1
Given the observed tokens V , inferring the latent representation h can be done very efficiently
D
J p(hk = 1|V ) = σ(Dβk +
AjkVij )
( 3 ) i=1 j=1 where σ(x ) = 1/(1 + exp(−x ) ) is the logistic function . The model parameters can be learned by maximizing the data likelihood L({Vn}N n=1 ; A , α , β ) using the contrastive divergence [ 12 ] method , which is essentially a gradient ascent approach . 3.2 Diversify Restricted Boltzmann Machine The first step towards diversifying Restricted Boltzmann Machine is to measure the diversity of the hidden units . In RBM , each hidden unit possesses a weight vector where the weights are associated with the edges connecting this hidden unit and the visible units . These weight vectors are the parameters to be learned in RBM . Given the weight vectors A = [ a1,··· , aK ] of K hidden units , where each column in matrix A corresponds to one hidden unit , their diversity can be informally described as how different each vector is from others . While there are many ways to measure the difference between vectors ai and aj , we prefer to use the angle between them since the angle is invariant to translation and scaling ( with positive factors ) of the two vectors . In addition , we do not care about the orientation of vectors , thus preferring the angle to be acute or right . If the angle θ is obtuse , we replace it with π − θ . To sum up , we define the angle θ(ai , aj ) between vector ai and aj to be |ai·aj| aiaj ) . Given a set of weight vectors θ(ai , aj ) = arccos( A , we define the diversity metric as Ω(A ) = Ψ(A ) − Π(A ) , where Ψ(A ) = 1 j=1 θ(ai , aj ) is the mean of the K2 angles between all pairs of weights vectors and Π(A ) =
K
K i=1
1h1v2v3vJv……2hKh1317 given in Lemma 12 . arcsin( global optimal . det(ATA))−( π
2 −arcsin( and non convex , ( sub)gradient methods are not applicable . gradient ascent ( PGA ) as well . Since Ω(A ) is non smooth To solve this problem , we derive a smooth lower bound Γ(A ) of Ω(A ) and require Γ(A ) to have the following two traits : 1 ) the gradient of Γ(A ) is easy to compute ; 2 ) optimizing Γ(A ) with PGA can increase Ω(A ) . The lower bound is Lemma 1 . Let det(ATA ) denote the determinant of the Gram matrix of A , then 0 < det(ATA ) ≤ 1 . Let Γ(A ) = det(ATA)))2 , then Γ(A ) is a lower bound of Ω(A ) . Γ(A ) and Ω(A ) have the same Next we prove that maximizing Γ(A ) using PGA can increase the diversity metric Ω(A ) . The statement is formally Theorem 1 . Let G(t ) be the gradient of Γ(A ) wrt A(t ) at iteration t . ∃τ > 0 , such that ∀η ∈ ( 0 , τ ) , Ω(A(t+1 ) ) ≥ Ω(A(t) ) , where A(t+1 ) = P(A(t ) + ηG(t ) ) and P(· ) denotes Note that Ω(A ) consists of two parts Ψ(A ) and Π(A ) . To prove Theorem 1 , we prove that maximizing Γ(A ) using PGA can increase the mean Ψ(A ) and reduce the variance Π(A ) simultaneously , which are stated in Theorem 2 and 3 . Theorem 2 . Let G(t ) be the gradient of Γ(A ) wrt A(t ) at iteration t . ∃τ1 > 0 , such that ∀η ∈ ( 0 , τ1 ) , Ψ(A(t+1 ) ) ≥ Ψ(A(t) ) , where A(t+1 ) = P(A(t ) + ηG(t) ) . Theorem 3 . Let G(t ) be the gradient of Γ(AT ) wrt A(t ) at iteration t . ∃τ2 > 0 , such that ∀η ∈ ( 0 , τ2 ) , Π(A(t+1 ) ) ≤ Π(A(t) ) , where A(t+1 ) = P(A(t ) + ηG(t) ) . the projection to the unit sphere . described in Theorem 1 .
These two theorems immediately imply Theorem 1 . To prove Theorem 2 , the following lemma is needed . decomposed into ˜ai = xi + liei , where xi = K li is a scalar . Then the gradient of Γ(A ) wrt ai is kiei ,
Lemma 2 . Let the weight vector ˜ai of hidden unit i be j=1,j=i αj˜aj lies in the subspace L spanned by {˜a1,··· , ˜aK}\{˜ai} , ei is in the orthogonal complement of L , ei = 1 , ei · ˜ai > 0 , where ki is a positive scalar .
Given Lemma 2 , we can justify why the weight vectors can be always linearly independent during the projected gradient ascent procedure . First , we assume they are initialized to be linearly independent . From Lemma 2 , we know that the gradient direction of each weight vector ai is orthogonal to all other weight vectors . So moving along such a direction prevents each vector to fall into the span of all other vectors . And projecting the vectors onto the unit sphere does not change the directions of these vectors . Thereby , these weight vectors would be always linearly independent .
The proofs of Lemma 2 and Theorem 2 are presented in Appendix B and C respectively . To prove Theorem 3 , we need the following lemma :
2The proof of Lemma 1 is given in Appendix A .
#categories #samples vocab . size
TDT
20 News Reuters
30 20 9
9394 18846 7195
5000 5000 5000
Table 1 : Statistics of Datasets
K
25 RBM 11.2 DRBM 78.4
50 11.4 84.2
100 11.9 78.6
200 12.1 79.9
500 47.4 77.6
Table 2 : Precision@100 on TDT dataset i=1
Lemma 3 . Given a nondecreasing sequence b = ( bi)n and a strictly decreasing function g(x ) which satisfies 0 ≤ g(bi ) ≤ min{bi+1 − bi : i = 1 , 2,··· , n − 1 , bi+1 = bi} , we define a sequence c = ( ci)n i=1 where ci = bi + g(bi ) . If b1 < bn , then var(c ) < var(b ) , where var(· ) denotes the variance of a sequence . Furthermore , let n = max{j : bj = bn} , we define a sequence b = ( b i=1 where b i)n i = bi + g(bn ) + ( g(bn ) − g(bn))I(i ≤ n ) and I(· ) is the indicator function , then var(c ) ≤ var(b ) < var(b ) .
The proofs of Lemma 3 and Theorem 3 are given in Appendix D and E respectively .
4 . EXPERIMENTS
In this section , we present experimental results on two tasks and on three datasets , which demonstrate that with diversification , DRBM can learn much more effective representations than RBM . 4.1 Experimental Setup
Three datasets were used in the experiments . The first one [ 5 ] is a subset of the Nist Topic Detection and Tracking ( TDT ) corpus which contains 9394 documents from the largest 30 categories . 70 % documents were used for training and 30 % were used for testing . The second dataset is the 20 Newsgroups ( 20 News ) , which contains 18846 documents from 20 categories . 60 % documents were used for training and 40 % were used for testing . The third dataset [ 4 ] is the Reuters 21578 ( Reuters ) dataset . Categories with less than 100 documents were removed , which left us 9 categories and 7195 documents . 70 % documents were used for training and 30 % were used for testing . Each dataset used a vocabulary of 5000 words with the largest document frequency . Table 1 summarizes the statistics of three datasets .
We used gradient methods to train RBM [ 13 ] and DRBM . The mini batch size was set to 100 . The learning rate was set to 1e 4 . The number of gradient ascent iterations was set to 1000 and the number of Gibbs sampling iterations in contrastive divergence was fixed to 1 . The tradeoff parameter λ in DRBM was tuned with 5 fold cross validation . We compared with the following baselines methods : ( 1 ) bag of words ( BOW ) ; ( 2 ) Latent Dirichlet Allocation ( LDA ) [ 3 ] ; ( 3 ) LDA regularized with Determinantal Point Process prior ( DPP LDA ) [ 17 ] ; ( 4 ) Pitman Yor Process Topic Model ( PYTM ) [ 22 ] ; ( 5 ) Latent IBP Compound Dirichlet Allocation ( LIDA ) [ 1 ] ; ( 6 ) Neural Autoregressive Topic Model ( DocNADE ) [ 18 ] ; ( 7 ) Paragraph Vector ( PV ) [ 19 ] ; ( 8 ) Restricted Boltzmann Machine [ 13 ] . The parameters in baseline methods were tuned using 5 fold cross validation .
1318 Category ID
1
2
3
4
5
6
7
8
9
Number of Documents
3713 Precision@100 ( % ) of RBM 68.5 Precision@100 ( % ) of DRBM 89.7
Relative Improvement
2055 44.4 80.2
110 2.6 12.9 31 % 81 % 245 % 289 % 324 % 421 % 148 % 366 % 397 %
298 10.1 39.5
114 3.0 14.0
321 9.1 31.3
245 6.3 26.5
197 4.4 22.7
142 3.8 9.4
Table 5 : Precision@100 on each category in Reuters dataset
K
25 6
RBM DRBM 14.5
50 6.1 24.9
100 5.7 15.4
200 9.2 20.3
500 22.3 21.1
Table 3 : Precision@100 on 20 News dataset
K
25 RBM 37.7 DRBM 67.8
50 38.1 73.3
100 50.1 75.9
200 64.0 70.3
500 70.1 66.2
Table 4 : Precision@100 on Reuters dataset
4.2 Document Retrieval
In this section , we evaluate the effectiveness of the learned representations on retrieval . Precision@100 is used to evaluate the retrieval performance . For each test document , we retrieve 100 documents from the training set that have the smallest Euclidean distance with the query document . The distance is computed on the learned representations . Precision@100 is defined as n/100 , where n is the number of retrieved documents that share the same class label with the query document .
Table 2 , 3 and 4 show the precision@100 under different number K of hidden units on TDT , 20 News and Reuters dataset respectively . As can be seen from these tables , DRBM with diversity regularization largely outperforms RBM which has no diversity regularization under various choices of K , which demonstrates that diversifying the hidden units can greatly improve the effectiveness of document representation learning . The improvement is especially significant when K is small . For example , on TDT dataset , under K = 25 , DRBM improves the precision@100 from 11.2 % to 784 % Under a small K , RBM allocates most ( if not all ) hidden units to cover dominant topics , thus long tail topics have little chance to be modeled effectively . DRBM solves this problem by increasing diversity of these hidden units to enforce them to cover not only the dominant topics , but also the long tail topics . Thereby , the learned representations are more effective in capturing the long tail semantics and the retrieval performance is greatly improved . As K increases , the performance of RBM increases . This is because under a larger K , some hidden units can be spared to model topics in the long tail region . In this case , enforcing diversity still improves performance , though the significance of improvement diminishes as K increases .
To further examine whether DRBM can effectively capture the long tail semantics , we show the precision@100 on each of the 9 categories in Reuters dataset in Table 5 . The 2nd row shows the number of documents in each category . The distribution of document frequency is in a power law fashion , where dominant categories ( such as 1 and 2 ) have a lot of documents while most categories ( called long tail categories ) have a small amount of documents . The 3rd and 4th row show the precision@100 achieved by RBM and
BOW
LDA [ 3 ]
DPP LDA [ 17 ]
PYTM [ 22 ]
LIDA [ 1 ]
DocNADE [ 18 ]
PV [ 19 ] RBM [ 13 ]
DRBM
TDT 20 News Reuters 40.9 79.4 81.9 78.7 77.9 80.3 81.7 47.4 84.2
69.3 68.5 69.9 70.6 71.4 72.6 76.9 70.1 75.9
7.4 19.6 18.2 20.1 21.8 16.8 19.1 22.3 24.9
Table 6 : Precision@100 on three datasets
K
25 RBM 19.7 DRBM 52.4
50 19.1 46.2
100 14.4 46.5
200 13.0 41.4
500 23.3 39.5
Table 7 : Clustering accuracy ( % ) on TDT test set
Prbm
DRBM on each category . The 5th row shows the relative improvement of DRBM over RBM . The relative improvement is defined as Pdrbm−Prbm , where Pdrbm and Prbm denote the precision@100 achieved by DRBM and RBM respectively . While DRBM improves RBM over all the categories , the improvements on long tail categories are much more significant than dominant categories . For example , the relative improvements on category 8 and 9 are 366 % and 397 % while the improvements are 31 % and 81 % on category 1 and 2 . This indicates that DRBM can effectively capture the longtail topics , thereby improve the representations learned for long tail categories significantly .
One great merit of DRBM is that it can achieve notable performance under a small K , which is of key importance for fast retrieval . On the TDT dataset , DRBM can achieve a precision@100 of 78.4 % with K = 25 , which cannot be achieved by RBM even when K is raised to 500 . This indicates that with DRBM , one can perform retrieval on lowdimensional representations , which is usually much easier than on high dimensional representations . For example , in KD tree [ 10 ] based nearest neighbor search , while building a KD tree on feature vectors with hundreds of dimensions is extremely hard , feature vectors whose dimension is less than one hundred are much easier to handle .
Table 6 presents the comparison with the state of the art document representation learning methods . As can be seen from this table , our method achieves the best performances on the TDT and 20 News datasets and achieves the second best performance on the Reuters dataset . The bagof word ( BOW ) representation cannot capture the underlying semantics of documents , thus its performance is inferior . LDA , RBM and neural network based approaches including DocNADE [ 18 ] and PV [ 19 ] can represent documents into the latent topic space where document retrieval can be performed more accurately . However , they lack the mechanisms
1319 K
25 RBM 6.4 DRBM 18.2
50 6.8 29.4
100 21.5 19.8
200 12.7 25.9
500 22.7 25.6
Table 8 : Clustering accuracy ( % ) on 20 News test set
K
25 RBM 45.0 DRBM 51.4
50 41.7 58.6
100 38.4 60.9
200 46.8 53.4
500 47.6 48.5
BOW
LDA [ 3 ]
DPP LDA [ 17 ]
PYTM [ 22 ]
LIDA [ 1 ]
DocNADE [ 18 ]
PV [ 19 ] RBM [ 13 ]
DRBM
TDT 20 News Reuters 51.3 45.2 46.3 46.9 47.3 45.7 48.2 23.3 52.4
49.7 51.2 49.3 51.7 53.1 48.7 52.8 47.6 60.9
21.3 21.9 10.9 21.5 17.4 18.7 24.3 22.7 29.4
Table 9 : Clustering accuracy ( % ) on Reuters test set
Table 10 : Clustering accuracy ( % ) on three datasets to cover long tail topics , hence the resultant representations are less effective . PYTM [ 22 ] and LIDA [ 1 ] use power law priors to encourage new topics to be generated , however , the newly generated topics may still be used to model the dominant semantics rather than those in the long tail region . DPP LDA [ 17 ] uses a correlation kernel Determinantal Point Process ( DPP ) prior to diversify the topics in LDA . However , it does not improve LDA too much . 4.3 Clustering
Another task we study is to do k means clustering on the learned representations . In our experiments , the input cluster number of k means was set to the ground truth number of categories in each dataset . In each run , k means was repeated 10 times with different random initializations and the solution with lowest loss value was returned . Following [ 6 ] , we used accuracy to measure the clustering performance . Please refer to [ 6 ] for the definition of accuracy . Table 7 , 8 and 9 show the clustering accuracy on TDT , 20 News and Reuters test set respectively under different number K of hidden units .
As can be seen from these tables , with diversification , DRBM achieves significantly better clustering accuracy than the standard RBM . On TDT test data , the best accuracy of RBM is 233 % DRBM dramatically improves the accuracy to 524 % On Reuters test set , the best accuracy achieved by DRBM is 60.9 % , which is largely better than the 47.6 % accuracy achieved by RBM . The great performance gain achieved by DRBM attributes to the improved effectiveness of the learned representations . RBM lacks the ability to learn hidden units to cover long tail topics , which largely inhibits its ability to learn rich and expressive representations . DRBM uses the diversity metric to regularize the hidden units to enhance their diversity . The learned hidden units under DRBM can not only represent dominant topics effectively , but also cover long tail topics properly . Accordingly , the resultant representations can cover diverse semantics and dramatically improve the performance of kmeans clustering .
DRBM can achieve a high accuracy with a fairly small number of hidden units , which greatly facilitates computational efficiency . For example , on TDT dataset , with 25 hidden units , DRBM can achieve an accuracy of 52.4 % , which cannot be achieved by RBM with even 500 hidden units . The computational complexity of k means is linear to the feature dimension . Thus , on this dataset , with the latent representations learned by DRBM , k means can achieve a significant speed up . Similar observations can be seen from the other two datasets .
K
25
RBM 2602 DRBM 1699
50
2603 1391
100 2606 1658
200 2609 1085
500 2350 859
Table 11 : Perplexity on TDT test set
Table 10 presents the comparison of DRBM with the baseline methods on clustering accuracy . As can be seen from this table , our method consistently outperforms the baselines across all three datasets . The analysis of why DRBM is better than the baseline methods follows that presented in Section 42 4.4 Perplexity on Testing Data
Following [ 13 ] , we computed perplexity on the held out test set to assess the document modeling power of RBM and DRBM . Table 11 , 12 and 13 compare the perplexity of RBM and DRBM computed on TDT , 20 News and Reuters dataset respectively . As can be seen from these tables , DRBM can achieve significant lower ( better ) perplexity than RBM , which corroborates that by diversifying the hidden units , the document modeling power of RBM can be dramatically improved . 4.5 Sensitivity to Parameters
We study the sensitivity of DRBM to the tradeoff parameter λ . Figure 2 shows how precision@100 in retrieval varies as λ increases on the TDT , 20 News and Reuters dataset respectively . The number of hidden units was fixed to 100 . As can be seen from the figures , starting from 0 , increasing λ improves precision@100 . That is because a larger λ induces more diversity of the hidden units , enabling them to better cover long tail topics . However , further increasing λ causes the precision to drop . This is because , if λ is too large , too much emphasis is paid to the diversify regularizer and the data likelihood of RBM is ignored . 4.6 Topic Visualization
Other than evaluating the learned hidden units of DRBM quantitatively , we also visualize and evaluate them in a qualitative way . We can interpret each hidden unit as a topic and its weight vector as a pseudo distribution over the vocabulary . To visualize each hidden unit , we pick up the top 10 representative words which correspond to the ten largest values in the weight vector . Table 14 shows 5 topics learned by RBM and 5 topics learned by DRBM . As can be seen from the table , topics learned by RBM have many nearduplicates and are very redundant . In contrast , the topics learned by DRBM are much more diverse , with a broad cov
1320 Figure 2 : Sensitivity of DRBM to tradeoff parameter λ on ( a ) TDT dataset ( b ) 20 News dataset ( c ) Reuters dataset
Topic 1 president clinton iraq united spkr house people lewinsky government white
Topic 2 iraq united un weapons iraqi nuclear india minister saddam weapons military white
Table 14 : Exemplar topics learned By RBM and DRBM DRBM RBM Topic 3 Topic 3 iraq iraq united un un iraqi weapons lewinsky saddam iraqi baghdad clinton council baghdad inspectors inspectors nations military
Topic 2 Topic 1 olympic president games iraq olympics clinton nagano united team million gold lewinsky game thailand spkr hockey government medal jones winter
Topic 5 spkr voice tobacco olympic games people olympics nagano game gold
Topic 4 olympic games nagano olympics game team gold japan medal hockey
Topic 4 lawyers kaczynski ms defense trial judge people prosecutors kaczynskis government
Topic 5 students japanese japan school ms united yen gm tokyo south
K
25
RBM 764.9 DRBM 713
50
765.1 623
100 765 659
200 741 558
500 633 497
Table 12 : Perplexity on 20 News test set
K
25
RBM 1147 DRBM 1028
50
1129 859
100 1130 746
200 881 734
500 849 848
Table 13 : Perplexity on Reuters test set erage of various topics including American politics , sports , Iraq war , law and Japanese education . This demonstrates the ability of DRBM to learn diverse topics .
5 . CONCLUSIONS
In this paper , we study the problem of diversifying Restricted Boltzmann Machine . Due to the skewed distribution of topic popularity , existing RBM tends to learn redundant hidden units to represent dominant topics with best efforts and fails to learn hidden units to effectively cover long tail topics . To solve this problem , we propose to diversify the hidden units to make them to cover not only dominant topics , but also long tail topics . We define a diversity metric and use it to regularize the learning of the hidden units . Considering the diversity metric is not amenable for optimization , we instead optimize its lower bound . We prove that maximizing the lower bound with projected gradient ascent can increase the diversity metric . Experiments on document retrieval and clustering show that through diversification , the representations learned by DRBM can be greatly improved .
6 . ACKNOWLEDGMENTS
The authors would like to thank the anonymous reviewers for the helpful suggestions . This work is supported by the following grants to Eric P . Xing : ASFOR FA95501010247 ; NSF IIS1111142 , IIS447676 .
7 . REFERENCES [ 1 ] C . Archambeau , B . Lakshminarayanan , and G . Bouchard . Latent ibp compound dirichlet allocation . Pattern Analysis and Machine Intelligence , IEEE Transactions on , 2015 .
[ 2 ] Y . Bengio , H . Schwenk , J S Sen´ecal , F . Morin , and J L Gauvain . Neural probabilistic language models . In Innovations in Machine Learning . Springer , 2006 .
[ 3 ] D . M . Blei , A . Y . Ng , and M . I . Jordan . Latent dirichlet allocation . the Journal of machine Learning research , 2003 .
[ 4 ] D . Cai and X . He . Manifold adaptive experimental design for text categorization . Knowledge and Data Engineering , IEEE Transactions on , 2012 .
[ 5 ] D . Cai , X . He , and J . Han . Document clustering using locality preserving indexing . Knowledge and Data Engineering , IEEE Transactions on , 2005 .
[ 6 ] D . Cai , X . He , and J . Han . Locally consistent concept factorization for document clustering . Knowledge and Data Engineering , IEEE Transactions on , 2011 .
[ 7 ] Z . Cao , S . Li , Y . Liu , W . Li , and H . Ji . A novel neural topic model and its supervised extension . AAAI Conference on Artificial Intelligence , 2015 .
[ 8 ] Z . Chen and B . Liu . Mining topics in documents : standing on the shoulders of big data . In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining . ACM , 2014 .
[ 9 ] M . Denil , A . Demiraj , N . Kalchbrenner , P . Blunsom , and N . de Freitas . Modelling , visualising and summarising documents with a single convolutional neural network . arXiv preprint arXiv:1406.3830 , 2014 .
[ 10 ] J . H . Friedman , J . L . Bentley , and R . A . Finkel . An algorithm for finding best matches in logarithmic expected time . ACM Transactions on Mathematical Software , 1977 .
00001000100101100102030405060708Tradeoff Parameter λPrecision@100(a)000010001001011000400600801012014016Tradeoff Parameter λPrecision@100(b)000010001001011005055060650707508085Tradeoff Parameter λPrecision@100(c)1321 [ 11 ] R . Guha . Towards a model theory for distributed representations . arXiv preprint arXiv:1410.5859 , 2014 . [ 12 ] G . Hinton . Training products of experts by minimizing contrastive divergence . Neural computation , 2002 . [ 13 ] G . E . Hinton and R . R . Salakhutdinov . Replicated softmax : an undirected topic model . In Advances in neural information processing systems , 2009 .
[ 14 ] T . Hofmann . Probabilistic latent semantic analysis . In Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence , 1999 .
[ 15 ] C . Huang , X . Qiu , and X . Huang . Text classification with document embeddings . In Chinese Computational Linguistics and Natural Language Processing Based on Naturally Annotated Big Data . Springer , 2014 . [ 16 ] A . Kulesza and B . Taskar . Determinantal point processes for machine learning . arXiv preprint arXiv:1207.6083 , 2012 .
[ 17 ] J . T . Kwok and R . P . Adams . Priors for diversity in generative latent variable models . In Advances in Neural Information Processing Systems . 2012 .
[ 18 ] H . Larochelle and S . Lauly . A neural autoregressive topic model . In Advances in Neural Information Processing Systems , 2012 .
[ 19 ] Q . V . Le and T . Mikolov . Distributed representations of sentences and documents . International Conference on Machine Learning , 2014 .
[ 20 ] T . Mikolov , I . Sutskever , K . Chen , G . S . Corrado , and
J . Dean . Distributed representations of words and phrases and their compositionality . In Advances in Neural Information Processing Systems , 2013 .
[ 21 ] J . Pitman and M . Yor . The two parameter poisson dirichlet distribution derived from a stable subordinator . The Annals of Probability , 1997 .
[ 22 ] I . Sato and H . Nakagawa . Topic models with power law using pitman yor process . In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining . ACM , 2010 .
[ 23 ] I . R . Shafarevich , A . Remizov , D . P . Kramer , and
L . Nekludova . Linear algebra and geometry . Springer Science & Business Media , 2012 .
[ 24 ] P . Smolensky . Information processing in dynamical systems : Foundations of harmony theory . 1986 .
[ 25 ] R . Socher , C . C . Lin , C . Manning , and A . Y . Ng . Parsing natural scenes and natural language with recursive neural networks . In Proceedings of the 28th international conference on machine learning , 2011 .
[ 26 ] N . Srivastava , R . Salakhutdinov , and G . Hinton .
Modeling documents with a deep boltzmann machine . In Uncertainty in Artificial Intelligence , 2013 .
[ 27 ] Y . Wang , X . Zhao , Z . Sun , H . Yan , L . Wang , Z . Jin ,
L . Wang , Y . Gao , C . Law , and J . Zeng . Peacock : Learning long tail topic features for industrial applications . arXiv preprint arXiv:1405.4402 , 2014 .
APPENDIX A . PROOF OF LEMMA 1
To prove Lemma 1 , the following lemma is needed . decomposed into ˜ai = xi + liei , where xi = K
Lemma 4 . Let the weight vector ˜ai of hidden unit i be j=1,j=i αj˜aj lies in the subspace L spanned by {˜a1,··· , ˜aK}\{˜ai} , ei is fififififififififi fifififififififififififififi fififififififififi fifififififififififififififi
( 8 ) in the orthogonal complement of L , ei = 1 , ei · ˜ai > 0 , li is a scalar . Then det(ATA ) = det(AT−iA−i)(liei · ˜ai ) , where A−i = [ ˜a1,··· , ˜ai−1 , ˜ai+1,··· , ˜aK ] with ˜ai excluded
Proof . Part of the proof follows [ 23 ] .
˜a1 · ˜a1 ˜a2 · ˜a1 det(ATA ) = Let ci denote the ith column of the Gram matrix ATA . Subtractingk
··· ˜a1 · ˜aK ··· ˜a2 · ˜aK . . . ··· ˜aK · ˜aK
··· ˜a1 · ˜ai ··· ˜a2 · ˜ai . . . ··· ˜aK · ˜ai j=1,j=i αjcj from ci [ 23 ] , where αj is the lin
˜aK · ˜a1
( 7 )
˜a1 · ˜a1 ˜a2 · ˜a1 det(ATA ) = ear coefficient in xi , we get ··· ··· . . . ··· . . . ···
˜aK · ˜a1
˜ai · ˜ai
0 0 liei · ˜ai
0
··· ˜a1 · ˜aK ··· ˜a2 · ˜aK . . . ··· . . . ··· ˜aK · ˜aK
˜ai · ˜aK
Now we proceed to prove Lemma 1 .
Expanding the determinant according to the ith column , we the mean and variance of the pairwise angles respectively . We bound the two terms separately . We first bound the between ˜ai and ˜aj is : θ(˜ai , ˜aj ) = arccos(|˜ai · ˜aj| ) = arccos(|xi · ˜aj| )
As ∀j , det(˜aT and apply the inequality repeatedly to draw the conclusion get det(ATA ) = det(AT−iA−i)(liei · ˜ai ) . Proof . The diversity metric Ω(A ) comprises of two terms : Ω(A ) = Ψ(A ) − Π(A ) , in which Ψ(A ) and Π(A ) measure mean Ψ(A ) . Since the weight vectors are assumed to be linearly independent , we have det(ATA ) > 0 . As liei · ˜ai ≤ liei˜ai ≤ 1 and det(ATA ) = det(AT−iA−i)(liei · ˜ai ) ( according to Lemma 4 ) , we have det(ATA ) ≤ det(AT−iA−i ) . j ˜aj ) = 1 , we can eliminate the columns of A−i that det(AT−iA−i ) ≤ 1 ( and det(ATA ) ≤ 1 ) . So liei · ˜ai = liei2 ≥ det(ATA ) . For any j = i , the pairwise angle ≤ arccos(xi˜aj ) = arccos(xi ) = arccos(1 − liei2 ) Thus Ψ(A ) ≥ arcsin( Now we bound the variance Π(A ) . For any i = j , we det(ATA) ) . From Ψ(A ) is the mean value of all pairwise θ(˜ai , ˜aj ) , we have det(ATA ) ) ≤ Ψ(A ) ≤ π 2 . So |θ(˜ai , ˜aj ) − Ψ(A)| ≤ det(ATA) ) . So Π(A ) ≤ ( π det(ATA)))2 . Combining the lower bound of Ψ(A ) and upper bound 2 −arcsin( 2 −arcsin( det(ATA ) ) − of Π(A ) , we have Ω(A ) ≥ Γ(A ) = arcsin( det(ATA)))2 . Both Ω(A ) and Γ(A ) obtain the optimal value of π/2 when the weight vectors in A are 2 − arcsin( ( π have proved that θ(˜ai , ˜aj ) ≥ arcsin( the definition of θ(˜ai , ˜aj ) , we also have θ(˜ai , ˜aj ) ≤ π
1 − det(ATA ) ) = arcsin( det(ATA) ) . det(ATA ) )
≥ arccos(
2 . As arcsin(
( 9 )
π orthogonal to each other . The proof completes .
1322 Proof . Using the Taylor expansion of arccos(x ) at x = x(t ) ij , we have arccos(x(t+1 ) = −
1 ij
1−x
2
( t ) ij
) − arccos(x(t ) ij ) − x(t ) ( x(t+1 ) ij ) + o(x(t+1 ) ij ij
− x(t ) ij )
( 10 )
According to the definition of x(t+1 ) ij
, we have x(t+1 ) ij
= x(t ) ij
√
|1+η2
·e ( t ) ( t ) ki kj e j i ·˜a ( t ) ( t ) j i i η2
˜a
1+2likiη+k2
1+2lj kj η+k2 j η2
|
( 11 )
1
1
1 i η2
√
1+2lj kj η+k2
Using the Taylor expansion of = 1 − 1 that
1√ 2 ( 2likiη + k2 k2 i η2 ) . As η2 = o(η ) and o(2likiη + k2 can obtain that
1+2likiη+k2 i η2 √ 1+2likiη+k2
1+x at x = 0 , we can obtain i η2 ) + o(2likiη + i η2 ) = o(η ) , we = 1 − likiη + o(η ) . Sim= 1 − ljkjη + o(η ) . When η → 0 , j η2 | = 1 . Thereby when η is small enough , |1+ | =
| = 1 + η2 kikj e(t ) i ·˜a(t ) ˜a(t ) i
, so |1 + η2 kikj e(t ) i ·a(t ) a(t ) i 1 + o(η ) . Substituting the above equations to x(t+1 ) ij = x(t ) can obtain that x(t+1 ) o(η))(1 − ljkjη + o(η ) ) − 1 ) = −x(t )
, we ij ( (1 + o(η))(1 − likiη + ij ( liki + ljkj)η + o(η ) . So
|1+η2 kikj e(t ) i ·˜a(t ) ˜a(t ) i ·e(t ) η2 kikj e(t ) i ·˜a(t ) ˜a(t ) i
− x(t )
·e(t )
·e(t )
·e(t ) ij ij j j j j j j j j x
( t+1 ) ij
( t ) ij
−x η lim η→0
= −x(t ) ij ( liki + ljkj ) . As lim η→0 o(x
( t+1 ) ij ( t+1 ) ij
−x −x
( t ) ij ) ( t ) ij x
=
= 0 , we can draw the conclusion lim ij →x
( t+1 ) x
( t ) ij o(x
( t+1 ) ij ( t+1 ) ij
−x −x
( t ) ij ) ( t ) ij x
C . PROOF OF THEOREM 2 ilarly ,
∂˜ai
√
2 − arcsin(
B . PROOF OF LEMMA 2
Proof . According to chain rule , the gradient of Γ(A ) wrt ˜ai can be written as g(det(ATA ) ) ∂ det(ATA ) , where √ x ) − ( π x))2 . It is easy to check g(x ) = arcsin( that g(x ) is an increasing function and g(x ) > 0 . Now we discuss the ∂ det(ATA ) have det(ATA ) = det(AT−iA−i)liei · ˜ai . From this equaterm . According to Lemma 4 , we = det(AT−iA−i)liei . As assumed tion , we have ∂ det(ATA ) earlier , the weight vectors in A are linearly independent and hence det(ATA ) > 0 and det(AT−iA−i ) > 0 . From det(ATA ) = det(AT−iA−i)liei · ˜ai and ei · ˜ai > 0 , we know li > 0 . Overall , the gradient of Γ(A ) wrt ˜ai can be written as kiei , where ki = g(det(ATA ) ) det(AT−iA−i)li > 0 . The
∂˜ai
∂˜ai proof completes . i i i i i i j j j
, ˜a(t )
θ(˜a(t )
· ˜a(t )
· ˜a(t )
, ˜a(t+1 )
= |˜a(t+1 )
( i,j)∈V ( θ(˜a(t+1 )
( i,j)∈N ( θ(˜a(t+1 )
= 0} , where ˜a(t )
)−θ(˜a(t ) ij = |˜a(t ) j ) ) , ∆N =
Let V = {(i , j)|1 ≤ i , j ≤ K , i = j , ˜a(t ) {(i , j)|1 ≤ i , j ≤ K , i = j , ˜a(t ) ith column of A(t ) . Let ∆V = then Ω(A(t+1))−Ω(A(t ) ) = ∆V +∆N . Let x(t )
To prove Theorem 2 , we first introduce some notations . j = 0} , N = is the i ) − , ˜a(t+1 ) , ˜a(t ) j ) ) , j | , ·˜a(t ) | . According to Lemma 2 presented in ·˜a(t+1 ) x(t+1 ) ij ˜a(t ) = ˜a(t ) j +ηkj ej the main paper , ˜a(t+1 ) i +ηkiei , ˜a(t+1 ) j +ηkj ej ˜a(t ) ˜a(t ) and ei · ˜a(t ) i = 0 . According to the definii + ηkiei = tion ˜ai = xi + liei in Lemma 4 , we have ˜a(t ) i η2 ·e(t ) j | The following lemmas are needed for proving Theorem 2 . )−θ(˜a(t )
1 + 2likiη + k2
Lemma 5 . ∀(i , j ) ∈ V , we have θ(˜a(t+1 ) j = 0 , ej · ˜a(t )
√ 1+2likiη+k2 i η2 , and x(t+1 ) j +η2kikj e(t )
, ˜a(t+1 ) i +ηkiei
|˜a(t )
·˜a(t )
=
= ij j j i i i i i i j
= o(η ) , where lim η→0 o(η ) η = 0 . Proof . For ( i , j ) ∈ V , ˜a(t ) i , ˜a(t ) i i j ij
, ˜a(t+1 )
θ(˜a(t+1 ) arccos(x(t+1 )
)−θ(˜a(t ) ) − π Plugging in ˜a(t ) i η2 |η2kikj e(t ) √ 1+2likiη+k2 alently , x(t+1 ) i i ij
·˜a(t ) j = 0 , thereby x(t ) ij = 0 and j ) = arccos(x(t+1 ) ij
)−arccos(x(t ) ij ) =
2 . Now we prove lim η→0 · ˜a(t ) j = 0 into x(t+1 ) ·e(t ) j | 1+2lj kj η+k2 ij j η2
. Thereby lim η→0 arccos(x
( t+1 ) ij η
)− π
2
= 0 .
, we have x(t+1 ) ij
= x
( t+1 ) ij
η = 0 ( equiv
= o(η ) ) and lim η→0 x(t+1 ) ij
= 0 . According to the Taylor expansion of arccos(x ) at x = 0 , arccos(x ) = x(t+1 ) π = 0 , ij = −1 . Since
2 − x + o(x ) , so lim x→0 )− π
= −1 . Since lim η→0 arccos(x)− π
( t+1 ) ij
( t+1 ) ij arccos(x arccos(x
)− π x
2
2
2 lim η→0 x
( t+1 ) ij
= lim ( t+1 ) ij →0 x
1+2lj kj η+k2 j η2 x
( t+1 ) ij lim η→0
η = 0 , we have lim η→0 arccos(x
( t+1 ) ij
)− π
2 x
( t+1 ) ij x
( t+1 ) ij arccos(x
( t+1 ) ij η
)− π
2
= lim η→0 Lemma 6 . ∀(i , j ) ∈ N , ∃cij > 0 , such that θ(˜a(t+1 )
η = 0 . The proof completes .
( t+1 ) ij x
, ˜a(t+1 ) j
) i
− θ(˜a(t ) i
, ˜a(t ) j ) = cijη + o(η ) , where lim η→0 o(η ) η = 0 .
. that lim η→0 o(x
( t+1 ) ij
η
−x
( t ) ij )
= lim η→0 o(x
( t+1 ) ij ( t+1 ) ij
−x −x
( t ) ij ) ( t ) ij x hence o(x(t+1 ) 1 − ij −x(t ) ( x(t+1 ) ij ) = o(η ) . So arccos(x(t+1 ) ij ) = − ij )+o(x(t+1 ) ij −x(t ) ij x
( t+1 ) ij
( t ) ij
−x η
= 0 , )−arccos(x(t ) 1 ( −x(t )
1−x
2
( t ) ij ij ) = ij ( liki+
, ˜a(t ) j ) i
1−x
2
( t ) ij ljkj)η + o(η ) ) + o(η ) =
η + o(η ) . Let cij = ij −x(t )
( t ) ij ( liki+lj kj )
1−x
2
( t ) ij x x
( t ) ij ( liki+lj kj )
1−x
2
( t ) ij
, clearly cij > 0 . The proof completes .
Given these two lemmas , we can prove Theorem 2 now . Proof .
Ψ(A(t+1 ) ) − Ω(A(t ) ) = ∆V + ∆N = = o(η ) + Ψ(A(t+1))−Ψ(A(t ) )
( i,j)∈V o(η ) + o(η)+
( i,j)∈N cijη
( i,j)∈N cij η
( i,j)∈N ( cijη + o(η ) )
( 12 )
=
η lim η→0
0 . So ∃τ > 0 such that ∀η ∈ ( 0 , τ ) we have Ψ(A(t+1))−Ψ(A(t ) )
( i,j)∈N cij > 0 . The proof completes .
( i,j)∈N cij > ≥
= lim η→0
η
η
1 2
D . PROOF SKETCH OF LEMMA 3
Due to space limit , we present the proof sketch of Lemma 3 here . Please refer to the external supplementary material3 for the detailed proof . 3The proofs are available at http://wwwcscmuedu/ ~pengtaox/papers/kdd15_supp.pdf
1323 Proof . First , we construct a sequence of sequences with decreasing variance , in which the variance of the first sequence is var(b ) and the variance of the last sequence is var(c ) . We sort the unique values in b in ascending order and denote the resultant sequence as d = ( dj)m j=1 . Let l(j ) = max{i : bi = dj} , u(i ) = {j : dj = bi} , we construct a sequence of sequences h(j ) = ( h(j ) i=1 where j = 1 , 2,··· , m + 1 , in the following way : i )n
, j = 1,··· , m and l(m − j + 1 ) < i ≤ n ; i = bi , i = 1,··· , n ; i
• h(1 ) • h(j+1 ) • h(2 ) • h(j+1 ) i = h(1 ) i
= h(j ) i + g(dm ) , 1 ≤ i ≤ l(m ) ; = h(j ) i and 1 ≤ i ≤ l(m − j + 1 ) . i + g(dm−j+1 ) − g(dm−j+2 ) , j = 2,··· , m
From the definition of h(j ) , we know var(h(1 ) ) = var(b ) . As b1 < bn , we have m ≥ 2 . We can prove that var(h(m+1 ) ) = var(c ) and ∀j = 1 , 2,··· , m , var(h(j+1 ) ) < var(h(j) ) , which further imply var(c ) < var(h(1 ) ) = var(b ) . Furthermore , let n = max{j : bj = bn} , then ∀i , h(2 ) i = h(1 ) i = h(2 ) h(3 ) = bi + g(bn ) + ( g(bn ) − g(bn))I(i ≤ n ) = b i + g(dm ) = bi + g(bn ) i + ( g(dm−1 ) − g(dm))I(i ≤ l(m − 1 ) ) i
( 13 ) so var(c ) ≤ var(b ) < var(b ) . The proof completes .
E . PROOF OF THEOREM 3
The intuition is when the stepsize η is sufficiently small , we can make sure the changes of smaller angles ( between consecutive iterations ) are larger than the changes of larger angles , then Lemma 3 can be used to prove that the variance decreases . The proof of Theorem 3 utilizes Lemma 5 and 6 . i k ij ij
, ˜a(t ) k )n g(θ(t ) ij ) =
Proof . Let θ(t ) ij denote θ(˜a(t ) j ) . We sort θ(t )
η if θ(t ) ij < π ij = π 2 , k=1 , then var((θ(t ) in nondecreasing order and denote the resultant sequence as θ(t ) = ( θ(t ) ij ) ) = var(θ(t) ) . We use the same order to index θ(t+1 ) and denote the resultant sequence as ij θ(t+1 ) = ( θ(t+1 ) k=1 , then var((θ(t+1 ) ) ) = var(θ(t+1) ) . Let )n ( t ) 2cos(θ ij ) 2 and 0 if θ(t ) 1−cos(θ 2 ( t ) ij ) ij ) is a strictly decreasing function . Let ˜θ(t ) then g(θ(t ) k = k + g(θ(t ) θ(t ) k + ckη = θ(t ) k ) . It is easy to see when η is sufficiently small , 0 ≤ g(θ(t ) : k = 1 , 2,··· , n − 1 , θ(t ) k } . We continue the proof from k+1 = θ(t ) two complementary cases : ( 1 ) θ(t ) n . If θ(t ) 1 < θ(t ) < var(θ(t) ) , where ˜θ(t ) = ( ˜θ(t ) max{j : θ(t ) g(θ(t ) where θ(t ) = ( θ k=1 . Furthermore , let n = n ) − n ))I(k ≤ n ) , then var(˜θ(t ) ) ≤ var(θ(t ) ) < var(θ(t) ) ,
1 = θ(t ) n , then according to Lemma 3 , we have var(˜θ(t ) ) k=1 . var(θ(t ) ) can be written as : k ) ≤ min{θ(t )
( t ) k = θ(t ) k+1 − θ(t ) n ) + ( g(θ(t ) k + g(θ(t ) n ; ( 2 ) θ(t )
1 < θ(t ) n } , θ
= θ(t ) k )n k j
( t ) k )n n n n n i − 1 ( t ) )2 j=1 θ n ) − g(θ(t ) n ))I(i ≤ n ) i + ( g(θ(t ) j − n n ( g(θ(t ) n ) − g(θ(t ) n )))2
( t ) j n i=1(θ i=1(θ(t ) j=1 θ(t ) n
= 1 n = 1 n − 1 n n n j )(g(θ(t ) n ) n n
Let λ = 2( 1 n
−g(θ(t ) −g(θ(t )
= var(θ(t ) ) + 2( 1 n j=1 θ(t ) j=1(g(θ(t ) n ) i=1(θ(t ) n ))(I(i ≤ n ) − n n ))2(I(i ≤ n ) − n i − 1 n ) + 1 n n )2 i − 1 i=1(θ(t ) j=1 θ(t ) n can be further written as n j )(1 − n i − 1 n n ) j )(− n i − 1 i=n+1(θ(t ) j=1 θ(t ) n n ) j )(1 − n i − n i=1 θ(t ) n−nn n ) i − n−n j )(− n i=n+1 θ(t ) j=1 θ(t ) n ) i − 1 i=n+1 θ(t ) i=1 θ(t ) ( 1 i ) n n ( n n ( n n ( (n n ( (n
= 2n(n−n ) i=1(θ(t ) j=1 θ(t ) j=1 θ(t )
= 2
= 2
+ 2
+ 2 n n n n n2 j )(I(i ≤ n ) − n
( 14 ) n ) , it
( 15 )
As θ(t ) k
Let µ = is nondecreasing and θ(t ) n
− 2cos(θ 1−cos(θ
2cos(θ 1−cos(θ
( t )
( t ) n ) ( t ) n ) n , we have λ < 0 .
= θ(t ) 2 when θ(t ) n < π
2 and n nn
2
( t ) n ) n ) when θ(t )
2
( t )
( t )
µ = n ) n )
2cos(θ 1−cos(θ n = π
2 , then g(θ(t ) n ) = µη and µ > 0 . Substituting λ and µ into var(θ(t) ) , we can obtain : var(θ(t ) ) = var(θ(t ) ) + λµη + 1 n j=1(I(i ≤ n ) − n n ) − g(θ(t ) n n )2µ2η2
= var(θ(t ) ) + λµη + o(η )
Note that λ < 0 and µ > 0 , so ∃δ1 , such that η < δ1 ⇒ var(θ(t ) ) < var(θ(t ) ) + λµ 2 η . As var(˜θ(t ) ) < var(θ(t) ) , we can draw the conclusion that var(˜θ(t ) ) < var(θ(t ) ) + λµ 2 η . On the other hand , n i=1(θ(t+1 ) n var(θ(t+1 ) ) = 1 n i + o(η ) − 1 i=1(˜θ(t ) = 1 n i − 1 i=1(˜θ(t ) = 1 n n = var(˜θ(t ) ) + o(η ) n n n
− 1 j=1 n i n j j=1 θ(t+1 ) j=1 n ˜θ(t ) j )2 + o(η )
)2 ˜θ(t ) j + o(η))2
So ∃δ2 > 0 such that η < δ2 ⇒ var(θ(t+1 ) ) < var(˜θ(t))− λµ Let δ = min{δ1 , δ2} , then
4 η .
η < δ ⇒ var(θ(t+1 ) ) < var(θ(t ) ) + λµ
⇒ var((θ(t+1) ) ) < var(θ(t ) )
4 η < var(θ(t ) ) i i j j i1 j2 i1j1
√
· ˜a(t )
· ˜a(t )
= ˜a(t ) i2
= θ(t ) i2j2
1 = θ(t )
N∪V , θ(t )
√ x)−( π
For the second case θ(t )
˜a(t ) j1 for i = j and p2 = ˜a(t ) n , ie , ∀(i1 , j1 ) , ( i2 , j2 ) ∈ , we prove that var(θ(t+1 ) ) = var(θ(t) ) . In · · ˜a(t ) this case , ∀(i1 , j1 ) , ( i2 , j2 ) ∈ N ∪ V , ( (A(t))TA(t))i1j1 = ˜a(t ) = ( (A(t))TA(t))i2j2 . Denote p1 = ˜a(t ) for i = j . As A(t+1 ) = A(t ) + cA(t)((A(t))TA(t))−1 , where c = 2ηg(det((A(t))TA(t) ) ) det((A(t))TA(t ) ) and g(x ) = arcsin( we have ( A(t+1))TA(t+1 ) = ( A(t))TA(t)+2cI+c2((A(t))TA(t))−1 . 2 −arcsin( It is clear that ∀(i1 , j1 ) , ( i2 , j2 ) ∈ N ∪ V , ( (A(t))TA(t ) + 2cI)i1j1 = ( (A(t))TA(t ) + 2cI)i2j2 . For c2((A(t))TA(t))−1 , ( (A(t))TA(t))−1 = ( (p2 − p1)−1IK − ( p2−p1)−11K 1T implies that ∀(i1 , j1 ) , ( i2 , j2 ) ∈ N ∪ V , ( (A(t))TA(t ) ) ( (A(t))TA(t ) ) = ( (A(t+1))TA(t+1))i2j2 , so var(θ(t+1 ) ) = 0 = var(θ(t) ) . 0 , such that ∀η ∈ ( 0 , τ2 ) , Π(A(t+1 ) ) ≤ Π(A(t) ) . write it as c2((p2 − p1)IK + p11K 1T K )−1 , where IK is the identity matrix and 1K is a vector of 1s whose length is K . Applying Sherman Morrison formula , we can obtain that
Putting these two cases together , we conclude that ∃τ2 >
, so ( (A(t+1))TA(t+1))i1j1
1+K(p2−p1 )
) which
−1 i1j1
−1 i2j2 x))2 ,
=
K
1324
