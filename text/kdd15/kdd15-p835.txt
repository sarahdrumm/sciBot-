Flexible and Robust Multi Network Clustering
Jingchao Ni 1 , Hanghang Tong 2 , Wei Fan 3 , and Xiang Zhang 1
1Department of Electrical Engineering and Computer Science , Case Western Reserve University
2School of Computing , Informatics , Decision Systems Engineering , Arizona State University
1{jingchao.ni , xiangzhang}@caseedu , 2hanghangtong@asuedu ,
3Baidu Research Big Data Lab
3weifan@gmailcom
ABSTRACT Integrating multiple graphs ( or networks ) has been shown to be a promising approach to improve the graph clustering accuracy . Various multi view and multi domain graph clustering methods have recently been developed to integrate multiple networks . In these methods , a network is treated as a view or domain . The key assumption is that there is a common clustering structure shared across all views ( domains ) , and different views ( domains ) provide compatible and complementary information on this underlying clustering structure . However , in many emerging real life applications , different networks have different data distributions , where the assumption that all networks share a single common clustering structure does not hold . In this paper , we propose a flexible and robust framework that allows multiple underlying clustering structures across different networks . Our method models the domain similarity as a network , which can be utilized to regularize the clustering structures in different networks . We refer to such a data model as a network of networks ( NoN ) . We develop NoNClus , a novel method based on non negative matrix factorization ( NMF ) , to cluster an NoN . We provide rigorous theoretical analysis of NoNClus in terms of its correctness , convergence and complexity . Extensive experimental results on synthetic and real life datasets show the effectiveness of our method .
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications— Data mining
Keywords Network of Networks ; Graph clustering
1 .
INTRODUCTION
Graph ( or network1 ) clustering is a fundamental problem with numerous applications . Traditional clustering meth1In this paper , we use graph and network interchangeably .
11
9
2
B 1 4
5 3
D 1
3
12
10
2
7
9
7
4
6
A 1 4
3
11 9
5
13
10 7
F 1
3
6
5
2
4
C 11 4
10 7
9
3
E
2
7
8
4 6
1
2
1
5
3
Figure 1 : An example of NoN . The main network is represented by dashed nodes and edges . The domain specific networks are represented by solid nodes and edges . ods are usually designed for a single network [ 29 , 18 , 34 ] . In many emerging real life applications , networks collected from different conditions or domains are becoming available . For example , gene co expression networks are being collected from different tissues of model organisms [ 24 , 5 , 26 ] ; co author networks can be constructed for different research areas [ 28 ] . Since a single network can be noisy and incomplete , a promising approach is to exploit the shared clustering structure in multiple networks to improve the accuracy of the results .
Several approaches have been recently developed to cluster multiple networks . One popular approach is multi view clustering [ 37 , 20 , 19 ] . In this approach , a set of data objects may have multiple representations ( views ) . Different views provide compatible and complementary information on the underlying data distribution . The existing multi view graph clustering methods assume that all views consist of the same set of data objects and are generated from the same underlying distribution . Multi domain graph clustering [ 8 ] generalizes multi view clustering to allow many to many relationships between the nodes in different networks . Thus different networks may consist of different sets of nodes and have different sizes . Similar to multi view graph clustering , this approach also assumes that there is a single underlying
835 clustering structure shared across different domains . The ensemble clustering methods [ 30 , 12 , 13 ] aim at integrating multiple intermediate clustering results into a consensus one . The intermediate results can be obtained by applying the same clustering method on different views or different methods on the same view .
The key assumption of the existing multi network clustering methods is that different networks share the same underlying clustering structure . However , this assumption may not hold in real life applications . Figure 1 shows an example with six domains {A , B , C , D , E , F} , each of which corresponds to a network . Domains {A , B , C} are similar to each other and so are domains {D , E , F} . But these two sets of domains are not similar to each other . Clearly , we cannot assume domain sets {A , B , C} and {D , E , F} share a common clustering structure .
Note that the similarity among different domains can also be modeled as a network ( represented by dashed lines in Figure 1 ) . We refer to the structure shown in Figure 1 as a network of networks ( NoN ) . The dashed network represents the main network among six domains {A , B , C , D , E , F} . Each node in the main network corresponds to a domainspecific network represented by solid lines .
Consider an important bioinformatics problem : clustering gene co expression networks [ 15 , 24 ] . In a gene co expression network , each node is a gene and an edge represents the functional association between two connected genes . To improve the clustering accuracy , we can utilize multiple gene co expression networks collected in different tissues . Some tissues are similar to each other while others are not . The similarities among tissues can be modeled as a network . For example , in Figure 1 , the main network of domains {A , B , C , D , E , F} may represent the similarity among six different tissues . For each tissue , its domain specific network represents the tissue specific gene co expression network . As another example , consider the co author networks of different research areas . The main network may represent the similarity among different areas and a domain specific network may represent the co author network in a particular area .
The NoN setting illustrated in Figure 1 is different from the existing multi view or multi domain clustering scenario . In the NoN setting , there can be more than one underlying clustering structures among the domain specific networks . In Figure 1 , the clustering structure shared by domain specific networks {A , B , C} may be different from the one shared by {D , E , F} . For example , in domains {D , E , F} , nodes {1 , 2 , 3} are very likely to be in the same cluster , but they are in three different clusters in domains {A , B , C} . Thus assuming one common clustering structure for all six domainspecific networks is not reasonable . Consider the previous tissue specific gene co expression network and area specific co author network examples . Given a set of tissue specific gene co expression networks , the same set of genes may form a cluster ( eg , a pathway or functional module ) in several related tissues but not in others . Given a set of area specific co author networks , an author can be in different clusters ( eg , research sub communities ) in different areas .
In this paper , we propose NoNClus , a robust and flexible multi network clustering method that allows multiple underlying clustering structures . Our contributions are summarized as follows . • We investigate a new clustering problem in the novel network setting , NoN , where multiple underlying clus tering structures can co exist among domain specific networks . It generalizes the single clustering structure assumption of the existing multi view and multidomain clustering methods and has wider applicability in many emerging real life problems .
• We develop a novel two phase clustering framework , NoNClus , which can simultaneously cluster domainspecific networks with the guidance from the main network . NoNClus allows partial mapping across different sized domain specific networks , which is more general and realistic than the multi view setting . We also provide rigorous theoretical analysis of NoNClus in terms of its correctness , convergence and complexity . • We perform extensive experiments on both synthetic and real life datasets to evaluate the effectiveness of the proposed method .
The rest of the paper is organized as follows . Sec 2 is the related work . Sec 3 formulates the problem . Sec 4 presents NoNClus and its theoretical analysis . Sec 5 presents the experimental evaluations . Sec 6 gives concluding remarks .
2 . RELATED WORK
The existing multi network clustering methods are mostly developed for multi view networks [ 37 , 20 , 19 ] . In multi view clustering , views can be networks [ 37 , 20 , 19 ] or data feature matrices [ 1 , 23 , 35 ] . Ensemble clustering [ 30 , 12 , 13 ] is related to multi view clustering , where a consensus clustering is obtained by applying the same clustering algorithm on different views or applying different clustering algorithms on the same view . All these methods assume different views are compatible and share the same underlying clustering structure . Moreover , they are usually designed for views with the same set of data objects and the same number of clusters .
Several methods [ 33 , 11 , 22 ] extend the traditional multiview clustering to allow incomplete views . The method in [ 33 ] requires at least one view to be complete . The method in [ 11 ] focuses on constrained clustering where a set of mustlink and cannot link constraints are needed . The two view NMF based model proposed in [ 22 ] may suffer efficiency problem when applied to multi view scenario due to its pairwise regularization between views . A recent work on multidomain graph clustering [ 8 ] allows flexible network sizes and number of clusters . This method also uses pairwise regularization between domains thus may have efficiency problem . In contrast to [ 22 ] and [ 8 ] , our method allows efficient regularization by centroid matrices . More importantly , the methods in [ 33 , 11 , 22 , 8 ] have the same single underlying clustering structure assumption as other multi view clustering methods do .
Tensor factorization methods [ 17 , 16 ] can be applied to cocluster multiple matrices [ 32 , 14 ] . However , the existing tensor factorization methods , such as CP and Tucker decompositions [ 16 ] , are not designed for graph data where two modes of the tensor are symmetric . Furthermore , when applied to cluster multi view graphs , tensor factorization methods also have the limitation that all views must have the same size and share a single underlying clustering structure .
3 . THE PROBLEM
We first introduce the definition of a network of networks . The main symbols used in this paper are listed in Table 1 .
836 Table 1 : Main symbols
Meaning the g × g main network the ith domain specific network the factor matrix of A(i ) the jth hidden factor matrix the mapping matrix between U(i ) and V(j ) . the mapping matrix D(ij ) = O(ij)(O(ij ) ) the number of nodes in the main network the number of nodes in A(i ) the number of main clusters the number of domain clusters in A(i ) domain specific networks A = {A(1 ) , , A(g)} the set of nodes in A(i ) a network of networks R =< G , A >
Symbol
G A(i ) U(i ) V(j ) O(ij ) D(ij ) g ni k ti A V ( i ) R
Definition 1 . A Network of Networks ( NoN ) is defined as R =< G,A > , where G is the g × g main network , A = {A(1 ) , , A(g)} is a set of g domain specific networks . Node i ( i = 1 , , g ) in the main network G corresponds to domain specific network A(i ) .
We refer to the nodes in the main network and domainspecific networks as the main nodes and domain nodes respectively . We use V ( i ) to represent the set of domain nodes in domain specific network A(i ) , and I ( ij ) to represent the common nodes between A(i ) and A(j ) , ie , I ( ij ) = V ( i ) ∩ V ( j ) . For example , in Figure 1 , the dashed network is the main network G , which has six main nodes {A , B , C , D , E , F} . Each of these main nodes corresponds to a domain specific network ( the solid network ) . The main node A corresponds to the domain specific network with nine domain nodes {1 , 3 , 4 , 5 , 7 , 9 , 10 , 11 , 13} . The common nodes between domain specific networks of A and B are {1 , 3 , 4 , 5 , 7 , 9 , 10 , 11} .
We refer to the clusters in the main network and domainspecific networks as the main clusters and domain clusters , respectively . For example , in Figure 1 , there are two main clusters {A , B , C} and {D , E , F} . In the domain specific network B , there are three domain clusters {1 , 4 , 11 , 12} , {2 , 7 , 10} and {3 , 5 , 9} .
Our goal is to partition the domain specific networks while respecting the clustering structure in the main network . More formally , let H = {H1 , , Hk} be a partition of the main network , we want to find C = {C(1 ) , , C(g)} , the collection of partitions of all domain specific networks , where C(i ) ( i = 1 , , g ) is a partition of domain specific network A(i ) , with respect to the main clusters in H . Note that in this paper , we focus on finding non overlapping clusters . This is also the goal of the existing multi view and multi domain graph clustering methods [ 37 , 20 , 19 , 8 , 33 ] .
4 . THE NONCLUS ALGORITHM
Our NoNClus method clusters a NoN using a two phase approach . In Phase I , we partition the main network . To partition the domain specific networks , in Phase II , we develop a regularized non negative matrix factorization ( NMF ) clustering method , which respects the clustering information obtained in Phase I .
4.1 Main Network Clustering
In Phase I , we treat the main network clustering problem as a single network clustering problem . We adopt the widely used non negative matrix factorization ( NMF ) approach [ 21 ] . In particular , we use the symmetric version of NMF ( SNMF ) [ 10 , 18 ] to partition the main network , which minimizes the following objective function
JM = fiG − HH
.fi2
F
( 1 ) where fi · fiF is the Frobenius norm and H ∈ Rg×k is the factor matrix of the main network . An element hij of H indicates to which degree main node i belongs to the jth main cluster . We solve Eq ( 1 ) using the method in [ 10 ] :
+
. fi
H ← H ◦
1 − β + β
GH
H(H).H
( 2 )
[ · ] are element wise operators and 0 ≤ β ≤ 1 is a [ · ]
◦ and parameter which is suggested to be set to 0.5 in practice .
4.2 Domain specific Network Clustering
In Phase II , we incorporate the main cluster information to cluster domain specific networks . We first consider a simple case , where every domain specific network has a set of n nodes and t clusters . Note that in general , different domainspecific networks may have different number of nodes and clusters .
421 The Simplified Case
( i ) x∗ ∈ R
1×t +
( j )
1×t +
( j = 1 , , k ) to regularize u
For any domain node x , let u
( i = 1 , , g ) represent its domain cluster assignment vector in A(i ) . We assume that domain specific networks in the same main cluster share a common underlying clustering structure . Since there are k main clusters {H1 , , Hk} , for domain node x , we introduce k hidden domain cluster assignment vectors x∗ ∈ R If A(i ) bev longs to main cluster Hj , we want to minimize the clus(j ) x∗ , ie , ter assignments inconsistency between u fiu x∗ − v Furthermore , recall that hij denotes the strength of the main cluster membership . For domain node x , we can collectively penalize the inconsistencies between its domain cluster assignment vectors and hidden domain cluster assignment vectors by minimizing
( i ) x∗ and v
( j ) x∗ fi2 F .
( i ) x∗ .
( i ) g' k'
Jx = i=1 j=1 hijfiu
( i ) x∗ − v
( j ) x∗ fi2
F
( 3 )
Note that if two domain specific networks A(p ) and A(q ) have high hpj and hqj values , ie , they are likely to belong to the same main cluster Hj , the inconsistency between clus(q ) x∗ of node x will be penalized ter assignments u ( j ) x∗ . This is intuitive since if A(p ) and A(q ) are in through v the same main cluster , the clustering structures of A(p ) and A(q ) should be similar .
( p ) x∗ and u
837 Generalizing Eq ( 3 ) to all domain nodes , we have the following objective function : min
U(i)≥0 ( i=1,,g ) V(j)≥0 ( j=1,k )
JD = fiA
( i ) − U
( i )
( U
( i )
)
.fi2 (
F domain−specif ic network clustering
Summing up the inconsistencies over all domain nodes , we have the following objective function that allows partially aligned domain specific networks to have different sizes and number of clusters : g' g' k'
( 4 ) min
U(i)≥0 ( i=1,,g ) V(j)≥0 ( j=1,k )
JD =
JA + a hij JR i=1 i=1 j=1
( 5 ) where JA = fiA ni'
( i ) − U ni'
( i )
( U
( i )
)
JR =
( ˆu
( ij ) x∗ ( ˆu
( ij ) y∗ )
F
.fi2 . − ˆv
( ij ) y∗ )
.
2
)
( ij ) x∗ ( ˆv . − ( O x=1 y=1 ( ij )
)
( i )
( i )
U
U
( ij )
)(D
= fi(D In Eq ( 5 ) , a is a regularization parameter for the relative importance between the domain specific network clustering and the main cluster guided regularization . Intuitively , the more reliable the main network , the larger the value of a .
.fi2
)(O
( ij )
( ij )
V
V
( j )
( j )
)
F
Discussions : The existing NMF based multi view clustering methods either assume a single shared factor matrix among all views [ 1 ] or regularize all factor matrices towards a single centroid factor matrix [ 23 ] . In contrast , NoNClus introduces multiple hidden factor matrices to differentially regularize domain specific clusters guided by the main clusIf there is only one main cluster , NoNClus degenters . erates to a multi view graph clustering method . Moreover , NoNClus allows different network sizes and number of clusters among domain specific networks . Therefore , NoNClus can be viewed as a generalization of the existing multi view graph clustering methods to different sized networks with multiple underlying clustering structures . 4.3 Learning Algorithm
Since the objective function Eq ( 5 ) is not jointly convex , we optimize it by an alternating minimization approach , ie , the objective function is alternately minimized with respect to one variable while fixing others . This procedure repeats until convergence . Theorem 1 in the following gives the solution of U(τ ) ( 1 ≤ τ ≤ g ) when fixing other variables . Theorem 2 gives the solution of V(η ) ( 1 ≤ η ≤ k ) when fixing other variables .
Theorem 1 . Updating U(τ ) . When other variables are fixed , updating U(τ ) according to Eq ( 6 ) monotonically decreases Eq ( 5 ) until convergence . At convergence , the solution is a KKT fixed point .
( τ ) ← U
( τ ) ◦
U
A(τ )U(τ ) + a U(τ )(U(τ )).U(τ ) + a j=1 hτ jW(τ j)U(τ ) j=1 hτ j Y(τ j ) k k fi 1
4
. where
( τ j )
W
= ( D
( τ j )
( τ j )
Y
= ( D
( τ j )
)
D
( τ j )
U
.
( τ j )
( O
) .
( j )
V ( τ )
)(O ( τ )
( τ j ) .
V
( j )
) ( τ j )
D .
( U
)
( D
( τ j )
( τ )
U
D
)
( 6 )
( τ j )
.
Theorem 2 . Updating V(η ) . When other variables are fixed , updating V(η ) according to Eq ( 7 ) monotonically decreases Eq ( 5 ) until convergence . At convergence , the solution is a KKT fixed point . ( η ) ◦
. i=1 hiηQ(iη)V(η ) fi 1
( η ) ← V
( 7 )
V g
4 g i=1 hiηR(iη ) i=1 g' ff g' ff i=1 k' j=1
+ a hijfiU ( i ) − V
( j)fi2 (
F main cluster guided regularization
In Eq ( 4 ) , U(i ) ∈ Rn×t is the factor matrix of A(i ) , and V(j ) ∈ Rn×t represents the underlying clustering structure of domain specific networks in main cluster Hj . In the next , we refer to V(j ) as the jth hidden factor matrix .
+
+
422 The General Case
In general , the domain specific networks may have different node sets and sizes . To generalize the basic model discussed in the previous section , we allow factor matrix U(i ) to have different number of rows ( nodes ) for different domain specific networks . We further allow hidden factor matrix V(j ) to contain all nodes in the domain specific net)V ( i ) ( i ∈ Hj ) . Thus V(1 ) , , V(k ) works that belong to main cluster Hj . That is , the set of nodes in V(j ) is V ( j ) also have different number of rows ( nodes ) .
V =
V ni×˜nj +
Furthermore , different domain specific networks may share some common nodes . For example , a gene may be expressed in multiple tissues ; a user may have accounts in multiple social networks . Let ni = |V ( i)| and ˜nj = |V ( j ) | . We introduce mapping matrices O(ij ) ∈ R , such that O(ij)(x , y ) = 1 if the xth row of U(i ) and the yth row of V(j ) represent the same data object ; O(ij)(x , y ) = 0 otherwise . Note that each row of O(ij ) has at most one 1 because of the oneto one relationship between the common nodes in different domain specific networks . Since not all nodes in A(i ) have corresponding rows in V(j ) , we also introduce the diagonal mapping matrices D(ij ) ∈ Rni×ni , such that D(ij)(x , x ) = 1 if the xth row of U(i ) has a corresponding row in V(j ) ; D(ij)(x , x ) = 0 otherwise . Note that D(ij ) = O(ij)(O(ij ) )
+
.
.
Next , we further generalize our method to allow domainspecific networks to have different number of clusters . If hij is large , ie , strong main cluster membership , we want the same rows in D(ij)U(i ) and O(ij)V(j ) to be similar , since they denote the cluster assignments for the common nodes . However , different number of clusters will result in different number of columns in U(i ) and V(j ) . This makes the direct inconsistency penalty of domain clusters in the simple case Eq ( 4 ) no longer applicable . We address this issue by taking an indirect regularization .
( ij ) x∗ and ˆv
( ij ) x∗ and ˆu
Let ˆU(ij ) = D(ij)U(i ) and ˆV(ij ) = O(ij)V(j ) . Consider two nodes x and y in A(i ) that have similar cluster assign(ij ) y∗ . ments ˆu If hij is large , their corresponding cluster assignments ˆv should be similar . For example , in Figure 1 , if nodes 1 and 3 have similar cluster assignments in domain specific network D , their cluster assignments in the underlying clustering structure shared by {D , E , F} should be similar as well . We measure cluster assignment similarity by their inner product , and minimize the inconsistency ( ˆu
. − ˆv
( ij ) y∗
( ij ) x∗ ( ˆu
( ij ) x∗ ( ˆv
( ij ) y∗ )
( ij ) y∗ )
)2 .
.
838 Algorithm 1 : NoNClus Input : ( 1 ) a network of networks R =< G , A > ; ( 2 ) the mapping matrices {O(ij)} and {D(ij)} ; ( 3 ) the number of main clusters k ; ( 4 ) the number of domain clusters in {A(i)} and {V(j)} ; ( 5 ) the parameter a Output : a collection of partitions C = {C(1 ) , ,C(g)} of all domain specific networks
Normalize G , A(i ) ( i = 1 , , g ) by Frobenius norm ; Phase I : Initialize H with random values within ( 0 , 1 ] ; repeat
Update H by Eq ( 2 ) ; until Convergence Normalize H by H ← D−1 Phase II : for τ ← 1 to g do
H H ; end for η ← 1 to k do
Initialize U(τ ) with random values within ( 0 , 1 ] ;
Determine the size of V(η ) based on main cluster membership of domain specific networks ; Initialize V(η ) with random values within ( 0 , 1 ] ; end repeat for τ ← 1 to g do end for η ← 1 to k do
Update U(τ ) by Eq ( 6 ) ;
Update V(η ) by Eq ( 7 ) ; end until Convergence return C = {C(1 ) , ,C(g)} based on U(1 ) , , U(g ) .
1 2 3 4 5 6
7 8 9 10 11 12 13
14 15 16 17 18 19 20 21 22 23
24
( iη )
.
( i )
) ( iη )
O .
)
O
( iη )
( η )
V where
( iη )
Q
( iη )
= ( O
)
( iη )
( D
.
.
( iη )
( iη )
( iη )
R
= ( O
V In Eq ( 6 ) and Eq ( 7 ) , ◦ ,
O
)
( i )
U ( η )
)(D ( η )
( iη ) .
U
)
( O
( V [ · ] and ( · ) [ · ]
1 4 are element wise operators . Algorithm 1 summarizes our alternating minimization algorithm according to Theorems 1 and 2 . 4.4 Correctness Analysis
In the next , we provide theoretical analysis of the updating rule in Theorem 1 . We first prove the correctness of Eq ( 6 ) according to the Karush Kuhn Tucker ( KKT ) condition [ 7 ] . Then we analyze its convergence using the auxiliary function approach [ 21 ] . The proofs for Theorem 2 are similar and omitted here .
Theorem 3 . Correctness of Eq ( 6 ) . At convergence , the solution found by updating U(τ ) according to Eq ( 6 ) is a KKT fixed point .
Proof . Omitted for brevity . The formal proof can be found in an online Supplementary Material2 . 4.5 Convergence Analysis
Next we prove the convergence of Eq ( 6 ) using the auxil iary function approach [ 21 ] .
Definition 2 . [ 21 ] A function Z(h , ˜h ) is an auxiliary function for a given function J(h ) if the conditions Z(h , ˜h ) ≥ J(h ) and Z(h , h ) = J(h ) are satisfied . 2http://filercaseedu/jxn154/NoNClus
Lemma 1 . [ 21 ] If Z is an auxiliary function for J , then J is non increasing under the update h(t+1 ) = arg min
Z(h , h(t) ) .
Theorem 4 gives the auxiliary function for the objective function Eq ( 5 ) wrt U(τ ) . h
Theorem 4 . Auxiliary function of J(U(τ ) ) . Let J(U(τ ) ) denote the sum of all terms in Eq ( 5 ) that contains U(τ ) , then the following function fi
.
. pqr
Z(U(τ ) , ˜U(τ ) ) = −2
A(τ ) rp
˜U(τ ) pq
˜U(τ ) rq
1 + log
.
+ pq
− 2a
( ˜U(τ )( ˜U(τ ) ) . . hτ j j pqr
. ˜U(τ ))pq
( U(τ ) ( ˜U(τ ) pq )4 pq )3 fi
W(τ j ) rp
˜U(τ ) pq
˜U(τ ) rq
1 + log
+ a hτ j j pq
U(τ ) ˜U(τ ) pq pq U(τ ) rq ˜U(τ ) rq
' pq U(τ ) rq ˜U(τ ) rq
U(τ ) ˜U(τ ) . pq pq
˜Y(τ j ) '
( U(τ ) ( ˜U(τ ) pq )4 pq )3
( 8 ) where
( τ j )
( τ j )
= ( D
( τ j )
( τ j )
= ( D
)
W ˜Y
( j )
) .
( τ j )
( O ( τ j ) ˜U
D
V ( τ )
)(O ( τ ) ( ˜U
( τ j ) .
V
( j )
) ( τ j )
)
( D
( τ j )
.
D .
)
D
( τ j ) ˜U
( τ )
. is an auxiliary function for J(U(τ ) ) . function in U(τ ) and its global minimum is
.
( τ )
U
= ˜U
( τ ) ◦
A(τ ) ˜U(τ ) + a ˜U(τ )( ˜U(τ ) ) . ˜U(τ ) + a k
It is also a convex j=1 hτ jW(τ j ) ˜U(τ ) j=1 hτ j ˜Y(τ j ) k fi 1
4
( 9 ) Proof . Omitted for brevity . The formal proof can be found in the Supplementary Material .
Next we show the convergence of updating U(τ ) by Eq ( 6 ) in Theorem 5 .
Theorem 5 . Convergence of Eq ( 6 ) . When other variables are fixed , updating U(τ ) according to Eq ( 6 ) monotonically decreases Eq ( 5 ) until convergence .
Proof . According to Definition 2 , Lemma 1 and Theorem 4 ( note Eq ( 9 ) is consistent with Eq ( 6) ) , at any iteration κ ≥ 0 during updating U(τ ) , we have J((U
, ( U
( κ )
( κ )
( κ )
( τ )
( τ )
( τ )
)
)
) = Z((U ≥ Z((U
( κ+1 )
( τ )
)
, ( U
) ( τ )
) ( κ )
)
) ≥ J((U
( κ+1 )
( τ )
)
) where ( U(τ ))(κ ) denotes the updated U(τ ) at κth iteration . Thus J(U(τ ) ) monotonically decreases . Since the objective function Eq ( 5 ) is bounded below by 0 , the updating of U(τ ) will converge .
Similarly , Theorem 2 can be proved . Therefore , alternately updating U(τ ) and V(η ) by Eq ( 6 ) and Eq ( 7 ) monotonically decreases Eq ( 5 ) until convergence and the stationary point is a KKT fixed point , which guarantees the correctness and convergence of Algorithm 1 . 4.6 Complexity Analysis
Let N be the maximal number of nodes in any domainspecific network . There can be at most N non zero entries in O(ij ) and D(ij ) ( i = 1 , , g , j = 1 , , k ) because of the oneto one mapping between common nodes in different domainspecific networks . In practice , A(1 ) , , A(g ) and G can be
839 D
I e d o n n a M i
1 2 3 4 5 6 7 8 9 10
1 1 1 0.5 0.5 0.5 0.5 0.5 0.5 0.5 1
1 1 1 0.5 0.5 0.5 0.5 0.5 0.5 0.5 2
1 1 1 0.5 0.5 0.5 0.5 0.5 0.5 0.5 3
0.5 0.5 0.5 1 1 1 0.5 0.5 0.5 0.5 5
0.5 0.5 0.5 0.5 0.5 0.5 0.5 1 0.5 1 0.5 1 1 0.5 1 0.5 1 0.5 1 0.5 4 7 Main node ID
0.5 0.5 0.5 1 1 1 0.5 0.5 0.5 0.5 6
0.5 0.5 0.5 0.5 0.5 0.5 1 1 1 1 8
0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 1 1 1 1 1 1 1 1 9 10
0
D
I e d o n i n a m o D
100
200 0
100
Domain node ID
200
( a ) Simulated main network
( b ) An example of simulated underlying clustering structure
0
D
I e d o n i n a m o D
100
0
100
D
I e d o n i n a m o D
200 0
100
Domain node ID
200
215 0
100
Domain node ID
215
( c ) Embedding noise
( d ) An example of simulated domain specific network
Figure 2 : Synthetic dataset generation sparse . Let M and m be the maximal number of non zero entries in any A(i ) and G , respectively . Let T be the maximal number of clusters in any A(i ) .
Based on Eq ( 6 ) and Eq ( 7 ) , updating each U(τ ) and V(η ) require O(M T + kN T 2 ) and O(gN T 2 ) time , respectively . Thus the overall time complexity of Algorithm 1 is O(Im(mk + gk2 ) + Id(gM T + gkN T 2 ) ) considering both Phase I and Phase II , where Im and Id are the total number of iterations before the convergence of Phase I and Phase II , respectively . Moreover , Im , g , m and k are usually much smaller than Id , N , M and T , respectively . T is much smaller than N , and k can be regarded as a small constant . Therefore , the actual time complexity can be denoted as O(Idg(M T + N T 2) ) . The experimental results show that the our algorithm is almost linear with respect to M .
5 . EXPERIMENTAL RESULTS
In this section , we evaluate NoNClus on synthetic and real world datasets and compare it with the state of the art multi view and multi domain graph clustering methods . 5.1 Effectiveness Evaluation
511 Simulation Study
We first evaluate our method using synthetic datasets . We generate a main network containing three main clusters with sizes 3 , 3 , 4 as shown in Figure 2(a ) . The domain specific networks are generated as follows . We first generate an underlying domain specific clustering structure for each main cluster . Figure 2(b ) shows an example containing five clusters of the same size , where non zero entries are set to 1 . Then domain specific networks are generated from each un derlying clustering structure . To embed noise , we randomly flip α ( 0 ≤ α ≤ 1 ) fraction of 1 entries in the matrix to 0 and β ( 0 ≤ β ≤ 1 ) fraction of 0 entries to 1 . An example is shown in Figure 2(c ) with α = 80 % and β = 5 % . To generate domain specific networks with different sizes , we randomly remove or add ε fraction of nodes in the previous matrix . ε follows normal distribution with mean μ and standard deviation σ and its value is set between 0 and 1 . An example with 215 domain nodes generated by μ = 0.3 and σ = 0.05 is shown in Figure 2(d ) .
Using the above approach , we generate two different types of synthetic datasets . In the first dataset , all three underlying clustering structures contain 5 clusters , and all domainspecific networks have the same set of 200 nodes . α and β are set to 80 % and 5 % respectively to simulate noise . We refer to this dataset as the SynNoN view dataset . This dataset is used to evaluate the multi view graph clustering methods , since they assume all views have the same size .
In the second dataset , the three underlying clustering structures contain 5 , 6 , 7 clusters , and 200 , 300 , 350 nodes , respectively . They share 100 common nodes . The domainspecific networks are generated with α = 80 % , β = 5 % , μ = 0.3 and σ = 005 We refer to this dataset as the SynNoN dom dataset . This dataset is used to evaluate the multi domain graph clustering methods , which allow different domain sizes .
We compare NoNClus with several state of the art clustering methods , including ( 1 ) Symmetric NMF ( SNMF ) [ 18 ] ; ( 2 ) spectral clustering ( Spectral ) [ 29 ] ; ( 3 ) multi view cotraining spectral clustering ( CTSC ) [ 19 ] ; ( 4 ) multi view pairwise co regularized spectral clustering ( PairCRSC ) [ 20 ] ; ( 5 ) multi view centroid based co regularized spectral clustering ( CentCRSC ) [ 20 ] ; ( 6 ) Tensor Factorization ( TF ) [ 16 ] ; and ( 7 ) multi domain co regularized graph clustering ( CGC ) [ 8 ] . Note that the SNMF and spectral clustering methods can only be applied to a single network . CTSC , PairCRSC and CentCRSC are multi view graph clustering methods and can only be applied on the SynNoN view dataset . For TF , we test both CP and Tucker decompositions [ 16 ] and use three different strategies to assign a data object to a cluster : ( 1 ) the highest value in a row of the factor matrix ; ( 2 ) the highest absolute value in a row of the factor matrix [ 32 ] ; and ( 3 ) applying k means [ 25 ] on the factor matrix . The best results are reported . Note that TF is similar to multi view methods thus can only be applied on the SynNoN view dataset . Moreover , TF does not distinguish individual networks and only gives an overall clustering result of all nodes . CGC is a recent multi domain graph clustering method that can be applied on the SynNoN dom dataset . The common node relationships between different domain specific networks are used as the cross domain relationships in CGC .
Table 2 shows the averaged accuracy of different methods over 500 runs . The parameters are tuned for optimal performance of all methods . It can be seen that NoNClus achieves better individual and overall performance compared to other methods on both datasets . The multi view/domain methods , CTSC , PairCRSC , CentCRSC , TF and CGC assume a single underlying clustering structure . In contrast , NoNClus allows more flexible underlying clustering structures . This demonstrates that utilizing domain similarity network can dramatically improve the accuracy .
Next , we study a degraded version of NoNClus , which assumes that all domain specific networks share the same
840 Table 2 : Accuracy of different methods on synthetic datasets
Main cluster 1
Net 1 Net 2 Net 3 0.8735 0.8751 0.8675 0.8587 0.6279 0.6249 0.9227 0.9166 0.9050 0.9090
0.8716 0.8586 0.6258 0.9174 0.9031
−
−
−
Main cluster 2
Net 4 Net 5 Net 6 0.8754 0.8796 0.8624 0.8619 0.6196 0.6221 0.9173 0.9186 0.9021 0.9077
0.8732 0.8571 0.6236 0.9176 0.9090
−
−
−
Dataset Method
SNMF Spectral CTSC
PairCRSC CentCRSC
TF CGC
NoNClus
SNMF Spectral
CGC
NoNClus view dom
0.6364 0.9444 0.6584 0.5554 0.7303 0.7882
0.6337 0.9403 0.6687 0.5618 0.7297 0.7960
0.6407 0.9463 0.6583 0.5556 0.7229 0.7914
0.6385 0.9447 0.7123 0.5799 0.7992 0.8649
0.6273 0.9435 0.7063 0.5768 0.7962 0.8650
0.6316 0.9418 0.7129 0.5811 0.7965 0.8654
Main Cluster 3
0.8746 0.8622 0.9181 0.9353 0.9378
−
−
0.8682 0.8583 0.9106 0.9378 0.9342
0.8690 0.8582 0.9118 0.9335 0.9408
Net 7 Net 8 Net 9 Net 10 Overall 0.8732 0.8722 0.8607 0.8626 0.7400 0.9157 0.9252 0.9355 0.9188 0.9391 0.6505 0.6724 0.9512 0.6787 0.5490 0.7797 0.8388
0.7365 0.9621 0.6596 0.5188 0.7840 0.8363
0.7251 0.9643 0.6620 0.5241 0.7837 0.8367
0.7210 0.9629 0.6630 0.5242 0.7876 0.8389
0.7332 0.9617 0.6558 0.5167 0.7859 0.8409
−
−
SNMF
Spectral
CTSC
PairCRSC
CentCRSC
TF
CGC
NoNClus’
SNMF
Spectral
K−means
NoNClus’
NoNClus
1
0.9 y c a r u c c A
0.8
0.7
0.6
0.5 1
2
3
4
5
6
# Networks
1 y c a r u c c A
0.95
0.9
0.85
7
8
9
10
0.8 1
2
3
4
0.6
0.55
0.5
0.45
0.4
0.35 y c a r u c c A
5
6
# Networks
7
8
9
10
0.3
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Common node ratio η
0.4
0.3
I
M N
0.2
0.1
0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Common node ratio η
( a ) Networks of different sizes
( b ) Networks of same size
( a ) Overall accuracy
( b ) Overall NMI
Figure 3 : Clustering accuracy with different number of networks when all networks share a single common underlying clustering structure
Figure 4 : Clustering performance on 20 Newsgroup dataset with various rate of common nodes underlying clustering structure , but allows different sized domain specific networks . We refer to this degraded version as NoNClus’ . As discussed in Sec 422 , NoNClus’ has the same assumption as the multi view clustering methods and can be treated as a generalization of these methods on different sized networks .
We generate multiple domain specific networks with different sizes sharing the same underlying clustering structure by setting α = 80 % , β = 5 % , μ = 0.3 , σ = 005 Note that the multi view graph clustering methods cannot be applied to this dataset . Figure 3(a ) shows the accuracy when varying the number of domain specific networks . All results are averaged over 500 runs . It can be seen that NoNClus’ is effective in incorporating information from multiple domainspecific networks . It performs better than single network clustering methods and is similar to CGC on this dataset .
We also generate domain specific networks with the same size by setting α = 80 % , β = 5 % , μ = 0 , σ = 0 , so that the multi view graph clustering methods can be applied . Figure 3(b ) shows the results on this dataset ( CTSC and TF requires at least two views to run ) . From the results , we can observe that NoNClus’ is comparable to the multi view methods when applied to networks of the same size but is more general than them . Also , NoNClus’ is similar to the multi domain method CGC . Note that CGC uses a pairwise regularization . NoNClus’ utilizes a centroid regularization , thus is more efficient than CGC ( see Sec 52 ) Because of its competitive performance and efficiency , in the following , we use NoNClus’ as an alternative to multi view/domain graph clustering methods for datasets with different sized domain specific networks .
512 20 Newsgroup Dataset
We further evaluate the effectiveness of NoNClus using 20 Newsgroup dataset3 . Following a similar approach as in [ 9 ] , we preprocess the data by removing stop words , ignoring headers and subject lines . For each newsgroup , we select the top 2000 words using the mutual information based feature selection method .
We use 12 news groups of three categories , Comp , Rec and Talk4 , corresponding to three underlying clustering structures , each with four clusters ( news groups ) . In this study , we generate 10 domain specific networks from each category . Each domain specific network contains randomly sampled 200 documents from the 4 news groups ( 50 documents from each group ) in a category . The affinity matrix of documents is computed based on cosine similarity . The main network is generated by the cosine similarity between the overall word frequencies of domain specific networks . As a result , the main network contains 30 main nodes forming three main clusters corresponding to the three categories . Each main node corresponds to a domain specific network .
The common nodes in different domain specific networks are generated as follows . For any two domain specific networks generated from the same underlying clustering structure , a document in one domain specific network is randomly mapped to a document with the same cluster label ( eg , comp.graphics ) in another domain specific network . For any two domain specific networks generated from different un
3http://qwone.com/%7Ejason/20Newsgroups/ 4Comp : comp.graphics , composms windowsmisc , comp . sysibmpchardware , compsysmacpchardware ; Rec : rec . autos , rec.motorcycles , recsportbaseball , recsporthockey ; Talk : talkpoliticsguns , talkpoliticsmideast , talkpolitics misc , talkreligionmisc
841 SNMF
Spectral
K−means
NoNClus
0.5
0.4
0.3
0.2
0.1
I
M N y c a r u c c A
0.85 0.8 0.75 0.7 0.65 0.6 0.55 0.5
0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 parameter a
0 0
0.5
1
1.5 2.5 parameter a
2
3
3.5
4
( a ) SynNoN dom dataset
( b ) 20 Newsgroup dataset
Figure 5 : Parameter sensitivity evaluation
12000
10000
8000
6000
4000
2000
) . c e s ( e m T i
PairCRSC CentCRSC CGC NoNClus’ NoNClus
0 0
1
0.5
Domain−specific network size
2 x 108 ( a ) Varying network size
1.5
3000
2500
2000
1500
1000
500
) . c e s ( e m T i
0 6
PairCRSC CentCRSC CGC NoNClus’ NoNClus
8
10
12
# Domain−specific networks
14
( b ) Varying # of Networks
Figure 6 : Running time evaluation derlying clustering structures , the documents are randomly mapped with one to one relationship . We vary the ratio of common nodes , η , from 0 to 1 to evaluate its effect .
The clustering performance is evaluated using both purity accuracy and normalized mutual information ( NMI ) according to the labels provided in the dataset . Since multi view clustering methods CTSC , PairCRSC , CentCRSC and TF cannot be applied on networks with partial common nodes , NoNClus’ is used as a generalization of the multi view clustering methods . Also , single network clustering methods SNMF and Spectral clustering ( Spectral ) are performed on individual domain specific networks . The widely used kmeans method [ 25 ] is also selected as a baseline method in this comparison . It is applied on the original document word matrix instead of the network data .
Figures 4 shows the average accuracy and NMI on all domain specific networks when varying η . All results are averaged over 100 runs . As can be seen from the figures , NoNClus becomes better than SNMF when there are around 40 % common nodes . NoNClus’ performs worse than NoNClus and does not obviously increase the accuracy over SNMF . This is because NoNClus’ cannot handle multiple underlying clustering structures . The results demonstrate that NoNClus can effectively improve the accuracy with a small number of common nodes among different networks .
5.2 Performance Evaluation
In this section , we evaluate NoNClus in terms of its sensitivity to the regularization parameter a and its scalability . The evaluation of its convergence property can be found in the Supplementary Material . The datasets used include SynNoN view , SynNoN dom and 20 Newsgroup .
Figure 5 shows the clustering accuracy and NMI when varying a . The results of single network clustering methods are used as references . The accuracy and NMI are averaged over 100 runs . We observe that NoNClus is not sensitive to the regularization parameter a . The accuracy and NMI increase as a increases and become stable after a ≥ 1 .
Table 3 : Tissue specific gene co expression networks
Tissue specific network # nodes # edges
Blood
Lymph node
Tonsil Thymus
Brain
Caudate nucleus Hypothalamus
Cerebellum
Total
633 648 682 786 746 640 641 679 5 , 455
2 , 573 2 , 256 2 , 480 2 , 939 3 , 135 2 , 578 2 , 500 2 , 636 21 , 097
Blo .
Cer .
Lym .
Hyp .
Ton .
Cau .
Thy .
Bra .
Figure 7 : Tissue tissue similarity network ( the main network in NoN )
Next , we evaluate the efficiency of NoNClus using the SynNoN view dataset . Other multi view/domain graph clustering methods are also evaluated as references ( CTSC is omitted since it does not guarantee convergence ) . The experiments are performed on a 2.10GHz machine with 48GB memory . The reported results are averaged over 10 runs .
Figure 6(a ) shows the running time when varying the size of the domain specific networks . There are 6 domainspecific networks . The network size is measured by the total number of edges in all domain specific networks . Figure 6(b ) shows the running time when varying the number of domain specific networks . There are 2 , 500 nodes in each network . We omit some results of PairCRSC , CentCRSC and CGC because of their high memory or running time costs . As can be seen , the running time of NoNClus is almost linear wrt the size and number of domain specific networks . This is consistent with the time complexity analysis in Sec 46 In addition , NoNClus is faster than other methods since PairCRSC and CGC require pairwise regularizations , and the eigendecomposition process of PairCRSC and CentCRSC for non sparse matrices are time and space consuming . NoNClus runs faster than NoNClus’ because of its faster convergence rate . 5.3 A Case Study of Tissue Specific Gene Co
Expression Networks
In this section , we apply NoNClus on tissue specific gene co expression networks . We use the recently published global map of human gene expression dataset [ 24 ] to generate tissuespecific gene co expression networks . The dataset contains 5372 samples for 128 different tissues in four different cell types , ie , normal , disease , neoplasm and cell line . Following a similar approach as in [ 5 ] , we consider tissues of normal
842 130
120
110
100
90
80
70
60 s r e t s u c t l n a c i f i n g s f i o r e b m u N
50
30
SNMF Spectral MCL ClusterOne NoNClus’ NoNClus
0.2
0.18
0.16
0.14
0.12
0.1
0.08
0.06
0.04
0.02
0.05 l e u a v − p d e j t s u d a R D F
70 110 Number of domain clusters
90
130
150
0 0
15
30
45
60 90 Clusters number
75
105
120
135
150
SNMF Spectral MCL ClusterOne NoNClus’ NoNClus 50
( a ) Number of significant clusters
( b ) FDR adjusted p values
Figure 8 : Performance comparison on tissue specific gene co expression NoN status and experiments with at least five replicates . We select 8 tissues , ie , blood , lymph node , tonsil , thymus , brain , caudate nucleus , hypothalamus and cerebellum to form the main network . The tissue similarity matrix is constructed using the pairwise correlation ( normalized between [ 0 , 1 ] ) of the expression data of tissue specific genes . The main network is shown in Figure 7 , which contains two main clusters . For each tissue , we construct the tissue specific gene coexpression network using the gene expression data for that tissue . We extract genes that are expressed in each tissue ( with expression values greater than 10 ) . The edges in a gene co expression network are weighted by the Pearson ’s correlation coefficient ( normalized between [ 0 , 1 ] ) between two connected genes . The statistics of the gene co expression networks are summarized in Table 3 .
We compare NoNClus with ( 1 ) SNMF ; ( 2 ) Spectral clustering ( Spectral ) ; ( 3 ) Markov clustering ( MCL ) [ 34 ] ; ( 4 ) ClusterOne [ 27 ] ; and ( 5 ) NoNClus’ . MCL has been widely applied to detect functional modules in biological networks [ 3 ] . ClusterOne can detect overlapping clusters . Its overlapping rate is set such that any two clusters of size 5 can have at most 3 common genes ( ie , match coefficient [ 27 ] 036 ) This rate also applies to clusters of other sizes . Note that multi view clustering methods cannot be applied because of the different network sizes . We use NoNClus’ as an alternative .
The clustering performance are evaluated using the standard Gene Set Enrichment Analysis ( GSEA ) [ 31 ] . The most significant Gene Ontology ( GO ) term in the biological process category [ 2 ] is assigned to each identified gene cluster ( we evaluate clusters with sizes at least 5 ) . The significance is assessed by Hypergeometric distribution [ 3 ] . Raw p values are adjusted for multiple testing [ 36 ] by False Discovery Rate ( FDR ) [ 4 ] .
We first assume that all gene co expression networks have the same number of clusters . If a method needs initialization , we run it with 10 random initializations and report the optimal performance . Figure 8(a ) shows the total number of significant clusters detected in all gene co expression networks wrt the input number of domain clusters . As we can see , for the methods that need input cluster number , the best performance occurs when the number of clusters is set to 70 . Before that , all methods perform similarly because of the limited numbers of clusters . After that , NoNClus is more stable than NoNClus’ , since NoNClus allows multi
# significant clusters
Table 4 : Number of significant clusters p value 4.64 × 10 6.66 × 10 6.45 × 10 1.43 × 10 4.87 × 10 1
Method SNMF Spectral MCL ClusterOne NoNClus’ NoNClus
116 119 70 89 121 130
−5 −3 −17 −10 −2 ple underlying clustering structures . NoNClus also detects more significant clusters than other methods do .
Next we present the results of the selected methods when their parameters are tuned for their optimal performance . In particular , the numbers of domain clusters for NoNClus’ and NoNClus can be tuned by using the optimal values individually given by SNMF or spectral clustering . In this experiment , they are around 70 .
The p values of detected clusters are shown in Figure 8(b ) . The clusters are sorted in ascending order of their p values . We observe that the clusters detected by NoNClus are more significant than those identified by other methods .
Table 4 shows the number of significant clusters identified by different methods using a significance threshold 005 It can be seen that NoNClus detects more significant clusters than other methods do . For each alternative method , we further perform the two sample t test on the p values of the 155 most significant clusters detected by that method and those by NoNClus . The significance of the test results are reported in the third column of Table 4 . Clearly , NoNClus performs significantly better than other methods .
The reason for the better performance of NoNClus is that the gene co expression networks are very noisy . Single network clustering methods can be sensitive to these noises . Utilizing common clustering structure shared by similar tissues can help improve the robustness of the method . On the other hand , the same set of genes forming a cluster in similar tissues may not form a cluster in dissimilar tissues . In particular , there are some housekeeping genes that are universally expressed in different tissues . These genes achieve their functions in different tissues by interacting with genes that are tissue specific . These tissue specific genes are expressed only in some tissues but not in others [ 6 ] . Thus it is more reasonable to distinguish different tissue ( main )
843 clusters when integrating multiple tissue specific gene coexpression networks .
6 . CONCLUSION
Clustering multiple networks has been widely recognized as promising to improve graph clustering performance . Existing multiple network clustering methods , such as multiview/domain graph clustering , assume a single underlying clustering structure is shared among all networks . In this paper , we propose a new clustering framework that clusters multiple domain specific networks sharing multiple underlying clustering structures . We model domain similarity as a main network where main nodes represent domain specific networks and formulate the clustering problem on this novel network of networks ( NoN ) setting as a two phase regularized optimization problem . We develop NoNClus to solve this problem and provide rigorous theoretical analysis concerning its correctness , convergence and complexity . Experimental results on both synthetic and real world datasets demonstrate the effectiveness of NoNClus .
7 . ACKNOWLEDGEMENT
This work was partially supported by the National Science Foundation grants IIS 1162374 , IIS 1218036 and IIS1017415 , by the Army Research Laboratory under Cooperative Agreement Number W911NF 09 2 0053 , by National Institutes of Health under the grant number R01LM011986 , and by Region II University Transportation Center under the project number 49997 33 25 .
8 . REFERENCES [ 1 ] Z . Akata , C . Thurau , C . Bauckhage , et al . Non negative matrix factorization in multimodality data for segmentation and label prediction . In 16th Computer Vision Winter Workshop , 2011 .
[ 2 ] M . Ashburner , C . A . Ball , J . A . Blake , D . Botstein ,
H . Butler , J . M . Cherry , A . P . Davis , K . Dolinski , S . S . Dwight , J . T . Eppig , et al . Gene ontology : tool for the unification of biology . Nat . Genet . , 25(1):25–29 , 2000 . [ 3 ] S . Asur , D . Ucar , and S . Parthasarathy . An ensemble framework for clustering protein–protein interaction networks . Bioinformatics , 23(13):i29–i40 , 2007 .
[ 4 ] Y . Benjamini and D . Yekutieli . The control of the false discovery rate in multiple testing under dependency . Ann . Stat . , pages 1165–1188 , 2001 .
[ 5 ] D . B¨ornigen , T . H . Pers , L . Thorrez , C . Huttenhower ,
Y . Moreau , and S . Brunak . Concordance of gene expression in human protein complexes reveals tissue specificity and pathology . Nucleic Acids Res . , 41(18):e171–e171 , 2013 .
[ 6 ] A . Bossi and B . Lehner . Tissue specificity and the human protein interaction network . Mol . Syst . Biol . , 5(1 ) , 2009 .
[ 7 ] S . Boyd and L . Vandenberghe . Convex optimization .
Cambridge university press , 2009 .
[ 8 ] W . Cheng , X . Zhang , Z . Guo , Y . Wu , P . F . Sullivan , and
W . Wang . Flexible and robust co regularized multi domain graph clustering . In KDD , 2013 .
[ 9 ] I . S . Dhillon , S . Mallela , and D . S . Modha .
Information theoretic co clustering . In KDD , 2003 .
[ 10 ] C . H . Ding , X . He , and H . D . Simon . On the equivalence of nonnegative matrix factorization and spectral clustering . In SDM , 2005 .
[ 11 ] E . Eaton , M . Desjardins , and S . Jacob . Multi view clustering with constraint propagation for learning with an incomplete mapping between views . In CIKM , 2010 .
[ 12 ] X . Z . Fern and C . E . Brodley . Solving cluster ensemble problems by bipartite graph partitioning . In ICML , 2004 .
[ 13 ] D . Greene and P . Cunningham . A matrix factorization approach for integrating multiple data views . In ECML PKDD , 2009 .
[ 14 ] J . C . Ho , J . Ghosh , and J . Sun . Marble : high throughput phenotyping from electronic health records via sparse nonnegative tensor factorization . In KDD , 2014 .
[ 15 ] H . Hu , X . Yan , Y . Huang , J . Han , and X . J . Zhou . Mining coherent dense subgraphs across massive biological networks for functional discovery . Bioinformatics , 21(suppl 1):i213–i221 , 2005 .
[ 16 ] T . G . Kolda and B . W . Bader . Tensor decompositions and applications . SIAM review , 51(3):455–500 , 2009 .
[ 17 ] T . G . Kolda and J . Sun . Scalable tensor decompositions for multi aspect data mining . In ICDM , 2008 .
[ 18 ] D . Kuang , H . Park , and C . H . Ding . Symmetric nonnegative matrix factorization for graph clustering . In SDM , 2012 .
[ 19 ] A . Kumar and H . Daum´e . A co training approach for multi view spectral clustering . In ICML , 2011 .
[ 20 ] A . Kumar , P . Rai , and H . Daume . Co regularized multi view spectral clustering . In NIPS , 2011 .
[ 21 ] D . D . Lee and H . S . Seung . Algorithms for non negative matrix factorization . In NIPS , 2000 .
[ 22 ] S Y Li , Y . Jiang , and Z H Zhou . Partial multi view clustering . In AAAI , 2014 .
[ 23 ] J . Liu , C . Wang , J . Gao , and J . Han . Multi view clustering via joint nonnegative matrix factorization . In SDM , 2013 .
[ 24 ] M . Lukk , M . Kapushesky , J . Nikkil¨a , H . Parkinson ,
A . Goncalves , W . Huber , E . Ukkonen , and A . Brazma . A global map of human gene expression . Nat . Biotechnol . , 28(4):322–324 , 2010 .
[ 25 ] J . MacQueen et al . Some methods for classification and analysis of multivariate observations . In Proceedings of the fifth Berkeley symposium on mathematical statistics and probability , volume 1 , pages 281–297 . California , USA , 1967 .
[ 26 ] O . Magger , Y . Y . Waldman , E . Ruppin , and R . Sharan .
Enhancing the prioritization of disease causing genes through tissue specific protein interaction networks . PLoS Comput . Biol . , 8(9):e1002690 , 2012 .
[ 27 ] T . Nepusz , H . Yu , and A . Paccanaro . Detecting overlapping protein complexes in protein protein interaction networks . Nat . Methods , 9(5):471–472 , 2012 .
[ 28 ] J . Ni , H . Tong , W . Fan , and X . Zhang . Inside the atoms : ranking on a network of networks . In KDD , 2014 .
[ 29 ] J . Shi and J . Malik . Normalized cuts and image segmentation . IEEE Trans . Pattern Anal . Mach . Intell . , 22(8):888–905 , 2000 .
[ 30 ] A . Strehl and J . Ghosh . Cluster ensembles a knowledge reuse framework for combining partitionings . In AAAI/IAAI , 2002 .
[ 31 ] A . Subramanian , P . Tamayo , V . K . Mootha , S . Mukherjee ,
B . L . Ebert , M . A . Gillette , A . Paulovich , S . L . Pomeroy , T . R . Golub , E . S . Lander , et al . Gene set enrichment analysis : a knowledge based approach for interpreting genome wide expression profiles . Proc . Natl . Acad . Sci . USA , 102(43):15545–15550 , 2005 .
[ 32 ] J . Sun , D . Tao , and C . Faloutsos . Beyond streams and graphs : dynamic tensor analysis . In KDD , 2006 .
[ 33 ] A . Trivedi , P . Rai , H . Daum´e III , and S . L . DuVall .
Multiview clustering with incomplete views . In NIPS Workshop , 2010 .
[ 34 ] S . Van Dongen . A cluster algorithm for graphs . In Centrum voor Wiskunde en Informatica ( CWI ) , 2000 .
[ 35 ] H . Wang , F . Nie , and H . Huang . Multi view clustering and feature learning via structured sparsity . In ICML , 2013 .
[ 36 ] P . H . Westfall . Resampling based multiple testing :
Examples and methods for p value adjustment , volume 279 . John Wiley & Sons , 1993 .
[ 37 ] D . Zhou and C . J . Burges . Spectral clustering and transductive learning with multiple views . In ICML , 2007 .
844
