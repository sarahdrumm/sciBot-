Mining Frequent Itemsets through Progressive Sampling with Rademacher Averages
Matteo Riondato
Dept . of Computer Science
Brown University
Providence , RI 02912 matteo@csbrownedu
Eli Upfal
Dept . of Computer Science
Brown University
Providence , RI 02912 eli@csbrownedu
ABSTRACT We present an algorithm to extract an high quality approximation of the ( top k ) Frequent itemsets ( FIs ) from random samples of a transactional dataset . With high probability the approximation is a superset of the FIs , and no itemset with frequency much lower than the threshold is included in it . The algorithm employs progressive sampling , with a stopping condition based on bounds to the empirical Rademacher average , a key concept from statistical learning theory . The computation of the bounds uses characteristic quantities that can be obtained efficiently with a single scan of the sample . Therefore , evaluating the stopping condition is fast , and does not require an expensive mining of each sample . Our experimental evaluation confirms the practicality of our approach on real datasets , outperforming approaches based on one shot static sampling .
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications— Data mining
General Terms Algorithms , Theory , Performance , Experimentation
Keywords Frequent Itemsets ; Pattern Mining ; Rademacher Averages ; Sampling ; Statistical Learning Theory
1 .
INTRODUCTION
The task of Frequent Itemsets ( FIs ) mining is to extract all sets of items that appear in at least a fraction θ of a transactional dataset D , or the k most frequent set of items [ 2 ] . It is a fundamental primitive of knowledge discovery and is useful , among the others , for market basket analysis , inference , classification , and network management [ 13 ] . Exact algorithms to mine FIs have since long been available but their practicality is hindered by the need to scan the dataset multiple times [ 1 , 12 ] . When the dataset is too large to fit into main memory , as it is the case for many modern datasets , the running time of exact FIs mining algorithms may be too high to be practical . A natural way to reduce the dependency on the dataset size is to only analyze a small random sample of the dataset that can reside in main memory . The collection of FIs obtained from the sample will be an approximation to the exact collection , due to the fact that only a subset of the dataset is analyzed . Approximate collections of FIs are nevertheless acceptable in most cases due to the exploratory nature of the FIs mining step in the knowledge discovery process . There is an intrinsic trade off between the size of the sample ( number of transactions in the sample ) and the accuracy of the estimation , but a loose analysis of this trade off may result in sample sizes much larger than what is needed to obtain and approximation with the desired level of accuracy and confidence . It is therefore necessary , although challenging , to develop algorithms that leverage on tight bounds to the trade off between sample size and accuracy in order to fully exploit the power of sampling . Contributions . In this work we study the trade off between approximation quality and sample size using concepts and results from statistical learning theory [ 29 ] . We present a randomized algorithm to mine a high quality approximation of the collection of FIs wrt a minimum frequency threshold θ ( and of the top k most frequent itemsets ) from random samples of the dataset D . With probability at least 1−δ , for some user specified δ ∈ ( 0 , 1 ) , the returned approximation is a superset of the exact collection of FIs and no itemset in the approximation may have frequency less than θ − ε , for some user specified ε ∈ ( 0 , 1 ) . Moreover , the estimation of the frequency of all itemsets in the output is within ε/2 of their exact value . The algorithm uses progressive sampling , ie , it starts from a small sample and enlarges it until a suitable stopping condition is verified , meaning that an high quality approximation can be obtained from the sample . The stopping condition is based on bounds to the empirical Rademacher average of the problem at hand , a key concept from statistical learning theory [ 5 ] . In particular we prove that we can bound the empirical Rademacher average and therefore the maximum deviation between the frequency of an itemset in the dataset and the frequency of that itemset in the sample using a function of the sample size , of δ , and of a partitioning of the set of Closed Itemsets ( CIs ) [ 19 ] in the sample . We also give a bound , which is of independent interest , to the number of CIs in the sample . To our knowledge this is the first algorithm that uses
1005 bounds on the empirical Rademacher average in the domain of pattern mining , and one of the first to adapt these highly theoretical concepts to develop an efficient algorithm for an important practical task . We conducted an extensive experimental evaluation to test our algorithm and assess its performances in terms of the quality of the returned collection of itemsets and of the runtime , comparing it with standard baselines . Outline . We start by reviewing related works in Sect . 2 . We then formalize the problem of FIs mining and formally define the concept of approximation in Sect . 1 . Our algorithm and its analysis are presented in Sect . 4 . The goals , methodology , and results of the experimental evaluation can be found in Sect . 5 . Finally , we draw some conclusions and suggest some future directions in Sect . 6 .
2 . RELATED WORK
The idea of using random samples to speed up the extraction of FIs has been studied since shortly after the first efficient exact algorithms had been presented [ 28 ] . Many works focused on deriving bounds for the size of a single sample to obtain high quality approximation . Riondato and Upfal [ 24 ] present what is currently the best available bound . We refer the interested reader to their extensive discussion of previous results on fixed sample sizes and we focus here on the works that examined progressive sampling [ 7 , 8 , 14 , 18 , 20 , 21 , 26 ] . The use of progressive sampling , in contrast with a sample of fixed size , can contribute to an even greater speed up of the extraction of FIs , especially when combined with an appropriate schedule and starting sampling size [ 3 , 14 , 21 ] . Developing a stopping rule that allows to obtain approximations of guaranteed quality is a challenging task . Chen et al . [ 7 ] , Parthasarathy [ 18 ] , and Chuang et al . [ 8 ] propose progressive sampling based algorithms that use heuristics based on self similarity or the frequency of single items to determine the stopping sampling size . Because of the use of heuristics , these approaches offer no guarantee on the quality of the obtained collection . In contrast , our algorithm returns , with high probability , a collection of itemsets with strong approximation guarantees .
Pietracaprina et al . [ 20 ] and Scheffer and Wrobel [ 26 ] focuses on extracting the top k most frequent itemsets using progressive sampling . The stopping condition suggested by Scheffer and Wrobel [ 26 ] employs progressive filtering of the set of candidate FIs based on Chernoff bounds until only k itemsets are left , but offers no guarantee on whether the returned collection contains any of the actual top k FIs , rather a much weaker guarantee is offered . Our algorithm instead guarantees that the returned collection of itemsets is a superset of the top k FIs . The algorithm by Pietracaprina et al . [ 20 ] uses a stopping condition based on the frequency of all itemsets in the sample . The analysis is based on traditional Chernoff and union bounds and limited to itemsets up to a fixed length . Our algorithm does not suffer from this limitation and our analysis uses powerful deviation bounds based on Rademacher averages .
Another major point of difference between our work and the ones previously presented is the fact that checking the stopping condition of our algorithm does not require to run an exact FI mining algorithm on the sample . As a consequence , our stopping condition is much more efficient to evaluate , resulting in lower running time .
We use bounds to the Rademacher averages [ 4 , 16 ] , an important concept from statistical learning theory . We only introduce the necessary notation and results , and we refer the reader to the book by Shalev Shwartz and Ben David [ 27 ] for an in depth presentation of these topics .
To the best of our knowledge , the only previous use of bounds or estimates of the Rademacher averages in a progressive sampling setting is the work by Elomaa and Kääriäinen [ 9 ] on learning two level decision trees , whose settings and problem are very different from the ones we study .
3 . DEFINITIONS AND PRELIMINARIES Let I be a set of items with an arbitrary fixed total order “ < ” . A transaction is a subset of I , and a transactional dataset is a collection of transactions . An itemset is a set of items that appear together in a transaction ( ie , a subset of a transaction ) . Given an itemset A and a transaction τ st A ⊆ τ , we say that A appears or is contained in τ and that τ contains A . The support set TD(A ) of A in D is the subset of transactions in D that contain the itemset A , and the frequency of itemset A in dataset D is the fraction of transactions of D that contain A : fD(A ) = |TD(A)|/|D| .
Given a frequency threshold θ ∈ ( 0 , 1 ] , the set FI(D,I , θ ) of Frequent Itemsets ( FIs ) in D wrt θ is the collection of all itemsets with frequency at least θ in D :
FI(D,I , θ ) = {(A , fD(A ) ) : A ⊆ I ∧ fD(A ) ≥ θ} .
Similarly , let f least k itemsets have frequency at least f set of the top k FIs is
( k ) D be the maximum frequency such that at D in D , then the
( k )
TOPK(D,I , k ) = FI(D,I , f
( k ) D ) .
Note that |TOPK(D,I , k)| ≥ k . Goal . We aim at approximating the collection of ( top k ) FIs by mining ( ie , extracting the FIs from ) random samples of D ( ie , random collections of transactions from D ) .
Definition 1 . For ε , δ ∈ ( 0 , 1 ) , a ( ε , δ) approximation of FI(D,I , θ ) is a collection C = {(A , fA ) : A ⊆ I , fA ∈ ( 0 , 1]} such that , with probability at least 1 − δ :
C ; and
1 . for any ( A , fD(A ) ) ∈ FI(D,I , θ ) there is a pair ( A , fA ) ∈ 2 . for any ( A , fA ) ∈ C , it holds fD(A ) ≥ θ − ε ; and 3 . for any ( A , fA ) ∈ C , it holds |fD(A ) − fA| ≤ ε/2 .
An ( ε , δ) approximation of FI(D,I , f of TOPK(D,I , k ) .
( k ) D ) is an ( ε , δ) approximation
4 . A PROGRESSIVE SAMPLING ALGORI
THM WITH GUARANTEES
We want to compute an ( ε , δ) approximation to FI(D,I , θ ) ( or to TOPK(D,I , k ) ) from random samples of D of progressively increasing size ( ie , through progressive sampling ) . In the rest of this section we focus on FI(D,I , θ ) , while the case for top k FIs is presented in Sect . 45
The basic steps of the iterative progressive sampling pro cess are :
1 . at iteration i , create a random sample Si of some predefined size |Si| by drawing transaction uniformly and independently at random ( with replacement ) from D ;
1006 X
τ∈D nX i=1
2 . check a stopping condition to determine if an ( ε , δ)approximation of FI(D,I , θ ) can be extracted from Si ; 3 . if the stopping condition is satisfied , return the collection FI(Si,D , γ ) for an appropriate value of γ and exit , otherwise increase i and return to step 1 .
In order to obtain an algorithm from this high level description , it is necessary to formally specify the following components :
1 . a sampling schedule ( |Si|)i≥1 of sample sizes . 2 . a stopping condition involving the sample Si , and an efficient procedure to check this condition . 3 . a revised minimum frequency threshold γ . Any non decreasing sequence ( |Si|)i≥1 can act as a sample schedule , giving complete freedom to the algorithm designer In Sect . 4.6 we show how to and the user in this sense . compute the next sample size using information from the current sample . The choice of γ and of the stopping condition are intertwined . We choose γ = θ−ε/2 , as motivated by the following lemma , and this choice defines rigorous requirements for the stopping condition .
Lemma 1 . Let S be a sample of D , and consider the event ( 1 )
ES : “ |fD(A ) − fS(A)| ≤ ε
2 for all A ⊆ I ” .
If
( 2 ) then the collection FI(S,I , θ− ε/2 ) is a ( ε , δ) approximation to FI(D,I , θ ) .
Pr(ES ) ≥ 1 − δ ,
Proof . Assume that the event ES in ( 1 ) is verified , which happens by hypothesis with probability at least 1− δ . Then for no itemset B ∈ FI(D,I , θ ) we may have fS(B ) < θ− ε/2 , hence B ∈ FI(S,I , θ − ε/2 ) , as required by property 1 from Def . 1 . Let C be any itemset with fD(C ) < θ − ε . We have that fS(C ) < θ − ε/2 , so C /∈ FI(S,I , θ − ε/2 ) , which is the condition specified by property 2 from Def . 1 . Property 3 from Def . 1 follows from the fact that the event ES is verified .
This lemma gives the intuition behind the stopping condition of our algorithm : we can stop when ( 2 ) holds for the sample S under consideration , as we can then use γ = θ−ε/2 to extract FI(D,I , γ ) , which is an ( ε , δ) approximation to FI(D,I , θ ) .
The rest of this section is devoted to formalize this condition and derive a procedure to check whether ( 2 ) holds for a sample S . with probability at least 1 − δ ,
Checking whether ( 2 ) holds is equivalent to checking whether , sup A⊆I
|fD(A ) − fS(A)| ≤ ε 2 , hence we focus on bounding this quantity . 4.1 Rademacher averages φA : 2I → {0 , 1} as : fl 1 if A ⊆ B
φA(B ) =
0 otherwise for any A ⊆ I , B ⊆ I .
For each itemset A ⊆ I , we define the indicator function
When B is a transaction , φA(B ) = 1 if the itemset A appears in the transaction B . Hence , we have fD(A ) = 1 |D|
φA(τ ) and analogously for the frequency fS(A ) of A in a sample S . Assume that the sample S has size |S| = n . For each τi ∈ S , 1 ≤ i ≤ n , let σi be a Rademacher random variable , ie , a random variable taking value −1 or 1 , each with probability 1/2 . The random variables σi are independent . The ( sample ) conditional Rademacher average is the quantity
"
#
RS = Eσ
1 n sup A⊆I
σiφA(τi )
, where Eσ denotes the expectation taken only wrt the random variables σi , 1 ≤ i ≤ n ( ie , conditionally on the sample ) [ 5 , 16 ] . An important result from statistical learning theory bounds the supremum of the deviations with the conditional Rademacher average .
( Thm . 3.2 [ 5] ) . With probability at least
Theorem 1
1 − δ ,
|fD(A ) − fS(A)| ≤ 2RS + sup A⊆I
Note that the bound in the above theorem depends only on properties of the sample . Computing RS directly is not easy . It would first require to mine all itemsets from S ( ie , extracting FI(S,I , 1/|S| ) , which is excessively expensive , and then to find the expectation over the σi variables . Given that no analytical methods are currently available to compute this expectation in general , this second step would require an expensive MonteCarlo simulation [ 5 ] . Nevertheless a different result from statistical learning theory allows us to bound RS using combinatorial properties of the sample . For any itemset A ⊆ I , let vS(A ) be the n dimensional vector vS(A ) = ( φA(τ1 ) , . . . , φA(τn) ) , and let VS = {vS(A ) , A ⊆ I} . Since VS is a set rather than a bag , we have |VS| ≤ 2|I| ( and potentially |VS| 2|I| ) . ( Massart ’s Lemma , Thm . 3.3 [ 5] ) .
Theorem 2 r2 ln(2/δ )
. n p2 ln |VS|
,
RS ≤ max A⊆I kvS(A)k n where k · k denotes the Euclidean norm .
Although the above is the form in which the Theorem is usually stated , a careful reading of its proof allows us to state the following stronger version .
Theorem 3 . Let w : R+ → R+ be the function 2 ) ) . w(s ) = 1
2kvk2 exp(s
/(2n ln X
( 3 )
Then s v∈VS
RS ≤ min s∈R+ w(s ) .
1007 Proof . As in the proof for [ 5 , Thm . 3.3 ] we can use the independence of the σi ’s and the Hoeffding ’s inequality to show that , for any s > 0 and for any itemset A ⊆ I , we have
"
1 n s nX
!#
Eσ exp
σiφA(τi )
≤ exp
We can use this inequality to write
.
2n2 s2kvS(A)k2 #! !# !#
σiφA(τi )
σiφA(τi )
σiφA(τi )
. esRS = exp sEσ i=1
" ≤X ≤X
≤ Eσ
A⊆I
A⊆I exp
Eσ exp i=1
1 n max A⊆I
" nX nX
" nX s2kvS(A)k2 s max A⊆I exp
1 n
1 n i=1 i=1 s
2n2
We can now take the logarithm on both sides and divide by s ( which is strictly positive ) and we obtain w . Since the above inequalities are true for any s > 0 , we can choose the one that minimizes the rhs to obtain the thesis .
As we show later , computing the function w is too expensive for our purposes as it requires the computation of the set VS , therefore , in the following , we develop an upper bound to w that is easy and fast to compute . 4.2 Connection with Closed Itemsets collection of Closed Itemsets [ 19 ] .
We now show a connection between the set VS and the We recall that a Closed Itemset ( CI ) is an itemset A ⊆ I such that none of its proper supersets has the same frequency of A ( ie , there is no B A st fS(B ) = fS(A ) ) [ 19 ] . Let CI(S ) be the set of CIs in the sample .
Lemma 2 . The set VS contains all and only the vectors vS(A ) for all A ∈ CI(S ) :
VS = {vS(A ) , A ∈ CI(S)} , and |VS| = |CI(S)| .
To prove Lemma 2 , we need the following result . Lemma 3 . Let S ⊆ S . There is at most one CI A in S whose support set in S is TS(A ) = S . Proof . Suppose that there could be more than one CI in S with support set S , for example , wlog , two itemsets C and D . Then the support set of C ∪ D in S would be exactly S , so C and D can not be closed , as there is a superset of them with the same support set . We reached a contradiction , so the thesis is true .
We can now prove Lemma 2 . Proof of Lemma 2 . Let A be a CI in S , and let SA be the set of subsets of A with the same frequency in S as A :
SA = {B ⊆ A : fS(B ) = fS(A)} .
The elements of SA are the itemsets that appear in all and only the transactions of S where A appears . This means that , for all B ∈ SA , vS(B ) = vS(A ) . To conclude the proof it is sufficient to show that there can not be two CIs C and D in S st vS(C ) = vS(D ) . This is an immediate consequence of Lemma 3 and the proof is complete .
This result explains why computing the function w from ( 3 ) is expensive : we would need to extract the set CI(S ) of all CIs in the sample ( ie , mine the sample at frequency 1/|S| ) . In the following we develop an upper bound to w that can be computed efficiently with a single scan of the sample . 4.3 Bounding the Rademacher Average In this section we show how to efficiently bound the conditional Rademacher average RS . To do so , we define a function ˜w which is an upper bound to w from ( 3 ) in every point of R+ . The advantage of ˜w is that it can be computed using just the frequencies in the sample of the items in I and some additional information that can be obtained with a single scan of the sample . To define ˜w we need a partitioning of CI(S ) that we now introduce . Assume to sort the items in IS in increasing order by their frequency in S , ties broken arbitrarily ( eg , according to the order < on I ) . Let <i denote the resulting ordering . Given an item a , assume to sort the transactions of TS({a} ) in increasing order by the number of items they contain that come after a in the ordering <i , ties broken arbitrarily ( eg , using unique transaction identifiers ) . Let <a denote the resulting ordering . Let C1 = CI(S ) ∩ IS and C2+ be the subset of CI(S ) containing only the CIs of size at least two . We partition C2+ as follows . Let A ∈ C2+ and let a ∈ A be the item in A that comes before any other item in A according to the order “ <i ” . Let τ be transaction containing A that comes before any other transaction containing A in the order “ <a ” . Clearly a ∈ τ . We assign A to the set Ca,τ . Consider now a transaction τ ∈ TS({a} ) , and assume that it contains exactly ka,τ items that come after a in the ordering <i . In the ordering <a , the transaction τ comes
1 . before all transactions with more than ka,τ items that come after a in the ordering <i and
2 . before zero or more of the transactions with exactly ka,τ items that come after a in the ordering <i ( the exact number of such transactions depends on the tiebreaking criteria ) .
For each r ≥ 1 , let ga,r be the number of transactions in TS({a} ) containing exactly r items that come after a in the ordering <i . Let χa = max{r : ga,r > 0} and let
χaX ha,r = ga,j . j≥r
The value χa is the maximum r such that there exists at least one transaction in TS({a} ) containing exactly r items that come after a in the order <i . Each value ha,r is the number of transactions in TS({a} ) that contain at least r items that come after a in the order <i . Now , assume that τ is the ‘a,τ th transaction in the ordering <a that contains exactly ka,τ items that come after a in the ordering <i . In other words , if we consider only the transactions containing exactly ka,τ items that come after a in the ordering <i , then τ is the ‘a,τ th of such transactions in the ordering <a . We have the following result on the size of Ca,τ .
Lemma 4 . We have
|Ca,τ| ≤ 2min{ka,τ ,ha,ka,τ −‘a,τ }
.
1008 Proof . The quantity 2ka,τ is the number of subsets B of τ such that B = {a} ∪ C where C is any non empty subset of τ containing only items that come after a in the order <i . Since Ca,τ contains only itemsets that appear in τ and are in the form of B , then |Ca,τ| ≤ 2ka,τ . Consider now an itemset A ∈ Ca,τ . Apart from τ , A can only appear in transactions τ0 ∈ TS({a} ) such that τ <a τ0 , as A = {a}∪ C , for C as above . This is true for any itemset A ∈ Ca,τ . Let T denote the set of such transactions , then |T | = ha,ka,τ − ‘a,τ . From Lemma 3 we have that there is at most one CI for each set D = {τ} ∪ F of transactions , where F ⊆ T , so there at most 2ha,ka,τ −‘a,τ CIs in Ca,τ .
From Lemma 4 and the fact that
CI(S ) = C1 ∪ C2+ = C1 ∪
[ a∈IS
[
Ca,τ
τ∈TS({a} )
 ( 4 ) we have the following result on the number of Closed Itemsets in S , which is of independent interest .
Corollary 1 .
|CI(S)| ≤ |IS| + X
X a∈IS
τ∈TS({a} )
2min{ka,τ ,ha,ka,τ −‘a,τ }
.
The following lemma puts together the above results to obtain an upper bound to RS .
Lemma 5 . Let ˜w : R+ → R+ be the function lnX
χaX ga,rX a∈IS r=1 j=1
1 +
2min{r,ha,r−i}
!
!
. s2fS ( a )
2n e
˜w(s ) = 1 s
Then
RS ≤ min s∈R+
˜w(s ) .
Proof . Consider the function w from ( 3 ) . From the definition of Euclidean norm , we have that , for any A ⊆
I , kvS(A)k = pnfS(A ) . Using this fact and combining  .
Lemma 2 and the equality from ( 4 ) , we can rewrite w as
2n + X w(s ) = 1
X
X s2fS ( A ) s2fS ( a ) ln
2n e e s a∈IS
τ∈TS(a )
A∈Ca,τ
We now show that w(s ) ≤ ˜w(s ) for any s ∈ R+ . The thesis will then follow from Thm . 3 .
First of all , since C1 ⊆ IS , we have e s2 fS ( a )
2n ≤ X 2n ≤ X a∈IS s2fS ( A ) e
Then , for any a ∈ IS ,
X
X s2fS ( a )
2n e
.
2min{ka,τ ,ha,ka,τ −‘a,τ } e
X X a∈C1 a∈C1
A∈Ca,τ
τ∈TS({a} )
τ∈TS({a} ) where we used Lemma 4 to bound the size of Ca,τ and the fact that for any A ∈ Ca,τ , fS(A ) ≤ fS(a ) , given the antimonotonicity property of the frequency . Finally , we can rewrite the right hand side of this last equation as
χaX ga,rX r=1 j=1
2min{r,ha,r−j} e s2fS ( a )
2n
. r2 ln(2/δ ) |Si| ≤ ε 2 .
By combining these equations we have that w(s ) ≤ ˜w(s ) for any s ∈ R+ , and the thesis follows from Thm . 3 .
We are now ready to formally state our stopping condition that guarantees that an ( ε , δ) approximation can be computed when the condition is satisfied .
Theorem 4
( Stopping condition ) . Let i be the min imum index for which it holds that
∗ ) +
2 ˜w(s
( 5 ) Then FI(Si,I , θ−ε/2 ) is an ( ε , δ) approximation to FI(D,I , θ ) . Proof . The proof follows by combining Lemma 1 , Thm . 1 ,
Thm . 3 , and Lemma 5 . 4.4 Computing the bound
We now discuss how it is possible to check the stopping condition with a single scan of the sample . In particular , it is possible to obtain the expression for ˜w with a single scan , then its minimum of ˜w can be found by computing the value s∗ which minimizes ˜w .
Computing ˜w . To compute the expression for ˜w we only need the quantities ga,k and ha,r for any a ∈ IS and for all r , 1 ≤ r ≤ χa . These can be computed with a single scan of the sample . Indeed , the order <i can be obtained from the frequencies of the items in the sample , which we assumed to have been computed during the sample creation . Then , it is sufficient to look at each transaction τ once , sort its items according to the order <i and , for any item a ∈ τ , increment ga,ka,τ by one and increase by one all counters ha,r for 1 ≤ r ≤ ka,τ .
Minimizing ˜w . The function ˜w has first and second derivatives wrt s everywhere in R+ and it is convex , so it has a global minimum which can be found efficiently using a non linear optimization solver like NLopt [ 15 ] .
Algorithm 1 presents the pseudocode of our progressive sampling algorithm to compute an ( ε , δ) approximation to FI(D,I , θ ) . The function random_sample(D , m ) returns m transactions sampled at random with replacement from D . 4.5 Top k Frequent Itemsets
Only minor modifications are needed to obtain an algorithm for computing ( ε , δ) approximations to the set of top k FIs . The main differences from the algorithm presented in the previous section are : 1 . a stricter stopping condition ; and 2 . the need to run an exact mining algorithm on the final sample twice , one to find the top k th highest frequency ( k ) in the sample and one to extract the approximation at f S a lowered frequency threshold that depends on f
( k ) S .
Theorem 5 . Let i be the minimum index for which it s2fS ( a )
2n
, holds that r2 ln(2/δ ) |Si| ≤ ε 4 ,
2 ˜w(s
∗ ) +
( k ) Si and let f quent itemset in Si . Then FI(Si,I , f approximation to TOPK(D,I , k ) . be the frequency in Si of the k th most fre− ε/2 ) is an ( ε , δ )
( k ) Si
The proof leverages on Thm . 4 , following the same steps as the proof for [ 24 , Lemma 53 ]
1009 Algorithm 1 : Progressive sampling algorithm input : a dataset D built on alphabet I , parameters θ , ε , δ ∈ ( 0 , 1 ) , a sampling schedule ( |Si|)i≥1 . output : An ( ε , δ) approximation to FI(D,I , θ ) i ← 0 , S0 ← ∅ , |S0| ← 0 repeat i ← i + 1 S∗ ←random_sample ( D , |Si| − |Si−1| ) Si ← Si−1 ∪ S∗ //We assume that the frequencies of the items have ga,r ← 0,∀a ∈ ISi , r ∈ N ha,r ← 0,∀a ∈ ISi , r ∈ N for τ ∈ Si do been computed while creating the sample . for a ∈ τ do ka,τ ← number of items in τ that come after a in the order <a ga,ka,τ ← ga,ka,τ + 1 for j ← 1 , . . . , ka,τ do ha,j ← ha,j + 1 end end χa ← max{r : ha,r > 0} end //In the following expression , s is a symbol . ˜w(s ) ←
!
1 +
2min{r,ha,r−j} s2fS ( a )
2n e
X a∈IS ln
!
χaX ga,rX q 2 ln(2/δ ) r=1 j=1 s s∗ ← arg mins∈R+ ˜w(s ) η ← 2 ˜w(s∗ ) + |Si| until η ≤ ε/2 return FI(Si,I , θ − ε/2 )
4.6 Selecting the sampling schedule
Any non decreasing sequence ( |Si|)i≥1 can act as a sampling schedule and Provost et al . [ 21 ] showed that a geometric sampling schedule ( ie , a schedule where Si = αiS0 for some constant α ) is asymptotically optimal when checking the stopping condition takes time O(|Si| ) . Nevertheless , even such a schedule requires the user to specify two parameters : an initial sample size S0 , and a “ growth rate ” α > 1 . In our case it is possible to avoid forcing the choices of these parameters to the user , and instead allow the algorithm to select an initial sample size and then choose successive sample sizes based on the quality of the current sample . This has the net result of removing two parameters from the algorithm .
Choosing the initial sample size . We ask whether it is possible to choose the initial sample size wisely so that the algorithm does not waste time in creating and analyzing samples that are just too small for the stopping condition to be satisfied ( exceeding in the other direction , ie , having an initial sample size that is a bit too large , is not a significant issue ) . In our case it is possible to compute the “ necessary ” initial sample size S∗ 0 , ie , the minimum sample size which makes it possible for the stopping condition to be satisfied . In other words , for sample size smaller than S∗ 0 it is deterministically impossible that the stopping condition is satisfied , and therefore it is useless to create and analyze samples smaller than S∗ 0 .
Lemma 6 . Let
0 = 8 ln(2/δ ) ∗
ε2
S
( 6 )
The stopping condition ( 5 ) from Thm . 4 can not be satisfied on samples with size smaller than S∗ 0 .
Proof . Assume that there exists a sample S of size smaller 0 for which the stopping condition ( 5 ) in Thm . 4 can than S∗ be satisfied . For such a sample , we have r2 ln(2/δ )
|S|
>
ε 2 .
From this and the fact that ˜w(s ) ≥ 0 , we have that the stopping condition can not be satisfied , so we reached a contradiction and the thesis holds .
Computing the size of the next sample . We can exploit the information obtained from mining the current sample to compute a meaningful sample size for the next iteration . Assume to be at iteration i ≥ 0 of the algorithm , and let ηi be the value of the lhs of ( 5 ) at the end of the current iteration , and let |Si| be the size of the sample used in iteration i . Then at the next iteration i + 1 we use a sample of size |Si+1| , with |Si+1| =
2 |Si| .
2ηi
( 7 )
ε
The intuition behind the above formula is that ηi is , through Thm . 1 , an upper bound to the maximum deviation between the frequency in Si of any itemset and the frequency of that itemset in the original dataset . There is a necessary quadratic dependency between this measure and the sample size [ 17 ] , hence we can use εi and |Si| to compute a sample size |Si+1| for which , everything else unvaried , the error allowed in a sample of that size ( ie , the lhs of ( 5 ) ) would be at most ε/2 , as required by the stopping condition of our algorithm .
Although the method we just presented does not give any guarantee on the optimality of the schedule , our experimental evaluation results ( Sect . 5 ) show that is highly effective and results in a faster execution of the algorithm than using a geometric sample schedule , thanks to the fact that intermediate sample sizes that are probably not sufficient for computing an ( ε , δ) approximation are skipped . 4.7 Discussion
To the best of our knowledge , our algorithm improves over all progressive sampling approaches previously presented in the literature [ 7 , 8 , 14 , 18 , 20 , 21 , 26 ] because it does not require the execution of any expensive Frequent Itemsets mining algorithm on each sample to check the stopping condition . Indeed the computation of the stopping condition only requires one scan of the sample . More straightforward stopping conditions with the same requirements are possible : we explored a number of them , both empirically and theoretically , and found them substantially looser ( ie , satisfied only at larger sample size ) than the one presented in
1010 this work . We plan to include a presentation of these alternative sub par stopping conditions , and the comparison of their performances with the one from our algorithm in the extended version of this work . It is indeed necessary to strike a delicate balance between the speed of checking the stopping condition and its strictness ( ie , how early it becomes satisfied ) , otherwise the advantages of using sampling rather than analyzing the entire dataset are lost .
As the stopping condition does not depend in any way on θ , this parameter can be fixed at a later stage . This is again a consequence of the fact that we do not need to run a mining algorithm on the sample to check the stopping condition .
We remark that the dependency on 1/ε2 of the sample size can not be improved , as shown by Liberty et al . [ 17 ] . 4.8 Static sampling variant
A variant of the approach presented in previous sections can be used in a static sampling fashion . Consider the following scenario : rather than having access to the entire dataset D and being able to sample from it as much as desired , we are given a single random sample S of the dataset of some size n , a fixed parameter δ ∈ ( 0 , 1 ) , and a minimum frequency threshold θ ∈ ( 0 , 1 ] . The task requires to compute an ( ε , δ) approximation to FI(D,I , θ ) for the best possible ε . No access to the dataset is possible and no other information about the dataset is available . This scenario is realistic and actually common , as it may be easy to create ( and maintain ) one single random sample of the dataset of a specific size while the dataset is created , while obtaining access to the entire dataset may not be feasible . Previous approaches like those presented by Riondato and Upfal [ 24 ] and Chakaravarthy et al . [ 6 ] rely on characteristic quantities of the dataset ( eg , the d index [ 24 ] , or the longest transaction in the dataset [ 6 ] ) to compute a single sample size that allows to obtain the desired quality guarantees . Computing such quantities require scanning the entire dataset . Not only this may be extremely expensive for modern datasets , but it is not even possible in the scenario we just described . These approaches would then be useless in this scenario , as they have no information on the characteristic quantities they need . On the other hand , our approach only uses sampledependent quantities ( namely , the distribution of the sample frequencies of single items and related quantities ) , and can therefore compute the best ( ie , smallest ) ε obtainable from the given sample S . Indeed , it follows from Thm . 4 that such value is r2 ln(2/δ ) r2 ln(2/δ )
|Si|
|Si|
( 8 ) from Thm . 5
.
.
ε = 2
2 ˜w(s
∗ ) +
ε = 4
2 ˜w(s
∗ ) +
Similarly , to approximate TOPK(D,I , k ) : we get
Even if we relax the scenario and assume that the algorithm by Riondato and Upfal [ 24 ] ( currently the best available for static sampling ) has knowledge of an upper bound to the d index of the dataset , the results of our experimental evaluation ( Sect . 5 ) show that the value for ε computed by our approach using ( 8 ) is consistently ( although not always ) better ( ie , smaller ) than the one computed by the algorithm in [ 24 ] .
Riondato et al . [ 22 ] presented PARMA , a MapReduce algorithm for mining approximate collections of FIs . The variant presented in this section can be used in PARMA to obtain even higher quality approximation or even smaller samples .
5 . EXPERIMENTAL EVALUATION
We evaluate the performances of our algorithm by assessing the accuracy of the returned collection of FIs and by evaluating the algorithm runtime , comparing it with the time needed to extract the exact collection of FIs and to extract an approximate collection with the same guarantees using the algorithm by Riondato and Upfal [ 24 ] ( from now on denoted as VC , as it is based on VC dimension ) . We choose to compare to this static sampling approach rather than to other existing progressive sampling approaches due to the fact that no existing progressive sampling approach offers the same guarantees of our algorithm . Due to space limitations , we only report here a subset of the results . More results are available in the Appendix of the extended version [ 23 ] .
Implementation , datasets , and parameters . We implemented our algorithm in C++11 and used the C implementation by Grahne and Zhu [ 11 ] for the mining step . Our implementation is publicly available at http://csbrown edu/~matteo/radeprogrfitarbz2 We use NLopt [ 15 ] to compute the minimum of ˜w for the stopping condition ( Thm . 4 ) . We run the experiments on a machine with a quad core AMD PhenomTM II X4 955 processor and 16GB of RAM , running GNU/Linux 320 We used datasets from the FIMI’03 repository ( http://fimiuaacbe/data/ ) [ 10 ] . The characteristics of the datasets are reported in Table 1 . Each dataset is replicated a number of times ( between 200 and 1000 ) wrt its FIMI’03 version , so that its size is representative of modern datasets and the real life distributions of the frequencies of the itemsets and of the transaction length are preserved . The d bound d is a quantity used by VC to compute the sample size n as
. n = 4 ε2 d + ln 1
δ
It is , informally , the maximum index d for which the dataset contains at least d different transactions of length at least d [ 24 , Sect . 4.1 ] , and can be computed with a scan of the whole dataset .
Name accidents connect BMS POS kosarak pumsb_star retail
Repl . factor 200 1000 200 200 1000 400
Size ( |D| ) 68036601 67557000 103119400 1980001400 49046000 35264804
|I| 468 129 1657 41270 2088 16470 d bound [ 24 ] 46 43 81 443 59 58
Table 1 : Dataset characteristics
In all our experiments we fixed δ = 01 The initial sample sizes are computed according to ( 6 ) . Except when otherwise specified , we used the “ automatic ” sampling schedule described in Sect . 4.6 , ie , we used ( 7 ) to compute the size of the sample to analyze at the next iteration . The value for ε ranged in the set {0.01 , 0.012 , 0.015 , 0.017 , 002} We
1011 run our algorithm five times for each combination of parameters , in order to evaluate fluctuations in accuracy and running time due to the randomized nature of our algorithm . Unless otherwise specified , the reported quantities are the averages over the runs .
Accuracy . We evaluate the accuracy of our algorithm in terms of the recall , precision , and error in frequency estimation for the output collection . Recall . The first result of our experimental evaluation is that in all the hundreds of runs of our algorithm , for all datasets and combinations of parameters , the returned collection of itemsets always has the three properties from Def . 1 , not just with probability 1 − δ . This holds in particular for the first property ( all itemsets in FI(D,I , θ ) are in the output collection ) , which means that the recall of our algorithm is always 100 % . Precision . As for the precision of our algorithm , ie , the ratio between the size of FI(D,I , θ ) and the output size , we remark that our algorithm gives no guarantee in this sense , as any itemsets with frequency in [ θ − ε , θ ) may be in the output collection . Hence the precision depends on the distribution of the dataset frequencies in this interval . In our experiments , it varied between 15 % and 92 % , depending on the parameters and on the dataset . We also measure the fraction of the itemsets with frequencies in [ θ − ε , θ ) that are included in the output . This quantity ranges from 49 % to a vanishingly small quantity when ε ≥ θ ( indeed in this case any itemset appearing in the dataset may be included in the output ) . We report the behavior of this quantity for various values of ε in Figure 1 , for the connect dataset at θ = 072 In this figure we report the size of FI(D,I , θ ) ( “ FI ” line ) , the number of possible “ False Positives ” ( ie , the number of itemsets with frequencies in [ θ − ε , θ ) in the dataset ) , the average number of FP in the output collection , and the ratio between this latter quantity to the former ( aligned to the right vertical axis ) . We can see that the number of included FP grows slower than the number of possible FP , and therefore the ratio goes down . These False Positives are the price to pay when mining a sample of the dataset , and , by setting ε , the user understands that such False Positives are possible . In any case , our algorithm still returns a relatively compact collection of itemsets , rather than including any itemset that could theoretically be included ( ie , all the itemsets with frequencies in [ θ− ε , θ) ) . Indeed the collection can still be used , in all cases , as a set of candidates from which to compute efficiently the exact collection of FIs with a single linear scan of the dataset . The cost of this operation is almost always negligible . We remark once more that no itemset with frequency less than θ − ε was ever included in the output nor any itemset with frequency at least θ was ever missed . Frequency Estimation . For every itemset A in the output collection , we measure the absolute frequency error errS(A ) = |fS(A ) − fD(A)| , where S is the last sample analyzed . The third property from Def . 1 requires errS(A ) to be at most ε/2 . Figure 2 shows the behavior of errS on the retail dataset , with θ = 0015 The behavior for other datasets and combination of parameters was similar and can be found in the extended online version [ 23 ] . We can see that the absolute error is almost an order of magnitude less than ε/2 , both for the average and for the maximum error , and that it is very concentrated around the average . These low numbers
Figure 1 : Precision for connect , θ = 072 are not due to the fact that many itemsets in the output collection have a low frequency in the dataset : we also measure the relative frequency error , defined as 100errS(A)/fD(A ) , and we report it in Figure 2 , aligned to the right vertical axis . As we can see , this quantity was always less than 14 % In the future , we plan to develop an algorithm that gives guarantee on the relative frequency error , rather than on the absolute error .
Figure 2 : Frequency error for retail , θ = 0015
Discussion . The results of the accuracy experiments allow us to state that the algorithm performs in practice even better than what the theory guarantees . This is due to the fact that the theoretical analysis uses upper bounds that are developed for the worst case which almost never corresponds to naturally arising datasets . The results also suggest that there is room for further improvements in the derivation of these bounds and their use in pattern mining .
Runtime . The main motivation of our work is that a FI mining algorithm based on progressive can be faster than one based on static sampling as it avoids the need to compute ( or assume as known ) characteristic quantities of the dataset which would require access to the entire dataset , and it can use properties of the sample to stop at smaller sample sizes . We compare the running time of our algorithm to that of VC , to that of an exact algorithm for mining FI(D,I , θ ) from the whole dataset [ 11 ] , and to the running time of our algorithm using a geometric sampling schedule |Si| = αi|S0| for different values of α ( in these cases , the initial sample size |S0| was still computed using ( 6 ) as in all our experiments ) . The results are reported for BMS POS , θ = 0.015 in Figure 3 . Results for other datasets are sim
0008001001200140016001800200E+050E+510E+615E+620E+625E+630E+635E+604304404504604704804905FIpossible FPavg FPratioepsilonitemsetsratio0008001001200140016001800200220000020000400006000080001000120001400204060811214maxavg + stdevavgmax relavg relepsilonabsolute freq . errorrelative freq . error1012 ilar and are not reported due to space limitations , but can be found in the extended version [ 23 ] . From the plot , it is possible to appreciate that our algorithm vastly outperforms the exact algorithm and also VC . While the first fact should be expected , the latter is due to VC having to scan the dataset in order to compute the d bound , which can be relative expensive compared to our algorithm which needs no such computation . Moreover , as we discuss later , the sample size computed by VC is in most cases larger than the final sample size used by our algorithm . We also report the running time for our algorithm using a geometric sample schedule with different values ( 2.0 , 2.5 , 3.0 ) for the scaling parameter α . This allows us to evaluate the performances of the “ automatic ” sampling schedule described in Sect . 46 We can see that the automatic sampling schedule is more efficient as it allows our algorithm to run faster than with a geometric sample schedule by avoiding the creation and analysis of samples whose size is probably not sufficient for the stopping condition to be satisfied , based on information obtained from the current sample . In almost all the runs of our algorithm , for all combinations of parameters and datasets , our algorithm stops after only two iterations ( the only exception ( 3 iterations ) happens for larger values of on the kosarak dataset ) . This means that the information obtained at the minimum reasonable sample size ( as computed by ( 6 ) ) is extremely useful to compute a sufficient sample size using ( 7 ) . Instead , the runs using the geometric sample schedule stops after a variable number of iterations , which was not possible to predict in advance , and does not behave monotonically , as can be seen from Fig 3 . Hence , the use of the automatic sampling schedule is highly recommended , as it allows faster or comparable execution times and the removal of the parameter α ) , whose impact on the algorithm performances may not be clear a priori to the user .
Figure 3 : Running time for BMS POS , θ = 0015
We also analyze the breakdown of the runtime of our algorithm , splitting it between time needed to sample the transaction , time needed to evaluate the stopping condition , and time needed to perform the mining of the sample after the stopping condition is satisfied . The results are reported for the pumsb_star dataset , θ = 0.32 in Figure 4 . We can see that the runtime decreases as ε grows . This is due to the sampling time and the mining time decreasing because the algorithm stops at smaller samples for larger values of ε . The fact that the mining time decreases as ε increases is particularly interesting : the lowered frequency threshold θ − ε/2 at which the final sample is mined is smaller for larger values of ε and , on a sample of the same size , it would imply longer mining time than for lower values of ε . Instead the time saved due to the smaller sample dominates the impact of the lower threshold . It is also clear that at small values of ε , the sampling time accounts for the majority of the running time . As expected , the sampling time depends quadratically on 1/ε while the time needed to evaluate the stopping condition is almost constant . This suggests that it is indeed important to achieve a delicate balance between the cost of evaluating the stopping condition and the possibility that it is satisfied at smaller sample sizes . This was indeed one of our main guiding principles when designing our stopping condition .
Figure 4 : Breakdown of runtime for pumsb_star , θ = 032
Static sampling . We also evaluate whether the static sampling variant presented in Sect . 4.8 could outperform VC . We compared for a given sample size n , the value for ε obtained using ( 8 ) on a sample of size n , to the value εVC obtained using VC for the same sample size , which is r
εVC = d + ln(1/δ ) n
, where d is the d bound of the dataset ( values in Table 1 ) . In Figure 5 we show the results for the datasets kosarak and accidents . It is possible to see that εVC is smaller than the one computed by our method at smaller sample size on the accidents dataset , but the ε computed using ( 8 ) decreases faster as n grows and becomes smaller than εVC at larger but reasonable sample sizes . On the other hand , on kosarak our method vastly outperforms VC , with a ε that is half the one computed by VC . The datasets BMS POS , pumsbstar , and retail showed results similar to those for kosarak , while the comparison for the dataset connect was similar to that on accidents . Looking at the characteristics of the datasets connect and accidents we noticed that they have a smaller number of items , a smaller d bound , and more items with very high frequency than the other datasets . Of these characteristics , the last two are intuitively the ones with major impact on the results we see : a low d bound results in a smaller εVC , while high frequency items will have a high frequency also in the sample , resulting in higher values for w∗(s ) , which depends on the items frequencies , and therefore in a higher ε . We are currently investigating how to improve our stopping condition in these cases .
We remark again that VC requires access to the entire dataset in order to compute d , which makes it unusable in some situation , as mentioned in Sect . 48 Moreover , computing d , as we showed when presenting the runtime results , can be extremely expensive , and the loss in terms of the accuracy parameter ε may be traded off by the gain in speed .
0008001001200140016001800200E+020E+440E+460E+480E+410E+512E+514E+516E+5exactvcgeom 20geom 25geom 30avgepsilontotal runtime ( ms)00100120015001700200E+010E+420E+430E+440E+450E+460E+470E+480E+490E+410E+5miningstop conditionsamplingepsilonruntime ( ms)1013 Figure 5 : Static sampling evaluation .
The comparison is therefore a slightly unfair to our methods , given that VC is allowed to obtain crucial information by performing additional computation on the entire dataset . For these reasons , we consider our method more flexible and more powerful than VC .
6 . CONCLUSIONS
We present an algorithm for extracting a high quality approximation of the collection of FIs with probabilistic guarantees . The algorithm employs progressive sampling with a stopping condition that relies on bounding the conditional Rademacher average of the problem using easy to compute characteristic quantities of the sample . The stopping condition can therefore be evaluated very efficiently without the need to perform an expensive in depth mining of the frequent itemsets in the sample at each step . To our knowledge this is the first work that uses Rademacher averages in a knowledge discovery setting . The experimental results confirm that the algorithm is extremely successful at stopping fast at early iterations , and allows to extract very highquality approximation of the collection of FIs . Among the possible directions for future work , it would be particularly interesting to better study the trade off between the computational complexity of the stopping condition and its ability to stop at small sample sizes . We are currently investigating algorithms that give relative/multiplicative approximation guarantees , and extensions of our work to additional significance measures different from frequency [ 25 , 26 , 30 ] .
7 . ACKNOWLEDGMENTS
This work was supported by NSF grant IIS 1247581 and
NIH grant R01 CA180776 .
8 . REFERENCES
[ 1 ] R . Agrawal and R . Srikant . Fast algorithms for mining association rules in large databases . VLDB ’94 .
[ 2 ] R . Agrawal , T . Imieliński , and A . Swami . Mining association rules between sets of items in large databases . SIGMOD Rec . , 22:207–216 , June 1993 .
[ 3 ] B . Gu , B . Liu , F . Hu , and H . Liu . Efficiently determining the starting sample size for progressive sampling . ECML’01 .
[ 4 ] P . L . Bartlett , S . Boucheron , and G . Lugosi . Model selection and error estimation . Mach . Learn . , 48:85– 113 , 2002 .
[ 5 ] S . Boucheron , O . Bousquet , and G . Lugosi . Theory of classification : A survey of some recent advances . ESAIM : Probability and Statistics , 9:323–375 , 2005 .
[ 6 ] V . T . Chakaravarthy , V . Pandit , and Y . Sabharwal . Analysis of sampling techniques for association rule mining . ICDT’09 .
[ 7 ] B . Chen , P . Haas , and P . Scheuermann . A new twophase sampling based algorithm for discovering association rules . KDD’02 .
[ 8 ] K T Chuang , M S Chen , and W C Yang . Progressive sampling for association rules based on sampling error estimation . PAKDD’05 .
[ 9 ] T . Elomaa and M . Kääriäinen . Progressive Rademacher sampling . AAAI’02 .
[ 10 ] B . Goethals and M . J . Zaki . Advances in frequent itemset mining implementations : report on FIMI’03 . SIGKDD Explor . Newsl . , 6(1):109–117 , June 2004 .
[ 11 ] G . Grahne and J . Zhu . Efficiently using prefix trees in mining frequent itemsets . FIMI’03 .
[ 12 ] J . Han , J . Pei , and Y . Yin . Mining frequent patterns without candidate generation . SIGMOD’00 .
[ 13 ] J . Han , H . Cheng , D . Xin , and X . Yan . Frequent pattern mining : current status and future directions . Data Mining and Knowl . Disc . , 15:55–86 , 2007 .
[ 14 ] G . H . John and P . Langley . Static versus dynamic sam pling for data mining . KDD’96 .
[ 15 ] S . G . Johnson . The NLopt nonlinear optimization pack age . URL http://ab initiomitedu/nlopt
[ 16 ] V . Koltchinskii . Rademacher penalties and structural IEEE Trans . Inf . Theory , 47(5 ) : risk minimization . 1902–1914 , July 2001 .
[ 17 ] E . Liberty , M . Mitzenmacher , J . Thaler , and J . Ullman . Space lower bounds for itemset frequency sketches . CoRR , 1407.3740 , July 2014 .
[ 18 ] S . Parthasarathy . Efficient progressive sampling for as sociation rules . ICDM’02 .
[ 19 ] N . Pasquier , Y . Bastide , R . Taouil , and L . Lakhal . Discovering frequent closed itemsets for association rules . ICDT’99 .
[ 20 ] A . Pietracaprina , M . Riondato , E . Upfal , and F . Vandin . Mining top k frequent itemsets through progressive sampling . Data Mining Knowl . Disc . , 21(2 ) : 310–326 , 2010 .
[ 21 ] F . Provost , D . Jensen , and T . Oates . Efficient progres sive sampling . KDD’99 .
[ 22 ] M . Riondato , J . A . DeBrabant , R . Fonseca , and E . Upfal . PARMA : A parallel randomized algorithm for association rules mining in MapReduce . CIKM’12 .
[ 23 ] M . Riondato and E . Upfal . Mining frequent itemsets through progressive sampling with Rademacher averages . Extended Version . URL http://csbrownedu/ %7Ematteo/papers/progrsamplfi extpdf
[ 24 ] M . Riondato and E . Upfal . Efficient discovery of association rules and frequent itemsets through sampling with tight performance guarantees . ACM Trans . Knowl . Disc . from Data , 8(2 ) , 2014 .
[ 25 ] M . Riondato and F . Vandin . Finding the true frequent
[ 29 ] V . N . Vapnik . The Nature of Statistical Learning Theory . Statistics for engineering and information science . Springer Verlag , New York , NY , USA , 1999 .
[ 30 ] G . I . Webb . Discovering significant patterns . Mach .
Learn . , 68(1):1–33 , 2007 . itemsets . SDM’14 .
[ 26 ] T . Scheffer and S . Wrobel . Finding the most interesting patterns in a database quickly by using sequential sampling . J . Mach . Learn . Res . , 3:833–862 , Dec . 2002 . [ 27 ] S . Shalev Shwartz and S . Ben David . Understanding Machine Learning : From Theory to Algorithms . Cambridge University Press , 2014 .
[ 28 ] H . Toivonen . Sampling large databases for association rules . VLDB’96 .
00E+020E+640E+60002004006008kosarakVCThis worksample sizeepsilon00E+020E+640E+60001002003004accidentsVCThis worksample sizeepsilon1014
