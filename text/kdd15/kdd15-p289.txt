Who Supported Obama in 2012 ?
Ecological Inference through Distribution Regression
Seth R . Flaxman
Machine Learning Department and H . J . Heinz III College Carnegie Mellon University sflaxman@cscmuedu
Yu Xiang Wang
Machine Learning Department
Carnegie Mellon University yuxiangw@cscmuedu
Alexander J . Smola
Machine Learning Department
Carnegie Mellon University and Marianas Labs alex@smola.org
ABSTRACT We present a new solution to the “ ecological inference ” problem , of learning individual level associations from aggregate data . This problem has a long history and has attracted much attention , debate , claims that it is unsolvable , and purported solutions . Unlike other ecological inference techniques , our method makes use of unlabeled individual level data by embedding the distribution over these predictors into a vector in Hilbert space . Our approach relies on recent learning theory results for distribution regression , using kernel embeddings of distributions . Our novel approach to distribution regression exploits the connection between Gaussian process regression and kernel ridge regression , giving us a coherent , Bayesian approach to learning and inference and a convenient way to include prior information in the form of a spatial covariance function . Our approach is highly scalable as it relies on FastFood , a randomized explicit feature representation for kernel embeddings . We apply our approach to the challenging political science problem of modeling the voting behavior of demographic groups based on aggregate voting data . We consider the 2012 US Presidential election , and ask : what was the probability that members of various demographic groups supported Barack Obama , and how did this vary spatially across the country ? Our results match standard survey based exit polling data for the small number of states for which it is available , and serve to fill in the large gaps in this data , at a much higher degree of granularity .
Keywords Machine learning ; supervised learning ; kernel methods ; Gaussian processes ; distribution regression
1 .
INTRODUCTION
The name ecological inference refers to the idea of ecological correlations [ 28 ] , that is correlations between variables observed for a group of individuals , as opposed to individual correlations , where the individuals are the unit of anal ysis . The ecological inference problem has much in common with the “ modifiable areal unit problem ” [ 20 ] and Simpson ’s paradox . Simply put , it is the problem of inferring individual correlations from ecological correlations . This challenge arises in computational advertising , healthcare data , opinion survey data , and population health data , because in each case for privacy or cost reasons , we are missing individuallevel data , we have access to aggregate level data , and we want to make individual level predictions . One way to understand the reason it is called a “ problem ” is to consider a two by two contingency table , with unknown entries inside the table , and known marginals . As shown in the contingency table below , we might know that a certain electoral district ’s voting population is 43 % men and 57 % women and that in the last election , the outcome was 63 % in favor of the Democratic candidate and 37 % in favor of the Republican candidate . These percentages correspond to the numIs it possible to infer the bers of individuals shown below :
Democrat Republican
Men
Women
? ?
? ?
1,500 2,000
2,200
1,300 joint and thus conditional probabilities , for example can we ask , what was the Democratic candidate ’s vote share among women voters ? It is clear that only very loose bounds can be placed on these probabilities without any more information . Based on the fact that rows and columns must sum to their marginals , we know , eg , that the number of Democrats who are men is between 0 and 1,500 . These types of deterministic bounds have been around since the 1950 ’s , under the name the method of bounds [ 4 ] .
What if we are given a set of electoral districts , where for each we know the marginals of the two by two contingency table , but none of the inner entries ? Then , thinking statistically , we might be tempted to run a regression , predicting the electoral outcomes based on the gender breakdowns of the districts . But this approach , formalized as Goodman ’s method [ 8 ] a few years after the method of bounds was proposed , can easily lead us astray—there is not even a guarantee that outcomes be bounded between 0 and 1 , and it ignores potentially useful information provided by deterministic bounds .
We review related work in Section 2 and provide the necessary background on kernel embeddings of distributions , distribution regression , and GP regression in Section 3 . We formalize the ecological inference problem in Section 4 and propose our method in Section 5 . We apply it to the case of the 2012 US presidential election in Section 6 , comparing our results to survey based exit polls .
289 2 . RELATED WORK
The ecological inference problem has a long history of solutions , counter solutions , and it is often taught with a note of grave caution and stark warnings that ecological inference is to be avoided at all costs , usually in favor of individual level surveys . As with Simpson ’s paradox , it should come as no surprise that correlations at one level of aggregation can and do flip signs at other levels of aggregation . But abandoning all attempts at ecological inference in favor of surveys is not feasible or appropriate in many circumstances—relevant respondents are no longer alive to answer historical questions of interest ; subjects are reluctant to answer questions about sensitive topics like drug usage or cheating—meaning social scientists have been hard pressed and even discouraged from studying many interesting and important questions . Ecological inference problems appear in demography , sociology , geography , and political science , and—as discussed in [ 13]— landmark legislation in the US such as the Voting Rights Act requires a solution to the ecological inference problem to understand racial voting patterns1 .
This problem has attracted a variety of approaches over the years as summarized in [ 13 ] , which also proposes a Bayesian statistical modeling framework incorporating the method of bounds ( thus uniting the deterministic and probabilistic approaches ) . [ 13 ] sparked a renewed interest in ecological inference , much of which is summarized in [ 14 ] . A parametric Bayesian approach to this setting was proposed in [ 12 ] and a semiparametric approach was proposed in [ 23 ] .
Our method differs from existing methods in fours ways . First , it uses more information than is typically considered in a standard ecological regression setting : we assume that we have access to representative unlabeled individual level data . In the voting example , this means having a sample of individual level census records ( “ microdata ” ) about each electoral district . Second , our method incorporates spatial variation . Spatial data is a common feature of ecological regressions ( which , after all , usually have much to do with geography ) but it is only very recently that ecological inference methods have begun to address spatial variation explicitly [ 14 ] . Third , while our method may be applied to the classic ecological inference problem of inferring individual level correlations from aggregate data , we propose that it is most well suited to a related ecological problem , common in political science : inferring the unobserved behavior of subgroups based on the aggregate behavior of groups of which they are part . For our application , this means inferring the voting behavior of men and women separately by electoral district , given aggregate voting information by district . Finally , our work is nonparametric . Kernel embeddings are used to capture all moments of the probability distribution over covariates , and Gaussian process regression is used to non parametrically model the dependence between predictors and labels .
A related line of work , termed “ learning from label proportions ” by some authors [ 24 , 15 , 30 , 21 ] , has the individuallevel goal in mind , and aims to build a classifier for individual instances based only on group level label proportions . While in principle , this approach could be used in our set
1Long standing solutions have proved quite inadequate : in one court case involving the Voting Rights Act , a qualified expert testified , based on Goodman ’s method , that the percentage of blacks who were registered to vote in a certain electoral district exceeded 100 % [ 13 ] . This evidently false claim was apparently made earnestly . ting , since we are only interested in subgroup level predictions the extra task of estimating individual level predictions is probably not worth the effort considering we are working with n = 10 million individuals .
Our method is based on recent advances in distribution regression [ 6 , 33 ] , which we generalize to address the ecological inference case . Previous work on distribution regression has relied on kernel ridge regression , but we use Gaussian process ( GP ) regression instead , thus enabling us to incorporate spatial variation , learn kernel hyperparameters , and provide posterior uncertainty intervals , all in a fully Bayesian setting . For scalability ( our experiments use n = 10 million individuals ) , we use a randomized explicit feature representation ( “ FastFood ” ) [ 16 ] rather than the kernel trick . 3 . BACKGROUND
In this section we review kernel embeddings for probability distributions , distribution regression , FastFood , and Gaussian process ( GP ) regression . 3.1 Kernel embeddings of distributions
Kernel embeddings of distributions , eg [ 31 , 32 , 5 ] , are a powerful class of reproducing kernel Hilbert space ( RKHS ) techniques that map joint , marginal and conditional probability distributions to vectors in a high ( or infinite ) dimensional feature space . Let φ : Rn → H . It has been shown that if a kernel map φ is universal/characteristic ( eg a Gaussian RBF kernel ) , then for iid samples x ∼ X , the mean embedding in feature space , denoted :
µX = Ex∼X [ φ(x ) ]
( 1 ) completely characterizes the distribution in the sense that any two distributions with a difference in any moment will be mapped to a different point in the Hilbert space . This result has been used in a variety of kernel based statistical tests , including tests of independence and two sample tests [ 10 ] . It is a key feature of our method , because it will allow us to link aggregate labels to individual level data without throwing out any information .
In this work , we use the simple empirical mean estimator for the kernel mean:µX = j
1 N
φ(xj )
( 2 )
It is shown in Smola et al . [ 31 ] that this plug in estimator is a consistent , and it converges to µX with rate O(Rn(H ) + √ n ) , where Rn(H ) is the Rademacher complexity of the 1/ RKHS . As long as Rn(H ) = O(n−1/2 ) we have the ( optimal ) parametric rate . Recent work has focused on improving this estimator using James Stein shrinkage [ 17 ] . 3.2 Distribution regression
In this section , we formalize distribution regression , the task of learning a classifier or a regression function that maps probability distributions to labels . The problem is fundamentally challenging because we only observe the probability distributions through groups of samples from these distributions . Specifically , our dataset is structured as follows :
1}N1 j=1 , y1
2}N2 j=1 , y2
, . . . n}Nn j=1 , yn
( 3 )
{xj
{xj
,
{xj where group i has a single real valued label yi and Ni individual observations ( eg demographic covariates for Ni individuals ) denoted xj i ∈ Rd .
290 To admit a theoretical analysis , it is assumed that the probability distributions themselves are drawn randomly from some unknown meta distribution of probability distributions . The intuition behind why distribution regression is possible is that if each group of samples are iid draws from a distribution which is itself an iid drawn from the meta distribution , then we will be able to learn .
Recently , this “ two stage sampled ” structure was analyzed , showing that a ridge regression estimator is consistent [ 33 ] with polynomial rate of convergence for almost any metadistribution of distributions that are sufficiently smooth . The basic approach is as follows : use the kernel mean estimator of Eq ( 2 ) for each group separately to estimate :
µ1 =
1 N1
N1 j=1
φ(xj
1 ) ,
Nn j=1
1 Nn
φ(xj n )
( 4 )
Next , use kernel ridge regression [ 29 ] to learn a function f :
. . . , µn = y = f ( µ ) +
( 5 )
( 6 ) where the objective is to minimize the L2 loss subject to a “ ridge ” complexity penalty weighted by a positive constant λ :
[ yi − f ( µi)]2 + λf2Hf
ˆf = arg minf∈Hf i
In [ 33 ] a variety of kernels for f corresponding to the Hilbert space Hf are considered . We follow the simplest choice of the linear kernel k(µi,µj ) = µi,µj , motivated by the fact that we are already working in Hilbert space over the µi . Following the standard derivation of kernel ridge regression [ 29 ] , we can find the function f in closed form for a new test group µ∗ :
∗
( 7 )
−1[y1 , . . . , yn]T
( K + λI ) f ( µ∗ ) = k
= [ µ1 , µ∗ , . . . ,µn , µ∗ ] and Kab = µa,µb . In where k∗ practice , it is hard to know whether the conditions under which the proofs in these papers hold are met . As a partial remedy , our Bayesian approach allows us to quantify the degree of uncertainty in our posterior predictions . Also , as shown in the experiments , a useful diagnostic is to measure the distance between training and test distributions . 3.3 FastFood for explicit kernel expansion
1 j1j2
NaNb
Naively implementing distribution regression using the kernel trick is not scalable in the setting we consider : to com pute just one entry in K requires computing Kab = µa,µb = a , xj2 k(xj1 b ) . This computation is O(N 2 ) ( where we assume for simplicity Ni = N , ∀i ) so computing K is O(n2N 2 ) . In our application , N ≈ 104 , so we need a much more scalable approach . Since we ultimately only need to work with the mean embeddings µi rather than the individual observations xj i , an explicit feature representation , even if it is very high dimensional , will drastically reduce our computational costs . We use an approximate kernel transformation called FastFood [ 16 ] , which finds a d dimensional approximation ˆφ(x ) ∈ Rd of φ(x ) for every x . Here φ can be any radial basis function ( RBF ) kernel . Take Gaussian RBF kernel as an example , FastFood boils down to the following transformation due to Rahimi and Recht [ 25 ] :
φ(x ) = p
−1/2 exp(i[V x ] ) where i is the imaginary unit of a complex number and V is a appropriately scaled p × d Gaussian random matrix with p > d . FastFood allows us to approximately compute V x without explicitly construct V . In particular , FastFood transformation takes V = [ V T 2 , , V T(cid:100)p/d]T and each square d × d matrix is given by :
1 , V T
Vj =
√ 1 d
σ
SHGΠHB ,
It is shown in [ 16 , Theorem 6 ] that for any x , x ) with rate O( log(2/δ )
Here S , B , G are diagonal random matrices ( nonnegative scaling , Rademacher and Gaussian respectively ) , Π is a random permutation , and H is the Walsh Hadamard matrix . Every single one of these transformation can be computed in almost linear time . The whole transformation φ(x ) can be therefore computed in O(p log d ) time . This is orders of magnitude faster than random kitchen sinks [ 25 ] which costs O(pd ) per transformation or the kernel trick which needs to do an O(N 3 ) inversion of a dense N × N kernel matrix . , ˆφ(x ) , ˆφ(x converges to φ(x ) , φ(x ) where δ is the failure probability . This can be viewed as a JohnsonLindenstrauss transformation of an infinite dimensional space to a finite dimensional Euclidean space while preserving the angles and distances in the original space . While this is not a uniform convergence bound as with the random features in [ 25 ] , the exponential tail enables us to simultaneously guarantee an exponential number of kernel evaluations via the union bound . Not surprisingly , it has been empirically shown to be comparable in accuracy and to approximate the kernel transformation for all data points quite well . For simplicity , from here onwards we will overload notation and refer to φ(x ) ∈ Rp as our feature mapping which is understood to be approximated with FastFood . 3.4 Gaussian process regression p1/2
)
In this section , we briefly state the main results we need from Gaussian process regression [ 26 ] , reviewing the wellknown connection between the posterior mean in GP regression and the kernel ridge regression estimator of Eq ( 7 ) .
Given observations ( s1 , y1 ) , . . . ( sn , yn ) a Gaussian process prior on a function f where our model is y = f ( s ) + is written : f ∼ GP(0 , k(s , s
) ) with mean 0 and covariance function k . This implies that for a finite set of locations X = {s1 , . . . , sn} , the distribution of f = [ f ( s1 ) , . . . , f ( sn ) ] is multivariate Gaussian : f ∼ N ( 0 , K )
( 8 ) where Kij = k(si , sj ) . Notice that we have switched from a function f ( s ) to a vector f . This is because it is only formally correct to consider a probability distribution over the finite dimensional vector f , not over the infinite dimensional function f ( s ) . For a formal discussion see [ 35 ] . Conditional on the latent variable f , we have a Gaussian observation model : yi|f ( si ) ∼ N ( 0 , σ2 ) , ∀i
( 9 ) for variance parameter σ2 which can be thought of as measurement error ( known as the “ nugget ” in geostatistics ) . For a fixed set of locations X , it is straightforward to sample f from its prior distribution in Eq ( 8 ) . Due to conjugacy , we can marginalize out f in closed form to find the distribution : y ∼ N ( 0 , K + σ2I )
( 10 )
291 ∗
∗
∗
, X , y ∼ N ( k
If we wish to make a prediction at a new location s∗ , the standard predictive equations for GP regression [ 26 ] , derived by conditioning a multivariate Gaussian distribution , tell us : ∗ | s ∗ ( K +σ2I ) y ) ( 11 ) where Kij = k(si , sj ) and k∗ ) ] and k∗∗ ) . Thus we have a way of combining a prior over f , parametrized by k(s , s ) , with observed data to obtain a posterior distribution over a new prediction y∗ at a new location s∗ . This is a very powerful method , as it enables a fully Bayesian treatment of regression , a coherent approach to kernel learning through the marginal likelihood ( for details see [ 26] ) , and posterior uncertainty intervals .
∗∗−k −1y , k = [ k(s1 , s∗
( K +σ2I ) ) . . . k(sn , s∗
= k(s∗ , s∗
−1k
We can immediately see the connection between the kernel ridge regression estimator in Eq ( 7 ) and the posterior mean of the GP in Eq ( 11 ) . ( A superficial difference is that in Eq ( 7 ) our predictors are µi while in Eq ( 11 ) they are generic locations si , but this difference will go away in Section 5 when we propose using GP regression for distribution regression . ) The predictive mean of GP regression is exactly equal to the kernel ridge regression estimator , with σ2 corresponding to λ . In ridge regression , a larger penalty λ leads to a smoother fit ( equivalently , less overfitting ) , while in GP regression a larger σ2 favors a smoother GP posterior because it implies more measurement error . For a full discussion of the connections see [ 2 , Sections 622 623 ]
4 . ECOLOGICAL INFERENCE
In this section we state the ecological inference problem that we intend to solve . We use the motivating example of inferring Barack Obama ’s vote share by demographic subgroup ( eg men versus women ) in the 2012 US presidential election , without access to any individual level labels . Vote totals by electoral precinct are publicly available , and these provide the labels in our problem . Predictors are in the form of demographic covariates about individuals ( eg from a survey with individual level data like the census ) . The challenge is that the labels are aggregate , so it is impossible to know which candidate was selected by any particular individual . This explains the terminology : “ ecological correlations ” are correlations between variables which are only available as aggregates at the group level [ 28 ] i ∈ Rd be a vector of covariates for individual i in region j . Let wj i be survey weights2 . Let yi be labels in the form of twodimensional vectors ( ki , ni ) where ki is the number of votes received by Obama out of ni total votes in region i . Then our dataset is :
We use the same notation as in Section 32 Let xj
1}N1 j=1 , y1
2}N2 j=1 , y2
, . . . , n}Nn j=1 , yn
( 12 )
{xj
,
{xj
{xj
We will typically have a rich set of covariates available , in addition to the demographic variables we are interested in stratifying on , so the xj i will be high dimensional vectors denoting gender , age , income , education , etc .
Our task is to learn a function f from a demographic subgroup ( which could be everyone ) within region i to the probability that this demographic subgroup supported Obama , 2Covariates usually come from a survey based on a random sample of individuals . Typically , surveys are reported with survey weights wj i for each individual to correct for oversampling and non response , which must be taken into account for any valid inference ( eg summary statistics , regression coefficients , standard errors , etc ) ie the number of votes this group gave Obama divided by the total number of votes in this group . 5 . OUR METHOD
In this section we propose our new ecological inference method . Our approach is illustrated in a schematic in Figure 1 and formally stated in Algorithm 1 .
Illustration of our approach .
Figure 1 : Labels y1 , y2 and y3 are available at the group level giving Obama ’s vote share in regions 1 , 2 , and 3 . Covariates are available at the individual level giving the demographic characteristics of a sample of individuals in regions 1 , 2 , and 3 . We project the individuals from each group into feature space using a feature map φ(x ) and take the mean by group to find high dimensional vectors µ1 , µ2 and µ3 , eg µ1 = 1 1) ) . Now our problem is reduced to supervised learning , where we want to learn a function f : µ → y . Once we have learned f we make subgroup predictions for men and women in region 3 by calculating mean embeddings for the men µm 3 ) ) and women µw 3 = 1 3 ) ) and then calculating f ( µm 3 ) . For a more rigorous description of our algorithm see Algorithm 1 .
3 = 1 3 ) + φ(x5
3 ) and f ( µw
3 ) + φ(x2
1 ) + φ(x2
1 ) + φ(x3
3 ) + φ(x4
3 ( φ(x1
3 ( φ(x1
2 ( φ(x3
Recall the two stage distribution regression approach introduced in Section 32 Our method has a similar approach . To begin , we use FastFood as introduced in Section 3.3 with an RBF kernel to produce an explicit feature map φ and calculate the mean embeddings3 , one for each region i , of Eq ( 4 ) with survey weights : j wj
1φ(xj 1 ) j wj
1
,
. . . , µn = j wj nφ(xj n ) j wj n
( 13 )
µ1 =
3 Distribution regression with explicit random features was previously considered in Oliva et al . [ 19 ] using Rahimi and Recht [ 25 ] to speed up an earlier distribution regression method based on kernel density estimation [ 22 ] . This approach has comparable statistical guarantees to distribution regression using RKHS mean embeddings but inferior empirical performance [ 33 ] . As far as we are aware , using FastFood kernel mean embeddings for distribution regression is a novel approach . x11x21x31µ1x13x23x33x43x53µ3µw3µm3womenmenµ2x12x22x32%voteforObamafeaturespaceregion1region2region3bothy1y2y3??292 j=1 , sn , yn
1 , wj
, . . . ,,{(xj
Algorithm 1 Ecological inference algorithm
{(xj Calculate µi using Eq ( 13 ) with FastFood .
1)}N1 Input : 1 : for i = 1 . . . n do 2 : 3 : 4 : end for 5 : Learn hyperparameters ˆθ = ( σ2 i using Eq ( 17 ) with FastFood .
Calculate µm j=1 , s1 , y1 n)}Nn n , wj x , σ2 s , ) of the GP model specified by Eqs . ( 14)–(15 ) with observations yi at lo cations ( µ1 , s1 ) , . . . , ( µn , sn ) using gradient descent and the Laplace approximation .
6 : Make posterior predictions using locations n , sn ) using the Laplace approximation .
( µm
1 , s1 ) , . . . , ( µm
ˆθ at
Output : Posterior means and variances for ym
1 , . . . , ym n
Next , instead of kernel ridge regression , we use GP regression . Recall that unlike in distribution regression our labels yi are given by vote counts ( ki , ni ) . We use a Binomial likelihood as the observation model in GP regression ( this is sometimes known as a logistic Gaussian process [ 27] ) . We transform each component of the latent real valued vector f of Section 3.4 by the logistic link function σ(f ) = 1 1+e−f and we replace Eq ( 9 ) with the following : ki|f ( xi ) ∼ Binomial(ni , σ(f ( xi) ) )
( 14 ) where we use the formulation for the Binomial distribution of ni trials and probability of success σ(f ( xi) ) . This is the generalized linear model ( GLM ) specification for binary data , combining a Binomial distribution with logistic link function [ 3 , Ch . 7 ] .
The predictors in our GP are the mean embeddingsµ1 , . . . ,µn .
We also include spatial information in the form of 2 dimensional spatial coordinates si giving the centroid of region i . Putting these predictors together we adopt an additive covariance structure : f ∼ GP(0 , σ2 xµi,µj + ks(si , sj ) )
( 15 )
Where we have used a linear kernel between mean embeddings weighted by a variance parameter σ2 x . Since the mean embeddings are already in feature space using the FastFood approximation to the RBF kernel , we are approximately using the RBF kernel . For the spatial coordinates we use the Mat´ern covariance function which is a popular choice in spatial statistics [ 11 ] , with ν = 3/2 , length scale and variance parameter σ2 s : exp s − s√
3
−s − s√
3
( 16 ) k(s , s
) = σ2 s
1 +
By adding together the linear kernel between mean embeddings and the spatial covariance function , we allow for a smoothly varying surface over space and demographics . The intuition is that this additive covariance encourages predictions for regions which are nearby in space and have similar demographic compositions to be similar ; predictions for regions which are far away or have different demographics are allowed to be less similar . GP regression with a spatial covariance function is equivalent to the spatial statistics technique of kriging—we are effectively smoothly interpolating y values over a very high dimensional space of predictors . Another way to think about additivity is that we are accounting for a spatially autocorrelated error structure in
µm i = jm wj
1φ(xj 1 ) jm wj
1
, ∀i
( 17 ) the predictions we get from covariates alone . ( We also considered a multiplicative structure , which had slightly worse performance . )
Eq s ( 14) (15 ) complete our hierarchical model specification . For non Gaussian observation models like Eq ( 14 ) , the posterior prediction in Eq ( 11 ) is no longer available in closed form due to non conjugacy . We follow the standard approach for GP classification and logistic Gaussian processes and use the Laplace approximation [ 36 , 27 ] . The Laplace approximation gives an approximate posterior distribution for f , from which we can calculate a posterior distribution over the ki of Eq ( 14 ) as explained in detail in [ 26 , Section 342 ] The Laplace approximation also allows us to calculate the marginal likelihood , which is the probability of the observed data , integrating out f . To learn σ2 x , σ2 s , and , we use gradient ascent to maximize the log marginal likelihood .
Once we have learned the best set of hyperparameters for our model we can make predictions for any demographic subgroup of interest . To predict the fraction of men who voted for Obama , we create new mean embedding vectors by gender and region , modifying Eq ( 13 ) : i and µm i where jm are the indices of the observations of men in region is the mean embedding of the covariates for the men in region i . We then make posterior predictions using the Laplace approximation as above at these new genderregion predictors . Notice that for a new µ∗ this requires calculating k∗ = [ k1∗ , k2∗ , . . . , kn∗ ] of Eq ( 11 ) where ki∗ = σ2 will be similar to existing predictions in regions with similar covariates and they will be similar to existing predictions at the same ( and nearby ) locations . xµi , µ∗ + ks(si , s∗ ) using Eq ( 15 ) . Thus new predictions
Our algorithm is stated in Algorithm 1 . We now analyze its complexity . Lines 2–3 are calculated by streaming through the data for individuals . For each individual , calculating the FastFood feature transformation φ(xj i ) takes i ∈ Rd and φ(xj O(p log d ) where xj i ) ∈ Rp . To save memory , weighted average µi by adding wi there ’s no need to store each φ(xj i ) . We simply update the jφ(xj i ) to it . Notice that the demographic subgroup considered in line 3 is simply a subset of the observations calculated in line 2 , so there is no added cost to calculate the µm for q different demographic subgroups of interest . Overall , if we have N individuals the for loop takes time O(N p log d ) . Usually p N and d N so this is practically linear and trivially parallelizable . i or indeed a set of µm1
, . . . , µmq i i
On line 5 to learn the hyperparameters in the GP regression requires calculations involving the covariance matrix K ∈ Rn×n . Each entry in K requires computing a dot product µi,µj which takes O(p ) and it requires computing the Mat´ern kernel for the spatial locations , which is a fast arithmetic calculation . Once we have K , the Laplace approximation is usually implemented with Cholesky decompositions for numerical reasons . The runtime of computing the marginal likelihood and relevant gradients is O(n3 ) [ 26 ] , and gradient ascent usually takes less than a hundred steps to converge . Posterior predictions on line 6 require calculating k∗ ∈ R1×n for each µm so this is O(n2 ) . Reusing the Cholesky decompositions above means predictions can be made in O(n2 ) . GP regression requires O(n2 ) storage . i
293 Overall , we expect n N , so our algorithm is practically O(N ) , with little extra computational cost arising from the GP regression as compared to the work of streaming through all the observations . The N observations do not need to be stored in memory , so the overall memory complexity is only O(n2 ) .
6 . EXPERIMENTS
In this section , we describe our experimental evaluation , using data from the 2012 US Presidential election , and compare our results to survey based exit polls , which are only available for the 18 states for which large enough samples were obtained . Our method enables us to fill in the full picture , with much finer grained spatial estimation and results for a much richer variety of demographic variables . This demonstration shows the applicability of our new method to a large body of political science literature ( see , eg [ 7 ] ) on voting patterns by demographics and geography . Because voting behavior is unobservable and due to the ecological inference problem , previous work has been mostly based on exit polls or opinion polls .
We obtained vote totals for the 2012 US Presidential Election at the county level4 . Most voters chose to either reelect President Barack Obama or vote for the Republican party candidate , Mitt Romney . A small fraction of voters ( < 2 % across the country ) chose a third party candidate . Separately , we obtained data from the US Census , specifically the 2006 2010 American Community Survey ’s Public Use Microdata Sample ( PUMS ) . The American Community Survey is an ongoing survey that supplements the decennial US census and provides demographically representatives individual level observations . PUMS data is coded by public use microdata areas ( PUMAs ) , contiguous geographic regions of at least 100,000 people , nested within states . We used the 5 year PUMS file ( rather than a 1 year or 3 year sample ) because it contains a larger sample and thus there is less censoring for privacy reasons . To merge the PUMS data with the 2012 election results , we created a mapping between counties and PUMAs5 , merging individual level census data and aggregating vote totals as necessary to create larger geographic regions for which the census data and electoral data coincided . The mapping between PUMAs and counties is many to many , so we were effectively finding the connected components . Since counties and PUMAs do not cross state borders , none of the geographic regions we created cross state borders . An example is shown in Figure 2 .
In total , we ended up with 837 geographic regions ranging from Orleans Parish in New Orleans , which voted 91 % for Barack Obama to Davis County , a suburb of Salt Lake City , Utah which voted 84 % for Mitt Romney . For the census data , we excluded individuals under the age of 18 ( voting age in the US ) and non citizens ( only citizens can vote in presidential elections ) . There were a total of 10,787,907 individual level observations , or in other words , almost 11 million people included in the survey . The mean number of people per geographic region was 12,812 with standard deviation 21,939 .
There were 223 variables in the census data , including both categorical variables such as race , occupation , and educational attainment and real valued variables such as in
4https://github.com/huffpostdata/election 2012 results 5using and the tool http://mcdcmissouriedu/websas/geocorr12html the PUMA 2000 codes at
( a )
( b )
Figure 2 : Election outcomes were available for the 67 counties in Florida shown in ( a ) . Demographic data from the American Community Survey was available for 127 public use microdata areas ( PUMAs ) in Florida , which sometimes overlapped parts of multiple counties and sometimes contained multiple counties . We merged counties and PUMAs as described in text to create a set of disjoint regions with the result of 37 electoral regions as shown in ( b ) . come in past 12 months ( in dollars ) and travel time to work ( in minutes ) . We divided the real valued variables by their standard deviation to put them all on the same scale . For the categorical variables with D categories , we converted them into D dimensional 0/1 indicator variables , ie for the variable “ when last worked ” with categories 1 = “ within the past 12 months , ” 2 = “ 1 5 years ago , ” and 3 = “ over 5 years ago or never worked ” we mapped 1 to [ 1 0 0]T , 2 to [ 0 1 0 ] and 3 to [ 0 0 1 ] .
Putting together the indicator variables and real valued variables , we ended up with 3,251 variables total . For every single individual level observation , we used FastFood with an RBF kernel to generate a 4,096 dimensional feature representation . Using Eq ( 13 ) we calculated the weighted mean embedding for each region . The result was a set of 837 vectors which were 4,096 dimensional .
We treated the vote totals for Obama and Romney as is , discarded the remaining third party votes as the exit polls we use for validation did not report third party votes . Thus for each region , we had a positive integer valued 2 dimensional label giving the number of votes for Obama and the total number of votes .
We focused on the ecological inference problem of predicting Obama ’s vote share by the following demographic groups : women , men , income ≤ US$50,000 per year , income between $50,000 and $100,000 per year , income ≥ 100,000 per year , ages 18 29 , 30 44 , 45 64 , and 64 plus . For each region , we used the strategy outlined above , restricting our census sample to only those observations matching the subgroup of interest and creating new mean embedding predictors as in Eq ( 17 ) , µsubgroup . We made predictions for each region demographic pair . Note that we have made our task harder than necessary to demonstrate our method ; we could have trained our model using the exit polling data , where available , and we would certainly recommend practitioners use all available data to get the best possible estimates . i
All of our models were fit using the GPstuff package with scaled conjugate gradient optimization and the Laplace approximation [ 34 ] . Since n N , the time required to fit the GP model and make predictions is much less than the time
294 ( a ) Exit poll results for women
( b ) Exit poll results for men
( c ) Ecological regression results for women
( d ) Ecological regression results for men
Figure 3 : Support for Obama among women ( a ) and men ( b ) in the 18 states for which exit polling was done ; due to cost , no representative data was collected for the majority of states or for regions smaller than states . Support for Obama among women ( c ) and men ( d ) in 837 different regions as inferred using our ecological regression method . required to preprocess the data to create the mean embeddings at the beginning of Algorithm 1 . 7 . RESULTS x is much larger than σ2
We learned the following hyperparameters for our GP : σ2 s = 0.18 , = 7.92 , and σ2 x = 456 The σ2 parameters can be roughly interpreted as the “ fraction of variance explained ” so the fact that σ2 s means that the demographic covariates encoded in the mean embedding are much more important to the model than the spatial coordinates . The length scale for the Mat´ern kernel is a little more than half the median distance between locations , which indicates that it is performing a reasonable degree of smoothing . We used 10 fold crossvalidation to evaluate our model and ensure that it was not overfitting , an important consideration as generalization performance is critical . The root mean squared error of the model was 2.5 and the mean log predictive density was 19 Predictive density is a useful measure because it takes posterior uncertainty intervals into account . For comparison , predicting the national average of Obama receiving 51.1 % of the vote in every location has a root mean squared error of 83 As a sensitivity analysis , we also considered a multiplicative model , for which the performance was comparable .
To validate our models , we compared to the 2012 exit polls , conducted by Edison Research for a consortium of news organizations . National results were based on interviews with voters in 350 randomly chosen precincts , and state results in 18 states were based on interviews in 11 to 50 random precincts . In these interviews , conducted as voters left polling stations , voters were asked who they voted for and a variety of demographic questions about themselves . Bias due to factors such as unrepresentativeness of the sampled precincts and inadequate coverage of early or absentee voters could be an issue [ 1 ] . The national results had a margin of error ( corresponding to a 95 % uncertainty interval ) of 4 percentage points6 and the state results had a margin of error of between 4 and 5 percentage points [ 18 ] . For comparing to the 18 state level exit polls , we aggregated our geographic regions by state , weighting by subgroup population .
As a preview of our results by gender , income , and age , and to get an idea of the power of our method , Figure 3 shows four maps visualizing Obama ’s support among women and men . In Figures 3a–3b , we show the results from the exit polls , at the state level , for only 18 states . In Figures 3c– 3d we fill in the missing picture , providing estimates for 837 different regions . We compare to competing methods below for national level gender estimates . In the supplementary materials , we consider the non binary demographic covariates age and income and the case of regional level estimates , which present a difficulties for the competing methods . 7.1 Gender
Voting by gender is shown in Figure 4 , where we compare our results to the exit poll results . The fit is quite good , with correlations equal to 0.96 for men and 0.94 for women . The inference that we are most interested in is the gender
6This presumably corresponds to a sample size of only n = 600 individuals , since the usual margin of error reported by news organizations is 1.96
.52 n−1
295 296 297 Implementing a racially stratified homogenous precinct approach . PS : Political Science and Politics , 39(3):pp . 477–483 , 2006 . ISSN 10490965 .
[ 2 ] N . Cristianini and J . Shawe Taylor . An introduction to support vector machines and other kernel based learning methods . Cambridge university press , 2000 .
[ 3 ] A . Dobson . An introduction to generalized linear models . Chapman & Hall texts in statistical science , 2002 .
706–714 . JMLR.org , 2014 .
[ 20 ] S . Openshaw . Ecological fallacies and the analysis of areal census data . Environment and Planning A , 16 ( 1):17–31 , 1984 .
[ 21 ] G . Patrini , R . Nock , T . Caetano , and P . Rivera .
( almost ) no label no cry . In Z . Ghahramani , M . Welling , C . Cortes , N . Lawrence , and K . Weinberger , editors , NIPS 27 , pages 190–198 . Curran Associates , Inc . , 2014 .
[ 4 ] O . B . Duncan and D . B . An alternative to ecological
[ 22 ] B . Poczos , A . Singh , A . Rinaldo , and L . Wasserman . correlation . American Sociological Review , 16:665–666 , 1953 .
[ 5 ] K . Fukumizu , L . Song , and A . Gretton . Kernel bayes’ rule . In NIPS , pages 1737–1745 , 2011 .
[ 6 ] T . G¨artner , P . A . Flach , A . Kowalczyk , and A . J .
Smola . Multi instance kernels . In ICML , volume 2 , pages 179–186 , 2002 .
[ 7 ] A . Gelman , D . Park , B . Shor , J . Bafumi , and
J . Cortina . Red State , Blue State , Rich State , Poor State : Why Americans Vote the Way They Do . Princeton University Press , Aug . 2008 . ISBN 069113927X .
[ 8 ] L . A . Goodman . Some alternatives to ecological correlation . American Journal of Sociology , pages 610–625 , 1959 .
Distribution free distribution regression . In Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics , pages 507–515 , 2013 .
[ 23 ] R . L . Prentice and L . Sheppard . Aggregate data studies of disease risk factors . Biometrika , 82(1 ) : 113–125 , 1995 .
[ 24 ] N . Quadrianto , A . J . Smola , T . S . Caetano , and Q . V . Le . Estimating labels from label proportions . JMLR , 10:2349–2374 , 2009 .
[ 25 ] A . Rahimi and B . Recht . Weighted sums of random kitchen sinks : Replacing minimization with randomization in learning . In Advances in neural information processing systems , pages 1313–1320 , 2008 .
[ 9 ] A . Gretton , A . Smola , J . Huang , M . Schmittfull ,
[ 26 ] C . E . Rasmussen and C . K . Williams . Gaussian
K . Borgwardt , and B . Sch¨olkopf . Covariate shift by kernel mean matching . Dataset shift in machine learning , 3(4):5 , 2009 .
[ 10 ] A . Gretton , K . M . Borgwardt , M . J . Rasch ,
B . Sch¨olkopf , and A . Smola . A kernel two sample test . JMLR , 13:723–773 , 2012 .
[ 11 ] M . S . Handcock and M . L . Stein . A bayesian analysis of kriging . Technometrics , 35(4):403–410 , 1993 .
[ 12 ] C . Jackson , N . Best , and S . Richardson . Improving ecological inference using individual level data . Statistics in medicine , 25(12):2136–2159 , 2006 . [ 13 ] G . King . A Solution to the Ecological Inference
Problem . Princeton University Press , Mar . 1997 . [ 14 ] G . King , M . A . Tanner , and O . Rosen . Ecological inference : New methodological strategies . Cambridge University Press , 2004 .
[ 15 ] H . Kueck and N . de Freitas . Learning about individuals from group statistics . In 21st Uncertainty in Artificial Intelligence ( UAI ) , pages 332–339 , 2005 .
[ 16 ] Q . Le , T . Sarlos , and A . Smola . Fastfood : approximating kernel expansions in loglinear time . In Proceedings of the international conference on machine learning , 2013 . processes for machine learning , 2006 .
[ 27 ] J . Riihim¨aki and A . Vehtari . Laplace approximation for logistic gaussian process density estimation and regression . Bayesian Analysis , 9(2):425–448 , 2014 .
[ 28 ] W . S . Robinson . Ecological correlations and the behavior of individuals . American Sociological Review , 15:351–57 , 1950 .
[ 29 ] C . Saunders , A . Gammerman , and V . Vovk . Ridge regression learning algorithm in dual variables . In ( ICML 1998 ) Proceedings of the 15th International Conference on Machine Learning , pages 515–521 . Morgan Kaufmann , 1998 .
[ 30 ] D . R . Sheldon and T . G . Dietterich . Collective graphical models . In NIPS , pages 1161–1169 , 2011 . [ 31 ] A . Smola , A . Gretton , L . Song , and B . Sch¨olkopf . A hilbert space embedding for distributions . In Algorithmic Learning Theory , pages 13–31 . Springer , 2007 .
[ 32 ] L . Song , J . Huang , A . Smola , and K . Fukumizu .
Hilbert space embeddings of conditional distributions with applications to dynamical systems . In Proceedings of the 26th Annual International Conference on Machine Learning , pages 961–968 . ACM , 2009 .
[ 17 ] K . Muandet , K . Fukumizu , B . Sriperumbudur ,
[ 33 ] Z . Szabo , A . Gretton , B . Poczos , and
A . Gretton , and B . Schoelkopf . Kernel mean estimation and stein effect . In Proceedings of The 31st International Conference on Machine Learning , pages 10–18 , 2014 .
[ 18 ] New York Times . President exit polls election 2012 . http://electionsnytimescom/2012/results/ president/exit polls , 2012 . Accessed : 16 February 2015 .
B . Sriperumbudur . Two stage Sampled Learning Theory on Distributions . Artificial Intelligence and Statistics ( AISTATS ) , Feb . 2015 .
[ 34 ] J . Vanhatalo , J . Riihim¨aki , J . Hartikainen , P . Jyl¨anki ,
V . Tolvanen , and A . Vehtari . Gpstuff : Bayesian modeling with gaussian processes . JMLR , 14(1 ) : 1175–1179 , 2013 .
[ 35 ] G . Wahba . Spline models for observational data ,
[ 19 ] J . B . Oliva , W . Neiswanger , B . P´oczos , J . G . volume 59 . Siam , 1990 .
Schneider , and E . P . Xing . Fast distribution to real regression . In Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics , AISTATS 2014 , Reykjavik , Iceland , April 22 25 , 2014 , volume 33 of JMLR Proceedings , pages
[ 36 ] C . K . Williams and D . Barber . Bayesian classification with gaussian processes . Pattern Analysis and Machine Intelligence , IEEE Transactions on , 20(12 ) : 1342–1351 , 1998 .
298
