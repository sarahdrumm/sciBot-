A Deep Hybrid Model for Weather Forecasting
Aditya Grover∗
IIT Delhi adityagrover1@gmailcom
Ashish Kapoor Microsoft Research akapoor@microsoft.com
Eric Horvitz
Microsoft Research horvitz@microsoft.com
ABSTRACT Weather forecasting is a canonical predictive challenge that has depended primarily on model based methods . We explore new directions with forecasting weather as a dataintensive challenge that involves inferences across space and time . We study specifically the power of making predictions via a hybrid approach that combines discriminatively trained predictive models with a deep neural network that models the joint statistics of a set of weather related variables . We show how the base model can be enhanced with spatial interpolation that uses learned long range spatial dependencies . We also derive an efficient learning and inference procedure that allows for large scale optimization of the model parameters . We evaluate the methods with experiments on real world meteorological data that highlight the promise of the approach .
Categories and Subject Descriptors I26 [ Artificial Intelligence ] : Learning
General Terms Machine Learning , Graphical Models , Weather Forecasting
Keywords Gaussian Processes , Deep Learning
1 .
INTRODUCTION
Making inferences and predictions about weather has been an omnipresent challenge throughout human history . Challenges with accurate meteorological modeling brings to the fore difficulties with reasoning about the complex dynamics of Earth ’s atmospheric system . Methods have sought to define weather in terms of sets of fundamental quantities , and various characterizations have been proposed and employed ∗Research performed during an internship at Microsoft Research . in forecasting systems . We explore weather as a fundamental challenge for machine data mining and inference . We introduce methods that show promise for advancing the state of the art of weather forecasting systems .
In the US , the National Oceanic and Atmospheric Administration ( NOAA ) is responsible with providing publicly available weather forecasts , based on periodic observations . These measurements are logged in the Integrated Global Radiosonde Archive ( IGRA ) [ 4 ] . Forecasts for winds and temperature are accessible via NOAA ’s Winds Aloft program . To date , the best approaches to weather modeling rely on mathematical simulations . The methodology centers on the use of a generative model to capture atmospheric dynamics , where samples are drawn from physical simulations to make predictions [ 18 , 12 ] . In contrast , we take a data centric approach . Rather than define a generative model , we discriminatively train predictive models from the historic data , considering a historical data on a core set of variables for learning and inference about weather : atmospheric pressure , temperature , dew point , and winds . We use boosted decision trees as predictors in the studies .
Several challenges must be addressed in taking a datacentric approach to weather prediction . First , we note that the set of weather variables under consideration are tightly coupled . For example , pressure and temperature follow natural gas laws ( ie , the well known formula , P V = nRT ) . Similarly , there is a tight relationship between relative humidity and temperature . Consequently , any model that jointly aims to predict the set of weather variables should leverage knowledge of the tight statistical couplings that are based in physics . Secondly , dependencies among the variables may have long range influences across space and time . For instance , wind vectors across large geographic distances may follow isobaric contours . As another consideration , the weather phenomena may be affected by local geography and associated natural processes ( eg isolated thunderstorms ) , as well as shifts in the large scale structure of atmospheric phenomena ( eg shifting of jet streams ) .
We aim to tackle these challenges via a representation that jointly predicts winds , temperature , pressure , and dew point across space and time . The proposed architecture combines a bottom up predictor for each individual variable with a top down deep belief network that models the joint statistical relationships . Another key component in the framework is a data driven kernel , based on a similarity function that is learned automatically from the data . The kernel is used to impose long range dependencies across space and to ensure that the inferences respect natural laws . We present an
379 efficient procedure for combining inferences from separate predictors of local phenomena while considering constraints imposed by the deep belief network such that the predictions respect the natural regularities expected with the large scale phenomena .
The main contributions of this work can be summarized as follows :
1 . We present a novel hybrid model with discriminative and generative components for spatiotemporal inferences about weather .
2 . We design and implement a data driven kernel function that shapes predictions in accordance with physical laws .
3 . We provide an efficient inference procedure that enables optimization of the predictive model in accordance with large scale phenomena .
4 . We evaluate the methods with a set of experiments that highlight the performance and value of the methodology .
The rest of the paper is structured as follows : We next discuss background and related work . In Section 3 , we describe the technical details of our approach , showing the components of a comprehensive graphical model that we call a Deep Hybrid Model . The learning and inference algorithms based on this model are discussed in Section 4 . In Section 5 , we present the results of experiments with the model on realworld data . We conclude with a brief summary and discuss future work in Section 6 .
2 . BACKGROUND AND RELATED WORK The proliferation of satellites , radar , sensors , coupled with rapidly decreasing costs of storing and distributing information have catalyzed an explosion in quantities of weather data available for studies . Most work in weather forecasting to date rely on the use of generative approaches , where the weather systems are simulated via numerical methods [ 18 , 12 , 10 ] , or rely on time series analysis such as ARIMA models and simple classifiers based on Artificial Neural Networks [ 11 , 10 , 8 , 2 , 21 ] or Support Vector Machines [ 16 , 19 ] . These statistical models often make strong assumptions such as spatial independence to overcome the curse of dimensionality , which do not hold well in practice .
Despite the success of machine learning in a variety of tasks , applications to the problem of weather forecasting has been limited . Exceptions include the use of Bayesian Networks for precipitation forecasts [ 3 ] and temporal modeling via Restricted Boltzmann Machines ( RBM ) [ 20 , 15 ] . A separate thread of research has also focused on efficient representation of relational spatiotemporal data in Random Forests for prediction of severe surface level weather processes , such as droughts and tornadoes [ 14 , 13 ] . More recently , large scale wind prediction has been presented [ 9 ] using a Bayesian framework with Gaussian Processes [ 17 ] .
To date , uses of machine learning for weather prediction have been limited in several ways . First , almost all methods consider only one variable at a time and do not explore the joint spatiotemporal statistic of multiple weather phenomena . Also , to our knowledge , long range spatiotemporal dependencies have not been modeled explicitly . Thus , models have been blind to long range phenomena based on the laws of nature , such as winds aligning by pressure as captured by the structure and dynamics of isobars .
We introduce methods that address these limitations , via introduction of a hybrid representation . With a hybrid representation , individual predictors are discriminatively trained from historic data and local inferences from these models are combined with a deep neural network that overlays statistical constraints among key weather variables . We additionally apply a spatial interpolation scheme that respects constraints of long range statistical dependencies . The methodology employs covariance matrix for Gaussian Process regression constructed from a large dataset . Here , the covariance matrix , also referred to as the kernel , allows us to enforce smoothness constraints over the weather variables . By ensuring that the kernel captures the dynamics of the system as informed by the training data , we are able to align estimates according to spatial constraints imposed by natural laws .
3 . THE DEEP HYBRID MODEL
We seek a prediction model that respects spatiotemporal dependencies among weather variables induced by atmospheric physics . We test the framework with data drawn from a continental scale weather corpus composed of data captured via balloons . In particular , we consider the IGRA dataset consisting of balloon observations made at 60 stations across the US These balloons transmit observations about wind speed and direction , temperature , geopotential height , dew point , and other weather variables . These observations are released in real time by the NOAA and later by the National Climatic Data Center following preprocessing . The data is eventually integrated into the curated IGRA dataset which is updated daily and contains historical weather data spanning decades compiled from eleven source datasets . Any data added to the archive undergoes a cycle of quality assurance to resolve potential inconsistencies among variables [ 4 , 5 ] .
Formally , we consider four weather variables in the model : wind velocity , v ; pressure , p ; temperature , t and dew point , d . The wind observations are represented as a two dimensional vector , v = [ vx , vy ] while all other weather variables are scalars . We represent weather stations ( where the balloons are released ) as SL = {s1,,sNs} where Ns is the total number of weather stations . For each of these stations , we have historical weather data logged at a frequency of approximately six hours over several years .
Our approach to building the weather model was governed by the following guidelines :
1 . Temporal mining : Our model should be able to identify and learn from recurring weather patterns over time .
2 . Spatial interpolation : The dynamic influence of atmospheric laws on weather phenomena need to be accounted for in our predictions .
3 . Inter variable interactions : The local interdependencies between weather variables should be captured by our model .
Accordingly our model can be viewed as having three main components . The first component is a set of individual pre
380 Figure 1 : Spatial Interpolation of winds in a static and hybrid field . Filled contours represent temperature and isobar lines are marked in red . In the hybrid field , the interpolated wind vectors are closely aligned with the true values . However , the static field fails to account for the long range dependencies . dictors for the weather variables that are trained using historical data . A variety of off the shelf machine learning procedures can be applied to the recorded data to build these individual predictors . The second component works to refine inferences produced by the separate predictors by constraining the output to be spatially smooth and aligned with constraints imposed by physical laws . The interplay of these constraints is dynamic and hence , we develop a data centric approach . The third component consists of a deep belief network which leads to a preference for solutions that respect the expected joint statistics of the weather variables . We describe the three key components in detail below and finally conclude this section with an integrated graphical model of our framework .
3.1 Base Level Predictors
The base level predictors are individual regression functions that are trained using historical data at different temporal granularities . The intuition is that long term historical records of weather should provide insights about the weather at particular locations , given sets of observations in the immediate past . In general , the weather conditions change gradually over time and also exhibit cyclicity through seasons , consequently enabling some success in predicting the signals . We need to train different predictors for each station and range of altitudes considered as weather conditions change significantly across the vertical profile .
The performance of the local regressions depends critically on evidential features . We consider features over short and long term spans of time . For the short term features , we consider the values of weather variables over the last seven days . As observations come at twelve hour intervals , we consider shorter term features by time of the day . Such shortterm segmentation can be useful because winds , temperature , and other weather variables may differ significantly over day and night due to the influence of solar heating . We consider separate short term features for day and night rather than averaging over the daily variation . Features spanning longer periods of time incorporate average seasonal data on weather variables . The long term features are computed for several years in the past to reduce the influence of atypical weather phenomena . Given the set of engineered features , we use an ensemble of boosted decision tree learners to make predictions . 3.2 Data Centric Kernel for Spatial Interpo lation
The individual predictors provide predictions only for particular locations ( the weather stations ) , and we need to interpolate the results across larger spatial regions . To extend predictions in a smooth manner beyond the weather stations , we rely on smoothness constraints induced via the GP prior .
The covariance or kernel matrix K captures the notion of similarity among data points that are close in space and time and is the key in determining the accuracy of spatial interpolation . While static Radial Basis Function ( RBF ) kernels based on distance give reasonable estimates , they fail to capture the dynamics of the system . For instance , predictions about wind velocity at location s∗ , are not necessarily influenced similarly by weather at equidistant stations , per factors such as regional turbulence . We need to have an ability to capture a preferential bias towards classes of functions that respect certain physical constraints among the weather variables . The physical constraints include longrange spatial dependencies , such as wind vectors aligning with isobars1 and modeling the direct relationship between pressure , temperature , and dew point due to natural gas laws .
We use a novel kernel defining a GP prior . For any pair of locations i and j , if the current pressure , temperature , and the wind direction are denoted as p , t and θ respectively , then we define our kernel as :
Ki,j = K D i,j · K θ i,j · ( K p i,j + ( 1 − )K t i,j ) .
( 1 ) i,j , K θ i,j , K p
Here , K D i,j and K t i,j are RBF kernels over geographic distance , the angle of the wind , pressure and temperature respectively and is a tunable parameter such that 0 ≤ ≤ 1 . The resulting kernel matrix would be positive semi definite as the proposed kernel function is a linear combination and Hadamard product of kernels .
1Since we are not doing surface level predictions , the effect of friction is negligible .
381 Multiple kernels are commonly used to integrate similarity notions from different sources [ 6 ] . In our case , the similarity between any two sites is a function of the geographic distance as well as the similarity in the weather variables . We note that the kernel K θ over the wind direction plays a critical role in inducing long range dependencies . As an example , consider two stations A and B with wind vectors [ ax , ay ] and [ bx , by ] , respectively . We are performing interpolation separately in X and Y directions . Hence , for any station , eg , station A , we can assume independence in the two directions such that a neighboring station B can only induce an air flow change in ax through bx and similarly , ay is only influenced by by . K θ captures this intuition by defining an RBF over the angles made by the wind vectors with the corresponding axis for which the kernel matrix is defined . The balance between the pressure gradient force and Coriolis effect ( geostrophic force ) causes the winds to follow isobars . This implies that stations in close vicinity having similar pressure will have winds aligned in the same direction , justifying the contribution of K p in computing the similarity . 3.3 Joint Modeling of Weather Variables
Weather variables are influenced heavily by the interaction of several factors . At the most fundamental level , these dependencies are based in the natural laws of thermodynamics . Approaches to inferences about weather relying on numerical simulation seek to characterize these dependencies analytically . However these interdependencies are complex and unpredictable , which explains the limited success of analytical techniques . At the same time , discriminative statistical analysis beyond temporal and spatial techniques described above , does not generalize well for domains with the dynamism of weather phenomena . For weather , it is natural to consider architectures that can automatically learn rich representations from raw data . Hence , we model the joint distribution between weather variables through a deep belief network ( DBN ) .
The DBN consists of layers of stacked Restricted Boltzmann Machines ( RBM ) where the connections between any two layers of a RBM form a bipartite graph . The top layer of the DBN consists of five units corresponding to the normalized values of the latent weather variables ( two units for representing 2D vector winds ) . We assume a Gaussian prior over these variables , such that Wi ∼ N ( mi , di ) , each unit having a bias ai . The primary level interactions between the variables give rise to a secondary set of features represented as the layer 2 hidden units , H . Similarly , we can have another RBM below to capture the interactions between H and the layer 3 units , G . The hidden units follow a Bernoulli distribution and have a biases bj and ck . The weights between the first two layers are U= [ uij ] and the next two layers are V = [ vjk ] . Several structural and tunable design parameters are involved , which we discuss in the next section . 3.4 Probabilistic Graphical Model
The graphical model for the proposed approach is shown in Figure 2 . The matrix W is the collection of all the weather variables wi denoting the true value at each location i . The i , pi , ti , di} recorded at any of the observations zi = {vx sites is simply a noisier version of this true value . We use plate notation to show observations at Ns number of weather i , vy
Figure 2 : Deep hybrid model . Probabilistic graphical model for weather prediction where weather stations denoted by S , induce a Gaussian process ( GP ) prior over the true values of the weather variables W . Only noisier versions ( zi ) of the true values are observed at all the sites and are related via φ . The forecasts given by the pre trained predictor are related to the future observations via the potential ψ(· ) . The joint distribution of the true weather variables is further constrained via a deep belief network ( η(·) ) . All potentials arise at a test site s∗ , except that there is no pre trained predictor . stations . Each weather station has random variables for wind velocity , dew point , pressure and temperature . Each of these random variables are constrained via ( a ) an individual predictor that is trained on the historical data ( ψ(·) ) , ( b ) a Gaussian process prior ( GP ( · ) ) and the Gaussian likelihood function φ(· ) that use data dependent prior to impose spatial and functional smoothness and ( c ) a deep belief network that encourages solutions that respect the joint statistics observed in historical data and that are also aligned with physical laws . Similarly , at a test site s∗ , we use the GP prior and the likelihood to first interpolate observations made at the weather stations . These interpolations are then further constrained via the deep belief network to impose the joint statistical distribution of the weather variables . Formally , we have the following distribution corresponding to the graphical model : p(W , Z|S ) ∝ GP ( W ; S )
φ(wi , zi)η(zi )
ψ(zi ) . i∈L∪∗ i∈L
Here , Z is the collection of random variables representing the observations at any of the locations . The terms GP ( · ) and φ(· ) enforce the smoothness constraints as defined by the data dependent kernel ( described in section 32 ) The potential term η(· ) arises due to the deep belief network component ( Section 33 ) Finally , the term ψ(· ) applies only to the weather station sites and enforces consistency with the prediction of the pre trained regression functions . In particular , the observations are related to the output of an individual predictor via a simple Gaussian function : eg ψ(p ) = N ( µp , κ2 ) , where µp is the individual prediction for the pressure variable .
SWjt=N(W;0,K)ϕ(z,w)=e(−||z−w||22σ2)Balloon  Observa ons  Observa on  at  Test  Posi on  sN);(SWGPyjvjdjp*p*d*txv*Hidden  Layer  1  Hidden  Layer  2  Hidden  Layer  1  Hidden  Layer  2  Pre ­‐trained  Predictor  ),()(2κµψjtjNt=)(jpψ)(jdψ)(xjvψyv*xjv)(yjvψ)(jzη)(*zη∓"382 Algorithm 1 Deep hybrid model learning . procedure TrainWeatherModels ( cid:66 ) Boosted Decision Trees for Every Location , Variable for all x ∈ {v , p , t , d} do for all s ∈ S do trainData ← getTrainData(x , getHistData(s ) ) param ← getBestParam(trainData ) BstDecT ree[x , s ] ← TrainBDTree(param ) end for end for
( cid:66 ) GP Hyperparameters for Every Weather Variable for all x ∈ {v , p , t , d} do hyP aram ← getBestHParam(x , getAllHistData( ) )
Algorithm 2 Deep hybrid model inference . procedure ForecastWeatherVariable(x , s∗ , Z ) ( cid:66 ) Prediction Variable : x , Test Site : s∗ , Observations : Z if s∗ ∈ S then ( cid:66 ) uses corresponding BstDecTree model from Alg . 1 tmp∗ ← getBDTreePred(x , s∗ , Z ) else for all si ∈ S do tmpi ← getBDTreePred(x , si , Z ) wi ← DBNinference(x , tmpi ) ( cid:66 ) uses DBN model from Alg . 1 end for
( cid:66 ) DBN joint model training through CD
DBN model ← ContDivergence(getAllHistData( ) ) end procedure
4 . ALGORITHMIC DETAILS
To make the Deep Hybrid Model work in practice , we need to learn several parameters pertaining to the three components and design an efficient inference procedure for testing . Here we note that since we operate our model in batch mode , we can afford to have an elaborate learning procedure . Specifically , the deep belief network component indeed has high training time requirements . On the other hand , since our forecasts are made in real time , inference at test time needs to be extremely efficient . We now discuss the learning and inference algorithms which achieve these objectives . 4.1 Learning
Given the historical observations at various weather stations , we train the various components of our model in order to get the best predictive capability . We perform piecewise training of individual components , where the individual predictors , the parameters of the DBN and the kernel hyperparameters of the GP kernel are estimated . A simplified workflow for the training procedure is given in Algorithm 1 . In particular , we trained Boosted Tree based Learners using the set of short and long term features described previously and used the best models that were obtained for each weather station in the US for a range of altitudes from 3000 feet up to 39000 feet with an interval of 3000 feet . The optimal parameters with regard to the number of leaves , number of iterations , and the learning rate , were obtained through analysis with a 10 fold cross validation study .
Similarly , the hyperparameters of the data driven kernel were set via 10 fold cross validation and the final values of the kernel bandwidths were set to 150 , 0.1 , 0.05 and 1 for distance , wind angle , temperature and pressure respectively and was set to 02 Finally , the DBN component of our model was trained via a standard contrastive divergence procedure [ 7 ] . We explored the following parametric ranges : the learning rate ( 01 001 ) , the number of greedy iterations of convergence divergence ( 1 100 ) and the batch size ( 10 1000 ) . The structural properties of the neural net such as the number of hidden layers ( 1 3 ) and the number of neurons in each hidden layer ( 50 500 ) were also experimented with . The con
Append wi to w end for tmp∗ ← GPinterpolate(x , s∗ , w , Z ) end if w∗ ← DBNinference(x , tmp∗ ) return w∗ end procedure figurations yielding best cross validation results comprised of two stacked RBMs consisting of 50 and 150 hidden neurons , trained with a learning rate close to 0.1 , batch size of 100 and 20 greedy iterations . The limitation to these set of parameters is purely because of the high computational requirements and engineering effort in training deep networks , and indeed , the gains could be potentially more significant if the deep belief network is trained over a richer range of parameters . 4.2 Inference
Given the trained components , we seek to determine the posterior distribution over the set of observations z∗ at the test site s∗ . Exact inference in the proposed model is hard due to the presence of potential functions η(· ) induced via the deep belief network . We apply piecewise approximate inference as illustrated in Algorithm 2 . For the trivial case , when the prediction needs to be made at a weather station site , we invoke the pre trained predictor models to provide a forecast which is then refined using the deep belief network . If , however , we need to make a prediction at an arbitrary test site , the refined estimates computed for all weather stations are interpolated . These refined estimates are then interpolated to the test site via the Gaussian Process component . We note that , since the kernel function is data driven , we use simple interpolated values of the weather variable at the test site in order to compute the kernel . Given the interpolated values at the test site , we then carry out a last refinement of prediction in order to resolve the estimates with the joint statistical constraints imposed by the deep model .
At the heart of the inference scheme , we employ an iterative procedure that aligns the predicted estimates with the potential induced via the deep model . We use a variational approximation to resolve and refine the posterior distribution over the observations z . Formally , we denote the approximation of the refined posterior by q(zi ) ∼ N ( µi , σi ) .
383 we considered a cluster of stations spread across the central US ( states demarcated by black lines in Fig 1 ) and interpolated winds via Gaussian Process regression at these stations , considering the wind measurements from the rest of the US stations . Thus , each station served as an independent test point , whose value is interpolated using a GPR model and compared against the true winds shown in Fig 1 ( a ) . Fig 1 ( b ) shows the interpolated wind vectors when a static kernel matrix is used . Here an entry Ki,j in the matrix is simply a decreasing exponential in the geographical distance between two stations i and j . In contrast , the hybrid approach , as illustrated in Fig 1 ( c ) , captures the similarity between each pair of stations such that every entry Ki,j of the matrix is computed dynamically at training time using the formula given in Eq 1 . The pressure , temperature , and angle θ between the wind vectors are the known values for the current time step .
Now consider the following stations : Topeka ( Kansas ) , Omaha ( Nebraska ) , Springfield ( Missouri ) and Norman ( Oklahama ) , referred to in Fig 1 as stations P , A , B and C , respectively . For a static interpolation of winds at P , a higher contribution would come from B than C , as B is geographically closer . However , the temperature and pressure conditions at C are closely aligned to that of P and end up contributing more in the hybrid approach . We observe that KP,A is maximum in both cases . Hence , the hybrid kernel does not ignore distance as a similarity criteria . However , in cases involving a tradeoff between distance and other weather variables , their combined contribution might alter the relative importance of a particular neighboring station , as in the aforementioned case . The quantitative gains in predication accuracy are displayed in the RMS plots in Fig 3 ( a , b ) .
Weather Variable RMS Error Reduction ( in %age )
6 hours 12 hours 24 hours
X
2.17 1.05 1.05
Y
2.05 1.01 0.97
Overall
2.11 1.03 1.01
Table 1 : Improvement in performance obtained using the deep belief network . The final step of refinement uses the DBN results to further improve prediction accuracy .
5.2 Dynamic Prediction and Deep Learning
In another experiment , we evaluate the performance gains due to the final refinement step of the deep belief network . The percentage reduction in error for wind forecasts for three time steps in the future are shown in Table 1 . We see that the DBN leads to an additional 1 2 % error reduction and clearly , modeling the joint statistics of the weather variables helps in making better predictions . We observed a performance improvement of similar magnitude for the other weather variables as well .
After establishing the superiority of the data centric kernel and the DBN independently , we evaluate the prediction accuracy of the full deep hybrid model for each weather variable2 , aggregated over all stations in the continental US , where current and historical data is available .
Layer 1 to 2 : 1/γt+1 l = 1 + e
Layer 2 to 3 : 1/βt+1
Layer 3 to 2 : 1/γt+1 l l = 1 + e
= 1 + i
−[bl+ −[cl+ 1 − γt+1 j l γt+1 l
µt i uil ]
γt+1 i vjl ]
−[bl+ e k
γt+1 j
βt+1 k vlk ]
Figure 3 : True versus interpolated wind plots for static and hybrid kernel . Static interpolation shows high deviations from true winds . Atmospheric dynamics are more effectively captured with use of a hybrid data centric kernel .
Additionally , we also approximate the posterior over the latent variables and q(Hj ) ∼ Bern ( γj ) and q(Gk ) ∼ Bern ( βk ) for the two hidden layers respectively . The following variational updates are then used to estimate the parameters of the distribution ( lth component ) :
Layer 2 to 1 : µt+1 l = ( ml + d2 l al + d2 l ulj)/(1 + d2 l )
σt+1 l = dl/ j
1 + d2 l
The mean parameters µl are initialized to the estimates ml , while γ and β are initialized randomly . The parameter dl corresponds to the variance in the initial estimates and signifies our confidence in those predictions . We set these variances via a cross validation procedure over historical data . The derivation for the above update equations follows from the application of prior work in variational inference [ 1 ] to deep belief networks .
5 . EXPERIMENTAL EVALUATION
We performed a set of experiments to evaluate the proposed methodology . In the experiments , we explored three main questions . First , we compare and highlight the advantage of the spatial interpolation procedure that relies on a data centric dynamic kernel matrix to the more commonly used static kernel matrix . Second , we seek to compare the proposed model with a baseline approach . Third , we explore the importance of modeling the joint statistics of predictive variables via the deep belief network . Finally we compare the wind forecast results with those of state of the art systems .
The experiments were based on five years of historical data , from 2009 to present , extracted from the IGRA dataset . The data consists of balloon observations recorded at 60 locations across the continental US . 5.1 Interpolation in a Hybrid Field
To illustrate the efficacy of a hybrid kernel in handling long range spatial dependencies among weather variables ,
2The IGRA dataset provides the geopotential height and dew point at roughly constant pressures . These quantities ,
−40−30−20−1001020−40−30−20−1001020True WindsPredicted WindsStatic Kernel RMS ErrorAll values in knotsX direction : 9.8274Y direction : 6.0501−40−30−20−1001020−40−30−20−1001020True WindsPredicted WindsHybrid Kernel RMS ErrorAll values in knotsX direction : 4.9907Y direction : 3.0086384 Figure 4 : Results on predicting weather variables for different approaches . The temporal predictors that employ a hybrid data centric scheme for interpolation and use DBNs for modeling the joint relationship among weather variables show significant improvements over the baselines .
The accuracy of the proposed model is compared with two baseline models in Fig 4 for three future time steps as before . The baseline prediction , marked as A in Fig 4 , uses the current values as estimates for the future ; the intuition with using current values to predict the future is that weather conditions typically do not change greatly over a day . For the second baseline ( marked B ) , we construct a typical spatiotemporal prediction model , where baseline boosted decision tree predictors are augmented with a static interpolation scheme . We observe that the deep hybrid model ( marked C ) comprising of a dynamic data driven interpolation scheme and a DBN in addition to the boosted decision tree predictors used in B , significantly outperforms both the baselines . In a couple of cases involving shortterm temperature forecasts , B marginally outperformed the DHM , suggesting limited interdependences between temperature and other variables for short term predictions . 5.3 Comparison with State of the art
Apart from winds , forecasts for other variables across the vertical atmospheric profile are not available for comparative analyses . We compare the wind predictions of the proposed model against two forecast systems . The first one proposed by [ 9 ] makes predictions using a static GPR interpolation scheme , coupled with relative velocity data obtained through airplanes . Our second set of comparisons is with the Winds Aloft forecast , released by NOAA every six hours , for three under reasonable assumptions , serve as proxies for pressure and specific humidity , respectively .
Time Step
Model
RMS Error ( in knots ) Overall
X
Y
6 hours
12 hours
24 hours
Deep Hybrid Model Kapoor et al . 2014
NOAA
Deep Hybrid Model Kapoor et al . 2014
NOAA
Deep Hybrid Model Kapoor et al . 2014
NOAA
2.29 3.94 3.18 4.44 5.03 5.13 6.57 8.93 8.79
1.33 2.16 3.44 2.59 3.93 4.34 3.82 5.24 6.37
1.81 3.05 3.31 3.56 4.48 4.88 5.19 7.08 7.58
Table 2 : Comparison of the proposed methodology with state of the art in wind prediction . Results summarized here are for weather stations in Washington for a period of one month . We observe that the new model results in significantly lower errors than competitive models . The best performance is indicated in bold . time steps into the future : 6 , 12 and 24 hours . Table 2 show the accuracy of the two forecast systems for the Seattle station . The results summarize the predictions made for the weather stations in the state of Washington for a period of one month . We observe that , while Kapoor et al . 2014 achieve better performance than NOAA , the proposed method shows significantly better performance than both of the competitors .
B : Typical Prediction Model6 hours12 hours24 hoursA64617141292B76761419613511C162739912569020406080100120140160180RMS Error ( in meters)Geopotential Height6 hours12 hours24 hoursA72912941458B62311771212C5017879150246810121416RMS Error ( in degrees)Dew Point6 hours12 hours24 hoursA145371291B11825195C1392751210051152253354RMS Error ( in degrees)Temperature6 hours12 hours24 hoursA74413231489B59110581213C3719588740246810121416RMS Error ( in knots)WindsA : BaselineC : Deep Hybrid Model385 6 . CONCLUSION AND FUTURE WORK
We presented a weather forecasting model that makes predictions via considerations of the joint influence of key weather variables . We introduced a data centric kernel and showed how using GPR with such a kernel can effectively interpolate over space , taking into account weather phenomena such as turbulence . We performed temporal analysis using short and longer term features within a gradient tree based learner . We augmented the system with a deep belief network and tuned the parameters to model the dependencies among weather variables . A set of experiments on real world data shows that the new methodology can provide better results than NOAA benchmarks , as well as recent research that had demonstrated improvements over the benchmarks .
Future work includes projecting weather predictions to more distant times into the future . We are also interested in exploring the use of computations of the value of information to guide sensing at weather stations . We note that airplanes in flight can serve as sensors of wind speeds , as explored in [ 9 ] . We wish to investigate the boosts in predictive power that might be achieved via integrating such additional data into the hybrid model . Acknowledgments We are grateful to Imke Durre for answering our queries concerning the IGRA dataset . The first author would like to thank Microsoft Research , Redmond for conducting the Worldwide Internship Program that made this research possible . 7 . REFERENCES [ 1 ] M . J . Beal . Variational algorithms for approximate
Bayesian inference . PhD thesis , University of London , 2003 .
[ 2 ] L . Chen and X . Lai . Comparison between ARIMA and
ANN models used in short term wind speed forecasting . In Power and Energy Engineering Conference ( APPEEC ) , 2011 Asia Pacific , pages 1–4 . IEEE , 2011 .
[ 9 ] A . Kapoor , Z . Horvitz , S . Laube , and E . Horvitz .
Airplanes aloft as a sensor network for wind forecasting . In Proceedings of the 13th international symposium on Information Processing in Sensor Networks ( IPSN ) , pages 25–34 . IEEE Press , 2014 .
[ 10 ] V . M . Krasnopolsky and M . S . Fox Rabinovitz .
Complex hybrid models combining deterministic and machine learning components for numerical climate modeling and weather prediction . Neural Networks , 19(2):122–134 , 2006 .
[ 11 ] R . J . Kuligowski and A . P . Barros . Localized precipitation forecasts from a numerical weather prediction model using artificial neural networks . Weather and Forecasting , 13(4):1194–1204 , 1998 .
[ 12 ] G . Marchuk . Numerical methods in weather prediction .
Elsevier , 2012 .
[ 13 ] A . McGovern , D . John Gagne , N . Troutman , R . A .
Brown , J . Basara , and J . K . Williams . Using spatiotemporal relational random forests to improve our understanding of severe weather processes . Statistical Analysis and Data Mining : The ASA Data Science Journal , 4(4):407–429 , 2011 .
[ 14 ] A . McGovern , T . Supinie , I . Gagne , M . Collier ,
R . Brown , J . Basara , and J . Williams . Understanding severe weather processes through spatiotemporal relational random forests . In 2010 NASA conference on intelligent data understanding , 2010 .
[ 15 ] R . Mittelman , B . Kuipers , S . Savarese , and H . Lee .
Structured Recurrent Temporal Restricted Boltzmann Machines . In Proceedings of the 31st International Conference on Machine Learning ( ICML ) , pages 1647–1655 , 2014 .
[ 16 ] Y . Radhika and M . Shashi . Atmospheric temperature prediction using support vector machines . International Journal of Computer Theory and Engineering , 1(1):1793–8201 , 2009 .
[ 17 ] C . E . Rasmussen . Gaussian processes for machine learning . 2006 .
[ 18 ] L . F . Richardson . Weather prediction by numerical process . Cambridge University Press , 2007 .
[ 3 ] A . S . Cofıno , R . Cano , C . Sordo , and J . M . Gutierrez .
[ 19 ] N . I . Sapankevych and R . Sankar . Time series
Bayesian networks for probabilistic weather prediction . In 15th Eureopean Conference on Artificial Intelligence ( ECAI ) , 2002 . prediction using support vector machines : a survey . Computational Intelligence Magazine , IEEE , 4(2):24–38 , 2009 .
[ 4 ] I . Durre , R . S . Vose , and D . B . Wuertz . Overview of
[ 20 ] I . Sutskever , G . E . Hinton , and G . W . Taylor . The
Recurrent Temporal Restricted Boltzmann Machine . In Advances in Neural Information Processing Systems , pages 1601–1608 , 2009 .
[ 21 ] C . Voyant , M . Muselli , C . Paoli , and M L Nivet . Numerical Weather Prediction ( NWP ) and hybrid ARMA/ANN model to predict global radiation . Energy , 39(1):341–355 , 2012 . the Integrated Global Radiosonde Archive . Journal of Climate , 19(1):53–68 , 2006 .
[ 5 ] I . Durre , R . S . Vose , and D . B . Wuertz . Robust automated quality assurance of radiosonde temperatures . Journal of Applied Meteorology and Climatology , 47(8):2081–2095 , 2008 .
[ 6 ] M . G¨onen and E . Alpaydın . Multiple kernel learning algorithms . The Journal of Machine Learning Research , 12:2211–2268 , 2011 .
[ 7 ] G . Hinton , S . Osindero , and Y W Teh . A fast learning algorithm for deep belief nets . Neural computation , 18(7):1527–1554 , 2006 .
[ 8 ] I . Horenko , R . Klein , S . Dolaptchiev , and C . Sch¨utte .
Automated Generation of Reduced Stochastic Weather Models I : simultaneous dimension and model reduction for time series analysis . Multiscale Modeling & Simulation , 6(4):1125–1145 , 2008 .
386
