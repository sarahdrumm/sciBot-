Improved Bounds on the Dot Product under Random
Projection and Random Sign Projection
Ata Kabán akaban@csbhamacuk
School of Computer Science
University of Birmingham Edgbaston , UK , B15 2TT
ABSTRACT Dot product is a key building block in a number of data mining algorithms from classification , regression , correlation clustering , to information retrieval and many others . When data is high dimensional , the use of random projections may serve as a universal dimensionality reduction method that provides both low distortion guarantees and computational savings . Yet , contrary to the optimal guarantees that are known on the preservation of the Euclidean distance cf . the Johnson Lindenstrauss lemma , the existing guarantees on the dot product under random projection are loose and incomplete in the current data mining and machine learning literature . Some recent literature even suggested that the dot product may not be preserved when the angle between the original vectors is obtuse . In this paper we provide improved bounds on the dot product under random projection that matches the optimal bounds on the Euclidean distance . As a corollary , we elucidate the impact of the angle between the original vectors on the relative distortion of the dot product under random projection , and we show that the obtuse vs . acute angles behave symmetrically in the same way . In a further corollary we make a link to sign random projection , where we generalise earlier results . Numerical simulations confirm our theoretical results . Finally we give an application of our results to bounding the generalisation error of compressive linear classifiers under the margin loss .
Categories and Subject Descriptors H28 [ Database Management ] : Database applications— Data mining ; I51 [ Pattern Recognition ] : Models—Statistical
General Terms Theory
Keywords Random projection ; Dot Product Preservation ; Margin Preservation ; Compressive Classification
1 .
INTRODUCTION
Random Projection ( RP ) is a simple and effective method of universal dimensionality reduction that enjoys nice theoretical guarantees . It simply consists of pre multiplying the high dimensional data with a random matrix that has the Johnson Lindenstrauss ( JL ) property . Matrices with entries drawn iid from a large class of distributions , ie the subgaussian distributions , are known to have the JL property . Successful application areas of RP include signal processing [ 17 , 7 ] , theoretical computer science [ 14 , 25 , 22 , 29 ] , machine learning [ 4 , 13 , 6 , 19 , 24 ] , data and text mining [ 9 , 28 ] , data streaming [ 35 ] , and optimisation of certain functions [ 8 ] . The use of RP in conjunction with these methods gains computational efficiency for a small amount of loss in performance . Other uses of RP include cancelable biometrics [ 5 ] and privacy preserving data mining [ 33 ] .
The main theoretical justification of RP is the famous Johnson Lindenstrauss lemma ( JLL ) [ 23 , 15 ] , which guarantees that the projection of two points to a k dimensional random subspace preserves their Euclidean distance up to a multiplicative distortion factor of 1 ± ǫ with probability higher than 1 − exp(−kǫ2/8 ) . This result is known to be optimal for linear dimensionality reduction [ 31 , 3 ] . Many data mining and machine learning algorithms have dot product as a key operation , for example regression and classification methods ( including the SVM ) , and RP has been applied to these successfully in practice [ 9 , 20 , 39 , 37 , 34 ] . However , while it has been folklore that RP also preserves dot products and angles – since there is a close connection between the Euclidean distance and the dot product – the existing probability bounds on the preservation of the dot product under RP are considerably looser than those on the preservation the Euclidean distance given by the JLL . One technical reason for this is that the bounds on the dot product are typically derived by combining multiple applications of the JLL for Euclidean distances via the Bonferroni inequality ( known also as the union bound ) – yielding a constant factor larger than 1 . It has not yet been investigated whether the use of the union bound is necessary or not for obtaining a JLL type bound on the dot product under RP . Beyond the above technical reason , in [ 32 ] the authors suggest a more fundamental reason why the dot product under RP may be difficult to bound as tightly as the Euclidean distance : More specifically , [ 32 ] point out that the ratio of
487 the standard deviation of the projected dot product and the original dot product is unbounded .
Furthermore , a recent study [ 39 ] raised some controversy as to whether the dot product would be preserved at all for obtuse angles . The authors derive a new bound on the dot product under random projection for the case when the angle between the original vectors is acute . The proof technique used in that work only applies to acute angles and the authors then suggest on the basis of numerical simulations that obtuse angles are not preserved under random projection . Unfortunately this result is highly incomplete since a particular choice of obtuse angle , as considered in the simulations , can be misleading . Indeed , as we shall see , the suggestion that obtuse angles are not preserved under RP is in fact false .
In this paper we obtain new improved bounds on the dot product under RP that take the same form as the JLL bounds for the Euclidean distance . We do this by eliminating the need to use the union bound . Instead we exploit the convexity of the exponential function within a standard Chernoff bounding argument . Our proof uses elementary techniques and works from first principles .
We also give several corollaries of this result , which make connections with previous works , including sign random projections , and we clear the confusion about the preservation of obtuse angles . Our analysis sheds light onto the issue of how the angle between the original vectors affects the relative distortion of the dot product , and reveals that the unboundedness of the coefficient of variation noted in [ 32 ] occurs only for the dot product of RPs of perpendicular vectors . This is also the only case in which we have no guarantees on the preservation of the dot product under RP . We will demonstrate numerical simulation results that confirm our theoretical findings . Finally , as an example of application of our results , we show how we can bound the generalisation error of a compressive linear classifier under the margin loss .
1.1 Background and motivation
We briefly review definitions and existing results needed in the paper .
Definition ( subgaussian random variables ) . A 0 mean random variable X is subgaussian with parameter σ2 if ∃σ2 > 0 such that ∀λ ∈ R ,
E{exp(λX)} ≤ exp˘σ2λ2/2¯
( 1 )
The moment generating function of X is dominated by the moment generating function of a 0 mean , σ2 variance Gaussian that . Also , it is easy to see ( by Taylor expansion ) that a 0 mean subgaussian random variable has variance upper bounded by σ2 . Many useful properties of subgaussians may be found in [ 11 ] .
RP uses matrices with iid subgaussian entries to linearly compress high dimensional data . The Johnson Lindenstrauss lemma ( JLL ) establishes the following guarantee for the Euclidean distance of randomly projected points . The proof of this result may be found in [ 15 , 1 ] .
Theorem 1.1 ( JLL [ 15 , 1] ) . Let x , y ∈ Rd . Let R ∈ Mk×d , k < d , be a random projection matrix with entries drawn iid from a 0 mean subgaussian distribution with parameter
σ2 , and let Rx , Ry ∈ Rk be the images of x , y under R .
Then , ∀ǫ ∈ ( 0 , 1 ) : Pr{kRx − Ryk2 < ( 1 − ǫ)kx − yk2kσ2} < exp„− Pr{kRx − Ryk2 > ( 1 + ǫ)kx − yk2kσ2} < exp„− kǫ2
8 « ( 2 ) 8 « ( 3 ) kǫ2
As a side note , in practice we may take the variance of the entries of R to be 1/k to keep the original lengths of the vectors on average . While this would simplify the notations throughout , since then kσ2 = 1 , we prefer not to commit to this choice at this point for the sake of clarity , since other choices for σ2 are also in use in practice .
Bounding the dot product under RP is typically treated simply as a corollary of JLL , as follows ( see eg [ 4 ] Corollary 2 , [ 8 ] Lemma 3.2 , [ 30 ] Corollary 1 , etc ) . Rewrite :
( Rx)T Ry =
1
4 `kR(x + y)k2 − kR(x − y)k2´
Now , applying the JLL on both terms separately and applying the union bound yields :
Pr{(Rx)T Ry < xT ykσ2 − ǫkσ2 · kxk · kyk} < 2 exp„− Pr{(Rx)T Ry > xT ykσ2 + ǫkσ2 · kxk · kyk} < 2 exp„− kǫ2
8 « 8 « kǫ2 where the factors of 2 are brought in by the use of the union bound . When we need both tails to be bounded we will of course get a factor of 4 in front of the exponential : Pr{kσ2(xT y − ǫ · kxk · kyk ) ≤ ( Rx)T Ry ≤ kσ2(xT y + ǫ · kxk · kyk)} 8 «
≥ 1 − 4 exp„− kǫ2
A variation on the same idea that is also in use ( see eg in [ 16] ) , is to decompose the dot product differently :
( Rx)T Ry =
1
2 `kR(x − y)k2 − kRxk2 − kRyk2´
Then three separate applications of the JLL combined by the union bound yield the constants 3 and 6 for the one tail and 2 tail bounds respectively .
Although for certain types of analyses the constants are not important and need not be optimal , knowing the optimal constant does matter in practice since a smaller constant implies a better guarantee than a larger one . From both theoretical and practical standpoints it is unpleasing to have weaker guarantees for the dot product under RP than we have for its induced distance – moreover since many core machine learning and data mining algorithms , such as classification and regression , rely on dot product operations , and this issue was indeed raised in [ 32 , 35 , 40 ] .
In the light of this , and since the recent controversy about the preservation of obtuse vs . acute angles raised in [ 39 ] , we give a different proof for the preservation of the dot product under RP in the next section , which eliminates the suboptimal constant factors .
2 .
IMPROVED BOUNDS ON THE DOT PRODUCT OF RANDOMLY PROJECTED POINTS
Here we give a new and elementary proof to bound the tails of the probability of deviation between the dot product of RP ed vectors and the dot product of the original vectors . Our resulting bounds match the JLL bounds , which
488 are known to be optimal for the Euclidean distance of RP ed vectors . jection matrix having iid 0 mean subgaussian entries with
Theorem 2.1 ( Dot Product under Random Projection ) . Let x , y ∈ Rd . Let R ∈ Mk×d , k < d , be a random proparameter σ2 , and let Rx , Ry ∈ Rk be the images of x , y under R . Then , ∀ǫ ∈ ( 0 , 1 ) : Pr{(Rx)T Ry < xT ykσ2 − ǫkσ2 · kxk · kyk} < exp„− Pr{(Rx)T Ry > xT ykσ2 + ǫkσ2 · kxk · kyk} < exp„−
8 « ( 4 ) 8 « ( 5 ) kǫ2 kǫ2
Proof . We prove the statement in eq . ( 4 ) . Eq ( 5 ) may be proved in much the same way .
Without loss of generality we replace x ← x kxk , y ← y kyk , and rewrite : bounded by Jensen inequality :
Using that exp(· ) is a convex function , the above is upperαEexp»− ( kR(x + y)k2 − ( 1 − ǫ)kx + yk2kσ2)–ff +(1 − α)Eexp» λ 1 − α
( kR(x − y)k2 − ( 1 + ǫ)kx − yk2kσ2)–ff ( 10 )
λ α where we also used the linearity of the expectation operator . Now , since this upper bound holds for any λ > 0 , we are free to choose its value to tighten the bound . In fact , observe that we are also free to chose α ∈ ( 0 , 1 ) , which comes in handy . Denote
λ1 :=
λ α
;
λ2 :=
λ
1 − α
( 11 )
We may now conveniently optimise eq . ( 10 ) in λ1 and λ2 individually , and choose the following to approximately minimise the expression in eq . ( 10 ) :
Pr{(Rx)T Ry < xT ykσ2 − ǫkσ2} = Pr{−(Rx)T Ry > −xT ykσ2 + ǫkσ2} ≤ Pr{−xT RT Ry > −xT ykσ2 +
1 4
ǫkσ2(kx + yk2 + kx − yk2)}
4 ( 2kxk2+ Now , we rewrite the dot products using the parallelogram
The last line holds because 1 2kyk2 ) = 1 since kxk = kyk = 1 . law as follows :
4 ( kx+yk2+kx−yk2 ) = 1 xT RT Ry = xT y =
1
1
4 `kR(x + y)k2 − kR(x − y)k2´ 4 `kx + yk2 − kx − yk2´
Replacing and dividing through both sides by 1/4 , the probability of our interest is : Pr{−kR(x + y)k2 + kR(x − y)k2 > −kx + yk2kσ2 + kx − yk2kσ2 + ǫkσ2kx + yk2 + ǫkσ2kx − yk2} Re grouping , we get :
Pr{−kR(x + y)k2 + ( 1 − ǫ)kx + yk2kσ2
+kR(x − y)k2 − ( 1 + ǫ)kx − yk2kσ2 > 0}
( 6 )
Now , using ideas from the Chernoff bounding technique , we employ the Laplace transform of both sides , so ∀λ > 0 the probability in eq . ( 6 ) equals : Pr{exp[−λ(kR(x + y)k2 − ( 1 − ǫ)kx + yk2kσ2 )
+λ(kR(x − y)k2 − ( 1 + ǫ)kx − yk2kσ2 ) ] > 1} ( 7 )
By Markov inequality this is upper bounded by : E{exp[−λ(kR(x + y)k2 − ( 1 − ǫ)kx + yk2kσ2 )
+λ(kR(x − y)k2 − ( 1 + ǫ)kx − yk2kσ2)]}
( 8 )
Next , we introduce a new parameter : For any choice of α ∈ ( 0 , 1 ) , the expectation in eq . ( 8 ) equals the following : Eexp»α„− +(1 − α )
( kR(x + y)k2 − ( 1 − ǫ)kx + yk2kσ2)« 1 − α
( kR(x − y)k2 − ( 1 + ǫ)kx − yk2kσ2)–ff ( 9 )
λ α
λ
λ =
λ1λ2
λ1 + λ2
( 12 )
= λ2
This choice ensures that α is in the correct interval : Indeed , from the definition of λ1 we have α = λ λ1+λ2 ∈ ( 0 , 1 ) λ1 as required . Thus , in eq . ( 10 ) the probability of our interest is now bounded by a convex combination of two subexponential moment generating functions . Notice these have the same form as those that appear in the proof of the JohnsonLindenstrauss lemma , and may be computed as follows . By Lemma 16 in [ 11 ] , if X is subgaussian with parameter
σ2 then1 for any t ∈ [ 0 , 1 ) , E[exp(tX 2/(2σ2 ) ] ≤ 1/√1 − t . Applying this to both terms in eq . ( 10 ) , and noting that R(x + y ) is subgaussian with parameter σ2kx + yk2 , and R(x − y ) is subgaussian with parameter σ2kx − yk2 , ( so t := −2λ1σ2kx + yk2 in the first expectation and t := 2λ2σ2kx − yk2 in the second one ) , we have that eq . ( 10 ) is upper bounded by the following : α`1 + 2λ1σ2kx + yk2´−k/2 ( 1 − α)`1 − 2λ2σ2kx − yk2´−k/2 provided that λ1 and λ2 are such that λ1σ2kx + yk2 > − 1 and λ2σ2kx − yk2 < 1 2 . Computing derivatives wrt λ1 and λ2 , equalling them to zero and solving these stationary equations yields the following optimal values after some algebra : exp[λ1(1 − ǫ)kσ2kx + yk2 ] +
2 exp[−λ2(1 + ǫ)kσ2kx − yk2 ] ( 13 )
λ1 =
λ2 =
ǫ
( 1 − ǫ)2σ2kx + yk2 ( 1 + ǫ)2σ2kx − yk2
ǫ
( 14 )
( 15 ) and it is easy to verify that they both satisfy the conditions required above .
Plugging back into eq . ( 13 ) we get after some algebra :
α(1 − ǫ)k/2 exp„ kǫ
2 « + ( 1 − α)(1 + ǫ)k/2 exp„−
2
+ k 2
1This follows from eq.(1 ) : Multiply both sides of eq.(1 ) by log(1 − ǫ)« + ( 1 − α ) exp„−
α exp„ kǫ σ/√2πt exp(−λ2σ2/(2t ) ) when t 6= 0 ( and when t = 0 the stated inequality holds with equality trivially ) , then integrate both sides wrt λ ∈ R . kǫ 2 k 2
+ kǫ
2 « = log(1 + ǫ)«
489 We use the following inequalities , which are easy to verify : log(1 + ǫ ) ≤ ǫ − ǫ2/4 ǫ2 log(1 − ǫ ) ≤ −ǫ − 2
< −ǫ −
ǫ2 4
( 16 )
( 17 )
Eg to see eq . ( 16 ) , define f ( ǫ ) = log(1 + ǫ ) − ǫ + ǫ2/4 and note that f ( 0 ) = 0 and f ′(ǫ ) = ǫ(ǫ−1 )
Replacing , we get the claimed result :
α exp„− kǫ2
8 « + ( 1 − α ) exp„−
4(1+ǫ ) < 0,∀ǫ ∈ ( 0 , 1 ) . 8 « = exp„− kǫ2 kǫ2
8 « ¤
An advantage of the above proof over previous ones is that it allowed us to reduce the constant in front of the exponential to 1 , which now matches the JLL bounds for the Euclidean distance . We should point out that reducing this constant to 1 was previously achieved only for the special case of Gaussian RP matrices [ 40 ] . Their proof technique relies on the rotational invariance property of the Gaussian distribution , and hence it cannot be directly generalised to subgaussian RPs . Our result holds for subgaussian RPs , which represent a larger class that subsumes the Gaussian . Some examples particularly designed for computational efficiency may be found in [ 1 ] .
3 . COROLLARIES , DISCUSSION , AND LINKS
WITH PREVIOUS RESULTS
The following immediate corollaries may be useful in applications . These will also serve us to make further comparisons and links to several previous results .
Corollary 3.1 ( Relative distortion bounds ) . Denote by θ the angle between the vectors x , y ∈ Rd . Then we have the following :
1 . Relative distortion bound : Assume xT y 6= 0 . Then , Pr| xT y − kσ2| > ǫff < 2 exp„−
ǫ2 cos2(θ)« ( 18 ) xT RT Ry
8(kσ2)2 k
2 . Multiplicative form of relative distortion bound :
Pr{xT RT Ry < xT y(1 − ǫ)kσ2} < exp„− Pr{xT RT Ry > xT y(1 + ǫ)kσ2} < exp„− k 8 k 8
ǫ2 cos2(θ)« ( 19 ) ǫ2 cos2(θ)« ( 20 )
Proof . 1 . Assuming xT y 6= 0 we divide both sides in eqs ( 4) (5 ) by xT y , and note that kxk·kyk cos(θ ) . We get : xT y
Pr xT RT Ry Pr xT RT Ry xT y
< kσ2 −
> kσ2 + xT y = 1 ǫkσ2 cos(θ)ff < exp„− cos(θ)ff < exp„−
ǫkσ2 kǫ2
( 21 )
8 « 8 « ( 22 ) kǫ2
Putting ǫkσ2/ cos(θ ) := η and solving for ǫ gives ǫ = η cos(θ)/kσ2 . Replacing and renaming η to ǫ completes the proof by employing the union bound to join the upper and lower tail bounds .
2 . The multiplicative form follows from eqs ( 21) (22 ) by putting ǫ/ cos(θ ) := η and solving for ǫ to get ǫ = η cos(θ ) . Replacing and renaming η to ǫ yields the stated eqs . ( 19) (20 ) . ¤
Corollary 3.1 tells us how the guarantees for the relative distortion of the dot product under RP depend on the angle between the original vectors : As we can see , there is no guarantee when the two vectors are orthogonal on each other , but for all other angles the chance of getting a relative distortion larger than a fixed tolerance ǫ decreases exponentially with cos2(θ ) and with k .
Observe that our low distortion guarantee is symmetric around orthogonal angles – that is , contrary to the suggestion / conjecture in [ 39 ] , we have symmetrically the same guarantees on the obtuse angles as we do on the acute angles . Section 4 will verify the validity of this result against empirical evidence .
It is also interesting to relate this result to the arguments and findings in [ 32 ] . In [ 32 ] the authors conjecture that it is more difficult to derive practically useful tail bounds for xT RT Ry than it is for kRx−Ryk2 . Their argument is essentially that in the case of xT RT Ry the coefficient of variation is unbounded . In particular , [ 32 ] computed the following for the case when the entries of R are iid Gaussian with mean 0 and variance 1/k : pVar(xT RT Ry ) xT y
≥r 2 k
( unbounded )
( 23 )
We are interested to see what makes the above coefficient of variation unbounded . The variance is :
Var(xT RT Ry ) =r 1 k
( kxk2kyk2 + ( xT y)2 )
( 24 )
So , the coefficient of variation equals the following : pVar(xT RT Ry ) xT y
=s 1 k „1 +
1 cos2(θ)«
( 25 )
We can see again , in perfect agreement with our result in Corollary 3.1 , that an unbounded coefficient of variation occurs only when x and y are perpendicular . Furthermore , the coefficient of variation is symmetric around the angles π/2 and 3π/2 , and decreases as we move further away from these angles . Our results in Corollary 3.1 capture this behaviour while providing tail bounds rather than just a variance .
The next corollary highlights further facets of our result .
Corollary 3.2 ( Margin type bounds and sign projection ) . Denote by θ the angle between the vectors x , y ∈ Rd . Then we have the following :
1 . Margin bound : Assume xT y 6= 0 . Then , k
1)kxk · kykkσ2 , Pr{xT RT Ry < ρ} < exp −
8 „cos(θ ) −
• for all ρ that satisfy ρ < xT ykσ2 and ρ > ( cos(θ)− kxk · kykkσ2«2! ( 26 ) • for all ρ that satisfy ρ > xT ykσ2 and ρ < ( cos(θ)+ kxk · kykkσ2 − cos(θ)«2! ( 27 )
1)kxk · kykkσ2 , Pr{xT RT Ry > ρ} < exp −
8 „ k
ρ
ρ
2 . Dot product under sign random projection : Assume xT y 6= 0 . Then , Pr xT RT Ry xT y
< 0ff < exp„− k 8 cos2(θ)«
( 28 )
490 ρ
Proof . 1 . In eq . ( 4 ) require that xT ykσ2 − ǫ · kxk · kykkσ2 = ρ and solve for ǫ . This yields ǫ = xT y−ρ/(kσ2 ) kxk·kyk = cos(θ)− kx|·kykkσ2 . Since ǫ must be in ( 0 , 1 ) we need to require that ρ < xT ykσ2 and ρ > ( cos(θ ) − 1)kxk · kykkσ2 . This proves eq . ( 26 ) . Likewise , put xT ykσ2 + ǫkσ2 = ρ in eq . ( 5 ) to get ǫ = ρ/(kσ2)−xT y kx|·kykkσ2 − cos(θ ) . Again , we need ǫ ∈ ( 0 , 1 ) , which is ensured by the conditions ρ > xT ykσ2 and ρ < ( cos(θ ) + 1)kxk · kykkσ2 . This completes the proof of eq . ( 27 ) . kxk·kyk =
ρ
2 . The sign projection bounds follow as a special case of the margin bounds by taking ρ = 0 and considering two cases depending on the sign of cos(θ ) . If cos(θ ) > 0 , it is easy to see that the conditions on eq . ( 26 ) are satisfied and we get :
Pr{xT RT Ry < 0} < exp„− k 8 cos2(θ)«
( 29 )
If cos(θ ) < 0 , then we have the conditions on eq . ( 27 ) , which yields :
Pr{xT RT Ry > 0} < exp„− k 8 cos2(θ)«
( 30 )
Combining the two cases gives eq . ( 28 ) . ¤
The form of the bound given in eq . ( 26 ) is useful eg in learning theory , taking ρ > 0 , to bound the margin loss of compressive classifiers . We will develop this application in Section 5 . It also serves as an intermediate step to the bound on the dot product under sign random projection , eq . ( 28 ) . This bound generalises previous results on sign projection as discussed below .
The analysis of the dot product under sign projection was considered in [ 32 ] , [ 21 ] , and [ 10 ] for the special case when R has Gaussian entries and k = 1 , and for this special case the exact probability of sign change is available . This was initially derived to solve a semidefinite programming problem [ 21 ] but has found use is a wide range of applications including machine learning [ 10 ] , certain classes of hash functions [ 12 ] , sign random projections for storage efficient data sketches and recovery of angles between high dimensional points [ 32 ] . In [ 19 ] the sign flipping probability was computed for any k ≥ 1 , while the RP matrix R is still restricted to Gaussian . These results use the rotation invariance property of the Gaussian distribution and hence they are not directly generalisable to the subgaussian case .
In turn , our eq . ( 28 ) holds for R with any iid subgaussian entries . Also interesting to notice that , despite derived in a very different way , our probability bound has the same exponential form as the bound in the case of Gaussian R obtained in [ 19 ] , with the only difference that we now have a slightly worse constant inside the exponential , ie 8 instead of 2 – this is a small price to pay for allowing the more general class of subgaussian RPs .
Applications of sign RP , and in particular in conjunction with dot products arose in learning theory for bounding the 0 1 loss of compressive classifiers [ 19 ] . Other uses include similarity search methods in high dimensions [ 29 ] , and data classification with a better than chance guarantee [ 6 ] . These exploit the idea that , if x is closer to the query point than y in the data space , then there is greater chance than random guessing that this is also the case following projection onto a random line . The mentioned works used the special case k = 1 , but one may choose a larger k to control this chance as desired . More applications of both RP and sign RP are described in [ 26 ] .
}
)
ε
+ 1 (
<
) y
T x (
/ y R T R T x
<
)
ε
− 1 (
{ r P −
1
}
)
ε
+ 1 (
<
) y
T x (
/ y R T R T x
<
)
ε
− 1 (
{ r P − 1
1
0.8
0.6
0.4
0.2
0 0
1
0.95
0.9
0.85
0.8
0.75
0.7
0.65 0 cos(θ)=0.827 , ε=0.3 cos(θ)=0.827 , ε=0.1 cos(θ)=0.527 , ε=0.3 cos(θ)=0.527 , ε=0.1
50
150
100 200 Target dimension k ( a ) Acute angles
250
300 cos(θ)=0.062 , ε=0.3 cos(θ)=0.062 , ε=0.1 cos(θ)=0.0165 , ε=0.3 cos(θ)=0.0165 , ε=0.1
200
100 Target dimension k ( c ) Acute angles
300
}
)
ε
+ 1 (
<
) y
T x (
/ y R T R T x
<
)
ε
− 1 (
{ r P −
1
}
)
ε
+ 1 (
<
) y
T x (
/ y R T R T x
<
)
ε
− 1 (
{ r P − 1
1
0.95
0.9
0.85
0.8
0.75
0.7
0.65 0
1
0.8
0.6
0.4
0.2
0 0 cos(θ)=−0.062 , ε=0.3 cos(θ)=−0.062 , ε=0.1 cos(θ)=−0.0165 , ε=0.3 cos(θ)=−0.0165 , ε=0.1
100 Target dimension k
200
300
( b ) Obtuse angles cos(θ)=−0.827 , ε=0.3 cos(θ)=−0.827 , ε=0.1 cos(θ)=−0.527 , ε=0.3 cos(θ)=−0.527 , ε=0.1
50
100 200 Target dimension k
150
250
300
( d ) Obtuse angles
Figure 1 : Empirical rejection probabilities of dot product preservation , ie the probability that the relative distortion of the dot product after RP falls outside the allowed error tolerance . The target dimension varies from 1 to the original dimension d = 300 . ( a ) acute angles , ( b ) obtuse angles , ( c ) acute angles , ( d ) obtuse angles We see that the acute vs . obtuse nature of the angle is not the reason for the observed differences in the extent to which the dot product is preserved under RP . Note that the acute obtuse pairs of angles that are equally distanced on the two sides of π/2 – such as ( a)&(c ) and ( b)&(d ) – have identical preservation profile .
4 . NUMERICAL VALIDATION
In this section we empirically validate our theoretical results , and elucidate the issues around the preservation of obtuse angles in previous work . Specifically , our analysis predicted that the relative distortion of the dot product under RP depends on the cosine of the angle between the original vectors in a way that is symmetric about orthogonal angles , ie angles whose cosine is zero . So we expect to see the same behaviour for both acute and obtuse angles as long as the angle has the same distance from π/2 , on either side of π/2 . This is in contrast with the claim in [ 39 ] that acute angles would be preserved while obtuse angles would be not . We therefore start by replicating the experiments in [ 39 ] and demonstrating why the particular cases tested have been misleading .
We generate two vectors x ∈ Rd and y ∈ Rd of d = 300 , at a specified angle θ between them , and generate 2000 in
491 dependent instances of the RP matrix R . For the sake of concreteness and to directly compare with the results in [ 39 ] we used iid Gaussian entries N ( 0 , 1/k ) for these simulations . Using each of these RP matrices , we project both x and y to dimensionalities k ∈ {1 , , 300} . For two different distortion tolerances , ǫ = 0.1 and ǫ = 0.3 , we empirically estimate the probability that the relative distortion of the dot product after RP falls outside the allowed error tolerance , ie we compute : gles where the relative distortion can get arbitrarily large , and from our theoretical analysis the relative distortion at other angles should be symmetric around these values . This is exactly what we see in these figures . Hence we conclude that our bounds derived in the earlier sections accurately reflect the behaviour of dot product preservation , and the empirical evidence confirms our theoretical results . The extent of distortion of the dot product under RP is indeed symmetrically identical for both acute and obtuse angles .
Empirical : 1 − Pr{ ( 1−0.3 ) < xTRTRy / ( xT y ) < ( 1+0.3 ) } k i n o s n e m d i t e g r a T
0
50
100
150
200
250
300 0 pi/2 pi
Angle θ ( radians )
3pi/2 k i n o s n e m d i t e g r a T
2pi
Bound : 1 − Pr{ ( 1−0.3 ) < xTRTRy / ( xT y ) < ( 1+0.3 ) }
0
50
100
150
200
250
300 0 pi/2 pi
Angle θ ( radians )
3pi/2
2pi
( a ) ǫ = 0.3
Empirical : 1 − Pr{ ( 1−0.1 ) < xTRTRy / ( xT y ) < ( 1+0.1 ) }
Bound : 1 − Pr{ ( 1−0.1 ) < xTRTRy / ( xT y ) < ( 1+0.1 ) } k i n o s n e m d i t e g r a T k i n o s n e m d i t e g r a T
0
50
100
150
200
250
300 0
0
50
100
150
200
250
300 0
1 − ˆPr(1 − ǫ ) ≤
( Rx)T Ry xT y
≤ ( 1 + ǫ)ff
( 31 )
We repeated this for the two acute angles cos(θ ) ∈ {0.827 , 0.527} and the two obtuse angles cos(θ ) ∈ {−0.062 , −0.0165} used in [ 39 ] in the first instance . These results are shown in Figure 1 ( a) (b ) and these are indeed in agreement with the corresponding figures in [ 39 ] .
However , note that the particular acute angles tested above have a much larger absolute difference from π/2 than the particular obtuse angles tested . In fact , the latter are quite close to π/2 . This is what misled the authors to the false conclusion that obtuse angles would not be preserved . To see this , we now choose the symmetrical of these angles : Two obtuse ones ( cos(θ ) ∈ {−0.827 , −0.527} ) and two acute ones ( cos(θ ) ∈ {0.062 , 00165} ) These results are given in Figure 1 ( c) (d ) . As expected , these new results with the new choices of angles suggest the exactly opposite conclusion . This is because now the chosen acute angles happen to be quite close to π/2 while the two chosen obtuse ones are much farther from π/2 . In fact – as predicted by our theoretical analysis – it is most apparent from comparing the plots ( a ) vs . ( d ) in Figure 1 , that this matching pair of acute and obtuse angles ( ie that lie at equal absolute distance from π/2 ) do indeed exhibit the same behaviour . Likewise , the plots ( b ) vs . ( c ) of Figure 1 show another matching pair of acute & obtuse angles with identical behaviour .
In Figure 2 , the leftmost column of plots depict the full picture of relative distortion estimates ( cf . eq . ( 31 ) ) when the angle between the original high dimensional points varies on the full range θ ∈ [ 0 , 2π ] , and the target dimension varies in k ∈ [ 0 , 300 ] . We repeat for ǫ = 0.3 and ǫ = 0.1 as before , and we also show in addition the analogous experiments for the dot product under sign random projections . For the latter we report the empirical probability of sign flipping after RP :
ˆPr ( Rx)T Ry xT y
< 0ff
( 32 )
As before , all empirical probabilities are estimated from 2000 independent draws of the RP matrix R . In all of these plots , a darker grey level indicates higher probability .
The second column of plots in Figure 2 show the corresponding theoretical bounds on these probabilities , for comparative visual inspection . For sign projection we used our generic bound for subgaussian RP from Corollary 3.2 , and did not take advantage of the tighter special case for Gaussian R . In all cases it is most apparent that the true behaviour of these probabilities is well captured by the theoretical bounds . For instance , according to our theoretical analysis , the sign flipping should exhibit the same sort of dependence on the angle θ as the relative distortion of dot products under regular RP do . The angles θ = π/2 and θ = 3π/2 are the ones where cos(θ ) = 0 so these are the an k i n o s n e m d i t e g r a T
2pi pi/2 pi
Angle θ ( radians )
3pi/2
0
50
100
150
200
250
300 0 pi/2 pi
Angle θ ( radians )
3pi/2
( b ) ǫ = 0.1
Empirical : Pr{ xTRTRy / ( xT y ) < 0 }
Bound : Pr{ xTRTRy / ( xT y ) < 0 }
0
50
100
150
200
250 k i n o s n e m d i t e g r a T
2pi
300 0 pi/2 pi
Angle θ ( radians )
3pi/2 pi/2 pi
Angle θ ( radians )
3pi/2
2pi
2pi
( c ) Sign projection
Figure 2 : The full chart of rejection probability estimates for the tolerance values of ǫ = 0.1 and ǫ = 0.3 , and sign flips , as the angle between the original vectors varies in [ 0 , 2π ] and the target dimension varies from 1 to the original dimension d = 300 . The plots in the leftmost column represent empirical probability estimates computed from 2000 independent realisations ; the rightmost column of plots are the corresponding theoretical bounds . The grey levels indicate the probability of distortion : Darker means higher probability . The match between empirical behaviour and theoretical bounds is most apparent . Most importantly , all of these probabilities are symmetric around the angles of π/2 and 3π/2 ( ie orthogonal vectors before RP ) , which means that the preservation of the dot product is symmetrically identical for both acute and obtuse angles .
5 . AN APPLICATION IN MACHINE LEARN
ING THEORY
There have been numerous successful practical applicaIn this tions of RP , as already pointed out in Section 1 .
492 section we consider one of them , namely compressive classification with a norm constrained linear classifier . SVM is a good example of this , and previous studies eg in [ 20 , 39 , 13 ] already demonstrated experiments using RP ed high dimensional data sets . What is still lacking is a more complete theoretical understanding of the implications of using RP for dimensionality reduction before performing the classification . A theory that establishes guarantees on the performance , and that would explain the observed empirical performances is much desirable . 5.1 Margin bound on the generalisation error of compressive linear classif ers
In this section we show how we can use our results presented in the earlier sections to bound the generalisation error of norm constrained linear classifiers that receive RPed data . This will be given in terms of the margin loss and the empirical Rademacher complexity of the class of classifiers under study . To do this we will apply eq ( 26 ) of our Corollary 3.2 in a special case .
Consider the hypothesis class of linear classifiers defined by a unit length parameter vector :
H = {x → h(x ) = wT x : kwk2 = 1}
( 33 ) where w ∈ Rd,kwk = 1 . The parameters w are estimated from a training set of size N , denoted as T N = {(xn , yn)}N n=1 , where ( xn , yn ) iid∼ D over X × {−1 , 1},X ⊆ Rd . following :
We will work with the margin loss , which is defined as the if ρ ≤ u
ℓρ(u ) =8>< 0 1 − u/ρ if u ∈ [ 0 , ρ ] > : 1 if u ≤ 0
( 34 )
N PN n=1 ℓρ(h(xn)yn ) .
The generalisation error ( or risk ) of a learner h ∈ H is defined as E(x,y)∼D[h(x ) 6= y ] , and the empirical margin error of h is 1 We are interested in the case when d is large and N not proportionately so . This is the case in domains where obtaining labelled examples is relatively expensive . Denote the RP matrix R ∈ Rk×d , k < d , with entries Rij drawn iid from a subgaussian distribution . The classifier only receives the randomly projected data T N In the reduced k dimensional space we have analogous definitions , and we will use a subscript to refer to the reduced space . So the hypothesis class in the reduced space is
R = {(Rxn , yn)}N n=1 .
RRx : kwRk2 = √kσ2}
HR = {x → hR(Rx ) = wT
( 35 ) where wR ∈ Rk are the parameters , which are estimated from T N ˆhR = arg min hR∈HR
R by minimising the empirical margin error : n=1 ℓρ(hR(Rxn ) , yn ) .
1
We take the parameter of the entries of R to be σ2 = 1/k so the scale of the original data geometry remains the same after RP on average . Thus , kσ2 = 1 .
N PN
The quantity of our interest is the generalisation error of
ˆhR as a random function of both T N , and R – that is ,
E(x,y)∼DhˆhR(Rx ) 6= yi
( 36 )
511 Generalisation bound We will make use of the empirical Rademacher complexity of HR , which is defined as
1 hR∈HR
N PN n=1 γnhR(Rxn) ] , where
R = {(Rxn , yn)}N
ˆRN ( HR ) = Eγ[ sup γ = ( γ1 , , γN ) and γn takes values in {−1 , 1} with equal probability . Here we used that HR is closed under negation ie HR = −HR so that no absolute value is needed in the expression of the Rademacher complexity . Theorem 51 Let R by a k × d , k < d matrix with iid 0 mean subgaussian entries with parameter 1/k , and a compressed training set T N n=1 , where ( xn , yn ) from some distribution D . For any δ ∈ are drawn iid ( 0 , 1 ) , the following holds with probability at least 1 − 3δ for the empirical minimiser of the margin loss in the RP space , uniformly any margin parameter ρ ∈ ( 0 , 1 ) : h∈H( 1 E(x,y)∼DhˆhR(Rx ) 6= yi ≤ min vuuut 0 @1 +s 8 log(1/δ ) +q3 log(1/δ)pSkff + +s log log2(2/ρ )
1 Tr„ XX T N « A + 3s log(4/δ )
1 ( h(xn)yn < ρ ) + Sk
Xn=1
1 √N
4 ρ ·
2N
N
N k
N where θn is the angle between the parameter vector of h and the vector xnyn . The function 1(· ) takes value 1 if its argument is true and 0 otherwise . X is an N × d matrix that holds the input points , and Sk =
0
8>>>>>< >>>>> :
0 BBBB@
ρs1 + r 8 log(1/δ ) k
1
N
8
1 k
N
−
+ δ exp kxn k cos(θn ) −
Xn=1
1 ( h(xn )yn ≥ ρ )
BBBBB@ and ( ·)+ = max{· , 0} . Proof . Fixing R at first , we start from an existing margin bound that holds for any hR ∈ HR , uniformly for any margin ρ ∈ ( 0 , 1 ) [ 36 ] ( Theorem 4.5 ) , and which we apply in the reduced space :
CCCCCA
+
2
1 CCCCA
9>>>>>= >>>>> ;
N
1 N
E(x,y)∼DhˆhR(Rx ) 6= yi ≤
Xn=1 ˆRN ( HR ) +r log log2(2/ρ )
+
4 ρ
N
ℓρ(ˆhT
R(Rxn)yn )
+ 3r log(4/δ )
2N
( 37 )
Now , for the empirical risk minimiser of the margin loss we have that :
N
N
N
1 N
1 N
ℓρ(ˆhT
Xn=1
R(Rxn)yn ) ≤
ℓρ„ ( Rw)T Xn=1 kRwk 1„ wT RT Rxnyn Xn=1
Rxnyn« ( 38 ) < ρ« ( 39 ) for any choice of h ∈ H defined by parameter vector w , because Rw kRwk ∈ HR , and eq . ( 39 ) is due to an upper bound on the margin loss . n=1 1(wT xnyn < ρ ) , and notice
We add and subtract 1 kRwk
1 N
≤ that :
N PN < ρ! − kRwk 1 wT RT Rxnyn
1 wT RT Rxnyn Xn=1 kRwk
1 N
N
1 N
N
Xn=1 ≤
1 N
N
Xn=1
1(wT xnyn < ρ )
< ρ! · 1 “ wT xnyn ≥ ρ ”
( 40 )
Now , we exploit the fact that the right hand side in eq . ( 40 ) is a random variable that takes values in the bounded interval [ 0 , 1 ] . Hence we can apply a Chernoff bound ( Theorem
493 1.1 in [ 18 ] ) to get the following upper bound :
N
N
≤
1 N
1 N kRwk
1 “ wT xnyn ≥ ρ ” PrR( wT RT Rxnyn
< ρ ) 1 ( wT xnyn ≥ ρ ) PrR wT RT Rxnyn
Xn=1 +q3 log(1/δ)vuut < ρff ( 41 ) Xn=1 wp 1− δ . From eq . ( 41 ) we see that it is enough to bound PrRn wT RT Rxnyn < ρo when wT xnyn ≥ ρ . We will do this by applying our Corollary 3.2 to the vectors w and xnyn . First , rewrite for any ǫ > 0 : kRwk kRwk
PrR wT RT Rxnyn kRwk
< ρff ≤ . . .
PrRn “ wT RT Rxnyn < ρ√1 + ǫkwk ” ∨`kRwk > √1 + ǫkwk´o ( 42 )
This is easy to see as the statement in the probability on the lhs implies the one on the rhs ( the negation of the second statement implies the negation of the first ) .
Now , eq . ( 42 ) is upper bounded using the union bound :
≤ PrRnwT RT Rxnyn < ρ√1 + ǫkwko + PrR˘kRwk > √1 + ǫkwk¯
( 43 )
We apply our Corollary 3.2 eq . ( 26 ) to the first term , and the Johnson Lindenstrauss lemma to the second term . Since we have wT xnyn ≥ ρ , and kwk = 1 , and the first condition needed to apply eq.(26 ) is wT xnyn > ρ√1 + ǫkwk we will use the function ( ·)+ = max{· , 0} . The second condition is trivially satisfied because ρ > 0 . Thus , we get that eq . ( 43 ) is upper bounded by the following : k k 8 exp(−
8 „cos(θn ) −
+ ) + exp−
We may chose to have the last term equal to δ and express
ρ√1 + ǫ kxnk «2 ǫ as a function of δ ie ǫ =q 8 log(1/δ ) Finally , ˆRN ( HR ) can be bounded by exploiting that kˆhRk2 ≤
Replacing this into eq . ( 41 ) completes the bound on the empirical margin loss term of eq . ( 37 ) .
ǫ2ff ( 44 )
1 , in the same way as in [ 27 ] with a modification that allows us efficient bounding whp with respect to the random draws of R : k
.
ˆRN ( HR ) = Eγ[ sup hR∈HR
1 N
N
Xn=1
γnhR(Rxn ) ]
= Eγ[ sup wR:kwRk2≤1
1 N wR
γnRxn ]
N
Xn=1
( by Cauchy Schwartz )
≤
≤
=
≤
γnRxnk2
N
1 N
N
1
Eγk
Xn=1 NvuutEγk Xn=1 Nvuut Xn=1 kRxnk2 √Nvuut(1 + ǫ )
1 N
1
1
N
γnRxnk2
2
( by Jensen ineq . )
2 ( independence of γ1 , γN )
N
Xn=1 kxnk2
2
( 45 ) with probability 1 − exp(−kǫ2/8 ) . Eq ( 45 ) follows by a Chernoff bound for dependent variables [ 38 ] . Rearranging , we get that wp 1 − δ ,
ˆRN ( HR ) ≤
1
√Nvuut 1 +r 8 log(1/δ ) k
! Tr„ XX T
N « ( 46 ) where X is the matrix that holds the input points .
Combining the three probability bounds , ie eq . ( 46 ) , eq . ( 44 ) and eq . ( 37 ) that each hold wp 1−δ , the union bound gives the statement of the theorem . ¤ 512 Discussion Some comments are now in order . There has been previous work bounding the error of compressive classifiers based on the preservation of a margin [ 4 , 6 , 13 , 37 ] . The works of [ 4 ] and [ 6 ] concern the case of classes that are linearly separable by a margin . The results in [ 13 ] and [ 37 ] do not make this assumption but they assume that the data has a sparse representation . The result in [ 37 ] is specific to the SVM .
In contrast , we do not assume class separability , and do not assume a sparse representation – in fact we do not make any assumption on the data . Margin bounds hold true irrespective of class separability , and so does our bound in Theorem 51 Of course , as all margin bounds , our bound is tightest when most points do have a large margin , since this is when the empirical error term ( the first term on the rhs ) is small . Also note that the margins of all points participate in the bound . In particular , cos(θn ) is the normalised margin of the n th point , and the terms of Sk decay exponentially with this quantity . The average of unnormalised margins also features in the first term on the rhs , ie the empirical error term . Overall , the bound may be seen as a margin distribution bound for compressive linear classifiers .
It is insightful to look at what the obtained bound tells us . Notice that , apart from the Sk terms , the rhs of eq(51 ) has the same flavour as the margin bound in data space – with the margin ρ balancing between the empirical margin ( first term ) and the Rademacher complexity ( fourth term ) . The latter shrinks with ρ/pTr(XX T /N ) ( ie margin divided by diameter ) just like the empirical Rademacher complexity of the uncompressed function class H does . To make this link more precise , we can rewrite eq . ( 46 ) as the following :
ˆRN ( HR ) ≤s1 +r 8 log(1/δ ) k
· ˆRN ( H )
( 47 )
. k
We see this is no larger than ( 1 + ǫ ) times the empirical Rademacher complexity of the original H , where ǫ = q 8 log(1/δ )
Now , Sk is a penalty we get for working in the RP space : It penalises the good points to some extent – ie those points that have margin at least ρ – by how close their margin actually is from ( an √1 + ǫ multiple of ) ρ . These penalties shrink exponentially with k , and the points whose margin is in the small interval [ ρ , ρ√1 + ǫ ] will get penalty 1 just as the points that didn’t have a margin ρ . Overall , this is the term that reveals some influential characteristics of the data distribution that permit the classification task to be solved well and efficiently in a random subspace by the considered function class . In the present case this is governed by the distribution of margins of the uncompressed train
494 ing points , namely most points should have a margin larger than ρ√1 + ǫ .
It might be interesting to note also that the bound does not depend on d but only on the margin parameter ρ . Due to the norm constraint on the parameter vectors wR and w , this is also the case for the dataspace margin bound – the Rademacher complexity does not depend on d as the VC dimension would . So , for the class of classifiers analysed here we do not get a reduction of the complexity term by doing RP – in contrast to the VC bound based analysis in [ 19 ] . Such reduction would only occur in the unregularised linear function class analysed in [ 19 ] , and could be more pronounced in the case of peculiar data distributions where VC bounds are tight .
Figure 3 illustrates the predictive behaviour of our bound on the UCI data set of Advert classification , in comparison with the alternative VC type bound in [ 19 ] , and the actual performance estimated on holdout sets using SVM with default settings and 30 random splits ( in proportion 2/3 training & 1/3 testing ) of the data . We used δ = 0.1 and ρ = 005 This data set was previously used in [ 20 ] , it has d = 1554 features and N = 3279 points . We stann∈{1,,N }kxnk = 1 . dardised the data first , and scaled it to max s d n u o b n o i t a s i l a r e n e G
4.9 4.8 4.7 4.6 4.5 4.4 4.3 4.2 t e a m i t s e r o r r e t u o d o H l
500
0.14
0.12
0.1
0.08
0.06
0.04
New bound Thm 5.1 Bound from [ 19 ]
100
200
300
Reduced dimension ( k )
400 optimal bounds on the Euclidean distance in the JohnsonLindenstrauss lemma . This is pleasing for at least two reasons : From the conceptual point of view , it is the inner product that defines a norm , in particular the dot product defines the Euclidean norm – hence a guarantee that holds on the latter was quite natural to expect to hold also on the former . From the practical point of view , the dot product is ubiquitous in algorithms for data mining and machine learning . The use of random projections to speed up such algorithms or to perform privacy preserving data mining without much distortion in performance can now be better justified . Previous guarantees on the preservation of the dot product under random projections have been loose and incomplete , and left room for controversy . Our results shed light on these issues and give the precise way in which the relative distortion of the dot product under random projection depends on the angle between the original vectors . This dependence turned out to have an intimate relationship with the notion of margin in the context of generalisation theory . We also obtained connections with sign random projections , generalising earlier results . We demonstrated numerical simulations that confirm the validity of our theoretical results . Finally we provided an application of our results in learning theory , where we derived a new margin distribution type generalisation error bound for compressive linear classifiers under the margin loss .
Our proof technique applies to any independent entry subgaussian random projection matrix , which includes sparse and bit flipping variants [ 1 ] . An interesting question for future work would be to investigate whether it could be adapted to other Fast JL transforms such as those in [ 2 ] .
100
200
300
400
Reduced dimension ( k )
500
7 . REFERENCES
Figure 3 : Illustration of the theoretical error bound of Theorem 5.1 in comparison with the VC type bound in [ 19 ] and the actual performance of SVM on holdout sets , using the Advert classification data set . The error bars on the latter plot represent one standard deviation over 30 independent random train test splits of the data . The gain in the tightness of our new bound is most apparent , as well as the agreement between the theoretical error bound ( which predicts performance based on training errors only ) and the actual holdout estimates of the performance .
As expected at this sample size , our new bound is tighter than the VC bound in [ 19 ] and both bounds predict well the trend in the holdout errors . The gain in tightness is due to the tighter complexity term that is able to take advantage of the norm constraint on the parameter vector and does not grow with the input data dimension ( k ) , whereas the VC complexity grows with √k . This is of course offset by the magnitudes of Sk that are obviously larger than the corresponding flip probabilities in [ 19 ] ( which correspond to the case ρ = 0 ) – in line with the spirit of margin bounds . We have not investigated whether the expression of Sk could be improved , eg if the union bound in eq.(43 ) in the proof could be avoided as well , this remains for future work . 6 . CONCLUSIONS
[ 1 ] D . Achlioptas . Database friendly Random Projections :
Johnson Lindenstrauss with Binary Coins . Journal of Computer and System Sciences , 66(4):671–687 , 2003 .
[ 2 ] N . Ailon , B . Chazelle . The fast Johnson
Lindenstrauss transform and approximate nearest neighbors . SIAM J . Comput . , 39(1 ) , 302 322 .
[ 3 ] N . Alon . Problems and results in extremal combinatorics , Part I . Discrete Math , 273:31 53 , 2003 .
[ 4 ] RI Arriaga , and S . Vempala . An algorithmic theory of learning : Robust concepts and random projection . In 40th Annual Symposium on Foundations of Computer Science ( FOCS ) , pp . 616–623 , 1999 .
[ 5 ] D . Arpit , I . Nwogu , G . Srivastava , V . Govindaraju .
An analysis of random projections in cancelable biometrics . arXiv:1401.4489 [ cs.CV ] , 2014 .
[ 6 ] MF Balcan , A . Blum , S . Vempala . Kernels as features : On kernels , margins , and low dimensional mappings , Machine Learning 65 ( 1 ) , 79 94 , 2006 . [ 7 ] RG Baraniuk , M . Davenport , RA DeVore , and
MB Wakin . A simple proof of the restricted isometry property for random matrices . Constructive Approximation 28 , no . 3 , pp . 253 263 , 2008 . [ 8 ] A . Barvinok . Integration and optimization of multivariate polynomials by restriction onto a random subspace . Foundations of Computational Mathematics , 7 , pp . 229 244 , 2007 .
In this paper we proved new bounds on the dot product under random projection that take the same form as the
[ 9 ] E . Bingham and H . Mannila . Random projection in dimensionality reduction : Applications to image and
495 text data . In Knowledge Discovery and Data Mining ( KDD ) , pp . 245–250 , ACM Press , 2001 .
[ 10 ] A . Blum . Random projection , margins , kernels , and feature selection . In SLSFS 2005 , ed . Saunders et al . No . 3940 in LNCS . pp . 55 68 , 2006 .
[ 11 ] VV Buldygin , YV Kozachenko . Metric characterization of random variables and random processes . American Mathematical Society , 2000 .
[ 12 ] M . Charikar . Similarity estimation techniques from rounding algorithms . In Proceedings of the thirty fourth annual ACM symposium on Theory of computing . ACM . pp . 380 388 , 2002 .
[ 13 ] R . Calderbank , S . Jafarpour , and R . Schapire .
Compressed learning : Universal sparse dimensionality reduction and learning in the measurement domain . Technical report , Rice University , 2009 .
[ 14 ] S . Dasgupta . Learning mixtures of Gaussians . In Annual Symposium on Foundations of Computer Science ( FOCS ) , vol . 40 , pp 634 644 , 1999 .
[ 25 ] AT Kalai , A . Moitra , G . Valiant . Disentangling
Gaussians . Communications of the ACM 55 , no . 2 , pp . 113 120 , 2012 .
[ 26 ] ZS Karnin , Y . Rabani , A . Shpilka . Explicit dimension reduction and its applications . SIAM Journal on Computing , 41(1 ) , pp . 219 249 , 2012 .
[ 27 ] SM Kakade , K . Sridharan , A . Tewari . On the complexity of linear prediction : Risk bounds , margin bounds , and regularization . Advances in Neural Information Processing Systems 21 , pp . 793–800 , 2008 .
[ 28 ] S . Kaski . Dimensionality reduction by random mapping : fast similarity computation for clustering . In Proceedings of the 1998 IEEE International Joint Conference on Neural Networks , vol . 1 , pp 413 418 , 1998 .
[ 29 ] J . Kleinberg . Two algorithms for nearest neighbor search in high dimensions . In Proceedings of the twenty ninth annual ACM Symposium on Theory of Computing ( STOC 1997 ) , ACM . p . 608 , 1997 .
[ 15 ] S . Dasgupta , and A . Gupta . An elementary proof of
[ 30 ] S . Krishnan , C . Bhattacharyya , and R . Hariharan . A the Johnson–Lindenstrauss Lemma . Random Structures & Algorithms , 22:60–65 , 2002 .
[ 16 ] MA Davenport , MB Wakin and RG Baraniuk .
Detection and estimation with compressive measurements . Technical Report TREE 0610 , Rice University , January 2007
[ 17 ] DL Donoho . Compressed sensing , IEEE Trans .
Information Theory 52 , no . 4 , pp . 1289 1306 , 2006 .
[ 18 ] DP Dubhashi , A . Panconesi . Concentration of measure for the analysis of randomized algorithms . Cambridge University Press , 2012 .
[ 19 ] RJ Durrant , A . Kab´an . Sharp generalization error bounds for randomly projected classifiers . 30th International Conference on Machine Learning ( ICML ) , Journal of Machine Learning Research Proceedings Track 28(3):693 701 , 2013 .
[ 20 ] D . Fradkin , D . Madigan . Experiments with random projections for machine learning . Proc . 19 th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining ( KDD ) , pp . 522 529 , 2003 . [ 21 ] M . Goemans , D . Williamson . Improved approximation algorithms for maximum cut and satisability problems using semidenite programming , Journal of the ACM ( JACM ) 42 , 1145 , 1995 .
[ 22 ] P . Indyk , R . Motwani . Approximate nearest neighbors : towards removing the curse of dimensionality . Proceedings of the 30 th annual ACM Symposium on Theory of computing , New York , NY , USA , pp . 604 613 , 1998 .
[ 23 ] WBJohnson , J . Lindenstrauss . Extensions of
Lipschitz mappings into a Hilbert space . Conference in Modern Analysis and Probability ( New Haven , Conn . , 1982 ) , Contemporary Mathematics 26 , Providence , RI : American Mathematical Society , pp . 189 206 , 1984 . [ 24 ] A . Kab´an . New bounds on compressive linear least squares regression . Proc . of the 17 th International Conference on Artificial Intelligence and Statistics ( AISTATS ) , Journal of Machine Learning Research Proceedings Track , vol . 33 , pp . 448 456 , 2014 . randomized algorithm for large scale support vector learning . In Advances in 20th Neural Information Processing Systems . 793–800 , 2008 .
[ 31 ] KG Larsen , J . Nelson . The Johnson Lindenstrauss lemma is optimal for linear dimensionality reduction , arXiv preprint arXiv:1411.2404 , 2014 .
[ 32 ] P . Li , T . Hastie , K . Church . Improving random projections using marginal information . In Proc . Conference on Learning Theory ( COLT ) 4005 , 635 649 , 2006 .
[ 33 ] K . Liu , H . Kargupta , J . Ryan . Random projection based multiplicative data perturbation for privacy preserving distributed data mining . IEEE Transactions on Knowledge and Data Engineering 18(1 ) , pp . 92 106 , 2006 .
[ 34 ] S . Marukatat . Classification with sign random projections . PRICAI 2014 : Trends in Artificial Intelligence Lecture Notes in Computer Science Volume 8862 , pp 708 719 , 2014 .
[ 35 ] AK Menon , GVA Pham , S . Chawla , A . Viglas . An incremental data stream sketch using sparse random projections . In Proceedings of the 2007 SIAM conference on data mining ( SDM ) , Minnesota , USA .
[ 36 ] M . Mohri , A . Rostamizadeh , A . Talwalkar .
Foundations of machine learning . The MIT Press , 2012 .
[ 37 ] S . Paul , C . Boutsidis , M . Magdon Ismail , P . Drineas .
Random projections for linear support vector machines . ACM Trans . Knowl . Discov . Data 8 , 4 , Article 22 , 2014 .
[ 38 ] A . Siegel . Toward a usable theory of Chernoff bounds for heterogeneous and partially dependent random variables . Technical Report , New York Univ . , 1995 .
[ 39 ] Q . Shi , C . Shen , R . Hill , A . Hengel . Is margin preserved after random projection ? Proceedings of the 29th International Conference on Machine Learning ( ICML ) , pp . 591–598 , 2012 .
[ 40 ] E . Skubalska Rafajlowicz . Neural networks with sigmoidal activation functions : dimension reduction using normal random projection . Nonlinear Analysis 71 , pp . e1255 e1263 , 2009 .
496
