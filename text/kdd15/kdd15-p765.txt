Dimensionality Reduction via Graph Structure Learning
Qi Mao† , Li Wang‡ , Steve Goodison(cid:92 ) , Yijun Sun†¶
†Bioinformatics Laboratory , SUNY at Buffalo , Buffalo , NY 14201
‡Department of Mathematics and Statistics , University of Victoria , BC V8P 5C2 ( cid:92)Department of Health Sciences Research , Mayo Clinic , Jacksonville , FL 32224
¶Department of Microbiology and Immunology , SUNY at Buffalo , Buffalo , NY 14201
{maoq1984 , liwangucsd}@gmail.com , goodisonsteven@mayoedu , yijunsun@buffalo.edu
ABSTRACT We present a new dimensionality reduction setting for a large family of real world problems . Unlike traditional methods , the new setting aims to explicitly represent and learn an intrinsic structure from data in a high dimensional space , which can greatly facilitate data visualization and scientific discovery in downstream analysis . We propose a new dimensionality reduction framework that involves the learning of a mapping function that projects data points in the original high dimensional space to latent points in a lowdimensional space that are then used directly to construct a graph . Local geometric information of the projected data is naturally captured by the constructed graph . As a showcase , we develop a new method to obtain a discriminative and compact feature representation for clustering problems . In contrast to assumptions used in traditional clustering methods , we assume that centers of clusters should be close to each other if they are connected in a learned graph , and other cluster centers should be distant . Extensive experiments are performed that demonstrate that the proposed method is able to obtain discriminative feature representations yielding superior clustering performance , and correctly recover the intrinsic structures of various real world datasets including curves , hierarchies and a cancer progression path .
Categories and Subject Descriptors I52 [ Design Methodology ] : Feature evaluation and selection ; I53 [ Clustering ] : Algorithms
Keywords Dimensionality Reduction , Graph Structure Learning , Clustering , Unsupervised Learning
1 .
INTRODUCTION
Contemporary simulation and experimental data acquisition technologies enable scientists and engineers to generate progressively large and inherently high dimensional data Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . Copyrights for components of this work owned by others than ACM must be honored . Abstracting with credit is permitted . To copy otherwise , or republish , to post on servers or to redistribute to lists , requires prior specific permission and/or a fee . Request permissions from Permissions@acmorg KDD '15 , August 10 13 , 2015 , Sydney , NSW , Australia . cfl2015 ACM . ISBN 978 1 4503 3664 2/15/08$15.00 DOI : http://dxdoiorg/101145/27832582783309 sampled from sources with unknown multivariate probability distributions . Data expressed with many degrees of freedom imposes serious problems for data analysis . It is often difficult to directly analyze these datasets in the original high dimensional space , and is desirable to reduce the data dimensionality in order to overcome the curse of dimensionality and associate data with intrinsic structures for data visualization and subsequent scientific discovery .
Dimensionality reduction is a learning paradigm that transforms high dimensional data into a low dimensional representation . Ideally , the reduced representation should correspond to the intrinsic dimensionality of the original data , and it can often be of advantage in practical applications to analyze the intrinsic structure of data . Examples include clustering of gene expression data and text documents [ 11 ] , and high dimensional data visualization of image datasets [ 25 , 31 ] . Accordingly , many dimensionality reduction methods have been proposed in the literature with the aim to preserve certain information of data . Principal component analysis ( PCA ) [ 18 ] is a classic method for this purpose . It is to learn a subspace linearly spanned by some orthonormal bases by minimizing a reconstruction error [ 6 ] . However , a complex structure of data may be misrepresented by a linear manifold constructed by PCA .
Certain complex structures have been studied to deal with the misrepresentation issue of PCA . Kernel PCA [ 27 ] first maps the original space to a reproducing kernel Hilbert space ( RKHS ) by a kernel function and then performs PCA in the RKHS space . Hence , KPCA is a non linear generalization of traditional PCA . A principal curve is an infinitely differentiable curve with a finite length that passes through the middle of data [ 16 ] . It can also be extended to describe surfaces in a high dimensional space . A few other methods can also handle complex principal objects . Generative topographic mapping ( GTM ) [ 4 ] was proposed to maximize the posterior probability of data that is assumed to be generated by a predefined low dimensional discrete grid mapped onto the original space and corrupted by additive Gaussian noise . GTM provides a principled alternative to self organizing map ( SOM ) [ 20 ] for which it is impossible to define an optimality criterion [ 12 ] . Regularized principal manifold [ 28 ] can be considered as a deterministic version of GTM , and the choice of a regularizer is more flexible than GTM . These methods construct a set of points in a lowdimensional discrete grid . The number of discrete points is a critical parameter [ 4 ] , and constructing discrete points in a moderate dimensionality might be impractical . For example , if the discrete grid is constructed in a d dimensional
765 space , one needs to generate kd discrete points by taking k distinct values for each dimension . Other methods [ 19 , 14 , 22 ] aim to construct principal objects represented by principal points which are still in the original high dimensional space . Hence , they neither perform dimensionality reduction nor learn an explicit structure .
Another family of dimensionality reduction methods is manifold learning [ 7 ] , which aims to find a manifold close to the intrinsic structure of data . By projecting data onto a manifold , the low dimensional representation of data can be obtained by unfolding the manifold . Isometric feature mapping ( Isomap ) [ 31 ] first estimates geodesic distances between data points using shortest path distances on a K nearest neighbor graph , and then use multidimensional scaling to find points in a low dimensional Euclidean space whose interpoint distances match the corresponding geodesic distances . Local linear embedding ( LLE ) [ 26 ] was proposed to find a mapping that preserves local geometry where local patches based on K nearest neighbors are nearly linear and overlap with one another to form a manifold . Laplacian Eigenmap [ 2 ] is based on spectral graph theory , and a K nearest neighbor graph is used to construct a Laplacian matrix . Other related methods are referred to a survey paper [ 7 ] . Since Knearest neighbor graphs are generally constructed from data resided in a high dimensional space , most manifold learning methods suffer from the curse of dimensionality . On one hand , there is little difference in the distances between different pairs of data points [ 3 ] . On the other hand , hubs are closely related to nearest neighbors [ 24 ] , that is , points that appear in many more K nearest neighbor lists than other points , effectively making them “ popular ” nearest neighbors . As alluded to by [ 24 ] , hubs can have a significant effect on dimensionality reduction and clustering , so we should take hubs into account in a way equivalent to the existence of outliers . As a result , it is less reliable to directly construct K nearest neighbor graphs in a high dimensional space .
To overcome above issues , in this paper we present a new dimensionality reduction setting that imposes three constraints : ( i ) the data points in a reduced space form an explicit structure ; ( ii ) the local geometry is captured by only using distances between any two data points in the reduced space ; ( iii ) clusters of the data points in the reduced space should follow the formed structure . To the best of our knowledge , there is no prior work that satisfies all three constraints in a unified framework . In order to deal with the new setting , we propose a new dimensionality reduction framework by learning a mapping function that projects data points in the original space to data points in a reduced space that directly form a graph . The main contributions of this paper are summarized as follows :
1 ) We present a new dimensionality reduction setting where data points form an explicit structure in a reduced space . This setting is not uncommon in many fields of science and of great advantage for analyses of intrinsic structures of data . 2 ) To deal with the new setting , we propose a new dimensionality reduction framework , where the intrinsic structure in a reduced space is represented by a graph and learned automatically from data by structure learning . Hence , our framework can obtain the mapping function and intrinsic graph structure simultaneously .
3 ) To learn a graph structure from data , dimensionality reduction with a tree is instantiated . We propose a new method by incorporating clustering information into a tree representation . A simple learning algorithm is designed with guaranteed convergence . Moreover , the proposed method satisfies all three constraints imposed by the new setting .
4 ) Extensive experiments are conducted on seven benchmark datasets to validate our proposed methods for clustering problems , and on three high dimensional real world datasets for learning their intrinsic structures . Experimental results demonstrate that the proposed method with discriminative information outperforms baselines in terms of clustering performance , and can correctly recover the intrinsic structures of the three real world datasets .
2 . DIMENSIONALITY REDUCTION VIA
GRAPH STRUCTURE LEARNING
2.1 Datasets with Intrinsic Structures
In many fields of science , experimental data resides in a high dimensional space . However , the distance between two data points may not directly reflect the distance measured on the intrinsic structure of data . Hence , it is desirable to uncover the intrinsic structure of data before conducting further analysis .
It has been demonstrated that many high dimensional datasets generated from real world problems contain special structures embedded in their intrinsic dimensional spaces . One example is a collection of teapot images viewed from different angles [ 33 ] . Each image contains 76 × 101 RGB pixels , so the pixel space has a dimensionality of 23 , 028 , but the intrinsic structure has only one degree of freedom : the angle of rotation . Figure 1 in [ 33 ] illustrates that distances computed by following an intrinsic curve ( or circle ) ( Figure 3 in [ 33 ] ) are more meaningful than distances computed in the original space . Another example is given in [ 29 ] where it is demonstrated that a collection of facial expression images ( 217× 308 RGB pixels ) contains a two layer hierarchical structure ( Figure 3(b ) in [ 29] ) . The images from three facial expressions of one subject are grouped together to form the first layer , while all images from three subjects form the second layer . In other words , images of one subject should be distant from images of all other subjects , but images from three expressions of the same subject should be close . Other examples with specific intrinsic structures are also discussed in [ 31 , 25 ] .
We are particularly interested in studying human cancer , a dynamic disease that develops over an extended time period . Once initiated from a normal cell , the advance to malignancy can to some extent be considered a Darwinian process a multistep evolutionary process that responds to selective pressure [ 15 ] . The disease progresses through a series of clonal expansions that result in tumor persistence and growth , and ultimately the ability to invade surrounding tissues and metastasize to distant organs . As shown in Figure 1 , the evolution trajectories inherent to cancer progression are complex and branching [ 15 ] . Due to the obvious necessity for timely treatment , it is not typically feasible to collect time series data to study human cancer progression [ 28 ] . However , as massive molecular profile data from excised tumor tissues ( static samples ) accumulates , it becomes possible to design integrative computation analyses that can approximate disease progression and provide insights into the molecular mechanisms of cancer . We have previously shown that it is indeed possible to derive evolutionary tra
766 local geometric information in the intrinsic space , reversed graph embedding is proposed to learn the mapping function and data points in the intrinsic space by solving the following optimization problem min fG∈F min
{z1,,zM }
( Vi,Vj )∈E bi,j||fG ( zi ) − fG ( zj )||2 .
( 1 )
As discussed in [ 22 ] , function fG possesses several appealing properties . Besides , the curse of dimensionality can be alleviated . Although the learned points {fG(zi)}N i=1 still reside in X , distances between any two of them are explicitly altered by comparing with the distances between two corresponding points of original data . When the number of features is larger than the intrinsic dimensionality , reversed graph embedding may significantly weaken the hubness effect , but other methods such as Isomap do not have this property [ 24 ] . Moreover , edge weights {bi,j} are not computed from data before learning ; instead , they are learned from data by using distances between data points in the intrinsic space , which will be further clarified in Section 3 , 2.3 Dimensionality Reduction via Learning a
Latent Graph
For a given graph G , reversed graph embedding provides a proper way to model the latent graph structure of a given dataset . As discussed in Section 2.1 , the intrinsic structure represented by a graph is generally unknown , and the data points used to construct a graph in a low dimensional space are also unknown . In order to explicitly obtain a graph in an intrinsic dimensionality , we have to learn the graph and its associated latent points in an intrinsic space . Given a dataset D = {xi}N i=1 , we formulate the following optimization problem min
G∈Gb min fG∈F min
{z1,,zN }
||xi − fG ( zi)||2
( 2 ) bi,j||fG ( zi ) − fG ( zj )||2 ,
N i=1 λ 2
+
( Vi,Vj )∈E tion of reverse graph embedding , and Gb is a feasible set where λ ≥ 0 is a parameter that controls the tradeoff between the data reconstruction error and the objective funcof graphs with the set V of vertices and a set E of edges specified by a set {bi,j} of edge weights .
Our formulation ( 2 ) is different from that used in regularized principal graph ( RPG ) [ 22 ] , even though reversed graph embedding is used as the same criterion for learning a latent graph structure . First , our goal is to perform dimensionality reduction , while RPG is to learn principal points in the original space . This can be clearly seen in the two loss functions they optimize : problem ( 2 ) minimizes a regularized data reconstruction error , while RPG minimizes a regularized empirical quantization error . Second , the interpretation of the latent points {zi}N i=1 are different . In contrast to RPG , the latent point zi is a transformed point in a low dimensional space which cannot be interpreted as the center of K means in the original space . Third , we intentionally treat fG and {zi}N i=1 as separate variables to optimize . In this case , we can control the reduced dimensionality of the intrinsic space where the intrinsic structure may reside .
Figure 1 : Branching architecture of cancer evolution . Selective pressures allow some tumor clones to expand while others become extinct or remain dormant . The figure is modified from Figure 2 in [ 15 ] . jectories from static molecular data , and that breast cancer progression can be represented by a high dimensional manifold with multiple branches [ 30 ] .
These concrete examples convince us that many datasets in a high dimensional space can be represented by a certain intrinsic structure in a low dimensional space . Existing dimensionality reduction methods exploit the intrinsic structure of data implicitly either by learning a parametric mapping function or approximating a manifold via local geometric information as discussed in Section 1 . However , these methods do not aim to directly learn a function that maps data points in a high dimensional space to the data points that explicitly form an intrinsic structure in a lowdimensional space . In contrast , our new method of dimensionality reduction is designed to express the intrinsic structure of data by an explicit representation in a reduced space . As such , we can obtain a mapping function for dimensionality reduction and intrinsic structure of data simultaneously . 2.2 Reversed Graph Embedding
A natural choice to explicitly represent an intrinsic structure of data is an undirected graph . Given a graph , one possible way to learn a graph structure from data is to use reversed graph embedding [ 22 ] . However , reversed graph embedding was proposed to learn a set of principal points in the original space . In our new dimensionality reduction setting , we aim to learn a set of points and an intrinsic structure , both of which reside in a reduced space . Let G = ( V,E ) be an undirected graph , where V = {V1 , . . . , VN} is a set of vertices and E is a set of edges . Suppose that every vertex Vi corresponds to a point zi ∈ Z ⊂ Rd , which lies on a structure with an intrinsic dimension d . Let X ⊂ RD be the input space and D = {xi}N i=1 ⊂ X be a given dataset . We consider learning a function fG ∈ F and fG : Z → X over G that maps the intrinsic space Z to the input space X . Here , we impose a constraint d ≤ D . Given a graph G , denote as bi,j the weight of edge ( Vi , Vj ) , where bi,j represents the similarity ( or connection indicator ) between zi and zj in the intrinsic space Z . Intuitively , if zi and zj are neighbors on G with a high degree of similarity , fG(zi ) and fG(zj ) are also close to each other . To capture the
Selection Pressure Normal cell Metastases Clone Dormant Progression path Genetic mutation genetically distinct clone genetic mutation driving clonal expansion 767 3 . DIMENSIONALITY REDUCTION VIA
LEARNING A TREE
Formulation ( 2 ) is a general framework for dimensionality reduction by learning an intrinsic graph structure in a lowdimensional space . In order to instantiate a new method , we have to specify two variables : a feasible set Gb of graphs and the mapping function fG . 3.1 Minimum Spanning Tree
To avoid the difficulty of learning a general graph , we investigate a family of tree structures , which can be used to deal with most of the above mentioned problems . Given a connected undirected graph G = ( V,E ) with a cost di,j associated with edge ( Vi , Vj ) ∈ E,∀i,∀j , let T = ( V,ET ) be a tree with the minimum total cost and ET be the edges forming a tree . In order to represent and learn a tree , we consider {bi,j} as binary variables where bi,j = 1 if ( Vi , Vj ) ∈ ET , and 0 otherwise . Denote B = [ bi,j ] ∈ {0 , 1}N×N . The integer linear programming formulation of minimum spanning tree ( MST ) can be written as : min B∈B0 bi,jdi,j ,
2
2 i,j bi,j = |V| − 1} ∩ { 1 where B0 = {B ∈ {0 , 1}N×N} ∩ B and B = {B = BT} ∩ Vi∈S,Vj∈S bi,j ≤ |S| − 1,∀S ⊆ { 1 V} . The first constraint of B enforce the symmetric connection of undirected graph , eg bi,j = bj,i . The second constraint states that the spanning tree only contains |V|−1 edges . The third constraint imposes the acyclicity and connectivity properties of a tree . It is difficult to solve an integer programming problem optimally . Instead , we resort to a relaxed problem by letting bi,j ≥ 0 , that is , i,j bi,j di,j ,
( 3 ) i,j min B∈B i,j
N i=1 where the set of linear constraints is given by B = {B ≥ 0}∩ B . Problem ( 3 ) can be solved by Kruskal ’s algorithm [ 9 ] . 3.2 Linear Projection via Learning a Tree To specify the mapping function fG , a linear projection model is employed , ie , fG(z ) = Wz where W ∈ RD×d and WT W = I . By incorporating the feasible set of trees , we have the following specific formulation instantiated from ( 2 ) : min
W,Z,B
||xi − Wzi||2 +
λ 2 bi,j||Wzi − Wzj||2
( 4 ) st WT W = I , B ∈ B where W = [ w1 , . . . , wd ] ∈ RD×d is an orthogonal set of d linear basis vectors wl ∈ RD,∀l , Z = [ z1 , . . . , zN ] ∈ Rd×N is represented by the projected data points of D in the lowdimensional space Rd , B = [ bi,j ] ∈ RN×N is an adjacent matrix of a tree T = ( V,ET ) where ET = {(i , j ) : bi,j = 0} . If λ = 0 , problem ( 4 ) is equivalent to the optimization problem of PCA , otherwise the data points {xi} are mapped into a low dimensional space where {zi}N i=1 form a tree . Therefore , PCA is a special case of problem ( 4 ) . Another important observation is ||Wzi − Wzj||2 = ||zi − zj||2 due to the orthogonal constraint . Since latent points {zi}N i=1 are in Rd , the distances between any two latent points are computed in a low dimensional space . As a result , problem ( 4 ) can effectively mitigate the curse of dimensionality .
3.3 Alternating Structure Optimization
We propose to solve problem ( 4 ) by alternating structure optimization , which has been successfully applied in multiple task learning to solve a similar optimization problem [ 1 ] . To this end , we divide variables of problem ( 4 ) into two groups {B} and {W , Z} , and solve one group by fixing the other group alternatively until convergence . Given a tree encoded in B ∈ B , we can solve W and Z analytically . Let X = [ x1 , . . . , xN ] ∈ RD×N . After simple matrix manipulation , problem ( 4 ) with respect to W and Z can be rewritten as the following optimization problem
||X − WZ||2 + λtrace(ZLZT ) : WT W = I ,
( 5 ) min W,Z where L = diag(B1)− B is a Laplacian matrix . Setting the first derivative of the objective function of ( 5 ) with respect to Z to zero yields
Z = WT X(I + λL)−1 .
( 6 )
By substituting Z of ( 6 ) into problem ( 5 ) , we have the following equivalent optimization problem trace(XXT ) − trace(WT X(I + λL)−1XT W ) : WT W = I , min W which is equivalent to the following maximization problem trace(WT X(I + λL)−1XT W ) : WT W = I .
( 7 ) max W
Let C = X(I + λL)−1XT . The optimal solution of W is the concatenation of the d eigenvectors corresponding to the largest d eigenvalues of C . This is similar to the optimization problem of PCA applied to the newly transformed data X(I+λL)−1/2 . This further clarifies the equivalence of PCA to problem ( 4 ) if λ = 0 as discussed in Section 32
Given W and Z , we obtain the optimal tree by solving the following linear programming optimization problem i,j min B∈B bi,j||zi − zj||2 , which is a minimum spanning tree problem and can be optimally solved by Kruskal ’s method according to Section 31 Since problem ( 4 ) is non convex , there might be many local optimal solutions . For simplicity , we initialize Z to be the solution of the induced problem , minW,Z ||X − WZ||2 : WT W = I , which is equivalent to initializing Z as the lowdimensional points obtained by PCA . The pseudo code of our method of dimensionality reduction via learning a spanning tree is given in Algorithm 1 . For ease of reference , we named Algorithm 1 as Dimensionality Reduction Tree ( shortly DRTree ) . The theoretical convergence analysis of Algorithm 1 is presented in the following theorem .
Theorem 1 . Let {W , Z , B} be the solution of Problem ( 4 ) in the th iteration , and g = g(W , Z , B ) be the corresponding objective function value , then we have :
( 1 ) {g} is monotonically decreasing ; ( 2 ) Sequences {W , Z , B} and {g} converge . Proof . Let {W , Z , B} be a solution obtained in the th iteration . By Algorithm 1 , at the ( + 1)th iteration , since each subproblem is solved exactly , we have g(W , Z , B ) ≥ g(W , Z , B+1 ) ≥ g(W+1 , Z , B+1 )
≥ g(W+1 , Z+1 , B+1 ) .
768 Algorithm 1 Dimensionality Reduction Tree 1 : Input : Data matrix X , parameter λ 2 : Initialize Z by PCA 3 : repeat 4 : 5 : 6 : 7 : C = X(I + λL)−1XT 8 : dk.k = ||zk − zk||2 , ∀k , ∀k Obtain B by solving ( 3 ) via Kruskal ’s algorithm L = diag(B1 ) − B
Perform eigen decomposition on C such that C = UΛUT and diag(Λ ) is sorted in a descending order Z = WT X(I + λL)−1
9 : W = U( : , 1 : d ) 10 : 11 : until Convergence
So sequence {g} is monotonically decreasing . Furthermore , function g(W , Z , B ) is lower bounded by 0 , and then by Monotone Convergence Theorem , there exists g∗ ≥ 0 , such that {g} converges to g∗ . Next , we prove that the sequence {W , Z , B} generated by Algorithm 1 also converges . Due to the compactness of feasible sets W and B , we have the sequence {W , B} converges to {W∗ , B∗} as → ∞ . Since Z = WT X(I + λL)−1 , {Z} converges to Z∗ = ( W∗)T X(I + λL)−1 .
4 . DISCRIMINATIVE DIMENSIONALITY REDUCTION VIA LEARNING A TREE
4.1 Discriminative Projection
DRTree projects data points in a high dimensional space to latent points that directly form a tree structure in the low dimensional space , and the projection matrix has incorporated the local geometric information through a Laplacian matrix constructed from the learned tree . However , the tree structure achieved might be at the risk of losing clustering information . In other words , some data points are supposed to form a cluster , but they are scattered to different branches of the tree , and distances between them on the intrinsic structure become large . To incorporate the discriminative information , we introduce another set of lai=1 where yk ∈ Rd tent points {yk}K so as to minimize the tradeoff between the objective functions of K means and DRtree . As a result , we formulate the following optimization problem k=1 as the centers of {zi}N min
W,Z,B,Y,S
||xi − Wzi||2 + bk,k||Wyk − Wyk||2 k,k
λ 2
N i=1
+ γ
K k=1
Vj∈Sk
||zj − yk||2 : WT W = I , B ∈ B ( 8 ) i=1} and {yk}K where the third term of the objective function is the objective function of K means , S = {S1 , . . . ,SK} is a partition of {V1 , . . . , VN} , and γ ≥ 0 is a tradeoff parameter between the objective function of DRTree and empirical quantization error of latent points {zN Note that inversed graph embedding is now regularized on centers {yk}K i=1 , which is different from problem ( 4 ) . However , problem ( 4 ) is a special case of problem ( 8 ) if K = N and γ → ∞ since the third term can be removed without changing the optimal solution of ( 8 ) due to zi = yi,∀i at optimum . Therefore , problems ( 4 ) and ( 8 ) are exactly the same . Except for the special case , problem ( 8 ) is able to achieve discriminative and compact feature k=1 instead of {zi}N k=1 . min
W,Z,B,Y,R
N i=1
+ γ
||xi − Wzi||2 +
K
N k=1 i=1 st WT W = I , B ∈ B , k,k
λ 2
K k=1 bk,k||Wyk − Wyk||2 ri,k||zi − yk||2 + σΩ(R )
( 9 ) ri,k = 1 , ri,k ≥ 0 , ∀i , ∀k , representation for dimensionality reduction since clustering objective and DRTree are optimized in a unified framework . The hard partition imposed by K means , however , has several drawbacks . First , parameter K is data dependent , so it is hard to set properly . Second , it is sensitive to noise , outliers , or some data points that cannot be thought of as belonging to a single cluster [ 13 ] . Soft partition methods such as Gaussian mixture modeling have also been used in modeling principal curves [ 4 , 32 ] . However , the likelihood of a Gaussian mixture model tends to be infinite when a singleton is formed [ 32 ] . To alleviate the problems from which the aforementioned methods suffer , we propose to replace the hard partition K means with a relaxed regularized empirical quantization error given by
N
K i=1 where R ∈ RN×K with the ( i , k)th entry as ri,k , Ω(R ) = k=1 ri,k log ri,k is the negative entropy regularization , and σ > 0 is the regularization parameter . The negative entropy regularization transforms hard assignment used in K means to soft assignment used in Gaussian mixture models [ 22 ] , and is also used in other tasks [ 34 , 21 ] . The following proposition shows that problem ( 9 ) with respect to {Y , R} by fixing the remaining variables is equivalent to mean shift clustering method [ 8 ] , which is able to determine the number of clusters automatically and initialize centers {yi} by latent points {zi},∀i .
Proposition 1 . Given {W , Z , B} , λ = 0 and assuming K = N , problem ( 9 ) with respect to {Y , R} can be solved by a mean shift clustering method by initializing Y = Z . i=1 i=1 k=1 obtained by the optimal solution ( 12 ) and the simplex constraint of ( 11 ) . The optimization problem now becomes unconstrained optimization problem
Proof . According to Proposition 4 , we have an analytical solution of R shown in ( 12 ) . By substituting ( 12 ) back into the objective function of ( 11 ) , we have following derivak=1 ri,k log ri,k = . The equality is tionsK −σN i=1 ri,k||zi−yk||2+σN N K k=1 exp,−||zi − yk||2/σlogK
N ie , ∀k,∇yk = 2N i=1 ri,kzi/N of yk as N
According to first order optimal condition [ 5 ] , we can obtain the optimal solution by letting the first derivative be zero , i=1(yk − zi)ri,k = 0 . By solving the optimal condition problem , we have the optimal solution i=1 ri,k , which is equivalent to the update rule used in mean shift [ 8 ] if we consider the kernel function as a Gaussian distribution with bandwidth σ . exp,−||zi − yk||2/σ
K min
Y
−σ log k=1 i=1
.
The key difference between problem ( 9 ) and the traditional mean shift is that the latent points {zi}N i=1 in our model are variables and can be affected by dimensionality
769 reduction and tree structure learning . We also build a connection between problem ( 9 ) and problem ( 8 ) as shown in the following proposition .
Proposition 2 . If σ → 0 , problem ( 9 ) is equivalent to problem ( 8 ) .
I + R
1+γ
γ
λ
γ L + Γ
− RT R
−1
RT
, U where Q = 1 1+γ and diag(Λ ) are the eigenvectors and eigenvalues of matrix XQXT with diag(Λ ) sorted in a descending order , respectively , Γ = diag(1T R ) and the Laplacian matrix over a tree encoded in B is defined as L = diag(B1 ) − B .
Proof . The optimal solution ( 12 ) is a softmin function with respect to distance ||zi − yk||2 . If σ → 0 , ri,k = 1 if N k = mink=1,,K ||zi − yk||2 , and otherwise ri,k = 0 . In the i=1 ri,k||zi − yk||2 = case of σ → 0 , we have limσ→0 ||zi − yk||2 and the negative entropy is equal
K
K k=1 k=1 i∈Sk to zero . This completes the proof .
The above properties of problem ( 9 ) facilitates the setting of parameters in different contexts of applications . In the case of dimensionality reduction , discriminative information might be important for some applications such as clustering problems , and Proposition 1 provides a natural way to form a cluster without predefining the number of clusters . In the case of finding K clusters , we prefer problem ( 8 ) to ( 9 ) since ( 8 ) is formulated in terms of K clusters directly . According to Proposition 2 , the purpose of clustering can also be achieved by solving problem ( 9 ) with a small σ . 4.2 Discriminative DRTree Alternating structure optimization is used to solve problem ( 9 ) . We first partition variables into two groups {W , Z , Y} and {B , R} , and then solve each subproblem iteratively until the convergence is achieved . Given {B , R} , we can obtain an analytical solution by solving problem ( 9 ) with respect to {W , Z , Y} , which is detailed in Proposition 3 . Before presenting Proposition 3 , we first state a necessary condition of the proposition in Lemma 1 by proving the existence of the inverse matrix of 1+γ γ ( λ
γ L + Γ ) − RT R . exists ifN
γ ( λ
Lemma 1 . The inverse of matrix 1+γ
γ L + Γ ) − RT R i=1 ri,k > 0,∀k , where Γ = diag(1T R ) and Laplacian matrix over a tree encoded in B is L = diag(B1 ) − B . Proof . To prove the existence of the inverse of matrix γ L + Γ ) − RT R , we prove that this matrix is positive γ ( λ definite . Given any non zero vector v ∈ RK , we have the 1 + γ following derivations
1+γ v ≥ 1 + γ vT Γv − vT RT Rv
L + Γ ) − RT R vT
1 + γ
γ vT Γv + vT ( diag(RT R1 ) − RT R)v − vT diag(RT R1)v
γ
(
λ γ
γ vT Γv + vT Γv − vT diag(RT 1)v = vT Γv
1 γ where the first and second inequalities follow the fact that i=1 ri,k > 0,∀k , the matrix 1+γ the Laplacian matrix is positive semi definite . IfN The conditionsN
γ L+Γ)−RT R is positive definite . γ ( λ i=1 ri,k > 0,∀k , always hold in the case of the soft assignment obtained by Proposition 1 .
Proposition 3 . By fixing {B , R} , problem ( 9 ) with re spect to {W , Z , Y} has the following analytical solution : −1
W = U( : , 1 : d ) , Z = WT XQ , Y = ZR
L + Γ
( 10 )
λ
γ
=
≥ 1 γ
Proof . With simple matrix manipulation , problem ( 9 ) with respect to {W , Z , Y} by fixing {B , R} can be written as trace(ZZT ) − 2 trace(RT ZT Y ) + trace(YΓYT ) min
W,Z,Y
γ + ||X − WZ||2
F + λtrace(YLYT ) : WT W = I , where Γ = diag(1T R ) and the Laplacian matrix over a tree represented by B is L = diag(B1K ) − B . Let h(W , Z , Y ) be the objective function of the above optimization problem . By setting the partial derivative of h(W , Z , Y ) with respect to Y to zero ∂h(W , Z , Y)/∂Y = 2λYL−2γZR+2γYΓ = 0 , we have an analytical solution of Y given by
λ
γ
−1 λ
YZ = ZR
L + Γ
.
F + γtrace(ZZT )− γtrace
Substituting YZ into h(W , Z , Y ) yields h(W , Z , YZ ) = ||X− WZ||2 . Similarly , by setting the partial derivative of h(W , Z , YZ ) with respect to Z to zero , we obtain
γ L + Γ
RT ZT
ZR
−1
−1
λ
γ
−1 − RT R
ZW = WT X
( 1 + γ)I − γR
L + Γ
RT
.
According to the Woodbury formula [ 17 ] , the optimal solution of ZW can be further reformulated as YZ = ZRQ where Q = 1 1+γ
γ L + Γ
I + R
RT
γ
. Ac
1+γ
λ
−1 cording to Lemma 1 , the inverse matrix exists . The objective function can thus be further written as a function with respect only to W , which is given by h(W , ZW , YZW ) = trace(XXT )−trace(WT XQXT W ) . The optimization problem of h(W , Z(W ) , Y(Z(W) ) ) with respect to W is equivalent to the following maximization optimization problem , maxW trace(WT XQXT W ) : WT W = I , which is similar to problem ( 7 ) and can be solved optimally by eigendecomposition on XQXT .
Given {W , Z , Y} , problem ( 9 ) with respect to {B , R} is summarized as follows .
γ min B∈B,R ri,k||zi − yk||2 + σ
N K i=1 k=1
+
λ 2 k,k bk,k||Wyk − Wyk||2 : ri,k log ri,k
( 11 ) ri,k = 1 , ri,k ≥ 0 , ∀i , ∀k ,
N i=1
K K k=1 k=1 which is jointly convex optimization problem with respect to B and R . Importantly , the subproblems with respective to B and R can be solved independently .
The following proposition shows that the optimal solution
R has an analytical expression .
Proposition 4 . Problem ( 11 ) has the optimal solution
R given by the following analytical form , ∀k,∀i ri,k = exp,−||zi − yk||2/σ K exp,−||zi − yk||2/σ .
( 12 ) k=1
770 Algorithm 2 Discriminative DRTree 1 : Input : Data matrix X , parameters λ , σ and γ 2 : Initialize Z by PCA 3 : K = N , Y = Z 4 : repeat 5 : 6 : 7 : 8 : 9 : 10 : Q = 1 1+γ dk.k = ||yk − yk||2 , ∀k , ∀k Obtain B by solving ( 3 ) via Kruskal ’s algorithm L = diag(B1 ) − B Compute R with each element as ( 12 ) Γ = diag(1T R )
− RT R −1
1+γ
λ
γ L + Γ
I + R
γ
RT
( a ) DRTree+K means
11 : C = XQXT 12 :
Perform eigen decomposition on C such that C = UΛUT and diag(Λ ) is sorted in a descending order .
Z = WT XQ
13 : W = U( : , 1 : d ) 14 : 15 : Y = ZR( λ 16 : until Convergence
γ L + Γ)−1
Figure 2 : Results of convergence analysis of DRTree and DDRTree applied to USPS data . λ = N and d = 32 .
+ k=1 ri,k
,||zi − yk||2 + σ log ri,k
Proof . Each row ri of R in Problem ( 11 ) can be solved independently . By introducing dual variables α , we have the Lagrangian function defined for each subproblem with rek=1 ri,k − 1 ) . The KKT conditions can be obtained as k=1 ri,k = 1 , ri,k ≥ 0,∀k , which lead to the following analytic solution ri,k = exp(−||zi − yk||2/σ − ( 1 + α/σ) ) . According to the k=1 ri,k = 1 , we have exp(1 + α/σ ) = spect to ri as L(ri , α ) =K α(K ||zi − yk||2 + σ(1 + log ri,k ) + α = 0,∀k and K KKT condition K K k=1 exp(−||zi − yk||2/σ ) . This completes the proof . respect to B , minB∈B
To obtain the optima B , the optimization problem with k,k bi,j||yk − yk||2 , can again be solved by Kruskal ’s method .
Variable Z can be initialized by PCA as discussed in Section 32 By Proposition 1 , we can set K = N and initialize Y = Z . The pseudo code of Discriminative DRTree is given in Algorithm 2 , briefly named as DDRTree .
5 . EXPERIMENTS 5.1 Convergence and Sensitivity Analysis
By Proposition 2 , if σ goes to 0 , equation ( 12 ) approximates to hard assignment of K means . Moreover , a large value of γ enforces the decrease of the quantization error of K means . To achieve an approximate effect of K means and avoid the difficulty of tuning too many parameters , we fix σ = 0.001 and γ = 10 in all the following experiments .
( b ) DDRTree+K means
Figure 3 : Results of sensitivity analysis of two proposed methods performed on USPS data using different reduced dimensionality d and parameter λ .
We first perform an empirical convergence analysis of our proposed methods on the USPS data ( sample size N = 2007 and dimension D = 256)1 . We set λ = N and the dimensionality of the reduced space d = 32 that retains 95 % of total data variance ( or energy ) . Figure 2 shows that the objective values of DRTree and DDRTree converge monotonically in terms of the number of iterations . This result is consistent with our theoretical analysis given in Theorem 1 . We next perform a sensitivity analysis to study how our proposed methods behave with respect to parameter λ and reduced dimension d . To this end , we use two widely used clustering evaluation criteria , accuracy and normalized mutual information ( NMI ) , to assess the performance of our methods using different parameters . Although our methods take a different view of clustering from traditional clustering methods , they all can be used for solving a clustering problem . Traditional clustering methods assume that the centers of different clusters should be far away . In contrast , our methods assume that cluster centers should be treated differently based on their locations on an intrinsic graph structure : if two centers are connected , they should be close to each other ; otherwise , they should be distant from each other . To evaluate clustering performance , we first use DRTree and DDRTree to reduce data dimensionality and then apply K means to obtain clusters . It is well known that K means may lead to local minimum solutions . To alleviate the issue , we repeat K means with random initialization 20 times and the result with the smallest quantization error is reported . The number of clusters is set to be the true number of clusters .
Figure 3 reports the results of the sensitivity analysis of DRTree and DDRTree using different values of parameters : λ ∈ [ 10−2 , 102 ] × N , and d ∈ [ 13 , 16 , 22 , 32 , 48 ] corresponding to [ 80 % , 85 % , 90 % , 95 % , 98 % ] of energy retained in a reduced data space , respectively . We can see that for a fixed d , the accuracy and NMI scores do not change significantly with respect to varying λ values . On the other hand , the clustering performance of both methods are greatly affected 1http://wwwcsientuedutw/∼cjlin/libsvmtools/datasets/
0510152024252627282933132x 104The number of iterationsObjective function DRTreeDDRTree10−21001021316223248050550606507075λdAccuracy10−21001021316223248050550606507075λdNMI10−210010213162232480550606507075λdAccuracy10−21001021316223248050550606507075λdNMI771 Table 1 : Clustering results of seven datasets in terms of accuracy and NMI . N is the number of data points . c is the true number of clusters . D is the original dimensionality and d is the reduced dimensionality .
Dataset ( N , c ) ( D , d ) K means PCA+K means LLE+K means Laplacian+K means DRTree+K means DDRTree+K means K means PCA+K means LLE+K means Laplacian+K means DRTree+K means DDRTree+K means
Iris
Letter
( 150 , 3 )
( 5000 , 26 )
( 4,2 )
0.8867 0.8867 0.4733 0.5400 0.8600 0.8867 0.7364 0.7364 0.3592 0.2779 0.7118 0.7364
( 16,12 ) 0.2632 0.2634 0.0662 0.0672 0.3112 0.3178 0.3621 0.3591 0.0236 0.0214 0.4487 0.4359
Vehicle ( 846 , 4 ) ( 18,6 ) 0.3664 0.3676 0.2837 0.2884 0.4090 0.4208 0.1000 0.0997 0.0044 0.0075 0.1241 0.1337
Glass
( 214 , 6 )
Segment ( 2310 , 7 )
( 9,6 ) 0.4346 0.4346 0.3131 0.2570 0.4393 0.4626 0.3236 0.3264 0.0465 0.0422 0.3269 0.3536
( 18,7 ) 0.6658 0.6649 0.2827 0.1740 0.6706 0.6913 0.6115 0.6099 0.1184 0.0065 0.6163 0.6437
USPS
( 2007 , 10 ) ( 256,32 ) 0.6153 0.6208 0.1550 0.1570 0.6104 0.6667 0.5657 0.5664 0.0112 0.0091 0.6321 0.6687
Pendigits ( 3498 , 10 )
( 16,9 ) 0.6544 0.6527 0.1244 0.1289 0.6261 0.7459 0.6669 0.6627 0.0059 0.0060 0.6961 0.7702
Accuracy
NMI by varying d . This suggests that our method is not sensitive to the selection of λ values ; however , dimensionality reduction does significantly affect the clustering performance . Therefore , in the subsequent experiment , we fix λ = N and set d by keeping 95 % energy for clustering evaluation of our proposed methods for ease of parameter setting , although this setting may not yield the best clustering performance . 5.2 Clustering with Dimensionality Reduction We carry out an experiment on seven datasets taken from the UCI and Statlib repositories . Although there are many dimensionality reduction methods proposed in the literature , we are only interested in methods that are most closely related to our proposed methods . K means performed on the original data is used as the baseline . PCA+K means employs PCA to map data points in the original space to a reduced space that retains 95 % of energy and then performs K means on the data points in the reduced space . Two state of the art manifold learning based methods , LLE and Laplacian eigenmap , are also configured in the same way , named as LLE+K means and Laplacian+K means , respectively . We conduct the experiment by using drtoolbox2 that employs the normal neighborhood selection strategy to construct neighborhood graphs . For a fair comparison , all methods use the true number of clusters and the same reduced dimensionality for each dataset .
Table 1 reports the clustering results of six compared methods . The clustering results are assessed by the accuracy and NMI score criteria . We can clearly see that DDRTree+K means outperforms all other methods . Note that DRTree+K means achieves results comparable to Kmeans and PCA+K means , but it performs worse on Iris and Pendigits . In contrast , DDRTree+K means achieves the best performance over all datasets . This suggests that DDRTree can obtain the discriminative feature representation for clustering problems . On the other hand , DDRTree+Kmeans performs much better than K means and PCA+Kmeans . This suggests that the projection space can be better learned by explicitly obtaining the intrinsic structure of data . We also see that manifold learning based approaches ( ie , LLE+K means and Laplacian+K means ) perform the worst among all compared methods . This is due to that fact that the neighborhood graph constructed in the original space is not reliable . Our results suggest that DDRTree
2http://lvdmaatengithubio/drtoolbox/
( a ) curve
( b ) adjacency matrix
Figure 4 : Experimental results of DDRTree applied to Teapot images . ( a ) principal curve generated by DRTree . Each dot represents one teapot image . Images following the principal curve are plotted at intervals of 30 for visualization . ( b ) The adjacency matrix of the curve follows the ordering of the 400 consecutive teapot images with 360◦ rotation . can learn a low dimensional discriminative feature representation for clustering problems and the local geometry constructed from the learned graph is helpful . 5.3 Learning a Latent Structure from Data
We next investigate the ability of our methods to automatically discover latent structures from various real world datasets . The latent structures considered in this paper include principal curves , hierarchical tree structures , and a cancer progression path , as described in Section 21
531 Principal Curve A collection of 400 teapot images from [ 33 ] are used3 . These images were taken successively as a teapot was rotated 360◦ . Our goal is to construct a principal curve that organizes the 400 images . Each image consists of 76 × 101 pixels and is represented as a vector . The data in each dimension is normalized to have zero mean and unit standard deviation . Similar to [ 29 ] , a kernel matrix X is generated where X(i , j ) = exp(−||xi − xj||2/D ) . We run our proposed DDRTree method using the kernel matrix as the input . We set λ = 0.1 × N and d = 36 that keeps 95 % of total energy . The experimental results of DDRTree are shown in Figure 4 . The principal curve ( Figure 4(a ) ) is shown in terms of the first 3 columns of the learned 3http://wwwccgatechedu/∼lsong/data/teapotdatazip
050100150200250300350400050100150200250300350400consecutive images with 360° rotation772 ( a ) tree
( b ) adjacency matrix
Figure 5 : Experimental results of our DDRTree method performed on facial expression images . ( a ) A hierarchical tree generated by DRTree . Each dot represents one face image . Images of three types of facial expressions from three subjects are plotted for visualization . The black circle is the root of the hierarchical structure ; ( b ) The adjacency matrix of the tree on nine blocks indicates that each block corresponds to one facial expression of one subject . projection matrix W as the coordinates where each dot yi represents the ith image and the sampled images at intervals of 30 are plotted for the purpose of visualization . Figure 4(b ) shows the linear chain dependency among teapot images following the consecutive rotation process . We can see that the curve generated by our method is consistent with the rotating process of the 400 consecutive teapot images .
A similar result is also recovered by CLUHSIC , which assumes that the label kernel matrix is a ring structure [ 29 ] . However , there are three main differences . First , we learn a projection space where images are arranged in the form of a principal curve , while CLUHSIC applies KPCA to transform original data to an orthogonal space where clustering is performed . Second , the principal curve generated by our DDRTree method is much smoother than that obtained by CLUHSIC ( see Figure 4 in [ 29] ) . Third , our method learns the adjacency matrix from the given dataset , but CLUHSIC requires a label matrix as a priori . A two dimensional representation of the same set of teapot images is given in [ 33 ] , where Maximum Variance Unfolding ( MVU ) is used that arranges the images in a circle ( see Figure 3 in [ 33] ) . We attempted to run MVU by keeping 95 % energy , ie , d = 36 . However , storage allocation fails due to the large memory requirement of solving a semi definite programming problem in MVU . Hence , MVU cannot be applied to learn a relatively large intrinsic dimensionality . However , our method does not have this issue .
532 Hierarchical Tree Facial expression data4 is used for hierarchical clustering , which takes into account both the identities of individuals and the emotion being expressed [ 29 ] . This data contains 185 face images ( 308 × 217 RGB pixels ) with three types of facial expressions ( NE : neutral , HA : happy , SO : shock ) taken from three subjects ( CH , AR , LE ) in an alternating order , with around 20 repetitions each . Eyes of these facial images have been aligned , and the average pixel intensities have been adjusted . As with the teapot data , each image is
4http://wwwccgatechedu/∼lsong/data/facedatazip
Figure 6 : Graph structure learned by DDRTree on breast cancer dataset with d = 80 and visualized in three dimensional space spanned by the first three components of the learned projection matrix . represented as a vector , and is normalized in each dimension to have zero mean and unit standard deviation . A kernel matrix is used as the input to DDRTree . λ = 0.1 × N and d = 185 . The experimental results are shown in Figure 5 . We can clearly see that three subjects are connected through different branches of a tree . If we take the black circle in Figure 5(a ) as the root of a hierarchy , the tree forms a two level hierarchical structure . As shown in Figure 5(b ) , all three facial expressions from three subjects are also clearly separated . A similar two level hierarchy is also recovered by CLUHSIC ( Figure 3(b ) in [ 29] ) . However , the advantages of using DDRTree discussed above for teapot images are also applied here . In addition , we can observe more detailed information from the tree structure . For example , LE@SO is the junction to other two subjects , ie , AR@SO and CH@SO , which can be observed from the 9th row of the adjacency matrix 5(b ) . This observation suggests that the shock is the most similar facial expression among three subjects . However , CLUHSIC is not able to obtain this information .
533 Cancer Progression Path We interrogate a large scale , publicly available breast cancer dataset [ 10 ] for cancer progression modeling . The dataset contains the expression levels of over 25 , 000 gene transcripts obtained from 144 normal breast tissue samples and 1 , 989 tumor tissue samples . By using a non linear regression method , a total of 359 genes were identified that may play a role in cancer development [ 30 ] . In the analysis , we set λ = 5 × N and d = 80 that retains 90 % of energy .
Figure 6 shows the learned latent structures and latent points in a reduced dimensional space . Each tumor sample is colored with its corresponding PAM50 subtype label , a molecular approximation that uses a 50 gene signature to group breast tumors into five subtypes including normallike , luminal A , luminal B , HER2+ and basal [ 23 ] . Basal and HER2+ subtypes are known to be the most aggressive breast tumor types . The learned graph structure in the lowdimensional space suggests a linear bifurcating progression path , starting from the normal tissue samples , and diverging to either luminal A or basal subtypes . The linear trajectory through luminal A continues to luminal B and to the HER2+ subtype . Significant side branches are evident
020406080100120140160180020406080100120140160180CH@HACH@SOCH@NEAR@HAAR@NELE@NELE@HALE@SOAR@SO−10−505101520−10−50510−50510 BasalHer2+luminal Aluminal Bnormal−likenormaltree structure773 for both luminal A and luminal B subtypes , suggesting that these subtypes can be further delineated . The revealed data structure is consistent with the proposed branching architecture of cancer progression shown in Figure 1 .
6 . CONCLUSIONS
In this paper , we proposed a general framework for dimensionality reduction , where the projected data points in a low dimensional space are used directly to form a graph . As a special case , we developed a new method of dimensionality reduction that learns a latent tree structure and low dimensional feature representation simultaneously . We extended the proposed method for clustering problems by imposing the constraint that data points belonging to the same cluster are likely to be close along the learned tree structure . The experimental results demonstrated the effectiveness of the proposed methods for recovering intrinsic structures from real world datasets . Dimensionality reduction via learning a graph is formulated from a general graph , so the development of new dimensionality reduction methods for other specific structure is also possible .
Acknowledgments This work is supported in part by the fund from NSF ( ABI1322212 ) , NIH ( 1R01DE024523 01 ) and SUNY Research Foundation .
7 . REFERENCES [ 1 ] R . K . Ando and T . Zhang . A framework for learning predictive structures from multiple tasks and unlabeled data . JMLR , 6:1817–1853 , 2005 .
[ 2 ] M . Belkin and P . Niyogi . Laplacian eigenmaps and spectral techniques for embedding and clustering . In NIPS , 2001 .
[ 3 ] K . Beyer , J . Goldstein , R . Ramakrishnan , and
U . Shaft . When is “ nearest neighbor ” meaningful ? In Database Theory ICDT’99 , 1999 .
[ 4 ] C . M . Bishop , M . Svens´en , and C . K . I . Williams .
GTM : the generative topographic mapping . Neural Comput , 10(1):215–234 , 1998 .
[ 5 ] S . Boyd and L . Vandenberghe . Conex Optimization .
Cambridge University Press , 2004 .
[ 6 ] C . J . C . Burges . Dimension reduction : a guided tour .
FTML , 2(4):275–365 , 2009 .
[ 7 ] L . Cayton . Algorithms for manifold learning .
Technical report , UCSD , 2005 .
[ 8 ] Y . Cheng . Mean shift , mode seeking , and clustering .
IEEE T PAMI , 17(8):790–799 , 1995 .
[ 9 ] M . Cheung . Minimum cost spanning trees . http://peopleoriecornelledu/dpw/orie6300 /fall2008/Recitations/rec09pdf
[ 10 ] C . Curtis , S . P . Shah , S . Chin , et al . The genomic and transcriptomic architecture of 2,000 breast tumours reveals novel subgroups . Nature , 486(7403):346–352 , 2012 .
[ 11 ] C . Ding and X . He . K means clustering via principal component analysis . In ICML , 2004 .
[ 12 ] E . Erwin , K . Obermayer , and K . Schulten .
Self organizing maps : ordering , convergence properties and energy functions . Biol Cybern , 67:47–55 , 1992 .
[ 13 ] M . Filippone , F . Camastra , F . Masulli , and
S . Rovetta . A survey of kernel and spectral methods for clustering . Pattern Recogn , 41:176–190 , 2008 .
[ 14 ] A . Gorban , B . K´egl , D . Wunsch , and A . Zinovyev .
Principal Manifolds for Data Visualisation and Dimension Reduction , volume 58 . Springer , 2007 . [ 15 ] M . Greaves and C . C . Maley . Clonal evolution in cancer . Nature , 481(7381):306–313 , 2012 .
[ 16 ] T . Hastie and W . Stuetzle . Principal curves . JASA ,
84:502–516 , 1989 .
[ 17 ] R . A . Horn and C . R . Johnson . Matrix Analysis .
Cambridge University Press , 2012 .
[ 18 ] J . T . Jolliffe . Principal Component Analysis .
Springer Verlag , Berlin , 1986 .
[ 19 ] B . K´egl and A . Kryzak . Piecewise linear skeletonization using principal curves . IEEE T PAMI , 24(1):59–74 , 2002 .
[ 20 ] T . Kohonen . Self organizing Maps . Springer , 1997 . [ 21 ] Q . Mao , I . Tsang , S . Gao , and L . Wang . Generalized multiple kernel learning with data dependent priors . IEEE T NNLS , 29(6):1134–1148 , 2015 .
[ 22 ] Q . Mao , L . Yang , L . Wang , S . Goodison , and Y . Sun .
SimplePPT : A simple principal tree algorithm . In SDM , 2015 .
[ 23 ] J . Parker , M . Mullins , M . Cheang , et al . Supervised risk predictor of breast cancer based on intrinsic subtypes . J Clin Oncol , 27(8):1160–1167 , 2009 .
[ 24 ] M . Radovanovi´c , A . Nanopoulos , and M . Ivanovi´c .
Hubs in space : popular nearest neighbors in high dimensional data . JMLR , 11:2487–2531 , 2010 .
[ 25 ] S . T . Roweis and L . K . Saul . Nonlinear dimensionality reduction by locally linear embedding . Science , 290(5500):2323–2326 , 2000 .
[ 26 ] L . Saul and S . Roweis . Think globally , fit locally : unsupervised learning of low dimensional mainfolds . JMLR , 4:119–155 , 2003 .
[ 27 ] B . Sch¨olkopf , A . Smola , and K . Muller . Kernel principal component analysis . Advances in Kernel Methods Support Vector Learning , pages 327–352 , 1999 .
[ 28 ] A . J . Smola , S . Mika , B . Sch¨olkopf , and R . C .
Williamson . Regularized principal manifolds . JMLR , 1:179–209 , 2001 .
[ 29 ] L . Song , A . Smola , A . Gretton , and K . Borgwardt . A dependence maximization view of clustering . In ICML , 2007 .
[ 30 ] Y . Sun , J . Yao , N . Nowak , and S . Goodison . Cancer progression modeling using static sample data . Genome Biol , 15(8):440 , 2014 .
[ 31 ] J . B . Tenenbaum , V . deSilva , and J . C . Landford . A global geometric framework for nonlinear dimensionality reduction . Science , 290:2319–2323 , 2000 .
[ 32 ] R . Tibshirani . Principal curves revisited . Stat Comput ,
2:183–190 , 1992 .
[ 33 ] K . Q . Weinberger and L . K . Saul . An introduction to nonlinear dimensionality reduction by maximum variance unfolding . In AAAI , 2006 .
[ 34 ] J . Yao , Q . Mao , S . Goodison , V . Mai , and Y . Sun . Feature selection for unsupervised learning through local learning . Pattern Recogn Lett , 53:100–107 , 2015 .
774
