Heterogeneous Network Embedding via Deep
Architectures
Shiyu Chang1 , Wei Han1 , Jiliang Tang2 ,
Guo Jun Qi3 , Charu C . Aggarwal4 , Thomas S . Huang1
1 Beckman Institute , University of Illinois at Urbana Champaign , IL 61801 .
2 Computer Science and Engineering , Arizona State University , Tempe , AZ 85281 .
3 University of Central Florida , Orlando , FL , 32816 .
4 IBM TJ Watson Research Center , NY , 10598 .
{chang87 , weihan3 , t huang1}@illinois.edu , jiliangtang@asuedu , guojunqi@ucfedu charu@usibmcom
ABSTRACT Data embedding is used in many machine learning applications to create low dimensional feature representations , which preserves the structure of data points in their original space . In this paper , we examine the scenario of a heterogeneous network with nodes and content of various types . Such networks are notoriously difficult to mine because of the bewildering combination of heterogeneous contents and structures . The creation of a multidimensional embedding of such data opens the door to the use of a wide variety of off the shelf mining techniques for multidimensional data . Despite the importance of this problem , limited efforts have been made on embedding a network of scalable , dynamic and heterogeneous data . In such cases , both the content and linkage structure provide important cues for creating a unified feature representation of the underlying network . In this paper , we design a deep embedding algorithm for networked data . A highly nonlinear multilayered embedding function is used to capture the complex interactions between the heterogeneous data in a network . Our goal is to create a multi resolution deep embedding function , that reflects both the local and global network structures , and makes the resulting embedding useful for a variety of data mining tasks . In particular , we demonstrate that the rich content and linkage information in a heterogeneous network can be captured by such an approach , so that similarities among cross modal data can be measured directly in a common embedding space . Once this goal has been achieved , a wide variety of data mining problems can be solved by applying off the shelf algorithms designed for handling vector representations . Our experiments on real world network datasets show the effectiveness and scalability of the proposed algorithm as compared to the state of the art embedding methods .
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications–Data Mining .
Keywords Heterogeneous embedding , network embedding , feature learning , crossdomain knowledge propagation , deep learning , dimensionality reduction .
1 .
INTRODUCTION
Vectorized data representations frequently arise in many data mining applications . They are easier to handle since each data can be viewed as a point residing in an Euclidean space [ 24 , 30 ] . Thus , similarities between different data points can be directly measured by an appropriate metric to solve traditional tasks such as classification [ 37 ] , clustering [ 21 , 43 , 45 ] and retrieval [ 10 ] . As shown in [ 3 ] , learning good representations is one of the fundamental problems in data mining and Web search , and it often has a stronger impact on performance than designing a more sophisticated model . Unfortunately , many networked data sources ( eg Facebook , YouTube , Flickr and Twitter ) cannot be naturally represented as vectorized inputs . A combination of graphs and relational data is commonly used to represent these social networks and social media data . Current research has focused on either pre defined feature vectors [ 42 ] or sophisticated graph based algorithms [ 48 ] to solve the underlying tasks . A significant amount of research has been accomplished on various topics , such as collective classification [ 37 ] , community detection [ 44 ] , link prediction [ 47 ] , social recommendation [ 35 ] , targeted advertising [ 22 ] , and so on . The development of a unified representation for networked data in embedded vector form is of great importance to encode both content and links . The basic assumption is that , once the vectorized representation is obtained , the network mining tasks can be readily solved by offthe shelf machine learning algorithms .
Nevertheless , feature learning of networked data is not a trivial task because the data possesses many unique characteristics , such as its size , dynamic nature , noise and heterogeneity . First , the amount of multimedia data on social Websites has increased exponentially . The estimated number of photos on Facebook has reached 100 billion by mid 2011 and it has been continuously increasing by 350 million new uploaded photos each day1 . Given the varied backgrounds of the users , social media tends to be noisy . In addition , researchers have noticed that spammers generate more data than legitimate users [ 4 ] , which makes network mining even more difficult . Furthermore , social media data contains diverse and heterogeneous information . For instance , different model types are reported when an event takes place . As a Google search example of
1http://wwwbusinessinsidercom/ facebook 350 million photos each day 2013 9
119 Figure 1 : Illustration of the heterogeneity of different data sources describing the same topic “ MH 17 ” .
“ Malaysia Airlines MH 17 ” illustrates in Figure 1 , relevant results include not only text documents but also images and videos .
Moreover , social media data does not exist in isolation , but in combination with various data types [ 28 ] . These interactions can be formed either explicitly or implicitly with the linkages between them . Images and texts co occurring within the same Web page provide explicit linkages between them , whereas text to text linkages are formed by the hyper links between different Web documents . On the other hand , interactive activities by users can be viewed as implicit feedback , which connect different social media components . If users describe multiple images with similar tags , it is reasonable to assume that a semantic relationship exists between such images . It is evident that such huge amounts of data results in complex heterogeneous networks that impose tremendous challenges on learning uniform representations .
To address the aforementioned challenges , we present a novel idea on network representation learning , termed Heterogeneous Network Embedding ( HNE ) , which jointly considers both the content as well as the relational information . HNE maps different heterogeneous objects into a unified latent space so that objects from different spaces can be directly compared .
Unlike to traditional linear embedding models [ 29 , 44 , 50 ] , the proposed method decomposes the feature learning process into multiple nonlinear layers of a deep architecture . Both Deep Neural Networks ( DNN ) [ 1 , 14 ] and Convolutional Neural Networks ( CNN ) [ 12 , 18 ] are aggregated to handle vectorized data ( eg , text documents ) or tensor based multimedia objects ( eg , color images and videos ) . Our model coordinates two mutually reinforcing parts by iteratively solving an optimization problem with respect to feature learning and objective minimization . The deep architecture models both the local and global linkage structures of underlying networks through the proposed framework . This makes it more powerful than a shallow embedding solution to capture the network links , especially when the linkage information plays the key role in revealing semantic correlations between different objects due to the homophily property . Along this line of research , we utilize the network ’s inter connections to design a proper loss function that enforces the similarities between different objects in the embedded feature space such that they are consistent with network links . The idea of a network preserved embedding is illustrated in figure 2 . The key advantages of the proposed HNE framework are the following : ‚ Robust : HNE explores global consistency between different heterogeneous objects to learn unified feature representations guided by network structures .
‚ Unsupervised : HNE is unsupervised and task independent , which makes it suitable for many network orientated data mining applications .
‚ Out of sample : HNE is able to handle the out of sample problem . This addresses the challenge associated with a dynamic network in which new nodes may be added over time .
2 . RELATED WORK 2.1 Network Embedding
A branch of latent feature embeddings is motivated by applica tions such as collaborative filtering and link prediction in networks that model the relations between entities from latent attributes [ 16 ] . These models often transfer the problem as learning an embedding of the entities , which corresponds algebraically to a matrix factorization problem of observed relationships . Zhu et . al . [ 50 ] proposed a joint factorization approach on both the linkage adjacency matrix and document term frequency for Web page categorization . Similar concepts also include [ 29 , 44 ] . In addition , Shaw etal [ 38 ] proposed a structure preserving embedding framwork that embeds graphs to a low dimensional Euclidean space with global topological properties preserved . Moreover , DeepWalk [ 31 ] learns latent representations of vertices in a network from truncated random walks . However , these models focus only on single relations that do not adapt to heterogeneous settings and most of them are hardly to generate to other unseen samples .
A natural extension of these methods to heterogeneous settings is by stacking multiple relational matrices together , and then applying a conventional tensor factorization [ 26 , 29 , 39 ] . The disadvantage of such multi relational embeddings is the inherent sharing of parameters between different terms , which does not scale to large graphs . A nonlinear embedding model proposed by Yuan et . al . [ 46 ] , used Restricted Boltzmann Machines ( RBMs ) for crossmodel link analysis . However , it did not utilize all of the social information from the raw data ( required a feature vectorization step ) , which resulted in suboptimal solutions . Moreover , work in computer vision and speech [ 18 ] has shown that layer wise RBM training is inefficient for large scale settings when compared to DNNs and CNNs .
2.2 Deep Learning
In recent years , machine learning research has seen a marked switch from hand crafted [ 49 ] features to those that are learned from raw data , mainly due to the success of deep learning . Deep learning models have become increasingly important in speech recognition , object recognition/detection , and more recently in natural language processing . Deep learning techniques are universal function approximators . However , historically , there has been very limited success in training deep networks with more than two hidden layers . The difficulty lies in the high dimensional parameter space and highly non convex objective function , where gradient based methods are trapped by local minima .
Recent advances in deep learning have benefited from a confluence of factors , such as the availability of large scale datasets , computational resources , and advances in both unsupervised and supervised training algorithms . Unsupervised deep learning , often referred to as “ pre training , ” provides robust initialization and regularization with the help of unlabeled data , which is copiously available . For example , Hinton and Salakhutdinov [ 14 ] first employed layer wise initialization of deep neural networks with the use of RBMs . A similar approach is weight initialized with autoencoders , as proposed by Bengio et . al [ 1 ] . More recently , supervised learning with multilayer neural networks has become possible . The method of dropout [ 15 ] has shown particular promise . A seven layer convolutional network developed by Krizhevsky et . al . [ 18 ] achieved state of the art performance on the ImageNet Large
ImagesVideosText120 Unsupervised Feature
Learning
HNE
Supervised Task specific
Learning
Off the shelf Learning
Algorithms
Classifications
Clustering
Link Prediction
Retrieval
Recommendation
Heterogeneous Networks
Final Tasks Figure 2 : The flowchart of the proposed Heterogeneous Network Embedding ( HNE ) framework .
Uniformed Vector representation
Scale Visual Recognition Challenge [ 36 ] , one of the most challenging tasks in computer vision . Later on , features from one of network ’s intermediate layers proved to be a superior feature representation for other vision tasks , such as object detection [ 11 ] .
There has been a growing interest in leveraging data from multiple modalities within the context of deep learning architectures . Unsupervised deep learning methods such as auto encoders [ 25 ] and RBMs [ 40 ] are deployed to perform feature learning by joint reconstruction of audio and video with a partially shared network , and successfully applied to several multimodal tasks . Alternatively , there has also been effort on learning the joint representation with multiple tasks [ 6 ] . For image and text , a particular useful scenario is zero shot learning of image classification on unseen labels achieved by incorporating the semantically meaningful embedding space to image labels [ 9 , 27 ] .
To the best of our knowledge , there have been few previous attempts to make use of the linkage structure in a network for representation learning . In [ 19 ] a conditional temporary RBM was used to model the dynamics of network links , and the main task was to predict future links with historical data . The most similar work to ours is that of [ 46 ] . A content RBM was trained on multimedia link data . We differ from their work in several respects . We use a completely supervised learning scheme in an unsupervised setting ; our method can scale up to large scale datasets ; we perform multi task learning to fuse the information from different modalities . 3 . PRELIMINARIES
In this section , we introduce our notation as well as the mathematical definitions we will use to describe heterogeneous networks . 3.1 Notation
Throughout this paper , all vectors are column vectors and are denoted by bold lower case letters ( eg , x and y ) , while matrices are represented by bold upper case letters ( eg , X and M ) . The ith row of a matrix X is given by Xi¨ , while X¨j represents the jth column . We use calligraphic letters to represent sets ( eg , V and E ) . The notation |¨| denotes the cardinality of a given set . The empty set is denoted by   . 3.2 Heterogeneous Networks
A heterogeneous network [ 41 ] is defined as a network with multiple types of objects and/or multiple types of links . As a mathematical abstraction , we define an undirected graph G “ pV,Eq , where V “ tv1 , . . . , vnu is a set of vertices and E is a set of edges .
An edge eij , @i , j P t1,¨¨¨ , nu belongs to the set E if and only if an undirected link exists between nodes i and j . Moreover , the graph G is also associated with an object type mapping function fv : V Ñ O and a link type mapping function fe : E Ñ R , where O and R represent the object and relation sets , respectively . Each node vi P V belongs to one particular object type as fvpviq P O . Similarly , each link eij P E is categorized in different relations as fvpeijq P R . It is worth mentioning that the linkage type of an edge automatically defines the node types of its end points . The heterogeneity of a network is reflected by the size of the sets O and R , respectively . In the case of |O| “ |R| “ 1 , the network is homogeneous ; otherwise , it is heterogeneous . An example of a heterogeneous network is illustrated in the left hand side of Figure 2 , which contains two object and three link types . For further ease in understanding , we will assume object types of image ( I ) and text ( T ) . The link relationships R correspond to image to image ( red dotted line ) , text to text ( green dashed line ) and image to text ( blue solid line ) , which are denoted by RII , RT T and RIT , respectively . Therefore , in this case , we have |O| “ 2 and |R| “ 3 . While this simplified abstraction in the text and image domain is both semantically and notationally convenient for further discussion in this paper , this assumption is without loss of generality because the ideas are easily generalizable to any number of types .
Thus , any vertex vi P V can be categorized into two disjoint subsets VI and VT corresponding to the text image domains , respectively . Therefore , we have VI Y VT “ V and VI X VT “   . Similarly , the edge set E can be partitioned into three disjoint subsets , which are denoted by EII , ET T and EIT , respectively . Furthermore , each node is summarized by unique content information . In particular , images are given as a squared tensor format as Xi P RdIˆdIˆ3 for every vi P VI , while texts are represented by a dT dimensional feature vector as zj P RdT for all vj P VT . For example , the content representations could be a raw pixel format in RGB color space for images , or it could be the Term Frequency Inverse Document Frequency ( TF IDF ) [ 32 ] scores of a text document . We represent linkage relationship as a symmetric adjacency matrix A P Rnˆn , which the pi , jqth entry of A equals one if eij P E ; otherwise Aij “ ´1 ( for model simplicity ) . 4 . HETEROGENEOUS NETWORK EMBED
DING
In this section , we present our HNE framework mathematically by first introducing a novel loss function to measure correlations
121 across networks . Essentially , the embedding process encodes both heterogeneous content and linkage information to a multidimensional representation for each object . A linkage guided deep learning framework is then proposed to jointly model the latent space embedding with feature learning simultaneously . This can be solved efficiently by using back propagation techniques . Finally , we discuss the straightforward extension of the HNE algorithm to the general case of more than two object types . 4.1 Latent Embedding in Networks
The main goal of the heterogeneous embedding task is to learn mapping functions to project data from different modalities to a common space so that similarities between objects can be directly measured . Assume that the raw content Xi associated with an image node can be transformed to a d1I dimensional vector representation as xi . The conversion of the raw input data into this d1I dimensional vector representation will be described in the following subsection . A naive approach to do so is by stacking each column of an image as a vector or through feature machines [ 7 , 23 ] . It is worth pointing out that , the values of d1I and dT need not be the same , because images and text are defined in terms of completely different sets of features .
We transform two type of samples to a uniform latent space with the use of two linear transformation matrices , denoted by U P Rd1Iˆr and V P RdT ˆr , for the image and text domains , respectively . The transformed samples are denoted by ˜x and ˜z for images and text documents , respectively , where we have :
˜x “ U T x , and ˜z “ V T z .
( 1 ) Even though the image and documents may be represented in spaces of different dimensionality , the transformation matrices U and V map them into a common r dimensional space . The similarity between two data points with the same object type can be presented as an inner product in the projected space as follows : spxi , xjq “ ˜xi spzi , zjq “ ˜zi
T ˜xj “ pU T xiqT U T xj “ xi T ˜zj “ pV T ziqT V T zj “ zi
T MII xj , T MT T zj ,
Note that the embedding into a common space also enables similarity computation between two objects of different types , such as text and images , as follows : spxi , zjq “ ˜xi “ ˜zj
T ˜zj “ pU T xiqT V T zj “ xT T ˜xi “ pV T zjqT U T xi “ zT i MIT zj j M T IT xi ,
Here , MII P Rd1Iˆd1I and MT T P RdT ˆdT are positive semidefinite matrices while MIT P Rd1IˆdT . The latent embedding is closely related to similarity and metric learning that has been widely studied in the literature [ 20 ] . It suggests that the correlations between two nodes in a network can be either parameterized by the projection matrices U and V or through a bilinear function defined by the matrices MII , MT T and MIT . This provides the flexibility to model the heterogeneous relationship in an application specific way .
The heterogeneous objects interact with each other either explicitly and implicitly . These interactive pieces of information are represented as heterogeneous linkages in networks . The assumption is that if two objects are connected , the similarity measure between them should reflect this fact by providing a larger value compared to the ones that are isolated . Consider a pair of images denoted as xi and xj . To encode the link information , we design a pairwise decision function dpxi , xjq as follows : dpxi , xjq" ° 0 if Aij “ 1 ,
† 0 otherwise .
Note that to infer d we need not know the respective entry value of A , or require heterogeneous nodes to be in sample [ 34 ] . This means that the approach has the generalization ability to embed samples from unseen nodes . Consider dpxi , xjq “ spx i , xjq ´t II ,
Lpxi , xjq “ log p1 ` expp´Ai,j dpxi , xjqqq ,
( 5 ) for all vi , vj P VI , where tII is a relational based bias value . Then , the loss function can be formulated as follows : ( 6 ) which can be seen as a binary logistic regression guided by network linkages . The loss function of text text and image text are similar to equation ( 6 ) by simply replacing sp¨q with that of the corresponding modality . Similarly , the bias terms , denoted by tT T and tIT , can be set to the corresponding ones . It leads to our objective functions in the form of : 1 min U ,V
NII ÿvi,vjPVI `  2
NIT
ÿviPVI ,vjPVT
Lpxi , xjq `  1
NT T ÿvi,vjPVT Lpxi , zjq `  3p}U}2
Lpzi , zjq
F ` }V }2
Fq ,
( 7 ) where NII , NT T and NIT are the numbers of the three types of links in the network . Furthermore ,  1 ,  2 and  3 are the three balancing parameters , in which the first two control the emphasis among three types of linkages and the last one is used balance the so called bias variance trade off [ 13 ] . Moreover , in equation ( 7 ) , }¨}F denotes the Frobenius norm . The bias terms in the loss function can be either treated as learning variables or set to fixed values . For simplicity , we set these bias terms to constants .
The aforementioned objective function can be efficiently solved with coordinate descent methods , which solve for each individual variable while keeping the others fixed : Solving U : Fixing parameter V , the objective function ( 7 ) can be reduced as follows : min U
1
NII ÿvi,vjPVI `  2
NIT log´1 ` e´Ai,j xT ÿviPVI ,vjPVT i U U T xj¯ `  3}U}2 i U V T zj¯ log´1 ` e´Ai,j xT
F
( 8 )
Bp¨q BU “ 1
The gradient is given by the following : ´Ai,jpxj xT 1 ` eAi,j xT ÿviPVI ,vjPVT
NII ÿvi,vjPVI `  2
NIT i ` xixT i U U T xj ` 2 3U j qU
´Ai,j xizT j V
1 ` eAi,j xT i U V T zj
( 9 )
.
Solving V : Similarly , the variable V can be handled as follows : min V
 1
NT T ÿvi,vjPVT `  2
NIT log´1 ` e´Ai,j zT ÿviPVI ,vjPVT i V V T zj¯ `  3}V }2 i U V T zj¯ . log´1 ` e´Ai,j xT
F
Taking the derivative with respect to V , we obtain : j qV i ` zizT i V V T zj ` 2 3V
Bp¨q BV “  1
NT T ÿvi,vjPVT `  2
NIT
´Ai,jpzj zT 1 ` eAi,j zT ÿviPVI ,vjPVT
´Ai,j zj xT i U
1 ` eAi,j xT i U V T zj
.
( 10 )
( 11 )
( 2 )
( 3 )
( 4 )
122 So far , we have shown that our loss function integrates network structures that map different heterogeneous components into a unified latent space . However , such embedding functions are still linear , which might lack the power to model complex network connections . In the following , we will present how Equation ( 7 ) fits into the deep learning framework . 4.2 The Deep Architecture
In the previous section , we broken things down into two steps : 1 ) manually construct a feature representation , 2 ) embed different modalities into a common space . In this section we tightly integrate these two steps into a deep learning framework by learning the feature representation and embedding together . min
U ,V ,DI ,DT
LppDI pXiq , pDI pXjqq `  3p}U}2
F ` }V }2 Fq
1
NII ÿvi,vjPVI NT T ÿvi,vjPVT `  1 ÿviPVI ,vjPVT `  2
NIT
LpqDT pziq , qDT pzjqq
LppDI pXiq , qDT pzjqq ,
( 12 ) Here , pp¨q and qp¨q are two nonlinear functions parameterized by DI and DT . DI and DT are two sets of parameters associated with the deep image and text networks , respectively . Specifically , we utilizes the CNNs structure as building blocks to learn image features while fully connected ( FC ) layers are used to extract discriminative representations for pre processed texts . The feature learning and information embedding are mutually reinforced by our approach . The image module exploits spatially local correlations by enforcing a local connectivity between neurons from adjacent layers . The parameters on each layer are referred to as filters . The architecture confines the learned filters to reflect the spatial local patterns of images . In addition , each sparse filter is replicated across the entire visual field , which share the same parameters ( both weights W k I and bias bk I ) . The output of each filter is usually termed as “ feature map ” , and conceptually , a feature map is obtained by convolving an input image with a linear filter , adding a bias term and then applying a non linear function . We denote the k th feature map at a given layer ( a given depth ) as hk , which is determined by the corresponding weights W k and bias bk . Then , the feature map is obtained as follows : hk “ maxt0,pW k ˚ Mq `b ku ,
( 13 ) Here , ˚ denotes the convolution operation and M is an input from the previous layer of the deep image module . The definition of convolution of a filter g with a 2D signal f is as follows : orm , ns “ frm , ns ˚ grm , ns
“
8ÿu “ ´8
8ÿv “ ´8 fru , vsgru ´ m , v ´ ns .
( 14 )
Moreover , the maxt0,¨u operator , called the rectified linear unit , provides the non linearity . To form rich representations of a given dataset , each layer is composed of multiple feature maps so that each filter W k forms a three dimensional tensor for every combination of source feature map , vertical and horizontal size . A graphical illustration of the image module is provided in figure 3 , which contains five convolution layers and two FC layers . Each input image X P RdIˆdiˆ3 is represented as a 4096 dimensional vector through a series of nonlinear operations in both the training and testing phases . Once the set of parameters DI is fixed , the feature of each individual input images is deterministic . In contrast , since text documents are unstructured that do not contain a spatial information , fully connected layers are commonly
Figure 3 : An example of the deep image module which consists of five convolution layers and two fully connected layers . used to extract application orientated feature on top of TF IDF inputs . The feature transformation is expressed as follows : qDT pzq “ maxt0 , WT z ` bTu
( 15 ) This is performed through a single fully connected layer , where WT P RrˆdT and bT P R . r indicates the number of neurons in a given layer . Similarly , rich representations can be learned by stacking multiple fully connected layers with different number of neurons ( r can be set to different values in different layers ) to construct the deep text architecture for word documents .
Since the linear heterogeneous embedding in section 4.1 can be viewed as transforming inputs to a common space , we can achieve this by cascading an extra linear embedding layer to each deep module . Define
˜pD1IpXq “ U T pDIpXq , and ˜qD1T pzq “ V T pDT pzq ,
( 16 ) where D1I “ DI YtUu and D1T “ DT YtV u . Then , the objective function in equation ( 12 ) is equivalent to the following : L1p˜pD1IpXiq , ˜pD1IpXjqq
1 min D1I ,D1T
NII ÿvi,vjPVI NT T ÿvi,vjPVT `  1 ÿviPVI ,vjPVT `  2
NIT
L1p˜qD1T pziq , ˜qD1T pzjqq
( 17 )
L1p˜pD1IpXiq , ˜qD1T pzjqq ,
The problem of over fitting can be effectively prevented by using dropout [ 15 ] instead of L2 regularizations . The new loss term L1p¨,¨q is defined as
L1pa , bq “ log ´1 ` exp´´Ai,j aT b¯¯ ,
( 18 ) for any vector a , b with a same dimensionality . For simplicity , we refer to both deep image and text modules as a series of nonlinear feature transformations with an additional linear common space embedding .
To perform end to end HNE learning , we connect the deep image and text modules accordingly to the image image , text text , and image text losses in equation ( 17 ) . As an example , we illustrate the text text module in figure 4 , and the other two can be extended in a similar manner . Figure 4 contains two text modules that comprise the pairwise text text module . The illustrated deep text text module contains two FC layers followed by a linear embedding layer . A pair of text documents are fed from the left and computed in a leftto right direction . The outputs from the embedding layer are the vectorized representation of corresponding objects in the common latent space . These are further channeled to a prediction layer to calculate the loss using equation ( 17 ) . To make the text text module symmetric ( feeding the same objects from the top or the bottom pass of the text text modules will lead to a same latent representation ) , we need to tighten these parameters . In figure 4 , if two neurons have the same color , they share the same weight and bias .
128 Feature Maps 55x55256 Feature Maps 27x27384 Feature Maps 13x13384 Feature Maps 13x13256 Feature Maps 13x1340964096Pretrained on ILSVRC2013 datasetembeddingconvolutionpoolingresponse normalizationconvolutionpoolingresponse normalizationconvolutionconvolutionfullyconnected123 prediction layer similarity predictions
Linear embedding
Linear embedding
Linear embedding
ConvNet
ConvNet
ConvNet
FC layer
FC layer
FC layer
Nonlinear embeddings
Nonlinear embeddings
Nonlinear embeddings
ConvNet
ConvNet
ConvNet
FC layer
FC layer
FC layer weights sharing weights sharing weights sharing weights sharing image image module image text module text text module
Pairwise nodes from networks
( )
,
( )
,
( )
( )
( )
,
Mini batch
Figure 5 : The overall architecture of HNE . The same color indicates the shared weights . The arrows are directions of forward feeding and back propagation .
´Ai,j
1`e
BD1I
BD1I
Ai,j ˜pD1I pXiqT ˜pD1I pXiq
. It is worth mentioning that where cij “ the summation from both Xi and Xj parts is because we tie the parameters of each image module within the image image subnetwork ( symmetric to pairwise inputs ) . Moreover , the gradient B ˜pD1I pXiq are dependent only on the structure of the deep neural network . In other words , once the deep architecture has been fixed , their gradients are automatically defined . Furthermore , for each input text pair , the gradient is similar but changing the input and network parameters in equation ( 19 ) to that of the corresponding text case . and B ˜pD1I pXjq
We can see that image image inputs only contribute to learning discriminative representations for image modules . On the other hand , the cross model inputs will affect the learning specific to both image and text . Their gradients are shown respectively as follows :
Figure 4 : An example of the deep text text module by concatenating a pair of text modules . Same coloring indicates shared weights . The overall architecture of learning such a heterogeneous embedding function from a given network is visualized in figure 5 . Three modules are shown in the figure , corresponding to imageimage , image text and text text from left to right . These are connected to the prediction layer . Pairwise training samples are formed as mini batches feeding from the bottom to the top . Once the value of the loss has been obtained , the gradients of each parameter in the deep network are calculated using backpropagation techniques . 4.3 Optimization
The objective function in equation ( 17 ) can be efficiently minimized by stochastic gradient descent ( SGD ) by sampling minibatches from the training set . The advantage of training stochastically is that each mini batch can be loaded onto GPU and computed in a parallel scheme if needed . Popular open source deep learning packages using GPU based implementations include Cuda convnet [ 18 ] , Caffe [ 17 ] and Theano [ 2 ] , etc For each input image pair , the gradient of D1I is given as follows : Bp¨q BD1I “ ` BD1I B ˜pD1IpXiq “ cij ¨˜˜pD1IpXiq B ˜pD1IpXjq
B ˜pD1IpXjq ` ˜pD1IpXjq
BD1I B ˜pD1IpXiq
B ˜pD1IpXjq
B ˜pD1IpXiq
¸ ,
Bp¨q
Bp¨q
( 19 )
BD1I
BD1I
Bp¨q BD1I “
1 ` e
´Ai,j ˜pD1T pzjq Ai,j ˜pD1I pXiqT ˜pD1T pzjq ¨
B ˜pD1IpXiq
BD1I and
Bp¨q BD1T “
1 ` e
´Ai,j ˜pD1IpXiq Ai,j ˜pD1I pXiqT ˜pD1T pzjq ¨
B ˜pD1Ipziq
BD1T
4.4 Discussion
,
.
( 20 )
( 21 )
The trained deep neural network assigns different types of data to some points in a unified space so that similarities can be directly compared . So far , we have shown the proposed embedding scheme for heterogeneous networks with only two object types : text and images . While these two types are a natural representative in many real settings , it is conceivable to expect more than two types of inputs . The proposed methods can be easily extended to handle multiple input types by considering an individual deep module for each type of data . Then , the objective function in equation ( 17 ) will consider all possible pairs of input types . If there are |O| input types , the new objective will contain |O| ``|O|2˘ object types . Because deep learning is highly nonlinear and non convex , globally optimal convergence is not assured . The initialization of parameters are crucial to the final performance . The literature has shown that well designed pre training can significantly improve final performances even when the final task is different than the pretraining task . It is worth mentioning that the proposed embedding method is unsupervised and can be used as pre training step for any further fine tuning . In other words , if we want to classify network
𝑧1𝑖 𝑧2𝑖 𝑧𝑑𝑇𝑖 𝑧1𝑗 𝑧2𝑗 𝑧𝑑𝑇𝑗 ⁞ ⁞ ⁞ ⁞ ⁞ ⁞ Input layer Input layer 1𝑠𝑡 hidden layer 2𝑛𝑑 hidden layer Embedding layer 𝑊𝑇2 𝑊𝑇2 𝑊𝑇1 𝑊𝑇1 ⁞ ⁞ 𝑉 𝑉 𝑧𝑖 𝑧𝑗 124 Figure 6 : Linkage structures between 500 randomly selected nodes in the BlogCatalog dataset . The node color indicates the label of each node .
Table 1 : Detailed statistics of the BlogCatalog dataset .
Number of nodes Number of links Number of classes Content dimensionality Balanced classes
Statistics 5196 171,743 6 8189 yes nodes , we can either obtain final features from the embedding layer and apply off the shelf machine learning algorithms or we can replace the prediction layer to a soft max layer , and then fine tune the entire deep network to a task specific one .
5 . EXPERIMENTAL RESULTS
In this section , we evaluate our proposed algorithm on several real world datasets for both homogeneous and heterogeneous settings . The experimental results show evidence of significant improvement over many conventional baselines . 5.1 Datasets and Experiment Settings
We use two publicly available datasets from real world social sites . The first one is BlogCatalog which is used in [ 42 ] to select features in linked social media data . The second one is a heterogeneous dataset , which is referred to as NUS WIDE [ 5 ] . This dataset contains both images and text . All experiment results are averaged over five different runs . The detailed descriptions and statistics for both datasets are provided below . ‚ BlogCatalog [ 42 ] : It is a social blogging site where registered users are able to categorize their blogs under predefined classes . Such categorizations are used to define class labels , whereas “ following ” behaviors are used to construct linkages between users . The TF IDF features are extracted from blogs as a vector representation of each individual user . Thus , blog users are represented as different nodes of the constructed networks associated with content features . It is worth mentioning that , the user blogging networks is undirected , where the co following and cofollowed relationships are the same . Some detailed statistics are summarized in Table 1 .
‚ NUS WIDE [ 5 ] : The dataset was originally collected by the Lab for Media Search in the National University of Singapore in the year 2009 . The dataset includes 269,648 unique images with associate tags from Flickr . The total number of tags is 5,018 . Additionally , there are 81 groundtruth attribute labels on each image and tag . Since the original dataset injected many “ noise ” samples that did not originally belong to any of the 81 concepts , these samples were removed . Moreover , we used the most frequent 1,000 tags as text documents and extracted their TF IDF features . We further removed those image text pairs that did not contain any considered words . Finally , we randomly sampled 53,844 and 36,352 image text pairs for training and testing , respectively . We constructed a heterogeneous network as the input of our proposed framework by treating images and text as sep
Figure 7 : The Linkage reconstruction rate on the BlogCatalog . arate nodes . In total , the training network contained 107,688 nodes while the testing network had 72,704 . The semantic linkages between two nodes are initially constructed if they share at least one concept . We then random sample at most 30 links per node to construct the sparse matrix A . It is worth mentioning that we only evaluate our framework in an out of sample manner . In other words , we ensure the training information absolutely does not appear in any of the testing cases .
5.2 Network Reconstructions
Before proceeding to evaluate the proposed method in the task of classification , clustering or retrieval , we will first provide a basic and intuitive evaluation of the quality of network linkage reconstruction to validate our assumptions . Since the goal of the proposed formulation in equation ( 17 ) is that a good latent embedding brings objects with links closer while it pushes objects without linkage structures further , the ideal performance of the learned model can reach perfect network linkage reconstructions using equation ( 4 ) . We first visualize the network linkage structure of the BlogCatalog dataset by randomly selecting 500 nodes and plotting their connectivities in figure 6 . The color of each node indicates its class . As we can see , the social “ following ” relationships tend to connect users with similar attributes , at least from a relative point of view . On the other hand , they are noisy from an absolute point of view , in which 59.89 % of links in the entire dataset connect to nodes with different classes .
We apply the proposed algorithm to learn an embedding function while monitoring the link reconstruction accuracy as shown in figure 7 . The stochastic learning is conducted by randomly selecting 128 pairs of nodes to use as a mini batch . The horizontal axis indicates the index of the epoch . And each epoch contains 500 mini batches . On average , each mini batch can be trained in less than 0.15 seconds on a single Nvidia Tesla K40 GPU . In figure 7 , the reconstruction performance on each mini batch is recorded , and the line indicates the median filtered values . As more samples have been viewed by the deep HNE learner , it is able to correctly reconstruct more than 80 % of the pairwise connections as compared to the initial number of 55 % . Similarities propagate through sparse links across the whole network to obtain a global consistency . 5.3 BlogCatalog
In this section , we evaluate the performance of the HNE framework and compare it with state of the art algorithms in various tasks in the field of data mining and Web search .
531 Classification To demonstrate the effectiveness of the representation provided by HNE , we compare our learned features with those of other
125 The classification accuracy for BlogCatelog data set
Table 2 : The clustering result for BlogCatalog dataset . y c a r u c c A n o i t a c i f i s s a C l
90
85
80
75
70
65
60
55 content link content−link LUFS LCMF HNE+NN HNE+softmax
5 %
10 %
20 %
Percentage of labeled data
30 %
Methods content link content link
LUFS LCMF HNE 5.4 NUS WIDE
Accuracy NMI 49.06 % 0.3192 40.76 % 0.2482 51.69 % 0.3457 49.88 % 0.3221 53.91 % 0.3678 62.37 % 0.4388
Compared to the BlogCatalog dataset , the NUS WIDE dataset forms a heterogeneous network that contains both images and text . We illustrate the performance of our framework for the task of classification and cross modal retrieval in the following subsections . Note that the latter application is not possible in the homogeneous scenario of the previous dataset . 541 Heterogeneous Classifications Given the heterogeneous scenario of this dataset , we compared our proposed method to a different set of unsupervised baselines that can specifically handle multimodal data inputs : ‚ CCA : The Canonical Correlation Analysis embeds two types of input sources into a common latent space by optimizing with respect to their correlations . tion in equation ( 7 ) . distances between image and text by latent embeddings .
‚ DT [ 33 ] : A transfer learning method is used to bridge semantic ‚ LHNE : The linear version of HNE solves the optimization funcSince our proposed method is an end to end learning framework , it does not require feature extraction for image inputs . We extract 4096 dimensional Cuda convnet [ 18 ] features for all other baseline methods . The output ( data in the common space ) dimensionality is set to 400 . Since the NUS WIDE dataset is multi label with unbalanced classes , we use the average precision ( AP ) to evaluate the classification performance for each possible label outcome . AP uses precision recall curves for algorithmic quantification for each label . These curves are used to obtain the mean average precision ( mAP ) . The mAP in multi label classification domains is the standard metric which is widely used in PASCAL challenges [ 8 ] in computer vision communities . To ensure fair comparison , we use linear support vector machines ( SVM ) as a common classification algorithm for all algorithms . The reason of using SVM is that calculating AP requires probabilistic interpreted confidence scores , which is inconvenient to obtain from NN classifiers .
The classification results are illustrated in table 4 , which contains three different settings . The “ image only ” setting means that we learn embedding functions from the heterogeneous training set , and then train an SVM , and test classification performance on image nodes . Under the “ Image + text ” setting , we consider all objects in the testing network . We observe that , for all methods , categorizing text documents only is the most difficult task . This may be because of the fact that the input text is sparse compared to images . Moreover , without the deep training , the linear version of our proposed method obtains comparable results as DT which outperforms CCA . The deep architecture HNE improves the performance further under all three different settings , which demonstrates the advantage of jointly optimizing the feature learning and latent embedding with nonlinear functions . 542 Multimodal Search To further demonstrate that the learned features can be leveraged with many data mining and web search tasks , we compared our proposed method with the aforementioned baselines in the task of
Figure 8 : The classification accuracies among different methods under various size of training sets . social media data considering both content and links . feature learning methods , while keeping the classification scheme fixed . The other baseline representations are as follows : ‚ Content : Only the content feature from the original space . ‚ Links : We treat the adjacency structures as the features . ‚ Link content : We combine features from the previous two . ‚ LUFS [ 42 ] : Unsupervised feature selection framework for linked ‚ LCMF [ 50 ] : A matrix co factorization method that utilizes both To ensure a fair comparison , we used the same representation dimensionality and used the standard Nearest Neighbor ( NN ) classifier . In other words , the number of latent factors for LCMF is set to be the same as our output dimensionality and the first three methods are projected to a low dimensional space using the Principal Component Analysis . linkage structure and content features .
The average classification accuracies for the BlogCatalog dataset are shown in figure 8 , with the output dimensionality fixed to 100 . As shown , the proposed HNE method consistently outperforms to other baselines under different training set sizes . This is because the network linkage information encodes useful insights for learning a low dimensional embedding space by bridging linked nodes .
The rightmost bar under each setting is achieved by treating latent embedding learning as a pre training step and fine tuning the entire deep network by replacing the loss layer with a multi class soft max layer . It shows that the unsupervised latent spacing learning provides very good initializations for the supervised classification task using deep architectures and also shows that we can also achieve much higher accuracies .
532 Clustering We also compared different feature representations under the clustering task . Compared to classification , clustering is totally unsupervised , and it heavily relies on the similarity measure between different objects . We adopted the commonly used cosine similarity . The results are reported in table 2 using both accuracy and normalized mutual information ( NMI ) as evaluation metrics .
The results are similar to those for the classification task . Using only links provides the worst results . This may be because , without global content information , the similarity measurements tend to be local and sensitive to noisy links . On the other hand , content similarities alone are insufficient to capture the relational knowledge . Therefore , a naive combination of the links and content provides comparable performance with other baselines . The proposed method of jointly learning the embedded space outperforms other baselines and achieves the state of the art .
126 Table 3 : Some cross model retrieval results of the proposed HNE method .
Query rank 1 rank 2 rank 3 rank 4 rank 5
Mountain
Sunset
Cow
Leaf
Table 4 : The classification result in terms of mAP ( mean average precision ) for the NUS WIDE dataset . DT
Sample
Image only Text only
Image + Text
LHNE
CCA HNE 51.96 % 52.07 % 53.16 % 54.28 % 51.37 % 51.88 % 51.34 % 52.76 % 52.54 % 53.22 % 53.32 % 54.99 %
Table 5 : The cross modal retrieval result ( p@k ) for the NUS WIDE dataset .
Method CCA DT
LHNE HNE rank 5 rank 10 rank 1 rank 20 21.05 % 16.84 % 18.95 % 18.68 % 20.53 % 25.26 % 22.63 % 22.37 % 26.32 % 21.05 % 21.02 % 22.27 % 36.84 % 29.47 % 27.89 % 26.32 % cross modal retrieval . Among all 81 labels , about 75 of them appear in the TF IDF text vector . We manually constructed 75 query vectors in the original 1000 dimensional text domain by setting the corresponding label entries to one and the remaining to zero . Using the learned embedding function , we projected these query vectors to the common latent space to retrieve all image samples in the test set using the standard Euclidean distance .
The average precision at rank k ( p@k ) over all queries is reported in table 5 . We observe consistent results as other tasks , and the proposed method significantly outperforms other baselines . Table 3 illustrates some sample retrieval results . For the query “ mountain ” , the third retrieved result is incorrect . This might due to the extreme visual similarities between the other mountain images and the one with a cow . The retrieval results for the query “ cow ” is not as good as the others . The first five returned images contain three deer . This is because these images have multiple labels and are connected by the concept “ animal ” . Since our method as well as the ranking functions are totally unsupervised , these links between “ deer ” and “ cow ” objects confuse our embedding learning . We expect performance gains by using supervised ranking methods . 5.5 Convergence
In this section , we examine the convergence of the HNE algorithm . For illustrative purposes , we demonstrate the value of objec
Figure 9 : The objective convergence study of proposed method in the BlogCatalog dataset . tive function changes for the BlogCatalog dataset in figure 9 . The X axis is same as the one used in figure 7 . As shown in the graph , the objective value continuously decreases in the first 60 epochs and then stabilizes . This result shows that that the algorithm usually converges to a stable result in practice .
6 . CONCLUSION
In this paper , we proposed a novel embedding scheme in the field of network science . This approach transfers different objects in heterogeneous networks to unified vector representations . The proposed method jointly considers contents and topological structures in networks for creating the embedding . We use deep learning techniques to capture the complex interactions between heterogeneous components . Such a highly nonlinear multi layered embedding architecture is robust , scalable and beneficial to many data mining and Web search applications . Furthermore , the approach has generic applicability because a robust feature representation is useful in many tasks . The experimental studies show that the proposed method significantly outperforms conventional baselines in various settings .
Acknowledgment This work was founded in part to Shiyu Chang , Wei Han and Thomas S . Huang by the National Science Foundation under Grand No .
127 1318971 . This work was partially sponsored by the Army Research Laboratory under Cooperative Agreement Number W911NF 09 20053 . Jiliang Tang is , in part , supported by National Science Foundation under Grant No . IIS 1217466 .
7 . REFERENCES [ 1 ] Y . Bengio , P . Lamblin , D . Popovici , and H . Larochelle . Greedy layer wise training of deep networks . NIPS , 19:153 , 2007 .
[ 2 ] J . Bergstra , O . Breuleux , F . Bastien , P . Lamblin , R . Pascanu ,
G . Desjardins , J . Turian , D . Warde Farley , and Y . Bengio . Theano : a cpu and gpu math expression compiler . In Proceedings of SciPy , volume 4 , page 3 , 2010 .
[ 3 ] S . Chang , W . Han , X . Liu , N . Xu , P . Khorrami , and T . S . Huang . Multimedia classification . Data Classification : Algorithms and Applications , page 337 , 2014 .
[ 4 ] Z . Chu , S . Gianvecchio , H . Wang , and S . Jajodia . Who is tweeting on twitter : human , bot , or cyborg ? In ACSAC , pages 21–30 . ACM , 2010 . [ 5 ] T S Chua , J . Tang , R . Hong , H . Li , Z . Luo , and Y . Zheng . Nus wide : a real world web image database from national university of singapore . In ICIVR , page 48 , 2009 .
[ 6 ] R . Collobert and J . Weston . A unified architecture for natural language processing : Deep neural networks with multitask learning . In ICML , pages 160–167 , 2008 .
[ 7 ] N . Dalal and B . Triggs . Histograms of oriented gradients for human detection . In CVPR , volume 1 , pages 886–893 . IEEE , 2005 . [ 8 ] M . Everingham , L . Van Gool , C . K . Williams , J . Winn , and
A . Zisserman . The pascal visual object classes ( voc ) challenge . IJCV , 88(2):303–338 , 2010 .
[ 9 ] A . Frome , G . S . Corrado , J . Shlens , S . Bengio , J . Dean , T . Mikolov , et al . Devise : A deep visual semantic embedding model . In NIPS , pages 2121–2129 , 2013 .
[ 10 ] Y . Fu , H . Xiong , Y . Ge , Z . Yao , Y . Zheng , and Z H Zhou .
Exploiting geographic dependencies for real estate appraisal : A mutual perspective of ranking and clustering . In ACM SIGKDD , pages 1047–1056 . ACM , 2014 .
[ 11 ] R . Girshick , J . Donahue , T . Darrell , and J . Malik . Rich feature hierarchies for accurate object detection and semantic segmentation . In CVPR , pages 580–587 . IEEE , 2014 .
[ 12 ] X . Glorot , A . Bordes , and Y . Bengio . Deep sparse rectifier networks .
In JMLR , volume 15 , pages 315–323 , 2011 .
[ 13 ] T . Hastie , R . Tibshirani , J . Friedman , T . Hastie , J . Friedman , and
R . Tibshirani . The elements of statistical learning , volume 2 . Springer , 2009 .
[ 14 ] G . Hinton and R . Salakhutdinov . Reducing the dimensionality of data with neural networks . Science , 313(5786):504–507 , 2006 . [ 15 ] G . Hinton , N . Srivastava , A . Krizhevsky , I . Sutskever , and
R . Salakhutdinov . Improving neural networks by preventing co adaptation of feature detectors . arXiv:1207.0580 , 2012 .
[ 16 ] R . Jenatton , N . L . Roux , A . Bordes , and G . R . Obozinski . A latent factor model for highly multi relational data . In NIPS , pages 3167–3175 , 2012 .
[ 17 ] Y . Jia , E . Shelhamer , J . Donahue , S . Karayev , J . Long , R . Girshick , S . Guadarrama , and T . Darrell . Caffe : Convolutional architecture for fast feature embedding . arXiv:1408.5093 , 2014 .
[ 18 ] A . Krizhevsky , I . Sutskever , and G . Hinton . Imagenet classification with deep convolutional neural networks . NIPS , 25 , 2012 .
[ 19 ] X . Li , N . Du , H . Li , K . Li , J . Gao , and A . Zhang . A deep learning approach to link prediction in dynamic networks , 2014 .
[ 20 ] Z . Li , S . Chang , F . Liang , T . S . Huang , L . Cao , and J . R . Smith .
Learning locally adaptive decision functions for person verification . In CVPR , pages 3610–3617 . IEEE , 2013 .
[ 21 ] C . Liu , K . Zhang , H . Xiong , G . Jiang , and Q . Yang . Temporal skeletonization on sequential data : Patterns , categorization , and visualization . In ACM SIGKDD , pages 1336–1345 . ACM , 2014 . [ 22 ] D . Liu , G . Ye , C T Chen , S . Yan , and S F Chang . Hybrid social media network . In ACM MM , pages 659–668 . ACM , 2012 . [ 23 ] D . G . Lowe . Distinctive image features from scale invariant keypoints . IJCV , 60(2):91–110 , 2004 .
[ 24 ] T . Mikolov , K . Chen , G . Corrado , and J . Dean . Efficient estimation of word representations in vector space . arXiv:1301.3781 , 2013 .
[ 25 ] J . Ngiam , A . Khosla , M . Kim , J . Nam , H . Lee , and A . Y . Ng .
Multimodal deep learning . In ICML , pages 689–696 , 2011 .
[ 26 ] M . Nickel , V . Tresp , and H P Kriegel . A three way model for collective learning on multi relational data . In ICML , 2011 .
[ 27 ] M . Norouzi , T . Mikolov , S . Bengio , Y . Singer , J . Shlens , A . Frome ,
G . S . Corrado , and J . Dean . Zero shot learning by convex combination of semantic embeddings . arXiv:1312.5650 , 2013 .
[ 28 ] M . Ou , P . Cui , F . Wang , J . Wang , W . Zhu , and S . Yang . Comparing apples to oranges : a scalable solution with heterogeneous hashing . In ACM SIGKDD , pages 230–238 . ACM , 2013 .
[ 29 ] A . Paccanaro and G . E . Hinton . Learning distributed representations of concepts using linear relational embedding . TKDE , 2001 .
[ 30 ] J . Pennington , R . Socher , and C . D . Manning . Glove : Global vectors for word representation . EMNLP , 12 , 2014 .
[ 31 ] B . Perozzi , R . Al Rfou , and S . Skiena . Deepwalk : Online learning of social representations . In KDD , pages 701–710 . ACM , 2014 .
[ 32 ] G J Qi , C . Aggarwal , and T . Huang . Towards semantic knowledge propagation from text corpus to web images . In WWW , pages 297–306 . ACM , 2011 .
[ 33 ] G J Qi , C . C . Aggarwal , and T . S . Huang . Transfer learning of distance metrics by cross domain metric sampling across heterogeneous spaces . In SDM , pages 528–539 . SIAM , 2012 .
[ 34 ] D . E . Rapach and M . E . Wohar . In sample vs . out of sample tests of stock return predictability in the context of data mining . Journal of Empirical Finance , 13(2):231–247 , 2006 .
[ 35 ] X . Ren , J . Liu , X . Yu , U . Khandelwal , Q . Gu , L . Wang , and J . Han .
Cluscite : Effective citation recommendation by information network based clustering . In KDD . ACM , 2014 .
[ 36 ] O . Russakovsky , J . Deng , H . Su , J . Krause , S . Satheesh , S . Ma ,
Z . Huang , A . Karpathy , A . Khosla , M . Bernstein , A . C . Berg , and L . Fei Fei . ImageNet Large Scale Visual Recognition Challenge , 2014 .
[ 37 ] P . Sen , G . Namata , M . Bilgic , L . Getoor , B . Galligher , and T . Eliassi Rad . Collective classification in network data . AI magazine , 29(3):93 , 2008 .
[ 38 ] B . Shaw and T . Jebara . Structure preserving embedding . In ICML , pages 937–944 . ACM , 2009 .
[ 39 ] A . P . Singh and G . J . Gordon . Relational learning via collective matrix factorization . In KDD , pages 650–658 . ACM , 2008 .
[ 40 ] N . Srivastava and R . Salakhutdinov . Multimodal learning with deep boltzmann machines . In NIPS , pages 2222–2230 , 2012 .
[ 41 ] Y . Sun , B . Norick , J . Han , X . Yan , P . S . Yu , and X . Yu . Integrating meta path selection with user guided object clustering in heterogeneous information networks . In KDD . ACM , 2012 .
[ 42 ] J . Tang and H . Liu . Unsupervised feature selection for linked social media data . In KDD , pages 904–912 . ACM , 2012 .
[ 43 ] F . Tian , B . Gao , Q . Cui , E . Chen , and T Y Liu . Learning deep representations for graph clustering . In AAAI , 2014 .
[ 44 ] T . Yang , R . Jin , Y . Chi , and S . Zhu . Combining link and content for community detection : a discriminative approach . In KDD , pages 927–936 . ACM , 2009 .
[ 45 ] J . Yi , L . Zhang , J . Wang , R . Jin , and A . K . Jain . A single pass algorithm for efficiently recovering sparse cluster centers of high dimensional data . In ICML , pages 658–666 , 2014 .
[ 46 ] Z . Yuan , J . Sang , Y . Liu , and C . Xu . Latent feature learning in social media network . In ACM MM , pages 253–262 . ACM , 2013 .
[ 47 ] J . Zhang , S . Y . Philip , and Z H Zhou . Meta path based multi network collective link prediction . In KDD . ACM , 2014 . [ 48 ] P . Zhao , J . Han , and Y . Sun . P rank : a comprehensive structural similarity measure over information networks . In CIKM , pages 553–562 . ACM , 2009 .
[ 49 ] J . Zhou , F . Wang , J . Hu , and J . Ye . From micro to macro : data driven phenotyping by densification of longitudinal electronic medical records . In ACM SIGKDD , pages 135–144 . ACM , 2014 .
[ 50 ] S . Zhu , K . Yu , Y . Chi , and Y . Gong . Combining content and link for classification using matrix factorization . In SIGIR , pages 487–494 . ACM , 2007 .
128
