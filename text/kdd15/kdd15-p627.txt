Reducing the Unlabeled Sample Complexity of
Semi Supervised Multi view Learning
Chao Lan clan@ittckuedu
Jun Huan jhuan@ittckuedu
School of Engineering University of Kansas
Lawrence , KS 66047 , USA
ABSTRACT In semi supervised multi view learning , unlabeled sample complexity ( usc ) specifies the size of unlabeled training sample that guarantees a desired learning error . In this paper , we improve the state of art usc from O(1/ ) to O(log 1/ ) for small error , under mild conditions . To obtain the improved result , as a primary step we prove a connection between the generalization error of a classifier and its incompatibility , which measures the fitness between the classifier and the sample distribution . We then prove that with a sufficiently large unlabeled sample , one is able to find classifiers with low incompatibility . Combining the two observations , we manage to prove a probably approximately correct ( PAC ) style learning bound for semi supervised multi view learning . We empirically verified our theory by designing two proof ofconcept algorithms , one based on active view sensing and the other based on online co regularization , with real world data sets .
Keywords Sample Complexity , Multi View Learning , Semi Supervised Learning
1 .
INTRODUCTION
In semi supervised learning [ 30 ] , a classifier is trained with both a labeled sample and an unlabeled sample , where each sample is a set of data . These two types of samples differ in whether the true label set is known during training : for labeled sample the set is known , whereas for unlabeled sample it is not . Sample complexity [ 5 ] is a common measurement on the ‘efficiency’ of training samples . It specifies the size of a training sample that guarantees a desired learning error . In this paper , such complexities of labeled and unlabeled samples are termed as labeled sample complexity ( sc ) 1 and unlabeled sample complexity ( usc ) , respectively .
1In active learning , sc is also called label complexity [ 19 ] or abbreviated as sample complexity [ 7 ] .
Semi supervised multi view learning is a popular learning scheme when data can be represented by multiple views , such as in web classification [ 10 ] , natural language processing [ 3 ] , computer vision [ 13 ] , medical diagnosis [ 29 ] and chemical classification [ 14 ] . A key notion in this learning scheme is the view incompatibility of a classifier , that is , the fraction of data ( over the sample space ) whose views are disagreed by the classifier in predictions [ 5 ] . The notion is also called view disagreement rate in some discussions such as [ 16 ] . A typical multi view algorithm ( eg[10 , 24 ] ) uses the unlabeled sample to estimate the view incompatibility of classifiers , and returns those with low view incompatibility . A recent survey on multi view learning can be found in [ 28 ] .
As noted by many previous studies , semi supervised multiview learning has delivered an effective scheme of training with the unlabeled sample . A natural follow up question is whether such scheme can be more efficient in terms of usc We have witnessed significant progress on improving the efficiency of sc , ie it is improved from O(1/ ) to O(log 1/ ) in active multi view learning [ 27 ] , and even to O(log1/ 1/δ ) in semi supervised multi view learning [ 5 ] , which aims at trading usc for an efficient sc We notice that limited progress has been made on usc , and the stateof the art result is O(1/ ) [ 5 ] . Improving usc would not only advance our understanding on unlabeled data , which is commonly demanded in large quantity , but also help to lift the computational burden induced by these data , which usually dominate the training time ( eg[18 , 23] ) .
In this paper , we improve the state of art usc of semisupervised multi view learning from O(1/ ) to O(log 1/ ) under mild conditions , without trading the associated sc We begin our study by proving a connection between the generalization error of a classifier and its incompatibility , under a general expanding condition of sample distribution [ 27 ] . This connection induces a transformation from the problem of removing high error classifiers to the problem of removing high incompatibility classifiers . We then prove , with high probability , an unlabeled sample of size O(ξ log 1/ ) suffices to find a classifier , whose error is bounded by . Here , ξ is the incompatible coefficient developed to characterize usc , and as part of our proof , we showed it is bounded under mild conditions and does not depend on . Joining all results we conclude the improved usc Detailed proofs will be presented in Section 3 .
Our result highlights the importance of incompatible region for the better utilization of unlabeled data . Incompatible region is the set of data whose views are disagreed by some classifier(s ) in the search space . Algorithmic issues re
627 garding the estimation of this region is discussed in Section 4 . In Section 5 , we empirically verify this theoretic insight by designing two proof of concept multi view learning algorithms . One is based on active view sensing [ 29 ] , which aims at actively collecting missing views in the training sample to improve learning , and the other is based on online coregularization [ 17 ] , which trains data in an online fashion . We show that the two algorithms converge faster with the insights gained through our theoretic study . Discussions are presented in Section 6 and conclusions in Section 7 .
2 . PRELIMINARIES 2.1 Notations and General Setting Similar to previous analysis , we focus on the two view binary classification problem . Let X = X1 × X2 be a twoview sample set , where Xi is the space of view i . Assume data are drawn iid from X according to some distribution D , and let P denote the probability defined on X according to D . Let H = H1 × H2 be a two view hypothesis space , where Hi is a set of hypotheses ( classifiers ) mapping from Xi to a label set {−1 , +1} . For simplicity H is assumed finite . For any hypothesis h , let hi(x ) represent its classification on data x , based on the view i of x . Then h is said to be compatible on x if h1(x ) = h2(x ) and incompatible otherwise . The compatibility of h with D is χ(h,D ) = P(h1(x ) = h2(x ) ) and the incompatibility χ(h,D ) = P(h1(x ) = h2(x) ) . It is clear that χ(h,D ) + χ(h,D ) = 1 . Suppose the true labels of all data are assigned by a target hypothesis h∗ in H , known as the realizable setting . Then , the generalization error of any h is er(h ) = P(h(x ) = h∗(x) ) , where h(x ) is the final classification of h on x , obtained by fusing h1(x ) and h2(x ) under certain strategy . Also assume h∗ is full compatible with D , ie χ(h∗,D ) = 1 . For any sample S ⊆ X , let H(S ) ⊆ H represent the set of hypotheses that are compatible on S , ie compatible on every data in S . In places where conclusions are dependent on the uncertainty of S , notation PS is used to represent the probability defined over the random choice of S .
For better understanding , the notions introduced in this paper will be walked through the following example .
Example 21 Figure 1(a ) demonstrates a two view binary classification problem , formed by crossing two single view threshold problems . The sample space X = {[0 , .5]×[0 , .5]}∪ {[.5 , 1]× [ .5 , 1]} is colored as the gray region , and the hypothesis space H = {h(· ; z)}z∈Z is indexed by a parameter set Z = [ 0 , 1 ] × [ 0 , 1 ] such that for any z ∈ Z and each view i , hi(x ; z ) = +1 if xi ≥ zi and hi(x ; z ) = −1 otherwise . The target hypothesis is indexed by z∗ = ( .5 , .5 ) , which partitions X into the region of positive data ∪4 i=1Ri and the region of negative data R5 . Sample distribution D is assumed uniform . Then , take z = ( .75 , .75 ) for example , the compatibility of h(· ; z ) is the ( normalized ) area of R1 ∪ R4 ∪ R5 ,
χ(h,D ) = |R1 ∪ R4 ∪ R5|/|X| = 0.75 , and the incompatibility is the area of R2 ∪ R3 , ie
χ(h,D ) = |R2 ∪ R3|/|X| = 0.25 , where |X| = | ∪5 i=1 Ri| = 0.5 is the area of the entire sample space . The incompatibilities of other hypotheses can be computed in a similar manner , and are summarized as a heat map in Figure 1(b ) .
( a )
( b )
Figure 1 : ( a ) Two view sample space . ( b ) Heat map of the hypothesis incompatibility .
2.2 Definitions
To facilitate discussion , we first introduce two notions : Definition 1 . The true compatibility of h with D is χT ( h,D ) = P{x : h1(x ) = h2(x ) ∧ h1(x ) = h 1(x)} , ∗
( 1 ) and the false compatibility of h with D is
χF ( h,D ) = P{x : h1(x ) = h2(x ) ∧ h1(x ) = h
1(x)} . ∗
( 2 )
Note that the true compatibility represents the fraction of data that h is not only compatible on but also correct about their classifications , whereas the false compatibility presents the fraction that h is compatible on but wrong about their classifications . Clearly , χ(h,D ) = χT ( h,D ) + χF ( h,D ) . Example 22 Continue the example in Figure 1(a ) . The true compatibility of h(· ; z ) is the area of R1 ∪ R5 , ie
χT ( h,D ) = |R1 ∪ R5|/|X| = 0.625 , and the false compatibility is the area of R4 , ie
χF ( h,D ) = |R4|/|X| = 0125
The expanding condition of sample distribution was first proposed in [ 6 ] and then generalized in [ 27 ] , for analyzing multi view learning . It was previously defined in terms of sample sets , and stated that unlabeled data helps when view disagreed sample sets are expanding . Here , by Definition 1 , we can rephrase the expanding condition [ 27 ] in terms of hypothesis compatibility such that
Definition 2 . D is α expanding with respect to H if
χ(h,D ) ≥ α min{χT ( h,D ) , χF ( h,D)} holds for all h ∈ H .
( 3 )
Definition 2 provides a new angle of interpreting the expanding condition . It states that most hypotheses are incompatible on a notable fraction of data , which means they could be relatively easy to rule out by regularizing incompatibility during training . On the other hand , only extremely good ( low χF ) or extremely bad ( low χT ) hypotheses could have low incompatibility χ . Since χ can be estimated by unlabeled data alone , semi supervised multi view learning seems to admit a training scheme that ‘decouples’ the roles of labeled and unlabeled sample : the unlabeled sample is used to rule out hypotheses with high χ , and the labeled sample is used to separate the ( rest ) extreme hypotheses .
00510051R5R2R4R1R3z=(75,75)z*=(5,5)View OneView Two 011000102030405060708091628 ( a ) r=0.7
( b ) r=0.5
( c ) r=0.25
( d ) r=0.1
( e ) Θ(A(0.1 ) )
Figure 2 : ( 2a 2d ) Convergence of A(r ) as a function of r ; ( 2e ) Incompatible region with respect to A(r ) when r = 01
Recall that a trade off between the labeled sample complexity ( sc ) and unlabeled sample complexity ( usc ) has been suggested in [ 5 ] . The decoupled roles observed here seems to suggest that , it is possible to break the trade off at some point , so that usc can be improved without increasing sc We will later prove this is indeed the case .
Another use of Definition 1 is to clarify er(h ) , where h(x ) is obtained by fusing h1(x ) and h2(x ) under any strategy . Although the performance of fusion strategy is not of primary interest here , we maintain a parameter γ to characterize it , and say h adopts a γ fusion strategy if 1 ) when h is compatible on any given x , then h(x ) = h1(x ) ; 2 ) if h is incompatible on x , then the probability of h(x ) choosing the correct classification from {h1(x ) , h2(x)} is 1 − γ . For example , a random guess has r = 05 It is also clear that er(h ) = χF ( h,D ) + γ χ(h,D ) .
( 4 )
The last piece of information needed is an incompatible coefficient , which we generalize from the single view disagreement coefficient developed in [ 19 ] . Prior notions will be reviewed later when being connected to our definitions . Definition 3 . The incompatible region of X wrt H is Θ(H ) = {x ∈ X : ∃h ∈ H , h is incompatible on x} . ( 5 ) Definition 4 . The incompatible coefficient wrt X ,H is
ξ = sup r>0
P(Θ(A(r) ) ) r
,
( 6 ) where A(r ) = {h ∈ H : χ(h,D ) ≤ r} .
Below is an example of the incompatible coefficient . Later we will prove its connection to the disagreement coefficient , as a further evidence of it being bounded .
Example 23 Continues Example 21 Figure 2 ( a d ) demonstrate how A(r ) converges as a function of radius r . The gray regions represent the hypotheses excluded from A(r ) . Note that the convergence can be realized by unlabeled data alone , and it is clear that H is converging to the extremely good hypothesis h( ; ( .5 , .5 ) ) ( target ) and extremely bad ones h( ; ( 0 , 0 ) ) and h( ; ( 1 , 1) ) , which simply classify all data as positive or negative . In Figure 2d , the coordinates of boundary hypotheses from F 1 to F 8 are ( .5 , .6 ) , ( .6 , .5 ) , ( .5 , .4 ) , ( .4 , .5 ) , ( .9 , 1 ) , ( 1 , .9 ) , ( 0 , .1 ) , ( .1 , 0 ) , respectively . Figure 2(e ) shows the Θ(A(r ) ) when r = 0.1 as the darkened region in X . It is obtained by unifying the incompatible regions of all hypotheses in A(0.1 ) ( the red region ) . Based on this , one may observe that Θ(A(r ) ) always covers the whole X when r > 0.25 and hence P(Θ(A(r) ) ) = 1 . When r ≤ 0.25 , Θ(A(r ) ) starts to shrink and by simple geometry argument its ( normalized ) area is P(Θ(A(r) ) ) = 8r(1 − 2r ) . Combining two cases of r , we have
ξ = sup r>0
P(Θ(A(r) ) ) r
= 8 .
3 . MAIN RESULT
In this section , we first present the main theorem , and then walk through its proof with a number of lemmas . In essence , our analysis does not rely much on the labeled data . However , for completeness a labeled sample complexity ( sc ) will be included in the final result , which is attainable by standard PAC arguments . Ideally , any established sc of multi view learning ( eg [ 5 , 27 ] ) should be able to plug in , by asking the algorithm to first learn the labeled sample ‘smartly’ . Our unlabeled sample complexity ( usc ) will be achieved by an iterative learning process on the unlabeled sample , for any given sc , as described in in Algorithm 1 . For convenience , the labeled sample is assumed to be contained in the initial training set S . Whenever this occurs , H(S ) will represent the set of hypotheses not only compatible on the unlabeled sample in S , but also consistent on the labeled sample in S . Our main result is stated as follows .
Theorem 1 . Let D be α expanding with respect to H , and S and Su be the labeled and unlabeled samples in the training set S respectively . Suppose all hypotheses adopt a γ fusion strategy . Then , for any , δ > 0 and 1 > κ > 0 , and any hypothesis h returned by Algorithm 1 , er(h ) ≤ with proba bility at least 1 − δ if |S| = O , 1
and log |H| + log
1 t
6 log2 δ
,
( 7 )
|Su| ≥ log2
· 2ξ
1 t 1−κ
1/α+1/α2 , where t = min{
κ
1/α+γ } .
In essence , Theorem 1 states that given any labeled sample and its induced τ , one could always use an unlabeled sample of size O(log 1/ ) to find a hypothesis whose error is bounded by a small . Here , should small enough so that /(1/α + γ ) ≤ ( 1 − κ)/(1/α + 1/α2 ) and thus t = /(1/α + γ ) . Later we show τ is an initial error induced by the labeled sample S . If S is large or representative so as to induce a big τ , then our usc applies to a larger range of , and vice verse . Fortunately , in reality we often focus on small .
The impact of the rest parameters are clear : a greater expanding degree α leads to a smaller usc , and a more
View OneView Two0110View OneView Two0110View OneView Two0110View OneView Two0110F2F5F6F8F1F4F3F7629 Algorithm 1 Learning unlabeled sample iteratively
Input : Unlabeled Data Pool Q , Training Set S for i = 1 to n do
1 : Select k data from Q ∩ Θ(H(S ) ) 2 : Add selected data to S and remove from Q end for Output : Hypothesis h ∈ H(S ) accurate fusion strategy leads to a smaller usc Moreover , a greater α strengthens the impact of γ , suggesting that the more expanding a distribution is , the more attention one should pay to designing an accurate fusion strategy . 3.1 Justification of the Main Theorem
In this section , we present a number of lemmas that lead to the conclusion of Theorem 1 . Since our focus is on unlabeled data , the labeled sample will be first abstracted away and their influence will be encoded as an initial bound 0 ∈ [ 0 , 1 ] such that χF ( h,D ) ≤ 0 for all hypotheses h . Clearly , a larger and representative labeled sample implies a smaller 0 and vice verse . In particular , 0 = 1 implies no ( representative ) labeled sample is used for training .
Our justification begins with an observation on a general connection between hypothesis error and its incompatibility . 311 Connecting error and incompatibility Through inspection , we have noticed an effort of establishing a connection between generalization error and view incompatibility . Sanjoy et al [ 16 ] showed that if a partial hypothesis 2 largely agrees on a sufficiently sampled unlabeled set , whose views are conditional independent , then the view incompatibility of that hypothesis approximates its generalization error from above . A natural follow up question is whether the connection holds for regular hypotheses compatible on any unlabeled sample . Moreover , the conditional view independence has been argued to be too strong in the multi view learning literature [ 1 , 4 ] , and it is hence wondered if the connection can hold under a weaker condition .
Although not being mentioned by the author , another connection naturally follows in Wang et al ’s [ 27 ] analysis on the sc of active multi view learning . However , their connection holds by essentially assuming the true compatibility of hypothesis is greater than 0.5 , which rules out the most challenging case suggested in the expanding condition , partly defeating its generosity . How to obtain a general connection remains an open question .
Our connection is stated as follows .
Lemma 1 . Let D be α expanding wrt H and S ⊆ X be any sample . Suppose χF ( h,D ) ≤ 0 for all h ∈ H(S ) , and all h adopt some γ fusion strategy . Then , for any h ∈ H(S ) and > 0 , we have
PS{er(h ) > } ≤ 3PS{χ(h,D ) > t} ,
( 8 ) where t = min{
1− 0
1/α+1/α2 ,
1/α+γ } .
Proof . The expanding condition has suggested two cases , and we will study them separately and unify results at last . The first case is easy : with χ(h,D ) ≥ αχF ( h,D ) , by ( 4 ) we have er(h ) > implies χ(h,D ) > ( 1/α + γ)−1 . This 2A hypothesis that does not output classification result when not being confident . connection also follows in [ 27 ] , which assumes er(hi ) ≤ α/8 initially and α ≤ 1 . Here , we notice the expanding condition alone is sufficient to conclude a general connection , which will be completed in our second case study . The second case states that χ(h,D ) ≥ αχT ( h,D ) , where ( 4 ) does not help directly . However , observe that χ(h,D ) = 1−χT ( h,D)−χF ( h,D ) , and initially χF ( h,D ) ≤ 0 . Therefore , if we manage to further upper bound χT ( h,D ) , a lower bound of χ(h,D ) can be obtained . To this end , introduce an auxiliary parameter > 0 and consider two cases : if χT ( h,D ) < , then χ(h,D ) ≥ 1 − − 0 ; if χT ( h,D ) > , then by the expanding condition χ(h,D ) > α .
Wrapping up above discussions , we have proved that if er(h ) > , then χ will be lowered bounded by at least one of the following three values : ( 1/α + γ)−1 , 1 − − 0 , or α . Our next intention is to unify them by choosing a proper value for , so that one of above bounds is always below others . We choose to solve α ≤ ( 1/α + γ)−1 and 1+γα} . By α ≤ 1 − − 0 , which gives ≤ min{ 1− 0 1+1/α , setting = min{ 1− 0 1+γα} , we have α = min{α , 1 − 1+1/α , − 0 , ( 1/α + γ)−1 } , and thus er(h ) > always implies α . It is easy to verify that ≤ 1 . Taking in the random S and by a union bound of three cases proves the lemma .
In essence , Lemma 1 states that under mild conditions , if the incompatibility of a hypothesis is sufficiently small , its generalization error must also be small . One may notice the truncation of usc in Theorem 1 terms from here . In addition , Lemma 1 induces a transformation from the problem of removing high error hypotheses to the problem of removing high incompatibility hypotheses . To achieve an efficient usc , it remains to prove the latter problem admits an efficient sample complexity , which we defer to Section 313 In the next section , we will present evidence on the boundedness of incompatible coefficient , which will be used to characterize our usc 312 Bounding incompatible coefficient In this section , we show the incompatible coefficient ξ can be bounded in terms of its single view counterpart disagreement coefficients [ 19 ] , which has been proved bounded under mild conditions . To do so , additional machinery is needed . Notice probability P induces two ( single view ) probabilities Q1 and Q2 that are defined on X1 and X2 respectively . Then , the generalization error of any single view hi ∈ Hi is er(hi ) = Qi{xi ∈ Xi : hi(xi ) = h i ( xi)} . ∗
( 9 )
All related single view notions in [ 19 ] are listed as follows : for any view i , a r ball centered at hi with radius r is B(hi , r ) = {fi ∈ Hi : Qi{x ∈ Xi|fi(x ) = hi(x)} ≤ r} . ( 10 ) The disagreement region of Hi is
Θi(Hi ) = {x ∈ Xi : ∃fi , gi ∈ Hi , fi(x ) = gi(x)} .
( 11 )
And the disagreement coefficient at hi is
θhi = sup r>0
Qi(Θi(B(hi , r)))/r .
( 12 )
Note that θhi is defined for a specific hypothesis hi , and . We will connect ξ the disagreement coefficient of Hi is θh∗ with θh∗ Claim 1 . 2χF ( h,D ) + χ(h,D ) = er(h1 ) + er(h2 ) .
’s . The following claim helps our proof . i i
630 Proof Sketch . It can be proved by splitting events in χ and regrouping them with χF ’s . Define events
ω∧(x ) := h is wrong on both x1 and x2 , ω1(x ) := h is wrong on x1 but right on x2 , ω2(x ) := h is wrong on x2 but right on x1 .
It is clear that χ(h,D ) = P{x : ω1(x)} + P{x : ω2(x)} and 2χF ( h,D ) = 2P{x : w∧(x)} . By regrouping probabilities , we have P{x : ω1(x)} + P{x : w∧(x)} = P{x : h is wrong on x1} = Q1{x : h is wrong on x1} = er(h1 ) . Applying similar arguments to er(h2 ) and combining both results prove the claim .
For convenience , abbreviate θh∗ as θi , and construct a function π0.5 so that π0.5(γ ) = γ if γ ≤ 0.5 and π0.5(γ ) = 1 − γ if γ > 05 Then , the connection between ξ and θi ’s is Lemma 2 . Under the settings in Lemma 1 , i
ξ ≤ max{c(θ1 + θ2 ) ,
1/α + 1/α2
1 − 0
} ,
( 13 )
−1 . where c = 1 + ( α · π0.5(γ ) ) Proof . It is easier to consider γ ≤ 0.5 and γ > 0.5 separately . Besides , since their arguments are similar , we will focus on proving the first case . Assume γ ≤ 05 By definition ( 4 ) and Claim 1 , we have er(h ) ≥ 2γχF ( h,D ) + γχ(h,D ) = γ ( er(h1 ) + er(h2 ) ) .
Thus , for any r > 0 , the event er(h1 ) > r/γ or er(h2 ) > r/γ implies er(h ) > r , which further implies χ(h,D ) > t for some t ( as a function of r ) by Lemma 1 . Notice that by definition χ(h,D ) ≤ t means h ∈ A(t ) , and er(hi ) ≤ r/γ means hi ∈ B(h∗ h ∈ A(t ) implies h1 ∈ B(h∗ i , r/γ ) . By contrapositive , we have 1 , r/γ ) ∧ h2 ∈ B(h∗
2 , r/γ ) .
As a result ,
Θ(A(t ) ) ⊆ Θ ( B(h 1 , r/γ ) × B(h ∗ ⊆ Θ1(B(h 1 , r/γ ) ) ∪ Θ2(B(h ∗ ∗ 2 , r/γ) ) ,
∗ 2 , r/γ ) ) where the second line is by Proposition 2 , which we defer to Section 3.2 for it is interesting in its own right . Then ,
∗ 2 , r/γ) ) )
P(Θ(A(t) ) ) ≤ P ( Θ1(B(h 1 , r/γ ) ) ∪ Θ2(B(h ∗ ≤ Q1(Θ1(B(h ∗ ∗ 1 , r/γ) ) ) + Q2(Θ2(B(h 2 , r/γ)) ) , where the second inequality is by definition of Qi ’s . We are now ready to bound ξ . Recall in Lemma 1 , we have proved er(h ) > r implies χ(h,D ) > t where t = 1/α+γ } . Consider two cases of r : if r is large min{ enough , then t becomes a constant , ie t = 1− 0 1/α+1/α2 . Then it follows that
1/α+1/α2 ,
1− 0
( 14 ) r
P(Θ(A(t) ) )
1/α + 1/α2
ξ = sup t>0
1 − 0 On the other hand , if r is small so that t = r
= t
.
≤ 1 t
Q1(Θ1(B(h∗
1/α+γ , by ( 14 )
P(Θ(A(t) ) ) t
≤ c
1 , r
γ ) ) )
Q2(Θ2(B(h∗ r/γ
2 , r
γ ) ) )
+ r/γ where c = 1 + ( αγ)−1 . Taking supremum on both sides of the inequality yields ξ ≤ c(θ1 + θ2 ) .
Combining both cases of r proves the lemma for γ ≤ 05
When γ > 0.5 , we have er(h ) ≥ ( 1 − γ)(er(h1 ) + er(h2) ) , and the rest arguments are similar to the first case . Combining two cases of γ completes the proof .
Since it is reasonable to assume any fusion strategy is better than random guess , ie γ ≤ 0.5 , the constant c in Lemma 2 can be further pruned to c = 1 + ( αγ)−1 . In the next section , we will use ξ to characterize the sample complexity of searching for low incompatibility hypotheses . Searching for low incompatibility hypotheses 313 In this section , we will prove that an unlabeled sample of size O(ξ log 1/ ) suffices to find , with high probability , a hypothesis with incompatibility no greater than . Our analysis follows the strategy of analyzing disagreement based active learning [ 20 ] , ie we let searching proceed in rounds , where in each round only data in the incompatible region will be used for training . Our own technical challenge is that no labeled data is involved , and thus existing supervised bounds ( eg[22 ] ) cannot be applied as in analyzing active learning . As a remedy , we propose the following claim , whose proof follows a standard PAC argument and is omitted here . Claim 2 . Let S be a random unlabeled sample and h ∈ H(S ) be any hypothesis . For any , δ , with probability at least 1−δ , χ(h,D ) ≤ if m ≥ 1
,log |H| + log 1
.
δ
Then , the usc of finding low incompatibility hypothesis can be stated as Lemma 3 . Let Su ⊆ S be the unlabeled training sample . For any , δ > 0 and all h ∈ H(S ) returned by Algorithm 1 , with probability at least 1 − δ , we have χ(h,D ) ≤ if
|Su| ≥ log2
· 2ξ
1 log |H| + log
1 log2 δ
.
( 15 )
Proof . First , notice for any h ∈ H(S ) , any data x satisfying h1(x ) = h2(x ) is in Θ(H(S) ) , which implies χ(h,D ) ≤ P(Θ(H(S)) ) . Hence to bound the former , it suffices to bound P(Θ(H(S)) ) . Let S be an update of S after one round in Algorithm 1 . Clearly H(S ) ⊆ H(S ) . Then for any h ∈ H(S ) ,
χ(h,D ) = P(h1(x ) = h2(x ) )
= P(h1(x ) = h2(x ) | Θ(H(S) ) ) · P(Θ(H(S) ) )
+ P(h1(x ) = h2(x ) | Θ(H(S) ) ) · P(Θ(H(S) ) ) = P(h1(x ) = h2(x ) | Θ(H(S) ) ) · P(Θ(H(S)) ) , where Θ ( respectively , Θ ) represents event x /∈ Θ ( respectively , x ∈ Θ ) , and the third equality holds because by definition P(h1(x ) = h2(x ) | Θ(H(S) ) ) = 0 .
Based on Claim 2 , by setting k =2ξ,log |H| + log 1 δ ,
P(h1(x ) = h2(x)|Θ(H(S) ) ) ≤ ( 2ξ )
−1 ,
, with probability at least 1 − δ , for any δ > 0 . This implies H(S ) ⊆ A(r ) by Definition 4 , where r = P(Θ(H(S)))/2ξ . Consequently , P(Θ(H(S
) ) ) ≤ P(Θ(A(r) ) ) ≤ ξr = P(Θ(H(S)))/2 , where the second inequality is based on Definition 4 .
631 Till now , we have proved that the incompatible region can be reduced efficiently at each round , and it remains to specify the number of rounds n for achieving . Clearly , it suffices to run n = log2(1/ ) rounds , each with confidence 1 − δ . By a union bound , the overall confidence is δ = δ . Solving it for δ , plugging the result back in k , and log2 setting m ≥ nk proves the lemma . 314 Wrapping up Combining Lemma 1 and Lemma 3 immediately yields a
1 self standing usc for multi view learning . Corollary 1 . Let D be α expanding wrt H , and Su be the unlabeled sample contained in the training set S . Suppose χF ( h,D ) ≤ 0 and all hypotheses adopt a γ fusion strategy . Then , for any , δ > 0 and any hypothesis h returned by Algorithm 1 , er(h ) ≤ with probability at least 1 − δ if
|Su| ≥ log2
· 2ξ
1 t 1− 0 where t = min{
1/α+1/α2 ,
1/α+γ } . log |H| + log
1 t
3 log2 δ
,
( 16 )
From Corollary 1 , it is clear that the role of labeled sample is to set an upper bound of χF , ie to specify the value of 0 . Identifying the sc for this problem is equivalent to identifying the sc for single view supervised learning . Replacing 0 with κ and halving confidence δ , one half for usc and another half for sc , prove Theorem 1 .
4 . ALGORITHMIC ISSUE
Our theoretical study highlights the importance of incompatible region , which turns out to be the key for Algorithm 1 to achieve an efficient unlabeled sample complexity . In this section , we present two propositions that provide guidance on detecting data in this region algorithmically . Ideally , to determine whether a data x is in Θ(H ) , one has to search through all h ∈ Θ(H ) . This may be expensive or even impossible in many cases , eg when hypothesis space is infinite . The following proposition implies that searching through a random hypothesis sample is also acceptable . For rigorousness , assume a probability measure µ defined on H and all sets considered are measurable . The confidence parameter δ will be hidden in the notation ˜O . Proposition 1 . Let R = ( H , Σ ) be a range space with V Cdimension d < ∞ , where Σ is the collection of incompatible regions of all h ∈ H . For any sample B ⊆ H of size η ) and any x ∈ X , if B is compatible on x , then ˜O( d with probability at least 1 − δ , µ{h ∈ H : h is incompatible on x} ≤ η .
η log d
( 17 )
Proof Sketch . Our argument is based on the generation of an net in the hypothesis space . According to [ 21 ] , with probability at least 1 − δ , B will form an η net , hitting all ‘heavy’ incompatible regions , where one region is the set of hypotheses incompatible on one data . Therefore , if data x manages to miss the hit , ie B is compatible on x , then the incompatible region induced by x must be small . The specific bound follows directly from [ 21 ] .
Proposition 1 states that a sufficiently sampled B is very unlikely to miss data in the incompatible region . Even if it does , the data being missed are less likely to be helpful in reducing H for their induced incompatible regions are small . In fact , we may say that , if B is not sufficiently sampled , it may miss more data , but the data it detected are more efficient in reducing H .
The second proposition we proposed helps to address the problem when examining both views of data is expensive or even impossible , such as in the missing view cases . It establishes a connection between the incompatible region Θ and the disagreement region Θ1 and Θ2 of each single view . Proposition 2 . x ∈ Θ(H ) if and only if x1 ∈ Θ1(H1 ) or x2 ∈ Θ2(H2 ) , where the disjunction is inclusive . Proof . One direction is obvious : suppose x1 ∈ Θ1(H1 ) , then there are f1 , g1 ∈ H that f1(x1 ) = g1(x1 ) . Hence one of them , eg f1 , must disagree with any given f2 ∈ H2 on x . Since ( f1 , f2 ) ∈ H , x ∈ Θ(H ) . The other direction holds in the realizable setting . Suppose x ∈ Θ(H ) . Then there is an h ∈ H incompatible on x . This implies h makes mistake on at least one view , eg h1(x ) = h∗
1 ∈ H1 , we have x1 ∈ Θ1(H1 ) .
1(x ) . Since h∗
Proposition 2 points out a sufficient condition of detecting data in the incompatible region , that is , if any view of a data is in the single view disagreement region , then that data must be in the incompatible region . This would be helpful in problems with missing view data , where one could examine only the observed views to find a set of data in the incompatible region . However , such strategy does not guarantee to find all data in the incompatible region .
5 . EMPIRICAL VERIFICATION 5.1 Experimental Setup
We verify our theory on the CBCL Face Database 1 developed by MIT Center For Biological and Computation Learning [ 2 ] . The data set contains 6997 gray scale images of size 19 × 19 , divided into face images and non face images . The data set was split into a training set containing 2429 faces and 4548 non faces , and a test set containing 472 faces and 23573 non faces .
Our training set is a subset of the given training sample , consisting of all 2429 faces images but only the first 2799 non face images for constructing a more balanced ( face vs non face ) training set . Similarly , our test set consists of all 472 face images and the first 548 non face images from the original test set . Two views are generated from the data set . The first view is the original 361 dimensional density vector of each image , and the second view is the 36 dimensional HOG feature vector [ 15 ] extracted from each image . 5.2 Active View Sensing
Active view sensing [ 29 ] considers multi view learning with missing view data , and aims at improving learning by actively collecting certain missing views . It is a wrapper algorithm that proceeds in an iterative manner : in each round , the current hypothesis will help to select a candidate sample whose missing views will be completed ; then , a new hypothesis will be returned based on the updated training sample . From the face training set , we select 200 and 400 data and treat them as labeled sample , and treat the rest as unlabeled sample . All labeled data are assumed complete , while all unlabeled data are assumed to have one random view missing .
632 ( a ) |L| = 200
( b ) |L| = 400
( a ) k=10
( b ) k=20
Figure 3 : Test error versus number of sensing iterations .
Figure 4 : Test error versus number of online iterations .
To determine whether a missing view data is in the incompatible region , we rely on Proposition 2 . The hypothesis pool is sampled by the following protocol : first randomly assign pseudo labels to a subset of unlabeled sample and add them to the labeled sample , then return a hypothesis consistent on this augmented labeled sample . By repeating this process , a hypothesis pool can be sampled . Coregularization [ 24 ] is used as the base multi view learner for its efficiency , and is initialized by the labeled sample .
To verify the importance of incompatible region ( ir ) , we divide the candidate sample selected by the existing maximal predictive variance criterion into two groups , such that one group contains the candidates in the ir , and the other contains candidates outside the ir Two sensing algorithms are compared : one is only updated with candidates in ir , marked as P V ∧Incompatibility ( PVI ) , and the other is only updated with candidates outside ir , P V ∧ Compatibility ( PVC ) . Our hypothesis is that PVI should converge faster than PVC . Figure 3 reports the performance of two compared algorithms versus the number of sensing iterations , averaged over 10 trails .
From Figure 3 , we observe both PVI and PVC improve as more missing views are being collected . It is clear that PVI converges faster than PVC . It seems that when the labeled sample is smaller ( |L| = 200 ) , the gap between their convergence rate decreases . It may be because for small labeled sample , the consistent hypothesis space remains large , and the hypothesis pool is less likely to be sample sufficiently . As a result , more data from the incompatible region will be mis grouped into PVC , which improves its convergence rate and hence bridges its gap to PVI .
From another perspective , when labeled sample is large and hence hypothesis pool more sufficiently sampled , PVC will be updated largely by data outside ir In this case , we may expect it to converge slowly . By comparing the convergence rate of PVC in Figure 3(a ) and 3(b ) , this seems to be indeed the case .
5.3 Online Co Regularization
Online co regularization [ 17 ] considers learning through the training set in an online fashion . Learning proceeds in rounds , where in each round , a batch of data is selected to update the model .
In the training set , we select 100 labeled examples as the labeled sample , and the rest are treated as the unlabeled sample . The labeled sample is used to initialize the model , and only unlabeled examples are selected at each round for update based on the formula proposed in [ 17 ] , ie i = ( 1 − ηt ht+1 i )ht i − 4µηt i x∈S|k| i(x ) − ht
( ht i ( x))xi , i(x ) = wT where ht i x is the hypothesis on view i at round t , and µ , ηi are regularization coefficient and learning rate , and S|k| is an unlabeled sample of size k . Index i is the complement of index i in {1 , 2} , ie i = 1 when i = 2 and i = 2 when i = 1 .
To verify the importance of incompatible region , we compare two algorithms that use different strategies to select the data for update . One is to randomly select data for update , marked as RandomSubset(RS ) , while the other is to select data in the incompatible region for update , marked as IncompatibleSubset(IS ) . Their test errors versus the number of rounds are presented in Figure 4 .
It is clear that algorithm IS converges faster than RS . Moreover , as the labeled sample becomes larger , the gap between two algorithms decreases . This may be because a larger random sample is more likely to contain more data from the incompatible region . As an extreme case , when both algorithms choose all unlabeled data in one round , then they would perform exactly the same . Since we have fixed the parameters of both algorithms to be the same , it seems that RS is more vulnerable to local optimum . Here , although the connection between learning and optimization is not very clear , we conjecture that random data is less efficient in pruning the model .
6 . DISCUSSIONS
At the early stage of investigation , we realized a straightforward strategy to analyze usc , that is , first assign pseudolabels to unlabeled sample under certain confidence , and then perform regular sc analysis within that confidence . However , in reality the unlabeled sample is usually too large to endorse this strategy , eg for an unlabeled sample of merely size 10 , a hypothesis with labeling confidence as high as 0.9 would could only draw conclusions within confidence 10 · 0.9 = 0.09 , which is hardly informative . Comparatively , our strategy was to establish a general connection between error and incompatibility , which did not involve ( and thus deteriorate ) learning confidence .
A strategy being applied to analyze multi view unlabeled data is to assume the hypothesis class is never confident but wrong about its classification . In [ 6 , 27 ] , such assumption seems to suggest an usc of O(log 1/ ) . However , we argue that these results are essentially sc , because the true label sets of their unlabeled samples are ‘known for free’ . Certainly , the assumption was tailored to capture the na
5010015030354045Number of IterationsTest Error ( % ) PV ∧ CompatiblePV ∧ Incompatible0100200300400303234Number of IterationsTest Error ( % ) PV ∧ CompatiblePV ∧ Incompatible050100150212325Number of IterationsTest Error ( % ) Random SubsetIncompatible Subset020406080100192123Number of IterationsTest Error( % ) Random SubsetIncompatible Subset 633 ture of co training that fixs the pseudo labels , and clearly the algorithm would be biased away from the target hypothesis even if one pseudo label is incorrect . In this paper , our analysis did not make such strong assumption , yet still manage to achieve an O(log 1/ ) usc What freed us from the restriction is we did not assume pseudo labels were fixed . All Algorithm 1 asked was to be compatible at each round . A true usc of semi supervised multi view learning was presented in [ 5 ] ( Theorem 15 ) , where a trade off was suggested between the O(1/ ) usc and the O(log1/ 1/δ ) sc In this paper , our result suggested that , for small , the usc can be further improved without sacrificing sc
Unlabeled data is widely known to help learning in large quantity [ 30 ] , and we noticed most single view usc [ 25 , 26 , 9 , 8 , 5 ] have merely achieved O(1/ ) or even worse . Under certain conditions , some [ 11 , 12 ] suggested that unlabeled sample should be exponentially larger than labeled sample to reach the same level of error . It might be interesting to see if our analysis could be generalized to the single view case and suggest improved results .
While the theory is interesting in its own right , it also sheds some light on algorithm design . Our results suggested to update the learner in an iterative manner , and used unlabeled data from the incompatible region for update in each round . This can be regarded as the ‘unsupervised’ version of disagreement based active learning [ 19 ] . The theorem also suggested one should pay more attention to designing an accurate fusion strategy when the sample distribution is largely expanding . Ideally these implications apply to any semi supervised multi view learner that admits an iterative learning process . However , the implication of our results on unsupervised multi view learning remains unclear .
7 . CONCLUSION
In this paper , we improved the state of art unlabeled sample complexity ( usc ) from O(1/ ) to O(log 1/ ) for semisupervised multi view learning , without sacrificing the labeled sample complexity ( sc ) We proved a general connection between the generalization error of a hypothesis and its incompatibility under mild conditions . Our results highlighted the importance of incompatible region in achieving an efficient usc , and empirical study supported the claim .
8 . REFERENCES [ 1 ] S . Abney . Bootstrapping . In ACL . ACM , 2002 . [ 2 ] M . Alvira and R . Rifkin . An empirical comparison of snow and svms for face detections . Tech.Report , 2001 .
[ 3 ] M R Amini , N . Usunier , C . Goutte , et al . Learning from multiple partially observed views an application to multilingual text categorization . In NIPS , 2009 . [ 4 ] M F Balcan and A . Blum . A pac style model for learning from labeled and unlabeled data . In Learning Theory , pages 111–126 . Springer , 2005 .
[ 5 ] M F Balcan and A . Blum . A discriminative model for semi supervised learning . Journal of the ACM , 57(3):19 , 2010 .
[ 6 ] M F Balcan , A . Blum , and K . Yang . Co training and expansion : towards bridging theory and practice . In NIPS , 2004 .
[ 7 ] M F Balcan , S . Hanneke , and J . W . Vaughan . The true sample complexity of active learning . Machine learning , 80(2 3):111–139 , 2010 .
[ 8 ] N . Balcan , C . Berlind , S . Ehrlich , and Y . Liang .
Efficient semi supervised and active learning of disjunctions . In ICML , 2013 .
[ 9 ] S . Ben David , T . Lu , and D . P´al . Does unlabeled data provably help ? worst case analysis of the sample complexity of semi supervised learning . In COLT , 2008 .
[ 10 ] A . Blum and T . Mitchell . Combining labeled and unlabeled data with co training . In COLT , 1998 .
[ 11 ] V . Castelli and T . M . Cover . On the exponential value of labeled samples . Pattern Recognition Letters , 16(1):105–111 , 1995 .
[ 12 ] V . Castelli and T . M . Cover . The relative value of labeled and unlabeled samples in pattern recognition with an unknown mixing parameter . Information Theory , IEEE Transactions on , 42(6):2102–2117 , 1996 .
[ 13 ] C . Christoudias , R . Urtasun , and T . Darrell .
Multi view learning in the presence of view disagreement . In UAI , 2008 .
[ 14 ] M . Culp and G . Michailidis . A co training algorithm for multi view data with applications in data fusion . Journal of chemometrics , 23(6):294–303 , 2009 . [ 15 ] N . Dalal and B . Triggs . Histograms of oriented gradients for human detection . In CVPR , 2005 .
[ 16 ] S . Dasgupta , M . L . Littman , and D . McAllester . Pac generalization bounds for co training . NIPS , 2002 .
[ 17 ] T . de Ruijter , E . Tsivtsivadze , and T . Heskes . Online co regularized algorithms . In Discovery Science , 2012 . [ 18 ] F . Fogelman Souli´e et al . Large scale semi supervised learning . Mining Massive Data Sets for Security , 2008 . [ 19 ] S . Hanneke . Theory of disagreement based active learning . Foundations and Trends Rfl in Machine Learning , 7(2 3):131–309 , 2014 . [ 20 ] S . Hanneke . Theory of disagreement based active learning . Foundations and Trends Rfl in Machine Learning , 7(2 3):131–309 , 2014 .
[ 21 ] D . Haussler and E . Welzl . Epsilon nets and simplex range queries . Discrete and Computational Geometry , 2(1):127–151 , 1987 .
[ 22 ] J . Shawe Taylor . Neural network learning : Theoretical foundations . AI Magazine , 22(2):99 , 2001 .
[ 23 ] V . Sindhwani and S . S . Keerthi . Large scale semi supervised linear svms . In SIGIR , 2006 .
[ 24 ] V . Sindhwani , P . Niyogi , and M . Belkin . A co regularization approach to semi supervised learning with multiple views . In ICML Workshop on Learning with Multiple Views , 2005 .
[ 25 ] A . Singh , R . Nowak , and X . Zhu . Unlabeled data : now it helps , now it doesn’t . In NIPS , 2009 .
[ 26 ] K . Sinha and M . Belkin . The value of labeled and unlabeled examples when the model is imperfect . In NIPS , 2007 .
[ 27 ] W . Wang and Z H Zhou . On multi view active learning and the combination with semi supervised learning . In ICML , 2008 .
[ 28 ] C . Xu , D . Tao , and C . Xu . A survey on multi view learning . arXiv preprint arXiv:1304.5634 , 2013 .
[ 29 ] S . Yu , B . Krishnapuram , R . Rosales , and R . B . Rao .
Active sensing . In AISTATS , 2009 .
[ 30 ] X . Zhu . Semi supervised learning literature survey .
2005 .
634
