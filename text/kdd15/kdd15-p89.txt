Online Outlier Exploration Over Large Datasets
Lei Cao† , Mingrui Wei † , Di Yang ‡ , Elke A . Rundensteiner†
Worcester Polytechnic Institute Worcester , MA 01609 , USA
†
‡
Oracle Corporation Nashua , NH 03062 , USA lcao|netwrm01|rundenst@cswpiedu,diyang@oraclecom
ABSTRACT Traditional outlier detection systems process each individual outlier detection request instantiated with a particular parameter setting one at a time . This is not only prohibitively time consuming for large datasets , but also tedious for analysts as they explore the data to hone in on the appropriate parameter setting and desired results . In this work , we present the first online outlier exploration platform , called ONION , that enables analysts to effectively explore anomalies even in large datasets . First , ONION features an innovative interactive anomaly exploration model that offers an “ outliercentric panorama ” into big datasets along with rich classes of exploration operations . Second , to achieve this model ONION employs an online processing framework composed of a one time offline preprocessing phase followed by an online exploration phase that enables users to interactively explore the data . The preprocessing phase compresses raw big data into a knowledge rich ONION abstraction that encodes critical interrelationships of outlier candidates so to support subsequent interactive outlier exploration . For the interactive exploration phase , our ONION framework provides several processing strategies that efficiently support the outlier exploration operations . Our user study with real data confirms the effectiveness of ONION in recognizing “ true ” outliers . Furthermore as demonstrated by our extensive experiments with large datasets , ONION supports all exploration operations within milliseconds response time .
Categories and Subject Descriptors H.2 [ Information Systems ] : Database Management
Keywords Outlier ; Online Exploration ; Parameter Setting
1 .
INTRODUCTION
This big data era provides tremendous opportunities for extracting insights from big datasets via advanced analytics . Among these data analytical tasks , understanding “ abnormalities ” in the data is one of the fundamental services essential for applications ranging from credit fraud prevention , climate change analysis , to financial strategy planning . They all rely on effective outlier detection techniques to discover suspicious card usage and potential identity theft , to forecast disastrous weather phenomena , and to predict market changes and trade opportunities , respectively [ 7 ] .
In this context , we focus on one well established abnormality definition [ 13 ] called “ distance based outlier ” , that effectively captures “ outliers ” [ 7 , 16 ] − data points behaving significantly differently from others in a dataset .
Limitations of Traditional One At A Time Query Approach . Traditional outlier detection systems require the analyst to select a fixed set of parameter values , most notably a distance threshold r and a count threshold k [ 13 ] , and then to submit this instantiated request to attempt to detect outliers of interest . This request is then executed from scratch as a one time query to compute the outliers from the target dataset that match that specification . This one at atime query approach suffers from severe limitations .
First , similar to many other data analytical tasks , a good input parameter setting ( in this case a pair of appropriate values for k and r parameters ) is the key for the analysts to gain insight in data and identify the “ true ” outliers . However using the current systems , to achieve this the analyst has to continuously re submit individual requests with different parameter settings in a trial and error fashion and interactively analyze the respective results . This is extremely ineffective and would be a taxing process for the analysts because of the infinite number of possible parameter settings .
Second , although optimization strategies for executing such requests have been proposed [ 1 , 4 , 11 , 5 ] , mining outliers according to a particular parameter setting from scratch on large data still tends to take hours as confirmed in our experiments ( Sec 431 ) This is clearly not matching the stringent response time of seconds or so required by interactive systems − thus risks losing the attention of the analysts during the process .
Worst yet , if the system only supports such one at a time queries , each individual query would independently generate an outlier set as answers . However without establishing an explicit connection among these “ isolated outlier views ” , it is challenging for the analyst to compare and contrast the outlier sets produced by the different queries over time − especially when working with a big dataset and in turn a large outlier base .
Furthermore important insights , such as how stable the outlier status of each point is , how the detected outlier set migrates across different parameter settings , or what the relationship among different outlier points is ( for example , whether some points are “ stronger ” outliers than others ) , might be missed during this tedious yet expensive exploration process . This information is critical for the analysts to interpret the characteristics of the outliers hidden in the
89 dataset . In short , this one at a time approach is neither effective nor efficient for online interactive analytics .
Proposed Solution . We propose a novel online outlier exploration platform , called ONION , that addresses these problems .
ONION Model . ONION offers users an innovative “ outliercentric panorama ” into the outliers present within the raw dataset by establishing an interactive anomaly exploration model . The ONION model is composed of a comprehensive knowledge base supported by powerful outlier exploration operations .
First , the ONION knowledge base explicitly models the distribution of the outliers with respect to their associated parameter settings by abstracting the k , r parameters and the points p of a dataset D into a three dimensional space called ONION space ( in short OSpace ) . Further higher level abstractions ( P Space and D Space ) including the stability of the parameter settings in recognizing outliers and the hierarchial domination relationships in abnormality among the outlier candidates independent of any particular parameter setting are extracted and modeled .
Second , rich classes of outlier exploration operations beyond the traditional concept of outlier detection are proposed that not only allow the analysts to understand how parameter changes would impact the captured outliers , but also offer analysts a “ parameter free ” approach to identify outliers based on their domain knowledge . Together these operations provide a powerful yet flexible methodology for analysts to explore and interpret the outliers within a large dataset with respect to an infinite parameter space . This enables them to quickly approach the true outliers even in a completely unknown dataset with zero knowledge about the appropriate parameter settings .
ONION Framework . We develop a novel outlier exploration framework that implements this ONION model . The ONION framework consists of two components , namely offline one time ONION knowledge base construction and subsequent interactive outlier exploration . The offline component constructs the ONION knowledge base , while leveraging this knowledge base the interactive component achieves true sense making by supporting the outlier exploration operations with real time responsiveness .
Our offline component rests upon the key observation that given any outlier candidate oci in a dataset D there exists a small set of data points in D whose distances to oci delimit the entire parameter space P composed of all possible parameter settings into two segments , so called space delimiter . The parameter settings that fall into the same segment classify pi to be the same outlier status . By collecting the space delimiters of all outlier candidates utilizing a quadratic complexity algorithm and leveraging their relative positions we successfully represent the key components of outlier analytics , namely the distribution characteristics of outliers , the property of parameter space , and the linkage between outliers and input parameter settings into a compact hierarchical structure .
This structure is proven to be sufficient yet necessary to support all outlier exploration operations , while being compact enough to be stored in the main memory of a standard configuration PC even for a big dataset . Therefore the DISK I/O costs , confirmed to be the dominating costs of outlier detection , are completely avoided by our interactive explorer . In particular by leveraging the relative abnormality of the outlier candidates and the relative strictness of the parameter stable regions in recognizing outliers , we are able to answer all outlier exploration operations in logarithmic time on the cardinality of the outlier candidates .
Our user study with real GMTI data [ 9 ] confirms the effectiveness of our ONION platform in quickly honing in on the appropriate parameter settings and in turn approaching the “ true ” outliers . Furthermore ONION consistently outperforms its state of the art competitors at least five orders of magnitude in the processing times for the traditional outlier detection queries in a rich diversity of scenarios tested with big real geolocation datasets . Better yet , it supports a rich set of outlier exploration operations not previously supported − all in milliseconds response time .
Contributions . The contributions of this work include : 1 ) We propose the first interactive outlier analytics platform that enables analysts to pinpoint appropriate parameter settings and explore outliers in a systematic way .
2 ) We establish for the analysts an “ outlier centric panorama ” into big datasets by integrating the input data and parameter space into a comprehensive multi space ONION knowledge base .
3 ) We design logarithmic complexity algorithms for the processing of each outlier exploration operations with realtime responsiveness by leveraging the compact ONION knowledge base .
4 ) We confirm the superiority of ONION compared to the traditional mining platform in effectiveness of recognizing true outliers by conducting a user study with real GMTI dataset .
5 ) Our experimental performance study demonstrates that ONION is at least five orders of magnitude faster than its state of the art competitors for traditional outlier detection queries .
2 . ONION MODEL
We propose the online outlier exploration model or in short ONION for modeling and exploring the characteristics of distance based outliers in a dataset D . We first introduce the concept of distancebased outlier proposed in [ 13 ] . We use the term data point or point to refer to a multi dimensional tuple . Let D be a set with n points p1 , p2 , p3 , pn . The function d ( pi , pj ) denotes the distance between data points pi and pj in D .
Definition 1 . Distance Based Outlier . Given a dataset D , a range threshold r ( r ≥ 0 ) and a count threshold k ( k ≥ 1 ) , a point pi ∈ D is an outlier if fewer than k points pj exist in D whose distance to pi d ( pi , pj ) is no larger than r .
Fig 1 sketches a high level view of the ONION model . It is composed of the multi space abstraction capturing the key characteristics and interrelationships of outliers and a rich set of outlier exploration operations .
KͲ^ƉĂĐĞ
KE/KE
KƉĞƌĂƚŝŽŶƐ
KE/KE
KƉĞƌĂƚŝŽŶƐ
Ͳ^ƉĂĐĞ
WͲ^ƉĂĐĞ
Figure 1 : ONION Model
2.1 Multi Space Abstraction
Our multi space abstraction is composed of three interlinked spaces that we will now define below . ONION Space . ONION space or in short O Space is a threedimensional space that models the distribution of the outliers with respect to their associated parameter settings .
Definition 2 . O Space denoted as O
S ( Dimk , Dimr , Dimd ) is a three dimensional space with the possible settings of parameters r , k and data points p in dataset D being its three dimensions . The dimension Dimk ranges over the values that the parameter k can take in the universe of natural number Uk : [ kmin , kmax ] , where kmin and kmax are the user specified lower and upper bounds
90 P Space , partitioning the infinite number of parameter settings into finite number of stable regions , explicitly reveals the influence of the parameter setting adjustment . This offers the analysts an opportunity to determine the appropriate parameter settings using a systematic methodology instead of a random trial and error process . Data Space . Data space or in short D Space leverages the key abnormality properties demonstrated in the points of dataset D , namely outlier candidacy and domination relationship .
Outlier Candidacy . Despite the infinite cardinality of P Space P , given a point pi in dataset D , its outlier status might be constant with respect to all parameter settings in P . In other words , some points are guaranteed to be outliers in the entire P Space so called const outliers , while some other points are guaranteed to be permanent inliers through the entire P Space so called const inliers . In our O Space these points would thus correspond to a slice that is all 1 ’s for constant outlier or all 0 ’s for constant inlier . For example , as shown in Fig 2 p1 is a const inlier , while p3 is a const outlier .
Any point pi in D , that is neither a const outlier nor const inlier , is called an outlier candidate oc with respect to P , meaning pi has opportunity to be classified as outlier for at least some of the parameter setting psi in P . In O Space , an outlier candidate would have at least one cell in its corresponding slice that is 0 ( white ) and one that is 1 ( black ) . In Fig 2 p2 is an outlier candidate .
In practice as confirmed by our experiments ( Sec 4.2 ) , outlier candidates tend to be a strict minority among all points in D . This important outlier candidacy observation allows us to significantly reduce the number of data points to be maintained in D Space . By this , ONION can concentrate the resource utilization on strictly serving these minority outlier candidates , rather than on computing and recording neighborhoods for the general and much larger data population when exploring outliers . Therefore ONION is able to efficiently explore outliers over even big datasets .
Domination Relationship . In dataset D some outlier candidates demonstrate a much stronger abnormality than others independent of any particular parameter setting in P . In other words , some data points dominate others in abnormality as defined below .
Definition 4 . Given a P Space P , outlier candidate oci in dataset
D dominates ocj if for all parameter settings in P ocj is guaranteed to be outlier when oci is classified as outlier .
By Def . 4 if outlier candidate oci dominates ocj , we say that the abnormality of oci is stronger than ocj .
Revealing the domination relationships among outlier candidates ONION offers the analysts an opportunity to better understand several characteristics of the detected outliers from sensitivity to stability . Without such understanding the detected outliers might only be some abstract points indistinguishable from each other for the analysts instead of some true unique abnormal phenomena .
Now we are ready to define our data space or in short D Space .
.
.
Definition 5 . D Space D = D1 D2 ( 1 ) ∀ oci ∈ D , oci is an outlier candidate ; ( 2 ) given any two outlier candidate subsets Di and Dj of D
Dm , such that : fi
( 1 ≤ i , j ≤ m ) , Di
Dj = ∅ .
( 3 ) the outlier candidates in the same group Di are sorted into a linear structure . Given two points ocj and ocl with j , l representing their positions in Di , ocj dominates ocl if j < l .
Figure 2 : ONION Space of the k values . Similarly the dimension Dimr corresponds to the domain of real numbers Ur : [ rmin , rmax ] with rmin and rmax the lower and upper bounds of the values of parameter r . Lastly the dimension Dimd represents all points p ∈ D randomly organized into a linear order . Each point is assigned a position in [ 1,| D | ] . Each coordinate ( ki , ri , pi ) ∈ O maps to a boolean value v ∈ {0,1} indicating whether point pi is an outlier with respect to parameter values ki and ri .
S
In this O Space any combination of k and r values on the dimensions Dimk and Dimr forms a parameter setting psi denoted by psi ( ki , ri ) . Conceptually O Space encodes the outlier status of all points in D with respect to all possible parameter settings .
Since dimension Dimd represents all data points in dataset D , Dimd corresponds to a discrete domain of positions . In other words the three dimensional O Space can be thought as a sequence of two dimensional slices as shown in Fig 2 . Each slice models the outlier status distribution with respect to all possible parameter settings for one particular point pi in dataset D .
Based on this O Space , we further design two additional higher level abstractions called parameter space and data space respectively as shown below .
͙
͙
͙
͙
͙
ŽĐ
ϳ
ŽĐ
ϭϮ
ŽĐ
ϴ
ŽĐ
ϭϯ
ŽĐ
ϭϰ
ŽĐ
ϭϱ
Փ
ϯ
ŽĐ
ϵ
ŽĐ
ϭϬ
ŽĐ
ϭϭ
Փ
Ϯ
ŽĐ
ϭ
ŽĐ
Ϯ
ŽĐ
ϯ
ŽĐ
ϰ
ŽĐ
ϱ
ŽĐ
ϲ
Փ
ϭ
>ŝŶŬĂŐĞ
WͲ^ƉĂĐĞ
Ͳ^ƉĂĐĞ
Figure 3 : P Space & D Space
Parameter Space . Parameter space or in short P Space is based on the observation that despite the infinite number of possible parameter settings , a large range of continuous parameter settings often generate the same set of outliers .
.
.
Definition 3 . P Space P = P1 ( 1 ) given any two parameter setting subsets Pi and Pj of P ( 1 ≤
Pm , such that :
P2 fi i , j ≤ m ) , Pi
Pj = ∅ ;
( 2 ) given any two parameter settings psj and psl in the same Pi
( 1 ≤ i ≤ m ) , psj and psl generate the same set of outliers .
In other words P Space divides the two dimensional space formed by the set of all possible values on the Dimk , Dimr axes into a set of disjoint regions . Within each region no matter how the parameter settings are adjusted , the set of outliers generated from dataset D remains unchanged . Each such region is called a stable region .
In general , leveraging the outlier candidacy and domination relationship properties , D Space partitions all outlier candidates into multiple disjoint groups . Within each group the domination relationship holds among all members of the group , so called domination group . Furthermore the outlier candidates falling in the same
91 domination group are ordered based on the strongness of their abnormality . For example , in Fig 3 , D Space D contains three subspaces D1 , D2 , and D3 . In D1 candidates oc1 to oc6 are ordered by the domination relationship . That is , oc1 dominates other members in D1 . oc2 is dominated by oc1 , but dominates oc3 to oc6 . Linkage between P Space and D Space . Furthermore as shown in Fig 3 , our ONION model explicitly establishes linkages between P Space and D Space , or in short PD linkage .
Definition 6 . PD Linkage . Given a stable region Pi in P Space P and a domination group Dj in D Space D , there exists a link l ( i , j ) connecting Pi to an outlier candidate oct ∈ Dj such that : ( 1 ) ∀ parameter setting psi in Pi , oct is classified as outlier ; ( 2 ) ∀ parameter setting psi in Pi , oct−1 is classified as inlier .
By the domination relationship definition in Def . 4 , if oct is an outlier with respect to psi , then any outlier candidates listed behind oct in Dj ( oct+1 , oct+2 , ) are guaranteed to be outliers . Therefore the PD Linkage explicitly connects the stable regions with their generated outliers . In Fig 3 , stable region P1 is linked to oc1 of D1 , oc8 of D2 , and oc14 of D3 . Based on the links we immediately get the outlier set O1 generated by P1 , that is {oc1 , , oc6 , oc8 , , oc11 , oc14 , oc15 } .
Overall the multi space abstraction explicitly models the distribution of the outliers over all parameter settings , the relationships among the parameter settings , the stability and uniqueness of the outlier candidates . It establishes an innovative “ outlier centric panorama ” into the outliers within dataset D .
2.2 ONION Operations
Based on the multi space abstraction we further envision a rich classes of outlier exploration operations that allow users to explore and interpret outliers as well as pinpoint appropriate parameters .
Definition 7 . Comparative Outlier Analytics ( CO ) . Given an outlier set Oin as input , we report set of outliers OD from dataset D , such that :
( 1 ) ∀ point pi ∈ Oin , pi ∈ OD ; and ( 3 ) ∀ point pi ∈ OD − Oin , if any pj ∈ Oin is classified as outlier with respect to one psl ∈ P , pi is guaranteed to be classified as outlier by psl .
Leveraging the domination relationship in D Space , CO operation returns all outliers dominated by the outliers specified in the input set Oin . CO offers users a “ parameter free ” approach to identify outliers based on their domain knowledge about the dataset . More specifically this CO operation helps analysts to identify outliers in a dataset based on sampling some typical outliers .
Definition 8 . Outlier Centric Parameter Space Exploration ( PSE ) .
Given an outlier set Oin and a δ ( −1 < δ <1 ) as input , report all parameter settings psj ∈ P Space P , such that :
( 1 ) if δ ≥ 0 , psj identifies an outlier set Oj ⊆ Oin where | Oj |
( 2 ) if δ ≤ 0 , psj identifies an outlier set Oj ⊇ Oin where | Oj |
= ( 1 δ ) | Oin | ;
= ( 1 δ ) | Oin | .
Furthermore PSE provides a tool for analysts to examine how changes in parameter settings may impact the resulting outliers . PSE achieves this , for example , by allowing the analysts to apply PSE to ask for the parameter settings that would return around ( 1 δ ) % of Oin as the results and then compare them against the parameter settings that generate Oin .
Definition 9 . Outlier Detection ( OD ) . Given a dataset D and a parameter setting psi as input , outlier detection returns :
( 1 ) all outliers pj ∈ D with respect to psi if psi ∈ P Space P ; or ( 2 ) all points pj ∈ D that are classified as outliers with respect to any parameter setting ∈ P if psi = NULL .
As shown in Def . 9 unlike the traditional distance based outlier definition , OD leverages the outlier candidacy observation of DSpace to allow the input parameter set as NULL . This will return all points that are guaranteed to be outliers with respect to the entire P Space , that is , the constant outliers . Use Case . Those operations in combination provide a powerful tool for analysts to quickly approach the parameter settings appropriate for her application . For example , when facing a new dataset recording stock market transactions , an analyst may not have any experience to be able to appropriately determine values for parameters k and r . However , given her domain expertise , she may be aware that certain records are abnormal ( outliers ) . Then a CO operation can be applied to help her identify all outliers satisfying her intuition . If the analyst finds the volume of the outliers O returned by the CO request too overwhelming , then she could apply PSE to ask for the parameter settings that would return , for example , around 60 % of O as the result by setting δ as 04 Eventually the OD operation is applied to catch the true outliers to her interest .
Note that the above running example we gave here is just one of many combinative usages of our proposed operations . Those operations can be used individually or in other combinations to serve the ever changing outlier analysis demands .
3 . ONION FRAMEWORK
To achieve the ONION model we designed the novel ONION framework . As shown in Fig 4 ONION framework consists of two phases ( a ) offline multi space abstraction construction and ( b ) online exploration operation processing using the corresponding ONION spaces .
Offline ONION Spaces Construction
Online Outlier Exploration
O Space Construction Algorithm
O Space
D Space Construction Algorithm
P Space Construction Algorithm
D Space
P Space
OD O Space PSE O Space CO O Space
OD P Space PSE P Space CO P Space
OD D Space PSE D Space CO D Space
O Space
P Space D Space
PSE leverages the stable region property of P Space and allows analysts to conveniently evaluate the stability of a given outlier set Oin . This is one important indicator of how significant the observed abnormal phenomena is . For example , if we set the δ as 0 , PSE will return all the parameter settings that are guaranteed to generate the outliers identical to Oin , namely a stable region of P . The scope of the returned parameter settings ( the size of the stable region ) represents how stable the outlier set is across P Space .
Figure 4 : ONION Framework
3.1 O Space
311 Offline O Space Construction
As shown in Fig 2 , the three dimensional O Space can be decomposed into a set of two dimensional slices . Each slice corre
92 sponds to the outlier status of one point pi in dataset D with respect to all parameter settings psi in the two dimensional space P formed by the dimensions Dimk and Dimr . Therefore O Space can be established by modeling the outlier status distribution in P for each point pi in dataset D , called O Space(pi ) .
The key insight here is that given a point pi in dataset D , it is not necessary to establish O Space(pi ) by evaluating pi for each possible psi in P . In fact the outlier status of pi with respect to any psi in P can be correctly determined by collecting only a small amount of meta information .
We first introduce our k distance observation . Generally speaking given a set of outlier detection requests with the same parameter value k for Dimk , but random values for Dimr , the outlier status of any point pi for any of those requests can be determined by checking the distance of pi towards one single point in D . This observation is formally defined in Lemma 1 .
Lemma 1 . Given a set of parameter settings Pk ⊂ P , where ∀ two parameter settings psx(kx , rx ) , psy(ky , ry ) ∈ Pk , kx = ky = k , then the outlier status of pi with respect to any psx in Pk is determined by the distance between pi and its kth nearest neighbor pj denoted as Dk pi .
Proof . Given any parameter setting psx ( k , rx ) ∈ Pk , if Dk > rx , pi then by the definition of the kth nearest neighbor , there are at most k 1 other points pj ∈ D whose distance towards pi is not larger than rx . In other words , pi has at most k 1 neighbors . By Def . 1 , pi is an outlier . On the other hand , if Dk ≤ rx , then there are at least pi k points pj with d ( pi , pj ) ≤ rx , namely pj are all neighbors of pi . pi is then classified as an inlier by Def . 1 . Therefore ∀ psx ( k , rx ) ∈ Pk , the outlier status of pi can be correctly determined by comparing rx against Dk pi . Lemma 1 is proven . .
Now we are ready to introduce the space delimiter insight as the foundation for building O Space .
Lemma 2 . Given a dataset D and parameter setting space P , ∀ pi ∈ D the distance set DS(pi ) = {D kx |kmin ≤ kx ≤ kmax } is pi sufficient to determine the outlier status of pi with respect to any parameter setting ps ∈ P .
∪ Pkj
∪ Pkmin+1
∪ Pkmin+2
Proof . P = Pkmin ∪ Pkmax−1 ∪ Pkmax , where Pkj is composed by any psx(kx , rx ) ∈ P with kx = kj ( kmin ≤ kj ≤ kmax ) . Therefore given any ps ∈ P ps is guaranteed to be covered by some Pkj . By Lemma 1 , ∀ps ∈ Pkj the status of pi can be determined by examining Dkj pi . Since Dkj pi ∈ DS(pi ) , therefore DS(pi ) is sufficient to determine the status of pi with respect to any ps ∈ P . Lemma 2 is proven
As shown in Fig 5 this distance set DS(pi ) delimits P into two segments . The parameter settings in different segments will classify pi to different outlier status . Therefore DS(pi ) is called space delimiter of pi . The set of space delimiters { DS(pi ) |pi ∈ D} effectively represents the three dimensional O Space .
Furthermore the space delimiter structure also provides us an ap proach to quickly discover constant inliers and constant outliers .
Lemma 3 . A point pi is a const inlier if Dkmax pi
≤ rmin .
Proof . If the distance to pi ’s kmaxth nearest neighbor is ≤ rmin , then pi has at least kmax neighbors or more even under most restricted neighbor criteria , namely Dimr = rmin . Then pi is an inlier for ps(kmax ,rmin ) that is the most restricted parameter setting in P in terms of recognizing outlier . If pi is not an outlier in the most restricted setting , then of course it cannot be outlier in any part of P . Therefore pi is a const inlier . .
ƌ
ŵĂǆ

͙

ƌ
ŵŝŶ
Ŭ
ŵŝŶ
Figure 5 : Space Delimiter
Ŭ
ŵĂǆ
Lemma 4 . A point pi is a const outlier if Dkmin pi
> rmax .
Lemma 4 can be proven in the similar way of proving Lemma 3 .
Due to space limitation , the proof is omitted .
Naturally any point that is not a const outlier nor a const inlier , is an outlier candidate oc . Among all points only for oc it is necessary to maintain its space delimiter .
Therefore constructing O Space has two tasks , namely : ( 1 ) discovering all ocs and ( 2 ) collecting the space delimiter DS(oc ) for each oc . Intuitively this can be done by first collecting DS(pi ) for each point pi , then locating the constant inliers and outliers by applying Lemmas 3 and 4 . Collecting DS(pi ) is straightforward . We can acquire the k nearest neighbors ( kNN ) of pi by applying any kNN algorithm with k set as kmax . Since we only care for the range kmin to kmax , we then discard the kmin−1 nearest neighbors .
However to discover const inliers it is not necessary to acquire the actual kmax nearest neighbors . Once pi acquires kmax neighbors whose distance to pi is not larger than rmin , pi is guaranteed to be const inlier . Then the kNN search can be terminated immediately . Since const inliers are typically the majority of the dataset , this optimization significantly speeds up the preprocessing process . Space Complexity . The O Space data structure is composed of a set of arrays . Each of the arrays contains ( kmax − kmin + 1 ) float values ( distance ) corresponding to the space delimiter of one outlier candidate . Therefore the space complexity is linear in the number of outlier candidates | OC | . More precisely it is O(| OC | ( kmax − kmin + 1 ) ) .
As confirmed by our experiments ( Fig 9 , Sec 431 ) , only a small fraction of points is classified as outlier candidates . Most of the points are recognized as const inliers . Therefore L is much smaller than the actual cardinality n of the input dataset . Hence the O Space structure is found to be rather compact and in fact small enough to be accommodated in the main memory of a standard PC even when handling a fairly large dataset in order of 10GB . Time Complexity . The time complexity of constructing O Space is O(n 2 ) because of the potential KNN search on each point . Here n represents the cardinality of the input dataset D . Furthermore it is worth to emphasize that in fact the cost of building O Space is similar to the cost of answering one single outlier detection request as confirmed in our experiments ( Figures 7 , 8 , Sec 421 ) In ONION , the expensive exact KNN search is only conducted on outlier candidates . This significantly speeds up the construction of O Space .
312 Online Outlier Exploration
O Space is sufficient to support all three classes of outlier exploration operations . In particular by intelligently maintaining the space delimiter information of each outlier candidate , we are able
93 to drive down the time complexity of supporting online outlier detection ( OD ) operation from quadratic to linear .
Outlier Detection ( OD ) . For each outlier candidate oc we maintain its space delimiter DS in an array structure by the order of its kminth neighbor at the head and the kmaxth neighbor at the end . Then for any parameter ps ∈ P , the outlier status of oc can be immediately determined by applying the following examination rule .
Definition 10 . Given an outlier candidate oc and its DS structure , ∀ parameter setting ps(kx , rx ) in P , oc is an outlier if DS[kx − kmin ] > rx . Otherwise pi is an inlier .
Therefore to answer OD we only need to perform one scan on the outlier candidate set OC and sequentially apply the examination rule in Def . 10 on each oc . Hence the time complexity is linear to the cardinality of OC .
Outlier Centric Parameter Space Exploration ( PSE ) . Given an outlier set Oin , the parameter settings that recognize Oin as outliers is the intersection of a set of parameter space segments Si with respect to each point pi in Oin . All parameter settings in Si with respect to pi will classify pi as outlier . By Lemma 2 this can be done by checking and comparing the space delimiters DS of all points in Oin . The time complexity is O(| Oin |(kmax kmin) ) .
Comparative Outlier Analytics ( CO ) . Similar to PSE , given an outlier set Oin , CO can be answered by checking the parameter space segment Si with respect to each outlier candidate oci in OC Oin . Point oci is dominated by all points oj in Oin if Si ⊇ Sj for all ocj . The time complexity is O(| OC |(kmax kmin) ) .
3.2 P Space
321 Offline P Space Construction
To construct the P Space we first introduce the concept of k domination between two outlier candidates .
Definition 11 . Given two outlier candidates oci and ocj and ocj , then oci k a k value of Dimk ∈ [ kmin , kmax ] , if D k oci dominates ocj . fi D k
The following monotonic property holds if the k domination re lationship holds between oci and ocj .
Lemma 5 . Given two outlier candidates oci and ocj with oci k dominating ocj , then for any parameter setting ps(k , rx ) ∈ P ( rmin ≤ rx ≤ rmax ) , if oci is classified as outlier by ps , then ocj is guaranteed to be outlier with respect to ps .
≥ D k
If oci is an outlier with respect to ps(k , rx ) , D k oci
> rx . Therefore ocj is an outlier with respect to ps . .
> rx . oci by the k domination definition in Def . 11 ,
Proof . Since D k ocj D k ocj In other words , if one parameter setting ps(k , rx ) classifies pi as an outlier , then any point k dominated by pi is guaranteed to also be an outlier . On the other hand , if one parameter setting classifies pi as an inlier , then any point that k dominates pi is also guaranteed to be an inlier as well .
It is straightforward to prove that the k domination relationship also satisfies the transitive property .
Lemma 6 . Given three candidates och , oci , and ocj , if och kdominates oci and oci k dominates ocj , thenoc h k dominates ocj .
The above properties of the k domination relationship now enable us to divide the infinite parameter setting space P into a finite number of stable parameter regions .
Lemma 7 . Given the outlier candidate set OC ⊂ dataset D and Pki ⊂ P , where | OC | = n and Pki is composed by any parameter setting ps in P sharing the same Dimk value ki , then Pki ∈ can be divided into n+1 stable regions P , D ki [ rmin , D ki ∈ [ D ki , D ki ∈ [ D ki ocn ocj , < D ki ocj guaranteed to be generated for all ps ∈ P
∈ [ D ki oc1 ) , Dimr of P2 oc1 ki n+1 ocj +1 ) , , Dimr of P oc2 , ki ocj +1 , , < D ki ocn ) . The identical set of outliers are ki , where Dimr of P1 ki j+1 oc2 ) , , Dimr of P ki
, rmax ] ( D ki oc1
< D ki
< D ki j j ki . j ocj −1 ocj −2
< D ki
< D ki
≤ rx < D ki ki , since D ki ocj −1 and D ki ocj
Proof . ∀ ps(ki , rx ) ∈ P ocj , ps(kj , rx ) will classify ocj −1 as inlier , while ocj would be classified as outlier . Since D ki ocj +1 , we get ocj−2 k dominates ocj−1 and ocj dominates ocj+1 . Based on the monotonic property of k domination , ocj−2 will also be classified as an inlier , while ocj+1 remains as outlier . Furthermore by the transitive property of k domination , ∀ ps(ki , rx ) ∈ P j+1 , oc1 , oc2 , , ocj −2 , ocj −1 are guaranteed to be inliers , while ocj , ocj +1 , , ocn are guaranteed to be outliers . Therefore the identical set of outliers will be generated for any ps ∈ P . Lemma 7 has thus been proven ki j+1 ki
ƌ
ŵĂǆ
ŽĐ
ϱ
ŽĐ
ϰ
ŽĐ
ϯ
ŽĐ
Ϯ
ŽĐ
ϭ
ƌ
ŵŝŶ
Ŭ
Ŭ
ŵŝŶ
ŵŝŶнϭ
ŽƌĚĞƌ
ŽĐ
ϱ
ŽĐ
ŽĐ
ϯ
ϰ
ŽĐ
Ϯ
^ƚĂďůĞ ZĞŐŝŽŶ
ŽĐ
ϭ
͙
͙
Ŭ
Ŭ
ŵĂǆͲϭ
ŵĂǆ
Figure 6 : Stable Region
Leveraging Lemma 7 we design an light weight algorithm ( Alg . 1 ) to build P Space P . We first define the parameter node structure .
Definition 12 . A parameter node , or in short pn , is a data struc ture composed of the following three elements :
pn.obj : an outlier candidate oc in OC ; pn.k : k parameter value ( k ∈ [ kmin , kmax ] ) ; pnr r parameter value , pn.r = D pn.k oc
;
By Def . 12 , each outlier candidate oc in OC will be mapped to m nodes , where m = kmax − kmin + 1 . Each of the nodes corresponds to one element in the space delimiter DS of oc , that is D ki ( kmin ≤ ki ≤ kmax ) . Then we organize the parameter nodes based on pn.k into m array lists . Each array list called kthList contains the nodes with the same pn.k value . Therefore each outlier candidate oc is represented by exactly one node pn in each kthList . oc
The key idea behind Alg . 1 is to sort the parameter nodes in each kthList in ascending order based on their pn.r values . By this each subspace Pki ( kmin ≤ ki ≤ kmax ) ofP is divided into multiple oc in DS(oc ) with respect to each oc in OC . stable regions P kmax is a stable region of Pkmax For example as shown in Fig 6 , P2 bounded by D kmax oc2 of oc2 ( [D kmax oc2 ) ) . All oc1 parameter settings in P2
, D kmax kmax classify oc2 , , oc5 as outliers . oc1 of oc1 and D kmax ki by D ki j
P Space then is represented by a hash map with pn.k as the key and the corresponding kthList as value .
94 kthList = ∅ ; for each oc ∈ ocs do
Algorithm 1 constructPSpace 1 : P Space = ∅ ; 2 : for each k from kmin to kmax do 3 : 4 : 5 : 6 : 7 : 8 : 9 : end for 10 : return P Space ; end for kthList.sort( ) ; P Space.put(k , kthList ) ; kthList.add(new pn(oc , oc.DS , k) ) ;
322 Online Outlier Exploration
Outlier Detection ( OD ) . Given a parameter setting psi ( ki , ri ) to detect the outliers we only need to locate a particular parameter node pnmin in P Space where pnmin .k = ki and pnmin .r = min({pn.r | pn.r > ri } ) . Then the outliers for psi will be the outlier candidates corresponding to the parameter nodes in the ki thList of P Space and listed behind pnmin .
Complexity Analysis . Since each array list is sorted by the pn.r value , pnmin can be located in O(log(| OC | ) ) time using a binary search style algorithm [ 8 ] .
Outlier Centric Parameter Space Exploration ( PSE ) . Utilizing P Space to support PSE operation is straightforward . We can traverse through each kthList of P Space to locate the stable regions that return the outlier set Oin specified in the input . Given one particular array list ki thList , we first locate the parameter node pn1st of ocj corresponding to the first outlier in Oin . Then we compare the outliers in Oin with the objects listed behind pn1st in j ki thList one by one . If all objects match , one stable region P ki : [ D ki ocj −1 , D ki Complexity Analysis . The cost of supporting PSE relies on the number of kthList and the outliers in Oin . Therefore the time complexity is O(m | Oin | ) , where m =k max − kmin + 1 . ocj ) will be returned .
Comparative Outlier Analytics ( CO ) . Given an outlier set Oin , CO operation can be answered by checking each outlier candidate oci in OC OCin . oci is dominated by all points oj in Oin if oci is listed behind all oj in every kthList .
Complexity Analysis . The time complexity is O(m | OC | ) , where m = kmax − kmin + 1 .
3.3 D Space
331 Offline D Space Construction
By Def . 5 , to construct D Space D , we have to divide all outlier candidates oc into multiple domination groups Di . The domination relationship holds among all ocs falling in the same group Di .
Next we introduce the domination rule in Lemma 8 to evaluate whether the domination relationship holds between two outlier candidates based on their space delimiters in O Space .
Lemma 8 . Given two outlier candidates oci and ocj , oci dom inates ocj if ∀ kl ∈ [ kmin , kmax ] , D kl oci ≤ D kl ocj
≤ D k
Proof . By Def . 11 , given one k ∈ [ kmin , kmax ] , if D k oci ocj , then oci k dominates . By Lemma 5 given any parameter setting ps ∈ parameter subspace Pk ⊂ P , ocj is guaranteed to be outlier if kl ocj holds for any kl oci is classified as outlier by ps . Since D ∈ [ kmin , kmax ] , then oci kl dominates ocj for any kl . Therefore if oci is classified as an outlier by any parameter setting ps in P , then ocj is guaranteed to be an outlier . By the definition of domination relation in Def . 4 , Lemma 8 is proven . . kl oci ≤ D
As shown in Fig 6 , oc1 dominates oc2 , because D k oc1 for any k ∈ [ kmin , kmax ] .
< D k oc2
It is straightforward to prove that domination relationship satis fies the transitivity property .
Lemma 9 . Given three outlier candidates och , oci , and ocj , if och dominates oci and oci dominates ocj , then och dominates ocj .
Next we propose a graph based solution that successfully constructs D Space . First we construct an undirected graph based on the domination relationships among all outlier candidates .
Definition 13 . Domination Graph . The domination graph of the outlier candidate set OC is a graph G(V , E ) , such that ( 1 ) a node vi exists in V to represent a point oci in OC , and ( 2 ) an edge eij = ( vi , vj ) exists in E if domination relationship does not hold between oci and ocj ∈ OC corresponding to nodes vi and vj in V .
. ki+1 oci
This domination graph G(V , E ) tends to be a sparse graph , because the domination relationship tends to hold among most points in OC . This is the case because the distance of oci towards its kNN usually does not dramatically change . If D ki ocj , also tends to be smaller than D then D oci is smaller than D ki ki+1 ocj
Given a domination graph G , a completely disjointed graph with zero edge can always be derived by removing some nodes and the corresponding edges . This indicates by removing a small number of points corresponding to these nodes , we can get a subset of OC such that the domination relationship holds among all points in it . If we could determine the minimal number of nodes whose removal will completely isolate the remaining nodes , then we could build the largest domination group out of OC . We now note that the problem of finding the minimal number of nodes to remove so that no edge remains in G can be mapped to the minimum vertex cover problem − a classical NP complete problem .
Clearly any minimum vertex cover algorithm can be applied here . Then D Space can be built by recursively applying the minimum vertex cover algorithm on the removed nodes as shown in Alg . 2 . The domination group built in each iteration is guaranteed to be the largest at that round . Therefore this process concurrently also minimizes the number of the domination groups . Since the domination graph tends to be a sparse graph , the number of the domination groups generated is small . As confirmed in our experiments , usually two or three trees are sufficient to cover all outlier candidates . return ∅ ;
Algorithm 2 construct _DForest Input : OC // outlier candidates Output : domination_f orest // constructed domination forest ; 1 : if ( OC == ∅ ) then 2 : 3 : else 4 : 5 : 6 : OC = OC removed ; 7 : 8 : domination_tree = buildDtree(OC ) ; return domination_f orest + domination_tree + Construct _DForest(removed ) ; domination_tree = ∅ ; removed = minVertexCover(OC ) ;
9 : end if
Given a domination group Di a domination tree treei can be constructed by sorting the outlier candidates in the ascending order based on the distance to their kth nearest neighbors , where k can be any element in [ kmin , kmax ] . In this domination tree , each oc will dominate the points listed behind it , while it in turn will be dominated by the points listed in front of it by the transitive property of
95 the domination relationship . Therefore D Space is represented by a domination forest composed of multiple domination trees .
Furthermore domination forest also incorporates the stable re gion concept of P Space along its linkage to D Space .
≤ rx < D kx
Lemma 10 . Given two adjacent points oci and oci+1 in domination tree treel , any parameter setting psx ( kx , rx ) with kmin ≤ kx ≤ kmax and D kx oci+1 will classify the same set of oci points ocj in treel as outliers , where j > i . Proof . Since D kx oci+1 , by Lemma 1 psx will classify oci oci+1 as outlier and oci as inlier . Since oci+1 dominates ocj , all ocjs are outliers . Any other point och in treel will be classified as inlier because och dominates inlier oci . Lemma 10 is proven
≤ rx < D kx
As shown in Fig 6 , the parameter settings bounded by lines of oc1 and oc2 generate the same set of outliers : oc2 , , oc5 .
332 Online Outlier Exploration
The domination forest can efficiently support all classes of out lier exploration operations .
Comparative Outlier Analytics ( CO ) . CO can be supported by locating the first point p1st in each domination tree dominated by the weakest outlier oi in the outlier input set Oin using a binary search style algorithm . Then all points listed behind p1st in each of the domination trees are guaranteed to be outliers .
Outlier Detection ( OD ) . Similar to CO , OD can be supported by applying the binary search style algorithm on each domination tree to locate the first outlier candidate classified as outlier by the input parameter setting psi .
The time complexity of processing CO and OD is O(log | tree1 | + log | tree2 | ++ log | treen | ) . It relies on the size of each tree and the number of the trees .
Outlier Centric Parameter Space Exploration ( PSE ) . By Lemma
10 , the parameters that generate the same outliers Oin can be loi cated by examining the strongest outlier oc in O in and the first i point in front of oc in each domination tree treei . Here O in = Oin ∩ treei . The intersection of the parameters returned from each tree will be the final result of PSE . The time complexity is O(n+ | Oin | ) , where n is the number of the trees .
In summary the time complexity of the online phase relies on the size of each tree and the number of the trees . It is easy to see that the smaller the number of the trees is , the lower the costs will be .
As for the size of each tree , suppose two forests f t1 and f t2 composed of the same number of trees are derived from outlier candidate set OC . For forest f t1 , | ft1 .tree1 | fl | ft1 .tree2 | fl | ft1 .treen | , while for forest f t2 , | ft2 .tree1 | ≈ | ft2 .tree2 | ≈ | ft2 .treen | . Then the cost of the binary search amounts to binary(ft1 ) < binary(ft2 ) . For example suppose OC contains 2m points . f t1 consists of two trees including the largest possible tree | ft1 .tree1 | = 2m − 1 and the smallest tree | ft1 .tree2 | = 1 , while | ft2 .tree2 |=| ft2 .tree2 |= 2 m−1 . Then binary(ft1 ) = log(2m − 1 ) + 1< m +1 , while binary(ft2 ) = 2log(2m−1 ) = 2(m 1 ) . Obviously when m is reasonably large , binary(ft1 ) is far smaller than binary(ft2 ) . Therefore instead of making each tree equal size , the ideal forest construction algorithm should produce the largest possible trees out of OC .
As shown in Sec 331 our graph based D Space construction algorithm ( Alg . 2 ) not only minimizes the number of trees created , but also maximizes the size of the trees in the forest . Therefore it effectively optimizes the performance of outlier exploration .
4 . EXPERIMENTAL EVALUATION
Environment . All experiments ran on a Linux Server with 8 GB memory 2.6GHz Quad Core CPU using Java 160 64bit runtime .
Real Datasets . We utilize the GMTI ( Ground Moving Target Indicator ) dataset [ 9 ] to conduct user study . GMTI contains around 10,000 records regarding the information of soldiers , vehicles , and helicopters deployed in a certain region . The outliers are detected based on targets’ latitude and longitude . We use the outliers manually labeled by the experts familiar with the data as ground truth .
We also use the geolocation data from OpenStreetMap
( http://downloadgeofabrikde/ ) to evaluate the performance of ONION when handling large dataset . It contains the geolocation information of 50 million buildings ( 10G ) over Australia and Oceania , such as houses , cafes , stations , etc A location on the map is considered to be outlier based on their distances to other locations .
Methodology . We evaluate the processing time and scalability of both our offline preprocessing and online mining algorithms by varying the sizes of the dataset D , parameter space P , and the number of mining requests . We compare against the state of the art DOLPHIN [ 1 ] in a rich variety of representative use cases .
In particular at the offline phase , we evaluate the processing time of constructing O Space that builds the foundation of ONION in comparison to the index construction cost of DOLPHIN . At the online phase , the performance of our online algorithms associated with O Space , P Space , and D Space respectively is evaluated and contrasted for all three outlier exploration types , namely outlier detection ( OD ) , outlier centric parameter space exploration ( PSE ) , and comparative outlier analytics ( CO ) . The algorithms associated with each ONION abstraction are named in the format of “ operation type ” + “ _ ” + “ Abstraction type ” . For example the algorithm supporting OD operation on O Space is named as “ OD_OSpace ” . Furthermore we also compare our ONION against DOLPHIN on the processing time of traditional outlier detection query − the only exploration type that Dolphin supports .
4.1 User Study
We conduct a user study to evaluate the effectiveness of ONION in recognizing outliers contrasting against the traditional one ata time query approach ( TRAD ) that only supports outlier detection operation . Since TRAD takes hours to process a large dataset ( 10G ) as confirmed in Sec 431 , it is not acceptable for interactive analytics . Therefore in this study we adopt the relative small dataset ( GMTI ) − a clear bias to TRAD .
We invited 50 users from both WPI and Yantai University , China . The users are divided into two groups . Each group only evaluates one system . Each user is allowed to continuously submit mining requests supported by the target system until the generated results meet the precision and recall requirement ( 09,09 ) set by us . In each round the precision and recall are automatically calculated and feedbacked to the users . In any case the study will terminate after 15 minutes . Users are provided a distribution plot of GMTI dataset that assists them to initialize the parameter setting . For each user , we count the number of trials ( the submitted mining requests ) on each exploration operation . Then the trial number is averaged on the users belonging to the same group .
System ONION TRAD
Success Rate 1 0.36
Overall 5.6 16.2
OD 1.8 16.2
CO 2.6
PSE 1.2
−
−
Table 1 : Statistics
As shown in Table 1 , only 36 % of the TRAD users are able to eventually meet the precision and recall requirement in 15 minutes , while all users using ONION succeed . In average TRAD takes users 16.2 trials to meet the requirement , while the ONION users only need 56 In particular in average the ONION users submit
96 Single Detection O Space Construction DOLPHIN
40000
35000
30000
25000
20000
15000
10000
5000 d n o c e S n I e m i T
40000
35000
30000
25000
20000
15000
10000
5000 d n o c e S n I e m i T
Single Detection O Space Construction
OD_OSpace OD_PSpace OD_DSpace OD_DOLPHIN
1e10
1e9
1e8
1e7
1e6
1e5
1e4
1e3
1e2
] e l a c s g o l [ d n o c e S n I e m i T
OD_OSpace OD_PSpace OD_DSpace OD_DOLPHIN
1e10
1e9
1e8
1e7
1e6
1e5
1e4
1e3
1e2
] e l a c s g o l [ d n o c e S n I e m i T
0
1e7
2e7
3e7
4e7
5e7
Dataset Size
0
10
12
14
16
18
20
1e1
1e7 k
2e7
3e7
4e7
5e7
Dataset Size
1e1
10000
20000
30000
40000
50000
Number of Queries
Figure 7 : O Space Construction : Varying Data Size
Figure 8 : O Space Construction : Varying k
Figure 9 : OD : Varying Dataset Size
Figure 10 : OD : Varying Number Of Requests
] e l a c s g o l [ d n o c e S n I e m i T
1e8
1e7
1e6
1e5
1e4
1e3
1e2
1e1
PSE_OSpace PSE_PSpace PSE_DSpace
PSE_OSpace PSE_PSpace PSE_DSpace
1e8
1e7
1e6
1e5
1e4
1e3
1e2
] e l a c s g o l [ d n o c e S n I e m i T
CO_OSpace CO_PSpace CO_DSpace
1e8
1e7
1e6
1e5
1e4
1e3
1e2
] e l a c s g o l [ d n o c e S n I e m i T
2
4
6 k
8
10
1e1
1e7
2e7
3e7
4e7
5e7
Dataset Size
1e1
1e7
2e7
3e7
4e7
5e7
Dataset Size
1e8
1e7
1e6
1e5
1e4
1e3
1e2
] e l a c s g o l [ d n o c e S n I e m i T
1e1
10
CO_OSpace CO_PSpace CO_DSpace
25 15 Input Outlier Set Size
20
30
Figure 11 : PSE : Varying parameter space size
Figure 12 : PSE : Varying Dataset Size
Figure 13 : CO : Varying Dataset Size
Figure 14 : CO : Varying input outlier set size
CO operation 1.8 times , PSE operation 2.6 times , and the traditional outlier detection ( OD ) 1.2 times . This confirms that our new outlier exploration operations indeed save users significantly effort on pinpointing appropriate parameter settings .
4.2 Offline Preprocessing
421 O Space Construction
We first focus on the processing time of constructing O Space ( construct_OSpace ) from raw data by varying the parameter space size , as well as the dataset size . The costs of one time outlier detection without employing any index is used as the baseline to evaluate the extra overhead introduced by constructing O Space .
Varying dataset size . Fig 7 illustrates the results when dataset size increases from 10 million up to 50 million . We vary the dataset size by including more and more buildings belonging to different regions in Australia . The parameter space is fixed with kmax as 10 and rmax as 4000 . Clearly constructing O Space has ignorable overhead compared to the cost of one time outlier detection when parameter setting ps specified as ( kmax ,rmin ) . Both our O Space construction process and one time detection process need to detect up to kmax neighbors within rmin radius for each point pi . The additional overhead of O Space construction is introduced by having to track and maintain all possible outlier candidates with respect to the entire parameter space . However as shown in Fig 7 such overhead is small ( around 10% ) . Furthermore constructing O Space is significantly faster than constructing DOLPHIN index .
Varying parameter space P . Dolphin is excluded from this case because it does not have the parameter space concept . The influence of varying range of k is evaluated . Fig 8 represents the results when varying kmax from 10 to 20 , while holding rmax at 4000m and dataset size at 50 million . The overhead is still around 10 % for the same reason explained above . As kmax increases , the cost of O Space construction grows in the trend similar to one time outlier detection . Varying the range of r shows the similar influence . Due to space constraint , the results are not included .
4.3 Online Outlier Exploration
431 Online Outlier Detection
Varying dataset size . Fig 9 shows the advantage of ONION for outlier detection . We ran 10,000 requests with randomly chosen parameter settings from the entire parameter space and show the total processing time . P Space and D Space methods show very similar performance . Therefore their lines in Fig 9 are overlapped . In average each request can be processed in milliseconds . Both consistently outperform DOLPHIN 5 orders of magnitude . Furthermore for D Space and P Space the detection cost grows only logarithmically in the size of outlier candidates that are the strict minority of the whole dataset ( fewer than 10% ) , while DOLPHIN grows linearly . Therefore , ONION scales to large dataset .
Varying number of request . We increase the number of OD requests from 10,000 up to 50,000 , while holding dataset size constant at 50 million . The total detection time is measured . Fig 10 shows that our algorithms scale linearly in the number of requests . Again P Space and D Space algorithms are at least 5 orders of magnitude faster than DOLPHIN . Even our linear complexity O Space method is 3 order of magnitude faster than DOLPHIN in average .
432 Outlier Centric Parameter Space Exploration We evaluate the performance of processing PSE request not supported by DOLPHIN . Each chart shows the accumulated processing time for 10,000 requests .
Varying parameter space size . Fig 11 measures the influence to the processing time of PSE when varying the size of the parameter space . This is achieved by increasing kmax from 2 to 10 . For P Space and D Space the cost of supporting PSE relies on the number of domination trees and the parameter node lists . Therefore , the cost of P Space and D Space is not sensitive to the change of kmax . On the other hand O Space method has to check all outlier candidates . Since the number of outlier candidates grows as kmax increases , the cost of O Space method will also increase lineally .
Varying dataset size : outlier set as input . Fig 12 demonstrates the performance of our PSE algorithms . That is , given a PSE request , we use a set of randomly selected outlier candidates as input . The size of the datasets is varied from 10 million up to 50 million . Similar to the experiment that uses parameter settings as input , P Space and D Space methods significantly outperform O Space method 3 orders of magnitude .
We evaluating the processing time of online outlier detection by
433 Comparative Outlier Analytics varying the size of the datasets and the number of the requests .
Next we evaluate the performance of supporting CO operation .
97 Varying dataset size . Fig 13 illustrates the processing time of supporting CO operation by varying dataset sizes . We use a randomly selected outlier set as input . The operation returns all outlier candidates that are dominated by the input outliers . D Space supports CO operation by only looking at each domination tree in the domination forest once , while the number of the domination tree is small ( at most 3 when the dataset contains all 50 millions buildings ) . On the other hand , O Space method has to scan all candidates , while P Space method has to search the parameter node lists for every possible k value . Therefore D Space method is about 1 order of magnitude faster than P Space method , and about 3 to 4 orders of magnitude faster than O Space method .
Varying size of input outlier set . In Fig 14 we vary the size of the input outlier set from 10 to 30 , while keeping the sizes of dataset and parameter space stable . For each method we only need to check the weakest outlier of the input outlier set . Since the cost of determining the weakest outlier is negligible , all our three methods are not sensitive to the size of the input outlier set . to statistical or clustering based approaches , they still suffer from unacceptable response times such that hours or even days for online queries . Furthermore none of these works tackles the important and hard problem of choosing proper parameter setting from the infinite number of possible options . Our work not only successfully satisfies the real time responsiveness requirement , but also saves users the significant effort otherwise spent on parameter tuning .
Parameter Space Exploration in Clustering . In [ 2 ] the OPTICS algorithm creates an augmented ordering of the dataset to represent the clustering structure corresponding to a set of parameter settings . However , the producing of outliers as by products of clustering has already been shown to be not effective in capturing abnormal phenomena [ 16 ] . Furthermore the ordering information is only effective in representing the clusterings with respect to a small range of parameter settings , that is the parameters with only the neighbor range threshold variable . Our work instead supports a full range of possible parameter settings composed of both range and neighbor count thresholds .
5 . RELATED WORK
6 . CONCLUSION
Outlier Detection . Outlier detection has been the focus of much research in the statistics literature for over a century [ 12 , 3 ] . The most common approach is to assume that all points follow a distribution with known distribution parameters ( eg , mean and variance ) . The points that do not properly fit the model are considered to be outliers . However , such approaches suffer from the serious limitation that the data distribution and underlying parameters must either be explicitly known apriori or be easily inferred .
Approaches that do not rely on data distributions have also been proposed . In [ 10 , 15 , 18 ] all points that are not a core part of any cluster are classified as outliers . In other words the outliers are in this case the by products of data clustering . However we note here that a point that is not a member of any cluster is not necessarily abnormal . This is so because the goal of clustering is to group together points that are extremely similar to one another . Therefore such approaches lack strong notion of what constitutes an outlier .
To address this limitation , the notion of an outlier based on density ( of neighborhood ) or based on distance ( of neighbors ) has been defined . Density based approaches [ 6 , 17 ] assign an outlier score to any given point by measuring the density relative to its local neighborhood restricted by a pre defined threshold . Therefore densitybased outliers , regarded as “ local outliers ” , are able to identify outliers often missed by other methods . However it has been observed that such methods do not scale well to large datasets [ 14 ] .
Furthermore explicit distance based approaches , based on the well known nearest neighbor principle , were first proposed by Ng and Knorr [ 13 ] . They employ a well defined distance metric to detect outliers , that is , the greater is the distance of the point to its neighbors , the more likely it is an outlier . The basic algorithm for such distance based definition , the nested loop ( NL ) algorithm , calculates the distance between each pair of points and then set as outliers those that are far from most points . The NL algorithm has quadratic complexity with respect to the number of points . Thus it is not suitable for truly large datasets .
As a result , extensive effort has been focusing on identifying practical sub quadratic algorithms [ 1 , 4 , 11 , 5 ] . Several optimization principles have been proposed such as the use of compact data structures [ 11 ] , of lightweight outlier detection oriented indices [ 1 ] , and of pruning and randomization [ 4 ] . In particular by indexing the possible neighbors of each point pi in dataset D based on their distances to pi , [ 1 ] is able to approximate whether pi is an outlier in the time complexity near linear to the cardinality of D . However , while these methods offer improved performance compared
Interactive outlier exploration over large dataset is an extremely important yet difficult task . Our novel ONION framework achieves this by bridging the data space and parameter space . By extracting the outlier candidates along with their interrelationships and abstracting them into successive more powerful structures , ONION is able to effectively discover outliers with real time responsiveness .
7 . REFERENCES [ 1 ] F . Angiulli and F . Fassetti . Dolphin : An efficient algorithm for mining distance based outliers in very large datasets . TKDD , 3(1 ) , 2009 .
[ 2 ] M . Ankerst , M . M . Breunig , H . Kriegel , and J . Sander . OPTICS : ordering points to identify the clustering structure . In SIGMOD 1999 , Proceedings ACM SIGMOD International Conference on Management of Data , June 1 3 , 1999 , Philadelphia , Pennsylvania , USA . , pages 49–60 , 1999 .
[ 3 ] V . Barnet and T . Lewis . Outliers in statistical data . International Journal of
Forecasting , 12(1):175–176 , 1996 .
[ 4 ] S . Bay and M . Schwabacher . Mining distance based outliers in near linear time with randomization and a simple pruning rule . In KDD , pages 29–38 , 2003 . [ 5 ] K . Bhaduri , B . L . Matthews , and C . Giannella . Algorithms for speeding up distance based outlier detection . In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , San Diego , CA , USA , August 21 24 , 2011 , pages 859–867 , 2011 .
[ 6 ] M . M . Breunig and et al . Lof : Identifying density based local outliers . In
SIGMOD Conference , pages 93–104 , 2000 .
[ 7 ] V . Chandola , A . Banerjee , and V . Kumar . Anomaly detection . ACM Computing
Surveys , 41(3):1–58 , 2009 .
[ 8 ] T . H . Cormen , C . E . Leiserson , R . L . Rivest , and C . Stein . Introduction to
Algorithms ( 3 . ed ) MIT Press , 2009 .
[ 9 ] Entzminger and et al . Jointstars and gmti : past , present and future . Aerospace and Electronic Systems , IEEE Transactions , 35(2):748 –761 , Apr . 1999 .
[ 10 ] M . Ester , H P Kriegel , J . Sander , and X . Xu . A density based algorithm for discovering clusters in large spatial databases with noise . In KDD , pages 226–231 , 1996 .
[ 11 ] A . Ghoting , S . Parthasarathy , and M . E . Otey . Fast mining of distance based outliers in high dimensional datasets . Data Min . Knowl . Discov . , 16(3):349–364 , 2008 .
[ 12 ] D . M . Hawkins . Identification of Outliers . Springer , 1980 . [ 13 ] E . M . Knorr and R . T . Ng . Algorithms for mining distance based outliers in large datasets . In VLDB , pages 392–403 , 1998 .
[ 14 ] H P Kriegel , P . Kröger , and A . Zimek . Outlier detection techniques . In In
Tutorial of the 13th PAKDD , 2009 .
[ 15 ] R . T . Ng and J . Han . Efficient and effective clustering methods for spatial data mining . In VLDB , pages 144–155 , 1994 .
[ 16 ] G . H . Orair , C . H . C . Teixeira , Y . Wang , W . M . Jr . , and S . Parthasarathy .
Distance based outlier detection : Consolidation and renewed bearing . PVLDB , 3(2):1469–1480 , 2010 .
[ 17 ] S . Papadimitriou , H . Kitagawa , P . B . Gibbons , and C . Faloutsos . LOCI : fast outlier detection using the local correlation integral . In Proceedings of the 19th International Conference on Data Engineering , March 5 8 , 2003 , Bangalore , India , pages 315–326 , 2003 .
[ 18 ] T . Zhang , R . Ramakrishnan , and M . Livny . Birch : An efficient data clustering method for very large databases . In SIGMOD , pages 103–114 , 1996 .
98
