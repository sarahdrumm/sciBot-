The Child is Father of the Man : Foresee the Success at the Early Stage
Liangyue Li
Arizona State University liangyue@asu.edu
ABSTRACT Understanding the dynamic mechanisms that drive the highimpact scientific work ( eg , research papers , patents ) is a long debated research topic and has many important implications , ranging from personal career development and recruitment search , to the jurisdiction of research resources . Recent advances in characterizing and modeling scientific success have made it possible to forecast the long term impact of scientific work , where data mining techniques , supervised learning in particular , play an essential role . Despite much progress , several key algorithmic challenges in relation to predicting long term scientific impact have largely remained open . In this paper , we propose a joint predictive model to forecast the long term scientific impact at the early stage , which simultaneously addresses a number of these open challenges , including the scholarly feature design , the non linearity , the domain heterogeneity and dynamics . In particular , we formulate it as a regularized optimization problem and propose effective and scalable algorithms to solve it . We perform extensive empirical evaluations on large , real scholarly data sets to validate the effectiveness and the efficiency of our method .
Categories and Subject Descriptors H28 [ Database Management ] : Database applications— Data mining
Keywords long term impact prediction ; joint predictive model
1 .
INTRODUCTION
Understanding the dynamic mechanisms that drive the high impact scientific work ( eg , research papers , patents ) is a long debated research topic and has many important implications , ranging from personal career development and recruitment search , to the jurisdiction of research resources . Scholars , especially junior scholar , who could master the key
Hanghang Tong
Arizona State University hanghangtong@asuedu to producing high impact work would attract more attentions as well as research resources ; and thus put themselves in a better position in their career developments . Highimpact work remains as one of the most important criteria for various organization ( eg companies , universities and governments ) to identify the best talents , especially at their early stages . It is highly desirable for researchers to judiciously search the right literature that can best benefit their research .
Recent advances in characterizing and modeling scientific success have made it possible to forecast the long term impact of scientific work . Wuchty et al . [ 29 ] observe that papers with multiple authors receive more citations than soloauthored ones . Uzzi et al . [ 27 ] find that the highest impact science work is primarily grounded in atypical combinations of prior ideas while embedding them in conventional knowledge frames . Recently , Wang et al . [ 28 ] develop a mechanistic model for the citation dynamics of individual papers . In data mining community , efforts have also been made to predict the long term success . Carlos et al . [ 3 ] estimate the number of citations of a paper based on the information of past articles written by the same author(s ) . Yan et al . [ 31 ] design effective content ( eg , topic diversity ) and contextual ( eg , author ’s h index ) features for the prediction of future citation counts . Despite much progress , the following four key algorithmic challenges in relation to predicting long term scientific impact have largely remained open .
C1 Scholarly feature design : many factors could affect scientific work ’s long term impact , eg , research topics , author reputations , venue ranks , citation networks’ topological features , etc . Among them , which bears the most predictive power ?
C2 Non linearity : the effect of the above scholarly features on the long term scientific impact might be way beyond a linear relationship .
C3 Domain heterogeneity : the impact of scientific work in different fields or domains might behave differently ; yet some closely related fields could still share certain commonalities . Thus , a one size fits all or one sizefits one solution might be sub optimal .
C4 Dynamics : with the rapid development of science and engineering , a significant number of new research papers are published each year , even on a daily basis with the advent of arXiv1 . The predictive model needs to handle such stream like data efficiently , to reflect the recency of the scientific work .
1arxiv.org
655 Table 1 : Symbols
Symbols Definition nd ni mi d X(i ) t x(i ) t+1
Y(i ) t y(i ) t+1
A w(i ) K(i )
K(ij ) number of domains number of training samples in the i th domain number of new training samples in the i th domain feature dimensionality feature matrix of training samples from the i th domain at time t feature matrix of new training samples from the i th domain at time t + 1 impact vector of training samples from the i th domain at time t impact vector of new training samples from the i th domain at time t + 1 adjacency matrix of domain relation graph model parameter for the i th domain kernel matrix of training samples in the i th domain cross domain kernel matrix of training samples in the i th and j th domains observation of the AMiner citation network dataset . Section 4 proposes our joint model and the fast algorithm . Section 5 shows the experimental results . Section 6 reviews related work and the paper concludes in Section 7 .
2 . PROBLEM STATEMENT
In this section , we first present the notations used throughout the paper and then formally define the long term scientific impact prediction for scholarly entities ( eg , research papers , researchers , conferences ) .
Table 1 lists the main symbols used throughout the paper . We use bold capital letters ( eg , A ) for matrices , bold lowercase letters ( eg , w ) for vectors , and lowercase letters ( eg , λ ) for scalars . For matrix indexing , we use a convention similar to Matlab as follows , eg , we use A(i , j ) to denote the entry at the i th row and j th column of a matrix A , A(i , : ) to denote the i th row of A and A( : , j ) to denote the j th column of A . Besides , we use prime for matrix transpose , eg , A is the transpose of A .
To differentiate samples from different domains at different time steps , we use superscript to index the domain and subscript to indicate timestamp . For instance , X(i ) t denotes the feature matrix of all the scholarly entities in the i th domain at time t and x(i ) t+1 denotes the feature matrix of new scholarly entities in the i th domain at time t + 1 . Hence , X(i ) t denotes the impact vector of scholarly entities in the i th domain at time t and y(i ) t+1 denotes the impact vector of new scholarly entities in the ith domain at time t + 1 . Hence , Y(i ) t+1 ] . We will omit the superscript and/or subscript when the meaning of the matrix is clear from the context . t+1 ] . Similarly , Y(i ) t+1 = [ X(i ) t ; x(i ) t+1 = [ Y(i ) t ; y(i )
With the above notations , we are ready to define the longterm impact prediction problem in both static and dynamic settings as follows :
Problem 1 . Static Long term Scientific Impact Predic tion
Given : feature matrix X and impact Y of scholarly entities
Figure 1 : An illustrative example of the proposed joint predictive model . Papers from the same domain ( eg , AI , Databases , Data Mining and Bio ) share similar patterns in terms of attracting citations over time . Certain domains ( eg , AI and Data Mining ) are more related with each other than other domains ( eg , AI and Bio ) . We want to jointly learn four predictive models ( one for each domain ) , with the goal of encouraging the predictive models from more related domains ( eg , AI and Data Mining ) to be ‘similar’ with each other .
In this paper , we propose a joint predictive model–Impact Crystal Ball ( iBall in short ) – to forecast the long term scientific impact at an early stage by collectively addressing the above four challenges . First ( for C1 ) , we found that the citation history of a scholarly entity ( eg , paper , researcher , venue ) in the first three years ( eg , since its publication date ) is a strong indicator of its long term impact ( eg , the accumulated citation count in ten years ) ; and adding additional contextual or content features brings little marginal benefits in terms of prediction performance . This not only largely simplifies the feature design , but also enables us to forecast the long term scientific impact at its early stage . Second ( for C2 ) , our joint predictive model is flexible , being able to characterize both the linear and non linear relationship between the features and the impact score . Third ( for C3 ) , we propose to jointly learn a predictive model to differentiate distinctive domains , while taking into consideration the commonalities among these similar domains ( see an illustration in Figure 1 ) . Fourth ( for C4 ) , we further propose a fast on line update algorithm to adapt our joint predictive model efficiently over time to accommodate newly arrived training examples ( eg , newly published papers ) .
Our main contributions can be summarized as follows : • Algorithms : we propose a joint predictive model – iBall– for the long term scientific impact prediction problem , together with its efficient solvers .
• Proofs and analysis : we analyze the correctness , the approximation quality and the complexity of our proposed algorithms .
• Empirical evaluations : we conduct extensive experiments to demonstrate the effectiveness and efficiency of our proposed algorithms .
The rest of the paper is organized as follows . Section 2 gives the problem definition . Section 3 provides empirical
AIData MiningDatabaseBioPaper 1Paper 2Paper 3Paper 4Paper 5Paper 6Paper 7Paper 8656 Predict : the long term impact of new scholarly entities
We further define the dynamic impact prediction problem as :
Problem 2 . Dynamic Long term Scientific Impact Pre diction
Given : feature matrix Xt and new training feature matrix xt+1 of scholarly entities , the impact vector Yt , and the impact vector of new training samples yt+1
Predict : the long term impact of new scholarly entities
3 . EMPIRICAL OBSERVATIONS
In this section , we perform an empirical analysis to highlight some of the key challenges ( summarized in introduction section ) , on AMiner citation network [ 24 ] . This is a rich real dataset for bibliography network analysis and mining . The dataset contains 2,243,976 papers , 1,274,360 authors , and 8,882 computer science venues . For each paper , the dataset provides its titles , authors , references , publication venue and publication year . The papers date from year 1936 to 2013 . In total , the dataset has 1,912,780 citation relationships extracted from ACM library . 3.1 Feature design
Prior work [ 3 , 31 ] has proposed some effective features for citation count prediction , eg , topic features ( topic rank , diversity ) , author features ( h index , productivity ) , venue features ( venue rank , venue centrality ) . Other work [ 28 ] make predictions only on the basis of the early years’ citation data and find that the future impact of majority papers fall within the predicted citation range . We conduct experiment to compare performance of different features . Figure 2 shows the root mean squared error using different features with a regression model for the prediction of 10 years’ citation count . For example , ‘3 years’ means using the first 3 years’ citation as feature , and ‘3 years + content’ means using the first 3 years’ citation along with content features ( eg , topic , author features ) . The result shows that adding content features ( the right three bars in the figure ) brings little improvement for citation prediction . 3.2 Non linearity
To see if the feature has linear relationship with the citation , we compare the performance of different methods using only the first 3 years’ citation history . In Figure 3 , the nonlinear models ( iBall fast , iBall kernel , Kernel combine ) all outperform the linear models ( iBall linear , Linear separate , Linear combine ) . See Section 4 and 5 for details of these models . It is clear that complex relationship between the features and the impact cannot be well characterized by a simple linear model the prediction performance for all the linear models is even worse than the baseline method ( using the summation of the first 3 years’ citation counts ) . 3.3 Domain heterogeneity
To get a sense of the dynamic patterns of the citation count , we construct a paper age citation matrix M , where Mij indicates the number of citations the i th paper receives in the j th year after it gets published . The matrix M is then factorized as M ≈ WH using Non negative Matrix Factorization ( NMF ) [ 14 ] . We visualize the first six rows of
H in Figure 4 , which can give us different clustering citation dynamic patterns . As can be seen from the figure , the cyan line has a very small peak in the first 3 years and then fades out very quickly ; the blue line picks up very fast in the early years and then fades out ; the yellow line indicates a delayed pattern where the scientific work only receives some amount of attentions decades after it gets published . This highlights that impact of scientific work from different domains behaves differently .
4 . PROPOSED ALGORITHMS
In this section , we present our joint predictive model to forecast the long term scientific impact at an early stage . We first formulate it as a regularized optimization problem ; then propose effective , scalable and adaptive algorithms ; followed up by theoretical analysis in terms of the optimality , the approximation quality as well as the computational complexity .
4.1 iBall – Formulations
Our predictive model applies to different types of scholarly entities ( eg , papers , researchers and venues ) . For the sake of clarity , we will use paper citation prediction as an example . As mentioned earlier , research papers are in general from different domains . We want to jointly learn a predictive model for each of the domains , with the design objective to leverage the commonalities between related domains . Here , the commonalities among different domains is described by a non negative A , ie , if the i th and j th domains are closely related , its corresponding Aij entry will have a higher numerical value . Denote feature matrix for papers in the i th domain by X(i ) , citation count of papers in the i th domain by Y(i ) and the model parameter for the i th domain by w(i ) , we have the following joint predictive model : nd i=1
+θ nd nd i=1 j=1 nd i=1
Ω(w(i ) ) min w(i),i=1,,nd
L[f ( X(i ) , w(i) ) , Y(i ) ]
Aijg(w(i ) , w(j ) ) + λ
( 1 ) where f ( X(i ) , w(i ) ) is the prediction function for the i th domain , L( . ) is a loss function , g(w(i ) , w(j ) ) characterizes the relationship between the model parameters of the i th and j th domains , Ω(w(i ) ) is the regularization term for model parameters and θ , λ are regularization parameters to balance the relative importance of each aspect .
As can be seen , this formulation is quite flexible and general . Depending on the loss function we use , our predictive model can be formulated as regression or classification task . Depending on the prediction function we use , we can have either linear or non linear models . The core of our joint model is the second term that relates parameters of different models . If Aij is large , meaning the i th and j th domains are closely related to each other , we want the function value g( . ) that characterizes the relationship between the parameters to be small . iBall– linear formulation : if the feature and the output can be characterized by a linear relationship , we can use a linear function as the prediction function and the Euclidean distance for the distance between model parameters . The
657 Figure 2 : Prediction error comparison with different features .
Figure 3 : RMSE comparisons using different methods . The citation count is normalized in this figure . See Section 5 for normalization details .
Figure 4 : Visualization of papers’ citatoin behavior . Different colors encodes different citation behaviors . linear model can be formulated as follows : nd i=1
+θ nd nd i=1 j=1 min w(i),i=1,,nd
X(i)w(i ) − Y(i)2
2
Aijw(i ) − w(j)2
2 + λ nd i=1 w(i)2
2 min w(i),i=1,,nd
K(i)w(i ) − Y(i)2
2 nd i=1
+θ
+λ nd nd i=1 nd w(i ) j=1
K(i)w(i )
AijK(i)w(i ) − K(ij)w(j)2
2
( 3 )
( 2 ) where θ is a balance parameter to control the importance of domain relations , and λ is a regularization parameter . From the above objective function we can see that , if the i th domain and j th domain are closely related , ie , Aij is a large positive number , it encourages a smaller Euclidean distance between w(i ) and w(j ) . The intuition is that for a given feature , it would have a similar effect in predicting the papers from two similar/closely related domains . iBall– non linear formulation : As indicated in our empirical studies ( Figure 3 ) , the relationship between the features and the output ( citation counts in ten years ) is far beyond linear . Thus , we further develop the kernelized counterpart of the above linear model . Let us denote the kernel matrix of papers in the i th domain by K(i ) , which can be computed as K(i)(a , b ) = k(X(i)(a , : ) , X(i)(b , :) ) , where k(·,· ) is a kernel function that implicitly computes the inner product in a high dimensional reproducing kernel Hilbert space ( RKHS ) [ 1 ] . Similarly , we define the cross domain kernel matrix by K(ij ) , which can be computed as K(ij)(a , b ) = k(X(i)(a , : ) , X(j)(b , :) ) , reflecting the similarity between papers in the i th domain and j th domain . Different from the linear model where the model parameters in different domains share the same dimensionality ( ie , the dimensionality of the raw feature ) , in the non linear case , the dimensionality of the model parameters are the same as the number of training samples in each domain , which is very likely to be different across different domains . Thus , we cannot use the same distance function for g( ) To address this issue , the key is to realize that the predicted value of a test sample using kernel methods is a linear combination of the similarities between the test sample and all the training samples . Therefore , instead of restricting the model parameters to be similar , we impose the constraint that the predicted value of a test sample using the training samples in its own domain and using training samples in a closely related domain to be similar . The resulting non linear model can be formulated as follows : i=1 where θ is a balance parameter to control the importance of domain relations , and λ is a regularization parameter . From the above objective function we can see that , if the i th domain and j th domain are closely related , ie , Aij is a large positive number , the predicted value of papers in the i th domain computed using training samples from the i th domain ( K(i)w(i ) ) should be similar to that using training samples from the j th domain ( K(ij)w(j) ) .
4.2 iBall – Closed form Solutions
It turns out that both iBall linear and non linear formulations have the following closed form solutions ( see proof in subsection 4.4 ) : w = S
−1Y
( 4 ) iBall linear formulation . In the linear case , we have that w = [ w(i ) ; . . . ; w(nd) ] , Y = [ X(i ) Y(nd) ] , and S is a block matrix composed of nd × nd blocks , each of size d × d , where d is the feature dimensionality . S can be computed as follows :
Y(1 ) ; . . . ; X(i )
 i th block column j th block column
. . . . . . X(i ) . . .
X(i ) + (
. . . nd j=1
. . .
Aij + λ)I
. . .
−θAijI
. . .
 i th block row
( 5 ) iBall non linear formulation . In the non linear case , we have that w = [ w(i ) ; . . . ; w(nd) ] , Y = [ y(1 ) ; . . . ; y(nd) ] , and S is a block matrix composed of nd × nd blocks with the ( i , j) th block of size ni × nj , where ni is the number of training samples in the i th domain . S can be computed as follows :
012345678Root Mean Squared Error1 year2 years3 years1 year + content2 years + content3 years + content0020406081121416Root Mean Squared ErroriBall−fastiBall−kernelKernel−separateKernel−combineiBall−linearLinear−separateLinear−combinePredict 0Sum of first 3 years01020304050607080050100150200250300350Age658 
. . .
. . .
. . . i th block column j th block column
. . .
Aij)K(i ) + λI
( 1 + θ nd j=1
. . .
. . .
−θAijK(ij )
. . .
 i th block row
( 6 )
4.3 iBall – Scale up with Dynamic Update n =nd
The major computation cost for the closed form solutions lies in the matrix inverse S−1 . In the linear case , the size of S is ( dnd)× ( dnd ) ; and so its computational cost is manageable . However , this is not the case for non linear closed form solution since the matrix S in Eq ( 6 ) is of size n× n , where i=1 ni , which is the number of all the training samples . It would be very expensive to store this dense matrix ( O(n2 ) space ) and to compute its inverse ( O(n3 ) time ) ; especially when the number of training samples is very large , and the model receives new training examples constantly over time ( dynamic update ) . In this subsection , we devise an efficient algorithm to scale up the non linear closed form solution and efficiently update the model to accommodate the new training samples over time . The key of the iBall algorithm is to use the low rank approximation of the S matrix to approximate the original S matrix to avoid the matrix inversion ; and at each time step , efficiently update the low rank approximation itself .
After new papers in all the domains are seen at time step t + 1 , the new St+1 computed by Eq ( 6 ) becomes :

. . .
. . .
. . . i th block column j th block column nd
. . . Aij)K(i ) t+1 + λI
( 1 + θ j=1
. . .
. . .
−θAijK(ij ) t+1
. . .
 i th block row
( 7 ) where K(i ) t+1 is the new within domain kernel matrix for the i th domain and K(ij ) t+1 is the new cross domain kernel matrix for the i th and j th domains . The two new kernel matrix can be computed as follows :
K(i ) t+1 =
K(i ) t k(i ) t+1 t+1 ) ( k(i ) h(i ) t+1
K(ij ) t+1 = t
K(ij ) k(i∗j ) t+1 k(ij∗ ) h(i∗j∗ ) t+1 t+1
( 8 ) t+1(a , : ) , X(i ) t+1(a , b ) = k(x(i ) t+1(a , b ) = k(x(i ) where k(i ) t+1 is the matrix characterizing the similarity between new training samples and old training samples and can be computed as : k(i ) t ( b , :) ) ; h(i ) t+1 is the similarity matrix among new training samples and can be computed as : h(i ) t+1(b , : ) ) . k(i∗j ) t+1 is the matrix characterizing the similarity between new training samples in the i th domain and old training samples in the j th domain and can be computed as : k(i∗j ) t+1 ( a , b ) = k(x(i ) t+1 measures the similarity between old training samples in the ith domain and new training samples in the j th domain and can be computed as : k(ij∗ ) t+1(b , :) ) ; h(i∗j∗ ) is the similarity matrix between new training sam t ( b , : ) . Similarly , k(ij∗ ) t+1(a , :) ) , X(j ) t+1 = k(X(i ) t+1(a , : ) , x(i ) t ( a , : ) , x(j ) t+1 t+1(b , :) ) . t+1(a , : ) , x(j ) ples from both i th and j th domains and is computed as : h(i∗j∗ ) t+1 = k(x(i ) Given that St is a symmetric matrix , we can approximate it using top r eigen decomposition as : St ≈ UtΛtU t , where Ut is an n× r orthogonal matrix and Λt is an r× r diagonal matrix with the largest r eigenvalues of St on the diagonal . If we can directly update the eigen decomposition of St+1 after seeing the new training samples from all the domains , we can efficiently compute the new model parameters as follows : wt+1 = S
= Ut+1Λ
−1 t+1Yt+1 −1 t+1U t+1 ; . . . ; Y(nd ) t t+1Yt+1
( 9 ) t
; y(1 )
−1 where Yt+1 = [ Y(1 ) t+1 a r×r diagonal matrix , whose diagonal entries are the reciprocals of the corresponding eigenvalues of Λt+1 . In this way , we avoid the computationally costly matrix inverse in the closed form solution . t+1 ] . Here , Λ
; y(nd )
Compare St+1 with St , we find that St+1 can be obtained by inserting into St at the right positions with some rows and columns of the kernel matrices involving new training samples , ie,k(i ) . From this perspective , St+1 can be seen as the sum of the following two matrices : t+1 ,k(i∗j∗ ) t+1 ,k(ij∗ ) t+1,k(i∗j ) t+1 , h(i ) t+1 i th block column j th block column
αiK(i )
. . . t
0 . . .
0 0

. . .
. . .
. . .
. . .
. . .
. . .
. . .
+
αik

( i ) t+1
αih . . .
0 ( i ) t+1
αi ( k ( i ) t+1 i th block column )
 where we denote 1 + θnd def= ˜St + ∆S
+ λI t
. . .
0 . . .
−θAijK(ij )


˜St
0
−θAijk
∆S
( 10 ) j=1 Aij by αi . The top r eigendecomposition of ˜St can be directly written out from that of St as : ˜St ≈ ˜UtΛt ˜U t , where ˜Ut can be obtained by inserting into Ut corresponding rows of 0 , the same row positions as we insert into St the new kernel matrices . We propose Algorithm 1 to update the eigen decomposition of St+1 , based on the observation that St+1 can be viewed as ˜St perturbed by a low rank matrix ∆S . In line 5 of Algorithm 1 , the only difference between the partial QR decomposition and the standard one , is that since ˜Ut is already orthogonal , we only need to perform the Gram Schmidt procedure starting from the first column of P .
Building upon Algorithm 1 , we have the fast iBall algorithm ( Algorithm 2 ) for scaling up the non linear solution with dynamic model update . 4.4 iBall – Proofs and Analysis
In this subsection , we will provide some analysis regarding the optimality , the approximation quality as well as the computational complexity of our proposed algorithms .
A Correctness of the closed form solutions of the iBall linear and non linear formulations : In Lemma 1 , we prove that the closed form solution given in Eq ( 4 ) with

0 0 i th block row j th block column
. . .
−θAijk −θAijh
( ij∗ ) t+1 ( i∗j∗ ) t+1
. . .
( i∗j ) t+1

 i th block row
659 Algorithm 1 : Eigen update of St+1 Input : ( 1)eigen pair of St : Ut , Λt ; ( 2)feature matrices of new papers in each domain : x(i ) ( 3)adjacency matrix of domain relation graph A ; ( 4)balance parameters θ , λ Output : eigen pair of St+1 : Ut+1 , Λt+1 t+1 , i = 1 , . . . , nd ;
1 Obtain ˜Ut by inserting into Ut rows of 0 at the right positions ;
2 Compute k(i ) t+1,k(i∗j ) i = 1 , . . . , nd , j = 1 , . . . , nd ; t+1 , h(i ) t+1 ,k(ij∗ ) t+1 ,k(i∗j∗ ) t+1 for
3 Construct sparse matrix ∆S ; 4 Perform eigen decomposition of ∆S : ∆S = PΣP ; 5 Perform partial QR decomposition of
[ ˜Ut , P]:[ ˜Ut , ∆Q]R ← QR( ˜Ut , P ) ;
6 Set Z = R[Λt 0 ; 0 Σ]R ; 7 Perform full eigen decomposition of Z : Z = VLV ; 8 Set Ut+1 = [ ˜Ut , ∆Q]V and Λt+1 = L ; 9 Return : Ut+1 , Λt+1 .
Algorithm 2 : iBall –scale up with dynamic update t+1 , i = 1 , . . . , nd ;
Input : ( 1)eigen pair of St : Ut , Λt ; ( 2)feature matrices of new papers in each domain : x(i ) ( 3)citation count vectors of new papers in each domain : y(i ) ( 4)adjacency matrix of domain relation graph A ; ( 5)balance parameters θ , λ Output : ( 1 ) updated model parameters wt+1 , ( 2 ) t+1 , i = 1 , . . . , nd ; eigen pair of St+1 : Ut+1 , Λt+1
1 Update the eigen decomposition of St+1 using
Algorithm 1 as : St+1 ≈ Ut+1Λt+1U 2 Compute the new model parameters : t+1 ; wt+1 = Ut+1Λ
−1 t+1U t+1Yt+1 ;
3 Return : wt+1 , Ut+1 and Λt+1 .
S computed by Eq ( 5 ) is the fixed point solution to the linear formulation in Eq ( 2 ) and the closed form solution given in Eq ( 4 ) with S computed by Eq ( 6 ) is the fixedpoint solution to the non linear formulation in Eq ( 3 ) .
Lemma 1 . ( Correctness of closed form solution of the iBall lin ear and non linear formulations . ) For the closed form solution given in Eq ( 4 ) , if S is computed by Eq ( 5 ) , it is the fixed point solution to the objective function in Eq ( 2 ) ; and if S is computed by Eq ( 6 ) , it is the fixed point solution to the objective function in Eq ( 3 ) .
Proof . Omitted for brevity . See [ 16 ] for detail .
B Correctness of the eigen update of St+1 : The critical part of Algorithm 2 is the subroutine Algorithm 1 for updating the eigen decomposition of St+1 . According to Lemma 2 , the only place that approximation error occurs is the initial eigen decomposition of S0 . The eigen updating procedure won’t introduce additional error .
Lemma 2 . ( Correctness of Algorithm 1 . ) If St = UtΛtU t holds , Algorithm 1 gives the exact eigen decomposition of St+1 .
Proof . Omitted for brevity . See [ 17 ] for details .
C Approximation Quality : We analyze the approximation quality of Algorithm 2 to see how much the learned model parameters deviate from the parameters learned using the exact iBall non linear formulation . The result is summarized in Theorem 1 .
Theorem 1 . ( Error bound of Algorithm 2 . )
In Algo rithm 2 , if
< 1 , the error of the learned model
( i ) i /∈H λ t ( i ) i λ t+1 parameters is bounded by : wt+1 − ˆwt+12 ≤
Yt+12
( 11 ) i /∈H λ(i ) t+1)2(1 − δ ) i λ(i ) t
( where wt+1 is the model parameter learned by the exact iBall non linear formulation at time t+1 , ˆwt+1 is the updated model parameter output by Algorithm 2 from time t to t + 1 , λ(i ) t and λ(i ) t+1 are the largest i th eigenvalues of St and St+1 respectively , δ = ( ˜UtΛt ˜U t)F , H is the set of integers between 1 and r , ie , H = {a|a ∈ [ 1 , r]} . Proof . Suppose we know the exact St at time t and its top r approximation : ˆSt = UtΛtU t . After one time step , we can construct ∆S and the exact St+1 can be computed as St+1 = ˜St + ∆S . The model parameters learned by the exact non linear model is : t + ∆S)−1(˜St − ˜UtΛt ˜U wt+1 = S
−1 t+1Yt+1
= ( ˜St + ∆S)−1Yt+1
( 12 )
If we allow approximation as in Algorithm 2 , the approx imated model parameter is : −1 t+1Yt+1 = ( ˜UtΛt ˜U
ˆwt+1 = ˆS t + ∆S)−1Yt+1
( 13 )
Denote ˜St + ∆S by B and ˜UtΛt ˜U t + ∆S by C,we have the following :
B − CF = ˜St − ˜UtΛt ˜U tF i aiuiu t i /∈H λ(i )
≤ iF =tr( = = ≤ i a2 i a2 i |ai| i i uiu i a2 i ) i tr(uiu i ) where the last inequality is due to the following fact :
( 14 )
( 15 )
( 16 )
Denote C−1(B − C)F by δ , we know that
δ ≤ C−1FB − CF
≤
( i ) i /∈H λ t ( i ) i λ t+1
< 1
From matrix perturbation theory [ 11 ] , we will reach the following : wt+1 − ˆwt+12 = B−1Yt+1 − C−1Yt+12 ≤ B−1 − C−1FYt+12 ≤ C−12 Yt+12
( ≤ Yt+12
F B−CF 1−δ ( i ) i /∈H λ t t+1)2(1−δ ) ( i ) i λ
( 17 )
660 D Complexities : Finally , we analyze the complexities of Algorithm 1 and Algorithm 2 . In terms of time complexity , the savings are two folds : ( 1 ) we only need to compute the kernel matrices involving new training samples ; ( 2 ) we avoid the time consuming large matrix inverse operation . In terms of space complexity , we don’t need to maintain the huge St matrix , but instead store its top r eigen pairs which is only of O(nr ) space .
Theorem 2 . ( Complexities of Algorithm 1 and Algorithm 2 . )
Algorithm 1 takes O((n + m)(r2 + r2 ) ) time and O((n + m)(r+r ) ) space . Algorithm 2 also takes O((n+m)(r2+r2 ) ) time and O((n + m)(r + r ) ) space , where m is total number of new training samples .
Proof . Omitted for brevity .
5 . EXPERIMENTS older publications for the prediction of the latest ones . For authors , we start with 0.2 % initial training data and at each update add another 0.2 % training data and use the last 10 % for testing . For venues , we start with 20 % , add 10 % at each update and use last 10 % for testing .
The root mean squared error ( RMSE ) between the the actual citation and the predicted one is adopted for accuracy evaluation . All the experiments were performed on a Windows machine with four 3.5GHz Intel Cores and 256GB RAM .
Repeatability of Experimenal Results : The AMiner citation dataset is publicly available . We will release the code of the proposed algorithms through authors’ website . For all the results reported in this section , we set θ = λ = 0.01 in our joint predictive model . Gaussian kernel with σ = 5.1 is used in the non linear formulations . 5.2 Effectiveness Results
In this section , we design and conduct experiments mainly
We perform the effectiveness comparisons of the following to inspect the following aspects : nine methods :
• Effectiveness : How accurate are the proposed algorithms for predicting scholarly entities’ long term impact ?
• Efficiency : How fast are the proposed algorithms ?
5.1 Experiment Setup
We use the real world citation network dataset AMiner2 to evaluate our proposed algorithms . The statistics and empirical observations are described in Section 2 . Our primary task is to predict a paper ’s citations after 10 years given its citation history in the first three years . Thus , we only keep papers published between year 1936 and 2000 to make sure they are at least 10 years old . This leaves us 508,773 papers . Given that the citation distribution is skewed , the 10 year citation counts are normalized to the range of [ 0 , 7 ] . Our algorithm is also able to predict citation counts for other scholarly entities including researchers and venues . We keep authors whose research career ( when they publish the first paper ) begin between year 1960 and 2000 and venues that are founded before year 2002 . This leaves us 315,340 authors and 3,783 venues .
For each scholarly entity , we represent it as a three dimensional feature vector , where the i th dimension is the number of citations the entity receives in the i th year after its life cycle begins ( eg , paper gets published , researchers publish the first paper ) . We build a k nn graph ( k = 5 ) among different scholarly entities ; use METIS [ 13 ] to partition the graph into balanced clusters ; and treat each cluster as a domain . We set the domain number ( nd ) to be 10 for both papers and researchers ; and 5 for venues . The Gaussian kernel matrix of the cluster centroids is used to construct the domain domain adjacency matrix A .
To simulate the dynamic scenario where training samples come in stream , we start with a small initial training set and at each time step add new training samples to it . The training samples in each domain are sorted by starting year ( eg , publication year ) . In the experiment , for papers , we start with 0.1 % initial training data and at each update add another 0.1 % training samples . The last 10 % samples are reserved as test samples , ie , we always use information from
2http://arnetminer.org/billboard/citation
1 Predict 0 : directly predict 0 for test samples since ma jority of the papers have 0 citations .
2 Sum of the first 3 years : assume the total number of citations doesn’t change after three years .
3 Linear combine : combine training samples of all the domains for training using linear regression model .
4 Linear separate : train a linear regression model for each domain separately .
5 iBall linear : jointly learn the linear regression models as in our linear formulation .
6 Kernel combine : combine training samples of all the domains for training using kernel ridge regression model [ 19 ] .
7 Kernel separate : train a kernel ridge regression model for each domain separately .
8 iBall kernel : jointly learn the kernel regression models as in our non linear formulation .
9 iBall fast : proposed algorithm for speeding up the joint non linear model .
A Overall paper citation prediction performance . The RMSE result of different methods for test samples from all the domains is shown in Figure 5 . We have the following observations : ( 1 ) the non linear methods ( iBall fast , iBall kernel , Kernel separate , Kernel combine ) outperform the linear methods ( iBall linear , Linear separate , Linear combine ) and the straightforward ‘Sum of first 3 years’ is much better than the linear methods , which reflects the complex non linear relationship between the features and the impact . ( 2 ) The performance of iBall fast is very close to iBall kernel and sometimes even better , which confirms the good approximation quality of the model update and the possible denoising effect offered by the low rank approximation . ( 3 ) The iBall family of joint models is better than their separate versions ( Kernel separate , Linear separate ) . To evaluate the statistical significance , we perform a t test using 1.4 % of the training samples and show the p values in Table 2 . From the result , we see that the improvement of our method is significant . To investigate parameter sensitivity , we perform parametric studies with three parameters in iBall fast , namely , θ ,
661 Table 2 : p value of statistical significance
Predict 0
Linearcombine
Linearseparate iBalllinear iBall kernel
0
5.53e 16
6.12e 17
1.16e 13
Sum of first 3 years 1.56e 219
Kernelcombine
Kernelseparate iBall fast
1.60e 72
8.22e 30
3.39e 14
Figure 5 : Overall paper citation prediction performance comparisons . Lower is better .
Figure 6 : Author citation prediction performance comparison . Lower is better .
Figure 7 : Venue citation prediction performance comparison . Lower is better .
λ and r . Figure 8 shows that the proposed method is stable in a large range of the parameter space . B Domain by domain paper citation prediction performance . In Figure 9 we show the RMSE comparison results for two domains with different total training sizes . iBall kernel and its fast version iBall fast consistently outperform other methods in both of the domains . In the first domain , some linear methods ( Linear separate and Linear combine ) perform even worse than the baseline ( ‘Predict 0’ ) . C Prediction error analysis . We visualize the actual citation vs . the predicted citation using iBall as a heat map in Figure 10 . The ( x , y ) square means among all the test samples with actual citation y , the percentage that have predicted citation x . We observe a very bright region near the x = y diagonal . The prediction error mainly occurs in a bright strip at x = 1 , y ≥ 1 . This is probably due to the delayed high impact of some scientific work , as suggested by the blue and green lines in Figure 4 , ie , some papers only pick up attentions many years after they were published .
D Author and venue citation prediction performance . We also show the RMSE comparison results for the impact prediction of authors and venues in Figure 6 and 7 respectively . Similar observations can be made as the paper impact prediction , except that for the venue citation prediction , iBalllinear can achieve the similar performance as iBall fast and iBall kernel . This is probably due to the effect that venue citation ( which involves the aggregation of the citations of all of its authors and papers ) prediction is at a much coarser granularity , and thus a relatively simple linear model is sufficient to characterize the correlation between features and outputs ( citation counts ) . 5.3 Efficiency Results A Running time comparison : We compare the running time of different methods with different training sizes and show the result in Figure 11 with time in log scale . All the linear methods are very fast ( < 0.01s ) as the feature dimensionality is only 3 . Our iBall fast outperforms all other non linear methods and scales linearly .
Figure 11 : Comparison of running time of different methods . The time axis is of log scale .
Figure 12 : Quality vs . speed with 88,905 training samples .
B Quality vs . speed : Finally , we evaluate how the proposed methods balance between the prediction quality and speed . In Figure 12 , we show the RMSE vs . running time of different methods with 88,905 total training samples . For iBall fast , we show its results using different rank r for the
Figure 10 : Prediction error analysis : actual citation vs . predicted citation . Best viewed in color .
10002000300040005000600070008000900010000110000911112131415161718Training SizeRMSE iBall−fastiBall−kernelKernel−separateKernel−combineiBall−linearLinear−separateLinear−combinePredict 0Sum of first 3 years2000400060008000100001200014000141516171819221Training SizeRMSE iBall−fastiBall−kernelKernel−separateKernel−combineiBall−linearLinear−separateLinear−combinePredict 0Sum of first 3 years160018002000220024002600152253354455Training SizeRMSE iBall−fastiBall−kernelKernel−separateKernel−combineiBall−linearLinear−separateLinear−combinePredict 0Sum of first 3 yearsPredicted CitationActual Citation 0123456776543210001020304050607080910051152253x 10410−510−410−310−210−1100101102103Training SizeRunning Time ( second ) iBall−fastiBall−kernelKernel−separateKernel−combineLinear−separateiBall−linearLinear−combine10110210310408509095110511Running Time ( second)RMSE Sum of first 3 yearsKernel−separateKernel−combine≤ 0.1iBall−kerneliBall−fast662 ( a ) RMSE vs . θ
( b ) RMSE vs . λ
( c ) RMSE vs . r
Figure 8 : Sensitivity study on iBall fast : study the effect of the parameters θ , λ and r in terms of RMSE .
( a ) Prediction performance comparison in the first domain .
( b ) Prediction performance comparison in the second domain .
Figure 9 : Paper citation prediction performance comparison in two domains . low rank approximation . Clearly , iBall fast achieves the best trade off between quality and speed as its results all lie in the bottom left corner . 6 . RELATED WORK
In this section , we review the related work . Impact/popularity prediction : As a pilot study , Yan et al . [ 31 , 30 ] identify effective features to address citation count prediction problem . Davletov et al . [ 8 ] address the same problem by first clustering papers according to their temporal change in citation counts over time and assigning a polynomial to each cluster for regression . In light of the difficulty posed by power law distribution of citations , Dong et al . [ 9 ] instead consider whether a paper can increase the primary author ’s h index . Yu et al . [ 34 ] address predicting citation relations in heterogeneous bibliographical networks . A close line of work is to predict the popularity of other online contents , eg , posts , videos , TV series . Yao et al . [ 32 ] predict the long term impact of questions/answers . Notice that in terms of methodology , the method in [ 32 ] can be conceptually viewed as a special case of our iBall model when there are only two domains and the instance level correspondence across different domains ( eg , question answers association ) is known . Li et al . [ 15 ] conduct an study on popularity forecast of videos shared in social networks . They consider both the intrinsic attractiveness of a video and the influence from the underlying diffusion structure . Chang et al . [ 5 ] are the first to comprehensively study for predicting the popularity of online serials with autoregressive models . As online serials have strong sequence dependence and release date dependence , they develop an autoregressive model to capture the dynamic behaviors of audiences . Though the focus of this paper is to propose a tailored method to predict the long term citation counts , our method could be naturally applied to other related applications , eg , popularity prediction .
Multi task learning : Our joint model is also related to multi task learning as we jointly learn the models for each domain ( task ) . Multi task learning aims to improve the generalization performance of a learning task with the help of other related tasks . A key challenge in multi task learning is how to exploit the relationship among different tasks to allow information shared across tasks . One way is by sharing of parameters . In neural networks , hidden units are shared across tasks [ 2 ] . It can also be induced by assuming that the parameters used by all tasks are close to each other by minimizing the Frobenius norms of their differences in methods based on convex optimization formulations [ 10 ] . In Bayesian hierarchical models , parameter sharing can be imposed by assuming a common prior they share [ 33 ] . A second way is assuming a common basis of the parameter space . A low rank and sparse structure of the underlying predictive hypothesis has been applied to capture the tasks relatedness as well as outlier tasks [ 6 , 7 , 12].Our method is directly applicable when the correlation/similarity among different tasks is known and enjoys a closed form solution .
00000100001000100101109095110511115121251313514θRMSE00010010111009095110511115121251313514λRMSE1020304050090910920930940950960970980991rRMSE05122005115225Total Training Size ( %)RMSE iBall−fastiBall−kerneliBall−linearKernel−separateKernel−combineLinear−separateLinear−combinePredict 0Sum of first 3 years051220020406081121416182Total Training Size ( %)RMSE iBall−fastiBall−kerneliBall−linearKernel−separateKernel−combineLinear−separateLinear−combinePredict 0Sum of first 3 years663 In terms of computation , we also provide an efficient way to track the joint predictive model in the dynamic setting .
Scholarly data mining : Scholarly data can be viewed as a heterogeneous information network of papers , authors , venues and terms [ 23 ] . Mining of such scholarly data is often from following perspectives : ( 1 ) similarity search to find a similar scholarly entities given a query entity or a set of query entities [ 22 , 26 , 25 ] ; ( 2)literature recommendation to recommend related research papers on a topic [ 18 , 4 ] ; and ( 3)co author collaboration prediction , to predict if two researchers will collaborate in the future [ 20 , 21 ] . 7 . CONCLUSIONS
In this paper , we propose iBall – a family of algorithms for the prediction of long term impact of scientific work given its citation history in the first few years . The proposed algorithms collectively address a number of key algorithmic challenges in impact prediction ( ie , feature design , nonlinearity , domain heterogeneity and dynamics ) . It is flexible and general in the sense that it can be generalized to both regression and classification models ; and in both linear and non linear formulations ; it is scalable and adaptive to new training data . 8 . ACKNOWLEDGMENTS
We would like to thank Dr . Jie Tang for providing the dataset . This material is supported by the National Science Foundation under Grant No . IIS1017415 , by the Army Research Laboratory under Cooperative Agreement Number W911NF 09 2 0053 , by National Institutes of Health under the grant number R01LM011986 , Region II University Transportation Center under the project number 49997 33 25 .
The content of the information in this document does not necessarily reflect the position or the policy of the Government , and no official endorsement should be inferred . The US Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on . 9 . REFERENCES [ 1 ] N . Aronszajn . Theory of reproducing kernels . Transactions of the American mathematical society , pages 337–404 , 1950 .
[ 2 ] R . Caruana . Multitask learning . Machine learning ,
28(1):41–75 , 1997 .
[ 3 ] C . Castillo , D . Donato , and A . Gionis . Estimating number of citations using author reputation . In String processing and information retrieval , pages 107–117 . Springer , 2007 .
[ 4 ] K . Chandrasekaran , S . Gauch , P . Lakkaraju , and H . P . Luong . Concept based document recommendations for citeseer authors . In Adaptive Hypermedia and Adaptive Web Based Systems , pages 83–92 . Springer , 2008 .
[ 5 ] B . Chang , H . Zhu , Y . Ge , E . Chen , H . Xiong , and C . Tan .
Predicting the popularity of online serials with autoregressive models . In CIKM , pages 1339–1348 , 2014 .
[ 6 ] J . Chen , J . Liu , and J . Ye . Learning incoherent sparse and low rank patterns from multiple tasks . In KDD , pages 1179–1188 , 2010 .
[ 7 ] J . Chen , J . Zhou , and J . Ye . Integrating low rank and group sparse structures for robust multi task learning . In KDD , KDD ’11 , pages 42–50 , 2011 .
[ 8 ] F . Davletov , A . S . Aydin , and A . Cakmak . High impact academic paper prediction using temporal and topological features . In CIKM , pages 491–498 , 2014 .
[ 11 ] G . H . Golub and C . F . Van Loan . Matrix Computations
( 3rd Ed ) Johns Hopkins University Press , Baltimore , MD , USA , 1996 .
[ 12 ] A . Jalali , S . Sanghavi , C . Ruan , and P . K . Ravikumar . A dirty model for multi task learning . In NIPS , pages 964–972 , 2010 .
[ 13 ] G . Karypis and V . Kumar . A fast and high quality multilevel scheme for partitioning irregular graphs . SIAM J . Sci . Comput . , 20(1 ) , Dec . 1998 .
[ 14 ] D . D . Lee and H . S . Seung . Learning the parts of objects by non negative matrix factorization . Nature , 401(6755):788–791 , 1999 .
[ 15 ] H . Li , X . Ma , F . Wang , J . Liu , and K . Xu . On popularity prediction of videos shared in online social networks . In CIKM , pages 169–178 , 2013 .
[ 16 ] L . Li and H . Tong . The child is father of the man : Foresee the success at the early stage . CoRR , abs/1504.00948 , 2015 .
[ 17 ] L . Li , H . Tong , Y . Xiao , and W . Fan . Cheetah : fast graph kernel tracking on dynamic graphs . In SDM , pages 280–288 , 2015 .
[ 18 ] X . Liu , Y . Yu , C . Guo , and Y . Sun . Meta path based ranking with pseudo relevance feedback on heterogeneous graph for citation recommendation . In CIKM , CIKM ’14 , pages 121–130 , 2014 .
[ 19 ] C . Saunders , A . Gammerman , and V . Vovk . Ridge regression learning algorithm in dual variables . In ICML , ICML ’98 , pages 515–521 , 1998 .
[ 20 ] Y . Sun , R . Barber , M . Gupta , C . C . Aggarwal , and J . Han .
Co author relationship prediction in heterogeneous bibliographic networks . In ASONAM , pages 121–128 . IEEE , 2011 .
[ 21 ] Y . Sun , J . Han , C . C . Aggarwal , and N . V . Chawla . When will it happen ? : relationship prediction in heterogeneous information networks . In WSDM , pages 663–672 . ACM , 2012 .
[ 22 ] Y . Sun , J . Han , X . Yan , P . S . Yu , and T . Wu . Pathsim :
Meta path based top k similarity search in heterogeneous information networks . VLDB , 4(11):992–1003 , 2011 .
[ 23 ] Y . Sun , Y . Yu , and J . Han . Ranking based clustering of heterogeneous information networks with star network schema . In KDD , pages 797–806 . ACM , 2009 .
[ 24 ] J . Tang , J . Zhang , L . Yao , J . Li , L . Zhang , and Z . Su . Arnetminer : extraction and mining of academic social networks . In KDD , pages 990–998 , 2008 .
[ 25 ] H . Tong and C . Faloutsos . Center piece subgraphs : problem definition and fast solutions . In KDD , pages 404–413 , 2006 . [ 26 ] H . Tong , C . Faloutsos , and J . Pan . Fast random walk with restart and its applications . In ICDM , pages 613–622 , 2006 . [ 27 ] B . Uzzi , S . Mukherjee , M . Stringer , and B . Jones . Atypical combinations and scientific impact . Science , 342(6157):468–472 , 2013 .
[ 28 ] D . Wang , C . Song , and A L Barab´asi . Quantifying long term scientific impact . Science , 342(6154):127–132 , 2013 .
[ 29 ] S . Wuchty , B . F . Jones , and B . Uzzi . The increasing dominance of teams in production of knowledge . Science , 316(5827):1036–1039 , 2007 .
[ 30 ] R . Yan , C . Huang , J . Tang , Y . Zhang , and X . Li . To better stand on the shoulder of giants . In JDCL , pages 51–60 , 2012 .
[ 31 ] R . Yan , J . Tang , X . Liu , D . Shan , and X . Li . Citation count prediction : learning to estimate future citations for literature . In CIKM , pages 1247–1252 , 2011 .
[ 32 ] Y . Yao , H . Tong , F . Xu , and J . Lu . Predicting long term impact of CQA posts : a comprehensive viewpoint . In KDD , pages 1496–1505 , 2014 .
[ 33 ] K . Yu , V . Tresp , and A . Schwaighofer . Learning gaussian
[ 9 ] Y . Dong , R . A . Johnson , and N . V . Chawla . Will this paper processes from multiple tasks . In ICML , 2005 . increase your h index ? scientific impact prediction . In WSDM , 2015 .
[ 10 ] T . Evgeniou and M . Pontil . Regularized multi–task learning . In ACM SIGKDD , pages 109–117 , 2004 .
[ 34 ] X . Yu , Q . Gu , M . Zhou , and J . Han . Citation prediction in heterogeneous bibliographic networks . In SDM , pages 1119–1130 , 2012 .
664
