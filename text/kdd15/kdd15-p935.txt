A PCA Based Change Detection Framework for
Multidimensional Data Streams
Abdulhakim Qahtan King Abdullah University of Science and Technology
Thuwal , 23955 , SA abdulhakimqahtan@kaustedusa
Suojin Wang
Texas A&M University
College Station , TX , 77843 , USA sjwang@stattamuedu
Basma Alharbi
King Abdullah University of Science and Technology
Thuwal , 23955 , SA basmaharbi@kaustedusa
∗
Xiangliang Zhang
King Abdullah University of Science and Technology
Thuwal , 23955 , SA xiangliangzhang@kaustedusa
ABSTRACT Detecting changes in multidimensional data streams is an important and challenging task . In unsupervised change detection , changes are usually detected by comparing the distribution in a current ( test ) window with a reference window . It is thus essential to design divergence metrics and density estimators for comparing the data distributions , which are mostly done for univariate data . Detecting changes in multidimensional data streams brings difficulties to the density estimation and comparisons . In this paper , we propose a framework for detecting changes in multidimensional data streams based on principal component analysis , which is used for projecting data into a lower dimensional space , thus facilitating density estimation and change score calculations . The proposed framework also has advantages over existing approaches by reducing computational costs with an efficient density estimator , promoting the change score calculation by introducing effective divergence metrics , and by minimizing the efforts required from users on the threshold parameter setting by using the Page Hinkley test . The evaluation results on synthetic and real data show that our framework outperforms two baseline methods in terms of both detection accuracy and computational costs .
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications—Data Mining
Keywords Data Streams , Density Estimation , Change Detection , Principal Component Analysis ∗Corresponding author .
1 .
INTRODUCTION
Discovering changes in the properties of data is very important for building data mining models . Change detection covers a wide range of applications such as intrusion detection in computer networking [ 20 ] , suspicious motion detection in vision systems [ 10 ] , studying the effects of nuclear radiation on the environment [ 11 ] , and online clustering and classification . For example , change detection can be used in online classification for reporting when the classifier should be re trained ( only if a change in the data stream is observed ) . The problem of change detection has been widely studied and referred to as data evolution [ 1 ] , event detection [ 6 ] or change point detection [ 9 , 11 ] .
Unsupervised window based change detection is based on comparing the distribution in a current stream window with a reference distribution [ 2 , 5 , 9 , 11 , 12 , 17 ] , where density estimation techniques and divergence metrics are essential to model and compare the distributions . Modeling data distribution is a challenging task for multidimensional data , where most of the existing approaches either do not support data streams [ 12 , 17 ] or work only for univariate data [ 2 , 11 , 19 ] . However , multidimensional data streams are ubiquitous in our application fields , because change detection in multidimensional data streams is receiving a lot of attention recently . Dasu et al . [ 5 ] used the kdq tree data structure to model data distribution . However , this technique becomes inaccurate when data dimensionality increases to a moderate level , ( eg 10 ) . Kawahara and Sugiyama [ 9 ] used the density ratio estimation that is based on the Kullback Leibler Importance Estimation Procedure ( KLIEP ) to model data distribution . The method ’s complexity is quadratic with respect to ( wrt ) the window size , which increases the computational costs significantly when large windows are used . Small windows make the model cheap , but they also make it sensitive and cause many false positives .
Here , we propose a new framework for detecting abrupt changes in unlabeled multidimensional data streams . The framework uses Principal Component Analysis ( PCA ) to project the original data stream into a lower dimensional space . Different types of changes in the original data variables , such as changes in mean and variance of data variables or changes in correlations between variables , can be observed on one or several Principal Components ( PCs ) , which are orthogonal by their construction . In our proposed framework , data are projected onto each of the selected PCs , resulting in multiple univariate data streams , which exhibit variations if the
935 original data streams have changes . We monitor these streams of projections separately , and we quantitatively measure the variation in each stream by a change score . Determining the final changescore is done by taking the aggregation from all selected components . The main advantage of monitoring changes on selected PCs separately is the reduction in computational costs , compared with monitoring all original variables individually or collectively .
In addition to using PCA , the accuracy and efficiency of our pro posed framework in change detection are further assured by 1 . Employing an accurate and efficient density estimation method , which inherits the efficiency of the KDE Track in our previous study [ 16 ] and improves estimation accuracy by including a new method of allowing adaptive bandwidth settings . 2 . Using divergence metrics for valid distribution comparisons : the Log LikeliHood ( LLH ) [ 17 ] , a symmetric Kullback Leibler ( KL ) divergence , and the intersection area of two distributions . 3 . Applying a dynamic threshold setting to change detection , which reduces the effort required from users for parameter ’s setting .
We validated the proposed framework on several synthetic and real world datasets including various types of changes . We also compared the performance of the framework with two baseline methods , namely kdq tree [ 5 ] and another PCA based change detection [ 12 ] that was designed differently not for data streams . The experimental results show that our framework with the divergence metric of the intersection area of the two density functions is more accurate in detecting changes in most of the datasets with fewer false alarms and higher efficiency when used with histograms . The results also show that our framework is robust against the parameters’ settings and performs well for a wide range of the parameters . The rest of the paper is organized as follows . Section 2 presents related work and Section 3 discusses the theoretical bases for our framework . Sections 4 and 5 present our change detection framework and evaluation results . Section 6 concludes our work with perspectives .
2 . RELATED WORK
Table 1 summarizes the key characteristics of the existing methods for change detection : ability to handle MultiVariate ( MV ) data , applicability for streaming data , use of distribution comparison or predictive model , as well as how to set the threshold . As we study the change detection in unlabeled multivariate data streams , we focus our discussion on related work with MV=Yes . Table 1 : Summary of the key characteristics of change detection methods ( MV = Multivariate ) .
Technique kdq tree [ 5 ] PCA SPLL [ 12 ] KLIEP [ 9 ] Kifer [ 11 ] ADWIN [ 2 ] Density test [ 17 ] CF [ 19 ]
MV
Yes Yes Yes No No Yes No
Data streams
Compare Distribution /
Prediction
Yes No Yes Yes Yes No Yes
Compare Distribution Compare Distribution Compare Distribution Compare Distribution other
Compare Distribution
Prediction
Threshold Settings Bootstrap
Fixed Fixed
Bootstrap Dynamic Bootstrap
Fixed
Dasu et al . [ 5 ] developed a window based approach for detecting changes in unlabeled multidimensional data streams . Their approach follows a 3 step process : 1 ) updating the test distribution over the current window ; 2 ) computing the change score between the test and reference distributions ; and 3 ) emitting an alarm signal if the change score reaches the threshold specified using bootstrap sampling and the permutation test used in [ 11 ] . This approach is efficient because the change score is updated from the previous value . However , modeling the distributions of the reference and test windows is done by space partitioning using kdq trees , which becomes inaccurate when the dimension of the data increases .
A method called the Kullback Leibler Importance Estimation Procedure ( KLIEP ) was proposed to directly measure the distribution difference without density estimation [ 18 ] . The method is unsupervised and works for multivariate data streams . An online version of KLIEP was studied in [ 9 ] and [ 15 ] . One common limitation of these methods is the user based settings of key parameters , such as the threshold for change detection , regularization parameter λ , and learning rate η . Moreover , small windows have to be used to reduce the running time that cause false alarms of changes . A statistical test , called the density test [ 17 ] , determines whether the newly observed data S are sampled from the same underlying distribution as the reference dataset S . The method works for unlabeled multivariate data ; however , the high computational cost of the method reduces its applicability for data stream problems .
Change detection based on PCA in multidimensional data ( not streams ) was performed by considering only PCs with small eigenvalues in [ 12 ] . PCs with small eigenvalues are selected for reducing data dimensionally because they are analyzed to be more likely influenced by a change . However , there is an issue in the analysis of selecting such PCs : changes are assumed to occur in projected data on PCs with uniform distributions . In fact , changes occurring in data characteristics are demonstrated as variations in the original features’/variables’ values . We will analyze and show that using PCs with large eigenvalues better capture the changes .
The framework proposed in this paper is featured as MV=Yes , Data Streams = Yes , Compare Distribution , Threshold Settings = Dynamic . It differs from existing approaches on 1 ) what data to monitor ( original data or projections on PCs ) ; 2 ) how to estimate data distribution functions ; 3 ) how to compare distributions ; and 4 ) how to set a threshold for reporting changes .
3 . THEORETICAL ANALYSES
We discuss the building blocks for our framework of change detection in multidimensional data streams including PCA for change detection and divergence metrics for distribution comparison . 3.1 Principal Component Analysis ( PCA )
2
P1 = argmax P=1
Principal component analysis is a well known technique for dimensionality reduction . The central idea of PCA is to identify a set of orthogonal ( uncorrelated ) principal components from the original feature space such that the new space spanned by PCs better represents the data ( ie , the 1st PC captures the largest variability , the 2nd PC captures the 2nd largest variability , and so on ) . Let S = {x1,··· , xn} be a dataset containing n data samples with xi ∈ Rd and i = 1,··· , n . The first principal component P1 maximizes the variance of the projected data on it : n n n i=1 ( xi − x ) ( xi − x)T is the sample covariwhere Σ = 1 n−1 i=1 xi is the sample mean . The residance matrix and x = 1 i = xi − P1P1 n ual x T xi , represents the distortion introduced by reducing the dimensionality to 1 . The second principal component P2 is selected in the direction that maximizes the projected variance from the set of vectors that are orthogonal to P1 . Let S = {x n} be the set of the residuals ; P2 is defined as : 1,··· , x 1 n i − PT x2
PT xi − PT x
= argmax P=1
PT ΣP . n
PT ΣP ,
PT x
1 n i=1
P2 = argmax P=1 , PT P1=0 i=1
= argmax P=1 , PT P1=0
936 In other words , the k th principal component is equal to the eigenvector that corresponds to the k th largest eigenvalue of the covariance matrix Σ . 3.2 PCA for Change Detection
Change detection in data streams refers to the problem of finding time points , where for each point , there exists a significant change in the current data distribution . A typical window based solution is to extract a fixed S1 ( reference window ) from streaming samples and to update an S2 ( test window ) with newly arriving samples [ 5 , 11 ] . Changes are then detected by measuring the difference between the distributions in S1 and S2 .
Modeling the data distribution and selecting a comparison criterion are essential for change detection in data streams . However , density estimation of multidimensional data is difficult . It becomes less accurate and more computationally expensive with increasing dimensionality . In our framework , we apply PCA to project the multidimensional data samples from the stream on the principal components to obtain multiple streams of 1D data . Density estimation , distribution comparison , and change score calculations can then be conducted in parallel on those 1D data streams . Compared with projecting the data on the original coordinates ( ie , using the original variables ) , projecting on PCs has the following advantages : 1 ) it allows the detection of changes in data correlations , which cannot be detected in the original individual variables ; 2 ) it can guarantee that any changes in the original variables are reflected in PC projections ; and 3 ) it reduces the computation cost by discarding trivial PCs . We use a 2D Gaussian example to theoretically analyze how different types of changes are reflected in PC projections . For simplicity , we assume the data without change has a mean . A widely µ = [ 0 , 0]T and the covariance matrix Σ1 = used divergence metric , KL divergence is used for distribution comparison , which is defined as DKL ( g||f ) =
σ2 g(x ) g(x ) log r σ2
( 1 ) dx . r x f ( x )
Here , f represents the Probability Density Function ( PDF ) before the change and g represents the PDF after the change .
LEMMA 1 . For univariate normal distributions f , g where f ∼
N ( µ1 , σ2
1 ) and g ∼ N ( µ2 , σ2 2 ) ( µ1 − µ2)2
DKL ( g||f ) =
+
1 2
2σ2 1
σ2
2 σ2 1
− 1 − log
σ2 2 σ2 1
.
( 2 )
PROOF . Eq ( 2 ) is obtained by substituting the normal distribu tion expression in Eq ( 1 ) and simplifying the formula .
σ2
1 ) Case 1 : change of correlation When changes happen to the correlation of the two variables , the covariance matrix after the change can be Σ2 = where η > 0 . r + η r + η
σ2
,
If PCA is not applied , data are projected on the original coordinates u1 = [ 1 , 0]T and u2 = [ 0 , 1]T . Before the change of correlation , the projection on u1 has the distribution f u 1 Σ1u1 ) = N ( 0 , σ2 ) . After the change , the projection on u1 has the distri1 Σ2u1 ) = N bution gu = N ( 0 , σ2 ) , which is the same as f u ing the difference between f u 1 and gu DKL(gu
1 . The KL divergence measur1 can be calculated by Eq ( 2 )
0 η
1 ∼ N ( 0 , uT
1 ∼ N ( 0 , uT
0 , σ2 + uT 1
1 ||f u
1 ) = 0 . u1
0
η
0 η bution f u
0 , σ2 + uT 2
2 ∼ N ( 0 , uT
2 Σ2u2 ) = N
2 ∼ N ( 0 , uT
1 ∼ N ( 0 , PT
Similarly , the projection on u2 before the change has the distri2 Σ1u2 ) = N ( 0 , σ2 ) . After the change , the u2 projection is gu 0 η 2 ||f u = N ( 0 , σ2 ) , which is the same as f u 2 . That is , DKL(gu 2 ) = 0 √ too , and the change will not be detected . √ 2 with eigenvalue λ1 = σ2 + r and the second PC P2 = [ −1 , 1]T / 2 with eigenvalue λ2 = σ2 − r . On P1 , the projection before the 1 Σ1P1 ) = N ( 0 , λ1 ) and after the change change is f P =
If we apply PCA , we obtain the first PC P1 = [ 1 , 1]T / is gP P1 N ( 0 , λ1 + η ) . The KL divergence value when comparing f P 1 is DKL(gP gP Similarly on the second PC , the projection before change is f P
2 ∼ 2 Σ1P2 ) = N ( 0 , λ2 ) , and the projection after change is
N ( 0 , PT 2 ∼ N ( 0 , PT gP N ( 0 , λ2 − η ) . From Eq ( 2 ) , the change score DKL(gP
0 η
η
1 ∼ N ( 0 , PT
1 Σ2P1 ) = N
0 η
2 Σ2P2 ) = N
− log λ1+η
0 , λ2 + PT 2
0 , λ1 + PT 1
= 2 ||f P
1 ||f P
1 ) = 1 2
1 and
P2
λ1
λ1
0
0
η
η
.
− η
2 ) = i ) > 0 with i ||f P
,
1 2 i = 1 , 2 , which means that changes will be detected on PCs .
. We can see that DKL(gP
+ log λ2 λ2−η
λ2
In conclusion , in the case of correlation changes , zero changescores are obtained when checking the difference in the original variables . However , non zero change scores are calculated in PCA projections , which indicates that changes are detectable . 2 ) Case 2 : change of variance When changes happen to the variance of the two variables , the covariance matrix after the change can be Σ2 = where ηi ≥ 0 , i = 1 , 2 .
σ2 + η1
σ2 + η2 r r
1 2
σ2
η1+η2
1 ∼ N ( 0 , uT 2 ∼ N ( 0 , uT i ||f u
, i = 1 , 2 . 1 ∼ N ( 0 , PT 2 ∼ N ( 0 , PT
1 ∼ N ( 0 , σ2 ) and gu 2 ∼ N ( 0 , σ2 ) and gu
1 Σ2P1 ) = N ( 0 , λ1 + 1 2 Σ2P2 ) = N ( 0 , λ2 + 1 i ||f P
In the original space , before and after the change , we compare the projections on u1 , f u 1 Σ2u1 ) = N ( 0 , σ2 + η1 ) . Similarly on u2 , the projections before and afηi ter the change are f u 2 Σ2u2 ) = N ( 0 , σ2 + η2 ) , respectively . Then we have DKL(gu i ) = σ2 − log σ2+ηi If we apply PCA , before and after the change , we have on P1 , 1 ∼ N ( 0 , λ1 ) and gP f P 2 ( η1 + η2) ) . Similarly on P2 , the projections before and after changes are 2 ∼ N ( 0 , λ2 ) and gP f P 2 ( η1 + η2) ) , respectively . Then the change score is DKL(gP i ) = , i = 1 , 2 . We see that any changes 1 2 with positive η1 or η2 can result in non zero DKL(gP i ) , i = 1 , 2 . In other words , changes in the variances of the original variables are observable on projections of PCs . 3 ) Case 3 : change of mean Assume that the mean of the data shifts to a new one , µnew = [ η1 , η2]T . In the original space , the projections on u1 before and 1 ∼ N ( η1 , σ2 ) , respecafter the change are f u 2 ∼ N ( 0 , σ2 ) and tively . Similarly on u2 , the projections are f u i ||f u 2 ∼ N ( η2 , σ2 ) , respectively . Then DKL(gu gu 2σ2 , i = 1 , 2 .
1 ∼ N ( 0 , σ2 ) and gu
− log 2λi+η1+η2 i ) = η2 i ||f P
2λi
2λi i
1 ∼ N ( µT
1 ∼ N ( 0 , λ1 ) and gP
If PCA is applied , the projections on P1 before and after the newP1 , λ1 ) , respecchange are f P tively . Then DKL(gP . Similarly on P2 , we newP2 , λ2 ) . Then compare f P 2 ||f P . We can see that a mean shift occurDKL(gP ring at any original variables is reflected in the projections on PCs ,
2 ∼ N ( 0 , λ2 ) and gP 2 ) = ( η1−η2)2
1 ) = ( η1+η2)2
2 ∼ N ( µT
1 ||f P
4λ2
4λ1
937 i ||f P leading to the non zero change score DKL(gP i ) , i = 1 , 2 . There is one special case where the mean shifts in the direction in parallel with one PC Pi . For example , η1 = η2 in µnew indicates the mean shifts in the direction of P1 . In such a case , change score on the other PC is zero , except that on this PC Pi , ( eg , DKL(gP 2 ) = 0 in the example ) . It is worth noting that the shift in the direction of Pi is fully captured by the projection on Pi , both the magnitude and the direction .
1 ) is non zero , and DKL(gP
1 ||f P
2 ||f P i=1 j=1 λj
λid that satisfyk
Our analysis shows that changes in the original variables are reflected in PC projections . However , it is expensive to examine projections of each PC to detect changes , especially when data dimensionality is high . We select the first k PCs with large eigenvalues λi ≥ 0999 Such a threshold on the percentage of explained variance is a conservative setting . It ensures selecting a sufficient number of PCs even when the distribution of λ is flat . An experimental evaluation of this threshold setting will be presented in Section 5 . The discarded PCs have very small ( negligible ) eigenvalues . The variance of projected data on these PCs are also very small ( 0.1 % at most ) . Densities with small variances are difficult to estimate and to compare due to their sensitivity to sample size and density model parameters . False alarms of changes may be reported due to the error in the density estimation . Thus , discarding such PCs has benefits in reducing the computational cost and minimizing false positive rates . When PCA is employed in [ 12 ] for change detection , PCs with small eigenvalues are selected but those with large eigenvalues are discarded , according to the analysis that changes are more noticeable from the projected data on the space spanned by PCs with small eigenvalues . It should be noted that the analysis in [ 12 ] assumed changes to occur in the projected data on PCs . However , in practice , changes of data characteristics are demonstrated as variations in the original features’/variables’ values . 3.3 Divergence Metrics Divergence metrics are crucial for computing change scores . The KL divergence defined in Eq ( 1 ) is a non negative ( ≥ 0 ) and nonsymmetric measure . It is 0 when the two distributions are completely identical , and becomes larger as the two distributions deviate from each other . The non symmetry property of the DKL complicates the procedure of setting the threshold for detecting changes in data streams . To overcome the problem of the KL divergence , we focus our study on three important divergence metrics . The first divergence metric is a modified symmetric KL divergence [ 14 ]
DM KL(ˆg|| ˆf ) = max
DKL
ˆg|| ˆf
, DKL
.
( 3 )
ˆf||ˆg
This divergence metric mimics powerful order selection tests developed in current statistics literature ; see , for example , [ 8 ] .
The second divergence metric we use is a measure of the inter section area under the curves of two density functions [ 3 ] ,
ˆg|| ˆf
= 1 −
DA min x
ˆf ( x ) , ˆg(x ) dx .
( 4 )
This DA takes values in [ 0 , 1 ] , where the value one means that the two distributions are completely different and zero means the two distributions are identical . This measure can also be computed using numerical integration techniques on the intersection area .
The third divergence metric that we study is based on measuring the likelihood of the data samples in the test window to be extracted from the same distribution as the reference window . It is a modified version of the LLH metric used in [ 17 ] . Let ˆf be the density function estimated using data samples in reference win dow S1 . The log likelihood of the test window S2 wrt LLH(S2| ˆf ) = log and S2 is
ˆf is . The divergence between S1
ˆf ( x ) x∈S2
DLLH ( S1 , S2| ˆf ) =
LLH(S2| ˆf )
|S2|
− LLH(S1| ˆf )
|S1|
.
( 5 )
When the data distribution is stationary and the samples in S1 and S2 are independent and identically distributed , DLLH will have a limiting normal distribution with mean 0 . The normality of DLLH was used in [ 17 ] to define a critical region such that the divergence values falling in that region indicate a change in the distribution from S1 to S2 .
When there is a change , DLLH can be extremely positive or negative depending on the type of change . In order to use this metric in our framework , we take
DLLH ( S1 , S2| ˆf ) = fififififi LLH(S2| ˆf )
|S2| fififififi
− LLH(S1| ˆf )
|S1|
( 6 ) as the divergence metric , which takes values in the interval [ 0,∞ ) . This metric is asymmetric and the change from S2 to S1 might be easier to detect . Song et al . [ 17 ] suggested using the average of DLLH ( S1 , S2| ˆf ) and DLLH ( S1 , S2|ˆg ) as the final change score , where ˆg is the PDF estimated using the samples in S2 . However , computing the DLLH ( S1 , S2|ˆg ) will require computing LLH(S1|ˆg ) after receiving every data sample from the stream as the density function ˆg will change , which is very expensive to deploy for data streams . We leave this issue for future study .
4 . CHANGE DETECTION FRAMEWORK In this section , we present our framework for change detection in data streams . The different components of the framework will be discussed in detail including the divergence metrics , the density estimation , and the dynamic threshold setting . The framework is given in Algorithm 1 , where DM denotes any divergence metric . 4.1 Setting Windows
Line 3 in Algorithm 1 sets the reference window S1 to be the first w samples arriving after the change point tc . Intuitively , when a data distribution shifts to a new one , the reference window should be updated to represent the new distribution . This update also enables the detection of further changes . Line 8 sets the test window S2 as a collection of w samples after the reference window . This S2 will slide along the data stream to include the newest w samples .
The window size w is usually set according to the application problems . A small window size will allow for detecting short term changes and reduces the delay but may lead to a large number of false positives . A large window size will make the algorithm more robust against short term changes but may miss alarms [ 1 ] . The setting of this parameter is usually left to the user to give them the ability to monitor the long/short term changes , depending on their interests and the application sensitivity against changes [ 1 ] . 4.2 Projecting the Data for 99.9 % of data variance ( ie , k
After receiving the first w data samples in reference window S1 , PCA is applied to extract the principal components from S1 . The first k PCs with the largest eigenvalues are selected if they account ≥ 0999 ) The data in the reference and test windows are then projected on these k components . On each component , projections of the reference and test windows are compared and a change score value is recorded .
λid j=1 λj i=1
938 P1 , P2 , · · · , Pk
Algorithm 1 Change Detection Framework Parameters : window size w , ξ , δ Online flow in : streaming data S = {x1 , · · · , xt , · · · } Online output : time t when detecting a change Procedure : 1 : Initialize tc = 0 , step = min(0.05w , 100 ) 2 : Initialize Sc , m , M to NULL . 3 : Set reference window S1 = {xtc+1 , · · · , xtc+w} 4 : Extract principal components by applying PCA on S1 to obtain 5 : Project S1 on P1 , P2 , · · · , Pk to obtain ˘S1 6 : ∀ i ( 1 ≤ i ≤ k ) estimate ˆfi using data of the i th component of ˘S1 7 : Clear S1 and ˘S1 8 : Set test window S2 = {xtc+w+1 , · · · , xtc+2w} 9 : Project S2 on P1 , P2 , · · · , Pk to obtain ˘S2 10 : Clear S2 11 : Estimate ˆgi using data of the i th component of ˘S2 12 : while a new sample xt arrives in the stream do Project xt on P1 , P2 , · · · , Pk to obtain ˘xt 13 : Remove ˘xt−w from ˘S2 14 : ∀ i ( 1 ≤ i ≤ k ) update ˆgi using ˘x(i ) 15 : if mod(t , step ) = 0 then 16 : 17 : and ˘x(i ) t−w
ˆgi|| ˆfi t i
DM curScore = max if Change(curScore , Sc , m , M , ξ , δ ) then Report a change at time t and set tc = t Clear ˘S2 and GOTO step 2
18 : 19 : 20 : 21 : Subprocedure : Change(curScore , Sc , m , M , ξ , δ ) 22 : Update Sc to include the curScore in the average . 23 : new_m = m + Sc − curScore + δ 24 : if |new_m| > M then new_M = new_m . 25 : 26 : τt = ξ ∗ Sc 27 : if curScore > τt then 28 : 29 : else 30 : M = new_M , m = new_m 31 : return True return False
The maximum value among the k change score values is considered as the final change score . Any new data sample is projected on the k components and the density functions of the projection of the test window are updated and compared with the reference densities .
When the number of considered principal components k is very large , maintaining an accurate PDF for data projections becomes very expensive . A tradeoff between accuracy and efficiency should be considered . Histograms , the oldest and most efficient density estimators , can be used to estimate PDFs ˆfi and ˆgi . The bins of the histogram are specified using an origin point x0 and a bin width h . The bins are defined as the intervals [ x0 + mh , x0 + ( m + 1)h ] for positive and negative integers m . The density at a given point is estimated using ˆp(x ) = Ni/(N ∗ h ) , where N is the total number of samples and Ni is the number of samples in the same bin as x . Note that the bin width of the histogram can vary from one bin to another . Histograms provide discontinuous , less accurate density estimation . The estimated density might change considerably based on the selected origin point x0 and the bin width h . For such reasons , histograms can be used only when efficiency is an important issue . In this paper , we use histograms for estimating the density in experiments with high dimensional data . 4.3 Estimating Density Functions estimated . KDE Track , a dynamic density estimator we studied in [ 16 ] , adapts KDE to handle the evolving underlying distribution in data streams . It gains linear time complexity by adopting linear interpolation and adaptive resampling . However , it inherits the issue of bandwidth setting in KDEs . A small bandwidth value causes more fluctuation in the density function ’s curve , which might give incorrect information about the density , whereas selecting a large bandwidth value over smoothes the function ’s curve and hides information . In data streams , density estimators should update the bandwidth h online rather than using a fixed setting .
In [ 16 ] , we selected the bandwidth for KDE Track using the normal rule , which is the most widely used method for selecting the bandwidth . It selects the bandwidth with the assumption that the density is normal . It is efficient , but the estimated densities are over smoothed when the density deviates much from normality , ( eg , in the cases of multimodal data ) .
In this paper , we employ KDE Track with an adaptive bandwidth h for density estimation . Merits of both efficiency and accuracy advance the calculation of divergence scores and thus change detection . We minimize the effect of the normality assumption of f by estimating f , the second derivative of f . The motivation for doing this is that the estimated ˆf may often be a better approximation of f than assuming f to be a normal density . Using the KDE Track model allows for estimating ˆf in constant time and memory cost . In this way , the estimated ˆf will serve two roles . First , it is used to approximate R(f ) = ( f(x))2 dx to estimate the bandwidth ˆh .
Second , it is used as a more accurate indicator of the high curvature of the density function , which facilitates the adaptive resampling in KDE Track for obtaining more accurate estimation .
The new KDE Track1 , a more advanced density estimator , is used for estimating the density functions of the projected data of S1 ( line 6 ) and S2 ( line 11 ) on the considered PCs , as well as for updating the test densities when a new sample arrives ( lines 1315 ) . Note that the update at line 15 ensures that ˆgi is the current distribution of the i th component in the data stream . 4.4 Computing the Change Score Values
Change scores are computed by using a divergence metric on two density functions ˆfi and ˆgi ( line 17 ) , which are updated upon the arrival of each sample . However , it is not necessary to compute change score at each time step , as the change of a distribution cannot be observed after a single data sample . Therefore , to reduce unnecessary repeated comparisons that may increase the execution time noticeably , we compute change scores every min(0.05 ∗ w , 100 ) samples ( line 16 ) . This setting of checkpoints complies with the monitoring requirements of users . Monitoring short term changes with a small w needs frequent check points while monitoring long term changes by setting a large w allows bigger checkpoint intervals . The granularity can be adjusted by changing the 5 % according to the users’ needs . When using histograms for density estimation , it is feasible to compute the change score after receiving every data sample . Note that the change detection methods based on our framework are named according to which divergence metric is used . The method use the maximum KL divergence ( Eq ( 3 ) ) is called CD MKL , the one that uses the Area metric ( Eq ( 4 ) ) is called CD Area , and the one that uses the LLH metric ( Eq ( 6 ) ) is called CD LLH .
After computing the change scores on all the PCs , the different change score values are aggregated by taking the maximum over all values . This is necessary to maintain a single statistical quan
Because the change score is used directly to trigger change alarms ,
PDFs for distribution comparison must be accurately and efficiently
1Demos , http://minekaustedusa/Pages/Softwareaspx code and additional evaluation results are available at :
939 tity . The maximum is preferable for aggregating the change score values as it allows for treating any changes happening at any component equally important . Also , when a change happens in a single PC , the change score will not be affected by the small change score values obtained from the other PCs . 4.5 Dynamic Threshold Settings
Typical statistical tests for change detection start by considering the null hypothesis , which assumes that the data distribution is stationary . A change score value is then calculated to determine the probability of rejecting the null hypothesis . The most popular technique to reject the null hypothesis is to specify a threshold and declare a change whenever the change score becomes greater than the threshold . Most existing methods require a user specified threshold [ 4 ] , which has two main issues . First , the fixed threshold cannot be used to detect changes in different magnitudes . Second , the threshold is difficult to set , as it is sensitive to the divergence metric , window size , underlying distribution , and change types . In this work , we propose a dynamic threshold setting , which is adjustable to the evolving distribution ( procedure change , lines 21 31 ) .
Since evolving data streams can be viewed as concatenated segments such that the distribution of each segment is different from the distribution of its neighboring segments [ 7 ] , we learn first the regularity of the data by recording the change scores between the reference and test windows . A change can be detected by monitoring how the current change score deviates from the historical values ( ie , by comparing the current change score with the mean of historic values since the last reported change ) .
The divergence metrics that we are studying generate small values ( close to zero ) when the two windows have the same distribution . The change score values increase when more data samples from the new distribution arrive , as well as the mean value of the change scores . We employ the Page Hinley ( PH ) test to track changes in the change score . A change is reported when the current change score value significantly deviates for a reasonable period of time from the history of the score values .
The PH test is designed as a sequential test to detect changes in the average of a Gaussian signal . The test defines a cumulative variable ( mt ) to store the cumulated difference between the observed values and the mean of the previously observed values , defined as : t
( st − st + δ ) ,
( 7 ) mt = t=1 where st is the observed value at time t , st = 1/tt i=1 si , δ is the magnitude of the allowed change , which is often set close to zero , ( eg , half the minimum of the obtained change score values in [ 21] ) . A change is reported when the difference between Mt = max{m1,··· , mt} and mt is greater than a threshold value θ . The value of θ controls the sensitivity of the model . A large θ makes the model more robust but changes might go undetected , and a small θ makes small changes detectable but increases the false alarm rate . We use a dynamic threshold setting which adjusts θt according to the observed change score values , θt = ξ ∗ st , where ξ is a constant value called the θ factor and represents the number of witnesses that should be observed before declaring a change [ 21 ] . This approach has a high accuracy for setting the threshold value , which results in higher detection accuracy with less false positives as we will demonstrate in the experimental evaluation section .
5 . EXPERIMENTAL EVALUATION
This section describes the evaluation of the proposed change detection framework . The performance of the framework is compared with the performance of two baseline methods presented in [ 5 ] and [ 12 ] . Our framework uses three divergence metrics presented in Eqs . ( 3 ) , ( 4 ) and ( 6 ) . The performance of the different methods is measured according to the number of True Positives ( TP ) , Late detections ( L ) , False Positives ( FP ) and False Negatives ( FN ) . By true positives , we mean the changes that were reported correctly before receiving 2w from the new distribution where w is the window size . Late detections are the changes reported after processing 2w data samples from the new distribution . False positives are changes reported by the method when there are no changes , and false negatives are the missed changes . The experimental results show that our framework outperforms the two baseline methods in terms of the number of correctly detected changes with less false positives . Also , the performance of the framework when using the Area metric ( Eq ( 4 ) ) outperforms the performance of the framework when using the MKL ( Eq ( 3 ) ) and LLH ( Eq ( 6 ) ) metrics in most of the evaluation datasets . We will start our discussion by providing more detail about the baseline methods before we present the experimental results .
The first baseline change detection method was proposed by Dasu et al . in [ 5 ] , which adopts a spatial partitioning scheme to construct an empirical distribution of the data . The partitioning scheme , kdqtree , partitions the data space into separate cells not necessarily covering the whole space of the data . Each node in the kdq tree has two variables to store the number of data samples falling within that cell from the reference and test windows . The divergence metric is used to compute the change score between the reference and test windows . When the change score is greater than a threshold determined using a bootstrap technique , a change in the data distribution is reported . Frequent changes require repeating the bootstrapping routine for the new reference window in order to obtain a threshold for the new distribution . This method uses both the Kullback Leibler ( KL ) and the Average Absolute Difference n ( AAD ) as divergence metrics . We were able to reproduce the results reported in [ 5 ] using the AAD metric2 , which is defined as i=1 |nr i − nti| , where n is the window size , D(S1 , S2 ) = 1 n nr i ( nti ) is the number of data samples from the reference ( test ) window that falls in the i th node . The results obtained using the KL divergence are inaccurate with many false negatives . The method with AAD seems to work well for datasets with dimensions less 10 but it is vulnerable to the curse of dimensionality as we will see in the experimental results .
The second baseline method was proposed by Kuncheva and Faithfull in [ 12 ] . The method employs PCA to reduce data dimensionality by eliminating the PCs with large variances , and uses a newly proposed asymmetric divergence metric called the SemiParametric Log Likelihood ( SPLL ) to measure distribution difference . A threshold for reporting changes is left for the user to tune . 5.1 Experiments on Synthetic Data
The first experiment evaluates the performance of our change detection framework and compares it with the two baseline methods using two dimensional datasets with three types of changes . Each dataset contains 5×106 data samples with changes that occur every 5× 104 data samples with a total of 99 change points . The datasets are generated following the same generation mechanism of [ 5 ] and contain the same types of changes . The datasets are given symbols to indicate the type of change ( M ( ) means varying the mean value , D( ) means varying the standard deviation , and C( ) means varying the correlation ) . At each change point , a set of random numbers in the interval [ − ,− /2 ] ∪ [ /2 , ] are generated and added to the
2We thank the authors for providing the code to replicate the results .
940 Table 2 : Evaluation results of Dasu ’s method ( kdq tree ) with AAD and KL divergence metrics , Kuchneva ’s method ( PCA SPLL ) , CD Area , CD LLH , and CD MKL with KDE Track and histograms as density estimators . The results are in the form TP/L/FP/FN with TP = True Positives , L = Late detections , FP = the False Positives and FN = the False Negatives . The best results are in bold .
CD Area
CD LLH
CD MKL
Dataset
M(0.01 ) M(0.02 ) M(0.05 ) D(0.01 ) D(0.02 ) D(0.05 ) C(0.1 ) C(0.15 ) C(0.2 ) kdq tree [ 5 ]
AAD
KL
PCA SPLL [ 12 ]
30/15/1/54 77/14/6/8 98/1/4/0 42/18/2/39 98/1/9/0 99/0/2/0 67/13/2/19 78/9/5/12 96/3/10/0
3/7/0/89 3/7/0/89 12/21/0/66 4/2/0/93 12/7/0/80 24/4/2/71 8/4/1/87 8/7/1/84 21/5/3/73
10/16/5/73 18/9/3/72 66/11/9/42 29/5/4/65 85/0/2/14 89/2/2/8 55/4/2/40 63/6/4/30 75/3/3/21
Histograms 25/17/0/57 69/14/1/16 96/0/3/3 32/13/0/54 94/0/0/5 99/0/0/0 69/9/0/21 68/5/2/26 93/1/1/5
KDE Track 38/23/0/38 87/12/0/0 99/0/0/0 45/29/0/25 97/0/1/2 99/0/0/0 68/19/0/12 84/7/1/8 97/1/1/1
Histograms 28/23/7/48 39/27/5/33 76/6/1/17 84/0/10/15 87/5/8/7 97/0/3/2 89/1/6/9 90/3/5/6 97/0/1/2
KDE Track 10/11/0/78 23/16/1/60 70/6/2/23 84/6/3/9 93/1/4/5 98/0/1/1 93/0/4/6 94/1/2/4 97/0/2/2
Histograms 34/12/0/53 78/10/1/11 97/0/2/2 41/17/0/41 98/1/0/0 98/0/1/1 60/17/1/22 81/2/3/16 93/1/3/5
KDE Track 34/31/1/34 89/7/0/3 95/0/4/4 54/25/0/20 94/1/2/4 98/0/2/1 76/17/2/6 85/9/0/5 93/2/0/4 distribution ’s parameters that will be changed . The parameter controls the magnitude of the change , where smaller values for make changes harder to detect and vice versa .
In the first three datasets M ( 0.01 ) , M ( 0.02 ) and M ( 0.05 ) , the standard deviation values are fixed ( σ1 = σ2 = 0.2 ) and the correlation coefficient is fixed ( ρ = 05 ) The mean values µ1 , µ2 are changing by η1 , η2 randomly selected from the interval [ − ,− /2]∪ [ /2 , ] . The second three datasets D(0.01 ) , D(0.02 ) and D(0.05 ) are generated by fixing the mean values µ1 = µ2 = 0.5 and the correlation coefficient value ρ = 05 The standard deviation values σ1 , σ2 are changing by adding random values from [ − ,− /2 ] ∪ [ /2 , ] . In the last three datasets C(0.01 ) , C(0.02 ) and C(0.05 ) , the mean values and the standard deviation values are fixed µ1 = µ2 = 0.5 and σ1 = σ2 = 02 The coefficient ρ makes random walks in the interval ( −1 , 1 ) with random steps selected from [ − ,− /2 ] ∪ [ /2 , ] .
The parameters’ setting for the kdq tree method follow the settings in [ 5 ] with δ = 0.01 , k = 500 and the window size is set to 104 . For CD LLH , CD Area and CD MKL , the parameters of the PH test for setting the threshold are set as δ = 0.005 and θt = ξ ∗ st = 500 ∗ st . The window size is set to 104 .
Table 2 presents the results obtained by the evaluated methods on the nine datasets . When deployed with the KL divergence metric , the kdq tree method , has poor performance . The kdq tree method with AAD metric , CD Area , and CD MKL have comparably good results with CD Area showing more accurate results . CD LLH has lower performance on datasets with varying means while it shows good performance when the changes affect the standard deviation or the correlation . PCA SPLL [ 12 ] shows the worst performance , especially for detecting mean shifts .
The second set of experiments evaluates the detection accuracy in high dimensional datasets . As changes become less observable when the data dimensionality increases , we used only datasets with a reasonable magnitude of change ( M ( 0.05 ) , D(0.05 ) , and C(02 ) ) For each type of change , we generated three datasets with 10 , 20 , and 30 dimensions . The changes in the data distribution affect only two dimensions ( variables ) and for more complicated change detection cases , we selected the variables that are affected by the change randomly at each change point . When the changes affect the same variables , the methods’ accuracy is comparable to the accuracy on the 2D data .
We report the results for the kdq tree method with AAD divergence only because KL divergence shows very low accuracy for 2D data . The results for CD LLH , CD MKL , and CD Area were obtained using histograms since KDE Track becomes expensive for high dimensional data especially when all the PCs are considered . Using histograms improves the running time noticeably without affecting the performance of the framework in reporting the changes correctly and maintaining a low number of false positives .
The results in Table 3 show that CD Area and CD MKL are not affected by data dimensionality . CD LLH performs better when the changes affect the data variances and correlation . Like all space partitioning methods , the kdq tree method , suffers from the curse of dimensionality as data dimensionality increases . The PCA SPLL has low accuracy in detecting the changes again because the PCs with large eigenvalues that we selected in our framework are dismissed and only the PCs with small eigenvalues are used for data projection and for change detection . The changes in the directions of the major ( ignored ) PCs will not be detected .
Table 3 : Evaluation results of kdq tree with AAD metric , PCASPLL , CD LLH , CD MKL , and CD Area for ( a ) changes in Gaussian high dimensional data ( 1st 9 datasets ) , ( b ) changes in density shape from a list of non Gaussian distributions ( DistCh ) , and ( c ) changes in non linear dependent data streams ( DEMC , DDC and SWRL ) . The best results are in bold .
Dataset
M(10D ) M(20D ) M(30D ) D(10D ) D(20D ) D(30D ) C(10D ) C(20D ) C(30D ) DistCh DEMC DDC SWRL kdq tree ( AAD ) [ 5 ] 96/1/9/2 75/7/3/17 59/8/4/32 93/3/3/3 87/3/7/9 78/4/4/17 64/7/10/28 29/12/12/58 11/4/5/84 99/0/7/0 84/6/2/9 96/2/3/1 99/0/6/0
PCA SPLL
[ 12 ]
63/10/10/26 32/11/1/56 33/13/2/53 70/1/1/28 59/0/0/40 56/1/4/42 67/1/0/31 49/0/0/50 48/0/0/51 72/0/44/27 24/15/28/60 19/18/29/62 23/10/29/66
CD LLH
CD MKL
CD Area
51/17/4/31 45/14/1/40 30/25/0/44 93/1/4/5 93/0/5/6 88/0/9/11 98/0/1/1 93/1/2/5 94/0/4/5 97/1/1/1 70/6/3/23 61/7/7/31 73/5/0/21
99/0/0/0 96/1/2/2 97/0/1/2 99/0/0/0 99/0/0/0 96/1/1/2 95/0/5/4 98/0/2/1 97/0/2/2 99/0/0/0 84/6/3/9 94/1/3/4 95/0/2/4
99/0/0/0 99/0/0/0 97/1/1/1 99/0/0/0 99/0/0/0 96/0/0/3 98/1/0/0 95/2/2/2 99/0/0/0 99/0/0/0 88/0/4/11 97/0/0/2 99/0/2/0
The third set of experiments evaluates the detection accuracy in two special cases : ( 1 ) changes in the density shape of data streams , and ( 2 ) changes in data streams with non linear dependencies . Four datasets were generated for evaluation , where each dataset contains 100 batches with batch size of 5 × 104 , resulting in a total of 99 changes . DistCh is generated by changing the data distribution in consecutive batches , where a distribution is randomly drawn from a list of preset distributions including : standard Normal , highly skewed Normal , bimodal Normal , trimodal Normal , Gamma and Laplace distributions . In each batch , the mean , variance and correlation are kept constant . The remaining three datasets evaluate the detection accuracy on non linear dependent data streams . The first dataset includes a Disc with EMpty Circle in the middle ( DEM C ) . Changes in consecutive batches are introduced by randomly altering the radius of the empty circle without affecting the mean , variance , or correlation values . The second dataset includes Disc with Dense Circle in the middle ( DDC ) . Non linear changes in the distribution are introduced by altering the radius of the dense circle in the center of the data points . The third dataset is a Swiss roll
941 Figure 1 : The effects of the parameters’ settings on the detection accuracy of our method . The evaluated parameters are the window size ( at the first line ) , the factor that is multiplied by the average score to set the threshold ( at the second line ) , and the percentage of the captured data variance ( at the third line ) . The datasets used for evaluation are M(0.05 ) dataset ( left column ) , D(0.05 ) dataset ( middle column ) , and C(0.2 ) dataset ( right column ) , in 2D ( the first and second lines ) and 30D ( the third line ) .
( SWRL ) dataset with changes designed by altering the distance between any two consecutive contours of the Swiss roll .
The results presented in Table 3 , show that both CD MKL and CD Area outperform other methods in DisCh , and that CD Area obtained better results in all non linear dependent data streams . While the experimental results are promising , the proposed method is restricted by PCA ’s limitation to handling only linearly dependent data streams . Change detection in non linear dependent data streams will be part of our future study . 5.2 Sensitivity to the Parameters’ Settings
In this experiment , we study the effects of the most important parameters on the performance of the change detection framework . These parameters are 1 ) the window size ; 2 ) the factor ξ that is multiplied by the average score to set the threshold value θt ; and 3 ) the number of used PCs , which are selected by considering the percentage of the data variance they captured ( explained ) .
Figure 1 shows the detection accuracy of the framework mea sured by
F1 Score =
2 ∗ precision ∗ recall precision + recall
.
In Figure 1 , the first row of subfigures shows the sensitivity of window size ; CD Area and CD MKL are less sensitive to the window size setting than CD LLH . The detection of change in correlation is more sensitive to the setting of window size ( third column in the first row ) , while detection of mean shift and variance change are largely unaffected by different window sizes . Sensitivity of the k i=1 threshold factor is shown in the second row . Generally , a small threshold factor is not preferred because it causes more false positives . When it increases to moderate values , the good performance of our approaches stays stable for a wide range of values . j=1 λj
λid
The subfigures in the last row of Figure 1 evaluate the sensitivity of the framework wrt the percentage of explained data variance ∗ 100 % by k PCs . The y axis on the left represents the F1 Score and the y axis on the right represents the running time in seconds . The x axis represents the number of considered PCs . Note that unlike the previous two parameter sensitivity analysis , this evaluation highly depends on evaluation data . The datasets used , M(0.05 ) , D(0.05 ) , and C(0.2 ) in 30D , have a typical L shape distribution of λ . The subfigures show that it is possible to detect most of the changes accurately when selecting only the first ten PCs that capture more than 60 % variance , and the detection accuracy stays stable when more PCs are selected , except CD LLH on the datasets with mean shift and variance change . As expected , the running time increases linearly when additional PCs are selected . 5.3 Experiments on Real Data
We also evaluated our method with the kdq tree and PCA SPLL methods on ten real datasets obtained from machine learning repositories . The information of each dataset is presented in Table 4 . Evaluation required that datasets be large enough and have change points where the distribution before it differs from that after it . To increase the data size while maintaining the distributional characteristics of the data , we follow the technique in [ 17 ] . The general
Window size500020000F1 Score00.51CD AreaCD LLHCD MKLWindow size500020000F1 Score00.51Window size500020000F1 Score00.51Threshold factor010002000F1 Score00.51CD AreaCD LLHCD MKLThreshold factor010002000F1 Score00.51Threshold factor010002000F1 Score00.51Number of PCs0102030F1 Score0030609Running time300500700CD Area ( F1 Score)CD MKL ( F1 Score)CD LLH ( F1 Score)CD Area ( Time)CD MKL ( Time)CD LLH ( Time)Number of PCs0102030F1 Score0030609Running time300500700Number of PCs102030F1 Score0030609Running time300500700942 Dataset D1 : Jogging ( 3D )
D4 : Spruce ( 10D )
D2 : Walking ( 3D ) D3 : El Nino ( 5D )
Table 4 : The sources of the ten real datasets used in the evaluation . Description This dataset and the next dataset contain reading for physical activities in a gym recorded by smart phones’ applications [ 13 ] . The dataset contains 342 , 177 records , each of which has three features representing the acceleration on the x , y and z axes . Refer to Jogging ( 3D ) . It includes 424 , 400 records . The dataset is obtained from the UCI KDD archive and contains , after removing the records with missing values , 93 , 935 data records , which are readings of oceanographic and surface meteorological from a series of buoys placed throughout the equatorial pacific . This dataset and the next one from the UCI KDD archive describe the forest cover type for 30× 30 meter cells . There are 211 , 840 records . We use the 10 quantitative features of each record . Refer to Spruce ( 10D ) . It includes 283 , 301 records . This dataset and the following four datasets are from UCI machine learning repository . The dataset ( PAMAP2 ) contains more than 3 million records for physical activities from 12 classes . We use readings from three sensors placed on arm , chest , and knee . After removing incorrect readings and invalid features , the dataset contains 30 features . The number of records in Ascending Stairs dataset is 117,216 . Refer to Ascending Stairs ( 30D ) . The dataset contains 164,600 records . D7 : Cycling ( 30D ) Refer to Ascending Stairs ( 30D ) . The dataset contains 104,944 records . D8 : Descending stairs ( 30D ) D9 : Ironing ( 30D ) Refer to Ascending Stairs ( 30D ) . The dataset contains 238,690 records . D10 : Vacuum cleaning ( 30D ) Refer to Ascending Stairs ( 30D ) . The dataset contains 175,353 records .
D5 : Lodgepole Pine ( 10D ) D6 : Ascending Stairs ( 30D )
Table 5 : Evaluation results of the kdq tree method with the AAD metric , Kuchneva ’s method ( PCA SPLL ) , CD LLH , CD MKL , and CD Area for real world data . The values in bold face are the best performance results . The best results are in bold .
Change
G1D
S1D
Method kdq tree
PCA SPLL CD LLH CD MKL CD Area kdq tree
PCA SPLL CD LLH CD MKL CD Area
D1 0.674 0.423 0.816 0.821 0.859 0.674 0.298 0.813 0.765 0.833
D2 0.674 0.375 0.768 0.851 0.866 0.674 0.324 0.784 0.717 0.825
D3 0.957 0.458 1.0 1.0 1.0 0.966 0.337 1.0 1.0 1.0
D4 0.676 0.492 0.913 0.846 0.925 0.676 0.346 0.778 0.772 0.884
D5 0.674 0.446 0.872 0.856 0.904 0.674 0.735 0.794 0.741 0.839
D6 0.585 0.504 0.838 0.785 0.848 0.592 0.772 0.995 0.761 0.821
D7 0.614 0.352 0.814 0.832 0.849 0.658 0.387 1.0 0.805 0.833
D8 0.563 0.535 0.809 0.828 0.811 0.746 0.406 0.989 0.878 0.872
D9 0.599 0.559 0.862 0.753 0.772 0.649 0.615 0.995 0.794 0.828
D10 0.473 0.628 0.809 0.831 0.822 0.691 0.691 0.985 0.782 0.814 idea is to draw a sample x randomly from the dataset and then take five samples ( x1,··· , x5 ) from the five nearest neighbors of x with replacement . A new sample is then generated by taking the average of x , x1,··· , x5 . This process is repeated until the dataset reaches the required size . In order to create change points in a dataset , we used similar techniques to [ 17 ] . We change the data distribution every 2 × 104 data samples by sampling a batch from the original data and the distribution of the following batch is changed by applying one of the following techniques : 1 ) Gauss 1D ( G1D ) : the batch is changed by adding a standard 1D Gaussian random variable to a randomly selected dimension . 2 ) Scale 1D ( S1D ) : the batch is changed by multiplying the values of a randomly selected dimension by two . We used these two types of changes because they affect only a single dimension of the dataset and they are harder to detect . The results in Table 5 show the performance evaluated by F 1 Score of precision and recall . The results confirm that our framework outperforms the kdq tree and PCA SPLL methods in detecting different types of changes . The kdq tree has a large number of false positives , even though we used a more strict threshold than the one recommended by the authors . 5.4 Computational Cost Analysis
The time complexity of our framework and the kdq tree method is linear wrt the size of the data stream . However , the constant in the complexity formula controls the efficiency of the evaluated methods . The cost of our method depends on three main subroutines : 1 ) The PCA routine for extracting PCs , which is called only when a change is reported . It has a complexity of O(d2 × w× Rc ) , where d is the data dimensionality , w is the window size , and Rc is the number of reported changes . The frequency of calling this subroutine depends on the nature of the data . 2 ) The incremental density update by KDE Track or in histograms , which requires a constant time at the arrival of a new data sample from the stream . 3 ) Computing the divergence metric , which is done incrementally and costs a constant time upon the arrival of a new data sample . Therefore , the computational cost of our framework is affected more by data dimensionality , as the PCA routine has quadratic complexity wrt dimensionality d , and maintaining the histograms and computing the change score may have to be done on more PCs when d is higher . The window size has a relatively less effect on the running time of our framework as it affects only the PCA routine . The running time of the kdq tree method increases fast wrt window size and data dimensionality , mainly due to the expensive bootstrap sampling routine for computing the threshold to report ν ) ) to set changes . The bootstrap techniques requires O(kdw log( 1 the threshold , where k is the number of bootstrap samples and ν is the minimum cell width . The complexity for the kdq tree construcν ) ) , and the kdq tree update tion using w samples is O(dw log( 1 and change score maintenance requires O(d log( 1 ν ) ) upon the ar
943 7 . REFERENCES [ 1 ] C . C . Aggarwal . A framework for diagnosing changes in evolving data streams . In SIGMOD , 2003 .
[ 2 ] A . Bifet and R . Gavaldà . Learning from time changing data with adaptive windowing . In SDM , 2007 .
[ 3 ] S . Cha . Comprehensive survey on distance/similarity measures between probability density functions . Intl . J . of Math . Models and Methods in App . Sci . , 1:300–307 , 2007 .
[ 4 ] X . L . Dai and S . Khorram . Remotely sensed change detection based on artificial neural networks . Photogrammetric Engineering & Remote Sensing , 65 , 1999 . [ 5 ] T . Dasu , S . Krishnan , S . Venkatasubramanian , and K . Yi . An information theoretic approach to detecting changes in multi dimensional data streams . In Symp . on the Interface of Stat . , Computing Science , and Applications , 2006 .
[ 6 ] V . Guralnik and J . Srivastava . Event detection from time series data . In KDD , 1999 .
[ 7 ] S . Ho . A martingale framework for concept change detection in time varying data streams . In ICML , 2005 .
[ 8 ] L . Jin , S . Wang , and H . Wang . A new nonparametric stationarity test of time series in time domain . Journal of the Royal Statistical Society : Series B , to appear , 2015 .
[ 9 ] Y . Kawahara and M . Sugiyama . Change point detection in time series data by direct density ratio estimation . In SDM , 2009 .
[ 10 ] Y . Ke , R . Sukthankar , and M . Hebert . Event detection in crowded videos . In ICCV , 2007 .
[ 11 ] D . Kifer , S . Ben David , and J . Gehrke . Detecting change in data streams . In VLDB , 2004 .
[ 12 ] L . I . Kuncheva and W . J . Faithfull . PCA feature extraction for change detection in multidimensional unlabeled data . IEEE Transactions on Neural Networks and Learning Systems , 25:69–80 , 2014 .
[ 13 ] J . Kwapisz , G . Weiss , and S . Moore . Activity recognition using cell phone accelerometers . In Proc . of the 4th Int . Workshop on Knowledge Discovery from Sensor Data , 2010 .
[ 14 ] D . Liu , D . Sun , and Z . Qiu . Feature selection for fusion of speaker verification via maximum kullback leibler distance . In ICSP , 2010 .
[ 15 ] S . Liu , M . Yamada , N . Collier , and M . Sugiyama .
Change point detection in time series data by relative density ratio estimation . Neural Networks , 43:72–83 , 2013 . [ 16 ] A . Qahtan , X . Zhang , and S . Wang . Efficient estimation of dynamic density functions with an application to outlier detection . In CIKM , 2012 .
[ 17 ] X . Song , M . Wu , C . Jermaine , and S . Ranka . Statistical change detection for multi dimensional data . In KDD , 2007 .
[ 18 ] M . Sugiyama , T . Suzuki , S . Nakajima , H . Kashima ,
P . Bünau , and M . Kawanabe . Direct importance estimation for covariate shift adaptation . Annals of the Institute of Statistical Math . , 60:699–746 , 2008 .
[ 19 ] J . Takeuchi and K . Yamanishi . A unifying framework for detecting outliers and change points from time series . TKDE , 18:482–492 , 2006 .
[ 20 ] K . Yamanishi , J . Takeuchi , G . Williams , and P . Milne .
On line unsupervised outlier detection using finite mixtures with discounting learning algorithms . Data Mining and Knowledge Discovery , 8:275–300 , 2004 .
[ 21 ] X . Zhang , C . Furtlehner , C . Germain Renaud , and M . Sebag .
Data stream clustering with affinity propagation . TKDE , 26:1644–1656 , 2014 .
Figure 2 : Running time of the evaluated methods for different window sizes ( left ) and different number of dimensions ( right ) . The stream size is 5 × 106 data samples . Changes occur every 5 × 104 data samples and the number of changes is 99 . rival of a new data sample . Hence , the methods overall complexity ν Rc) ) , where Rc is the number of reported changes . is O(kdw log( 1 The results in Figure 2 confirm our analysis . We see a slight increase in the running time of CD Area and CD MKL when increasing the window size ( Figure 2 Left ) , while the runtime increases almost linearly wrt data dimensionality ( Figure 2 Right ) . CDLLH requires extra O(dw ) compared to CD Area and CD MKL for computing the initial change score values in Eq ( 6 ) after each change . Thus , it costs more than CD Area and CD MKL . The running time of kdq tree increases linearly with the window size and data dimensionality .
Our framework is also superior to kdq tree in processing a real data stream . The kdq tree requires that the data be normalized ( into [ 0 , 1 ] ) before running to ensure that growth of the tree growing is not stopped prematurely by the stopping condition of the minimum cell width . However , it is unrealistic to normalize data streams before hand , as data arrives continuously . Our framework does not require normalization .
6 . CONCLUSION
In this paper , we presented a new framework for detecting abrupt changes in multidimensional data streams . The framework is based on projecting data on selected principal components . On each projection , densities in reference and test windows are estimated and compared . Then a change score value is calculated by one of the divergence metrics . By treating all selected PCs with equal importance , the maximum change score among different PCs is considered as the final change score .
The framework uses accurate and efficient density estimators , different divergence metrics to compare the data distributions , and a dynamic threshold setting to report changes . The performance results on both synthetic and real data show that our framework outperforms two baseline methods for detecting changes in unlabeled multidimensional data streams . The results also show that our framework scales well for high dimensional data .
Our next target is to explore the deployment of change detection in other data mining applications such as online classification and online clustering .
Acknowledgments Research reported in this publication was supported by the King Abdullah University of Science and Technology ( KAUST ) .
Window size01000020000Running time ( sec.)100200300400kdq treeCD AreaCD LLHCD MKLNumber of Dimensions01020Running time ( sec.)100200300400kdq treeCD AreaCD LLHCD MKL944
