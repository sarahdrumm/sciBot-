State Driven Dynamic Sensor Selection and Prediction with State Stacked Sparseness
Guo Jun Qi† , Charu Aggarwal‡ , Deepak Turaga‡ , Daby Sow‡ and Phil Anno§
†Department of Electrical Engineering and Computer Science ,
University of Central Florida
4000 Central Florida Blvd , Orlando , FL 32816 guojunqi@ucfedu
‡IBM TJ Watson Research Center
19 Skyline Drive , Hawthorne , NY 10532
{charu,turaga,sowdaby}@usibmcom
§Geoscience and Reservoir Engineering Technology
ConocoPhillips , Houston , TX 77252 phildanno@conocophillipscom
ABSTRACT An important problem in large scale sensor mining is that of selecting relevant sensors for prediction purposes . Selecting small subsets of sensors , also referred to as active sensors , often leads to lower operational costs , and it reduces the noise and information overload for prediction . Existing sensor selection and prediction models either select a set of sensors a priori , or they use adaptive algorithms to determine the most relevant sensors for prediction . Sensor data sets often show dynamically varying patterns , because of which it is suboptimal to select a fixed subset of active sensors . To address this problem , we develop a novel dynamic prediction model that uses the notion of hidden system states to dynamically select a varying subset of sensors . These hidden system states are automatically learned by our model in a data driven manner . The proposed algorithm can rapidly switch between different sets of active sensors when the model detects the ( periodic or intermittent ) change in the system state . We derive the dynamic sensor selection strategy by minimizing the error rates in tracking and predicting sensor readings over time . We introduce the notion of state stacked sparseness to select a subset of the most critical sensors as a function of evolving system state . We present experimental results on two real sensor datasets , corresponding to oil drilling rig sensors and intensive care unit ( ICU ) sensors , and demonstrate the superiority of our approach with respect to other models .
1 .
INTRODUCTION
The deployment of sensor networks have become ubiquitous in many real world systems to provide continuous monitoring [ 2][7][10 ] . Sensor networks are used to monitor dysfunctions in oil drilling rigs [ 9 ] , and abnormal patient conditions in intensive care units ( ICU ) [ 17 ] , and enemy activity in military scenarios . Indeed , our experimental results in this paper will explore some of these scenarios .
One of the pervasive properties of most sensor networks is that they are highly redundant in terms of the collected data . For example , a bird call at one sensor will typically also be detected by a sensor a few meters away . Even in cases where the sensors measure different quantities , they are strongly correlated by recording the same outside event . Therefore , a salient question arises as to whether one can retain only a smaller subset of sensors without losing a significant amount of information . An important practical motivation in such scenarios is to reduce the cost of operation and the information overload , while losing only a limited amount of information . For example , deploying , maintaining , and continuously using sensors in the earth subsurface ’s drilling environments is extremely expensive . By selecting a smaller set of sensors to track , significant direct and indirect cost savings are achieved . Moreover , depending on the nature of the correlations , all sensors are not equally useful to track . In many cases , including irrelevant sensors may even have negative qualitative effects in predictive applications . Many state of the art methods consider a static sensor selection model , in which a fixed set of sensors are actively selected . This paper deviates from these methods by developing a dynamic sensor selection model in which the set of selected sensors change over time as the hidden state of the system evolves . Such hidden states often have semantic interpretations in real scenarios . For example , in an oil drilling rig , the sensors have different patterns of correlations during periods when the drill is moving forward , or when the drill is moving backward . Furthermore , the presence
945 of “ typical ” dysfunctions or abnormal events will create different patterns of correlations . Similarly , in an ICU sensor network , the state of the patient might affect the underlying patterns of correlations between medical sensors . This implies that it is suboptimal to select a static correlation or selection model . Furthermore , in many application specific scenarios , such as event detection , the system states might have important explanatory power , providing deep insight into the monitored events . Therefore , we view the approach in this paper as a first step to fully modeling the temporal sensor stream . This can provide a gateway to exploring other important problems in sensor network analysis .
In this paper , we develop a state driven dynamic prediction model to track and predict the system states over time . These states are used to dynamically reveal the patterns of correlations between sensors , and select sets of nonredundant sensors as a function of changing states . Experimental results on the oil drilling rig and the ICU sensor networks demonstrate the competitive performance of the proposed algorithm .
The remainder of this paper is organized as follows . The problem is formulated in Section 3 . The learning and inference algorithms are presented in Section 4 . In 5 , we present the details behind dynamic sensor selection and prediction algorithm , which can reduce to an ad hoc static sensor selection model as a special case . The experiment results on the oil drilling sensor and the ICU sensor datasets are demonstrated in Section 6 , followed by the conclusion in 7 .
2 . RELATED WORK
The analysis and modeling of the dynamics of sensor data streams has been extensively studied in the literature [ 16 , 14 , 1 , 15 , 6 ] . For this purpose , one of the most important problems is that of modeling the dynamic correlations between multiple sensor data streams over time in an effective and scalable fashion . The methods in [ 14 , 16 ] model lagcorrelations to forecast the future data streams over time . The techniques in [ 3 , 8 ] develop a sensor selection model with a graph structure that models the domain specific correlations between sensors . Aggarwal et al . [ 1 ] also develops a dynamic model for functional dependency between sensors in real time to predict the changing trends in the sensor data streams . The work in [ 20 ] uses linear regression to model the sensor correlations for prediction . An adaptive model [ 19 ] is proposed to predict the future sensor readings so that the unnecessary communications can be saved when the prediction model offers a satisfactory estimates of sensor measurements . The algorithm proposed in [ 4 ] develops adaptive querying based on pairwise covariances between sensor streams in order to reduce the communication overhead with limited bandwidth . In contrast , [ 5 ] proposes a hypothesis testing based prediction model that can bring the sensor sample series closer to weak stationarity to achieve low energy dissipation .
The aforementioned models and algorithms share a common characteristic – they reveal a dynamic model that is stationary in time . Although this is not a big obstacle to model the real world over a short time frame , the model will be unable to take advantage of the repetitions in the evolving trends over longer time frames . To address this deficiency , we propose a state driven dynamic model for time series sensor data . The states provide a model that can handle the typical forms of dynamic evolution over longer time frames .
Any key sets of unique correlations over longer time periods can be essentially encoded and leveraged by these states .
3 . FORMULATION
In this section , we will first introduce the formulation for the dynamic sensor selection problem . 3.1 Dynamic Sensor Selection
Consider a set of sensors indexed with N = {1 , 2 , · · · , N} . The nth sensor generates a real valued time series stream xt(n ) ∈ R at each time step t . The the sensor selection problem may be formulated as that of finding a small set of active sensors Mt ⊆ N from which the readings of inactive sensors can be predicted as accurately as possible . Note that Mt is indexed with t to reflect the fact that active sensors are dynamically selected .
Definition 1
( Dynamic Sensor Selection ) . Given a set of sensors N = {1 , 2 , · · · , N} , the dynamic sensor selection model is defined as that of selecting a smaller subset of sensors Mt ⊆ N at each time step t , whose signals can accurately predict all sensor signals {xt′ ( n)|n ∈ N } of both active and inactive sensors for t′ ≥ t . 3.2 State Evolution Model
The aforementioned definition specifies a new paradigm of dynamic sensor selection model to track and predict sensor readings . The set of selected sensors can be dynamically adjusted as the system state evolves over time . For example , in the oil drilling application , different sets of active sensors should be selected to reflect the change in the mechanical and electrical state of the system as a consequence of changes in the nature of the of drilling operations . In many sensor applications , such states are not visible to the user , but they can only be perceived in terms of the changes in the patterns of underlying stream correlations . Therefore , we assume that these states are hidden , and they need to be learned and detected in a data driven manner .
To model the impact of system conditions on sensor selection and prediction , we define the following hidden state evolution model for the system ( eg , an oil drilling rig or a patient ) monitored by a sensor network N .
Definition 2
( State Evolution Model ) . A system has a set of K states denoted by S = {1 , 2 , · · · , K} . At each time t , the system has a hidden state st ∈ S . The initial state of the system is modeled with a probability vector Π = [ π1 , π2 , · · · , πK]T , ie , Pr ( s0 = k|Π ) = πk . The system transits from state st to state st+1 with a probability of Ast,st+1 , where Ast,st+1 is an element of a transition matrix A . Therefore , the overall state evolution model is fully defined by E = {S , Π , A} .
This model defines a Markov chain of the states [ 11 ] , where the state at each time only depends on the state at the preceding step . In other words , once the state is given for the current time , all the states of the future steps are independent of the past states . More complex higher order Markov chains can be defined in which the system state depends on more previous states beyond the immediately previous one . However , increasing complexity can also cause overfitting . Therefore , for simplicity , we adopt the aforementioned state evolution model .
946 State 1
State 2 State 3
State 1
Sensor 1
Sensor 2
Sensor 3
Sensor 4
Selected sensor
Unselected sensor
Dynamically selected sensors in each state
• State 1 : M(1)={1,3}
• State 2 : M(2)={3}
• State 3 : M(3)={1,4}
Figure 1 : The dynamic sensor selection model .
3.3 State Driven Sensor Selection and
Prediction
Given the aforementioned state evolution model , the dynamic sensor selection model in Definition 1 can be re casted as a problem of selecting a subset of active sensors M(st ) ⊆ N as a function of the system state st at each time t . In other words , different sets of active sensors are used to track and predict the streams of sensor data . This yields a novel dynamic sensor selection and prediction model as opposed to the static sensor selection model .
Figure 1 illustrates an example of dynamic sensor selection model . Four sensors are shown in the figure with 3 hidden states for the system . In each state , a distinct set of sensors are selected . For example , for state 1 , the sensors in M(1 ) = {1 , 3} are actively selected . Then , the system evolves into state 2 , in which the sensor in M(2 ) = {3} is selected . The system keeps changing its state , and the corresponding set of sensors are selected along with each state .
We can define a P step state driven linear prediction model . It uses the signals collected from the active sensors to predict the signals of the ( both active and inactive ) sensors P steps into the future .
Definition 3
( P step model with fixed window R ) .
Given the states {st−r|0 ≤ r < R} up to time t , the linear prediction model predicts xt+P = [ xt+P ( 1 ) , · · · , xt+P ( N )]T by xt+P =
R−1X r=0
W ( st+P , st−r ) xt−r + b(st+P ) + εt+P
( 1 ) where W ( st+P , st−r ) ∈ RN ×N is a N × N prediction matrix from state st−r at time t − r to state st+P at time t + P , and b(st+P ) ∈ RN is the bias vector for the state st+P at time t + P . The notation εt+P represents the noise drawn from a Gaussian distribution N ( 0N , σ2IN ) with zero mean and isotropic variance σ2 . It is assumed that the noise terms at various time instants are independent .
Remark : In the aforementioned model , each prediction matrix W(s′ , s ) makes use of sensor signals in state s to predict the future sensor readings in state s′ . For this reason , we call the second argument s of this prediction matrix the sender state , and the first argument st as the receiver state . The use of both sender and receiver states to specify the prediction matrix enhances the model fidelity – the predicted sensor readings depend not only on the past state from which the sensor readings are collected , but also on the future state for which the prediction is made . This is in contrast to the Auto Regressive Model ( ARM ) [ 12 ] , a popular dynamic model that predicts the sensor readings independent of the rapid changing states .
Now consider a sender state s in ( 1 ) , in which the columns of W ( s′ , s ) with all zero entries correspond to the sensors that can be turned off without affecting the prediction result according to the model ( 1 ) . Therefore , as long as all the matrices {W ( s′ , s ) |s′ ∈ S} with the same sender state s have a common set of zero columns , the corresponding sensors can be turned off when the sensor network runs into state s , no matter which receiver state it will run into in the future . Thus , those nonzero columns correspond to the selected sensors that ought to be turned on , which we denote by M(s ) .
Observation 1
( Zero Column Sensor Selection ) . If all the prediction matrices W(s ) , {W(s′ , s)|s′ ∈ S} with the same sender state s have a common set of zero columns , the corresponding sensors can be turned off without affecting the prediction result . In other words , we only need to select for each state s a set of sensors M(s ) corresponding to those common nonzero columns for the prediction matrices in W(s ) .
In the next section , we will see that how this requirement is fulfilled when we formulate sensor selection problem .
4 . MODEL LEARNING AND INFERENCE The general idea of solving a dynamic sensor selection and prediction problem is to implement an EM ( ExpectationMaximization ) algorithm , which alternates between inference of states {st|t = 0 , 1 , · · · , T } ( E Step ) and the learning of the parameters including {W(s′ , s)|(s′ , s ) ∈ S×S} , {b(s)|s ∈ S} in the prediction model and {Π , A} in the state evolution model ( M Step ) . In the following two subsections , we will show how to solve these two subproblems . 4.1 Model Learning : M Step
Suppose we have a time series of sensor signals {xt|t = 0 , 1 , · · · , T } , and we have sampled the corresponding sequence of states s = {st|t = 0 , 1 , · · · , T } . Now , we are ready to estimate the parameters for the state evolution model in Definition 2 and the state driven prediction model in Definition 3 .
According to Eq
( 1 ) , with the independent Gaussian noise terms εt , the maximum likelihood criterion leads to the least squares problem for learning the prediction matrices {W(s′ , s)|r , s′ , s ∈ S} ) by minimizing the following loss function :
T
R−1
Es||xt+P −
W ( st+P , st−r ) xt−r − b(st+P )||2 2
Lls =
1 2σ2
Xt=P
Xr=0
( 2 ) Here , Es(· ) denotes the expectation over the hidden states s = {st|t = 1 , 2 , · · · } . This yields the smallest prediction errors on the training set in the least squares sense .
947 In addition , by the maximum likelihood criterion , the transition parameter A of the state evolution model can be solved as follows :
T −1Pt=0
As,s′ =
Est,st+1 δ [ [st = s , st+1 = s′ ] ]
T
( 3 )
Here , δ [ [· ] ] is the indicator function , which outputs 1 if the included condition is satisfied and 0 , otherwise . This equation counts how many times a transition from s to s′ has occurred in expectation sense .
In Section 4.2 , we will present how to sample a sequence of states {st|t = 1 , 2 , · · · , T } , which are used to approximate the aforementioned expectations . 411 Dynamic Sensor Selection To select a smaller set of sensors for each state , we introduce the following L1,2 norm which encourages zero columns on the prediction matrix .
Definition 4
( L1,2 matrix norm ) . The L1,2 norm for a matrix M is defined as the sum of L2 norms of the column vectors for this matrix , ie , kMk1,2 =
NX i=1 kMik2
( 4 )
Here , Mi is the ith column vector of M . The minimization of an objective function with such L1,2 norms on a matrix tends to make some of entire columns have zero elements . Forcing such sparseness with carefully tailored regularization is achieved in many data mining and machine learning applications such as Lasso [ 21 ] . Just as Lasso sparsity is used for feature selection , we use this form of regularization with the following stacked sparseness for sensor selection .
The goal is to fulfill the zero column sensor selection requirement established in Observation 1 . We can stack all the matrices corresponding to the same sender state s , which forms a bigger matrix as follows :
W(s ) = 
W(1 , s ) · · · W(k , s ) · · · W(K , s )
 
∈ RKN ×N
( 5 )
Since each column of W(s ) is a concatenation of the columns of all the matrices with the same sender state s , minimizing kW(s)k1,2 can force some columns of these matrices to be zero . This results in common zero columns , which is referred to as stacked sparseness for the common sender state in this paper . Then , we only need to select the sensors corresponding to nonzero columns at state s and leverage their measurements to predict the future sensor readings , without affecting the prediction model .
Putting together the least square loss and kW(s)k1,2 , we have the following minimization problem 1 : min W
L(W ) , Lls(W ) + µ
KX s=1 kW(s)k1,2
( 6 )
1Note that the objective is a function of both the prediction matrices W and the bias vectors {b(s)|s ∈ S} . Since it is much easier to optimize with respect to the bias vectors , we concentrate on W and discard the bias vectors in the argument . The discussion about optimizing with respect to {b(s)|s ∈ S} can be found after Theorem 1 .
Here , W is a matrix that concatenates all the prediction matrices following the notation in ( 5 ) , ie ,
W , [ W(1 ) , W(2 ) , · · · , W(K ) ] ∈ RKN ×KN , and µ is a positive balancing parameter that trades off between the loss function and the L1,2 norm . A large value of µ will result in a smaller set of selected sensors for each state . This parameter is set by balancing between the required level of training accuracy and the number of sensors that should be maintained in the field . 412 Cost Sensitive Sensor Selection We can also consider the scenario where the costs of maintaining different types of sensors are unequal in the field . In this case , associated with each sensor n ∈ N , we can explicitly assign a positive weight δn to represent the cost . This weight can be multiplied with each column of W(s ) . Then , minimizing the L1,2 norm of this cost weighted matrix results in de selection of the sensors with larger cost weights by zeroing out the corresponding columns rather than the sensors with smaller cost weights . The L1,2 norm of the cost weighted matrix can be written in a compact matrix form as kW(s ) · diag(δ1 , · · · , δN )k1,2 with diag(· ) denoting a diagonal matrix with the cost weights as its elements . 413 Optimization Algorithm This section provides the details of finding the optimal solution to the objective function ( 6 ) . It is nontrivial to optimize this objective function over W with a non smooth L1,2 norm term , since the conventional gradient descent method is ineffective in such cases . For example , the sub gradient decent can converge very slowly and the generated minimizer may not fully exploit the sparseness property associated with the L1,2 norm . Thus , we present an alternative gradient based algorithm which can handle such cases . If desired , the reader can skip the following derivation , and go to Algorithm 1 directly .
At a broad level , we adopt the proximal gradient family of algorithms . It is an optimization method that solves the original problem via a sequence of approximations . At the current estimate of W ( l ) at each iteration l , we expand the first order Taylor series of Lls(W ) . Then , the objective function ( 2 ) becomes the following :
Qτ ( W,W ( l ) ) , Lls(W ( l ) ) + h∇Lls(W ( l) ) , W − W ( l)i kW − W ( l)k2
F + µ kW(s)k1,2 kW − G(l )
τ k2
F + µ kW(s)k1,2 + const
( 7 ) Here , ∇Lls(W ( l ) ) is the gradient of the loss function Lls(W ) at W ( l ) , k · kF is the Frobenius norm , and const is a term irrespective of W :
G(l )
τ = W ( l ) − τ −1∇Lls(W ( l ) )
( 8 )
Here , τ is a parameter that weights the quadratic term in ( 7 ) , which is usually set to a larger value than the Lipschitz constant of the least squares loss Lls(W ) .
+
=
τ 2
τ 2 k=1
KX KX k=1
948 Algorithm 1 Optimization Algorithm for Problem ( 6 ) input Choose W ( 0 ) = W ( −1 ) , t(0 ) = t(−1 ) = 1 . for l = 0 , 1 , 2 , · · · do
// generate W ( l+1 ) from W ( l ) according to the following iterations . t(l−1 ) − 1
Step 1 Set Y ( l ) = W ( l ) + Step 2 Set G(l ) Step 3 Solve W ( l+1 ) as in Eq ( 10 ) ; // update the bias vector b(l )
τ = Y ( l ) − τ −1∇Lls(Y ( l) ) ; t(l )
( W ( l ) − W ( l−1) ) ; t(l−1 ) − 1
Step 4 Set d(l ) = b(l ) + Step 5 Set b(l+1 ) = d(l ) − τ −1 ∂Lls(d(l ) ) ∂b // update the interpolation parameter t(l )
( b(l ) − b(l−1) ) ;
Step 6 Set t(l+1 ) = end for
1 +q1 + 4(t(l))2
2
Algorithm 2 Sampling State Sequence input a sequence of sensor measurements {xt|t =
1 , 2 , · · · , T } Sample s0 according to Π ; for t = 1 , 2 , · · · , T do
Sample st according to Eq ( 12 ) . end for inf W
L(W ) + ǫ , we have
; min l=0,1,··· ,L+1
L(W ( l ) ) ≤ L(W ⋆ ) +
4Cf kW ⋆ − W ( 0)k2 F
( L + 2)2
. where Cf is the Lipschitz constant of Lls(W ) .
Since the least squares loss Lls is a differentiable function , its gradient ∇Lls(W ( l ) ) in Eq ( 8 ) can be derived easily .
Note that Qτ ( W , W ( l ) ) is a convex function of W and thus it has a minimizer , yielding W ( l+1 ) for the next iteration as follows :
W ( l+1 ) = arg min
W
Qτ ( W , W ( l ) )
( 9 )
Fortunately , Problem ( 9 ) has a closed form solution . Denote the ith column of W ( l+1 ) by [ W ( l+1)]:,i . Then , we have : hW ( l+1)i:,i
1 −
= 
 +hG
( l )
τ i:,i
( 10 )
µ ( l )
τflflflhG
τ iflflfl:,i
τ ]:,i is the ith column of G(l )
Here , [ G(l ) τ defined in Eq ( 8 ) , and ( z)+ outputs the positive component of z , ie , it outputs z if z > 0 , and 0 . otherwise . At each iteration , this minimizer tends to yield zero columns corresponding to the sensors that can be excluded for each state .
Remark : The above theorem can be proved in straightforward way with a similar idea to that in [ 18 ] . The detail is omitted here due to space limitations . This theorem shows the convergence of Algorithm 1 to an ǫ optimal minimizer W ⋆ .
It is worth noting that the optimization with respect to the bias vectors b = [ b(1 ) , b(2 ) , · · · , b(K ) ] can be easily obtained since the objective function is differentiable with respect to them . Note that the non smooth L1,2 norm term is independent of the bias vectors . Thus , the conventional gradient decent algorithm is directly applicable as in Steps 4 5 in Algorithm 1 with a similar interpolation strategy .
4.2 Sampling State Sequence
In this section , we present a sequential sampling algorithm to sample the sequence of states {st|t = 0 , 1 , · · · , T } which are used to approximate the expectation in Eq ( 2 ) .
Given the current estimate of model parameters , the past states s1:t−1 = {s1 , s2 , · · · , st−1} and the observations x1:t = {x1 , x2 , · · · , xt} up to time t , the posterior probability for state st at step t can be written as
When the cost weights δn , n ∈ N are considered , the aforementioned update should be changed to
Pr(st|s1:t−1 , x1:t ) ∝ Ast−1 ,st · hW ( l+1)i:,i
1 −
= 
 +hG
( l )
τ i:,i
δnµ ( l )
τflflflhG
τ iflflfl:,i
Therefore , the column for a sensor with larger δn is more likely to be zeroed out .
Algorithm 1 summarizes the optimization approach for Problem ( 2 ) . It is noteworthy that in Step 1 , we compute Y ( l ) that plays the same role of W ( l ) in Step 2 3 as in the aforementioned derivation . Here , Y ( l ) interpolates between the solutions in two successive steps . Such a Nesterov optimization strategy [ 13 ] can accelerate the convergence of Algorithm 1 to a rate of O(1/L2 ) where L is the number of the iterations . The following theorem formally states this result .
Theorem 1
( Convergence ) . Let {W l} be the sequence generated by Algorithm 1 . Then for any W ⋆ with L(W ⋆ ) ≤
( 11 ) exp −
1 2σ2
||xt −
W ( st , st−P +r ) xt−P +r − b(st)||2
2!
R−1
Xr=0
( 12 ) The first factor on the right hand side of the equation is the prior transition probability between two consecutive states , and the second factor is the likelihood proportional to the Gaussian distribution over the remainder of the least square prediction error . Accordingly , st can be sampled according to this posterior distribution directly . This process is iterated over t from 1 to T . Algorithm 2 summarizes the algorithm of sampling the sequence of states .
It is worth noting that we can handle missing sensor measurements when sampling the state at a time step t , by only considering the observed entries in xt to compute the k · k2 term in the exponent since the unobserved entries can be marginalized out for a multivariate Gaussian distribution defined by Eq ( 1 ) . This is important because sensor observations are traditionally noisy with many missing values .
949 Algorithm 3 Learning Model Parameter input a sequence of sensor measurements {xt|t =
1 , 2 , · · · , T } Initialize the model parameter // Iterate between M Step , and E Step . E Step Sample the sequence of the states according to Algorithm 2 . M Step Apply Algorithm 1 to update the current estimate of model parameter ;
Algorithm 4 Dynamic Sensor Selection and Prediction input the learned model parameter . for each step t do
Sample the current state st according to Eq ( 12 ) ; if the new state differs from st−1 then
Sensor selection : Turn on the sensors in M(st ) corresponding to the nonzero columns in W(st ) ; the other sensors not in M(st ) can be turned off . end if Data prediction : Apply the model in ( 1 ) to predict the sensor measurements xt+P for P steps ahead of time . end for
Additionally , in the learning phase , we also need to sample the posterior distribution over st′ with t′ < t
Pr(st′ |s1:t , x1:t ) ∝ Ast′
−1,st′
Ast′ ,st′ +1 ·
R−1 exp(−
1 2σ2
||x t′+p
P
Yp=0
−
Xr=0
W,st′+p , st′+p−P −r x t′+p−P −r − b(st′+p)||2 2 )
( 13 ) In this case , we need to consider both the information of the past t < t′ and the future t > t′ time steps over a sequence of states . Specifically , the first two terms on the right hand side are the transition probabilities to and from st′ of the current state , and each term in the product is the likelihood proportional to the Gaussian distribution on the least square remainder from time t′ to time t′ + P .
The sampling process can be performed iteratively to sample the states a posteriori given the sensor measurements up to time t , and the sampled sequence s1:t can be used to approximate the expectation in Eq ( 2) (3 ) . Markov Chain Monte Carlo ( MCMC ) method provides theoretical guarantee on this sampling based approximate . 4.3 Summary
Algorithm 3 summarizes the algorithm of learning the model parameters . Given the sampled states {st|t = 0 , 1 , · · · , T } , we apply Algorithm 1 to update the current estimate of model parameters . Given the current model parameters , we sample the sequence of the states according to Algorithm 2 . We iterate between these two subroutines of algorithms to find the optimal model parameters .
5 . DYNAMIC MODEL
With the learned model from the previous section , we can apply it to dynamically select the sensors and predict the sensor data streams ahead of time . We summarize our ap proach to dynamic sensor selection and prediction in Algorithm 4 .
For dynamic sensor selection , at each time t , if the current state changes from the previous one , the set of active sensors should also change accordingly . Those sensors that are not in the active set M(st ) can be turned off without affecting the prediction of the future sensor measurements . This is also discussed in the previous section .
At each step t , the model is applied to predict the sensor measurements bxt+P at time t + P , ie , P step ahead of the current time t , by substituting t + P for t in Eq ( 1 ) . Specifically , we take the expectation of xt+P over the state st+P at time t + P ,
[ xt+P |s1:t ] st+P bxt+P , E R−1X st+P
E
= r=0
[ W ( st+P , st−r ) xt−r|s1:t ] + E st+P
[ b(st+P )|s1:t ]
( 14 ) Here , given the states s1:t up to time t , the distribution of state st+P only depends on st according to the Markovian property of the state evolution model , ie
Pr(st+P |s1:t ) = Pr(st+P |st ) = [ AP ]st ,st+P
( 15 ) where A is the transition matrix , and [ AP ]st ,st+P is the entry of AP corresponding to the sender state st−r and the receiver state st+P . Then , we have :
[ W ( st+P , st−r ) xt−r|s1:t ]
E st+P
= X st+P ∈S
Pr(st+P |s1:t)W ( st+P , st−r ) xt−r
( 16 ) and
E st+P
[ b(st+P )|s1:t ] = X st+P ∈S
Pr(st+P |s1:t)b(st+P )
( 17 )
Plugging Eq ( 16) (17 ) back into Eq ( 14 ) , we get the estimated sensor measurement at time t + P . 5.1 A Special Case : Static Sensor Selection
So far , we have focused on a dynamic sensor selection model , in which different sets of sensors can be selected for different states . However , in some applications , we may be interested in a static sensor selection model , where a fixed set of sensors are selected no matter in which state the sensor network might be in . For this purpose , we can stack all the prediction matrices in the column direction , and apply a L1,2 norm to enforce all the matrices have the same set of zero columns . Then , the sensors corresponding to these zero columns can be excluded without affecting the prediction of the future sensor readings . In this spirit , the static sensor selection is a special case of its dynamic counterpart . The corresponding optimization algorithm does not need to change much to handle the similar stacked L1,2 matrix norm . Note that , although this is a static sensor selection model , we can still use the state evolution model in Definition 1 to capture the dynamics of the sensor network , and the state driven prediction model ( 1 ) to dynamically predict the sensor signals . The only difference is that the set of selected sensors will not change as the states evolve . In the experiment , we will compare between the dynamic and the static model .
950 Table 1 : ICU sensors for monitoring patient health conditions . sensor id abbreviation measurement
1 2 3 4 5 6 7
Hr Temp SpO2 BPd BPs BPm RESP heart rate body temperature saturation of peripheral oxygen diastolic blood pressure systolic blood pressure mean blood pressure respiration
6 . EXPERIMENTS
In this section , we conduct experiments to test the proposed sensor selection and prediction model with two real sensor networks . First , we will introduce the data sets collected from these two sensor networks . We will explain how they are collected and some statistics about them . 6.1 Data Sets
Figure 2 shows three examples of a pair of sensor data time series ( top row ) and their correlations ( bottom row ) extracted from the two datasets . The datasets exhibit both periodic and gradually changing patterns of the correlations between sensors .
611 Oil drilling rig sensor network This sensor network is deployed on oil rigs , and is used is used to monitor the status of the oil drilling system , and diagnosing and predicting the potential risk of dysfunction . The sensor network consists of sensors deployed at the surface , as well as sensors along the drill string , that operate within the wellbore in the subsurface of the earth . As the drill moves for vertical and sometimes horizontal drilling , and encounters different formations , the position and properties of these sensors with relation to the earth ’s surface changes affecting the observed measurements , and their correlations .
We consider a set of 33 such surface and wellbore sensors and demonstrate how our state based sensor selection approach can enable significant reductions in the number of sensors needed , while minimizing prediction error . 612 ICU sensor network The second data set is generated by medical sensors used to monitor the status of patients in ICUs . We collected the readings of seven different types of ICU sensors ( see Table 1 ) from 357 patients , which closely monitor the health conditions ranging from heart rate , blood pressure to respiration . For different patients , the length of sensor measurements vary considerably . Some patients only have a few hundred measurements for each sensor , while others have several million measurements . We wished to track and predict the main trends of each patient ’s states , and select a set of sensors whose readings are the most critical to predict these states . It is often beneficial to understand which sensors contain key indications on the health condition of the monitored patient in ICUs . 6.2 Results on Oil Drilling Rig Sensors
Figure 3 presents the comparison of error rates among the following algorithms : s e t t a S f o x e d n
I
1 2 3 4 5
5
10
15
20 Index of Sensors
25
30
Figure 4 : Active sensors in each state for oil drilling rig sensor dataset : the red cells in each row represent the active sensors for a state .
• Sampled Uni ARM : the baseline benchmark . This model uses an auto regressive model ( ARM ) [ 12 ] to predict the future sensor readings , and a univariate ARM is created to model each individual sensor . Also , at each time step , a subset of sensors is selected in random .
• Sampled Multi ARM : this model is the same as Sampled Uni ARM , except that a multi variate ARM is created to model the sensors simultaneously .
• Uni ARM PES : this model uses a univariate model for each individual sensor , but a power efficient selection ( PES ) criterion is adopted to select a subset of the most critical sensors that can provide the best accurate prediction over time on the future readings [ 1 ] .
• Multi ARM PES : It is the same as Uni ARM PES , except a multi variate ARM is used to model the dynamics of sensor readings over time [ 1 ] .
• SDSSP ( State driven sensor selection and prediction ) : this is the proposed method in this paper .
In Figure 3(a ) , the error rates are compared versus the varying the number of active sensors selected by different algorithms . Since the proposed SDSSP is a dynamic sensor selection model , the number of active sensors keep changing over time with the evolving states , so that the number of active sensors in the figure is an average number of sensors selected by the states over time . The other algorithms are compared by setting the numbers of active sensors to integers nearest to these average numbers . Figures 3(b) 3(c ) compare the performances with varying window sizes R and prediction lags P as in Eq ( 1 ) . The results show the proposed SDSSP consistently outperforms the other methods . Figure 4 shows the active sensors in each state derived from the oil drilling sensor dataset . The sensors indexed from 1 − 8 and 20 − 33 are global sensors that are deployed on the surface of the rig platform to monitor the operation of the whole oil drilling system . On the other hand , the sensors indexed from 9 − 19 are deployed in the vertical or curvedlateral segment of the wellbore . We obtain the active sensors by setting the cost for each undersurface sensor to twice that for each global sensor . All five states , except state 4 , select both global and vertical sensors ; on the contrary , state 4 only selects the vertical sensors , implying that this state corresponds to a drilling operation in the vertical borehole reaching deep into the earth ’s subsurface .
Figure 5 compares the error rates between the dynamic model ( SDSSP ) and the static model as presented in Section 5.1 where a fixed set of sensors are selected . The comparison is made by varying the window size R and the prediction lag P . Clearly , the dynamic model usually provides more
951 100 i s g n d a e R r o s n e S
50 x 105
4 x 105
4 sensor #4 sensor #10
2 i s g n d a e R r o s n e S
2
200
150
Sensor #6 Sensor #15
100 i s g n d a e R r o s n e S
100
0
0 0
0.5 0.5
1 1
1.5 1.5
Time Steps
2 2
2.5 2.5
0
3 3 x 104 x 104
0
0 0
0.5 0.5
1 1
1.5 1.5
Time Steps
2 2
2.5 2.5
0
3 3 x 104 x 104
50
0 0
1000 1000
2000 2000
3000 3000
4000 4000
5000 5000
6000 6000
Time Steps
( a )
( b )
( c ) s n o i t l a e r r o C
1
0.8
0.6
0.4
0.2
0
−0.2
−0.4
−0.6 0
0.5
1
1.5
Time Steps
2
2.5
3 x 104
( d ) s n o i t l a e r r o C
1
0.5
0
−0.5
−1 0
0.5
1
1.5
Time Step
( e )
2
2.5
3 x 104 s n o i t l a e r r o C
1
0.5
0
−0.5
−1 0
2000
4000
6000
Time Steps
( f )
39
38
Heart rate Body temperature
7000 7000
8000 8000
9000 9000
37 10000 10000
8000
10000
Figure 2 : Examples of oil rig sensor dataset ( first two columns ) and ICU sensor dataset ( the last column ) . Subfigure ( a) (c ) : a pair of sequences of sensor readings ; Subfigure ( d) (f ) : the sequence of Pearson ’s correlation between the two sensors .
0.08
0.07
0.06
0.05
0.04
0.03 e t a R r o r r
E e g a r e v A
SDSSP Multi−ARM PES Uni−ARM PES Sampled Multi−ARM Sampled Uni−ARM
0.02
2
4
6
8
10
12
( Average ) Number of Active Sensors
14
16 e t a R r o r r
E e g a r e v A
0.085
0.08
0.075
0.07
0.065
0.06
0.055
0.05
0.045
0.04
1
5
SDSSP Multi−ARM PES Uni−ARM PES Sampled Multi−ARM Sampled Uni−ARM
3
Window Size R e t a R r o r r
E e g a r e v A
0.085
0.08
0.075
0.07
0.065
0.06
0.055
0.05
0.045
1
SDSSP Multi−ARM PES Uni−ARM PES Sampled Multi−ARM Sampled Uni−ARM
5
3
Prediction Lag P
( a ) Prediction Lag P = 1 , Window Size R = 3
( b ) Prediction Lag P = 1
( c ) Window Size R = 3
Figure 3 : Comparison of error rates by the different algorithms on the oil drilling rig sensor dataset : ( a ) the error rates with varying numbers of active sensors ( achieved with P = 1 and R = 3 ) ; ( b ) the error rates with varying window sizes R when the prediction lag is fixed to P = 1 ; ( c ) the error rates with varying prediction lags P when window size is fixed to R = 3 . s e t t a S f o x e d n
I
1
2
3
1
2
3 5 Index of Sensors
4
6
7
Figure 7 : Active sensors in each state for ICU sensor dataset : the red cells in each row represent the active sensors for a state . accurate prediction on the sensor data streams over time than its static counterpart . 6.3 Results on ICU Sensors
Similar to the experimental set up for oil drilling rig sensors , Figure 6 presents the results in comparison with other algorithms . Through the comparison , SDSSP outperforms the other algorithm on this ICU sensor dataset in terms of different number of active sensors , varying window sizes and prediction lags .
Figure 7 illustrates the active ICU sensors in each state , where the indices of these ICU sensors have been shown in
Table 1 . We can find that for the first two states , the selected sensors usually measure the instant conditions of the patients , such as heart rates and diastolic/systolic blood pressure . On the other hand , the third state selects the sensors that are relevant to a relatively long time health conditions , including mean blood pressure and body temperature . In accordance with these findings , we observe that when the monitored patient is in a stable condition ( ie , the sensor readings do not change much ) , state 3 is often activated ; otherwise , patients are more likely in instant state 1 or state 2 . This is yet another example of the kind of semantic interpretation that one can often associate with the hidden states that are discovered in a data driven manner .
Figure 8 compares the proposed dynamic model with the static counterpart on seven different ICU sensors with the window size P and prediction lag R both set to 1 . Most of time , the dynamic model achieves better performance than the static model on these seven ICU sensors . 6.4 Effect of the Number of States
In the aforementioned experiments , the number of states are set based on the performance on an independent validation set . It is also intriguing to study the change of perfor
952 4
3.5
3 r o r r e
2.5 dynamic model static model
4
3.5
3 r o r r e
2.5 dynamic model static model r o r r e dynamic model static model
4.5
4
3.5
3
2.5
2
1.5
1
0.5 n o i i t c d e r p n o i i t c d e r p r o r r e n o i i t c d e r p
2
1.5
1
0.5
0
0
4.5
4
3.5
3
2.5
2
1.5
1
0.5 n o i i t c d e r p
1000
2000
3000
4000 5000 time step
6000
7000
8000
9000
( a ) P = 1 , R = 1 dynamic model static model r o r r e n o i i t c d e r p
2
1.5
1
0.5
0
0
4.5
4
3.5
3
2.5
2
1.5
1
0.5
1000
2000
3000
4000 5000 time step
6000
7000
8000
9000
( b ) P = 1 , R = 2
0
0
1000
2000
3000
4000 5000 time step
6000
7000
8000
9000
( c ) P = 1 , R = 3 dynamic model static model r o r r e n o i i t c d e r p dynamic model static model
4.5
4
3.5
3
2.5
2
1.5
1
0.5
0
0
1000
2000
3000
4000 5000 time step
6000
7000
8000
9000
0
0
1000
2000
3000
4000 5000 time step
6000
7000
8000
9000
0
0
1000
2000
3000
4000 5000 time step
6000
7000
8000
9000
( d ) P = 2 , R = 1
( e ) P = 2 , R = 2
( f ) P = 2 , R = 3
Figure 5 : Comparison between dynamic and static models for predicting P step measurement with a window of length R . The six figures plot the average prediction errors over 33 sensors with varying P and R on the test time series . The result shows the dynamic prediction model usually has smaller prediction errors than its static counterpart most of time .
0.4
0.35
0.3
0.25
0.2
0.15
0.1 e t a R r o r r
E e g a r e v A
0.05
2.5
3
SDSSP Multi−ARM PES Uni−ARM PES Sampled Multi−ARM Sampled Uni−ARM
3.5
4
( Average ) Number of Active Sensors
4.5
5
5.5 e t a R r o r r
E e g a r e v A
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
1
SDSSP Multi−ARM PES Uni−ARM PES Sampled Multi−ARM Sampled Uni−ARM
3
Window Size R
5 s e t a R r o r r
E e g a r e v A
0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
1
SDSSP Multi−ARM PES Unit−ARM PES Sampled Multi−ARM Sampled Uni−ARM
3
Prediction Lag P
5
( a ) Error rates
( b ) window size R
( c ) Prediction lag P
Figure 6 : Comparison of error rates by the different algorithms on the ICU Sensor dataset : ( a ) the error rates with varying numbers of active sensors ( achieved with P = 1 and R = 3 ) ; ( b ) the error rates with varying window sizes R when the prediction lag is fixed to P = 1 ; ( c ) the error rates with varying lag predictions P when window size is fixed to R = 3 .
0.065
0.06
0.055
0.05
0.045 s e t a r r o r r e e g a r e v A
0.04 1
2
3
4
5
6
Number of states
0.24
0.23
0.22
0.21
0.2
0.19 s e t a r r o r r e e g a r e v A
7
8
9
10
0.18 1
1.5
2
( a ) Oil drilling rig
2.5 3.5 Number of states
3
( b ) ICU
4
4.5
5
Figure 9 : Change of average error rates versus different number of states . mance when the number of states varies ( cf . Figure 9 ) . To ensure fair comparison between different numbers of states , the average number of active sensors is set to 8 on the oil drilling dataset and to 3 on ICU sensor dataset . From the result , we observe that neither too many nor too few states is adequate to perform well . The model with too few states underestimates the system complexity , making it incapable of capturing the conditions of a rapid changing system . On the other hand , a model with too many states tends to overfit with the sensor time series data .
7 . CONCLUSION
This paper presents a state driven dynamic sensor selection and prediction model in which a subset of active sensors are dynamically selected over time . A state stacked sparseness algorithm is developed to select the active sensors by minimizing the prediction error over time in a state driven dynamic system . We derive an efficient algorithm which converges at a rate of O(1/L2 ) to an optimal sensor selection and prediction model , where L is the number of iterations . We apply the algorithm to real world applications of two different industry domains – oil drilling sensors and ICU sensors . The results show that the algorithm can achieve the best performance in terms of error rates in tracking and predicting the sensor readings over time .
8 . REFERENCES
[ 1 ] C . Aggarwal , Y . Xie , and P . Yu . On dynamic data driven selection of sensor streams . In ACM SIGKDD Conference on Knowledge Discovery and Data Mining , August 2011 .
[ 2 ] C . C . Aggarwal , editor . Data Streams : Models and
Algorithms . Springer , 2007 .
953 ( c ) SpO2 dynamic static
5
( f ) BPm
10 x 104 dynamic static
5
10 x 104
50
40
30
20
10
0
0
250
200
150
100
50
0
0
( a ) Hr dynamic static
5
( d ) BPd
10 x 104 dynamic static
5
10 x 104
30
25
20
15
10
5
0
0
250
200
150
100
50
0
0
150
100
50
0
0
40
30
20
10
0
0
250
200
150
100
50
0
0
( b ) Temp dynamic static
5
( e ) BPs
10 x 104 dynamic static
5
( g ) Resp
10 x 104 dynamic static
5
10 x 104
Figure 8 : Comparison between dynamic and static models on seven ICU sensors over a series of 10 , 0000 sensor measurements . The result is obtained with P = 1 , and R = 3 .
[ 3 ] C . C . Aggarwal , A . Bar Noy , and S . Shamoun . On sensor selection in linked information networks . In IEEE DCOSS Conference , 2011 .
[ 4 ] C . Anagnostopoulos , N . M . Adams , and D . J . Hand .
Streaming covariance selection with applications to adaptive querying in sensor networks . The Computer Journal , 53(9):1401–1414 , 2010 .
[ 5 ] T . Arici , T . Akgun , and Y . Altunbasak . A prediction error based hypothesis testing method for sensor data acquisition . ACM Transactions on Sensor Networks , 2:529–556 , 2006 .
[ 6 ] S . Chang , G J Qi , C . C . Aggarwal , J . Zhou ,
M . Wang , and T . S . Huang . Factorized similarity learning in networks . pages 60–69 , 2014 .
[ 7 ] A . Deligiannakis and Y . Kotidis . Data reduction techniques in sensor networks . IEEE Data Engineering Bulletin , 28(1):19–25 , 2005 .
[ 8 ] D . Golovin , M.Faulkner , and A . Krause . Online distributed sensor selection . In IPSN Conference , 2010 .
[ 9 ] V . Klemas . Tracking oil slicks and predicting their trajectories using remote sensors and models : case studies of the sea princess and deepwater horizon oil spills . Journal of Coastal Research , pages 789–797 , 2010 .
[ 10 ] G . Kollios , J . Byers , J . Considline ,
M . Hadjielefttheriou , and F . Li . Robust aggregation in sensor networks . IEEE Data Engineering Bulletin , 28(1):26–32 , 2005 .
[ 11 ] A . Markov . Extension of the limit theorems of probability theory to a sum of variables connected in a chain . 1971 .
[ 12 ] S . Nassar , K P SCHWARZ , N . EL SHEIMY , and A . Noureldin . Modeling inertial sensor errors using autoregressive ( ar ) models . Navigation , 51(4):259–268 , 2004 .
[ 13 ] Y . Nesterov . A method of solving a convex programming problem with convergence rate o(1/k2 ) . Soviet Mathematics Doklady , 27:372–376 , 1983 .
[ 14 ] S . Papadimitriou , J . Sun , and C . Faloutsos . Data
Streams : Models and Algorithms , chapter Dimensionality Reduction and Forecasting of Time Series Data Streams , pages 261–288 . Springer , 2007 .
[ 15 ] G J Qi , C . C . Aggarwal , and T . S . Huang . Online community detection in social sensing . In WSDM , pages 617–626 . ACM , 2013 .
[ 16 ] Y . Sakurai , S . Papadimitriou , and C . Faloutsos . Braid :
Stream mining through group lag correlations . In ACM SIGMOD Conference , July 2005 .
[ 17 ] S . Sharshar , L . Allart , and M C Chambrin . A new approach to the abstraction of monitoring data in intensive care . In Artificial Intelligence in Medicine , pages 13–22 . Springer , 2005 .
[ 18 ] P . Tseng . On accelerated proximal gradient methods for convex concave optimization . submitted to SIAM J . Optim . , May 2008 .
[ 19 ] L . Yann Ael , S . Santini , and G . Bontempi . Adaptive model selection for time series prediction in wireless sensor networks . Signal Processing , 87:3010–3020 , 2007 .
[ 20 ] B K Yi , N . Sidiropoulos , T . Johnson , H . Jagadish , C . Faloutsos , and A . Biliris . Online data mining for co evolving time sequences . In ICDE , 2000 .
[ 21 ] M . Yuan and Y . Lin . Model selection and estimation in regression with grouped variables . Journal of the Royal Statistical Society : Series B ( Statistical Methodology ) , 68(1):49–67 , 2006 .
954
