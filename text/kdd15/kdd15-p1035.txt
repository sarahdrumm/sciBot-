Stochastic Divergence Minimization for Online Collapsed Variational Bayes Zero Inference of Latent Dirichlet Allocation
Issei Sato
The University of Tokyo sato@rdlitcu tokyoacjp
Hiroshi Nakagawa The University of Tokyo n3@dlitcu tokyoacjp
ABSTRACT The collapsed variational Bayes zero ( CVB0 ) inference is a variational inference improved by marginalizing out parameters , the same as with the collapsed Gibbs sampler . A drawback of the CVB0 inference is the memory requirements . A probability vector must be maintained for latent topics for every token in a corpus . When the total number of tokens is N and the number of topics is K , the CVB0 inference requires O(N K ) memory . A stochastic approximation of the CVB0 ( SCVB0 ) inference can reduce O(N K ) to O(V K ) , where V denotes the vocabulary size . We reformulate the existing SCVB0 inference by using the stochastic divergence minimization algorithm , with which convergence can be analyzed in terms of Martingale convergence theory . We also reveal the property of the CVB0 inference in terms of the leave one out perplexity , which leads to the estimation algorithm of the Dirichlet distribution parameters . The predictive performance of the propose SCVB0 inference is better than that of the original SCVB0 inference in four datasets .
Categories and Subject Descriptors G.3 [ Probability and Statistics ] : Nonparametric statistics
General Terms Algorithms
1 .
INTRODUCTION
Latent Dirichlet allocation ( LDA ) [ 1 ] has been one of the most studied probabilistic latent variable models over the past decade . Originally , the variational Bayes ( VB ) inference was used for learning LDA [ 1 ] . The collapsed Gibbs ( CG ) sampler [ 2 ] was then proposed for LDA .
Teh et al . [ 3 , 4 ] developed the collapsed variational Bayes ( CVB ) inference as an alternative ( deterministic ) inference to solve some of the problems of the VB inference and CG sampler . The CVB inference is a variational inference improved by marginalizing out parameters the same as with the CG sampler [ 5 , 2 ] . Sung et al.[6 ] c⃝ 2015 ACM . ISBN 978 1 4503 3664 2/15/08 $1500
DOI : http://dxdoiorg/101145/27832582783355
Inference
Table 1 : Summary of this work . SCVB0 O(V K )
CVB0 This work O(N K ) O(V K ) O(V K )
Memory usage
Update cost / mini batch
HDP ( Asym . Dir )
Convergence analysis
✓
✓
✓ ✓ generalized the CVB inference for conjugate exponential family models
Since the CVB inference for LDA requires intractable integrals , the original work involved a second order Taylor expansion . Asuncion et al . [ 7 ] proposed another approximation that uses only the zero order information , called the CVB0 inference . The CVB0 inference for LDA converges more quickly than does the CG sampler since it is deterministic . Furthermore , their empirical results suggest that the CVB0 inference learns models that are as good or better ( predictively ) than those learned using the CG sampler . We [ 8 ] simplified the CVB0 inference for the hierarchical Dirichlet process ( HDP ) [ 5 ] and argued that the CVB0 inference performed well in terms of not only perplexity but also a link prediction of a social network and a nearest neighbor search of documents .
A drawback of the CVB0 inference is the memory requirements . A probability vector must be maintained for latent topics for every token in a corpus . We denote wd,i as the i th word in document d and zd,i as a latent variable corresponding to wd,i . The notation q(zd,i = k ) is a probability that latent variable zd,i takes topic k . The CVB0 inference must maintain topic probability q(zd,i = k ) for every token wd,i in the corpus because it has to subtract the previous value of q(zd,i = k ) from expected counts when updating q(zd,i = k ) . If the total number of tokens is N and the number of topics is K , the CVB0 inference requires O(N K ) memory . The purpose of this paper is to reduce O(N K ) to O(V K ) where V denotes the vocabulary size .
Foulds et al.[9 ] developed a stochastic approximation of CVB0 ( SCVB0 ) inference for LDA to solve the problem of the CVB0 inference . They ignored the subtraction of q(zj,i = k ) and applied a Robbins Monro type stochastic approximation [ 10 ] for calculating the expected counts . Therefore , their SCVB0 inference does not need to maintain q(zd,i ) for all tokens . They showed that their SCVB0 inference outperformed the stochastic variational Bayes ( SVB ) inference proposed by Hoffman et al . [ 11 ] . However , there are three concerns with this SCVB0 inference .
( 1 ) It cannot be applied to LDA with the asymmetric Dirichlet distribution in which we have to estimate the parameters of the
1035 Dirichlet distribution . The derivation of the SCVB0 inference by Foulds et al.[9 ] is based on the fact that the CVB0 inference by ignoring the subtraction of q(zj,i = k ) is identical to the maximum a posterior ( MAP ) inference with smoothing by incrementing the Dirichlet parameters by one . That is , since we have to manually adjust the parameters , we cannot estimate the parameters in the mathematically right way .
It is well known that it is important to use the asymmetric Dirichlet distribution over document topic distribution . Wallach et al . [ 12 ] found that the asymmetric Dirichlet distribution over document topic distributions has substantial advantages over the symmetric one . The asymmetric Dirichlet distribution over document topic distribution can learn meaningful topics even when no stop words are removed .
( 2 ) We have a question regarding why the SCVB0 inference outperformed the SVB inference because the SCVB0 inference actually approximates the MAP inference rather than the CVB0 inference . When the number of data poins is large , Bayesian inference , including the VB inference , can be approximated by the MAP inference . However , this is not the answer to the question since the SVB0 inference works better than the SVB inference . ( 3 ) The computational cost of one update of the SCVB0 is O(V K ) , which is too expensive when we update parameters for every document . Therefore , using minibatch updates was suggested . However , in a large scale dataset , the number of minibatchs is also large ; thus , the cost O(V K ) per minibatch can be also expensive when V is too large . Contributions : For these problems , we propose an SCVB0 inference by using the stochastic divergence minimization ( SDM ) algorithm . We found that the SCVB0 inference proposed by Foulds et al.[9 ] is a good approximation of the SDM algorithm .
Recall that the CVB0 inference was derived as the Taylor approximation method of the CVB inference in the first place ; thus , its derivation was not mathematically formulated as an optimization problem . To solve this problem , we [ 13 ] derived the CVB0 inference as the α divergence minimization problem . This reformulation showed that the CVB0 inference is not just an approximation method of the CVB inference and clarified the properties of the CVB0 inference for LDA , eg , the CVB0 inference is not affected by the zero forcing effect which induces the mode seeking property of the statistical inference [ 14 ] .
Martingale convergence theory . not need minibatchs whose computational cost is O(V K ) .
( 4 ) Since the SDM algorithm is a word wise optimization , we do
On contribution 1 : We [ 8 ] proposed a simplified CVB0 inference with the parameter estimation of the asymmetric Dirichlet dis
By using the reformulation of the CVB0 inference as an optimization problem , we can apply the stochastic approximation to this optimization problem . That is , the proposed SDM algorithm is a stochastic approximation of the divergence minimization reformulation of the CVB0 inference . There are four contributions in this work .
( 1 ) We modify our previously proposed divergence minimization formulation of the CVB0 inference [ 13 ] , which enables us to understand a novel property of the CVB0 inference and provide an estimation method of the asymmetric Dirichlet distribution of LDA in the CVB0 inference .
( 2 ) The proposed SCVB0 inference more accurately approximate the CVB0 inference , which can lead to a higher predictive performance of LDA .
( 3 ) The proposed SCVB0 inference can be analyzed in terms of tribution , which is a truncation approximation of the hierarchical Dirichlet process ( HDP ) [ 5 ] . This approach is based on the objective function of the CVB inference , not that of the CVB “ 0 ” inference . However , we [ 13 ] previously found that the CVB0 inference is not just an approximation method of the CVB inference .
Our SCVB0 inference is not based on the CVB inference but on the divergence minimization framework . Therefore , we need an estimation method for the parameters of the asymmetric Dirichlet distribution in terms of the divergence minimization framework for mathematical coherent .
To solve this problem , we have to modify the original divergence minimization formulation of the CVB0 inference that we previously proposed . Interestingly , this novel reformulation reveals that the CVB0 inference is related to the leave one out ( LOO ) likelihood estimation , which is also related to the LOO peplexity . We use this property to estimate the parameters of the asymmetric Dirichlet distribution .
On contribution 3 : In the stochastic approximation literature , the update is usually interpolated by a step size in the space of parameters . However , since the CVB0 inference integrates out model parameters , the SCVB0 update needs to be interpolated by a step size in the space of sufficient statistics . This is not the same and we cannot directly use the logic of the original stochastic approximation to explain the convergence of the proposed SCVB0 inference . Therefore , its convergence analysis is a challenging theme . In this work , by using the SDM reformulation , we can analyze the convergence of the proposed SCVB0 inference in terms of Martingale convergence theory .
Table 1 summarizes this work . The remainder of this paper is organized as follows . In Sections 3 , 4 , 5 , and 6 , we give an overview of related work such as LDA , CVB0 and SCVB0 . In Section 7 , we propose the SDM algorithm to derive the SCVB0 inference , and analyze it in Section 8 . In Section 9 , we evaluate the SDM based SCVB0 inference .
2 . NOTATIONS
The notations Dir(· ) and Multi(· ) denote the Dirichlet distribution and the multinomial distribution , respectively . The notation D denotes the total number of documents , N the total number of words ( tokens ) , V the vocabulary size , K the total number of topics , d a document index , ie , d = 1,··· , D , v a vocabulary index , ie , v = 1,··· , V , nd the number of words in documents d , nv the number of times word v appears in whole documents , nd,k the number of times topic k appears in document d , nd,k,v the number of times word v generated from topic k in document d , and nk,v the number of times word v appears in topic k . The bold face letters denote sets of the corresponding variables , eg , wd,i denotes the i th word in document d , wd = {wd,i}nj i=1 , and w = {wd}D d=1 . The notation zd,i denotes the assigned topic at the i th word in document d , θd,k the probability of topic k appearing in document d , ϕk,v the probability of word v appearing in topic k , and E[x ] denotes the expectation of x , ,(x ) the gamma function , and )(x ) the digamma function .
3 . OVERVIEW OF LDA
In LDA , the document topic distribution θd and topic word dis tribution ϕk are generated by θd ∼ Dir(γ ) ( d = 1,··· , D ) , ϕk ∼ Dir(β ) ( k = 1,··· , K ) , ( 1 ) where γ = ( γ1,··· , γK ) is a K dimensional vector and β = ( β1,··· , βV ) is a V dimensional vector . For each document d ,
1036 generate the i th topic zd,i and word wd,i : zd,i ∼ Multi(θd ) , wd,i ∼ Multi(ϕzd;i
) .
( 2 )
They ignored the subtraction of q(zj,i = k ) and applied a stochastic approximation for calculating the expected counts E[nd,k ] , E[nk,v ] , and E[nk,· ] . Their update is approximated by
Wallach et al .
[ 12 ] explored the effects of choosing γ and β in LDA . They found that using asymmetric γ and symmetric β = ( β,··· , β ) results in better performance . In particular , asymmetric γ is useful for robustness against stop words . It can learn meaningful topics even when no stop words are removed . Therefore , we use symmetric β = ( β,··· , β ) in this paper .
4 . CVB0 INFERENCE FOR LDA
Teh et al .
[ 4 ] proposed the CVB inference for LDA . They marginalize over θ and ϕ in the CVB inference the same as with the CG sampler . By integrating out θd and ϕk , a join distribution over w and z is given by
K∏
]
[
D∏ d=1 p(z ; wjfl ; fi ) =
[
,(fl0 )
K∏
,(fl0 + nd )
,(flk + nd,k )
,(flk )
V∏ k=1
,(V β )
,(V β + nk,· ) v=1 k=1
,(β )
,(β + nk,v )
]
.
( 3 )
( 4 )
The CVB inference approximates the posterior distribution p(z|w , γ , β ) by using the variational posterior distribution
D∏ nd∏ q(z ) = q(zd,i ) , i=1 whose update equations are given by d=1 qCVB(zd,i = k ) ∝ exp(E exp(E q(z
\d;i)[log(n \d;i)[log(n
+ β ] )
\d,i k,wd;i \d,i k,· + V β) ] ) q(z
× exp(E
\d,i d,k + γk] ) , q(z
( 5 ) where “ \d , i ” denotes subtracting the counts related to wd,i and zd,i .
\d;i)[log(n
The problem is that the expectation in the above update cannot be analytically calculated . Therefore , we need an approximation for this expectation .
Asuncion et al . [ 7 , 15 ] showed the usefulness of a simple approximation that uses only zero order information of the Taylor approximation , called the CVB0 inference , in LDA . An update with the CVB0 inference is given by qCVB0(zd,i = k ) ∝E[n E[n
] + β
\d,i k,wd;i \d,i k,· ] + V β
( E[n
\d,i d,k ] + γk ) .
( 6 )
We [ 8 ] argued that the CVB0 inference performed well in terms of not only perplexity but also a link prediction of a social network and a nearest neighbor search of documents .
5 . STOCHASTIC CVB0 INFERENCE FOR
LDA
A disadvantage of the CVB0 inference is the memory requirements . The q(zd,i = k ) must be maintained for every token in the corpus because it has to subtract the previous value of q(zj,i = k ) from expected counts when updating q(zj,i = k ) . Foulds et al.[9 ] proposed a SCVB0 inference for LDA . They showed that the SCVB0 inference outperformed the SVB inference proposed by Hoffman et al . [ 11 ] . q(zd,i = k ) ∝ N ϕ N Z k,v + β k + V β
( N θ d,k + αk ) ,
( 7 ) k,v , and N Z where N θ stochastic approximation , ie , N θ and N Z k d,k , N ϕ ≈ E[nk,· ] . k are expected counts estimated by a ≈ E[nk,v ] ,
≈ E[nd,k ] , N ϕ d,k k,v
These calculations of the approximated counts are performed as follows . We denote qd,i = ( q(zd,i = 1 ) , q(zd,i = 2),··· , q(zd,i = K) ) . Let Qd,i be a K by V matrix with the v th column being qd,i and with zeros in the other entries when wd,i = v , N ϕ be a K by V matrix whose ( k , v) th element is N ϕ d be a K dimensional vector whose k th element is N θ d,k , and N Z be a K dimensional vector whose k th element is N Z k . They used one step size , ρϕ t , for N ϕ and N Z , and another step size ρθ k,v , N θ t for N θ d .
For each token wd,i , after using ( 7 ) , update d = ( 1 − ρθ
N θ t )N θ d + ρθ t ndqd,i ,
( 8 ) where t is the update counts of N θ d and is not equal to i because Eqs . ( 7 ) and ( 8 ) are performed throughout a document in a small number of times as a burn in period before updating N ϕ and N Z . Since it is too expensive to update the N ϕ after every document , Foulds et al . suggested the use of minibatch updates . Let Bt be the t th subset of whole documents and |Bt| be the total number of words ( tokens ) in Bt . After processing Bt by using Eqs.(7 ) and ( 8 ) , update
N ϕ = ( 1 − ρϕ t )N ϕ + ρϕ t
N Z = ( 1 − ρϕ t )N Z + ρϕ t
N|Bt| N|Bt|
∑ ∑ d,i∈Bt
Qd,i , qd,i . d,i∈Bt
( 9 )
( 10 )
6 . DIVERGENCE MINIMIZATION
In this section , we review a statistical inference based on the α divergence minimization and reformulate the divergence minimization formulation of the CVB0 inference that we previously proposed [ 13 ] . 6.1 Local α divergence Minimization ( KL ) divergence [ 16 , 17 , 18 ] , indexed by α ∈ ( −∞,∞ ) .
The α divergence is a generalization of the Kullback Leibler
In this paper , we define α divergence used by Minka [ 14 ] as αp(x ) + ( 1 − α)q(x ) − p(x)αq(x)1−αdx Dα[p||q ] =
.
∫
α(1 − α )
( 11 )
If p = q , α divergence is zero .
The important property of this definition in this work is that p(x ) and q(x ) do not need to be normalized . For example , α = 1 corresponds to the unnormalized KL divergence ( see Sato et al . [ 13] ) . Let our task be to approximate a complex probabilistic distribu∏ tion p(x ) where x = {x1 , x2,··· , xn} . We approximate p(x ) by q(x ) , which is a fully factorized distribution , ie , q(x ) = i=1 q(xi ) . A basic approach to obtaining q(x ) is to minimize the divergence . We focus on the α divergence minimization , ie , n
∗ q
( x ) = argmin q(x )
Dα[p(x)||q(x) ] .
( 12 )
1037 This minimization is intractable in many cases . The alternative way is the local α divergence minimization given by \i)||q(xi)q(x
Dα[p(xi|x
( xi ) = argmin
\i)q(x
∗ q
\i) ] .
( 13 ) q(xi )
\i)q(x
\i ) . In other words , we replace q(xi ) with p(xi|x
This means that we replace p(x ) = p(xi|x \i ) with p(xi|x \i ) . When α = 1 , ie , KL divergence , this local divergence minimization is equal to the expectation propagation algorithm . 6.2 CVB0 as Divergence Minimization
\i)p(x
We [ 13 ] showed that the CVB0 inference is formulated by the local α divergence minimization , which means that the CVB0 inference is a kind of power ( alpha ) expectation propagation . The divergence minimization in LDA is formulated as Dα[p(z|w , γ , β)||q(z) ] .
( 14 ) min q(z )
Since this minimization is intractable , we [ 13 ] solved the local divergence minimization problem instead of this problem , which recovers the CVB0 inference . However , this formulation cannot give the framework to estimate parameter γ of the asymmetric Dirichlet distribution Dir(θd|γ ) . Therefore , we modify this formulation .
We consider estimating the variational distribution over w and z given by q(w , z ) = q(z|w)q(w ) = q(zd,i|wd,i)q(wd,i ) .
( 15 )
D∏ nd∏ d=1 i=1
We estimate q(w , z ) by using the α divergence minimization : min q(w,z )
= min q(w,z )
Dα[p(w , z|γ , β)||q(w , z ) ] \d,i , z \d,i , γ , β ) \d,i , γ , β)p(w
Dα[p(wd,i , zd,i|w \d,i|w
× p(z
We now focus on the local divergence minimization for b given by
\d,i k,v
D1[qb→n(wd,i , zd,i)q(z
\d,i|w
\d,i)pemp(w
\d,i)||q(w , z) ] , min \d;i b k;v
( 21 ) \d,i|γ , β ) with empirical word distribuwhere we replace p(w \d,i|γ , β ) is intion pemp(w tractable . The empirical word distribution is formulated by the product of the delta function δ(wd,i = v ) when the i th word , wd,i , in document d is actually word v .
\d,i ) because the expectation over p(w
In fact , the local divergence minimization ( 21 ) recovers the result of Sato et al[13 ] Therefore , the solution of this minimization is obtained as the closed form solution ( see Sato et al . [ 13 ] for details . ) ,
\d,i k,v = E b
\d;i)[n q(z
\d,i k,v ] + β .
( 22 )
Similarly ,
\d,i d,k = E a
\d;i)[n q(z
\d,i d,k ] + γk , c
\d,i k = E
\d;i)[n q(z
\d,i k,· ] + V β
( 23 ) are closed form solutions of
D1[qa→n(wd,i , zd,i)q(z
\d,i|w
\d,i)pemp(w
\d,i)||q(w , z) ] ,
D−1[qc→n(wd,i , zd,i)q(z
\d,i|w
\d,i)pemp(w
( 24 ) \d,i)||q(w , z) ] .
( 25 )
By substituting optimized a v , zd,i = k ) of Eq ( 26 ) , we have
\d,i d,k , b
\d,i k,v , and c
\d,i k into q(wd,i = q(wd,i , zd,i ) =
E E q(z
\d;i)[n \d;i)[n
\d,i k,v ] + β \d,i k,· ] + V β q(z
E q(z
\d,i d,k ] + γk
\d;i)[n \d,i n d + γ0
.
( 26 ) min \d;i a d;k min \d;i c k
K∑ K∑ k=1
Since this minimization is intractable , we use the following local divergence minimization .
We parameterize q(wd,i , zd,i ) with a
Note that we have q(wd,i = v , zd,i = k ) =
.
( 17 ) q(wd,i = v ) =
\d,i|γ , β)||q(w , z ) ] ( 16 )
\d,i k,v , and c
\d,i k as
\d,i d,k , b \d,i k,v \d,i k n b c
\d,i a d,k \d,i d + γ0
Note that
\d,i , z
\d,i , γ , β ) = p(wd,i = v , zd,i = k|w
\d,i d,k + γk n \d,i d + γ0 n ( 18 ) \d,i ) and q(w , z|b ) to explicitly emphasize that q(wd,i , zd,i ) is parameterized by b
We sometimes use the notations q(wd,i , zd,i|b
\d,i k,v + β n \d,i k + V β
\d,i = {{b
}V v=1 .
}K n k=1
\d,i k,v
We define
\b(wd,i = v , zd,i = k ) = q(wd,i = v , zd,i = k)/b q
\d,i k,v
=
1 \d,i k c n
\d,i a d,k \d,i d + γ0
.
( 19 )
By replacing b
\d,i k,v with n
\d,i k,v + β , we define qb→n(wd,i = v , zd,i = k ) = ( n
\d,i k,v + β)q
\b(wd,i = v , zd,i = k ) . ( 20 ) q(wd,i = v , zd,i = k )
E E q(z
\d;i)[n \d;i)[n
\d,i k,v ] + β \d,i k,· ] + V β q(z
E q(z
\d,i d,k ] + γk
\d;i)[n \d,i n d + γ0
.
( 27 )
= k=1
.
Therefore , we recover Eq ( 6 ) of the CVB0 inference by calculating q(zd,i = k|wd,i = v ) = q(wd,i = v , zd,i = k ) q(wd,i = v )
= qCVB0(zd,i = k ) ( 28 ) in Eq ( 26 ) . 6.3 Connection to LOO Perplexity and Parameter Estimation of Asymmetric Dirichlet Distribution
In this section , we describe the relationship between our novel reformulation of the CVB0 inference and the leave one out likelihood and perplexity . This relationship provides us the parameter estimation method of asymmetric Dirichlet Distribution , Dir(θd|γ ) .
1038 ∏
∏ nd
D d=1
Note that we estimate variational joint distribution q(w , z ) = i=1 q(zd,i|wd,i)q(wd,i ) . The notation q(z|w)q(w ) = q(z|w ) indicates the approximated posterior distribution given w , and q(w ) indicates the approximated marginal likelihood . The marginal likelihood may be referred to as the evidence or data evidence . From Eq ( 27 ) , since q(w ) depends on γ and β , we expressly denote q(w ) as q(w|γ , β ) . Since we want to maximize the data evidence , it is natural to estimate γ and β by
∗
( γ
, β
∗
) = argmax fl,β log q(w|γ , β ) .
( 29 )
It is well known that the maximization of the true marginal likelihood , sometimes called the evidence , of the observed data , p(w|γ , β ) , with respect to γ and β is called the empirical Bayes estimation . Therefore , our approach is a variational approximation of the empirical Bayes estimation . This is typically used for performing model selection . The general idea is that a higher marginal likelihood for a given model indicates a better fit of the data . We now analyze q(w|γ , β ) in more detail . The LOO prediction of wd,i is given by K∑ p(wd,i|w K∑
\d;i,β,fl)[p(wd,i , zd,i = k|w \d,i d,k + γk \d,i n d + γ0
\d,i n k,v + β \d,i k,· + V β
\d,i , β , γ ) ]
\d,i , β , γ )
= k=1
\d;i|w p(z
\d,i , z
\d;i,β,fl )
\d;i|w
]
[
( 30 ) k=1
E
E p(z
= n
.
The leave one out log likelihood estimation for β and γ is given by
∗
( γ
, β
∗
) = argmax log p(wd,i|w
\d,i , β , γ) ] .
( 31 ) n
∑ fl,β d,i
The LOO likelihood cross validation is often used for model selections [ 19 , 20 , 21 , 22 ] . Optimization ( 29 ) can be considered an approximation of the LOO likelihood estimation Eq ( 31 ) . More interestingly , q(w|γ , β ) is related to the LOO perplexity .
We calculate the perplexity for held out test words wtest by
 , 1
N test exp
∑
K∑ wd;i∈wtest log k=1
Eq(z)[nk,wd;i ] + fi Eq(z)[nk,· ] + V fi
Eq(z)[nd,k ] + flk nd + fl0
( 32 ) where N test is the total number of test words . From Eq ( 27 ) , we find that
− 1
N
∑
 exp log q(wd,i ) d,i
( 33 ) indicates the LOO perplexity . This pseudo perplexity can approximately represent the held out perplexity . Therefore , minimizing this pseudo perplexity is expected to lead to a lower perplexity of held out data .
We show the relationship between LOO and Test perplexities for the NIPS corpus in Figs . 1 ( 1 ) and ( 3 ) and NewYork Times corpus in Figs . 1 ( 2 ) and ( 4 ) . The NIPS corpus includes randomly chosen D = 1 , 500 documents with vocabulary size V = 12 , 245 . The NewYork Times corpus includes randomly chosen D = 1 , 500 documents with vocabulary size of V = 44 , 520 . Stop and low frequency words were eliminated . These were created from the corpora of Frank and Asuncion [ 23 ] . We ran the CVB0 inference
Initialize fq(zd,ijwd,i)g , randomly . for iterations 1 ; ; Lb //Burn in period . for i = 1 ; :: : ; nd // We actually select word types . Update q(zd,ijwd,i ) by using Eq ( 28 ) . end for
Algorithm 1 SDM based SCVB0 inference 1 : Randomly initialize bk,v and flk = 0:01 for k = 1 ; ; K . 2 : Set Lb = 5 , which is the number of iterations in a burn in period . 3 : for d = 1 ; ; D //We randomly order documents . 4 : 5 : 6 : 7 : 8 : 9 : 10 : 11 : 12 : 13 : 14 : 15 : 16 : end for end for Update fl0 and by using Eqs . ( 45 ) and ( 47 ) with minibatch . Increment t = t + 1 .
Update bt(v ) Increment t(v ) = t(v ) + 1 . end for for word type v in document d k,v by using Eq ( 39 ) . of LDA with the symmetric Dirichlet prior . The number of iterations was 100 . We randomly split the words in each document into training words ( 80 % ) and test words ( 20% ) . Test perplexity was calculated using 20 % of the test words , and LOO perplexity was calculated using 80 % of the training words ( lower perplexity is better ) . We tried several hyperparameters ( K ∈ {50 , 100 , 150} , γk ∈ {0.01 , 0.05 , 0.1 , 0.2 , 0.5} , and β ∈ {0.01 , 0.05 , 0.1 , 0.2 , 0.5} ) and five random initializations . Each point in Figs . 1 ( 1 ) and ( 2 ) indicates performance with a certain hyperparameter setting . Figures 1 ( 3 ) and ( 4 ) show the results of the best LOO perplexity for all hyperparameter setting in terms of the number of iterations . These results suggested that the CVB0 inference minimizes the LOO perplexity , and the LOO perplexity highly correlates with the test perplexity . Therefore , we can use the LOO perplexity as a criterion of model selection .
7 . STOCHASTIC DIVERGENCE MINI
MIZATION
∑
We proposed a stochastic divergence minimization ( SDM ) framework for deriving the SCVB0 inference . In the divergence \d,i minimization , a d,k can be optimized in a document and c can \d,i \d,i be obtained as c k,v , which means that k = \d,i \d,i(new ) we can update c . Therefore , we k = c k,v focus on the SDM for b 7.1 Stochastic Approximation of Divergence
\d,i k,v if we have b − b + b v b \d,i k \d,i k,v .
\d,i(old ) k,v
\d,i k
Minimization for b
\d,i k,v
We start with stochastically approximating the calculation of
 ;
\d,i k,v = E[n
\d,i k,v ] + β , b
( 34 ) where wd,i = v . This calculation can be rewritten by the fixed point iteration \d,i(t+1 ) k,v
\d,i(t ) k,v + ρt[E[n b
\d,i k,v ] + β ] k,v ] + β − b \d,i(t ) \d,i k,v
]
( 35 ) q(zd′,i′ = k|wd′,i′ = v)δ(wd′,i′ = v ) . ( 36 )
\d,i(t ) k,v + ρt[E[n
= ( 1 − ρt)b = b ∑ where ρt is a stepsize .
We also rewrite E[n
\d,i k,v ] = d′,i′̸=d,i
1039 ( 1 )
( 3 )
( 2 )
( 4 )
Figure 1 : Relationship between LOO perplexity and test perplexity .
We replace these statistics with
\d,i v
\d,i k,v = n en ) ̸= ( d , i ) is a random sample with wd′,i′ = v .
= ( nv − 1)q(zd′,i′ = k|wd′,i′ = v ) , q(zd′,i′ = k|wd′,i′ = v )
′
( 37 )
′ where ( d
, i
Therefore , we approximate Eq ( 35 ) by k,v + β − b \d,i k,v + ρt[en
\d,i(t+1 ) k,v
\d,i(t )
= b b
\d,i(t ) k,v
] .
( 38 )
It is worth noting that we do not iterate update Eq ( 38 ) for each wd,i many times . Update Eq ( 38 ) is actually a word type wise update . Moreover , we process a document in an online situation . Thus , we can use a previously processed word for q(zd′,i′|wd′,i′ ) and b indicates the latest update value when word v appeared in the previously processed document . That is , update Eq ( 38 ) can be reformulated as follows .
\d,i(t ) k,v
When we process word wd′,i′ = v , we can update bk,v for the next word with wd,i = v as follows . k,v + ρt(v)[en b(t(v)+1 ) k,v
= b(t(v ) ) k,v + β − b(t(v ) ) \d,i k,v
] ,
( 39 )
= b where t(v ) is an update count for word v , ie , it indicates how many times word v previously appeared until the next word wd,i = v appears . That is , b(t(v)+1 ) , which is a stochastic approximation of E[n
\d,i(t+1 ) k,v k,v \d,i k,v ] + β .
Note that the divergence minimization is not a document wise optimization but a word wise optimization , ie , a coordinate wise \d,i k,v after processing a document optimization . That is , we update b but update it only if word v appears in the document . Therefore , we do not need a minibatch operation . Moreover , by using bt(v ) k,v calculated with the previously processed documents , we can parallel update q(zd,i|wd,i = v ) in multiple documents in a multi core setting .
The remaining problem is to show that update Eq ( 38 ) minimizes the divergence in Eq ( 21 ) . This problem is not obviq(zd′,i′ = k|wd′,i′ = v ) + β − b \d,i \d,i ous because n k,v , even v k,v ] + β − b \d,i \d,i E[n k,v , is not a gradient of the divergence in Eq ( 21 ) . We present the theoretical analysis of update Eq ( 38 ) in terms of Martingale convergence theory in Sec 8 . 7.2 Stochastic Approximation of Divergence
Minimization for a
\d,i k,v
∑ ∑
Although this update should be performed for each distinct token in each document , we actually perform this update for each word type in each document by approximating E[nd,k ] = \d,i d,k ] =
E[nd,k,v ] = nd,vq(zd,i = k|wd,i = v ) and E[n E[n d,k,v ] = ( nd,v − 1)q(zd,i = k|wd,i = v ) . \d,i
V v=1 V v=1
7.3 Stochastic Optimization for Estimating γ We proposed the stochastic approximation of Optimization ( 29 ) . In this section , we focus on estimating γ . We explain the setting of β in Sec 91 This is because β is scalar and easy to set an effective value from the result of Asuncion et al . [ 7 ] .
From LOO perplexity ( 33 ) , we estimate γ by
∑ d,i
∗
γ
= argmax fl
1 N log q(wd,i|γ ) ,
( 40 ) where N is the total number of words . This formulation is useful because the optimal solution is the same as Eq ( 29 ) and the scale of the gradient of the objective function does not depend on N . We can use the stochastic gradient method by
γ(t+1 ) = γ(t ) + ηt∂fl log q(wd,i = v|γ ) ,
( 41 )
1040 q(zd,i = k|wd,i = v ) \d,i E d,k ] + γk
\d;i)[n q(z
−
1 \d,i d + n where ∂γk log q(wd,i = v|γ ) = k−1∏
In this work , we use the construction of γ given by ( 1 − πl ) ( k = 1 , . . . , K ) , γk = γ0πk 0 ≤ πk ≤ 1 ( k = 1 , . . . , K − 1 ) , πK = 1 . l=1
∑ k γk ( 42 )
( 43 )
( 44 )
This is called a truncated stick breaking process ( TSBP ) construction .
Ishwaran and James [ 24 ] give a bound for the error introduced by truncating the SBP . They argued that truncating the number of mixture components to a moderate level is sufficient to successfully approximate the SBP . They give a bound on the truncation error in the order of exp{−(K − 1)/γ0} . Therefore , we expect that we can estimate the effective number of topics K + if we set K > K + .
The modeling by the asymmetric Dirichlet distribution is useful for robustness against stop words . Wallach et al . [ 12 ] found that the asymmetric γ has substantial advantages over the symmetric one which means fixed γk = γ0/K . The asymmetric γ can learn meaningful topics even when no stop words are removed .
We can use the stochastic gradient method for estimating γ given by
0 + ηt∂γ0 log q(wd,i = v|γ ) ,
= γ(t )
γ(t+1 ) 0 ∂γ0 log q(wd,i = v|γ ) =
∂γk log q(wd,i = v|γ )
γk γ0
K∑ k=1 k + ηt∂πk log q(wd,i = v|γ ) ,
= π(t )
π(t+1 ) k ∂πk log q(wd,i = v|γ ) =
γk πk
∂γk log q(wd,i = v|γ ) k−1∑
γk
πk′ ∂γk′ log q(wd,i = v|γ ) .
+ k′=1
( 48 ) This approach is similar to that of Liang et al . [ 25 ] , in which they use the point estimation for the TSBP . The main difference with their approach and ours is that they use the VB inference ; thus , they can use the variational lower bound as an objective function , ie , they can use the gradient method to maximize the variational lower bound in terms of γ . However , we cannot use this approach because the CVB0 inference is not directly related to the variational lower bound .
8 . THEORETICAL ANALYSIS
We analyze the convergence of the stochastic approximation of the divergence minimization introduced in Sec 71
For mathematical convenience , we denote yd,i complete data , ie , yd,i = ( wd,i , zd,i ) , q(yd,i ) = q(wd,i , zd,i ) . Again , we sometimes use the notation q(yd,i|b q(wd,i , zd,i|b rameterized by b
( 49 ) \d,i ) = \d,i ) to explicitly emphasize that q(wd,i , zd,i ) is pa
}V v=1 . k=1
\d,i = {{b
}K Also , for simplicity , we define , \d,i(t ) ) = D1[qb→n(yd,i)q(z D1(b
\d,i k,v
\d,i|w
\d,i)pemp(w
\d,i)||q(y) ] . ( 50 )
That is , Optimization ( 21 ) is formulated as min \d;i b We have the convergence results given by
D1(b
\d,i ) .
THEOREM 81 When stepsize ρt is a decreasing positive number and satisfies
∞∑
ρt = ∞ ,
∞∑ ( ρt)2 < ∞ , t=1 t=1 update Eq ( 38 ) ( or Eq ( 39) ) ) satisfies the following hold with probability one : ( 1 ) The sequence D1(b ( 2 ) We have
\d,i(t ) ) converges .
∇D1(b
\d,i(t ) ) = 0 . lim t→∞
( 3 ) Every limit point of b
D1(b
\d,i ) .
\d,i(t ) is a stationary point of
PROOF . The proof is provided in Appendix1 .
9 . EXPERIMENT
In this section , we compared the proposed algorithm with the SCVB0 inference proposed by Foulds et al . , [ 9 ] and the SVB inference by Hoffman et al . [ 11 ] . We also used the Bayesian optimization framework [ 26 , 27 ] for tuning hyperparamters of the original SCVB0 and SVB inferences . 9.1 Experimental Setup
( 45 )
( 46 )
( 47 )
We explain experimental settings for the SVB , original and
SDM based SCVB0 inference .
Setup of SVB :
SVB denotes the SVB inference proposed by Hoffman et al . [ 11 ] . Snoek et al . [ 28 ] used Bayesian optimization for tuning the step size of SVB , which we call SVB BO . We used their framework as follows . We defined the form of the step size as ρt = a/(b + t)κ for SVB of the variational distribution over ϕk , ie , q(ϕk ) , and the stochastic gradient method of γ , with which we formulate γ as the stickbreaking construction and the objective function is the variational lower bound . We assumed that a = 1 , b ∈ {1 , 10 , 100 , 1000} , and κ ∈ {0.51 , 0.6 , 0.7 , 0.8 , 0.9 , 1} for the SVB inference of q(ϕk ) , and a ∈ {0.0001 , 0.001 , 0.01 , 0.1 , 1} , b ∈ {1 , 10 , 100 , 1000} , and κ ∈ {0.51 , 0.6 , 0.7 , 0.8 , 0.9 , 1} for the stochastic gradient method of γ . We also tuned β ∈ {0.01 , 0.05 , 0.1 , 0.2 , 0.5} and the initialization of γk ∈ {0.01 , 0.05 , 0.1 , 0.2 , 05} t = 1/(10 + t)0.9 and ρϕ
Setup of SCVB0 : SCVB0 denotes the original SCVB0 inference proposed by Foulds et al . [ 9 ] that uses the setting in which step sizes were ρθ t = 10/(1000 + t)0.9 , and the minibatch size is 100 documents . We used Bayesian optimization for tuning the hyperparamters of SCVB0 , denoted as SCVB0 BO . We assumed that the form of the step size is ρt = a/(b + t)κ , a = 1 , b ∈ {1 , 10 , 100 , 1000} , t , and a = 1 , b ∈ and κ ∈ {0.51 , 0.6 , 0.7 , 0.8 , 0.9 , 1} for ρθ {1 , 10 , 100 , 1000} , and κ ∈ {0.51 , 0.6 , 0.7 , 0.8 , 0.9 , 1} for ρϕ t . We also tuned symmetric γk ∈ {0.01 , 0.05 , 0.1 , 0.2 , 0.5} and β ∈ {0.01 , 0.05 , 0.1 , 0.2 , 05} 1Longer version : http://wwwrdlitcu tokyoac jp/~sato/SIGKDD2015_scvb0_long.pdf
1041 Table 2 : Dataset Information .
Dataset DBLP
612,080 Wikipedia1M 1,000,000 Pubmed1M 1,000,000 Pubmed5M 5,000,000
# of Docs . Vocab . size 19,466 130,843 50,161 122,335
# of Tokens 44,007,273 43,921,563 83,592,356 431,244,050
Table 3 : SCVB0 BO Hyperparamters .
Dataset DBLP
γk 0.01 Wikipedia1M 0.01 Pubmed1M 0.01
β 0.05 0.01 0.01
ρθ t
ρϕ t
1/(1 + t)0.7 1/(1 + t)0.6 1/(1 + t)0.7
1/(1 + t)0.7 1/(1 + t)0.7 1/(1 + t)0.6
Setup of SDM : SDM S and SDM denote the proposed SDMbased SCVB0 inference with symmetric and asymmetric Dirichlet priors . We used simple forms of stepsize ρt(v ) = 1/(1 + t(v))0.51 for Eq ( 39 ) , ηt = 1/(1 + t)0.51 for Eqs . ( 45 ) and ( 47 ) . For SDMS , we fixed γk = 0.01 and β = 001 For SDM , we conducted the estimation of γ after processing 1,000 documents because the estimation of the first set of documents was too noisy . The initialization of γk was 0.01 and we set β = 0.01 from the result of the CVB0 inference given by Asuncion et al . [ 7 ] . Note that we did not use Bayesian optimization for SDM and we show that these simple settings of SDM outperformed fully tuned SCVBO BO and SVBBO .
Common Setup : We used the minibatch size of 1000 documents for SVB BO , SCVB0 BO , and SDM because the computational complexity of the minibatch of the SVB and SCVB0 inferences is O(KV ) and minibatch size 100 induces a slow running time . The number of iterations in the burn in period was Lb = 5 . We set the number of topics to K = 1000 . 9.2 Datasets and Evaluation Metric
We used four datasets :
“ Pubmed corpus ( Pubmed1M and Pubmed5M ) , ” “ Wikipedia corpus ( Wikipedia ) ” , and “ DBLP abstract corpus ( DBLP ) . ” The Pubmed corpus was created by Frank and Asuncion [ 23 ] . We randomly chose one million ( 1M ) documents for Pubmed1M and 5M for Pubmed5M .
We chose documents that include more than 50 words . Stop and low frequency words were eliminated . In the Wikipedia corpus , we used the first 300 words for each document and randomly chose 1M English documents because we need a large amount of time for SVB BO and SCVB0 BO . The basic information of these datasets is summarized in Table 2 .
We used test set perplexity as the evaluation metric to compare performances . We calculated the perplexity as follows . For each dataset , we used 2000 documents as a test set Dtest . For testing , we split words into two parts , wd = ( wd,1 ; wd,2 ) ∈ Dtest . We used the first part wd,1 ( 80 % of the words ) to estimate the topic statistics of document d and computed the perplexity of the second part wd,2 ( 20 % of the words ) conditioned on the first part and the training data . 9.3 Results
We first analyzed the effect of Bayesian optimization for hyperparameter tuning , Figure 2 shows the perplexities with respect to the number of trials of Bayesian optimization in the DBLP , Wikipedia , Pubmed1M datasets , respectively . For speeding up
Table 4 : Averaged CPU Time [ second ] .
Inference DBLP Wikip.1M Pubm.1M Pubm.5M 58315.8 SCVB0 SDM 56384.4
9444.2 9173.0
4251.0 4432.8
7233.8 6769.4
Bayesian optimization , we used only one random initialization , ie , one seed , for q(zd,i|wd,i ) in the hyperparameter tuning step . We did not use Pubmed5M for Bayesian optimization because we need a large amount of time for SVB BO and SCVB0 BO .
As shown in Fig 2 , SCVB0 BO outperformed SCVB0 tuned by experts [ 9 ] . Interestingly , SVB BO also outperformed SCVB0 . This means that we need to tune hyperparameters when we compare algorithms and Bayesian optimization is useful . Since SCVB0 BO outperformed SVB BO , SCVB0 is considered to be a better inference even where we tune hyperparameters .
Figures 3 shows the perplexities with respect to the proportion of the number of training documents in the DBLP , Wikipedia , Pubmed1M and Pubmed5M datasets , respectively . We show the results of SCVB0 BO and SDM because we found that SCVB0BO outperformed SCVB0 and SVB BO . All results are averaged values with error bars from five experimental runs , ie , five seeds , with random initialization for q(zd,i|wd,i ) . We applied hyperparameters tuned with Pubmed1M to SCVB0 BO in Pubmed5M .
SDM outperformed SCVB0 BO for each dataset and the perplexity of SDM was much lower than that of SCVB0 BO . For example , as shown in Fig 3 ( a ) DBLP , even SDM with 10 % training documents was competitive with SCVB0 BO with 100 % training documents . In Pubmed datasets shown in Fig 3 ( c ) and ( d ) , SDM with 30 % training documents was competitive with SCVB0 BO with 100 % training documents . In Wikipedia dataset shown in Fig 3 ( b ) , we needed 60 % training documents for SDM to outperform SCVB0 BO with 100 % training documents .
These results mean that SDM more accurately approximates the CVB0 inference . Moreover , since we use a simple hyperparameter setting for SDM , it is more easy to use SDM than SCVB0s . We may improve the results of SDM by using Bayesian optimization but it is important that SDM has better results even if we do not use hyperparameter tuning .
Table 4 shows that the running time of SDM is faster than that of SCVB0 BO excluding the DBLP corpus . Since the vocabulary size of DBLP is smaller than those of over datasets , the computational complexity of a minibatch was not matter . Note that we took additional processing time for estimating γ in SDM . Table 4 indicates that SDM showed an advantage in terms of computational cost when the vocabulary size was large , such as Wikipedia corpus .
10 . CONCLUSION
We proposed a stochastic approximation of the CVB0 inference for LDA by using the stochastic divergence minimization . We analyzed the proposed SCVB0 inference in terms of Martingale convergence theory . Its predictive performance is better than that of the existing SCVB0 inference . We also reveal the property of the CVB0 inference in terms of the leave one out likelihood or perplexity , which leads to the estimation algorithm of the Dirichlet distribution parameters . For future work , we will apply recent advances in the variational inference to our approximated CVB0 inference such as the sparse update combined with the sampling method proposed by [ 29 , 30 , 31 ] and the split merge update proposed by [ 32 ] .
1042 ( a ) DBLP
( b ) Wikipedia1M
Figure 2 : Experimental results . ( a ) , ( b ) and ( c ) show test set perplexity with respect to the number of Bayesian optimization ( BO ) trials where each perplexity is minimum value in past trials .
( c ) Pubmed1M
( a ) DBLP
( b ) Wikipedia
( c ) Pubmed1M
( d ) Pubmed5M
Figure 3 : Experimental results . ( a ) , ( b ) ( c ) , and ( c ) show the relationships between test set perplexity and the number of documents . Table 3 lists the parameter settings that achieved the best performance of SCVB0 BO .
1043 [ 19 ] M . Stone . Cross validatory Choice and Assessment of
Statistical Predictions . In Journal of the Royal Statistical Society , Series B , pages 111–147 .
[ 20 ] R . P . W . Duin . On the Choice of Smoothing Parameters for
Parzen Estimators of Probability Density Functions . In IEEE Transactions on Computers , pages 1175–1179 . 1976 .
[ 21 ] C . Stone . An Asymptotically Optimal Window Selection
Rule for Kernel Density Estimates . In Annals of Statistics , pages 1285–1297 .
[ 22 ] B . Silverman . A Fast and Efficient Cross validation Method for Smoothing Parameter Choice in Spline Regression . In Journal of the American Statistical Association , pages 584–589 .
[ 23 ] A . Frank and A . Asuncion . UCI Machine Learning
Repository , 2010 .
[ 24 ] H . Ishwaran and L . F . James . Gibbs Sampling Methods for Stick Breaking Priors . Journal of the American Statistical Association , 96(453):161–173 , 2001 .
[ 25 ] P . Liang , S . Petrov , M . Jordan , and D . Klein . The Infinite
PCFG Using Hierarchical Dirichlet Processes . In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ( EMNLP CoNLL ) , pages 688–697 , 2007 .
[ 26 ] H . J . Kushner . A new method of locating the maximum point of an arbitrary multipeak curve in the presence of noise . Journal of Fluids Engineering , 86(1):97–106 , 1964 .
[ 27 ] J . Moˇckus , V . Tiesis , and A . Zilinskas . The Application of
Bayesian Methods for Seeking the Extremum . Towards Global Optimization , 2:117–129 , 1978 .
[ 28 ] J . Snoek , H . Larochelle , and R . P . Adams . Practical Bayesian optimization of machine learning algorithms . In Neural Information Processing Systems , pages 2951–2959 , 2012 . [ 29 ] I . Porteous , D . Newman , A . Ihler , A . Asuncion , P . Smyth , and M . Welling . Fast collapsed gibbs sampling for latent dirichlet allocation . In Proceeding of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining , KDD ’08 , pages 569–577 , 2008 .
[ 30 ] L . Yao , D . Mimno , and A . McCallum . Efficient methods for topic model inference on streaming document collections . In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining , KDD ’09 , pages 937–946 , 2009 .
[ 31 ] D . M . Mimno , M . D . Hoffman , and D . M . Blei . Sparse stochastic inference for latent Dirichlet allocation . In Proceedings of the 29th International Conference on Machine Learning , 2012 .
[ 32 ] M . Bryant and E . Sudderth . Truly Nonparametric Online
Variational Inference for Hierarchical Dirichlet Processes . In Advances in Neural Information Processing Systems 25 , pages 2708–2716 . 2012 .
11 . REFERENCES
[ 1 ] D . M . Blei , A . Y . Ng , and M . I . Jordan . Latent Dirichlet
Allocation . Journal of Machine Learning Research , 3:993–1022 , 2003 .
[ 2 ] T . L . Griffiths and M . Steyvers . Finding scientific topics .
Proc Natl Acad Sci U S A , 101 Suppl 1:5228–5235 , 2004 .
[ 3 ] Y . W . Teh , D . Newman , and M . Welling . A Collapsed Variational Bayesian Inference Algorithm for Latent Dirichlet Allocation . In Advances in Neural Information Processing Systems 19 : 20th Annual Conference on Neural Information Processing Systems 2007 , 2007 .
[ 4 ] Y . W . Teh , K . Kurihara , and M . Welling . Collapsed
Variational Inference for HDP . In Advances in Neural Information Processing Systems 20 : 21th Annual Conference on Neural Information Processing Systems 2008 , 2008 .
[ 5 ] Y . W . Teh , M . I . Jordan , M . J . Beal , and D . M . Blei .
Hierarchical Dirichlet Processes . Journal of the American Statistical Association , 101(476):1566–1581 , 2006 .
[ 6 ] J . Sung , Z . Ghahramani , and S Y Bang . Latent Space
Variational Bayes . IEEE Transactions on Pattern Analysis and Machine Intelligence , 30:2236–2242 , 2008 .
[ 7 ] A . Asuncion , M . Welling , P . Smyth , and Y . W . Teh . On
Smoothing and Inference for Topic Models . In Proceedings of the International Conference on Uncertainty in Artificial Intelligence , 2009 .
[ 8 ] I . Sato , K . Kurihara , and H . Nakagawa . Practical collapsed variational bayes inference for hierarchical dirichlet process . In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining . ACM , 2012 .
[ 9 ] J . Foulds , L . Boyles , C . Dubois , P . Smyth , and M . Welling .
Stochastic Collapsed Variational Bayesian Inference for Latent Dirichlet Allocation . In Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining . ACM , 2013 .
[ 10 ] H.Robbins and SMonro A stochastic approximation method . In Annals of Mathematical Statistics , pages 400–407 , 1951 .
[ 11 ] M . D . Hoffman , D . M . Blei , and F . R . Bach . Online Learning for Latent Dirichlet Allocation . In Advances in Neural Information Processing Systems 23 : 24th Annual Conference on Neural Information Processing Systems 2010 , pages 856–864 , 2010 .
[ 12 ] H . Wallach , D . Mimno , and A . McCallum . Rethinking LDA :
Why Priors Matter . In Advances in Neural Information Processing Systems 22 , pages 1973–1981 . 2009 .
[ 13 ] I . Sato and H . Nakagawa . Rethinking Collapsed Variational
Bayes Inference for LDA . In Proceedings of the 29th International Conference on Machine Learning , 2012 . [ 14 ] T . Minka . Divergence Measures and Message Passing .
Technical report , Microsoft Research , 2005 .
[ 15 ] A . Asuncion . Approximate Mean Field for Dirichlet Based
Models . In Topic Models Workshop , ICML . 2010 .
[ 16 ] S . Amari . Differential Geometrical Methods in Statistic .
Springer , New York , 1985 .
[ 17 ] M . Trottini and F . Spezzaferri . A generalized predictive criterion for model selection . In Canadian Journal of Statisticse , 2002 .
[ 18 ] H . Zhu and R . Rohwer . Information Geometric
Measurements of Generalisation . Technical report , Aston University , 1995 .
1044
