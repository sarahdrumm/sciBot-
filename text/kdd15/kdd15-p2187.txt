Client Clustering for Hiring Modeling in Work Marketplaces
Vasilis Verroios∗ Stanford University verroios@stanford.edu
Ramesh Johari∗ Stanford University rameshjohari@stanfordedu
ABSTRACT An important problem that online work marketplaces face is grouping clients into clusters , so that in each cluster clients are similar with respect to their hiring criteria . Such a separation allows the marketplace to “ learn ” more accurately the hiring criteria in each cluster and recommend the right contractor to each client , for a successful collaboration . We propose a Maximum Likelihood definition of the “ optimal ” client clustering along with an efficient Expectation Maximization clustering algorithm that can be applied in large marketplaces . Our results on the job hirings at oDesk over a sevenmonth period show that our client clustering approach yields significant gains compared to “ learning ” the same hiring criteria for all clients . In addition , we analyze the clustering results to find interesting differences between the hiring criteria in the different groups of clients .
1 .
INTRODUCTION
Online work marketplaces such as oDesk.com , Elance.com and Freelancer.com help “ clients ” and “ contractors ” across the globe to connect with each other and work for more than $1 billion in annual contractor earnings just in 2014 . Typically , in such marketplaces , contractors apply to a job posted by a client and clients hire the applicant(s ) that seems to be the best fit for the job posted . As these platforms grow , a fundamental problem they have to solve is the understanding of successful client hiring practices so that they can help clients make the right hiring decisions . Without such help , clients will have to deal with the friction of screening tens to hundreds of contractors to determine the ideal candidates for their jobs . The screening process is not only timeconsuming , but it is also error prone , since clients often lack the necessary knowledge to assess the qualifications of contractors ( eg , education and work experience from schools and companies that are unknown to a client ) .
Understanding and modeling the hiring behavior of clients is challenging not only due to the variety of jobs that are ∗Work done while authors were at Elance oDesk
Panagiotis Papadimitriou
Elance oDesk papadimitriou@elance odesk.com
Hector Garcia Molina
Stanford University hector@csstanfordedu
Figure 1 : Left : The hiring decisions of all of the marketplace clients in a two dimensional feature space . Middle : The hiring decisions of quality optimizers . Right : The hiring decisions of cost optimizers . posted and the diversity of contractors , but also due to the heterogeneity of client hiring criteria . For example , two different clients that have posted two seemingly similar jobs looking for “ php developers ” may be looking for totally different people . Say that the first client is a quality optimizer that is willing to pay a high hourly rate to get the most qualified contractor while the second client is a cost optimizer who is willing to take the risk of working with an inexperienced contractor to reduce his costs . To make recommendations that satisfy both of these clients we would ideally develop a dedicated model for each client . However , in practice , developing a model for each client is not an option , since the marketplace rarely has sufficient data points for a single client to make training possible .
Although all clients are not the same , there are usually sufficiently large groups of clients with similar hiring criteria that can provide us with data for model training . To illustrate our hypothesis , we show in Figure 1 the hiring decisions in a marketplace that is composed of quality and cost optimizers . In the first plot of the figure we illustrate the hiring decisions of all clients in a two dimensional feature space . Each “ + ” point looks at an application that ended up in a hire while a “ − ” point looks at an application that got rejected . The point positions on the X axis indicate the bid prices asked by the contractors and the positions on the Y axis indicate the years of contractor experience . A linear model trained on all of the client decisions would “ learn ” a linear separator w0 to distinguish hired from rejected applications . Such a separator would misclassify many rejected applications as hires ( “ ” points to the left of w0 ) and many hires as rejected applications ( “ + ” points to the right of w0 ) . However , if we could split the clients into quality ( middle plot ) and cost optimizers ( right plot ) we could then learn a different model for each client group . The derived separators w1 and w2 would then almost perfectly separate the hires in each of the two groups .
( cid:85)(cid:76)(cid:72)(cid:72)(cid:85)(cid:76)(cid:72)(cid:72)ffiffiffiffiffiffiffiffiffiffiffiffiffi(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)ffi(cid:66)ffiffiffiffi(cid:72)(cid:85)(cid:76)(cid:72)(cid:72)(cid:85)(cid:76)(cid:72)(cid:85)(cid:76)(cid:72)(cid:72)(cid:85)(cid:76)(cid:72)(cid:72)ffiffiffiffiffiffiffiffiffiffiffiffiffiffi(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)ffiffiffiffiffiffiffiffiffiffiffiffiffi(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)ffi(cid:66)ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)2187 31 ] , fit well in domains like electronic commerce or streaming media recommendations , but have an inherent limitation for online job marketplaces : each contractor cannot be considered as a “ fixed ” item as with movies or products . For instance , a contractor may be much more appropriate for a task involving Fortran debugging than Python debugging if she is a Fortran expert but a Python novice . That is , clients do not really “ vote ” for a contractor when they hire her but “ vote ” for her application on a specific task .
Since the quality of the client clustering drastically affects the percentage of successful collaborations , we had to develop a new approach tailored to our context . The approach we developed is driven by our findings in papers [ 4 ] and [ 18 ] . In summary , in those two papers we studied the problem of learning how to rank the applicants on a specific task posted by a client . Our main finding was that a simple sparse logistic regression model , using the accepted applications as positive examples and the rejected ones as negatives examples , proves to be very effective for applicant ranking . Hence , the approach we propose in this paper for the hiringcriteria clustering problem , builds on top of a logistic regression model : we develop a Finite Mixture Logit model and a simple , yet effective and scalable algorithm based on Expectation Maximization with hard assignments ( hard EM ) . In simple words , the algorithm starts with a random assignment of clients into clusters and for each cluster it applies sparse logistic regression to learn the hyperplane expressing the client criteria in this cluster . It then re assigns a client to the hyperplane/cluster that best “ explains ” her choices . These two steps are repeated until convergence , ie , until no re assignments are needed . After the last step , the algorithm has computed a partition of clients into clusters along with the criteria of clients in each cluster ; given by sparse logistic regression .
Although our model bears some resemblance to models proposed in the past ( eg , mixture models proposed for clusterwise regression [ 8 , 27 ] , mixtures of Support Vector Machines [ 3 , 9 , 32 ] or models found in the marketing and economics literature [ 1 , 15 , 20 , 23 , 30] ) , there are some important differences that do not allow previous models to be applied in the context discussed here :
1 . Most previous models ( eg , [ 3 , 6 , 8 , 9 , 27 , 32 ] ) apply clustering directly on the samples of the dataset . On the contrary , our clustering takes place in a different level : we cluster the clients that produced the samples . 2 . As previous models were designed to solve different problems , they do not capture important aspects of the problem discussed in this paper . ( For example , in most of the models in the marketing and economics literature , a client ’s decision refers to the selection of a product or brand from a set of “ fixed ” alternatives , while in our case a client ’s decision refers to a “ unique ” application . ) Moreover , most of those models come with unnecessary complexity and in large scale platforms like oDesk , any unnecessary model or algorithmic complexity incurs a critical cost in terms of computational overhead and software maintenance .
In our experiments , we use the 865 , 000 accept/reject decisions made by oDesk clients in a seven month period . As our results show , the predictive hiring models trained on the clusters yielded by our algorithm can improve the performance of a global model ( ie , a single hiring model for
Figure 2 : The Hiring Criteria Clustering component in the overall oDesk recommendations workflow .
Clustering clients based on their hiring criteria is a critical component in our oDesk recommendations workflow . Figure 2 depicts the overall workflow and shows how the clustering component fits in the workflow . The clustering component generates groups of clients along with one hiring model per group . Our first objective is to predict more accurately who is the right contractor for a client ’s task , based on the hiring practices of that client . Nevertheless , predicting accurately the contractor that a client will hire is not our only objective . We observed that very often unsuccessful collaborations take place because of clients relying on the “ wrong ” hiring practices . Therefore , we have to monitor the collaborations in each group of clients and “ intervene ” when we detect that a group ’s hiring practices often lead to unsuccessful collaborations . We “ intervene ” by adjusting the group ’s hiring model based on the “ problematic ” criteria that we detected . For example , we may adjust a hiring model when we detect that clients are biased against working with contractors from specific countries , while we “ know ” that those countries provide a large pool of experts for the tasks posted by those clients .
The monitoring and re adjustment components typically require both algorithmic models and manual effort . Hence , it is important to use human interpretable hiring models in our workflow ( see also paper [ 18] ) . Furthermore , our workflow also involves a component that assigns new clients without any hiring history to the right group/model . ( Note that a positive experience for new clients is particularly important for the marketplace . )
In this paper , we choose to focus on the hiring criteria clustering component , which forms the basis for the overall workflow and is the most interesting component both from an applied and a research perspective . Below we discuss two classic approaches for client clustering and point out why they are not suitable for our context .
Traditional Clustering Methods : One straightforward approach for client clustering is to apply an algorithm like k means using client attributes like age or occupation . However , as we observed in the oDesk platform , clients that have the exact same characteristics ( age , occupation , etc. ) , often have very different criteria on hiring contractors . Clearly , a clustering method suitable for our problem must be based on the clients’ decisions and not just attributes like age and occupation . Of course , we can also include client attributes related to their past decisions . However , we decided not to rely on a “ custom ” approach , where we would have to come up with past decisions’ attributes and a distance metric to represent the distance between clients . ( Instead , we rely on a bayesian approach , as we discuss later in the introduction . )
Collaborative Filtering ( CF ) Methods : CF techniques , like the ones presented in papers [ 2 , 5 , 7 , 10 , 11 , 19 , 24 ,
ClientsHiringCriteriaClusteringClient Group 1ModelClient Group NModelMonitor CollaborationsRe adjust ModelsAssign to ModelNewClients2188 all clients ) by 43 % . Furthermore , our analysis on the clusters produced by our method reveals some very interesting differences on the hiring practices of different client groups . To the best of our knowledge , our study is the first one on the detection and analysis of the differences between hiring practices in online work marketplaces .
Section 3 . hiring criteria clustering problem , in Section 2 .
In summary , our contributions are the following : • We provide a Maximum Likelihood formulation of the • We present a hard EM algorithm for this problem , in • We present our model for oDesk job applications , in Section 4.2 , and we study the impact of client clustering for different sizes of training sets and sets of features , in Section 43 • We study the hiring criteria in different groups of clients to discover some very interesting differences on how clients choose the contractor they work with , in Section 44
1.1 Running Example
We will use a running example throughout the definition of our model and algorithm in Sections 2 and 3 .
For simplicity , let us assume that there are only three features , experience , score , bidding , affecting a client ’s decision and that all information for the contractors’ applications are organized in a single table :
Apps(clientID , experience , score , bidding , decision ) The semantics of the 5 columns are the following : 1 . clientID : The client that opened the task to which the current application refers .
2 . experience : The total number of hours the contractor(applicant ) has worked in the platform , ie , the total number of hours in the contractor ’s past contracts with clients .
3 . score : The aggregated rating of the contractor based on the past reviews from the clients she worked for .
4 . bidding : The amount asked by the contractor for per forming the task .
5 . decision : The decision of the client ( “ APPROVED ” /
“ REJECTED ” ) on this application .
In practice , the three features , experience , score , bidding , are normalized so that their value range is [ 0.0 , 10 ] For example , the bidding can be normalized by the maximum amount a contractor may ask for a task ; a restriction that could be enforced by the platform ’s provided functionality . In our example , the dataset consists of only four clients , each having ten contractor applications approved and ten contractor applications rejected . In addition , we want to form two clusters , ie , we want to split the four clients into two groups such that the clients in each group have “ very similar ” criteria regarding the experience , score , and bidding of an application . Our model in the next section quantifies the notion of “ similar ” by defining the optimal clustering .
2 . MODEL
In this section , we formally define the problem of finding the optimal client partition based on the clients’ hiring criteria . Intuitively , our definition requires that the reject/accept applications in each cluster of clients are as
“ well separated ” as possible . For example , the reject/accept applications ( “ − ” s/ “ + ” s ) in the middle and right side of Figure 1 are “ well separated ” ; a different partition of clients into two clusters could result in having “ + ” s diffuse over the “ − ” s area , and the opposite . We use a logit model to quantify how “ well separated ” the applications in one cluster are . Based on the cost defined by the logit model , the optimal partition of clients is the one minimizing the cost across all clusters . We start by describing the dataset notation and the cost for a single cluster and then define the clustering optimization problem ( equations ( 11 ) to ( 13) ) . 2.1 Dataset Notation
All the past applications are stored in a single table : Apps(clientID , a1 , a2 , . . . , aF , decision ) The features describing each application are denoted by a1 , a2 , . . . , aF and they are normalized so that their value range is [ 0.0 , 10 ] In our running example , we use only three features : a1 ≡ experience , a2 ≡ score , a3 ≡ bidding .
We denote by xi the row i in table Apps ( Apps[i] ) , projected over columns a1 , a2 , . . . , aF . Thus , in our running example , each xi is a 3 dimensional vector , eg , if an application involves an experience of 0.2 , a score of 0.9 , and a bidding of 0.8 , then xi = ( 0.2 , 0.9 , 0.8)T .
In addition to xi , we use ui to express the clientID of row i ( Apps[i].clientID ) , in a 1 of K scheme . In our running example , where we have K = 4 clients , if the second client approved/rejected application i , then ui = ( 0 , 1 , 0 , 0)T . subsets {P,N} , such that :
To simplify notation we split past applications into two
P = {(xi , ui ) | Apps[i].decision = APPROVED} ( 1 ) N = {(xi , ui ) | Apps[i].decision = REJECTED} ( 2 ) In the running example , |P| = 40 since each of the four clients has approved ten contractor applications , and |N| = 40 since each of the four clients has rejected ten applications .
Moreover , we denote with : • K : the number of clients . • C : the number of clusters . • F : the number of features . 2.2 Single Cluster Cost
The single cluster cost is based on the logistic regression model . Here , we give a brief overview of logistic regression in the context of our running example . Note that , in this section , we focus only on the applications {P,N} of the clients that belong to a single cluster .
We denote by w the vector expressing the criteria of clients for approving/rejecting the applications . Note that all the clients of a cluster share the same w . In the running example , a w = ( 1.0 , 0.0 , 0.0)T expresses that clients prefer contractors with a lot of experience and do not care about the score and the bidding in an application . ( In practice , w involves an additional coefficient for the general bias . That is , in our running example , an application xi = ( 0.2 , 0.9 , 0.8)T would be extended with a constant term on a fourth dimension : xi would become ( 0.2 , 0.9 , 0.8 , 1.0)T , and w would become a 4 dimensional vector . )
In logistic regression , the probability of an application i being approved is given by the logistic function : g(wTxi ) =
1
1 + e−wTxi
( 3 )
2189 That is ,
P ( xi approved|w ) = g(wTxi ) P ( xi rejected|w ) = 1 − g(wTxi )
( 5 ) Therefore , as the value of the dot product wTxi approaches +∞ , P ( xi approved|w ) approaches 1.0 , while when wTxi approaches −∞ , P ( xi rejected|w ) approaches 10
The objective in logistic regression is finding the criteria
( 4 ) w , maximizing the likelihood :
P ( {P,N}|w ) = g(wTxi )
( 1 − g(wTxi ) )
( 6 )
N
P
Algorithm 1 Input : C : the number of clusters P : applications approved N : applications rejected
Output : W : local optimum criteria matrix
M : local optimum membership matrix
1 : M:= randomly assign the clients into C clusters 2 : while M = Mpre do 3 : Mpre := M 4 : W := solve problem of ( 11) (13 ) , with M fixed ( M step ) 5 : M := solve problem of ( 11) (13 ) , with W fixed ( E step ) 6 : end while
Taking into account regularization , the cost of a single cluster is the negative log likelihood plus a regularization term involving a hyperparameter λ and the 1 norm of the criteria vector w :
Cost(w ) : λw1 − ln(g(wTxi ) ) − ln(1 − g(wTxi ) )
P
N
Therefore , the cost of a client partition defined by mem bership and criteria matrices M and W , is :
Cost(W , M ) : λ wj1 − ( W , M )
( 10 )
Note that the cost involves the sum of the regularization j=1
C
( 7 ) terms for each cluster .
2.3 Optimal Client Partitioning In this section , we generalize the model to many clusters . Thus , our dataset {P,N} refers to the applications from all clients . We use the matrix M = [ m1 , . . . , mC ] ∈ {0 , 1}K×C to express the clients’ membership , ie , how the K clients are partitioned into C clusters . Column j , mj , gives the clients that belong to cluster j , while we denote by m k the row k of M , which gives the cluster where client k belongs . In our running example , suppose that the first cluster contains only the third client while the second cluster contains the other three clients . In that case ,
 0
0 1 0
 , m2 =
 1
1 0 1
 , M =
 0 1
0 1 1 0 0 1
 m1 = and m
1 = ( 0 , 1 ) , m 2 = ( 0 , 1 ) , m Therefore , the dot product uT
3 = ( 1 , 0 ) , m
4 = ( 0 , 1 ) i mj is 1 if the client that approved/rejected application i belongs to cluster j and 0 otherwise . For instance , if the second client approved/rejected application i and we have the same clustering as in the example above , uT i m1 = ( 0 , 1 , 0 , 0)(0 , 0 , 1 , 0)T = 0 , while uT i m2 = ( 0 , 1 , 0 , 0)(1 , 1 , 0 , 1)T = 1 . The criteria vector for cluster j is given by wj . We use the matrix W = [ w1 , . . . , wC ] ∈ IRF×C to refer to the union of vectors for all clusters . The likelihood of the evidence P ( {P,N}|W , M ) becomes :
C j=1
P i mj
N g(wT j xi)uT
( 1 − g(wT j xi))uT i mj
( 8 )
Note that the term for a cluster j involves only the applications of clients that belong to that cluster . ( For all of these applications the exponent uT i mj is 1 , while for all other applications that do not belong to the cluster j the exponent uT i mj is 0 . ) The log likelihood , ( W , M ) , becomes : uT i mj ln(g(wT j xi ) ) + i mj ln(1 − g(wT uT j xi ) )
C j=1
P
N
( 9 )
Our objective is to find the membership and criteria ma trices that solve the following optimization problem : min W,M st
Cost(W , M ) m k1 = 1 , ∀k ∈ {1 , . . . , K}
M ∈ {0 , 1}K×C , W ∈ IRF×C
( 11 )
( 12 )
( 13 )
The constraint ( 12 ) expresses that each client must be part of exactly one cluster , ie , we do not allow overlapping clusters .
3 . ALGORITHM
The exhaustive approach for solving the optimization problem in equations ( 11) (13 ) involves a O(C K ) time complexity ; for C clusters and K clients . Therefore , we propose a scalable algorithm based on Expectation Maximization with hard assignments . In each iteration two steps are involved : • E step : compute the optimal client memberships , ie , • M step : compute the optimal client criteria for each the optimal M , while keeping W fixed . cluster , ie , the optimal W , while keeping M fixed .
Our algorithm is given by Algorithm 1 . The input is the set of all clients’ applications , {P,N} , along with the number of clusters C . Note that in practice there are many ways to compute the number of clusters to use as input . The simplest approach is to try several different C values and keep the one maximizing a metric like Mean Average Precision or Discounted Cumulative Gain on a testing set .
In each E step , the value of the objective function in ( 11 ) decreases or remains the same ; in the worst case there are no changes in the client memberships that would decrease the value of the objective function . Likewise , in each M step the value of the objective function always decreases ; or at least remains the same . Hence , the algorithm eventually converges to a minimum ; when the client memberships remain the same for two consecutive iterations . Nevertheless , the minimum may be a local minimum and not a global one , since the problem of ( 11) (13 ) is not convex . In practice , we run Algorithm 1 more than once , using different initial assignments of clients to clusters , and keep the solution that gives the lowest value for the objective function . In our technical report [ 29 ] , we examine how the number of runs affects
2190 the convergence of our algorithm to the global minimum using synthetic data ; so that the true global optimum is known to us in advance .
One of the main advantages of our algorithm is its scalability . In the E step , a single pass over the clients is needed . ( For each client we find the cluster that “ explains ” better her decisions on her applications , while keeping the criteria for each cluster fixed . ) In the M step , we just need to solve C sparse logistic regression problems , ie , one problem per cluster . ( Solving sparse logistic regression problems is a process that has been highly optimized for large datasets , eg , [ 14 ] , [ 17] . ) In Section 414 , we discuss our scalability experiments and results . 3.1 E step
In the E step , we keep criteria matrix W fixed and we find for each client the cluster that “ explains ” better her decisions . That is , if Ua is the set of applications that were approved/rejected by a client a , we compute for each cluster j the log likelihood ( W ; j , Ua ) : ln(g(wT j xi ) ) + ln(1 − g(wT j xi ) )
( 14 )
N∩Ua
P∩Ua
Then , we assign client a to the cluster that gives the highest ( W ; j , Ua ) ( or , equivalently , the lowest negative loglikelihood ) . In our running example , if the second cluster gives the highest ( W ; j , U3 ) for the third client , the third row of M , ie , m
3 , becomes ( 0 , 1 ) .
At the end of E step , the assignment changes for each client will be reflected on the membership matrix M . 3.2 M step
In the M step , we keep M fixed and we find for each cluster j the criteria vector wj that best “ explains ” the clients’ decisions in that cluster . That is , for a cluster c , if U is the set of applications that were approved/rejected by the clients in c , we solve the following sparse logistic regression problem :
λwc1 −
P∩U min wc c xi ) ) −
N∩U ln(g(wT ln(1 − g(wT c xi ) )
( 15 ) The optimal solutions to the logistic regression problems form the W for the next E step .
4 . EXPERIMENTAL RESULTS
Our experiments are organized in three parts : • Before applying our algorithm on production data , we wanted to test the algorithm ’s behavior on synthetic data where we know in advance the ground truth , ie , the number of client groups and the hiring criteria in each of the groups . The results from the synthetic data experiments allow us to better interpret the results on the production site .
• In Section 4.3 we evaluate the performance of our algorithm on oDesk transactions . The results show that our clustering approach can scale to large datasets and improve the accuracy of base classifiers by more than 40 % . We also observe that the improvements of our algorithm are negligible in cases where the training dataset is small relatively to the model complexity , since the formed clusters do not contain sufficient training data for model learning .
Figure 3 : Angle distance to ground truth , as we increase the number of samples(applications ) from 8 to 128 applications per client , for 2 and 4 clusters . • Finally , in Section 4.4 we present a qualitative analysis of the clusters that are formed by our algorithm . The analysis shows that client clustering not only improves the accuracy of hiring predictions , but it also reveals latent characteristics of clients that use the work marketplace in different ways .
4.1 Synthetic Data
In this section , we briefly discuss our experiments with synthetic data . A detailed description of the settings and the synthetic data generation process can be found in our technical report [ 29 ] . 411 Number of samples We first try to quantify how our algorithm is affected by the number of samples available . Figure 3 depicts the behavior of our algorithm as we increase the number of applications ( X axis ) from 8000 to 128000 , for a fixed number of K = 1000 clients . The number of dimensions ( ie , the features that affect a client ’s decision ) is fixed to 8 . The Y axis gives the average angle distance between the vectors ( hiring criteria ) computed by our algorithm and the ground truth vectors . We run experiments for C = 2 and 4 clusters and we plot one curve for each C value .
For C = 2 we see a huge increase in accuracy ( the distance to the ground truth drops from 50 to 12 degrees ) when we go from 8 to 16 applications per client . As we keep increasing the number of applications , the accuracy further increases and approaches a zero degree distance to the ground truth ( less than 3 degrees for 128 applications per client ) .
For C = 4 the steep decrease in the distance to the ground truth , happens from 16 to 32 applications per client . In fact , if we look closely at the two curves we observe that for 4 clusters we need , roughly , twice as many applications as we need for 2 clusters , to reach the same distance to the ground truth . This can be explained by the fact that we have twice as many clusters and , hence , each cluster is assigned half the applications . 412 Number of dimensions In Figure 4(a ) , we plot the number of applications needed ( Y axis ) to reach within a 10 degree distance to the ground truth , as we increase the number of dimensions , from 4 to 24 ( X axis ) . The number of clusters , C , is 4 and the number of clients , K , is 1000
As we increase the number of dimensions from 4 to 12 , the number of applications needed for the 10 degree distance , increases slowly . For more than 12 dimensions , there is a steep increase for the number of applications needed , reaching almost 500 per client for 24 dimensions .
We also tried a second experiment showing the behavior of our approach when the number of dimensions increase ,
8163264128applications ( x1000)0102030405060Angle distance to ground truth ( Degrees)2 clusters4 clusters2191 ( a ) non sparse case
( b ) sparse case
( a ) Running Time
( b ) #iterations
Figure 4 : In the non sparse case , we plot the number of applications needed to get a 10 degree angle distance to ground truth , as we increase the number of dimensions from 4 to 24 , for 4 clusters . In the sparse case , we increase the number of dimensions from 64 to 256 , for 4 , 8 , and 16 clusters .
( a ) Distance to ground truth
( b ) #iterations
Figure 5 : Distance to ground truth and number of iterations needed for convergence , when we increase the number of clients while we keep the number of total samples(applications ) fixed . however , this time we used sparse ground truth criteria vectors . That is , out of all dimensions/criteria only a few of them play an important role in the clients’ decisions ( for details see our technical report [ 29] ) .
As in Figure 4(a ) , Figure 4(b ) depicts the number of applications needed ( Y axis ) to reach a 10 degree distance to the ground truth , as we increase the number of dimensions ( X axis ) , for the sparse case . For 4 clusters , although the number of dimensions is much greater than the number of dimensions in the non sparse case of Figure 4(a ) , the number of applications needed for the 10 degree distance , is two orders of magnitude lower ; requiring around 20 applications per client for 256 dimensions .
For a larger number of clusters , ie , C = 8 and C = 16 clusters , we need considerably more applications to reach the 10 degree distance , compared to C = 4 clusters . Still , even for C = 16 and 256 dimensions , we need less than 60 applications per client , which is an order of magnitude lower than the 500 applications per client for C = 4 and 24 dimensions , in the non sparse case of Figure 4(a ) . 413 Number of clients The experiments we discussed until now had a fixed number of 1000 clients . The next direction we explore is to keep the total number of applications fixed , and vary the number of clients , K , that decide upon those applications . We use 64000 applications for 8 dimensions and C = 4 clusters .
Figure 5(a ) depicts the distance to the ground truth , as K increases from 312 to 5000 . For K up to 2500 , the distance to the ground truth stays below 10 degrees . However , if we increase K further , to 5000 clients , the distance steeply increases from 10 degrees to 35 degrees . Moreover , there is a significant increase in the variance for K = 5000 .
Figure 6 : Running time and number of iterations needed for convergence , when we increase the number of samples(applications ) and dimensions .
The steep increase from K = 2500 to K = 5000 can be explained by the decrease in the number of samples per client ; 12.8 per client on average for 5000 clients . Note that when having a small number of samples per client , it is “ difficult ” to accurately predict in which cluster a client should be assigned , in the E step . Therefore , while the total number of samples remains constant , the convergence of the algorithm to the ground truth vectors becomes more “ difficult ” , as we keep decreasing the number of samples per client .
The above fact is confirmed by the number of iterations , until converging to an optimum , for the experiments of Figure 5(a ) . As we see in Figure 5(b ) , the number of iterations steadily increases as K increases from 312 to 5000 .
Scalability
414 In this section , we study the scalability of our algorithm as we increase the number of samples and dimensions . In Figure 6(a ) , we plot the running time of our algorithm ( Yaxis ) on a 1.8 GHz Intel Core i5 processor with 4 GB of RAM . The algorithm is implemented in Python using scikitlearn , and the local optima are explored in parallel . We use the same non sparse setting with the one used in Figure 4(a ) . The computational overhead for solving the sparse logistic regression problems increases as the number of samples increases ( X axis ) . Surprisingly , though , the overall time of our algorithm is not monotonically increasing : up to a point it is constant or decreasing and then slowly increases . Moreover , that point is different for each number of dimensions . The outcome of Figure 6(a ) is due to the trade off between the computational overhead for solving the logistic regression problems and the number of iterations needed to converge to an optimum . As the number of samples increases , the computational overhead increases but the number of iterations decreases , as Figure 6(b ) shows . This causes the overall running time to decrease or remain constant . In addition , the point where the number of iterations essentially stops decreasing is different for 8 , 12 , and 16 dimensions : the more the dimensions , the more the samples needed for the number of iterations to stop decreasing . This explains why the overall time starts to slowly increase in different points for each number of dimensions , in Figure 6(a ) . 4.2 oDesk Job Applications
We start with a brief overview of the datasets , the fea tures , and the metric , and then we discuss our findings .
Training and Testing Datasets : We collected all the applications of contractors to tasks opened by clients in the oDesk platform , from September 1st of 2012 to December 15 of 2012 . In addition , we filtered this set of applications in order to keep only the applications rejected/approved by
4812162024#dimensions0100200300400500Applications needed for convergence to ground truth ( x1000)4 clusters64128192256#dimensions0102030405060Applications needed for convergence to ground truth ( x1000)4 clusters8 clusters16 clusters312624124825005000#clients01020304050Angle distance to ground truth ( Degrees)4 clusters312624124825005000#clients01020304050607080#steps to converge to optimum4 clusters326496128160192224256applications ( x1000)020406080Running Time ( Seconds)8 dimensions12 dimensions16 dimensions326496128160192224256applications ( x1000)51015202530354045#steps to converge to optimum8 dimensions12 dimensions16 dimensions2192 Feature Contractor ’s Score
Matched Skills
Contractor ’s Total Hours Bid Amount Bid Rate Independent Contractor Contractor ’s Assignments Contractor ’s Total Revenue Contractor ’s Tests Passed Contractor ’s Bill Rate
Contractor ’s Experience Contractor ’s Portfolio Items Matched Cost
Matched Region Matched Task Category
Contractor ’s Recent Activity
Contractor ’s English Skills Contractor ’s Profile Information
Contractor ’s Active Interviews
Contractor ’s Rank Percentile
Description A score summarizing how good the contractor is , based on the feedback received from clients that worked with the contractor . Jaccard similarity between the set of skills the contractor has in her profile and the set of skills required for the task . The aggregated number of hours for the contracts the contractor had in the platform . The fixed amount of money the contractor asks for completing the task she applies for . The hourly rate earnings the contractor asks for completing the task she applies for . Indicates if the contractor is a company or agency that runs a profile in the platform . The number of tasks the contractor is assigned to and haven’t been completed yet . The aggregated earnings of the contractor in the platform . The number of tests that the contractor has taken and successfully completed in the platform . The hourly rate the contractor charges for the tasks he gets . Contractors usually set their bill rate in their profile to indicate the “ quality ” of their work . A score summarizing the working experience of the contractor based on her resume . The number of items the contractor has in her portfolio . The distance between the asking price and the price the client indicates she is willing to pay for the task . Indicates if the contractor ’s geographical region matches one of the regions the client prefers . Indicates if the task category is one of the categories the contractor has specified in her profile , as her areas of expertise . A score summarizing the contractor ’s recent activity , eg , tasks recently completed or the date she completed the last task . A score summarizing how fluently the contractor speaks english . A score summarizing how detailed the contractor ’s profile information is , eg , full name appearing or not . The number of pending applications the contractor has made , where the client has responded back to him ; and are thus considered as the active interviews . For each test a contractor passes she receives a score . This feature gives in which percentile the contractor belongs , based on her test scores .
Table 1 : Features representing each application
Tiers 1 , 2 , 3 , 4
1 , 2 , 3 , 4
1 , 2 , 3 , 4 1 , 2 , 3 , 4 1 , 2 , 3 , 4 2 , 3 , 4 2 , 3 , 4 2 , 3 , 4 2 , 3 , 4 2 , 3 , 4
3 , 4 3 , 4 3 , 4
3 , 4 3 , 4
4
4 4
4
4
( a ) approved = 10
( b ) approved = 20
( c ) approved = 30
( d ) approved = 40
( e ) approved = 50
Figure 7 : AUC ratio(Y axis ) for different numbers of clusters(X axis ) . Each curve refers to a different feature tier ( Table 1 ) and each plot to a different pair of training and testing sets ( Table 2 ) . approved 10 apps 20 apps 30 apps 40 apps 50 apps clients 2281 480 172 84 48 training 760k apps 260k apps 120k apps 80k apps 50k apps testing 105k apps 32k apps 22k apps 13k apps 12k apps
Table 2 : Datasets used in experiments clients who had a “ sufficient ” number of approved applications within this period . For example , consider a threshold of 10 approved applications , a client A that had 9 approved applications within that period , and a client B with 11 approved applications : all the applications for tasks opened by A are excluded from the training set , while all applications for tasks opened by B are included in the training set .
In particular , we generated 5 training sets using 5 different thresholds : 10 , 20 , 30 , 40 , and 50 approved applications . Note that each of these sets is a subset of the previous one . Table 2 summarizes the details for the 5 training sets : column approved refers to the threshold of approved applications , column clients refers to the number of clients satisfying the respective threshold constraint , and column training refers to the number of rejected/approved applications by those clients , ie , the actual samples in the training set .
Furthermore , we generated the 5 testing sets corresponding to the 5 training sets : for each training set we kept the clients that satisfy the respective threshold constraint , and we collected the rejected/approved applications for their tasks from 01/15/2013 to 03/01/2013 ( testing in Table 2 ) .
Features : We ran experiments with 4 different tiers of features : tier 1 includes 5 features , tier 2 includes the 5 features of tier 1 plus 5 more features and so on . Table 1 gives the names and descriptions of the features , along with the tiers that include each feature . Note that each application in our training/testing sets is represented by the values for those features the moment the application is made . We normalize all the feature values to [ 0.0 , 1.0 ] , taking into account what is the maximum value the feature may have . For example , for the contractor ’s score that has a maximum of 5.0 stars , the score in each application is divided by 50
Metric : The metric we use is the area under the ROC curve ( AUC ) . Moreover , when there are more than one clusters , we compute a separate AUC for each cluster . Let us describe how we quantify the improvement over the singlecluster approach through an example . Consider running Algorithm 1 with 2 clusters as input . For each cluster i Algorithm 1 computes , we use the criteria wi learned for this cluster , and we compute the AUC : assume an AUC 1 = 0.8 for cluster 1 and an AUC 2 = 0.9 for cluster 2 . Note that in order to compute AUC i , we use the testing set applications of the clients that were placed in cluster i . In addition , we run Algorithm 1 using a single cluster as input ( ie , we run the classic logistic regression model ) and we compute the criteria ws for a single cluster . For each of the 2 clusters Algorithm 1 initially produced , we use ws to compute the AUC for the testing set applications that belong to that
24816#Clusters0608101214Improvement over Baseline ( AUC ratio)5 features10 features15 features20 features24816#Clusters0608101214Improvement over Baseline ( AUC ratio)5 features10 features15 features20 features24816#Clusters0608101214Improvement over Baseline ( AUC ratio)5 features10 features15 features20 features24816#Clusters0608101214Improvement over Baseline ( AUC ratio)5 features10 features15 features20 features24816#Clusters0608101214Improvement over Baseline ( AUC ratio)5 features10 features15 features20 features2193 1 = 0.8 and an AUC cluster : assume an AUC 2 = 05 Then , we compute the ratio AUC AUC for each cluster and the weighted average of all ratios : assume that in the testing set of cluster 1 there are 200 approved applications and in the testing set of cluster 2 there are 800 approved applications . The combined ratio will be : 200 = 1.64 , indicating 1000 an improvement of 64 % over the single cluster approach .
AUC 1 AUC 1
+ 800 1000
AUC 2 AUC 2
It is important to note that the AUC cannot be greater than 1.0 and will not be less than 0.5 ; assuming a prediction accuracy not worse than random . Hence , the combined AUC ratio will always be less than 2.0 , or in other words , the improvement over the single cluster approach cannot be more than 100 % . 4.3 Improvement over a Global Model
Figure 7 depicts the ratio between the AUC for the multicluster approach and the AUC for the single cluster approach ( baseline ) on the Y axis , as we increase the number of clusters ( X axis ) . Each curve refers to a different feature tier ( see Table 1 ) : a)5 features in tier 1 , b)10 features in tier 2 , c)15 features in tier 3 , and d)20 features in tier 4 . In Figure 7(a ) , we used the training/testing set for an approved threshold of 10 , in Figure 7(b ) , we used the training/testing set for an approved threshold of 20 , and so on .
There are three main factors that can explain the results in Figure 7 . First , as we increase the number of dimensions ( features ) , the improvement over the baseline ( same criteria for all clients ) drops : note that in most cases the curves for fewer features stay above the curves for more features . This factor is aligned with our observations in Section 412 , ie , as the number of features increase , our algorithm needs more samples and at some point the number of samples in the respective training set is not sufficient to improve over the prediction accuracy of the baseline approach .
Second , if we increase the number of clusters beyond a certain point , there is no improvement over the baseline ( in fact , the baseline gives a higher AUC ) : in Figure 7(c ) for 16 clusters , the curve for 10 features falls below 1.0 , in Figure 7(d ) for 16 clusters , both curves for 10 and 15 features fall below 1.0 , and in Figure 7(e ) for 16 clusters , the curves for 10 , 15 , and 20 features fall below 10 This observation is aligned with the discussions for Figures 3 and 4(b ) .
Third , if we reduce the number of samples ( applications ) beyond a certain point , the improvement over the baseline diminishes : the curve for 5 features drops from a 30% 40 % improvement over the baseline in Figures 7(a) 7(c ) to a 20 % improvement in Figures 7(d) 7(e ) , while the curve for 10 features does not show any improvement over the baseline ( or even drops below 1.0 ) in Figures 7(c) 7(e ) . This observation is aligned with the discussion in Section 411
Nevertheless , the three factors discussed above , fail to explain why in some cases where the number of samples decreases , the improvement over the baseline increases . For example , from Figure 7(a ) to Figure 7(b ) , there is a large increase in the curves for 5 and 10 features , for 2 and 4 clusters . ( Or , from Figure 7(b ) to Figure 7(c ) , the improvement for 5 features increases from 30 % to almost 40 % , for 16 clusters . ) This behavior is the outcome of an important trade off : by increasing the approved threshold , the number of samples in the training set drops from 760000 to 50000 , from Figure 7(a ) to Figure 7(e ) , however , the number of applications per client increases . Hence , we have more samples per client in order to accurately “ learn ” her criteria and place
Figure 9 : Weights of the criteria vector when a single cluster is used ( traditional logistic regression ) . her in the “ right ” cluster . ( The positive effect of having more samples per client was also discussed in Section 413 ) 4.4 Clients Hiring Criteria
Let us now focus on one of the most interesting outcomes of our algorithm : The 4 clusters produced for 10 features and an approved threshold of 20 , an outcome achieving a substantial 40 % improvement over the baseline , in Figure 7(b ) . ( As we discussed in the description of the metric , the improvement cannot be greater than 100% . )
Figures 8(a ) to 8(d ) depict the values of the criteria vectors , w1 to w4 , for the 4 clusters . Specifically , the figures show the 4 , out of 10 , criteria for which at least one of the vectors had a non zero value . As these figures indicate , there are important differences in the criteria of clients for : a ) clusters 1 and 3 ( Figures 8(a ) and 8(c) ) , b ) cluster 2 ( Figure 8(b) ) , and c ) cluster 4 ( Figure 8(d) ) .
Clients in cluster 2 have a strong tendency in selecting contractors whose skills match well with the task they post ( large positive weight for the “ Matched Skills ” feature ) , clients in cluster 4 tend to reject contractors that have completed many hours working in the platform ( large negative weight for “ Contractor ’s Total Hours ” ) , and clients in clusters 1 and 3 decide based on the contractor ’s score , skills , and agency independence ( see Table 1 for a description of the features ) . Before discussing further the differences between client criteria in different clusters , let us compare these criteria with those learnt when using a single cluster , ie , when learning the same criteria for all clients . Figure 9 depicts the non zero values of the criteria vector when a single cluster is used , ie , when we run the traditional logistic regression . There are two main inconsistencies in those values : 1 ) they do not capture the fact that only a specific group of clients ( cluster 4 ) has a negative bias towards contractors with a lot of hours completed in the platform , and 2 ) they miss the strong positive bias of a group of clients ( cluster 2 ) towards contractors with the appropriate skills for a task . These two inconsistencies explain why our algorithm gives this substantial 40 % improvement ( Figure 7(b ) ) over the baseline approach of learning the same criteria for all clients .
Some of the differences in the clients’ criteria can be explained by observing the IT/KPO and FP/HR percentages of the applications in the different clusters . IT stands for Information Technology and refers to applications for all tasks related to software development and administration , eg , web development , DB administration , or software for scientific experiments . Applications for all other tasks fall under the Knowledge Processing Outsourcing ( KPO ) category , eg , translation , marketing , or logo design . A second categorization for tasks is the Fixed Price(FP)/Hourly Rate(HR ) categorization that refers to whether a fixed amount will be given to the contractor upon the completion of the task or the client and the contractor will get into a contract with
−8−6−4−202468WeightContractor's Total HoursIndependent ContractorMatched SkillsContractor's ScoreCriteria weights2194 ( a ) criteria vector w1
( b ) criteria vector w2
( c ) criteria vector w3
( d ) criteria vector w4
( e ) IT vs KPO cluster 1
( f ) IT vs KPO cluster 2
( g ) IT vs KPO cluster 3
( h ) IT vs KPO cluster 4
( i ) FP vs HR cluster 1
( j ) FP vs HR cluster 2
( k ) FP vs HR cluster 3
( l ) FP vs HR cluster 4
Figure 8 : Figures ( a ) to ( d ) depict the weights of the criteria vectors in each of the 4 clusters computed for 10 features and an approved threshold of 20 . Figures ( e ) to ( h ) depict the percentage of Information Technology tasks in each cluster , while figures ( i ) to ( l ) depict the percentage of Hourly Rate tasks . predefined hourly rate earnings . Figures 8(e ) to 8(h ) depict the IT/KPO percentages , while Figures 8(i ) to 8(l ) depict FP/HR percentages , in the 4 clusters .
In cluster 2 there is a large percentage of IT tasks ( 57.9% ) , as we see in Figure 8(f ) . This large percentage provides an explanation for the criteria in cluster 2 . Most IT contractors receive good reviews by clients ; although clients are not always fully satisfied with the contractor ’s work ( eg , software consistent with the specifications that does not do exactly what the client had in mind ) . As a result , IT contractors tend to have a very high score and , hence , the contractor ’s score stops being a powerful signal for a client to base her decision . On the other hand , the technologies where the contractor is an expert , in many cases indicate if a contractor is a good fit for an IT task . Therefore , the clients in cluster 2 have a good reason to base their decisions on the “ Matched Skills ” ( Figure 8(b) ) .
Although the criteria in clusters 1 and 3 ( Figures 8(a ) and 8(c ) ) seem similar , there is an interesting difference : clients in cluster 1 give a large weight to the “ Matched Skills ” while the value for this feature in cluster 3 is almost zero . As Figures 8(i ) and 8(k ) point out , there is an important difference in the percentage of the HR tasks in the two clusters : in cluster 1 only 18.9 % of the tasks are HR , while in cluster 3 50.5 % of the tasks are HR . The “ Matched Skills ” signal is usually more useful for FP tasks that require expertise in a very specific piece of work that needs to be completed , while HR tasks mostly require a good collaboration between the client and the contractor .
As for the large negative weight on the “ Contractor ’s Total Hours ” for cluster 4 ( Figure 8(d) ) , it appears that the
IT/KPO and HR/FP percentages cannot provide any plausible explanation . Cluster 4 simply consists of clients that prefer to work with contractors that are “ new ” to the platform . In fact , there is a good reason for selecting such contractors : in many cases “ new ” contractors are eager to produce a high quality outcome in order to build a good reputation in the system .
5 . RELATED WORK
A problem similar to the one we study is addressed by collaborative filtering ( CF ) approaches ( see [ 28 ] for a recent survey ) . Traditional memory based CF approaches estimate the preference of a user U to an item I using the ratings of U for items “ similar ” to I or the ratings of users “ similar ” to U for I [ 11 , 26 ] . Other CF approaches are based on latent factor models such as matrix factorization [ 24 , 31 ] , latent Dirichlet allocation [ 2 ] , Boltzmann machines [ 25 ] , Latent Semantic Analysis [ 12 ] , or user/item co clustering [ 7 , 10 ] , while others [ 19 ] have proposed models that combine the memory and model based approaches .
The hetereogeneity of user preferences is also studied by Lenk et al [ 21 ] : user preferences depend on users’ spatial region , in that study . User partitioning can also be based on whether a user is an “ innovator ” or “ imitator ” [ 13 ] : innovators make decisions based on their own preferences , while imitators decide based on a product ’s stage of maturity .
The main limitation for directly applying CF methods to the problem discussed in this paper , is the fact that contractor applications to a task posted by a client are “ unique ” . That is , while most CF methods assume that users rate , explicitly or implicitly , the same products , in our case , clients
−8−6−4−202468WeightContractor's Total HoursIndependent ContractorMatched SkillsContractor's ScoreCriteria weights−8−6−4−202468WeightContractor's Total HoursIndependent ContractorMatched SkillsContractor's ScoreCriteria weights−8−6−4−202468WeightContractor's Total HoursIndependent ContractorMatched SkillsContractor's ScoreCriteria weights−8−6−4−202468WeightContractor's Total HoursIndependent ContractorMatched SkillsContractor's ScoreCriteria weightsIT196%KPO804%IT579%KPO421%IT324%KPO676%IT273%KPO727%HR189%FP811%HR172%FP828%HR505%FP495%HR226%FP774%2195 do not actually rate the same contractors but their applications to a specific task .
The effect of clients having different preferences when selecting among alternatives , is studied thoroughly in the economics/marketing literature [ 15 , 23 , 20 , 1 ] . In particular , most studies focus on how a market is partitioned in segments , with each segment having clients with same preferences when selecting among brands . Most related to our work , are the studies using Finite Mixture Logit models [ 22 , 15 , 23 ] . An important difference of those studies with our work is that we don’t focus on “ learning ” the distribution of market segments . On the contrary , we focus on partitioning the existing clients , with each client belonging to exactly one segment ; instead of belonging to any segment with some probability ( see [ 16 ] for an analysis of the tradeoffs between “ hard ” and “ soft ” assignments ) . Moreover , as noted earlier in the discussion for CF , the preferences of clients refer to “ unique ” applications in our model , as opposed to a fixed set of alternatives , eg , brands . Our model is simpler than the Finite Mixture Logit models used in economics and marketing studies , and aims for a scalable solution to a problem met in most of the online work marketplaces . 6 . CONCLUSION
Identifying the groups of clients with similar hiring criteria is of great importance in online work marketplaces . In this paper , we presented our model for hiring criteria clustering and we developed a clustering algorithm that can be applied effectively on large datasets . When applied on oDesk job applications , our approach significantly improves the prediction accuracy for future hirings of clients .
Furthermore , the analysis of the clusters generated by our algorithm reveals some interesting facts about the way different groups of clients choose contractors for their tasks : some clients are positively biased to contractors that are “ new ” to a marketplace ( probably because many new contractors are eager to build a competitive profile ) , while other clients ignore the contractor ’s reputation and focus on how well the contractor ’s skills match to the task requirements . Our approach discovers such differences in client hiring criteria and can drastically improve the matching between clients and contractors in work marketplaces . 7 . REFERENCES [ 1 ] G . M . Allenby and P . E . Rossi . Marketing models of consumer heterogeneity . J . Econometrics , 89:57–78 , 1998 .
[ 2 ] D . M . Blei , A . Y . Ng , and M . I . Jordan . Latent dirichlet allocation . J . Machine Learning Research , 3:993–1022 , 2003 .
[ 3 ] R . Collobert , S . Bengio , and Y . Bengio . A parallel mixture of SVMs for very large scale problems . In NIPS , 2002 .
[ 4 ] M . Daltayanni , L . de Alfaro , and P . Papadimitriou .
Workerrank : Using employer implicit judgements to infer worker reputation . In WSDM , 2015 .
[ 5 ] A . S . Das , M . Datar , A . Garg , and S . Rajaram . Google news personalization : Scalable online collaborative filtering . In WWW , 2007 .
[ 6 ] O . Dekel and O . Shamir . There ’s a hole in my data space : Piecewise predictors for heterogeneous learning problems . In AISTATS , 2012 .
[ 7 ] M . Deodhar and J . Ghosh . Scoal : A framework for simultaneous co clustering and learning from complex data . ACM Trans . Knowl . Discov . Data , 4(3):11:1–11:31 , 2010 .
[ 8 ] W . DeSarbo and W . Cron . A maximum likelihood methodology for clusterwise linear regression . J . of Classification , 5(2):249–282 , 1988 .
[ 9 ] Z . Fu , A . Robles Kelly , and J . Zhou . Mixing Linear SVMs for Nonlinear Classification . IEEE Trans . Neural Networks , 21(12):1963–1975 , 2010 .
[ 10 ] T . George and S . Merugu . A scalable collaborative filtering framework based on co clustering . In ICDM , 2005 .
[ 11 ] J . L . Herlocker , J . A . Konstan , A . Borchers , and J . Riedl .
An algorithmic framework for performing collaborative filtering . In SIGIR , 1999 .
[ 12 ] T . Hofmann . Latent semantic models for collaborative filtering . ACM Trans . Information Systems , 22:89–115 , 2004 .
[ 13 ] K . Hu , W . Hsu , and M . L . Lee . Utilizing users’ tipping points in e commerce recommender systems . In ICDE , 2013 .
[ 14 ] S . in Lee , H . Lee , P . Abbeel , and A . Y . Ng . Efficient l1 regularized logistic regression . In AAAI , 2006 .
[ 15 ] W . A . Kamakura and G . Russell . A probabilistic choice model for market segmentation and elasticity structure . J . Marketing Research , XXVI:379–390 , 1989 .
[ 16 ] M . J . Kearns , Y . Mansour , and A . Y . Ng . An information theoretic analysis of hard and soft assignment methods for clustering . In UAI , 1997 .
[ 17 ] K . Koh , S J Kim , and S . Boyd . An interior point method for large scale l1 regularized logistic regression . J . Machine Learning Research , 8:1519–1555 , 2007 .
[ 18 ] M . Kokkodis , P . Papadimitriou , and P . G . Ipeirotis . Hiring behavior models for online labor markets . In WSDM , 2015 .
[ 19 ] Y . Koren . Factorization meets the neighborhood : A multifaceted collaborative filtering model . In KDD , 2008 .
[ 20 ] P . J . Lenk , W . S . DeSarbo , P . E . Green , and M . R . Young . Hierarchical bayes conjoint analysis : Recovery of partworth heterogeneity from reduced experimental designs . Marketing Science , 15:173–191 , 1996 .
[ 21 ] J . J . Levandoski , M . Sarwat , A . Eldawy , and M . F . Mokbel .
Lars : A location aware recommender system . In ICDE , 2012 .
[ 22 ] G . Mclachlan and D . Peel . Finite Mixture Models . Wiley ,
2000 .
[ 23 ] D . C . J . Pradeep K . Chintagunta and N . J . Vilcassim .
Investigating heterogeneity in brand preferences in logit models for panel data . J . Marketing Research , 28:417–428 , 1991 .
[ 24 ] R . Salakhutdinov and A . Mnih . Probabilistic matrix factorization . In NIPS , 2007 .
[ 25 ] R . Salakhutdinov , A . Mnih , and G . Hinton . Restricted boltzmann machines for collaborative filtering . In ICML , 2007 .
[ 26 ] B . Sarwar , G . Karypis , J . Konstan , and J . Riedl .
Item based collaborative filtering recommendation algorithms . In WWW , 2001 .
[ 27 ] H . Sp¨ath . Algorithm 39 clusterwise linear regression .
Computing , 22(4 ) , 1979 .
[ 28 ] X . Su and T . M . Khoshgoftaar . A survey of collaborative filtering techniques . Advances in Artificial Intelligence , 2009 .
[ 29 ] V . Verroios , P . Papadimitriou , R . Johari , and
H . Garcia Molina . Client clustering for hiring modeling in work marketplaces . Technical report , Stanford University , available at http : // ilpubs . stanford . edu : 8090/ 1106/ .
[ 30 ] M . Wedel and J B E . M . Steenkamp . A clusterwise regression method for simultaneous fuzzy market structuring and benefit segmentation . J . Marketing Research , 28(4):385–396 , 1991 .
[ 31 ] J . Yuan , B . Kanagal , V . Josifovski , L . Garcia Pueyo ,
A . Ahmed , and S . Pandey . Focused matrix factorization for audience selection in display advertising . In ICDE , 2013 .
[ 32 ] J . Zhu , N . Chen , and E . Xing . Infinite svm : a dirichlet process mixture of large margin kernel machines . In ICML , 2011 .
2196
