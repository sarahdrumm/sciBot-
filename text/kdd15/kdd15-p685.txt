MASCOT : Memory efficient and Accurate Sampling for
Counting Local Triangles in Graph Streams
Yongsub Lim
School of Computing
KAIST yongsub@kaistackr
School of Computing
U Kang
KAIST ukang@cskaistackr
ABSTRACT How can we estimate local triangle counts accurately in a graph stream without storing the whole graph ? The local triangle counting which counts triangles for each node in a graph is a very important problem with wide applications in social network analysis , anomaly detection , web mining , etc .
In this paper , we propose Mascot , a memory efficient and accurate method for local triangle estimation in a graph stream based on edge sampling . To develop Mascot , we first present two naive local triangle counting algorithms in a graph stream : Mascot C and Mascot A . Mascot C is based on constant edge sampling , and Mascot A improves its accuracy by utilizing more memory spaces . Mascot achieves both accuracy and memory efficiency of the two algorithms by an unconditional triangle counting for a new edge , regardless of whether it is sampled or not . In contrast to the existing algorithm which requires prior knowledge on the target graph and appropriately set parameters , Mascot requires only one simple parameter , the edge sampling probability . Through extensive experiments , we show that for the same number of edges sampled , Mascot provides the best accuracy compared to the existing algorithm as well as Mascot C and Mascot A . Thanks to Mascot , we also discover interesting anomalous patterns in real graphs , like core peripheries in the web and ambiguous author names in DBLP . Categories and Subject Descriptors G22 [ Graph Theory ] : Graph algorithms ; H28 [ Database Applications ] : Data mining General Terms Design , Experimentation , Algorithms Keywords Local triangle counting ; graph stream mining ; edge sampling ; anomaly detection c . 2015 ACM . ISBN 978 1 4503 3664 2/15/08 $1500
DOI : http://dxdoiorg/101145/27832582783285 .
1 .
INTRODUCTION
How can we count local triangles in a graph stream with a limited memory space ? How accurate are edge sampling strategies for local triangle counting ? Triangle counting is one of the most widely studied graph mining problems . The number of triangles in a graph becomes an important index indicating cohesiveness of the graph . In many cases , one wants to count triangles adjacent to every node , which helps understand whether the node belongs to a tightly connected group or has diverse neighbors . This problem , called local triangle counting , has various applications . For instance , in social networks , triangle counting is used to detect fake accounts [ 40 ] ; the number of triangles of a user is examined to identify a social role of the user in the network [ 39 ] , and is shown to be a good feature in assessing the content quality provided by the user [ 8 ] . In web mining , it is also used to find spam pages [ 8 ] and to uncover hidden thematic layers [ 15 ] . Other applications include network community detection [ 9 ] and motif detection in bioinformatics [ 24 ] .
Despite enormous bodies of researches on triangle counting , it is still challenging to handle a massive graph due to the complexity of the problem—superlinear time on the graph size is inevitable . Moreover , a number of real graphs appearing especially in recent days are given in a stream fashion whose size is unknown , or even infinite : eg packet transmission in the Internet , phone call history , financial transactions , etc . Often , such real graph streams should be analyzed in real time . Thus , designing a streaming algorithm is required for efficient online analysis of a huge size graph . A number of algorithms to count triangles in a graph stream have been proposed [ 7 , 8 , 11 , 17 , 18 , 19 , 27 ] . Most of them , however , focus on global triangle counting . Although the first one pass streaming algorithm for local triangle counting was developed with rigorous theoretical analysis [ 22 ] , it is unsuitable for an evolving graph unless an efficient update scheme is explicitly designed . To be used in practice , it also requires knowing the number of nodes in the stream to set hash functions , and a user defined threshold to count local triangles for nodes having degrees above the threshold .
In this paper , we propose Mascot , a memory efficient , and accurate one pass local triangle counting algorithm for a graph stream based on edge sampling . Mascot provides unbiased estimation of the number of local triangles for every node . We first develop two naive algorithms , Mascot C and Mascot A , with simple edge sampling . Mascot C is based on constant edge sampling , and Mascot A performs Mascot C with unconditionally sampling an edge making a
685 ●
MASCOT
MASCOT−C
MASCOT−A
KP
●
0 0 1
.
BEST
●
●
●
● ● ● ● ●
0 9 0
.
) ● ρ ( n o i t l a e r r o C f o n a e M
0 8 0
.
0 7 0
.
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
Memory Usage ( η )
) ε ( r o r r
E
5 1 0 0
.
0 1 0 0
.
5 0 0 0
.
●
●
●
●
● ●
●
● ●
BEST ● 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
Memory Usage ( η )
( a ) True vs . Estimation
( b ) Corr . Coef .
( c ) Mean of Error
Figure 1 : Summary of our results on the BerkStan graph described in Table 3 . ( a ) Scatter plot between the true local triangle counts and its estimations by Mascot for all nodes . Mascot is accurate especially for nodes with many triangles . ( b ) , ( c ) : Comparison of our proposed algorithm Mascot and competitors including KP [ 22 ] for local triangle estimation in a graph stream . Note that for a fixed memory space , Mascot provides the best accuracy in terms of the correlation coefficient and the error .
Table 1 : Comparison of Mascot and other algorithms . Our main proposed method Mascot shows the best performance for all metrics . For Corr . Coef . and Error , we compare at the same memory usage , and for the memory usage , we do at the same accuracy .
[ Proposed ]
Basic
Mascot
Mascot A Mascot C
Corr . Coef . Error Memory Usage Estimated Node Range
Incremental Update
High Small Low
All
Yes
High
Medium Medium
Medium Medium Medium
All
Yes
All
Yes
Existing KP [ 22 ]
Low Large Large
Top k
No triangle . Mascot A has lower variance but uses more memory spaces than Mascot C . Our proposed algorithm Mascot provides the advantages of both accuracy and memoryefficiency by the strategy of “ unconditional counting before sampling ” . Table 1 compares Mascot and other algorithms including KP [ 22 ] by various aspects .
Conducting extensive experiments on real world graphs , we show that Mascot estimates local triangle counts better than Mascot C and Mascot A in terms of Pearson correlation coefficient and mean of absolute relative error . We also demonstrate that Mascot outperforms the existing algorithm [ 22 ] in both metrics which to the best of our knowledge is the only one pass local triangle counting algorithm for a graph stream . Figure 1 illustrates our results for BerkStan graph . Applying Mascot to real graphs , we show that local triangle counts are effective in discovering anomalous patterns in a graph .
Our contributions are summarized as follows . • Algorithm . We propose Mascot , a memory efficient and accurate one pass local triangle counting algorithm for a graph stream . This improves two naive edge
Table 2 : Table of symbols .
Symbol
Description
G , V , E n , m Nu Nuv T Tu fiu τu , τ c p η quv , qe u , τ a u
Undirected graph , set of its nodes , set of its edges Numbers of nodes and edges Set of neighbors of a node u Nu ∩ Nv Set of triangles in a graph Set of triangles of the node u Number of triangles of the node u , equals to|T u| Estimation for fiu Default probability of sampling each edge Ratio of sampled edge over the total edges True probability that an edge e = ( u , v ) is sampled sampling based algorithms Mascot C and Mascot A to achieve both small variance and memory efficiency . Unlike the previous algorithm [ 22 ] , Mascot requires only one simple parameter , the edge sampling probability , without requiring knowledge on the input graph . • Performance . We show that Mascot is more accurate not only than Mascot C and Mascot A but also than the previous algorithm [ 22 ] in terms of Pearson correlation coefficient and the mean of absolute relative error . • Discovery . We discover several anomalous patterns in real graphs by applying Mascot , including coreperiphery structures in the web and ambiguous author names in a collaboration network .
The codes and data used in this paper are available at http://kdmlaborg/mascot The rest of this paper is organized as follows . In Section 2 , we discuss related works of our paper . We describe our proposed algorithm Mascot and two naive ones in Section 3 . After showing the performance of Mascot in accuracy , and comparing it with the previous algorithm in Section 4 , we conclude in Section 5 .
2 . RELATED WORK
In this section , we describe related works . Table 2 lists the symbols used in our paper .
686 2.1 Triangle Counting
The global and local triangle counting in a graph has been extensively studied . The simplest and time efficient algorithm is to use matrix multiplication [ 3]—A3 for an adjacency matrix A results in exact local triangle counting . Despite the fast running time of O(m1.41 ) , the algorithm is not suitable for large scale graphs due to its high space complexity of O(n2 ) where n and m are the numbers of nodes and edges , respectively . To achieve a fast running time with a reasonable space requirement , various approaches have been proposed [ 23 , 32 ] .
To handle large scale graphs , researchers have been also interested in devising external and distributed algorithms . Chu and Cheng [ 12 ] proposed a graph partitioning based algorithm , and Hu et al . [ 16 ] developed an algorithm by iteratively loading a part of edges and counting triangles among them . Recently , Pagh and Silvestri [ 26 ] achieved the currently minimum I/O complexity O(m3/2/(B M ) ) where M is the total space and B is a size of data transfer block . To make an algorithm more scalable , parallel computing has been also widely considered . Cohen [ 13 ] proposed the first MapReduce algorithm for triangle enumeration . Suri and Vassilvitskii [ 36 ] proposed a graph partitioning based algorithm , which is further improved by Park and Chung [ 28 ] . For more parallel algorithms , we refer to [ 4 , 21 , 29 ] . 2.2 Graph Stream Mining
√
There have been numerous studies on graph stream mining . We first present studies on triangle counting in a graph stream , and then those on other graph mining problems . 221 Triangle Counting Recently , there have been numerous studies on triangle counting in a graph stream .
Global Triangle Counting . The first study was conducted theoretically by Bar Yossef et al . [ 7 ] . Afterwards , its space bound was improved by [ 18 ] and [ 11 ] successively . Recently , Jha et al . [ 17 ] proposed a single pass algorithm based on wedge sampling to estimate the number of triangles and √ the clustering coefficient from a graph stream . The algon ) memory spaces with an additive error rithm takes O( guarantee . Kane et al . [ 19 ] devised a sketch based streaming algorithm to count subgraphs with a constant size , which allows edge deletion . This result was improved in the space complexity by [ 30 ] . Tsourakakis et al . [ 38 ] presented a graph sparsification method by edge sampling for estimating the number of triangles in a graph , which requires one pass to the graph and can be applied to a stream model . A more general edge sampling framework was proposed by [ 1 ] , which can be applied to estimating various graph statistics including the number of triangles .
Local Triangle Counting .
Counting triangles in a graph stream for each node has been also studied . Becchetti et al . [ 8 ] proposed a semi streaming algorithm requiring log n passes , and Kutzkov and Pagh proposed a single pass algorithm ( “ KP ” henceforth ) [ 22 ] based on node coloring [ 27 ] . However , they are limited in practice : [ 8 ] requires multiple passes to a graph , and [ 22 ] requires to know the number of nodes in a graph stream in advance . In contrast , our proposed Mascot requires only one pass to the graph with only one parameter , the edge sampling probability , while achieving a better accuracy . Note that although the one pass local triangle counting methods , [ 22 ] and our proposed Mascot , assume no duplicated edges , both can be easily extended to a multigraph : eg duplicated edges can be checked using a hash scheme like Bloom filter [ 10 ] . 222 Other Problems Other graph mining tasks for a stream model have been extensively studied as well . Balanced graph partitioning in a graph stream was initiated in [ 35 ] by suggesting several simple heuristics . Their method not only significantly outperforms a naive hashing scheme , but also is comparable to the state of the art method for some cases . Stanton [ 34 ] theoretically showed the lower bound o(n ) of approximation ratio , and proposed two randomized greedy algorithms . We refer to [ 25 , 37 ] for more related studies .
Another interesting topic is online PageRank computation . Sarma et al . [ 31 ] studied performing a random walk from a graph stream , and their method enables computing PageRank and conductance . Also PageRank computation in an evolving graph has been studied in [ 5 , 6 , 14 ] .
3 . PROPOSED METHOD
In this section , we propose Mascot , a memory efficient accurate algorithm to provide unbiased estimation of local triangle counts for every node in a graph stream . We first present two naive algorithms Mascot C and Mascot A based on edge sampling . While Mascot A provides lower variance than Mascot C , it requires more memory spaces for the same sampling probability . Mascot achieves both memory efficiency and small variance . Also Mascot requires only one simple parameter of edge sampling probability and no prior knowledge on an input graph . 3.1 MASCOT C
Mascot C ( Memory efficient Accurate Sampling for Count ing Local Triangles with Constant Sampling ) is based on Doulion [ 38 ] , a method to estimate the number of triangles in a massive graph by sparsification . Mascot C samples every edge from a graph stream with a constant probability p while keeping local triangle estimation up to date for every node . For an efficient update , given a new edge ( u , v ) , only the estimations for c ∈ Nuv ∪ {u , v} , where Nuv is the set of common neighbors of u and v , are updated . Note that triangle counts of the other nodes are not affected by ( u , v ) . The dominant factor of the total cost per new edge is the computation of Nuv which takes O(du + dv ) time where du is the degree ofu . The whole procedure of Mascot C is shown in Algorithm 1 .
Mascot C provides unbiased estimation as stated in the following lemma .
Lemma 1 . Let u be the true number of local triangles of u be its estimation by Mascot C . For every u ∈ V , u , and τ c
Proof . Let δλ indicate whether the triangle λ is sampled or not . At any time , the expected value of τ c u becomes
E [ τ c u ] = u . .
E [ τ c u ] =
1 p3
E [ δλ ] ,
λ∈Tu where Tu is the set of triangles containing u . Since every edge is sampled with probability p , E [ δλ ] =p 3 . Hence , we obtain E [ τ c u ] = u .
687 Algorithm 1 : Mascot C ( Memory efficient Accurate Sampling for Counting Local Triangles with Constant Sampling ) edge sampling probability p .
Input : Graph stream S , and 1 G ← ( V , E ) withV = E = ∅ . 2 foreach edge e = ( u , v ) from S do 3
ReadyN ode(u ) . ReadyN ode(v ) . x ← SampleEdge(e , p ) . if x = 1 then
CountT riangles(e , 1/p3 ) . end
8 9 end 10 Function ReadyN ode(u ) 11 12 end 13 Function SampleEdge((u , v ) , p ) → int if u /∈ V then V ← V ∪ {u} and τu = 0 . x ← Bernoulli(p ) . if x = 1 then
E ← E ∪ {(u , v)} . end return x .
18 19 end 20 Function CountT riangles((u , v ) , s ) 21
Nuv ← Nu ∩ Nv . foreach c ∈ Nuv do τc ← τc + s . end
τu ← τu + |Nuv|s . τv ← τv + |Nuv|s .
26 27 end
4
5
6
7
14
15
16
17
22
23
24
25
Algorithm 2 : Mascot A ( Memory efficient Accurate Sampling for Counting Local Triangles with Adaptive Sampling ) Input : Graph stream S , and 1 G ← ( V , E ) withV = E = ∅ . 2 foreach edge e = ( u , v ) from S do 3 edge sampling probability p .
4
5
6
7
8
9
14
15
16
21
22
23
24
25
ReadyN ode(u ) . ReadyN ode(v ) . if e forms a triangle then
SampleEdgeA(e , 1 ) . CountT rianglesA(e ) . else
SampleEdgeA(e , p ) . end
10 11 end 12 Function SampleEdgeA((u , v ) , p ) 13 x ← Bernoulli(p ) . if x = 1 then
E ← E ∪ {(u , v)} . quv = p . end
17 18 end 19 Function CountT rianglesA((u , v ) ) 20
Nuv ← N ( u ) ∩ N ( v ) . foreach c ∈ Nuv do s ← 1/quvqucqvc . τc ← τc + s . τu ← τu + s . τv ← τv + s . end
26 27 end
The following lemma analyzes variance of Mascot C . Lemma 2 . Let u be the true number of local triangles u be its estimation by Mascot C . At any time , share at most one edge , the number of triangle pairs adjacent to u which share an edge becomes
.
Nuv ( Nuv − 1 ) . of u , and τ c for every u ∈ V , ff
Var [ τ c u ] = fi u
1 − p3 p2 − p3
' fi
+ ru p3
'
,
|Nuv| ( |Nuv| − 1 ) , Nu is the set of neigh v∈Nu where ru = bors of u , andN uv = |Nu ∩ Nv| . or not . At any time , the variance of τ c
Proof . Let δλ indicate whether the triangle λ is sampled
Var [ τ c u ] = Var
δλ
=
Cov [ δλ , δγ ]
1 p3
. ⎛ ⎜⎜⎝ .
λ∈Tu
λ∈Tu
=
1 p6 u becomes
1 p6
. . . .
λ∈Tu
γ∈Tu
⎞ ⎟⎟⎠
Var [ δλ ] +
Cov [ δλ , δγ ]
λ∈Tu
γ∈Tu γfi=λ
By definition , Var [ δλ ] = p3 − p6 for any triangle λ . If two triangles λ and γ do not share an edge , Cov [ λ , γ ] = 0 ; if they share one edge , Cov [ λ , γ ] =p 5 − p6 . The number of triangle pairs adjacent to u which share an edge is calculated by examining each edge of u . For each edge e = ( u , v ) , u has
Nuv number of triangles sharing e . Since any two triangles ru =
Thus , .
. v∈Nu fi
'
. p5 − p6
Cov [ δλ , δγ ] = ru
λ∈Tu
γ∈Tu γfi=λ
Since |Tu| = u by definition , the lemma is proved . 3.2 MASCOT A
Mascot A further improves Mascot C by decreasing the variance using a non uniform sampling : if a new edge e = ( u , v ) closes a triangle ( ie , the new edge e increases the number of triangles at least by 1 ) , Mascot A unconditionally samples e . Let Nuv = Nu ∩ Nv be a set of nodes constructing triangles {(u , v , c ) | c ∈ Nuv} where Nu is a set of neighbors of u . By the non uniform sampling , Mascot A increases the triangle count of the nodes u , v and every c ∈ Nuv . Since each edge has a different sampling probability , Mascot A maintains the sampling probability quv ∈ {p , 1} for every sampled edge to give an appropriate weight for each sampled triangle . Mascot A is fully described in Algorithm 2 . Like Mascot C , Mascot A provides unbiased estimation .
688 E [ τ a u ] = u . .
Proof . Let δλ indicate whether the triangle λ is sampled or not . At any time , the expected value of τ a u becomes
E [ τ a u ] =
1
E [ δλ ] ,
λ=(e,f,g)∈Tu qeqf qg where Tu is the set of triangles containing u . Since qe is the probability that the edge e is sampled , the probability of sampling λ becomes qeqf qg , leading to E [ δλ ] = qeqf qg . Consequently , we obtain E [ τ a u ] = |Tu| = u .
For Mascot A , we analyze an upper bound of variance which is smaller than the variance of Mascot C in Lemma 2 . Lemma 4 . Let u be the true number of local triangles u be its estimation by Mascot A . At any time , of u , and τ a for every u ∈ V , u ] ≤ u
Var [ τ a
1 − p2 fi fi p − p2
'
+ ru p2
Proof . In this proof , qλ = qef g = qeqf qg for a triangle λ = ( e , f , g ) . Let δλ indicate whether the triangle λ is sampled or not . At any time , the expected value of τ a u becomes
'
. ffi
Lemma 3 . Let u be the true number of local triangles u be its estimation by Mascot A . At any time , of u , and τ a for every u ∈ V ,
Var [ τ a u ] = Var
.
.
λ∈Tu 1 q2 λ
. . . .
λ∈Tu
γ∈Tu
=
1 qλ
δλ ffl
Cov
δλ qλ
,
δγ qγ
1
Cov [ δλ , δγ ] .
=
λ∈Tu
Var [ δλ ] +
λ∈Tu
γ∈Tu λfi=γ qλqγ
Note that for any sampled triangle λ , qλ ≥ p2 by construc
( 1 ) tion of the algorithm . Thus , the first term of Eq ( 1 ) is bounded as follows : fi qλ − q2
λ fi
' ≤ 1 p2
'
1 − p2
.
( 2 )
1 q2 λ
Var [ δλ ] =
1 q2 λ
The remaining task is to bound the following in the second term :
1 qλqγ
Cov [ δλ , δγ ] =
=
1
( E [ δλδγ ] − E [ δλ ] E [ δγ ] ) qλqγ E [ δλδγ ] qλqγ
− 1 .
Since Cov [ δλ , δγ ] = 0 for independent triangles λ and γ , we focus on the case where λ = ( e , f , g ) andγ = ( g , h , ) share the one edge g . There are two cases :
• g is the last among all the edges of λ and γ : In this case , qg = 1 ; then the following holds :
E [ δλδγ ] qλqγ
= qeqf qhq' qef gqgh'
=
= 1 .
1 q2 g
• g is not the last among all the edges of λ and γ : There are at least two edges ( fl= g ) that are unconditionally sampled . Let them be e and without loss of generality , ie qe = q' = 1 ; then
E [ δλδγ ] qλqγ
= qf qgqh qef gqgh'
=
1 qeqgq'
≤ 1 p
.
( 3 )
Substituting Eq ( 2 ) and ( 3 ) to Eq ( 1 ) , we prove the lemma .
Note that both u and ru are graph characteristics independent of the algorithms . As a result , Mascot A has a lower variance than Mascot C . The main drawback of Mascot A is that the number of edges sampled depends on the order of edges in a stream . It makes hard to exactly analyze the space requirement of Mascot A : the size of sampled edges may vary highly between pm and m . 3.3 MASCOT
Although Mascot A has lower variance than Mascot C , it may sample too many edges . By the following observation , however , we do not need to unconditionally sample an edge even though it constructs a triangle .
Observation 1 . For a new edge e = ( u , v ) , let c be a node having edges to both u and v . In the future , e will not be used for constructing a triangle involving c .
Namely , only u and v need e . Thus , c can safely discard e after counting the triangle , and u and v sample e with the given probability p for the future use . Based on this idea , we propose Mascot as shown in Algorithm 3 .
Algorithm 3 : Mascot ( Memory efficient Accurate Sampling for Counting Local Triangles ) Input : Graph stream S , and 1 G ← ( V , E ) withV = E = ∅ . 2 foreach edge e = ( u , v ) from S do 3 edge sampling probability p .
ReadyN ode(u ) . ReadyN ode(v ) . CountT riangles(e , 1/p2 ) . SampleEdge(e , p ) .
4
5
6 7 end
Mascot provides unbiased estimation as follows . Lemma 5 . Let u be the true number of local triangles of u , and τu be its estimation by Mascot . At any time , for every u ∈ V ,
E [ τu ] = u .
Proof . Every new incoming edge remains in our sampled graph in the future with probability p . We show that every triangle λ = ( e , f , g ) in the graph is counted with probability p2 . Without loss of generality , assume that e , f and g are given in order from the graph stream . Let δλ be an indicator representing whether λ is counted or not . Forλ to be sampled , e and f should be sampled before g whose probability is p2 . In that case , when g is observed , λ is unconditionally counted . Thus , E [ δλ ] =p 2 . With the weight of 1/p2 for sampled edges , the lemma is proved by following the proof of Lemma 3 .
Lemma 6 . Let u be the true number of local triangles of u , and τu be its estimation by Mascot . At any time , for every u ∈ V ,
Var [ τu ] ≤ u
1 − p2 fi
'
+ ru p2 fi p − p2
'
.
689 Table 3 : Summary of the graph data used in our experiments . The number of nodes and edges are counted after removing direction , weights , and selfloops . Name
Edges Description
Nodes
1
2
2
Advogato Enron Wiki Conflict Gowalla Stanford NotreDame 2 BerkStan
2
2
1
5,155 36,692 116,836 196,591 281,903 325,729 685,230
39,285 183,831 2,027,871
Trust network Enron email exchanges Edit confliction
950,327 Online social network
1,992,636 Web graph of Stanford.edu 1,090,108 Web graph of Notre Dame 6,649,470 Web graph of Berkeley and
LiveJournal
2
4,846,609
42,851,237
MovieRev9
2
1
DBLP
253,045
6,611,899
1,314,050
5,362,414
Stanford LiveJournal online social network
Co reviewed movies Amazon Co author DBLP network in in
1http://konectuni koblenzde/
2http://snapstanfordedu/data/indexhtml
Proof . With E [ δλ ] =p 2 , the proof is almost the same as that for Lemma 2 . The only difference is to compute Cov [ δλ , δγ ] . In Mascot , E [ δλδγ ] for two triangles λ , γ ∈ Tu sharing one edge varies depending on the order of edges in a graph stream . If the shared edge of λ and γ is the last or the second last among all edges of λ and γ , E [ δλδγ ] =p 4 ; otherwise E [ δλδγ ] =p 3 . Thus , E [ δλδγ ] ≤ p3 , which proves the lemma .
While the upper bound of variance of Mascot is the same as that of Mascot A , Mascot samples a smaller number of edges than Mascot A , which means that it requires smaller memory spaces for the same p . Moreover , the number of sampled edges of Mascot is easily estimated as pm where m is the number of edges occurring in the stream until currently .
4 . EXPERIMENTS
In this section , we show experimental results on performance of Mascot and comparison with competing methods . Especially , we answer the following questions .
Q1 How accurate is Mascot ? Q2 How better is Mascot compared with the basic ver sions of Mascot and the previous work [ 22 ] ?
Q3 How can Mascot be applied to graph anomaly detec tion ?
4.1 Dataset
We gather graph data from diverse domains such as social networks , router connectivity , hyperlinks in webpages , collaboration networks , citation networks , etc . We make them simple , ie directions , self loops , and weights are removed . Their edges are given in a random order . Table 3 lists the datasets used in our experiments . 4.2 Evaluation Metric
To evaluate local triangle counting algorithms , we con sider the following metrics .
• Pearson Correlation Coefficient ρ : This measures how well the relationship between two variables x and y is
●
MASCOT
●
0 0 1
.
BEST
●
●
● ● ● ● ●
) ρ ( n o i t l a e r r o C
●
●
5 9 0
.
0 9 0
.
5 8 0
.
0 8 0
.
0.1
0.2
) ●
5 9 0
.
ρ ( n o i t l a e r r o C
0 9 0
.
5 8 0
.
0.1
0.2
0.4
0.3 Memory Usage ( η )
0.5
0.6
0.7
0.1
0.2
0.4
0.3
0.5 Memory Usage ( η )
( a ) Enron
( b ) NotreDame
●
0 0 1
.
BEST
●
●
●
● ● ● ● ●
MASCOT−A
● ● ● ●
●
●
MASCOT−C
●
0 0 1
.
BEST
) ρ ( n o i t l a e r r o C
●
●
●
8 9 0
.
6 9 0
.
4 9 0
.
2 9 0
.
0 9 0
.
● ● ● ●
●
●
●
BEST
●
●
●
0 0 1
.
9 9 0
.
8 9 0
.
7 9 0
.
6 9 0
.
5 9 . 0
) ρ ( n o i t l a e r r o C
0.6
0.7
0.6
0.7
0.3
0.5 Memory Usage ( η )
0.4
0.6
0.1
0.2
0.4
0.3
0.5 Memory Usage ( η )
( c ) Gowalla
( d ) LiveJournal
Figure 2 : Pearson correlation coefficient ρ over different ratio η of sampled edges , which dominate the required memory spaces . Mascot shows the best performance . represented by a linear function . Given two vectors x and y , the definition is as follows :
ρ(x , y ) =
Cov [ x , y ]
.
σxσy
• Mean of Error : This measures how close our estimation is to the ground truth . Given an estimation x ∗ , we use the following absolute for the ground truth x relative error :
∗
∗ i i=1 zi ,
) = and x where zi = |xi − x
( x , x | /(x ∗ i + 1 ) . We add 1 to both x for the case that x • Ratio η of Sampled Edges : This dominates the amount of memory spaces required by an algorithm . It is equal to p for Mascot and Mascot C , and larger than p for Mascot A in expectation .
∗ i = 0 . n .
∗
1 n
Note that the first two metrics are calculated for the true and estimated local triangle counts .
We use the average of measurements obtained by 10 independent runnings since our algorithms are randomized . For the competing method KP [ 22 ] , the same averaging scheme is used since the implementation is based on random hashing . 4.3 Performance of MASCOT
Figure 2 shows Pearson correlation coefficients ( PCC ) over ratios of the number of sampled edges for our proposed algorithms . In general , all algorithms improve PCC as the
690 ) ε ( r o r r
E f o n a e M
0
.
1
8
.
0
6
.
0
4
.
0
BEST ● 0.1
0.2
0.4
0.3 Memory Usage ( η )
0.5
0.6
MASCOT
MASCOT−C
MASCOT−A
6
.
0
5
.
0
4
.
0
3
.
0
) ε ( r o r r
E f o n a e M
0.7
2
.
0
BEST ● 0.1
0.2
) ε ( r o r r
E f o n a e M
9
.
0
8
.
0
7
.
0
6
.
0
5
.
0
4
.
0
3
.
0
BEST ● 0.1
0.2
0.6
0.7
0.3
0.5 Memory Usage ( η )
0.4
) ε ( r o r r
E f o n a e M
0
.
1
8
.
0
6
.
0
4
.
0
0.6
BEST ● 0.1
0.2
0.4
0.3
0.5 Memory Usage ( η )
0.6
0.7
0.4
0.3
0.5 Memory Usage ( η )
( a ) Enron
( b ) NotreDame
( c ) Gowalla
( d ) LiveJournal
Figure 3 : Mean of absolute relative error over ratio η of the number of sampled edges of Mascot and the basic versions of Mascot . For the same η , Mascot always results in the lowest error . For all graphs , as expected , standard deviations of Mascot and Mascot A are smaller than that of Mascot C—notable for large graphs like LiveJournal . Sometimes , Mascot C is more accurate than or at least comparable to Mascot A . One reason is that for the same number of the total sampled edges , Mascot A samples large degree nodes more than Mascot C . This results in sacrificing accuracy for many nodes with extremely small degrees . The result with excluding nodes whose degrees are smaller than 2/p is shown in Figure 4 .
●
MASCOT
MASCOT−C
MASCOT−A
) ε ( r o r r
E f o n a e M
4
.
1
2
.
1
0
.
1
8
.
0
●
●
●
6
.
0
4
.
0
BEST ● 0.1
0.2
●
●
● ●
● ●
0.4
0.3 Memory Usage ( η )
0.5
0.6
2
.
1
0
.
1
8
.
0
●
6
.
0
) ε ( r o r r
E f o n a e M
●
●
●
0.7
4
.
0
BEST ● 0.1
0.2
0.6
0.7
BEST ● 0.1
0.2
●
●
●
● ●
0.4
0.3
0.5 Memory Usage ( η )
●
●
●
●
0.3
0.5 Memory Usage ( η )
0.4
6
.
1
4
.
1
2
.
1
0
.
1
8
.
0
6
.
0
) ε ( r o r r
E f o n a e M
●
●
●
●
●
) ε ( r o r r
E f o n a e M
6
.
1
4
.
1
2
.
1
0
.
1
8
.
0
6
.
0
4
.
0
●
●
●
BEST ● 0.1
0.2
0.6
●
●
●
●
●
●
0.4
0.3
0.5 Memory Usage ( η )
0.6
0.7
( a ) Enron
( b ) NotreDame
( c ) Gowalla
( d ) LiveJournal
Figure 4 : Mean of relative error for nodes with degrees larger than 2/p over ratio η of the number of sampled edges of Mascot and its variants . For the same η , sampled edges of Mascot A are more concentrated on nodes having many triangles than those of Mascot C are ; for those nodes the error of Mascot A is smaller than that of Mascot C . sampling rate gets larger . Note that while the sampling rates of Mascot C and Mascot are determined by p in expectation , that of Mascot A depends on both p and the edge order in a graph stream . For all the graphs , Mascot and Mascot A show higher correlations than Mascot C at the same number of sampled edges . The difference between Mascot and Mascot A is insignificant .
Figure 3 shows the mean of absolute relative error over the ratio η of sampled edges . Note that we also present the standard deviation of obtained by 10 runnings , as stated in Section 4.2 , for every point . For all graphs , including those not shown in the figure , Mascot works the best under the same sampling rate . As shown in Section 3 , standard deviations of Mascot and Mascot A are smaller than MascotC—especially remarkable when memory usage ( η ) is small . In contrast to the PCC case , the mean error of MascotC and Mascot A varies depending on graphs . For example , Mascot C outperforms Mascot A for Enron and NotreDame in general , while Mascot A outperforms MascotC for Gowalla ; for LiveJournal they are comparable . The reason of this result is as follows . The triangle estimation is inaccurate for nodes with degrees smaller than 2/p . This is because if a node has a degree less than 2/p , the number of sampled incident edges of the node is less than 2 in expectation , leading to no chance to count its local triangles . For the same η , the edge sampling probability pc for Mascot C is larger than the probability pa for Mascot A , ie 2/pc ≤ 2/pa . As a result , Mascot A becomes accu rate for a small number of large degree nodes , and not for a large number of small degree nodes , leading to large errors in total . This is shown in Figure 4 where the error is calculated for nodes whose degrees are above 2/p . Note that Mascot A always outperforms Mascot C . 4.4 Comparison with Competing Method
441 Competing Method We consider the first and the only local triangle counting algorithm KP for a graph stream , proposed in [ 22 ] . KP generates multiple independent sparsified graphs , and aggregates triangles from them . Since the method requires determining hash functions to map every node to a color before running , we assume that the number of nodes is known in advance . Following their experiments , we set d = 1000 , that
691 t i n e c i f f e o C g n i r e t s u C d e a m t l i t s E
0.12
0.1
0.08
0.06
0.04
0.02
0
●
●
● ●
●
●
●
● t i n e c i f f e o C g n i r e t s u C d e a m t l i t s E
0.05
0.04
0.03
0.02
0.01
0
● ● ● ● ● ●● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ●● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ●
●
● ● ● ● ● ●
● ● ● ● ● ● ● ● ● ● ●● ●
● ● ● ● ● ● ●
●
● ●
● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ●● ● ● ● ●● ● ● ● ●● ● ● ●● ●● ● ● ● ●● ● ●● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ●● ● ● ●● ● ●● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
● ● ●
● ●
● ● ● ● ● ●
●
● ● ●
●
●
●
● ●
● ● ● ●
● ● ●
●
●
●
●
● ●
● ●
●
● ● ● ● ● ● ● ● ● ● ● ● ● ●
●
●
● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●
●
● ● t i n e c i f f e o C g n i r e t s u C d e a m t l i t s E
0.15
0.1
●
●
●
●
● ●
●
●
● ● ● ● ●
● ●
●
● ●
●
●
●
● ●
●
●
● ●
●
● ●
●
●
0.05
● ●
● ●
● ● ● ● ● ● ●
● ●
●
● ●
● ● ● ● ● ● ●
● ● ●● ● ● ● ● ● ● ● ● ●
● ●
● ●
●
●
● ● ● ● ● ●●● ●● ●● ● ● ● ●● ● ● ● ● ● ● ● ● ● ●
● ● ● ● ● ● ● ● ● ● ● ●
● ●
●
●
●
●
● ● ● ● ● ● ● ● ● ● ● ●● ● ●● ● ● ● ● ● ● ● ● ● ● ● ●●● ● ● ● ● ● ●● ● ● ● ● ●●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●
● ●
●
● ● ● ● ● ● ● ● ● ● ●●● ● ● ● ●● ● ● ● ● ●●● ● ● ● ● ● ● ● ● ● ●● ● ● ●● ● ● ● ● ● ●
● ● ●
● ●
0
● ●
●
●
●
●
0
0.02 0.04 0.06 0.08 0.1 0.12 True Clustering Coefficient
0
0.02
0.01 True Clustering Coefficient
0.03
0.04
0.05
( a ) Wiki Conflict by Mascot
( b ) Stanford by Mascot
0
0.05
0.1
0.15
True Clustering Coefficient ( c ) Wiki Conflict by KP t i n e c i f f e o C g n i r e t s u C d e a m t l i t s E
0.1
0.08
0.06
0.04
0.02
●
●
●
●
● ●
●
●
● ●
●
● ●
●
●
● ●
●
●
● ●
●
● ● ● ● ● ●
●
● ● ●
● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●
●
●
● ●
● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●
● ●
● ● ● ● ● ●
●● ●
● ● ●
● ● ●
● ●
● ●
● ●
●
0
●● ● ● ● ● ● ● ● ● ●● ● ● ● ●●● ● ● ● ●● ● ● ●●● ● ●● ● ●● ● ● ● ● ● ● ● ● ●●● ● ● ● ●● ● ● ● ● ●● ●● ● ● ● ● ● ● ● ● ● ●●● ●●●
● ●
● ●
● ●●
0
0.06
0.04
0.02 True Clustering Coefficient ( d ) Stanford by KP
0.08
0.1
Figure 5 : The true clustering coefficient vs . its estimation by Mascot with p = 0.3—(a ) and ( b)—and by KP with K = 80—(c ) and ( d ) . In this setting , both algorithms sample similar numbers of edges : 0.3m and 0.32m in expectation for Mascot and KP , respectively . All plots are with respect to nodes having degrees at least 1000 . Note that Mascot estimates local triangles more accurately than KP—the points of Mascot are nearly on the y = x line in contrast to those of KP .
●
MASCOT
MASCOT−C
MASCOT−A
KP
●
0
.
1
BEST
●
●
●
●
) ρ ( n o i t l a e r r o C
● ● ● ● ● ●
●
0
.
1
BEST ●
●
●
9
.
0
8
.
0
7
.
0
6
.
0
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
Memory Usage ( η )
) ρ ( n o i t l a e r r o C
9
.
0
8
.
0
7
.
0
6
.
0
● ● ● ● ●
5 1 0 0
.
0 1 0 0
.
5 0 0 0
.
) ε ( r o r r
E f o n a e M
●
●
●
● ●
0.1
0.2
0.4
0.3 0.6 Memory Usage ( η )
0.5
0.7
● ● ● ●
BEST ● 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
Memory Usage ( η )
) ε ( r o r r
E f o n a e M
0 1 0 0
.
6 0 0 0
.
2 0 0 0
.
●
●
●
●
●
●
● ● ●
BEST ● 0.1 0.2
0.4
0.3 0.6 Memory Usage ( η )
0.5
0.7
( a ) Wiki Conflict
( b ) Stanford
( c ) Wiki Conflict
( d ) Stanford
Figure 6 : Pearson correlation coefficient ρ and mean of absolute relative error for the nodes with degrees at least 1000 . While Mascot is the best , Mascot C and Mascot A also outperform KP in terms of both metrics ρ and . is , our focus is on nodes whose degrees are at least d . The other parameters are set according to Theorem 2 in their paper—the number C of colors to d/4 = 250 and the sparsification threshold t to 9m/d = 0.009m—while the number K of independent sessions sparsifying an input graph is varied from 50 to 150 at the interval of 10 . Note that the number of edges sampled becomes ≈ mK/C . 442 Comparison Setup . We compare Mascot with KP for estimation of local clustering coefficients of nodes with degrees larger than 1000 . Note that KP was primarily developed for local clustering coefficient estimation , and Mascot provides it by dividing τu by du(du − 1)/2 for each node u since τ is an unbiased estimator1 . Calculating the degree of each node in a graph stream is trivial , ie counting for each node its occurrences in the stream . For this experiment , we use the following three graphs : Stanford , Wiki Conflict , and BerkStan .
Result . Figures 1b , 1c and 5 show how well two methods Mascot and KP estimate local clustering coefficients for the top 1000 high degree nodes . Note that all points for Mascot are nearly on the y = x line while KP ’s estimations are not that accurate despite somewhat positive 1This is the same for Mascot C and Mascot A . correlations . Figure 6 shows comparison results of Mascot and KP with respect to ρ and . For both graphs , Mascot shows a high Pearson correlation coefficient ρ even with a small ratio η of sampled edges while KP ’s PCCs are below 0.8 regardless of the number of sampled edges . Furthermore Mascot outperforms in mean and standard deviation .
Additionally , we compare Mascot with the semi streaming algorithm ( BA ) proposed in [ 8 ] for local triangle counting , which requires multiple passes to a graph . We set the number of passes to 50 and the number of random bits to log(n ) for BA . We use the four graphs Advogato , Wiki Conflict , BerkStan , and LiveJournal . In general , Mascot is faster than BA , with larger absolute relative errors . However , for the top 1 % high degree nodes , Mascot is more accurate than BA in most cases . Furthermore , the correlation coefficient of Mascot is better than BA in most cases .
4.5 Anomaly Detection in Graph Streams
It has been known that local triangle counts play an important role in determining characteristics of nodes [ 2 , 8 ] . In this section , we show that our proposed algorithm Mascot detects anomalous nodes or patterns in real world graph streams . We use the Stanford , MovieRev9 and DBLP graphs listed in Table 3 .
692 5
.
1
0
.
1
5
.
0
0
.
0
Group Density Common Neighbors Density ( Group Size ) / ( Comm . Neighbors Size )
Group A ( 1.00 )
Group B ( 0.81 )
Group C
( 0.72 )
( a ) Stanford
( b ) Stanford
( c ) MovieRev9
( d ) DBLP
Figure 7 : Anomaly detection results for Stanford , Movie9 and DBLP . ( a ) Each color group forms a near clique , which is observed as a short vertical line in the scatter plot . The nodes in the group have a large number of common neighbors whose connections are sparse . The size of the common neighbors is much larger than that of the group such that the two groups form a core periphery . ( b ) Bar graph showing the internal densities of each discovered group and their common neighbor group . Note that the discovered group has an edge density larger than 0.95 while the corresponding neighbor group is rarely connected . The value below each group means the ratio of common neighbors over all neighbors of the group members . ( c ) Each color group corresponds to a movie series which can be favored by people with diverse preferences : classic movies ( green , blue and purple ) and religious ones ( red ) . ( d ) The steep linear pattern in red corresponds to researchers who participate in at least one paper with many coauthors : they are close to the clique line . The gradual linear pattern in blue corresponds to popular names . Since people with the same name becomes one point , the point covers various domains , leading to a large degree but weak local cohesiveness .
Setup . We use Mascot with p = 03 Stanford is a hyperlink network of webpages : a node and an edge correspond to a web page and a hyperlink , respectively . In analysis of Stanford , we focus on structural anomaly since the graph is unlabeled . MovieRev9 is originally given as a list of movie reviews , containing the information of products and users , of Amazon . Each node of MovieRev9 corresponds to a product of the reviews ; we make an undirected edge if two products have at least 9 common reviewers . DBLP is a collaboration network where there is an undirected edge between two authors if they participate in the same work . For all the graphs , we remove duplicated edges and edge directions .
Result . Following the strategy used in [ 2 , 20 ] , we especially focus on relation between degrees and local triangle counts of nodes . There are two types of patterns of interest : a group of anomalous nodes with similar characteristics and a node far from a general pattern in the degree triangles scatter plot .
Observation 2
( Core periphery in Web ) . In Stanford , there are core peripheries that can be divided into two subgroups : the first subgroup is a small dense graph and the other is a large sparse graph . The two subgroups are tightly connected .
Figure 7a shows the result of discovering anomalous structures in the Stanford web graph . In the scatter plot of the degree and the local triangle count for the nodes , we observe several near clique structures shown as short vertical lines in the plot . They are not only tightly connected to each other but also share a large portion of neighbors outside the group . The number of those neighbors are relatively large and they are sparsely interconnected . As a result , each group and its neighbors form a core periphery . Figure 7b shows the edge densities of the discovered groups and their neighbor groups , and the ratios of the group sizes over the neighbor sizes . The three groups that we discover show similar patterns .
Observation 3
( Broad Popularity of Movies ) . In MovieRev9 , several sets of movies are loved by many users of various tastes .
Figure 7c shows anomalous groups of nodes discovered in MovieRev9 . The red group corresponds to religious films like “ Holy Night ” and “ Return to Nazareth ” ; all of them are made with the same actors , writers and producers . Such movies can be preferred by various people believing in that religion regardless of their movie preferences , resulting in small triangles compared with degrees . The green , blue and purple groups correspond to classic movies , which are released in various forms and reissued until recently—the green for a series of “ Planet of the Apes ” , the blue for “ Casablanca ” , and the purple for “ You Only Live Twice ” . Since they have been loved a long period of time , regardless of preferences , many people with diverse spectra in their personalities watch those movies . As a result , those movies form less tightly connected ego networks .
Observation 4
( Papers by Many Coauthors ) . In DBLP , there is a group of authors each of which participates in at least one paper with many coauthors , forming a small tightly connected group .
Observation 5
( Ambiguity by Common Names ) . In DBLP , there are groups of very active researchers each of which corresponds to people having a same name .
Figure 7d shows two linear patterns discovered in the coauthorship relations of DBLP . The first pattern is formed by the red and green groups which correspond to authors whose coauthors are tightly connected . We observe that this is due to papers written by a large number of coauthors . Especially , the green group contains 85 authors who participate in one paper written by 119 authors ; some of them have only one publication .
693 The second linear pattern marked in blue is observed on the area of large degrees and small local triangles . Those correspond to authors actively collaborating with other researchers , but we observe that each of them consists of a collection of a common name . Most of them are Chinese : eg the point with the largest degree is a result with many researchers whose names are “ Wei Wang ” expressed in 8 different words in Chinese [ 33 ] . 5 . CONCLUSION
In this paper , we propose Mascot , a local triangle counting algorithm in a graph stream . The main contributions are as follows . • Algorithm . We propose a one pass local triangle counting algorithm Mascot . Mascot improves two basic algorithms Mascot C and Mascot A to provide both accuracy and memory efficiency . In contrast to the previous algorithm [ 22 ] , Mascot requires only one parameter of edge sampling probability and no prior knowledge on an input graph . • Performance . Experimental results show that Mascot is the best among our proposed algorithms and outperforms the existing one . • Discovery . Applying Mascot to real graphs , we discover anomalous patterns like the core periphery structure in Web , and ambiguity of author names in DBLP .
Our future work includes developing streaming algorithms to solve various graph mining problems such as subgraph matching and graph partitioning . 6 . REFERENCES [ 1 ] N . K . Ahmed , N . Duffield , J . Neville , and R . Kompella .
Graph sample and hold : A framework for big graph analytics . In KDD , 2014 .
[ 15 ] J P Eckmann and E . Moses . Curvature of co links uncovers hidden thematic layers in the World Wide Web . PNAS , 99(9):5825–5829 , 2002 .
[ 16 ] X . Hu , Y . Tao , and C W Chung . Massive graph triangulation . In SIGMOD , 2013 .
[ 17 ] M . Jha , C . Seshadhri , and A . Pinar . A space efficient streaming algorithm for triangle counting using the birthday paradox . In KDD , 2013 .
[ 18 ] H . Jowhari and M . Ghodsi . New streaming algorithms for counting triangles in graphs . In COCOON , 2005 .
[ 19 ] D . M . Kane , K . Mehlhorn , T . Sauerwald , and H . Sun .
Counting arbitrary subgraphs in data streams . In ICALP , 2012 .
[ 20 ] U . Kang , B . Meeder , E . Papalexakis , and C . Faloutsos .
Heigen : Spectral analysis for billion scale graphs . TKDE , 26(2):350–362 , February 2014 .
[ 21 ] J . Kim , W S Han , S . Lee , K . Park , and H . Yu . Opt : A new framework for overlapped and parallel triangulation in large scale graphs . In SIGMOD , 2014 .
[ 22 ] K . Kutzkov and R . Pagh . On the streaming complexity of computing local clustering coefficients . In WSDM , 2013 . [ 23 ] M . Latapy . Main memory triangle computations for very large ( sparse ( power law ) ) graphs . Theor . Comput . Sci . , 407(1 3):458–473 , 2008 .
[ 24 ] R . Milo , S . Shen Orr , S . Itzkovitz , N . Kashtan ,
D . Chklovskii , and U . Alon . Network motifs : Simple building blocks of complex networks . Science , 298(5594):824–827 , 2002 .
[ 25 ] J . Nishimura and J . Ugander . Restreaming graph partitioning : simple versatile algorithms for advanced balancing . In KDD , 2013 .
[ 26 ] R . Pagh and F . Silvestri . The input/output complexity of triangle enumeration . In PODS , 2014 .
[ 27 ] R . Pagh and C . E . Tsourakakis . Colorful triangle counting and a mapreduce implementation . Inf . Process . Lett . , 112(7):277–281 , 2012 .
[ 28 ] H M Park and C W Chung . An efficient mapreduce algorithm for counting triangles in a very large graph . In CIKM , 2013 .
[ 2 ] L . Akoglu , M . McGlohon , and C . Faloutsos . oddball :
[ 29 ] H M Park , F . Silvestri , U . Kang , and R . Pagh . Mapreduce
Spotting anomalies in weighted graphs . In PAKDD , 2010 . triangle enumeration with guarantees . In CIKM , 2014 .
[ 3 ] N . Alon , R . Yuster , and U . Zwick . Finding and counting
[ 30 ] A . Pavan , K . Tangwongsan , S . Tirthapura , and K L Wu . given length cycles . Algorithmica , 17(3):209–223 , 1997 .
[ 4 ] S . Arifuzzaman , M . Khan , and M . V . Marathe . Patric : a parallel algorithm for counting triangles in massive networks . In CIKM , 2013 .
Counting and sampling triangles from a graph stream . Proc . VLDB Endow . , 6(14):1870–1881 , Sept . 2013 .
[ 31 ] A . D . Sarma , S . Gollapudi , and R . Panigrahy . Estimating pagerank on graph streams . J . ACM , 58(3):13 , 2011 .
[ 5 ] B . Bahmani , A . Chowdhury , and A . Goel . Fast incremental
[ 32 ] T . Schank and D . Wagner . Finding , counting and listing all and personalized pagerank . PVLDB , 4(3):173–184 , 2010 .
[ 6 ] B . Bahmani , R . Kumar , M . Mahdian , and E . Upfal .
Pagerank on an evolving graph . In KDD , 2012 . triangles in large graphs , an experimental study . In WEA , 2005 .
[ 33 ] G . D . Sprouse . Editorial : Which Wei Wang ? Physical
[ 7 ] Z . Bar Yossef , R . Kumar , and D . Sivakumar . Reductions in
Review Letters , 99(23):230001 , 2007 . streaming algorithms , with an application to counting triangles in graphs . In SODA , 2002 .
[ 34 ] I . Stanton . Streaming balanced graph partitioning for random graphs . In SODA , 2014 .
[ 8 ] L . Becchetti , P . Boldi , C . Castillo , and A . Gionis . Efficient
[ 35 ] I . Stanton and G . Kliot . Streaming graph partitioning for algorithms for large scale local triangle counting . TKDD , 4(3 ) , 2010 . large distributed graphs . In KDD , 2012 .
[ 36 ] S . Suri and S . Vassilvitskii . Counting triangles and the
[ 9 ] J . W . Berry , B . Hendrickson , R . A . LaViolette , and C . A . curse of the last reducer . In WWW , 2011 .
Phillips . Tolerating the community detection resolution limit with edge weighting . Phys . Rev . E , 83:056119 , 2011 .
[ 10 ] B . H . Bloom . Space/time trade offs in hash coding with allowable errors . Commun . ACM , 13(7 ) , 1970 .
[ 11 ] L . S . Buriol , G . Frahling , S . Leonardi ,
A . Marchetti Spaccamela , and C . Sohler . Counting triangles in data streams . In PODS , 2006 .
[ 12 ] S . Chu and J . Cheng . Triangle listing in massive networks and its applications . In KDD , 2011 .
[ 13 ] J . Cohen . Graph twiddling in a mapreduce world .
Computing in Science and Engineering , 11(4):29–41 , 2009 .
[ 14 ] P . K . Desikan , N . Pathak , J . Srivastava , and V . Kumar .
Incremental page rank computation on evolving graphs . In WWW ( Special interest tracks and posters ) , 2005 .
[ 37 ] C . E . Tsourakakis , C . Gkantsidis , B . Radunovic , and
M . Vojnovic . Fennel : Streaming graph partitioning for massive scale graphs . In WSDM , 2014 .
[ 38 ] C . E . Tsourakakis , U . Kang , G . L . Miller , and C . Faloutsos .
Doulion : counting triangles in massive graphs with a coin . In KDD , 2009 .
[ 39 ] H . T . Welser , E . Gleave , D . Fisher , and M . Smith .
Visualizing the signatures of social roles in online discussion groups . The Journal of Social Structure , 8(2 ) , 2007 .
[ 40 ] Z . Yang , C . Wilson , X . Wang , T . Gao , B . Y . Zhao , and Y . Dai . Uncovering social network sybils in the wild . In Internet Measurement Conference , 2011 .
694
