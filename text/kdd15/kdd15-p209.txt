Unsupervised Feature Selection with
Adaptive Structure Learning
State Key Laboratory of Computer Science , Institute of Software , Chinese Academy of
State Key Laboratory of Computer Science , Institute of Software , Chinese Academy of
Liang Du
Sciences
Yi Dong Shen
Sciences ydshen@iosaccn
School of Computer and Information Technology ,
Shanxi University duliang@iosaccn
ABSTRACT The problem of feature selection has raised considerable interests in the past decade . Traditional unsupervised methods select the features which can faithfully preserve the intrinsic structures of data , where the intrinsic structures are estimated using all the input features of data . However , the estimated intrinsic structures are unreliable/inaccurate when the redundant and noisy features are not removed . Therefore , we face a dilemma here : one need the true structures of data to identify the informative features , and one need the informative features to accurately estimate the true structures of data . To address this , we propose a unified learning framework which performs structure learning and feature selection simultaneously . The structures are adaptively learned from the results of feature selection , and the informative features are reselected to preserve the refined structures of data . By leveraging the interactions between these two essential tasks , we are able to capture accurate structures and select more informative features . Experimental results on many benchmark data sets demonstrate that the proposed method outperforms many state of the art unsupervised feature selection methods .
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications— Data Mining
General Terms Algorithms
Keywords unsupervised feature selection ; adaptive structure learning
1 .
INTRODUCTION
Real world applications usually involve big data with high dimensionality , presenting great challenges such as the curse of dimensionality , huge computation and storage cost . To tackle these difficulties , feature selection techniques are developed to keep a few relevant and informative features . According to the availability of label information , these algorithms can be categorized into supervised [ 25 ] , [ 22 ] , [ 20 ] , [ 17 ] , semi supervised [ 33 ] , [ 27 ] and unsupervised algorithms [ 6 ] , [ 4 ] . Compared to supervised or semi supervised counterparts , unsupervised feature selection is generally more challenging due to the lack of supervised information to guild the search of relevant features .
Unsupervised feature selection has attracted much attention in recent years and a number of algorithms have been proposed [ 8 , 4 , 36 , 28 , 16 ] . Without class label , unsupervised feature selection chooses features that can effectively reveal or maintain the underlying structure of data . Recent research on feature selection and dimension reduction has witnessed that several important structures should be preserved by the selected features . These important structures include , but not limited to , the global structure [ 36 , 16 ] , the local manifold structure [ 9 , 10 ] and the discriminative information [ 28 , 14 ] . And these structures can be captured by widely used models in the form of graph , such as , the sample pairwise similarity graph [ 36 ] , the k nn graph [ 8 ] , the global integration of local discriminant model [ 28 , 31 ] , the local linear embedding ( LLE ) [ 16 ] .
Clearly , many of existing unsupervised feature selection methods rely on the structure characterization through some kind of graph , which can be computed within the original feature space . And once the graph is determined , it is fixed in the next procedures , such as sparse spectral regression [ 3 ] , to guide the search of informative features . Consequently , the performance of feature selection is largely determined by the effectiveness of graph construction . Ideally , such graphs should be constructed only using the informative feature subset rather than all candidate features . Unfortunately , the desired subset of features is unknown in advance , and the irrelevant or noisy features would be inevitably introduced in many real applications . As a result , unrelated or noisy features will have an adverse effect on the characterization of the structures and henceforth hurt the following feature selection performance .
In unsupervised scenario , this is actually the chicken andegg problem between structure characterization and feature selection . Facing with such dilemma , we propose to perform
209 Figure 1 : An illustration of unsupervised filter methods and four type embedded methods . feature selection and structure learning in a unified framework , where each sub task can be iteratively boosted by using the result of the other one . Concretely , the global structure of data is captured within the sparse representation framework , where the reconstruction coefficient is learned from the selected features . The local manifold structure is revealed by a probabilistic neighborhood graph , where the pairwise relationship is also determined by the selected features . When the global and local structures are given in the form of graph Laplacians , we seek the relevant features via sparse spectral regression with the help of graph embedding for cluster analysis . In this way , both the global and local structure of data can be better captured by only using the selected features ; Moreover , with the refined characterization of the structure , a better search of the informative features could also be expected .
It is worthwhile to highlight several aspects of the pro posed approach here
1 . Based on the different learning paradigms for unsupervised feature selection , we investigate most of existing unsupervised embedded methods and further classify them into four closely related but different types . These analyses provide more insight into what should be further emphasized on the development of more essential unsupervised feature selection algorithm .
2 . We propose a novel unified learning framework , called unsupervised Feature Selection with Adaptive Structure Learning ( FSASL in short ) , to fulfil the gap between two essential sub tasks , ie structure learning and feature learning . In this way , these two tasks can be mutually improved .
3 . Comprehensive experiments on benchmark data sets show that our method achieves statistically significant improvement over state of the art feature selection methods , suggesting the effectiveness of the proposed method .
2 . RELATED WORKS
In this section , we mainly review most existing unsupervised feature selection methods , ie filter and embedded methods . Unsupervised filter methods pick the features one by one based on certain evaluation criteria , where no learning algorithm is involved . The typical methods include : max variance ( MaxVar ) [ 12 ] , Laplacian score ( LapScore ) [ 8 ] , spectral feature selection ( SPEC ) [ 34 ] , feature selection via eigenvalue sensitive criterion ( EVSC ) [ 4 ] . A common limitation of these approaches is the correlation among features is neglected [ 1 ] .
Unsupervised embedded approaches are developed to perform feature selection and fit a learning model simultaneously . Based on the different sub steps involved in the feature selection procedure , these embedded methods can be further divided into four different types as illustrated in Figure 1 .
The first type of embedded methods first detect the structure of the data and then directly select those features which is used to best preserve the enclosed structure . The typical methods include : trace ratio ( TraceRatio ) [ 19 ] and unsupervised discriminative feature selection ( UDFS ) [ 28 ] . TraceRatio is prone to select redundant features [ 16 ] and the learning model of UDFS is often too restrictive [ 21 ] .
The second type of embedded methods first construct various graph Laplacians to capture the data structure , then flat the cluster structure via graph embedding , and finally use the sparse spectral regression [ 3 ] to select those features that are best aligned to the embedding . Instead of directly selecting features as the first type , these approaches resorted to an intermediate cluster analysis sub step to reveal the cluster structure for guiding the selection of features . The cluster structure discovered by either the graph spectral embedding or other clustering module can be seen as an approximation of the unseen labels . The typical methods include : multicluster feature selection ( MCFS ) [ 4 ] , minimum redundancy spectral feature selection ( MRSF ) [ 35 ] , similarity preserving feature selection ( SPFS ) [ 36 ] , and joint feature selection and subspace learning ( FSSL ) [ 7 ] , global and local structure preserving feature selection ( GLSPFS ) [ 16 ] .
Unlike the second type methods , the clustering analysis in the third type of embedded methods is co determined by the embedding of the graph Laplacian and the adaptive discriminative regularization [ 29 ] , [ 31 ] , which can be obtained from the result of sparse spectral regression . By using the feedback from feature selection , the whole learn
Structure learningwith All featuresRanking criteriaFilterMaxVar , LapScore , SPEC , EVSCFeature searchMethod typeTypical algorithmA learning modelEmbedded Type ITraceRatio , UDFSMCFS , MRFS , SPFS,FSSL , GLSPFSJELSR , NDFS,RUFS , CGSSLLLCFS , FSASLIntermediate AnalysisClusteringClusteringClusteringEmbedded Type IIEmbedded Type IIIEmbedded Type IVA learning modelA learning modelA learning modelStructurecharacterizationStructure learningwith All featuresStructure learningwith All featuresStructure learningwith All featuresAdaptive structure learningwith selected features210 ing procedure can provide better cluster analysis , and vice versa . The typical methods include : joint embedding learning and spectral regression ( JELSR ) [ 11 ] , [ 10 ] , nonnegative discriminative feature selection ( NDFS ) [ 14 ] , robust unsupervised feature selection ( RUFS ) [ 21 ] , feature selection via clustering guided sparse structural learning ( CGSSL ) [ 13 ] .
The fourth type of embedded methods try to feed the result of feature selection into the structure learning procedure for improving the quality of structure learning . In [ 32 ] , a feature selection method is proposed for local learningbased clustering ( LLCFS ) , which incorporates the relevance of each feature into the built in regularization of the local learning model , where the induced graph Laplacian can be iteratively updated . However , LLCFS uses the discrete knearest neighbor graph and does not optimize the same objective function in structure learning and feature search .
It can be seen that all these above methods ( except LLCFS ) share a common drawback : they use all features to estimate the underlying structures of data . Since the redundant and noisy features are unavoidable in real world applications , that is also why we need feature selection , the learned structures using all features will also be contaminated , which would degrade the performance of feature selection . By leveraging the coherent interactions between structure learning and feature selection , our proposed method seamlessly integrates them into a unified framework , where the result of one task is used to improve the other one .
3 . UNSUPERVISED FEATURE SELECTION WITH ADAPTIVE STRUCTURE LEARNING
Let X = {x1 , , xn} ∈ Rd×n denotes the data matrix , whose columns correspond to data instances and rows to features . The generic problem of unsupervised feature selection is to find the most informative features . With the absence of class label to guild the search of relevant features , the data represented with the selected features should well preserve the intrinsic structure as the data represented by all the original features .
To achieve this goal , we propose to jointly perform unsupervised feature selection and data structure learning simultaneously , where both global and local structure are adaptively updated using the result of current feature selection . We first summarize some notations used throughout this paper . We use bold uppercase characters to denote matrices , bold lowercase characters to denote vectors . For an arbitrary matrix A ∈ Rr×t , ai means the i th column vector of A and aT j means the j th row vector of A , Aij denotes the ( i , j) th entry of A . The 2,1 norm is defined as ||A||21 = r t i=1 j=1 A2 ij .
3.1 Adaptive Global Structure Learning
Over the past decades , a large number of algorithms have been proposed based on the analysis of the global structure of data , such as the Principal Component Analysis ( PCA ) and the Maximum Variance ( MaxVar ) . Recently , the global pairwise similarity ( eg , with a Gaussian kernel ) between high dimensional samples has demonstrated promising performance for unsupervised feature selection [ 36 , 16 ] . However , such dense similarity becomes less discriminative for high dimension data , especially when there are many unfavorable features in the original high dimensional space .
Inspired by the recent development on compressed sensing and sparse representation [ 26 ] , we use the sparse reconstruction coefficients to extract the global structure of data . In sparse representation , each data sample xi can be approximated as a linear combination of all the other samples , and the optimal sparse combination weight matrix S ∈ Rn×n can be obtained by solving the following problem n i=1 min
S
,||xi − Xsi||2 + α||si||1 st Sii = 0
( 1 ) where α is used to balancing the sparsity and the reconstruction error . Compared with the pairwise similarity , the sparse representation is naturally discriminative : among all the candidates samples , it selects the samples which most compactly expresses the target and rejects all other possible but less compact candidates [ 26 ] .
Clearly , the selected features should preserve such global and sparse reconstruction structure . To achieve this , we introduce a row sparse feature selection and transformation matrix W ∈ Rd×c to the reconstruction process , and get
||WT xi − WT Xsi||2 + α||S||1 + γ||W||21
( 2 ) n i=1 min S,W st Sii = 0 , WT XXT W = I where γ is regularization parameter . Compared with the Eq ( 1 ) , the benefits of Eq ( 2 ) are two folds : 1 ) The global structure captured by S can be used to guide the search of relevant features ; 2 ) By largely eliminating the adverse effect of noisy and unfavorable features , the global structure can also be better estimated . 3.2 Adaptive Local Structure Learning
The importance of preserving local manifold structure has been well recognized in the recent development of unsupervised feature selection algorithms , especially considering that high dimensional data often presents a low dimensional manifold structure [ 8 , 4 , 16 ] . To detect the underlying local manifold structure , these algorithms usually first construct a k nearest neighbor graph and then compute the graph Laplacian with different models . Clearly , both the knn graph and the graph Laplacian are determined by all the relevant and irrelevant features . As a result , the manifold structure captured by such graph Laplacian would be inevitably affected by the redundant and noisy features . Moreover , the iterative updating of discrete neighborhood relationship using the result of feature selection still suffers from the lack of theoretical guarantee of its convergence [ 32 , 24 ] . Instead of using the graph Laplacian with the determinate neighborhood relationship , we introduce to directly learn a euclidean distance induced probabilistic neighborhood matrix [ 18 ] . For each data sample xi , all the data points {xj}n j=1 are considered as the neighborhood of xi with probability Pij , where P ∈ Rn×n can be determined by solving the following problem :
( ||xi − xj||2
2Pij + µP2 ij ) , st P1n = 1n , P ≥ 0 ( 3 ) i,j min
P where µ is the regularization parameter . The regularization term is used to 1 ) avoid the trivial solution ; 2 ) add a prior
211 It can be found that a large disof uniform distribution . tance ||xi − xj||2 2 will lead to a small probability Pij . With such nice property , the estimated weight matrix P and the induced Laplacian LP = DP − ( P + PT )/2 can be used for local manifold characterization , where DP is a diagonal matrix whose i th diagonal element is j(Pij + Pji)/2 .
To leverage the result of feature selection and iteratively improve the probabilistic neighborhood relationship , we also introduce the feature selection and transformation matrix W as used in global structure adaptive learning , and we get
( ||WT xi − WT xj||2
2Pij + µP2 ij ) + γ||W||21 ( 4 ) n i,j min P,W st P1n = 1n , P ≥ 0 , WT XXT W = I
With the sparsity on W , the irrelevant and noisy features can be largely removed , thus we can learn a better probabilistic neighborhood graph for local structure characterization based on the result of feature selection , ie WT X . Moreover , we aim to seek those features to preserve the local structure encoded by P . Thus , the optimization problem in Eq ( 4 ) can be used to perform feature selection and local structure learning , simultaneously . 3.3 Unsupervised Feature Selection with Adap tive Structure Learning
Based on the two adaptive structure learning models presented in Eq ( 2 ) and Eq ( 4 ) , we propose a novel unsupervised feature selection method by solving the following optimization problem ,
||WT X − WT XS||2 + α||S||1 n
||WT xi − WT xj||2Pij + µP2 ij
+ β
( 5 )
+ γ||W||21 min W,S,P i,j st Sii = 0 , P1n = 1n , P ≥ 0 , WT XXT W = I where β and γ are regularization parameters balancing the fitting error of global and local structure learning in the first and second group and the sparsity of the feature selection matrix in the third group .
It can be seen that when both S and P are given , our method selects those features to well respect both the global and local structure of data . When the feature selection matrix W is given , our method learns the global and local structure of data in a transformed space , ie WT X , where the adverse effect of noisy features is largely alleviated with sparse regularization . In this way , these two essential tasks can be boosted by the other one within a unified learning framework . Since both the global and local structure can be adaptively refined according to the result of feature selection , we call Eq ( 5 ) unsupervised Feature Selection with Adaptive Structure Learning ( FSASL ) . 3.4 Optimization Algorithm
Because the optimization problem in Eq ( 5 ) comprises three different variables with different regularizations and constraints , it is hard to derive its closed solution directly . Thus we derive an alternative iterative algorithm to solve the problem , which converts the problem with a couple of variables ( S , P and WT ) into a series of sub problems where only one variable is involved . n
First , when W and P are fixed , we need to solve n decou pled sub problems in the following form :
||x i − X si||2 + α|si| , min si st Sii = 0
( 6 ) is the new transformed data by projecting the relwhere X = WT X . evant features into a low dimension space , and X The above LASSO problem can be efficiently solved by routine optimization tools , eg proximal methods [ 2 , 15 ] .
Next , when WT and S are fixed , we need to solve n de coupled sub problems in the following form :
||x j||2Pij + µ||Pij||2 , i − x
( 7 ) min pT i j=1 st 1T n pi = 1 , Pij ≥ 0
2µ||x Denote A ∈ Rn×n be a square matrix with Aij = − 1 j||2 , then the above problem can be written as follows x ij ≤ 1 i 1n = 1 , 0 ≤ pT i − aT st pT i ||2 ,
||pT i−
( 8 ) min pT i
1 2 where pT is the i th row of P . The above euclidean projeci tion of a vector onto the probability simplex can be efficiently solved by Algorithm 1 without iterations . More details can be found in Eq ( 19 ) .
Algorithm 1 The optimization algorithm of Eq ( 8 )
Input : a sort a into b where b1 ≥ b2 ≥ , , bn find ρ = max{1 ≤ j ≤ n : bj + 1 define z = 1
ρ ( 1 −ρ i=1 bi )
Output : p with pj = max{aj + z , 0} , j = 1 , , n j ( 1 −j i=1 bi ) > 0}
Next , when S and P are fixed , we need to solve the fol lowing problem :
||WT X − WT XS||2 + β min W n i,j
||WT xi − WT xj||2Pij + γ||W||21 st WT XXT W = I ( 9 ) Using LS = ( I − S)(I − S)T , LP = DP − ( P + PT )/2 and let L = LS + βLP , the above problem can be rewritten as
T r(WT XLXT WT ) + γ||W||21 min W st WT XXT W = I
( 10 )
Due to the non smooth regularization , it is hard to obtain the close form solution . We solve it in an iterative way . Given the t th estimation Wt and denote DWt be a diagonal matrix with the i th diagonal element as i||2 , Eq ( 10 ) can be rewritten as :
1 2||wt
WT X(L + γDWt )XT W
( 11 )
T r min W st WT XXT W = I
The optimal solution of W are the eigenvectors corresponding to the c smallest eigenvalues of generalized eigen problem :
X(L + γDWt )XT W = ΛXXT W
( 12 )
212 where Λ is a diagonal matrix whose diagonal elements are eigenvalues . To get a stable solution of this eigen problem , the matrices XXT is required to be non singular which is not true when the number of features is larger than the number of samples . Moreover , the computational complexity of this approach scales as O(d3 + nd2 ) , which is costly for high dimensional data . Thus , such solution is less attractive in real world applications . To improve the effectiveness and the efficiency to optimize Eq ( 10 ) , we further resort to a two steps procedure inspired from [ 3 ] .
Theorem 1 . Let Y ∈ Rn×c be a matrix of which each If column is an eigenvector of eigen problem Ly = λy . there exists a matrix W ∈ Rd×c such that XT W = Y , then each column of W is an eigenvector of the generalized eigenproblem XLXT w = λXXT w with the same eigenvalue λ .
Proof . With XT W = Y , the following equation holds
XLXT w = XLy = Xλy = λXy = λXXT w
( 13 )
Thus , y is the eigenvector of the generalized eigen problem XLXT w = λXXT w with the same eigenvalue λ .
Theorem 1 shows that instead of solving the generalized eigen problem in Eq ( 12 ) , W can be obtained by the following two steps :
1 . Solve the eigen problem LY = ΛY to get Y corre sponding to the c smallest eigenvalues ;
2 . Find W which satisfies XT W = Y . Since such W may not exist in real applications , we resort to solve the following optimization problem :
||Y − XT W||2 + γ||W||21
( 14 ) min W
The optimal solution of Eq ( 14 ) can also be obtained from routine optimization tools , such as the iterative re weighted method and the proximal method [ 15 ] .
The complete algorithm to solve FSASL is summarized in algorithm 2 .
Algorithm 2 The optimization algorithm of FSASL Input : The data matrix X ∈ Rd×n , the regularization parameters α , β , γ , µ , the dimension of the transformed data c . repeat
For each i , update the i th column of S by solving the problem in Eq ( 6 ) ; For each i , update the i th row of P using Algorithm 1 ; Compute the overall graph Laplacian L = LS + βLP ; Compute W by Eq ( 12 ) or Eq ( 14 ) ; until Converges
Output : Sort all the d features according to ||wi||2(i = 1 , , d ) in descending order and select the top m ranked features .
3.5 Convergence Analysis
FSASL is solved in an alternative way , the optimization procedure will monotonically decrease the objective of the problem in Eq ( 5 ) in each iteration . Since the objective function has lower bounds , such as zero , the above iteration converges . Besides , the experimental results show that it converges fast , the time of iteration is often less than 20 .
3.6 The determination of parameter µ
Since the parameter µ is used to control the trade off between the trivial solution ( µ = 0 ) and the uniform distribution ( µ = ∞ ) , we would like to keep only top k neighbors for local manifold structure characterization as the k nn graph [ 18 ] . Inspired by recent work on adaptive clustering in [ 18 ] , we provide an effective method to achieve this . For each sub problem in Eq ( 8 ) , the Lagrangian function is
||pT i − aT i ||2 − τ ( pT i 1n − 1 ) − ηT i pi
( 15 )
1 2 where τ and ηi are the Lagrangian multipliers . According to KKT condition , the optimal value can be obtained by
Pij = ( Aij + τ )+
( 16 )
By sorting each row of A into B with ascending order , the following inequality holds
Bik + τ > 0 for k = 1 , , k Bik + τ ≤ 0 for k = k + 1 , , n
( 17 )
Considering the simplex constraint on pT i , we further get
( 1 − k k=1
τ =
1 k
Bik )
( 18 )
By replacing Eq ( 18 ) into Eq ( 16 ) , the optimal value of P can be obtained by
Pij = ( Aij − 1 k
( 1 − k k=1
Since Bik = − 1 lem we have k 2 ik − 1 dW 2
2µ||Wxi −Wxk||2 = dW k ik < µ ≤ k dW 2 i,k+1 − 1 dW 2 k=1 k k=1
Bik ))+
( 19 ) ik , for each subprob dW ik
( 20 )
When µ satisfies the above inequality for i th example , the corresponding pT i has k non zero component . Therefore the average non zero elements in each row of P is close to k when we set n i=1 k k=1
µ =
1 n k 2 i,k+1 − 1 dW 2 dW ik
( 21 )
In this way , the search of parameter µ can be better handled by searching the neighborhood size k , which is more intuitive and easy to tune .
4 . DISCUSSION
In this section , we discuss some approaches which are closely related to our method .
Zeng and Cheung [ 32 ] proposed to integrate feature selection within the regularization of local learning based clustering ( LLCFS ) , which involves two sub steps :
1 . It constructs the k nearest neighbor graph in the weight ed feature space .
213 2 . It performs joint clustering and feature weight learning by solving the following problem
β(Yic − xT j Wi c − bi c )2
−1)Wi c
( 22 ) min
Y,{Wi,bi}n i=1,z
 n c i=1 c=1 xj∈Nxi c )T diag(z
+ ( Wi d z = 1 , z ≥ 0 st 1T where z is the feature weight vector and Nxi is the k nearest neighbor of xi based on z weighted features .
Compared with LLCFS , FSASL performs both the global and local structure learning in an adaptive manner , where only local structure is explored by LLCFS . Moreover , LLCFS uses the discrete k nearest graph and does not optimize the same objective function in structure learning and feature search , while FSASL is optimized within a unified framework with the probabilistic neighborhood relationship .
Hou et al . [ 10 ] proposed the joint embedding learning and sparse regression ( JELSR ) method , which can be formulated as follows :
5.1 Data Sets
The experiments are conducted on 8 publicly available datasets , including handwritten and spoken digit/letter recognition data sets ( ie , MFEA from UCI reporsitory and USPS49 [ 32 ] which is a two class subset of USPS ) , three face image data sets ( ie , UMIST [ 10 ] , JAFFE [ 14 , 23 ] , AR [ 30] ) , one object data set ( ie COIL [ 4 , 5 ] ) and one biomedical data sets ( ie , LUNG [ 17 ] , TOX ) . The details of these benchmark data sets are summarized in Table 3 .
Table 3 : Summary of the benchmark data sets and the number of selected features
Data Sets sample feature class selected features
MFEA USPS49 UMIST JAFFE
AR
COIL LUNG TOX
2000 1673 575 213 840 1440 203 171
240 256 644 676 768 1024 3312 5748
10 2 20 10 120 20 5 4
[ 5 , 10 , . . . , 50 ] [ 5 , 10 , . . . , 50 ] [ 5 , 10 , . . . , 50 ] [ 5 , 10 , . . . , 50 ] [ 5 , 10 , . . . , 50 ] [ 5 , 10 , . . . , 50 ]
[ 10 , 20 , . . . , 150 ] [ 10 , 20 , . . . , 150 ] min
W,YT Y=I tr(YT L2Y ) + λ1(||Y − XT W||2 + λ2||W||21 )
( 23 )
5.2 Experiment Setup
Comparing the formulation in Eq ( 5 ) and Eq ( 23 ) , the main differences between FSASL and JELSR include : 1 ) FSASL selects those features to respect both the global and local manifold structure , while JELSR only incorporates the local manifold structure ; 2 ) The local structure in JELSR is based on k nearest neighbor graph , while FSASL learns a probabilistic neighborhood graph , which can be easily refined according the result of feature selection . 3)JELSR iteratively perform spectral embedding for clustering and sparse spectral regression for feature selection as illustrated in Fig ( 1 ) . However , the local structure itself ( ie L2 ) is not changed during iterations . FSASL can adaptively improve both the global and local structure characterization using selected features .
Most recently , Liu et al .
[ 16 ] proposed a global and local structure preservation framework for feature selection ( GLSPFS ) . It first constructs the pairwise sample similarity matrix K with Gaussian kernel function to capture the global structure of data , then decompose K = YYT . Using Y as the regression target , GLSPFS solve the following problem :
||Y − XT W||2 + λ1tr(WT XL3XT W ) + λ2||W||21 min W
( 24 )
The main differences between FSASL and GLSPFS include : 1 ) GLSPFS uses the Gaussian kernel , while FSASL captures the global structure within sparse representation , which is more discriminant ; 2 ) Both the global and local structures ( ie K and L3 ) in GLSPFS are based on all features , while FSASL refines these structures with selected features .
5 . EXPERIMENTS
In this section , we conduct extensive experiments to evaluate the performance of the proposed FSASL for the task of unsupervised feature selection .
To validate the effectiveness of our proposed FSASL1 , we compare it with one baseline ( ie , AllFea ) and states of theart unsupervised feature selection methods ,
• LapScore2 [ 8 ] , which evaluates the features according to their ability of locality preserving of the data manifold structure .
• MCFS3 [ 4 ] , which selects the features by adopting spec tral regression with 1 norm regularization .
• LLCFS [ 32 ] , which incorporates the relevance of each feature into the built in regularization of the local learningbased clustering algorithm .
• UDFS4 [ 28 ] , which exploits local discriminative infor mation and feature correlations simultaneously .
• NDFS5 [ 14 ] , which selects features by a joint framework of nonnegative spectral analysis and 2,1 norm regularized regression .
• SPFS6 [ 36 ] , which selects a feature subset with which the pairwise similarity between high dimensional samples can be maximally preserved .
• RUFS7 [ 21 ] , which performs robust clustering and robust feature selection simultaneously to select the most important and discriminative features .
1For the purpose of reproducibility , we provide the code at https://github.com/csliangdu/FSASL 2http://wwwcadzjueducn/home/dengcai/Data/code/ LaplacianScore.m 3http://wwwcadzjueducn/home/dengcai/Data/code/ MCFS_p.m 4http://wwwcscmuedu/~yiyang/UDFSrar 5https://sitesgooglecom/site/zcliustc 6https://sitesgooglecom/site/alanzhao 7https://sitesgooglecom/site/qianmingjie
214 Table 1 : Aggregated clustering results measured by Accuracy ( % ) of the compared methods .
Data Sets AllFea
LapScore MCFS
MFEA
68.73
USPS49
77.70
UMIST
42.40
JAFFE
71.57
AR
30.26
COIL
59.17
LUNG
72.46
TOX
43.65
Average
58.24
51.78± 5.51
0.00
69.21± 8.95
0.00
36.73± 1.18
0.00
67.62± 8.49
0.00
25.29± 2.89
0.00
45.60± 6.16
0.00
58.97± 5.24
0.00
40.25± 0.65
0.00 49.43
51.04± 8.13
0.00
53.74± 3.50
0.00
44.46± 3.26
0.00
73.56± 4.83
0.00
29.05± 1.19
0.00
51.50± 5.38
0.00
70.42± 3.41
0.00
43.10± 1.86
0.00 52.11
LLCFS
60.38± 8.58
0.00
94.96± 1.44
0.03
47.31± 0.83
0.00
64.79± 4.08
0.00 34.22 ± 2.70 0.05
50.84± 3.76
0.00
71.58± 5.85
0.00
39.28± 0.49
0.00 57.92
UDFS
48.94± 3.32
0.00
94.05± 1.13
0.00
48.04± 1.92
0.00
75.48± 1.63
0.00
30.87± 0.35
0.00
31.40± 16.89
0.00
65.46± 3.88
0.00
47.14± 0.75
0.00 55.17
NDFS
67.13± 7.53
0.01
68.12± 8.18
0.00
52.80± 2.26
0.00
74.98± 2.15
0.00
32.34± 1.52
0.00
44.22± 6.33
0.00
75.52± 1.57
0.00
38.28± 1.64
0.00 56.67
SPFS 68.20 ± 9.43 0.22
83.43± 6.66
0.00
46.72± 1.70
0.00
73.93± 2.85
0.00
31.06± 2.14
0.00
56.94± 3.43
0.00
73.49± 3.43
0.00
39.93± 1.13
0.00 59.21
RUFS
64.58± 7.99
0.00
85.86± 2.58
0.00
50.87± 1.95
0.00
75.75± 2.53
0.00
34.84± 1.90
0.04
59.20± 3.28
0.00
77.35± 2.62
0.00
47.67± 0.83
0.00 62.02
JELSR GLSPFS
67.01± 8.37
0.01
95.16± 0.55
0.00
53.52± 1.54
0.01
77.77± 1.87
0.00
34.19± 2.52
0.02
59.53± 4.01
0.03
77.86± 3.12
0.00
43.96± 1.56
0.00 63.63
61.00± 8.70
0.00
94.75± 0.61
0.00
50.53± 0.59
0.00
75.46± 1.61
0.00
34.12± 1.60
0.00
57.96± 2.27
0.00
77.83± 2.70
0.00
47.38± 1.93
0.00 62.38
FSASL 69.94 ± 7.19 1.00 95.95 ± 0.48 1.00 54.92 ± 1.89 1.00 79.29 ± 2.24 1.00 36.11 ± 0.75 1.00 60.93 ± 2.50 1.00 81.93 ± 1.63 1.00 49.17 ± 0.67 1.00 66.03
• JELSR8 [ 11 , 10 ] , which joins embedding learning with sparse regression to perform feature selection .
• GLSPFS9 [ 16 ] , which integrates both global pairwise sample similarity and local geometric data structure to conduct feature selection .
There are some parameters to be set in advance . For all the feature selection algorithms except SPFS , we set k = 5 for all the datasets to specify the size of neighborhoods [ 4 , 13 ] . The weight of k nn graph for LapScore and MCFS , and the pairwise similarity for SPFS and GLSPFS is based on the Gaussian kernel , where the kernel width is searched from the grid {2−3 , 2−2 , . . . , 23}δ0 , where δ0 is the mean distance between any two data examples . For GLSPFS , we report the best results among three local manifold models , that is locality preserving projection ( LPP ) , LLE and local tangent space alignment ( LTSA ) as in [ 16 ] . For LLCFS , UDFS , NDFS , RUFS , JELSR , GLSPFS and FSASL , the regularization parameters are searched from the grid {10−5 , 10−4 , . . . , 105} . And the regularization parameter for γ is searched from the grid {0.001 , 0.005 , 0.01 , 0.05 , 0.1}γmax , where γmax is automatically computed from SLEP [ 15 ] . For FSASL , µ is determined by Eq ( 21 ) with k = 5 and c is set to be the true number of classes . To fairly compare different unsupervised feature selection algorithms , we tune the parameters for all methods by the grid search strategy [ 21 , 16 , 5 ] .
With the selected features , we evaluate the performance in terms of k means clustering by two widely used metrics , ie , Accuracy ( ACC ) and Normalized Mutual Information
8http://wwwesciencecn/people/chenpinghou 9We also use the implementation provided by the authors .
( NMI ) . The results of k means clustering depend on the initialization . For all the compared algorithms with different parameters and different number of selected features , we first repeat the clustering 20 times with random initialization and record the average results .
5.3 Clustering with Selected Features
Since the optimal number of selected features is unknown in advance , to better evaluate the performance of unsupervised feature selection algorithms , we finally report the averaged results over different number of selected features ( the range of selected features for each data set can be found in Table 3 ) with standard derivation . For all the algorithms ( except for AllFea ) , we also report its p value by the paired t test against the best results . The best one and those having no significant difference ( p > 0.05 ) from the best one are marked in bold .
The clustering results in terms of ACC and NMI are reported in Table 1 and Table 2 , respectively . For different feature selection algorithms , the results in each cell of Table 1 and 2 are the mean ± standard deviation and the p value . The last row of Table 1 and Table 2 shows the averaged results of all the algorithms over the 8 datasets .
Compared with clustering using all features , these unsupervised feature selection algorithms not only can largely reduce the number of features facilitating the latter learning process , but can also often improve the clustering performance . In particular , our method FSASL achieves 11.8 % and 15.04 % improvement in terms of accuracy and NMI respectively with less than 10 % features . These results can well demonstrate the effectiveness and efficiency of unsupervised feature selection algorithm . It can also be observed
215 Table 2 : Aggregated clustering results measured by Normalized Mutual Information ( % ) of the compared methods .
Data Sets AllFea LapScore MCFS LLCFS UDFS
NDFS
SPFS
RUFS
JELSR GLSPFS
FSASL
MFEA
70.33
USPS49
23.51
UMIST
64.15
JAFFE
81.52
AR
65.48
COIL
75.58
LUNG
60.37
TOX
15.87
Average
57.10
53.74± 4.77
0.00
15.88± 17.98
0.00
55.57± 2.32
0.00
77.28± 8.98
0.00
63.59± 2.36
0.00
62.21± 4.98
0.00
50.14± 4.13
0.00
10.92± 0.68
0.00 48.67
54.72± 9.14
0.00
4.60± 2.57
0.00
63.46± 4.93
0.00
79.04± 5.88
0.00
66.41± 0.85
0.00
66.19± 6.78
0.00
55.68± 2.31
0.00
16.53± 2.68
0.00 50.83
52.77± 9.76
0.00
72.03± 5.56
0.03
63.42± 1.42
0.00
66.97± 3.47
0.00
69.01± 1.45
0.01
64.04± 4.34
0.00
60.12± 4.65
0.00
9.68± 0.75
0.00 57.26
49.19± 3.83
0.00
68.12± 4.46
0.00
65.19± 2.96
0.00
84.25± 1.74
0.00
67.49± 0.27
0.00
44.27± 12.61
0.00
54.88± 4.21
0.00
22.16± 1.36
0.00 56.94
64.97± 7.54
0.03
12.27± 9.62
0.00
71.19± 2.77
0.01
82.53± 3.49
0.00
67.89± 0.89
0.00
56.29± 6.91
0.00
60.57± 1.54
0.00
9.07± 1.87
0.00 53.10
64.92 ± 8.27 0.11
38.10± 16.66
0.00
64.90± 3.06
0.00
80.01± 3.06
0.00
66.94± 1.11
0.00
69.91± 4.38
0.00
61.75± 3.32
0.00
10.13± 1.03
0.00 57.08
63.98± 7.22
0.00
41.73± 7.23
0.00
68.19± 2.61
0.00
82.00± 3.56
0.00
69.54± 1.10
0.01
70.54± 4.48
0.00
65.47± 1.87
0.00
23.58± 1.60
0.00 60.63
64.51 ± 9.07 0.06
72.28± 2.24
0.00
71.33± 2.06
0.00
85.23± 3.31
0.00
69.02± 1.32
0.00
71.37± 4.97
0.00
63.54± 2.94
0.00
17.46± 3.36
0.00 64.34
59.26± 7.59
0.00
70.43± 2.57
0.00
69.16± 0.97
0.00
83.20± 3.17
0.00
69.44± 0.84
0.00
69.89± 4.00
0.00
63.50± 2.99
0.00
23.49± 2.77
0.00 63.55
66.70 ± 6.71 1.00 75.88 ± 2.28 1.00 72.39 ± 2.39 1.00 86.42 ± 3.34 1.00 70.78 ± 0.63 1.00 72.93 ± 4.44 1.00 66.78 ± 1.72 1.00 25.79 ± 1.62 1.00 67.21 that FSASL consistently produces better performance than the other nine feature selection algorithms , and the improvement is in the range from 3.63 % to 25.14 % in terms of clustering accuracy and from 4.27 % to 27.59 % in terms of NMI . This can be mainly explained by the following reasons . First , both global and local structure are used to guide the search of relevant features . Second , the structure learning and feature selection are integrated into a unified framework . Third , both the global and local structures can be adaptively updated using the results of selected features . 5.4 Effect of Adaptive Structure Learning
Here , we investigate the effect of adaptive structure learn ing by empirically answering the following questions :
1 . What kind of structure should be captured and preserved by the selected features , either global or local or both of these structures ?
2 . Does the adaptive structure learning lead to select more informative features ?
We conduct different settings of FSASL on USPS200 , which consists the first 100 samples in USPS49 . We solve the optimization problem in Eq ( 2 ) , Eq ( 4 ) and Eq ( 5 ) , which uses global , local , and both global and local structures , respectively . We also distinguish these problems with and without adaptive structure learning . Thus , we have 6 settings in total . Figure 3 and Figure 4 show the results of these different settings with different number of selected features . The aggregated result over different number of selected features is also provided in Table 4 .
From these results , we can see that : 1 ) The exploitation of both global and local structures ( ie , Eq ( 5 ) + W ) out
Figure 3 : Clustering accuracy wrt 6 different settings of FSASL on USPS200 . perform another two alternatives with only global ( ie , Eq ( 2 ) + W ) or local ( ie , Eq ( 4 ) + W ) structure . It validates that the integration of both global and local structure is better than the single one . 2 ) With the update of structure learning ( ie , Eq ( 2 ) + W , S , Eq ( 4 ) + W , P and Eq ( 5 ) + W , S , P ) is better than their counterparts without adaptive structure learning respectively . It shows that the adaptive learning in either global and/or local structure learning can further improve the result of feature selection .
216 ( a )
( b )
( c )
( d )
( e )
( f )
Figure 2 : Clustering accuracy wrt different parameters on JAFFE ( a c ) and TOX ( d f ) .
Table 4 : Aggregated clustering results ( % ) of 6 different settings of FSASL on USPS200 .
W
W
Problem Variables Eq ( 2 ) Eq ( 2 ) W , S Eq ( 4 ) Eq ( 4 ) W , P Eq ( 5 ) Eq ( 5 ) W , S , P
W
ACC
89.17 ± 3.22 91.90 ± 2.51 91.48 ± 2.62 92.86 ± 2.53 94.65 ± 1.24 95.53 ± 1.10
NMI
52.01 ± 9.69 61.95 ± 7.21 59.10 ± 9.31 64.65 ± 8.30 69.94 ± 4.22 74.20 ± 4.83
Figure 4 : Clustering NMI wrt 6 different settings of FSASL on USPS200 .
In the next , we investigate the sensitivity with respect to the regularization parameters α , β and γ .
5.5 Parameter Sensitivity
When we vary the value of one parameter , we keep the other parameters fixed at the optimal value . We plot the clustering accuracy with respect to these parameters on JAFFE and TOX in Figure 2 . The experimental results show that our method is not very sensitive to α , β and γ with wide ranges . However , the performance is relatively sensitive to the number of selected features , which is still an open problem .
6 . CONCLUSION
In this paper , we proposed a novel unsupervised feature selection method to simultaneously perform feature selection and the structure learning . In our new method , the global structure learning and feature selection are integrated within the framework of sparse representation ; the local structure learning and feature selection are incorporated into the probabilistic neighborhood relationship learning framework . By combining both the global and local structure learning and feature selection , our method can boost both these two essential tasks , ie , structure learning and feature selection , by using the result of the other task . We derive an efficient algorithm to optimize the proposed method and discuss the connections between our method and other feature selection methods . Extensive experiments have been conducted on real world benchmark data sets to demonstrate the superior performance of our method .
217 7 . ACKNOWLEDGMENTS
We would like to thank Prof . Feiping Nie and Prof . Mingyu Fan for their helpful suggestions to improve this paper . This work is supported in part by the China National 973 program 2014CB340301 , the Natural Science Foundation of China ( NSFC ) grant 61379043 , 61322211 and Program for New Century Excellent Talents in University ( No . NCET 121031 ) .
8 . REFERENCES [ 1 ] S . Alelyani , J . Tang , and H . Liu . Feature selection for clustering : A review . Data Clustering : Algorithms and Applications , 29 , 2013 .
[ 2 ] F . Bach , R . Jenatton , J . Mairal , and G . Obozinski .
Optimization with sparsity inducing penalties . Foundations and Trends Rfl in Machine Learning , 4(1):1–106 , 2012 .
[ 3 ] D . Cai , X . He , and J . Han . Spectral regression : A unified approach for sparse subspace learning . In ICDM , pages 73–82 , 2007 .
[ 4 ] D . Cai , C . Zhang , and X . He . Unsupervised feature selection for multi cluster data . In SIGKDD , pages 333–342 , 2010 .
[ 5 ] L . Du , Z . Shen , X . Li , P . Zhou , and Y . Shen . Local and global discriminative learning for unsupervised feature selection . In ICDM , pages 131–140 , 2013 . [ 6 ] J . G . Dy and C . E . Brodley . Feature selection for unsupervised learning . JMLR , 5:845–889 , 2004 .
[ 7 ] Q . Gu , Z . Li , and J . Han . Joint feature selection and subspace learning . In IJCAI , pages 1294–1299 , 2011 .
[ 8 ] X . He , D . Cai , and P . Niyogi . Laplacian score for feature selection . NIPS , 18:507–514 , 2006 .
[ 9 ] X . He , M . Ji , C . Zhang , and H . Bao . A variance minimization criterion to feature selection using laplacian regularization . PAMI , ( 99):2013–2025 , 2011 .
[ 10 ] C . Hou , F . Nie , X . Li , D . Yi , and Y . Wu . Joint embedding learning and sparse regression : A framework for unsupervised feature selection . Cybernetics , IEEE Transactions on , 44(6):793–804 , 2014 .
[ 11 ] C . Hou , F . Nie , D . Yi , and Y . Wu . Feature selection via joint embedding learning and sparse regression . In IJCAI , pages 1324–1329 , 2011 .
[ 12 ] W . Krzanowski . Selection of variables to preserve multivariate data structure , using principal components . Applied Statistics , pages 22–33 , 1987 .
[ 13 ] Z . Li , J . Liu , Y . Yang , X . Zhou , and H . Lu .
Clustering guided sparse structural learning for unsupervised feature selection . TKDE , 26(9):2138–2150 , 2014 .
[ 14 ] Z . Li , Y . Yang , J . Liu , X . Zhou , and H . Lu .
Unsupervised feature selection using nonnegative spectral analysis . In AAAI , 2012 .
[ 15 ] J . Liu , S . Ji , and J . Ye . SLEP : Sparse Learning with Efficient Projections . Arizona State University , 2009 .
[ 16 ] X . Liu , L . Wang , J . Zhang , J . Yin , and H . Liu . Global and local structure preservation for feature selection . IEEE Transactions on NNLS , 25(6):1083–1095 , 2014 . [ 17 ] F . Nie , H . Huang , X . Cai , and C . Ding . Efficient and robust feature selection via joint 2,1 norms minimization . NIPS , 23:1813–1821 , 2010 .
[ 18 ] F . Nie , X . Wang , and H . Huang . Clustering and projected clustering with adaptive neighbors . In SIGKDD , pages 977–986 , 2014 .
[ 19 ] F . Nie , S . Xiang , Y . Jia , C . Zhang , and S . Yan . Trace ratio criterion for feature selection . In IJCAI , volume 2 , pages 671–676 , 2008 .
[ 20 ] H . Peng , F . Long , and C . Ding . Feature selection based on mutual information criteria of max dependency , max relevance , and min redundancy . PAMI , 27(8):1226–1238 , 2005 .
[ 21 ] M . Qian and C . Zhai . Robust unsupervised feature selection . In IJCAI , pages 1621–1627 , 2013 .
[ 22 ] M . Robnik ˇSikonja and I . Kononenko . Theoretical and empirical analysis of relieff and rrelieff . Machine learning , 53(1 2):23–69 , 2003 .
[ 23 ] L . Shi , L . Du , and Y . Shen . Robust spectral learning for unsupervised feature selection . In ICDM , pages 977–982 , 2014 .
[ 24 ] I . Takeuchi and M . Sugiyama . Target neighbor consistent feature weighting for nearest neighbor classification . In NIPS , pages 576–584 , 2011 .
[ 25 ] R . Tibshirani . Regression shrinkage and selection via the lasso . Journal of the Royal Statistical Society . Series B ( Methodological ) , pages 267–288 , 1996 .
[ 26 ] J . Wright , A . Y . Yang , A . Ganesh , S . S . Sastry , and
Y . Ma . Robust face recognition via sparse representation . PAMI , 31(2):210–227 , 2009 .
[ 27 ] Z . Xu , I . King , M T Lyu , and R . Jin . Discriminative semi supervised feature selection via manifold regularization . Neural Networks , IEEE Transactions on , 21(7):1033–1047 , 2010 .
[ 28 ] Y . Yang , H . Shen , Z . Ma , Z . Huang , and X . Zhou .
21 norm regularized discriminative feature selection for unsupervised learning . In IJCAI , pages 1589–1594 , 2011 .
[ 29 ] Y . Yang , H . T . Shen , F . Nie , R . Ji , and X . Zhou .
Nonnegative spectral clustering with discriminative regularization . In AAAI , 2011 .
[ 30 ] Y . Yang , D . Xu , F . Nie , S . Yan , and Y . Zhuang .
Image clustering using local discriminant models and global integration . TIP , 19(10):2761–2773 , 2010 .
[ 31 ] Y . Yang , Y . Yang , H . T . Shen , Y . Zhang , X . Du , and
X . Zhou . Discriminative nonnegative spectral clustering with out of sample extension . TKDE , 25(8):1760–1771 , 2013 .
[ 32 ] H . Zeng and Y . Cheung . Feature selection and kernel learning for local learning based clustering . PAMI , 33(8):1532–1547 , 2011 .
[ 33 ] Z . Zhao and H . Liu . Semi supervised feature selection via spectral analysis . In SDM , pages 641–646 , 2007 .
[ 34 ] Z . Zhao and H . Liu . Spectral feature selection for supervised and unsupervised learning . In ICML , pages 1151–1157 , 2007 .
[ 35 ] Z . Zhao , L . Wang , and H . Liu . Efficient spectral feature selection with minimum redundancy . In AAAI , pages 673–678 , 2010 .
[ 36 ] Z . Zhao , L . Wang , H . Liu , and J . Ye . On similarity preserving feature selection . TKDE , 25(3):619–632 , 2013 .
218
