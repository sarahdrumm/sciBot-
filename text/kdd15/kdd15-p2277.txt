Building Discriminative User Profiles for Large scale
Content Recommendation
Erheng Zhong , Nathan Liu , Yue Shi and Suju Rajan
Personalization Sciences , Yahoo Labs , CA , USA
{erheng,nanliu,yueshi,suju}@yahoo inc.com
ABSTRACT Content recommendation systems are typically based on one of the following paradigms : user based customization , or recommendations based on either collaborative filtering or low rank matrix factorization methods , or with systems that impute user interest profiles based on content browsing behavior and retrieve items similar to the interest profiles . All of these systems have a distinct disadvantage , namely data sparsity and cold start on items or users . Furthermore , very few content recommendation solutions explicitly model the wealth of information in implicit negative feedback from the users . In this paper , we propose a hybrid solution that makes use of a latent factor model to infer user interest vectors . The hybrid approach enables us to overcome both the data sparsity and cold start problems . Our proposed method is learned purely on implicit user feedback , both positive and negative . Exploiting the information in the negative feedback allows the user profiles generated to be discriminative . We also provide a Map/Reduce framework based implementation that enables scaling our solution to real world recommendation problems . We demonstrate the efficacy of our proposed approach with both offline experiments and A/B tests on live traffic on Yahoo properties .
Categories and Subject Descriptors : H.4 [ Information Systems Applications ] : Miscellaneous
General Terms : Design , Experimentation
Keywords : Collaborative filtering , content based recommendation , large scale recommender systems , latent factor models , user profiling .
1 .
INTRODUCTION
The explosion of online content from news sites , digital magazines , self publishing platforms and blogs has necessitated an industry around the concept of personalized content recommendation . Products , such as the Yahoo Stream , LinkedIn Feed , Google News and Facebook Feeds , attempt to surface content that is relevant to each user . At any given instance , each of those products attempt to discover and present the most relevant news items from a corpus of around a few million items to the millions of users visiting their sites [ 8 ] . While there is a slew of work around recommendation systems , classical methods of recommendation , such as collaborative filtering methods or low rank factorization models [ 11 ] assume a long shelf life of items to be recommended , such that each item can garner sufficient user interaction over time . Such assumptions do not hold true in the domain of news content recommendation . Thus , even such a paradigm has been shown to work very well for content items with a long shelf life , such as movies or consumer products [ 6 ] , they are generally less effective for news item recommendations as news items tend to expire quickly . A simple solution to solving the problem of personalized news recommendation is to allow the user to configure personal filters or provide explicit input on news providers , topics or entities that the user would like to be shown content on . Examples of such products are Flipboard , Zite , MyYahoo etc . Needless to say moving the burden of configuration is non ideal as the user has to continually update his interests to keep pace with the ever changing nature of news topics or be satisfied with following a limited number of providers .
To overcome these problems , in this paper , we propose another approach by building user interest imputation or user profiling based on the user ’s feedback on content items . The feedback can be explicit , in the form of ratings or binary preference indicators or implicit , ie inferred from the user ’s interaction with the content item . As with explicit interest declarations , assuming the presence of explicit feedback is infeasible . On the other hand , there is a rich set of implicit feedback that can be obtained from the user ’s behavior on a content item . As an example , a user can indicate positive preference by clicking on an item or dwelling on it beyond a threshold [ 29 ] , while a negative feedback can be construed from a lack of click or very low time spend on an item . Given the user feedback on a set of content items , one can then attempt to characterize or build a profile for the user . Although user profiles can be specialized for every specific product domain , in a general sense , a user profile can be thought of as a numeric representation that indicates the user ’s interest in a feature space . For example , in the case of Netflix Prize competition [ 11 ] , the user profile can be seen as a vector of ratings over all the items , ie , the item space is then the feature space . Such a setting leads itself to collaborative filtering based solutions to the recommendation problem . Alternatively , a user profile can also be built using
2277 the features of the content items that the user has interacted with . Examples of content features for news items are topical categories the content belongs to , the set of named entities in the content item or even the word tokens that appear in item . The feature space of the user profile is then the same as the space of content features and the feature value in the user profile indicates the relevance of the content feature to the user . At the time of serving content items to users , the relevance between a user and an item can be measured based on the similarity between the user ’s profile and the content features of the item .
The advantage of building such user interest profiles is that this method suffers less from the item cold start case . That is , even if the user is the very first person to interact with that item , it can be deemed as being relevant to the user as long as there is an overlap in the user interest profile and the item ’s features . Furthermore , as will be demonstrated later , such profile building methods can be built from a combination of explicit , implicit positive or negative feedback from the user . Thus , building user profiles is a viable solution to the content recommendation systems [ 15 ] . However , there are also a few challenges to building an effective user profiling method .
• Scalability : Online companies may serve millions of users and items everyday , and thus , personalized recommendation is by default a task at a very large scale . In addition , the content features , based on which the user profiles are built , may probably lie in a very high dimension space , eg , one text document can be represented by thousands of word tokens or n grams . As a result , building profiles for individual users requires computation over a massive feature space . • Complex implicit feedback : Explicit feedback from the user , such as ratings , is a sparse signal . A more practical solution is to rely on the implicit feedback provided by the user . For example , a click on an item from a user may indicate that the user is interested in the item , and thus , we can infer that the content features of the item are relevant to the user . However , there can be multiple types of user feedback happening between users and items beyond clicks . In particular , those signals that can be seen as negative feedback of the users are of great value , since they open the possibilities to improve the discriminative ability of user profiles . Then , one of the key challenges is considered as how to effectively exploit the complex implicit feedback data for modeling user profiles . • Data sparsity : While profiling based content recommendation suffers less from the item cold start case , the problem could still exist . Since user interest profiles are built on a high dimensional feature space , it is often that a user could have only a small set of observed content features in his profile . Such a case is common for users who interact with a limited number of news items . In an extreme case , the feature set in a user ’s profile can be so small that it has no intersection with the content features of any items in the corpus . Furthermore , limited interaction data between a user and a content item makes it difficult to confidently estimate the relevance of the corresponding content features to the user . In other words , the number of available instances can be insufficient for an accurate estimation of user profiles .
In this work , we propose a method for performing personalized news recommendation that addresses the drawbacks of the current approaches . Our solution follows the paradigm of imputing user interest vectors or user profiles that can then be used to retrieve relevant content items for the user . The proposed approach tackles the challenges mentioned above by making use of a hybrid approach that combines user interest inference with latent factor models . Unlike conventional approaches that only build user profiles based on positive implicit feedback such as click events [ 26 ] , we also exploit the rich signal that is present in the negative feedback of users on items . By incorporating both positive and negative user feedback , we train user interest profiles that are not only discriminative but also more effective than the conventional approaches . A straightforward baseline is to build a linear model for each individual user based on the user ’s positive and negative feedback on items . However , such linear model still suffers from the data sparsity issue , as it requires sufficient training examples for each individual user . Motivated by this observation , we propose to build user profiles by leveraging the latent factor model formulation . We project each user into a latent feature space , ie , each user is represented by a vector of latent factors . We also project each content feature into the same latent space , ie , each content feature is also represented by a vector of latent factors . The underlying principle is then the inner product of the latent factors of a user and a content feature indicates the relevance between the two . Based on the positive and negative user feedback , we learn the latent factor of users and content features through a well defined optimization framework . The key advantage of this model lies in the fact that the latent factors of the content features are shared across all the users . As a result , even users with limited interaction data could still be represented with a richer content feature interest vector .
Our system is built on the Hadoop based Map/Reduce framework . This implementation can compute user profiles for millions of users in a few hours , thus making it suitable for real world content recommendation problems . Besides performing large scale offline experiments , the proposed method is also A/B being tested in an online test on the Yahoo Recommends ( to be detailed in Section 2 ) property , which is a news recommendation product serving millions of users .
The main contributions of this paper are : • We propose a user interest profile generation method based on latent factor models that effectively alleviates the data sparseness issue . An added benefit of our model is that it allows us to bootstrap even users with limited content interaction data by learning from the collective set of users . • We provide a method to leverage the rich signal in the • We provide a Map reduce framework that can scale to building user interest vectors for millions of users in a few hours . • We demonstrate the effectiveness of our user profiling model through large scale offline experiments and A/B testing on real world applications . negative implicit feedback provided by the user .
The paper is organized as follows . In Section 2 , we introduce the Yahoo Recommends product , as an example of a content recommendation system that can benefit from our proposed approach . We then present the details of latent
2278 factor based model used to infer user interest profiles in Section 3 . We then introduce the Map Reduce framework design in Section 4 . The offline and real world experimental results are demonstrated in Section 5 , followed by Section 6 that differentiates our work from other related approaches . Section 7 concludes the paper .
2 . USE CASES OF USER PROFILE
We introduce two use cases of user profile in Yahoo . The first one is the Yahoo stream , where we recommend items for the content feed or stream on the Yahoo ’s homepage , shown in Figure 1 . In this module , news and ads are ranked based on the relevance of the item to the user and on its popularity . The second one is Yahoo Recommends , which is a recently released product that generates personalized content recommendations and a native advertising experience for users on external publishers . The product makes it easier for users to discover the publisher ’s content . The Yahoo Recommends product has been deployed on several hundreds of websites including CBS , Vox , and Hearst and is driving several millions of clicks on these partner sites every day . Figure 2 presents an example of the Yahoo Recommends module on an external site . Any external publisher can insert a Yahoo Recommends module on their site by filling out a simple form on the recommendsyahoocom portal . The module is activated by including a JavaScript snippet for the page , which then triggers content crawling , and event logging and invokes the visible module on the page .
In these products , items are extracted at real time from a content corpus and ranked via a machine learned ranker . The machine learned ranking models makes use of the following classes of features which attempt to characterize three types of relevance :
Universal relevance : Features that capture the popularity and trendiness of a content item by tracking dozens of time series that characterize how users engage with each piece of content within the module , on a publisher site , on social media , and on search engines .
Contextual relevance : Features that capture the context of the page the recommendation module is shown in by leveraging a knowledge graph to measure content relatedness as well as a collaborative filtering style approach to uncover novel patterns about “ people who read X who would also read Y . ” . For example , if a user is currently reading an iPhone 6 review , it may be better to recommend other articles about smartphones or Apple products , and if one user prefers to read sport news in the morning , it is also straightforward to display sport news for this user in the morning time in future .
Personal relevance : Features that cater to the individual preferences of the users . These features are computed using the similarities between user profiles and item features .
The variety and the scale of available user interaction data make the content stream on Yahoo homepage and Yahoo Recommends very relevant applications for testing out the efficacy of the proposed user interest profiling method . We explain the nature of the data collected and the experimental setup for both offline and online A/B tests in Section 5 .
Figure 1 : Stream on Yahoo Homepage ( Red Box )
Figure 2 : Yahoo Recommends
3 . USER PROFILE MODELING
The modeling of user interest profiles is a key component of the content recommendation system . In this section , we first formulate the task of user profile modeling and then discuss the three types of models that are tested in our experiments . As baselines we describe two generative statistical models , namely TF IDF and Chi Square ( CS ) based models and an independent learning model ( ILM ) . We then describe our proposed solution the latent factor model ( LFM ) . 3.1 Problem Formulation
Generally , user profile modeling in content recommendation system aims to represent user interests in the same space as that of the content features in order to effectively retrieve content items that are relevant to the user . Assume , there are m users , n items and f item features . Also , assume that the user is presented with a ranked list of items on which they provide implicit feedback . As an example of implicit feedback , users can either click or not click ( skip ) each item in the list . This implicit positive and negative feedback from the users on content items is represented as a sparse matrix C with m × n dimensions , where Cij = +1 means user ui clicked an item xj and Cij = −1 means ui skipped item xj . There are still many missing values ? in C as each user may interact with only a few items . We further represent the items as another matrix X with n × f dimensions , where X is sparse and each entry represents a content
2279 Table 1 : Definition of notations i=1 i=1
Notation X = {xi}n P = {pi}m C ∈ [ 0 , 1 , ?]m×n User item interactions , I ∈ [ 0 , 1]m×n ( · ) , R(· ) m n f
Description Item features , a sparse matrix , xi ∈ Rf Profiles , a sparse matrix , pi ∈ Rf 1 for a click , −1 for a skip , ? for unknown Indicator matrix Loss function and regularization term Number of users Number of items Number of features feature of the item . Examples of content features are named entities , topical categories , noun phrase tokens , n grams etc . Then , the user profiling problem is to build a model Ω that can construct a sparse matrix P∈Rm×f from X and C , ie , P = Ω(C,X ) . Generally , we aim to construct such a model in an optimization framework by minimizing the following objective : min
Ω
( Ω , C , X ) + λR(Ω )
( 1 ) where is the loss function , λ is the regularization parameter and R is the regularization function to control the model complexity . 3.2 Generative Statistical Model
An intuitive solution to building profiles is to combine all features of items clicked by the user using some statistical models , such term frequency . The motivation is that if a user interacts a lot with content items containing a specific feature , that feature may be highly relevant to this user and the user may like to read other items that contain the feature as well . A user interest profile can then be represented as a vector over content features . pi = xj
( 2 )
Cij =1
However , some content features , such as stop words , may exist in all items and using these features to build the user profile is pointless . Thus , we need to introduce several normalization strategies to adjust the value of each feature . We introduce two methods for normalization . One is a classical document vector model , the term frequency inverse document frequency ( TF IDF ) [ 24 ] based model . The main idea of TF IDF is to use document frequency to penalize those features that are common to all users , and highlight those specific features that can distinguish one user from another . Formally , the score of each feature pik ∈ pi is defined as follows : pik = ¯pik × log
¯pik = xjk m
|{¯pc : ¯pck > 0.0}|
( 3 )
( 4 )
Cij =1
If one feature is common in many users , then the value of the second part in Eq ( 3 ) becomes small and thus lowers the value of the feature .
Another statistical method is derived from the Chi Square test [ 19 ] . Its motivation is to select those features , which have much higher values for the user as compared to the expected value . This means that if one user reads items containing a specific feature at a rate that is statistically significantly above the average rate for that feature , that feature will be included in the user interest profile . Formally , the score of each feature is pik = eik =
¯pik + κ eik + κ ¯pik × k k c ¯pck c ¯pck
( 5 )
( 6 ) where κ is the smoothing term . One advantage of this method over TF IDF is that it can estimate the confidence of each feature value by using the z test . The confidence value can be used for further feature selection . zik =
√ log(¯pik + κ ) eik + κ
( 7 )
We can observe that generative statistical models are intuitive and easy to compute . However , these methods only use the positive feedback signal . By not taking into account , the rate at which a feature appears in the items with negative feedback such user profiles can misrepresent the relevance of the feature to the user . In other words , these models lack the ability to represent discriminative user interests . 3.3 User Independent Learning Model
In order to build user profiles that contain discriminative user interest features , we propose to leverage supervised learning algorithms that construct models from both positive and negative users’ feedbacks on items and use the model coefficients on the content features as user profiles . Specifically , for each user the click or skip feedback on displayed items is used to construct the training data for that user , where each item is a feature vector with labels , click or skip . Due to the reason that we use inner product between user profiles and item features to get the relevance score for the ranking model , we apply a linear model that uses the value of coefficients as the users profiles . Generally , we aim to construct a vector pi for each user to minimize the following objective m n i=1 j=1 minP m n i=1 j=1 minP
Iij(Cij , pixT j ) + λR(pi )
( 8 ) where I is an indicator matrix . Iij = 1 if Cij is not empty and Iij = 0 otherwise . There are multiple approaches to get pi by using different loss functions and regularization terms , such as linear support vector machine , logistic regression , linear discriminant analysis and etc . In this paper , we use logistic regression with L2 norm regularization as the learning model . Specifically , we aim to minimize the following objective
Iijγ(CijpixT j ) + λ pi 2
( 9 ) where γ(x ) = log(1 + exp(−x ) ) is the logistic loss . To guarantee efficiency and minimize effects from noisy data , we use the Stochastic Gradient Descent ( SGD ) technique to optimize the learning objective . Besides having the property of selecting those features that are preferred by the user , another advantage of this kind of models is that they can also represent the content features that the users do not want to engage with . Despite these properties , there is still a fundamental drawback in this approach . Most users may interact
2280 with only a few items and thus the generated labeled data is limited . If we build independent models for each user , we may overfit the rare or noisy observations and fail to produce profiles that generalize well . In addition , as with the statistical models , the system has no natural property that will allow us to explore user interests . 3.4 Latent Factor Model
As stated above , although the independent learning model produces user interest features that are discriminative , the data sparsity problem can still affect the model performance . To solve this aforementioned challenge , we propose to construct each user ’s profile by using data from all users . The main idea is to construct a latent space for all users , where users’ interests can be distinguished while still learning from all available data . Motivated by the success of matrix factorization methods in the domain of multi task learning [ 30 ] and collaborative filtering [ 11 ] , we introduce a matrix factorization based latent factor model to build profiles . Specifically , we decompose each user ’s profiles into two components , one is a common mapping from content item features to latent factors , which is shared by all users ; and the other contains the latent factors of each user . Each latent factor represents the user ’s interests in that latent topic . As the membership of each item feature to latent factors is stable for all users , these feature factors can be considered as a bridge to propagate knowledge across users . Thus users who lack interaction data , can benefit from the enriched information provided by interactions from other users . In addition , each user ’s latent factor membership distribution is specific thereby allowing the model to reflect the users’ personalized interests . We formally describe this model as follows . Firstly , each user is represented as a latent factor vector ui∈R1×k that indicates users’ distribution over latent factors , where k is the number of latent factors . We define U = {ui}m i=1 as the latent factor matrix for the whole user set . To further build the correlation between item features and latent factors , we introduce another latent matrix S∈Rk×f to represent the mapping from item features to latent factors . The user profile matrix can be constructed as P = US . Then the problem is turned into how to learn U and S . Following the matrix factorization framework , we propose to use U , S and X to approximate users’ interaction records C as C ≈ U SX T . By replacing Ω in Eq ( 1 ) as U and S , we can obtain the following objective function : minU ,S ( USX T ,C ) + λuRu(U ) + λsRs(S )
( 10 ) where λu and λs are the regularization parameters for U and S respectively . In practice , we can choose different loss functions and regularization terms according to different tasks . In this paper , we use L2 norm and three loss functions to handle different types of user feedback . The first one uses square loss for rating feedbacks :
Iij(Cij − uiSxT j )2 + λu U 2 +λs S 2
( 11 ) m n i=1 j=1 minU ,S
The second one uses logistic loss for binary feedback , such as the click vs . skip feedback in our problem setting : j ) + λu U 2 +λs S 2
Iijγ(CijuiSxT m n
( 12 ) minU ,S i=1 j=1
The last one uses AUC loss for ranking problems [ 20 ] , when only implicit feedback , such as clicks are available . ln σ(uiSxT j −uiSxT k )+λu U 2 +λs S 2 m i=1 j∈I+ i k∈I
− i minU ,S
1
( 13 ) 1+exp(−x ) is the logistic transform and I + where σ(x ) = i contains the indices of positive feedbacks ( eg , clicks ) and − I i contains that of negative ones . Optimization In this paper , we use Stochastic Gradient Descent ( SGD ) to optimize the proposed LF model . The gradients with respect to U and S of one training instance are calculated as
1 . Square Loss
∇ui = ( uiSxT ∇S = ( uiSxT j − Cij)xjS T + λuui j + λsS j − Cij)uT i xT
( 14 )
( 15 )
2 . Logistic Loss
∇ui =
∇S =
−CijxjS T
1 + exp(CijuiSxT j )
−CijuT i xT j
1 + exp(CijuiSxT j )
+ λuui
( 16 )
+ λsS
( 17 )
3 . AUC ( Area Under Curve ) Loss
∇ui = exp(−yijk ) 1 + exp(−yijk ) exp(−yijk ) 1 + exp(−yijk ) j − uiSxT k
∇S = yijk = uiSxT
( xj − xk)ST + λuui ( 18 ) i ( xj − xk)T + λsS(19 ) uT
( 20 )
To calculate the gradients for AUC loss , we employ a bootstrap strategy that samples a positive negative feedback pair randomly . Then the updates of U and S are performed alternatively till convergence using the following equations .
( 22 )
( 21 )
U ← U − ∇U S ← S − ∇S In practice , we update S user wisely . In each round , we randomly shuffle the user set to make the order of input data different . Then , for each interaction of this user , we update the users’ latent factors using SGD but keep the feature factor mapping S the same . After renewing the user ’s factors with all interactions , we go through the interactions again to update S . The motivation is that one interaction record may be related to only a few item features and will not affect the values too much . So we can update it only once for each user . This approach also provides savings on computation time , which is critical in real world systems . These two steps are performed alternatively till the optimization convergence .
Discussion We shall emphasize that the key contribution in our latent factor model lies in the feature factor matrix S , which is shared across different users . Through S those users who have limited data can benefit from the interaction data of other users . This characteristic specifically addresses the issue we mentioned in Section 1 , ie , even if a user is only associated with a narrow range of features via his own interaction with items , we can still extrapolate his profile to a wide range of features by using the feature factor matrix
2281 S . Therefore , the latent factor model can substantially contribute to improving the recall of the user interest profiles . In addition , as we maximize the correlation between useritem similarities ( USX T ) and interactions ( C ) by using both positive and negative feedbacks , the selected features will have higher discriminative ability which further contributes to improving the precision of the system .
4 . MAP/REDUCE FRAMEWORK FOR
LARGE SCALE PROFILE BUILDING
In real world applications , the number of users ( m ) can be in the order of hundreds of millions , the number of items ( n ) can be up to tens of millions and the number of user item interactions can be tens of billions . Thus , making the proposed model scale up is crucial in real world application . We propose to address this issue through the Map/Reduce [ 9 ] implementation : Hadoop . The data is partitioned based on each user and the partitioned data is stored on different machines . The computation of each model is then abstracted into two parts . We need to utilize data from multiple users to obtain global statistics or models . This means that the different machines needs to communicate to exchange data . These operations require both Map and Reduce jobs . Generally , in Map phase , we calculate sub models based on the partial data and combine all pieces of results in the Reduce phase . This applies for both computing the global statistics of generative statistical models and the learning process of the feature factor mapping in latent factor models . The remaining operations only use the local data , such as the learning of coefficients of each user in the independent learning model and the construction of user factors in the latent factor model . This can be done locally without communications with other machines and thus only the Map job is needed . The general paradigm to generate user profiles using all these three proposed profiling building approaches through Map/Reduce is shown in Figure 3 . The details are described as follows .
Generative Model In the first Map operation , we calculate the feature counts of each user and then combine all pieces of results in the Reduce phase to obtain the global statistics , such as the document frequency for TFIDF , and the general expectation of each feature of Chi Square test . After that , as these statistics are small , they can be loaded into the distributed cache for usage in second step . Distributed cache is a feature of Hadoop and can be understood as a small dataset maintained in the memory by all machines . In the second Map phase , we calculate the term frequency locally for each user and use the global statistics to normalize the values and obtain the profiles for each user . Independent Learning Model In this model , we consider each user independently , so for each user , we only require the relevant data of this user to build profiles . Thus , we build profiles in the first step by using logistic regression in Map phase using Eq ( 9 ) . We can also do some optional operations in the first step to provide a prior for all users , such as training a global logistic regression and using the model coefficients as the initial profiles for each user . Latent Factor Model Basically , in the first step , we aim to construct the feature factors S that is shared by all users . So , in the first Map phase , we build a sub model , ˆS and ˆU , based on only those users’ data in the same machine locally . Then in the Reduce phase , we use an averaging strategy to obtain the final feature factors as S = AV G( ˆS ) . In the second step , we first load S into the distributed cache and then learn ui for each user using Eq ( 21 ) with fixed S . Finally , we use inner product between users’ and features’ latent factors to get the profiles . Considering that the correlation between factors and features may be stable in a long term , we can also first construct S using only the feedbacks from active users and then construct factors for all users . We compare these two strategies in our experiments .
For data communication and storage , we also define a message format for user signals required by profiling models using protocol buffers [ 27 ] . One user signal record contains user ID , timestamp , item ID with features , as well as the related action : message UserSignals { optional string user_id = 1 ; optional string item_id = 2 ; optional fixed32 timestamp = 3 ; repeated Feature features = 4 ; optional Action action_type = 5 ;
} message Feature { optional string name = 1 ; optional float value = 2 ;
} enum Action { SKIP = 0 ; CLICK = 1 ;
}
After grouping all messages by user , we can obtain one input instance for the framework as public class User {
String userID ; List<UserSignals> signals ;
}
The advantage is that , a protocol message can be serialized efficiently and methods to access data are also automatically included , that makes us focus on the model logic without worrying about the data manipulation details . In addition , it also provides a unified data interface for other data and model pipelines in the system .
5 . EXPERIMENTS
In this section , we present both the offline and online experiments to evaluate the performance of our proposed user profiling models . The purpose of our experiments is to verify the usefulness of discriminative user profiles and their impact on news recommendation systems . 5.1 Offline Experiments
Our offline experiments are conducted based on the a sample of the event log data from Yahoo Recommends , as described in Section 2 , collected over a period of four months . Overall , this dataset contains over 2.4 billion click events from ∼ 67 million users and ∼ 54 million news items . The number of content features of the news items is ∼ 500,000 , which defines the feature space of our user profiles .
In order to evaluate the quality of user profiles , we split the dataset into a training set and a test set according to the timestamps of events . We use the data from all weeks but the last one as training data and the data of last week
2282 Figure 3 : Map/Reduce Framework for User Profile Building
( a )
( b )
( c )
Figure 4 : Data Analysis . ( a ) User distribution over interactions , which follow the power log distribution with a long tail . ( b ) User distribution over ratio of test entities that exist in training data . ( c ) User distribution over ratio of clicked entities that also appear in skipped items . as the test set . Specifically , the training set contains about 2.2 billion click events , and the test set contains about 230 million click events . The training set is used to train the user profile models , ie , each user ’s profile is built based on the data from the training set . In the test set , we generated for each user a ranked list of news items based on the users’ profiles , and measure the performance of ranking against the ground truth labels . As explained earlier , the labels on the data comes from the implicit positive and negative feedback on the content items .
We first highlight some challenges of building user profiles . In Figure 4(a ) , we show the distribution of users in terms of the number of their interactions with content items . We can observe that the distribution follows a long tail , indicating that a large number of users only have a limited number of interactions with items . This observation justifies our choice of utilizing the data from multiple users collectively to solve the data sparsity problem .
In addition , we calculated the number of common features In other in both training set and test set for each user . words , we quantify how large is the overlap between the features in the user profile and the features that appear in the test set . As shown in Figure 4(b ) , we can observe that all users have at least 40 % new content feature terms in their profiles , and a large number of users have more than 90 % features that do not make it into the user profiles because of its absence in the training data . This again implies that if we only use the clicked events for each individual user , it would result in a situation in which a large number of items will have a zero relevancy score for the user . Finally , we calculate the number of features that exist in both clicked and skipped items for each user . In Figure 4(c ) , we can see that on average 45 % features in clicked entities are shared in both of these items of each user . Thus , if we select profile features only based on clicked items , these features also end up being present in items that are skipped . When such user profiles are used to measure relevancy of a content item to the user , they end up providing misleading information . This observation motivates our model choice to enforce user profile terms to be discriminative .
2283 Metrics TFIDF Chi Square Test Independent Learning Model Latent Factor Model ( K=50 )
Table 2 : Offline Performance MRR 0.65085 0.66807 0.69366 0.70490
MAP 0.61053 0.62360 0.64478 0.66278
AUC
0.78427 0.78875 0.80565 0.82139
In the experiments , we use inner product value between user profiles and item features to generate the ranking score for each user item pair . Then we rank the items for each user based on these scores and check the positions of those clicked items . Basically , if clicked items can be ranked higher than skipped items , model performance is better . Three ranking metrics are used in our offline evaluation : mean average precision ( MAP ) , and mean reciprocal rank ( MRR ) and area under the curve ( AUC ) . The criteria are defined as ni i=1 m m m i=1 i=1
MAP =
MRR =
AUC =
1 m
1 m
1 m k=1 P ( k ) ni
1 r1 i
( i ) − Pi(Pi + 1)/2 j rj
Pi ∗ Ni
( 23 )
( 24 )
( 25 ) where P ( k ) is precision at k , ni is the number of items related to user ui , r1 is the rank of the first clicked item of i user ui , rj i is the position of the j th clicked item of user ui , Pi is the number of clicked items and Ni is the number of skipped items of user ui . 511 Performance Table 2 summarizes the results of our offline evaluation for all the four approaches investigated in our work . Latent factor model ( LFM ) consistently outperforms other baselines on all metrics . LFM on average achieves better MAP , compared to TFIDF , Chi Square ( CS ) test , and independent learning model ( ILM ) respectively . LFM also shows significant improvements on MRR and AUC . The better performance of LFM over the independent model can be ascribed to the fact that , LFM considers more knowledge from different users collectively and captures more aspects of users’ interests . Due to the data sparseness , not all users’ interests can be reflected using only the data from the user . Due to the transferability of the feature factors , relevant but unobserved features can be included into users’ profiles confidently . This gives us the added advantage of exploring users’ interests . Thus the recall of the model can be improved . On the other hand , although generative statistical models consider data from multiple users and select those features that can distinct the user from others , they do not consider discriminability of those features for each user . Thus , they may introduce unnecessary features from irrelevant items . However , LFM not only captures the crossuser influences but keeps the user specific distribution over factors and maximizes the dependency of profiles and users’ feedbacks , hence solving the above problems . 512 Latent Factor Model Analysis In this section , we analyze the performance of latent factor model in depth by studying its convergence and the effective ness of the number of dimensions , K . We plot the metrics , including MAP , MRR and AUC on training data with different numbers of iterations to check the convergence . From Figure 5(a ) , we can observe that , all of ranking metrics become stable and close to 1 after 40 iterations . This suggests that the proposed model can fit the data well . In addition , for different numbers of latent factors , we plot its effectiveness in Figure 5(b ) . As can be seen , the performance improves as the number of factors increases . However , when the number of factors is large enough , the relative performance gain becomes small . For example , the metrics with 100 factors are similar with those with only 50 factors . This helps control the model complexity to avoid overfitting . We also test different strategies to construct the feature factors S under the distributed environments . As we stated in Section 4 , the learning of S requires communications among different machines under the Map/Reduce framework . If we implement the optimization fully distributed , eg , collecting sub gradients from each machine , broadcasting the aggregated gradients and then updating the model , the communication overhead becomes very high . Thus , we propose to use a pseudo parallel approach which averages all learned matrices S from all machines . As a comparison , we also introduce a baseline that learns S in a single machine based on those users with sufficient historical data ( 1 million users in our experiment ) . The results can be found in Figure 5(c ) , where AV G1 means only one averaging operation is executed and AV G2 means we use the averaging result from the first iteration as the initialization value , call the learning process again to get new sub models from each machine and do the second averaging operation . The motivation is to ensure the entity factors can be updated based on the updated user factors to obtain a better representation as averaging may not guarantee to get an optimal value . The results show that , after two averaging operations , the performance of the model using averaging strategy is similar to that trained on a single machine . This suggests the effectiveness of our training strategy . We are still developing a more general solution based on parameter server [ 13 ] , which can achieve better performance .
5.2 Online A/B Tests
We also tested our proposed models via A/B tests with live traffic on Yahoo Stream . As detailed in Section 2 , items from a large corpus are ranked using a machine learned ranking model that uses three broad classes of features . In our experimental setup , a random sample of users were partitioned into a control and a treatment group . The users in the control group had their profiles built via the Chi Square test model and the users in the treatment group had their profiles built via LFM . Holding all other features constant , the only difference between the control and treatment group was the computation of the relevance score between the corresponding profile and the content item . The performance of the two models was measured by CTR ( click through rate ) . The experiment was run till the CTR metric achieved statistical significance . As indicated by the offline experiments , the LFM showed a substantial , statistically significant , lift of ∼ 10 % as compared to the control bucket with the ChiSquare based model . In the future , we plan to test out all baselines not only on the news stream but in other content recommendation properties , including Yahoo Recommends .
2284 ( a )
( b )
( c )
Figure 5 : Model Analysis . ( a ) shows the convergence of LFM with increasing number of iterations . ( b ) shows the performance of LFM with increasing number of latent factors . ( c ) shows the performance with different approaches to construct S .
6 . RELATED WORK
Our work in this paper is in the intersection of three research areas , user profiling , recommendation systems and latent factor models .
User profiling in principle is to find a representation of the user ’s interest in the same feature space as that of the items being recommended [ 7 ] . One of the most typical tasks involving user profiling is content recommendation [ 18 , 28 , 32 ] . A recent review of the interaction between user profiling and content based recommendation can be found in [ 15 ] . User profiling was also applied on personalized web search to enhance the user experience [ 26 , 22 ] . However , previous approaches only focus on features that users click or read , that is positive interactions , and ignore the negative feedback . Such profiles may fail to detect the true user interests and lack discriminability in prediction . In addition , the proposed LFM model can leverage relevant features from other users and thus provide an exploration function while prior work does not .
Recommendation systems aim to push relevant content to users based on users’ preference . Basically , there are two kinds of recommendations , collaborative filtering and content based recommendation . Collaborative filtering is the process tof filtering information or patterns using techniques involving collaboration among users , etc . Typical methods include nearest neighborhood models [ 21 ] and matrix factorization [ 11 ] methods . Content based recommendation differentiates itself from collaborative filtering mainly in the sense that explicit content features of items ( and/or users ) are exploited [ 1 ] . In addition , content based recommendation is particularly critical for applications where a significant number of cold start items constantly appear , which is a typical case in the domain of news recommendation . A variety of methods have been proposed and shown to be effective for news recommendation , such as spatialtemporal model [ 3 ] , probabilistic models [ 14 ] , click shaping [ 4 ] , hyper graph learning [ 12 ] and activity ranking [ 5 ] . The method proposed in this paper can be considered as a hybrid method that embeds content based recommendation in a collaborative filtering framework . Besides this , our work exploits latent factor models that learns user profiles via addressing a multi task learning problem . Our focus is to improve user profiling and demonstrate its efficacy for content recommendation . The user profiles generated by our models can be easily integrated with previous work to further improve content recommendation . We also noticed that some techniques were proposed to correct click and position bias in real world applications [ 5 ] , which can be applied on our framework as well to improve the effectiveness .
Last but not least , our contribution is closely related to latent factor models , a particular set of models well suited for recommendation systems [ 2 , 11 ] . Conventionally , latent factor models decompose user item relation matrix into latent user factors and latent item factors , which can be used for future recommendation . Our proposed model learns latent user factors for each individual user , and in meanwhile , a latent correlation matrix of content features that is shared across all the users . Note that our model is similar to the general framework of collective matrix factorization [ 23 ] , but our model is specialized for the content recommendation domain , and the joint factorization is designed to learn improved user profiles . Recently , large scale matrix factorization is also gaining much traction [ 31 , 10 , 25 ] . There are also some open source products that are developed to handle big data , such as MLlib [ 17 ] and GraphLab [ 16 ] . However , comparing to these prior works , we integrate our end to end user profiling system into real products , demonstrating how such systems work in real world applications .
7 . CONCLUSIONS
In this paper , we proposed a user profiling method based on latent factor models for large scale content recommendations . We tackled three challenges in real world applications : data sparsity and cold start items , complex implicit feedback and large scale applications . The proposed latent factor model ( LFM ) collectively exploits positive and negative implicit feedback from all users on content items to construct both user and feature factors . We then leverage the LFM to represent each user as an interest vector . Thus , brand new items with no user interaction can still be retrieved for recommendation using the users’ interest vectors . In addition , since the feature factors are shared by all users , we can bootstrap profiles even for users who have very limited data . We also proposed a large scale framework based on Map/Reduce to cope with massive amounts of interaction data . The proposed framework can easily accommodate different loss functions . We conducted empirical studies on real world datasets from Yahoo Recommends , where LFM significantly outperforms other baselines . A/B test on the Yahoo Stream also indicates that LFM can work well in realworld applications . In future , we plan to develop a fully distributed version of our model by decoupling the learning process using the parameter server implementation [ 13 ] .
2285 8 . REFERENCES
[ 1 ] G . Adomavicius and A . Tuzhilin . Toward the next generation of recommender systems : A survey of the state of the art and possible extensions . IEEE Trans . on Knowl . and Data Eng . , 17(6):734–749 , 2005 .
[ 2 ] D . Agarwal and B C Chen . Regression based latent factor models . In Proceedings of KDD , pages 19–28 . ACM , 2009 .
[ 3 ] D . Agarwal , B C Chen , and P . Elango . Spatio temporal models for estimating click through rate . WWW ’09 , pages 21–30 , 2009 .
[ 4 ] D . Agarwal , B C Chen , P . Elango , and X . Wang .
Personalized click shaping through lagrangian duality for online recommendation . SIGIR ’12 , pages 485–494 , 2012 .
[ 5 ] D . Agarwal , B C Chen , R . Gupta , J . Hartman , Q . He , A . Iyer , S . Kolar , Y . Ma , P . Shivaswamy , A . Singh , and L . Zhang . Activity ranking in linkedin feed . In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD ’14 , pages 1603–1612 , New York , NY , USA , 2014 . ACM .
[ 6 ] J . Bobadilla , F . Ortega , A . Hernando , and A . Guti´eRrez .
Recommender systems survey . Know. Based Syst . , 46:109–132 , July 2013 .
[ 7 ] L . Chen and P . Pu . Survey of preference elicitation methods . Technical Report EPFL REPORT 52659 , EPFL , 2004 . to have arisen from random sampling . Philosophical Magazine , 50:157–175 , 1900 .
[ 20 ] S . Rendle , C . Freudenthaler , Z . Gantner , and
L . Schmidt Thieme . Bpr : Bayesian personalized ranking from implicit feedback . In Proceedings of the Twenty Fifth Conference on Uncertainty in Artificial Intelligence , UAI ’09 , pages 452–461 , Arlington , Virginia , United States , 2009 . AUAI Press .
[ 21 ] B . Sarwar , G . Karypis , J . Konstan , and J . Riedl .
Item based collaborative filtering recommendation algorithms . In Proceedings of the 10th International Conference on World Wide Web , WWW ’01 , pages 285–295 , New York , NY , USA , 2001 . ACM .
[ 22 ] A . Sieg , B . Mobasher , and R . Burke . Web search personalization with ontological user profiles . In Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management , CIKM ’07 , pages 525–534 , New York , NY , USA , 2007 . ACM .
[ 23 ] A . P . Singh and G . J . Gordon . Relational learning via collective matrix factorization . In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining , pages 650–658 , 2008 .
[ 24 ] K . Sparck Jones . Document retrieval systems . chapter A
Statistical Interpretation of Term Specificity and Its Application in Retrieval , pages 132–142 . Taylor Graham Publishing , London , UK , UK , 1988 .
[ 8 ] A . S . Das , M . Datar , A . Garg , and S . Rajaram . Google
[ 25 ] D . H . Stern , R . Herbrich , and T . Graepel . Matchbox : Large news personalization : scalable online collaborative filtering . In Proceedings of the 16th International Conference on World Wide Web , pages 271–280 . ACM , 2007 .
[ 9 ] J . Dean and S . Ghemawat . Mapreduce : Simplified data processing on large clusters . Commun . ACM , 51(1):107–113 , Jan . 2008 .
[ 10 ] R . Gemulla , E . Nijkamp , P . J . Haas , and Y . Sismanis .
Large scale matrix factorization with distributed stochastic gradient descent . In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD ’11 , pages 69–77 , New York , NY , USA , 2011 . ACM .
[ 11 ] Y . Koren , R . Bell , and C . Volinsky . Matrix factorization techniques for recommender systems . Computer , 42(8):30–37 , Aug . 2009 .
[ 12 ] L . Li and T . Li . News recommendation via hypergraph learning : Encapsulation of user behavior and news content . WSDM ’13 , pages 305–314 , 2013 .
[ 13 ] M . Li , D . G . Andersen , J . W . Park , A . J . Smola , A . Ahmed , V . Josifovski , J . Long , E . J . Shekita , and B Y Su . Scaling distributed machine learning with the parameter server . In Proceedings of the 11th USENIX Conference on Operating Systems Design and Implementation , OSDI’14 , pages 583–598 , Berkeley , CA , USA , 2014 . USENIX Association .
[ 14 ] J . Liu , E . Pedersen , and P . Dolan . Personalized news recommendation based on click behavior . IUI ’10 , 2010 . scale online bayesian recommendations . In Proceedings of the 18th International Conference on World Wide Web , WWW ’09 , pages 111–120 , New York , NY , USA , 2009 . ACM .
[ 26 ] K . Sugiyama , K . Hatano , and M . Yoshikawa . Adaptive web search based on user profile constructed without any effort from users . In Proceedings of the 13th International Conference on World Wide Web , WWW ’04 , pages 675–684 , New York , NY , USA , 2004 . ACM .
[ 27 ] K . Varda . Protocol buffers : Google ’s data interchange format . Technical report , Google , 6 2008 .
[ 28 ] G . I . Webb , M . J . Pazzani , and D . Billsus . Machine learning for user modeling . User Modeling and User Adapted Interaction , 11:19–29 , 2001 .
[ 29 ] X . Yi , L . Hong , E . Zhong , N . N . Liu , and S . Rajan . Beyond clicks : Dwell time for personalization . In Proceedings of the 8th ACM Conference on Recommender Systems , RecSys ’14 , pages 113–120 , New York , NY , USA , 2014 . ACM .
[ 30 ] J . Zhang , Z . Ghahramani , and Y . Yang . Flexible latent variable models for multi task learning . Mach . Learn . , 73(3):221–242 , Dec . 2008 .
[ 31 ] Y . Zhuang , W S Chin , Y C Juan , and C J Lin . A fast parallel sgd for matrix factorization in shared memory systems . In Proceedings of the 7th ACM Conference on Recommender Systems , RecSys ’13 , pages 249–256 , New York , NY , USA , 2013 . ACM .
[ 15 ] P . Lops , M . De Gemmis , and G . Semeraro . Content based
[ 32 ] I . Zukerman and D . W . Albrecht . Predictive statistical models for user modeling . User Modeling and User Adapted Interaction , 11:5–18 , 2001 . recommender systems : State of the art and trends . In Recommender systems handbook , pages 73–105 . Springer , 2011 .
[ 16 ] Y . Low , D . Bickson , J . Gonzalez , C . Guestrin , A . Kyrola , and J . M . Hellerstein . Distributed graphlab : A framework for machine learning and data mining in the cloud . Proc . VLDB Endow . , 5(8):716–727 , Apr . 2012 .
[ 17 ] X . Meng , J . K . Bradley , B . Yavuz , E . R . Sparks ,
S . Venkataraman , D . Liu , J . Freeman , D . B . Tsai , M . Amde , S . Owen , D . Xin , R . Xin , M . J . Franklin , R . Zadeh , M . Zaharia , and A . Talwalkar . Mllib : Machine learning in apache spark . CoRR , abs/1505.06807 , 2015 . [ 18 ] S . E . Middleton , N . R . Shadbolt , and D . C . De Roure .
Ontological user profiling in recommender systems . ACM Trans . Inf . Syst . , 22(1):54–88 , 2004 .
[ 19 ] K . Pearson . On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that can be reasonably supposed
2286
