Tornado Forecasting with Multiple Markov Boundaries
Kui Yu∗,1 , Dawei Wang∗,2 , Wei Ding†,2
Jian Pei1 , David L . Small3 , Shafiqul Islam3 , Xindong Wu4,5
1School of Computing Science , Simon Fraser University , Burnaby , BC , Canada
2Department of Computer Science , University of Massachussets Boston , Boston , USA
3Department of Civil and Environmental Engineering , Tufts University , Boston , USA
4Department of Computer Science , Hefei University of Technology , Hefei , China
5Department of Computer Science , University of Vermont , Burlington , USA
{kuiy,jpei}@cssfuca , {daweiwang,ding}@umbedu
{DavidSmall,ShafiqulIslam}@tuftsedu , xwu@uvm.edu
ABSTRACT Reliable tornado forecasting with a long lead time can greatly support emergency response and is of vital importance for the economy and society . The large number of meteorological variables in spatiotemporal domains and the complex relationships among variables remain the top difficulties for a long lead tornado forecasting .
Standard data mining approaches to tackle high dimensionality are usually designed to discover a single set of features without alternating options for domain scientists to select more reliable and physical interpretable variables .
In this work , we provide a new solution to use the concept of multiple Markov boundaries in local causal discovery to identify multiple sets of the precursors for tornado forecasting . Specifically , our algorithm first confines the extremely large feature spaces to a small core feature space , then it mines multiple sets of the precursors from the core feature space that may equally contribute to tornado forecasting . With the multiple sets of the precursors , we are able to report to domain scientists the predictive but practical set of precursors .
An extensive empirical study is conducted on eight benchmark data sets and the historical tornado data near Oklahoma City , OK in the United States . Experimental results show that the tornado precursors we identified can help to improve the reliability of long lead time catastrophic tornado forecasting .
Categories and Subject Descriptors I54 [ Pattern Recognition ] : Applications Weather Forecasting ∗K . Yu and D . Wang contributed equally to this work †Corresponding Author
Keywords Tornado Forecasting , Multiple Markov boundaries , Distributed feature data , Core feature space
INTRODUCTION
1 . 1.1 Tornado Forecasting
A tornado is one of the most powerful , unpredictable and destructive weather phenomena on Earth . Tornado disasters result in billions of dollars of damages each year and cause more loss of life than most other weather related hazards . For example , in the year of 2013 , 811 confirmed tornadoes cause 55 fatalities and damages up to $3.6 billion in the United States1 . Reliable tornado forecasts with a longlead time can greatly support emergency response and are of vital importance for the economy and society . Currently National Weather Service ( NWS ) issues tornado warnings based on “ detection ” : a tornado threat is broadcasted after being observed , either on site or through radars [ 3 ] . Simulation based warning systems can increase the prediction lead time up to several hours , but most of the predictions are false alarms [ 15 ] .
Comparing to other domains like electronic advertising , the success of big data induced process in studying the weather system is limited [ 7 , 19 ] . Among all the reasons accounting for the slow progress , for data driven methods trying to model weather dynamics the main difficulty lies in addressing the high dimensionality and the complex relationships among variables . The number of meteorological variables contributing to tornadoes , from the Cartesian product between the spatial and temporal domains , is enormous . Tornadoes are usually developed from thunderstorms and the quasi geostrophic theory [ 8 ] demonstrates that the development of storm events requires a coupling between upper and lower levels of the atmosphere . The atmospheric variables related to this vary both horizontally and vertically and have complex relationships between each other . Even the most efficient Monte Carlo methods will suffer from computational infeasibility for such high dimensional and complex data sets .
Dimension reduction is the standard machine learning approach to deal with high dimensionality [ 10 , 20 , 21 ] . These methods usually report a single subset of features without
1http://wwwspcnoaagov/climo/torn/STATIJ13txt
2237 alternative options . In our study , the tornado precursors identified are meteorological predictor variables with certain spatial and temporal information , but due to the complex interactions within the variables , the problems of only identifying a single choice precursors are as follows .
( 1 ) Key interpretable precursors may be missed . Due to spatial and temporal adjacency , many precursors contain equivalent information for prediction , because these precursors are not physically independent of one another . For example , when there is a dropping in sea level pressure ( anomalously low sea level pressure is an important characteristic of atmospheric regimes , which may lead to extreme precipitations ) , at the same location there must also be convergence of the winds near the surface and divergence of the winds at the top of the atmosphere . Also , the field of pressure vertical velocity may be strongly negative . In this case , the low level winds , high level winds , and vertical motions in the same location are considered equivalent precursors . A typical feature selection method will only identify one of the three equivalent precursors , the others will not be reported to the domain scientists .
( 2 ) The precursors identified may be considered impractical . It is useful to explore alternative cost effective but equally effective solutions in the cases where different precursors may have different costs/utilities associated with their acquisition/quality in a forecasting model at the next step . For instance , although the three precursors mentioned in the above example are considered as equivalent precursors , compared to high level and low level winds , vertical motion is a noisy and unreliable precursor . Using vertical motion as a precursor in a prediction model introduces extra uncertainty . Accordingly , if we know all of the three precursors containing equivalent information for forecasting , we can select the low level winds or high level winds for reliable prediction .
1.2 Multiple Markov Boundaries for Tornado
Forecasting
It is an important yet new research topic to identify multiple Markov boundaries from data using Bayesian inference in the domain of causal discovery [ 17 ] . A Bayesian network is presented by a directed acyclic graph G and a joint probability distribution P over a set of features F [ 11 ] . If every conditional independence entailed by G is present in P , G is said to be faithful to P [ 11 ] . If a data distribution and an underlying Bayesian network which models that domain are faithful to each other , the Markov boundary of a node is unique and contains its parents , children , and the parents of the children ( spouses ) [ 11 , 1 ] . In the past decade , people focused on identification of a single Markov boundary under the faithfulness assumption for causal discovery [ 1 , 18 ] . However , tornado and other many real world data distributions often violate the faithfulness condition due to various factors , such as ( but not limited to ) small sample size , noise in data , and hidden variables . Thus the Bayesian networks built from such data often contains multiple Markov boundaries [ 12 , 17 ] . Developing methods of discovery of multiple Markov boundaries would improve the discovery of the underlying mechanisms to avoid overlooking key causal variables , and thus this can be useful to explore alternative cost effective but equally effective solutions in prediction in applications where different variables may have different costs/utilities associated with their acquisition/quality .
1.3 Our Contributions
Our contributions to tornado forecasting are below . • To cope with high dimensionality , we define a new concept of a core feature space to represent the feature space of all possible Markov boundaries of a target variable , and then confine mining multiple Markov boundaries to this core feature space instead of the original entire feature space .
• To discover this core feature space from distributed feature data sets , we process distributed feature data sets in a sequential scan without loading the whole tornado data set in memory in advance . Specifically , we process each distributed tornado data set one at a time , and features in each set are processed one by one in a sequential scan .
• We design and implement the MB DEA algorithm , to discover multiple Markov Boundaries from Distributed fEature dAta for tornado forecasting . MB DEA firstly discovers the core feature space from which multiple Markov boundaries are mined .
• We apply the MB DEA algorithm to study the historical tornado data near Oklahoma City , OK . Our empirical study includes 810,000 features of 35 years data . We are able to identify physically meaningful and practical tornado precursors which may lead to reliable and long lead time of catastrophic tornado forecasting .
The remainder of the paper is organized as follows . Section 2 presents related work . Section 3 describes the tornado data set and Section 4 proposes the MB DEA algorithm , respectively . Section 5 reports our experimental results on tornado forecasting . Finally , Section 6 concludes the paper .
2 . RELATED WORK
We will give a brief review of tornado forecasting and multiple Markov boundaries in this section . Tornadoes are usually developed within thunderstorms and most tornado warning systems are based on the prediction of thunderstorms [ 3 ] . One official National Weather Service product is the tornado watch , which is based on weather forecasting model outputs and observations , with a lead time up to several hours . Simulations of supercell thunderstorms are performed in [ 16 ] with the aim of discovering precursors of tornadoes . Such process usually resulted in a large number of false alarms ( false positives ) because only very few storms would produce tornadoes . A three dimensional ( in space and time ) object identification algorithm is applied in [ 4 ] for forecasting tornado path lengths through the predictions of hourly maximum updraft helicity as a measure of storm severity . In [ 13 ] the authors implement principal component analysis ( PCA ) on the outputs from a simulation model ( the Weather Research and Forecasting model ) , and build a tornado prediction model using supported vector machine with the PCA results . They have achieved an accuracy of 0.7 with a one day lead time .
Causal discovery is of fundamental and practical interest in many areas of science and technology [ 11 ] . Global causal induction attempts to learn a complete Bayesian network over all features on training data . However , global causal induction is not scalable and cannot even handle thousands
2238 of features [ 5 ] . Local causal discovery aims to learn a local causal structure closely related to a target feature of interest , ie , a Markov boundary of a target feature of interest . If a joint probability distribution satisfies the faithful condition , it is guaranteed to have a unique Markov boundary for every node in a Bayesian network [ 1 ] . Accordingly , in the past decade , most of the existing algorithms of local causal discovery focused on identification of a single Markov boundary under the faithfulness assumption [ 1 , 12 , 18 ] .
However , many real world data distributions often violate the faithfulness condition due to small sample sizes , noise in data , and hidden variables , and thus contain multiple Markov boundaries [ 12 , 17 , 22 ] . Algorithms of multiple Markov boundaries attempts to discover all Markov boundaries of a target feature containing in data without missing causative variables [ 17 ] . Among the most notable advances in the field of discovery of multiple Markov boundaries are the KIAMB algorithm and the TIE* algorithm [ 17 ] . Pe˜na et al . [ 12 ] proposed a stochastic Markov boundary algorithm , called KIAMB by employing a stochastic search heuristic that repeatedly disrupts the order in which features are selected for inclusion into a Markov boundary with the probability p at each round , thereby introducing a chance of identifying alternative Markov boundaries of a target feature . The limitation is that we do not know how many iterations the KIAMB algorithm needs to run because the exact number of Markov boundaries of a target feature is unknown and varies in different data . To solve this problem , Statnikov et al . [ 17 ] recently proposed the TIE* ( Target Information Equivalence ) algorithm and proved that TIE* can discover all Markov boundaries under the non faithful condition .
Integrating the drawbacks of the existing work for longlead tornado forecasting and multiple Markov boundaries , it provides a natural choice to find multiple precursors for longlead tornado prediction using multiple Markov boundaries . However , both the KIAMB and TIE* algorithms focus on mining multiple Markov boundaries from data on a single feature set . With the real world tornado data , the KIAMB and TIE* algorithms face the challenges of both very high dimensionality and multiple feature sets . This motivates us to further investigate new algorithms of multiple Markov boundaries for long lead tornado prediction .
3 . THE TORNADO DATA SET
The tornado data set contains eight explanatory variables and tornado information near Oklahoma City , OK , one of the most tornado prone areas in the United States . All the explanatory variables are sampled at the spatial domain of 90◦E to 357.5◦E and 0◦N to 90◦N with a horizontal resolution of 2.5◦ ×2.5◦ ( totally 2 , 700 locations ) and a daily temporal resolution for the months of March , April and May ( MAM , when the highest frequency of violent tornadoes occurs in the studied area ) for the years 1979 2013 . The tornado study area near Oklahoma City is located in the spatial region of 261.25◦E− 263.75◦E and 33.75◦N − 36.25◦N . The eight variables at different levels ( Table 1 ) are selected from the NCEP NCAR Reanalysis data set [ 9 ] by the domain scientists ( co authors of the paper ) for the study . In particular , the Relative Humidity data only goes from 1000hPa to 300hPa because the amount of water in the upper troposphere was thought to be negligible when the dataset was designed . Two variables only have one single level ( for Sea Level Pressure , the values represent the surface level ; for
Precipitable Water , the values are column integrated from all the levels ) . A particular day of the tornado study area is labeled as a positive instance if one or more EF 1(Enhanced Fujita scale [ 6 ] ) or above tornadoes are reported , otherwise a negative instance . In total there are 98 tornado days ( positive instances ) out of the 3 , 045 days study period .
We set our “ look ahead ” days as 5 . For example , to forecast tornado situations at tomorrow ( day0 ) in the study area , we will look at the explanatory variables from today ( day 1 ) back to previous five days ( day 5 ) . The tornado data set presents two characteristics of a difficult task :
• Extremely high dimensionality . With such a setting , the total number of precursors ( features ) for one instance is 810 , 000 in the tornado data set ( eg , the variable of Relative Humidity has 8 levels , 5 days , and 2700 locations . In total there are 8×5×2700=108 , 000 features ) . Any existing algorithm of multiple Markov boundaries cannot cope with such large number of features .
• Distributed feature data . The whole tornado data set is large ( more than 13GB in total ) , and thus we have the data distributed in eight data sets according to the eight explanatory variables and one class attribute set . On average each explanatory variable set still has more than 100 , 000 features . Mining multiple Markov boundaries from multiple feature data sets is an untouched research topic .
4 . MACHINE LEARNING FORMULA
TION
In this section , we discuss the MB DEA algorithm for tornado forecasting . The algorithm tackles the discovery of multiple Markov Boundaries from Distributed fEature dAta . The design of the MB DEA algorithm consists of ( 1 ) finding the core feature space from the distributed tornado data , and ( 2 ) mining multiple Markov boundaries from the core feature space . 4.1 Notations and Definitions
Given a training data set D containing N training instances and M features , we define a distributed feature data set as that D is vertically divided into W feature blocks without overlapping features between each block , that is , D = {B1 , B2,··· , BW} which is distributed in W files . Fi ( 1 ≤ i ≤ M ) is an feature within Bj ( 1 ≤ j ≤ W ) and C is the class attribute . Let P be a joint probability distribution on a set of random variables F via a directed acyclic graph G . We call the triplet F , G , P a Bayesian network if F , G , P satisfies the Markov condition : every variable is conditionally independent of any subset of its non descendant variables given its parents [ 11 ] .
In the rest of the paper , the terms “ variable ” and “ feature ” are used interchangeably . To measure the statistical relationships between features , we adopt the measure of mutual information [ 14 ] . Given two variables X and Y , the mutual information between X and Y is defined as follows .
I(X ; Y ) = H(X ) − H(X|Y ) where H(Y ) and H(Y |Z ) are computed as follows .
H(X ) = − xi∈X
( P ( xi ) log2(P ( xi ) )
( 1 )
( 2 )
2239 Table 1 : Meteorological variables
Name Temperature Geopotential Height Meridional Wind Zonal Wind Pressure Vertical Velocity Relative Humidity Sea Level Pressure Precipitable Water
Level(hPa ) 200,250,300,400,500,600,700,850,925,1000 200,250,300,400,500,600,700,850,925,1000 200,250,300,400,500,600,700,850,925,1000 200,250,300,400,500,600,700,850,925,1000 200,250,300,400,500,600,700,850,925,1000 300,400,500,600,700,850,925,1000 – –
H(X|Y ) = − yj∈Y xi∈X
P ( yj )
( P ( xi|yj ) log2(P ( xi|yj ) ) ( 3 )
= −
P ( xjyk|zi )
From Equations ( 1 ) to ( 3 ) , the conditional mutual information is computed by I(X ; Y |Z ) = H(X|Z ) − H(X|Y Z )
P ( xjyk|zi ) log2 zi∈Z
P ( zi )
P ( xj|zi)P ( yk|zi ) ( 4 ) The lower cases xi , yi , and zi in the above equations de xj∈X yk∈Y note possible values that the variables X , Y and Z take .
Definition 1
( Faithfulness ) .
[ 11 ] Give a Bayesian network F , G , P , G is faithful to P over F if and only if every independence present in P is entailed by G and the Markov condition . P is faithful if and only if there exists a directed acyclic graph G such that G is faithful to P .
Definition 2
( Markov boundary ) .
[ 11 ] a Bayesian network satisfies the faithfulness , the Markov boundary of any node T in this Bayesian network is unique with the set of parents , children and spouses ( the parents of the children of T ) of T .
If
However , if a Bayesian network does not satisfy the faithfulness , the Markov boundary of any node may not be unique [ 17 ] . We use Theorem 1 to explicitly construct and verify multiple Markov boundaries when the distribution P violates the faithful condition .
Theorem 1 .
[ 17 ] If M B1 is a Markov boundary of T that contains a feature set S1 , and there exists a subset S2 such that M B2 ⊆ ( M B1 − S1)∪ S2 , if I(T ; M B1|M B2 ) = 0 , then M B2 is also a Markov boundary of T .
In Theorem 1 , both M B1 and M B2 are Markov boundaries of T , since S1 ⊂ M B1 and S2 ⊂ M B2 contain the equivalent information about T . 4.2 The Core Feature Space
According to the design of the MB DEA algorithm , the key to the algorithm is how to discover the core feature space from distributed feature data . The core feature space we are looking for is defined as a feature space that contains all possible Markov boundaries of a target feature . By theorem 1 , we get Corollary 1 below .
Corollary 1 . Assuming M B1 is a Markov boundary of T and M B2 ⊆ {M B1 − {Fi} ∪ {Fj}} , if I(C ; Fj|Fi ) = 0 , M B2 is also a Markov boundary of T .
Figure 1 : An example of M B(T ) and CF S(T )
Definition 3 . Assuming M B(C ) is a Markov boundary of C and Fi ∈ M B(C ) , we define ( Fi ) as the set of features that satisfies the term ∀Fj ∈ ( Fi ) st I(C ; Fj|Fi ) = 0 .
By Corollary 1 and Definition 3 , we define the core feature space of C as follows .
Definition 4
( Core feature space ) . Assuming
M B(C ) is a Markov boundary of C and Fi ∈ M B(C ) , the core feature space ( CFS ) of C is defined as
CF S(C ) = {M B(C ) ∪
( Fi)} .
Fi∈M B(C )
With Definition 4 , our first problem is how to find M B(C ) efficiently . To find M B(C ) , in general , we can use the existing single Markov boundary algorithms , such as HITIONMB [ 1 ] and MMMB [ 18 ] . For example , assuming a data set contains 20 features F = {F1 , F2,··· , F20} and feature F20 is considered as a target feature T , if M B(T ) = {F5 , F2 , F12 , F10} , a Markov blanket of T , is found by HITION MB or MMMB and ( F5 ) = {F1 , F4 , F9} , Figure 1 illustrates the relationships between M B(T ) ( the left figure ) and CF S(T ) ( the right figure ) .
To find the core feature space from distributed feature data , we further analyze the relationships between features Fi and Fj in Corollaries 2 and 4 as follows .
Corollary 2 . A feature X is correlated to Y with re spect to a feature subset S if I(X ; Y |S ) > 0 .
Proof . By Eq ( 4 ) , we can see that the term I(X ; Y |S ) = 0 holds only when X and Y are conditionally independent given S . The corollary is proven accordingly .
Corollary 3 . If Fi is correlated to C and I(C ; Fj|Fi ) =
0 holds , then I(Fi , C ) > I(Fj , C ) .
( cid:1832)(cid:2873 ) ( cid:1832)(cid:2870 ) T ( cid:1832 ) ( cid:1832)(cid:2870 ) T ( cid:1832)(cid:2870 ) ( cid:1832)(cid:2874)(cid:481)(cid:1832)(cid:2876 ) ( cid:1832)(cid:2873 ) ( cid:1832)(cid:481)(cid:1832)(cid:2872)(cid:481)(cid:1832)(cid:2877 ) ( cid:1832)(cid:2870 ) ( cid:1832)(cid:481)(cid:1832)(cid:2873 ) ( cid:1832 ) ( cid:1832)(cid:2875)(cid:481)(cid:1832)(cid:2871 ) , ( cid:1832)(cid:2875 ) ( cid:1832)(cid:2870)(cid:1515)(cid:1336)(cid:4666)(cid:1832)(cid:2870)(cid:4667 ) ( cid:1832)(cid:1515)(cid:1336)(cid:4666)(cid:1832)(cid:4667 ) ( cid:1832)(cid:2870)(cid:1515)(cid:1336)(cid:4666)(cid:1832)(cid:2870)(cid:4667 ) ( cid:1839)(cid:1828)(cid:4666)(cid:1846)(cid:4667 ) ( cid:1832)(cid:2873)(cid:1515)(cid:1336)(cid:4666)(cid:1832)(cid:2873)(cid:4667 ) CFS(T)=MB(T)(cid:1515)(cid:1666)(cid:1336)(cid:4666)(cid:513)(cid:3014)(cid:3003)(cid:513)(cid:2880)(cid:1832)(cid:513)(cid:1832)(cid:1488)ff(cid:4666)(cid:4667)(cid:4667 ) 2240 Algorithm 1 : The MB DEA Algorithm Data : D = {B1 , B2,··· , BW} ;
M B(C ) = {} ;CF S(C ) = {} ;i = 0 ; δ = max(I(Y ; C ) , I(Fi ; C ) )
Discard Fi ; goto step 23 ;
Get new feature set(Bj ) ; repeat end for each feature Y ∈ M B(C ) do
( Y ) = ( Y ) + Fi ; = + Fi ; Goto step 23 ; if I(Y ; C ) > I(Fi ; C ) and I(Fi ; Y ) ≥ δ then i = i + 1 ; Get new feature(Fi , Bj ) ; if I(Fi ; C ) = 0 then
1 /*Steps 2 25 : Mining the core feature space CF S*/ 2 for j=1 to W do 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 end 25 CF S(C ) = M B(C ) ∪ 26 /*Steps 27 35 : Mining Markov boundaries from CF S*/ 27 Use HITION PC to learn a Markov boundary M B1 of
M B(C ) = M B(C ) − Y ; ( Fi ) = ( Fi ) + Y ; = − ( Y ) ; end if I(Fi ; C ) > I(Y , C ) and I(Fi ; Y ) ≥ δ then end end M B(C ) = M B(C ) ∪ Fi ; until Fi is the last feature of Bj ;
C from D over CF S(C ) ( the original distribution ) ;
28 Output M B1 ; 29 repeat 30
31
Generate a data set De from the embedded distribution by removing a subset of features within the discovered Markov boundaries from CF S(C ) ; Use HITION PC to learn a Markov boundary M Bnew of C from De ; if acc(M Bnew ) ≥ acc(M B1 ) then
32 33 34 35 until all data sets De generated have been considered ; output M Bnew ; end stages are given in Algorithm 1 . One is to mine CF S(C ) from Steps 2 to 25 , and the other is to discover Markov boundaries from CF S(C ) from Steps 27 to 35 .
Steps 2 to 25 . As a feature set Bj is achieved , features within Bj are processed one by one in a sequential scan . Step 7 is to determine whether a new coming feature Fi is correlated to C ; if I(Fi ; C ) > 0 , then we consider Fi is correlated to C ; otherwise , Fi is discarded .
If Fi is correlated to C , Step 11 determines whether to keep Fi given the current set M B(C ) . If not , Fi is added to and ( Y ) , respectively . If Fi can be added to M B(C ) at Step 11 , Step 16 further prunes M B(C ) due to Fi ’s inclusion . If Y in M B(C ) is removed , the set ( Y ) is removed from accordingly . Meanwhile , Y is added to ( Fi ) . With the MB DEA algorithm , we can discover without any additional computation costs .
Figure 2 : Discovery of the core feature space of C from distributed feature data
Proof . By I(A ; B|C)− I(A ; B ) = I(A ; C|B)− I(A ; C ) , we get I(C ; Fj|Fi ) − I(C ; Fj ) = I(C ; Fi|Fj ) − I(C ; Fi ) . Since the term I(C ; Fj|Fi ) = 0 hold , then I(C ; Fi ) = I(C ; Fi|Fj)+ I(C ; Fj ) . By Corollary 2 , the corollary is proven .
Corollary 4 . If both M B1 and M B2 ⊆ ( M B1−{Fi})∪ {Fj} are Markov boundaries of C and I(Fj ; C|Fi ) = 0 holds , then I(Fi ; Fj ) ≥ max(I(Fi ; C ) , I(Fj ; C) ) .
Proof .
I(Fj ; Fi , C ) = I(Fj ; Fi ) + I(Fj ; C|Fi ) = I(Fj ; C ) + I(Fj ; Fi|C )
( 5 ) Since I(Fj ; C|Fi ) = 0 holds and Fi and Fj are correlated , we can get I(Fi ; Fj ) ≥ I(Fj ; C ) . And at the same time , the following equation also holds .
I(Fi ; Fj , C ) = I(Fi ; Fj ) + I(Fi ; C|Fj ) = I(Fi ; C ) + I(Fi ; Fj|C )
( 6 ) By Corollary 1 , we can also get I(Fi ; C|Fj ) = 0 . By Eq ( 6 ) , we have I(Fi ; Fj ) ≥ I(Fi ; C ) . With the equations ( 5 ) and ( 6 ) , Corollary 4 is proved .
By Corollaries 3 and 4 , we can get the following . Observation 1 . If I(Fi ; C ) > I(Fj ; C ) and I(Fi ; Fj ) ≥ max(I(Fi ; C ) , I(Fj ; C) ) , then Fi is in a Markov boundary of C ( M B(C) ) , and Fj is included by ( Fi ) .
4.3 Discovery of Multiple Markov Bound aries from Distributed Feature Data
Figure 2 gives the new framework to efficiently find the core feature space of C from distributed feature data . In Figure 2 , each feature block Bi is processed independently at a time , and as a feature block Bi arrives , features in Bi are processed one by one in a sequential scan .
As illustrated in Figure 2 , we discuss the pseudocode of the MB DEA algorithm in Algorithm 1 . To achieve a relatively low computational complexity to deal with highdimensional yet distributed tornado data , according to Observation 1 , Corollaries 3 and 4 , the MB DEA algorithm uses online pairwise comparisons as the selection criterion for adding features into the core feature space . In Algorithm 1 , ( Fi ) keeps the set of features correlated to Fi that satisfies Definition 3 ; dynamically keeps the ( Fi ) ; M B(C ) is defined in Definition 4 ; and CF S(C ) denotes the core feature space of C . Two key features in|M B(C)| i=1
( cid:1828 ) ( cid:1828)(cid:2870 ) ( cid:1828)(cid:3024 ) ( cid:857 ) Get feature set ( cid:1828)fi Process features in ( cid:1828 ) in a sequential scan Features as The new feature belongs to CFS(C ) ? Get a new feature from ( cid:1828 ) Add the new feature to CFS(C ) Y N No more features infi(cid:1828 ) are available ? N Y ( cid:1835)fi(cid:1828 ) the last feature set ? Y N Output CFS(C ) ( cid:1828)(cid:2871 ) 2241 Table 2 : Summary of the benchmark data sets
Data set arcene dexter dorothea colon leukemia lung cancer ovarian cancer thrombin features 10,000 20,000 100,000 2,000 7,129 12,533 2,190 139,351 training set 100 300 800 42 48 121 144 2,000 testing set 100 300 300 20 24 60 72 543
1 2 3 4 5 6 7 8
Steps 27 to 35 . We integrate the TIE* algorithm into our MB DEA algorithm to mine Markov boundaries from the discovered core feature space . The main idea of TIE* [ 17 ] is to first identify a Markov boundary of a target feature T in the original data distribution and then iteratively run a single Markov boundary induction algorithm from the embedded distributions that are obtained by removing subsets of features from the original Markov boundary in order to identify new Markov boundaries in the original distribution . From Steps 27 to 35 , with the core feature space , the TIE* algorithm can search for multiple Markov boundaries in a smaller feature space .
Step 27 uses a single Markov boundary induction algorithm HITION PC [ 1 ] to learn a Markov boundary , called M B1 , from the data set D defined on the feature set CF S(C ) ( ie , in the original distribution ) . Step 30 generates a data set De ( the embedded distribution ) that removes a subset of features from CF S ( Regarding how to generate an embedded distribution , please see the IGS algorithm in [ 17 ] , Page18 , Figure 9 ) . The motivation is that De may lead to identification of a new Markov boundary of T that was previously “ invisible ” to a single Markov boundary induction algorithm , because it is shielded by another subset of features within the discovered Markov boundaries . Step 32 uses prediction accuracy as a criterion to verify whether a discovered feature set from the embedded distribution is a new Markov boundary or not . If the prediction accuracy of M Bnew in Step 32 is not less than that of M B1 , M Bnew is also considered as a Markov boundary of C . Steps 3034 are repeated until all data sets De generated have been considered .
5 . EMPIRICAL STUDY
In this empirical study , since the state of the art multiple Markov boundary discovery algorithm , the TIE* algorithm [ 17 ] , cannot deal with the high dimensional yet distributed tornado data set , we will first validate the effectiveness and efficiency of the MB DEA algorithm against the TIE* algorithm using the benchmark data sets , and then use the MB DEA algorithm for tornado prediction . 5.1 Experiments on the Benchmark Data Sets We have chosen eight benchmark data sets as described in Table 2 , which cover a wide range of real world application domains . In Table 2 , for the first four NIPS 2003 challenge data sets and the hiva data set from the WCCI 2006 performance prediction challenge , we use the originally provided training and validation sets ; for the other five data sets we adopt the first 2/3 instances for training and the last 1/3 instances for testing . As for discrete data sets , we
Table 3 : The F ratio and MB ratio
Data set arcene dexter dorothea colon leukemia lung cancer thrombin ovarian cancer
F ratio MB ratio 0.1157 0.0057 0.0636 0.033 0.0537 000608 0.0249 0.1685
1 1 0.5 1 1 1
Figure 3 : Prediction ( classification ) accuracies of MB DEA against TIE* use mutual information to compute the correlation between features while for the continuous data sets , the Fisher ’s Ztest is employed [ 2 ] in which the significance level for the Fisher ’s Z test is set to 001 All experiments on the benchmark data sets are conducted on a computer with Intel(R ) i7 3770 3.4GHz CPU and 32GB memory .
In our experiments , each training data set in Table 2 is partitioned into ten feature sets evenly to simulate distributed feature data for MB DEA while TIE* is directly implemented on each training data set . TIE* is parameterized with HITON PC [ 1 ] as the base Markov blanket induction algorithm , and classification accuracy as a criterion is used to verify whether a new feature subset is a Markov boundary or not . The parameter alpha of HITION PC is set to 001
In the following figures and tables , we report the comparison results of MB DEA and TIE* . The running time on the dexter and thrombin data sets exceed 3 days for TIE* and we do not show the experimental results of TIE* on those two data sets .
Table 3 reports the F ratio and MB ratio . An F ratio is the ratio of the number of features of the core feature space discovered by MB DEA divided by the number of total features on the same data set . An MB ratio is the ratio of the number of Markov boundaries discovered by MB DEA in the number of Markov boundaries identified by TIE* . In Table 3 , we can see that on each data set , the number of features within the core feature space is a very small fraction of the total number of features . Furthermore , from the MB ration , except for the madelon and thrombin , MB DEA discovers the same number of Markov boundaries as TIE* on the remaining data sets . For the dexter and thrombin data sets , we do not have their MB ratios due to long running time of TIE* and denote them as “ ” . arcenedexterdorotheacolonleukemialung−cancerthrombinovarian−cancer flfl00102030405060708091Data setPrediction accuracy MB−DEATIE*2242 Table 4 : Efficiency of MB DEA vs.TIE*(seconds )
Data set arcene dexter dorothea colon leukemia lung cancer thrombin ovarian cancer
TIE* MB DEA 19 6,465 5 15 79 6
6 17,091 25 1 6 16 24,935 3
Figure 4 : Running time of the discovery of the core feature space . The labels of the x axis in both figures from 1 to 8 denote the data sets:1.arcene , 2.dexter , 3.dorothea , 4.colon , 5.leukemia , 6.lung cancer , 7.thrombin , 8ovarian cancer
Meanwhile , Figure 3 gives prediction ( classification ) accuracies of MB DEA against TIE* using the K Nearest Neighbor classifier . We select the highest prediction accuracy among all of the Markov boundaries discovered by MB DEA and TIE* , respectively . From Figure 3 , we can see that with the core feature space , MB DEA gets the same prediction accuracy as TIE* . Although MB DEA does not find all of Markov boundaries on the colon data set , it still gets the Markov boundaries with the highest prediction accuracies . Figure 4 gives the running time of the discovery of the core feature space from Steps 2 to 25 in Algorithm 1 . From Figure 4 , we can see that the computational cost of the discovery of the core feature space is very low due to identifying the feature set ( see Definition 3 ) without additional time costs . Table 4 gives the running time of MB DEA against TIE* . From Table 4 , we can see that in term of efficiency , MB DEA is faster than TIE* on all of ten data sets . From the dorothea , dexter , and thrombin data sets in Table 4 , with the core feature space , MB DEA can efficiently deal with data with very high dimensionality while TIE* is very expensive , or even impossible due to its exhaustive search over the entire feature space .
In summary , with the above results , we can conclude that the core feature space that the MB DEA algorithm has discovered is only a small fraction of the entire feature space , but MB DEA gets a very promising prediction accuracy with a reasonable running time . Therefore , the MB DEA algorithm is a scalable and accurate method to deal with distributed feature data .
Figure 5 : Flow chart of the experimental process . Using an undersampled data set as input , MB DEA outputs the multiple Markov Boundaries and a best Markov Boundary , or the tornado precursor set , can be found according to the interesting measures .
5.2 Tornado Forecasting 521 Experiment Setup Tornadoes are rare events even during the most frequent months . The positive and negative instances ratio in our study is about 1:30 . We use both accuracy ( Equation 7 ) and F1 score ( Equation 8 ) as the performance metrics in our empirical study .
Accuracy =
T P + F P
T P + T N + F P + F N
F 1 =
2T P
2T P + F P + F N
( 7 )
( 8 ) where T P is the number of true positives , FP is the number of false positives , TN is the number of true negatives , and FN is the number of false negatives .
We divide the data set into two sets , 30 years ( 1979 2008 , 2 , 610 days with 81 positive instances ) as the training set and 5 years ( 2009 2013 , 435 days with 17 positive instances ) as the test set . As illustrated in Figure 5 , we randomly under sample the negative instances to achieve class ratios of positive : negative ( P/N ) examples that are equal to 1:5 , 1:10 , 1:20 , and 1:25 respectively . At each ratio level we perform the random process 10 times and run the MB DEA algorithm with the correspondent training data set to identify tornado precursors from multiple Markov boundaries . We apply the tornado precursors to the K Nearest Neighbor classifier with K=1 on the test set . The average and best prediction results are reported in Table 5 , in which the average values are calculated using the best precursors identified from each training data set . We plot the experimental results in Figures 6 to 7 . 522 Results and Discussion Our goal is to identify key interpretable tornado precursors that can latterly be used in forecasting models . As discussed earlier , we start from a single Markov boundary , then derive a core feature set from it , eventually identify the best tornado precursors . The average F1 values of the best tornado precursor sets are much higher than those of the initial single Markov boundaries at all the sampling levels ( Table 5 ) . The best average forecasting result ( average accuracy=0.93 , average F1=0.31 ) is achieved at the undersampling level of P/N ratio 1:20 and the best predicting
1234567805101520253035Data setsRunning time ( seconds ) Tornado Data SetP/N =1:5P/N =1:10MB(C)MB1MB1CFS(C)MBMB(cid:859)MB(cid:859)(cid:859)BestMBUnder Sampling with Different P/N RatiosMB_DEA AlgorithmFinding Single Markov BoundaryGenerating Core Feature Set Mining Multiple Markov Boundaries 2243 Table 5 : The average and best prediction results
P/N Ratio
1:5 1:10 1:15 1:20 1:25
Without Undersampling
0.1041 0.1333 0.1579 0.1739 0.1778 0.1429
Ave . F1 of M B(C ) Ave . Size of CFS(C ) Ave . Acc . Ave . F1 Best Acc . Best F1 0.2667 0.2917 0.2941 0.3478 0.3500 0.2778
0.1907 0.2440 0.2688 0.3129 0.3018 –
0.9241 0.9253 0.9448 0.9310 0.9402 0.9218
269 217 233 243 245 238
0.9097 0.9218 0.9296 0.9255 0.9264 – result ( accuracy=0.94 , F1=0.35 ) is achieved with P/N ratio of 1:25 , in which we have successfully predicted 7 tornado events one day ahead out of 17 during the testing period ( MAM 2009 2013 , 435 days ) , with 16 false positives . To the best of our knowledge , our result among the most promising tornado forecasting results at daily level compared to the state of the art algorithms [ 3 ] .
Figure 6 uses our empirical results to explain how MBDEA works . We plot the single Markov Boundary ( M B(C ) in Algorithm 1 ) , the core feature set CF S(C ) , and the tornado precursors ( the best Markov boundary according to the criteria of prediction powers ) with respect to the experiment having best prediction results ( F1=0.35 ) in Figure 6 . Firstly , the single Markov boundary M B(C ) is learned from the tornado data set ( Figure 6a ) , then the core feature set CF S(C ) is built based on the M B(C ) ( Figure 6b 6e ) ; finally , the best Markov boundary according to prediction powers , is reported ( Figure 6f ) . Different variables in the core feature set CF S(C ) fall into clusters in the spatial domains ( blue circles in Figure 6b 6e ) . The arrows from Figure 6a to Figure 6d show that how a feature from the field of Pressure Vertical Velocity helps to generate a cluster of related features ( mostly from the field of Pressure Vertical Velocity and Relative Humidity ) , and later contributes to the identification of precursor 12 ( Figure 6f ) . The variables from the same field closed to each other in the spatial domains will tend to have similar values due to the spatial autocorrelation effect [ 23 ] . These spatial clusters can be considered as real world examples of the CF S(C ) illustrated in Figure 2 . Our algorithm picks individual precursors out of each cluster and successfully finds the best combination ( Markov Boundary ) according to the forecasting task .
As an algorithm of discovering multiple Markov boundaries , MB DEA is not limited to finding the feature set with the best prediction result . MB DEA is able to report different precursor sets with similar prediction powers according to other domain interests . For example , compared to other variables in the tornado data set , the Pressure Vertical Velocity is considered as a noisy and unreliable variable . In the feature set with the best prediction result ( Figure 6f , bset for short ) , we have two features ( No.7 and No.12 ) from the field of Pressure Vertical Velocity . From all the MBDEA outputs generated from the same input data set , we are able to find a feature set whose features are all from fields other than Pressure Vertical Velocity ( Figure 7 , we call it the “ most reliable ” set , or m set for short . ) and have similar prediction power ( F1=03265 ) From Figure 6f and Figure 7 , we find that more than half of the features in the two sets ( No.1,2,3,4,8,10,and 11 ) are exactly the same , and others ( except No.6 ) are from the same spatial clusters in the core feature set . In the m set instead of Pressure Vertical Velocity , variables from Relative Humidity are selected ( No.5 and No.7 in m set , compared to No.12 and No.7 in b set , respectively ) . MB DEA provides a new approach of bridging data driven and knowledge driven processes for better data interpretation to be used in various forecasting models .
6 . CONCLUSION
The development of reliable tornado forecasting techniques able to provide warnings at a long lead time is crucial to help human beings better prepare and respond to disastrous events . In this paper , we propose the MB DEA algorithm to identify multiple sets of the precursors for reliable tornado forecasting using multiple Markov boundaries . Our empirical study reveals that the precursors identified by our algorithm are considered more reliable and practical than the ones identified by the single Markov boundary discovery algorithm , and lead to advancements in reliable and long lead time of catastrophic tornado forecasting .
7 . ACKNOWLEDGMENTS
This work is partly supported by a PIMS Post Doctoral Fellowship Award of the Pacific Institute for the Mathematical Sciences , Canada , the Program for Changjiang Scholars and Innovative Research Team in University ( PCSIRT ) of the Ministry of Education , China ( under grant IRT13059 ) , the National 973 Program of China ( under grant 2013CB329604 ) , the National Natural Science Foundation of China ( under grants 61229301 and 61305064 ) , an NSERC Discovery grant and a BCIC NRAS Team Project . All opinions , findings , conclusions and recommendations in this paper are those of the authors and do not necessarily reflect the views of the funding agencies .
8 . REFERENCES [ 1 ] C . F . Aliferis , A . Statnikov , I . Tsamardinos , S . Mani , and X . D . Koutsoukos . Local causal and markov blanket induction for causal discovery and feature selection for classification part i : Algorithms and empirical evaluation . Journal of Machine Learning Research , 11:171–234 , 2010 .
[ 2 ] T . W . Anderson , T . W . Anderson , T . W . Anderson , and T . W . Anderson . An introduction to multivariate statistical analysis , volume 2 . Wiley New York , 1958 .
[ 3 ] J . Brotzge and W . Donner . The tornado warning process : A review of current research , challenges , and opportunities . Bulletin of the American Meteorological Society , 94(11):1715–1733 , 2013 .
[ 4 ] A . J . Clark , J . S . Kain , P . T . Marsh , J . Correia Jr ,
M . Xue , and F . Kong . Forecasting tornado pathlengths using a three dimensional object identification algorithm applied to convection allowing forecasts . Weather and Forecasting , 27(5):1090–1113 , 2012 .
2244 Figure 6 : A real world example on the processes of how MB DEA works . MB DEA firstly discovers a single Markov Boundary ( Figure ( a ) : M B(C ) from Algorithm 1 ) . The partial CF S ( Figures ( b ) to ( e ) ) shows feature clusters in the spatial domain built based on features from M B(C ) , and finally the best feature set ( b set ( Figure ( f ) ) , according to the prediction power ) is found from CF S . The red square in each map is the target area for tornado labeling . f.Feature Set with Best Prediction Result(F1=03500)a Single Markov Boundary ( MB(C ) , F1=01905)b Zonal Wind and Meridional Windc . Geopotential Heightd . Relative Humidity and Pressure Vertical Velocitye.Temperature2245 Figure 7 : The most reliable feature set ( according to the types of variables in the set ) . The red square in the map is the target area for tornado labeling .
[ 5 ] C . P . De Campos and Q . Ji . Efficient structure learning of bayesian networks using constraints . Journal of Machine Learning Research , 12:663–689 , 2011 .
[ 6 ] C . A . Doswell , H . E . Brooks , and N . Dotzek . On the implementation of the enhanced fujita scale in the usa . Atmospheric Research , 93(1):554–563 , 2009 .
[ 7 ] J . H . Faghmous and V . Kumar . A big data guide to understanding climate change : The case for theory guided data science . Big data , 2(3):155–163 , 2014 .
[ 8 ] I . M . Held , R . T . Pierrehumbert , S . T . Garner , and K . L . Swanson . Surface quasi geostrophic dynamics . Journal of Fluid Mechanics , 282:1–20 , 1995 .
[ 9 ] E . Kalnay , M . Kanamitsu , R . Kistler , W . Collins ,
D . Deaven , L . Gandin , M . Iredell , S . Saha , G . White , J . Woollen , et al . The ncep/ncar 40 year reanalysis project . Bulletin of the American meteorological Society , 77(3):437–471 , 1996 .
[ 10 ] H . Liu and L . Yu . Toward integrating feature selection algorithms for classification and clustering . Knowledge and Data Engineering , IEEE Transactions on , 17(4):491–502 , 2005 .
[ 11 ] J . Pearl . Probabilistic reasoning in intelligent systems : networks of plausible inference . Morgan Kaufmann , 1988 .
[ 12 ] J . M . Pe˜na , R . Nilsson , J . Bj¨orkegren , and J . Tegn´er . Towards scalable and data efficient learning of markov boundaries . International Journal of Approximate Reasoning , 45(2):211–232 , 2007 .
[ 13 ] C . M . Shafer , A . E . Mercer , L . M . Leslie , M . B .
Richman , and C . A . Doswell III . Evaluation of wrf model simulations of tornadic and nontornadic outbreaks occurring in the spring and fall . Monthly Weather Review , 138(11):4098–4119 , 2010 . [ 14 ] C . E . Shannon . A mathematical theory of communication . ACM SIGMOBILE Mobile Computing and Communications Review , 5(1):3–55 , 2001 .
[ 15 ] K . Simmons and D . Sutter . Economic and societal impacts of tornadoes . Springer Science & Business Media , 2013 .
[ 16 ] N . Snook and M . Xue . Effects of microphysical drop size distribution on tornadogenesis in supercell thunderstorms . Geophysical Research Letters , 35(24 ) , 2008 .
[ 17 ] A . Statnikov , J . Lemeir , and C . F . Aliferis . Algorithms for discovery of multiple markov boundaries . Journal of Machine Learning Research , 14(1):499–566 , 2013 . [ 18 ] I . Tsamardinos , L . E . Brown , and C . F . Aliferis . The max min hill climbing bayesian network structure learning algorithm . Machine learning , 65(1):31–78 , 2006 .
[ 19 ] D . Wang , W . Ding , K . Yu , X . Wu , P . Chen , D . L .
Small , and S . Islam . Towards long lead forecasting of extreme flood events : a data mining framework for precipitation cluster precursors identification . In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining , pages 1285–1293 . ACM , 2013 .
[ 20 ] X . Wu , K . Yu , W . Ding , H . Wang , and X . Zhu . Online feature selection with streaming features . Pattern Analysis and Machine Intelligence , IEEE Transactions on , 35(5):1178–1192 , 2013 .
[ 21 ] K . Yu , X . Wu , W . Ding , and J . Pei . Towards scalable and accurate online feature selection for big data . In Data Mining ( ICDM ) , 2014 IEEE International Conference on , pages 660–669 . IEEE , 2014 .
[ 22 ] K . Yu , X . Wu , Z . Zhang , Y . Mu , H . Wang , and W . Ding . Markov blanket feature selection with non faithful data distributions . In Data Mining ( ICDM ) , 2013 IEEE International Conference on , pages 857–866 . IEEE , 2013 .
[ 23 ] P . Zhang , Y . Huang , S . Shekhar , and V . Kumar .
Exploiting spatial autocorrelation to efficiently process correlation based similarity queries . In Advances in Spatial and Temporal Databases , pages 449–468 . 2003 .
2246
