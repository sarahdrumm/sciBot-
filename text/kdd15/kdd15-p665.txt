0 Bit Consistent Weighted Sampling
Ping Li
Department of Statistics and Biostatistics
Department of Computer Science
Piscataway , NJ 08854 , USA pingli@statrutgersedu
ABSTRACT We1 develop 0 bit consistent weighted sampling ( CWS ) for efficiently estimating min max kernel , which is a generalization of the resemblance kernel originally designed for binary data . Because the estimator of 0 bit CWS constitutes a positive definite kernel , this method can be naturally applied to large scale data mining problems . Basically , if we feed the sampled data from 0 bit CWS to a highly efficient linear classifier ( eg , linear SVM ) , we effectively ( and approximately ) train a nonlinear classifier based on the min max kernel . The accuracy improves as we increase the sample size .
In this paper , we first demonstrate , through an extensive classification study using kernel machines , that the min max kernel often provides an effective measure of similarity for nonnegative data . This helps justify the use of min max kernel . However , as the min max kernel is nonlinear and might be difficult to be used for industrial applications with massive data , we propose to linearize the min max kernel via 0 bit CWS , a simplification of the original CWS method .
The previous remarkable work on consistent weighted sampling ( CWS ) produces samples in the form of ( i∗ , t∗ ) where the i∗ records the location ( and in fact also the weights ) information analogous to the samples produced by classical minwise hashing on binary data . Because the t∗ is theoretically unbounded , it was not immediately clear how to effectively implement CWS for building large scale linear classifiers . We provide a simple solution by discarding t∗ ( which we refer to as the “ 0 bit ” scheme ) . Via an extensive empirical study , we show that this 0 bit scheme does not lose essential information . We then apply 0 bit CWS for building linear classifiers to approximate min max kernel classifiers , as extensively validated on a wide range of public datasets .
We expect this work will generate interests among data mining practitioners who would like to efficiently utilize the nonlinear information of non binary and nonnegative data .
1The work is partially supported by NSF III 1360971 , ONRN00014 13 1 0764 , and AFOSR FA9550 13 1 0137 .
1 .
INTRODUCTION
Nonnegative data are common in practice and the existence of negative entries in a dataset is often due to shifting or normalization . In this paper we show that the min max kernel can provide an effective measure of similarity for nonnegative data and should be useful for building effective large scale data mining tools via hashing techniques .
Given two nonnegative data vectors , u , v ∈ RD , we define min max : KMM ( u , v ) = PD PD i=1 min{ui , vi} i=1 max{ui , vi}
( 1 ) which is a generalization of the well known resemblance : resemblance : KR(u , v ) = PD i=1 1{ui > 0 and vi > 0} PD i=1 1{ui > 0 or vi > 0}
( 2 )
The resemblance is a popular measure of similarity for binary data [ 4 , 21 ] . The prior work [ 22 ] used the term “ resemblance kernel ” because the resemblance can be written as the ( expectation ) of an inner product ( and hence it is a positive definite kernel ) . It will be soon clear that KMM ( 1 ) can also be written as the expectation of an inner product .
Readers probably have realized that the min max kernel in ( 1 ) is related to the so called intersection kernel [ 23 ] : intersection : KI ( u , v ) =
D
X i=1 min{ui , vi} ,
( 3 )
D
X i=1 ui = 1 ,
D
X i=1 vi = 1
In this paper , we will extensively compare the min max kernel with the intersection kernel in the context of kernel machines for classification . Interestingly , for most datasets in our experimental study , the min max kernel outperforms the intersection kernel , and in some cases significantly so .
The sum to one normalization in ( 3 ) appears natural , since the data vectors ( eg , u and v ) were treated as histograms when the intersection kernel was designed . For our curiosity , we also define the following “ normalized min max kernel ” : n min max : KNMM ( u , v ) = PD PD i=1 min{ui , vi} i=1 max{ui , vi}
( 4 )
D
X i=1 ui = 1 ,
D
X i=1 vi = 1
Our experiments will show that , for most datasets , this normalization step only affects the accuracies very marginally .
665 In this paper , we often use “ min max kernels ” to refer to both the min max kernel and the n min max kernel .
It is worth mentioning that the above three kernels ( minmax , intersection , and n min max ) have no tuning parameters . Thus , it is often possible to further improve the performance by , for example , using multiple kernels or kernels combined in a special fashion ( eg , the CoRE kernels [ 20 ] by multiplying resemblance with correlation ) .
We will compare these three types of parameter free ker nels with the basic ( tuning free ) linear kernel : linear : Kρ(u , v ) =
D
X i=1 uivi ,
( 5 )
D
X i=1 u2 i = 1 ,
D
X i=1 v2 i = 1
For convenience , we enforce the normalization ( to unit length ) because in practice ( eg , when running linear SVM ) the normalization step is typically recommended .
The min max kernel was sparsely discussed in the literature [ 24 , 14 ] . In contrast , the resemblance kernel ( 2 ) has been widely used in practice on binary ( or binarized ) data [ 4 , 5 , 28 , 9 , 26 , 7 , 6 , 11 , 8 , 16 , 13 , 1 ] . For example , [ 22 ] demonstrated the use of b bit minwise hashing [ 21 ] for training large scale ( resemblance kernel ) SVM and logistic regression .
Summary of our contributions : This paper aims at addressing several interesting and important issues regarding the use of min max kernels for data mining applications : 1 . Why using min max kernels ? Table 1 and Figures 1 to 3 provide an extensive empirical study of kernel SVMs for classification on a sizable collection of public datasets , for comparing linear kernel , min max kernel , n min max kernel , and intersection kernel . The results illustrate the advantages of the min max kernels over the linear kernel as well as the intersection kernel .
2 . The “ 0 bit ” CWS hashing for min max kernels .
The remarkable prior work on consistent weighted sampling ( CWS ) provides a recipe for sampling min max kernels ( ie , the collision probability of the samples is the min max kernel ) , in the form of ( i∗ , t∗ ) . Because t∗ is theoretically unbounded , it was not immediately clear how to effectively implement a “ b bit ” version of CWS which is needed in order to apply the method for large scale industrial applications . We provide a ( surprisingly ) simple solution by completely discarding t∗ ( after hashing ) , which we refer to as the “ 0 bit ” scheme and is validated by a large set of experiments .
3 . Large scale learning with 0 bit CWS hashing .
In light of our contributions 1 and 2 , we apply the proposed 0 bit CWS hashing for efficiently building large scale linear classifiers approximately in the space of minmax kernels , as verified by extensive experiments .
2 . KERNEL SVM EXPERIMENTS
This section presents an experimental study for classification using kernel machines based on the four types of kernels we have introduced : the linear kernel , the min max kernel , the n min max kernel , and the intersection kernel . We use the LIBLINEAR package [ 10 ] for training linear classifiers and the LIBSVM pre computed kernel functionality for three nonlinear kernels . Table 1 summarizes the test accuracies .
There is a regularization term C for l2 regularized SVM . To ensure repeatability , we report test accuracies for C from 10−2 to 103 , in Figures 1 to 3 . The accuracies reported in Table 1 are the ( individually ) highest points on the curves . The results in Table 1 and Figures 1 to 3 confirm that using min max kernels typically results in better performance compared to linear as well as intersection kernel . This helps justify the use of min max kernels in learning applications .
CoverType10k
85
80
75
70
65
60
55
)
%
( y c a r u c c A
85
80
75
70
65
60
55
)
%
( y c a r u c c A
Linear Min−Max N−Min−Max Intersection
CoverType20k
Linear Min−Max N−Min−Max Intersection
50
10
−2
−1
10
0 10
1 10
2 10
3 10
50
10
−2
−1
10
0 10
1 10
2 10
3 10
C
C
96
95
94
93
92
91
)
%
( y c a r u c c A
IJCNN5k
Linear Min−Max N−Min−Max Intersection
IJCNN10k
Linear Min−Max N−Min−Max Intersection
96
95
94
93
92
91
)
%
( y c a r u c c A
90
10
−2
−1
10
0 10
1 10
2 10
3 10
90
10
−2
−1
10
0 10
1 10
2 10
3 10
C
C
Isolet
97
95
93
91
89
87
85
)
%
( y c a r u c c A
100
Letter
)
%
( y c a r u c c A
90
80
70
60
Linear Min−Max N−Min−Max Intersection
Linear Min−Max N−Min−Max Intersection
83
10
−2
−1
10
0 10
1 10
2 10
3 10
50
10
−2
−1
10
0 10
1 10
2 10
3 10
C
C
)
%
( y c a r u c c A
90
80
70
60
50
Letter4k
100
M−Basic
Linear Min−Max N−Min−Max Intersection
)
%
( y c a r u c c A
95
90
85
80
Linear Min−Max N−Min−Max Intersection
10
−2
−1
10
0 10
1 10
2 10
3 10
75
10
−2
−1
10
0 10
1 10
2 10
3 10
C
C
M−Image
90
80
70
60
50
)
%
( y c a r u c c A
100
)
%
( y c a r u c c A
95
90
85
80
Linear Min−Max N−Min−Max Intersection
MNIST10k
Linear Min−Max N−Min−Max Intersection
40
10
−2
−1
10
0 10
1 10
2 10
3 10
75
10
−2
−1
10
0 10
1 10
2 10
3 10
C
C
Figure 1 : Test classification accuracies for four types of kernels using l2 regularized SVM ( with a tuning parameter C , ie , x axis ) . Each panel presents the results for one particular dataset ( see data information in Table 1 ) . The two solid curves represent the min max kernel ( red , if color is available ) and the n min max kernel ( green ) . The dashed curve ( blue ) and the dot dashed ( black ) curve represent , respectively , the linear kernel and the intersection kernel . See Figures 2 and 3 for results on more datasets .
666 M−Noise1
80
70
60
50
40
)
%
( y c a r u c c A
80
70
60
50
40
)
%
( y c a r u c c A
Linear Min−Max N−Min−Max Intersection
M−Noise2
Linear Min−Max N−Min−Max Intersection
10
−2
−1
10
0 10
1 10
2 10
3 10
10
−2
−1
10
0 10
1 10
2 10
3 10
C
C
M−Noise3
80
70
60
50
40
)
%
( y c a r u c c A
80
70
60
50
40
)
%
( y c a r u c c A
Linear Min−Max N−Min−Max Intersection
M−Noise4
Linear Min−Max N−Min−Max Intersection
80
60
40
20
)
%
( y c a r u c c A
0
10
−2
100
)
%
( y c a r u c c A
90
80
70
60
Protein
98
RCV1
)
%
( y c a r u c c A
96
94
92
Linear Min−Max N−Min−Max Intersection
Linear Min−Max N−Min−Max Intersection
−1
10
0 10
1 10
2 10
3 10
90
10
−2
−1
10
0 10
1 10
2 10
3 10
C
C
Satimage
100
)
%
( y c a r u c c A
95
90
85
80
75
Linear Min−Max N−Min−Max Intersection
Segment
Linear Min−Max N−Min−Max Intersection
10
−2
−1
10
0 10
1 10
2 10
3 10
10
−2
−1
10
0 10
1 10
2 10
3 10
C
C
50
10
−2
−1
10
0 10
1 10
2 10
3 10
70
10
−2
−1
10
0 10
1 10
2 10
3 10
C
C
80
70
60
)
%
( y c a r u c c A
M−Noise5
M−Noise6
80
70
60
)
%
( y c a r u c c A
Linear Min−Max N−Min−Max Intersection
Linear Min−Max N−Min−Max Intersection
SensIT20k
90
85
80
75
)
%
( y c a r u c c A
Linear Min−Max N−Min−Max Intersection
100
)
%
( y c a r u c c A
95
90
85
80
75
50
10
−2
−1
10
0 10
1 10
2 10
3 10
50
10
−2
−1
10
0 10
1 10
2 10
3 10
70
10
−2
−1
10
0 10
1 10
2 10
3 10
70
10
−2
C
C
C
Shuttle1k
Linear Min−Max N−Min−Max Intersection
−1
10
0 10
1 10
2 10
3 10
C
M−Rand
90
80
70
60
50
)
%
( y c a r u c c A
100
)
%
( y c a r u c c A
80
60
40
20
Linear Min−Max N−Min−Max Intersection
40
10
−2
−1
10
0 10
1 10
2 10
3 10
0
10
−2
C
M−Rotate
Linear Min−Max N−Min−Max Intersection
−1
10
0 10
1 10
2 10
3 10
C
M−RotImg
100
Optdigits
)
%
( y c a r u c c A
95
90
Linear Min−Max N−Min−Max Intersection
Linear Min−Max N−Min−Max Intersection
96
94
92
90
88
)
%
( y c a r u c c A
Spam
100
Splice
)
%
( y c a r u c c A
90
80
70
60
Linear Min−Max N−Min−Max Intersection
Linear Min−Max N−Min−Max Intersection
86
10
−2
100
)
%
( y c a r u c c A
90
80
−1
10
0 10
1 10
2 10
3 10
50
10
−2
−1
10
0 10
1 10
2 10
3 10
C
C
USPS
70
60
50
40
30
20
10
)
%
( y c a r u c c A
Linear Min−Max N−Min−Max Intersection
Vowel
Linear Min−Max N−Min−Max Intersection
45
40
35
30
25
20
15
)
%
( y c a r u c c A
10
10
−2
100
)
%
( y c a r u c c A
90
80
−1
10
0 10
1 10
2 10
3 10
85
10
−2
−1
10
0 10
1 10
2 10
3 10
70
10
−2
−1
10
0 10
1 10
2 10
3 10
0
10
−2
−1
10
0 10
1 10
2 10
3 10
C
C
C
C
Pendigits
95
Phoneme
100
WebspamN1−20k
)
%
( y c a r u c c A
90
85
80
Linear Min−Max N−Min−Max Intersection
)
%
( y c a r u c c A
95
90
85
Linear Min−Max N−Min−Max Intersection
80
70
60
50
40
)
%
( y c a r u c c A
Linear Min−Max N−Min−Max Intersection
YoutubeVision
Linear Min−Max N−Min−Max Intersection
70
10
−2
−1
10
0 10
1 10
2 10
3 10
75
10
−2
−1
10
0 10
1 10
2 10
3 10
80
10
−2
−1
10
0 10
1 10
2 10
3 10
30
10
−2
−1
10
0 10
1 10
2 10
3 10
C
C
C
C
Figure 2 : Test classification accuracies for four types of kernels using l2 regularized SVM .
Figure 3 : Test classification accuracies for four types of kernels using l2 regularized SVM .
667 Table 1 : Classification accuracies ( in % ) using 4 different kernels . We use LIBLINEAR package [ 10 ] for training l2 regularized linear kernel SVMs and LIBSVM ’s “ pre computed ” kernel functionality for training nonlinear l2 regularized kernel SVMs . There is an important tuning parameter C . The reported test classification accuracies ( ie , the rightmost 4 columns ) are the best accuracies from a wide range of C values ; see Figures 1 to 3 for more details . The datasets are public ( and mostly well known ) , from various sources including the UCI repository , the LIBSVM web site , the book web site of [ 12 ] , and the papers [ 17 , 18 , 19 ] which compared deep nets , boosting and trees , kernel SVMs , etc . ( Also see http://hunch.net/?p=1467 for interesting discussions . ) Whenever possible , we use the conventional partitions of training and testing sets .
We have made efforts to ensure the repeatability of our experiments by using pre computed kernels and reporting the results for a very wide range of C values . However , this strategy also limits the scale of the experiments because most workstations do not have sufficient memory to store the kernel matrix for datasets of even moderate sizes ( for example , a merely 60 , 000 × 60 , 000 kernel matrix has 3.6 × 109 entries ) . Thus , for the sake of repeatability , for a few datasets we only use a subset of the samples . Please contact the author if more information is needed in order to reproduce the experiments . Several special notes about the datasets :
( i ) Whenever possible , we always use the datasets “ as they are ” from the sources . Although we agree it is a very important research task to study how to transform/modify the data to favor certain type of similarities , it is not the focus of our paper ( and may hurt the repeatability of the experiments if we try to alter the data ) . ( ii ) Several datasets downloaded from the LIBSVM site were already scaled to [ 1 , 1 ] . To make use of these datasets , we simply transform them by ( z + 1)/2 , where z is the original feature value .
Dataset Covertype10k Covertype20k IJCNN5k IJCNN10k Isolet Letter Letter4k M Basic M Image MNIST10k M Noise1 M Noise2 M Noise3 M Noise4 M Noise5 M Noise6 M Rand M Rotate M RotImg Optdigits Pendigits Phoneme Protein RCV1 Satimage Segment SensIT20k Shuttle1k Spam Splice USPS Vowel WebspamN1 20k ( 1 gram ) YoutubeVision
# train samples # test samples 50,000 50,000 91,701 91,701 1,559 4,000 16,000 50,000 50,000 60,000 4,000 4,000 4,000 4,000 4,000 4,000 50,000 50,000 50,000 1,797 3,498 1,169 6,621 60,000 2,000 1,155 19,705 14,500 1,536 2,175 2,007 462 60,000 10,000
10,000 20,000 5,000 10,000 6,238 16,000 4,000 12,000 12,000 10,000 10,000 10,000 10,000 10,000 10,000 10,000 12,000 12,000 12,000 3,823 7,494 3,340 17,766 20,242 4,435 1,155 20,000 1,000 3,065 1,000 7,291 528 20,000 11,736 linear min max 70.9 71.1 91.6 91.6 95.4 62.4 61.2 90.0 70.7 90.0 60.3 62.1 65.2 68.4 72.3 78.7 78.9 48.0 31.4 95.3 87.6 91.4 69.1 96.3 78.5 92.6 80.5 90.9 92.6 85.1 91.7 40.9 93.0 63.3
80.4 83.3 94.4 95.7 96.4 96.2 91.4 96.2 80.8 95.7 71.4 72.4 73.6 76.1 79.0 84.2 84.2 84.8 41.0 97.7 97.9 92.5 72.4 96.9 90.5 98.1 86.9 99.7 95.0 95.2 95.3 59.1 97.9 72.4 n min max intersection
80.2 83.1 95.3 96.0 96.6 95.0 90.2 96.0 77.0 95.4 68.5 70.7 71.9 75.2 78.4 84.3 84.1 83.9 38.5 97.4 98.0 92.0 70.7 96.9 87.8 97.5 87.0 99.6 94.7 94.9 95.3 53.5 97.8 72.4
74.3 75.2 94.0 94.5 96.4 92.1 87.9 93.4 76.2 93.1 68.2 70.0 71.6 74.8 77.9 83.9 83.7 60.8 37.0 96.8 97.5 91.6 69.6 96.7 86.9 97.0 85.5 99.6 94.2 93.8 94.8 49.8 96.6 70.8
668 The purpose of this experimental study on kernel SVMs is not try to show that min max kernels achieve the best classification accuracies among all learning methods . In fact , compared to trees or deep nets [ 17 , 18 , 19 ] , simply using min max kernels usually does achieve the best accuracies , although the results are close . Since min max kernels have no tuning parameters , we can expect to boost the performance by using additional parameters or by combining multiple the same ( or different ) types of kernels , for example , using the idea from CoRE kernels [ 20 ] .
For large scale industrial applications , typically it is difficult to directly use nonlinear kernels . Fortunately , with CWS ( consistent weighted sampling ) , we can linearize the min max kernel . In other words , it is possible to achieve the good performance of min max kernels at the cost of linear kernels . One of our contributions is the development of a simpler scheme , which we refer to as the “ 0 bit ” CWS .
3 . HASHING MIN MAX KERNEL
The classification experiments reported in Table 1 and Figures 1 to 3 have demonstrated the effectiveness of minmax kernels in terms of prediction accuracies . However , in order to make min max kernels practical for large scale data mining tasks , we need to resort to hashing techniques to ( approximately ) transform nonlinear kernels into linear kernels . It is well understood [ 3 ] that computing kernels are expensive and the kernel matrix , if fully materialized , does not fit in memory even for relatively small applications . In contrast , highly efficient linear algorithms , eg , [ 15 , 27 , 2 , 10 ] , have been widely used in practice for truly large scale applications such as click predictions in online advertising [ 25 ] .
3.1 Consistent Weighted Sampling ( CWS )
The prior efforts [ 24 , 14 ] have lead to “ consistent weighted sampling ( CWS ) ” for hashing min max kernels . Here the following Alg . 1 adopts the clean description of CWS in [ 14 ] .
Algorithm 1 Consistent Weighted Sampling ( CWS ) Input : Data vector u = ( ui ≥ 0 , i = 1 to D ) Output : Consistent uniform sample ( i∗ , t∗ )
For i from 1 to D ri ∼ Gamma(2 , 1 ) , ci ∼ Gamma(2 , 1 ) , βi ∼ U nif orm(0 , 1 ) ti ← ⌊ log ui +βi⌋ , yi ← exp(ri(ti −βi) ) , ai ← ci/(yi exp(ri ) ) ri
End For i∗ ← arg mini ai , t∗ ← ti∗
Given a data vector u ∈ RD , Alg . 1 provides the procedure for generating one CWS sample ( i∗ , t∗ ) . In order to generate k such samples , we have to repeat the procedure k times using an independent set of random numbers ri , ci , βi . For clarity , we denote the samples for data vectors u and v as
,i∗ u,j , t∗ u,j and ,i∗ v,j , t∗ v,j , j = 1 , 2 , , k
( 6 )
Basically we need to generate 3 matrices : {r} , {c} , and {β} , of size D × k . All the data vectors will use the same 3 matrices . This has essentially the same cost as random projections ( which however approximate linear kernels ) .
The basic theoretical result of CWS says the “ collision probability ” is exactly KMM : u,j = ,i∗
Pr ( ,i∗ u,j , t∗ v,j , t∗ v,j ) = KMM ( u , v )
( 7 )
Thus , it is clear that , at least conceptually , we can express KMM ( u , v ) as the expectation of an inner product and hence
KMM is positive definite , just like how [ 22 ] showed the resemblance is a type of positive definite kernel .
3.2 Drawback of CWS for Data Mining
Although the basic probability result ( 7 ) says conceptually we can use CWS for building linear classifiers ( approximately in the space of min max kernels ) , it is not immediately clear how it can be implemented efficiently .
[ 14 ] briefly mentioned that one can “ uniformly map ” the sample space ( i∗ , t∗ ) to a space b bits : {0 , 1 , 2 , , 2b − 1} . This however can not be ( easily ) achieved . While i∗ is bounded by D , t∗ is actually unbounded ( see Alg . 1 ) . Also note that the space of samples is very large . If we represent i∗ by bi bits and t∗ ( approximately ) by bt bits , the space will be 2bi +bt . Thus , we must find an efficient representation of CWS samples in order to use this nice method effectively for machine learning and data mining applications .
3.3 Our “ 0 bit ” Proposal for CWS
It is now known how to use b bit minwise hashing to approximate the resemblance kernel and use it for large scale applications [ 21 , 22 ] . Therefore , in this paper , we focus on representing t∗ . Perhaps surprisingly , our proposal is simple : just ignore t∗ in the sample ( i∗ , t∗ ) , ie , the 0 bit scheme .
If we examine Alg . 1 , we can see that i∗ has already encoded the information about the weights of the data . A rigorous proof however turns out to be a difficult probability problem , which is outside the scope of this paper . Here , we try to empirically demonstrate the following observation :
Pr ( i∗ u,j = i∗ v,j ) ≈ Pr ( ,i∗ u,j , t∗ u,j = ,i∗ v,j , t∗ v,j )
( 8 )
We call our proposal the “ 0 bit ” scheme only to mean that we use 0 bit for coding t∗ . We also call the original proposal as the “ full ” scheme since it stores all the bits needed for t∗ .
3.4 An Experimental Study on 0 Bit CWS
Table 2 : Information of the 13 pairs of English words . For example , “ HONG ” refers to the vector of occurrences of the word “ HONG ” in 216 documents . f1 and f2 are the numbers of nonzeros in word 1 and word 2 respectively . For each pair , we include the numerical values for both the resemblance ( “ R ” ) and the min max kernel ( M M ) .
Word 1
Word 2
A
THE f1 39063 f2 42754
R
0.6444
MM 0.3543
ADDICT
PRICELESS
AIR
DOCTOR
CREDIT
CARD
GAMBIA
KIRIBATI
HONG
OF
KONG
AND
PAPER
REVIEW
PIPELINE FLUSH
SAN
THIS
TIME
FRANCISCO
TODAY
JOB
77
3159
2999
206
940
77
860
0.0065
0.0052
0.0439
0.0248
2697
0.2849
0.2091
186
948
0.7118
0.6070
0.9246
0.8985
37339
36289
0.7711
0.6084
1944
139
3194
27695
3197
0.0780
0.0502
118
0.0158
0.0143
1651
5775
0.4758
0.2885
0.1518
0.0658
37339
36289
0.1279
0.0794
UNITED
STATES
4079
3981
0.5913
0.5017
Table 2 lists 13 pairs of English words . Each word represents a vector of occurrences of that word in a total of 216 documents . This is a typical example of heavy tailed data in that the weights vary dramatically . In common machine learning applications , the weights often do not vary as much ( at least at the point when we are prepared to compute distances/similarites from data ) . In that sense , we are actually testing our 0 bit proposal in a more challenging setting .
669 −3 x 10
6
4
2
0 s a B i
0−bit Full 1−bit
0 10
−1
10
E S M
−2
10
−3
10
A −− THE
−2
0 10
1 10
2 10
3 10 k
10
−4
0 10
−4 x 10
2
0
−2
−4
−6 s a B i
−2
10
−3
10
E S M
−4
10
−5
10
0−bit Full 1−bit
−8
0 10
ADDICT −− PRICELESS 2 10
1 10
3 10 k
10
−6
0 10
ADDICT −− PRICELESS
1 10
2 10 k
−3 x 10
6
4
2
0 s a B i
0−bit Full 1−bit
−1
10
−2
10
E S M
−3
10
−4
10
AIR −− DOCTOR
AIR −− DOCTOR
−2
0 10
1 10
2 10
3 10 k
10
−5
0 10
2
0
−2
−4
−6 s a B i
−3 x 10
CREDIT −− CARD
0 10
−1
10
E S M
−2
10
−3
10
0−bit Full 1−bit
−8
0 10
1 10
2 10
3 10 k
10
−4
0 10
−3 x 10
3
2
1
0
0−bit Full 1−bit
0 10
−1
10
E S M
−2
10
−3
10
3 10
0−bit Full 1−bit
−1
GAMBIA −− KIRIBATI
0 10
1 10
2 10 k
−3 x 10
6
4
2
0
HONG −− KONG
10
−4
0 10
0 10
−1
10
−2
10
−3
10
−4
10
E S M s a B i s a B i
1 10
2 10 k
CREDIT −− CARD
1 10
2 10 k
GAMBIA −− KIRIBATI
1 10
2 10 k
HONG −− KONG
3 10
0−bit Full Theore .
3 10
0−bit Full Theore .
3 10
0−bit Full Theore .
3 10
0−bit Full Theore .
3 10
0−bit Full Theore .
−2
0 10
1 10
2 10
3 10
0 10 k
1 10
2 10
3 10 k
Figure 4 : Results for estimating min max kernels using the “ full ” scheme by recording all the bits of ( i∗ , t∗ ) and the “ 0 bit ” scheme by discarding t∗ . The empirical MSE curves ( right column ) show that both the 0 bit and the full scheme match the theoretical variance . The empirical biases ( left column ) present a magnified view of errors . For a few pairs ( also see Figure 5 ) , the estimates by the 0 bit scheme have some very small ( ≪ 10−4 ) biases . By using the “ 1 bit ” scheme ( ie , by recording whether t∗ is even or odd ) , these biases vanish visually .
0−bit Full Theore .
−3 x 10
5
0
−5 s a B i
A −− THE
OF −− AND
0 10
−1
10
E S M
−2
10
−3
10
0−bit Full 1−bit
1 10
2 10 k
0−bit Full Theore .
3 10
0−bit Full Theore .
3 10
0−bit Full Theore .
3 10
0−bit Full Theore .
3 10
0−bit Full Theore .
3 10
0−bit Full Theore .
3 10
0−bit Full Theore .
−10
0 10
1 10
2 10
3 10 k
10
−4
0 10
−3 x 10
2
0 s a B i
−2
−4
−1
10
−2
10
E S M
−3
10
−4
10
0−bit Full 1−bit
PAPER −− REVIEW
−6
0 10
1 10
2 10
3 10 k
10
−5
0 10
−4 x 10
15
10
5
0 s a B i
0−bit Full 1−bit
−1
10
−2
10
E S M
−3
10
−4
10
PIPELINE −− FLUSH
−5
0 10
1 10
2 10
3 10 k
10
−5
0 10
−3 x 10
6
4
2
0 s a B i
0−bit Full 1−bit
0 10
−1
10
E S M
−2
10
−3
10
SAN −− FRANCISCO
−2
0 10
1 10
2 10
3 10 k
10
−4
0 10
OF −− AND
1 10
2 10 k
PAPER −− REVIEW
1 10
2 10 k
PIPELINE −− FLUSH
1 10
2 10 k
SAN −− FRANCISCO
1 10
2 10 k
−3 x 10
4
2
0 s a B i
−2
−4
0 10
0−bit Full 1−bit
−1
10
−2
10
E S M
−3
10
−4
10
THIS −− TODAY
THIS −− TODAY
1 10
2 10 k
TIME −− JOB
1 10
2 10 k
1 10
2 10
3 10 k
10
−5
0 10 s a B i s a B i
−3 x 10
TIME −− JOB
4
3
2
1
0
0−bit Full 1−bit
−1
10
−2
10
E S M
−3
10
−4
10
−1
0 10
1 10
2 10
3 10 k
10
−5
0 10
−3 x 10
6
4
2
0
0−bit Full 1−bit
0 10
−1
10
E S M
−2
10
−3
10
−2
UNITED −− STATES
0 10
1 10
2 10
3 10 k
10
−4
0 10
UNITED −− STATES
1 10
2 10
3 10 k
Figure 5 : Simulations for estimating min max kernels . See the caption of Figure 4 for more details .
670 We have experimented with many more pairs of words than these 13 pairs but the results look essentially the same , ie , no practical difference between the 0 bit scheme and the full scheme , as can be shown in Figure 4 and Figure 5 .
In the experiment , we let k vary from 1 to 1000 and estimate KMM from k measurements ( i∗ ,j ) , j = 1 to k . With the full scheme , we keep all the bits of t∗ . With the 0 bit scheme , we completely discard t∗ . For each k , we repeat the simulations 10 , 000 times to reliably compute the empirical mean square error ( MSE ) and the bias for each pair .
,j , t∗
The right columns of Figures 4 and 5 plot the empirical MSE , together with the theoretical variance : KMM ( 1 − KMM )/k ( ie , the variance of binomial ) . Because the curves for the 0 bit scheme and the full scheme overlap the theoretical variances , we can conclude , at least for these data , that our proposed 0 bit scheme is essentially unbiased and the variance matches the theoretical variance of the full scheme .
To avoid many “ boring ” figures , we let k be as small as 1 ( while typical simulations would use a much large number such as 10 to start with ) . Nevertheless , these MSE curves are still quite boring since all the curves essentially overlap . To make the presentations somewhat more interesting , we also present the empirical biases in the left columns of the two figures . Now we can see some discrepancies between the two schemes typically on the order of ≪ 10−4 ( in the stabilized zone , ie , when k is not too small ) . While such small biases ( at the 4th or 5th decimal points ) would not make any practical differences , they do serve the purpose to remind us that the 0 bit scheme is indeed an approximation . To make the plots even more interesting , we add the curves for the “ 1 bit ” scheme ( ie , by recording whether t∗ is even or odd ) . For “ CREDIT CARD ” , “ PIPELINE FLUSH ” , “ SANFRANCISCO ” , and “ THIS TODAY ” , we can observe ( very small ) differences between the 0 bit scheme and the fullscheme . The differences vanish once we use the 1 bit scheme .
From Table 2 , we can see that binarizing the data usually leads to very different similarities ( ie , the last two columns , ie , R and M M , differ significantly ) . The 0 bit scheme , which only uses i∗ , still very well approximates the original min max kernel instead of the resemblance kernel . This confirms that , even though our samples ( ie , i∗ ) are in the same format as samples from minwise hashing ( for example , both are integers bounded by D ) , they are statistically very different . In other words , our 0 bit scheme is not the same as simply conducting the original minwise hashing .
Finally , to entertain readers , we add Figure 6 to report the bias results by keeping all the bits of t∗ and only a few ( 0,1,2,4 ) bits of i∗ . Clearly , only using t∗ or t∗ with a few bits of i∗ will not lead to good estimate of the min max kernel .
0.6
0.4
0.2
0 s a B i i 0−bit i 1−bit i 2−bit i 4−bit i full
0.3
0.2
0.1
0 s a B i i 0−bit i 1−bit i 2−bit i 4−bit i full
A −− THE
−0.2
0 10
1 10
2 10
3 10 k
−0.1
0 10
UNITED −− STATES
1 10
2 10
3 10 k
Figure 6 : The biases by using full information of t∗ and only a few ( 0 , 1 , 2 , or 4 ) bits of i∗ .
4 . LEARNING WITH 0 BIT CWS
We conduct a set of experiments by using 0 bit CWS for approximately training min max kernel SVMs by using linear SVMs . Basically , for each dataset , we apply CWS hashing for k up to 4096 and , after hashing , we discard t∗ and only keep a matrix of {i∗} , which has k columns and the same number of rows as the number of examples in the dataset . We then use the popular LIBLINEAR package [ 10 ] for training a linear SVM on the data generated by {i∗} , following the ( data expansion ) scheme proposed by [ 22 ] .
There is one important detail . In practice , since the space ( ie , D ) is typically large , we often have to choose to store only a few ( say bi ) bits of i∗ . In other words , after we obtain sample ( i∗ , t∗ ) , we will use bi bits for storing each i∗ and 0 bit for storing each t∗ . The effective data matrix will be of 2bi × k dimension with exactly k 1 ’s in each row . In our experimental study , we always use four choices of bi ∈ {1 , 2 , 4 , 8} , corresponding to the four columns ( from left to right ) in Figure 7 and Figure 8 .
Figure 7 presents the results of the linear SVM experiments on a variety of datasets . In each panel , the two dashed curves ( red/top and blue/bottom ) correspond to the original test accuracies for the min max kernel and the linear kernel ( respectively ) . In each panel , the solid curves are the results obtained by feeding the hashed data from 0 bit CWS to LIBLINEAR , for k = 32 , 64 , 128 , 256 , 512 , 1024 , 2048 , 4096 ( from bottom to top ) . For most of the datasets , we can see that the test classification accuracies approach the results of min max kernels , when k is large enough , especially if we use 8 bits to store each i∗ .
Figure 8 presents an interesting study for comparing the 0 bit scheme ( ie , bt = 0 for t∗ ) with the 2 bit scheme ( ie , bt = 2 for t∗ ) . We can see that once we use ≥ 4 bits for i∗ , it makes no essential difference whether we use 0 bit or 2 bit scheme for t∗ , ie , the solid and dashed curves overlap .
5 . EXPERIMENT ON LARGER DATA
In Section 4 , we have only experimented with datasets of moderate sizes . Here we re iterate that , to prove the effectiveness of our proposal , we are obligated to show that the result of 0 bit CWS with enough samples could approach that of exact min max kernel . The strategy we adopt by using LIBSVM pre computed kernels , although most repeatable , is very memory expensive for datasets which are not even large [ 3 ] . On the other hand , once we have proved the effectiveness of 0 bit CWS , applying the method to larger data is straightforward , except that we would not be able to compute the exact classification result of min max kernel .
Figure 9 presents the detailed results on the complete WebspamN1 ( uni gram ) dataset , which has in total 350,000 examples . Note that WebspamN1 20k in Table 1 is just a small subset of WebspamN1 . We use half of the examples of WebspamN1 for training and the other half for testing . With linear SVM , the test classification accuracy is about 93 % . The proposed 0 bit CWS can achieve > 98 % accuracies given enough samples . This is a significant improvement . Note that 0 bit CWS achieves the accuracy of the original linear SVM by using merely k = 64 samples .
671 4096
1024 512
256
128 64 k = 32 3 10
256 128
64
)
%
( y c a r u c c A
90
80
70
60 10
−2
100
100
95
90
85
80
75
)
%
( y c a r u c c A
70 10
−2
100
)
%
( y c a r u c c A
80
60
40
)
%
( y c a r u c c A
90
80
70
60 10
−2
100
)
%
( y c a r u c c A
90
80
70
60 10 100
−2
90
80
70
60
50 10
−2
100
95
90
85
80
)
%
( y c a r u c c A
)
%
( y c a r u c c A
)
%
( y c a r u c c A
MNIST10k
4096 2048
1024
256
128
64
−1
10
0 10
1 10
2 10
3 10
C
M−Rotate
20 10
−2
−1
10
0 10
1 10
2 10
100
Pendigits
C k = 32
−1
10
0 10
1 10
2 10
3 10
C
Satimage
128256 64 k = 32
−1
10
0 10
1 10
2 10
3 10
Splice
C
4096 1024 512 256 128
64 k = 32
−1
10
0 10
1 10
2 10
3 10
C
USPS
4096 1024 512 256 128
64
75 10
−2
100
−1
10
0 10
1 10
2 10
3 10
C
WebspamN1−20k
95
90
85
80
4096 1024
256
128
64 k = 32
MNIST10k
100
95
90
85
80
75
)
%
( y c a r u c c A
MNIST10k
2048 1024 512
256 128
64
100
95
90
85
80
75
)
%
( y c a r u c c A
1024 512 256
128
64 k = 32
70 10
−2
100
−1
10
0 10
1 10
C k = 32 2 10
3 10
70 10
−2
100
−1
10
0 10
1 10
2 10
3 10
C
100
95
90
85
80
75
)
%
( y c a r u c c A
70 10
−2
100
MNIST10k
512 256 128
64 k = 32
−1
10
0 10
1 10
2 10
3 10
C
M−Rotate
)
%
( y c a r u c c A
80
60
40
M−Rotate
)
%
( y c a r u c c A
80
60
40
4096 2048
1024 256 128
64 k = 32
M−Rotate
4096 1024 512
256 64 k = 32
)
%
( y c a r u c c A
80
60
40
4096
1024 512 256
128 k = 32
20 10
−2
−1
10
0 10
1 10
2 10
3 10
20 10
−2
−1
10
0 10
1 10
2 10
3 10
20 10
−2
−1
10
0 10
1 10
2 10
3 10
100
Pendigits
C
100
Pendigits
C
100
Pendigits
C
128 64 k = 32
128 64 k = 32
128 64 k = 32
)
%
( y c a r u c c A
90
80
70
60 10
−2
100
)
%
( y c a r u c c A
90
80
70
60 10 100
−2
90
80
70
60
50 10
−2
100
95
90
85
80
75 10
−2
100
95
90
85
80
)
%
( y c a r u c c A
)
%
( y c a r u c c A
)
%
( y c a r u c c A
−1
10
0 10
1 10
2 10
3 10
C
Satimage
64 k = 32
−1
10
0 10
1 10
2 10
3 10
Splice
C
1024 256 128 64 k = 32
−1
10
0 10
1 10
2 10
3 10
C
USPS
256 128
64 k = 32
−1
10
0 10
1 10
2 10
3 10
C
WebspamN1−20k
4096 2048
128 64 k = 32
)
%
( y c a r u c c A
90
80
70
60 10
−2
100
)
%
( y c a r u c c A
90
80
70
60 10 100
−2
90
80
70
60
50 10
−2
100
95
90
85
80
75 10
−2
100
95
90
85
80
)
%
( y c a r u c c A
)
%
( y c a r u c c A
)
%
( y c a r u c c A
−1
10
0 10
1 10
2 10
3 10
C
Satimage
64 k = 32
−1
10
0 10
1 10
2 10
3 10
Splice
C
1024 256 128 64 k = 32
−1
10
0 10
1 10
2 10
3 10
C
USPS
256 128
64 k = 32
−1
10
0 10
1 10
2 10
3 10
C
WebspamN1−20k
4096 1024 128 64 k = 32
−1
10
0 10
1 10
2 10
3 10
C
Satimage
128 64 k = 32
−1
10
0 10
1 10
2 10
3 10
Splice
C
4096 1024 512 128256
64 k = 32
−1
10
0 10
1 10
2 10
3 10
C
USPS
1024 512 256
128 64 k = 32
−1
10
0 10
1 10
2 10
3 10
C
WebspamN1−20k
4096 2048 256 128
64 k = 32
)
%
( y c a r u c c A
90
80
70
60 10 100
−2
90
80
70
60
50 10
−2
100
95
90
85
80
75 10
−2
100
95
90
85
80
)
%
( y c a r u c c A
)
%
( y c a r u c c A
)
%
( y c a r u c c A
75 10
−2
−1
10
0 10
1 10
2 10
3 10
75 10
−2
−1
10
0 10
1 10
2 10
3 10
75 10
−2
−1
10
0 10
1 10
2 10
3 10
75 10
−2
−1
10
0 10
1 10
2 10
3 10
C
C
C
C
Figure 7 : Classification accuracies by using 0 bit CWS hashing and linear SVM . The original CWS algorithm produces samples in the form of ( i∗ , t∗ ) . The 0 bit scheme discards t∗ . From left to right , the four columns represent the results for coding i∗ using 1 bit , 2 bits , 4 bits , and 8 bits , respectively . In each panel , the two dashed curves represent the original classification results using min max kernel ( top and red ) and linear kernel ( bottom and blue ) . The solid curves are the results of linear SVM and 0 bit CWS with k = 32 , 64 , 128 , 256 , 512 , 1024 , 2048 , 4096 ( from bottom to top ) .
672 MNIST10k
100
95
90
85
80
)
%
( y c a r u c c A
MNIST10k
100
95
90
85
80
)
%
( y c a r u c c A k = 2048 k = 512 k = 128
MNIST10k k = 2048 k = 512 k = 128
100
95
90
85
80
)
%
( y c a r u c c A k = 2048 k = 512 k = 128
100
95
90
85
80
)
%
( y c a r u c c A
MNIST10k k = 2048 k = 512 k = 128
75 10
−2
−1
10
0 10
1 10
2 10
3 10
75 10
−2
−1
10
0 10
1 10
2 10
3 10
75 10
−2
−1
10
0 10
1 10
2 10
3 10
75 10
−2
−1
10
0 10
1 10
2 10
3 10
C
M−Rotate k = 2048 k = 512
−1
10
0 10
1 10
2 10
3 10 k = 128
C
Pendigits k = 2048 k = 512 k = 128
90
80
70
60
50
40
)
%
( y c a r u c c A
30 10
−2
100
)
%
( y c a r u c c A
95
90
85
80
90
80
70
60
50
40
)
%
( y c a r u c c A
30 10
−2
100
)
%
( y c a r u c c A
95
90
85
80
C
M−Rotate k = 2048 k = 512 k = 128
−1
10
0 10
1 10
2 10
3 10
C
Pendigits k = 2048 k = 512 k = 128
90
80
70
60
50
40
)
%
( y c a r u c c A
30 10
−2
100
)
%
( y c a r u c c A
95
90
85
80
C
M−Rotate k = 2048 k = 512 k = 128
−1
10
0 10
1 10
2 10
3 10
C
Pendigits k = 2048 k = 512 k = 128
90
80
70
60
50
40
)
%
( y c a r u c c A
30 10
−2
100
)
%
( y c a r u c c A
95
90
85
80
C
M−Rotate k = 2048 k = 512 k = 128
−1
10
0 10
1 10
2 10
3 10
C
Pendigits k = 2048 k = 512 k = 128
75 10
−2
−1
10
0 10
1 10
2 10
3 10
75 10
−2
−1
10
0 10
1 10
2 10
3 10
75 10
−2
−1
10
0 10
1 10
2 10
3 10
75 10
−2
−1
10
0 10
1 10
2 10
3 10
C
Satimage
95
90
85
80
75
)
%
( y c a r u c c A k = 2048 k = 512 k = 128
95
90
85
80
75
)
%
( y c a r u c c A
C
Satimage k = 2048 k = 512 k = 128
95
90
85
80
75
)
%
( y c a r u c c A
C
Satimage k = 2048 k = 512 k = 128
95
90
85
80
75
)
%
( y c a r u c c A
C
Satimage k = 2048 k = 512 k = 128
70 10 100
−2
)
%
( y c a r u c c A
90
80
70
60
50 10
−2
100
)
%
( y c a r u c c A
90
80
70 10
−2
100
−1
10
0 10
1 10
2 10
3 10
Splice
C k = 2048 k = 512 k = 128
−1
10
0 10
1 10
2 10
3 10
C
USPS k = 2048 k = 512 k = 128
−1
10
0 10
1 10
2 10
3 10
C
70 10 100
−2
)
%
( y c a r u c c A
90
80
70
60
50 10
−2
100
)
%
( y c a r u c c A
90
80
70 10
−2
100
−1
10
0 10
1 10
2 10
3 10
Splice
C k = 2048 k = 512 k = 128
−1
10
0 10
1 10
2 10
3 10
C
USPS k = 2048 k = 512 k = 128
−1
10
0 10
1 10
2 10
3 10
C
70 10 100
−2
)
%
( y c a r u c c A
90
80
70
60
50 10
−2
100
)
%
( y c a r u c c A
90
80
70 10
−2
100
−1
10
0 10
1 10
2 10
3 10
Splice
C k = 2048 k = 512 k = 128
−1
10
0 10
1 10
2 10
3 10
C
USPS k = 2048 k = 512 k = 128
−1
10
0 10
1 10
2 10
3 10
C
WebspamN1−20k
WebspamN1−20k
WebspamN1−20k
)
%
( y c a r u c c A
95
90 k = 2048 k = 512 k = 128
)
%
( y c a r u c c A
95
90 k = 2048 k = 512 k = 128
)
%
( y c a r u c c A
95
90 k = 2048 k = 512 k = 128
70 10 100
−2
)
%
( y c a r u c c A
90
80
70
60
50 10
−2
100
)
%
( y c a r u c c A
90
80
70 10
−2
100
)
%
( y c a r u c c A
95
90
−1
10
0 10
1 10
2 10
3 10
Splice
C k = 2048 k = 512 k = 128
−1
10
0 10
1 10
2 10
3 10
C
USPS k = 2048 k = 512 k = 128
−1
10
0 10
1 10
2 10
3 10
C
WebspamN1−20k k = 2048 k = 512 k = 128
85 10
−2
−1
10
0 10
1 10
2 10
3 10
C
85 10
−2
−1
10
0 10
1 10
2 10
3 10
85 10
−2
C
−1
10
0 10
1 10
2 10
3 10
C
85 10
−2
−1
10
0 10
1 10
2 10
3 10
C
Figure 8 : Classification accuracies by using linear SVM with 0 bit CWS ( solid and black curves ) and 2 bit CWS ( dashed and red curves ) . The original CWS algorithm produces samples in the form of ( i∗ , t∗ ) . The 0 bit scheme discards t∗ while the 2 bit scheme keeps 2 bits for each t∗ . From left to right , the four columns represent the results for coding i∗ using 1 bit , 2 bits , 4 bits , and 8 bits , respectively . In each panel , the 3 solid curves ( 0 bit scheme for k =128 , 512 , 2048 ) and the 3 dashed curves ( 2 bit scheme ) essentially overlap especially when we use ≥ 4 bits for coding i∗ .
673 100
98
96
94
92
90
88
)
%
( y c a r u c c A
WebspamN1
4096 2048 1024
512
256
128
86 10
−2 b = 1 i k = 64
−1
10
0 10
1 10
2 10
3 10
C
WebspamN1
100
98
96
94
92
90
88
)
%
( y c a r u c c A
86 10
−2 b = 8 i −1
10
0 10
4096
512 256 128
64 k = 32
1 10
2 10
3 10
C
Figure 9 : Classification accuracies by using 0 bit CWS hashing and linear SVM on WebspamN1 ( which is a dataset much larger than those in Table 1 ) . The left panel represents the results for coding each i∗ using 1 bit and the right panel for coding each i∗ using 8 bits . In each panel , the dashed curve represents the result of linear SVM on the original dataset . The solid curves are the results of linear SVM and 0 bit CWS with k = 32 , 64 , 128 , 256 , 512 , 1024 , 2048 , 4096 ( from bottom to top ) .
6 . CONCLUSION
Our contributions consist of three parts . Firstly , we conduct an extensive empirical study on training nonlinear kernel SVMs using min max kernels , on a wide variety of public datasets . This study answers why we should consider using min max kernels instead of linear kernels . Secondly , we propose an efficient ( and surprisingly simple ) implementation of consistent weighted sample , called “ 0 bit ” CWS , and we validate this proposal via an extensive simulation study using real word co occurrence data . Finally , we show that the proposed 0 bit CWS can be easily integrated into a linear learning system and we demonstrate , on a variety of datasets , that we can achieve the results of nonlinear SVMs at the cost of training linear SVMs by using samples generated from 0 bit CWS . Given the popularity of minwise hashing in industry , we expect 0 bit CWS will also be adopted in practice for search and learning from massive data .
7 . REFERENCES [ 1 ] R . J . Bayardo , Y . Ma , and R . Srikant . Scaling up all pairs similarity search . In WWW , pages 131–140 , 2007 .
[ 2 ] L . Bottou . http://leonbottouorg/projects/sgd [ 3 ] L . Bottou , O . Chapelle , D . DeCoste , and J . Weston , editors . Large Scale Kernel Machines . The MIT Press , Cambridge , MA , 2007 .
[ 4 ] A . Z . Broder . On the resemblance and containment of documents . In the Compression and Complexity of Sequences , pages 21–29 , Positano , Italy , 1997 .
[ 5 ] A . Z . Broder , S . C . Glassman , M . S . Manasse , and
G . Zweig . Syntactic clustering of the web . In WWW , pages 1157 – 1166 , 1997 .
[ 6 ] L . Cherkasova , K . Eshghi , C . B . M . III , J . Tucek , and A . C . Veitch . Applying syntactic similarity algorithms for enterprise information management . In KDD , pages 1087–1096 , 2009 .
[ 7 ] F . Chierichetti , R . Kumar , S . Lattanzi ,
M . Mitzenmacher , A . Panconesi , and P . Raghavan . On compressing social networks . In KDD , pages 219–228 , 2009 .
[ 8 ] G . Cormode and S . Muthukrishnan . Space efficient mining of multigraph streams . In Proceedings of the twenty fourth ACM SIGMOD SIGACT SIGART symposium on Principles of database systems , pages 271–282 . ACM , 2005 .
[ 9 ] A . S . Das , M . Datar , A . Garg , and S . Rajaram .
Google news personalization : scalable online collaborative filtering . In Proceedings of the 16th international conference on World Wide Web , pages 271–280 . ACM , 2007 .
[ 10 ] R E Fan , K W Chang , C J Hsieh , X R Wang , and C J Lin . Liblinear : A library for large linear classification . Journal of Machine Learning Research , 9:1871–1874 , 2008 .
[ 11 ] G . Forman , K . Eshghi , and J . Suermondt . Efficient detection of large scale redundancy in enterprise file systems . SIGOPS Oper . Syst . Rev . , 43(1):84–91 , 2009 .
[ 12 ] T . J . Hastie , R . Tibshirani , and J . H . Friedman . The
Elements of Statistical Learning:Data Mining , Inference , and Prediction . Springer , New York , 2001 .
[ 13 ] M . R . Henzinger . Finding near duplicate web pages : a large scale evaluation of algorithms . In SIGIR , 2006 .
[ 14 ] S . Ioffe . Improved consistent sampling , weighted minhash and L1 sketching . In ICDM , 2010 .
[ 15 ] T . Joachims . Training linear svms in linear time . In
KDD , pages 217–226 , 2006 .
[ 16 ] N . Koudas , S . Sarawagi , and D . Srivastava . Record linkage : similarity measures and algorithms . In SIGMOD , pages 802–803 . ACM , 2006 .
[ 17 ] H . Larochelle , D . Erhan , A . C . Courville , J . Bergstra , and Y . Bengio . An empirical evaluation of deep architectures on problems with many factors of variation . In ICML , pages 473–480 , 2007 .
[ 18 ] P . Li . Abc boost : Adaptive base class boost for multi class classification . In ICML , 2009 .
[ 19 ] P . Li . Robust logitboost and adaptive base class ( abc ) logitboost . In UAI , 2010 .
[ 20 ] P . Li . CoRE kernels . In UAI , 2014 . [ 21 ] P . Li and A . C . K¨onig . b bit minwise hashing . In
Proceedings of the 19th International Conference on World Wide Web , pages 671–680 , 2010 .
[ 22 ] P . Li , A . Shrivastava , J . Moore , and A . C . K¨onig .
Hashing algorithms for large scale learning . In NIPS , 2011 .
[ 23 ] S . Maji , A . Berg , and J . Malik . Classification using intersection kernel support vector machines is efficient . In CVPR , pages 1–8 , 2008 .
[ 24 ] M . Manasse , F . McSherry , and K . Talwar . Consistent weighted sampling . Technical Report MSR TR 2010 73 , Microsoft Research , 2010 .
[ 25 ] H . B . McMahan , G . Holt , D . Sculley , M . Young ,
D . Ebner , J . Grady , L . Nie , T . Phillips , E . Davydov , D . Golovin , S . Chikkerur , D . Liu , M . Wattenberg , A . M . Hrafnkelsson , T . Boulos , and J . Kubica . Ad click prediction : A view from the trenches . In KDD , pages 1222–1230 , 2013 .
[ 26 ] S . Pandey , A . Broder , F . Chierichetti , V . Josifovski ,
R . Kumar , and S . Vassilvitskii . Nearest neighbor caching for content match applications . In WWW , pages 441–450 , 2009 .
[ 27 ] S . Shalev Shwartz , Y . Singer , and N . Srebro . Pegasos :
Primal estimated sub gradient solver for svm . In ICML , pages 807–814 , 2007 .
[ 28 ] T . Urvoy , E . Chauveau , P . Filoche , and T . Lavergne . Tracking web spam with html style similarities . ACM Trans . Web , 2(1):1–28 , 2008 .
674
