Adaptation Algorithm and Theory Based on Generalized Discrepancy
Corinna Cortes Google Research 111 8th Avenue
New York , NY 10011
Mehryar Mohri
Courant Institute and Google
251 Mercer Street New York , NY 10012
Andrés Muñoz Medina
Courant Institute 251 Mercer Street New York , NY 10012
ABSTRACT We present a new algorithm for domain adaptation improving upon the discrepancy minimization algorithm ( DM ) , which was previously shown to outperform a number of popular algorithms designed for this task . Unlike most previous approaches adopted for domain adaptation , our algorithm does not consist of a fixed reweighting of the losses over the training sample . Instead , it uses a reweighting that depends on the hypothesis considered and is based on the minimization of a new measure of generalized discrepancy . We give a detailed description of our algorithm and show that it can be formulated as a convex optimization problem . We also present a detailed theoretical analysis of its learning guarantees , which helps us select its parameters . Finally , we report the results of experiments demonstrating that it improves upon the DM algorithm in several tasks .
1 .
INTRODUCTION
A standard assumption in much of learning theory and algorithms is that the training and test data are sampled from the same distribution . In practice , however , this assumption often does not hold . The learner then faces the more challenging problem of domain adaptation where the source and target distributions are distinct . This problem arises in a variety of applications such as natural language processing and computer vision [ Dredze et al . , 2007 , Blitzer et al . , 2007b , Jiang and Zhai , 2007 , Leggetter and Woodland , 1995 , Martínez , 2002 , Hoffman et al . , 2014 ] and many other others .
The theory of domain adaptation has been developed in recent years . Early generalization bounds were presented for this problem by Ben David et al . [ 2006 ] and Blitzer et al . [ 2007a ] using a dAdistance . In previous work [ Mansour , Mohri , and Rostamizadeh , 2009a , Cortes and Mohri , 2011 ] , we introduced the notion of discrepancy , which generalizes the dA distance to arbitrary loss functions . We further showed that the discrepancy measure can be accurately estimated from data and proved data dependent Rademacher complexity bounds for its estimation . We also gave new generalization bounds for domain adaptation based on the discrepancy measure , which we proved to be , under some plausible assumptions , superior to those previously derived by Ben David et al . [ 2006 ] or Blitzer et al . [ 2007a ] ( which we showed in fact suffer a factor of 3 of the error that can make them vacuous ) . We also gave a series of pointwise loss guarantees for the broad class of kernel based regularized empirical risk minimization algorithms in terms of the empirical discrepancy . In [ Mohri and Muñoz , 2012 ] we further introduced and used the related notion of Y discrepancy ( later rediscovered as integral probability metric [ Zhang , Zhang , and Ye , 2012 ] ) to derive guarantees for the problem of learning with drifting distributions . This notion was later used by Germain , Habrard , Laviolette , and Morvant [ 2013 ] to study the problem of domain adaptation in a PAC Bayesian setting . Altogether , these theoretical results suggest that the discrepancy is a key quantity in the analysis of adaptation appearing both in upper bounds and lower bounds .
Clearly , domain adaptation cannot always succeed . This depends on the discrepancy between the source and target distribution and some related properties of the labeling functions . This is also corroborated by some negative examples given by Ben David et al . [ 2010 ] and Ben David and Urner [ 2012 ] . As pointed out by these authors , the problem becomes trivially intractable where the hypothesis set contains no candidate with good performance on the training set . However , the adaptation tasks found in applications seem to be often more favorable than such worst cases and several empirical results suggest that adaptation can indeed succeed . Recent work by Wen et al . [ 2014 ] also uses a game theoretic approach to characterize some scenarios where domain adaptation is beneficial .
We can distinguish two broad families of adaptation algorithms . Some consist of finding a new feature representation . The core idea behind these algorithms is to map the source and target data into a new feature space where the difference between source and target distributions is reduced . Transfer Component Analysis ( TCA ) [ Pan et al . , 2011 ] and the work on Frustratingly Easy Domain Adaptation ( FE ) [ Daumé III , 2007 ] belong both to this family of algorithms . While some empirical evidence has been reported in the literature for the effectiveness of these algorithms , we are not aware of any theoretical guarantees in support of these techniques .
Many other adaptation algorithms can be viewed as reweighting techniques . Originated in the Statistics literature on sample bias correction , these techniques attempt to correct the difference between distributions by multiplying every training example by a positive weight . Most of the classical algorithms such as KMM [ Huang et al . , 2006 ] , KLIEP [ Sugiyama et al . , 2007 ] and discrepancy minimization ( DM ) [ Mansour et al . , 2009b , Cortes and Mohri , 2011 ] fall in this category .
The underlying idea behind common reweighting techniques is that of minimizing the distance between the reweighted empirical source and target distribution . A crucial component of these learn
169 ing algorithms is thus the choice of divergence distance between measures . The KLIEP algorithm is based on the minimization of the KL divergence , while algorithms such as KMM or the algorithm of Zhang et al . [ 2013 ] use the maximum mean discrepancy distance as the divergence to be minimized . It is worth noting that , under some realizability assumptions the algorithm of Zhang et al . [ 2013 ] can also be used for the case when the labeling functions shift . The aforementioned algorithms do not provide any learning guarantees . Instead , if the source and target distributions admit densities q(x ) and p(x ) respectively , the authors show that the weight on the sample point xi will converge to the importance ratio p(xi)/q(xi ) . The use of this ratio is commonly known as importance weighting and it provides and unbiased estimate for the expected loss on the target distribution . While this unbiasedness makes it a natural approach , it has been shown both empirically and theoretically that importance weighting algorithms can fail for the common case where the importance ratio becomes unbounded unless the second moment bounded , an assumption that cannot be tested in general [ Cortes , Mansour , and Mohri , 2010 ] .
In contrast , in [ Mansour , Mohri , and Rostamizadeh , 2009b ] and [ Cortes and Mohri , 2011 ] , we derived generalization bounds for domain adaptation and showed that these bounds directly depend on the discrepancy . We further derived a discrepancy minimization ( DM ) algorithm that seeks to minimize this generalization bound [ Cortes and Mohri , 2011 ] . This algorithm was shown to perform well in a number of adaptation tasks and to match or outperform several other algorithms such as KMM , KLIEP and a two stage algorithm by Bickel et . al [ Bickel et al . , 2007 ] . The main advantage of the DM algorithm is that it takes into account the hypothesis set and the loss function which were previously ignored by other reweighting techniques even though these are crucial elements of any learning algorithm .
One shortcoming of the DM algorithm , however , is that it seeks to reweight the loss on the training samples to minimize a quantity defined as the maximum over all pairs of hypotheses , including hypotheses that the learning algorithm might not ever consider as candidates . Thus , the algorithm tends to be too conservative . We present an alternative theoretically well founded algorithm for domain adaptation that is based on minimizing a finer quantity , the generalized discrepancy , and that seeks to improve upon DM . Unlike the DM algorithm , our algorithm does not consist of a fixed reweighting of the losses over the training sample . Instead , the weights assigned to training sample losses vary as a function of the hypothesis h . This helps us ensure that for every hypothesis , h , the empirical loss on the source distribution is as close as possible to the empirical loss on the target distribution for that particular h .
We first describe the learning scenario of domain adaptation in Section 2 . Then , we give a detailed description of our algorithm and prove that it can be formulated as a convex optimization problem ( Section 3 ) . Next , we analyze the theoretical properties of our algorithm , which guide us to choose the surrogate hypothesis set defining our algorithm ( Section 4 ) . In Section 5 , we further analyze the optimization problem defining our algorithm and derive an equivalent form that can be handled by a standard convex optimization solver . In Section 6 , we report the results of experiments demonstrating that our algorithm improves upon the DM algorithm in several tasks .
2 . LEARNING SCENARIO
This section defines the learning scenario of domain adaptation we consider , which coincides with that of Blitzer et al . [ 2007a ] , Mansour et al . [ 2009a ] , or Cortes and Mohri [ 2013 ] and introduces the definitions and concepts needed for the following sections . For the most part , we follow the definitions and notation of Cortes and Mohri [ 2013 ] . Let X denote the input space and Y ⊆ R the output space . We define a domain as a pair formed by a distribution over X and a target labeling function mapping from X to Y . Throughout the paper , ( Q , fQ ) denotes the source domain and ( P , fP ) the target domain with Q the source and P the target distribution over X while fQ , fP : X → Y , are the source and target labeling functions respectively .
In the scenario of domain adaptation we consider , the learner receives two samples : a labeled sample of m points from the source domain S = ( (x1 , y1 ) , . . . , ( xm , ym ) ) ∈ ( X × Y)m with x1 , . . . , xm drawn iid according to Q and yi = fQ(xi ) for i ∈ [ 1 , m ] ; n ) ∈ X n of size n drawn and an unlabeled sample T = ( x iid according to the target distribution P . We denote by Q the empirical distribution corresponding to x1 , . . . , xm and by P the
1 , . . . , x
1 , y s , y
1 ) , . . . , ( x empirical distribution corresponding to T . We will be in fact more interested in the scenario commonly encountered in practice where , in addition to these two samples , a small amount of labeled data from the target domain T = ( (x s ) ) ∈ ( X×Y)s is received by the learner . We consider a loss function L : Y × Y → R+ jointly convex in its two arguments . The Lp losses commonly used in regression and defined by Lp(y , y ) = |y − y|p for p ≥ 1 are special instances of this definition . For any two functions h , h : X → Y and any distribution D over X , we denote by LD(h , h ) the expected loss of h(x ) and h(x ) : LD(h , h ) = Ex∼D[L(h(x ) , h(x)) ] . The learning problem consists of selecting a hypothesis h out of a hypothesis set H with a small expected loss LP ( h , fP ) with respect to the target domain . We further extend this notation to arbitrary functions q : X → R with a finite support as follows : Lq(h , h ) = x∈X q(x)L(h(x ) , h(x) ) . 3 . ALGORITHM
In this section , we introduce our adaptation algorithm . We first review related previous work , next we present the key idea behind the algorithm and derive its general form , and finally formulate it as a convex optimization problem . 3.1 Previous work
It was shown by Mansour et al . [ 2009a ] and Cortes and Mohri [ 2011 ] ( see also the dA distance [ Ben David et al . , 2006 ] in the case of binary loss for classification ) that a key measure of the difference of two distributions in the context of adaptation is the discrepancy . Given a hypothesis set H , the discrepancy , disc , between two distributions P and Q over X is defined by : fifiLP ( h
, h)fifi . disc(P , Q ) = max h,h∈H
, h ) − LQ(h
( 1 )
The discrepancy has several advantages over a measure such as the L1 or total variation distance [ Cortes and Mohri , 2013 ] : it is a finer measure than the L1 distance , it takes into account the loss function and the hypothesis set , it can be accurately estimated from finite samples for common hypothesis sets such as kernel based ones , it is symmetric and verifies the triangle inequality . It further defines a distance in the case of an Lp loss used with a universal kernel such as a Gaussian kernel .
Several generalization bounds for adaptation in terms of the discrepancy have been given in the past [ Mansour et al . , 2009a , Cortes and Mohri , 2011 , 2013 ] , including pointwise guarantees in the case of kernel based regularized empirical risk minimization , which includes algorithms such as support vector machines ( SVM ) , kernel ridge regression , or support vector regression ( SVR ) . The bounds
170 given in [ Mansour et al . , 2009a ] motivated a discrepancy minimization algorithm . Given a positive semi definite ( PSD ) kernel K , the hypothesis returned by the algorithm is the solution of the following optimization problem
λh2
K + Lqmin ( h , fQ ) , min h∈H
( 2 ) where · K is the norm on the reproducing Hilbert space H induced by the kernel K and qmin is a distribution over the support of
Q such that qmin = argminq∈Q disc(q,P ) , where Q is the set of all distributions defined over the support of Q . Using qmin instead of Q amounts to reweighting the loss on the training samples to minimize the discrepancy between the empirical distribution and P .
Besides its theoretical motivation , this algorithm has been shown to outperform several other algorithms in a series of experiments carried out by Cortes and Mohri [ 2013 ] .
Observe that , by definition , the objective function optimized by qmin corresponds to a maximum over all pairs of hypotheses . But , the maximizing pair of hypotheses may not be among the candidates considered by the learning algorithm or available to it . Thus , a learning algorithm based on discrepancy minimization tends to be too conservative . 3.2 Main idea
Assume as in several previous studies [ Mansour et al . , 2009a , Cortes and Mohri , 2013 ] that the standard algorithm selected by the learner is regularized risk minimization over the Hilbert space H induced by a PSD kernel K . This covers a broad family of algorithms frequently used in applications . Ideally , that is in the absence of a domain adaptation problem , the learner would have access to the labels of the points in T . Therefore , he would return the hypothesis h∗ solution of the optimization problem minh∈H F ( h ) , where F is the convex function defined for all h ∈ H by K + LP ( h , fP ) ,
( 3 ) where λ ≥ 0 is a regularization parameter . Thus , h∗ can be viewed as the ideal hypothesis .
F ( h ) = λh2
In view of that , we can formulate our objective , in the presence of a domain adaptation problem , as that of finding a hypothesis h whose loss LP ( h , fP ) with respect to the target domain is as close as possible to LP ( h∗ , fP ) . To do so , we will seek in fact a hypothesis h that is as close as possible to h∗ , which would imply the closeness of the losses with respect to the target domains . We do not have access to fP and can only access the labels of the training sample S . Thus , we must resort to using in our objective function , instead of LP ( h , fP ) , a reweighted empirical loss over the training sample S . The main idea behind our algorithm is to define , for any h ∈ H , a reweighting function Qh : SX = {x1 , . . . , xm} → R such that the objective function G defined for all h ∈ H by
G(h ) = λh2
K + LQh ( h , fQ )
( 4 ) is uniformly close to F , thereby resulting in close minimizers . Since the first term of ( 3 ) and ( 4 ) coincide , the idea consists equivalently of seeking Qh such that LQh ( h , fQ ) and LP ( h , fP ) be as close as possible . Observe that this departs from the standard reweighting methods : instead of reweighting the training sample with some fixed set of weights , we allow the weights to vary as a function of the hypothesis h . Note that we have further relaxed the condition commonly adopted by reweighting techniques that the weights must be non negative and sum to one . Allowing the weights to be in a richer space than the space of probabilities over SX could raise over fitting concerns but , we will later see that this in fact does not affect our learning guarantees and leads to good empirical results .
Of course , searching for Qh to directly minimize |LQh ( h , fQ)− LP ( h , fP )| is in general not possible since we do not have access average loss LP ( h , fP ) is known to us for any h ∈ H . Qh could to fP , but it is instructive to consider the imaginary case where the then be determined via
Qh = argmin q∈F ( SX,R )
|Lq(h , fQ ) − LP ( h , fP )| ,
( 5 ) where F(SX , R ) is the set of real valued functions defined over SX . For any h , we can in fact select Qh such that LQh ( h , fQ ) = LP ( h , fP ) since Lq(h , fQ ) is a linear function of q and thus the optimization problem ( 5 ) reduces to solving a simple linear equation . With this choice of Qh , the objective functions F and G coincide and by minimizing G we can recover the ideal solution h∗ . Note that , in general , the DM algorithm could not recover that ideal solution . Even a finer discrepancy minimization algorithm exploiting the knowledge of LP ( h , fP ) for all h and seeking a dismin minimizing maxh∈H |Lq(h , fQ)−LP ( h , fP )| could tribution q not , in general , recover the ideal solution since we could not have ( h , fQ ) = LP ( h , fP ) for all h ∈ H . Lq Of course , in practice , LP ( h , fP ) is not available since the sample T is unlabeled . Instead , we will consider a non empty convex set of candidate hypotheses H ⊆ H that could contain a good approximation of fP . Using H as a set of surrogate labeling functions leads to the following definition of Qh instead of ( 5 ) : )| . h∈H |Lq(h , fQ ) − LP ( h , h
Qh = argmin q∈F ( SX,R ) max
( 6 ) min
The choice of the subset H is of course key . Our choice will be based on the theoretical analysis of Section 4 . Nevertheless , in the following section , we present the formulation of the optimization problem for an arbitrary choice of the convex subset H . 3.3 Formulation of optimization problem The following result provides a more explicit expression for LQh ( h , fQ ) leading to a simpler formulation of the optimization problem defining our algorithm . Proposition 1 . For any h ∈ H , let Qh be defined by ( 6 ) . Then , the following identity holds for any h ∈ H : h∈H LP ( h , h h∈H LP ( h , h
LQh ( h , fQ ) =
) + min max
)
.
1 2 l∈{Lq(h,fQ ) : q∈F ( SX,R)}
Proof . For any h ∈ H , the equation Lq(h , fQ ) = l with l ∈ R admits a solution q ∈ F(SX , R ) . Thus , for any h ∈ H , we can write LQh ( h , fQ ) = argmin
)| max max )| h∈H |l − LP ( h , h h∈H |l − LP ( h , h LP ( h , h max h∈H max h∈H LP ( h , h
) − l , l − LP ( h , h ) − l , l − min max max h∈H LP ( h , h
) h∈H LP ( h , h max
) + min h∈H LP ( h , h
)
,
= argmin l∈R
= argmin l∈R
= argmin l∈R
=
1 2
) since the minimizing l is obtained for max h∈H LP ( h , h
) − l = l − h∈H LP ( h , h min
) .
171
1 2
K + max h∈H LP ( h , h
In view of this proposition , with our choice of Qh based on ( 6 ) , the objective function G of our algorithm ( 4 ) can be equivalently written for all h ∈ H as follows : G(h ) = λh2 The function h → maxh∈H LP ( h , h ) is convex as a pointwise maximum of the convex functions h → LP ( h , h ) . Since the loss function L is jointly convex , so is LP , therefore , the function derived by partial minimization over a non empty convex set H for one of the arguments , h → minh∈H LP ( h , h ) , also defines h∈H LP ( h , h a convex function [ Boyd and Vandenberghe , 2004 ] . Thus , G is a convex function as a sum of convex functions .
)+ min
. ( 7 )
)
4 . LEARNING GUARANTEES
Our description of the algorithm leaves the choice of the hypothesis set H unspecified . Our choice will be guided by the theoretical analysis of this section . This will be carried out in two stages . First , we prove a pointwise loss guarantee and a generalization bound for an arbitrary choice of H . Next , we seek to minimize that bound by choosing H out of a family of hypothesis sets H parametrized by a single parameter r . Our choice of H is motivated by the proof of existence of parameter values r for which the bound we present is more favorable than that of the DM algorithm .
As in previous work , we assume that the loss function L is µ admissible : there exists µ > 0 such that
( x)|
|L(h(x ) , y ) − L(h
( x ) , y)| ≤ µ|h(x ) − h
( 8 ) holds for all ( x , y ) ∈ X × Y and h , h ∈ H , a condition that is somewhat weaker than µ Lipschitzness with respect to the first argument . The Lp losses commonly used in regression , p ≥ 1 , verify this condition [ Cortes and Mohri , 2013 ] . 4.1 Generalization bounds
The existing pointwise guarantees for the DM algorithm are directly derived from a bound on the norm of the difference of the ideal function h∗ and the hypothesis obtained after reweighting the sample losses using a distribution q . The bound is expressed in terms of the discrepancy and a term ηH ( fP , fQ ) measuring the difference of the source and target labeling functions defined by
ηH ( fP , fQ ) = min h0∈H x∈supp(P ) max
|fP ( x ) − h0(x)| x∈supp(Q )
|fQ(x ) − h0(x)|
,
+ max and is given by the following proposition .
Theorem 1 ( [Cortes and Mohri , 2013] ) . Let q be an arbitrary distribution over SX and let h∗ and hq be the hypotheses minimizing λh2 K + Lq(h , fQ ) respectively . Then , the following inequality holds :
K + LP ( h , fP ) and λh2
K ≤ µ ηH ( fP , fQ ) + disc(P , q ) .
λh
∗ − hq2
( 9 )
The DM algorithm is defined by selecting the distribution q min imizing the right hand side of the bound ( 9 ) , that is disc(P , q ) .
We will show a result of the same nature for our hypothesisdependent reweighting Qh by showing that its choice also coincides with that of minimizing an upper bound on λh∗ − h2 K . Let A(H ) be the set of all functions U : h → Uh mapping H to F(SX , R ) such that for all h ∈ H , h → LUh ( h , fQ ) is a convex function . A(H ) contains all constant functions U such that Uh = q for all h ∈ H , where q is a distribution over SX . By Proposition 1 , A(H ) also includes the function Q : h → Qh used by our algorithm . Definition 1 ( generalized discrepancy ) . For any U ∈ A(H ) , we define the generalized discrepancy between P and U as the quantity DISC(P , U ) given by DISC(P , U ) = We also denote by dP∞(fP , H ) the following distance of fP to H over the support of P : dP∞(fP , H h∈H,h∈H |LP ( h , h
) − LUh ( h , fQ)| .
|h0(x ) − fP ( x)| .
) = min
( 10 )
( 11 ) max x∈supp(P ) h0∈H max
( 12 )
λh
∗ − hU2
The following theorem gives an upper bound on the norm of the difference of the minimizing hypotheses in terms of the generalized discrepancy and dP∞(fP , H ) . Theorem 2 . Let U be an arbitrary element of A(H ) and let h∗ K + LP ( h , fP ) and and hU be the hypotheses minimizing λh2 λh2 K + LUh ( h , fQ ) respectively . Then , the following inequality holds for any convex set H ⊆ H : K ≤ µ dP∞(fP , H
) + DISC(P , U ) . Proof . Fix U ∈ A(H ) and let GP denote h → LP ( h , fP ) and K + GP ( h ) GU the function h → LUh ( h , fQ ) . Since h → λh2 is convex and differentiable and since h∗ is its minimizer , the gradient is zero at h∗ , that is 2λh∗ = −∇GP ( h∗ ) . Similarly , since h → λh2 K + GU(h ) is convex , it admits a sub differential at any h ∈ H . Since hU is a minimizer , its sub differential at hU must contain 0 . Thus , there exists a sub gradient g0 ∈ ∂GU(hU ) such that 2λhU = −g0 , where ∂GU(hU ) denotes the sub differential of GU at hU . Using these two equalities we can write ∗ − hU , g0 − ∇GP ( h ) ∗ − hU − ∇GP ( h = g0 , h ∗ − hU ∗ ) − GU(hU ) + GP ( hU ) − GP ( h ≤ GU(h ∗ = LP ( hU , fP ) − LUh ( hU , fQ ) + LUh ( h ∗ |LP ( h , fP ) − LUh ( h , fQ)| , ≤ 2 max h∈H
, fQ ) − LP ( h
∗ − hU2
K = h
2λh
, fP )
) , h
∗
∗
∗
) where we used for the first inequality the convexity of GU combined with the sub gradient property of g0 ∈ ∂GU(hU ) , and the convexity of GP . For any h ∈ H , using the µ admissibility of the loss , we can upper bound the operand of the max operator as follows : h∈H |LP ( h , h h∈H|LP ( h , h
|fP ( x ) − h0(x)| + max |fP ( x ) − h0(x)| + max
|LP ( h , fP ) − LUh ( h , fQ)| ≤ |LP ( h , fP ) − LP ( h , h0)| + |LP ( h , h0 ) − LUh ( h , fQ)| ≤ µ E x∼P ≤ µ max x∈supp(P )
) − LUh ( h , fQ)| ) − LUh ( h , fQ)| , where h0 is an arbitrary element of H . Since this bound holds for all h0 ∈ H , it follows immediately that x∈supp(P ) h0∈H max h∈H |LP ( h , h
) − LUh ( h , fQ)| ,
|fP ( x ) − h0(x)|
K ≤ µ min
∗ − hU2
λh max
+ max h∈H which concludes the proof .
172 The following pointwise guarantee for the solution hQ returned by our algorithm is a direct corollary . Corollary 1 . Let h∗ be a minimizer of λh2 hQ a minimizer of λh2 for any convex set H ⊆ H and for all ( x , y ) ∈ X × Y :
K + LP ( h , fP ) and K +LQh ( h , fQ ) . Then , the following holds
µ dP∞(fP , H ) + DISC(P , Q )
|L(hQ(x ) , y ) − L(h
( x ) , y)|
∗
≤ µR
,
λ where R2 = supx∈X K(x , x ) . Proof . By the µ admissibility of the loss , the reproducing property of H , and the Cauchy Schwarz inequality , the following holds for all x ∈ X and y ∈ Y :
∗
|L(hQ(x ) , y ) − L(h = |h ≤ Rh
− h ∗ − h
, K(x,·)K| ≤ h ∗K .
( x ) − h ( x ) , y)| ≤ µ|h
∗K − h
∗
( x)|
K(x , x )
Upper bounding h − h∗K using Theorem 2 and using the fact that Q : h → Qh is a minimizer of the bound over all choices of U ∈ A(H ) yields the desired result .
The pointwise loss guarantee just presented can be directly used to bound the difference of the expected loss of h∗ and hQ in terms of the same upper bounds , eg ,
LP ( hQ , fP )
≤ LP ( h ∗
, fP )| + µR
µ dP∞(fP , H ) + DISC(P , Q )
λ
.
( 13 )
4.2 Choice of H
In this section , we assume that L is the Lp loss for some p ≥ 1 . The results of the previous section suggest choosing H to minimize the generalization bound ( 13 ) . We will seek to do precisely that by selecting H out of the family H defined by
H = {B(r ) : r ≥ 0} , where B(r ) = {h ∈ H|Lq(h , fQ ) ≤ rp} . Thus , H is the set of all balls in H centered in fQ defined in terms of Lq , which is parametrized only by the radius r ≥ 0 . We provide a strong justification for this choice of H by proving that it contains balls H that lead to a generalization bound more favorable than that of the DM algorithm . Our algorithm is defined by selecting the radius r minimizing the generalization bound ( 13 ) . This can be done by using as validation set a small amount of labeled data from the target domain , which is typically available in practice . The following theorem proves the existence of a ball H ∈ H for which ( 12 ) is a uniformly tighter upper bound than ( 9 ) . The result is expressed in terms of the local discrepancy defined by : )| , discH ( P , q ) = h∈H,h∈H |LP ( h , h
) − Lq(h , h max which is a finer measure than the standard discrepancy for which the max is defined over a pair of hypothesis both in H ⊇ H . Theorem 3 . There exists H ∈ H such that the following holds : µ dP∞(fP , H
) − Lq(h , fQ)| h∈H,h∈H |LP ( h , h
) + max
≤ µ ηH ( fP , fQ ) + discH ( P , q ) .
, max x∈supp(P )
Proof . Let h∗
0 be the minimizer in the definition of ηH ( fP , fQ ) : h
∗ 0 = argmin h0∈H
|fP ( x ) − h0(x)| x∈supp(Q ) 0(x)| . Let q be a distribution over |fQ(x)−h ∗ and let r = max SX and choose H ∈ H as H = {h ∈ H|Lq(h , fQ ) ≤ rp} . Then , h∗
|fQ(x ) − h0(x)| ,
+ max
0(x ) − fQ(x)|pfi
∗ x∈supp(Q ) 0 is in H since Lq(h ∗ 0 , fQ ) = E x∼q
.|h ≤ max x∈supp(Q )
|h
0(x ) − fQ(x)|p = rp . ∗
For the Lp loss , it is not hard to show [ Cortes et al . , 2014][Lemma 14 ] that for all h , h ∈ H , |Lq(h , h)−Lq(h , fQ)| ≤ µ[Lq(h , fQ ) ] p . In view of this inequality , we can write : h∈H,h∈H |LP ( h , h ) − Lq(h , fQ)| max
1
) − Lq(h , h
)|
≤ max
+ max h∈H,h∈H |Lq(h , h h∈H µ[Lq(h h∈H,h∈H |LP ( h , h ≤ discH ( P , q ) + max ≤ discH ( P , q ) + µr = discH ( P , q ) + µ max x∈supp(Q )
) − Lq(h , fQ)|
1 p
, fQ ) ]
|fQ(x ) − h
0(x)| . ∗ 0 ∈ H , we can write Using this inequality and the fact that h∗ µ dP∞(fP , H h∈H,h∈H |LP ( h , h
|fP ( x ) − h0(x)| + discH ( P , q ) ≤ µ min
) − Lq(h , fQ)|
) + max x∈supp(P ) h0∈H max
+ µ max
≤ µ , max x∈supp(Q ) |fP ( x ) − h x∈supp(P ) + discH ( P , q ) , max x∈supp(P ) x∈supp(Q )
= µ min h0∈H
+ max
= µ ηH ( fP , fQ ) + discH ( P , q ) .
0(x)| |fQ(x ) − h ∗ 0(x)| + max ∗ x∈supp(Q )
0(x)|
∗
|fQ(x ) − h
|fP ( x ) − h0(x)|
|fQ(x ) − h0(x)| + discH ( P , q ) which concludes the proof .
The theorem shows that for that particular choice of H , for any constant function Uh ∈ A(H ) with Uh = q for some fixed distribution q over SX , the right hand side of the bound of Theorem 1 is lower bounded by the right hand side of the bound of Theorem 2 , since the local discrepancy is a finer quantity than the discrepancy : discH ( P , q ) ≤ disc(P , q ) . Thus , our algorithm benefits from a more favorable guarantee than the DM algorithm for that particular choice of H , especially since , our choice of Q is based on the minimization over all elements in A(H ) and not just the subset of constant functions mapping to a distribution . The following result readily follows from Theorem 3 . Corollary 2 . Let h∗ be a minimizer of λh2 a minimizer of λh2
K +LP ( h , fP ) and hQ K +LQh ( h , fQ ) . Let supx∈X K(x , x ) = R2 .
173 Proposition 2 . Let Y = ( Yij ) ∈ Rn×k be the matrix defined by k ) ∈ Rk the vector Yij = n−1/2hj(x 1 , . . . , y defined by y j)2 . Then , the dual problem of −1 ( 14 ) is given by i = n−1n i ) and y = ( y j=1 hi(x
−
Yα +
Yα +
λI +
( 15 )
Kt
Kt max α,γ,β
1 2
γ 2
γ 2 − 1 2
KtK
γ 1β ≥ −Y
† t γ + α
γ , y
− β α ≥ 0 ,
Figure 1 : Illustration of the sampling process on the set H . st 1
α =
1 2
,
∗
≤ µR
( x ) , y)|
Then , there exists a choice of H ∈ H for which the following inequality holds uniformly over over ( x , y ) ∈ X × Y :
|L(hQ(x ) , y ) − L(h
µηH ( fP , fQ)+discH ( P , qmin ) that is a larger supp(Q ) , results in a smaller minimal discrepancy discH ( P , q ) = minq∈supp(Q ) discH ( P , q ) , thereby leading to
We conclude this section by briefly discussing the effect of the sample sizes on our guarantees . Clearly , a larger source sample , a more beneficial pointwise guarantee , in view of Corollary 2 . A larger target sample , improves the guarantee on the expected loss E[L(h∗(x ) , y ) ] via standard supervised learning bounds , which , by Corollary 2 further improves the guarantee on the expected loss E[L(hQ(x ) , y) ] .
λ
.
5 . OPTIMIZATION SOLUTION
As shown in Section 3.3 , the function G defining our algorithm is convex and the problem of minimizing the expression ( 7 ) is a convex optimization problem . Nevertheless , the problem is not straightforward to solve , in particular because evaluating the term maxh∈H LP ( h , h ) that it contains requires solving a non con vex optimization problem . Here , we present an approximation to this problem based on a QP that can be efficiently solved . We have also derived an exact but less efficient solution by giving a semidefinite programming ( SDP ) formulation for the problem . Due to space limitations , we do not include that solution here , but it can be found in the full version of this paper [ Cortes et al . , 2014 ] . 5.1 QP formulation
The analysis presented in this section holds for an arbitrary convex set H . First , notice that the problem of minimizing G ( expression ( 7 ) ) is related to the minimum enclosing ball ( MEB ) problem . For a set D ⊆ Rd , the MEB problem is defined as follows : min u∈Rd max v∈D u − v2 .
Omitting the regularization and the min term from ( 7 ) leads to a problem similar to the MEB . Thus , we could benefit from the extensive literature and algorithmic study available for this problem [ Welzl , 1991 , Kumar et al . , 2003 , Sch˝onherr , 2002 , Fischer et al . , 2003 , Yildirim , 2008 ] . However , to the best of our knowledge , there is currently no solution available to this problem in the case of an infinite set D , as in the case of our problem . Instead , we present a solution for solving an approximation of ( 7 ) based on sampling . Let {h1 , . . . , hk} be a set of hypotheses in ∂H and let C = C(h1 , . . . , hk ) denote their convex hull . The following is the sampling based approximation of ( 7 ) that we consider : h∈H λh2
LP ( h , hi ) +
LP ( h , h i=1,,k
K +
( 14 ) max min h∈C min
) .
1 2
1 2 where 1 is the vector in Rk with all components equal to 1 . Furthermore , the solution h of ( 14 ) can be recovered from a solui=1 aiK(xi , x ) , where tion ( α , γ , β ) of ( 15 ) by ∀x , h(x ) = n a =,λI + 1
2 Kt)−1(Yα + 1
2 γ ) .
The proof of the proposition is given in Appendix A . The result shows that , given a finite sample h1 , . . . , hk on the boundary of H , ( 14 ) is in fact equivalent to a standard QP and therefore can be efficiently solved with one of the many off the shelf QP algorithms . We now describe the process of sampling from the boundary of H , a necessary step for defining problem ( 14 ) . Let H := {h ∈ H | gi(h ) ≤ 0} be a compact set , where the functions gi are continuous and convex . For instance , we can consider a family of sets H p = {h ∈ H| | m i=1 qmin(xi)|h(xi ) − yi|p ≤ rp} . tionh and define λi to be the minimal solution to the system
Assume h0 is given , where gi(h0 ) < 0 . Our sampling process is illustrated by Figure 1 and works as follows : pick a random direc
( λ ≥ 0 ) ∧ ( gi(h0 + λh ) = 0 ) .
Set λi = ∞ if no solution is found and define λ∗ = mini λi . The compactness of H guarantees λ∗ < ∞ . Moreover , the hypothesis λj = λ∗ . The latter is straightforward , to verify the former , assume h = h0 + λ∗h satisfies h ∈ H and gj(h ) = 0 for j such that gi(h0 +λ∗h ) > 0 for some i . The continuity of gi would imply the ih ) = 0 contradicting the choice of λi . Thus , gi(h0 + λ∗h ) ≤ 0 must hold i < λ∗ ≤ λi such that gi(h0 +λ existence of λ i with 0 < λ for all i .
Since a point h0 with gi(h0 ) < 0 can be obtained by solving a convex program and solving the equations defining λi is , in general , simple , the process described provides an efficient way of sampling points from the convex set H . 5.2 Implementation for the L2 loss
We now describe how to fully implement our algorithm for the case where L is equal to the L2 loss . In view of the results of Section 4 , we let H = {h|hK ≤ Λ ∧ Lq(h , fQ ) ≤ r2} . We begin by describing the necessary steps to find a point h0 ∈ H . Let hΛ be such that hΛK = Λ and λr ∈ R+ be such that the solution hr to the optimization problem h∈H λrh2 + Lq(h , fQ ) , min
λr is guaranteed for minh∈H Lq(h , fQ ) ≤ r2 ≤ m satisfies Lq(hr , fQ ) = r2 . It is easy to verify that the existence of i . i=1 q(xi)y2 2 ( hr + hΛ ) is in the inIt is now immediate that the point h0 = 1 terior of H . Of course , finding the value of λr with the desired properties may not be easy . However , since r is chosen through cross validation , we do not need to find λr as a function of r . Instead , we can simply select λr through cross validation too .
In order to complete the sampling process , we must have an ef ficient way of selecting a random directionh . If H ⊂ Rd is a set h0g1(h)=0g2(h)=0h0+ ⇤bhFriday , February 7 , 2014174 Figure 2 : Linear hypotheses obtained by training on the source ( green circles ) , target ( red triangles ) and by using the DM ( solid blue ) and GDM algorithms ( dashed blue ) . of linear hypotheses , a directionh can be sampled uniformly by lettingh = ξξ , where ξ is a standard Gaussian random variable we may only consider hypotheses h = m in Rd . If H is a subset of a RKHS , by the representer theorem , fore , we can sample a directionh by lettingh =m i=1 αiK(xi,· ) . ThereiK(xi,· ) where the vector α = ( α m ) is sampled uniformly from the unit sphere in Rm . A full implementation of our algorithm then • compute the distribution qmin = argminq∈Q disc(q,P ) . This consists of the following steps :
1 , . . . , α i=1 α can be done by using the smooth approximation algorithm of Cortes and Mohri [ 2013 ] ; • sample points from the set H using the sampling process • solve the QP introduced in Section 5.1 described above ;
6 . EXPERIMENTS
In this section , we report the results of extensive comparisons between GDM and several other adaptation algorithms , which show favorable results for our algorithm . 6.1 Synthetic data set
To give an empirical comparison of the GDM and DM algorithms , we adopted the following setup inspired by Huang et al . [ 2006 ] : we sampled source distribution examples from the uniform distribution over the interval [ .2 , 1 ] and target samples from the uniform distribution over [ 0 , 25 ] The labels were given by the map x → −x+x3 +ξ where ξ is a Gaussian random variable with mean 0 and standard deviation 0.1 and our hypothesis set was chosen to be that of linear functions .
Figure 2(b ) shows the regression hypotheses obtained by training the DM and GDM algorithms as well as those obtained by training on the source and the target distributions . Notice how the GDM solution approaches the ideal solution better than DM . These results can be better explained by Figure 3 which plots the objective function minimized by each algorithm as a function of the slope w of the linear function , the only variable of the hypothesis . Vertical lines show the value of the minimizing hypothesis for each loss . Keeping in mind that the regularization parameter λ used in ridge regression corresponds to a Lagrange multiplier for the constraint w2 ≤ Λ2 for some Λ [ Cortes and Mohri , 2013][Lemma 1 ] , the hypothesis set H = {w : |w| ≤ Λ} is shown at the bottom of this plot . The shaded region represents the set H = H ∩ {h|Lqmin ( h ) ≤ r} . It is clear from this plot that DM helps approximate the target loss function . Nevertheless , only GDM seems to uniformly approach it . This should come as no surprise since our algorithm was designed precisely for this purpose .
Figure 3 : Objective functions associated with training on the source distribution , training on the target distribution , as well as the GDM and DM algorithms . The hypothesis set H and surrogate hypothesis set H are shown at the bottom of the plot .
6.2 Adaptation Data Sets
We now present the results of evaluating the performance of our algorithm and comparing with several others . GDM is compared to DM and to training on the source distribution . The following algorithms were also used :
1 . The KMM algorithm [ Huang et al . , 2006 ] reweights data samples to match empirical target and source means on the feature space induced by Gaussian kernels . The hyper parameters of this algorithm were set to the recommended values of B = 1000 and =
√ m√ m−1 .
2 . KLIEP [ Sugiyama et al . , 2007 ] minimizes the KL divergence between the source and target empirical distributions . Distributions are modeled as a mixture of Gaussians . The bandwidth of the kernel for both KLIEP and KMM was selected from the set ( σd : σ = 2−5 , . . . , 1 ) via validation on the test set , where d is the mean distance between points sampled from the source domain .
3 . FE [ Daumé III , 2007 ] . This algorithm maps source and target data to a common high dimensional feature space where the difference of the distributions is hoped to be smaller
We refrained from comparing against the two stage algorithm of Bickel et al . [ 2007 ] , as it was already shown to perform similarly to KMM and KLIEP [ Cortes and Mohri , 2013 ] .
The hypothesis set H was given by linear functions . The learning algorithm used for all tasks was ridge regression and the performance evaluated by the mean squared error . We follow the setup of Cortes and Mohri [ 2011 ] . For all adaptation algorithms , we selected the parameter λ via 10 fold cross validation over the training data for λ ∈ Λ = {2−10 , . . . , 210} . The results of training on the target distribution are presented for a parameter λ tuned via 10 fold cross validation over the target data . We used the QP implementation of our algorithm with the sampling set H and the sampling mechanism defined in Section 51 The parameter λr ∈ Λ was chosen via cross validation on a small amount of data from the target distribution . To be complete , we also report the results of training only on the small amount of target data .
To make a fair comparison , we allowed the use of the small amount of labeled data to all other baselines . To do so , we simply added this data to the training set and ran the algorithms on the extended source data .
Our first task is given by the 4 kin 8xy Delve data sets [ Rasmussen et al . , 1996 ] . These data sets are variations of the same model : a realistic simulation of the forward dynamics of an 8 link all revolute robot arm . The task in all data sets consists of predicting the distance of the end effector from a target . Data sets dif
−0200020406081012−12−08−040002SourceTargetDMGDM175 ( a )
( b )
( a )
( b )
Figure 4 : ( a ) Normalized MSE performance for different adaptation algorithms when adapting from kin 8fh to the three other kin 8xy domains . Small denotes training on small labeled target sample . ( b ) Relative error of DM over GDM as a Λ . function of the ratio r fer by the degree of non linearity ( fairly linear , x=f , or non linear , x=n ) and the amount of noise in the output ( moderate , y=m , or high , y=h ) . A sample of 200 points from each domain was used for training and 10 labeled points from the target distribution were used to select H . The experiment was carried out 10 times . The results of testing on a sample of 400 points from the target domain are reported in Figure 4(a ) . The bars represent the median performance of each algorithm and error bars show the inter quartile range . All results were normalized in such a way that training on the source had error constantly equal to 1 . Notice that the performance of all algorithms is comparable when adapting to kin8 fm since both labeling functions are fairly linear , yet only GDM is able to significantly approach the performance on training on target for all three tasks . In order to better understand the advantages of GDM over DM we plot the relative error of DM against GDM as a function of the ratio r/Λ in Figure 4(b ) . Notice that when the ratio r/Λ is small , then both algorithms behave similarly which typically for the adaptation task fh to fm . On the other hand , a better performance of GDM can be obtained when the ratio is larger . This can be interpreted as follows : a small ratio means that the size of H is small and the hypothesis returned by GDM will be close to that of DM , while for H large , GDM can find a better hypothesis .
For our next experiment , we considered the cross domain sentiment analysis data set of Blitzer et al . [ 2007b ] . This data set consists of consumer reviews from 4 different domains : books , kitchen , electronics and dvds . We used the top 1,000 unigrams and bigrams as features . For each pair of adaptation tasks we sampled 700 points from the source distribution and 700 unlabeled points from the target . Only 50 labeled points from the target distribution were used to tune r . The final evaluation was done on a test set of 1,000 points . Figure 5(a ) shows normalized MSE of all algorithms when adapting from electronics to all other domains .
Figure 5 : ( a ) Normalized MSE for the sentiment adaptation task from the electronics domain to all others . ( b ) Normalized MSE of different algorithms adapting from the caltech256 dataset to all other datasets .
Finally , we considered a key domain adaptation task in the computer vision community [ Tommasi et al . , 2014 ] where the domains correspond to 4 well known collections of images : bing , caltech256 , sun and imagenet . These data sets have been standardized so that they all share the same feature representation and labeling function [ Tommasi et al . , 2014 ] . We used the data from the first 5 shared classes and sampled 800 labeled points from the source distribution and 800 unlabeled points from the target distribution , as well as 50 labeled target points used as validation to determine r . The results of testing on 1,000 points from the target domain are shown in Figure 5(b ) where we trained on caltech256 . Due to space limitations , we were not able to present the results of all possible adaptation tasks . They can be found in Cortes et al . [ 2014 ] . The results of this section show that GDM was the only algorithm that could consistently perform better than or on par with the best algorithm .
7 . CONCLUSION
We presented a new theoretically well founded domain adaptation algorithm seeking to minimize a less conservative quantity than the DM algorithm . We presented an SDP solution for the particular case of the L2 loss which can be solved in polynomial time . Our empirical results show that our new algorithm is the only adaptation algorithm consistently achieving a performance close to that of training on the target distribution .
8 . ACKNOWLEDGEMENTS
We thank reviewers for their insightful comments . This work was partially funded by the NSF award IIS 1117591 .
176 References S . Ben David and R . Urner . On the hardness of domain adaptation In Proceedings of and the utility of unlabeled target samples . ALT , pages 139–153 , 2012 .
S . Ben David , J . Blitzer , K . Crammer , and F . Pereira . Analysis of representations for domain adaptation . In Proceedings of NIPS , pages 137–144 , 2006 .
S . Ben David , T . Lu , T . Luu , and D . Pál . Impossibility theorems for domain adaptation . JMLR Proceedings Track , 9:129–136 , 2010 .
S . Bickel , M . Brückner , and T . Scheffer . Discriminative learning In Proceedings of for differing training and test distributions . ICML , pages 81–88 , 2007 .
J . Blitzer , K . Crammer , A . Kulesza , F . Pereira , and J . Wortman . Learning bounds for domain adaptation . In Proceedings of NIPS , 2007a .
J . Blitzer , M . Dredze , and F . Pereira . Biographies , bollywood , boom boxes and blenders : Domain adaptation for sentiment classification . In Proceedings of ACL , 2007b .
S . Boyd and L . Vandenberghe . Convex optimization . Cambridge
University Press , Cambridge , 2004 .
C . Cortes and M . Mohri . Domain adaptation in regression .
Proceedings of ALT , 2011 .
In
C . Cortes and M . Mohri . Domain adaptation and sample bias correction theory and algorithm for regression . Theoretical Computer Science , 9474 , 2013 .
C . Cortes , Y . Mansour , and M . Mohri . Learning bounds for importance weighting . In Proceedings of NIPS , pages 442–450 , 2010 .
C . Cortes , M . Mohri , and A . Muñoz . Adaptation algorithm and theory based on generalized discrepancy . ArXiv:1405.1503 , May 2014 .
H . Daumé III . Frustratingly easy domain adaptation . In Proceed ings of ACL , Prague , Czech Republic , 2007 .
M . Dredze , J . Blitzer , P . P . Talukdar , K . Ganchev , J . Graça , and F . Pereira . Frustratingly hard domain adaptation for dependency parsing . In EMNLP CoNLL , 2007 .
K . Fischer , B . Gärtner , and M . Kutz . Fast smallest enclosing ball computation in high dimensions . In Algorithms ESA 2003 , pages 630–641 . Springer , 2003 .
P . Germain , A . Habrard , F . Laviolette , and E . Morvant . A PACBayesian approach for domain adaptation with specialization to linear classifiers . In Proceedings of ICML , 2013 .
J . Hoffman , T . Darrell , and K . Saenko . Continuous manifold based adaptation for evolving visual domains . In Computer Vision and Pattern Recognition ( CVPR ) , 2014 .
J . Huang , A . J . Smola , A . Gretton , K . M . Borgwardt , and B . Schölkopf . Correcting sample selection bias by unlabeled data . In Proceedings of NIPS , volume 19 , pages 601–608 , 2006 .
J . Jiang and C . Zhai . Instance Weighting for Domain Adaptation in
NLP . In Proceedings of ACL , pages 264–271 , 2007 .
P . Kumar , J . S . B . Mitchell , and E . A . Yildirim . Computing coresets and approximate smallest enclosing hyperspheres in high dimensions . In ALENEX , Lecture Notes Comput . Sci , pages 45– 55 , 2003 .
C . J . Leggetter and P . C . Woodland . Maximum likelihood linear regression for speaker adaptation of continuous density hidden Markov models . Computer Speech & Language , 9(2):171–185 , 1995 .
Y . Mansour , M . Mohri , and A . Rostamizadeh . Domain adaptation : Learning bounds and algorithms . In Proceedings of COLT . Omnipress , 2009a .
Y . Mansour , M . Mohri , and A . Rostamizadeh . Domain adaptation with multiple sources . In Proceedings of NIPS . MIT Press , 2009b .
A . M . Martínez . Recognizing imprecisely localized , partially occluded , and expression variant faces from a single sample per class . IEEE Trans . Pattern Anal . , 24(6 ) , 2002 .
M . Mohri and A . Muñoz . New analysis and algorithm for learning with drifting distributions . In Proceedings of ALT . Springer , 2012 .
S . J . Pan , I . W . Tsang , J . T . Kwok , and Q . Yang . Domain adaptation via transfer component analysis . IEEE Transactions on Neural Networks , 22(2):199–210 , 2011 .
C . E . Rasmussen , R . M . Neal , G . Hinton , D . van Camp , M . R . Z . Ghahramani , R . Kustra , and R . Tibshirani . The delve project . http://wwwcstorontoedu/~delve/ data/datasets.html , 1996 . version 10
S . Sch˝onherr . Quadratic Programming in Geometric Optimization : Theory , Implementation , and applications . PhD thesis , Swiss Federal Institute of Technology , 2002 .
M . Sugiyama , S . Nakajima , H . Kashima , P . von Bünau , and M . Kawanabe . Direct importance estimation with model selection and its application to covariate shift adaptation . In Proceedings of NIPS , pages 1433–1440 , 2007 .
T . Tommasi , T . Tuytelaars , and B . Caputo . A testbed for crossdataset analysis . CoRR , abs/1402.5923 , 2014 . URL http:// arxivorg/abs/14025923
E . Welzl . Smallest enclosing disks ( balls and ellipsoids ) . In New results and new trends in computer science ( Graz , 1991 ) , volume 555 of Lecture Notes in Comput . Sci . , pages 359–370 . Springer , Berlin , 1991 .
J . Wen , C . Yu , and R . Greiner . Robust learning under uncertain test distributions : Relating covariate shift to model misspecification . In Proceedings of ICML , pages 631–639 , 2014 .
E . A . Yildirim . Two algorithms for the minimum enclosing ball SIAM Journal on Optimization , 19(3):1368–1391 , problem . 2008 .
C . Zhang , L . Zhang , and J . Ye . Generalization bounds for domain In Proceedings of NIPS , pages 1790–1798 . MIT adaptation . Press , 2012 .
K . Zhang , B . Schölkopf , K . Muandet , and Z . Wang . Domain adaptation under target and conditional shift . In Proceedings of ICML 2013 , pages 819–827 , 2013 .
177 Minimizing with respect to the primal variables yields the following KKT conditions :
1
α =
1β = δ − W
1 2
X
X u = γ
2
λI +
.
( 18 ) w = 2(X
Y )α + γ
γ XX
2
( 19 ) Condition ( 18 ) implies that the terms involving r and µ will vanish from the Lagrangian . Furthermore , the first equation in ( 19 ) implies that any feasible γ must satisfy γ = Xγ for some γ ∈ Rn . Finally , it is immediate that γu = uXXu and w = 2α(XY)w + γw . Thus , at the
2w
λI + XX u
X
− β u + α
X y α ≥ 0 ∧ δ ≥ 0 . optimal point , the Lagrangian becomes w − 1 2
1 X 2 1β = δ − W
− w
λI +
X
2
1 2
α = s . t . 1 The positivity of δ implies that 1β ≥ −W γ . Solving for w and u on ( 19 ) and applying the change of variable Xγ = γ we obtain the final expression for the dual problem :
γ
− max α,γ,β
Yα +
X
λI +
−1
X
Y α +
γ 2
†
)
γ
X ( X X 1β ≥ −Y
γ
− β y
γ + α α ≥ 0 , s . t . 1
α =
XX
2
X
γ 2 − 1 2 1 2
APPENDIX A . QP FORMULATION Proposition 2 . Let Y = ( Yij ) ∈ Rn×k be the matrix defined by k ) ∈ Rk the vector Yij = n−1/2hj(x 1 , . . . , y defined by y j)2 . Then , the dual problem of −1 ( 14 ) is given by i = n−1n i ) and y = ( y j=1 hi(x
−
Yα +
Yα +
λI +
Kt
Kt max α,γ,β
γ 2
1 2
− β y
( 16 )
γ
† t γ + α
KtK 1β ≥ −Y
γ ,
α ≥ 0 , st 1
α =
γ 2 − 1 2 1 2
, where 1 is the vector in Rk with all components equal to 1 . Furthermore , the solution h of ( 14 ) can be recovered from a solui=1 aiK(xi , x ) , where tion ( α , γ , β ) of ( 16 ) by ∀x , h(x ) = n a =,λI + 1
2 Kt)−1(Yα + 1
2 γ ) .
1 , . . . , x
We will first prove a simplified version of the proposition for the case of linear hypotheses , ie we can represent hypotheses in H and elements of X as vectors w , x ∈ Rd respectively . Define X = n−1/2(x n ) to be the matrix whose columns are the normalized sample points from the target distribution . Let also {w1 , . . . , wk} be a sample taken from ∂H and define W := ( w1 , . . . , wk ) ∈ Rd×k . With this notation , problem ( 14 ) can be rewritten as follows : λw2 +
( w − wi)2
X max
1 2 i=1,,k min w∈Rd
)2 . Lemma 1 . The Lagrange dual of problem ( 17 ) is given by
( w − w min w∈C
X
1 2
+
− max α,γ,β
Yα +
X
λI +
X
Y α +
−1
XX
2
X
†
)
γ
X ( X X 1β ≥ −Y
γ
− β y
γ + α α ≥ 0 , s . t . 1
α =
γ 2 − 1 2 1 2 where Y = XW and y Proof . Using the change of variable u = w − w , we obtain the following problem equivalent to ( 17 ) : i = Xwi2 . min w∈Rdu∈C−w
λw2 +
1 2
X w2 +
X u2
1 2
+
1 2 max i=1,,k
X wi2 − 2w i X
X w . r
Making the constraints on u explicit and replacing the maximization term with the variable r yield :
1 2
1 2
1 2 w,u,r,µ
X u2 +
λw2 + w2 + min s . t . 1r ≥ y µ = 1
X − 2Y
µ ≥ 0 W µ − w = u . For α , δ ≥ 0 , the Lagrangian of this problem is defined as L(w , u , µ , r , α , β , δ , γ = λw2 +
X w
1
)
1 2
X − 2(X
Y )
1 2 w2 + X w − 1r ) − δ u2 +
µ − 1 ) 1 r + β(1 2 ( W µ − w − u ) .
µ + γ
( y
+ α
( 17 )
γ 2 where we have used the fact that Yγ = WXγ to simplify the constraints . Notice also that we can recover the solution w of problem ( 17 ) as w = ( λI + 1
2 XX)−1X(Yα + 1
2 γ )
Using the matrix identities X(λI+XX)−1 = ( λI+XX)X and XX(XX)† = X(XX)†X , the proof of Proposition 2 is now immediate .
Proposition 2 . We can rewrite the dual objective of the previous lemma in terms of the Gram matrix XX alone as follows :
−1
X
X
λI +
XX
2
Y α +
γ 2 max α,γ,β
Yα +
−
†
)
X
γ
X
X ( X 1β ≥ −Y
γ
− β y
γ + α α ≥ 0 . s . t . 1
α =
γ 2 − 1 2 1 2
By replacing XX by the more general kernel matrix Kt ( which corresponds to the Gram matrix in the feature space ) we obtain the desired expression for the dual . Additionally , the same matrix identities applied to condition ( 19 ) imply that the optimal hypothi , x ) where a = ( λI + 2 Kt)−1(Yα + γ 2 ) . esis h is given by h(x ) = n i=1 aiK(x
1
178
