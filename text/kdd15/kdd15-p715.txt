Spectral Ensemble Clustering
Hongfu Liu1 , Tongliang Liu2 , Junjie Wu3∗ , Dacheng Tao2 , Yun Fu1
1 Department of Electrical & Computer Engineering , College of Engineering , Northeastern University ,
Boston . liuhongf@huskyneuedu , yunfu@eceneuedu
2 QCIS and FEIT , University of Technology , Sydney . tliangliu@gmailcom ( co first author ) , dachengtao@utseduau
3 School of Economics and Management , Beihang University , Beijing . wujj@buaaeducn
∗ corresponding author
ABSTRACT Ensemble clustering , also known as consensus clustering , is emerging as a promising solution for multi source and/or heterogeneous data clustering . The co association matrix based method , which redefines the ensemble clustering problem as a classical graph partition problem , is a landmark method in this area . Nevertheless , the relatively high time and space complexity preclude it from real life large scale data clustering . We therefore propose SEC , an efficient Spectral Ensemble Clustering method based on co association matrix . We show that SEC has theoretical equivalence to weighted K means clustering and results in vastly reduced algorithmic complexity . We then derive the latent consensus function of SEC , which to our best knowledge is among the first to bridge co association matrix based method to the methods with explicit object functions . The robustness and generalizability of SEC are then investigated to prove the superiority of SEC in theory . We finally extend SEC to meet the challenge rising from incomplete basic partitions , based on which a scheme for big data clustering can be formed . Experimental results on various real world data sets demonstrate that SEC is an effective and efficient competitor to some state of the art ensemble clustering methods and is also suitable for big data clustering . Categories and Subject Descriptors H28 [ Database Management ] : Database Applications— Data Ming General Terms Algorithm , Theory Keywords Ensemble Clustering , Co association Matrix , K means
1 .
INTRODUCTION
Ensemble clustering , also known as consensus clustering , is emerging as a promising solution for multi source and/or Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . Copyrights for components of this work owned by others than the author(s ) must be honored . Abstracting with credit is permitted . To copy otherwise , or republish , to post on servers or to redistribute to lists , requires prior specific permission and/or a fee . Request permissions from Permissions@acmorg KDD’15 , August 10 13 , 2015 , Sydney , NSW , Australia . Copyright is held by the owner/author(s ) . Publication rights licensed to ACM . ACM 978 1 4503 3664 2/15/08?$1500 DOI : http://dxdoiorg/101145/27832582783287 heterogeneous data clustering and is attracting increasing academic attention . It aims to find a single partition that most agrees with multiple existing basic partitions [ 22 ] . Consensus clustering is of recognized benefit in generating robust partitions , finding bizarre clusters , handling noise and outliers , and integrating solutions from multiple sources [ 21 ] .
Consensus clustering can be roughly divided into two categories : those with implicit or explicit objectives . Methods that utilize implicit objectives do not set objective functions , but instead directly adopt some heuristics to find approximate solutions . Representative methods include coassociation matrix based methods [ 27 , 26 , 10 , 14 ] , graphbased algorithms [ 22 , 8 ] , relabeling and voting methods [ 1 , 9 , 19 ] , locally adaptive cluster based methods [ 7 ] , and genetic algorithm based methods [ 32 ] . Methods with explicit objectives employ explicit objective functions for consensus clustering . For instance , [ 23 ] used K means to find the solution based on quadratic entropy , which was then generalized in [ 28 ] as the paradigm of K means based consensus clustering . Other solutions for different objective functions include non negative matrix factorization [ 12 ] , EM algorithm [ 24 ] , simulated annealing [ 15 ] , combination regularization [ 30 ] , and hill climbing method [ 5 ] . Many other algorithms for consensus clustering can be found in the survey [ 13 , 25 , 20 ] . Of these consensus clustering methods , the co association matrix based method is a landmark . First , the information represented by basic partitions is summarized into a co association matrix , which measures how many times a pair of instances appears in the same cluster ; then a graph partition method can be used to obtain the final consensus clustering . The main contribution of the co association method is the redefinition of the consensus clustering problem as a classical graph partition problem , so that agglomerative hierarchical clustering , spectral clustering , or other algorithms can directly run on the co association matrix without much modification . However , the co association matrixbased method also suffers from some limitations . For instance , the high time and space complexity prevents handling large scale data clustering and no explicit objective function is used to supervise clustering .
In light of this , we propose Spectral Ensemble Clustering ( SEC ) , which employs spectral clustering on the coassociation matrix to find the consensus partition . SEC equivalently results in weighted K means clustering , which decreases the time complexity from O(n3 ) to roughly O(n ) and decreases the space complexity from O(n2 ) to roughly O(n ) as well . We then derive the intrinsic consensus ob
715 jective of SEC and provide a robustness and generalization analysis . Further we extend SEC to handle incomplete basic partitions . Experimental results on various real world data sets demonstrate that SEC delivers efficient and high quality clustering compared to some state of the art consensus clustering methods . SEC is also highly robust to incomplete basic partitions with many missing values . Finally , SEC is used to explore big data clustering of Weibo data . 2 . SPECTRAL ENSEMBLE CLUSTERING Let X ∈ Rn×d denote the data matrix with n instances and d dimensions , and given r basic crisp partitions of X ( a basic partition is a partition of X given by running some clustering algorithm ) in Π = {π1 , π2,··· , πr} , a co association matrix [ 10 ] S is built as follows : fi r .
S(x , y ) =
δ(πi(x ) , πi(y) ) , δ(a , b ) = i=1
1 , 0 , if a = b if a fi= b
.
Similarity between a pair of instances simply counts the cooccurrence number in the same cluster in Π . Spectral Ensemble Clustering ( SEC ) applies spectral clustering on the co association matrix S to obtain final clustering π . Let H = [ H1,··· , HK ] be a n×K partition matrix , where K is the user specific cluster number , and D is the diagonal matrix whose diagonal entry is the sum of rows of S . The objective function of normalized cuts spectral clustering is the following trace maximization problem [ 33 ] : fi fi
−1/2SD fi
D tr(Z −1/2H(H
−1/2Z ) , st Z ( 1 ) −1/2 . A well known solution −1/2
Z = I ,
DH ) where Z = D −1/2SD to Eq 1 is to obtain the top k eigenvectors of D and run K means to get the final consensus clustering π . 2.1 From SEC to Weighted K means max
1 K
Performing spectral clustering on co association matrix also has high time complexity . We here propose a low timecost algorithm for this purpose ; that is , we transform SEC into weighted K means clustering via a binary matrix . Let B = {b(x)} be a binary data set derived from the set b(x ) = 'b(x)1,··· , b(x)rff , b(x)i = 'b(x)i1,··· , b(x)iKi of r basic partitionings Π as follows : ff , fi if πi(x ) = j
1 , 0 , otherwise b(x)ij = n ×' r where “ ' ff ” indicates a transverse vector . Therefore , B is an i=1 Ki binary data matrix with |b(x)i| = 1 , Ki is the number of clusters of πi , ∀ l , i . After introducing the binary matrix , we obtain the theorem to connect SEC and classical weighted K means clustering . All the proofs are given in the appendix for concision .
Table 1 : Contingency Matrix
π
C1 C2 · CK .
C(i ) 1 n(i ) 11 n(i ) · 21 n(i ) K1 n(i ) +1
πi
· · · · · · · · · · · · · · · · · ·
C(i ) 2 n(i ) 12 n(i ) · 22 n(i ) K2 n(i ) +2
C(i ) Ki n(i ) 1Ki n(i ) · 2Ki n(i ) KKi n(i )
+Ki
. n1+ n2+ · nK+ n
Weighted K means is conducted on an n×'
Remark 1 . Theorem 1 transforms SEC into weighted Kmeans clustering in an explicit theoretically equivalent way . r i=1 Ki data matrix . Recall that there is only one non zero element in b(x)i , the time complexity isO ( InrK ) , where I is the number of iterations . Thus , the time complexity decreases from O(n3 ) to roughly O(n ) , and the space complexity decreases from O(n2 ) to roughly O(n ) as well .
Remark 2 . Unlike [ 6 ] which built the connection between spectral clustering and weighted kernel K means , here we equivalently have figured out the mapping function of the kernel , which turns out to be the binary data dividing its corresponding weight . By this means , we transfer SEC to weighted K means rather than weighted kernel K means .
2.2
Intrinsic Consensus Objective Function
'
To understand the objective function of SEC in the partition level , here we derive the intrinsic consensus objective function of SEC from weighted K means . In Table 1 , given two partitions π and πi containing K and Ki clusters , respectively , let n(i ) kj denote the number of data objects belonging to both cluster C ( i ) in πi and cluster Ck in π , nk+ = kj , 1 ≤ j ≤ Ki , 1 ≤ k ≤ K . k=1 n(i ) K Let p(i ) +j/n . We then have a normalized contingency matrix ( NCM ) , based on which a wide range of utility functions can be accordingly defined . For instance , the well known category utility function [ 17 ] can be computed as follows : kj /n , pk+ = nk+/n , and p(i ) kj , and n(i )
+j = n(i ) kj = n(i ) j +j = j=1 n(i ) Ki
'
K .
Ki .
)2 − Ki . j=1 p(i ) kj pk+
(
Uc(π , πi ) = pk+ k=1 j=1
( p(i )
+j)2 .
( 3 )
,
Based on NCM , we have the following theorem .
Theorem 2 . If a utility function has the same or equiv alent formation as follows ,
K . k=1
U ( π , πi ) =
' nk+ wCk pk+ p(i ) kj pk+
(
)2 ,
( 4 )
Ki . j=1 r .
Theorem 1 . Given a set of basic partitions Π , the coassociation matrix based method with spectral clustering has the equivalent objective function to classical weighted K means clustering such that where wCk = b(x)∈Ck wb(x ) , then it satisfies max
1 K tr(ZT D
−1/2SD
−1/2Z ) ⇔ max
U ( π , πi ) .
( 5 ) i=1 max
1 K fi
−1/2SD
D tr(Z where fm1,,mK ( x ) = mink wb(x)( b(x )
D , and mk =
.
. x∈Ck x∈Ck b(x ) wb(x )
. i=1 wb(x )
( 2 ) fm1,,mK ( xi ) , −mk(2 , diag(wb(x ) ) =
Remark 3 . The same or equivalent formulation of U in Eq 4 can be used as a utility function to supervise the consensus process . It is worth noting that the category utility function is a special case of the SEC utility function U with all weights one . Recall that the co association matrix measures similarity in instance level ; by transforming SEC into
−1/2Z ) ⇔ n .
716 classical weighted K means , we derive the utility function to measure the similarity in partition level , ie , the two kinds of similarity in different levels can be interconvertible . 3 . ROBUSTNESS AND GENERALIZATION 3.1 Robustness
Robustness is a fundamental property for learning algorithms , which measures the tolerance of learning algorithms to perturbations ( noise ) . If an instance is close to a training instance , a good learning algorithm should make their errors close . This property of algorithms is formulated as robustness by the following definition . ( Robustness [ 31] ) . Let X n be the training sample . An algorithm is ( K , ( · ) ) robust , for K ∈ N and ( · ) : X n → R , if X can be partitioned into K disjoint sets , denoted by {Ci}K i=1 , such that the following holds for all s ∈ X n,∀xs ∈ s,∀x ∈ X ,∀i = 1 , , K : if xs , x ∈ Ci , then |ff(hs , xs ) − ff(hs , x)| ≤ ( s ) .
Definition 1
'
We then have Theorem 3 to measure SEC ’s robustness as Theorem 3 . Let N ( γ,X ,( · ( 2 ) be a covering number of ff(2 ≤ γ , we define ( b(x)i − ff ∈ X ,(x − x | ≤ γi )i(2 ≤ γi and |wi − wi ff b(xfi ) w , i = 1 , . . . , r , where n ' l=1 δ(πi(x ) , πi(xl) ) . Then , for any centroids m1 , b(x ) = follows . X . For any x , x b(x wi . . . , mK learned by SEC , we have )| ≤ 2
|fm1,,mK ( x ) − fm1,,mK ( x ff' b(x )
+ w
. ff r i=1 γi r
. r i=1 γ2 i r
( 6 ) r
.
+ r i=1 γ2 i r i=1 γi w r
Hence , SEC is ( N ( γ,X ,(·(2 ) , 2
) robust . Remark 4 . From Theorem 3 , we can see that even if some instances are “ poorly ” clustered by some basic partitions and cause the corresponding margins {γi} and {γi w} to be large , the good overall performance of SEC will be preserved , provided that these instances are “ well ” clustered by many other basic partitions . This means that SEC could benefit from the ensemble of basic partitions , namely that instances could be “ well ” clustered by the majority of the basic partitions . 3.2 Generalizability
The generalizability of SEC is highly dependent on the basic partitions . A small generalization error guarantees a small gap between the expected reconstruction error of the learned partition and that of the target one . In what follows , we prove that the generalization bound of SEC can converge quickly with the use of basic partitions and SEC can therefore achieve accurate clustering with a relatively small number of instances .
Theorem 4 . Let π be any partition learned by SEC . For any independently distributed instances x1 , . . . , xn and δ > 0 , with probability at least 1 − δ , the following holds Exfm1,,mK ( x ) − 1 n . n i=1 fm1,,mK ( xi ) √
−2 b(xi ) ) w
1 2 +
8πrK n minx∈{x1,,xn} wb(x )
√ n . 2πrK ( √ i=1 n
≤
+
√ n .
( i=1
Remark 5 . Theorem 4 shows that if the third term of the upper bound goes to zero as n goes to infinity , the empirical reconstruction error of SEC will go to its expected reconstruction error . So , the convergence of
√ 2πrK n n .
( i=1 w2 b(xi ) )
1 2
1 minx∈{x1,,xn} w2 b(x ) is a sufficient condition for the convergence of SEC . This sufficient condition is easily achieved by the consistency property of the basic partitions . k k
| diverge little , where |C i
Remark 6 . The consistency of the basic crisp partitions will make wb(xi)/|C i | denotes the cardinality of the cluster in which xi belongs . If we further assume that |Ck| = akn , where ak ∈ ( 0 , 1 ) , the convergence of SEC can be as fast as O(1/ n3 ) . However , for classical K means clustering , the fastest known convergence rate is O(1/ n ) [ 4 , 3 ] . The fast convergence rate will result in the expected risk of the learned partitioning decreasing quickly to the expected risk of the target partitioning [ 4 ] . This verifies the efficiency of SEC .
√
√
4 .
INCOMPLETE EVIDENCE r i=1
In practice , Incomplete Basic Partitions ( IBP ) are often obtained due to distributed systems or missing data . An incomplete basic partition πi is obtained by clustering a data subset Xi ⊆ X , 1 ≤ i ≤ r , with the constraint that Xi = X . Here , the problem is how to cluster X into K crisp clusters using SEC given r IBPs in Π = {π1,··· , πr} . The co association matrix built using incomplete basic partitions cannot continue to represent the similarity of points . To address this limitation , we start with the objective of weighted K means clustering and exploit it for handling incomplete basic partitions . Obviously , missing elements in basic partitions provide no utility in the ensemble process . Consequently , these missing elements do not contribute to the centroid ; thus , let |Xi| = n(i ) , we have
Theorem 5 . Given r incomplete basic partitions , we have
Ki . n . r .
K . i=1 p(i ) fm1,,mK ( xi ) ⇔ max n(i ) k+ w(i ) Ck ' where fm1,,mK ( x ) = mink wb(x)( b(x ) − mk(2 , with mk = ' 'mk,1,··· , mk,rff , mk,i = wb(x ) , p(i ) = n(i)/n , n(i ) x∈Ck∩Xi k+ = |Ck ∩ Xi| , w(i ) x∈Ck∩Xi
' wb(x)i .
( j=1 p(i ) kj pk+
)2 , ( 8 ) b(x)i/ wb(x ) pk+ x∈Ck∩Xi k=1 i=1
Ck
=
Remark 7 . In Theorem 5 , the utility function of SEC on IBPs has one more parameter p(i ) than that on complete basic partitions , indicating that basic partitions with more elements are naturally assigned more importance in the ensemble process . However , the co association matrix only reflects the summation information of basic partitions . On the surface , it seems that each point plays an equal role ; in fact , by Theorem 5 we find that these points contribute differently depending on the missing rate of their basic partitions .
2πrK n minx∈{x1,,xn} w2 b(x ) w2 b(xi ) )
1 2 + ( ln(1/δ )
2n
1 2 .
)
Theorem 6 . For the objective function in Theorem 5 , SEC with IBPs is guaranteed to converge in finite two phase iterations of weighted K means clustering .
( 7 )
For the convergence of the SEC with IBPs , we have :
717 Table 2 : Experimental Data Sets
Data set breast w iris wine cacmcisi classic cranmed hitech k1b la12 mm re1 reviews sports tr11 tr12 tr41 tr45 letter mnist
Source
#Instances #Features #Classes
UCI UCI UCI
CLUTO CLUTO CLUTO CLUTO CLUTO CLUTO CLUTO CLUTO CLUTO CLUTO CLUTO CLUTO CLUTO CLUTO LIBSVM LIBSVM
699 150 178 4663 7094 2431 2301 2340 6279 2521 1657 4069 8580 414 313 878 690
20000 70000
9 4 13
14409 41681 41681 126321 21839 31472 126373
3758
126373 126373
6429 5804 7454 8261
16 784
2 3 3 2 4 2 6 6 6 2 25 5 7 9 8 10 10 26 10
5 . BIG DATA CLUSTERING USING SEC
Consensus clustering seems not a good first choice for big data clustering ; because r basic partitions are first produced , then ensemble clustering is used to obtain the final clustering and this requires high time and space cost . In terms of large scale data , it is difficult to obtain basic partitions and perform the ensemble process . However , SEC with incomplete basic partitions enables big data processing .
In terms of large scale data clustering , we propose a type of row segmentation strategy . Generally speaking , we randomly select a certain percentage of data instances to obtain a data subset and run K means to obtain the label from 1 to K ; those unsampled data are labeled “ 0 ” . This selection process is repeated r times to obtain the IBPs , prior to using the SEC algorithm to complete the clustering . The benefit of the row segmentation strategy is two fold : first , a big data set is decomposed into several smaller ones , which can be handled separately and independently ; second the high dimensionality of the data is decreased to only r dimensions . The experimental results in the next section demonstrate that the row segmentation strategy even outperforms directly clustering on the whole data .
6 . EXPERIMENTAL RESULTS
In this section , we evaluate the performance of SEC on various real world data sets from different domains and compare it with state of the art consensus clustering algorithms . 6.1 Experimental Setup
Data . Various real world data sets with true cluster labels are used for experiments . Table 2 summarizes their important characteristics . Three data sets are used : the UCI Machine Learning Repository1 , CLUTO2 and LIBSVM3 .
Tool . SEC is completely coded in Matlab . The kmeans function with square Euclidean distance ( UCI and LIBSVM data sets ) and cosine similarity ( CLUTO data sets ) are run 100 times to obtain r = 100 basic partitions by varying the cluster number from the true cluster number K to n . The comparative methods included consensus clustering with category utility function ( CCC ) [ 28 ] , graph based consensus clustering ( GCC ) [ 22 ] , and co association matrix with ag
√
1https://archiveicsuciedu/ml/datasetshtml 2http://glarosdtcumnedu/gkhome/cluto/cluto/download 3http://wwwcsientuedutw/∼cjlin/libsvmtools/datasets/ glomerative hierarchical clustering ( HCC ) [ 10 ] . Note that the cluster number for each algorithm is set to the true cluster number K and that each algorithm runs 10 times and returns the average result .
Validity . Since true cluster labels are available for all data sets , we employed external measures to assess cluster validity . Although accuracy is a measure used in classification , here it might not be suitable due to the unknown mapping relationship between the true cluster labels and clustering result . Thus , we choose the widely used Rn [ 29 ] for cluster validity . Rn is a positive measure and a larger value indicates a better performance . 6.2 Effectiveness and Efficiency of SEC
Here , we illustrate the performance of SEC with some well established methods for consensus clustering . The clustering results of these methods are shown in Table 3 , with the best results highlighted in bold and the last column showing the baseline K means results .
For most data sets , consensus clustering algorithms enjoy superior performance to single K means clustering . Among consensus clustering algorithms , SEC is a powerful competitor and achieves the best performance 14 times , and the second best performance 4 times of 19 data sets . GP produces the best results of three graph based consensus clustering algorithms ( CSPA , HGPA , and MCLA ) and performs particularly well on some balanced datasets , such as iris and cranmed . Although HCC obtains satisfactory results on k1b and sports , the performance on cacmcisi and mm is extremely poor , indicating that HCC , which has no utility function to supervise the combining process , is less robust than the other algorithms . CCC has similar utility function to SEC ; however , by enforcing the weights of the instances in large clusters , SEC outperforms CCC in most cases . Note that the negative results , such as the cacmcisi via CCC and HCC , indicate that the clustering results are worse than random labeling .
Table 4 shows the average time of ten runs via these methods . In terms of efficiency , K means based consensus clustering has obvious advantages over other methods . SEC and CCC are tied , and HCC might be suitable for small datasets but struggles as the number of instances increases . Note that these experiments are all run on the same computer . 6.3 Stability and Scalability of SEC
√
To further validate the performance on large scale data , in this part we test the stability and scalability of SEC on letter and mnist with 20,000 and 70,000 instances , respectively . The basic partitioning generation strategy varying the cluster number from K to n seems not suitable for large scale data due to the large n ; thus we change the variation range of cluster number from 2 to 2K to generate 100 basic partitions . To test the scalability of SEC , we have the assumption that if an algorithm can scale down and work well on the sample data , the algorithm can also scale up and enjoy a good scalability . Here we conduct stratified sample according to the true cluster labels with the ratio from 20 % to 100 % with the 20 % step and repeat 50 times on each sampling ratio . Note that each run only calls one time SEC to see the stability of SEC as well .
From Figure 1 , the performance of SEC on each sampling ratio keeps still even on only 20 % instances . This indicates that SEC has a good scalability and suits for big data clus
718 Table 3 : Clustering Results ( by Rn )
Table 4 : Run Time ( by second )
SEC classic cranmed hitech cacmcisi iris wine
Data set breast w 0.8230 0.9222 0.3272 0.6388 0.7069 0.9512 0.3047 0.6236 0.5311 0.6184 0.2851 0.5036 0.4652 0.5926 0.4701 0.4578 0.4688 0.1202 0.4027 tr11 tr12 tr41 tr45 letter mnist k1b la12 mm re1 reviews sports
CCC 0.0556 0.7352 0.1448 0.0447 0.4224 0.9560 0.2227 0.5288 0.3455 0.5450 0.2630 0.3767 0.3211 0.5217 0.4219 0.3839 0.3947 0.1225 0.3795
GCC 0.6115 0.9222 0.1448 0.3177 0.3826 0.7482 0.1604 0.2677 0.3571 0.3753 0.2299 0.3942 0.2911 0.3755 0.4673 0.3649 0.3805 0.1350 0.3678
HCC 0.7809 0.7323 0.1490 0.0298 0.3828 0.9352 0.2676 0.6351 0.3647 0.0006 0.2788 0.4589 0.4840 0.5896 0.4485 0.4280 0.4554 0.1179 0.4482
K means
0.8391 0.6998 0.1283 0.0310 0.2928 0.9496 0.2530 0.4469 0.2151 0.7724 0.2382 0.6011 0.3345 0.4050 0.4073 0.3568 0.4096 0.1185 0.3912
Data set breast w iris wine cacmcisi classic cranmed hitech k1b la12 mm re1 reviews sports tr11 tr12 tr41 tr45 letter mnist
SEC 0.10 0.04 0.05 0.68 1.87 0.33 0.70 0.48 0.43 0.16 0.64 0.27 0.58 0.12 0.09 0.19 0.16 8.53 12.95
CCC 0.14 0.02 0.03 1.20 2.23 0.37 0.74 0.56 0.45 0.15 0.92 0.30 0.71 0.11 0.09 0.21 0.15 9.07 13.29
GCC 0.53 3.87 3.01 54.87 96.15 22.24 27.76 27.99 72.49 16.58 31.29 42.68 108.85 7.89 6.89 14.92 11.39 348.32 112.33
HCC 3.32 0.10 0.12 632.14 1908.10 141.91 144.44 130.28 1329.20 141.08 85.60 500.13 1697.00 2.74 0.99 20.17 10.01 1847.39 19995.68
0.2
0.15 n
R y b y t i l a u Q
0.1
0.05
0
1
20 %
2
40 %
3
60 %
4
80 %
5
100 % n
R y b y t i l a u Q
0.5
0.4
0.3
0.2
0.1
0
1
20 %
2
40 %
3
60 %
4
80 %
5
100 %
( a ) letter
( b ) mnist
Figure 1 : Stability and Scalability of SEC with Different Sampling Ratios
0.9
SEC K−means n
R y b y t i l a u Q
0.85
0.8
0.75
80 %
0.8 n
R y b y t i l a u Q
0.75
0.7
0.65
0.6
0.55
SEC K−means
60 %
40 % Incompleteness Ratio
20 %
60 % Incompleteness Ratio
40 %
20 %
0.5
80 %
( a ) mm
( b ) reviews
Figure 2 : Performance of SEC with Different Incompleteness Ratios tering . Beside we can also see that on letter the volatility even shrinks when then sampling ratio makes larger , which demonstrates that SEC enjoys good stability as well . 6.4 Performances of SEC with Incompleteness Next we demonstrate the performance of SEC with IBPs . The row segmentation strategy is used to generate IBPs . The instances are first randomly sampled from 20 % to 80 % , prior to calling kmeans on the data subset to generate the basic partitions . Unsampled instances are assigned the “ 0 ” label . The above process repeats 100 times to obtain 100 IBPs . Then SEC is used to ensemble these incomplete basic partitions and obtain the consensus clustering result .
The results of mm and reviews are shown in Figure 2 . SEC obtains a stably high performance on mm , even at low percentage ( 20% ) , and in reviews with increasing percent random selection the performance of SEC also improves . The black line represents the K means result with all instances , and it can be seen that SEC ( with different random selection percentages ) outperforms K means . This indicates SEC appears to be highly robust to incomplete data and provides a way to handle big data clustering with incomplete evidence via the row segmentation strategy .
Table 5 : Representative Keywords of Weio Clusters
Keywords No . term begins , campus , partner , teacher , school , dormitory Clu.3 Clu.21 Mid Autumn Festival , September , family , happy , parents Clu.40 China , powerful , history , victory , Japan , shock , harm Clu.65 Meng Ge , mother , apologize , son , harm , regret , anger Clu.83 travel , happy , dream , life , share , picture , plan , haha
6.5 SEC Applied to Weibo Data Clustering
Sina Weibo4 , a Twitter like service launched in China in 2009 , has accumulated more than 500 million users in less than five years . The Weibo platform produces very large volumes of user generated content ; for example , on September 1st 2013 , there were 97,231,274 tweets published on the Weibo platform , which provides a valuable data source for commercial applications and academic research . However , conducting cluster analysis on such big data is a difficult data mining task .
Here we employ SEC to cluster the entire Weibo data published on September 1st , 2013 to discover hot events . First , we remove over 30 million advertising tweets and use SCWS5 to conduct word segmentation ; this produces a dataset with 61,212,950 instances and 10,000 features . Next , the rowsegmentation strategy is carried out to acquire 100 data subsets with 10,000,000 instances , and 100 IBPs are performed on these data subsets using CLUTO ( note that the maximum capacity for CLUTO is 10,000,000 instances ) . Finally , SEC fuses the partial information into integrated clustering via parallel computing . The cluster number was set to 100 for both basic partitions and final consensus clustering .
To handle such big data , we use a distributed computing cluster with 10 servers to conduct consensus clustering in parallel , and each iteration takes approximately 3 hours . The results of the representative keywords of some clusters are shown in Table 4 . Cluster 3 , 21 , and 83 represent “ term begins ” , “ mid autumn festival ” , and “ travel ” events , respectively ; Cluster 40 identifies the conflict between China and Japan due to “ the September 18th incident ” ; and Cluster 65 represents the event that Meng Ge , a famous singer in China , apologized for her son ’s crime . Although the basic partitions are highly incomplete , some interesting events can be found using the row segmentation strategy . SEC appears to be a good choice for clustering big data .
4http://wwwweibocom/ 5http://wwwxunsearchcom/scws/
719 7 . CONCLUSIONS
In this paper , we propose SEC and uncover an equivalent relationship with weighted K means clustering that dramatically decreases the time and space complexity . Based on weighted K means , the intrinsic consensus objective function of SEC is derived , then we investigate its robustness and generalizability , and extend it to incomplete basic partitionings . Experimental results demonstrate that SEC produces high quality , efficient clustering compared with other state of the art methods , which is further illustrated in an application of big data clustering of Weibo data .
8 . ACKNOWLEDGEMENTS
This work was partially supported by National Natural Science Foundation of China ( 71322104 , 71171007 , 71471009 , 71490720 ) , National Center for International Joint Research on E Business Information Processing ( 2013B01035 ) , National High Technology Research and Development Program of China ( SS2014AA012303 ) . Dr . Dacheng Tao ’s work was supported by Australian Research Council Projects ( DP120103730 , DP 140102164 , FT 130101457 ) . Dr . Yun Fu ’s work was supported by NSF CNS award ( 1314484 ) . We thank KDD anonymous reviewers and SPC for their constructive comments , which help to improve this work to a new level in the final revision .
9 . REFERENCES [ 1 ] H . Ayad and M . Kamel . Cumulative voting consensus method for partitions with variable number of clusters . PAMI , 30(1):160–173 , 2008 .
[ 2 ] A . Banerjee , S . Merugu , I . S . Dhillon , and J . Ghosh .
Clustering with bregman divergences . JMLR , 2005 .
[ 3 ] P . Bartlett , T . L . T , and G . Lugosi . The minimax distortion redundancy in empirical quantizer design . IEEE Transactions on Information Theory , 1998 .
[ 4 ] G . Biau , L . Devroye , and G . Lugosi . On the performance of clustering in Hilbert spaces . IEEE Transactions on Information Theory , 2008 .
[ 5 ] C . Carpineto and G . Romano . Consensus clustering based on a new probabilistic rand index with application to subtopic retrieval . PAMI , 2012 .
[ 6 ] I . Dhillon , Y . Guan , and B . Kulis . Kernel k means :
Spectral clustering and normalized cuts . In Proceedings of KDD , 2004 .
[ 7 ] C . Domeniconi and M . Al Razgan . Weighted cluster ensembles : Methods and analysis . TKDD , 2009 .
[ 8 ] X . Z . Fern and C . E . Brodley . Solving cluster ensemble problems by bipartite graph partitioning . In Proceedings of ICML , 2004 .
[ 9 ] R . Fischer and J . Buhmann . Path based clustering for grouping of smooth curves and texture segmentation . PAMI , 2003 .
[ 10 ] A . Fred and A . Jain . Combining multiple clusterings using evidence accumulation . IEEE Transactions on Pattern Analysis and Machine Intelligence , 2005 .
[ 11 ] M . Ledoux and M . Talagrand . Probability in Banach
Spaces : isoperimetry and processes . Springer , 1991 .
[ 12 ] T . Li , D . Chris , and I . Michael . Solving consensus and semi supervised clustering problems using nonnegative matrix factorization . Proceedings of ICDM , 2007 .
[ 13 ] T . Li , M . Ogihara , and S . Ma . On combining multiple clusterings . In Proceedings of CIKM , 2004 .
[ 14 ] A . Lourenco , S . Bul`o , N . Rebagliati , A . Fred ,
M . Figueiredo , and M . Pelillo . Probabilistic consensus clustering using evidence accumulation . Machine Learning , 98(1 2):331–357 , 2013 .
[ 15 ] Z . Lu , Y . Peng , and J . Xiao . From comparing clusterings to combining clusterings . In Proceedings of AAAI . AAAI Press , 2008 .
[ 16 ] S . Mendelson . A few notes on statistical learning theory . Advanced Lectures on Machine Learning , 2003 .
[ 17 ] B . Mirkin . Reinterpreting the category utility function . Machine Learning , 2001 .
[ 18 ] M . Mohri , A . Rostamizadeh , and A . Talwalkar .
Foundations of machine learning . MIT Press , 2012 .
[ 19 ] S . Monti , P . Tamayo , J . Mesirov , and T . Golub .
Consensus clustering : a resampling based method for class discovery and visualization of gene expression microarray data . Machine learning , 2003 .
[ 20 ] M . Naldi , A . Carvalho , and R . Campello . Cluster ensemble selection based on relative validity indexes . Data Mining and Knowledge Discovery , 27(2 ) , 2013 .
[ 21 ] N . Nguyen and R . Caruana . Consensus clusterings . In
Proceedings of ICDM , 2007 .
[ 22 ] A . Strehl and J . Ghosh . Cluster ensembles — a knowledge reuse framework for combining partitions . Journal of Machine Learning Research , 2002 .
[ 23 ] A . Topchy , A . Jain , and W . Punch . Combining multiple weak clusterings . In Proceedings of ICDM , 2003 .
[ 24 ] A . Topchy , A . Jain , and W . Punch . A mixture model for clustering ensembles . In Proceedings of SDM , 2004 .
[ 25 ] S . Vega Pons and J . Ruiz Shulcloper . A survey of clustering ensemble algorithms . International Journal of Pattern Recognition and Artificial Intelligence , 2011 .
[ 26 ] F . Wang , X . Wang , and T . Li . Generalized cluster aggregation . In Proceedings of IJCAI , 2009 . [ 27 ] X . Wang , C . Yang , and J . Zhou . Clustering aggregation by probability accumulation . Pattern Recognition , 2009 .
[ 28 ] J . Wu , H . Liu , H . Xiong , and J . Cao . A theoretic framework of k means based consensus clustering . In Proceedings of IJCAI , 2013 .
[ 29 ] J . Wu , H . Xiong , and J . Chen . Adapting the right measures for k means clustering . In Proceedings of KDD , 2009 .
[ 30 ] S . Xie , J . Gao , W . Fan , D . Turaga , and P . Yu .
Class distribution regularized consensus maximization for alleviating overfitting in model combination . In Proceedings of KDD , 2014 .
[ 31 ] H . Xu and S . Mannor . Robustness and generalization .
Machine learning , 2012 .
[ 32 ] H . Yoon , S . Ahn , S . Lee , S . Cho , and J . Kim . Heterogeneous clustering ensemble method for combining different cluster results . Data Mining for Biomedical Applications , 2006 .
[ 33 ] S . Yu and J . Shi . Multiclass spectral clustering . In
Proceedings of ICCV , 2003 .
720 APPENDIX For the concision of math description , we let ZK denote {1 , . . . , K} and let Xn denote {x1 , . . . , xn} in the appendix .
A . PROOF OF THEOREM 1 Proof . Let Y = {y = b(x)/wb(x)} and Wk denote the diagonal matrix of the weights in cluster Ck , and Yk denote the matrix of binary data associated with cluster Ck . Then the centroid mk can be rewrote as mk = e WkYk/sk , where e is the vector of all ones with appropriate size and sk = e
Wke . According to [ 6 ] , we have fi fi
SSECk =
. x∈Ck wb(x)|| b(x ) wb(x )
− mk||2
= ||(I − W1/2 k ee sk
= tr(Y fi k W1/2
= tr(Y fi k W1/2
= tr(W1/2 k YkY fi
W1/2 k
)W1/2 k Yk||2 fi W1/2
F k k ( I − W1/2 k ee sk fi k ( I − W1/2 k ee sk fi k ) − e Wk√ sk fi k W1/2 k
W1/2
)2W1/2 k Yk )
)W1/2 k Yk ) Wke√ sk fi YkY k
If we sum up SSE of all the clusters , we have
'
'
According to the definition of centroids in K means , we kj /wCk = kj /pk+)(nk+/wCk ) . Note that ( γ ) have mk,ij = ( n(i ) is a constant , we get the utility function of SEC . . b(x)ij/ kj /nk+)(nk+/wCk ) = ( p(i ) wb(x ) = n(i ) x∈Ck x∈Ck
C . PROOF OF THEOREM 3
We first give a lemma as follows .
Lemma 1 . fm1,,mK ( x ) ∈ [ 0 , 1 ] .
Proof . It is easy to show ( b(x)(2 = r , wb(x ) ∈ [ r , ( n−K +1)r ] and fm1,,mK ( x ) ≤ max{ ( b(x)(2
, wb(x)(mk(2} . We have wb(x )
( b(x)(2 wb(x )
≤ r r
= 1 ,
. wb(x)(mk(2 = wb(x)(' '
( bl∈Ck bl∈Ck bl(2 wbl )2
≤ 1 .
( 9 )
K . k=1
. x∈Ck
= tr(W
− mk||2 fi
1 2 YY wb(x)|| b(x ) wb(x ) 2 ) − tr(G ,··· , fi −1 and S = BB
W
1
W1/2 k e√ sk where G = diag( W1/2 1 e√ −1BB W W I , so we have fi s1 fi max tr(Z
− 1
2 SD
D fi
W
1 2 YY fi
W
1 2 G ) ,
) . Recall that YY fi fi
, D = W and Z
Z = G
G = fi
=
2 Z ) ⇔ max tr(G − 1 fi − 1 fi
W
2 BB
2 G ) . − 1 2 ) finishes the proof . .
W fi
− 1
− 1
The constant tr(W
2 BB
W
B . PROOF OF THEOREM 2 Proof . Given the equivalence of SEC and weighted K means , we here derive the utility function of SEC . We start from the objective function of weighted K means as follows :
[
= k=1 k=1 x∈Ck x∈Ck
. K . wb(x)|| b(x ) . K . wb(x ) ||b(x)||2 . K . wb(x ) ||b(x)||2 . K . wb(x ) ||b(x)||2 K . . wb(x ) ||b(x)||2 ff ) ( wb(x ) x∈Ck x∈Ck x∈Ck k=1 k=1 k=1
=
=
=
[
( γ ) wb(x)||mk||2 ] . wb(x)||mk||2 ] x∈Ck b(x)m x∈Ck
− 2 fi k +
.
− mk||2 . . x∈Ck wb(x)||mk||2 + K . − r . r . K .
||mk,i||2 Ki .
− 2 x∈Ck wCk k=1 i=1
−n nk+ wCk pk+
( j=1 p(i ) kj pk+
)2 . i=1 k=1
A detailed proof of equation ( 9 ) : If |Ck| = 1 , the equation
This concludes the proof . holds trivially . When |Ck| ≥ 2 , we have ' ' ≤ wb(x ) ( wb(x)(' ' bl∈Ck
( bl∈Ck bl∈Ck
( bl(2 wbl )2 bl∈Ck
( wb(x ) + bl∈Ck−{b(x)} wbl )2 wb(x ) bl(2 ' wbl )2 '
' ' wb(x)|Ck|r wb(x ) bl∈Ck r bl∈Ck r
( wb(x ) + bl∈Ck−{b(x)} r)2
( wb(x ) + ( |Ck| −1)r )2 ( wb(x))2 + 2wb(x)(|Ck| −1)r wb(x ) + 2(|Ck| −1)r |Ck|r + |Ck|r − r wb(x)|Ck|r |Ck|r |Ck|r
≤ 1 .
=
≤
=
≤
≤
≤
The first inequality holds due to the triangle inequality . .
Now we begins the proof of Theorem 3 .
Proof . We have ff
)|
|fm1,,mK ( x ) − fm1,,mK ( x = | min k∈ZK ≤ max k∈ZK wb(x)( b(x ) wb(x ) |wb(x)( b(x ) wb(x )
− mk( − min k∈ZK − mk( − wb(xfi)( b(x ) wb(xfi ) wb(xfi)( b(x ) wb(xfi ) − mk(| ff ff
− mk(|
721 |
= max k∈ZK r fi wb(x ) ff b(x r
− 'b(x ) , mkff + wb(x)(mk(2 − r wb(xfi ) fl − wb(xfi)(mk(2| | + |fi b(x ) − b(x ff fl|
) , mk
≤ max k∈ZK
≤ max k∈ZK wb(xfi )
) , mk − r
+ ( | wb(x ) + ( mk(2|wb(x ) − wb(xfi)| ) ( | wb(x ) + ( mk(2|wb(x ) − wb(xfi)| ) .
− r wb(xfi ) r
| + ( b(x ) − b(x ff
)((mk(
Note that the last inequality holds due to the CauchySchwartz inequality . Recall that we have proved in Lemma 1 that ( mk(2 ≤
1 minx∈Xn wb(x )
, we have )| ff r wb(xfi )
− r
)((mk(
| + ( b(x ) − b(x ff
|fm1,,mK ( x ) − fm1,,mK ( x ≤ max k∈ZK
( | wb(x ) + ( mk(2|wb(x ) − wb(xfi)| ) minx∈Xn ( wb(x))2 + ( mk(2)|wb(x ) − wb(xfi)| ( + ( b(x ) − b(x ff ≤ r + minx∈Xn wb(x ) ' minx∈Xn ( wb(x))2 r . )((mk( minx∈Xn wb(x )
≤ max k∈ZK
γi w + (
'
' i=1 γ2 i=1 r
1 2
) r i
≤ 2 r i=1 γi r w
+ ( r i=1 γ2 r i
This completes the proof . .
D . PROOF OF THEOREM 4
The Glivenko Cantelli theorem [ 16 ] is often used , together with complexity measures , to analyze the non asymptotic uniform convergence of Enfm1,,mK ( x ) to Exfm1,,mK ( x ) , where Enf ( x ) denotes the empirical expectation of f ( x ) . A relatively small complexity of the function class FΠK = {fm1,,mK |π ∈ ΠK} , where ΠK denotes all possible Kmeans clustering for X is essential to prove a GlivenkoCantelli class . Rademacher complexity is one of the most frequently used complexity measures .
Rademacher complexity and Gaussian complexity are datadependent complexity measures . They are often used to derive dimensionality independent generalization error bounds and defined as follows :
Definition 2 . Let σ1 , . . . , σn and γ1 , . . . , γn be independent Rademacher variables and independent standard normal variables , respectively . Let x1 , . . . , xn be an independent distributed sample and F a function class . The empirical Rademacher complexity and empirical Gaussian complexity are defined as : and
Rn(F ) = Eσ sup f∈F
1 n
Gn(F ) = Eγ sup f∈F
1 n n . n . i=1
σif ( xi )
γif ( xi ) , i=1 respectively . The expected Rademacher complexity and Gaussian complexity are defined as :
R(F ) =E xRn(F )
1 2 .
)
Combining Theorem A 2 and Lemma 1 , we have and
G(F ) = ExGn(F ) .
Using the symmetric distribution property of random vari ables , we have :
Theorem A 1 . Let F be a real valued function class on
X and X = ( x1 , . . . , xn ) ∈ X n . Let
Φ(X ) = sup f∈F
1 n
( Exf ( x ) − f ( xi) ) . n . i=1
Then ,
ExΦ(X ) ≤ 2R(F ) .
The following theorem [ 18 ] , proved utilizing Theorem A 1 and McDiarmid ’s inequality , plays an important role in proving the generalization error bounds :
Theorem A 2 . Let F be an [ a , b] valued function class on X , and X = ( x1 , . . . , xn ) ∈ X n . For any f ∈ F and ff δ > 0 , with probability at least 1 − δ , we have Exf ( x ) − 1 n f ( xi ) ≤ 2R(F ) + ( b − a ) n . ln(1/δ )
2n
. i=1
Theorem A 3 . Let π be any partitioning learned by SEC . ff For any independently distributed instances x1 , . . . , xn and δ > 0 , with probability at least 1 − δ , the following holds fm1,,mK ( xi ) ≤ 2R(FΠK )+ Exfm1,,mK ( x)− 1 n . ln(1/δ )
.
2n n i=1
We use Lemmas 2 and 3 ( see proofs in [ 11 ] ) to upper bound R(FΠK ) by finding a proper Gaussian process which can easily be bounded .
Lemma 2 ( Slepian ’s Lemma ) . Let Ω and Ξ be mean zero , separable Gaussian processes indexed by a common set S , such that E(Ωs1
− Ξs2 )2,∀s1 , s2 ∈ S .
− Ωs2 )2 ≤ E(Ξs1
Then
E sup s∈S Ωs ≤ E sup s∈S Ξs .
The Gaussian complexity is related to the Rademacher complexity by the following lemma :
Lemma 3 .
R(F ) ≤ ffi π/2G(F ) .
Now , we can upper bound the Rademacher complexity
R(FW ) by finding a proper Gaussian process . ffi
Lemma 4 . R(FΠK ) ≤
π/2rK n n . n . i=1
( (
1
( wb(xi))2 )
1 2 +
√ n
2 minx∈Xn wb(x )
+ ( i=1
( wb(xi))2 )
1 2
1 minx∈Xn ( wb(x))2 ) .
722 .
Proof . Let MMM ∈ R i=1 Ki×K , whose k th column represents the k th centroid mk . Define the Gaussian processes indexed by MMM as r ffi
π/2 n
= Eγ ffi n . i=1
ΩMMM =
γi min k∈ZK wb(xi)( b(xi ) wb(xi )
− MMM ek(2 and n .
K . i=1 k=1
ΞMMM =
γikwb(xi)( b(xi ) wb(xi )
− MMM ek(2 ,
=
≤
− MMM ek(2 sup MMM i=1 i=1 k=1 k=1
γikwb(xi)(
K . n . γikwb(xi)( b(xi ) K . n . wb(xi ) ( b(xi)(2
( wb(xi))2 + ( MMM ek(2 ) ) n . K . K . n . K . n .
γik 'b(xi ) , MMM ekff wb(xi )
, MMM ek
γik k=1 k=1 i=1 i=1 r
γikwb(xi)(MMM ek(2 ) . ffl
π/2 n − 2 ffi
Eγ(sup MMM b(xi ) wb(xi )
π/2 n
( Eγ sup MMM
+ 2Eγ sup MMM
+ Eγ sup MMM i=1 k=1 where γi and γik are independent Gaussian random variables indexed by i and k . And ek are the natural bases indexed by k .
For any MMM and MMM
, we have ff n . E(ΩMMM − ΩMMMfi )2
= i=1
( min k∈ZK − min k∈ZK i=1 max k∈ZK
≤ n . − wb(xi)( b(xi ) ≤ n . K . wb(xi ) − wb(xi)( b(xi ) wb(xi ) = E(ΞMMM − ΞMMMfi )2 . k=1 i=1 wb(xi)( b(xi ) wb(xi ) wb(xi)( b(xi ) wb(xi ) ( wb(xi)( b(xi ) wb(xi ) − MMM ff
− MMM ek(2
− MMM ff ek(2)2
− MMM ek(2 ek(2)2
( wb(xi)( b(xi ) wb(xi ) − MMM
− MMM ek(2 ff ek(2)2
Note that the first and last inequalities hold because of the orthogaussian properties .
Using Slepian ’s Lemma and Lemma 3 , we have
2Eγ sup MMM n . i=1
1 n
σifm1,,mK ( xi ) wb(xi)( b(xi ) wb(xi )
− MMM ek(2
R(FΠK )
= Eσ
= Eσ sup MMM
1 n sup fm1,,mK n .
∈FΠK ffi ffi ffi i=1
σi min k∈ZK n .
π/2 n
π/2 n
π/2 n sup MMM i=1
γi min k∈ZK
ΩMMM sup MMM
ΞMMM sup MMM
≤ Eγ
= Eγ
≤ Eγ
We give upper bounds to the three terms respectively . r wb(xi )
γik wb(xi )
γik wb(xi )
)2
Eγ sup MMM
= Eγr
= Eγr
≤ r
γik i=1 i=1 k=1 k=1
K . n . n . K . ( n . K . n . K . n . k=1 k=1 i=1 i=1
1
1 w2 b(xi )
= rK
( wb(xi))2 . i=1
Note that the last inequality holds for the Jensen ’s inequality and the orthogaussian property of the Gaussian random variable . We therefore have i=1 k=1
K . n . γik 'b(xi ) , MMM ekff n . K . ( n . K . ( n .
γikb(xi)( k=1 k=1 i=1 i=1
γikb(xi ) , MMM ek
√ r
γikb(xi)((MMM ek(
= 2Eγ sup MMM
≤ 2Eγ sup K .
MMM
≤ 2Eγ
( note : by Cauchy Schwartz inequality ) minx∈Xn wb(x )
√ r minx∈ Xn wb(x ) k=1 i=1 n . K . ( √ k=1 2 i=1 nrK
≤ 2
( b(xi)(2 )
1 2
= minx∈Xn wb(x )
. wb(xi)( b(xi ) wb(xi )
− MMM ek(2
( note : by Jensen ’s inequality and the orthogaussian property of the Gaussian random variable )
723 The second inequality holds because
' ( MMM ek( = ( mk( '
' ' b(x)∈Ck b(x)∈Ck √ r
( b(x)( wb(x ) minx∈Xn wb(x )
.
( ≤ b(x )
= ( b(x)∈Ck b(x)∈Ck ≤ maxx ( b(x)( minx∈Xn wb(x ) wb(x )
=
For the upper bound Eγ supMMM n i=1
K k=1 γikwb(xi)(MMM ek(2 ,
'
' i=1 k=1
Eγ sup MMM n . K . K . | n . n . ≤ K . n .
≤ Eγ w2 k=1 k=1 i=1 i=1
(
= rK( i=1 w2 b(xi ) )
γikwb(xi)(MMM ek(2
γikwb(xi)|(
√ r minx∈Xn ( wb(x))2 )2 √ r b(xi ) )
1 2 ( minx∈Xn ( wb(x))2 )2
1 2
1 minx∈Xn ( wb(x))2 .
√
2 nrK minx∈Xn wb(x )
1 w2 b(xi )
1 2 +
)
=
+ rK( ffi
π/2rK n n .
+ ( i=1
1 2 b(xi ) )
1 minx∈Xn w2 b(x )
)
√ n
2
( i=1
1 w2 b(xi )
1 2 +
) minx∈Xn wb(x ) w2 b(xi ) )
1 2
1 minx∈Xn ( wb(x))2 ) .
Thus , we have ffi
R(FΠK ) ≤ π/2 n n . i=1
( rK( n . n . w2 i=1
This concludes the proof of Lemma 4 . .
Theorem 4 in the paper thus follows according to Theorem
A 3 and Lemma 4 .
− mk||2 wb(x)|| b(x)i wb(x )
− mk,i||2
= i=1 k=1 k=1 x∈Ck x∈Ck∩Xi
K . . wb(x)|| b(x ) . K . r . wb(x ) . K . r . r . K . . . r . K . ( ) x∈Ck∩Xi x∈Ck∩Xi x∈Ck∩Xi k=1 k=1 k=1 i=1 i=1 i=1
=
=
=
[
( γ )
||b(x)i||2 wb(x ) ||b(x)i||2 wb(x ) ||b(x)i||2 ff wb(x )
' k,i + wb(x)||mk,i||2 ] fi
− 2b(x)im − r . K . r . k=1 i=1
−n p(i ) w(i ) Ck
||mk,i||2 K . nk+ wCk pk+
Ki . j=1 p(i ) kj pk+
(
)2 . i=1 k=1
'
According to the definition of centroids in K means clustering , we have mk,i = wb(x ) , mk = 'mk,1,··· , mk,rff , p(i ) = |Xi|/|X| = n(i)/n , n(i ) k+ = |Ck ∩ Xi| , w(i ) wb(x)i . By noting that ( γ ) is a constant , we get the utility function of SEC with incomplete basic partitionings and complete the proof . . x∈Ck∩Xi x∈Ck∩Xi x∈Ck∩Xi
' b(x)i/
Ck
=
F . PROOF OF THEOREM 6 Proof . The weighted K means iterates the assigning and updating phase . In the assigning phase , each instance is assigned to the nearest centroid and so the objective function decreases . Thus , we analyze the change of objective function during updating phase under the circumstance of SEC with incomplete basic partitions . For any centroid g = 'g1,··· , gkff , gk = 'gk,i,··· , gk,rff , and gk fi= mk , wb(x)[||b(x)i − gk,i||2 −||b(x)i − mk,i||2 ] r .
K .
.
Δ = i=1 k=1 x∈Ck∩Xi
( 10 ) According to the Bergman divergence [ 2 ] , f ( a , b ) = ||a − b||2 = φ(a ) − φ(b ) − ( a − b ) fi∇φ(b ) , where φ(a ) = ||a||2 , Eq 10 can be rewritten as follows : r .
K .
. i=1 k=1 x∈Ck∩Xi + ( b(x)i − gk,i ) − ( b(x)i − mk,i ) r . K . . wb(x)[φ(b(x)i ) − φ(gk,i ) fi∇(gk,i ) − φ(b(x)i ) + φ(mk,i ) fi∇(mk,i ) ] wb(x)[φ(mk,i ) − φ(gk,i )
( 11 ) i=1 k=1 x∈Ck∩Xi
+ ( b(x)i − gk,i)T∇(gk,i ) ] r . K .
||mk,i − gk,i||2 > 0 . w(i ) Ck
Δ =
=
=
E . PROOF OF THEOREM 5 Proof . The proof of Theorem 5 is similar to the proof of Theorem 2 , with the only difference being that the missing elements are not taken into account in the objective function of weighted K means clustering . We therefore have : i=1 k=1
Hence , the objective value will decrease during the update phase as well . Given the finite solution space , the iteration will converge within finite steps . We complete the proof . .
724
