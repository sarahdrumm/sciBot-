Trading Interpretability for Accuracy : Oblique Treed Sparse Additive Models
Jialei Wang
University of Chicago jialei@uchicago.edu
Ryohei Fujimaki
NEC Laboratories America rfujimaki@nec labs.com
Yosuke Motohashi
NEC Corporation y motohashi@bkjpneccom
ABSTRACT Model interpretability has been recognized to play a key role in practical data mining . Interpretable models provide significant insights on data and model behaviors and may convince end users to employ certain models . In return for these advantages , however , there is generally a sacrifice in accuracy , ie , flexibility of model representation ( eg , linear , rule based , etc . ) and model complexity needs to be restricted in order for users to be able to understand the results . This paper proposes oblique treed sparse additive models ( OT SpAMs ) . Our main focus is on developing a model which sacrifices a certain degree of interpretability for accuracy but achieves entirely sufficient accuracy with such fully non linear models as kernel support vector machines ( SVMs ) . OT SpAMs are instances of region specific predictive models . They divide feature spaces into regions with sparse oblique tree splitting and assign local sparse additive experts to individual regions . In order to maintain OT SpAM interpretability , we have to keep the overall model structure simple , and this produces simultaneous model selection issues for sparse oblique region structures and sparse local experts . We address this problem by extending factorized asymptotic Bayesian inference . We demonstrate , on simulation , benchmark , and real world datasets that , in terms of accuracy , OT SpAMs outperform state of the art interpretable models and perform competitively with kernel SVMs , while still providing results that are highly understandable .
Keywords Interpretable Model , Model Selection , Sparseness
1 .
INTRODUCTION
Model interpretability has been recognized to play a key role in practical data mining . Interpretable models provide significant insights on data and model behaviors and may convince end users to employ certain models . It is well recognized that , despite the dramatic evolution of machine learning approaches , such as kernel machines [ 41 , 45 ] , boosting [ 13 ] , random forests [ 3 ] , and deep neural networks [ 19 , 24 ] , simple models , like linear regressions or decision trees , are still preferred in such applications as marketing , medical analytics , and science , for which understanding phenomena behind data are more important for end users than simply accurate prediction . In return for advantages in interpretability , however , there is generally a sacrifice in accuracy since flexibility of model representation ( eg , linear , rule based , etc . ) and model complexity need to be restricted in order for users to be able to understand the results .
There are two key concepts in discussions on the issue of model interpretability : 1 ) model representation and 2 ) model complexity . For the former , linear models ( eg , generalized linear models ( GLMs ) [ 31 ] ) and decision trees ( eg , classification and regression tree ( CART ) [ 4 ] ) may be considered to be the most interpretable . Although the simplicity of their model representations contributes significantly to end user understanding , it also limits their predictive ability . For the latter , feature sparseness is a key concept in improving interpretability for linear models ; ie , selecting a small number of key features makes understanding models a lot easier . Also , whiles deep rule chains for decision trees might improve predictive accuracy for complex data , it makes the rule structures hard to understand . The trade off between accuracy and interpretability remains an important issue .
This paper proposes oblique treed sparse additive models ( OTSpAMs ) , which provide more flexible representation than linear models and decision trees ( and , therefore , sacrifice a certain degree of interpretability . ) While offering the same accuracy as that of such fully non parametric models as kernel support vector machines ( KSVMs ) , they still maintain easily interpretable model structures . OT SpAMs are instances of region specific predictive models , which consist of region specifiers and region specific predictors ; the specifiers divide feature spaces into disjoint subspaces ( regions ) , and individual predictors perform predictions in corresponding subspaces ( as we note in Section 2 , region specific predictive models unify the above described two families of interpretable models ) . OT SpAMs employ an oblique treed split model as a region specifier and sparse additive models as individual region specific predictors .
As we have noted above , controlling model complexity is an important issue for maintaining model interpretability . For OTSpAMs , tree structures ( the depth of tree , the number of regions , etc. ) , feature selection for oblique region splitting , and feature selection for local sparse experts must be determined simultaneously . We address this challenging model selection issue by utilizing factorized asymptotic Bayesian ( FAB ) inference [ 11 , 14 ] . Through EM like iterative optimization , we are able to automatically obtain compact and interpretable OT SpAMs . We demonstrate , on simulation , benchmark , and real world datasets , that , in terms of accuracy , OT SpAMs outperform state of the art interpretable models
1245 and perform competitively with kernel SVMs , while still providing results that are highly understandable .
The rest of this paper is organized as follows . Section 2 provides literature reviews of region specific predictive models . OT SpAMs and the proposed learning algorithm are presented in Sections 3 and 4 , respectively . Simulation studies ( Section 5 ) and benchmark evaluations ( Section 6 ) quantitatively show advantages of OT SpAMs , and we demonstrate results on real world POS ( point of sales ) data in Section 7 .
2 . LITERATURE REVIEW
This section focuses mainly on region specific predictive models . Table 1 summarizes characteristics of region specific models , which are described below . A general and broader survey of interpretable models can be found in [ 12 ] .
One of the most naive examples is a linear model , which has only one global region and employs a linear prediction model as the region specific predictor . Some previous studies [ 18 , 21 ] have argued that oblique hyperplanes of linear models might be hard to understand for end users . Feature sparseness is a key concept in trying to mitigate this issue , ie , selecting a small number of key features makes understanding models a lot easier . To obtain sparse linear models ( SLMs ) , various approaches , including convex methods ( eg Lasso [ 37 ] , L1 regularized logistic regression [ 45 ] ) and greedy optimization ( eg , orthogonal matching pursuit [ 29 , 38] ) , have been proposed , though their primary focus is on model generalization ( to mitigate over fitting ) , rather than on enhancing model interpretability . Sparse additive models ( SAMs ) [ 20 , 32 , 34 ] introduce feature wise nonlinearity to improve accuracy . By restricting nonlinearity in individual features ( ie ignoring nonlinear interactions among features ) , we can still visualize their feature wise ( but nonlinear ) contributions and get insights from SAMs . Variants of SAMs ( kernel density logistic regression ( DLR ) [ 6 ] and fast flux discriminant ( FFD ) [ 7 ] ) have been proposed as accurate and interpretable models in recent KDD conferences , and research in this direction has become a topic of intense interest in the community . Decision trees , such as CART , have tree structured region specifiers and performs prediction using constant values in individual regions ( aka piecewise constant predictors ) . Oblique decision trees [ 33 ] extend region specifiers from single feature thresholding to linear hyperplanes , and Bayesian treed linear models ( BTLMs ) [ 8 ] employ linear hyperplanes for region specific predictors . Local supervised learning through space partitioning ( LSL SP ) [ 42 ] utilizes linear hyperplanes for both region specific predictors and region specifiers . Although such models improve predictive accuracy over simple decision trees , their dense linear hyperplanes make the models difficult to understand . [ 44 ] studied a sparse treed model that aims to reduce the test time cost . Eto et al . [ 11 ] proposed a variant of hierarchical mixture experts models that employs factorized asymptotic Bayesian inference for model selection ( FAB/HMEs ) . Using the FAB framework [ 14 ] , they enforce sparseness on region specific linear predictors , which significantly improves interpretability over dense linear predictors , though their single feature thresholding for region specifiers still restricts overall predictive ability . Supersparse linear integer models and their variants [ 26 , 40 ] also learn highly sparse and interpretable model structures , which was also presented as a KDD 2014 Industrial and Government Track Invited Talk . A family of locally linear models ( fast local KSVMs [ 35 ] , locally linear SVMs [ 25 ] , clustered SVMs [ 15 ] , and local deep kernel learning ( LDKL ) [ 23 ] ) uses testpoint specific linear predictors . They do not have explicit regions but , rather , generate linear predictors on the fly . A major drawback of this approach for our purposes is that they can provide model in formation only with every single test point , which makes it difficult to understand overall prediction behaviors .
3 . OT SPAMS : OBLIQUE TREED SPARSE
ADDITIVE MODELS
This section presents details of OT SpAMs . We first describe the region specifiers and region specific predictors for OT SpAM and then derive the factorized asymptotic Bayesian inference in order to address the simultaneous model selection challenge . 3.1 OT SpAMs
Our OT SpAM is a variant of HMEs [ 22 ] , which are tree structured probabilistic mixtures of experts models . In HMEs , regionspecific predictors ( leaf nodes in trees ) are referred to as experts . Suppose we have observations {xn , yn}N n=1 ∼ X × Y , where X ∈ RD is the domain of covariates , Y ∈ R ( for regression tasks ) or {0 , 1} ( for classification tasks ) , N is the number of samples , and D is the data dimensionality . Each gate ( non leaf node ) in the tree determines whether a data instance will go to its left or right branch . At the i th gate ( i = 1 , . . . , G , where G is the number of gates ) , let zi ∈ {0 , 1} be the binary variable indicating which branch the instance x should go down ( without loss of generality , let zi = 0 represents the instance for going left ) . OT SpAMs employ the following logistic hyperplane for their oblique region specifiers : p(zi|x ) =
1
1 + exp(−wi · x)1−zi
1
1 + exp(wi · x)zi
,
( 1 )
1 , . . . , ζ n
Let ζ n = ( ζ n where wi is expected to be sparse for maintaining interpretability . E ) ∈ {0 , 1}E ( E is the number of experts ) represent an indicator of the expert to which xn belongs , where ζ n j = 1 stands for the instance of belonging to the j th expert . Let Gi be the index set for the i th gate , where Gi contains the indices of experts on the sub tree of the i th gate . Let Ej be the index set for the j th expert , where Ej contains the indices of gates on the path from the root to the j th expert . Given the region specifier i=1 , the distribution on ζ n can be described as hyperplanes {wi}G follows : p(ζ n|xn , {wi}G i=1 ) =
E
Yj=1 Yi∈Ej
ψ(xn , i , j)ζn j ,
( 2 ) where ψ(xn , i , j ) is the probability of xn ’s going to the branch to which the j th expert belongs at gate i , more specifically :
ψ(xn , i , j ) =(p(zn p(zn i = 0|xn ) , j ∈ Gleft i = 1|xn ) , otherwise i
.
( 3 ) where Gleft gate . i is the index set of experts in the left sub tree of the i th
Let us consider the following SAM :
D fj ( x ) = fjd(xd ) ,
( 4 )
Xd=1 where fjd(· ) is any smooth univariate function and many of them are expected to be zero ( ie , sparse ) . Notice that , if we set fjd(xd ) = θdxd with linear coefficients θd , then ( 4 ) will be reduced to a standard linear model . The generating distributions of y on the j th expert is given by : p(y|x , φj ) = N ( fj(x ) − y , σ2 j ) ,
( 5 )
1246 Table 1 : Comparison of region specific predictive models ( sp.=sparseness , sf=single feature , fw=feature wise )
SLM
SAM region global region sp . predictor predictor sp .
X linear fw nonlinear X
DT sf threshold
ODT oblique
× constant ref .
[ 37 ]
[ 6 , 7 , 34 ]
[ 4 ]
[ 33 , 39 ] for regression , where φj = ( fj , σj ) , and p(y|x , φj ) =
1
1 + exp(fj(x))1−y exp(fj ( x ) )
1 + exp(fj ( x))y for classification , where φj = fj .
In summary , the entire likelihood is given by : p({yn}N n=1|{xn}N n=1 , {φj}E j=1 , {wi}G i=1 ) =
,
( 6 )
( 7 ) p(yn|xn , φj)p(ζ n j |xn , {wi}G i=1)! .
N
Yn=1 E Xj=1 Framework
3.2 Model Selection for OT SpAM using FAB
In order to learn OT SpAMs , as well as parameter estimation , we have to address three model selection issues simultaneously :
M1 : tree structure ( the number of gates and experts , etc )
M2 : sparseness of region specifiers ( logistic gates presented in ( 1) ) .
BTLM FAB/HME LSL SP
OT SpAM sf threshold
X
× [ 8 ] linear X [ 11 ] oblique
×
× [ 42 ]
X fw nonlinear
X this paper
LDKL test point specific NA linear
× [ 23 ]
¯Θ = [ ¯W , ¯Φ ] is the maximum complete likelihood estimator and D• denotes the dimensionality of • .
Although Eto et al . [ 11 ] asymptotically ignore | ¯Fwi |1/2 and | ¯Fφj |1/2 , using the law of large numbers , this paper considers the following upper bounds to obtain a better approximation , using Hadamard ’s inequality [ 30 ] :
N
| ¯Fwi |1/2 ≤ Xn=1 Xj∈Gi  | ¯Fφj |1/2 ≤ N Xn=1 j!
ζ n
D
ζ n j   Dφj N Xn=1
2
2
N wi  Xn=1 Xj∈Gi 
2 wi
D
ζ n j ∂2 log ψ(xn , i , j )
∂2(wi · xn )  
( 12 )
∂2 log p(yn|xn , φj )
∂2fj
2
Dφj
.
!
( 13 )
By substituting ( 9 ) , ( 12 ) and ( 13 ) into ( 8 ) , we obtain factorized information criterion ( FIC ) as follows :
FIC({x , y}N n=1 , Θ ) = max q,Θ nL({x , y}N n=1 , Θ , q)o ,
( 14 )
M3 : sparseness of sparse additive experts . where
To accomplish these model selection tasks , we employ FAB inference [ 14 ] for OT SpAMs . Note that FAB has recently been used for learning treed sparse linear models [ 11 ] , and this paper extends their framework to the learning of OT SpAMs .
FAB inference maximizes the following Bayesian marginal log likelihood : p({yn}N n=1|{xn}N n=1 ) =
( 8 ) q max n=1 ) n=1 , {ζ n}N q({ζ n}N
Eqlog p({yn}N n=1|{xn}N n=1 ) where q is an arbitrary distribution on {ζ n}N n=1 and the optimal q is q∗({ζ n}N n=1 ) . Let Θ be Θ = [ W , Φ ] where W = [ w1 , , wG ] and Φ = [ φ1 , , φE ] . Laplace ’s method [ 43 ] is then applied to the numerator inside the log function in ( 8 ) as follows : n=1 ) = p({ζ n}N n=1 , {xn}N n=1|{yn}N
, p({y}N n=1 , {ζ}N n=1|{x}N n=1 ) ≈ p({y}N n=1 , {ζ}N n=1|{x}N
G
Yi=1
D wi 2
( 2π )
D wi 2
ζ n j )
| ¯Fwi |1/2
( PN n=1Pj∈Gi
E
Yj=1 n=1 , ¯Θ ) Dφj
( 2π )
2
, n=1 ζ n j )
( PN
Dφj
2
| ¯Fφj |1/2
( 9 ) where
¯Fwi = −
¯Fφj = −
1
∂2 log p(ζ n|xn , {wi}G i=1 )
,
( 10 )
ζ n j
∂wi∂wiT
∂2 log p(yn|ζ n , xn , φj )
∂φj∂φT j
PN n=1Pj∈Gi PN
1 n=1 ζ n j
.
( 11 )
G
N
2
− log(
||wi||0 n=1 , Θ )
L({x , y}N n=1|{x}N n=1 , {ζ}N n=1 , Θ , q ) = Eqhlog p({y}N Xj=1
αn ij ζ n
Xn=1 Xj∈Gi
:::::::::::::::::::::::::::::::::::::::::::::::i Xi=1 Xn=1 j ) log q(ζ n
Xn=1
Xj=1 j ζ n ηn j )
||fj ||0 q(ζ n j ) − log(
( 15 ) j ) ,
−
2
N
N
E
E and
αn ij = exp ( wi · xn )
( 1 + exp ( wi · xn))2 =
∂2 log ψ(xn , i , j )
∂2(wi · xn )
,
( 16 )
1 σ2 j exp fj ( x n )
( 1+exp fj ( xn))2 for regression for classification
.
( 17 )
||wj||0 and ||fj ||0 are the cardinalities of wi and fj , i.e , the number of non zero wi d and fjd , respectively . Here , for computational simplicity , we assume that the data is appropriately scaled in advance such that xn ∈ [ −1 , 1]D . ij and ηn j ( by setting αn
Our new approximation , ( 12 ) and ( 13 ) , results in a key difference from FIC for HMEs derived by Eto et al . [ 11 ] , namely the regularization terms ( wave underline ) are adjusted with the factors αn j = 1 , ( 15 ) becomes consistent with that of Eto et al . [ 11] ) . These factors come from the diagonal elements of ¯Fwi and ¯Fφj , which are empirical Fisher information matrices and provide natural measurements on the likeIt is worth noting that the previous FIC ( ie , lihood spaces [ 1 ] . αn ij = 1 and ηn j = 1 ) regularizes the model without relation to ij = 1 , and ηn
ηn j = 
1247 Algorithm 1 FAB EM optimization for OT SpAM 1 : Input Data : {(xn , yn)}N 2 : Input Parameters : D ( maximum depth of the tree ) , δ ( stopping n=1 . condition ) , ε ( shrinkage threshold ) .
3 : Initialization : t = 0 , L(0 ) = −∞ , {ζ n}N 4 : while L(t ) − L(t−1 ) > δ do 5 : M Step : Update S(t ) , f ( t ) and σ(t ) j j j gorithm 3 . n=1 ∼ U [ 0 , 1 ] .
( regression ) using Al i
6 : M Step : Update w(t ) using Gate Optimization as Algo
7 : 8 : j ) using ( 18 ) . rithm 2 . E Step : Update q(t+1)(ζ n Expert Shrinkage : Eliminate “ non effective ” experts using ( 19 ) . t = t + 1 .
9 : 10 : end while 11 : Post processing : Execute hard gate post processing ( see [ 11 ] for details ) . the metric space of p({y}N n=1 , Θ ) . On the other hand , our regularizers ( wave underline ) can naturally adjust the effect by taking the metric into account . n=1 , {ζ}N n=1|{x}N
4 . OPTIMIZATION ALGORITHM
To obtain the model which maximizes FIC ( 15 ) , FAB employs EM like alternating optimization on Θ ( M step ) and q ( E step ) . The overall algorithmic framework is described in Algorithm 1 . The superscription ( t ) represents the t th EM iteration .
4.1 E Step : updating variational distribution
From ( 15 ) , we obtain the following update equation : q(t)(ζ n j ) ∝ p(yn|xn , φ(t−1 )
ψ(t)(xn , i , j )
( 18 ) expfl− kfjk0ηn j j
) Yi∈Ej 2Nj exp Xi∈Ej  j and Ni = PN j ζ n
−
, kwik0αn ij
2Ni   n=1Pj∈Gi
αn
::::::::::::::::::::::::::::::: : : ij ζ n n=1 ηn where Nj = PN j In contrast to standard EM algorithms , ( 18 ) has the additional terms marked with the waved underline . These terms come from the regularization terms in ( 15 ) ( also marked with a waved underline ) . This causes a shrinkage effect [ 11 , 14 ] through the EM iteration , i.e , more complex and smaller experts are penalized more , and we can safely eliminate “ non effective ” experts from the model using a simple thresholding rule as follows : problem ( the wi related terms in ( 15) ) : wi(t ) = arg max wi
N
Xn=1 Xj∈Gi q(t−1)(ζ n j ) log ψ(xn , i , j )
− kwik0
2 log(
N
Xn=1 Xj∈Gi
ηn j q(t−1)(ζ n j ) )
Let GiL be the index sets of experts on the left sub tree of gate i , and GiR be the index sets of experts on the right sub tree of gate i . We can re write the problem as follows : wi(t ) = arg max wi
Q(wi ) − kwik0
2 log(
N
Xn=1 Xj∈Gi
ηn j q(t−1)(ζ n j ) ) ,
( 20 ) where
Q(wi ) =
+
N
N
Xn=1 Xj∈Gi Xn=1 Xj∈Gi q(ζ n q(ζ n j ) Pj∈GiL Pj∈Gi j ) Pj∈GiR Pj∈Gi q(ζ n j ) q(ζ n j ) log q(ζ n j ) q(ζ n j ) log exp(wi · xn )
1 + exp(wi · xn)! 1 + exp(wi · xn)! .
1
( 21 )
This problem can be seen as a sparsity regularized generalized logistic regression problem : i ) unlike the standard regression , here q(ζn j ) the response is any number in [ 0 , 1 ] ( Pj∈GiL q(ζn Pj∈Gi j ) q(ζ n j ) and Pj∈GiR Pj∈Gi in this problem ) and ii ) there is a weight for each instance:Pj∈Gi
Problem ( 20 ) is non convex ( due to the L0 regularization ) , and we have adopted a greedy strategy [ 38 ] to get an approximate solution . Details are shown in Algorithm 2 . Let S ⊆ [ D ] be the set of selected features . Also , we denote the maximizer of Q as follows : q(ζn j ) q(ζn j ) .
ˆwi(S ) = max wi(S )
Q(wi(S) ) ,
( 22 ) where solving ( 22 ) is a constrained , weighted logistic regression problem . At each iteration , we selected the feature that maximizes the gradient absolute value |∇dQ(wi)| , which is where ◦ is the Hadamard product , and
|∇dQ(wi)| =fifififififi R1:N = Pj∈GiL Pj∈Gi q(ζ 1:N j
) ◦ R1:N ) · x1:N d
( Xj∈Gi
( 23 )
, fifififififi
) j q(ζ 1:N q(ζ 1:N ) j
−
1 + exp(−wi(k ) · x1:N )!
1 and then solved the constrained weighted logistic regression problem , until q(t)(ζ n j ) < δ .
( 19 )
Q(wi(k ) ) − Q(wi(k−1 ) ) ≤ log(
N
Xn=1 Xj∈Gi
ηn j q(t−1)(ζ n j ) ) ,
( 24 )
N
Xn=1
In practice , one could start from a sufficiently large tree , after which the “ shrinkage ” scheme of OT SpAM would find the proper size tree structure for capturing the data well . In this way , we have addressed the model selection issue M1 .
4.2 M Step : Learning Sparse Oblique Region
Specifiers
We update the i th gate by solving the following optimization was satisfied , where ( k ) is the iteration index in Algorithm 2 . In this way , we have addressed the model selection issue M2 . 4.3 M Step : Learning Local Experts
In order to optimize the j th local expert fj , we introduce the following model : fjd(x ) =
M
Xm=1
βm jdgm(x ) ,
( 25 )
1248 Algorithm 2 Gate Optimization for OT SpAM 1 : for i = 1 , , G do 2 : i = ∅ , k = 0 , wi(k ) = 0 , µ1:N(k ) i
Initialization S(k ) 1/(1 + exp(wi(k ) · x1:N ) . while TRUE do and fitted responses . Here we directly select the feature with the maximum gradient norm :
= d(k ) = arg max d6∈S
( k ) j kg1:M ( x1:N d
)(R(k ) ◦ q(t)(ζ 1:N j
))k2 .
( 27 )
|∇dQ(wi)| accord d6∈S
( k ) i i ∪d(k ) , wi(k ) = ˆw(S(k ) i
) , µ1:N(k ) i
=
Select feature d(k ) = arg max ing to ( 23 ) . k = k + 1 . Update S(k ) 1/(1 + exp(−wi(k ) · x1:N ) . if ( 24 ) is satisfied then i = S(k ) k = k − 1 , and Break .
7 : 8 : 9 : 10 : 11 : 12 : end for end if end while Output wi(k ) .
3 : 4 :
5 : 6 :
4 : 5 : 6 : 7 : 8 :
9 :
This gradient criterion avoids having to fit the model O(D ) times , which makes the selection process much faster ; ii ) [ 27 , 28 ] use an orthogonal matching pursuit type fitting procedure [ 38 ] ( that is , after selecting one new feature , the model is re fitted using the newly selected feature pool ) . Rather than this approach , we use a matching pursuit [ 29 ] type method to speed up the algorithms , ie , we just add the new fitted univariate function without re fitting the model . The fitting equation ( derived by solving a weighted least squares ) is described as follows : β1:M jd = ( GT HG)−1GT R(k ) .
( 28 ) where G ∈ RN ×M is the feature matrix such that Gnm = gm(xn d ) and H ∈ RN ×N is the diagonal weighting matrix such that Hnn = 2 . These special designs make the algorithm much faster q(t)(ζ n j ) than the procedures in [ 27 , 28 ] by avoiding repeated model fitting and re fitting . Though it might be less accurate in feature selection and model fitting ( but not too much when the basis functions are not highly correlated with each other ) , the hard gate post processing proposed in [ 11 ] ( the step 11 of Algorithm 1 ) makes the final model more stable and reliable , as we will see in Section 5 .
The difference between classification and regression is in the update of the residual . For regression , we can naturally define the residual as follows :
R(k ) = y1:N − ˆf ( k ) j
( x1:N )
( 29 )
The residual of logistic loss for classification is not so obvious , but we follow [ 17 ] , which defines it in terms of the updating direction of the Newton step as follows :
R(k ) = y1:N − µ1:N(k ) j
µ1:N(k ) j
◦ ( 1 − µ1:N(k ) j
,
)
µ1:N(k ) j
= exp( ˆf ( k ) j
( x1:N ) )
1 + exp( ˆf ( k ) j
( x1:N ) )
.
( 30 )
( 31 )
The stopping condition is defined as follows :
N
Xn=1 q(t−1)(ζ n j )log p(yn|xn , ˆβ(k ) j
− log p(yn|xn , ˆf ( k−1 ) j
, σ(t−1 ) j
, σ(t−1 ) j
)
( 32 )
) ≤ log(
N
Xn=1
ηn j ζ n j ) .
In this way , we have addressed the model selection issue M3 .
5 . SIMULATION STUDY : MODEL SELEC
TION AND VISUALIZATION
This section presents results of simulation studies and demonstrates our FAB based model selection for OT SpAMs . In order to make OT SpAMs interpretable , we proposes a visualization method that employs individual local sparse additive experts . 5.1 Simulation Setup
We generated N = 5000 data points in which each instance is described by D = 15 features , and the features are uniformly distributed in [ 0,1 ] , ie X ∼ U [ 0 , 1]D . The true tree structure is
Algorithm 3 Greedy Additive Regression for OT SpAM 1 : Input Data : {(xn , yn)}N 2 : for j = 1 , , E do 3 : n=1 , q(t)(ζ n j ) , σ(t−1 )
Initialization S(k ) α , Residual R(k ) = y1:N − ˆf ( k ) while TRUE do j = ∅ , k = 0 , α = PN
( 1:N j
) .
. j j n=1 yn/N , ˆf ( k ) j = k = k + 1 . Select feature d(k ) using ( 27 ) . Fit ˆf ( k ) Update S(k ) jd ( by updating β1:M jd ) using ( 28 ) j = S(k ) j ∪d(k ) , ˆf ( k )
ˆfjd(xj ) . Update residual R(k ) using ( 29 ) for regression and ( 30 ) for classification . if ( 32 ) is satisfied then j = α+Pd∈S
( k ) j k = k − 1 , and Break .
10 : 11 : 12 : 13 : 14 : 15 : end for end if end while Update ˆf ( t ) j = ˆf ( k ) j
, σ(t ) j = kR(k)k2 . where
Xj=1 Xn=1 Xj=1
−
E where gm is a pre defined smooth basis function and M is the number of basis functions ( in our experiments , we use P spline functions as gm ) . Here our parameterization is changed to φj = βj , where βj = ( β1:M jD ) . We then update the j th local expert by solving the following optimization problem : j1 , . . . , β1:M
N
E
β(t ) j = arg max β0 q(t−1)(ζ n j ) log p(yn|xn , βj , σ2(t−1 ) j
) kβjk∞,0
2 log(
N
Xn=1
βn j ζ n j ) ,
( 26 ) where kβjk∞,0 = kkβ1:M j1 k∞ , kβ1:M tice that we can simply ignore σ2(t−1 ) sification case . j j2 k∞ , , kβ1:M jD k∞k0 . Nowhen we consider the clas
Problem ( 26 ) is reduced to the optimization of weighted GLM under group sparsity regularization . This paper adopts the greedy optimization method summarized in Algorithm 3 . Note that existing works on greedy group selection [ 28 ] ( or additive forward regression [ 27 ] ) include proposals for addressing the greedy group feature selection problem . In contrast to these , Algorithm 3 has the following differences : i ) at the feature selection stage , [ 27 ] selects the feature that maximizes the alignment between residuals
1249 Figure 1 : Estimated additive functions for Expert1 in the simulations . The horizontal and vertical axes represent , respectively , the original feature and the estimated sparse additive feature . shown in ( A ) . It has 4 experts , each of which uses 2 − 3 features , and the partition nodes use linear functions of 2 features .
2.5X9 − 1.486X10 < 0.663
2.5X9 − 1.6X10 < 0.613
1.5X6 − 0.7X4 < 0.358
1.6X3 − 0.5X12 < 0.715
Expert 1 Expert 2
Expert 3 Expert 4
( A ) True tree model
We generated response Y of the instances , in accord with the experts they belongs to , in the following way :
• Expert 1 : Y = 5 sin(πX1 ) + 15(X2 − 0.5)2 − 4X7 + 1 + ε ,
ε ∼ 0.01N ( 0 , 1 ) .
• Expert 2 : Y = 7/(1 + exp(5 − 10X5 ) ) + 5 sin(πX 2
7 ) −
5X8 + 2 + ε , ε ∼ 0.01N ( 0 , 1 ) .
• Expert 3 : Y = 8|X1 − 0.5| + 7(3X8 − 2)2 − 4 + ε , ε ∼
0.01N ( 0 , 1 ) .
• Expert 4 : Y = 5 cos(2πX2 ) + 5X8 + 2 log(100X10 + 3 ) −
3 + ε , ε ∼ 0.01N ( 0 , 1 ) .
In this simulation , we set the initial tree depth to D = 4 ( ie , the initial number of experts was 16 ) , the shrinkage threshold to ǫ = 0.06N , and the stopping threshold to δ = 10−5 . Also , since oblique region specifiers using many features are hard to interpret , we set the maximum number of features used in each partition node to 3 . Additionally , we employed P spline functions ( a family of Bsplines with a smoothness penalty [ 10 ] ) as gm in ( 25 ) . We chose the penalty parameter for P splines as 0.5 , the spline degree as 3 , the number of knots as 6 .
5.2 Model Selection Results
The estimated tree structure is shown in ( B ) . There were 16 experts at the start , irrelevant experts were gradually pruned from the model by means of FAB regularization , and , at the convergence point , our method almost completely recovered1 the true tree structures with exactly the same features in each gate ( oblique hyperplane ) .
1The partition functions in ( B ) have been properly scaled for comparison with the functions in ( A ) , since scaling the functions does not change the decision boundary .
1.5X6 − 0.683X4 < 0.369
1.6X3 − 0.506X12 < 0.698
Expert 1 Expert 2
Expert 3 Expert 4
( B ) Estimated tree model
Figure 1 shows the estimated additive functions of Expert1 . Since we employ a matching pursuit type of optimization in the M step , there is marginal estimation error in the estimated feature functions before post processing ( green curves ) . After postprocessing , the true ( blue curves ) and estimated curves ( red curves ) are quite consistent . Although we omit results for the other experts , we obtained similarly good estimation results for those as well .
These results empirically demonstrate strong model selection ca pability in addressing M1 , M2 and M3 simultaneously . 5.3 Visualization of Local Sparse Additive Ex perts
Since each sparse additive feature fjd is ( feature wise ) nonlinear , visualization is critically important to maintain model interpretability . This paper proposes a stacked area plot to visualize sparse additive features . Figure 2 shows the visualization for Expert1 ( we shifted Expert1 to the negative side in this figure to more easily explain our visualization method ) . The left hand figure is a simple line plot of estimated feature functions wrt X1 , X2 , and X7 . The line plot would be difficult to see if several features were selected and overlapped one another in a single plot . To avoid this , we employ a stacked area plot that is constructed as follows . First , individual feature functions are separated into positive and negative sides , as shown in the middle column of Figure 2 . The stacked area plot was then built by combining positive and negative stacked area plots ( the right hand figure ) . As is shown , we are able to avoid “ ugly ” overlapping and can clearly see how each input feature “ nonlinearly ” contributes to the target signal . We visualize the stacked area plot for each expert , and the combination of standard tree visualization with the stacked area plots provides a full picture of nonlinear model behaviors .
6 . BENCHMARK EVALUATION OF PREDIC
TIVE ACCURACY
We evaluated OT SpAMs on 24 public benchmark data sets , available from the UCI Machine Learning Repository [ 2 ] , for both regression and classification tasks . Table 2 summarizes the statistics for these data sets . We used the same initial tree depth and ter
1250 Table 2 : List of benchmark datasets . Name
#Instances #Features
Task
Auto mpg
Boston housing
Stock
Space ga Abalone
ParkinsonM Cpusmall Kinematics Puma8nh Comp acti Ailerons Cadata Banana
Australian
Pima Diabetes
Fourclass
Splice
Banknote Titanic
Svmguide1 EEG eyestate
Magic04 Cod rna Ijcnn1
398 506 950 3107 4177 5875 8192 8192 8192 8192 13750 20640 400 690 768 862 1000 1372 2201 7089 14980 19120 59535 141691
4 13 9 6 8 20 12 8 8 21 40 8 2 14 8 2 60 4 3 4 14 10 8 22
Regression Regression Regression Regression Regression Regression Regression Regression Regression Regression Regression Regression Classification Classification Classification Classification Classification Classification Classification Classification Classification Classification Classification Classification
ID D1 D2 D3 D4 D5 D6 D7 D8 D9 D10 D11 D12 D13 D14 D15 D16 D17 D18 D19 D20 D21 D22 D23 D24 but it is worth noting that LDKL produces a predictor at every single data point and that no interpretation of regions is provided , as may be seen in Table 1 . We observed that OT SpAMs performed slightly worse than SVM RBFs and sacrificed accuracy for interpretability , though , except for D21 , the sacrifice was not significant .
• On these data sets , OT SpAM usually output treed models with 5 8 experts , and these models were reasonably interpretable . OTSpAM selected different fractions of features , depending on the data sets used .
In summary , we conclude that OT SpAMs sacrificed minimum accuracy loss for interpretability , wrt fully non parametric methods , by maintaining interpretable treed region structures and featurewise sparse nonlinear expert structures .
7 . REAL WORLD APPLICATION : SALES
FORECASTING
In the retail industry , sales forecasting is a key component of advanced store management . Let us consider three scenarios :
A ) store inventory management requires forecasting every 6 hours for 2 day to 1 week periods . Accurate forecasting reduces disposal loss , and model interpretability lets store managers safely use a forecasting based ordering system .
B ) store assortment planning requires forecasting every 1 day for 1 week to 3 week periods . Accurate forecasting increases revenue wrt shelf space , and model interpretability helps marketers to hypothesize good assortment strategies .
C ) production planning requires forecasting every 1 week for 2 month periods . Accurate forecasting reduces supply chain inventory losses , and model interpretability helps marketers to plan release timing for new products .
We applied OT SpAM to sales forecasting of sweet bakery products in a middle size supermarket located in a residential area of
Figure 2 : Stacked area visualization for the learned Expert1 in the simulation study . The horizontal and vertical axes represent , respectively , the original feature and the estimated sparse additive feature .
. mination conditions as in the simulations and evaluated root mean squared error ( RMSE ) for regression and accuracy in classification . For regression tasks , we compared OT SpAM with the following methods : OLS ( ordinary least squares using the full set of the features ) , RegTree2 ( a classical regression tree model [ 4] ) , FAB/HME [ 11 ] , AM ( additive models [ 16 ] using the full set of the features ) , and SVR RBF [ 36 ] . For classification tasks , we compared OT SpAM with the following methods : LLR ( linear logistic regression using the full set of the features ) , CART3 [ 4 ] , LSL SP4 [ 42 ] , FAB/HME ( we adopted the method described in [ 11 ] with logistic model ) , LDKL [ 23 ] , ALR ( additive logistic regression [ 16 ] using the full set of the features ) , DLR5 [ 6 ] , and SVM RBF6 [ 9 ] . The parameters in SVR RBF , LSL SP , LDKL , DLR , and SVM RBF were optimized on the basis of 10 fold cross validation on training data . Note that we used all features for linear and additive models ( OLS , AM , LLR and ALR ) . The primary focus here was on accuracy evaluation , and they performed better with all features ( without sparse regularization ) .
Table 3 and Table 4 report the 10 fold averaged cross validation RMSE and classification accuracy , respectively . From these results , we have the following observations :
• For regression tasks , OT SpAMs achieved the lowest RMSE values in most cases . Both AMs and FAB/HMEs also performed much better than OLS and RegTrees . OT SpAMs took advantages of both methods and performed competitively with SVRRBF ( or sometimes even outperformed it ) .
• For classification tasks , similar observations were obtained , ie , FAB/HMEs and ALRs performed better than LLRs , and OTSpAMs usually outperformed both them and state of the art additive models ( ALRs and DLRs ) . LDKL also performed well ,
2We use the built in RegressionTree class in MATLAB 3We use the built in ClassificationTree class in MATLAB 4http://blogsbuedu/joewang/code/ 5http://wwwcsewustledu/~wenlinchen/project/DLR/ 6For SVR RBF and SVM RBF , we use the LIBSVM package [ 5 ] .
1251 OLS
5.10 ± 0.53 5.38 ± 0.86 2.32 ± 0.09 0.14 ± 0.01 2.32 ± 0.17 7.30 ± 0.10 16.2 ± 0.31 0.29 ± 0.00 4.67 ± 0.09 15.5 ± 0.30
RegTree
6.67 ± 1.33 9.06 ± 2.92 1.99 ± 0.34 0.28 ± 0.03 5.57 ± 0.22 0.90 ± 0.34 7.57 ± 0.21 0.40 ± 0.01 8.44 ± 0.31 6.73 ± 0.28
ID D1 D2 D3 D4 D5 D6 D7 D8 D9 D10 D11 D12
FAB/HME 3.29 ± 0.32 3.72 ± 0.96 2.28 ± 0.39 0.13 ± 0.01 2.27 ± 0.15 4.96 ± 0.57 5.14 ± 0.62 0.24 ± 0.01 4.18 ± 0.21 5.12 ± 0.40
AM
2.88 ± 0.43 3.65 ± 0.72 1.33 ± 0.11 0.13 ± 0.02 2.31 ± 0.10 5.11 ± 0.06 3.79 ± 0.22 0.20 ± 0.00 4.24 ± 0.10 3.57 ± 0.18
OT SpAM 2.79 ± 0.52 3.41 ± 0.55 1.02 ± 0.10 0.12 ± 0.02 2.22 ± 0.12 2.99 ± 0.19 3.26 ± 0.35 0.19 ± 0.00 3.31 ± 0.08 2.79 ± 0.61
SVR RBF 3.13 ± 0.48 5.65 ± 0.80 0.91 ± 0.09 0.10 ± 0.01 2.17 ± 0.11 2.95 ± 0.07 4.56 ± 0.49 0.07 ± 0.01 3.34 ± 0.09 5.07 ± 0.35
Table 3 : Comparison of test RMSE values on benchmark datasets . The best and second best methods ( except SVR RBF ) are highlighted in bold and bold italic faces , respectively .
( 2.50 ± 2.2 ) ·10−4 ( 7.48 ± 0.1 ) ·104
( 4.27 ± 0.1 ) ·10−4 ( 12.6 ± 0.3 ) ·104
( 1.70 ± 0.0 ) ·10−4 ( 6.82 ± 0.1 ) ·104
( 1.73 ± 0.0 ) ·10−4 ( 6.48 ± 0.1 ) ·104
( 1.66 ± 0.0 ) ·10−4 ( 5.97 ± 0.1 ) ·104
( 5.97 ± 1.2 ) ·10−4 ( 11.8 ± 0.0 ) ·104
Table 4 : Comparison of test classification accuracy on benchmark datasets . The best and second best methods ( except SVM RBF ) are highlighted in bold and bold italic faces , respectively .
ID D13 D14 D15 D16 D17 D18 D19 D20 D21 D22 D23 D24
LLR
68.5 ± 4.4 84.6 ± 3.5 69.6 ± 4.7 75.0 ± 6.1 80.4 ± 3.3 86.0 ± 2.5 77.1 ± 0.4 89.1 ± 0.5 58.0 ± 0.6 78.9 ± 1.0 89.6 ± 0.4 92.1 ± 0.5
CART
84.7 ± 6.4 86.2 ± 4.2 73.9 ± 8.0 95.8 ± 2.2 90.7 ± 3.6 97.4 ± 1.5 79.1 ± 0.7 96.7 ± 0.8 82.7 ± 0.7 83.8 ± 0.8 93.8 ± 0.3 97.4 ± 0.4
LSL SP 69.7 ± 7.1 86.2 ± 3.7 74.9 ± 6.4 78.6 ± 6.2 83.0 ± 5.8 98.1 ± 1.6 78.1 ± 0.4 93.7 ± 0.7 76.5 ± 1.2 81.4 ± 1.3 91.1 ± 0.3 90.4 ± 0.3
FAB/HME 76.7 ± 8.5 85.3 ± 3.7 69.7 ± 8.3 76.5 ± 6.0 79.2 ± 3.3 93.4 ± 1.9 77.6 ± 0.5 93.8 ± 0.9 60.8 ± 1.4 81.7 ± 1.4 91.0 ± 0.5 93.9 ± 2.2
LDKL
88.7 ± 4.6 85.6 ± 4.1 75.7 ± 7.9 96.8 ± 3.3 85.4 ± 1.1 99.9 ± 0.2 79.0 ± 0.7 96.2 ± 1.1 55.1 ± 0.0 85.0 ± 0.8 94.0 ± 1.2 94.2 ± 1.7
ALR
76.2 ± 8.1 86.5 ± 2.4 76.5 ± 7.5 90.0 ± 3.4 90.7 ± 2.2 98.9 ± 0.9 77.8 ± 0.3 96.8 ± 0.7 57.9 ± 1.8 84.9 ± 0.8 94.0 ± 0.2 93.7 ± 0.6
DLR
67.0 ± 7.1 86.5 ± 3.5 76.5 ± 6.7 75.8 ± 4.7 90.8 ± 2.0 88.8 ± 1.4 77.6 ± 0.3 95.5 ± 1.0 55.1 ± 0.1 78.6 ± 0.9 77.6 ± 0.4 91.1 ± 0.8
OT SpAM SVM RBF 91.3 ± 4.0 82.2 ± 7.5 87.1 ± 3.9 85.5 ± 3.7 77.5 ± 7.1 75.8 ± 8.8 96.1 ± 1.8 99.8 ± 0.5 92.5 ± 2.2 86.1 ± 3.0 99.1 ± 0.7 100 ± 0.0 78.3 ± 0.5 78.5 ± 0.0 97.1 ± 0.6 96.9 ± 0.1 81.6 ± 0.1 59.7 ± 2.3 85.8 ± 1.1 87.2 ± 0.1 94.2 ± 0.2 95.6 ± 0.0 95.6 ± 1.9 98.6 ± 0.1
Tokyo7 . Our primary target scenarios were A ) and B ) , and we set the target variable to total sales of sweet bakery products for one day one week later . We used three years of historical data whose time resolution was daily . The first two years ( 731 samples ) were used for training , and the other year ( 365 samples ) was used for testing . Table 5 summarizes variables used for the forecasting . There are 30 input features in total . In addition to sales information ( x20 , x21 and the target variable ) , we independently collected weather related variables ( x2 x19 ) and also added calendar information ( x22 x30 ) . All numerical variables , including the target variable , were standardized in advance . Experimental settings were the same as those of Sections 5 and 6 , except the initial tree depth ( here we use D = 5 ) . Figure 3 shows the forecasting results for the test period . As can be seen , OT SpAM achieved fairly good forecasting .
−X17 + 0.55X20 ≤ −0.97
X20 ≤ 1.1
X29 ≤ 0.49
Expert 1 −X21 + 0.11X29 ≤ −0.7
Expert 4 Expert 5
Expert 2 Expert 3
( C ) Estimated tree model for sales forecasting
7The data has been provided by KDP SP Co . , LTD , http://wwwksp spcom
The estimated tree structure is shown in ( C ) . The region specifier employed average pressure ( x17 ) , sales histories ( x20 and x21 ) , and weekday flag ( x29 ) . Taking into account the fact that average pressure in Tokyo is relatively high during May to September , the region specifier identified the following clusters : • Expert1 : in season ( high average sales ) during early summer to autumn .
• Expert2 : off season ( low average sales ) during early summer to autumn .
• Expert3 : other season ( middle average sales ) during early sum mer to autumn .
• Expert4 : weekday during autumn to early summer . • Expert5 : holiday during autumn to early summer .
Figure 4 provides our stacked area plot for individual experts .
We can characterize the experts as follows : • Expert1 : Products in this category ( sweets bakery ) are sold a lot on Friday . The largest bias value among the experts supports our hypothesis that this cluster corresponds to in season .
• Expert2 : The small responses ( ie , the scale of the vertical axis is small ) supports the hypothesis that this cluster corresponds to off season , and the sales are small without relation to weather .
• Expert3 : The response for daylight ( x13 ) is high in the middle area of the horizontal axis . Since mid summer daylight hours are long in Tokyo , this result indicates that sunny days tend to have large sales .
• Expert4 : We can observe a strong response wrt the sales of 1 week previous but the peak is somehow shifted to the left hand side . This might indicate a natural decrease in sales following a promotional campaign .
1252 Figure 3 : Sales forecasting results for the test period ( one year ) using OT SpAMs .
Table 5 : List of variables in the sales forecasting dataset . N and B stand for numerical and binary values . weather1 and weather2 are forecasting ( 1 week ahead ) and history ( 1 week ago ) , respectively .
ID x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x11 x12 x13 x14 x15 x16 x17 x18 x19 x20 x21 x22 x23 x24 x25 x26 x27 x28 x29 x30 name value dates from Jan . 1st . rainfall forecast ( mm ) ave . temp . forecast ( degree ) daylight forecast ( hours ) snowfall forecast ( cm ) humidity forecast ( % ) cloudiness forecast ( 10 % ) ave . pressure forecast ( hPa ) max temp . forecast ( degree ) min temp . forecast ( degree ) rainfall history ( mm ) average temp . history ( degree ) daylight history ( hours ) snowfall history ( cm ) humidity history ( % ) cloudiness history ( 10 % ) ave . pressure history ( hPa ) max temp . history ( degree ) min temp . history ( degree ) sales ( 1 week ago ) sales ( 2 weeks ago )
Sunday Monday Tuesday
Wednesday Thursday
Friday Saturday Weekday Holiday
N N N N N N N N N N N N N N N N N N N N N B B B B B B B B B type date weather1 weather1 weather1 weather1 weather1 weather1 weather1 weather1 weather1 weather2 weather2 weather2 weather2 weather2 weather2 weather2 weather2 weather2 sales history sales history calendar information calendar information calendar information calendar information calendar information calendar information calendar information calendar information calendar information
• Expert5 : The sales are low on Saturdays .
The above observations can be transformed into insights for im prove store operations , such as :
A ) store inventory management : In order to avoid excessive inventory , store managers should take into account the strong possibility of a post promotional campaign slump in sales . Further , store managers should increase the number of displayed items of this product category , as this may increase store revenue .
B ) store assortment planning : store managers should consider pos sible adjustments to the product line up in this product category since the sales trends may change . stores , but we believe that the above results demonstrate high interpretability of OT SpAMs in the real world applications .
8 . SUMMARY AND FUTURE WORK
We have proposed oblique treed sparse additive models , novel extensions of generalized additive models for heterogeneous data analysis that employs the learning of hierarchical mixtures of sparse additive models . We have presented a Bayesian learning algorithm which fully automates space partitioning and feature selection , making the proposed approach nearly parameter free . Promising empirical results have been obtained for both simulated and real world data . Future work will address the theoretical understanding and computational efficiency of OT SpAMs , as well as extensions to such more general data mining problems as multiclass classification and Poisson regression .
9 . ACKOWLEDGEMENTS
The majority of the work was done during the internship of the first author at NEC Laboratories America , Cupertino , CA .
References [ 1 ] S . Amari and H . Nagaoka . Methods of Information
Geometry , volume 191 of Translations of Mathematical monographs . Oxford University Press , 2000 .
[ 2 ] K . Bache and M . Lichman . UCI machine learning repository , 2013 .
[ 3 ] L . Breiman . Random forests . Machine Learning ,
45(1):5–32 , 2001 .
[ 4 ] L . Breiman , J . Friedman , R . Olshen , and C . Stone .
Classification and Regression Trees . Wadsworth and Brooks , Monterey , CA , 1984 .
[ 5 ] C C Chang and C J Lin . Libsvm : A library for support vector machines . ACM Trans . Intell . Syst . Technol . , 2(3):27:1–27:27 , May 2011 .
[ 6 ] W . Chen , Y . Chen , Y . Mao , and B . Guo . Density based logistic regression . In KDD , pages 140–148 , 2013 . [ 7 ] W . Chen , Y . Chen , and K . Q . Weinberger . Fast flux discriminant for large scale sparse nonlinear classification . In KDD , pages 621–630 , 2014 .
[ 8 ] H . A . Chipman , E . I . George , and R . E . Mcculloch . Bayesian treed generalized linear models . Bayesian Statistics , 7 , 2003 . [ 9 ] C . Cortes and V . Vapnik . Support vector networks . Machine
Learning , 20(3):273–297 , 1995 .
[ 10 ] P . H . C . Eilers and B . D . Marx . Flexible smoothing with b splines and penalties . Statistical Science , 11(2):89–121 , 05 1996 .
These insights are still hypotheses and must be evaluated in real
[ 11 ] R . Eto , R . Fujimaki , S . Morinaga , and H . Tamano .
1253 Figure 4 : Estimated additive functions for sales forecasting data .
Fully automatic bayesian piecewise sparse linear models . In AISTATS , pages 238–246 , 2014 . matching pursuit for variable selection and prediction . In NIPS , pages 1150–1158 , 2009 .
[ 12 ] A . A . Freitas . Comprehensible classification models : A
[ 29 ] S . Mallat and Z . Zhang . Matching pursuits with position paper . SIGKDD Explor . Newsl . , 15(1):1–10 , Mar . 2014 .
[ 13 ] J . H . Friedman . Greedy function approximation : A gradient boosting machine . Annals of Statistics , 29:1189–1232 , 2000 .
[ 14 ] R . Fujimaki and S . Morinaga . Factorized asymptotic bayesian inference for mixture modeling . In AISTATS , pages 400–408 , 2012 . time frequency dictionaries . IEEE Transactions on Signal Processing , 41(12):3397–3415 , 1993 .
[ 30 ] V . G . Maz’ya and T . O . Shaposhnikova . Theory of multipliers in spaces of differentiable functions . Russ . Math . Surv . , 38(23 ) , 1983 .
[ 31 ] P . McCullagh and J . A . Nelder . Generalized linear models
( Second edition ) . London : Chapman & Hall , 1989 .
[ 15 ] Q . Gu and J . Han . Clustered support vector machines . In
[ 32 ] L . Meier , S . van de Geer , and P . Buhlmann .
AISTATS , pages 307–315 , 2013 .
[ 16 ] T . Hastie , R . Tibshirani , and J . H . Friedman . The elements of statistical learning : data mining , inference , and prediction . New York : Springer Verlag , 2001 .
[ 17 ] T . J . Hastie and R . J . Tibshirani . Generalized additive models . London : Chapman & Hall , 1990 .
[ 18 ] B . Hayete and J . R . Bienkowska . Gotrees : predicting go associations from protein domain composition using decision trees . Pacific Symposium on Biocomputing , pages 127–138 , 2005 .
[ 19 ] G . E . Hinton , S . Osindero , and Y . W . Teh . A fast learning algorithm for deep belief nets . Neural Computation , 18(7):1527–1554 , 2006 .
[ 20 ] J . Huang , J . L . Horowitz , and F . Wei . Variable selection in nonparametric additive models . Ann . Statist . , 38(4):2282–2313 , 08 2010 .
High dimensional additive modeling . Ann . Statist . , 37(6B):3779–3821 , 12 2009 .
[ 33 ] S . K . Murthy , S . Kasif , and S . Salzberg . A system for induction of oblique decision trees . Journal of Artificial Intelligence Research , 2:1–32 , 1994 .
[ 34 ] P . Ravikumar , J . Lafferty , H . Liu , and L . Wasserman . Sparse additive models . Journal of the Royal Statistical Society : Series B ( Statistical Methodology ) , 71(5):1009–1030 , 2009 .
[ 35 ] N . Segata and E . Blanzieri . Fast and scalable local kernel machines . Journal of Machine Learning Research , 11:1883–1926 , 2010 .
[ 36 ] A . J . Smola and B . Schölkopf . A tutorial on support vector regression . Statistics and Computing , 14:199–222 , 2004 . [ 37 ] R . Tibshirani . Regression shrinkage and selection via the lasso . Journal of the Royal Statistical Society , Series B , 58:267–288 , 1996 .
[ 21 ] J . Huysmans , K . Dejaeger , C . Mues , J . Vanthienen , and
[ 38 ] J . A . Tropp and A . C . Gilbert . Signal recovery from random
B . Baesens . An empirical evaluation of the comprehensibility of decision table , tree and rule based predictive models . Decis . Support Syst . , 51(1):141–154 , Apr . 2011 .
[ 22 ] M . I . Jordan and R . A . Jacobs . Hierarchical mixtures of experts and the em algorithm . Neural Computation , 6(2):181–214 , 1994 .
[ 23 ] C . Jose , P . Goyal , P . Aggrwal , and M . Varma . Local deep kernel learning for efficient non linear SVM prediction . In ICML , pages 486–494 , 2013 .
[ 24 ] A . Krizhevsky , I . Sutskever , and G . E . Hinton . Imagenet classification with deep convolutional neural networks . In NIPS , pages 1106–1114 , 2012 . measurements via orthogonal matching pursuit . IEEE Transactions on Information Theory , 53(12):4655–4666 , 2007 .
[ 39 ] A . K . Y . Truong . Fast growing and interpretable oblique trees via logistic regression models . DPhil Thesis , University of Oxford , 2009 .
[ 40 ] B . Ustun , S . Tracà , and C . Rudin . Supersparse linear integer models for predictive scoring systems . In AAAI Late Breaking Track , 2013 .
[ 41 ] V . N . Vapnik . The Nature of Statistical Learning Theory .
Springer Verlag New York , Inc . , New York , NY , USA , 1995 . [ 42 ] J . Wang and V . Saligrama . Local supervised learning through space partitioning . In NIPS , pages 91–99 , 2012 .
[ 25 ] L . Ladicky and P . H . S . Torr . Locally linear support vector
[ 43 ] R . Wong . Asymptotic approximations of integrals . Computer machines . In ICML , pages 985–992 , 2011 .
[ 26 ] B . Letham , C . Rudin , T . H . McCormick , and D . Madigan .
An interpretable model for stroke prediction using rules and bayesian analysis . In KDD Workshop on Data Science for Social Good , 2014 . science and scientific computing . Academic Press , Boston , San Diego , 1989 . Includes indexes .
[ 44 ] Z . E . Xu , M . J . Kusner , K . Q . Weinberger , and M . Chen .
Cost sensitive tree of classifiers . In ICML , pages 133–141 , 2013 .
[ 27 ] H . Liu and X . Chen . Nonparametric greedy algorithms for
[ 45 ] J . Zhu and T . Hastie . Kernel logistic regression and the the sparse learning problem . In NIPS , pages 1141–1149 , 2009 .
[ 28 ] A . C . Lozano , G . Swirszcz , and N . Abe . Grouped orthogonal import vector machine . In NIPS , pages 1081–1088 , 2001 .
1254
