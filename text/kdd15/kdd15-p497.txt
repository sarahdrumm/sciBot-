Accelerated Alternating Direction Method of Multipliers
Mojtaba Kadkhodaie
Department of Electrical and
Computer Engineering University of Minnesota kadkh004@umn.edu
Konstantina
Christakopoulou
Department of Computer Science and Engineering University of Minnesota christa@csumnedu Arindam Banerjee Department of Computer Science and Engineering University of Minnesota banerjee@csumnedu
Maziar Sanjabi
Department of Electrical and
Computer Engineering University of Minnesota maz@umn.edu
ABSTRACT Recent years have seen a revival of interest in the Alternating Direction Method of Multipliers ( ADMM ) , due to its simplicity , versatility , and scalability . As a first order method for general convex problems , the rate of convergence of ADMM is O(1/k ) [ 4 , 25 ] . Given the scale of modern data mining problems , an algorithm with similar properties as ADMM but faster convergence rate can make a big difference in real world applications . In this paper , we introduce the Accelerated Alternating Direction Method of Multipliers ( A2DM2 ) which solves problems with the same structure as ADMM . When the objective function is strongly convex , we show that A2DM2 has a O(1/k2 ) convergence rate . Unlike related existing literature on trying to accelerate ADMM , our analysis does not need any additional restricting assumptions . Through experiments , we show that A2DM2 converges faster than ADMM on a variety of problems . Further , we illustrate the versatility of the general A2DM2 on the problem of learning to rank , where it is shown to be competitive with the state of the art specialized algorithms for the problem on both scalability and accuracy .
Keywords Alternating Direction Method of Multipliers , Ranking on the Top of the List
1 .
INTRODUCTION
In the past decade the advances in many areas , such as data mining and machine learning , has led to formulating many arising problems into an optimization formulation . Therefore , the proposed methodologies in these areas , require solving an optimization problem in their core and their applicability is dependent on solving such problems as fast and efficiently as possible . The proposed algorithms for solving such optimization problems should remain efficient as the size of the problem grows . This scalability criterion would cross out many traditional optimization methods such as interior point methods [ 21 ] and Newton method which are based on complicated iterations ( due to requiring the second order information and matrix inversion ) . An alternative approach is to use first order methods that have lower cost per iteration , but show slower convergence . Unfortunately , general first order methods [ 15 , 2 , 10 ] require projections to the problem solution set . Such projections , even for solution sets that are defined by simple linear constraints , can be intractable in high dimensions . Therefore the applicability of such methods is limited .
Our focus in this work is on an alternative procedure that helps us deal with linear equality constraints . The algorithm that we will discuss is called Alternating Direction Method of Multiplier ( ADMM ) which has a long history in literature . It was first proposed in [ 12 ] and has recently gained lots of attention [ 4 , 25 ] due to its simplicity and wide range of problems that it covers . Specifically , ADMM is designed for solving convex optimization problems of the form min x,y f1(x ) + f2(y ) subject to Ax + By = c
( 1 ) where x ∈ Rn1 , y ∈ Rn2 are the optimization variables , A ∈ Rm×n1 , B ∈ Rm×n2 are linear operators , c ∈ Rm is a vector of data , and finally f1 and f2 are closed convex functions . As the formulation suggests , the objective is separable across the variables , while the constraints are coupling the variables . Such coupling linear equality constraints are not easy to deal with in general .
ADMM [ 4 , 25 ] is an iterative method that uses a Gauss Seidel type update to solve ( 1 ) . Given a penalty parameter τ > 0 , ADMM minimizes the augmented Lagrangian function L(x , y , λ ) = f1(x)+f2(y)−λ , Ax+By−c+ c−Ax−By2
τ 2 with respect to x and y alternatively and then updates the dual variable λ ∈ Rm . Steps of ADMM are summarized in Algorithm 1 .
One of the main advantages of ADMM framework to other methods is its flexibility towards parallel computation [ 4 ] . As a result , in many applications it might be favorable to
497 cast an unconstrained optimization problem into a constrained form ( by introducing new variables ) and solve the resulting constrained formulation by ADMM in a parallel fashion ( for examples of this type of reformulation , see [ 4] ) .
Note that the effectiveness of ADMM depends on the simplicity of its updates for x and y in Algorithm 1 . There are other variations of ADMM that consider inexact updates for x and y in order to make the algorithm more tractable in practice ( for such inexact variants of ADMM see [ 25 , 4] ) .
The iteration complexity of ADMM has been extensively studied in the literature ( see [ 25 ] and the references therein ) . It is shown that the algorithm has O(1/k ) convergence rate [ 4 , 25 ] under some mild conditions on the problem . Recently , some variants of this algorithm were studied which exhibit faster convergence rates while requiring only a little change in the computational effort of each iteration [ 14 , 13 ] . The acceleration methods considered in these works are of the form first proposed by Nesterov [ 20 ] for gradient descent algorithms . Nesterov ’s accelerated gradient descent scheme in [ 20 ] was initially designed for solving unconstrained smooth convex problems and was shown to provide a O(1/k2 ) rate of convergence . His acceleration scheme has inspired many researchers to develop accelerated variants of other existing iterative methods ( for instance see [ 3 ] which proposes an accelerated variant of the proximal splitting method ) .
In [ 13 ] , the authors propose a Nesterov type acceleration of ADMM for problems of the form ( 1 ) in the special case where both A and B are identity matrices , and one of f1 or f2 is differentiable . Their accelerated scheme has O(1/k2 ) convergence rate and is based on the “ symmetric ” ADMM method , which differs from Algorithm 1 in that it involves two dual updates per iteration rather than one . The authors handle weakly convex problems by introducing a “ stepskipping ” process that applies the acceleration selectively on certain iterations . However , the step skipping process turns the algorithm to one with a more complicated sequence of steps than conventional ADMM . The major drawback of their analysis is that it requires the matrices A and B to be identity . Such assumption restricts the application of their accelerated version of ADMM . In a related work [ 14 ] , it was shown that by applying Nesterov ’s acceleration scheme , ADMM can have a O(1/k2 ) convergence rate provided that some assumptions hold true about the problem . The proposed accelerated method is simply ADMM with a predictor corrector type acceleration step . The convergence rate of this algorithm is analyzed in [ 14 ] under the assumptions that both objective terms are strongly convex and one of them is quadratic . These assumptions enable the authors to use a similar proof technique as in [ 20 ] to show fast convergence of their algorithm . Instead of analyzing the convergence rate of the algorithm in terms of decrease in the primal objective sequence , [ 14 ] considers the dual problem of ( 1 ) which involves maximizing the dual function
λ max 1 and f∗ where f∗ and f2 , respectively , defined as
D(λ ) def= −f
1 ( AT λ ) − f ∗
2 ( BT λ ) + λ , c ∗
( 2 )
2 are Fenchel conjugate functions [ 11 ] of f1
∗
F
( u ) = max v u , v − F ( v ) ,
( 3 ) for any closed convex function F . In the case where f1 and f2 are strongly convex , the conjugate functions turn out to be smooth with Lipschitz continuous gradients and hence the dual problem ( 2 ) simply becomes an unconstrained smooth convex optimization . As a result , if an accelerated gradient ascent method , as the one in [ 20 ] , is applied to the dual problem ( 2 ) , a O(1/k2 ) convergence rate will be obtained . However , since the ADMM algorithm exploits inexact gradient ascent type of update for the dual variable λ , a further technical condition needs to be satisfied in every iteration of the accelerated algorithm . Therefore , in order to prove the iteration complexity of accelerated ADMM , the authors in [ 14 ] require both functions f1 and f2 to be strongly convex as well as f2 to be quadratic .
In this paper , we introduce a novel algorithm called Accel erated Alternating Direction Method of Multipliers ( A2DM2 ) , and prove that the algorithm has a O(1/k2 ) convergence rate as long as f1 , f2 are strongly convex . In particular , unlike [ 13 ] , the functions need not be differentiable , and unlike [ 14 ] , neither of them needs to be quadratic . Further , the analysis works out without any restricting assumptions on the matrices A and B . The analysis technique is similar to the ones in [ 14 , 20 ] . To illustrate the versatility of the proposed A2DM2 , we consider the problem of learning to rank with emphasis on accuracy at the top of the list , and show how A2DM2 can be applied to the problem . Through extensive empirical evaluation on a wide variety of datasets , we illustrate that A2DM2 is competitive , often by an order of magnitude , to specialized algorithms designed for the ranking problem . We also show the generality and wide applicability of A2DM2 by highlighting other problems , including dirty statistical models and elastic net problems , where it is readily applicable .
The rest of the paper is organized as follows . In Section 2 , we introduce the Accelerated ADMM ( A2DM2 ) algorithm , and prove the O(1/k2 ) convergence rate of the algorithm . In Section 3 , we present the problem of learning to rank and illustrate how A2DM2 can be applied to solve the problem . In Section 4 , we present experimental results comparing A2DM2 with ADMM on synthetic data , dirty statistical models , and elastic nets . In Section 5 , we present extensive comparisons of A2DM2 on the learning to rank problem with the state of the art and basic ADMM in terms of both optimization time and accuracy . We conclude in Section 6 .
2 . ACCELERATED ADMM ALGORITHM In this section we introduce our accelerated ADMM algorithm , which we call A2DM2 , for solving ( 1 ) . First , we need to assume strong convexity of f1 , and f2 with corresponding ∈ Rni constants σ1 and σ2 , ie for i = 1 , 2 and every x , x 2 , ∀ g ∈ ∂fi(x fi(x ) − fi(x
) , where ∂fi(· ) denotes the sub differential set of fi . As a result of strong convexity of fi , i = 1 , 2 , the conjugate function , defined in ( 3 ) , would have a Lipschitz continuous gradient with constant 1/σi , i = 1 , 2 .
) ≥ g , x − x x − x
+
σ2 i 2
Now we are ready to define the A2DM2 algorithm . Our accelerated ADMM algorithm uses Nesterov ’s method to extrapolate the update of λ in each iteration of ADMM . In order to guarantee the convergence , it is required to update the variable y based on this extrapolated version of λ . The overall algorithm is summarized in Algorithm 2 . Compared to the conventional ADMM algorithm , our method requires extrapolating λ and updating y twice ; therefore , each iteration for accelerated ADMM would be more costly than the usual
498 Algorithm 1 : ADMM
Require : y0 ∈ Rn2 , λ0 ∈ Rm , τ > 0 1 . for k = 0 , 1 , 2 , 3,··· do 2 . 3 . 4 . 5 . end for xk+1 = argminxL(x , yk , λk ) yk+1 = argminyL(xk+1 , y , λk ) λk+1 = λk − τ ( Axk+1 + Byk+1 − c )
ADMM . But as we will see in the numerical experiments , in many scenarios this extra cost is negligible compared to the speed up that is gained using accelerated ADMM .
One appealing feature of ADMM , which makes it suitable for large scale problems , is the capability of being executed on parallel machines . Similar to ADMM , A2DM2 is also parallelizable . This is because the primal and dual updates of A2DM2 include the ones for ADMM ( steps 2 4 of Algorithm 2 ) . The extra update in A2DM2 for the dual variable ˆλ ( step 6 ) is an element wise operation , which is easy to parallelize . Moreover , the additional variable ˆy is iteratively set to the minimizer of the associated objective term f2 augmented by a linear function . This can also be done in parallel provided that f2 is decomposable across variables .
Regarding the convergence rate of the algorithm , the fol lowing theorem states O(1/k2 ) convergence of A2DM2 .
Theorem 1 . Suppose that f1 and f2 are strongly convex with σ1 , σ2 . Moreover , assume that τ 3 ≤ ρ(AT A)ρ2(BT B ) , where ρ(· ) denotes the maximum singular value of the matrix . Then the iterates λk generated by Algorithm 2 would satisfy
σ1σ2 2
∗
) − D(λk ) ≤ 2ˆλ1 − λ∗2 τ ( k + 2)2 ,
D(λ
( 4 ) where λ∗ is an optimal solution of the dual problem ( 2 ) .
Proof . For an optimal Lagrange multiplier λ∗ of ( 2 ) , define sk = akλk − ( ak − 1)λk−1 − λ∗ . Then using the following lemma helps us establish Theorem 1 .
Lemma 1 . For the sequence sk defined as sk = akλk −
( ak − 1)λk−1 − λ∗ the assumptions of Theorem 1 imply sk+12 − sk2 ≤ 2a2 − 2a2 kτ ( D(λ k+1τ ( D(λ
∗
) − D(λk ) ) ∗
) − D(λk+1) ) .
( 5 )
Now using Lemma 1 , it is easy to see that 1
2a2 k+1τ ( D(λ
∗
) − D(λk+1 ) ) ≤ 2a2 kτ ( D(λ
∗
) − D(λk ) ) + sk2 .
Furthermore , rewriting ( 5 ) and using induction , it is easy to
1Lemma 1 can be proved in a similar way as Lemma 5 of [ 14 ] except that since f2 is not restricted here to be a quadratic function , we need our Lemma 2 to complete the proof .
Algorithm 2 : Accelerated ADMM ( A2DM2 )
Require : y0 = ˆy0 ∈ Rn2 , λ0 = ˆλ0 ∈ Rm , τ > 0 , a0 = 1 1 . for k = 0 , 1 , 2 , 3,··· do 2 .
3 .
4 .
5 .
6 . xk = argminxL(x , ˆyk , ˆλk ) yk = argminyL(xk , y , ˆλk ) λk = ˆλk − τ ( Axk + Byk − c )
√
1+
1+4a2 k 2 ak+1 = ˆλk+1 = λk + ak−1 ˆyk+1 = argminyf2(y ) + ˆλk+1,−By
( λk − λk−1 ) ak+1
7 . 8 . end for see that sk2 + 2a2
∗ kτ ( D(λ s12 + 2a2
) − D(λk ) ) ≤ ∗
1τ ( D(λ
) − D(λ1) ) , ∀ k .
( 6 )
Now in order to prove the result , we need the following lemma , for which the proof is relegated to the appendix .
Lemma 2 . When the conditions of Theorem 1 are satis fied , then for any γ ∈ Rm , ∀k , D(λk+1 ) − D(γ ) ≥ 1 τ
γ − ˆλk+1 , ˆλk+1 − λk+1
+
1 2τ
λk+1 − ˆλk+12 . ( 7 )
Applying Lemma 2 with k = 0 and γ = λ∗ , we get D(λ1 ) − D(λ
∗
γ − ˆλ1 , ˆλ1 − λ1 + ) ≥ 1 τ 1 2τ
λ1 − λ
∗2 − ˆλ1 − λ
1 2τ
λ1 − ˆλ12 ∗2
=
( 8 ) Combining ( 6 ) and ( 8 ) plus using definition of s1 = λ1 − λ∗ yields
.
2a2 kτ ( D(λ
∗
) − D(λk ) ) ≤ ˆλ1 − λ
∗2 .
Note that we have ignored the term sk2 ≥ 0 on the left In order to get the final result , note hand side of ( 6 ) . 2 . Thus , D(λ∗ ) − D(λk ) ≤ that ak > ak−1 + 1 2 > 1 + k 2ˆλ1−λ∗2 τ ( k+2)2
.
In the convergence analysis of ADMM , primal and dual residuals play an important role [ 25 , 14 ] . For the accelerated ADMM algorithm , the primal and dual residuals can def= b − Axk − Byk = λk − ˆλk and also be defined as rk def= AT B(yk − ˆyk ) , respectively . It can be shown that for dk A2DM2 these residuals will decrease in O(1/k2 ) ( The proof is similar to Lemma 6 of [ 14 ] . However , because of the different update rule for ˆyk , it can be shown through Lemma ( 1 ) that f2 is no longer needed to be quadratic . ) As we will see in the next section we can use these residuals to propose another variant of the accelerated ADMM . 2.1 Accelerated ADMM with Restarting
Here we introduce a variant of A2DM2 to address the issue of possible spiral movements around the optimal solution , which is quite common among accelerated algorithms
499 [ 9 ] . One common way to reduce such movements is to use a restarting rule . Similar to [ 14 ] , in order to find out when the good time to restart is , we use the sum of two terms , which are proportional to the primal and dual residuals respectively , mk def=
1 τ
λk − ˆλk2 + τB(yk − ˆyk)2 .
( 9 )
At every iteration of the accelerated algorithm with restarting , which we often refer to as A2DM2+Restart , we compare mk with mk−1 and if mk > η mk−1 , where 0 < η < 1 is a constant close to one , we restart the method by setting ak+1 = 1 , ˆyk+1 = yk and ˆλk+1 = λk .
Interestingly , our empirical studies show that while in some cases restarting really helps to improve the performance of A2DM2 ( see section 4 ) , in others its performance is inferior ( see section 5 ) .
3 . TOP RANKING OPTIMIZATION
Now we focus on using the A2DM2 framework to solve the problem of ranking on the top of a list . The goal is to provide an alternative optimization solution for pushing down the highest ranked negative example in the ranked list . In this section , we appropriately reformulate the TopPush optimization problem [ 18 ] , which is the most efficient pairwise ranking solution up to date , and derive its updates within the A2DM2 framework . This results in an algorithm competitive with the TopPush algorithm of [ 18 ] , in terms of both ranking performance and computational efficiency . 3.1 Related Work
Ranking problems are prevalent in a wide range of domains where a long list of objects , such as web links or products , needs to be ranked . Typically , in such scenarios , what matters the most is the quality of ranking near the top of the ranked list . Towards this direction , a number of learning to rank algorithms which put more emphasis towards the top of the ranked list have been developed ( see [ 5 , 6 , 7 , 19 , 22 ] and the references therein . )
One main group of ranking algorithms aims to optimize a convex upper bound of specific metrics which look at the top of the ranked list . Examples of such metrics are Average Precision and Normalized Discounted Cumulative Gain [ 17 , 6 ] . Another line of work was initiated by the so called p Norm Push ranking method [ 23 ] which optimizes a novel measure that concentrates harder on the high ranked negative examples and pushes them down . Extending [ 23 ] , Infinite Push [ 1 ] seeks to push down the top irrelevant item , by minimizing the maximum number of positive examples ranked below any negative . Both [ 23 ] and [ 1 ] are pairwise ranking approaches , thus inheriting the downside of computational cost proportional to the number of positive negative pairs , which is prohibitive for large datasets . Recently , [ 18 ] addressed this issue by reformulating the Infinite Push objective as the number of positive examples ranked below the highest ranked negative . This results in a pairwise ranking algorithm , named TopPush , with time complexity linear in the number of training instances . 3.2 Bipartite Ranking
Consider the bipartite setup for ranking in which the samples are either relevant ( positive ) or irrelevant ( negative ) . Assume that X ⊆ Rd is the instance space and that we are m i=1
I m i=1
1 ,··· , x+
1 ,··· , x− − m ) and S− = ( x given the training sample S = ( S+ , S− ) ∈ X m × X n , where S+ = ( x+ n ) are positive and negative samples , respectively . As in [ 18 ] , our goal is to learn a ranking function f : X → R that maximizes the number of positive instances that are ranked higher than the topranked negative sample . In other words , the ranking task is translated into minimizing the following loss over the choice of f
L(f ; S ) =
1 m f ( x+ i ) ≤ max 1≤j≤n
− j ) f ( x
( 10 ) where I(· ) is the indicator function which equals one if the input argument is true and zero otherwise . Here , we restrict ourselves to the class of linear scoring functions , ie f ( x ) = wT x for some weight vector w ∈ Rd . Since the indicator function I(· ) is not convex , [ 18 ] suggests replacing it with a convex loss function ( · ) and then solves the following problem min w∈Rd
λ 2 w2 +
1 m
( max 1≤j≤n wT x j − wT x+ − i )
( 11 )
2 w2 is added to avoid overwhere the regularization term λ fitting . The loss function ( · ) is further assumed to be nondecreasing and differentiable . When the loss is the truncated quadratic loss , ie ( z ) = ( [1 + z]+)2 , the authors of [ 18 ] solve the dual of ( 11 ) by using an accelerated gradient projection algorithm . Instead , A2DM2 solves ( 11 ) in its primal form . Even though in the sequel we restrict ourselves to the truncated quadratic loss , our framework is general enough to incorporate any other appropriate loss function . 3.3 A2DM2 for Ranking
We first illustrate how ADMM can be applied to solving
( 11 ) . Define a def= maxj wT x
− j , then ( 11 ) can be cast as min w∈Rd,a∈R w2 +
λ 2 subject to a = max 1≤j≤n
1 m wT x
− j
( a − wT x+ i )
Since the above constraint is not linear in w , we need to j − a , j = − define further extra variables . Let sj 1,··· , n . Note that sj has to be non positive since a is , by − definition , the maximum of all linear combinations wT x j for j = 1,··· , n . Moreover , let bi i , i = 1,··· , m and then the above problem translates to def= a− wT x+ def= wT x min w∈Rd,a∈R,b∈Rm subject to
( bi ) m 1 m j − a , 1 ≤ j ≤ n − i , 1 ≤ i ≤ m i=1 w2 +
λ 2 sj = wT x bi = a − wT x+ sj ≤ 0 , 1 ≤ j ≤ n .
To write the constraints in a compact form , we stack the negative and positive samples as rows of X−X− X− and X +X +X + , respectively , and let X−X− 1 )T ; ( x+ n )Tfi ∈ Rn×d m)Tfi ∈ Rm×d . Also defin and X +X +X + def=.(x+
− 1 )T ; ( x 2 )T ; . . . , ( x+
X− def=.(x ing the vector s = [ s1 , s2,··· , sn]T ∈ Rn , the problem be
− 2 )T ; . . . ; ( x−
500 ( a ) Primal Residual
( b ) Primal Objective Optimality Distance
( c ) Dual Objective Optimality Distance
Figure 1 : Convergence behavior of ADMM , A2DM2 and A2DM2 + Restart for the Elastic net with 1 regularization problem . A2DM2 performs better than the ADMM . ADM2+Restart has the best performance . comes min w∈Rd,a∈R b∈Rm,s∈Rn− subject to w2 +
λ 2
1 m m
( bi ) i=1 − − − a111n + s = X X X b = a111m − X +X +X +w , s ≤ 0 w
( 12 ) where 111m and 111n are all one vectors of lengths m and n . Introducing dual variables γ1 ∈ Rn and γ2 ∈ Rm corresponding to the first and second sets of linear constraints , we can formulate the augmented Lagrangian as
L(w , a , b , s , γ1 , γ2 ) =
( bi ) m
1 m w2 + λ 2 1 ( a111n + s − X + γT X X a111n + s − X − − − ρ X X 2
+ i=1 2 ( b − a111m + X +X +X +w ) − − − w ) + γT w2 + b − a111m + X +X +X +w2 ρ 2 where ρ > 0 is a penalty parameter . Then the ADMM algorithm will consist of the following steps . At every iteration :
Step 1 . Update ( w , a ) according to
( w,a )
( w , a ) ← arg min
λ 2 2 ( −a111m + X +X +X +w ) + + γT b − a111m + X +X +X +w2 ρ 2
+
ρ 2 w2 + γT
− − −
1 ( a111n − X X X w ) w2 − − − a111n + s − X X X which is a convex quadratic function with respect to ( w , a ) . Let us define
∈ R(n+m)×(d+1 ) .
−1
ρb + γ2
ρs + γ1
AT
0 0
Then the update of ( w , a ) will be
A def=
← − w a
111n
−XXX−
X +X +X + −111m λId m
ρAT A +
0
Step 2 . Update ( b , s ) according to
( b , s ) ← arg min ( b,s),s≤0 a111n + s − X − − − X X
+
1 m w2 + i=1
ρ 2
( bi ) + γT
1 s + γT 2 b b − a111m − X +X +X +w2
ρ 2
The variable s is simply updated as follows s ←
X ( X X
− − − w − a111n ) − 1 ρ
γ1
− where [ ·]− stands for projection on the negative orthant . Depending on the choice of the loss function ( · ) , the update of b may vary . For the case of the truncated quadratic loss , the update has closed form . More specifically , for i = 1 , 2,··· , m , let ci def= a − ( X +X +X +w)i − 1 ci bi ←
,− 2
1
ρ+ 2 m m + ρci
ρ ( γ2)i , then if ci ≤ −1 if ci > −1 .
Step 3 . Update ( γ1 , γ2 ) according to γ1 ← γ1 + ρ(a111n + s − X − − − X X γ2 ← γ2 + ρ(b − a111m + s + X +X +X +w ) w )
As shown by Theorem 1 , A2DM2 requires strong convexity of the objective function with respect to both ( w , a ) and ( b , s ) pairs . In order to make this condition hold , we add extra quadratic terms to the ranking objective function in equation ( 12 ) and change it to w2 +
λ 2
1 m
( bi ) +
λ1 2 a2 + s2 +
λ2 2 b2
λ2 2
( 13 ) m i=1 where λ1 and λ2 are small positive constants . Although adding these terms will slightly change the problem , our experimental results show the ranking performance of the algorithm is not worse than the existing state of the art approaches . We avoid deriving the iterative updates of A2DM2 here as they are quite similar to ADMM and instead summarize them in Algorithm 3 . 3.4 Computational Complexity
Updating the pair ( w , a ) requires O(d(m + n ) ) operations AT provided that the matrix H def=
λId
−1
ρAT A +
0 0
0 is computed before executing the algorithm and saved in memory . The computational cost of updating ( b , s ) and ( γ1 , γ2 ) is of the same order O(d(m + n) ) . Therefore , in √ order to achieve an accuracy solution , the ADMM and A2DM2 require O(d(m + n)/ ) and O(d(m + n)/ ) operations , respectively . Therefore , in terms of the computational complexity , A2DM2 is comparable with the state of the art algorithm in [ 18 ] .
010020030040050010−1510−1010−5100105Number of IterationsPrimal Residual ADMMFast ADMMFast ADMM + Restart Number of Iterations0100200300400500Log(Relative Error) 30 25 20 15 10 50ADMMA2DM2A2DM2 + Restart02004006008001000−40−35−30−25−20−15−10−50Number of IterationsLog(Relative Error ) ADMMA2DM2A2DM2 + Restart501 ( a ) Objective value vs . number of iterations
( b ) Top@Pos vs . number of iterations
Figure 2 : Study on spambase dataset . ( a ) Objective value versus number of iterations ( b ) Top Ranking performance ( Pos@Top ) on the test set after every 100 iterations of the training phase . A2DM2 converges to its final ranking performance after few hundreds of iteration , whereas TopPush [ 18 ] does not seem to achieve a stable number of Pos@Top .
Algorithm 3 : A2DM2 for Ranking
Require : b = ˆb , s = ˆs , γ1 = ˆγ1 , γ2 = ˆγ2 , λ , λ1 , λ2 , ρ , t0 = 1 1 . for k = 0 , 1 , 2 , 3,··· do ρAT A +
ρˆb + ˆγ2
λId
−1
← −
AT
2 .
0 λ1
0
ρˆs + ˆγ1 c ← a111n − X +X +X +w − 1
ρ ˆγ2 a ci
ρ+λ2 bi ←
ρ+λ2 1 ρ+ 2 m +λ2 w s ← ρ ρ ˆγ1
γ1
γ2  −(ˆγ2)i [ −ˆγ1]−
γ1 ˆγ1
← ˆγ2 ˆs ← 1 bi ←
( ˆγ2)i+ 2 m λ2+ 2 m
←
λ2
λ2
γ2
ˆγ2 √ 7 . t0 ← t , t ← 1+
3 .
4 .
5 .
6 .
8 .
9 .
10 .
X−w − a111n ) − 1 ( X−X−
ˆγ1
ρ+λ2
−
,− 2 m + ρci if ci ≤ −1 if ci > −1 . a111n + s − X−X− X−w b − a111m + s + X +X +X +w
+ ρ
1+4t2 0 2
+ t0−1 t
γ1 − γ0
γ2 − γ0
1
2 if
−(ˆγ2)i
λ2
≤ −1 otherwise .
7 . end for
4 . A2DM2 FOR DIRTY MODELS
In this section we demonstrate the effectiveness of our method for solving an extensively studied family of statistical problems that are often known as “ Dirty Models ” in the machine learning literature [ 27 , 8 ] .
Dirty Models . In many high dimensional statistical problems , the number of observations is far less than the dimension of the model to be estimated . Without any prior knowledge , the true model is not identifiable . Fortunately , in many practical applications , the model is known to have a low dimensional structure that can be used to resolve the identifiability issue . This prior knowledge can be exploited by adding to the objective function some appropriate con vex regularizers which capture the structure of the model . Formally speaking , assume that given the linear observations b = Ax + By , where A , B are known matrices , the goal is to estimate x and y . In addition , let us assume that R1(· ) and R2(· ) are the convex penalty functions which encode the prior knowledge of x and y , respectively . Then , the estimation problem can be formulated as follows min x,e
R1(x ) + R2(y ) subject to Ax + By = b .
( 14 )
Many famous formulations can be easily interpreted using 2y2 this model . For example , by specializing R2(y ) = 1 ( the 2 norm squared ) , R1(x ) = x1 ( the 1 norm ) , A as the design matrix and B as the identity matrix , we obtain the Lasso formulation [ 24 ] . When working with real world data , in order to increase the robustness of the estimation procedure , the elastic net regularizer , given by R1(x ) = x1 + 1 2x2 , can be used instead of the 1 norm penalty [ 28 ] .
Dirty Model with Elastic Net Regularizer . Another interesting special case of problem ( 14 ) is where R1 and R2 are both elastic net regularization functions . Given the linear observation model b = Ax + y , such a problem can be written as min x,y
F ( x , y ) def= x1 +
1 2 subject to Ax + y = b . x2 + µ(y1 + y2 )
( 15 )
1 2 where µ > 0 is a constant . Minimizing the 1 norm of the variables here is to exploit the prior knowledge about the sparsity of such unknowns [ 26 , 16 ] . This problem has the 2x2 and f2(y ) = same form as ( 1 ) with f1(x ) = x1 + 1 µ(y1 + 1 2y2 ) . Note that the objective function components f1 and f2 are strongly convex in terms of x and y , respectively . Therefore , our analysis guarantees the fast convergence of A2DM2 for this problem . The application of A2DM2 to this problem is quite straight forward . However , we omit the details of the variable updates here due to space limits . In the next section,we describe the results of our numerical tests with this problem .
Numerical Experiments . To test the performance of our accelerated method compared to ADMM , we carry out a set of simulations . We randomly generate the observation matrix A ∈ Rm×n of size m = 28 and n = 29 from a standard
Iteration Counter01000200030004000500060007000Objective Value005115225335445lambda=10 , eta=0.8TopPushADMMA2DM2A2DM2 + Restart100*Iteration02004006008001000Top Positives020406080100120Top Positives versus Number of IterationsTopPushADMMA2DM2A2DM2 + Restart502 Gaussian distribution . The true target vector x is sparse with only five percent of its entries being non zero . The non zero entries are standard Gaussian . The error vector e ∈ Rm is generated from an exponential distribution with average 001 Figure ( 1 ) shows the convergence behavior of ADMM , A2DM2 , and A2DM2 + Restart for solving problem ( 15 ) . Figure 1(a ) plots the primal residual sequence rk = Axk−b−yk , Figure 1(b ) shows the primal objective optimality gap F ( xk , yk ) − F ( x∗ , y∗ ) and Figure 1(c ) shows the dual objective optimality gap D(λ∗ ) − D(λk ) . Both of the objective sequences are normalized with their initial values , ie with their values at iteration k = 1 . As the three figures suggest , A2DM2 performs better than ADMM in terms of all three measures . The best performance is observed for A2DM2+Restart . Note that the improved performance is obtained at the cost of defining the auxiliary primal and dual variables ˆy and ˆλ whose updates can be done in O(m ) ( the details are omitted for the sake of brevity ) .
5 . EXPERIMENTS ON TOP RANKING 5.1 Settings
Datasets . To evaluate the performance of the proposed A2DM2 algorithm on the problem of learning to rank , we conduct a set of experiments on various datasets . The left column of Table 1 summarizes the datasets used in our experiments . All datasets used are publicly available binary classification datasets2 having varying sizes and coming from different domains .
Some datasets come from the medical domain ( breastcancer , diabetes ) , ecology ( covtype ) , biology ( cod rna , splice ) , others from email spam filtering ( spambase ) , web data ( w8a ) , census data ( a9a ) , and credit card approval ( australian ) . Also , competition data on generalization ability and text decoding ( ijcnn1 ) were used . The epsilon dataset is an artificial data set from the Pascal large scale learning challenge 2008 .
Setup & Parameters . On each dataset , we run experiments for ten trials and report the averaged results over those trials . In each run , the dataset is randomly divided into two subsets : 2/3 for training and 1/3 for test . For all algorithms , we set the precision parameter to 10−4 , choose other parameters by 3 fold cross validation ( based on the average value of Pos@Top ) on training set , and perform the evaluation on the test set . In particular , the regularization parameter λ is chosen from {10−4 , 10−3 , 10−2,10−1 , 100 , 101 , 102} based on cross validation on the TopPush algorithm . The parameters λ1 , λ2 were set to the value 001 The step size ρ of the proximal operator in the ADMM based algorithms was cross validated as followed : For ADMM ρ was chosen from the set {10−4,10−3,10−2,10−1 , 1 , 10} , for A2DM2 from {10−4,10−3,10−2} and for A2DM2+Restart from {10−4,10−3 , 10−2,10−1 , 1 , 10} .
Methods . We compared the standard ADMM , the proposed A2DM2 and A2DM2+Restart frameworks with TopPush [ 18 ] , which is the state of the art top rank algorithm . We implemented the ADMM based algorithms in MATLAB and used the publicly available source code for TopPush .
Top Ranking Metric . Since the objective of TopPush , and therefore of all compared approaches , is to push down the top ranked negative example in the ranked list , a natural performance measure is the number of positives ranked on top of the first negative example ( Pos@Top ) [ 1 ] . Larger values in this metric imply better top ranking performance .
Training Efficiency . In order to evaluate the computational efficiency of our proposed algorithms , we also report the average time ( in seconds ) it takes for the algorithms to be trained . For this experiment , we set the parameters of the different algorithms to be the best ones selected by cross validation and we run them on the training set . We do so for the ten different random runs and average out the training time . Regarding the stopping criterion , all three algorithms ie , ADMM , A2DM2 and A2DM2+Restart , are stopped when the iteration number is greater than 10 and the sum of the primal and dual residuals is less than . The TopPush stopping criterion is kept in its original form as given in the source code3 , ie , when the iteration number is greater than 10 and the relative dual objective gap of the TopPush algorithm is less than . 5.2 Results : Running Time Comparison
In the right most column of Table 1 we report the training performance ( in seconds ) of the algorithms A2DM2 , A2DM2 + Restart , compared to the TopPush algorithm and the standard ADMM . One can observe that in most datasets A2DM2 matches the training time of TopPush , and it can be even one order of magnitude faster ( spambase ) . In the cases where A2DM2 is slower than TopPush , A2DM2 achieves better ranking performance ( diabetes , a9a ) . In general , there is a tradeoff between accuracy at the top and time for convergence . A2DM2 usually manages to balance the tradeoff and achieves good Pos@Top at time comparable ( or better ) with TopPush , while ADMM often does not . 5.3 Results : Top Ranking Accuracy
In addition , in Table 1 we report the performance of the compared approaches in terms of the average Pos@Top . The A2DM2 algorithm almost always matches the ranking performance of TopPush and in most datasets it results in slightly better results .
From the results of Table 1 , we observe that as the size of the datasets increases at the bottom of the table , the acceleration that A2DM2 provides compared to ADMM becomes more considerable . For instance , for the cod rna dataset , the value of Pos@Top for A2DM2 is around twice that for ADMM and yet A2DM2 is , in average , two orders of magnitude ( 100 times ) faster than ADMM . Also , the results of ADMM for the two larger datasets , epsilon and covtype , are missing from Table 1 since the cross validation study for ADMM was time consuming .
For the spambase dataset , for a single random trainingtest split , Figure 3 ( b ) shows the Receiver Operating Characteristic ( ROC ) curves of the four compared algorithms . Since we focus on the ranking model where accuracy at the top is critical , good performance in the left most part of the ROC curve is necessary . In this regard , one can see similar ranking performance of the compared approaches . In detail , A2DM2 + Restart achieves slightly higher Pos@Top
2http://wwwcsientuedutw/~cjlin/libsvmtools/datasets/
3http://lamdanjueducn/code TopPush.ashx
503 Data breast cancer
239 / 444 d:10 australian
307 / 383 d:14 diabetes
500 / 268 d:34 spambase
1,813 / 2,788 d:57 splice
1,648 / 1,527 d:60 ijcnn1
4,853 / 45,137 d:22 a9a
11,687 / 37,155 d:122 w8a
1,933 / 62,767 d:300 covtype
283,301 / 297,711 , d : 54 cod rna
162,855 / 325,710 d:8 epsilon
249,778 / 250,222 d:2,000
Algorithm TopPush ADMM A2DM2
A2DM2+Restart
TopPush ADMM A2DM2
A2DM2+Restart
TopPush ADMM A2DM2
A2DM2+Restart
TopPush ADMM A2DM2
A2DM2+Restart
TopPush ADMM A2DM2
A2DM2+Restart
TopPush ADMM A2DM2
A2DM2+Restart
TopPush ADMM A2DM2
A2DM2+Restart
TopPush ADMM A2DM2
A2DM2+Restart
TopPush A2DM2 TopPush ADMM A2DM2
A2DM2+Restart
TopPush A2DM2
Pos@top
4.90×101 ± 1.39×101 5.33×101 ± 1.38×101 5.25×101 ± 1.48×101 5.23×101 ± 1.50×101 1.13×101 ± 5.14×100 1.62×101 ± 7.18×100 1.77×101 ± 1.08×101 1.71×101 ± 6.76×100 1.41×101 ± 2.14×101 2.36×101 ± 2.03×101 3.19×101 ± 1.73×101 1.87×101 ± 1.89×101 5.49×101 ± 8.29×101 4.48×101 ± 3.86×101 5.02×101 ± 3.64×101 5.48×101 ± 3.55×101 8.78×101 ± 4.85×101 9.99×101 ± 2.22×101 1.15×102 ± 2.63×101• 1.14×102 ± 2.70×101• 6.08×101 ± 2.06×101 1.24×102 ± 3.76×101• 5.56×101 ± 9.90×100 7.34×101 ± 3.20×101 1.78×101 ± 1.30×101 4.57×101 ± 2.19×101 5.47×101 ± 2.89×101 5.07×101 ± 2.43×101 1.39×102 ± 3.42×101 1.37×102 ± 4.19×101 1.38 × 102± 4.98×101 1.44×102 ± 3.86×101 7.97×102 ± 1.52×102• 2.63 × 101 ± 2.48 × 101 1.97×102 ± 9.95×101 1.03×102 ±1.36×102 2.24 × 102 ± 8.91 × 101 1.27 × 102 ± 8.38 × 101 1.82 × 103 ± 3.07 × 102 2.07 × 103 ± 4.16 × 102
Time ( sec . ) 5.58×10−1 ± 8.01×10−1 1.74×100 ± 1.46×100 9.24×10−1 ± 1.08×100 3.15×100 ± 3.53×100 2.22×10−1 ± 2.00×10−1 5.91×10−1 ± 4.79×10−1 5.12×10−1 ± 5.54×10−1 1.34×100 ± 2.05×100 4.66×10−2 ± 9.08×10−2 4.58×10−1 ± 2.51×10−1 5.03×10−1 ± 2.41×10−1 6.17×10−1 ± 4.60×10−1 1.63×101 ± 9.74×100 2.06×101 ± 1.59×101 3.35×100 ± 1.23×100 1.51×101 ± 8.07×100 1.86×100 ± 2.58×100 7.29×100 ± 5.02×100 3.18×100 ± 9.78×10−1 1.43×101 ± 2.13×100 7.39×100 ± 1.26×101 1.82×102 ± 6.77×101 1.50×100 ± 2.32×100 1.13×102 ± 1.39×102 8.50×10−1 ± 1.85×10−1 1.36×101 ± 8.59×100 1.44×101 ± 6.74×100 5.29×101 ± 3.43×101 2.20×101 ± 1.33×101 1.71×102 ± 6.28×101 5.98×101 ± 2.58×101 2.25×102 ± 7.54×101 1.78 ×101 ±4.26 × 100 2.02 × 101 ± 2.81 × 100 8.34 ×102 ±4.59 × 102 6.50×102 ± 7.33 × 102 2.01 × 100 ± 5.52 × 10−1 1.71 × 102 ± 4.75 × 102 5.78 × 102 ± 1.85 × 102 4.16 × 102 ± 1.79 × 101
Table 1 : Data statistics ( left column ) and experimental results . The mean and standard deviation of the training time ( sec ) and the Pos@Top over ten random splits of training test sets are reported . For each dataset , the number of positive and negative instances is below the data name as m/n , together with dimensionality d . For training time comparison , one or more algorithms are marked as if they are at least an order of magnitude faster compared to the remaining ones . For top ranking performance ( Pos@Top ) comparison , the entries marked with • are those for which the number of positives at top is at least 10 times greater than the Pos@Top achieved by the rest of the algorithms . In most datasets , one can observe that A2DM2 is very competitive with TopPush in terms of the order of magnitude for both top ranking accuracy and training time .
504 ( a ) Residual Plots
( b ) ROC curve
Figure 3 : Study on spambase dataset . ( a ) Residuals decay faster for the accelerated variants of ADMM compared to ADMM . ( b ) ROC Curve for test data : One can observe similar top ranking performance for the four approaches . performance followed by TopPush and A2DM2 and finally ADMM .
5.4 Effect of number of training iterations
In this subsection , we study the ranking performance of A2DM2 versus the number of iterations through some figures and compare it with TopPush , A2DM2 + Restart and ADMM . The shown plots are just for one random trainingtest split of the spambase dataset . However , we observed same trends as what follows with the other datasets and with different training test splits .
Fixing the regularization parameter λ = 10 and η = 0.8 , Figure 2 ( a ) shows how the value of the objective evolves as the number of iterations grows . One can observe that ADMM , A2DM2 and A2DM2+Restart converge in only a few hundred iterations . In contrast , TopPush needs to run for a few thousands of iterations to reach the optimal objective value . In Figure 2 ( b ) , we present how the number of Pos@Top in the test set evolves after every 100 iterations of the training phase . One interesting observation is that A2DM2 converges to its final test top ranking performance after few hundreds of iteration , whereas TopPush does not seem to achieve a stable number of Pos@Top . Figure 3 ( a ) shows how the sum of the primal and dual residuals behaves vs . the number of training iterations . As the figure implies , the residuals decay faster for the accelerated variants of ADMM .
6 . CONCLUSIONS
In this paper , we propose an Accelerated Alternating Direction Method of Multipliers , named A2DM2 . We prove that it has O(1/k2 ) convergence rate when the objective terms are strongly convex . This guarantees a faster convergence rate compared to ADMM [ 4 ] . A large number of real world machine learning problems formulated under the ADMM framework can benefit from this improvement . We illustrate the applicability of A2DM2 on the problem of learning to rank , and show that it is competitive with the state of the art TopPush algorithm [ 18 ] both in terms of ranking accuracy at the top and training efficiency .
7 . ACKNOWLEDGEMENTS
This work was supported in part by NSF ( IIS 1447566 , IIS 1422557,CCF 1451986 , CNS 1314560 , IIS 0953274 , IIS
1029711 ) , NASA ( NNX12AQ39A ) , and a gift from IBM and Yahoo! . 8 . REFERENCES [ 1 ] S . Agarwal . The infinite push : A new support vector ranking algorithm that directly optimizes accuracy at the absolute top of the list . In SDM , 839–850 , 2011 .
[ 2 ] A . Beck and M . Teboulle . Mirror descent and nonlinear projected subgradient methods for convex optimization . Operations Research Letters , 31(3):167–175 , 2003 .
[ 3 ] A . Beck and M . Teboulle . A fast iterative shrinkage thresholding algorithm for linear inverse problems . SIAM Journal on Imaging Sciences , 2(1):183–202 , 2009 .
[ 4 ] S . Boyd , N . Parikh , E . Chu , B . Peleato , and
J . Eckstein . Distributed optimization and statistical learning via the alternating direction method of multipliers . Foundations and Trends® in Machine Learning , 3(1):1–122 , 2011 .
[ 5 ] S . Boyd , C . Cortes , M . Mohri , and A . Radovanovic .
Accuracy at the top . In NIPS , 953–961 , 2012 .
[ 6 ] C . JC Burges . From RankNet to LambdaRank to
LambdaMART : An overview . In Learning , 11 : 23–581 , 2010 .
[ 7 ] Z . Cao , T . Qin , T Y Liu , M F Tsai , and H . Li .
Learning to rank : from pairwise approach to listwise approach . In Yahoo! Learning to Rank Challenge , 1–24 , 2011.9
[ 8 ] I . Dhillon and J . Ghosh . Dirty statistical models . [ 9 ] B . O’Donoghue and E . Candes . Adaptive restart for accelerated gradient schemes . Foundations of computational mathematics , 15.3:715 732 , 2012 .
[ 10 ] J . Duchi and Y . Singer . Efficient online and batch learning using forward backward splitting . The Journal of Machine Learning Research , 10:2899–2934 , 2009 .
[ 11 ] W . Fenchel . On conjugate convex functions . Canad . J .
Math , 1(73 77 ) , 1949 .
[ 12 ] D . Gabay and B . Mercier . A dual algorithm for the solution of nonlinear variational problems via finite element approximation . Computers & Mathematics with Applications , 2(1):17–40 , 1976 .
[ 13 ] D . Goldfarb , S . Ma , and K . Scheinberg . Fast alternating linearization methods for minimizing the
10010110210310 2100102104106Residual PlotsADMMA2DM2A2DM2 + RestartFalse Positives02004006008001000True Positives0100200300400500600700ROC Curve for Test DataTopPush : Pos@top=76ADMM : Pos@top=62A2DM2 : Pos@top=75A2DM2+Restart : Pos@top=79505 sum of two convex functions . Mathematical Programming , 141(1 2):349–382 , 2013 .
[ 14 ] T . Goldstein , B . O’Donoghue , S . Setzer , and
R . Baraniuk . Fast alternating direction optimization methods . SIAM Journal on Imaging Sciences , 7(3 ) , 1588 1623 .
[ 15 ] A . Juditsky and A . Nemirovski . First order methods for nonsmooth convex large scale optimization , i : general purpose methods . Optimization for Machine Learning , pages 121–148 , 2010 .
[ 16 ] S . P . Kasiviswanathan , H . Wang , A . Banerjee , and
P . Melville . Online l1 dictionary learning with application to novel document detection . In NIPS , pages 2267–2275 , 2012 .
[ 17 ] Q . Le , and A . Smola . Direct optimization of ranking measures . In arXiv preprint arXiv:0704.3359 , 2007 .
[ 18 ] N . Li , R . Jin , and Z H Zhou . Top rank optimization in linear time . In NIPS , 2014 .
[ 19 ] T Y Liu . Learning to rank for information retrieval .
Foundations and Trends in Information Retrieval , 3(3 ) : 225–331 , 2009 .
[ 20 ] Y . Nesterov . A method of solving a convex programming problem with convergence rate o ( 1/k2 ) . In Soviet Mathematics Doklady , volume 27 , pages 372–376 , 1983 .
[ 21 ] F . A . Potra and S . J . Wright . Interior point methods .
Journal of Computational and Applied Mathematics , 124(1):281–302 , 2000 .
[ 22 ] A . Rakotomamonjy . Sparse support vector infinite push . arXiv preprint arXiv:1206.6432 , 2012 .
[ 23 ] C . Rudin . The p norm push : A simple convex ranking algorithm that concentrates at the top of the list . JMLR , 10 : 2233–2271 , 2009 .
[ 24 ] R . Tibshirani . Regression shrinkage and selection via the lasso . Journal of the Royal Statistical Society . Series B ( Methodological ) , pages 267–288 , 1996 .
[ 25 ] H . Wang and A . Banerjee . Online alternating direction method . arXiv preprint arXiv:1206.6448 , 2012 .
[ 26 ] J . Wright and Y . Ma . Dense error correction via l1 minimization . Information Theory , IEEE Transactions on , 56(7):3540–3560 , 2010 .
[ 27 ] E . Yang and P . Ravikumar . Dirty statistical models .
In Advances in Neural Information Processing Systems , pages 611–619 , 2013 .
[ 28 ] H . Zou and T . Hastie . Regularization and variable selection via the elastic net . Journal of the Royal Statistical Society : Series B ( Statistical Methodology ) , 67(2):301–320 , 2005 .
APPENDIX Here we prove Lemma 2 . This lemma is the core in the analysis of accelerated ADMM algorithm . It is used in proof of Lemma 1 ( for details see [ 14] ) . Our proof is similar to the proof presented for a similar lemma in [ 14 ] .
Proof . In order to facilitate the proof let us define λ1/2 k = ˆλk +τ ( c−Axk−Byk ) . The optimality condition of update of xk in accelerated ADMM algorithm ( see Algorithm 2 ) gives
∇f1(xk ) − AT ˆλk − τ AT ( c − Axk − B ˆyk ) = 0 ∗ 1 ( AT λ1/2 k ) , k = 0 ⇒ xk = ∇f
⇒ ∇f1(xk ) − AT λ1/2 which is the desired result .
( 16 ) where the last equality is due to the definition of dual functions and strong convexity of f1 . Using the same argument , it is easy to see that yk = ∇f∗
2 ( BT λk ) .
As we mentioned earlier , when the function f1 is strongly convex , its dual be smooth with Lipschitz gradient . Therefore , for any γ 1 ( AT γ ) − f ∗
∗ 1 ( AT λk+1 ) =
1 ( AT γ ) − f ∗ f
∗ 1 ( AT λ1/2 f k+1 )
+
≥
−
( f
∗ 1 ( AT λ1/2
∗ 1 ( AT λ1/2 f
λk+1 − λ1/2
∗ 1 ( AT λ1/2
∗ 1 ( AT λk+1 ) ) k+1 ) − f k+1 ) + A∇f k+1 ) + k+1 , A∇f ∗ 1 ( AT λ1/2 k+1 ) − ρ2(A ) 2σ1
∗ 1 ( AT λ1/2
= γ − λk+1 , A∇f k+1 ) , γ − λ1/2
∗ 1 ( AT λ1/2 k+1 ) k+1 − f λk+1 − λ1/2 k+12
ρ2(A ) 2σ1 λk+1 − λ1/2 k+12 .
( 17 ) k+1 in ( 17 ) using Our goal is to bound the term λk+1 − λ1/2 λk+1− ˆλk+1 . Based on the updates of accelerated ADMM k+1 = −τ B(yk+1 − ˆyk+1 ) . algorithm , it is clear that λk+1 − λ1/2 Thus ,
λk+1 − λ1/2 k+1 = −τ B(∇f
2 ( BT λk+1 ) − ∇f ∗
∗ 2 ( BT ˆλk+1) ) , where the equality is due to the optimality conditions of the updates of yk+1 and ˆyk+1 . Now we use the Lipschitz continuity of the ∇f∗ λk+1 − λ1/2
λk+1 − ˆλk+1 .
2 to get k+1 ≤ τ
ρ2(B )
( 18 )
σ2
Combining ( 17 ) and ( 18 ) and using the fact that τ ≤
σ1σ2 2
ρ2(A)ρ4(B )
∗ 1 ( AT λk+1 )
1 ( AT γ ) − f ∗ f ≥ γ − λk+1 , A∇f
∗ 1 ( AT λ1/2 k+1 ) − 1 2τ
λk+1 − ˆλk+1 .
( 19 )
Using the convexity of f∗
2 , it is clear that
2 ( BT γ ) − f ∗ f
2 ( BT λk+1 ) ≥ γ − λk+1 , B∇f ∗
2 ( BT λk+1 ) . ∗
( 20 )
Combining ( 19 ) and ( 20 ) , we can easily get
D(λk+1 ) − D(γ ) ≥ γ − λk+1 , A∇f + B∇f
2 ( BT λk+1 ) − c − 1 ∗ 2τ
∗ 1 ( AT λ1/2 k ) λk+1 − ˆλk+1 .
( 21 )
1 ( AT λ1/2
From the optimality conditions of the updates of accelerated ADMM it is clear that xk+1 = ∇f∗ k ) and yk+1 = ∇f∗ 2 ( BT λk+1 ) . Replacing these in ( 21 ) and noting τ ( ˆλk+1− λk+1 ) = Axk+1 + Byk+1 − c yields D(λk+1 ) − D(γ ) ≥ 1 γ − λk+1 , ˆλk+1 − λk+1 − 1 τ 2τ γ − ˆλk+1 + ˆλk+1 − λk+1 , ˆλk+1 − λk+1 − 1 1 2τ τ λk+1 − ˆλk+12 , γ − ˆλk+1 , ˆλk+1 − λk+1 + 1 τ
λk+1 − ˆλk+12
1 2τ
=
=
λk+1 − ˆλk+12
506
