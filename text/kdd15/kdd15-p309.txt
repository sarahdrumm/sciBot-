Adaptive Message Update for Fast Affinity Propagation
Yasuhiro Fujiwara† , Makoto Nakatsuji‡ , Hiroaki Shiokawa† ,
Yasutoshi Ida† , Machiko Toyoda†
†NTT Software Innovation Center , 3 9 11 Midori cho Musashino shi , Tokyo , Japan
‡NTT Service Evolution Laboratories , 1 1 Hikarinooka Yokosuka shi , Kanagawa , Japan
{fujiwara.yasuhiro , nakatsuji.makoto , shiokawa.hiroaki , ida.yasutoshi , toyodamachiko}@labnttcojp
ABSTRACT Affinity Propagation is a clustering algorithm used in many applications . It iteratively updates messages between data points until convergence . The message updating process enables Affinity Propagation to have higher clustering quality compared with other approaches . However , its computation cost is high ; it is quadratic in the number of data points . This is because it updates the messages of all data point pairs . This paper proposes an efficient algorithm that guarantees the same clustering results as the original algorithm . Our approach , F AP , is based on two ideas : ( 1 ) it computes upper and lower estimates to limit the messages to be updated in each iteration , and ( 2 ) it dynamically detects converged messages to efficiently skip unneeded updates . Experiments show that F AP is much faster than previous approaches with no loss in clustering performance .
Categories and Subject Descriptors H28 [ Database Applications ] : Data mining
General Terms Algorithms , Theory
Keywords Affinity Propagation , efficient , clustering
1 .
INTRODUCTION
The abundance of data available these days demands techniques to process and manage them in an effective manner [ 26 ] . Clustering can extract groups of data objects or identify representative examples from raw data . From a data mining perspective , it can find the hidden patterns of the given dataset in an unsupervised way . From a practical perspective , it plays an outstanding role in scientific data exploration , information retrieval , Web analysis , computational biology , and many others applications [ 15 ] . be the set of data points it finds exemplars by maximizing!N
While many clustering approaches have been proposed [ 21 , 22 , 25 ] , one of the most successful is Affinity Propagation [ 4 ] . Affinity Propagation iteratively identifies clusters and corresponding representative data points called ex= emplars . Here , letting {x1 , x2 , . . . , xN} , s(xi , xj ) be the similarity of a data point pair ( xi , xj ) , and e(xi ) be the exemplar of a data point xi , i=1 s(xi , e(xi) ) , ie , the sum of similarities between data points and exemplars . A variant of k means , k medoids , is one of the most popular algorithms for finding representative data points [ 9 ] . It iteratively finds cluster centers in a dataset by minimizing the distances between data points and clustering centers ; clustering centers correspond to representative data points . In order to maximize the function , k medoids utilizes an intuitive assumption that the data are distributed around the clustering centers . Instead of using this simple assumption , Affinity Propagation employs a more sophisticated technique . It applies the max product algorithm to the factor graph that was developed in the machine learning community [ 13 ] ; it seeks the maximum of the function on the basis of message passing between data points . It uses the similarities of data point pairs to simultaneously consider all data points as potential exemplars and exchanges messages between data points until a set of exemplars is obtained . In each iteration , each message reflects the affinity that a data point has for another data point considered as its exemplar . Affinity Propagation has higher clustering quality compared with the previous approaches [ 4 , 10 , 19 ] . Therefore , it has been used in many applications such as mining of similar drugs [ 3 ] , key sentence extraction [ 4 ] , and representative image extraction [ 27 ] .
Affinity Propagation has , unfortunately , a weakness ; its computation cost is quadratic in the number of data points . This is due to the iterative message update process used to enhance the clustering quality ; fundamentally , the messages of all data point pairs are updated until convergence . Several approaches have been proposed to speed up the clustering . For example , Jia et al . proposed FSAP , which divides the clustering process into two stages [ 12 ] . In the first stage , they construct a K nearest neighbor graph to reduce the number of message updates by setting a parameter K . In order to enhance the clustering quality , their second stage adds edges to the graph constructed in the first stage . However , it does not guarantee the same clustering results as the original algorithm , and so loses the superior characteristics of Affinity Propagation [ 6 ] . Graph AP , proposed by Fujiwara et al . , is a state of the art efficient algorithm1 that constructs a sparse
1We refer to the approach by Fujiwara et al . as Graph AP .
309 Table 1 : Definitions of main symbols .
Symbol
N T
Definition Number of data points Number of iterations Similarity of data point pair ( xi , xj ) s(xi , xj ) rt(xi , xj ) Responsibility between xi and xj in the t th iteration at(xi , xj ) Availability between xi and xj in the t th iteration ρt(xi , xj ) Propagating responsibility corresponding to rt(xi , xj ) αt(xi , xj ) Propagating availability corresponding to at(xi , xj ) r(xi , xj ) Upper estimate of rt(xi , xj ) a(xi , xj ) Upper estimate of at(xi , xj ) Lower estimate of rt(xi , xj ) r(xi , xj ) Lower estimate of at(xi , xj ) a(xi , xj ) Damping factor , 0 ≤ λ< 1 Set of data points Set of similarities between data points Pair set to update responsibilities in the iterations Pair set to update availabilities in the iterations Pair set to compute exemplars Pair set to update responsibilities in the t th iteration Pair set to update availabilities in the t th iteration
λ t t graph structure from upper and lower estimates [ 6 ] . If the messages converge , Graph AP guarantees the same clustering results as the original algorithm . Although Graph AP is more efficient than FSAP , it still performs unnecessary message updates in each iteration . This is because it updates messages for all edges in the sparse graph until convergence is reached , even if most messages have reached convergence in earlier iterations .
This paper proposes F AP , a novel and efficient algorithm for Affinity Propagation that is based on two ideas ; ( 1 ) it limits message updates by computing upper and lower estimates and ( 2 ) it dynamically skips updates by identifying data point pairs whose message values do not change . F AP performs clustering more quickly than the original approach , FSAP , and Graph AP , while guaranteeing the same clustering results as the original algorithm .
The remainder of this paper is organized as follows : Section 2 overviews Affinity Propagation . Section 3 introduces the main ideas and details of F AP . Section 4 reviews the results of our experiments . Section 5 describes related work . Section 6 provides our conclusions .
2 . PRELIMINARIES
This section formally defines the notions and introduces the background of this paper . Table 1 lists the main symbols . Affinity Propagation identifies clusters and corresponding exemplars that represent the given dataset from the pairwise similarities = {s(xi , xj)|xi , xj ∈ } [ 4 ] . If xi = xj , s(xi , xi ) is referred to as preference . Preference is typically set to the median of the input similarities , and it impacts the number of clusters [ 4 ] . Affinity Propagation can deal with any type of similarity , such as negative Euclidean distance or dynamic time warping [ 24 ] . Furthermore , similarities may be set by hand . That is , the user can set arbitrary similarities . This is because Affinity Propagation can be applied even if the dataset does not lie in a continuous space . In addition , the clustering results do not depend on the initialization , and the number of clusters is automatically determined according to the hidden structure of the given dataset .
In order to obtain the clustering results , Affinity Propagation performs iterative computations the same as other approaches in machine learning [ 11 , 17 , 18 ] . Affinity Propagation is derived from a standard inference technique on a factor graph , and uses the max product algorithm to perform maximum a posteriori inference [ 13 ] . Given a set of , it finds exemplars that maximize the funcsimilarities , tion described in Section 1 , ie , the overall sum of similarities between all the exemplars and their member data points . It iteratively updates two types of message passed between data points until a set of exemplars is found ; responsibility and availability . The responsibility of the t th iteration , rt(xi , xj ) , is a message from data point xi to xj that denotes how well data point xj is suited to be the exemplar for data point xi . The availability of the t th iteration , at(xi , xj ) , is a message from data point xj to xi that reflects how appropriate it would be for data point xi to choose data point xj as its exemplar . Let λ be a damping factor introduced to avoid numerical oscillation ( 0 ≤ λ< 1 ) , Affinity Propagation recursively updates the responsibilities and availabilities in the t th iteration ( t ≥ 1 ) as follows : rt(xi , xj ) = ( 1 − λ)ρt(xi , xj ) + λrt−1(xi , xj ) at(xi , xj ) = ( 1 − λ)αt(xi , xj ) + λat−1(xi , xj )
( 1 ) ( 2 ) where ρt(xi , xj ) and αt(xi , xj ) are the propagating responsibility and propagating availability , respectively [ 6 ] ; ρt(xi , xj ) and αt(xi , xj ) are computed as follows :
ρt(xi , xj ) =
αt(xi , xj ) = s(xi , xj)− max s(xi , xj)− max
 xk"=xj{at−1(xi , xk ) + s(xi , xk)} ( xi &= xj ) xk"=xj{s(xi , xk)} ( xi = xj ) min&0,rt−1(xj,xj)+’xk"=xi,xj  ’xk"=xi max{0 , rt−1(xk,xj)}( ( xi&= xj ) max{0 , rt−1(xk , xj)}
( xi = xj )
( 3 )
( 4 )
Note that λ is typically set to 0.5 [ 4 ] . When t = 0 , the messages are initialized as r0(xi , xj ) = s(xi , xj ) − max a0(xi , xj ) = 0 xk"=xj{s(xi , xk)}
( 5 )
( 6 )
After the iterations , the exemplar of data point xi , e(xi ) , is determined : e(xi ) = argmaxxj{rt(xi , xj ) + at(xi , xj)}
( 7 )
The original algorithm iteratively updates the responsibilities and availabilities of all data point pairs by using Equations ( 1 ) and ( 2 ) and then determines the exemplars from Equation ( 7 ) after convergence . If N is the number of data points and T the number of iterations , it needs O(N 2T ) time . This is because it iteratively incurs a computation cost that is quadratic in the number of data points . As a result , if the number of data points is large , the computation cost of Affinity Propagation may be prohibitive .
3 . PROPOSED APPROACH
This section presents our proposal , F AP , which efficiently identifies clusters that are exactly the same as those identified by the original Affinity Propagation algorithm . Section 3.1 overviews the ideas underlying F AP , and Sections 3.2 , 3.3 , and 3.4 explain it and the clustering algorithm in detail . We perform a theoretical analysis in Section 35
310 3.1 Main Ideas
The original Affinity Propagation algorithm incurs a high clustering cost because it iteratively computes the messages of all data point pairs . To efficiently obtain the clustering results , we adaptively update the messages of only selected pairs instead of all the pairs in each iteration .
To make the message update efficient , we identify the messages whose values should be computed in the iterations by computing upper and lower estimates before entering each iteration . As shown in Section 2 , Affinity Propagation employs the max product algorithm , where the maximum values are used in the message updates . Therefore , by using upper and lower estimates , we can effectively detect , and thus ignore , messages that have no impact on the clustering results ( Section 32 ) Since we do not need any iterative computations to obtain the estimates , we can limit the message updates before commencing an iteration .
After limiting the updated messages by using the estimates , we dynamically skip unnecessary updates of messages that have already converged ( Section 33 ) This technique is based on the observation that most messages converge quickly , while a few messages take much longer to converge . Since most messages will be skipped by using this property , we can significantly reduce the computation cost of each iteration . Graph AP proposed different upper and lower estimates [ 6 ] . Our estimates differ from those ones in that ours guarantee upper and lower bounds even if the messages have not reached convergence ; the previous estimates do not guarantee the bounds until the convergence . We cannot directly apply the previous estimates without affecting the clustering results since our approach skips unnecessary message updates in the iterations after limiting messages by the estimates . In Figure 3 of Section 4 , we visually show updated messages in the iterations by each approach . 3.2 Message Estimates
Here , we show how to make the upper and lower estimates to limit message updates before entering the iterations . First , we define the estimates along with their theoretical properties in Section 321 After that , we describe the technique to limit the message updates in Section 322
321 Definition We compute the upper and lower estimates of the messages before entering the iterations ; we do not compute the estimates in each iteration . The upper estimates of responsibility and availability are defined as follows :
Definition 1
( Upper Estimates ) . If r(xi,xj ) and a(xi,xj ) are the upper estimates of rt(xi , xj ) and at(xi , xj ) , respectively , r(xi , xj ) and a(xi , xj ) are as follows : where ρ(xi , xj ) and α(xi , xj ) are the upper estimates of ρt(xi , xj ) and αt(xi , xj ) , defined as
ρ(xi , xj ) =
α(xi , xj ) = s(xi , xj ) − s(xi , xi ) s(xi , xj ) − max xk"=xj{s(xi , xk)} min&0 , r(xj , xj)+’xk"=xi,xj ’xk"=xi max{0 , r(xk , xj)}
 
( 10 )
( 11 )
( xi &= xj ) ( xi = xj ) max{0 , r(xk , xj)}( ( xi&= xj )
( xi = xj )
Equation ( 8 ) uses the initial responsibility r0(xi , xj ) and upper estimate ρ(xi , xj ) for the upper estimate r(xi , xj ) . As shown in Equation ( 5 ) and ( 10 ) , r0(xi , xj ) and ρ(xi , xj ) are computed from the similarities in a non iterative manner . Therefore , we do not need any iterative computation of r(xi , xj ) . In addition , as shown in Equation ( 9 ) , the upper estimate α(xi , xj ) is needed for the upper estimate a(xi , xj ) , and α(xi , xj ) can be computed from r(xi , xj ) by using Equation ( 11 ) . Consequently , a(xi , xj ) can be obtained in a noniterative manner as well .
The following lemma shows the properties of the above estimates :
Lemma 1
( Upper Estimates ) . In the iterations , we have r(xi,xj)≥ rt(xi,xj ) and a(xi,xj)≥ at(xi,xj ) for any pairs . Proof Prior to proving the properties of Lemma 1 , we first prove that at(xi , xj ) ≥ 0 in the iterations if xi = xj by mathematical induction [ 8 ] . Initial step : If t = 0 , we have a0(xi,xi ) = 0 from Equation ( 6 ) . Inductive step : We assume that at−1(xi , xi ) ≥ 0 . From Equation ( 4 ) , if xi = xj , we have
αt(xi , xi ) = ’xk"=xi max{0 , rt−1(xk , xi)}≥0
( 12 )
Since 0 ≤ λ<1 as described in Section 2 , it is clear that ( 1− λ)αt(xi , xi ) > 0 and λat−1(xi , xi ) ≥ 0 . Therefore , from Equation ( 2 ) , we have at(xi , xi ) = ( 1 − λ)αt(xi , xi ) + λat−1(xi , xi ) > 0
( 13 ) This completes the inductive step . Therefore , at(xi , xj ) ≥ 0 holds in the iterations if xi = xj . Next , we prove that ρ(xi , xj ) ≥ ρt(xi , xj ) holds in the iterations . If xi &= xj holds , maxxk"=xj{at−1(xi , xk)+s(xi , xk)}≥ maxxk=xi{at−1(xi , xk)+s(xi , xk)} = at−1(xi , xi)+s(xi , xi ) . Therefore , if xi &= xj , from Equation ( 3 ) , we have ρt(xi , xj ) = s(xi , xj ) − max xk"=xj{at−1(xi , xk)+s(xi , xk)}
( 14 ) r(xi , xj ) =

λr0(xi,xj)+ρ(xi,xj ) ( r0(xi,xj ) > 0 , ρ(xi,xj ) > 0 ) λr0(xi,xj)+(1−λ)ρ(xi,xj ) ( r0(xi,xj ) > 0 , ρ(xi,xj)≤ 0 ) ( r0(xi,xj)≤ 0 , ρ(xi,xj ) > 0 ) ρ(xi , xj ) ( r0(xi,xj)≤ 0 , ρ(xi,xj)≤ 0 ) ( 1−λ)ρ(xi,xj ) a(xi , xj ) =
)α(xi , xj )
( 1 − λ)α(xi , xj )
( α(xi , xj ) > 0 ) ( α(xi , xj ) ≤ 0 )
( 8 )
( 9 )
≤ s(xi , xj ) − at−1(xi , xi ) − s(xi , xi )
In Equation ( 14 ) , at−1(xi , xi ) ≥ 0 holds , as proved by mathematical induction . Therefore , if xi &= xj , we have ρt(xi , xj ) ≤ s(xi , xj ) − s(xi , xi ) = ρ(xi , xj )
( 15 )
From Equation ( 3 ) and ( 10 ) , if xi = xj ,
ρt(xi , xj ) = s(xi , xj ) − max xk"=xj{s(xi , xk)} = ρ(xi , xj )
( 16 )
Consequently , we find that ρ(xi , xj ) ≥ ρt(xi , xj ) .
311 Next , we prove that r(xi , xj ) ≥ rt(xi , xj ) . By recursively applying Equation ( 1 ) , we get rt(xi , xj ) = ( 1 − λ)ρt(xi , xj)+λrt−1(xi , xj ) = ( 1 − λ){ρt(xi , xj)+λρt−1(xi , xj)} + λ2rt−2(xi , xj ) = ( 1 − λ)!t−1 where t ≥ 1 . Since!t−1 i=0 λiρt−i(xi , xj ) +λ tr0(xi , xj ) holds in the iterations , we have i=0 λi = 1−λt
1−λ and ρ(xi , xj ) ≥ ρt(xi , xj )
( 17 ) rt(xi , xj ) ≤ λtr0(xi , xj ) + ( 1 − λt ) ρ(xi , xj )
( 18 ) Since the damping factor is 0 ≤ λ< 1 , as described in Section 2 , we have 0 <λ t ≤ λ in the t th iteration ( t ≥ 1 ) . Therefore , in Equation ( 18 ) , if r0(xi , xj ) > 0 , we have λtr0(xi , xj ) ≤ λr0(xi , xj ) ; otherwise λtr0(xi , xj ) < 0 . Similarly , we have ( 1 − λ ) ≤ ( 1 − λt ) ≤ 1 in the t th iteration . Therefore , ( 1−λt)ρ(xi , xj ) ≤ ρ(xi , xj ) holds if ρ(xi , xj ) > 0 ; otherwise ( 1− λt)ρ(xi , xj ) ≤ ( 1− λ)ρ(xi , xj ) holds in Equation ( 18 ) . Consequently , it is clear that Equation ( 8 ) gives the upper estimate for the responsibilities in the iterations . We finally show that α(xi , xj ) ≥ αt(xi , xj ) and a(xi , xj ) ≥ at(xi , xj ) . Since r(xi , xj ) ≥ rt(xi , xj ) , the following inequality holds from Equation ( 4 ) and ( 11 ) if xi &= xj : αt(xi , xj ) max{0 , r(xk,xj)}( = α(xi , xj )
≤ min&0,r(xj,xj)+’xk"=xi,xj αt(xi , xj ) ≤ ’xk"=xi
If xi = xj , since r(xi , xj ) ≥ rt(xi , xj ) , we have max{0 , r(xk , xj)} = α(xi , xj )
( 19 )
( 20 )
Consequently , α(xi , xj ) ≥ αt(xi , xj ) . Thus , similar to Equation ( 18 ) , we get the following inequality from Equation ( 2 ) : at(xi , xj ) ≤ ( 1 − λt ) α(xi , xj )
( 21 )
Note that a0(xi , xj ) = 0 , as shown in Equation ( 6 ) . Since ( 1 − λ ) ≤ ( 1 − λt ) ≤ 1 holds in the t th iteration , we have ( 1 − λt ) α(xi , xj ) ≤ α(xi , xj ) if α(xi , xj ) > 0 . Otherwise , we have ( 1 − λt ) α(xi , xj ) ≤ ( 1 − λ ) α(xi , xj ) . Therefore , a(xi , xj ) ≥ at(xi , xj ) holds , which completes the proof . 2
The lower estimates of the messages are as follows :
Definition 2
( Lower Estimates ) . The following equa tions define the lower estimates of responsibility r(xi , xj ) and availability a(xi , xj ) : r(xi , xj ) =

( 1−λ)ρ(xi,xj ) ( r0(xi,xj ) > 0,ρ ( xi,xj ) > 0 ) ( r0(xi,xj ) > 0,ρ ( xi,xj)≤ 0 ) ρ(xi,xj ) λr0(xi,xj)+(1− λ)ρ(xi,xj ) ( r0(xi,xj)≤ 0,ρ ( xi,xj ) > 0 ) ( r0(xi,xj)≤ 0,ρ ( xi,xj)≤ 0 ) λr0(xi,xj)+ρ(xi,xj ) a(xi , xj ) =
)(1 − λ)α(xi , xj )
α(xi , xj )
( α(xi , xj ) > 0 ) ( α(xi , xj ) ≤ 0 )
( 22 )
( 23 ) where ρ(xi , xj ) and α(xi , xj ) are the lower estimates of the propagating messages ρt(xi , xj ) and αt(xi , xj ) . ρ(xi , xj ) and
α(xi , xj ) are as follows :
ρ(xi , xj ) =
  s(xi , xj ) − max s(xi , xj ) − max xk"=xj{a(xi , xk ) +s ( xi , xk)} xk"=xj{s(xi , xk)}
( xi &= xj ) ( xi = xj )
( 24 )
α(xi , xj ) = min{0 , s(xi , xj ) − max 0 xk"=xj{s(xi , xk)}}
( xi &= xj ) ( xi = xj )
( 25 )
Note that r(xi , xj ) and a(xi , xj ) can be computed in a non iterative manner . This is because ( 1 ) r0(xi , xj ) and ρ(xi , xj ) used in Equation ( 22 ) can be obtained without iterative computations and ( 2 ) a(xi , xj ) in Equation ( 23 ) can be computed from just pair wise similarities .
The estimates of responsibility and availability given in
Definition 2 have the following lower bounding properties :
Lemma 2
( Lower Estimates ) . In the iterations , we have r(xi,xj)≤ rt(xi,xj ) and a(xi,xj)≤ at(xi,xj ) for any pairs . We will omit the proof of Lemma 2 because of space limitations . However , Lemma 2 can be proved similarly to Lemma 1 . Since we can compute the estimates from just the pair wise similarities in a non iterative manner , they can be obtained before entering the iterations .
Other upper and lower estimates are used in Graph AP [ 6 ] . However , they have different properties from ours ; they give upper and lower bounds only for converged messages . Our estimates guarantee the bounds even before convergence . This is because their estimates are based on the property that rt(xi , xj ) =ρ t(xi , xj ) and at(xi , xj ) =α t(xi , xj ) hold after convergence ( cf . Lemma 1 of the previous paper [ 6] ) . Since rt(xi , xj ) &= ρt(xi , xj ) and at(xi , xj ) &= αt(xi , xj ) before convergence , the previous estimates cannot guarantee the bounds in the iterations . It is necessary to give assured bounds before convergence if we are to guarantee the results will be the same as in the original approach . This is because we limit the message updates before the iterations and use the estimates to skip updates , as described in Section 33 Therefore , we cannot directly apply the previous estimates to the proposed approach . 322 Message Limitation As described in Section 321 , the estimates provide the upper and lower bounds in the iterations . By using the estimates , we can safely prune unnecessary messages while guaranteeing the same clustering results as the original algorithm . Since we do not update the messages of all data point pairs , we can efficiently find clusters for the given dataset . Our technique obtains three pair sets between data points when computing the messages before the iterations .
The first pair set ,
, is a set of pairs to update the re sponsibilities . The definition of is as follows :
Definition 3 be the set of data point pairs whose responsibilities are updated in the iterations .
( Responsibility Set ) . Let is defined as = {(xi , xj)|xi = xj or r(xi , xj ) > 0}
( 26 )
The responsibility set has the following properties :
312 Lemma 3
( Responsibility Set ) . If the responsibilities have been obtained for all pairs in the responsibility set , the availabilities of any pair of data points can be com from rt−1(xi , xj ) and ρt(xi , xj ) as shown in Equation ( 1 ) , it is clear that we can compute the responsibilities of any pair from the availability set 2
. puted in each iteration .
Proof We use mathematical induction . from Definition 3 .
Initial step : From Equation ( 6 ) , it is clear that availability is initialized to a0(xi , xj ) = 0 . Inductive step : We assume that the following statement holds in the ( t−1) th iteration ; at−1(xi , xj ) can be computed . If r(xi , xk ) ≤ 0 from the responsibilities of the pairs in holds for pair ( xi , xk ) such that xi &= xk , it is clear that the In addition , pair is not included in the responsibility of ( xi , xk ) does not affect the propagating availability αt(xi , xj ) in Equation ( 4 ) . This is because , from Lemma 1 , max{0 , rt(xi , xk)}≤max {0 , r(xi , xk)}≤ 0 holds and the responsibility rt−1(xj , xj ) can be computed without the pair . Therefore , we can compute the propagating availability αt(xi , xj ) without the responsibility of the pair . As shown in Equation ( 2 ) , the availability of pair ( xi , xj ) can be computed from at−1(xi , xj ) and αt(xi , xj ) ; hence , we can compute the availabilities from the responsibilities . This completes the of the pairs in the responsibility set inductive step . As a result , the availabilities of any data in point pair can be computed from the responsibility set each iteration . 2
The second pair set ,
, is the set of data point pairs whose availabilities are to be updated . is given as follows :
Definition 4
( Availability Set ) . The following equa , the set of pairs whose availabilities are to be tion defines updated in the iterations :
={(xi , xj)|a(xi , xj)+s(xi , xj)≥ a(xi , x##)+s(xi , x##)} ( 27 ) In Equation ( 27 ) , data point x## gives the second largest value of a(xi , xk ) +s(x i , xk ) , x## = argmaxxk"=x!{a(xi , xk ) + s(xi , xk)} , whereas data point x# gives the largest value of a(xi , xk ) + s(xi , xk ) , x# = argmaxxk{a(xi , xk ) + s(xi , xk)} . Equation ( 27 ) indicates that we can prune the iterative computations by using the upper and lower estimates of availability before the iterations . The properties of the availability set are as follows :
Lemma 4
( Availability Set ) . In the iterations , the responsibilities of any pair of data points can be computed have if the availabilities of all pairs in the availability set been found .
Proof We use mathematical induction .
.
Initial step : As shown in Equation ( 5 ) , the responsibilities can be computed from the similarities . Inductive step : We assume that the responsibility rt−1(xi , xj ) can be computed for any pair of availabilities in the set If a(xi , xk ) +s( xi , xk ) < a(xi , x## ) +s( xi , x## ) holds for pair ( xi , xk ) , from Definition 4 , that pair is not in . For the pair , a(xi , xk ) + s(xi , xk ) < a(xi , x## ) +s( xi , x## ) ≤ maxxl"=xk{at−1(xi , xl ) +s(x i , xl)} holds , where data point x## gives the second largest value of a(xi , xl ) + s(xi , xl ) , ie , a(xi , xk ) + s(xi , xk ) < maxxl"=xk{at−1(xi , xl ) +s ( xi , xl)} . Thus , from Equation ( 3 ) , the availability of the pair is not needed in order to compute the propagating responsibility ρt(xi , xj ) if xi &= xj . If xi = xj , ρt(xi , xj ) can be obtained from the pair wise similarities , as shown by Equation ( 3 ) . Since we can compute the responsibility of the pair ( xi , xj )
Finally , we introduce the following message set ,
, the set of pairs for the clustering results after the iterations :
Definition 5
( Message Set ) . Let be the set of data point pairs whose responsibility and availability are computed is deafter the iterations to obtain the clustering results . fined as follows :
={(xi,xj)|r(xi,xj)+a(xi,xj)≥ maxxk{r(xi,xk)+a(xi,xk)}} ( 28 ) Definition 5 has the following properties :
Lemma 5
( Message Set ) . If the responsibilities and have been availabilities of all pairs in the message set obtained , the message set yields the same exemplars as the original algorithm .
Proof The pair ( xi , xj ) is not included in the message if r(xi , xj ) + a(xi , xj ) < maxxk{r(xi , xk ) + a(xi , xk)} set holds for the pair from Definition 5 . From Lemma 1 and 2 , we have maxxk{r(xi , xk ) +a ( xi , xk)}≤ maxxk{r(xi , xk ) + a(xi , xk)} . As a result , r(xi , xj)+a(xi , xj ) < maxxk{r(xi , xk)+ a(xi , xk)} holds for the pair . Thus , from Equation ( 7 ) , data point xj cannot be an exemplar of data point xi so the responsibility and availability of ( xi , xj ) are not required in order to obtain the clustering results . Consequently , it is clear that the statement holds on the message set 2
.
Since the estimates can be computed from just pair wise similarities , we can limit the message updates before commencing the iterations . 3.3 Dynamic Message Update
We could reduce the clustering cost before commencing the iteration by exploiting the upper and lower estimates , in the same way as in the state of the art approach [ 6 ] . However , in so doing , the effectiveness of the upper and lower estimates would only be moderate because iterative computations still have to be performed on all the remaining messages , even if most of them quickly converge . How can we cut down on unnecessary computations in each iteration ? This is the motivation behind our second idea , dynamic message update ; we skip updates of the converged messages found in each iteration . Since the converged messages are dynamically detected before the updates , clustering speed can be increased without sacrificing performance . We terminate the iterations if all the messages are skipped . In this section , we define the set of data point pairs whose responsibilities and availabilities are updated in the iterations . Then , we describe three properties of the converged messages . Finally , we describe the properties of the set of data point pairs by using these three properties .
Let t be the set of data point pairs whose responsibilities are to be updated in the t th iteration . The responsibility set of the t th iteration , t , is defined as follows :
Definition 6
( t th Responsibility Set ) . Let i be a such that the responsibilsubset of the responsibility set i = {(xj , xk)|j = ities from data point xi are included , i and ( xj , xk ) ∈ } . Let x#t−1 = argmaxxj{at−1(xi , xj ) + s(xi , xj)} and x##t−1 = argmaxxj"=x!t−1{at−1(xi , xj)+s(xi , xj)} . When t &= 1 , t+1 of i is included in the responsibility set
313 the ( t + 1) th iteration if at least one of the following conditions holds for the pair ( xi , xj ) such that ( xi , xj ) ∈ i : ( 1 ) at(xi,xj)+s(xi,xj ) > at(xi,x##t−1)+s(xi,x##t−1 ) if xj /∈{ x#t−1 , x##t−1} ( 2 ) at(xi , xj ) &= at−1(xi , xj ) if xj ∈{ x#t−1 , x##t−1} ( 29 ) ( 3 ) rt(xi , xj ) &= rt−1(xi , xj ) If t = 1 , it is initialized as t = { i|1 ≤ i ≤ N} .
In Definition 6 , data points x#t−1 and x##t−1 give respectively the largest and second largest values of at−1(xi , xj ) + s(xi , xj ) . Definition 6 indicates that we can obtain from the messages of the t th and ( t − 1) th iterations ; the messages of the ( t + 1) th iteration are not required . The set of data point pairs for updating the availabilities t+1 in the t th iteration , t , is found as follows :
Definition 7
( t th Availability Set ) . Let i be a sub that includes the availabilities set of the availability set i = {(xk , xj)|j = i and ( xk , xj ) ∈ from data point xi , ie , } . If at least one of the following conditions holds for the pair ( xj , xi ) such that ( xj , xi ) ∈ i , the availability set of i as its element when the ( t + 1) th iteration t &= 1 : ( 1 ) rt(xj,xi)&= rt−1(xj,xi ) if rt(xj,xi ) > 0 or rt−1(xj,xi ) > 0 ( 2 ) at(xj , xi ) &= at−1(xj , xi ) t is initialized as t = { i|1 ≤ i ≤ N} when t = 1 . t includes
( 30 )
Note that we do not need any message from the ( t + 1) th iteration to obtain t+1 , as shown by Definition 7 . t and
In order to describe the properties of t , we introduce three lemmas related to ( 1 ) message convergence , ( 2 ) responsibility propagation , and ( 3 ) availability propagation . These lemmas form the theoretical guarantees of the t ; we can use them to introduce the properties of properties of the responsibility and availability sets of the t th iteration . The first lemma is as follows : t and
Lemma 6
( Message Convergence ) . For the data point pair ( xi , xj ) , we have rt+1(xi , xj ) = rt(xi , xj ) if rt(xi , xj ) = rt−1(xi , xj ) and ρt+1(xi , xj ) = ρt(xi , xj ) hold in the ( t + 1)In addition , if at(xi , xj ) =a t−1(xi , xj ) and th iteration . αt+1(xi , xj ) = αt(xi , xj ) , we have at+1(xi , xj ) = at(xi , xj ) for ( xi , xj ) .
Proof
Since rt(xi , xj ) = ( 1−λ)ρt(xi , xj)+λrt−1(xi , xj ) from Equation ( 1 ) , if rt(xi , xj ) = rt−1(xi , xj ) holds , we have rt(xi , xj ) = ρt(xi , xj ) . Thus , if ρt+1(xi , xj ) =ρ t(xi , xj ) , we have ρt+1(xi , xj ) = rt(xi , xj ) . Consequently , from Equation ( 1 ) , we get rt+1(xi , xj ) = ( 1−λ)ρt+1(xi , xj)+λrt(xi , xj ) =r t(xi , xj ) ( 31 ) Similarly , we get at+1(xi , xj ) = at(xi , xj ) ifa t(xi , xj ) = at−1(xi , xj ) and αt+1(xi , xj ) = αt(xi , xj ) . 2
The responsibility propagating lemma is as follows :
Lemma 7
( Responsibility Propagation ) . For each data point pair ( xi , xj ) ∈ i , we have ρt+1(xi , xj ) = ρt(xi , xj ) if all of the following conditions hold : ( 1 ) at(xi,xj)+s(xi,xj)≤ at(xi,x##t )+s(xi,x##t ) if xj /∈{ x#t,x##t } ( 2 ) at(xi , xj ) =a t−1(xi , xj ) if xj ∈{ x#t , x##t } where the data points x#t and x##t are as per Definition 6 .
( 32 )
( 33 ) xk=x!t,x!!t{at(xi,xk)+s(xi,xk)},max xk"=xj ,x!t,x!!t{at(xi,xk)+s(xi,xk)}}
Proof For data point xj such that xj &= x#t and xj &= x##t , we have at(xi , xj ) + s(xi , xj ) ≤ at(xi , x##t ) + s(xi , x##t ) ≤ at(xi , x#t)+s(xi , x#t ) . Thus , we have maxxk"=xj ,x!t,x!!t {at(xi , xk)+ s(xi , xk)}≤a t(xi , x#t)+s(xi , x#t ) . From the second condition of Lemma 7 , we get at(xi , x#t ) = at−1(xi , x#t ) . Thus , xk"=xj{at(xi , xk ) + s(xi , xk)} max = max{max =at(xi , x#t ) + s(xi , x#t ) = at−1(xi , x#t ) + s(xi , x#t ) From the definition of data point x#t−1 ( Definition 6 ) , we find that at−1(xi , x#t ) + s(xi , x#t ) = maxxk"=xj{at−1(xi , xk ) + s(xi , xk)} if xj /∈{ x#t , x##t } . Thus , maxxk"=xj{at(xi , xk ) + s(xi , xk)} = maxxk"=xj{at−1(xi , xk ) +s ( xi , xk)} holds from Equation ( 33 ) . Therefore , for data point pair ( xi , xj ) ∈ i such that xi &= xj , we get ρt+1(xi , xj ) xk"=xj{at−1(xi,xk)+s(xi,xk)} = ρt(xi,xj ) ( 34 ) = s(xi,xj)− max from Equation ( 3 ) if xj /∈{ x#t , x##t } . Similarly , if xj = x##t , we have ρt+1(xi , xj ) = ρt(xi,xj ) for data point pair ( xi , xj ) ∈ i such that xi &= xj when all the conditions of Lemma 7 hold . For data point xj = x#t , if all the conditions of Lemma 7 hold , we have ( 1 ) at(xi , x##t )+s(xi , x##t ) ≥ at(xi , xk)+s(xi , xk ) for any data point xk such that xk &= xj and ( 2 ) at(xi , x##t ) = at−1(xi , x##t ) for data point x##t . Therefore , for a data point pair ( xi , xj ) ∈ i such that xi &= xj , we find that xk"=xj{at(xi,xk)+s(xi,xk)} = at(xi,x##t )+s(xi,x##t ) = max at−1(xi,x##t )+s(xi,x##t ) = max xk"=xj{at−1(xi,xk)+s(xi,xk)}
( 35 )
As a result , we have ρt+1(xi , xj ) = ρt(xi,xj ) from Equation ( 3 ) , and thus , ρt+1(xi , xj ) = ρt(xi,xj ) ifx i &= xj .
If xi = xj , from Equation ( 3 ) , it is clear that ρt+1(xi , xj ) = s(xi , xj)− max which completes the proof . xk"=xj{s(xi , xk)} = ρt(xi,xj ) ( 36 )
2
The availability propagation lemma is as follows :
Lemma 8
( Availability Propagation ) . For each data point pair ( xj , xi ) ∈ i , αt+1(xi , xj ) = αt(xi , xj ) holds if rt(xk , xj ) = rt−1(xk , xj ) for each data point pair such that rt(xk , xj ) > 0 or rt−1(xk , xj ) > 0 .
Proof
Prior to proving Lemma 8 , we prove by mathematical induction that rt+1(xi , xj ) = rt(xi , xj ) holds if xi = xj . Initial step : From Equation ( 3 ) and ( 5 ) , we have ρ1(xj , xj ) = s(xj , xj ) − maxxk"=xj{s(xj , xk)} = r0(xj , xj ) . Thus , from Equation ( 1 ) , r1(xj , xj ) = ( 1 − λ)ρ1(xj , xj ) + λr0(xj , xj ) = r0(xj , xj ) holds . If rt(xj , xj ) = rt−1(xj , xj ) , rt(xj , xj ) = Inductive step : r0(xj , xj ) = s(xj , xj)−maxxk"=xj{s(xj , xk)} holds from Equation ( 5 ) . Moreover , from Equation ( 3 ) and ( 5 ) , ρt(xj , xj ) = s(xj , xj ) − maxxk"=xj{s(xj , xk)} = rt(xj , xj ) holds . Thus , rt+1(xj , xj ) = ( 1−λ)ρt(xj , xj)+λrt(xj , xj ) =r t(xj , xj ) ( 37 ) which completes the inductive step .
Next , we prove Lemma 8 . Suppose that the condition rt(xk , xj ) = rt−1(xk , xj ) holds for each data point xk , where
314 rt(xk , xj ) > 0 orr t−1(xk , xj ) > 0 . Then , we have
’xk"=xi,xj max{0 , rt(xk , xj)} = ’xk"=xi,xj max{0 , rt−1(xk , xj)} ( 38 )
Therefore , we get the following equation for data point pair ( xi , xj ) ∈ i such that xi &= xj from Equation ( 4 ) : αt+1(xi , xj ) max{0 , rt−1(x,xj)} = αt(xi,xj ) ( 39 )
= min{0,rt−1(xj,xj)+’xk"=xi,xj have!xk"=xi by using the result of the mathematical induction . For data point pair ( xi , xj ) ∈ i such that xi = xj , since we similarly we have αt+1(xi , xj ) = αt(xi , xj ) , if the condition holds . 2 max{0 , rt(xk , xj)} =!xk"=xi max{0 , rt−1(xk , xj)} ,
Lemmas 6 , 7 , and 8 lead us to the following theoretical properties of the responsibility set and availability set of the t th iteration :
Proof
Lemma 9 i /∈ Rt+1 . Let us assume that
( t th Responsibility Set ) . For each data point pair ( xi , xj ) ∈ i , the responsibility of the pair does not change in value in the ( t+1) th iteration if the corresponding i is not included in the responsibility set of the ( t+1)subset th iteration Rt+1 . That is , ∀(xi , xj ) ∈ i , rt+1(xi , xj ) = rt(xi , xj ) if i /∈ Rt+1 . For each data point pair included in subset i , we have at(xi , xj)+s(xi , xj ) ≤ at(xi , x##t )+s(xi , x##t ) if xj /∈{ x#t , x##t } , and we have at(xi , xj ) = at−1(xi , xj ) ifx j ∈{ x#t , x##t } from Definition 6 . Therefore , i , we get ρt+1(xi , xk ) = for each pair included in subset ρt(xi , xk ) from Lemma 7 . From Definition 6 , rt(xi , xj ) = rt−1(xi , xj ) holds for each data point pair in subset i under the assumption . Consequently , from Lemma 6 , rt+1(xi , xj ) = rt(xi , xj ) holds for all data point pairs in i , since ρt+1(xi , xk ) = ρt(xi , xk ) and rt(xi , xj ) = rt−1(xi , xj ) if i /∈ Rt+1 .
2
Lemma 10
( t th Availability Set ) . If the availability set of the ( t + 1) th iteration does not include the subset i , the availabilities of each pair ( xj , xi ) included in i do not change their values in the ( t + 1) th iteration , ie , if i /∈ At+1 , at+1(xj , xi ) = at(xj , xi ) , ∀(xj , xi ) ∈ i . Proof Let us assume that i /∈ At+1 . From Definition 7 , we have rt(xj , xi ) = rt−1(xj , xi ) for each pair included in i if rt(xj , xi ) > 0 orr t−1(xj , xi ) > 0 holds . Therefore , from Lemma 8 , we have αt+1(xj , xi ) = αt(xj , xi ) for each pair i /∈ At+1 . From Definition 7 , it is clear included in i if that at(xj , xi ) =a t−1(xj , xi ) for such data point pairs . As a result , from Lemma 6 , at+1(xj , xi ) = at(xj , xi ) holds for each pair in i if 2 i /∈ At+1 .
Lemma 9 and 10 indicate that the messages of the datapoint pairs can be changed in the ( t + 1) th iteration only if the pairs are included in Rt+1 or At+1 . We can compute Rt+1 and At+1 without messages in the ( t + 1) th iteration , from Definition 6 and 7 . Therefore , to speed up clustering , we can dynamically identify data point pairs whose messages are to be updated in the ( t + 1) th iteration from the messages of the t th and ( t−1) th iterations . Lemmas 9 and 10 also indicate that we can dynamically prune unnecessary updates by using i and i . Note that i and i are subsets of R and A , respectively . As described in Section 3.2 , we can exactly compute the responsibilities and availabilities from R and A since our estimates give the
, from Equation ( 26 ) , ( 27 ) , and ( 28 ) ; compute availability at(xj , xi ) from Equation ( 2 ) ; compute responsibility rt(xi , xj ) from Equation ( 1 ) ; t+1 = ∅ ; t+1 = ∅ ; if rt(xi , xj ) $= rt−1(xi , xj ) then for each ( xi , xj ) ∈ t do add i to if rt(xi , xj ) > 0 orr t−1(xi , xj ) > 0 then t+1 ;
, and t = { i|1 ≤ i ≤ N} ; t = { i|1 ≤ i ≤ N} ; for each ( xj , xi ) ∈ t do for each ( xi , xj ) ∈ t do
Algorithm 1 F AP Input : Set of similarities between data points Output : Exemplars of data points 1 : compute 2 : t = 1 ; 3 : 4 : 5 : repeat 6 : 7 : 8 : 9 : 10 : 11 : 12 : 13 : 14 : 15 : 16 : 17 : 18 : 19 : 20 : 21 : 22 : 23 : 24 : 25 : 26 : until 27 : for each ( xi , xj ) ∈ \ do 28 : 29 : for each ( xi , xj ) ∈ \ do 30 : 31 : for each xi ∈ 32 : t+1 ; for each ( xj , xi ) ∈ t do add i to if xj ∈{ x!t−1 , x!!t−1} then if at(xj , xi ) $= at−1(xj , xi ) then if xj /∈{ x!t−1 , x!!t−1} then add j to add j to add j to t = t + 1 ;
= ∅ t+1 ; t+1 ; t+1 ; do
, if at(xi , xj )+s(xi , xj ) > at(xi , x!!t−1)+s(xi , x!!t−1 ) then compute availability as at(xi , xj ) = αt(xi , xj ) ; compute responsibility as rt(xi , xj ) = ρt(xi , xj ) ; compute exemplar of xi from Equation ( 7 ) ; upper and lower bounds before convergence , unlike the estimates of the previous approach [ 6 ] . As a result , we can efficiently skip unnecessary message updates in the iterations without affecting the clustering results . In addition , since most messages converge in the early iterations , we can reduce the number of message updates in later iterations . 3.4 Clustering Algorithm t and
, availability set t and availability set first computes the responsibility set and message set ( line 1 ) . Next , it sets t = 1 ( line 2 ) and initializes
Algorithm 1 is our clustering approach , called F AP . It , by using Equation ( 26 ) , ( 27 ) , and ( 28 ) t and t , from Definition 3 and 4 ( lines 3 4 ) . If a pair is not int , the responsibility and availability of cluded in the pair do not affect the clustering results , from Lemma 3 and 4 . Therefore , it recursively updates the responsibilities and availabilities of the pairs included in the responsibility t of the t th iteration ( lines 6set 9 ) . Our approach uses the responsibilities and availabilities of the t th and ( t − 1) th iterations to obtain the responsibility set and availability set of the ( t + 1) th iteration . For a data point pair ( xi , xj ) such that ( xi , xj ) ∈ t , if rt(xi , xj ) &= rt−1(xi , xj ) holds , the subset i that includes ( xi , xj ) is added to the responsibility set of the ( t + 1) th itt+1 , from the third condition of Definition 6 ( lines eration 13 14 ) . If rt(xi , xj ) > 0 or rt−1(xi , xj ) > 0 , the subset j , which includes data point pair ( xi , xj ) , is added to the availt+1 , from the first ability set of the ( t + 1) th iteration condition of Definition 7 ( lines 15 16 ) . As for availability , if at(xj , xi ) &= at−1(xj , xi ) for a data point pair such that ( xj , xi ) ∈ t , i is added to the availability set of the ( t+1)t+1 , from the second condition of Definition 7 th iteration
315 t and availability set j is added to the responsibility ( lines 18 19 ) . In addition , t+1 when xj ∈{ x#t−1 , x##t−1} , set of the ( t + 1) th iteration from the second condition of Definition 6 ( lines 20 21 ) . If at(xi , xj ) + s(xi , xj ) > at(xi , x##t−1 ) +s ( xi , x##t−1 ) for data point xj such that xj /∈{ x#t−1 , x##t−1} , j is added to the responsibility set of the ( t+1) th iteration t+1 , from the first condition of Definition 6 ( lines 22 24 ) . If the responsibility t of the t th iteration are empty , set it is clear from Lemma 9 and Lemma 10 that all messages have reached convergence . At this point , the algorithm ter&= minates the iterations ( line 26 ) . Since in practice , it computes the responsibilities and availabilibut ties of the pairs that are included in the message set from ρt(xi , xj ) and at(xi , xj ) after not included in convergence ( lines 27 30 ) . Note that rt(xi , xj ) =ρ t(xi , xj ) and at(xi , xj ) = αt(xi , xj ) after convergence , as shown in the previous study [ 6 ] . Finally , the algorithm determines the exemplar of each data point from the messages in after convergence ( lines 31 32 ) . and
&= or
As shown in Algorithm 1 , F AP does not require any use specified inner parameters . Even though we have assumed single CPU implementation , our approach can be parallelized since the estimates and the message sets can be computed independently . For example , upper estimate r(xi , xj ) of Equation ( 8 ) can be parallelized for data point xj since maxxk"=xj{s(xi , xk)} in Equation ( 10 ) has the same score in all upper estimates if data point xj is common in each estimate . Our approach can be a simple alternative to the original Affinity Propagation algorithm . 3.5 Theoretical Analyses
This section contains the theoretical analyses of the clustering results and the computation cost of F AP . We will use the following theorem when discussing the clustering results :
Theorem 1
( Clustering Results of F AP ) . F AP outputs exactly the same clustering results as the original Affinity Propagation algorithm . t and availability set
Proof The original Affinity Propagation algorithm iteratively updates the messages of all data point pairs until convergence . On the other hand , our approach iteratively updates messages only for the pairs that are included in the t . From Lemma 9 responsibility set and 10 , the responsibility or availability of a pair can change value in the ( t + 1) th iteration only if it is included in t or t . Therefore , it is clear that F AP can obtain the same message values as the original even though it restricts the t . Algorithm 1 computes the message updates to exemplar of each data point from the messages of the pairs . Lemma 5 guarantees that it obtains the in message set . As a resame clustering results after the iterations of sult , it theoretically yields the same clustering results as the original algorithm . 2 t and
As described in Section 2 , the original algorithm needs O(N 2T ) time . This is because it iteratively updates the messages of all data point pairs until convergence , even if most of them have already converged . The following theorem describes the computation cost of F AP .
Theorem 2
( Computation Cost of F AP ) . Let M be the average number of updated pairs in the iterations , the proposed approach requires O(N 2 + M T ) time to obtain the clustering results .
Proof F AP first computes upper and lower estimates for the data point pairs before commencing the iterations . Since the estimates are computed in a non iterative manner , this process requires O(N 2 ) time . Next , it computes t and availability the messages for the responsibility set t of the t th iteration . The computation cost of the set iterative computations is O(M T ) time . After the iterations , our approach obtains the responsibilities and availabilities of , and computes exemplars from the pairs in message set the obtained messages ; this requires O(N 2 ) time . As a result , the proposed approach needs O(N 2 + M T ) time to obtain the clustering results . 2
Theorem 1 and 2 indicate that our approach is more efficient at obtaining clusters and corresponding exemplars than the original approach and does not sacrifice clustering performance .
4 . EXPERIMENTAL EVALUATION
We performed experiments to demonstrate the effectiveness of our approach . In what follows , the labels “ F AP ” , “ Graph AP ” , “ FSAP ” , and “ Original ” represent our approach , the state of the art graph based approach by Fujiwara et al . [ 6 ] , the two stage approach by Jia et al . [ 12 ] , and the original Affinity Propagation algorithm [ 4 ] . Note that the details of Graph AP and FSAP are described in Section 5 .
We used three standard datasets : • Vowel [ 1 ] : This dataset contains spoken vowels of English . For each speech signal , linear predictive analysis was carried out on Hamming windowed segments [ 23 ] from the steady part of the vowel . The reflection coefficients were used to compute 10 log area parameters , giving a 10 dimensional input space . The number of vowels is 528 .
• TDT2 [ 2 ] : This dataset is based on the Nist Topic Detection and Tracking corpus . This corpus is taken from six sources , ie , two newswires ( APW , NYT ) , two radio programs ( VOA , PRI ) and two television programs ( CNN , ABC ) . This dataset contains 36 , 771 distinct terms ; we used tf idf vector as the feature .
• PubFig [ 14 ] : This dataset consists of 58 , 797 images of famous people . We represented each image by using a set of attributes , a state of the art semantic image representation framework [ 14 ] . Each attribute itself is a semantic description of images relevant to human faces ; they were automatically detected by pre trained classifiers . We used 73 attributes published by [ 14 ] .
The experiments used the negative Euclidean distance as the similarity between data points . We set the preference to the median of the input similarities , the damping factor λ to 0.5 , and the number of iterations to T = 1 , 000 in accordance with the previous paper [ 4 , 6 ] . For FSAP , we set K = 0.3×N when computing the K nearest neighbor graph . All experiments were conducted on a Linux 2.70 GHz Intel Xeon server . We implemented all approaches using GCC . 4.1 Efficiency of F AP
We assessed the clustering times taken by each approach . Figure 1 indicates that F AP is much faster than the previous approaches . These results show that F AP cuts the clustering time by up to 80 % , 85 % , and 90 % from those of Graph AP , FSAP , and the original approach . As described
316 ] s [ e m a W i t k c o c l l l
106 105 104 103 102 101 100 10 1
F AP Graph AP FSAP Original
1
F AP Graph AP FSAP e r u s a e m F
0.75
0.5
Vowel
TDT2
PubFig
0.25
0
Vowel
TDT2
PubFig
Figure 1 : Clustering time .
Figure 2 : F measure . above , F AP updates much fewer messages than the original approach . In addition , it updates fewer messages than either Graph AP or FSAP because it dynamically skips unnecessary message updates . We investigate the effectiveness of dynamic skips in Section 43 4.2 Accuracy of Clustering Results
One major advantage of F AP is that it is theoretically guaranteed to get the same clustering results as the original approach . In order to demonstrate this advantage , we evaluated F measure of the obtained exemplars against those of the original approach for each dataset . F measure is the weighted harmonic means of precision and recall [ 16 ] . Here , precision is the fraction of exemplars by each approach that matches those of the original approach , and recall is the fraction of exemplars obtained by the original approach that were extracted by each approach . F measure is hence 1 if the exemplars exactly match those of the original approach . We show F measure of each approach in Figure 2 .
Figure 2 indicates that F AP obtains the same exemplars as the original approach . Even though it uses estimates to limit the message updates , it is theoretically assured of outputting the same responsibilities , availabilities , and exemplars as the original . In addition , its dynamic update has the property that it can compute exactly the same messages as the original approach . Therefore , as shown in Figure 2 , F measure of our approach is 1 . Graph AP ’s F measure is also 1 , since it computes the same messages as the original after convergence . On the other hand , FSAP outputs different clusters from those of the original . This is because FSAP is based on the heuristic technique of using K nearest neighbor graphs to link data points and reduce the clustering time . Figure 2 , along with Figure 1 , confirms that F AP is superior to Graph AP in terms of speed and to FSAP in terms of accuracy and speed . 4.3 Skipping Message Updates
We dynamically skip unnecessary message updates . In order to demonstrate the effectiveness of this technique , Figure 3 plots the updated messages in each iteration for the responsibility of the Vowel dataset . The ( i , j) th element is plotted as a red dot if its corresponding message is updated ; it is plotted as a blue dot otherwise . Note that the results for the responsibilities of the other data sets were similar to those depicted here .
As shown in the figure , F AP reduces the number of updated messages as the iterations increase , whereas the other approaches update a constant number of messages . As a result , F AP updates far fewer messages than the original and the state of the art approaches . The results also indicate that F AP successfully skips message updates of most data point pairs . For example , the number of updated messages at t = 100 is significantly smaller than at t = 10 . This t = 10 t = 50 t = 100 ( a ) F AP t = 500 t = 1 , 000 t = 10 t = 50 t = 100 t = 500 t = 1 , 000
( b ) Graph AP t = 10 t = 50 t = 100 t = 500 t = 1 , 000
( c 1 ) FSAP 1 st Stage t = 10 t = 50 t = 100 t = 500 t = 1 , 000
( c 2 ) FSAP 2 nd Stage
Figure 3 : Updated messages in each iteration . is because most messages converge at low iteration numbers . Consequently , clustering speed rises with iteration number . As shown in Figure 3 , F AP has similar block wise patterns in the message updates when t = 10 to Graph AP and FSAP . The block wise patterns imply that data points in a block have high similarities and construct a cluster as shown in the results of FSAP . Since we can effectively prune message updates between clusters as shown in Figure 3 , F AP is expected to increase its clustering speed as the number of clusters increases .
In addition , even though Graph AP uses a sparse graph structure to speed up clustering , the graph structure is static across the iterations ; Graph AP must update the messages for all edges in the graph even if most have already converged . This is because its estimates do not theoretically give upper and lower bounds before convergence ; it cannot guarantee the same clustering results if it skips message updates according to its estimates . On the other hand , FSAP constructs a K nearest neighbor graph in the first stage , and in second stage adds edges to improve clustering quality . However , as shown in Figure 2 , it fails to attain the same results as the original approach . This indicates that linking only data point pairs of high similarity is not sufficient to guarantee the same clustering results . In addition , although FSAP has fewer messages to update than Graph AP , as shown in Figure 3 , it is slower than Graph AP , as shown in Figure 1 . This is because FSAP has a two stage update , unlike Graph AP . The superiority of our approach over the previous approaches is that it can automatically determine the messages of data point pairs on the basis of the given similarities , thereby speeding up the clustering without affecting the results .
5 . RELATED WORK
Clustering is one of the major techniques for unsupervised discovery of hidden structures from complex datasets which is revealed by a large number of data points [ 5 , 7 ] . Since
317 clustering is used in a variety of application domains , many clustering algorithms have been proposed [ 21 , 22 , 25 ] .
Affinity Propagation has high clustering quality [ 4 , 10 , 19 ] , and for this reason , many approaches have been based on it . Shang et al . proposed a clustering approach that considers both the local and global structures of the datasets [ 20 ] . Zhang et al . studied the effectiveness of sparse L1 graphs [ 5 ] in terms of clustering quality [ 28 ] . Zhang et al . proposed a variant of Affinity Propagation that outputs a given number of clusters [ 29 ] .
Several approaches have been devised to tackle the high computation cost of Affinity Propagation . FSAP , proposed by Jia et al . , consists of two stages [ 12 ] . The first stage constructs a K nearest neighbor graph based on similarities to compute messages iteratively . In order to increase the clustering quality , the second stage adds edges to the graph on the basis of three criteria : ( 1 ) If a data point is the exemplar of another data point , they are connected ; ( 2 ) for data points xi and xj , if there exists two of the K nearest data points that take xi and xj as their exemplars , respectively , then xi and xj are connected ; and ( 3 ) if xi and xj are connected by the second criterion , all data points that choose xi as the exemplar are connected to xj , and vice versa . Although their approach does not guarantee the same clustering results as the original , it is faster than the original approach .
To increase the clustering speed of Affinity Propagation , Fujiwara et al . proposed Graph AP ; it constructs a sparse graph structure by computing the upper and lower estimates [ 6 ] . They compute estimates for responsibility and availability on the basis of the property that rt(xi , xj ) = ρt(xi , xj ) and at(xi , xj ) = αt(xi , xj ) hold after convergence . By pruning unnecessary messages before the iterations , Graph AP efficiently updates the messages . If all messages converge , it outputs the same results as the original approach . However , since its estimates do not , theoretically , give upper and lower bounds before the messages reach convergence , they cannot be used to skip unnecessary message updates . Therefore , Graph AP has to update messages for all edges in the iterations even if only a few messages remain unconverged . For this reason , Graph AP is less efficient than our approach .
6 . CONCLUSIONS
We addressed the problem of the low clustering speed of Affinity Propagation . Our approach , F AP , ( 1 ) computes upper and lower estimates for each message and ( 2 ) skips unnecessary message updates in order to efficiently obtain the same clustering results as the original approach . Experiments on real world data sets showed that F AP is efficient without sacrificing clustering performance . Affinity Propagation is fundamental to many applications . F AP will allow them to be implemented more efficiently , and it will help to improve the effectiveness of future applications as well .
7 . REFERENCES [ 1 ] K . Bache and M . Lichman . UCI Machine Learning
Repository , 2013 .
[ 2 ] D . Cai , X . Wang , and X . He . Probabilistic Dyadic Data Analysis with Local and Global Consistency . In ICML , pages 105–112 , 2009 .
[ 3 ] J . T . Dudley , T . Deshpande , and A . J . Butte . Exploiting
Drug ? Disease Relationships for Computational Drug Repositioning . Briefings in Bioinformatics , 12(4):303–311 , 2011 .
[ 4 ] B . J . Frey and D . Dueck . Clustering by Passing Messages between Data Points . Science , 315:972–976 , 2007 .
[ 5 ] Y . Fujiwara and G . Irie . Efficient Label Propagation . In
ICML , pages 784–792 , 2014 .
[ 6 ] Y . Fujiwara , G . Irie , and T . Kitahara . Fast Algorithm for
Affinity Propagation . In IJCAI , pages 2238–2243 , 2011 .
[ 7 ] Y . Fujiwara , G . Irie , S . Kuroyama , and M . Onizuka . Scaling
Manifold Ranking Based Image Retrieval . PVLDB , 8(4):341–352 , 2014 .
[ 8 ] D . S . Gunderson . Handbook of Mathematical Induction :
Theory and Applications . Chapman and Hall/CRC , 2010 .
[ 9 ] J . Han and M . Kamber . Data Mining : Concepts and
Techniques . Morgan Kaufmann , 2011 .
[ 10 ] R . Hu , B . M . Namee , and S . J . Delany . Off to a Good
Start : Using Clustering to Select the Initial Training set in active learning . In FLAIRS , 2010 .
[ 11 ] Y . Ida , T . Nakamura , and T . Matsumoto .
Domain dependent/independent Topic Switching Model for Online Reviews with Numerical Ratings . In CIKM , pages 229–238 , 2013 .
[ 12 ] Y . Jia , J . Wang , C . Zhang , and X S Hua . Finding Image
Exemplars Using Fast Sparse Affinity Propagation . In ACM MM , pages 639–642 , 2008 .
[ 13 ] F . R . Kschischang , B . J . Frey , and H . Loeliger . Factor
Graphs and the Sum product Algorithm . IEEE Transactions on Information Theory , 47(2):498–519 , 2001 . [ 14 ] N . Kumar , A . C . Berg , P . N . Belhumeur , and S . K . Nayar .
Attribute and Simile Classifiers for Face Verification . In ICCV , pages 365–372 , 2009 .
[ 15 ] J . Leskovec , A . Rajaraman , and J . D . Ullman . Mining of
Massive Datasets . Cambridge University Press , 2014 .
[ 16 ] C . D . Manning , P . Raghavan , and H . Schutz . Introduction to Information Retrieval . Cambridge University Press , 2008 .
[ 17 ] M . Nakatsuji and Y . Fujiwara . Linked Taxonomies to
Capture Users’ Subjective Assessments of Items to Facilitate Accurate Collaborative Filtering . Artif . Intell . , 207:52–68 , 2014 .
[ 18 ] M . Nakatsuji , Y . Fujiwara , H . Toda , H . Sawada , J . Zheng , and J . A . Hendler . Semantic Data Representation for Improving Tensor Factorization . In AAAI , pages 2004–2012 , 2014 .
[ 19 ] A . Rangrej , S . Kulkarni , and A . V . Tendulkar . Comparative Study of Clustering Techniques for Short Text Documents . In WWW , pages 111–112 , 2011 .
[ 20 ] F . Shang , L . C . Jiao , J . Shi , F . Wang , and M . Gong . Fast Affinity Propagation Clustering : A Multilevel Approach . Pattern Recognition , 45(1):474–486 , 2012 .
[ 21 ] H . Shiokawa , Y . Fujiwara , and M . Onizuka . Fast Algorithm for Modularity based Graph Clustering . In AAAI , 2013 .
[ 22 ] H . Shiokawa , Y . Fujiwara , and M . Onizuka . SCAN++ :
Efficient Algorithm for Finding Clusters , Hubs and Outliers on Large scale Graphs . PVLDB , 8(11 ) , 2015 .
[ 23 ] S . W . Smith . The Scientist & Engineer ’s Guide to Digital
Signal Processing . California Technical Pub , 1997 .
[ 24 ] M . Toyoda , Y . Sakurai , and Y . Ishikawa . Pattern Discovery in Data Streams under the Time Warping Distance . VLDB J . , 22(3):295–318 , 2013 .
[ 25 ] J . Vlasblom and S . J . Wodak . Markov Clustering versus
Affinity Propagation for the Partitioning of Protein Interaction Graphs . BMC Bioinformatics , 10 , 2009 .
[ 26 ] P . S . Yu , J . Han , and C . Faloutsos . Link Mining : Models ,
Algorithms , and Applications . Springer , 2010 .
[ 27 ] Z J Zha , L . Yang , T . Mei , M . Wang , and Z . Wang . Visual
Query Suggestion . In ACM MM , pages 15–24 , 2009 .
[ 28 ] X . Zhang and J . C . Lv . Sparse Affinity Propagation for
Image Analysis . JSW , 9(3):748–756 , 2014 .
[ 29 ] X . Zhang , W . Wang , K . Nørv˚ag , and M . Sebag . K AP :
Generating Specified K Clusters by Efficient Affinity Propagation . In ICDM , pages 1187–1192 , 2010 .
318
