Warm Start for Parameter Selection of Linear Classifiers
Bo Yu Chu
Dept . of Computer Science National Taiwan Univ . , Taiwan r02222047@ntuedutw
Chia Hua Ho
Dept . of Computer Science National Taiwan Univ . , Taiwan b95082@csientuedutw
Cheng Hao Tsai
Dept . of Computer Science National Taiwan Univ . , Taiwan r01922025@csientuedutw
Chieh Yen Lin
Dept . of Computer Science National Taiwan Univ . , Taiwan r01944006@csientuedutw
ABSTRACT In linear classification , a regularization term effectively remedies the overfitting problem , but selecting a good regularization parameter is usually time consuming . We consider cross validation for the selection process , so several optimization problems under different parameters must be solved . Our aim is to devise effective warm start strategies to efficiently solve this sequence of optimization problems . We detailedly investigate the relationship between optimal solutions of logistic regression/linear SVM and regularization parameters . Based on the analysis , we develop an efficient tool to automatically find a suitable parameter for users with no related background knowledge .
Keywords warm start ; regularization parameter ; linear classification
1 .
INTRODUCTION
Linear classifiers such as logistic regression and linear SVM are commonly used in machine learning and data mining . Because directly minimizing the training loss may overfit the training data , the concept of regularization is usually applied . A linear classifier thus solves an optimization problem that involves a parameter ( often referred to C ) to balance the training loss and the regularization term . Selecting the regularization parameter is an important but difficult practical issue . An inappropriate setting may not only cause overfitting or underfitting , but also a lengthy training time . Several reasons make parameter selection a time consuming procedure . First , usually the search involves sweeping the following sequence of parameters
Cmin , ∆Cmin , ∆2Cmin , . . . , Cmax ,
( 1 ) where ∆ is a given factor . At each parameter the performance must be estimated by , for example , cross validation ( CV ) . Thus a sequence of training tasks are conducted , and Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . Copyrights for components of this work owned by others than the author(s ) must be honored . Abstracting with credit is permitted . To copy otherwise , or republish , to post on servers or to redistribute to lists , requires prior specific permission and/or a fee . Request permissions from Permissions@acmorg KDD '15 , August 10 13 , 2015 , Sydney , NSW , Australia . Copyright is held by the owner/author(s ) . Publication rights licensed to ACM . ACM 978 1 4503 3664 2/15/08 . . . $1500 http://dxdoiorg/101145/27832582783332
Chih Jen Lin
Dept . of Computer Science National Taiwan Univ . , Taiwan cjlin@csientuedutw we may need to solve many optimization problems . Secondly , if we do not know the reasonable range of the parameters , we may need a long time to solve optimization problems under extreme parameter values .
In this paper , we consider using warm start to efficiently solve a sequence of optimization problems with different regularization parameters . Warm start is a technique to reduce the running time of iterative methods by using the solution of a slightly different optimization problem as an initial point for the current problem . If the initial point is close to the optimum , warm start is very useful . Recently , for incremental and decremental learning , where a few data instances are added or removed , we have successfully applied the warm start technique for fast training [ 20 ] . Now for parameter selection , in contrast to the change of data , the optimization problem is slightly modified because of the parameter change . Many considerations become different from those in [ 20 ] . As we will show in this paper , the relationship between the optimization problem and the regularization parameter must be fully understood .
Many past works have applied the warm start technique for solving optimization problems in machine learning methods . For kernel SVM , [ 5 , 15 ] have considered various initial points from the solution information of the previous problem . While they showed effective time reduction for some data sets , warm start for kernel SVM has not been widely deployed because of the following reasons . An initial point obtained using information from a related optimization problem may not cause fewer iterations than a naive initial point ( zero in the case of kernel SVM ) .
When kernels are used , SVM involves both regularization and kernel parameters . The implementation of a warmstart technique can be complicated . For example , we often cache frequently used kernel elements to avoid their repeated computation , but with the change of kernel parameters , maintaining the kernel cache is very difficult .
In fact , [ 15 ] was our previous attempt to develop warmstart techniques for kernel SVM , but results are not mature enough to be included in our popular SVM software LIBSVM [ 2 ] . In contrast , the situation for linear classification is simpler because the regularization parameter is the only parameter to be chosen , and
as pointed out in [ 20 ] and other works , it is more flexible to choose optimization methods for linear rather than kernel . We will show that for different optimization methods , the effectiveness of warm start techniques varies .
149 In this work we focus on linear classification and make warm start an efficient tool for the parameter selection .
Besides warm start techniques , other methods have been proposed for the parameter selection of kernel methods . For example , Hastie et al . [ 9 ] show that for L1 loss SVM , solutions are a piece wise linear function of regularization parameters . They obtain the regularization path by updating solutions according to the optimality condition . This approach basically needs to maintain and manipulate the kernel matrix , a situation not applicable for large data sets . Although this difficulty may be alleviated for linear classification , we still see two potential problems . First , the procedure to obtain the path depends on optimization problems ( eg , primal and dual ) and loss functions . Second , although an approximate solution path can be considered [ 8 ] , the regularization path may still contain too many pieces of linear functions for large data sets . Therefore , in the current study , we do not pursue this direction for the parameter selection of linear classification . Another approach for parameter selection is to minimize a function of parameters that approximates the error ( eg , [ 18 , 3 , 4] ) . However , minimizing the estimation may not lead to the the best parameters , and the implementation of a two level optimization procedure is complicated . Because linear classifiers involves a single parameter C , simpler approaches might be appropriate .
While traditional linear classification considers L2 regularization ( see formulations in Section 1.1 ) , recently L1 regularization has been popular because of its sparse model . Warm start may be very useful for training L1 regularized problems because some variables may remain to be zero after the change of parameters . In [ 13 , 7 ] , warm start is applied to speed up two optimization methods : interior point method and GLMNET , respectively . However , these works consider warm start as a trick without giving a full study on parameter selection . They investigate neither the range of parameters nor the relation between optimization problems and parameters . Another work [ 19 ] proposes a screening approach that pre identifies some zero elements in the final solution . Then a smaller optimization problem is solved . They apply warm start techniques to keep track of nonzero elements of the solutions under different parameters . However , the screening approach is not applicable to L2 regularized classifiers because of the lack of sparsity . While warm start for L1 regularized problems is definitely worth investigating , to be more focused we leave this topic for future studies and consider only L2 regularized classification in this work .
To achieve automatic parameter selection , we must identify a possible range of parameter values and decide when to stop the selection procedure . None of the work mentioned above except [ 19 ] has discussed the range of the regularized parameter . The work [ 21 ] finds a lower bound of C values for kernel SVM by solving a linear program , but for linear SVM , the cost may be too high . Another study [ 14 ] proposes using discrete optimization for the automatic parameter selection of any classification method . Their procedure stops if better parameters cannot be found after a few trials . By targeting at L2 regularized linear classification , we will derive useful properties to detect the possible parameter range .
In this work we develop a complete and automatic parameter selection procedure for L2 regularized linear classifiers with the following two major contributions . First , by carefully studying the relationship between optimization problems and regularization parameters , we obtain and justify an effective warm start setting for fast cross validation across a sequence of regularization parameters . Second , we provide an automatic parameter selection tool for users with no background knowledge . In particular , users do not need to specify a sequence of parameter candidates .
This paper is organized as follows . In the rest of this section , we introduce the primal and dual formulations of linear classifiers . In Section 2 , we discuss the relation between optimization problems and regularization parameters . Based on the results , we propose an effective warm start setting to reduce the training time in Section 3 . To verify our analysis , Section 4 provides detailed experiments . The conclusions are in Section 5 . This research work has lead to a useful parameter selection tool1 extended from the popular package LIBLINEAR [ 6 ] for linear classification . Because of space limitation , we give proofs and more experiments in supplementary materials.1 1.1 Primal and Dual Formulations
Although linear classification such as logistic regression ( LR ) and linear SVM have been well studied in literatures ( eg , a survey in [ 22] ) , for easy discussion we briefly list their primal and dual formulations by mainly following the description in [ 20 , Section 2 ] . Consider ( label , feature vector ) pairs of training data ( yi , xi ) ∈ {−1 , 1} × Rn , i = 1 , . . . , l . A linear classifier obtains its model by solving the following optimization problem . w2 + CL(w ) ,
( 2 ) where min f ( w ) ≡ 1 2 w i=1
L(w ) ≡l log(1 + e−ywT x )
ξ(w ; xi , yi ) , is the sum of training losses , ξ(w ; x , y ) is the loss function , and C is a user specified regularization parameter to balance the regularization term w2/2 and the loss term L(w ) . LR and linear SVM consider the following loss functions .
ξ(w ; x , y ) ≡ logistic ( LR ) loss , max(0 , 1 − ywT x ) L1 loss , max(0 , 1 − ywT x)2 L2 loss .
( 3 )
As indicated in [ 20 ] , these loss functions have different differentiability , so applicable optimization methods may vary . Instead of solving problem ( 2 ) of variable w , it is well known that the optimal w can be represented as a linear combination of training data with coefficient α ∈ Rl . w = l f D(α ) ≡l i=1 yiαixi .
( 4 )
Then one can solve an optimization problem over α . An example is the following dual problem of ( 2 ) . h(αi , C ) − 1 2
αT ¯Qα max i=1
α subject to 0 ≤ αi ≤ U,∀i = 1 , . . . , l , where ¯Q = Q + D ∈ Rl×l , Qij = yiyjxT with Dii = d,∀i , and
( 5 ) i xj , D is diagonal
U =
C ∞ d =
0 1 2C for L1 loss SVM and LR , for L2 loss SVM .
1http://wwwcsientuedutw/~cjlin/libsvmtools/ warm start/ .
150 for L1 loss and L2 loss SVM ,
αi C log C − αi log αi − ( C − αi ) log(C − αi ) for LR .
We refer to ( 2 ) as the primal problem .
We define some notations for later use .
y1
Y ≡
 and X ≡
xT
xT l
1
 ∈ Rl×n .
( 6 )
. . . yl
Further , e ≡ [ 1 , 1 , . . . , 1]T ∈ Rl and 0 ≡ [ 0 , 0 , . . . , 0]T .
2 . OPTIMIZATION PROBLEMS AND REG
ULARIZATION PARAMETERS
This section studies the relationship between the optimal solution and the regularization parameter C . We focus on the case of large C because the optimization problems become more difficult.2 We separately discuss primal and dual solutions in Sections 2.1 and 2.2 , respectively . All proofs are in the supplementary materials . 2.1 Primal Solutions
Because f ( w ) is strongly convex in w , an optimal solution of ( 2 ) exists and is unique [ 1 , Lemma 233 ] We define wC as the unique solution under parameter C , and W∞ as the set of points that attain the minimum of L(w ) . )} .
W∞ ≡ {w | L(w ) = inf w L(w
We are interested in the asymptotic behavior of the solution wC when C → ∞ . The following theorem shows that {wC} converges to a point in W∞ .
Theorem 1 Consider any nonnegative and convex loss function ξ(w ; x , y ) . If W∞ = φ , then
C→∞ wC = w∞ , where w∞ = arg min lim w∈W∞ w2 .
( 7 )
Next we check if Theorem 1 is applicable to the three loss functions in ( 3 ) . It is sufficient to prove that W∞ = φ . 211 L1 loss and L2 loss SVM For L1 loss , the asymptotic behavior of {wC} was studied in [ 12 ] . Theorem 3 of [ 12 ] proves that there exist C∗ and w∗ such that wC = w∗,∀C ≥ C∗ . Later in Theorem 6 , we prove the same result by Theorem 1 and properties in [ 11 ] . To see if W∞ = φ needed by Theorem 1 holds , we have that inf w L(w ) can be written as the following linear program . l min w,ξ
ξi i=1
Following [ 20 ] , we have h(αi , C ) = subject to ξi ≥ 1 − yiwT xi ,
ξi ≥ 0 ,
∀i .
For L2 loss SVM , the situation is similar . The following
It has a feasible solution ( w , ξ ) = ( 0 , e ) , and the objective value is non negative , so from [ 17 , Theorem 423 ( i) ] , a minimum is attained and W∞ = φ . theorem shows that W∞ = φ . Theorem 2 If L2 loss is used , then W∞ = φ and {wC} converges to w∞ .
2This has been mentioned in , for example , [ 10 ] .
212 Logistic Regression For LR , the situation is slightly different from that of linear SVM because W∞ may not exist . We explain below that if the data set is separable , then it is possible that
( 8 ) and no minimum is attained because L(w ) > 0,∀w . For separable data , generally there exists a vector w such that inf w L(w ) = 0 yiwT xi > 0,∀i .
Then ( 8 ) holds because l i=1
L(∆w ) = log(1 + e
−yi∆wT xi ) → 0 as ∆ → ∞ .
The above discussion indicates that only if data are not separable may we have a non empty W∞ . The following definition formally defines non separable data .
Definition 1 A data set is not linearly separable if for any w = 0 , there is an instance xi such that
( 9 ) We have the following theorem on the convergence of {wC} . yiwT xi < 0 .
Theorem 3 If LR loss is used and the non separable condition ( 9 ) holds , then w∞ exists and {wC} converges to w∞ . 2.2 Dual Solutions
Let αC be any optimal solution of the dual problem ( 5 ) . Subsequently we will investigate the relationship between αC and C . An important difference from the analysis for primal solutions is that αC may not be unique . This situation occurs if the dual objective function is only convex rather than strictly convex ( eg , L1 loss ) . Therefore , we analyze L2 and LR losses first because their αC is unique . 221 L2 loss SVM and Logistic Regression The following theorem shows the asymptotic relationship between αC and C .
Theorem 4 If L2 loss is used , then lim C→∞
( αC )i
C
= 2 max(0 , 1 − yiwT∞xi ) , i = 1 , . . . , l .
( 10 )
If LR loss is used and the non separable condition ( 9 ) is satisfied , then lim C→∞
( αC )i
C
= e−yiwT∞xi
1 + e−yiwT∞xi
, i = 1 , . . . , l .
( 11 )
From Theorem 4 , αC is unbounded for non separable data because the right hand side in ( 10 ) and ( 11 ) is non zero . Therefore , we immediately have the following theorem .
Theorem 5 If the problem is not linearly separable , then
αC → ∞ as C → ∞ .
Theorem 4 indicates that for non separable data , αC is asymptotically a linear function of C . In Section 3.1 , we will use this property to choose the initial solution after C is increased . For separable data , the right hand side in ( 10 )
151 l i=1 and ( 11 ) may be zero , so the asymptotic linear relationship between αC and C may not hold . For simplicity , we omit giving detailed analysis on separable data . Further , such data are often easier for training . 222 L1 loss SVM Although for L1 loss , the dual optimal αC may not be unique , there exists a solution path from the earlier work in [ 11].3 We extend their result to have the following theorem .
Theorem 6 If L1 loss is used , then there are vectors v1 and v2 , and a threshold C∗ such that after C ≥ C∗ ,
αC ≡ v1C + v2
( 12 ) is a dual optimal solution . The primal solution wC = ( Y X)T αC is suitable for L2 or LR loss . We explain that ( 14 ) is also useful for L1 loss although in Theorem 6 , αC is not a multiple of C , and αC is only one of the optimal solutions . Instead , we consider Theorem 7 , where αC is any optimal solution rather than the specific one in ( 12 ) of Theorem 6 . Because in general yiwT∞xi = 1 , most of αC ’s elements are either zero or C , and hence they are multiples of C . Thus , ( 14 ) is a reasonable initial solution . Note that if αC is feasible for ( 5 ) under C , then ¯α also satisfies the constraints under ∆C . Approaches similar to ( 14 ) to set the initial point for warm start can be found in some previous works such as [ 5 ] . 3.2 A Comparison on the Effectiveness of Pri mal and Dual Initial Solutions
In this subsection , we show that from some aspects , warm start may be more useful for a primal based training method .
First , from the primal dual relationship ( 4 ) , we have is a constant vector the same as w∞ , where X and Y are defined in ( 6 ) . Further , wC =
( αC )iyixi .
( 15 )
( Y X)T v1 = 0 and ( Y X)T v2 = w∞ .
The following extension of Theorem 6 shows that , similar to Theorem 4 , most elements of αC become a multiple of C .
Theorem 7 Assume L1 loss is used . There exists a C∗ such that after C ≥ C∗ , any dual optimal solution αC satisfies
( αC )i = C if yiwT∞xi < 1 , ( αC )i = 0 if yiwT∞xi > 1 .
This theorem can be easily obtained by the fact that wC = w∞,∀C ≥ C∗ from Theorem 6 and the optimality condition . Like Theorem 5 , we immediately get the unboundedness of {αC} from Theorem 7 .
3 . WARM START FOR PARAMETER SE
LECTION
In this section , we investigate issues in applying the warmstart strategy for solving a sequence of optimization problems under different C values . The purpose is to select the parameter C that achieves the best CV accuracy . 3.1 Selection of Initial Solutions
We consider the situation when
However , by Theorems 1 and 5 , interestingly wC → w∞ but αC → ∞ .
Therefore , on the right hand side of ( 15 ) apparently some large values in αC are cancelled out after taking yixi into consideration . The divergence of αC tends to cause more difficulties in finding a good initial dual solution .
Next , we check primal and dual initial objective values after applying warm start . The initial ¯α based on ( 14 ) gives the following dual objective value . For simplification , we use w and α to denote wC and αC , respectively . Then ,
α
D ∆ h(∆α , ∆C ) − 1 2
∆2αT
Q +
= ∆h(α , C ) − 1 2
∆2αT
Q +
D ∆
= ∆
αT ( Q + D)α + wT w + C
1 2
α
− 1 2
∆2αT
Q +
D ∆
1 l
2
= C∆ i=1
ξ(w ; xi , yi ) + ( ∆ − ∆2/2)wT w , where ( 16 ) is from h(∆α , ∆C ) = ∆h(α , C )
α l i=1
( 16 )
ξ(w ; xi , yi )
( 17 )
( 18 )
C is increased to ∆C , in Eq ( 31 ) of [ 20 ] , ( 17 ) is from that primal and dual optimal objective values are equal , and ( 18 ) is from that at optimum , where ∆ > 1 . At the current C , let wC be the unique primal optimal solution , while αC be any dual optimal solution . We then discuss suitable initial solutions ¯w and ¯α for the new problem with the parameter ∆C .
From Theorems 1 and 6 , wC is closed to ( or exactly the same as ) w∞ after C is large enough , so naturally can be an initial solution . If a dual problem is solved , from Theorem 4 ,
¯α = ∆αC
( 14 )
αT Qα = wT w .
∆ − ∆2/2
Note that l is a decreasing function after ∆ ≥ 1 . If ∆ = 2 , we have i=1
≤ primal or dual optimal objective value at 2C ≤ 1 2
¯wT ¯w + 2C
ξ( ¯w ; xi , yi ) . i=1 l
( 19 )
¯w = wC
( 13 )
( 18 ) = 2C
ξ(w ; xi , yi )
3In [ 11 ] , the loss function has an additional bias term : max(0 , 1 − y(wT x + b) ) . A careful check shows that their results hold without b .
Note that ¯w = w from ( 13 ) . If ¯w is close to w∞ , then the primal initial objective value should be close to the optimum . In contrast , from ( 19 ) we can see that the dual initial
152 objective value lacks a wT w/2 term . Therefore , it should be less close to the optimal objective value .
Another difference between primal and dual based approaches is on the use of high order ( eg , Newton ) or loworder ( eg , gradient ) optimization methods . It has been pointed out in [ 20 ] that a high order method takes more advantages of applying warm start . The reason is that if an initial solution is close to the optimum , a high order optimization method leads to fast convergence . Because the primal problem is unconstrained , it is easier to solve it by a high order optimization method . In Section 4 , we will experimentally confirm results discussed in this Section . 3.3 Range of Regularization Parameters
In conducting parameter selection in practice , we must specify a range of C values that covers the best choice . Because small and large C values cause underfitting and overfitting , respectively , and our procedure gradually increases C , all we need is a lower and an upper bound of C .
331 Lower Bound of C We aim at finding a C value so that for all values smaller than it , underfitting occurs . If the optimal w satisfies yiwT xi < 1,∀i ,
( 20 ) then we consider that underfitting occurs . The reason is that every instance imposes a non zero L1 or L2 loss .
We then check for what C values , ( 20 ) holds . For L1 and
LR losses , if
For small C values , the CV accuracy is stable , so the selection procedure may stop pre maturely . See more discussions in Section II of the supplementary materials .
The CV accuracy may not be a monotonic increasing function before the best C . If the CV accuracy is locally stable or even becomes lower in an interval , than the procedure may stop before the best C .
For linear classification , based on results in Section 2 , we will describe a setting to terminate the search process by checking the change of optimal solutions . In Section 2 , we showed that {wC} converge to w∞ , and for LR and L2 loss SVM , {αC /C} converges . Therefore , if for several consecutive C values , {wC} or {αC /C} does not change much , then wC should be close to w∞ and there is no need to further increase the C value ; see more theoretical support later in Theorem 8 . We thus propose a setting of increasing C until either C ≤ ¯C , where ¯C is a pre specified constant , or optimal wC or αC /C is about the same as previous solu tions . Deriving an upper bound ¯C is more difficult than a lower bound in Section 331 However , with the second condition , a tight upper bound may not be needed . Thus in our experiment , we simply select a large value .
For the second condition of checking the change of optimal solutions , our idea is as follows . For optimal wC/∆ or αC/∆ at C/∆ , we see if it is a good approximate optimal solution at the current C . For easy description , we rewrite f ( w ) as f ( w ; C )
( 21 ) to reflect the regularization parameter . We stop the procedure for parameter selection if
∇f ( w∆t−1C ; ∆tC ) ≤ ∇f ( 0 ; ∆tC ) for t = −2,−1 , 0 ,
( 24 )
1
C < l maxi xi2 , then from ( 4 ) and the property αi ≤ C , yiwT xi ≤ |wT xi| ≤l
|αj||xT j xi| < 1 . j=1
For L2 loss SVM , if
C <
1
2l maxi xi2 ,
( 22 ) then yiwT xi ≤ wxi ≤w2 + 2CL(w ) maxj xj ( 23 )
≤2f ( 0 ) maxj xj ≤
√ 2Cl maxj xj < 1 , where ( 23 ) is from that w minimizes f ( · ) .
The value derived in ( 21 ) and ( 22 ) can be considered as the initial Cmin for the parameter search . One concern is that it may be too small so that many unneeded optimization problems are solved . In Section 4.4 , we show that optimization methods are very fast when C is small , so the cost of considering some small and useless C values is negligible .
332 Upper Bound of C Clearly , an upper bound should be larger than the best C . However , unlike the situation in finding a lower bound , where optimization problems with small C values are easy to solve , wrongly considering a too large C value can dramatically increase the running time .
In an earlier study [ 14 ] on parameter selection for general classification problems , they terminate the search procedure if CV accuracy is not enhanced after a few trials . While the setting is reasonable , the following issues occur . where is a pre specified small positive value . The condition ( 24 ) implies that w∆t−1C , the optimal solution for the previous ∆t−1C , is an approximate solution for minimizing the function f ( w ; ∆tC ) . We prove the following theorem to support our use of ( 24 ) . In particular , we check the relationship between wC/∆ and ∇f ( w ; C )
Theorem 8 Assume L(w ) is continuously differentiable . 1 . If non separable condition ( 9 ) holds and w∞ > 0 , then
2 . We have lim C→0 wC1 = wC2 ,∀C1 = C2 .
( 25 )
∇f ( wC/∆ ; C )
∇f ( 0 ; C ) =
∇f ( wC/∆ ; C )
∆ − 1
∆
, and
( 26 ) lim C→∞
∇f ( 0 ; C ) = 0 .
( 27 ) The result ( 25 ) indicates that in general wC/∆ = wC , so ( 24 ) does not hold . One exception is when C is large , wC/∆ ≈ wC ≈ w∞ from Theorem 1 . Then ( 24 ) will eventually hold and this property is indicated by ( 27 ) . On the contrary , when C is small , from ( 26 ) and the property that
∆ ≥ 1 1 − implies
∆ − 1
∆
≥ , if ∆ is not close to one , ( 24 ) does not hold . Therefore , our procedure does not stop pre maturely .
153 Algorithm 1 A complete procedure for parameter section . 1 . Given K as number of CV folds . 2 . Initialize Cmin by ( 21 ) or ( 22 ) , Cmax by a constant . 3 . Initialize Cbest ← Cmin , best CV accuracy A ← −∞ . 4 . For each CV fold k , give initial ¯wk . 5 . For C = Cmin , ∆Cmin , ∆2Cmin , . . . , Cmax :
51 For each CV fold k = 1 , . . . , K :
511 Use all data except fold k for training .
Apply warm start with the initial point ¯wk . Obtain the solution wk C .
512 Predict fold k by wk C .
52 Obtain CV accuracy using results obtained in 51
If the new CV accuracy > A : A ← the new CV accuracy . Cbest ← C .
53 If ( 24 ) is satisfied : 54 For each CV fold k , ¯wk ← wk C . break
6 . Return Cbest .
Note that ( 24 ) can be applied regardless of whether a primal based or a dual based optimization method is used . If a dual based method is considered , an optimal wC is returned by ( 4 ) and we can still check ( 24 ) . 3.4 The Overall Procedure
With all results ready , we propose in Algorithm 1 a practically useful procedure for selecting the parameter of a linear classifier . We evaluate the CV accuracy from the smallest parameter Cmin , and gradually increase it by a factor ∆ ; see the sequence of C values shown in ( 1 ) . In the CV procedure , if the kth fold is used for validation , then all the remaining folds are for training . Therefore , several sequences of optimization problems are solved . Although it is possible to separately handle each sequence , here we consider them together . That is , at each C , the training/prediction tasks on all folds are conducted to obtain the CV accuracy . Then either the procedure is terminated or we go to the next ∆C . Regarding the storage , all we need is to maintain the K vectors of w , where K is the number of CV folds .
In Algorithm 1 , we see the change of primal solutions is checked for terminating the search process . Although both primal and dual based optimization methods can be used ( from αC , primal wC can be easily generated ) , we expect that a primal based method is more suitable because of the following properties . Primal solution wC is unique regardless of the loss function , but dual solution αC may not be unique for L1 loss . Primal solutions converge as C increases , but dual solu tions diverge ( Sections 2 and 32 )
After applying warm start to have initial solutions , the primal objective value tends to be closer to the optimum than the dual ( Section 32 )
The warm start strategy is more effective for a high order optimization approach ( eg , Newton method ) . Because the primal problem ( 2 ) has no constraints , it is easier to design a high order solver . ( [20 ] and Section 32 )
Further , [ 20 , Section 5 ] has pointed out that implementation issues such as maintaining α make dual solvers more challenging to support warm start . We will detailedly compare implementations using primal and dual based methods in Section 4 and supplementary materials . l : #instances n : #features
Table 1 : Data statistics : Density is the average ratio of non zero features per instance . Data set madelon ijcnn webspam rcv1 yahoo japan news20 density best C 500 100.00 % 2−25 25 22 59.09 % 22 0.02 % 16,609,143 0.15 % > 210 47,236 23 0.02 % 832,026 0.03 % > 210 1,355,191
2,000 49,990 350,000 677,399 176,203 19,996
4 . EXPERIMENTS
We conduct experiments to verify the results discussed in Sections 2 and 3 , and check the performance of Algorithm 1 . Because of the space limitation , we present only results of LR , but leave L2 loss SVM in the supplementary materials.4 We consider six data sets madelon , ijcnn , webspam ( trigram version ) , news20 , rcv1 , and yahoo japan with statistics in Table 1,5 where the last column is the best C with the highest CV accuracy . We tried some large C to empirically confirm that all these sets are not separable .
We consider Cmin to be the largest 2n that satisfies ( 21 ) . Following the discussion in ( 19 ) , we use ∆ = 2 , so the sequence of C values in ( 1 ) contains only powers of two . For Cmax , we set it to be a large number 210 , because as indicated in Section 3.3 , another condition ( 24 ) will be mainly used to stop our procedure .
Our implementations are extended from LIBLINEAR [ 6 ] , and we use five fold CV . We consider two optimization methods in our experiments . One is a dual coordinate descent method [ 10 ] for solving the dual problem , while the other is a Newton method [ 16 ] for solving the primal problem . At Cmin , because of no prior information for warm start , the default initial point in LIBLINEAR is used ; see Step 4 in Algorithm 1 . The two optimization methods have their respective stopping conditions implemented in LIBLINEAR . To fairly compare them , we modify the condition of the dualbased method to be the same as the primal one:6 min(l+ , l− )
∇f ( 0 ) ,
∇f ( w ) ≤ l
( 28 ) where l+ and l− are the numbers of instances labelled +1 and −1 , respectively . This condition is related to ( 24 ) used for terminating the parameter search . If not specified , the default = 10−2 in LIBLINEAR is used in our experiments . We also change some settings of the solvers . Details are in Section III of supplementary materials . Experiments are conducted on two four core computers with 2.0GHz/32GB RAM and 2.5GHz/16GB RAM for webspam and other data sets , respectively . 4.1 CV Accuracy and Training Time
We investigate in Figure 1 the relation between log2 C ( xaxis ) and CV accuracy ( y axis on the left ) . The purpose is to check the convergence of {wC} proved in Section 2 . We 4L1 loss SVM is not considered because the primal based optimization method considered here cannot handle nondifferentiable losses . 5All data sets except yahoo japan are available at http:// wwwcsientuedutw/~cjlin/libsvmtools/datasets 6For any dual based optimization method , we can easily obtain an approximate primal solution by ( 4 ) . On the contrary , we may not be able to modify a primal based method to use the stopping condition of a dual based method because from a primal w it is difficult to generate a dual α .
154 ( a ) madelon
( b ) ijcnn
( c ) webspam
( d ) rcv1
( e ) yahoo japan
( f ) news20
Figure 1 : CV accuracy and training time using LR with warm start . The two CV curves and the left y axis are the CV accuracy in percentage ( % ) . The dashed lines and the right y axis are the cumulative training time in the CV procedure in seconds . The vertical line indicates the last C value checked by Algorithm 1 . show CV rates of using ( 28 ) with = 10−2 and 10−6 as the stopping condition for finding wC ; see “ CV Rate ” and “ CV Tight ” in Figure 1 , respectively . We have the following observations . First , for most problems , the CV accuracy is stabilized when C is large . This result confirms the theoretical convergence of {wC} . However , for the data set yahoojapan , the CV accuracy keeps changing even when C is large . This situation may be caused by the following reasons . wC is not close enough to w∞ yet even though a large
C = 210 has been used .
Training a problem under a large C is so time consuming that the obtained wC is still far away from the optimum ; see the significant difference of CV rates between using loose and strict stopping tolerances for madelon , rcv1 , and yahoo japan.7 Later in this section we will discuss more about the relation between training time and C .
The above observation clearly illustrates where the difficulty of parameter selection for linear classification lies . The area of large C values is like a danger zone . Usually the best C is not there , but if we wrongly get into the region , not only does the lengthy training time may occur , but also the obtained CV rates may be erroneous .
The second observation is that when C is small , the CV accuracy is almost flat ; see explanation in Section II of the supplementary materials . Therefore , if we use the method in [ 14 ] to check the change of CV accuracy , the search procedure may stop too early . In contrast , we explained in Section 332 that our method will not stop until wC is close to w∞ .
7It is surprising to see that for rcv1 , a loose condition gives flat CV rates , a situation closer to the convergence of {wC} , but a strict condition does not give that . The reason is that because of using warm start , the initial ¯w from wC immediately satisfies the loose stopping condition at ∆C , so both the optimal solution and CV rate remain the same .
In Figure 1 , we also show the cumulative CV training and validation time of Algorithm 1 from Cmin to the current C ; see the dashed lines and the y axis on the right . The curve of cumulative time is up bended because both solvers become slower as C increases . Except news20 , this situation is more serious for the dual solver.8 A reason is that our dual solver is a low order optimization method . When C is large , the problem becomes harder to solve , and a high order method such as the Newton method for the primal problem tends to perform better . See more discussion in Section 43
To remedy the problem of lengthy training when C is large , recall in Section 332 we proposed a method to stop the procedure according to the stopping condition of the solver . The ending point of our procedure is indicated by a vertical line in Figure 1 . Clearly , we successfully obtain CV accuracy close to the highest in the entire range .
Figure 1 confirms that we rightly choose Cmin smaller than the best C . Although Cmin tends to be too small , in Figure 1 , the training time for small C values is insignificant . 4.2 Initial Objective Values We verify our discussion on primal and dual initial objective values in ( 19 ) . If wC ≈ w∆C ≈ w∞ , then the initial objective value of primal solvers should be close to the optimum , while the dual initial objective value is asymptoticly smaller than the primal by ¯w∞2/2 .
In Table 2 , we show the difference between initial and optimal objective values . f ( ¯w ) − f ( wC ) and f D( ¯α ) − f D(αC ) .
Note that f ( wC ) and f D(αC ) are equal . Because the optimal wC and αC are difficult to compute , we obtain their
8Note that warm start has been applied . If not , the time increase at large C values is even more dramatic .
−30−20−10010log2C50525456586062CVaccuracy0500010000150002000025000300003500040000CumulativeCVtimeCVRateCVTightPrimal wsDual ws−15−10−50510log2C900905910915920925CVaccuracy05101520253035CumulativeCVtimeCVRateCVTightPrimal wsDual ws−15−10−50510log2C6065707580859095100CVaccuracy05000100001500020000250003000035000CumulativeCVtimeCVRateCVTightPrimal wsDual ws−20−15−10−50510log2C92939495969798CVaccuracy02004006008001000120014001600CumulativeCVtimeCVRateCVTightPrimal wsDual ws−15−10−50510log2C905910915920925930CVaccuracy020040060080010001200CumulativeCVtimeCVRateCVTightPrimal wsDual ws−15−10−50510log2C7580859095100CVaccuracy050100150200250300CumulativeCVtimeCVRateCVTightPrimal wsDual ws155 Table 2 : Difference between the initial and optimal function values . Logistic regression is used . The approach that is closer to the optimum is boldfaced . log2 C primal dual ijcnn primal dual primal dual madelon
¯w2/2
¯w2/2
¯w2/2 −4 2.51e−03 −1.09e−01 4.57e−02 1.30e+01 −4.55e+01 5.85e+01 1.63e+02 −3.86e+02 5.49e+02 0 2.62e−04 −1.45e+00 5.72e−02 1.31e+01 −1.90e+02 2.03e+02 1.12e+03 −3.26e+03 4.38e+03 4 0.00e+00 −1.29e+01 5.82e−02 1.50e+00 −2.64e+02 2.66e+02 8.28e+03 −2.33e+04 3.16e+04 8 0.00e+00 −1.17e+02 5.83e−02 9.86e−02 −2.71e+02 2.71e+02 5.32e+04 −1.76e+05 2.29e+05 −4 3.23e+02 −1.04e+03 1.36e+03 6.19e+01 −1.27e+02 1.89e+02 2.19e+01 −1.43e+01 3.62e+01 0 1.60e+03 −6.02e+03 7.62e+03 9.00e+02 −1.40e+03 2.30e+03 3.78e+02 −6.32e+02 1.01e+03 4 1.20e+04 −3.33e+04 4.53e+04 1.85e+04 −2.66e+04 4.51e+04 2.26e+03 −7.91e+03 1.02e+04 8 9.53e+04 −2.70e+05 3.66e+05 1.21e+05 −4.07e+05 5.28e+05 5.33e+03 −3.64e+04 4.17e+04 yahoo japan webspam news20 rcv1 approximations by running a huge number of iterations . We show results of C = 2−4 , 20 , 24 , 28 , where ¯w and ¯α are obtained by solving problems of C = 2−5 , 2−1 , 23 , 27 and applying ( 13 ) and ( 14 ) . We use = 10−6 in ( 28 ) for this experiment to ensure that the solution of the previous problem is accurate enough . In Table 2 , we also show ¯w2/2 to see if , as indicated in ( 19 ) , f D( ¯α ) − f D(αC ) is close to − ¯w2/2 when C is large .
From Table 2 , except some rare situations with small C values , primal solvers have function values closer to the optimal value than the dual solvers . Further , as C increases , f D(αC ) − f D( ¯α ) becomes close to ¯w2/2 for most problems . Note that from ( 19 ) , f ( ¯w ) − f ( wC ) =
( ¯w2 − wC2)+
1 2
( ξ( ¯w ; xi , yi ) − ξ(wC ; xi , yi ) ) , and l
C i=1 f D( ¯α ) − f D(αC ) = − 1 wC2 + C 2 l i=1
( 29 )
( 30 )
( ξ( ¯w ; xi , yi ) − ξ(wC ; xi , yi ) ) .
For their first term , ¯w2 − wC2 ≈ 0 in ( 29 ) as C → ∞ , but in ( 30 ) , it converges to −w∞2/2 . The situation of the second term is unclear because ξ( ¯w ; xi , yi)−ξ(wC ; xi , yi ) → 0 as C → ∞ . However , C ( ξ( ¯w ; xi , yi ) − ξ(wC ; xi , yi ) ) in Table 2 is relatively smaller than −wC2/2 , and therefore , the primal initial objective value is closer to the optimal objective value than the dual . Finally , the dual solver fails on the data set madelon because of the slow convergence . 4.3 Effectiveness of Warm start Strategies
In Figure 2 , we compare the running time with/without implementing warm start . Each subfigure presents training time versus the following relative difference to the optimal objective value f ( w ) − f ( wC ) f ( wC ) and f D(α ) − f ( αC ) f ( αC )
, where wC or αC is an optimal solution and w or α is any iterate in the optimization process . We use the best C value before the vertical line in Figure 1 . The initial point of warm start is by ( 13 ) and ( 14 ) , which use solutions at C/2 .
If warm start is not applied , results in Figure 2 are con sistent with past works such as [ 10 ] : If the number of features is much smaller than instances and C is not large ( madelon and ijcnn ) , a primal based method may be suitable because of a smaller number of variables . For the opposite case of more features , a dualbased method may be used ( webspam and yahoo japan ) .
If C is large , a high order optimization approach such as
Newton methods is more robust ( news20 ) .
In practice , a stopping condition is imposed to terminate the optimization procedure ( eg , running up to the horizontal line in Figure 2 , which indicates that the condition ( 28 ) with LIBLINEAR ’s default = 10−2 has been established ) .
After applying warm start , both primal and dual solvers become faster . Therefore , warm start effectively reduces the training time . However , Figure 2 focuses on the convergence behavior under a given C . What we are more interested in is the total training time of the parameter selection procedure . This will be discussed in Section 44 4.4 Performance of the Parameter selection Pro cedure
We compare the running time with/without applying warm start techniques . Figure 3 presents the cumulative CV training and validation time from Cmin to the current C . We can make the following observations . The total running time ( log scaled in Figure 3 ) is significantly reduced after applying warm start on both primaland dual based optimization methods .
While the dual coordinate descent method is faster when C is small , its training time dramatically increases for large C values . This result corresponds to our earlier discussion that a low order optimization method is not suitable for hard situations such as when C is large . Even though warm start significantly reduces the training time , for running up to a large C , it is generally less competitive with the primal Newton method with warm start . Therefore , using a high order optimization method is a safer option in parameter selection for avoiding lengthy running time . 4.5 CV Folds and Models
In Algorithm 1 , data are split to K folds for the CV procedure . For each fold k , we maintain a vector wk that traces initial and optimal solutions all the way from Cmin to Cmax . To reduce the storage , an idea is to maintain only one vector across all folds . If fold 1 serves as a validation set , the training set includes fold 2 , fold 3 , . . . , fold K .
( 31 )
Next , for validating fold 2 , the training set becomes fold 1 , fold 3 , . . . , fold K .
( 32 )
156 ( a ) madelon ( C = 2−25 )
( b ) ijcnn ( C = 23 )
( c ) webspam ( C = 22 )
( d ) rcv1 ( C = 24 )
( e ) yahoo japan ( C = 23 )
( f ) news20 ( C = 29 )
Figure 2 : Objective values versus training time using LR and the best C found by Algorithm 1 . The solid lines correspond to settings without applying warm start , where default initial points in LIBLINEAR are used . Primal ws and Dual ws are primal and dual solvers with warm start settings , respectively , and the initial point is obtained by ( 13 ) and ( 14 ) . The horizontal line indicates that the condition ( 28 ) with LIBLINEAR ’s default = 10−2 has been established .
Table 3 : CV accuracy using a dedicated w for each fold and a shared w for all folds . See details in Section 45 The set yahoo japan is used . The highest CV rate is boldfaced .
C K models 21 92.60 23 92.69 25 92.59 27 92.34 29 92.21 one model 92.66 92.80 92.56 94.27 98.01
The two training sets differ in only two folds : fold 1 and fold 2 . We have a scenario of incremental and decremental learning [ 20 ] , where fold 2 is removed , but fold 1 is added . Then warm start can be applied . Specifically , the optimal solution after training ( 31 ) can be used as an initial solution for ( 32 ) . Although this technique may reduce the storage as well as the training time , we show that practically some difficulties may occur .
In Table 3 , we compare the two approaches of using K and one vectors for storing the solutions . The approach of maintaining one vector gives much higher CV accuracy and a larger best C value . An investigation shows that two reasons together cause an over estimation . In training folds 1 , 3 , . . . , K to validate fold 2 , our initial point from training folds 2 , 3 , . . . , K contains information from the validation set .
in training folds 2 , 3 , . . . , K , we obtain only an approxi mate solution rather than the optimum .
That is , the initial point is biased toward fitting fold 2 ; this issue should be fixed if we obtain the optimum of training folds 1 , 3 , . . . , K , but in practice we do not . This experiment shows that in applying the warm start technique , we often conveniently assume that optimal solutions are exactly obtained . This assumption is of course incorrect because of numerical computation . While solving optimization problems more accurately may address the issue , the training time also goes up , a situation that contradicts the goal of applying the warm start technique . Our experiences indicate that while warm start is very useful in machine learning , its practical implementation must be carefully designed .
5 . CONCLUSIONS
Although we have studied many issues on the parameter selection for linear classifiers , there are some future works . Training tasks in the CV procedure under a given C are It is interesting to make a parallel imple independent . mentation and investigate the scalability .
Our method can be easily extended to L1 regularized problems . However , the relationship between optimization problems and regularization parameters must be studied because the primal optimal solution w may not be unique . In conclusion , based on this research work , we have released an extension of LIBLINEAR for parameter selection . It is an automatic and convenient procedure for users without background knowledge on linear classification .
6 . ACKNOWLEDGMENTS
This work was supported in part by the National Science Council of Taiwan via the grant 101 2221 E 002 199 MY3 .
7 . REFERENCES [ 1 ] J . F . Bonnans and A . Shapiro . Pertubation analysis of optimization problems . Springer Verlag , 2000 .
000005010015020025time(sec)−10−8−6−4−20relativefuncvaldiff(log)primaldualprimal wsdual ws000204060810121416time(sec)−10−8−6−4−202relativefuncvaldiff(log)primaldualprimal wsdual ws010002000300040005000time(sec)−10−8−6−4−202relativefuncvaldiff(log)primaldualprimal wsdual ws050100150200time(sec)−10−8−6−4−202relativefuncvaldiff(log)primaldualprimal wsdual ws020406080100120140160time(sec)−10−8−6−4−202relativefuncvaldiff(log)primaldualprimal wsdual ws020406080100120140160time(sec)−10−8−6−4−202relativefuncvaldiff(log)primaldualprimal wsdual ws157 ( a ) madelon
( b ) ijcnn
( c ) webspam
( d ) rcv1
( e ) yahoo japan
( f ) news20
Figure 3 : Training time ( in seconds ) using LR with/without warm start techniques . The vertical line indicates the last C value checked by Algorithm 1 . Because the training time quickly increases when C becomes large , the y axis is log scaled .
[ 2 ] C C Chang and C J Lin . LIBSVM : A library for support vector machines . ACM TIST , 2(3):27:1–27:27 , 2011 .
[ 3 ] O . Chapelle , V . Vapnik , O . Bousquet , and
S . Mukherjee . Choosing multiple parameters for support vector machines . MLJ , 46:131–159 , 2002 .
[ 4 ] K M Chung , W C Kao , C L Sun , L L Wang , and
C J Lin . Radius margin bounds for support vector machines with the RBF kernel . Neural Comput . , 15:2643–2681 , 2003 .
[ 5 ] D . DeCoste and K . Wagstaff . Alpha seeding for support vector machines . In KDD , 2000 .
[ 13 ] S J Kim , K . Koh , M . Lustig , S . Boyd , and D . Gorinevsky . An interior point method for large scale l1 regularized least squares . IEEE J STSP , 1:606–617 , 2007 .
[ 14 ] R . Kohavi and G . H . John . Automatic parameter selection by minimizing estimated error . In ICML , 1995 .
[ 15 ] J H Lee and C J Lin . Automatic model selection for support vector machines . Technical report , 2000 .
[ 16 ] C J Lin , R . C . Weng , and S . S . Keerthi . Trust region
Newton method for large scale logistic regression . JMLR , 9:627–650 , 2008 .
[ 6 ] R E Fan , K W Chang , C J Hsieh , X R Wang ,
[ 17 ] J . Matouˇsek and B . G¨artner . Understanding and and C J Lin . LIBLINEAR : a library for large linear classification . JMLR , 9:1871–1874 , 2008 .
[ 7 ] J . H . Friedman , T . Hastie , and R . Tibshirani .
Regularization paths for generalized linear models via coordinate descent . JSS , 33:1–22 , 2010 .
[ 8 ] J . Giesen , M . Jaggi , and S . Laue . Approximating parameterized convex optimization problems . TALG , 9:10:1–10:17 , 2012 .
[ 9 ] T . Hastie , S . Rosset , R . Tibshirani , and J . Zhu . The entire regularization path for the support vector machine . JMLR , 5:1391–1415 , 2004 .
[ 10 ] C J Hsieh , K W Chang , C J Lin , S . S . Keerthi , and S . Sundararajan . A dual coordinate descent method for large scale linear SVM . In ICML , 2008 . [ 11 ] W C Kao , K M Chung , C L Sun , and C J Lin .
Decomposition methods for linear support vector machines . Neural Comput . , 16(8):1689–1704 , 2004 .
[ 12 ] S . S . Keerthi and C J Lin . Asymptotic behaviors of support vector machines with Gaussian kernel . Neural Comput . , 15(7):1667–1689 , 2003 .
Using Linear Programming . Springer , 2007 .
[ 18 ] J . Snoek , H . Larochelle , and R . P . Adams . Practical
Bayesian optimization of machine learning algorithms . In NIPS . 2012 .
[ 19 ] R . Tibshirani , J . Bien , J . Friedman , T . Hastie ,
N . Simon , J . Taylor , and R . J . Tibshirani . Strong rules for discarding predictors in lasso type problems . JRSSB , 74:245–266 , 2012 .
[ 20 ] C H Tsai , C Y Lin , and C J Lin . Incremental and decremental training for linear classification . In KDD , 2014 .
[ 21 ] Z . Wu , A . Zhang , C . Li , and A . Sudjianto . Trace solution paths for SVMs via parametric quadratic programming . DMMT , 2008 .
[ 22 ] G X Yuan , C H Ho , and C J Lin . Recent advances of large scale linear classification . PIEEE , 100:2584–2603 , 2012 .
−30−20−10010log2C10−1100101102103104105CumulativeCVtimeprimal wsdual wsprimaldual−15−10−50510log2C10−1100101102103CumulativeCVtimeprimal wsdual wsprimaldual−15−10−50510log2C102103104105106CumulativeCVtimeprimal wsdual wsprimaldual−20−15−10−50510log2C101102103104CumulativeCVtimeprimal wsdual wsprimaldual−15−10−50510log2C101102103104CumulativeCVtimeprimal wsdual wsprimaldual−15−10−50510log2C100101102103CumulativeCVtimeprimal wsdual wsprimaldual158
