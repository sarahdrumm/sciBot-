Maximum Likelihood Postprocessing for Differential
Privacy under Consistency Constraints
Jaewoo Lee jlee@csepsuedu
Yue Wang yuw140@csepsuedu
Daniel Kifer dkifer@csepsuedu
Department of Computer Science and Engineering
Penn State University
ABSTRACT When analyzing data that has been perturbed for privacy reasons , one is often concerned about its usefulness . Recent research on differential privacy has shown that the accuracy of many data queries can be improved by post processing the perturbed data to ensure consistency constraints that are known to hold for the original data .
Most prior work converted this post processing step into a least squares minimization problem with customized efficient solutions . While improving accuracy , this approach ignored the noise distribution in the perturbed data .
In this paper , to further improve accuracy , we formulate this post processing step as a constrained maximum likelihood estimation problem , which is equivalent to constrained L1 minimization . Instead of relying on slow linear program solvers , we present a faster generic recipe ( based on ADMM ) that is suitable for a wide variety of applications including differentially private contingency tables , histograms , and the matrix mechanism ( linear queries ) .
An added benefit of our formulation is that it can often take direct advantage of algorithmic tricks used by the prior work on least squares post processing .
An extensive set of experiments on various datasets demonstrates that this approach significantly improve accuracy over prior work .
Categories and Subject Descriptors H28 [ Database Applications ] : Statistical Database
Keywords Differential privacy ; ADMM ; PPDM ; Post processing
1 .
INTRODUCTION
Many publicly available datasets have been altered to protect privacy . In many cases , users were encouraged to use this perturbed data in exactly the same way they would have used the original data . However , recent work has shown that post processing the data with custom statistical inference algorithms can significantly improve accuracy of the results [ 20 , 11 , 14 , 15 , 6 ] .
One of the biggest sources of improvement is enforcing consistency constraints — the modification of perturbed data so that it satisfies constraints that are known to hold in the original data . For example , suppose Cnj is the number of cancer patients in New Jersey , Cny is the number in New York , and Call = Cnj + Cny . Suppose independent noise is added to each variable and the resulting noisy quantities some obvious constraints : Cnj ≥ 0 , Cny ≥ 0 , Call = Cnj+Cny but that is not necessarily true of their noisy versions . Thus we could seek new estimates , Cny , Cnj , Call that are close to
Cny , Cnj , Call are released . The original variables satisfy Cny , Cnj , Call and satisfy those constraints .
The most common approach is to formulate such problems in terms of least squares estimation [ 11 , 13 , 17 ] : choose the Cny , Cnj , Call that minimize
( Cny − Cny)2 + ( Cnj − Cnj)2 + ( Call − Call)2
( 1 ) subject to Cny + Cnj = Call ( sometimes dropping the inequality constraints ) . The resulting estimates Cnj , Cny of the number of cancer patients will generally be more accu rate than the unprocessed noisy values Cny , Cnj .
Such a formulation has two drawbacks – it drops the inequality constraints and it ignores the distribution of noise that was added to each variable . In the case of differential privacy [ 7 ] , the noise often comes from the Laplace distri2α e−|x|/α . If we want conbution with density f ( x ; α ) = 1 strained maximum likelihood ( instead of least squares ) estimates Cny , Cnj , Call for Cny , Cnj , Call , the optimization problem is to find Cny , Cnj , Call that minimize
|Cny − Cny| + |Cnj − Cnj| + |Call − Call| subject to Cny + Cnj = Call , Cny ≥ 0 , Cnj ≥ 0 .
( 2 )
Why was the least squares approach preferred in prior work ? First , it had a unique solution ( not guaranteed in Equation ( 2) ) . Second without inequality constraints , the problem becomes differentiable and fast custom algorithms ( that do not use matrix inversion or decomposition ) can be developed [ 11 ] .
In this paper , we leverage the alternating direction method of multipliers [ 3 ] to provide a generic recipe for developing maximum likelihood post processing algorithms for many differential privacy applications , including releasing contingency tables [ 2 ] , histograms [ 11 , 1 , 21 ] , frequent itemsets [ 12 ] , batch query processing [ 13 , 22 ] , etc .
635 programs . uses least squares estimation .
We specialize our approach for three target applications : differentially private contingency tables , histograms , and linear queries ( via the matrix mechanism [ 13] ) . The advantages of our proposed algorithms include the following . • They outperform ( in terms of accuracy ) prior work that • They outperform ( in terms of speed ) equivalent linear • They can directly take advantage of many algorithmic tricks that were used to speed up least squares algorithms in other differential privacy applications . • They can be easily modified to provide unique solutions . The rest of the paper is organized as follows . In Section 2 , related works are discussed . Section 3 defines the notations used throughout the paper and provides background knowledge about differential privacy . We discuss the applications of the proposed algorithm to contingency tables , histogram and matrix mechanism and introduce the least squares formulation . Section 4 describes the details of the proposed algorithm . In Section 5 , we provide algorithms for our target applications and their performance evaluation on a variety of datasets . Finally , Section 6 concludes our work .
2 . RELATED WORKS
The problem of finding an estimate of data or query answers that satisfies consistency constraints was first considered by Barak et al . [ 2 ] . They transform the data into Fourier domain and then add random noise to Fourier coefficients . They use linear programming to obtain non negative entries in contingency tables . Their objective function , however , is designed not to maximize the likelihood but to minimize the tolerance on the constraint violation . Although they do not formulate their problem as maximum likelihood estimation , the resulting linear program can be solved using the techniques discussed in this paper .
Hay et al . [ 11 ] introduced a post processing algorithm customized for releasing histograms . To release a histogram , they build a query sequence such that linear relationships are established between query answers . Those linear relationships are used as a consistency constraint during postprocessing . The post processing algorithm finds an estimate that minimizes the L2 distance to the noisy answer among those which satisfy the constraint ; in other words , it performs least squares under the linear constraint . This formulation , however , does not utilize the knowledge of noise distribution ( another source of improvement ) .
The least squares based post processing technique has been widely adopted in the literature for generating consistent estimates . For example , [ 18 ] used it to maintain the consistency of marginal counts between multiple contingency tables , [ 5 , 12 ] to generate consistent hierarchical tree data structures , [ 6 ] to derive consistent marginal counts between data cuboids , and [ 16 ] to produce consistent spatial grids .
Lin and Kifer [ 14 ] applied a Bayesian approach to estimate sorted histograms and pointed out that substantial improvements in accuracy can be obtained if the estimation procedure makes use of knowledge on the noise distribution . They proposed a sophisticated estimation algorithm that views sorted histograms as a Markov chain and imposes ordering constraints on the estimates . The employment of such a complex model , however , incurs high computational complexity and limits the general applicability of their approach to other problems .
Symbol Description
˜x
( x)+ x 0
I noisy version of x , x + Lap ( λ ) vector with components ( xi)+ = max{0 , xi} ∀i , xi ≥ 0 an identity matrix
Table 1 : Notations
Instead of maximum likelihood , Williams and McSherry [ 20 ] used a variational bayesian approximation to make estimates about the data . However , it cannot make use of linear and non negativity constraints .
Li et al . [ 13 ] introduced a general framework , called matrix mechanism , for answering linear queries . Given a strategy query A , the mechanism reconstructs the original input database from the noisy answer to A by solving a least squares problem . However , the use of least square solution does not take the noise distribution into account .
Smith [ 19 ] showed that , for statistical estimators T with asymptotic normality , there exists a differentially private estimator AT with the same asymptotic distribution as T . His result applies to maximum likelihood estimators for certain parametric model families , which is different from the problem considered in this paper in that we estimate the data or query answers under consistency constraints .
We now review two algorithms used as baseline perfor mance measure in our experiments .
Privlet [ 21 ] uses the domain transform technique for his togram publishing . Given a histogram , it applies Haar wavelet transform on the data to get wavelet coefficients and adds noise to the coefficients . The count of any arbitrary range query is represented as a linear sum of noisy coefficients .
The multiplicative weights mechanism ( MW ) [ 10 ] is a synthetic database based approach . It start with a poor estimates to the true database and iteratively refines the estimate . In each round , using the exponential mechanism , it identifies a query on which their estimate differs most significantly from the true database , and then the noisy answer to that query is used to improve the estimate .
3 . PRELIMINARIES 3.1 Notation and A
To denote vectors , we use boldface lowercase letters , eg , v . All vectors are assumed to be column vectors , unless noted otherwise . For a vector v , the ith component of v is denoted by vi . Matrices are written by boldface uppercase letters , eg , A . The transpose of a vector v and a matrix , respectively . For p ∈ ( A are represented by v [ 1,∞ ) , the Lp norm of a vector v ∈ Rn is defined as vp = i∈[n ] |xi|p)1/p . The notation ( v)+ is used to represent a vector obtained by replacing negative values with 0 , ie , ∀i , its component ( vi)+ = max{vi , 0} . An identity matrix is denoted by I . For a vector x , ˜x denotes x with Laplace noise added to each component . We use the notation x 0 to denote that x contains no negative components ( ie , ∀i , xi ≥ 0 ) . All notations are summarized in Table 1 . 3.2 Differential Privacy A database X = {x1 , x2,··· , xn} is a ( multi)set of records , where each xi ∈ U corresponds to an individual and each el
636 ement of xi represents an attribute value . Two databases X and X are referred to as neighboring if one can be obtained by adding or removing one record from the other , ie , |(X − X ) ∪ ( X − X)| = 1 . Informally , differential privacy protects individuals by ensuring that the output distribution of an algorithm is not significantly affected by the presence/absence of any individual in the database . The formal definition of differential privacy is as follows :
Definition 1
( differential privacy ) . Given a query function q , a randomized algorithm K satisfies differential privacy if for every possible pair of neighboring databases X and X and ∀S ⊆ Range(K ) ,
P [ Kq(X ) ∈ S ] ≤ e P.Kq(X ) ∈ Sfi .
A mechanism well known to achieve differential privacy is the Laplace mechanism [ 7 ] , which adds random noise to the output of a query function . The magnitude of noise to add depends on how sensitive the output of a query function is to a small change in the input . This notion is captured by the concept of sensitivity [ 8 ] . A query q is a function that maps a database X to a vector in Rd , ie , q : U|X| → Rd .
Definition 2
( Sensitivity ) . For two neighboring databases X and X , the sensitivity of a query function q is defined as flflq(X ) − q(X)flfl1
.
∆q = max ∈U
X,X
The sensitivity is the upper bound on the impact any individual can have on the outcome of function and this amount of uncertainty is required to make two neighboring databases indistinguishable . It is a function of query type and not dependent on the data .
Definition 3
( Laplace mechanism ) . Given a query function q : , the algorithm A that adds iid noise to each element of q(X ) achieves differentially privacy .
A(X ) = q(X ) + Lap ( ∆q/ )d
3.3 Applications Contingency tables . Let K = {1,··· , k} . Consider a database D with k categorical attributes ( A1,··· , Ak ) where , for j ∈ K , each Aj can take values from Ij = {1,··· , Ij} . A k way contingency table with entries x = {x(i1,··· , ik)} is a k dimensional array of non negative integers . With an ordering function Ψ : IK → N that maps a multi index to a one dimensional index lexicographically , a contingency table can be viewed as a vector x ∈ Nn j=1 Ij . For B = {i1,··· , ib} ⊆ K , let IB = Ii1 × ··· × Iib . Given b ∈ IB , b marginal is obtained by summing over all other attributes . where n = k m(b ) = x(b , j ) = xΨ(b,j )
( 3 ) a complete k ary tree such that each leaf node corresponds to a unit interval and a parent node covers the union of intervals of its k children . A complete k ary tree can easily be converted to a vector of length n = k+1−1 k−1 by storing the tree in breadth first order where is the height of tree . Let x = ( x1,··· , xn ) be a vector representing the tree . The jth child of xi has the index k(i − 1 ) + j + 1 . Thus , x satisfies the following linear constraint . xi = xk(i−1)+j+1 , for i = 1,··· , k − 1 k − 1
( 4 ) k j=1
Given a noisy tree ˜x , the goal of post processing is to find a new tree ( with non negative counts ) ¯x that is closest to ˜x and , at the same time , satisfies the constraint ( 4 ) .
Matrix mechanism ( MM ) . Suppose a database x that contains a set of non negative integers . Given the strategy matrix A , MM produces the noisy answer ˜r = Ax + Lap ( ∆A / ) according to the Laplace mechanism . From ˜r , the original input database is conveniently approximated by the least squares solution :
ˆx = arg min
¯x
1 2
˜r − A¯x2
2 = A†˜r = ( A
A)−1A
˜r where A† is the pseudo inverse of A . To achieve maximum likelihood , the post processing should solve the following optimization problem :
¯x = arg min
˜r − Aˆx1 .
ˆx0
( 5 )
3.4 Least Squares Formulation
The equality constraints in all of the above applications are linear and can be represented by a1x1 + a2x2 + ··· + anxn = b .
A set of p such linear equations can be expressed concisely in matrix notation as Ax = b where A ∈ Rp×n and b ∈ Rp . The post processing problem for the above applications shares the common form , and previous approaches [ 11 , 13 , 5 , 12 ] formulated it as a constrained least squares problem : minimize x x − ˜x2
2 subject to Ax = b .
( 6 )
For algorithmic simplicity , the inequality constraint is often excluded . The main disadvantage of the above formulation is that it doesn’t achieve maximum likelihood . Provided that the query answers are perturbed with Laplace noise , maximizing the likelihood function is equivalent to minimizing n i=1 |xi − ˜xi| = x − ˜x1 , as shown in Section 41 j∈K\B j∈K\B
Notice that a marginal count m(b ) is a linear constraint on the contingency table x : a1x1 + a2x2 + ··· + anxn = m(b ) . where ai is 1 if the projection of Ψ−1(i ) on IB = b and 0 otherwise . Let m(B ) = {m(b ) | b ∈ IB} . Given m(B ) and a noisy contingency table ˜x , the objective of post processing is to find a more accurate table ¯x that is consistent with m(B ) . Histograms . Let H = ( h1 , h2,··· , hd ) be a histogram of length d = k . As in [ 11 ] , given a histogram H , we build
4 . THE PROPOSED ALGORITHM FOR CON
STRAINED INFERENCE
In this section , we describe each step of the proposed ADMM algorithm in detail . For details on ADMM , refer to [ 3 ] and the references therein . 4.1 L1 Formulation
We use maximum likelihood to estimate the unknown dataset x , not model parameters , from the noisy data ˜x .
637 |xi − ˜xi| + const = −x − ˜x1 + const . xk+1 = arg min x
φ(x ) +
The efficiency of this iterative algorithm is dependent on how efficiently each subproblem can be solved . 4.2 Solving subproblems
The subproblems in our formulation are in the form of x − υ2
2
ρ 2
.
These subproblems have been actively studied in other fields and , for many types of function φ , there exists a closed form solution . Here we only introduce solutions for the subproblems that are shared by all of applications considered in this paper . For more examples , refer to [ 4 ] . ( i ) When φ(x ) = I
C(x ) , the subproblem reduces to a Euclidean projection of υ onto a closed convex set C and it is denoted by xk+1 = ΠC(υ ) .
( 12 )
See [ 3 , 4 ] for examples of how to calculate the projections on various convex sets .
( ii ) When φ(x ) = x1 , the solution yields xk+1 = S1/ρ ( υ ) =
S1/ρ ( υ1),··· , S1/ρ ( υn ) where Sτ ( · ) is an element wise soft thresholding operator defined as
Sτ ( υi ) =
υi − τ sign ( υi ) 0 if |υi| > τ if |υi| ≤ τ
( 13 )
( iii ) When φ(x ) = 1 a least squares problem :
2 , the subproblem reduces to
2Ax − ω2 flflflfl A√
ρI
1 2
ω√
ρυ flflflfl2
2
. x − xk+1 = arg min x
The solution to the above least squares problem is given by solving the following linear equation
( A
A + ρI)x = A
ω + ρυ .
In most cases , ˜x = x + Lap ( λ ) ( independent multivariate Laplace noise ) . Thus , the log likelihood function is
1 n
2λ i=1 exp
−|xi − ˜xi|
λ ln L(x ) = ln P [ ˜x | x ] = ln
∝ − n i=1
Therefore , the maximum likelihood estimator ¯x that satisfies consistency constraints known to hold over original data is the solution of optimization problems such as minimize x − ˜x1 subject to Ax = b x
( 7a ) x 0
( 7b ) ( 7c ) where x ∈ Rn . The above problem arises from various applications in differential privacy , where the constraints ( 7b ) and ( 7c ) represent the auxiliary partial information that data users may have before the data release . We note that having the constraints in our setting is not a requirement but a choice .
To derive an ADMM algorithm , we reformulate the prob lem ( 7 ) as follows : y1 + I x,y,z
C(z ) minimize subject to x − ˜x − y = 0 x − z = 0 Ax − b = 0
( 8a )
( 8b )
( 8c )
( 8d )
C(z ) denotes an indicator funcC(x ) = 0 if x ∈ C , ∞ otherwise . The first where C = {x | x 0} and I tion defined as I term in the objective function measures L1 distance between the solution and the noisy answer while the second term , if included , ensures the non negativity of the solution . By introducing a new variable z , the inequality constraint has been absorbed into the objective function .
With the scaled dual variables [ 3 ] , the dual augmented problem of ( 8 ) can be written as max µ,ν,η min x,y,z y1 + I
+
ρ 2 x−˜x−y+µ2
2 +
ρ 2
C(z ) + Ax − b + η2
2 x − z + ν2
2
ρ 2
( iv ) When φ(x ) = αx1 + ( 1 − α)x2
2 , the solution is obtained by applying a thresholding operator Θα,ρ to each element : where ρ > 0 is a penalty parameter . Note that x , y and z are primal variables and µ , ν and η are dual variables . The ADMM algorithm for solving the above problem consists of iterating the following steps . xk − ˜x − y + µk2
2 y1 + I 1
2
ρ 2 ρ 2 xk − z + ν k2
C(z ) + x − ˜x − yk+1 + µk2
2
2 yk+1 = arg min y zk+1 = arg min z xk+1 = arg min x
( 9 )
( 10 )
( 11 )
2 +
+
1 2 x − zk+1 + ν k2 µk+1 = µk + xk+1 − ˜x − yk+1 ν k+1 = ν k + xk+1 − zk+1 ηk+1 = ηk + Axk+1 − b .
Ax − b + η2
2
1 2 xk+1 = Θα,ρ(υ ) = ( Θα,ρ(υ1),··· , Θα,ρ(υn ) )
0
ρ where Θα,ρ(υi ) is defined as
Θα,ρ(υi ) =
ρ sign(υi )
υi− α 2(1−α)+ρ if |υi| ≤ α if |υi| > α ρ .
ρ
( 14 )
4.3 General Recipe
Armed with closed form solutions to subproblems , we now describe the general recipe for the maximum likelihood postprocessing under consistency constraints . The first step is to rewrite the original problem in ADMM form . The general rule of thumb is to split terms in the objective function that are difficult to optimize simultaneously by introducing a new variable , so that each subproblem has a simple closed form solution . In ( 8 ) , variable y is substituted with terms in L1 norm in the original problem , which results in the constraint ( 8b ) . If a constraint represents a convex set on which
638 Algorithm 1 : The proposed ADMM algorithm Input : A , b , ˜x , ρ Output : a private estimate ¯x
A + 2I )
Cholesky decomposition
1 x0 ← ˜x , µ0 = ν 0 = η0 ← 0 , k ← 0 2 LU ← chol(A
3 repeat 4 5 6 7 8 9 10 11 until ( 22 ) is met 12 return x yk+1 ← S1/ρ ( xk − ˜x + µk ) see ( 13 ) zk+1 ← ( xk + ν k)+ see ( 12 ) xk+1 ← U−1(L−1( rhs of ( 15) ) ) substitution µk+1 ← µk + xk+1 − ˜x − yk+1 ν k+1 ← ν k + xk+1 − zk+1 ηk+1 ← ηk + Axk+1 − b k ← k + 1 an efficient projection algorithm exists , the constraint can be removed by introducing a new variable . The introduction of the indicator function I C(z ) in the objective function enables to replace the non negativity constraint ( 7c ) with the simple constraint ( 8c ) . Notice that the non differentiable term , L1 norm of x − ˜x , is completely decoupled by introducing auxiliary variables , y and z .
The second step is to determine the solver for each subproblem . A solver that exploits the structure of the problem is desired .
The x update in Algorithm 1 is a standard least squares problem and its solution can be found by solving the following linear equation :
( A
A + 2I)x = ( yk+1 + ˜x − µk ) + ( zk+1 − ν k )
( b − ηk )
+ A
( 15 )
A + 2I is symmetric and positive definite . Notice that A Let G = A A + 2I and q be the right hand side of the equation ( 15 ) . A classical and generic method is to use a factorization of G . When the structure of G or A ( eg , sparsity ) is known , more efficient algorithms which exploit the structure can be employed . Let G = LL be the Cholesky factorization of G . Note that there exists a unique such L since G 0 . Given L , using a forward substitution followed by a backward substitution , the equation ( 15 ) can be solved directly . We denote it by xk+1 = U−1,L−1 ( q ) where U = L . Note that , since G does not change throughout the iterations , the factorization needs to be calculated only once and can be cached for use in subsequent iterations . A summary of the proposed algorithm is presented in Algorithm 1 .
The problem ( 8 ) can be further split by introducing an other variable w as follows : y1 + I minimize subject to x − y − ˜x = 0 , x − z = 0
C2 ( w ) x
C1 ( z ) + I x − w = 0
1 3
( yk+1+˜x − µk ) + ( zk+1−ν k ) + ( wk+1−ηk ) wk+1 = ΠC2 ( xk + ηk ) xk+1 = µk+1 = µk + xk+1 − yk+1 − ˜x ν k+1 = ν k + xk+1 − zk+1 ηk+1 = ηk + xk+1 − wk+1 . This algorithm is almost identical to the one derived above , except the feasibility of the linear constraint Ax = b is maintained by the projection operation .
Remark 1 . Geometrically , the Euclidean projection of a vector υ = yk + ηk on the set C2 is to find a point in C2 that is closest to υ . In other words , it is a solution to the problem : arg min x − υ2 2 . x∈C2
Notice that this exactly matches the least squares formulation ( 6 ) , which implies that we can directly re use the efficient algorithms developed in prior works on least squares based post processing .
This algorithm is especially useful when the projection onto the affine set Ax = b can be done more efficiently than solving linear equation involving A . We show one such case in Section 52 4.4 When to stop iterating
The optimal solution of ( 8 ) satisfies the primal feasibility , r1 = x(cid:63 ) − y(cid:63 ) − ˜x = 0 r2 = x(cid:63 ) − z(cid:63 ) = 0 r3 = Ax(cid:63 ) − b = 0 and dual feasibility ,
0 ∈ ∂f1(y(cid:63 ) ) − ρµ(cid:63 ) 0 ∈ ∂f2(z(cid:63 ) ) − ρν ( cid:63 ) 0 ∈ ∂f3(x(cid:63 ) ) + ρ(µ(cid:63 ) + ν ( cid:63 ) + η(cid:63 ) )
( 16 )
( 17 )
( 18 )
( 19 ) where f1(y ) = y1 , f2(z ) = I
C(z ) and f3(x ) = 0 .
From y update of the algorithm , we have 0 ∈ ∂f1(yk+1 ) − ρ(xk − yk+1 − ˜x + µk ) = ∂f1(yk+1 ) − ρ(µk+xk+1−yk+1 − ˜x ) + ρ(xk+1 − xk ) = ∂f1(yk+1 ) − ρµk+1 + ρ(xk+1 − xk ) . ( 20 )
The z update yields
0 ∈ ∂f2(zk+1 ) − ρ(xk − zk+1 + ν k ) = ∂f2(zk+1 ) − ρ(ν k + xk+1 − zk+1 ) + ρ(xk+1 − xk ) = ∂f2(zk+1 ) − ρν k+1 + ρ(xk+1 − xk ) .
( 21 )
Since xk+1 is the minimizer of subproblem ( 11 ) , we have 0 = ρ(xk+1 − yk+1 − ˜x + µk ) + ρ(xk+1 − zk+1 + ν k )
+ ρ(Axk+1 − b + ηk ) = ρ(µk+1 + ν k+1 + ηk+1 ) . where C1 = {x | x 0} and C2 = {x | Ax = b} . This leads to a slightly different ADMM algorithm : yk+1 = S1/ρ ( xk − ˜x + µk ) zk+1 = ΠC1 ( xk + ν k )
Therefore , the optimality of Algorithm 1 is attained when ( 16 ) , ( 20 ) , and ( 21 ) are satisfied . This means that the algorithm can stop iterations when ri2 ≤ τ1 for i = 1 , 2 , 3 and ρ(xk+1 − xk)2 ≤ τ2
( 22 ) for sufficiently small tolerance values τ1 and τ2 .
639 4.5 Unique Solution
One of potential drawbacks of L1 based approach is lack of unique solution . The L1 norm in our objective function is convex but not strongly convex , which means that its optimal solution may not be unique and sensitive to an initial guess . To avoid this , we augment the objective function by adding a strongly convex function :
αx − ˜x1 + ( 1 − α)x − ˜x2
2 minimize subject to Ax = b , x 0 x
( 23 ) where α is a mixing parameter in the range [ 0 , 1 ] . The objective function is in the form of elastic net regularization function [ 23 ] , and its solution is proven to converge to an L1 solution when α is sufficiently large . The problem ( 23 ) in ADMM form is
αy1 + ( 1 − α)y2
2 + IRn
+
( z ) x,y,z,w minimize subject to x − y − ˜x = 0 x − z = 0 Ax − b = 0 and the update steps of ADMM algorithm are yk+1 = Θα,ρ(xk − ˜x + µk ) zk+1 = ( xk + ν k)+ xk+1 = solve ( 24 ) for x using Cholesky decomposition µk+1 = µk + xk+1 − yk+1 − ˜x ν k+1 = ν k + xk+1 − zk+1 ηk+1 = ηk + Axk+1 − b where the equation ( 24 ) is
( A
A + 2I)x = A
( b − ηk ) + ( yk+1 + ˜x − µk ) + ( zk+1 − ν k ) .
( 24 )
4.6
L2 Formulation
The constrained inference algorithm presented in [ 11 ] is efficient but is specific to hierarchical tree constraints . In addition , it is unclear how to adjust the algorithm to incorporate additional constraints , eg , non negativity of the solution or box constraints . With our framework , a general L2 minimization algorithm for Problem ( 7 ) that works with any linear constraints can be easily derived . The only difference with ( 8 ) in Section 4.1 is that the first term in the objective function is expressed with L2 norm ; hence , the change is only in the x update step . The x update step requires to solve xk+1 = arg min x x2
2 +
ρ 2 yk − x − ˜x + µk2
2
2
.
1
The equation above is similar to Tikhonov ’s regularization and has the following closed form solution . yk − ˜x + µk xk+1 ← 1
1 + ρ
( 25 )
Replacing the x update step in Algorithm 1 with ( 25 ) gives an L2 solution .
Algorithm 2 : ADMM algorithm for contingency table Input : A , b , ˜x , ρ , Mt Output : a private estimate ˆx
Cholesky decomposition see ( 14 ) see ( 12 )
A + 2I ) xk+1 ← U−1,L−1 ( rhs of ( 24 ) )
1 x0 ← ˜x , µ0 = ν 0 = η0 ← 0 k ← 0 2 LU ← chol(A
3 repeat 4 5 6 7 8 9 10 11 until Convergence 12 return x yk+1 ← Θα,ρ(xk − ˜x + µk ) zk+1 ← ( xk + ν k)+ µk+1 ← µk + xk+1 − yk+1 − ˜x ν k+1 ← ν k + xk+1 − zk+1 ηk+1 ← ηk + Axk+1 − b k ← k + 1
5 . ALGORITHMS AND EMPIRICAL EVAL
UATIONS
In this section , algorithms for applications discussed in Section 3.3 are derived and evaluated . To evaluate the performances of algorithms , a series of experiments is conducted on a variety of datasets . All experiments were conducted on a machine with AMD Opteron 2216 dual core and 8 GB RAM . To obtain an average performance estimate , all results are averaged over 10 runs . For ADMM , the mixing parameter α and penalty parameter ρ are fixed to 0.9 and 2.0 , respectively .
Before presenting experimental results , we briefly discuss the performance and scalability of linear program ( LP ) solvers . We applied simplex and interior point algorithms to solve the histogram estimation problem in Section 5.2 and observed that they fail to run when the length of histogram is greater than 212 . The worst running time for ADMM was 21 sec . while built in Matlab LP solvers run for several minutes and fail to return results . For the histograms of length greater than 210 , ADMM consistently outperforms LP solvers in terms of both speed and accuracy . 5.1 Contingency Table A marginal of order h , denoted by Mh , is a set of marginals {m(B)|B ⊆ H,|B| = h} . Notice that M0 is equal to the sample size ( or number of individuals in the dataset ) . Given Mh and a noisy contingency table ˜x , finding an estimate consistent with Mh can be formulated as αy1 + ( 1 − α)y2
2 + IRn minimize subject to x − ˜x − y = 0 , x − z = 0
( z ) x
+
Ax = b where α ∈ [ 0 , 1 ] and Ax = b represents the marginal constraints . The algorithm for solving the above problem is given in Algorithm 2 .
To evaluate the performance , we adopt three datasets , described in Table 2 , from [ 9 ] . The proposed algorithm is applied on the datasets and compared with existing algorithms : the Laplace mechanism ( LM ) [ 7 ] , the wavelet method ( Privlet ) [ 21 ] and the multiplicative weight mechanism ( MWEM ) [ 10 ] . For a fair comparison , negative cell counts reported by algorithms that do not generate the consistent output are replaced with 0 .
640 Dataset Dims.(k ) # samples(N )
Sparsity( % )
Edwards
Czech
Rochdale
6 6 8
70
1,841 665
65.6 1.5 64.4
Table 2 : Binary contingency table datasets
Figure 511 shows the MSE of each algorithm by different values of . Note that the y axis is in log scale . The top , middle and bottom rows of the figure correspond to the cases where M0 , M1 and M2 are publicly known , respectively . It is common in many settings of empirical studies that the number of individuals participated in the study ( sample size ) , M0 , is publicly available , and in some cases the distribution of one or more binary variables may be known a priori ( eg , the number of men and women in the sample ) . Observe that the proposed algorithms , equivalent to the naive Laplace mechanism when there is no publicly available information about the input , consistently outperform other algorithms . One interesting observation is that the performance gap between the proposed and other algorithms becomes noticeable on sparse datasets . This is because of the different behavior of L1 and L2 norms . In L2 minimization , large residuals ( noise ) are restricted because they are severely penalized . This , however , comes at the price of increasing the amplitude of small residuals . As a result , it tends to produce residuals of equal variance ( or magnitude ) . Hence , the L2 solution is not likely to have zero values in the places of zero entries in the original contingency table . On the other hand , the L1 norm encourages small residuals become smaller while causing large residuals to increase . 5.2 Histogram
Given ˜x corresponding to a noisy k ary tree , the hierarchical constraint can be expressed in matrix notation as Ax = 0 , where
1
−1 0
A = {aij} = if i = j if xj is a child of xi otherwise .
The dimension of A is ( p , n ) where p and n correspond to the number of non leaf nodes and all nodes , respectively . The matrix A is very sparse ; for each row , it contains only k+1 non zero elements . This enables to solve ( 15 ) efficiently by employing sparse linear solvers . With A constructed as above , it is possible to re use Algorithm 2 , but a better algorithm can be derived by slightly modifying the formulation .
αy1 + ( 1 − α)y2 minimize subject to x − ˜x − y = 0 , x − z = 0
2 + I x,y,z
C(z ) + IRn
+
( w ) x − w = 0 where C = {x | Ax = 0} is the set of vectors that satisfies the hierarchical constraint . The z update step reduces to the Euclidean projection on the set C , which is defined as zk+1 = ΠC(xk + ν k ) = arg min z∈C z − ( xk + ν k)2 .
As discussed in Remark 1 , the definition of Euclidean projection exactly matches least squares based formulation . Let
Algorithm 3 : ADMM algorithm for histogram release Input : A , b , ˜x , ρ , Mt Output : a private estimate ˆx yk+1 ← Θα,ρ(xk − ˜x + µk ) zk+1 ←Htree(xk + ν k ) wk+1 ← ( xk + ηk)+ xk+1 ← 1
( (yk+1 + ˜x − µk ) + ( zk+1 − ν k ) +(wk+1 − ηk ) )
1 x0 ← ˜x , y0 ← 0 , z0 ← x , µ0 = ν 0 = η0 ← 0 2 k ← 0 3 repeat 4 5 6 7 8 9 10 11 12 13 until convergence 14 return x
µk+1 ← µk + xk+1 − yk+1 − ˜x ν k+1 ← ν k + xk+1 − zk+1 ηk+1 ← ηk + xk+1 − wk+1 k ← k + 1
3 see ( 14 ) see [ 11 ]
Dataset
Size(n ) mini xi maxi xi
Sparsity( % )
Search Logs
Nettace
Social Network
32,768 65,536 11,342
0 0 1
496 1,423 1,678
52.1 96.6
0
Table 3 : Histogram datasets
Htree(υ ) denote the algorithm for universal histogram proposed in [ 11 ] . Given υ = xk +ν k , the function Htree finds a new vector in C that is closest to υ . This means the function Htree can replace the Euclidean projection in z update . The resulting ADMM algorithm for the above formulation is given in Algorithm 3 .
To evaluate our algorithm , we use the three datasets , namely Search Logs , Nettrace and Social Network ; for descriptions on the datasets , refer to [ 11 ] . We consider six existing algorithms for releasing histograms : two versions of the Laplace mechanism , LM(H ) and LM(L ) , the constrained inference algorithm ( HTree ) , the wavelet algorithm ( Privlet ) , the fourier perturbation algorithm ( EFPA ) [ 1 ] , and the clustering based algorithm ( P HP ) [ 1 ] . LM(H ) builds hierarchical tree over a histogram while LM(L ) is pure Laplace mechanism . The performance of HTree algorithm is largely dependent on the branching factor k . To tune this parameter , we iteratively searched for a value that gives the best result , and that value is used for the experiments . We note that none of the existing algorithms do guarantee the nonnegativity of bin counts in the released histogram . When a range query answer is negative , we replace it with 0 . This trivial step slightly improves the accuracies of existing algorithms .
Figure 521 describes the performance of the proposed algorithm on the unit length queries . On sparse datasets , Search Logs and Nettrace , ADMM(1 ) clearly outperforms HTree which is an L2 minimization based algorithm while they perform similarly on the Social Network dataset , as it was the case for the contingency table estimation .
In Figure 522 , the performances of algorithms for fixed length range queries are compared . The performance of each algorithm over a set of range queries is measured in terms of MSE . The range sizes are increased from 21 to 2h−2 where
641 ( a ) Edwards
( b ) Czech
( c ) Rochdale
Figure 511 : MSE by varying given ( Top : M0 , Middle : M1 , Bottom : M2 ) h is the height of the tree . For each fixed range size , 1000 random ranges are selected at uniformly random . The top and bottom rows correspond to = 0.1 and = 1.0 , respectively . Given a histogram of length n , both HTree and
Privlet are known to have O( log3 nffi 2 ) error bound and they show similar performances throughout all range sizes . In contrast , the error of the LM(H ) and LM(L ) increases linearly with the size of the range . While ADMM(1 ) is much more accurate for smaller ranges than HTree and Privlet , the performance on large ranges is not as good as them . This is because that , while L1 minimization reduces the amount of noise in most bins of the histogram , the result of that is an increased noise in the small number of bins . As we increase the range size , large noise in those bins will be accumulated and it degrades the accuracy of large range queries . P HP shows a good performance on Social network dataset . This is because the dataset is sorted . However , it performs poorly on Search Log dataset . Especially , when = 1 , its performance is even worse than that of naive Laplace mechanism . 5.3 Matrix Mechanism
The matrix mechanism ( MM ) is a general framework for answering linear queries in a differentially private manner .
The problem ( 5 ) in ADMM form is
αy1 + ( 1 − α)y2 x,y,z minimize subject to Ax − ˜r + y = 0 x − z = 0 .
2 + IRn
+
( z )
A + I )
Algorithm 4 : ADMM algorithm for matrix mechanism Input : A , ˜r , ρ Output : a private estimate ¯x 1 x0 ← 0 , µ0 = ν 0 ← 0 , k ← 0 2 LU ← chol(A
3 repeat 4 5 6 7 8 9 yk+1 ← Θα,ρ(˜r − Axk − µk ) zk+1 ← ( xk + ν k)+ µk+1 ← µk + Axk+1 − ˜r + yk+1 ν k+1 ← ν k + xk+1 − zk+1 k ← k + 1 xk+1 ← U−1,L−1,A
( ˜r−yk+1 − µk)+(zk−ν k) see ( 14 ) see ( 12 )
Cholesky decomposition
10 until convergence 11 return x
The resulting ADMM algorithm is described in Algorithm 4 . For MM , negative range query answers are replaced with 0 . In this experiment , the privacy budget is fixed to 01
Although it is shown in [ 22 ] that the matrix mechanism has some limitations and generally does not perform well , we apply our algorithm to show that the proposed algorithm combined with other existing algorithms can enhance the accuracy . Since solving the semi definite programming provided in [ 13 ] is computationally infeasible , we use the approximated version of matrix mechanism provided in [ 22 ] .
01020304050607080910 100101102103104MSELMADMM(‘1)ADMM(‘2)PrivletMWEM01020304050607080910 101102103104MSELMADMM(‘1)ADMM(‘2)PrivletMWEM01020304050607080910 101102103104MSELMADMM(‘1)ADMM(‘2)PrivletMWEM01020304050607080910 100101102103104MSELMADMM(‘1)ADMM(‘2)PrivletMWEM01020304050607080910 101102103104MSELMADMM(‘1)ADMM(‘2)PrivletMWEM01020304050607080910 100101102103104MSELMADMM(‘1)ADMM(‘2)PrivletMWEM01020304050607080910 10−1100101102103104MSELMADMM(‘1)ADMM(‘2)PrivletMWEM01020304050607080910 101102103104MSELMADMM(‘1)ADMM(‘2)PrivletMWEM01020304050607080910 100101102103104MSELMADMM(‘1)ADMM(‘2)PrivletMWEM642 Figure 521 : MSE for unit length range query by varying ( Left : Search Logs , Middle : Nettrace , Right:Social Network )
Figure 522 : MSE by varying range size ( Top : = 0.1 , Bottom : = 1.0 )
Due to the computational inefficiency of matrix mechanism , the domain size is fixed to 1024 by aggregating datasets . The matrix mechanism that estimates the true databases using least squares and L1 minimization are denoted by MM(2 ) and MM(1 ) , respectively . We compare MM to the following algorithms : the constrained inference algorithm ( HTree ) , the wavelet algorithm ( Privlet ) , the Laplace mechanism ( LM ) , and the Low rank mechanism ( LRM ) [ 22 ] . Figure 531 describes the performance of MM on each dataset . The workload W={wij} considered is a Hadamard matrix , generated by setting each entry to either 1 or 1 with probability 1/2 . As shown in the figure , MM performs worse than other algorithms but replacing its estimation step with our algorithm improves its accuracy on all three test datasets .
In Figure 532 , we consider three different types of workloads , namely identity , predicate and all range . Identity is an identity matrix which represents a set of queries consisting of all unit length queries . A predicate query q is a linear combination of unit length intervals : a1x1 + a2x2 +··· anxn where ai ∈ {0 , 1} and ai is set to either 0 or 1 with equal probability . The predicate workload contains 512 randomly chosen predicate queries . All range workload consists of queries for all consecutive intervals in the domain . Applying the pro posed algorithm to MM improves the accuracy in all cases , except all range workload on Nettrace dataset .
6 . CONCLUSION
In this paper , we formulated the post processing data perturbed with noise as a constrained L1 minimization problem and used the consistency constraints to improve the accuracy . The decomposition of our formulation using ADMM revealed that one of subproblems involves solving the previous least squares based formulation . This means that our approach can re use and take advantage of existing algorithms developed for the prior least squares based formulation . The effectiveness of the algorithm in improving accuracy is demonstrated through a series of experiments . Although we only described ( and provided algorithms for ) three applications of differential privacy , the proposed approach is generic and highly flexible that it can be easily adapted to other problems . We expect the proposed algorithm can help other private algorithms to achieve better accuracy .
Acknowledgments This work was supported by NSF grant No . CNS 1228669 and a gift from Xerox .
01020304050607080910 100101102103104MSE01020304050607080910 10−110010110210310401020304050607080910 100101102103104HTreePrivletADMM(‘1)LM(H)LM(L)EFPAP HP103104105106107MSE101102103104105106107102103104105106107101102103104RangeSize101102103104105106107MSE101102103104RangeSize10−1100101102103104105106107101102103104RangeSize101102103104105106107(a)SearchLogs(b)Nettrace(c)SocialNetworkHTreePrivletADMM(‘1)LM(H)LM(L)EFPAP HP643 Figure 531 : MSE of matrix mechanism by varying ( Left : Search Logs , Middle : Nettrace , Right : Social Network )
Figure 532 : MSE of MM on three workloads ( = 0.1 , Left : Search Logs , Middle : Nettrace , Right : Social Network )
7 . REFERENCES [ 1 ] G . Acs , C . Castelluccia , and R . Chen . Differentially private histogram publishing through lossy compression . In ICDM , 2012 .
[ 2 ] B . Barak , K . Chaudhuri , C . Dwork , S . Kale ,
F . McSherry , and K . Talwar . Privacy , accuracy , and consistency too : A holistic solution to contingency table release . In PODS , 2007 .
[ 3 ] S . Boyd , N . Parikh , E . Chu , B . Peleato , and
J . Eckstein . Distributed optimization and statistical learning via the alternating direction method of multipliers . Found . Trends Mach . Learn . , 3(1):1–122 , 2011 .
[ 4 ] P . L . Combettes and J C Pesquet . Proximal splitting methods in signal processing . In Fixed point algorithms for inverse problems in science and engineering , pages 185–212 . Springer , 2011 .
[ 5 ] G . Cormode , C . Procopiuc , D . Srivastava , E . Shen , and T . Yu . Differentially private spatial decompositions . In ICDE , 2012 .
[ 6 ] B . Ding , M . Winslett , J . Han , and Z . Li . Differentially private data cubes : optimizing noise sources and consistency . In SIGMOD , 2011 .
[ 7 ] C . Dwork . Differential privacy . In ICALP , 2006 . [ 8 ] C . Dwork , F . McSherry , K . Nissim , and A . Smith .
Calibrating noise to sensitivity in private data analysis . In TCC , 2006 .
[ 9 ] S . E . Fienberg , A . Rinaldo , and X . Yang . Differential privacy and the risk utility tradeoff for multi dimensional contingency tables . In Privacy in Statistical Databases , 2010 .
[ 10 ] M . Hardt , K . Ligett , and F . Mcsherry . A simple and practical algorithm for differentially private data release . In NIPS , 2012 .
[ 11 ] M . Hay , V . Rastogi , G . Miklau , and D . Suciu . Boosting the accuracy of differentially private histograms through consistency . PVLDB , 3(1 2):1021–1032 , 2010 .
[ 12 ] J . Lee and C . W . Clifton . Top k frequent itemsets via differentially private fp trees . In KDD , 2014 .
[ 13 ] C . Li , M . Hay , V . Rastogi , G . Miklau , and
A . McGregor . Optimizing linear counting queries under differential privacy . In PODS , 2010 .
[ 14 ] B R Lin and D . Kifer . Information preservation in statistical privacy and bayesian estimation of unattributed histograms . In SIGMOD , 2013 . [ 15 ] D . Proserpio , S . Goldberg , and F . McSherry .
Calibrating data to sensitivity in private data analysis : A platform for differentially private analysis of weighted datasets . PVLDB , 7(8):637–648 , 2014 .
[ 16 ] W . Qardaji , W . Yang , and N . Li . Differentially private grids for geospatial data . In ICDE , 2013 .
[ 17 ] W . Qardaji , W . Yang , and N . Li . Understanding hierarchical methods for differentially private histograms . PVLDB , 6(14):1954–1965 , 2013 .
[ 18 ] W . Qardaji , W . Yang , and N . Li . Priview : Practical differentially private release of marginal contingency tables . In SIGMOD , 2014 .
[ 19 ] A . Smith . Privacy preserving statistical estimation with optimal convergence rates . In STOC , 2011 .
[ 20 ] O . Williams and F . McSherry . Probabilistic inference and differential privacy . In NIPS , 2010 .
[ 21 ] X . Xiao , G . Wang , and J . Gehrke . Differential privacy via wavelet transforms . TKDE , 23(8):1200–1214 , 2011 .
[ 22 ] G . Yuan , Z . Zhang , M . Winslett , X . Xiao , Y . Yang , and Z . Hao . Low rank mechanism : Optimizing batch queries under differential privacy . PVLDB , 5(11):1352–1363 , 2012 .
[ 23 ] H . Zou and T . Hastie . Regularization and variable selection via the elastic net . Journal of the Royal Statistical Society , Series B , 67:301–320 , 2005 .
01020304050607080910 104105106107108MSEHTreePrivletMM(‘1)MM(‘2)LMLRM01020304050607080910 104105106107108MSEHTreePrivletMM(‘1)MM(‘2)LMLRM01020304050607080910 104105106107108109MSEHTreePrivletMM(‘1)MM(‘2)LMLRMIdentityPredicateAllranges100101102103104105106107MSEMM(‘2)MM(‘1)IdentityPredicateAllranges100101102103104105106107MSEMM(‘2)MM(‘1)IdentityPredicateAllranges100101102103104105106107MSEMM(‘2)MM(‘1)644
