From : KDD 97 Proceedings . Copyright © 1997 , AAAI ( wwwaaaiorg ) All rights reserved .
Proposal and Empirical Comparison of a Parallelizable
Distance Based Discretization Method
Jestis Cerquides’ and Ramon Ldpez de MBntaras
IIIA Artificial Intelligence Research Institute , Spanish Council for Scientific Research , CSIC
08193 , Bellaterra , Barcelona , Spain
{cerquide,mantaras}@iiiacsices
Abstract
Many classification algorithms are designed to work with datasets that contain onIy discrete attributes . Discretization is the process of converting the continuous attributes of the dataset into discrete ones in order to apply some classification algorithm . In this paper we first review previous work in discretization , then we propose a new discretization method based on a distance proposed by LBpez de MSntaras and show that it can be easily in parallel , with a in its complexity . Finally we high improvement show that our method has an excelempirically lent performance compared with other state ofthe art methods . implemented
Introduction that is a process transforms into discrete ones . Performing
Discretization ous attributes cess , we can apply discrete classification methods datasets containing continuous values . continuthis proto
In this work we introduce a discretization method it is parallelizable and that it achieves and show that a top performance . The work begins introducing the problem and reviewing some work done in discretization . Then we explain our algorithm , and perform its parallelization . Next we give the results of a set of empirical comparisons between the different discretization methods , to end up with a set of conclusions in the final section .
Current discretization methods
This section introduces tion problem and five solutions from different viewpoints . into more detail the discretizathat have been given
‘Jestis Cerquides research is supported by a doctoral scholarship of the CIFUT ( Generalitat de Catalunya ) .
Copyright 1997 , American Association for Artificial
In telligence
( wwwaaaiorg ) All rights reserved .
Discretisation methods classification In ( Dougherty , Kohavi , & Sahami 1995 ) three different axis ( Global vs . Local , Supervised vs . Unsupervised and Static vs . Dynamic ) are used to make a classification of discretization methods . We will add two more axis :
Direct vs . Incremental
Direct methods divide the range in k intervals simultaneously , needing an additional criterion to determine the value of k . Incremental methods begin with a simple discretization and pass though a improvement process , needing an additional criterion to know when to stop the discretization .
Bottom Up vs . Top Down
Incremental methods usually can be divided into TopDown and Bottom Up . Top Down methods begin with and its improvement process an empty discretization is simply to add a new cutpoint to the discretization . Bottom Up methods begin with a discretization that has all the possible cutpoints and its improvement process consists in merging ( delete a cutpoint ) . two intervals
It calculates
Some discretixation methods Equal size The simplest discretization method is an unsupervised direct method named equal size discretization . the maximum and the minimum for the attribute is being discretized and that partitions the range observed into k equal sized intervals . Equal method . the attribute titions it into intervals containing examples . ChiMerge method described x2 statistic from the two adjacent frequency direct It counts the number of values we have from that we are trying to discretize and parthe same number of incremental , bottom up in ( Kerber 1992 ) . ChiMerge uses the independence of the class intervals , combining them if it is another unsupervised is a supervised , to determine
Cerquides
139 incremental , is a supervised , is independent , and allowing them to be separate otherwise . Entropy top down mein ( Fayyad & Irani 1992),(Fayyad & thod described Irani 1993 ) . Entropy discretization recursively selects the cutpoints minimizing entropy until a stopping criterion based on the Minimum Description Length criterion ends the recursion . D 2 top down method described in ( Catlett 1991 ) . D 2 recursively selects the cutpoints maximizing Quinlan ’s Gain until a stopping criterion based on a set of heuristic rules ends the recursion . is a supervised , incremental ,
Distance Based discretization
Our algorithm , based on Mantaras distance between partitions ( Lopez de Mantaras 1991 ) , is global , supervised , static and Top Down incremental . This means that it is required to have two main components , a cutpoint selection criterion and a stopping criterion . Once the examples have been sorted by the attribute value , the main loop of our implementation of the method is : function MDiscretization(Set S,Attribute A ) Discretieation = 0 NewCutPoint = SelectNewCutPoint(S,A,Discretization ) While
( ImprovesDiscretization(S,A,Discretization,NewCutPoint ) ) Discretization = Discretization U {NewCutPoint ) NewCutPoint = SelectNewCutPoint(S,A,Discretization ) return Discretization is iterative , considering
Our algorithm the whole set for the selection of each new cutpoint , while previously seen Top Down incremental methods were recursive divide and conquer algorithms .
Cutpoint selection criterion the classification In the ID3 algorithm , we estimate power of an attribute by some measure ( Gain , Gain Ratio , 1 Distance , ) We want to generate a set of cutpoints so that the classification power of the resulting discretized attribute is the highest possible . Our idea is to follow a greedy heuristic in this search .
Each discretization can be identified with a set of cutpoints . We denote by PO the partition induced by a discretization D . We will note PD ” {TJ the partition induced in our dataset when the discretization applied to our attribute is the result of adding cutpoint T to the discretization D . In these terms the requirement is to find a cutpoint TA so that it accomplishes :
Where PC is the partition generated in the dataset and Dist stands for Mantaras by the class attribute
140
KDD 97 normalized Distance which is defined as :
Dist(Pc , PO ) = J(pClpD> + I(pDIpC )
VC n PD )
( 2 ) n Po),I(PD ) where I(PcIPD),I(Pc Shannon measures of information . see ( Lopez de Mantaras 1991 ) . are the standard For more details
Once TA is found , the next step is checking whether the cutpoint is significant enough to accept it or if otherwise no further cutpoints are considered necessary for the discretization . improvement
The stopping criterion We needed a heuristic improvement . We developed a stopping criterion based on the Minimum Description Length Principle to evaluate
( MDLP ) .
The development is parallel to that followed to apply MDLP to our in ( Fayyad & Irani 1993 ) . problem that needs to be solved is a communiThe problem cation problem . We have to communicate a classifier method , that allows the receiver to determine the class of each example . The sender knows all the attributes of the examples , plus the class , and the receiver knows all the attributes of the examples but not the class . The sender must choose the shortest description for sending a message that allows the receiver to correctly classify each example .
The encoding
I Len(CZasseslDisc ) length of communicating the set of classes based on a p cutpoint discretization can be de . If we composed as Len(Disc ) note N the number of examples of the dataset , Ic the number of classes , ,%i the number of classes in the interval i of the discretization , Si the set of examples in the same interval and Ent(S ) Shannon Entropy for the set S , we have :
Len(Disc ) = p Zog(N 1 ) + ( p f 1)t + 2 ki.Z3nt(Si ) i=o
Len(Classes\Disc )
ISilEnt
= 2 id
Given two discretizations , one with p and the other with p+i cutpoints , we will choose that with the minimal length . If it is the one with p cutpoints , then we stop our algorithm and no more cutpoints are added to the discretization , otherwise we consider including another cutpoint .
Computational complexity is not The computational the stopping criterion deeasily measurable , because pends on the data in which we are working . The com complexity of the method plexity of the sorting step is O(N ZogN ) . The complexity of the function SeZectNemCutPoint in our imis O(lc i N ) where k is the number of plementation classes in the dataset , N the number of examples and i in this run . the number of intervals of the discretization The complexity of ImprovesDiscretization is O(lc i ) . We will not consider it , because 0 ( k i ) c O(k i N ) . If we discretize the total complexity of the method the attribute with p cutpoints , is given by :
O(N EogN + k&N )
= O((ZogN +p2k ) N )
( 3 ) i=l restriction
Ic is constant , and very small with respect to N . To ease the evaluation of the complexity , we can use a heuristic in D 2 , and say that discretizations cannot have more than a fixed number of intervals . With this assumptions , complexity is bounded by the sorting step , as for Entropy , D 2 or ChiMerge . as the one imposed
Parallelization of the met hod We have found that without with Point . We will parallelize this function to obtain a high improvement the complexity of the algorithm , the sorting step , is mainly related the complexity of the function SelectNewCut in the performance of the algorithm . including
To simplify the explanation we suppose we have asto each example in the dataset . The sign a processor parallelized version of the algorithm is as follows :
Step 1 . The sorting step can be parallelized with N processors in time O(EogN ) . From now on we assume the values are sorted by the attribute being discretized . table Step 2 . We have to calculate a contingency to be for each processor , in order for the processor able to evaluate the Distance between the partition generated by the class and the partition generated by fixing the cutpoint its value and the value of the neighbour processor on its left side . This can be done in O(k ZogN ) time in two steps , the first one by adding the information of all the processors following a processor binary tree until it arrives to the root , and the second one by descending the processors the information we have previously put together tree , distributing in the first step . just between l Step 3 . Now we have that each processor has its cortable . Each processor evaluthe Distance measure for its cut responding contingency ates independently point . This is done in time O(rC i ) .
Step 4 . We have to calculate the processor with minimal Distance . We use again the binary processor tree , and this gives us a time O(ZogN ) . the new cutpoint
If it turns out that the MDL criStep 5 . The root processor evaluates is not terion . it to the other processors , good enough , broadcasts the new and the algorithm stops here . Otherwise cutpoint is annotated by the root processor . The information of where the cutpoint has been fixed , and the contingency table of the processor whose cutpoint has been selected is broadcasted to all the processors . This can be bounded in time by O(lc i ZogN )
Step 6 . Each processor its contingency table considering that a new cutpoint has been fixed . This step is bounded by O(lc i ) transforms
Step 7 . We return to step 3 .
The time complexity , when having h processors ( h 5 N ) and the attribute discretized with p cutpoints , is bounded by O(lc p2 $Zog h ) , just assigning 9 of the to each processor . Concretely , when havexamples ing N processors , the time is O(rC p ” ZogN ) , which is clearly better than the time found for the sequential procedure . A parallel version of the method has been implemented in ( Cerquides 1997 ) . in MPI and its code can be examined
Empirical comparison to measure the discretization
Comparison design We will use the accuracy of two classification algorithms goodness . The two algorithms will be ID3 ( Quinlan 1986 ) ( with no ( Langley , Iba , & Thomppruning ) and Naive Bayes son 1992 ) . We will run each algorithm in 9 different domains with different characteristics ( see Table 1 ) . For each learning algorithm , discretization method and dataset we do 50 runs , each one with 70 % of the examples as training set and the remainder 30 % as test set . We take the average of the 50 runs as a measure of performance . We also keep the results of the 50 runs to make two statistical tests : Rank and Signed Rank . In ( Cerquides 1997),(Gibbons 1971 ) one can find a complete explanation of this tests . significance
Comparison results For each dataset Average accuracies comparison and classification algorithm we rank the 6 discretization methods , from the first place ( the most accurate ) to the sixth one . The results are displayed in the two tables that appear below . The rows are ordered with the best method on the top and the worst on the botranked tom . In the tables 55555 means the algorithm
Cerquides
141
