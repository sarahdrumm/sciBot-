From : KDD 97 Proceedings . Copyright © 1997 , AAAI ( wwwaaaiorg ) All rights reserved .
Development of Multi Criteria Metrics for Evaluation of Data Mining Algorithms
Gholamreza Nakhaeizadeh
Daimler Benz AG , Research and Technology F3S/E ,
P . 0 . Box 23 60 , D 89013 Ulm , Germany nakhaeizadeh@dbagulmDaimlerBenzCOM
Technical University Vienna , Department of
Econometrics , Operations Research and Systems Theory ,
Argentinierstr . 8/l , A 1040 Vienna , Austria
Alexander Schnabl e9025473@fbmatuwienacat
Abstract
The main aim of this paper is to suggest multi criteria based metrics that can be used as comparators for an objective evaluation of data mining algorithms @M algorithms ) . Each DM algorithm is characterized , generally , by some positive and negative properties , when it is applied to certain domains . Examples of properties are the accuracy rate , understandability , interpretability of the generated results and stability . Space and time complexity and maintenance costs can be considered as negative properties . By now there is no methodology to consider all of these properties , simultaneously , for a comprehensive evaluation of DM algorithms . Most of available studies in literature use only the accuracy rate as a unique criterion to compare the performance of DMalgorithms and ignore the other properties . Our suggested approach , however , can take into account all available positive and negative characteristics of DM algorithms and can combine them to construct a unique evaluation metric . This new approach is based on DEA ( Data Envelopment Analysis ) . We have applied this approach to evaluate 23 DM algorithms in 22 domains . The results are analyzed and compared with the results of alternative approaches . and use them
Introduction based on
( DM algorithms ) in Databases ( KDD ) is a process Knowledge Discovery that aims at finding valid , useful , novel and understandable patterns in data ( Fayyad et al . 1996 ) . The core of this process consists of the application of various Data Mining algorithms statistical approaches , Machine Learning , Neural Networks etc . One of the essential issues in both the development and application phases of DM algorithms is , however , the lack of objective metrics making a fair evaluation of the algorithms possible . In our opinion : 1 . Such metrics , on one hand , should take into account not only positive properties ( advantages ) but also negative characteristics ( disadvantages ) of DM algorithms . Only in this case is a fair evaluation possible . In buying a car eg people consider not only positive points like safety , comfort , quality and after sales service , but also the negative points like high price , high fuel consumption ,
Copyright 0 1997 , American Association for Artificial Intelligence ( wwwaaaiorg ) All rights reserved high repair cost , environmental issues etc . We appeal for consistency in applying the same philosophy in the evaluation of DM algorithms .
2 . On the other hand , a fast and comprehensive evaluation of various algorithms is then possible , when a unique metric is available that can reflect objectivity and not in ad hoc manner all known positive and negative properties of algorithms . In the above mentioned example , it would be a significant help for car buyers , if they can use a unique metric to conclude that car A is superior to car B , considering all known positive and negative properties of the cars .
The main aim of this paper is that to suggest metrics for the evaluation of Data Mining algorithms that cover both points 1 and 2 above . The rest of the paper is organized as follows : In section 2 , we critically review the available evaluation criteria in the literature and discuss the need for developing multi criteria based evaluation metrics reflecting all the available positive and negative properties of DM algorithms . In section 3 , we suggest a new evaluation approach based on the concept of Data Envelopment Analysis ( DEA ) that , in our opinion , covers both requirements 1 and 2 . In section 4 , we use this approach and evaluate the algorithms that have been ~ ~ I , : ,.L . 12 A E&^&.I ^ ,x2:_, : L mvOlveu m inns : proJeLl 3~ar~uy ( 1~11~1110 eL al . i994j . Tiie last section is devoted to discussions , conclusions and suggestions for further research .
Available Evaluation Criteria
The definition of the KDD process given by Fayyad et al . ( 1996 ) considers a lot of positive properties for the patterns one obtains at the end of a KDD Process . The patterns should be new , valid , understandable and usable . This leads to definition of interest@gness ( see also Hausdorf and Miiller 1996 , Silberschatz and Tuzhilin 1995 ) . Such characteristics can be used for the evaluation of DMalgorithms which are used to obtain the patterns . For example , algorithm A would be superior to algorithm B if it leads to more understandable or more valid patterns . If such characteristics can be described in a measurable metric then they can be used for an objective evaluation of DM algorithms . One important problem in dealing with such properties is that by now most of them are not measurable . For
Nakhaeizadeh
37 of understandability example , novelty or usefulness are only subjective and can not be measured . In dealing with complexity one can argue that the number and the length of the extracted rules might be a measure for complexity ( see also Klinkenberg and Clair 1996 ) . This argument is , however , only valid , if one compares two or more rule based DM algorithms . This approach can not be used to compare a rule based algorithm eg CN2 ( Clark and Niblett 1989 ) with an linear discrimination or neural algorithm based on or networks . Measuring interpretability is more difficult because in this case the domain specific background knowledge should be available . Only in the light of this background expertise can the results be interpreted ( Bratko 1995 ) . Specifically dealing with validity in one dimension there are , however , reasonable criteria like predictive accuracy rate , the cost of misclassification ( Knoll et al . 1993 ) , robustness ( Hsu et al . 1995 ) , generalization and domain coverage ( Klinkenberg and Clair 1996 ) and stability ( Tumey 1995 ) that can be used as objective measurable metrics to evaluate DM algorithms’ . Such criteria are applicable , however , only to the DM algorithms based on supervised learning . In the case of unsupervised learning , it is not easy to measure the validity ( see for example the discussion on vaiidity of ciuster anaiysis aigoritlnms in Gordon 1996 ) . As already mentioned , to perform a fair comparison of the alternative DM algorithms , one should take into account not only positive but also negative properties . To negative properties belong eg high complexity , high cost of misclassification , high training and testing time , high inference time and high maintenance costs . It is true that in some available contributions in the literature the authors measure such negative properties as well and discuss them ( see Michie et al . 1994 , Klinkenberg and Clair 1996 ) but by now there is no comprehensive metric available for evaluation of DM algorithms reflecting all known positive and negative properties of such algorithms , when they are applied to different domains . In the next section we introduce a multi criteria based metric that overcomes this shortcoming and can be used for an objective ranking of alternative DM algorithms . Our approach uses the DEA concept developed originally by Operations Research Community to measure technical efficiency .
’ It should be mentioned that in some cases is necessary to define standards . For example we need to standardize what is meant by max . memory . A linear discrimination algorithm ( LDA ) as implemented in SAS may need different memory as LDA impiemenieci by SPSS , though the accuracy rate wili be identicai . This idea was suggested during a useful discussion with Charles Taylor .
38
KDD 97
DEA Based Evaluation Metrics for
1996 ,
Main Idea The main idea of DEA is due to Chames et al . ( 1978 ) . More recently , however , it has been further developed in different directions and applied to different domains ( see a comprehensive Emrouznejad et al . bibliography on DEA ) . It is not the aim of this paper to discuss the different versions of DEA . Our aim is to discuss the main idea and to explain how this idea can be used to develop evaluation metrics for ranking alternative DM algorithms . Originally , DEA tries to develop a ranking system for Decision Making Units ( DMUs ) . In our case , each DMU is a DM algorithm . In DEA terminology , positive properties are called output components and negative properties input components . In our terminology , a typical example for an output component is the accuracy rate produced by a supervised DM algorithm . A typical input component is the computation time that the DM algorithm needs for training . Generally , output components are all components where higher values are better and input components are those where these components , we can now define the eficiency of a DMalgorithm as follows : lower values are better . Using eflciency = c weighted output components c weighted input components in this point , namely that can be used for
As the above relation shows , the definition of efficiency covers all positive and negative characteristics of a DMalgorithm and efficiency in this form can be regarded as a multi criteria based metric the evaluation of DM algorithms . In our opinion , efficiency as defined above is more general as interestingness defined by Fayyad et al . ( 1996 ) that covers only the positive properties of DM algorithms . Due to the fact that the values of input and output components are given , only the values of the weights are necessary to determine the value of efficiency . Precisely in determining the weights , DEA differs from the alternative approaches , for example , from the point awarding method . By using point awarding , one can award to the accuracy rate ( which is a positive property for DM algorithms ) a certain number of points , say 10 , to each percentage accuracy rate which exceeds a threshold value ( eg the accuracy rate of the naive predictor ) . This means that if the accuracy rate is three percent better than the naive nt rvli, tnr y~ “ ~ “ c ” ’ Awarding negative properties ( eg time consume or space complexity ) is done in the same way . The total achieved Score of each DM algorithm is determined by summing the points across different attributes . Such scores can be used now for ranking the DM algorithms , the higher the score the higher the rank . This approach suffers , however , from the some drawbacks . Specifically , thnn ~llrnrithm .n1v11 CIl ” U’~ “ “ ~ “ “ ’ it suffers will 1.111 “ V u uvu arrrmv brl from the he
‘VI nnintr J ” y ” ‘.‘C ” .
If subjective opinion of decision makers who determine the required awarded points . It might be eg that an attribute with the same importance is awarded different points . In many cases , it is very difficult or impossible to award or calculate objective weights . the weights are the corresponding prices of a unit of each input and output component ( this would be a natural way to determine the values of input and output and calculate the efficiency ) , then who can say eg how much cost a unit of the accuracy rate or a unit of the rule complexity ? Normally , such mice< are unknown . L . mr mL . _. __ DEA evades the ad hoc judgments described above . In DEA the awarded points ( weights ) are determined for each DM algorithm individually during the computation by maximizing the efficiency in the following way . Suppose that we are evaluating n algorithms with p input and q output components and for the algorithm k let :
= amount of input X ; kx Ok , = amount of output y ; UhX = weight for input x ; QY = weight for output y . Denoting the efficiency of the DM algorithm k by Rk now leads to :
_^^_
DEA chooses the weights so that the efficiency of the algorithm k is as close to 100 % as possible ( see Doyle and Green 1YYl ror more detail ) . At the same time , the other algorithms should not have an efficiency more than 100 % using the same particular weights . Obviously this is an optimization problem that can be transformed to the following linear program ( LP ) : ,%3kCt maximizing Rk in the relation ( 1 ) subject to :
VdUCS Of ukl , UE , , Ukp and Vkl , V,Q ,a , Vk by
( 2 ) for i = 1 , 2 , , k , . , It , uh 2 0 and Vb 2 0 for all x and y . If algorithm k does not achieve the given threshold ( lOO% ) , then it is not efficient and there is at least one algorithm among the others dominating it . There are different ways for setting the weights in ( 2 ) . The most used ones are the input oriented and the output oriented optimization ( Ali and Seiford 1993 ) . The goal in inputoriented optimization approach is to reduce radially the input levels as much as possible . Radially means that all component values are changed by the same proportion . Conversely in output oriented optimization approach the main goal is to enlarge radially the output levels as much as possible . Keeping the input as constant , the above LP is transformed to maximizing of :
Rk = x;= ,
%y Oky subject to :
( 3 ) the &jgifigc mentinned __ , it . An alredv . _ _ ‘ , for i = 1 , 2 , , k , , n , uh 2 0 and vky 2 0 for all k , x and y which can be solved for each algorithm using the Simplex Method . After solving this LP and determining the weights , the algorithms with Rk = 1 ( 100 % ) are efJicient algorithms and form the e@ciency ffontier or envelope . The other algorithms do not belong to the efficiency frontier and remain nutnid~ nf ^ I^ I I of eficiency is more general than interestingness as suggested by Fayyad et . al . ( 1996 ) . One can connect also both concepts in this form that more efficient algorithms are more interesting . For ranking the algorithms , one can use the approach suggested by Andersen and Petersen ( 1993 ) @P model ) . They use a criterion that we call it the AP value . In input oriented models the AP value measures how much an efficient algorithm can radially enlarge its ;~~,,+~IP.,P~~ uqJuL ‘Ld ” Y,o is analogous ) . For example , for an input oriented method an AP value equal to 1.5 means that the algorithm remains still efficient when its input values are all enlarged by 50 % . If the algorithm is inefficient then the AP value is equal to the efficiency value . To explain the above approach , we present the following simpie exampie .
/mmt \ “ urpuc “ uulrrju nff;Annt U,I1IbIUUL
,wLm,tmA
rh;la VVIUIC , mmo;n;nrr IUuuul‘lll~ o+;ll Dull
Example Suppose that we have four DM algorithms A , B , C and D with one positive property ( output ) and two negative properties ( input ) given in Table 1 . Figure 1 shows these data graphically in the input space .
A\BICiD
Comparision unit Input 1 Input2 output Table 1 : The data of the example . Selection output values equal to 1 is just for an easier interpretation .
I , 800 6.00 1 .oo
II : 5001 m , L lOOk
5.0 q" 1.001
LOOi
1.00 the input space . The efficiency
DEA creates an efficiency frontier ( bold lines ) , which is convex in is formed in Figure 1 by the algorithms A , B and C . Their efficiency values are equal to 1 . In the input space , D lies outside the efficiency frontier . If D could use , however , the input values of its corresponding projection D* , then it frontier . Graphical would representations like Fiure 1 are only possible for two or _ the efficiency lie on frontier
Nakhaeizadeh
39 three input components . Using LP ( 3 ) leads to the solution given in Table 2 . We can see in Table 2 that DEA classifies all efficient algorithms A , B and C with the efficiency value of 1 . The inefficient algorithm D gets the efficiency value 0.79 which is just the ratio of OD* to OD . It has to reduce the input values by 21 % to become efficient . In this case the values for input 1 and input 2 would be 4.32 and 4.47 , respectively , which correspond to the coordinates of point D ” . Input 2
0 .
Lput 1
Figure 1 : Efficiency frontier for DM algorithms A , B and C ( bold lines ) . Algorithm D is inefficient . To become efficient , this algorithm has to reduce both input values until it reaches the input values of the point D* . Algorithm B which is efficient could use the input levels of point B* and would still be efficient .
D
I
0.79
1 0.79
1
4
1
Table 2 : The solution of the DEA algorithm for example
As mentioned before , to rank efficient algoritts , we use the AP model . Algorithm B eg has an AP value equal to 1.20 , ie it can enlarge the input levels by 20 % remaining still efficient . Graphically we get this value from OB*/OB .
Empirical Analysis
The most comprehensive evaluation of DM algorithms known to us is the study of Michie , Spiegelhalter and Taylor ( MST 1994 ) . They compare the performance of 23 classification algorithms on 22 different domains . To rank different algorithms , when they are applied to a certain domain , MST use only one property namely the accuracy rate for the test data set although they have data about
40
KDD 97 maximum computer storage , training and testing time , training and testing error rates and training and testing misclassification costs ( where the costs are available ) . As an example , the results reported by MST for the Credit Management Data set is presented in Table 3 . Their ranking for the algorithms is given in the last column . The is used for missing ( or not applicable ) notation ,,* “ information , and ,,FD “ is used to indicate that an algorithm failed on this data set . To obtain a DEA based ranking for the algorithms applied by MST q?p , haye USed the ipAp ” &Cd output oriP,nted versions of DEA described above . In the following , these versions are denoted by I and 0 , respectively . To rank the algorithms , we have used the AP model . We have used three input components ( max . storage , training time and testing time ) and one output component ( accuracy rate defined as 1 error rate for the test data set ) . As an alternative , we have also used the version with an additional output component ( accuracy rate for training data set ) . Input oriented versions are denoted by 41 ( one output and three input components ) and by 51 ( two output and three input components ) . The same is valid for outputoriented versions denoted by 40 and 50 .
Al.&;hm “ 2ktirn ” . “ . “ ~_ Quadisc Logdisc . I , . , SMART ,,x ^ . , ” ALLOC*O k:NN CASTLE _ “ , “ ,_ , , CA.RT IndCART NWdD I “ , ” AC2 , , Baytree NaiveBay CN2 , , , C4,5 ‘We Cal5 _.,~,~ “ . “ _ “ , Kohonen DIPOL92 Backprop
. , , “ , ,I
. ” , , “ . , ”
, , “ , , , . , ,
, .
, .
._ ”
, .
, ,
, , , .
Testing
Training
Training
, ,
Testing .
.~ ~ . ~~ ~
*^^ . “ “ . u:uu o.oscl , 0.030 , 0.020 . , , 0.03! 0.088 0.047
3.8 . “ _ . “ ‘2.5 14,2 _._ 5.4 I ” . ?6! 81.4 !=P 415 7 ‘ 2.0 . . ^ .3607:0
Max . Storage Time ( sec . ) Time ( sec . ) Error Rates Error Rates Rank ” . s % E .,, , , “ . . “ . “ ciK “ . “ ,_ , ” , , , ?G . 21 , 0.051 7l.L . , 67.2 889 *s _ ’ 65.6 I?!??! “ , . , . ,__ 0.021 412 27930.0 1 IX ,,,, , , ” x , , ” ,, , , _ ” _ , “ . “ . “ , , ” , “ . “ , ^ , , z20 22069.7 !?.? ? ? . !9 0.028 'OS 124lVJ3 .A ? 370 1 0051 48 19 , , , ” , . , “ , , , ^ “ , “ _ “ “ . “ 1 , . . “ L “ ,I ,.,,,1 I _ “ II ” ,x JII “ .x, , , L , ,~ , ^ . , , ._ _,, , , , .I f ? n , 423 1 6 . : ‘656 13 . 3035.0 104 _ , . . * . ,725O 8 . 5?!80 53.1 7 1368 , . 16 24.3 956 12 2638.0 2100 , ” , “ , I,x . , . , , . 620 171.0 3 , 4470.0 ,377 553.0 167 ” ,715 2340.0 218 .5950,0 ~48 :~ ~
3.3 , 2.8 9.5 ,. ll ” ll ” , “ l . , ^ . “ II ” I ” , “ ,i , ” , 158.L J I. : ? 7.2 . . . ” 57.8 3.9 , b.025 b.oi3 O.??P 0.028 , , 0.043 0.032 , , wz
, . “ , , , . , “ ,
, , ,
,
L
~
FP , ,
, ,
?? ? , ‘0.010 .O~!xF ? .v? ? a:?! ? 0.041 0.000 .P:W . o!??! 0.018 OF ? 0.020 , 0.020 w3 0.024
.I8 4 , ” Ix _ , “ “ . , “ , . “ . “ . , ,, , , xI !6 1 4 i0 is
0.046 , 0.023 ??4 ? 0.020 0.023 ^ ^__ u.lJ31 0.040
ll ” l . “ , ,~ ,,,, , , “ I “ Ix , ” “ I ” Ix . . “ , ,,x_I_ I , “ ,_ ,. l “ l , “ l . “ ^ I ” , . “ “ IIx
., , , I , .
253 476
‘^ ^ ,qj>.0 212io
Q y$ Table 3 : Results for Credit Management data set ( 2 classes . 7 attributes , 20 000 observat:ons ) p . 133 , using 22 algorithms
.‘?68 , . 52.9
The DEA ranking results for the Credit Management data set are given in Table 4 . We have omitted algorithms CART and Kohonen from further analysis because as Table 3 shows there is not always enough information about input and output components for these algorithms . We can see from Table 4 that the MST ranking results based only on one comparison criterion differs , generally , from our results which are based on multi criteria metrics . For example , algorithm NaiveBayes is ranked by MST as 16* . Our ranking using different versions of DEA varies , however , between 7 and 10 . The reason is that the relatively low accuracy rate of this algorithm ( 1 0.043 = 0.957 ) is compensated by low training and testing time . This is the same for NewID and ITrule . For these algorithms low accuracy rate is adjusted by low max . storage and low testing time . On the other hand , MST rank DIPOL92 as the first . In 3 of the versions of DEA , it is ranked , however , between 6 and 9 . In this case , the high accuracy rate is compensated by high training time . Considering Qua&c , we can see that it is ranked by MST as 21St . Our results show that this algorithm is not efficient at all . It has got a rank between 12 and 18 using different versions of DEA . It seems that in this case the low accuracy rate can not be adjusted by the other components . There are some cases for which the MST ranking does not differ radically from our ranking . For example , SMART has got the first rank by MST and a ranking between 2 and 5 in our analysis . If we examine the input and output components of this algorithm , we can see that with the exception of time the other values are relatively good and apparently the high training time can not obscure , significantly , the positive effect of the other components . the training
Algorithm Discrim _ . , . . . Quadisc “ . ” , Ix ” l_ “ . . “ ll . Logdisc 1 , ” . “ ” SMART ,,I , , k NN , . CASTLE ” lll “ l~ “ l ” “ . , . IndCART l lll 1 ” 1 _( _ “ . ” NewID . “ ll . , , “ . AC2 IIIII ” .xII ” . “ .l ~B aytree ,l~l_l NaiveB ay Ix II CN2 _ lll _ll ” _ c4.5 11 ^ ITrule Cal5 DIPOL92 _I Backproe . RBF “ _ LVQ Table 4 : Ran data set using MST and different DEA models ( italic figures mean efficient )
11 6 ~ 1 _ ~7 I ~ 15 18 i? ig algorithms for the Credit 1
8 4 9 7 ” I 1_1 18 19
7 2 6 _I 3 I_ ~_ 14 18
1x 4 1 “ 4 . I10 __ . 15
9 2 6 3 16
* I
~~ “
Concerning the different versions of DEA , we can see from Table 4 that for Credit Management data set each algorithm which is efficient in 41 is also efficient in 51 , but not all efficient algorithms in 51 are efficient in 41 . The same is valid for 40 and 50 . The more components are included into the DEA model the more algorithms are classified as efficient . Our further examinations have shown that this is not valid , generally . Using different
DEA versions ( as it was expected ) does not lead to exactly the same but to similar rankings . Exceptions are DZPOL92 in 41 , NewID , Baytree and C4.5 in 51 and AC ? , CN2 which in some cases are not efficient at all . We have done this sort of analysis for all 22 data sets , but we can not report the whole results here . More details can be found in Jammemegg et al . ( 1997 ) . To make an additional comparison , we report , however , in Table 5 the top 5 algorithms selected by DEA model 41 for each data set and compare our results with those of MST reported in p . 185 of their book .
_
IIFirst
. ” ll “ ” __ , “ “ . , x
NewID Cascade Backprop DIPOLY 2 i.VQ k NN LVQ
Data set ISecond .Credman .!?!qw?2 SMART . , , , , , NaiveBay CCiUSl Dig44 C&+isc “ “ _ . , . LVQ ‘ “ .’ KL ALLOCSO Vehicle Letter NN’ &mn DIPOL92 c4.5 SatIm k NN Segm , “ . , . “ , , I _k NN Cut20 i&ID Cut50 CASTLE Heed IndCART Heati +ytriX Cffier .f$! “ . “ f!$%E
New ” ’ ?aytree Discrim SMART k NN ” y!KT .
. “ . . _. , , “ E?J!vee
_ I , , , . ,
. ,
IThird Discrim ITrule DIPOLYZ , . l . “ . ” ,, , , Cascade TMART Baytree . CASTLE Discrim c4.5 Discrim D&m Cascade b&ID Cal5 pi ,
IFourth c4.5 c4.5 Discrim ” “ “ , . Discrim NaiveBay Naiv+y NaiveBay NaiveEgy SMART , , . ,,‘, . , “ . ,I __ “ ,^xx , Backprop Backprop CART Baytree DIPOLj2 fJ?kJ?kT
IFifth BaytlW , DIPOL92 NaiveBay , DIPOLY 2 CART Discrim NewID Baytree CART . , “ . , . , LVQ DIPOL92 Baytree DIPOLYZ CART “ ” ‘w;ree c4.5 Baytree , , Logdisc Bayire.! DIPOLY 2 Baytree
DNA Tech Belg I+!gII Faults Tsetse Table 5 : The Top 5 algorithms of the DEA model 41 : each data set
Backprop’ Ca15 , Backprop Cal5 Discrim Cal5
Disc& ” ’ NaweBay NewID ‘iackwp CART NaiveBay
CASTLE’ k I6 k NN N+D NaiveBay NewID
NewID Discrim DIPOLYZ CA%E ITrule Discrim r
As it was expected , our results based on the DEA version 41 differ , generally , from the results of MST based on only one comparison criterion namely the accuracy rate of the test data set . For example , in our results DZPOL92 has the first rank for four data sets . In MST results for none . On the other hand , KNN is selected as the best algorithm by MST for four data sets . In our results only for two .
Discussion and Conclusions
As we mentioned before , the main idea of DEA is extended in different directions . These extensions can handle some of the limitations of the basic versions of DEA . To such extensions belong dealing with nondiscretionary inputs and outputs , handling of categorical inputs and outputs and dealing with flexible weights ( see Chames et al . 1996 , Chapter 3 ) . DEA models were originally developed in the Operations Research Community and are used to rank DMUs in different domains ( see eg Paradi et al . 1995 and Emrouznejad and Thanassoulis 1996 ) . In this paper we have shown that such models can be used effectively to rank DM algorithms and provide a fair evaluation . This to have a better enables understanding of the real performance of the developed DM algorithms . As discussed in section 2 , the number of the KDD community
Nakhaeizadeh
41 measurable input and output components characterizing the positive and negative properties of the DM algorithms is at present too low . We have shown in this paper that even in this situation using the DEA based multi criteria metrics is more suggestive than using a single criterion ie the accuracy rate . Further research can be done in different directions . First of all , the practicability of different extensions of DEA described above should be examined , when they are applied to evaluation of DM algorithms . Secondly , the adaptive DEA models the dynamic aspects ( changing of inputs , outputs , preferences etc . ) automatically . In the DEA Community some efforts have be done in this direction ( Piire and Grosskopf 1996 , Schnabl 1996 ) . Further basic research is still necessary in this field . are needed to handle
Acknowledgment
The Authors would like to thank Harald Kauderer , Werner Jammemegg , MikulaZ Luptacik , Thomas Reinartz , Charles Taylor and two anonymous reviewers for fruitful discussions and suggestions . The research of Alexander .AG , Qhnnhl LL II Research and Technology , Ulm , Germany . was amnnrtad ‘. I by &i~&~ fJ~~z fin~~ri~]y
I r
_
References
Ali , A . I . and Seiford L . M . 1993 . The Mathematical Programming Approach to Efficiency Analysis . The Measurement of Productive Efficiency . In Fried , H . O . , Lovell , C . A . K . and Schmidt , S . S . eds . Techniques and Applications , 120 159 , Oxford University Press . Andersen , P . and Petersen , N . C . 1993 . A Procedure for Ranking Efficient Units in Data Envelopment Analysis . Management Science , Vol . 39 , No . 10 : 1261 1264 . Bratko , I . 1995 . Machine Learning : Between Accuracy and Interpretability . In Aha , D . and Riddle P . eds . Proceedings the Workshop on Applying Machine Learning in of Practice at the Twelfth International Machine Learning Conference . Chames , A . , Cooper , W . W . and Rhodes , E . 1978 . Measuring the Efficiency of Decision Making Units . European Journal of Operational Research 2(6 ) : 429 444 . Char&s A . , Cooper W . W . , Lewin A . Y . and Seiford , L . M . Theory , Methodology and Applications . Kluwer Academic Publishers , Clark P . and Niblett , T . 1989 . ‘FL nhvi : 3 L: algorithms Machine Learning , 3 : 261 285 . Doyle , J . R . and Green R . H . 1991 . Comparing Products Using Data Envelopment Analysis . OMEGA . International Journal of Management Science , Vol . 19 , No . 6 : 631 638 . Emrouznejad , A . and Thanassoulis , E . 1996 . An Extensive Bibliography of Data Envelopment Analysis ( DEA ) .
1996 . Data Envelopment Analysis :
I Ilt :
LlYL ,llLu.lC ” “ ‘l
42
KDD 97 l 30 , AAAUMIT Press .
Volume I : Working Papers , Volume II : Journal Papers . Business School , University of Warwick , England . Fare , R . and Grosskopf , S . 1996 . Zntertemporal Production Frontiers with Dynamic DEA . Kluwer Academic Publishers . Fayyad , U . M . , Piatetsky Shapiro , G . and Smyth , P . 1996 . From data mining to knowledge discovery : An overview . In Fayyad , U . M . , Piatetsky Shapiro , G . , Smyth , P . and Uthurusamy , R Advances in Knowledge Discovery and Data Mining . Gordon , A . D . 1996 . Cluster Validation . Paper presented at IFCS 96 Conference , Kobe , March 1996 . Hausdorf , C . and Mtiller , M . 1996 . A Theory of Interestingness for Knowledge Discovery in Databases Exemplified in Medicine . In Lavrac , N . , Keravnou , E . and Zupan , B . eds Proceedings of the First International Workshop on Intelligent Data Analysis in Medicine and Pharmacology . 3 l 36 , Budapest . Hsu , C . N . and Knoblock , C . A . 1995 . Estimating the Robustness of Discovered Knowledge . In Fayyad , U . M . and Uthurusamy , R Proceedings of the First International Conference on Knowledge Discovery & Data Mining . 156 161 , AAAI Press . Jammernegg W . , Luptacik M . , Nakhaeizadeh G . and Schnabl A . 1997 . 1st ein fairer Vergleich von Data Mining Algorithmen moglich ? Forthcoming . Klinkenberg , R . and Clair , D . S . 1996 . Rule Set Quality Measures for Inductive Learning Algorithms . In Dagli , C . H . , Akay , M . , Chen , C . L . , Fernandez , B . R . and Ghosh , J Proceedings of in Engineering ( ANNIE 96 ) Conference . 161 168 , ASME Press , New York . Knoll , U . , Nakhaeizadeh , G . and Tausend , B . 1993 . Cost sensitive pruning of decision trees . In Proceedings of the Eight European Conference on Machine Learning ECML94 . 383 386 , Berlin . Michie , D . , Spiegelhalter , D . J . and Taylor , C . C . eds . 1994 . Machine Learning , Neural and Statistical Classification . Ellis Horwood , Chichester . Paradi , J . C . , Reese , D . N . and ‘Rosen , D . 1995 . Applications of DEA to Measure the Efficiency of Software Production at two Large Canadian Banks . The Annals of Operations Research . Schnabl , A . 1996 . Nichtparametrische Effizienzanalyse Data und Envelopment Analysis . Master Thesis , Technical University Vienna . Silberschatz , A . and Tuzhilin , A . 1995 . On Subjective Measures of Interestingness in Knowledge Discovery . In Fayyad , U . M . and Uthurusamy , R Proceedings of the First International Conference on Knowledge Discovery & Data Mining . 275 281 , AAAI Press . Turney , P . 1995 . Technical Note : Bias and Quantification of Stability . Machine Learning , 23 33 . the Artificial Neural Networks
Dynamische technischer
Forts&&t : the
