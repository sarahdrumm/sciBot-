From : KDD 97 Proceedings . Copyright © 1997 , AAAI ( wwwaaaiorg ) All rights reserved .
Optimal multiple intervals discret hat ion of continuous attributes for supervised learning
DA Zighed
R . Rakotomalala
F . Feschet
ERIC Laboratory
University of Lyon 2
5 , av Pierre Mend&s France
69676 BRON CEDEX FRANCE {zighed,rakotoma,ffeschet)@univ lyon2.fr e mail :
Abstract this paper , we propose an extension of Fischer ’s In algorithm to compute the optimal discretization of a continuous variable in the context of supervised learning . Our algorithm is extremely performant since its only depends on the number of runs and not directly on the number of points of the sample data set . We propose an empirical comparison between the optimal algorithm and two hill climbing heuristics .
Introduction from examples , such as the well known Rule induction induction trees ( Breiman et al . 1984 ) ) usually use categorial variables . Hence , to manipulate continuous variables , it is necessary to transform them to be compatible with the learning strategy . The processus of splitting the continuous domain of an attribute into a set of disjoints intervals is called discretization . In this paper , we focus on supervised learning where we take into account a class Y( . ) to predict .
Lechevallier ( Lechevallier 1990 ) has described an approach , based on Fischer ’s works ( Fischer 1958 ) , to determine the optimal partition in K intervals among all the ordered partitions in O(n2 ) . Thus , we can consider the discretization problem to be algorithmically solved since we can fastly compute the optimal discretization with Lechevallier ’s algorithm . However , the found solution is generaly specific to a finite learning set , so that another sample set on the same problem can lead to a different optimal discretization . Hence , in the context of supervised learning , the quality of the discretization must be measured by the quality of the prediction it implies on a test set . In this case , we wonder whether or not the optimal discretization performs really better than hill climbing heuristics such as Fusinter ( Zighed , Rakotomalala & Rabaseda 1996 ) or MDLPC ( Fayyad & Irani 1993 ) whose complexities are lower .
‘Copyright @ 1997 , American Association for Artificial
Intelligence ( wwwaaaiorg ) All rights reserved .
In the next section , we present a formulation of the problem of discretization , then we describe an extension of Lechevallier ’s algorithm to find the optimal discretisation and we insist on the use of runs instead of the points of the sample data set . After , we introduce two hill climbing strategies . Finaly , we present experiments and empirical studies of the performances of the various presented stategies .
Discretization
Formulation Let be Dx the domain of definition of a continuous attribute X( ) The discretization of X( . ) consist in splitting DX into k intervals 1j , j = 1 , . . . , k with k 2 1 . We note Ij = [ dj 1 , dj [ with dj ’s be the discretization points . points
Border Let be X(Q ) = ( 21 , . . . , xj , xj+l , . . . , za} the ordered set of the values of X( . ) over the set R , 21 < . . . < xn . the set of the examples whose Let us denote by fij image by X( . ) is situated between xj and ~j+l , such that dj = p X ZT~ + ( 1 p ) X xj+l ( 0 5 p 5 1 ) . dj is called a border point if and only if the classes of the elements of Rj are not all the same than those of the elements of !2j+l . is sj . Assume dj
U is the set of border points and we have u = Fayyad and Irani ( Fayyad & Irani 1993 ) Card(U ) . have proved that the discretization points dj can only be border points . Thus U is the set of possible points for discretization . Finding the optimal discretization is then equivalent to extract the subset U* ( U ” C U ) which induces an optimal split for the used criterion .
Runs A run is a set of points placed between two border points . A run is represented by a vector which describe , for each class , its number of observations ( the number of points of the run which belong to this class ) . We
Zighed
295 can then represent the sample set by an array T as the IT II L . Iollowlllg :
It is clear that the number of runs is equal to the number of border points plus one : T = u t 1 . d4 ds %
4 dl
4 x
000 x v
0 X
0 X
0 x x xx
0
o !z e s a )
R ,
R ,
RX
R4
Rs
%
R7
Figure 1 : The runs Ri and the border points dj for a sample set composed of two classes “ x ” and “ 0 ” . The sequences can be homogeneous : ( RI , R2 , RG , R7 ) or not ( izs , & , iis j . in the first case , all points have the same value for Y( . ) and in the last case , all points have the same value for X ( )
Optimal discretization the quaiity of a partition
Measuring The problem consists in finding the split from which we can predict the class Y( . ) at best . Every subset Ui C U of border points leads to a partition perfectly described by an array Ti whose structure is similar to T .
We necessarily have to compare partitions containing different numbers of intervals . The quality measure , we shall use , must take into account the increase in complexity induces by an excessive partitionning . There are several ways to introduce complexity bias to avoid excessive partitionning . We can quote measures based on Minimum description length principle ( Fayyad & Irani 1993 ) ) measures of the type X2 ( Tschuprow , Cramer ) , or measures using informational gain taking into account sample size ( Zighed , Rakotomaiaia & Rabaseda i996j . These measures can possibly be guided by resubstitution error rate ( Liu & Setiono 1995 ) .
From now on , we use Zighed ’s measure denoted ‘p( ) We have carried out several experiments which conclude that these different measures have the same be1 . c f ndv1ouL‘ which one verifies : cp(T* ) = mini[(p(Ti) ] .
I A.^ PIrl ^_^__ IS b" 111111 iirlll"IIg
1 , &I . , an UK
__ ." aLla,ycl
A. wu1'
, g"al rr* 1 for finding optimal
An algorithm discretization Finding optimal discretization in k classes with a set of n points could be done by testing all the possible partitions . In this case , the algorithm has a very high mwnnlPvit,r fu,J+11 t,"rrr~L"nr"J " \' Y 1990 ) has proposed’an algorithm based on Fischer ’s
T.prhmmllim Y ” v~~~w& “ ~
IT,nrhmmllier \ ** ,
R11t . YY ” ,
I
296
KDD 97 works ( Fischer 1958 ) for finding optimal discretization , mL,,WU"bt :
. . l^ r c"lup'eTuby
A( 21 "(7‘
15
) . This algorithm use two fundamental hypothesis which are : l ordering elements
: over the set property X(i 2 ) = {Xl ) ) x,} a partition in k intervals is oreiPmen.~s T’i SlTIrl V : && fyc 2 7 of X ( 0 ) which belong to the same interval 4 , every element situated between xi and zj belongs to the same interval ; if 5c d QI?iV J
PVPrV ‘ J fnr
if l Additivity of the quality measure : 12 , if a partition in k intervals is optimal , then in is an optimal partition
*** , 4 ) ( 12 , . . . . Ik )
( +%,xi ) , the partition k 1 intervals of the set ( IC;+~ , . . . . zcn ) .
The first property is not restrictive since X ( 0 ) C lR the element are necessarily ordered . But , the second property requires the additivity of the choozen measure . It has been proved by Lechevallier ( Lechevallier 1990 ) for the measure based on a x2 , and by Zighed & al . ( Zighed , Rakotomalala & Rabaseda 1996 ) for the previous cp( . ) measure . algorithm of Fischer ’s
An extension Fischer ’s algorithm is a dynamic programming procedure . The main idea is to find some relations between the optimal partition in k intervals of the inital data set and the optimal partitions in k 1 intervals of subsets of the data set . It uses the order to restrict the number of possible partitions . The additivity of the cp measure is then used to obtain a recurrent equation between optimal partitions . We present here an extention of Fischer ’s and Lechevallier ’s algorithms by the partitionning of a set of runs instead of considering is a consequence of the work of ( Fayyad points . This & Irani 1993 ) who have proved than a run can never be split in an optimal discretization .
Let us consider a set of runs ( Ri , 1 5 i 2 r ) . We for the search an ordered partition which is optimal ‘p measure . We denote by Pi this partition with k the number of intervals and 1 the first run taken into account . We then have : is cp the
Since additive , cp by 0 and oc ” g , Fr{ R,P_:FTY jh = r for the sake of simplicity . This additivity of cp implies that
, Rj< >> where r;c of given = value ’ partition in Ic 1 intervals of the set is an optimal partition jl + 1 < i 5 T} . Hence , there are connecof runs {Ri , in ir , intervals and tions between the optimal partition those in Ic 1 intervals . The problem is then to find the first cutting point jr . This point is one of the integers interval [ l , r k + 11 . The optimal partition Pi can then be obtained through a minimization process : ‘P(P,~ ) = I<jl~~k+l{~(IR1,,R31 ) ) +P ( ?:I :‘)}
The previous relation introduce a relation of recurrence between Pi and ‘PC ‘ . If we can compute the then by using the minimizajl ” various partitions Pk r tion procedure we deduce Pk . To compute the partitions ‘$y , it is possible to use again the relation of recurrence .
,
We obtain the following algorithm :
Computing the partitions Pi for 1 5 2 < T For all p , 2 5 p 5 k , compute the partitions Pz for each Q of the interval [ l , T p + l ] l Compute p ( Piml ) the summations v({R , , for q 5 0 I T
. . . ,R,} )
+ l cp ( ‘Pj ) is the minimum value of the previous ones l At step Ic of this algorithm , the optimal partition
Pi is determined when q = 1 .
The partition P* which is optimal among all the previous optimal partition , is given by : cp ( P* ) = minjr,r ‘p ( Pj’ )
Two hill climbing heuristics
( TD )
( BU ) and Top Down
Bottom Up strategies Beside Fischer ’s strategy whose complexity is 0(r2 ) , it is possible to use less complexity [ O(T ) ] methods but which are not optimal . They are used in most of the contextual discretization algorithms published in the litterature . These methods are based upon two hillclimbing heuristics : l l the first one , called “ top down ” , uses the “ divide and conquer ” principle . It recursively computes a binary partitionning of each previously computed sets until a stopping rule is verified ( Catlett 1991 ) . The set U* is iteratively built by adding discretization points . the second one , called “ bottom up ” uses an opposite principle . Its starts from an initial partition defined by U , the set of border points . Then , it iteratively tries to aggregate adjacents intervals until the partition optimizes the measure ( Zighed , Rakotomalala
& Rabaseda 1996 ) or until no aggregation is reliable ( Kerber 1991 ) . In the last case , the set U* is built by deleting points of the partition .
Theses two strategies run very fast but they have the disadvantage of being irrevocable . Each added point in U* with the “ top down ” strategy cannot be deleted ; each deleted point in U* cannot be reintroduced in the last strategy .
A previous studies
( Zighed , Rakotomalala & Rabaseda 1996 ) have showed that MDLPC ( Fayyad & Irani 1993 ) and Fusinter are very close , so we only use this last algorithm here . algorithm discretization
Is an optimal useful1 or useless ? We are now confronted to a simple choice : on one side we have a very fast algorithm , on the other side an algorithm , with a higher cost , but which provides a global optimization . Is it interesting to use one of these instead of the other one ?
In the context of supervised learning , one of our main goals is to build a model having the minimum error rate in prediction , which could be estimated by applying the model on a sample set not used for learning , called test set . It is generally supposed that a model which optimizes a criterion having good properties , especially the resistance to overfitting on noisy data , will perform better in prediction . Hence , the problem of learning is often reduced to an optimization problem . In this paper , we verify this hypothesis by confronting the hill climbing heuristics with our improvement of Fischer ’s algorithm .
Experiments method
Comparison We compare the Fusinter method with Fischer ’s strategy using the Breiman ’s waves dataset ( Breiman et aZ . 1984 ) . To do so , we have generated 11 learning samples of 300 points each and a test sample of 5000 points . For any w taken from the learning sample and the test sample , we dispose of a 21 components vectors noted ( Xr ( w ) , . . . , Xj(w ) , . . . ,X21(w ) ) and of a label Y(w ) . For each attribute Xj , we determine the best discretization obtained on the learning sample and we consider it like a decision tree with one depth level . Then , we measure the quality of the discretization on the test sample by the accuracy rate .
The two methods ( Fusinter , Fischer ) are compared using a t test for dependent samples . Critical value of the test is to.975 = 1.96 for a 5 % significance level , and we found t* = 1735 So , we conclude that Fischer ’s strategy is not significantly better than Fusinter .
Zighed
297
Table 1 : Comparison Fusinter vs Fischer and discussion
Results Three main results draw our attention : in our experiments , Fusinter almost always found the right number of intervals ; but nearly never find the optimal partition over 231 trials ) ;
( 29 times this disadvantage does not significantly modify its performance towards those of Fischer ’s strategy if we consider the error rate in prediction . Indeed , over the 231 files , Fusinter is better than Fischer 73 times and has similar performances 47 times . Using the test procedure described above , the difference is not significant for a 5 % risk ( table 1 ) .
The doubts of several authors ( Breiman et al . 1984 ) on the usefullness of optimization in induction process are confirmed in this paper . Our goal is to obtain the lowest error rate in prediction with the simplest model following Occam ’s razor principle . Then , it is probably not very interesting to use complex learning strategies . We can get better results ( in our experiments , for a 10 % risk , we can conclude to the superiority of Fischer ’s strategy ) but they are not significant . Hence , the choice of a method is more dependent on the faculty of understanding the model , on its simplicity or its running time .
Moreover , we wonder whether an optimization procedure , which only uses the contingency tables information , is reliable . In fact , in this case , we neglect the distribution of the samples . Let us consider a sample belonging to a class Yr which is surrounded by elements of a class Ya , then we can suppose that this point has the wrong label or that this point is aberrant . There c._^ ^ ^ 1 lL: dlx several S ” lUbl ” llD b ” c111s pJ’ “ u’~M . Lb 13 y ” D ” 1u’cT IJ ” +r\ mix supervised and unsupervised methods ( Dougherty , Kohavi & Sahmi 1995 ) by introducing , for instance , a measure which takes into account the relative distribution of the intervals in lR , by using the inertia ( de Merckt 1993 ) or the variance ( Lechevallier 1990 ) .
.A^ Ll.2 , ,,I, , ,
finn:l1
T4 .
:n
Conclusion
In this paper , we have established that the use of optimal discretization using a partition quality measure has no significant improvement on the error rate in prediction beside a simple hill climbing heuristic .
298
KDD 97
Nevertheless , we have to qualify this conclusion . Some works have proved that the loose of informations introduced by discretization can hide the relations between the variables ( Celeux & Robert 1993 ) . Thus , it would interesting to complete this study by trying t0 chacterize the problems and the data ( distribution , noise level ) for which it is necessary to use optimal discretization . to study the behaviour of different induction processes in relation with the various discretization methods .
It would be also interesting
References
Breiman , L . ; Friedman , J . ; Olshen , R . ; and Stone , C . 1984 . Classification and Regression Trees . California : Wadsworth International . Catlett , J . 1991 . On changing continuous attributes into ordered discrete attributes . Artificial IntelZigence Journal 164 178 . Celeux , G . , and Robert , G . 1993 . Une histoire de discretisation ( avec commentaires ) . La Revue du Modulad ( 11):7 43 . de Merck& T . V . 1993 . Decision trees in numerical attributes spaces . In Proceedings of the 13th IJCAI . Dougherty , J . ; Kohavi , R . ; and Sahmi , M . 1995 . Supervised and unsupervised discretization of continuIn Preiditis , A . , and Russel , S . , eds . , ous features . Proceedings of the I ” zueith Internationai i?onference in Machine Learning . Fayyad , U . , and Irani , K . 1993 . Multi interval cretization of continuous valued attributes fication learning . 1022 1027 . Fischer , W . 1958 . On grouping for maximum homogeneity . Journal of American Statistical Association ( 53):789 798 . Kerber , R . 1991 . Chimerge discretization of numeric attributes . Conference on Artificial Lechevallier , Y . 1990 . Recherche d’une partition optimale so us contrainte d’orurr ; Ir ” lal . ICbIIIIILLu rr;yvr b ) INRIA . Liu , H . , and Setiono , R . 1995 . Discretization of ordinal attributes and feature selection . Technical Report TRB4/95 , Department of Sys . and Comp . Sci , National University of Singapore . Zighed , D . ; Rakotomalala , R . ; and Rabaseda , S . 1996 . A discretization method of continuous attributes in induction graphs . In Proceedings of the 30th European Meetings on Cybernetic and Systems Research , 9971002 .
In Proceedings of the 13th IJCAI ,
In Proceedings of the 10th
Intelligence , 123 128 . disfor classi
International
9 h,l.n:nn1
lw
+,A‘.1
A,&
