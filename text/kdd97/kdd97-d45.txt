From : KDD 97 Proceedings . Copyright © 1997 , AAAI ( wwwaaaiorg ) All rights reserved .
Zeta : A Global Method for Discretization of Cotitinuous Variables
K . M . Ho and P . D . Scott
Department of Computer Science ,
University of Essex , Colchester , CO4 3SQ , UK hokokx@essexacuk and scolp@essexacuk
Abstract
This paper introduces a new technique for discretization of continuous variables based on zeru , a measure of strength of association between nominal variables developed for this purpose . Zeta is defined as the maximal accuracy achievable if each value of an independent variable must predict a different value of a dependent variable . We describe both how a continuous variable may be dichotomised by searching for a maximum value of zeta , and how a heuristic extension of tbis method can partition a continuous variable into more than two categories . Experimental comparisons with other published methods , show that zeta discretization runs considerably faster than other techniques without any loss of accuracy .
Introduction
Many machine learning techniques can only be applied to data sets composed entireiy of nominal variabies but a very large proportion of real data sets include continuous variables . One solution to this problem is to partition numeric variables into a number of sub ranges and treat each such sub range as a category . This process of partitioning continuous variables into categories is usually termed discrerization . In this paper we describe a new technique for discretization of continuous variables based on zeta , a measure of strength of association that we have developed for this purpose .
Qnjy Qp]i&ie
. , ~ ___ c ,
19Rhi 5lTP . j&rcflfly
Discretization for Decision Tree Construction Procedures for constructing decision trees using sets of pre classified examples ( Breiman , Friedman , Olshen & StQne 15)84! fhiinlnn to data sets composed entirely of nominal variables . If they are to be applied to continuous variables some means must be found of partitioning the range of values taken by a continuous variable into sub ranges which can be treated as discrete categories . Such a partitioning process is variety of frequently discretization methods have been developed in recent years . Dougherty , Kohavi and Sahami ( 1995 ) have provided a systematic review in which discretization termed discretizatiun . A
Copyright C 1997 , American Association for Artificial Intelligence ( wwwaaaiorg ) All rights reserved located along two dimensions : techniques are unsupervised vs . supervised , and global vs . local . Unsupervised discretization procedures partition a variable using only information about the distribution of values of that variable : in contrast , supervised procedures also use the classification label of each example . Typical unsupervised techniques include equal interval width and equal frequency width methods . Supervised techniques normally attempt to rnaximise some measure of the relationship between the partitioned variable and the classification label . Entropy or information gain is often used to measure the strength of the relationship ( Quinlan 1986 , 1993 ; Catlett 1991 ; Fayyad & Irani 1992 , 1993 ) . Both ChiMerge ( Kerber 1992 ) and StatDisc ( Richeldi & Rossotto 1995 ) employ procedures similar to agglomerative hierarchical clustering techniques . Holte ’s ( 1993 ) 1R attempts to form partitions such that each group contains a large majority of a single classification . Global discretization procedures are applied once to the entire data set before the process of building the decision tree begins ; local discretization procedures are applied to the subsets of examples associated with the nodes of the tree during tree construction . The majority of systems 1nrino UUlllb UL . “ q.wS 1 LVYV nmthnrlc discretizations . Examples of supervised global methods include D 2 ( Catlett 1991 ) , ChiMerge ( Kerber 1992 ) , Holte ’s ( 1993 ) 1R method , and StatDisc ( Richeldi & Rossotto 1995 ) . C4.5 ( Quinlan 1993,1996 ) and Fayyad and Irani ’s ( 1993 ) entropy minimisation method use a supervised technique to perform local discretization . The majority of supervised techniques could be used for either local or global discretization : for example , Fayyad and Tm:~ ” mad.&4 global discretizations ( Ting 1994 ) . Dougherty et al . ( 1995 ) report a comparative study of two unsupervised global methods ( equal width interval procedures ) , two supervised global methods ( 1RD ( Holte 1993 ) and Fayyad & Irani ’s ( 1993 ) entropy minimisation ) , and C4.5 , a supervised local method . They found only small differences between the classification accuracies achieved by resulting decision trees . None produced the highest accuracy for all data sets but our own replication of these experiments ( see below ) shows a clear speed/accuracy
IlnallnPnricl=rl trade off . lll ” Ul ” Y ” n,nann&ll , JubLGJJIuILJ
~nl,wLwl GUqJ’ “ yal
.A C ”
CAl ” llU llillll a
III~UI ” U l ” lldJ haa “ GGll cqr out olnhd pvv&u
Ho
191
Data Set allbp ann thyroid
. .
Entropy Continuous 9722+ 016 9745+ 010 99.61 I 0.1 1 9938+ 013 m m .^^
IRD
9605+ 032 9764+ 004 ^_^^ >^
Bin log I 9639+ 032 9406+ 019 ^>^^ ^A n Bins 9632vO13 9272wO24 ^*^^ ^
Zeta
9663+ 023 9838+ 016 ^^^^ _^^ c4.5
Average
11 l54.04
I
W./Y
I t5l.lti
I
W.UU
I fY.44
I
UY.YU
I
Table 1 : Classification accuracies and standard deviations using C4.5 ( Quinlan 1996 ) with different discretization methods . Continuous : C4.5 on undiscretized data . Entropy : Global variant of Fayyad & Irani ’s ( 1993 ) method . 1RD : Holte ’s ( 1993 ) IR discretizer . Bin log 1 and n Bins : Equal width binning . Zeta : Method proposed in this paper . ( cf Dougherty et al . 1995 )
Zeta : A New Measure of Association
Our initial attempts to develop a fast and accurate discretization technique based upon lambda , a widely used measure of strength of association between nominal variables ( Healey 1990 ) that measures the proportionate reduction in prediction error that would be obtained by using one variable to predict the other , using a modal value prediction strategy in all cases . Unfortunately lambda is an ineffective measure in those situations were the dependency between two variables is not large enough to produce different modal predictions since in such cases its value is zero .
A closely related measure , which we term zeta , has been developed that overcomes this limitation because it is not based on a modal value prediction strategy : the assumption made in determining zeta is that each value of the independent variable will be used to predict a difSerent value of the dependent variable .
Zeta is most readily understood by first considering the simplest case : using a dichotomous variable A to predict values of another dichotomous variable B . Suppose we have a sample of N items whose value distribution is given in the following 2 by 2 table :
AI nil n21
AZ ni2 n22
.
4 , B2 where
192
KDD 97
N = &Q i=l j=l
If each value of A is used to predict a different value of B then there are only two possibilities : either A+B , and A2+B2 ,or A,+B2 and A+B , . If the former is used then the number of correct prediction would be nll + nz2 ; if the latter then n12 + n2 ] would be correct . Zeta is defined to be the percentage accuracy that would be achieved if the pairings that lead to greater accuracy were used for prediction . Hence it is defined as follows : max(n , , + n , , ,n12 + nzl ) X 100 % z=
N
This definition may be generalised to the case of one kvalued variable A being used to predict the values of another variable B that has at least k values thus : k c nf G)i z = i=’ N x 100 % where f(i ) should be understood as follows . In order to make predictions each of the k values of A must be paired with a non empty set of values of B : these k sets must together form a partition of the set of possible values for B . If B has k distinct values there will be k! ways in which such sets of pairings could be made . One such set of pairing will give the greatest prediction accuracy ; call this the best pairing assignment . Then Br(i ) is the value of B that is paired with Ai in the best pairing assignment .
4392
0o:OO
81.00
82.00
83.00
Predictive Accuracy
Figure 2 : Total execution time for all data sets plotted as a function of average final classification accuracy for different diicretization methods ( see caption to Table 1 ) .
Discretization Using Zeta
We now proceed to describe how this measure may be used to partition a continuous variable . The underlying principle is very simple . In theory , given a k valued classification variable C , a continuous variable X could be partitioned into k sub ranges by calculating zeta for each of the possible assignments of the k l cut points and selecting that combination of cut points that gives the largest value of zeta . In general such a method will not be practicable because the number of combinations of cut points is extremely large .
Dichotomising Using Zeta However it is practicable for the special case of k = 2 since there will only be one cut point . Fortunately this is an extremely common special case in real world classification problems , many of which reduce to a choice between positive and negative diagnoses . If there are N examples in the data set there are at most N 1 candidate cut points . Zeta can be calculated for every one of these and the point yielding the maximum value selected .
In practice it is not necessary to consider every possible cut point . Fayyad and Irani ( 1992 ) have shown that optimal cut points for entropy minimisation must lie between examples of different classes . A similar result can be proved for zeta maximisation . Hence it is only necessary to calculate zeta at points corresponding to transitions between classes , the computational cost . reducing thus
More Than Two Classes The dichotomising procedure forms the basis of a heuristic method of discretizing a variable into k categories . This is turn .
It a stepwise hill climbing procedure that locates and fixes each cut point in therefore finds a good combination of cut points but offers no guarantee that it is the best . As noted above , examining all possible combinations is likely to be too time consuming .
The procedure for discretizing a variable A into k classes , given a classification variable B which takes k distinct values is as follows . First find the best dichotomy of A using the procedure described above . If k is greater than 2 then at least one of the resulting sub ranges of A will be associated with 2 or more values of B : use the dichotomising procedure again on such a sub range to place the second cut point . Proceed in a similar fashion until k 1 cutpoints have been placed and each sub range is associated with a different value of B .
This is a heuristic method : once a cut point is placed it is not moved so not all cut point combinations are considered . Nevertheless , as some of the results discussed later show , the cut points chosen lead to high predictive accuracy and hence the use of the heuristic is justified .
Experimental Results
A series of experiments were carried out using both real and artificial data sets to establish whether the zeta technique partitioned individual variable in a useful fashion . ( see Ho and Scott 1997 ) . These established that zeta discretization is an effective procedure for locating good cut points within the ranges of continuous variables . The next set of experiments was designed to evaluate the performance of zeta in the role for which it was developed : the construction of decision trees . Our experimental procedure was closely modelled on that employed by Dougherty et al . ( 1995 ) in their comparative study of five discretization techniques .
Ho
193
We compared the live methods considered by Dougherty et al . and zeta discretization . C4.5 ( Quinlan 1996 ) was used to construct all of the decision trees . In five of the six cases , the data was first processed by the global discretization procedure and then passed to C45 In the sixth case no prior discretization took place ; hence the local discretization procedures that form part of C4.5 were used .
The code for zeta discretization was written in C by one of the authors ( Ho ) ; the code for C4.5 , also written in C , was the version distributed to accompany Quinlan ( 1993 ) updated to Release 8 ( Quinlan 1996 ) ; all the remaining code was taken from the MLC++ machine learning library ( Kohavi , John , Long , Manley & Pfleger 1994 ) . The data sets used for these experiments were all obtained from the UC Irvine repository . Each set was tested five times with each discretization method .
The results are shown in Table 1 . As is to be expected the results for the first five columns are very similar to the results reported by Dougherty et al . ( 1995 ) . The zeta discretization method stands up to the comparison very well . The average accuracy over all the data sets was higher than all the other global methods and only slightly , but not significantly , less than that achieved by C4.5 using local discretization . Thus we can conciude that on average zeta discretization method achieves accuracies at least as good as the best global methods .
However , the zeta method is also fast . Figure 2 shows the total execution time required by each of the six methods to complete all the data sets listed in Table 1 , plotted as a function of final classification accuracy . It is clear that four of the six data points lie roughly in a straight line , indicating a time accuracy trade off . Two points lie well below this line : continuous ( ie C4.5’~ local method ) and zeta . These two methods not only achieve high accuracy but do so in appreciably less time .
Conclusion
These results show that zeta discretization is both an effective and a computationally efficient method of partitioning continuous variables for use in decision trees . Indeed of the methods considered in our comparative study it would appear to be the method of choice .
Acknowledgements
We are grateful to the Economic and Social Research Council ’s programme on the Analysis of Large and Complex Datasets for supporting part of the work reported in this paper under grant number H5i9255030 .
References
Breiman , L . , Friedman , J , H . , Olshen , R . A . , & Stone , C . J . 1984 . Classi$cation and Regression Trees . Wadsworth , Pacific Grove , CA
194
KDD 97
Catlett , J . 199 1 . On changing continuous attributes into ordered discrete attributes . In Machine Learning : EWSL91 , Proceedings European Working Session on Learning , Lecture Notes in Artificial Intelligence 482 . pp . 164 178 . Springer Verlag . Dougherty , J . , Kohavi , R . , & Sabami , M . 1995 . Supervised and Unsupervised Discretization of Continuous Features . In Proc . Twelfth Znternational Conference on Machine Learning . Morgan Kaufmann , Los Altos , CA . Fayyad , U . M . , & Irani , K . B . 1992 . On the handling of continuous valued attributes in decision tree generation . Machine Learning 8 pp . 87 102 . Fayyad , U . M . , & Irani , K . B . 1993 . Multi interval discretization of continuous valued attributes for classification learning . In Proc . 13th International Joirit Conference on Artificial Intelligence . pp 1022 1027 Morgan Kaufmann , Los Altos , CA . Healey , J . 1990 . Statistics : A Tool for Social Research . Wadsworth , Belmont , CA . Ho , K . M . and Scott , P . D . 1997 . Zeta : A Global Method for Discretization of Continuous Variables . Technical Report CSM 287 , Dept of Computer Science , University of Essex , Colchester , UK . Holte , R . C . 1993 . Very simple classification rules perform well on most commonly used datasets . In Machine Learning , 11 , pp . 63 91 . Kerber , R . 1992 . ChiMerge : Discretization of numeric attributes . In Proc . Tenth National Conference ofi Artificial ZnteZZigence , pp . 123 128 . MIT Press . Kohavi , R . , John , G . , Long , R . , Manley , D . & Pfleger , K . 1994 . MLC++ : A machine learning library in C++ . In Tools with Artificial Intelligence , IEEE Computer Society Press , pp . 740 743 . Quinlan , J . R . 1986 . Induction of Decision Trees Machine Learning 1 pp 81 106 . Quinlan , J . R . 1993 . Programs for Machine Learning . Morgan Kaufmann , Los Altos CA . Quinlan , J . R . 1996 . Improved use of continuous attributes in C45 Journal of Artificial Intelligence Research 4 pp . 77 90 . Richeldi , M . & Rossotto , M . 1995 . Class Driven statistical discretization of continuous attributes . In Machine Learning : ECML 95 Proceedings European Conference on Machine Learning , Lecture Notes in Artificial Intelligence 914 . pp 335 338 . Springer Verlag . Ting , K . M , 1994 . Discretization of continuous valued Rttrihuteq Renort = c c 491 , University of Sydney . Van de Merckt , T . 1993 . Decision trees in numerical attribute spaces . In Proc . 13th International Joint Conference on Artificial Intelligence . pp 1016 1021 Morgan Kaufmann , Los Altos , CA .
El . __ _ _ . instance had
L and learnim .
Technical
