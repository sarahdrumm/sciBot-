Cross modality Consistent Regression for Joint
Visual Textual Sentiment Analysis of Social Multimedia
Quanzeng You and Jiebo Luo
University of Rochester Rochester , NY 14623
{qyou , jluo}@csrochesteredu
Hailin Jin
Adobe Research
San Jose , CA 95110 hljin@adobe.com
Jianchao Yang
Snapchat Inc
Venice , CA 90291 jianchaoyang@snapchatcom
ABSTRACT Sentiment analysis of online user generated content is important for many social media analytics tasks . Researchers have largely relied on textual sentiment analysis to develop systems to predict political elections , measure economic indicators , and so on . Recently , social media users are increasingly using additional images and videos to express their opinions and share their experiences . Sentiment analysis of such large scale textual and visual content can help better extract user sentiments toward events or topics . Motivated by the needs to leverage large scale social multimedia content for sentiment analysis , we propose a cross modality consistent regression ( CCR ) model , which is able to utilize both the state of the art visual and textual sentiment analysis techniques . We first fine tune a convolutional neural network ( CNN ) for image sentiment analysis and train a paragraph vector model for textual sentiment analysis . On top of them , we train our multi modality regression model . We use sentimental queries to obtain half a million training samples from Getty Images . We have conducted extensive experiments on both machine weakly labeled and manually labeled image tweets . The results show that the proposed model can achieve better performance than the state of theart textual and visual sentiment analysis algorithms alone .
Categories and Subject Descriptors I.2 [ Artificial Intelligence ] : Vision and Scene Understanding ; I54 [ Pattern Recognition ] : Applications—Computer vision
Keywords sentiment analysis , cross modality regression , multimodality analysis
1 .
INTRODUCTION
The increasing popularity of social networks attracts more and more people to share their experiences and to express
Hello there sweetie . : )
PD Achilles meets a new friend . Special post for one of our followers who I met last night and had a good chat to
If anyone woke up in edinburgh this morning to discover their car missing i think i know where it is
( a )
( b )
( c )
Figure 1 : Examples of image tweets from Twitter . their opinions on virtually all events and subjects in online social network platforms . Each day , billions of messages and posts are generated . In this study , we focus on deriving people ’s opinions or sentiments towards topics and events happening in real world . In other words , we are interested in automatical detection of sentiment from online user generated content .
Figure 1 shows several example image tweets from Twitter . Image tweets refer to those tweets that contain images . If we take a look at these three example image tweets , we can observe that in example ( a ) , both image and the text indicate that this tweet carries a positive sentiment ; in ( b ) while it is difficult to tell the sentiment from the image in the middle image tweet , however , we can tell that this tweet expresses positive sentiment from the text ; in ( c ) on the contrary , it is hard to tell the sentiment from the text , however the worn out car in the image suggest an overall negative sentiment . These examples explain the motivation for our work . We would like to learn people ’s overall sentiment over the same object from different modalities of the object provided by the user . In particular , we focus on inferring people ’s sentiment according to the available images and the short and informal text .
Many researchers have contributed to sentiment analysis . For instance , there are related works on detecting users’ sentiment and applying sentiment analysis to predict box office revenues for movies [ 1 ] , political elections [ 23 , 29 ] and economic indicators [ 3 , 35 ] . In particular , recently published works started to focus on analyzing sentiment of informally user generated content from online social networks . However , current techniques are mostly based on the analysis of textual content to detect sentiment . On the other hand ,
13 visual content , including both images and videos , are becoming increasingly popular in all mainstream online social network platforms . For example , Twitter ’s support of image tweets and Vine as well as Facebook ’s Instagram are all designed to support people to share and post more visual content . More interestingly , statistics show that the usage of image in a tweet is able to increase the popularity of this tweet in terms of clicks , retweets , and favorites1 . This can encourage Twitter users to post more visual content . We cannot ignore the prevalently available visual content in analyzing online users’ sentiment .
To the best of our knowledge , little attention has been paid to the sentiment analysis of visual content as well as multi modality sentiment analysis . Only a few recent works attempted to predict visual sentiment using features from images [ 25 , 5 , 4 , 34 ] and videos [ 21 ] . Visual sentiment analysis is extremely challenging , as image sentiment involves a much higher level of abstraction and subjectivity in the human recognition process [ 16 ] , on top of a wide variety of visual recognition tasks including object , scene , action and event recognition . However , Convolutional Neural Networks [ 19 , 7 , 17 ] have been proved to be very powerful in solving computer vision related tasks . Due to the challenges of visual sentiment analysis , we propose a multi modality framework to analyze sentiment on top of state of the art techniques in both visual and textual analysis .
To that end , we address in this work two major challenges as follows : 1 ) we propose a novel multi modality regression model , which can integrate different modality features for sentiment analysis , and 2 ) we demonstrate the feasibility of using weakly labelled data for multi modal sentiment analysis and how to easily transfer models from one domain to another domain . The contributions of this paper include
• We employ the state of the art machine learning algorithms to a solve a challenging novel problem , multimodality sentiment analysis . In particular , we adopt Convolutional Neural Networks [ 17 , 32 ] to visual sentiment analysis and employ the state of the art distributed representation of documents [ 18 ] for textual sentiment analysis .
• We propose a novel multi modality regression model , CCR , which tries to impose consistent constraints across related but different modalities . The formulation of the model is simple yet generalizable and can be easily implemented . The analysis and experimental results on sentiment analysis validate the effectiveness of the proposed model .
• Our model can be trained on a large scale data set in a mini batch mode . In particular , we show that it is possible to employ weakly labeled data to learn models for highly abstract tasks , such as sentiment analysis and achieve satisfying performance .
• To evaluate our model against competing algorithms , we build a manually labeled sentiment data set using Amazon Mechanical Turk . This data set will be released to the research community to promote further investigations on both textual and visual sentiment .
1http://tinyurl.com/lb4xkak
2 . RELATED WORK
For sentiment analysis of online user generated textual content , dictionaries based approaches [ 29 , 2 , 8 , 13 ] have been widely used due to its efficiency and simplicity . Very recently , distributed representation of words started to attract research attention due to its ability in learning robust features for words [ 20 ] . Le and Mikolov [ 18 ] further proposed an approach to learn distributed representation for documents . They applied their document representations to sentiment analysis and achieve the best performance over existing competing algorithms .
There are also several recent works on visual sentiment analysis . The work in [ 25 ] is a machine learning algorithm to predict the sentiment of images using pixel level color histogram and SIFT bag of words visual features . Motivated by the fact that sentiment involves high level abstraction , which may be easier to explain by objects or attributes in images , both [ 4 ] and [ 34 ] proposed to employ visual entities or attributes as features for visual sentiment analysis . In [ 4 ] , 1200 adjective noun pairs ( ANP ) are extracted to crawl images from Flickr . The responses of the trained 1200 ANP classifiers can be considered as mid level features for visual sentiment analysis . The work in [ 34 ] employed a similar mechanism but using 102 scene attributes instead . More recently , You et al . [ 32 ] proposed a progressively trained Convolutional Neural Network for visual sentiment analysis . Compared with other approaches that employ low level or mid level features , CNN achieved the state of the art performance in predicting image sentiments . A bench marking analysis of CNN on emotion analysis is proposed in [ 33 ] .
There are only a few publications on analyzing sentiment using multi modalities , such as text and images . Both [ 30 ] and [ 6 ] employed both text and images for sentiment analysis , where late fusion is employed to combine the prediction results of using n gram textual features and mid level visual features [ 4 ] . In addition , researchers have investigated cross modal issues in other multimedia retrieval related tasks . Rasiwasia et al . [ 24 ] employed canonical correlation analysis ( CCA ) to learn the correlations between visual features and textual features for image retrieval . Besides that , Feng et al . [ 10 ] further developed so called correspondence autoencoder for cross modal retrieval , where a code layer is shared between the visual and textual autoencoder for unsupervised learning of parameters . Ngiam et al . [ 22 ] also proposed a multimodal unsupervised deep learning model , which achieved the best published results in visual speech classification . Meanwhile , Nitish and Ruslan [ 28 ] employed multimodal deep Boltzmann machine to learn joint representation of images and text by the sharing of high level abstract representation . On the other hand , learning semantic mappings between visual and textual feature spaces has become popular due to the success of deep learning . Socher et al . [ 26 ] learned the semantic mappings in order to classify unseen visual classes . Frome et al . [ 11 ] employed hinge loss instead of using squared loss in their objective function to learn the semantic mapping between words and deep visual features for image annotation . Socher et al . [ 27 ] tried to even learn the transformation between sentence descriptions and images by margin based loss function on Recursive Neural Network and Convolutional Neural Network . More recently , Gong et al . [ 12 ] employed largely weakly annotated images to help learn the visual and textual embedding in their proposed stacked auxiliary embedding .
14 Inspired by these works on learning joint visual and textual models , we also rely on the recently successful deep learning techniques to extract features from images and text . However , different from the previously mentioned works , all of which are intended for unsupervised learning of shared feature embedding space between images and short text for image annotation or retrieval , our work intends to use supervised learning to enforce the consistency between the prediction labels of different features for sentiment analysis .
3 . CROSS MODALITY CONSISTENT RE
GRESSION ( CCR )
In this section , we describe the details of our proposed model . Our main motivation is that different modalities should be consistent in terms of depicting the same subject . In sentiment analysis , given multiple modalities , we expect the utilization of features extracted from different modalities , such as images and text , to achieve more accurate sentiment analysis results . 3.1 Cross modality consistent regression
Our model accepts input from different modalities of the same subject . The penalties between the predicted label distributions of different modality features need to be taken into consideration . To measure the penalty between any two different predicted label distributions , we employ KL divergence . In particular , let p and q denote two probability distributions of the same length . We define D(p . q ) as the sum of KL divergence between them .
D(p . q ) = DKL(p . q ) +D KL(q . p )
( 1 )
M 2
Assume we have a total of M different modalities and a total of N training instances . If we consider a pair wise penalty of the given M modalities , we have to solve a total of
. fi penalty terms between any pair of modalities , which may be too complicated for a relatively large M . Instead , we first concatenate all the features from the M modalities and add penalty terms between the M individual modality features and the concatenated features . In this way , only M penalty terms need to be added . Motivated by these observations , the objective function is formulated in Eqn . ( 2 ) . ( for m ∈ {1,··· , M} ) them th modality We denote by xm i features of the i th instance and by xc i the concatenated features from all the M modality features of the i th instance . Θ = {θc , θ1,··· , θM} are the parameters that needs to be learned . λ and γs are the hyper parameters to control the weights of different components in the proposed model . min
Θ,λ,γ1,··· ,γM
J(Θ ) =
M'
+
λ
2
θmT θm + m=1 m=1 i=1
D(yi . pθc ( xc N'
D(pθc ( xc i ) ) +
θcT θc
λ
2 i ) . pθm ( xm i ) )
( 2 )
Let pθ(xi ) be the prediction function for the label distribution of xi given the parameter vector θ . We use softmax function to evaluate the probability distribution , which is defined as
N' i=1
γm N
1 N
M' pθ(xi ) =
1ffK k=1 eθT k xi
[ e(θT
1 xi ) , e(θT
2 xi),··· , e(θT
K xi)]T ,
( 3 )
+ m=1 where K is the number of classes and θk is the parameter vector for the k th class , ie θk is a sub vector of θ and θk = θ[(k−1)|x|,k|x| ) ( we use |x| to denote the length of the feature vector ) .
The first component of the objective function is the consistency constraint between the ground truth label and the predicted label distribution using concatenated features . Next , regularization terms are added to the framework to prevent overfitting . The last component considers the predicted distribution consistency between each single modality features and the concatenated features . In this way , we hope that it is possible to propagate knowledge learned from different modalities to each other to improve the overall performance of the system . 3.2 Relation to softmax regression
The proposed model is closely related to softmax regression , where the objective is to minimize the loss function in Eqn . ( 4 ) . Similarly , θ = [ θ1 ; θ2 ; . . . ; θK ] is the parameter vector , δ(· ) is the indicator function and xi is the feature vector for the i th instance . ffK exp(θT k xi ) j=1 exp(θT j xi )
( 4 ) min
θ
δ(yi = k ) ln i=1
− 1 N ffN i=1 D(yi . pθc ( xc k=1
Indeed , softmax regression is a special case of the first component 1 i ) ) of our proposed model . EN qn . ( 5 ) shows more details of this loss term , where C1 and C2 are constants determined by yi . We use pθ(k|xi ) to represent the probability of assigning xi to class k , ie the k th element in Eqn . ( 3 ) . If we are given ahard label for each instance , ie yik only has one and only one non zero entry , then the term −yik ln pθc ( k|xi ) in Eqn . ( 5 ) is the same with the loss objective of softmax regression .
N'
K'
K'
D(yi . pθc ( xc i ) ) =
− yik ln pθc ( k|xc k=1 i ) ln pθc ( k|xc i ) pθc ( k|xc i ) +C 1pθc ( k|xc i ) +C 2
( 5 )
In addition , there are two other terms related to pθc ( xi)k in Eqn . ( 5 ) , which are related to cross entropy . This is the main reason to use KL divergence instead of softmax regression for the first component of our model , which is favorable for tasks with noisy and uncertain labels , such as sentiment analysis . 3.3 Parameter learning
Even though KL divergence is convex , the proposed objective function is not convex on its parameters Θ . In this section , we explain in detail how to learn the parameters in our model .
331 Gradient descent We resort to gradient descent to optimize the objective function J(Θ ) in Eqn . ( 2 ) . The gradient of the J(Θ ) with respect to θc is
∂J(Θ ) ∂θc =
M'
N' N' i=1
1 N
γm N i=1
∂D(yi . pθc ( xc i ) )
+ λθc
∂θc i ) . pθm ( xm i ) ) ∂θc
( 6 )
.
∂D(pθc ( xc
15 Convolutional Neural Network
Extract Visual features
Fine tune CNN
Cross modality Consistent Regression
( CCR )
Distributed Paragraph Vector Model
Sentiment prediction
Cross modality consistent regression
Nearby word
Classifier
Embedded Vector ( Average )
Images
Multi modality Features
D
W
W
• • ( cid:537 )
W
Figure 2 : The framework for multi modality sentiment analysis . Left : We fine tune a CNN visual sentiment analysis model , which is employed to extract visual features for testing images . Right : We train a distributed paragraph vector model on the related titles and descriptions of the images to learn textual features . Middle : The proposed cross modality consistent regression model is trained on the visual and textual features to learn the final sentiment classifier .
( 7 )
( 8 )
Algorithm 1 Cross modality consistent regression ( CCR ) Require : X 1 , X 2,··· , X M a total of M different modality features on a collection of objects X . Y = {y1 , y2 , . . . , yN} sentiment labels of X
1 : Randomly split the objects into mini batches 2 : Concatenate the M modality features to get X c 3 : Randomly initialize Θ = {θc , θ1 , . . . , θM} 4 : repeat 5 : 6 :
Randomly select one mini batch Xb Apply L BFGS to update θc on Xb with derivative in Eqn . ( 6 ) and objective function in Eqn . ( 2 ) for m from 1 to M do
Randomly select one mini batch Xb Apply L BFGS to update θm on Xb with derivative in Eqn . ( 9 ) and objective function in Eqn . ( 2 )
7 : 8 : 9 : end for
10 : 11 : until Convergent or reach the maximum numbers of i ) . pθm ( xm i ) ) ∂θm iterations
12 : return Θ
. ( 9 )
For last derivative term i ) . pθm ( xm
∂DKL(pθc ( xc
∂D(pθc ( xc i ).pθm ( xm ∂θc i ) ) i ) ) + DKL(pθm ( xm
1 − pθm ( k|xm i ) pθc ( k|xc i )
∂θc jl − ln pθm ( k|xm i ) pθc ( k|xc i )
, we have2 i ) . pθc ( xc i ) ) fi
∂pθc ( k|xc i )
.
∂θc jl
.
K'
= k=1 where
∂
.K exp(θc k
∂pθc ( k|xc i )
T xc i ) exp(θc T xc i ) k ∂θc jl i ) ) pθc ( j|xc = ( δ(k = j ) − pθc ( k|xc
∂θc jl
= k=1 i )xc il ∂D(yi.pθc ( xc i ) )
,
∂θc
The gradient of the objective function J(Θ ) with respect
We can also calculate the first derivative term which is similar to Eqn . ( 7 ) . to θm for m ∈ {1 , 2 , . . . , M} is N'
M'
∂D(pθc ( xc
∂J(Θ ) ∂θm = λθm +
γm N i ) . pθm ( xm m=1 i=1 Since D(pθc ( xc i ) ) is symmetric in terms of θm and θc , we can apply Eqn . ( 7 ) to calculate the derivatives of Eqn . ( 9 ) . 332 Learning algorithm and convergence analysis There are two groups of parameters in our model , namely θc and {θ1 , θ2,··· , θM} . In our implementation , we learn those two groups of parameters iteratively . Specifically , in each iteration , the learning algorithm will try to update the two groups of parameters sequentially . Since , we built a large data set for our experiments , we employ mini batch L BFGS to learn the parameters3 .
Algorithm 1 summarizes the main steps for Cross modality Consistent Regression ( CCR ) . The proposed objective function is differentiable , thus the function is smooth in terms of Θ . Meanwhile , it is easy to prove that J(Θ ) ≥ 0 . The objective function is lower bounded . During each iteration of the learning algorithm , we are trying to find a smaller 2Recall that θc the l th element of θc j . 3When the whole data set can fit into the machine ’s memory , it is also possible to employ full batch L BFGS . j is a sub vector of θc , we use θc jl to represent
J(Θ ) using L BFGS . It is possible that mini batch training may lead to oscillations of the objective function between different batches . However , overall we are still able to conclude that the iterative learning algorithm in Algorithm 1 converges when there is enough number of iterations .
We will discuss in the experimental section on how to select the hyper parameters of our model , ie λ and γs .
333 Prediction algorithm We can employ the learned model for prediction of testing instances given their M modality features . Recall that our training objective is to enforce the consistency of prediction results using different sets of modality features . Similarly , in the prediction stage , we also intend to obtain the same objective . Let p denote the desired label distribution of the testing instances , then we have
M'
'
J(p|pθ1 , pθ2 , . . . , pθM ) = min p
' i k=1 i p(i ) = 1 . st p(i ) ln p(i ) pθk ( i )
( 10 )
16 Theorem 1 . The optimal solution to J(p|pθ1 , pθ2 , . . . , pθM ) is that p(i ) =
θk ( i ) θk ( j ) .
Πkp
Proof . The Lagrange function for the above problem is
Λ(p , λ ) = p(i ) ln p(i ) pθk ( i )
+ λ( p(i ) − 1 ) .
' i
. j
M
M
Πkp
√ √ ' M' k=1 i ln pθk ( i ) + M + λ .
Derivative of Λ with respect to p(i ) is
∂Λ ∂p(i )
= M ln p(i ) − M' M' k=1
Let the derivative equal 0 , we have
From the constraint i p(i ) = 1 , we conclude that
M ln p(i ) = ff p(i ) = ln pθk ( i ) − M − λ . ( (
Πkpθk ( i )
M
M
Πkpθk ( j ) k=1 ff j
4 . MULTI MODALITY SENTIMENT ANAL
YSIS
In this section , we describe in details on how to apply the proposed model to multi modality sentiment analysis . In particular , we focus on how to extract the state of the art visual and textual features and apply them to the proposed model . Figure 2 shows the framework for multi modality sentiment analysis . The recent developed Convolutional Neural Network ( CNN ) [ 17 ] has achieved the state of the art performance on a wide range of vision tasks . You et al . [ 32 ] conducted experiments on deploying CNN for visual sentiment analysis and achieved better performance than both low level [ 25 ] features and mid level [ 4 , 34 ] features . Inspired by their conclusion , we propose to use CNN for the extraction of visual features . In particular , we employ the pre trained CNN model on imagenet [ 15 ] to fine tune a CNN model for visual sentiment analysis . The details on finetuning the CNN model will be discussed in the experimental section . Next , the fine tuned CNN model is employed to extract visual features from the second to the last layer of the neural network .
For textual features , Le and Mikolov [ 18 ] developed an unsupervised language model to learn distributed representations for documents . They applied the learned representations to analyze textual sentiment , which achieved the best performance compared with other existing state of the art textual sentiment analysis models . We employed the proposed model to learn distributed representations for related text of each image . In particular , We use descriptions and titles of each image as the body of a document to learn the textual features of each image .
Given the visual and textual features , we are able to train a cross modality consistent regression model for sentiment analysis . Meanwhile , the trained visual and textual model can extract visual and textual features for testing images and text individually , which next can be used to predict the sentiment distribution for the image and related text respectively .
5 . EXPERIMENTS
In this section , we conduct experiments to evaluate the performance of the proposed cross modality consistent regression model on sentiment analysis . To train the visual and textual model in Figure 2 , we choose to crawl data from Getty Images4 . The main reasons to use Getty Images are its relatively formal descriptions of images and its convenient and powerful query based searching system . 5.1 Training visual and textual models
To fine tune the pre trained CNN model for sentiment analysis , we need a relatively large labeled data set , which can cost huge human efforts . Meanwhile , different people may have somewhat different opinions on the sentiment of the same object , which makes it harder to have a well labeled training data set . In our implementation , we propose to use weakly labeled data to train our neural network . To be more specific , we use a list of keywords for both positive and negative sentiment.5 We query Getty Images with these keywords and all the returned images are labeled using the sentiment labels of these keywords . In this way , we are able to collect a large weakly labeled data set consisting of both images and text , which is employed to fine tune the CNN model and learn the paragraph vector for related text of each image . Table 1 summarizes the statistics of our collected data set from Getty Images . In total , there are 101 keywords . We collect a total of over half million weakly labeled images as well as their titles and descriptions .
Table 1 : Summary of the dataset from Getty Image .
Sentiment Num of Keywords Num of Images Positive Negative Sum
311,940 276,281 588,221
37 64 101
Given the above collected data set , we randomly split them into 80 % for training and 20 % for testing . We finetune the CNN model on the publicly available implementation Caffe [ 15 ] . We run the GPU accelerated version of Caffe implementation with a total iteration of 200 , 000 on a Linux X86 64 machine with 32G RAM and two NVIDIA GTX Titan GPUs . The fine tuned model is then employed to extract features for both training and testing images .
For textual model , the title and description of each image are concatenated as a single document . We use the algorithm in [ 31 ] to pre process the textual data . First , numbers and special characters are removed . Then , we tokenize the text using the tokenizer model from NLTK ( http://wwwnltk org ) . We also remove those words that appear less than 5 times in all the documents . The size of the paragraph vectors is 400 and the size of the nearby word window is 5 , which are the default settings in [ 18 ] .
We compare the performance of the proposed model with several baseline algorithms , including the following several different approaches . We also tried to use canonical correlation analysis ( CCA ) on this task . However , due to the scalability issue of CCA , we cannot fit all the training data into memory to learn the correlation using CCA . Table 2 4http://wwwgettyimagescom 5http://wwwscisdsuedu/CAL/wordlist/origwordlisthtml
17 Table 2 : Performance of CCA on different testing data ( see following sections for detailed description of the data ) .
Testing Data Getty Twitter AMT Twitter
Precision Recall F1 0.697 0.769 0.66
0.718 0.698 0.52
0.708 0.731 0.559
Accuracy 0.687 0.727 0.526 l e u a v n o i t c n u f s s o L
60
50
40
30
20
10
0
0
Val Loss Train Loss
5
Iteration number
10
15 summarizes the results of CCA on different sets of testing data using the same visual and textual features with other approaches using a single randomly selected mini batch ( 10,240 instances ) . The results suggest poor performance compared with other approaches ( see following details for details of the testing data and results of other approaches ) . Thus , we do not further compare the results of CCA with other approaches in the following experimental sections .
For all the following results of CCR , we run the algorithm 10 times with randomized initialization of the parameters . The averaged results are reported . The results of the following baselines are also reported and analyzed .
• Single visual model . We only use the visual features to build a logistic regression model , which outperforms models on both low level and mid level visual features [ 32 ] .
• Single textual model . The paragraph feature vectors are also fed to a logistic classifier to predict the sentiment [ 18 ] .
• Early fusion . We concatenate both visual and textual features and build a logistic regression model on these concatenated features .
• Late fusion . The average of the prediction sentiment score of visual and textual models is used as the prediction score of the late fusion model [ 30 , 6 ] .
5.2 Performance on Getty Images testing data set
We extract visual features for the 20 % testing images given the fine tuned CNN model . In this paper , we use the second to the last layer to extract features , which has a total of 4096 features for each image . For textual features , since the training is unsupervised , all documents are given to the model to learn their features [ 18 ] .
Following the steps in Algorithm 1 , we split the training data into mini batches and train all visual , textual logistic regression model and CCR model on the same collection of mini batches . In our implementation , we use a batch size of 10240 , which is a trade off between memory load and convergence rate .
Figure 3 shows the changes of the objective loss functions with the increase of mini batch iteration numbers . The results show that the loss function value changes on some randomly chosen validation data set and training data set are comparable . Meanwhile , since we employ L BFGS , the loss function converges after about 10 iterations , ie running on 10 mini batches .
Since we have about 4 , 000 visual features and 4 , 00 textual features for each image , we try to balance the two modalities in selecting the hyper parameters . In all of our experiments ,
Figure 3 : Changes of the objective loss function on both training and validating data set .
Table 3 : Performance on the testing data set by different approaches .
Algorithm Textual Visual Early Fusion Late Fusion CCR
Precision Recall F1 0.806 0.747 0.778 0.785 0.846
0.544 0.745 0.769 0.775 0.759
0.655 0.746 0.774 0.780 0.800
Accuracy 0.696 0.732 0.763 0.769 0.800 we set λ to be 1 , γv for visual features also to be 1 and γt for textual features to be 56 . Table 3 shows the performance of different approaches on the 20 % weakly labeled testing data from Getty Images . The results show that visual features may have comparable precision and recall on these data . Textual features can achieve higher precision but lower recall . Both early fusion and late fusion can produce improved results over single modalities . However , both fail to improve the performance of precision than the single textual model . On the other hand , the proposed CCR model can improve the performance of both precision and recall than the two single models . Meanwhile , CCR performs best among all the methods in terms of both F1 and accuracy score . 5.3 Performance on Twitter data set
We also build a new data set from image tweets . In particular , we employ the Twitter streaming API to collect a large number of Tweets . In total , we collected about 15 million Tweets . Next , we keep Tweets that contain both images and English text . In total , we collect 220 , 000 image tweets . In our implementation , we employ the recent proposed VADER [ 14 ] to weakly label these tweets , which is a rule based textual sentiment analysis and attuned to Twitter contexts . Next , we select the top ranked positive and negative image tweets according to the VADER score . We manually filter out duplicates , low quality , porn and all text images . In this way , we collect a total of 31 , 584 weakly labeled image tweets , 16 , 844 of them are positive tweets and the rest are negative .
Since images from Twitter are much more diverse and different from Getty Images and tweets are also much more informal , we could not directly apply the trained model from Getty Images to these image tweets . Instead , we randomly 6Indeed , there is no significant different when we set γt ∈ [ 5 , 10 ] , which is close to the proportion of feature size |xv| and |xt| .
18 I hate Leesville so much
RT @PicturesEarth : This is so sad
RT @ICurvedYou : This why I be sad all the time
RIP to the off duty officer killed on Recker and Baseline today
RT @PupsPorn : Cutest little corgi ever ~
Happy Halloween from my little punkin and I!! : ) http://t.co/d53jBR1bIv
@GrubRestaurant thank you for an awesome meal for our anniversary!
3 years of bliss and it started there @heidijoypj
T 6 hours till I see this beauty
MVA in Khutsong left 2 critical on scene unfortunately 1passed away in hospitl
Best Friends Always @iHrithik @udaychopra
! Who said In Bollywood true friendship doesn't exists ?
More Than 30 People
Died And Several
Injured In Gombe Bus
Station Explosion
WHAT IS WRONG WITH PEOPLE this is MAD AF I'd cry if that happens to my brothers
( 3 bros 0 sis ) i am so sad
Sad afternoon in
Mozambique , 2 dead female rhino found , horns &amp ; front legs removed .
@PinkMiruku your making me cry
You're so kind my friend
I love me some Canton girls!
Happy birthday Molly Weasley! One of the strongest , yet loving , characters of the entire series . u make me happy toooo
&lt;3
Bike and gear bags checked in! Time to put up my feet and try to relax! Epic day tomorrow!!!
Figure 4 : Top confidently ranked examples by AMT workers . Top row ( in blue rectangle ) shows positive examples and bottom row ( in red rectangle ) shows negative examples .
When your phone dies but it didnt even reach 0 yet
I see these scary clowns on a daily basis driving on the 400 eh day let alone on Halloween dayBooooooo B safe
Seriously i hate waiting
Why is you messin up my photos I bet if I messed up yo photo you wouldn't like it
My son's Halloween costume was epic as hell but apparently his school doesn't seem to think so
This makes me kinda sad
I no longer wonder why boys were never interested in me im still upset over this
:((((((((((((((( RT
@ISupportOnikaM Still makes me sad
MY HEART HURTS
Street . My World
Tour . Believe Tour . So
#EMABiggestFansJustin proud .
Bieber
'Got your noseeee!'
@KELLYROWLAND LMFAO gets me every time xD #FreddyvsJason
Happy Halloween! #RealMilkPaint
#Pumpkins!
She really does love her costume , it's just nap time ; ) Happy Halloween! was waiting for u to say that ! check this the sexy beast !
Happy Halloween from BakingBar! Check out the @marksandspencer treats we're enjoying!
Happy #Halloween
#Sven #Frozen #homemade
Bob Marley enjoys a beer after a football match
HI TAYLOR I LOVE YOU SO SO MUCH #taylurking #ts1989
He looks so good
Figure 5 : Ambiguously ranked examples by AMT workers . Top row ( in blue rectangle ) shows positive examples and bottom row ( in red rectangle ) shows negative examples . See text for explanation . split this data set into batches with the same size of 10240 . We use the first batch the testing data set and the rest the training data set to fine tune both CNN and paragraph vector model . In particular , we slightly fine tune the CNN model previously trained on the Getty Images with 2 , 000 iterations with the learning rate set to 0001 For paragraph vector model , we feed both the descriptions from Getty Images and the tweet text of the 31 , 584 image tweets to this model to learn the vector representation for each tweet . For the tweets , we preprocessed them by further replacing hashtags , url links and user ids with special string sequences . Table 4 shows the results on the randomly selected 10240 testing image tweets . It is interesting to find that the textual features works better than the visual features . This may be due to the fact that we obtain the weak labels from text based system VADER and an insufficient number of images for CNN to find a relatively good local optima . However , the proposed CCR are able to improve the performance on the same set of visual and textual features .
531 Performance on manually labeled tweets Meanwhile , in order to have more accurate labels for these image tweets , we employed crowd intelligence , Amazon Mechanical Turk ( AMT ) , to generate sentiment labels for selected image tweets , in a similar fashion to [ 5 ] . We recruited 5 AMT workers for each of the candidate image tweet . We
Table 4 : Performance on the Twitter testing data set by different approaches .
Algorithm Textual Visual Early Fusion Late Fusion CCR
Precision Recall F1 0.746 0.584 0.730 0.634 0.831
0.693 0.561 0.744 0.610 0.805
0.727 0.573 0.737 0.622 0.818
Accuracy 0.722 0.553 0.717 0.604 0.809 test the performance of different models on this manually labeled data using the previously fine tuned models on the weakly labeled image tweets . We randomly select 2 , 000 image tweets and post them in AMT for sentiment annotation . After receiving the batch results from AMT , we keep those that have at least 4 agreements on the sentiment label and also exclude those that appears in the previously weakly labeled image tweets for fine tuning . Eventually , we have 613 image tweets , of which 389 are labelled positive and 224 are labelled negative by 5 AMT workers .
Table 5 gives the performance of different approaches . CCR performs best in terms of precision , F1 and accuracy . However , it has a slightly lower recall . Compared with the results in Table 4 , visual features show significant improve
19 t n e m i t n e s e v i t i s o p g n e b i f o y t i l i b a b o r P
1
0.8
0.6
0.4
0.2
0
2
( a ) Top negative examples
4
8 Index of example images
6
10 t n e m i t n e s e v i t i s o p g n e b i f o y t i l i b a b o r P
1
0.8
0.6
0.4
0.2
0
2
( b ) Top positive examples
4
8 Index of example images
6
10 t n e m i t n e s e v i t i s o p g n e b i f o y t i l i b a b o r P
1
0.8
0.6
0.4
0.2
0
2
( c ) Boarderline negative examples
4
8 Index of example images
6
10 t n e m i t n e s e v i t i s o p g n e b i f o y t i l i b a b o r P
1
0.8
0.6
0.4
0.2
0
2
( d ) Boarderline positive examples
Textual Visual Early Fusion Late Fusion CCR
10
4
8 Index of example images
6
Figure 6 : Machine performance on confident and uncertain examples labelled by AMT workers .
Table 5 : Performance on the AMT manually labeled data set by different approaches .
Table 6 : Top 100 most confident sentiment prediction distribution of different algorithms .
Algorithm Textual Visual Early Fusion Late Fusion CCR
Precision Recall F1 0.832 0.762 0.776 0.799 0.886
0.638 0.715 0.740 0.738 0.730
0.722 0.737 0.758 0.767 0.800
Accuracy 0.688 0.677 0.700 0.716 0.769
Senti Alg
5 Agree
4 Agree
Neg
Pos
Textual Visual Early Late CCR Textual Visual Early Late CCR
22 23 23 28 29 61 62 66 69 71
52 45 50 52 58 24 20 20 24 22
4 Obj 7 13 9 7 4 12 13 11 6 7
5 Obj 19 19 18 13 9 3 5 3 1 0 ment , which may be due to the fact that AMT workers take both text and image to label the sentiment . Meanwhile , it is possible that the labels by AMT workers are biased compared with the weak labels given by VADER , causing relatively poor performance of both CCR and Early Fusion compared with the results in Table 4 .
532 Analysis of top ranked examples We also compare and analyze the top ranked examples of both AMT workers and machines . Since each image is labeled by 5 AMT workers into one of strongly negative ( 2 ) , negative ( 1 ) , positive ( 1 ) and strongly positive ( 2 ) , we rank images according to the sum of their scores by these 5 workers . Next , we select the top ranked positive and negative examples as well as some borderline examples . Figure 4 shows the top 10 ranked negative and positive examples by AMT workers . For negative examples , most of them are related to some bad experienced topics , such as car accident , environmental change and so on . Most of the positive examples are kind of cute , happy images along with some funny short descriptions . For comparison , ambiguously ranked examples are also selected and shown in Figure 5 , where most of these examples also seem reasonable . It seems that the disagreement of these borderline examples may come from the inconsistency between the text and the visual content of an image tweet . Meanwhile , some of these image tweets may have celebrity related topic , which may also cause different opinions among different groups of fans .
Next , we conduct experiments on the performance of different machine approaches on these selected human labeled examples . Figure 6 shows the predicted results of different approaches on the two selected groups of example images in Figure 4 and Figure 5 . It is interesting to note that for negative examples , machines seem to be uncertain on the confident examples given by AMT workers . However , they are confident on those uncertain examples . For positive examples , it seems that machine is kind of having similar recognition ability on both the confident and uncertain examples . These results demonstrate that the trained machine model and human beings may have different recognition ability towards the sentiment of the same group of images , which may be due to difference between the limited training samples for machines and the constantly learning process for human beings .
We also extract the top 100 positive and top 100 negative image tweets by AMT workers . The prediction results of different approaches are given in Table 6 . Overall , CCR outperforms other approaches in both negative and positive categories in terms of accuracy . Meanwhile , all approaches seem to be more likely to agree with AMT workers on the positive category . This may be due to the biased nature of social networks , where users are more likely to post positive content than negative content .
In addition , we extract the most confident prediction examples of different approaches on the manually labeled image tweets by AMT workers . We rank the images by the prediction score of each model . Figure 7 shows the top ranked 5 images of each model on both positive and negative categories respectively ( red circles indicate wrongly predicted samples ) . All the image tweets are ranked from left to right in a decreasing order . There are many common highly ranked examples between different approaches . However , different approaches have different ranking orders . In particular , highly ranked examples using textual features seems to have strong discriminative words than those using visual features , which explains the main reason of the two wrongly predicted examples . Similarly , only using visual features may also lead to wrong confident examples due to the lack of knowledge from the text data . Meanwhile , we note that there are no shared top ranked examples with that given by human beings in Figure 4 . Again , these differences may come from different learning scenarios for both human beings and machines . Meanwhile , this also suggests the challenging nature of visual sentiment analysis .
6 . CONCLUSIONS
Sentiment analysis , in particular visual sentiment analysis , is a challenging and interesting problem . In this work ,
20 'were sorry lauren' 'no this cant be' 'lauren she's gone'
Blue Mountain Dark Hollow barrel aged is off . So sad but Terrapin Moo Hoo Stout is on . So delicious!!
She may be back but he's gonna make it really obvious that he hates this i was gonna be niall but i had to wear my cheer uniform so now i'm wearing a sweater and calling myself sacajawea
These are ugly af but those wannabe tumblr fake sad cyber slut bitches gonna go crazy over em
Passing out candy at kaiser children's hospital with and having an awesome words
Nothing says , 'Welcome to LA!' like this guy!
#Smart2014 if u r in #DTLA tomorrow night come out and see mealso special announcement tomorrow morning!!!
Street . My World
Tour . Believe Tour . So
#EMABiggestFansJustin proud .
Bieber
Thanks for the amazing memories , #Royals . My dad made sure I was a fan so I made sure he could be there last night .
( a ) Negative examples using textual features
( b ) Positive examples using textual features no everything is not 'ok' I'm showing you how close I am to losing my shit w/ you
This kinda makes me sad
These are ugly af but those wannabe tumblr fake sad cyber slut bitches gonna go crazy over
This is so sad
School nurses be like
'come back after lunch if it still hurts'
Getting bored now .
Booooo .
A Happy Halloween from @GoShockersBSB and @GoShockersSB
#HalloweenU
Happy #Halloween from New Orleans! Will you be dressing up today ?
HBD to the only chick as short and cool as me!!Ily so much and hope you have a fab day!! Eat lots of fab food get fat
Happy Halloween everyone!
( c ) Negative examples using visual features
( d ) Positive examples using visual features
Know why City didn't get any penalties ? Michael Oliver still haven't forgiven Hart for this . no everything is not 'ok' I'm showing you how close I am to losing my shit w/ you
This kinda makes me sad
'i hate everyone' 'black like my soul' 'stressed depressed but well dressed' 'sad but hella rad'
IM SCARED OKAY IM
SORRY
HBD to the only chick as short and cool as me!!Ily so much and hope you have a fab day!! Eat lots of fab food&amp;get fat
Happy Halloween from @GoShockersBSB and
@GoShockersSB
Happy Halloween everyone!
Getting bored now .
Booooo .
Gorgeous View!
( e ) Negative examples using early fusion
( f ) Positive examples using early fusion
Know why City didn't get any penalties ? Michael Oliver still haven't forgiven Hart for this .
Even my pups are upset over the Thunder injury situation
IM SCARED OKAY IM
SORRY
'i hate everyone' 'black like my soul' 'stressed depressed but well dressed' 'sad but hella rad' no everything is not 'ok' I'm showing you how close I am to losing my shit w/ you
Passing out candy at kaiser children's hospital with @gremlinskids and having an awesome
HBD to the only chick as short and cool as me!!Ily so much and hope you have a fab day!! Eat lots of fab food&amp;get fat
Salute to men ! who take their wives to IK's Jalsa to shown them their first love : )
How I enjoy my morning cup of coffee . I love it .
Spade must be acting like trailer trash forgot to attach the pic . My baby dressed for work shes so beautiful .
( g ) Negative examples using late fusion
( h ) Positive examples using late fusion
These are ugly af but those wannabe tumblr fake sad cyber slut bitches gonna go crazy over em
'i hate everyone' 'black like my soul' 'stressed depressed but well dressed' 'sad but hella rad'
Every Sunday without fail I get handed religious shit I must need saving or something #prasejesus no everything is not 'ok' I'm showing you how close I am to losing my shit w/ you
The walk of shame after Halloween is always the worst
How I enjoy my morning cup of coffee . I love it .
Passing out candy at kaiser children's hospital with @gremlinskids and having an awesome
Thanks for the amazing memories , #Royals . My dad made sure I was a fan so I made sure he could be there last night .
HBD to the only chick as short and cool as me!!Ily so much and hope you have a fab day!! Eat lots of fab food get fat
Salute to men ! who take their wives to IK's Jalsa to shown them their first love : )
( i ) Negative examples using CCR
( j ) Positive examples using CCR
Figure 7 : Examples of most confident image tweets of different approaches . Left column shows the most confident negative examples . Right column shows the most confident positive examples . we aim to analyze sentiment via both visual and textual content . The recently developed machine learning algorithms lead to the availability of robust visual and textual features for abstract tasks , such as sentiment analysis . Due to the largely easily accessible weakly labeled data , we can train both visual and textual models to extract robust features for sentiment analysis . We develop a cross modality consistency regression model , which tries to enforce the agreement between sentiment labels predicted by different modality features . The experimental results suggest that the proposed multi modality regression model outperforms both the stateof the art single textual and visual sentiment analysis models and two fusion models .
Meanwhile , the main advantage of using convolutional neural networks and unsupervised paragraph vector model is that we can transfer the knowledge to other domains using a much simpler fine tuning technique than those in the literature ie , [ 9 ] . We also hope our sentiment analysis results can encourage further research on online user generated multimedia content .
Acknowledgements This work was generously supported in part by Adobe Research , and New York State CoE IDS . Jianchao Yang performed related work while he was with Adobe Research .
21 7 . REFERENCES
[ 1 ] S . Asur and B . A . Huberman . Predicting the future with social media . In WI IAT , volume 1 , pages 492–499 . IEEE , 2010 .
[ 2 ] J . Bollen , H . Mao , and A . Pepe . Modeling public mood and emotion : Twitter sentiment and socio economic phenomena . In ICWSM , 2011 . [ 3 ] J . Bollen , H . Mao , and X . Zeng . Twitter mood predicts the stock market . Journal of Computational Science , 2(1):1–8 , 2011 .
[ 4 ] D . Borth , T . Chen , R . Ji , and S F Chang . Sentibank : large scale ontology and classifiers for detecting sentiment and emotions in visual content . In ACM MM , pages 459–460 . ACM , 2013 .
[ 5 ] D . Borth , R . Ji , T . Chen , T . Breuel , and S F Chang .
Large scale visual sentiment ontology and detectors using adjective noun pairs . In ACM MM , pages 223–232 . ACM , 2013 .
[ 6 ] D . Cao , R . Ji , D . Lin , and S . Li . A cross media public sentiment analysis system for microblog . Multimedia Systems , pages 1–8 , 2014 .
[ 7 ] D . C . Cire¸san , U . Meier , J . Masci , L . M . Gambardella , and J . Schmidhuber . Flexible , high performance convolutional neural networks for image classification . In IJCAI , pages 1237–1242 , 2011 .
[ 8 ] D . Davidov , O . Tsur , and A . Rappoport . Enhanced sentiment learning using twitter hashtags and smileys . In ICL , pages 241–249 , 2010 .
[ 9 ] L . Duan , D . Xu , I H Tsang , and J . Luo . Visual event recognition in videos by learning from web data . IEEE PAMI , 34(9):1667–1680 , 2012 .
[ 10 ] F . Feng , X . Wang , and R . Li . Cross modal retrieval with correspondence autoencoder . In ACM MM , pages 7–16 . ACM , 2014 .
[ 11 ] A . Frome , G . S . Corrado , J . Shlens , S . Bengio ,
J . Dean , T . Mikolov , et al . Devise : A deep visual semantic embedding model . In NIPS , pages 2121–2129 , 2013 .
[ 12 ] Y . Gong , L . Wang , M . Hodosh , J . Hockenmaier , and
S . Lazebnik . Improving image sentence embeddings using large weakly annotated photo collections . In ECCV , pages 529–545 . Springer , 2014 .
[ 13 ] X . Hu , J . Tang , H . Gao , and H . Liu . Unsupervised sentiment analysis with emotional signals . In WWW , pages 607–618 , 2013 .
[ 14 ] C . Hutto and E . Gilbert . Vader : A parsimonious rule based model for sentiment analysis of social media text . In ICWSM , 2014 .
[ 15 ] Y . Jia , E . Shelhamer , J . Donahue , S . Karayev ,
J . Long , R . Girshick , S . Guadarrama , and T . Darrell . Caffe : Convolutional architecture for fast feature embedding . arXiv preprint arXiv:1408.5093 , 2014 . [ 16 ] D . Joshi , R . Datta , E . Fedorovskaya , Q T Luong ,
J . Z . Wang , J . Li , and J . Luo . Aesthetics and emotions in images . IEEE Signal Processing Magazine , 28(5):94–115 , 2011 .
[ 17 ] A . Krizhevsky , I . Sutskever , and G . E . Hinton .
Imagenet classification with deep convolutional neural networks . In NIPS , pages 1097–1105 , 2012 .
[ 18 ] Q . Le and T . Mikolov . Distributed representations of sentences and documents . In ICML , 2014 .
[ 19 ] Y . LeCun , L . Bottou , Y . Bengio , and P . Haffner .
Gradient based learning applied to document recognition . Proceedings of the IEEE , 86(11):2278–2324 , 1998 .
[ 20 ] T . Mikolov , I . Sutskever , K . Chen , G . S . Corrado , and
J . Dean . Distributed representations of words and phrases and their compositionality . In NIPS , pages 3111–3119 , 2013 .
[ 21 ] L P Morency , R . Mihalcea , and P . Doshi . Towards multimodal sentiment analysis : Harvesting opinions from the web . In ICMI , pages 169–176 , 2011 .
[ 22 ] J . Ngiam , A . Khosla , M . Kim , J . Nam , H . Lee , and
A . Y . Ng . Multimodal deep learning . In ICML , pages 689–696 , 2011 .
[ 23 ] B . O’Connor , R . Balasubramanyan , B . R . Routledge , and N . A . Smith . From tweets to polls : Linking text sentiment to public opinion time series . ICWSM , 11:122–129 , 2010 .
[ 24 ] N . Rasiwasia , J . Costa Pereira , E . Coviello , G . Doyle , G . R . Lanckriet , R . Levy , and N . Vasconcelos . A new approach to cross modal multimedia retrieval . In ACM MM , pages 251–260 . ACM , 2010 .
[ 25 ] S . Siersdorfer , E . Minack , F . Deng , and J . Hare .
Analyzing and predicting sentiment of images on the social web . In ACM MM , pages 715–718 . ACM , 2010 .
[ 26 ] R . Socher , M . Ganjoo , C . D . Manning , and A . Ng . Zero shot learning through cross modal transfer . In NIPS , pages 935–943 , 2013 .
[ 27 ] R . Socher , Q . Le , C . Manning , and A . Ng . Grounded compositional semantics for finding and describing images with sentences . In NIPS Workshop , 2013 . [ 28 ] N . Srivastava and R . Salakhutdinov . Multimodal learning with deep boltzmann machines . In NIPS , pages 2222–2230 , 2012 .
[ 29 ] A . Tumasjan , T . O . Sprenger , P . G . Sandner , and
I . M . Welpe . Predicting elections with twitter : What 140 characters reveal about political sentiment . ICWSM , 10:178–185 , 2010 .
[ 30 ] M . Wang , D . Cao , L . Li , S . Li , and R . Ji . Microblog sentiment analysis based on cross media bag of words model . In ICIMCS , pages 76:76–76:80 . ACM , 2014 .
[ 31 ] S . Wang and C . D . Manning . Baselines and bigrams :
Simple , good sentiment and topic classification . In ACL , pages 90–94 , 2012 .
[ 32 ] Q . You , J . Luo , H . Jin , and J . Yang . Robust image sentiment analysis using progressively trained and domain transferred deep networks . In The Twenty Ninth AAAI Conference on Artificial Intelligence ( AAAI ) , 2015 .
[ 33 ] Q . You , J . Luo , H . Jin , and J . Yang . Building a large scale dataset for image emotion recognition : The fine print and the benchmark . In The Thirtieth AAAI Conference on Artificial Intelligence ( AAAI ) , 2016 .
[ 34 ] J . Yuan , S . Mcdonough , Q . You , and J . Luo .
Sentribute : image sentiment analysis from a mid level perspective . In WISDOM , page 10 , 2013 .
[ 35 ] X . Zhang , H . Fuehres , and P . A . Gloor . Predicting stock market indicators through twitter “ i hope it is not as bad as i fear ” . Procedia Social and Behavioral Sciences , 26:55–62 , 2011 .
22
