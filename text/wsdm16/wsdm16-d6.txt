The Probabilistic Hitting Set Paradigm : a General Framework for Search and Detection in Dynamic
Social Networks(cid:63 )
Ahmad Mahmoody and Eli Upfal
Department of Computer Science , Brown University , Providence , RI
{ahmad,eli}@csbrownedu
ABSTRACT We formulate and study the Probabilistic Hitting Set Paradigm ( PHSP ) , a general framework for design and analysis of search and detection algorithms in large scale dynamic networks . The PHSP captures applications ranging from monitoring new contents on the web , blogosphere , and Twitterverse , to analyzing influence properties in social networks , and detecting failure propagation on large electronic circuits . The Probabilistic Hitting Set Paradigm ( PHSP ) defines an infinite time generating process that places new items in subsets of nodes , according to an unknown probability distribution that may change in time . The freshness or relevance of the items decay exponentially in time , and the goal is to compute a dynamic probing schedule that probes one or a few nodes per step and maximizes the expected sum of the relevance of the items that are discovered at each step . We develop an efficient sampling method for estimating the network parameters and an efficient optimization algorithm for obtaining an optimal probing schedule . We also present a scalable solution on the MapReduce platform . Finally we apply our method to real social networks , demonstrating the practicality and optimality of our solution .
1 Introduction
An emerging trend in algorithmic stock trading is the use of automatic search through the Web , the blogosphere , and social networks for relevant information that can be used in fast trading , before it appears in the more popular news sites [ 16,2,1,41,34,3,40 ] . Similarly , intelligence , business and politics analysts are scanning online sources for new information or rumors . While new items are often reblogged , retweeted , and posted on a number of sites , the goal is to find the information once , as fast as possible , before it loses its relevance . There is no benefit in seeing more copies of the same news item , rumor , etc .
Such a search is an example of a fundamental search and detection problem in dynamic , distributed massive data repository . Data is distributed among a large number of nodes , new items appear in individual nodes , and items may propagate ( copied ) to neighboring nodes on a physical or a virtual network . The goal is to detect at least one copy of each new item as soon as possible . The search application can probe any node in the system , but it can only access and process a few nodes at a time . To minimize the time to find new items , the search application needs to optimize the schedule of probing nodes , taking into account ( i ) the distribution of copies of items among the nodes ( to see where to probe ) , and ( ii ) the decay of items’ freshness ( or relevance ) over time ( to focus the search on most relevant items ) .
( cid:63 ) Work supported by NSF grant IIS 1247581 and NIH grant R01 CA180776 .
5 1 0 2 r p A 3 1
] I S . s c [
1 v 5 7 2 3 0
.
4 0 5 1 : v i X r a
2
Ahmad Mahmoody and Eli Upfal
Another application that faces a similar search problem is processing security raw data ( eg gathered security raw footages that need to be processed ) . Ideally , given limited resources compared to the huge volume of the data , a schedule for processing ( probing ) different raw dataset has to minimize the value of missing information ( assuming newer information are more valuable than older ones ) .
Our goal is to provide an efficient tool for a user who does not have a prior information on the generation and distribution of items in the network . We focus on simple , memoryless probing schedules that are easy to compute and require minimum storage . In the next section , we formally introduce the model and define our problem .
1.1 Model and Problem Definition
The Probabilistic Hitting Set Paradigm defines a generating process that creates new items , at each step , and distributes copies of these items between different sets of nodes : Definition 1 ( Generating Process ) . Let U = {1 , . . . , n} be a set of nodes , F a family of subsets of U , and π : F → [ 0 , 1 ] . A generating process Γ = ( U,F , π ) is an infinite time process that at each time t , generates a collection of subsets It ⊆ F , such that a set S ∈ F is included in the sample It with probability π(S ) , independent of other sets . An item it,S is a copy of S ∈ F generated at time t , and thus , it,S ∈ It . We call the elements of F informed set of U .
A probing schedule is a probability distribution over the possible sets of nodes to probe in each step . Note that we are interested in efficient memoryless schedules that can be stored and computed efficiently .
Definition 2 ( Schedule ) . A c schedule , for a positive integer c , is a probability distribution p over U such that at any time step , the schedule probes ( up to ) c nodes independently chosen according to the distribution p . We say an item it,S is caught at time t ≥ t , if p probes a node of S at time t , and no node of S was probed in the interval [ t , t − 1 ] .
For simplicity , we may say schedule instead of c schedule if it is clear in the context . The cost of a schedule is the expected number of undetected items at a given step , weighted by their freshness , and averaged over time : Definition 3 ( θ Cost ) . Let θ ∈ ( 0 , 1 ] be a decaying factor . The freshness of an uncaught item it,S at time t ≥ t is θt−t . The load of Γ at time t , denoted by LΓ ( t ) , is the sum of the freshness of uncaught items at time t . The θ cost , or cost in short , of a schedule p is defined as lim t→∞
E(LΓ ( t) ) . We denote the cost of p by cost ( p ) . t
1 t t=0
Thus , the cost function is the limit expected load of the system , averaged over time . Note that this limit always exists as proven in Lemma 1 . Our goal is to compute a schedule with minimum cost :
Definition 4 ( Probabilistic Hitting Set Paradigm ) . The goal in a ( θ , c) Probabilistic Hitting Set Paradigm , or ( θ , c) PHSP , for a generating system Γ is to find a c schedule with the minimum θ cost . We call a schedule optimal if it has the minimum cost .
Note that in ( θ , c) PHSP , the parameter θ tunes our interest in older items . For instance , if θ is close to 0 , an ideal schedule should catch as many new items ( generated in very recent time steps ) as possible . In the other extreme , if θ = 1 , an ideal schedule should consider all the items for catching , regardless of their age .
In other words , an ideal schedule for a ( θ , c) PHSP is a schedule that spends more time on probing the nodes that are more likely to receive “ fresh ” items : if the items are
Probabilistic Hitting Set Paradigm
3 the “ information ” that flow over the network , these nodes play the role of information hubs among the nodes . Our search for information hubs can be view as the complement of the “ influence maximization problem ” [ 23,24 ] . In the influence maximization we look for a set of nodes that generate the information that reach most nodes . In the information hubs problem we are looking for the set of nodes that receive the most amount of information , thus the most informative nodes .
2 Related Work
There has been extensive work on Outbreak Detection using statistic or mobile sensor in physical domains , motivated in part by the “ Battle of Water Sensor Network ” challenge [ 43 ] , where the goal was to optimize the placement of sensors in water networks to detect contamination [ 35,29,20 ] . The optimization can be done with respect to a number of objectives , such as maximizing the probability of detection , minimizing the detection time , or minimizing the size of the subnetwork affected by the phenomena [ 35 ] . A related work [ 4 ] considered sensors that are sent along fixed paths in the network with the goal of gathering sufficient information to locate possible contaminations . Early detection of contagious outbreaks by monitoring the neighborhood ( friends ) of a randomly chosen node ( individual ) was studied in [ 13 ] . Efficient scheduling for minimizing energy consumption in battery operated sensors were studied in [ 32 ] , and distributed solutions with limited communication capacities and costs were studied in [ 31,18,30 ] .
In contrast , our work is geared to detection in virtual networks such as the Web or social networks embedded in the Internet , where a monitor can reach ( almost ) any node at about the same cost . The monitor is restricted to probing a small number of nodes per step , the optimization of the probing sequence is over a larger domain , and the goal is to identify the outbreaks ( items ) regardless of their size and solely by considering their interest value .
Our methods complement the work on Emerging Topic Detection where the goal is to identify emergent topics in a social network , assuming full access to the stream of all postings . Providers , such as Twitter or Facebook , have an immediate access to all tweets or postings as they are submitted to their server [ 9,38 ] . Outside observers need an efficient mechanism to monitor changes , such as the methods developed in this work . An interesting related research direction aims at controlling or stopping contagions in the networks by changing the topology of the network ( e.g , by protecting or blocking the edges and/or nodes ) [ 6,25,26,27,37,21,33 ] . Another related research direction is Anomaly Detection whose goal is to find the events ( usually structural ) that deviate from normal patterns [ 42,46,17,44,5,8 ] . Finally , web crawling is another interesting research area that relates to obtaining the most recent snapshots of the web . However , it differs from our model in two key points : our model allows items to propagate their copies , and they will be caught if any of their copies is discovered ( where snapshots of a webpage belong to that page only ) , and all the generated items should be discovered ( and not just the recent ones ) , eg see [ 14,48 ] .
3 Method
In this section , we first study the general problem of finding an optimal schedule in a ( θ , c) PHSP for a generating process Γ = ( U,F , π ) . We provide two different approaches assuming we know F and π in prior . Then , we show how we still can apply our methods if we observe a sample I of F generated according to π during a time interval of length ˜O(log(n) ) , ie , I = I1 ∪I2 ∪ . . .∪I ˜O(log(n ) ) ( see Section 1 ) . Next , we present our method in the MapReduce framework to show the scalability of our method . Finally , we
4
Ahmad Mahmoody and Eli Upfal conclude the section by studying the case when the parameters of the network ( topology , probabilities , etc ) change and show how our method can adapts itself in dynamic networks .
3.1 The Optimization Algorithm
As explained in the previous section , we study the ( θ , c) PHSP for a generating process Γ = ( U,F , π ) . Now , using the fact that θ ∈ ( 0 , 1 ] , we can give an explicit formula for computing the cost of a c schedule , which is useful in our optimization problem .
Lemma 1 . For a schedule p we have cost ( p ) def= lim t→∞
1 t
E(LΓ ( t ) ) = where p(S ) = i∈S pi . t t=0
S∈F
π(S )
1 − θ(1 − p(S))c ,
Proof . Let i = it,S be an item . The probability that i is still uncaught at time t is ( 1 − p(S))c(t−t ) , and its freshness at t is θt−t . Therefore , i imposes the load θt−t to the system at time t , with probability ( 1 − p(S))c(t−t ) , and zero otherwise . Since the probability of it,S being generated is π(S ) we have t
π(S),1 + θ · ( 1 − p(S))c + . . . + θt · ( 1 − p(S))ct
E(LΓ ( t ) ) = lim t→∞
E ( LΓ ( t ) ) t=0
1 t cost ( p ) = lim t→∞
= lim t→∞
=
S∈F
S∈F
π(S )
1 − θ(1 − p(S))c , t i=1 ai t
= where we used the fact that for any convergent sequence {ai}i∈N we have lim t→∞ lim i→∞ ai ( Cesaro means [ 19] ) .
Now , having the explicit formula for the cost function , we show that it is a convex function over its domain :
Theorem 1 . The cost function , cost ( p ) , is a convex function over its domain . Proof . Fix a subset S ⊆ V . Note that it suffices to show that the function fS(p ) = 1−θ(1−p(S))c is a convex function , as cost ( p ) is a linear combination of fS(p ) ’s with positive coefficients . Let gS(p ) = θ(1 − p(S))c . We claim that gS is a convex function . This is because
1 the Hessian matrix of gS is positive semidefinite : fl θc(c − 1)(1 − p(S))c−2 i , j ∈ S otherwise
∂
∂pi∂pj gS(p ) =
Suppose VS is a n×1 vector in Rn such that its i th coordinate is.c(c − 1)(1 − p(S))c−2fi1/2 if i ∈ S , and 0 otherwise . Therefore , we can write the Hessian matrix of gS as
0
∇2gS = VS ∗ V T S , and thus , ∇2gS is positive semidefinite matrix and g is convex . So , 1 − gS is a concave function . Finally , since fS(p ) = x is convex and non increasing , fS is a convex function .
1−gS ( p ) and the function h(x ) = 1
1
Probabilistic Hitting Set Paradigm
5
Note that in the proof of Theorem 1 , if for every i ∈ U , S = {i} ∈ F , the function gS , and therefore fS , are strictly convex .
Theorem 1 has the following immediate corollary :
Corollary 1 . A schedule p with locally minimum cost , has also a global minimum cost and is an optimal schedule . Furthermore , if for every i ∈ U , {i} ∈ I the optimal schedule is unique .
Our goal to find an optimal schedule is indeed to solve the following optimization problem : min p cost ( p ) n i=1 pi = 1 pi ≥ 0 ∀i ∈ {1 , . . . , n}
( 1 )
Here , we present two approaches for solving this optimization problem : one by using gradient descent and one based on Lagrange multipliers . Using gradient descent , provides theoretical guarantee for converging to the optimal schedule but is slow in practice for large networks . The other method is fast but does not give a theoretical guarantee , though if it converges in finite time to the final schedule , that schedule is provably optimal ( see below ) .
Gradient Descent . As we showed in Theorem 1 the cost function is ( smoothly ) convex . Thus , we can apply the gradient descent method for the optimization in ( 1 ) , as converging to any local minimum is guaranteed to give a global minimum .
Lagrange Multipliers : nonlinear iterative method . Generally , to use the nonlinear iterative method , we need a function Q : n−1 → n−1 such that Q(p∗ ) = p∗ for any optimal schedule p∗ . Note that if p∗ is an optimal schedule , by Lagrange multipliers , it is a solution to the following equality :
∇[cost ( p ) + λ(p1 + . . . + pn ) ] = 0 .
( 2 )
The i th equation induced by ( 2 ) is ∂ ∂pi cost ( p ) + λ = 0 , which is
S:i∈S
θcπ(S)(1 − p(S))c−1 ( 1 − θ(1 − p(S))c)2 = λ .
For notational convenience let Wi(p ) = θcπ(S)(1−p(S))c−1 invertible function , and define Q(p ) = ( Q1(p ) , . . . , Qn(p ) ) where
( 1−θ(1−p(S))c)2 . Suppose f : R → R is an
Qi(p ) = pif ( Wi(p ) ) j pjf ( Wj(p ) )
. i ·λ Note that Qi(p∗ ) = p∗ i , and thus Q(p∗ ) = p∗ . Although we do not provide any j ·λ = p∗ j p∗ guarantee on convergence of this method , the following theorem states that Q(p ) = p if and only if p is optimal , and later in experiments , we illustrate the convergence of this method in different cases .
Theorem 2 . If p is a schedule , then Q(p ) = p if and only if p is an optimal schedule .
6
Ahmad Mahmoody and Eli Upfal
Proof . As shown above , if p is an optimal schedule Q(p ) = p holds . Now assume that Q(p ) = p . Therefore pi = Qi(p ) for 1 ≤ i ≤ n , and we have
Qi(p ) = pi = pif ( Wi(p ) ) j pjf ( Wj(p ) )
⇒ Wi(p ) = f−1 pjf ( Wj(p ) )
 j
 .
Hence , Wi(p ) is a constant ( independent of i ) and we have W1(p ) = . . . = Wn(p ) . Now , by letting λ = Wi(p ) , we obtain a solution for the system of equations in Lagrange multipliers method , and thus , p is an optimal schedule .
In this work , we apply the second method as it is very fast on large datasets that we use in our experiments . Also , although the convergence of this method is not guaranteed , we show that in practice it converges quickly . For sake of simplicity we took the identity function for f , and our method for the optimization problem in ( 1 ) is given in Algorithm 1 .
Algorithm 1 : optimizer(iterations )
Inputs : The number of iterations . begin p ← ( 1/n , . . . , 1/n ) ; for i ∈ {1 , . . . , iterations} do W ← ( 0 , . . . , 0 ) ; for S ∈ F do for u ∈ S do
Wi ← Wi − θcπ(S)(1−p(S))c−1 ( 1−θ(1−p(S))c)2 ; p ← 1 i piWi
( p1W1 , . . . , pnWn ) ; return p ;
3.2 On the Sample Complexity of F In a generating process Γ = ( U,F , π ) , computing the cost of a schedule can be very challenging due to the parameters F and π as they might not be known to us . Instead we have access to sampled informed set from F during the time . In particular , at any time step an informed set S ∈ F may be generated , and observed or sampled , with probability π(S ) . A sample I of informed set during a time interval [ t1 , t2 ] is I = It1 ∪ . . . ∪ It2 where each It is the set of all observed informed set at time t . In this section , we study the optimization problem ( 1 ) in generating processes for which we only have access to samples of F . Specifically , we show that a sample I gathered during a time interval of length ˜O(log(n ) ) suffices to estimate and optimize the cost function .
We start by defining the cost of a schedule according to a sample :
Definition 5 ( Cost according to a sample ) . Suppose p is a c schedule and I is a sample of informed sets gathered during a time interval of length . The cost of p according to I , denoted by cost ( p,I ) , is 1 For a sample I , cost ( p,I ) is an approximation of cost ( p ) , and the larger the size of I is , by the Law of Large Numbers we get better approximation . Lets denote the length
1−θ(1−p(S))c .
1
S∈I
Probabilistic Hitting Set Paradigm
7 of the time interval from which the sample I is obtained by ( I ) . The question we want to answer in this section is “ How large should ( I ) be , so that for every schedule p we have |cost ( p,I ) − cost ( p)| < · cost ( p ) whp for > 0 ? ”
We have the following theorem .
Theorem 3 . Suppose I is a sample gathered during a time interval of length ( I ) ≥
3(r log(n)+log(2 ) )
2(1−θ )
= O
. Then , for every schedule p we have log(n )
2
Pr(|cost ( p,I ) − cost ( p)| ≥ · cost ( p ) ) <
1 nr .
1−θ . Also let X =
Proof . Let XS be a random variable which is zero otherwise . So , 1 ≤ XS ≤ 1
1
1−θ(1−p(S))c with probability π(S ) , and
S∈F XS , and thus , cost ( p ) = E(X ) =
E(XS ) ≥ |F| ,
( 3 ) and |F| ≤ X ≤ |F| sampled from and define X i =
1−θ . Let X i
|F| denote µ = ( I)(1−θ ) we have Pr ( |cost ( p,I ) − cost ( p)| ≥ · cost ( p ) ) = Pr
S∈F
S∈F X i
S . Note that cost ( p,I ) = 1 ( I )
S be the i th draw of XS during the time interval I was i X i . Now , cost ( p ) . Using the fact that 1−θ|F| X ∈ [ 0 , 1 ] and the Chernoff bound fififififi ≥ ( I)cost ( p ) fififififi ≥ µ fififififi fififififi 1 − θ
|F|
− 2µ 3 − 2(I)(1 − θ)cost ( p )
3|F| − 2(I)(1 − θ )
X i − ( I)cost ( p )
≤ 2 exp
≤ 2 exp
X i − µ
= 2 exp
= Pr
, i i
3
2(1−θ ) where the last inequality holds since cost ( p ) ≥ |F| , as shown in Inequality ( 3 ) . So , if ( I ) ≥ 3( log(n)+log(2 ) ) · cost ( p ) with probability at least 1 − 1/nr .
= O,log(n)/ 2 , for every schedule p , |cost ( p,I ) − cost ( p)| <
In the previous section we defined the Wi(p ) functions on schedules for a given generating process Γ = ( U,F , π ) , assuming we know π . However , similar to the cost function , we can define these functions according to a sample , namely , Wi(p,I ) = θc(1−p(S))c−1 1 ( 1−θ(1−p(S))c)2 . Using a very similar argument we have the following the(I ) orem : Theorem 4 . Suppose I is a sample gathered during a time interval of length ( I ) ≥
S∈I:i∈S
. Then , for every schedule p and i ∈ {1 , . . . , n} we have log(n )
2
3(r log(n)+log(2 ) )
2(1−θ )
= O
Pr(|Wi(p,I ) − Wi(p)| ≥ · Wi(p ) ) <
1 nr .
We conclude this section by providing an algorithm , Algorithm 2 , for the optimization problem ( 1 ) in the case that we only have access to a sample of F . Note that by Theorems 3 and 4 , in the iterative process of Algorithm 2 , our estimates of the cost and Wi ’s function are within an factor of their true value , with high probability ( using an appropriate r ) .
8
Ahmad Mahmoody and Eli Upfal
Algorithm 2 : approx optimizer(iterations,I ) Inputs : The number of iterations , and a sample of informed sets I . begin p ← ( 1/n , . . . , 1/n ) ; for i ∈ {1 , . . . , iterations} do W ← ( 0 , . . . , 0 ) ; for S ∈ F do for u ∈ S do
Wi ← Wi − θc(1−p(S))c−1
( I)(1−θ(1−p(S))c)2 ; p ← 1 i piWi
( p1W1 , . . . , pnWn ) ; return p ;
3.3 Scalability of the Method
In this section , we show the scalability of optimizer and approx optimizer ( see Algorithms 1 and 2 ) in MapReduce framework [ 15 ] . A MapReduce model consists of two parts : mapping by mappers , and reducing by reducers . At each round of MapReduce the input dataset is partitioned into independent chunks and each chunk is sent to a mapper . Each mapper will process the received chunk and outputs pairs of key values , k , v . Then , the platform shuffles the key value pairs and aggregate the values of a same key and sends them to reducers . Therefore , each reducer receives inputs of form k , ( v1 , . . . , vr ) , where v1 , . . . , vr are all the values with the key k generated by mappers . Finally , reducers process their inputs and return the final outputs . In following , we show how computing the cost and Wi functions ( and consequently Q ) can be done in a MapReduce given a sample I for a generating process Γ = ( U,F , π ) . Therefore , approx optimizer can be easily scaled to larger dataset . So , lets assume the input dataset is I , and the informed set in I are partitioned and sent to mappers . We also assume that mappers have a copy of p . Computing the cost function . Each mappers , for every S ∈ I that it receives , outputs the following key value pair : 1−θ(1−p(S))c . Next ,
, where f ( S ) =
1 ( I ) f ( S )
1
0 ,
1
* a reducer receives an input of the form
0 ,
( I ) f ( S )
S∈I
, and outputs
S∈I f ( S ) ( I )
.
Computing the Wi ’s functions . Each mappers , for every S ∈ I that it receives , outputs all the following key value pairs : i ,
*
−θc(1 − p(S))c−1
( I)(1 − θ(1 − p(S))c)2 i ,
−θc(1 − p(S))c−1
( I)(1 − θ(1 − p(S))c)2
,∀i ∈ S .
*
,
S∈I:i∈S
Next , each reducer receives an input of the form and would be able to compute piWi(p,I ) , by adding the “ values ” and multiply by pi . Finally , the aggregator ( or the central processing unit ) can normalize the vector ( p1W1(p ) , . . . , pnWn(p) ) , and compute Q(p ) . Using a very similar technique , one can compute the cost and Wi ’s functions in MapReduce framework when having full access to F and π parameters of the generating process Γ = ( U,F , π ) . This shows the scalability of optimizer .
Probabilistic Hitting Set Paradigm
9
3.4 Dynamic Parameters
In this short section , we consider the case when the parameters of the network change ( i.e , the distribution over the possible informed sets changes ) . To apply our method to a dynamic environment we first sample the process to generate a sufficiently large sample collection . This can be done by probing all nodes with uniform distribution during a short time interval , or using a round robin schedule ( Algorithm 3 ) . For an item the sampling process stores the set of nodes that received this item . We then compute an optimal schedule with respect to that sample ( Algorithm 1 ) . We use this schedule and monitor the cost to detect significant changes . In that case we obtain a new sample , optimize with respect to that sample and apply the new schedule .
Note that when we adapt our schedule to the new environment ( using the most recent sample ) the system converges to its stable setting exponentially ( in θ ) fast : Suppose L items have been generated since the change of the parameters until we adapt the new schedule . These items , if not caught , loose their freshness exponentially fast : after t steps their freshness is at most Lθt and gets diminished very quickly .
In our experiments we provide different examples that illustrate how load of the generating process stables after the algorithm adapts itself to the changes of parameters ( see Section4 ) .
Algorithm 3 : sampler(time length )
Inputs : The length of the time interval that we sample from , time length . beginD1 ← ∅ ; D2 ← ∅ ; for nodes u ∈ U do for items i at u , not older than time length time steps do
D1 ← D1 ∪ {i} ; D2 ← D2 ∪ {(i , u)} ;
I ← ∅ ; for i ∈ D1 do return I ;
S ← ∅ ; for ( i , u ) ∈ D2 do S ← S ∪ {u} ;
I ← I ∪ {S} ;
4 Experimental Results experiments illustrate that the sequence cost,p1,I , cost,p2,I , . . . is descending and
In this section , we present our experimental results on optimality and efficiency of approx optimizer , as in realistic scenarios we only have access to samples from the informed sets F . First , we show that for a given sample I , approx optimizer converges quickly to a schedule p∗ that minimizes cost ( p,I ) ( see Theorem 2 ) . In particular , our converges after few iterations . Note that pi+1 is solely a function of I and pi . Next , for further investigation , for each generated sequence of p1 , p2 , . . . according to a sample I , we compute the cost of pi ’s according to another test sample T , and see
10
Ahmad Mahmoody and Eli Upfal that the sequence cost,p1,T , cost,p2,T , . . . is still descending and converges after few steps .
Finally , we demonstrate how our method can adapt itself to the changes in the network parameters .
Our experiments were run on a Opteron 6282 SE CPU ( 2.6 GHz ) using 12G memory .
We tested our method on the following networks1 ( see Table 1 for details ) :
– Enron Email [ 28 ] : The email communication of Enron Corporation . – Brightkite [ 12 ] : A location Based online social network , where each edge represents a friendship tie .
– Epinion [ 45 ] : The trust relationships in Epinioncom – web Google [ 36 ] : A dataset released by Google in 2002 , where each node is a page and a directed edge represents a hyperlink between the pages .
– Twitter [ 39 ] : A combined ego network , where each edge is directed from a node to another node that it follows .
Parameters and model of propagation . Throughout our experiments , for sake of simplicity we consider c = 1 , 3 or 5 for our ( θ , c) PHSP , and we always assume the graphs are directed by replacing undirected edges with two directed ones . We also let the decaying factor θ = 075 By deg+(i ) ( resp . deg ( i ) ) we mean the out degree ( resp . in degree ) of the node i . In each network G = ( V , E ) we group the nodes into four groups ( see Table 1 ) :
−
– V1000 =(i ∈ V |deg+(i ) ≥ 1000 ) , – V500 =(i ∈ V |500 ≤ deg+(i ) < 1000 ) , – V100 =(i ∈ V |100 ≤ deg+(i ) < 500 ) , and – V0 =(i ∈ V |deg+(i ) < 100 ) . 
πi =
0.1 : i ∈ V1000 0.05 : i ∈ V500 0.01 : i ∈ V50 0
: otherwise
We assume each node may generate a new item , that will be propagated , at each time based on the group the node is assigned to . In particular , the probability that the node i generates a new item , πi , is as follows : based on the intuition that active individuals are likely to have more followers/friends . In Table 1 the expected number of new items at each time step is given , for each dataset , as the rate of new items . As the model of propagation , we consider the Independent Cascade model[23 ] : each directed edge e = v → w has a probability pe that a new item at node v is propagated through this edge to node w , and events for different items are independent . Following the parameters reported in the literature [ 23,11,10,22,47 ] , we set pv→w =
1 deg−(w ) .
4.1 The Efficiency and Accuracy of approx optimizer
In Section 3.1 we showed that when a run of approx optimizer converges ( according to a sample I ) the computed c schedule is optimal with respect to the sample I ( Theorem 2 ) . Our first set of experiments measure the rate of convergence and the execution time of the optimization algorithm , approx optimizer . Formally , suppose pi is the obtained schedule by approx optimizer , according to the sample I , at the i th round 1 Available at http://snapstanfordedu/
Probabilistic Hitting Set Paradigm
11
Datasets
Enron Email Brightkite Epinion web Google Twitter
# of nodes # of edges |V1000| |V500| |V100| rate of new items
36692 58228 75879 875713 81306
367662 428156 508837 5105039 1768149
9 2 8
134 43
23 7 45 180 108
517 399 917 3546 2674
7.22 4.54 12.22 57.86 36.44
Table 1 : The datasets and corresponding statistics . is the sequence cost,p1,T , cost,p2,T , . . . still descending and converging ? This is of iteration . An important question regarding the convergence of approx optimizer according to a sample I is this : If T is another sample ( with probably larger size ) important as we have to avoid over fitting our schedule with the sample I . To demonstrate the convergence of approx optimizer we sample all the sets generated during a time interval of length 2000 , I . For c ∈ {1 , 3 , 5} the cost of each c schedule during the iterative method according to I is shown in Figure 1 . Also , in Figure 1 , the cost of each intermediate c schedule generated during the iterative method ( applied to I ) is computed according to a test sample T that has been obtained during a time interval of length 10000 . In both cases the cost values are decreasing . As shown in Figure 1 , the sequences of cost values according to both sample I and the test sample T are descending and converge after few steps . Also note that the cost values according to I and T are close ( in some cases almost identical ) , which agrees with the theoretical analysis in Section 32
Remark 1 . The implementation of approx optimizer never loads the entire sample to the main memory , which makes it very practical for running large samples on conventional machines .
For each graph , the size of the sample I , the average size of sets in I , and the average time of each iteration is given in Table 2 . Note that the running time of each iteration is a function of both sample size and sizes of the sets ( informed sets ) inside the sample .
Datasets
Enron Email Brightkite Epinion webGoogle Twitter
Sample size Average size of informed sets Average time of each iteration(sec )
14726
12956.59 31.9256
8985
24507
17502.81 15890.41 26.7181 66.4317
115917 700.96 18.9658
72842
15519.71 194.9281
Table 2 : Sample Sizes & Running time of each iteration in approx optimizer .
4.2 Dynamic Settings
In this section , we provide some experiments that demonstrate the changes in the generating process and how our algorithm can adapt itself to the new situation . Simulations are provided in Figure 2 : For each graph , we start by following an 1 optimal schedule in the graph . At the beginning of each “ gray ” time interval , the labels of the nodes are permuted randomly , and at the beginning of each “ green ” time interval our algorithm starts gathering samples of informed set ( sampler ) . Then , algorithm computes the schedule for the new sample ( approx optimizer – 15 iterations ) and starts probing . The length of each colored time interval is R = 3(log(n)+log(2 ) ) motivated by Theorem 3 ( for = 1 ) , and other intervals ( three time intervals before , between , and after the colored time intervals ) have length 10R . Also , for sake of illustration , we assumed at each time step each node i may generated up to 10 items ( each having a chance of πi to be generated ) . Thus , the number of generated items at each node is a binomial random variable with
( 1−θ )
12
Ahmad Mahmoody and Eli Upfal
Enron Email ( c = 1 )
Enron Email ( c = 3 )
Enron Email ( c = 5 )
Brightkite ( c = 1 )
Brightkite ( c = 3 )
Brightkite ( c = 5 )
Epinion ( c = 1 )
Epinion ( c = 3 )
Epinion ( c = 5 ) web Google ( c = 1 ) web Google ( c = 3 ) web Google ( c = 5 )
Twitter ( c = 1 )
Twitter ( c = 3 )
Twitter ( c = 5 )
Fig 1 : The cost of intermediate c schedules at iterations of approx optimizer according to training sample I and test sample T .
Iterations051015Cost789101112131415TrainingTestIterations051015Cost75885995TrainingTestIterations051015Cost7576777879881828384TrainingTestIterations051015Cost45678910TrainingTestIterations051015Cost464855254565866264TrainingTestIterations051015Cost4647484955152535455TrainingTestIterations051015Cost161820222426283032TrainingTestIterations051015Cost151617181920212223TrainingTestIterations051015Cost1551616517175181851919520205TrainingTestIterations051015Cost212214216218220222224226228230232TrainingTestIterations051015Cost200205210215220225230235TrainingTestIterations051015Cost195200205210215220225230TrainingTestIterations051015Cost6065707580859095100TrainingTestIterations051015Cost56586062646668707274TrainingTestIterations051015Cost5657585960616263646566TrainingTest Probabilistic Hitting Set Paradigm
13 parameters 10 and π . Note that we start the generating process at time 0 . Hence , during the first few time steps the load of the generating process increases .
Based on our experiments , shown in Figure 2 , we observe that ( i ) a sample gathered during a very short time interval suffices to minimize the load ( and therefore the cost ) of the generating process , and ( ii ) after adapting to the new schedule , the effect of the perturbation disappears immediately ( see the Section 3.4 for theoretical upper bound ) . Finally , note that for each time t , we plot the loads LΓ ( t ) , and not the cost function . This is because the load is what we can observe ( which is a draw of random variables LΓ ( t) ) , as the cost function is the expected value of these random variables averaged over the time , and it explains the asymptotic behavior of the system .
( a ) Enron Email
( b ) Brightkite
( c ) Epinion
( d ) webGoogle
( e ) Twitter
Fig 2 : Perturbation , Sampling , and Adapting ( For details see Section 42 )
5 Conclusion
We formulate and study the Probabilistic Hitting Set Paradigm ( PHSP ) , a general framework for design and analysis of search and detection algorithms in large scale dynamic networks , that captures a wide range of applications . The goal in PHSP is to find an optimal probing schedule that captures the items as soon as possible ( based on their freshness or relevance ) .
In this work we studied the problem of finding optimal memoryless schedules for ( θ , c) PSHP and showed that the optimal probing schedule can be obtained efficiently via a non linear iterative method . In particular we provided a scalable iterative method for computing the probing schedule in MapReduce framework .
We also studied the sample complexity of the set of informed sets , and showed a sample of informed set gathered during a time interval of length O(log(n)/ 2 ) provides enough accuracy by using Chernoff bound arguments .
Time01000200030004000Load6080100120140160180200220240260Time01000200030004000Load406080100120140160Time01000200030004000Load150200250300350400450Time010002000300040005000Load60080010001200140016001800200022002400Time01000200030004000Load40050060070080090010001100120013001400 14
Ahmad Mahmoody and Eli Upfal
Finally , we applied our method on variety of real social networks , that illustrates the optimality and practicality of our solution .
References
1 . AlphaFlash Trader Automated trading based on economic events , http://www . alphaflash.com/product info/alphaflash trader
2 . Discovering The Web ’s Hidden Alpha ( 2014 ) , http://wwweaglealphacom/whitepaper_ pdf
3 . How Computers Trawl a Sea of Data for Stock Picks ( 2015 ) , http://wwwwsjcom/ articles/how computers trawl a sea of data for stock picks 1427941801 ? KEYWORDS=computers+trawl+sea
4 . Agumbe Suresh , M . , Stoleru , R . , Denton , R . , Zechman , E . , Shihada , B . : Towards optimal event detection and localization in acyclic flow networks . In : Proceedings of the 13th International Conference on Distributed Computing and Networking . pp . 179– 196 . ICDCN’12 , Springer Verlag , Berlin , Heidelberg ( 2012 ) , http://dxdoiorg/101007/ 978 3 642 25959 3_13
5 . Akoglu , L . , McGlohon , M . , Faloutsos , C . : oddball : Spotting anomalies in weighted graphs . In : Zaki , MJ , Yu , JX , Ravindran , B . , Pudi , V . ( eds . ) PAKDD ( 2 ) . Lecture Notes in Computer Science , vol . 6119 , pp . 410–421 . Springer ( 2010 )
6 . Aspnes , J . , Chang , KL , Yampolskiy , A . : Inoculation strategies for victims of viruses and the sum of squares partition problem . J . Comput . Syst . Sci . 72(6 ) , 1077–1093 ( 2006 )
7 . Berkhin , P . , Caruana , R . , Wu , X . ( eds. ) : Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , San Jose , California , USA , August 12 15 , 2007 . ACM ( 2007 )
8 . Bogdanov , P . , Faloutsos , C . , Mongiov`ı , M . , Papalexakis , EE , Ranca , R . , Singh , AK : Netspot : Spotting significant anomalous regions on dynamic networks . In : SDM . pp . 28– 36 . SIAM ( 2013 )
9 . Cataldi , M . , Di Caro , L . , Schifanella , C . : Emerging topic detection on twitter based on temporal and social terms evaluation . In : Proceedings of the Tenth International Workshop on Multimedia Data Mining . pp . 4:1–4:10 . MDMKDD ’10 , ACM , New York , NY , USA ( 2010 ) , http://doiacmorg/101145/18142451814249
10 . Chen , W . , Wang , C . , Wang , Y . : Scalable influence maximization for prevalent viral marketing in large scale social networks . In : Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining . pp . 1029–1038 . KDD ’10 , ACM , New York , NY , USA ( 2010 ) , http://doiacmorg/101145/18358041835934
11 . Chen , W . , Wang , Y . , Yang , S . : Efficient influence maximization in social networks . In : Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining . pp . 199–208 . KDD ’09 , ACM , New York , NY , USA ( 2009 ) , http://doi . acmorg/101145/15570191557047
12 . Cho , E . , Myers , SA , Leskovec , J . : Friendship and mobility : user movement in locationbased social networks . In : Apt´e , C . , Ghosh , J . , Smyth , P . ( eds . ) Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , San Diego , CA , USA , August 21 24 , 2011 . pp . 1082–1090 . ACM ( 2011 ) , http://doiacmorg/ 101145/20204082020579
13 . Christakis , NA , Fowler , JH : Social network sensors for early detection of contagious outbreaks . PLoS ONE 5(9 ) , e12948 ( 2010 )
14 . Dasgupta , A . , Ghosh , A . , Kumar , R . , Olston , C . , Pandey , S . , Tomkins , A . : The discoverability of the web . In : Proceedings of the 16th international conference on World Wide Web . pp . 421–430 . ACM ( 2007 )
15 . Dean , J . , Ghemawat , S . : Mapreduce : simplified data processing on large clusters . Commu nications of the ACM 51(1 ) , 107–113 ( 2008 )
16 . Delaney , A . : The Growing Role of News in Trading Automation ( Oct 2009 ) , http://wwwmachinereadablenewscom/images/dl/Machine_Readable_News_and_ Algorithmic_Trading.pdf
17 . Eberle , W . , Holder , LB : Discovering structural anomalies in graph based data . In : ICDM
Workshops . pp . 393–398 . IEEE Computer Society ( 2007 )
Probabilistic Hitting Set Paradigm
15
18 . Golovin , D . , Faulkner , M . , Krause , A . : Online distributed sensor selection . In : Proceedings of the 9th ACM/IEEE International Conference on Information Processing in Sensor Networks . pp . 220–231 . IPSN ’10 , ACM , New York , NY , USA ( 2010 ) , http://doiacmorg/ 101145/17912121791239
19 . Hardy , GH : Divergent series , vol . 334 . American Mathematical Soc . ( 1991 ) 20 . Hart , W . , Murray , R . : Review of sensor placement strategies for contamination warning systems in drinking water distribution systems . Journal of Water Resources Planning and Management 136(6 ) , 611–619 ( 2010 ) , http://ascelibraryorg/doi/abs/101061/ %28ASCE%29WR1943 54520000081
21 . He , J . , Liang , H . , Yuan , H . : Controlling infection by blocking nodes and links simultaneously . In : Chen , N . , Elkind , E . , Koutsoupias , E . ( eds . ) WINE . Lecture Notes in Computer Science , vol . 7090 , pp . 206–217 . Springer ( 2011 )
22 . Jung , K . , Heo , W . , Chen , W . : Irie : Scalable and robust influence maximization in social networks . arXiv preprint arXiv:1111.4795 ( 2011 )
23 . Kempe , D . , Kleinberg , J . , Tardos , E . : Maximizing the spread of influence through a social network . In : Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining . pp . 137–146 . KDD ’03 , ACM , New York , NY , USA ( 2003 ) , http://doiacmorg/101145/956750956769
24 . Kempe , D . , Kleinberg , J . , Tardos , E . : Influential nodes in a diffusion model for social networks . In : Proceedings of the 32Nd International Conference on Automata , Languages and Programming . pp . 1127–1138 . ICALP’05 , Springer Verlag , Berlin , Heidelberg ( 2005 ) , http://dxdoiorg/101007/11523468_91
25 . Kimura , M . , Saito , K . , Motoda , H . : Minimizing the spread of contamination by blocking links in a network . In : Fox , D . , Gomes , CP ( eds . ) AAAI . pp . 1175–1180 . AAAI Press ( 2008 )
26 . Kimura , M . , Saito , K . , Motoda , H . : Solving the contamination minimization problem on networks for the linear threshold model . In : Ho , TB , Zhou , ZH ( eds . ) PRICAI . Lecture Notes in Computer Science , vol . 5351 , pp . 977–984 . Springer ( 2008 )
27 . Kimura , M . , Saito , K . , Motoda , H . : Blocking links to minimize contamination spread in a social network . ACM Trans . Knowl . Discov . Data 3(2 ) , 9:1–9:23 ( Apr 2009 ) , http://doi . acmorg/101145/15148881514892
28 . Klimt , B . , Yang , Y . : Introducing the enron corpus . In : CEAS ( 2004 ) 29 . Krause , A . , Leskovec , J . , Guestrin , C . , VanBriesen , J . , Faloutsos , C . : Efficient sensor placement optimization for securing large water distribution networks . Journal of Water Resources Planning and Management 134(6 ) , 516–526 ( 2008 ) , http://ascelibrary.org/doi/ abs/10.1061/%28ASCE%290733 9496%282008%29134%3A6%28516%29
30 . Krause , A . , Guestrin , C . : Submodularity and its applications in optimized information gathering . ACM Trans . Intell . Syst . Technol . 2(4 ) , 32:1–32:20 ( Jul 2011 ) , http://doiacm org/101145/19897341989736
31 . Krause , A . , Guestrin , C . , Gupta , A . , Kleinberg , J . : Robust sensor placements at informative and communication efficient locations . ACM Trans . Sen . Netw . 7(4 ) , 31:1–31:33 ( Feb 2011 ) , http://doiacmorg/101145/19216211921625
32 . Krause , A . , Rajagopal , R . , Gupta , A . , Guestrin , C . : Simultaneous placement and scheduling of sensors . In : Proceedings of the 2009 International Conference on Information Processing in Sensor Networks . pp . 181–192 . IPSN ’09 , IEEE Computer Society , Washington , DC , USA ( 2009 ) , http://dlacmorg/citationcfm?id=16021651602183
33 . Kuhlman , CJ , Tuli , G . , Swarup , S . , Marathe , MV , Ravi , SS : Blocking simple and complex contagion by edge removal . In : Xiong , H . , Karypis , G . , Thuraisingham , BM , Cook , DJ , Wu , X . ( eds . ) ICDM . pp . 399–408 . IEEE ( 2013 )
34 . Latar , NL : The robot journalist in the age of social physics : The end of human journalism ?
In : The New World of Transitioned Media , pp . 65–80 . Springer ( 2015 )
35 . Leskovec , J . , Krause , A . , Guestrin , C . , Faloutsos , C . , VanBriesen , JM , Glance , NS : Cost effective outbreak detection in networks . In : Berkhin et al . [ 7 ] , pp . 420–429
36 . Leskovec , J . , Lang , KJ , Dasgupta , A . , Mahoney , MW : Community structure in large networks : Natural cluster sizes and the absence of large well defined clusters . Internet Mathematics 6(1 ) , 29–123 ( 2009 )
16
Ahmad Mahmoody and Eli Upfal
37 . Li , A . , Tang , L . : The complexity and approximability of minimum contamination problems . In : Ogihara , M . , Tarui , J . ( eds . ) TAMC . Lecture Notes in Computer Science , vol . 6648 , pp . 298–307 . Springer ( 2011 )
38 . Mathioudakis , M . , Koudas , N . : Twittermonitor : Trend detection over the twitter stream . In : Proceedings of the 2010 ACM SIGMOD International Conference on Management of Data . pp . 1155–1158 . SIGMOD ’10 , ACM , New York , NY , USA ( 2010 ) , http://doiacm org/101145/18071671807306
39 . McAuley , JJ , Leskovec , J . : Learning to discover social circles in ego networks . In : Bartlett , PL , Pereira , FCN , Burges , CJC , Bottou , L . , Weinberger , KQ ( eds . ) Advances in Neural Information Processing Systems 25 : 26th Annual Conference on Neural Information Processing Systems 2012 . Proceedings of a meeting held December 3 6 , 2012 , Lake Tahoe , Nevada , United States . pp . 548–556 ( 2012 ) , http://booksnipscc/papers/files/ nips25/NIPS2012_0272.pdf
40 . McKinney , W . : Structured Data Challenges in Finance and Statistics ( 2011 ) , http://www . slideshare.net/wesm/structured data challenges in finance and statistics
41 . Mitra , G . , Mitra , L . : The handbook of news analytics in finance , vol . 596 . John Wiley &
Sons ( 2011 )
42 . Noble , CC , Cook , DJ : Graph based anomaly detection . In : Getoor , L . , Senator , TE ,
Domingos , P . , Faloutsos , C . ( eds . ) KDD . pp . 631–636 . ACM ( 2003 )
43 . Ostfeld , A . , Uber , J . , Salomons , E . , Berry , J . , Hart , W . , Phillips , C . , Watson , J . , Dorini , G . , Jonkergouw , P . , Kapelan , Z . , di Pierro , F . , Khu , S . , Savic , D . , Eliades , D . , Polycarpou , M . , Ghimire , S . , Barkdoll , B . , Gueli , R . , Huang , J . , McBean , E . , James , W . , Krause , A . , Leskovec , J . , Isovitsch , S . , Xu , J . , Guestrin , C . , VanBriesen , J . , Small , M . , Fischbeck , P . , Preis , A . , Propato , M . , Piller , O . , Trachtman , G . , Wu , Z . , Walski , T . : The battle of the water sensor networks ( bwsn ) : A design challenge for engineers and algorithms . Journal of Water Resources Planning and Management 134(6 ) , 556–568 ( 2008 ) , http://ascelibrary . org/doi/abs/10.1061/%28ASCE%290733 9496%282008%29134%3A6%28556%29
44 . Papadimitriou , P . , Dasdan , A . , Garcia Molina , H . : Web graph similarity for anomaly de tection . J . Internet Services and Applications 1(1 ) , 19–30 ( 2010 )
45 . Richardson , M . , Agrawal , R . , Domingos , P . : Trust management for the semantic web . In :
The Semantic Web ISWC 2003 , pp . 351–368 . Springer ( 2003 )
46 . Sun , J . , Faloutsos , C . , Papadimitriou , S . , Yu , PS : Graphscope : parameter free mining of large time evolving graphs . In : Berkhin et al . [ 7 ] , pp . 687–696
47 . Tang , Y . , Xiao , X . , Shi , Y . : Influence maximization : Near optimal time complexity meets practical efficiency . arXiv preprint arXiv:1404.0900 ( 2014 )
48 . Wolf , JL , Squillante , MS , Yu , P . , Sethuraman , J . , Ozsen , L . : Optimal crawling strategies for web search engines . In : Proceedings of the 11th international conference on World Wide Web . pp . 136–147 . ACM ( 2002 )
