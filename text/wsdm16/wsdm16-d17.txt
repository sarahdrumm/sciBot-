Beyond Ranking : Optimizing Whole Page Presentation
∗
Yue Wang1
†
, Dawei Yin2 , Luo Jie3
, Pengyuan Wang2 , Makoto Yamada2,4 ,
Yi Chang2 , Qiaozhu Mei1,5
1Department of EECS , University of Michigan , Ann Arbor , MI , USA
2Yahoo Labs , 701 First Avenue , Sunnyvale , CA , USA
3Snapchat , Inc . , 64 Market St , Venice , CA , USA
4Bioinformatics Center , Institute for Chemical Research , Kyoto University , Uji , Kyoto , Japan
5School of Information , University of Michigan , Ann Arbor , MI , USA
1,5{raywang , qmei}@umich.edu
2{daweiy , pengyuan , yichang}@yahoo inc.com
3luojroger@gmailcom 2,4myamada@kuicrkyoto uacjp
ABSTRACT
Modern search engines aggregate results from different verticals : webpages , news , images , video , shopping , knowledge cards , local maps , etc . Unlike “ ten blue links ” , these search results are heterogeneous in nature and not even arranged in a list on the page . This revolution directly challenges the conventional “ ranked list ” formulation in ad hoc search . Therefore , finding proper presentation for a gallery of heterogeneous results is critical for modern search engines . We propose a novel framework that learns the optimal page presentation to render heterogeneous results onto search result page ( SERP ) . Page presentation is broadly defined as the strategy to present a set of items on SERP , much more expressive than a ranked list . It can specify item positions , image sizes , text fonts , and any other styles as long as variations are within business and design constraints . The learned presentation is content aware , ie tailored to specific queries and returned results . Simulation experiments show that the framework automatically learns eye catchy presentations for relevant results . Experiments on real data show that simple instantiations of the framework already outperform leading algorithm in federated search result presentation . It means the framework can learn its own result presentation strategy purely from data , without even knowing the “ probability ranking principle ” .
Categories and Subject Descriptors
H33 [ Information Systems ] : Information Search and
Retrieval ; I51 [ Pattern Recognition ] : Model
∗This work was done when the first author was on an in†This work was done when the third author was working at ternship at Yahoo! Labs .
Yahoo Labs .
1 .
INTRODUCTION
A decade ago , search engines returned “ ten blue links ” . Result presentation was straightforward : ranking webpages by estimated relevance . It naturally saves user effort as she scans down the list , hopefully hitting the desired information at top ranks . This “ probability ranking principle ” was long envisioned in the 1970s [ 36 ] , and later confirmed by eyetracking studies [ 20 , 19 ] and search log analysis [ 24 , 15 ] .
Today ’s search engines return far richer results than “ ten blue links ” . Aside from webpages , results can also include news , images , video , shopping , structured knowledge , and local business maps . Each specific corpus is indexed by a vertical search engine ; they are federated to serve the user ’s information need . Unlike “ ten blue links ” , vertical search results have different visual appearance , layouts and sizes . They span across multiple columns on the page , not restricted in the mainline list ( Figure 1 ) .
Federated search results have been transforming user interaction patterns on search result pages ( SERPs ) . Human eyeballs are spontaneously attracted by graphical results , causing a significant attention bias known as the vertical bias [ 12 , 26 , 31 ] . More interestingly , blue links surrounding a vertical result are also examined with increased probability [ 12 ] . In the presence of vertical results , user satisfaction towards an entire SERP cannot be reliably inferred from preference judgments for pairs of results [ 6 ] .
These observations indicate that users do not sequentially scan results returned by federated search . Although the conventional “ ranked list ” formulation can still be used for federated search result presentation [ 5 , 4 ] , it is essentially a first order approximation of the problem .
In this paper , we propose a novel framework that learns the optimal presentation for heterogeneous search results on SERP . Page presentation is broadly defined as the strategy to present a set of heterogeneous items on SERP , much more expressive than a ranked list . It can specify item positions , image sizes , text fonts , or any other styles as long as changes of these elements are allowed by business constraints 1 and page design templates . The goodness of a presentation is measured by user satisfaction metric : better presentation will make the user happier . The framework first learns a scoring function that maps search results and their presen
1For example , sponsored results must be placed on the top .
103 • Experiments on synthetic and real data demonstrate that the proposed framework is promising in solving the new problem .
2 . PROBLEM FORMULATION
The problem statement of page presentation optimization is as follows : “ given search results to be displayed on a page , to find the optimal presentation that maximizes user satisfaction ” . We assume the following setting : search engine returns a set of results upon receiving a query , and renders the items on SERP according to some presentation strategy . As the SERP is shown to the user , she interacts with it and get certain degree of satisfaction . Now let us define some important concepts in our setting :
Definition 1 ( Page Content ) : Page content is the set of search results to be displayed on a page . Each search result is an item . Upon receiving user ’s query , search engine backend returns a set of k items . Each item is represented 2 . Note that different users and different as a vector xi queries will produce different sets of items , so xi also encodes information from actual users and queries . Page content is represented as concatenation of k item vectors : x = ( x k ) . The domain of x is defined by all possible page content returned by the backend , denoted as X . i ,··· , x
1 ,··· , x
Definition 2 ( Page Presentation ) : Page presentation defines the way in which page content x is displayed , such as position , vertical type , size , and color . It is encoded as a vector p . The domain of p is defined by all possible page presentations permitted by business and design constraints , denoted as P .
Definition 3 ( Search Result Page , SERP ) : When page content x is put on page according to presentation strategy p , a search result page ( SERP ) is generated . In other words , content x and presentation p uniquely determine a SERP . It is represented as a tuple ( x , p ) ∈ X × P . Definition 4 ( User Response ) : User response includes her actions on SERP , such as number of clicks , positions of clicks , dwell time of clicks , and time to first click . This information is encoded in a vector y . The domain of y is defined by all possible user responses , denoted as Y .
Definition 5 ( User Satisfaction ) : The user experiences certain degree of satisfaction as she interacts with the SERP . We assume that user satisfaction can be calibrated as a real value s ∈ R : larger value of s means higher satisfaction .
The user response is a strong indicator for satisfaction . Intuitively , if a user opened the SERP , clicked on the top result right away , then spent long time dwelling on that result , she was highly likely to be happy with the result . With definitions above , we formulate our problem :
Page Presentation Optimization is to find the presentation p ∈ P for a given page content x ∈ X , such that when the SERP ( x , p ) is presented to the user , her satisfaction score s is maximized .
2Throughout the paper we use bold lowercase letters for column vectors .
Figure 1 : Modern search engine result page . tation on SERP to user satisfaction metric . Then , given search results of a new query , the framework computes a presentation that maximizes user satisfaction .
The framework is quite general . First , practitioners can flexibly define the scope of page presentation . It can encode item positions ( both horizontal and vertical ) as well as element styles , such as image sizes and text fonts . It naturally encompasses ranked list as a special case . Second , different application scenarios can adopt different user satisfaction metric . It is not limited to click based metric , but can also take other interactive behaviors into account , such as dwelling time and time to first click . Lastly , the framework can potentially be instantiated in other interactive search scenarios , such as presenting search results on mobile and tablet devices , displaying multimedia feeds in online social networks , and arranging items on retailing websites .
We conduct experiments on both synthetic and real data to demonstrate the potential power of the proposed framework . Simulation experiments show that the framework can adapt to different types of attention bias and learn to present relevant results to catch user ’s eyeballs . This means our approach directly targets the new challenge brought by federated search , where users may not scan the results sequentially , and results are not in a ranked list . In real data experiments , simple instantiations of the framework outperform the leading algorithm in federated search result ranking . This is encouraging , because ranking algorithms use the probability ranking principle in its result presentation , while our framework does not even know the existence of it . Nevertheless , it learns its own result presentation principle purely from data and is able to deliver the state of the art performance .
Our main contribution is summarized as follows : • We formulate a new problem , whole page presentation optimization , which extends the homogeneous document ranking in ad hoc search ;
• We propose a general framework that computes the optimal presentation for federated search results .
AdsNewsImageWebVideoKnowledge cardAds104 If we assume that there exists a scoring function F : X × P → R that maps SERP ( x , p ) to user satisfaction score s , then page presentation optimization problem can be formally written as
3.2 Learning Stage
The core of page presentation optimization is to estimate the scoring function s = F ( x , p ) . We might consider two approaches : max p∈P F ( x , p ) , subject to constraints on presentation p .
The problem of page presentation optimization is both new and challenging . It is new because page presentation can be flexibly defined , which opens up possibility to learn brand new ways to display information . Retrieval and recommender systems typically use a ranked list for displaying homogeneous content . As heterogeneous results are weaved onto webpages , it is critical to present them in a proper manner to maximize user ’s utility . The problem is challenging because it is rather unclear how to find the scoring function that maps the entire SERP ( content and presentation ) to user satisfaction . We propose our solution framework in the next section .
3 . PRESENTATION OPTIMIZATION
FRAMEWORK
We propose to solve page presentation optimization using a supervised learning approach . This section sets up a general framework for our approach , including data collection methodology , design of scoring function F ( ·,· ) , the learning and optimization stages . In the next section , we describe actual instantiations of the framework . 3.1 Data Collection Through Exploration
Supervised machine learning needs labelled training data . The caveat in data collection here is that normal search traffic cannot be used as the training data to learn the scoring function F ( x , p ) . This is because in normal search traffic , search engine has a deterministic policy to present page content x , which is controlled by existing model/rules within the system . In other words , page presentation p is uniquely determined given page content x . However , we expect the model F to tell us user satisfaction as we search through different page presentations . Confounding between x and p will bias the learned model , which will be a serious problem . To eliminate confounding , we allocate “ presentation exploration bucket ” to do randomized experiments . For each request in the bucket , we organize page content with random page presentation . Here “ random ” means to uniformly draw presentation strategies within business and design constraints , such that user experience is not hurt too much . Further , the presentation exploration traffic is controlled within a very small amount so as not to affect overall quality of the search service . Data collected in this way allow unbiased estimation of the scoring function .
In cases that showing random exploration results to the users is not desired , it would also be possible to either hire human annotators to label the page , or collect data from multiple buckets with different fixed presentation strategy as every Internet company is doing for testing their UI changes . Since we have already developed a good data collection through exploration framework in our production system , we choose to take this approach for data collection .
( I ) Direct approach : Collect page wise user satisfaction ratings and directly model the dependency between SERP and user satisfaction . The dependency path is “ ( x , p ) − s ” .
( II ) Factorized approach : First predict user response y on SERP , then find a function that measure user satisfaction from these responses . The dependency path is “ ( x , p ) − y − s ” .
Approach ( I ) is straightforward . However it is very difficult , particularly at a large scale , to obtain explicit user rating s towards the entire SERP . To construct such data set , we would have needed substantial amount of observations and human annotation to overcome training data sparseness .
Approach ( II ) takes two steps . The first step is to predict user responses on a given page ; the second step is to measure user satisfaction based on her page wise response . Introducing user response variable y permits a separation of concerns . On the one hand , user response on a page is a direct consequence of interacting with the page . On the other hand , user satisfaction is typically estimated from user responses only , eg using total number of clicks , or long dwell time . In Approach ( II ) , the complex dependency in F ( ·,· ) is decomposed into two relatively independent factors . Furthermore , on a practical note , Approach ( II ) is more realistic for current Web technology because user response on SERP can be easily collected via Javascript , whereas explicitly asking the users to evaluate the whole page is very uncommon . Therefore , we adopt the factorized approach .
In factorized approach , the first step is to learn a user response model y = f ( x , p ) , ∀ x ∈ X , p ∈ P .
This is a supervised learning task ; the actual form of f ( x , p ) can be chosen flexibly . We can simply build one model for each component yi in y , or we can jointly predict all components of y using structured output prediction [ 10 ] . In any case , user ’s responses on the page depends on both the content ( whether it is relevant , diverse , or attractive ) and the presentation ( whether it is close to the top , around a graphical block , or shown in big size ) .
The second step is a utility funciton which defines a user satisfaction metric s = g(y),∀ y ∈ Y .
Finding the right user satisfaction metric based on page wise user responses is not the focus of this paper , and can itself be a substantial research topic in interactive information systems [ 21 , 30 , 38 ] . Indeed , practitioners often heuristically define the metric as aggregation of fine grained user responses , such as click through rates , long dwell time clicks , time to first click .
Finally , our scoring function for the entire SERP is s = F ( x , p ) = ( g ◦ f )(x , p ) = g(f ( x , p) ) .
105 3.3 Optimization Stage
We compute the optimal presentation p∗ given content x by solving the following optimization problem : max p∈P g(f ( x , p) ) , subject to constraints on presentation p .
Computational cost of this optimization problem depends on actual form of the objective function F = g ◦ f and the constraints on presentation p . In the next section we show that for certain instantiations of f and g , p∗ can be computed quite efficiently .
4 .
INSTANTIATIONS OF PRESENTATION OPTIMIZATION FRAMEWORK
This section describes instantiations of the framework , including feature representation , user satisfaction metric , two user response models and their learning and optimization stages . We conclude this section by showing that the framework encompasses learning to rank . 4.1 Features
Both content and presentation on a SERP are represented in a feature vector , which will be the input to user response models . Content Features
Content features contain information of the query and corresponding search results , similar to those used in learning to rank . We adopt the same content features as used in [ 23 ] to facilitate direct comparison in experiments ( Section 6 ) : • Global result set features : features derived from all returned results . They indicate the content availability of each vertical .
• Query features : lexical features such as the query unigrams , bigrams and co occurrence statistics . We also use outputs of query classifiers , and historical session based query features , etc .
• Corpus level features : query independent features derived for each vertical and web document such as historical click through rates , user preferences , etc .
• Search result features : extracted from each search result . A list of statistical summary features such as relevance scores and ranking features of individual results . For some verticals , we also extract some domain specific meta features , such as if the movie is on screen and if the movie poster is available in the movie vertical , and the number of hits for the news articles from the news vertical in the last few hours .
Presentation Features
Presentation features encode the way in which search results are displayed on SERP , which are novel features in our framework . Concrete examples include :
• Binary indicators : whether to show an item on a position . The scheme can encode positions in a wireframe , such as a list or multi column panels . Let there be k positions in the frame , and k items to be displayed . Let i be the index of items , j be the index of positions , 1 ≤ i , j ≤ k . The presentation of item i , pi , is a 1 of k binary encoding vector . If document i is placed at position j , then the j th component of pi is 1 and all others are 0 . In this case we denote the value of pi as pij = 1 . The page presentation k ) consists of k × k binary indicap = ( p tor variables , essentially encoding the permutation of k objects .
1 ,··· , p
• Categorical features : discrete properties of page items , eg , multimedia type of an item ( shown as text or image ) , typeface of a textual item ;
• Numerical features : continuous properties of page items , eg brightness and contrast of a graphical item .
• Other features : eg certain interactions between page content and presentation may affect user response , such as “ a textual item immediately above a graphical item ” .
We use two types of presentation features in real data experiments . We encode positions of items with binary indicators . For the local search results , we encode presentation size as a categorical feature ( “ single ” vs . “ multiple ” entries ) . 4.2 User Satisfaction Metric
We assume that user satisfaction metric g(y ) is in the form of weighted sum of components in y : g(y ) = c y .
In experiments , we use the click skip metric for k items [ 23 ] : k g(y ) = yi , i=1 where yi = 1 if item i is clicked , and yi = −1 if item i is skipped and some item below is clicked . A skip often indicates wasted inspection , so we set it to be a unit of negative utility . This metric strongly prefers adjacent clicks at top positions . 4.3 User Response Models
We use two models for predicting page wise user response . The first model takes as features quadratic interaction between content and presentation . It permits an efficient optimization stage . The second model uses gradient boosted decision trees to capture more complex , nonlinear interaction between content and presentation . We expect it to generate improved performance . Quadratic Feature Model
First , let us consider a simple instantiation of user response model that has efficient solution in the optimization stage . Since it uses quadratic interaction features between x and p , we call it Quadratic Feature Model .
Assume there are k positions for k items . Page content x is the concatenation of k item vectors ; page presentation is encoded using binary indicators , p ∈ {0 , 1}k×k , as defined in Section 41 The model also contains fully interaction between x and p as features . Let vec(A ) denote the row vector containing all elements in matrix A , taken column by column , left to right . The augmented feature vector φ for Quadratic Feature Model is :
φ
= ( x
, p
, vec(xp
) ) .
106 Let y ∈ Rk be the user response vector ; each component yi is a user response ( eg click or skip ) on item i . A linear model fi is used to predict each yi in y : i p + x yi = fi(φ ) = w i φ = u i x + v
Qip .
( 1 ) ui , vi , and Qi are coefficients for content only , presentationonly , and content presentation quadratic interaction features , respectively . The coefficients wi = {ui , vi , Qi} can be estimated using regularized linear regression . To avoid overfitting , we regularize the L2 norm of ui and vi , and further impose low rank regularization on Qi to handle the sparsity issue of quadratic features .
In total , we will have k such models , each predicting one yi in y . To group the k models in notation , let us write coefficients as U = ( u1,··· , uk ) , V = ( v1,··· , vk ) , Q = diag(Q1,··· , Qk ) , and “ copy ” x and p k times to get the matrix X = diag(x,··· , x ) and the vector t = ( p,··· , p ) . To clarify dimensionality , if x ∈ Rn , p ∈ Rm , then U ∈ Rk×n , V ∈ Rk×m , X ∈ Rk×nk , Q ∈ Rnk×mk , and t ∈ Rmk . The user response model can be written as y = f ( x , p ) = Ux + Vp + XQt .
Denote user satisfaction metric as g(y ) = cy . Then the scoring function F = g ◦ f is
F ( x , p ) = g(f ( x , p ) ) where a = Vc +k
Ux + a
= c i=1 ciQ i x is a known vector .
To this end , the optimization stage is to find the p that maximizes ( 2 ) subject to the constraints on p . Since page content x is given , the first term in ( 2 ) is a constant and can be dropped . The second term ap is a linear term of p . Since p ∈ {0 , 1}k×k encodes a k permutation , Each component in a ∈ Rk×k represents the gain of user satisfaction if item i is placed in position j , 1 ≤ i , j ≤ k . Therefore , the optimization problem reduces to maximum bipartite matching , a special case of linear assignment problem . It can be efficiently solved by Hungarian algorithm [ 25 ] with time complexity O(|p|3 ) = O(k6 ) . On a single core computer with 2GHz CPU , the problem can be solved within 10 milliseconds for k = 50 items . Gradient Boosted Decision Tree Model
In order to capture more complex , nonlinear interaction between content x and presentation p , we replace the quadratic feature model fi in previous section with a gradient boosted decision trees model hGBDT . Gradient boosted decision trees ( GBDT ) is a very effective method for learning nonlinear functions [ 18 ] . i
Our feature vector is
φ
= ( x
, p
) , and each user response yi in y is predicted by a GBDT model :
The user satisfaction metric is g(y ) = cy =k k
In optimization stage , since each hi is now a nonparametric model , we cannot get the analytical form of F ( x , p ) = ( x , p ) in terms of p . That is , the optimization yi = hGBDT i=1 ciyi .
( x , p ) . i=1 cihGBDT i i
= c
Ux + c
Vp + c
XQt p
( 2 ) h(xi ) = fi(xi , pi1 = 1 ) . over p is intractable . Nevertheless , in realistic settings , the search space of p is usually pruned down to tens of possible values by business and design constraints . We implement parallel enumeration to quickly find the optimal presentation that maximizes user satisfaction . 4.4 Special Case : Learning to Rank
When we restrict page presentation to be a ranked list , and assume that users are more satisfied if more relevant results are placed at top ranks , then presentation optimization reduces to the traditional ranking problem . We point out this connection to demonstrate the generality of the proposed framework .
The instantiation is as follows . We use binary indicators in Section 4.1 to represent the ranked list . Let user response y decompose into k components , each representing the user ’s utility of seeing result i at rank ji . Let user response model f ( x , p ) decompose into k real valued component , each only taking as input xi and its rank ji . So we have f ( x , p ) = f ( x1,··· , xk , p1,··· , pk )
= ( f1(x1 , p1),··· , fk(xk , pk ) )
= ( f1(x1 , p1j1 = 1),··· , fk(xk , pkjk = 1 ) )
.
( 3 )
Typically , the ranking function h(xi ) of result i is positionindependent . It can also be interpreted as the score of result i seen on the top rank ( ji = 1 ) . That means
Furthermore , traditional ranking problem assumes that the utility of a result is discounted by a factor wj if it is ranked at position j . wj > 0 is a decreasing function of j . Eg in discounted cumulative gain ( DCG ) , wj =
1 log2(1 + j )
.
The discounting assumption implies : fi(xi , pij = 1 ) = wjfi(xi , pi1 = 1 )
= wjh(xi ) .
( 4 )
Combining ( 3 ) and ( 4 ) , user response model is realized as f ( x , p ) = ( wj1 h(x1),··· , wjk h(xk ) )
, where h(· ) is the ranking function . User satisfaction is measured by the quality of ranked list , which accumulate the gain at each position : k k g(y ) = yi = wji h(xi ) . i=1 i=1
Clearly , maximum user satisfaction g(y ) is always achieved by sorting the results by descending relevance scores . This is the default presentation strategy of learning to rank .
5 . SIMULATION STUDY
We demonstrate potential capability of presentation optimization framework by simulation . We use synthetic dataset so that we know the “ ground truth ” mechanism to maximize user satisfaction , and we can easily check whether the algorithm can indeed learn the optimal page presentation to maximize user satisfaction . We have two goals in this study :
107 ( 1 ) We show that the framework enables general definition of page presentation .
( 2 ) We use both position bias and item specific bias to show that the framework can automatically adapt to user interaction habits .
5.1 Overview
We first give a brief overview of simulation workflow . The simulated “ presentation exploration bucket ” will generate a page containing a set items with random presentation . Every time a new page is generated , each item is assigned some amount of reward ( eg relevant information ) drawn from an underlying distribution . The simulated “ user ” will have a certain type of attention bias : ( 1 ) position bias , in which more attention is paid to certain region of the page than elsewhere ( Figure 2a ) ; ( 2 ) vertical bias , or item specific bias , in which more attention is attracted by a specific type of item and its surrounding area ( Figure 2b ) .
Then , the “ user ” will “ interact ” with the page ( x , p ) : k binary values are drawn from k Bernoulli distributions , and recorded as a user response vector y ∈ {0 , 1}k . if item i is examined , yi = 1 , the user receives a reward xi . The user satisfaction equals the sum of reward of examined items . We generate 100,000 pages to train the Quadratic Feature Model described in Section 43 5.3 Discussion
To visualize the learned optimal presentation , we pick a random x and compute the corresponding optimal presentation p∗ , then arrange the xi ’s according to p∗ . A page is visualized as a heat map of xi ’s rewards . Ideally , the items with higher reward ( “ better content ” ) should be placed onto the position with higher probability of user attention .
( a ) Position bias [ 2 ]
( b ) Vertical bias [ 1 ]
Figure 2 : Different types of user attention bias .
An “ interaction ” happens when the “ presentation exploration bucket ” generates a page and the “ user ” examines it with attention bias . When the user examines an item , she receives the corresponding reward . User satisfaction towards the page is the sum of rewards . The page content , presentation , as well as the examined items and positions ( user responses ) , become data that the framework learns from . Finally , we test if the framework successfully learned user ’s attention bias . Given a new set of items , we expect to see that the framework will place items with higher rewards to positions with more concentrated attention to achieve maximum user satisfaction . Therefore , to visualize the model ’s current belief in user attention bias , we can plot the distribution of item rewards on the page . 5.2 Data Generating Process On the “ search engine ” side , a page ( either 1 D list or 2 D grid ) contains k positions . The page content x = ( x1,··· , xk ) , xi ∼ N ( µi , σ ) represents intrinsic rewards of k items . We set k = 10 for 1 D list and k = 7 × 7 for 2 D grid . µi ’s are random numbers drawn from in [ 0 , 1 ] , σ = 01 The page presentation p is drawn from length k permutations uniformly at random . The whole page is represented as ( x , p ) . On the “ user ” side , attention bias is simulated as follows : Position bias : whether to examine position j is a Bernoulli random variable with parameter pj . A real life example is the top down position bias , commonly observed when the user interacts with a ranked list .
Item specific bias : whether to examine item i is a Bernoulli random variable with parameter pi . A real life example is the vertical bias , commonly observed when the user interacts with a page that contains vertical search results ( images , videos , maps , etc ) .
( a ) Position bias
( b ) Ideal presentation
( c ) Learned presentation
Figure 3 : Top position bias and presentation on 1 D list .
( a ) Position bias
( b ) Ideal presentation
( c ) Learned presentation
Figure 4 : Top left position bias and presentation on 2 D canvas .
( a ) Position bias
( b ) Ideal presentation
( c ) Learned presentation
Figure 5 : Two end position bias and presentation on 2 D canvas .
Figure 3 , 4 , and 5 visualize the presentation results under various position biases . We can see that the algorithm indeed learns to put “ better content ” to positions with more user attention . Because the definition of page presentation is general , it is able to handle both 1 D list and 2 D grid . Furthermore , it can capture complicated distribution of position bias on a 2 D canvas : the top left position bias in Figure 4 , and the top bottom position bias in Figure 5 .
Figure 6 visualizes the result under item specific bias . This is an interesting case where an item on the page is very eyecatchy , and it also attracts user ’s attention to its surrounding items ( eg , an image attracts user ’s eyeballs on itself as well as its caption and description text ) . Also , suppose that for items farther away from that eye catchy item , the user ’s
123456789 000510000510000510108 ( a ) s = 4.07
( b ) s = 4.92
( c ) s = 5.41
Figure 6 : Item specific bias . s : page wise user satisfaction . When a specific item ( eg image ) attracts user attention to not only itself but also its surrounding results , then the page wise reward is highest when the vertical is placed at the center of the page . attention drops further down . Then the optimal presentation strategy is to place the item on the center of the page , so that the whole page delivers the most reward . In Figure 6 , we see that user satisfaction value s is highest when the item ( the dark red region ) is centered on the page .
6 . REAL DATA EXPERIMENTS
We demonstrate the effectiveness of page presentation optimization framework by conducting experiments on the realworld data set collected via a commercial search engine . 6.1 Data Collection
We use a very small fraction of search traffic as the presentation exploration buckets . The data was collected through the year 2013 . Vertical search results whose presentation are explored include news , shopping , and local listings . In the exploration buckets , the order of Web results are kept untouched and verticals are randomly slotted into allowed positions with uniform probability . Randomly generated SERPs are not influenced by any ranking algorithm in the system . As pointed out in Section 3.1 , this is required to eliminate page content confounding when training models . The exploration SERP is then presented to the user who interacts with it in a normal fashion . Users response on the SERP , along with page wise content information like the query , document features from backends , are logged . 6.2 Methods
We use two pointwise ranking models as baseline method . They are trained using the content features as described in Section 41 The first baseline method has been adopted in production ( Logit Rank ) [ 23 ] . It estimates a logistic regression model for each vertical result ( including web result ) : yi = σ(w i xi ) , where yi is a binary target variable that indicates whether the result is clicked ( yi = +1 ) or skipped ( yi = −1 ) as described in Section 4.2 , and σ(· ) is the sigmoid link function rescaled to [ −1 , 1 ] .
The second baseline method uses gradient boosted decision trees to learn a pointwise ranking function ( GbdtRank ) . This is essentially replacing the logistic regressor in Logit Rank with a GBDT regressor : yi = hGBDT i
( xi ) .
Table 1 : Match rate between random exploration presentation p and predicted optimal presentation p∗ . “ Until Web1 ” means that p and p∗ encode the same presentation above the 1st webpage result .
Logit Rank Quad Pres
Until Web1 Until Web2 Until Web3 30.85 % 33.42 %
68.68 % 71.63 %
46.76 % 50.68 %
Feature Model ( Quad Pres ) and the GBDT Model ( GbdtPres ) . They use page wise information ( x , p ) to predict the user response vector , ie the vector of clicks and skips .
In implementation , we use Vowpal Wabbit [ 27 ] to learn logistic regression models , and XGBoost [ 13 ] to learn the gradient boosted decision tree models . The hyperparameters of the models are tuned on a small holdout data set . 6.3 Evaluation
We use half of the exploration SERP as training set ( January – June ) , the rest as test set . It contains hundreds of millions of pageviews and was collected from real search traffic . Compared to standard supervised learning setup , it is difficult to do an unbiased offline performance evaluation because of the interactive nature of the task ( see Section 4.3 in [ 23] ) . This is because the offline data ( x(n ) , p(n ) , y(n ) ) is collected using a particular logging policy , so we only observe user response y(n ) for a specific page presentation p(n ) . In offline evaluation , when the algorithm is given page content x(n ) , it may output a presentation p∗(n ) = p(n ) , for which we do not observe user response , hence cannot evaluate its goodness . To address this problem , we use an offline policy evaluation method proposed by Li et al . [ 28 ] for evaluating online recommendation systems . It is simple to implement and provides an unbiased performance estimate , thanks to data collected through random exploration . Given a stream of events ( x(n ) , p(n ) , Pr(p(n) ) , y(n ) ) collected through random exploration , where Pr(p(n ) ) is the probability for the SERP ( x(n ) , p(n ) ) to be generated from uniform random exploration , the average user satisfaction for N offline events can be computed as
N
¯s =
1 N g(y(n))1{p∗(n)==p(n)}
, n=1
Pr(p(n ) ) where 1{·} is the indicator function , and g(y(n ) ) is user satisfaction towards SERP n . This means the algorithm is evaluated on those exploration SERPs whose presentation matches what is chosen by the algorithm ; otherwise the SERP is discarded in offline evaluation .
As the match goes deeper down the page , the match rate decreases ( Table 1 ) . If we require exact match between predicted p∗(n ) and actual p(n ) , a large fraction of test set will be discarded and the performance estimates tend to have large variance hence unreliable . Our evaluation only focuses on vertical results shown above the first , second , and third webpage result . Note that the first webpage result is not always on top rank ; the top rank is frequently occupied by the vertical results . 6.4 Results
We evaluate the two instantiations of presentation optimization framework described in Section 4.3 : the Quadratic
Table 2 shows the average page wise user satisfaction . It is encouraging to see that whole page optimization meth
010509109 ods outperform ranking methods , because ranking methods utilize probability ranking principle to rank results by relevance , which assumes a top down position bias . Quad Pres and Gbdt Pres do not make this assumption , but learns its own result presentation principle purely from data . The reason that GBDT models work better than logistic regression models , mainly because logistic regression assumes linear decision boundary , while GBDT is capable of modeling nonlinear decision boundary .
As the match goes deeper down the page , the local ranking algorithms show decreased performance in terms of CTR . This is because the local ranking methods tend to greedily put the high CTR items on top of the page , but ignores the content on the entire page . In contrast , the page presentation algorithms , especially Gbdt Pres , still get good CTR on News and Multi local verticals , which takes larger coverage . This is attributed to the fact that they model user response over the entire page .
Table 2 : Average user satisfaction ( ×10−3 ) .
Logit Rank Gbdt Rank Quad Pres Gbdt Pres
Until Web1 Until Web2 Until Web3 1.89 2.22 5.37 8.24
0.25 2.18 0.62 2.68
1.79 3.68 6.39 6.72
Table 3 : CTR , match until Web1
News Coverage 0.46 % Logit Rank 21.05 % Gbdt Rank 23.28 % Quad Pres 21.97 % Gbdt Pres 22.34 %
Shopping 0.02 %
0.02 % 40.79 % 11.58 % 53.32 % 38.26 % 49.85 % 47.16 % 46.15 % 48.12 %
S . Local M . Local 0.71 % 30.02 % 52.27 % 39.93 % 49.18 %
Note that in our definition of user satisfaction metric g(y ) , a skip causes negative utility ( yi = −1 ) . The fact that Quad Pres and Gbdt Pres generally work better than the baseline methods is because they take into consideration the retrieved items , the page presentation , and their interaction on the entire SERP , not just a single result . The presentation blind models Logit Rank and Gbdt Rank always want to put on top the results that will most probably gets clicked . However , for certain queries people might intentionally skip the graphical results ( eg , when shopping ads are shown when the search intent is in fact informational ) . In such cases , a click tends to happen below the top rank . In contrast , presentation optimization methods will consider both the result and its position on the page . That leads to more sensible arrangement of results . We see that Gbdt Pres attracts more clicks and has less skips when we evaluate deeper down the SERP .
Table 3 , 4 , and 5 shows the click through rate ( CTR ) above Web1 , Web2 , and Web3 , respectively . “ S . Local ” means a single entry of local business result ( such as restaurants ) ; “ M . Local ” means multiple entries of local business results . They are the same vertical/genre results presented in different sizes . In terms of CTR , ranking methods have very strong performance because they are directly optimized for high CTR . However , whole page optimization methods still achieve competitive or sometimes better CTR by taking into account page wise information .
It is interesting to see that for News vertical , there is not much help to know about other results on the SERP , neither their presentation . In contrast , knowing page wise results helps improve the CTR of top ranked local listings by a large margin . A possible explanation is that news , more like general webpages , contain rich text information and their content relevance can be readily modeled by standard ranking functions . On the other hand , local listings are in drastically different nature compared to webpages and news , therefore knowing the complementary information from other webpage results helps predicting the click/skip patterns . We can also observe small improvements in CTR of the shopping results . Since the shopping results shown on the top are most likely to be skipped , the algorithm learns to become extremely conservative in showing shopping verticals on top . This leads to a much smaller coverage of shopping results in the entire traffic .
Table 4 : CTR , match until Web2
News Coverage 2.0 % Logit Rank 16.44 % Gbdt Rank 16.31 % Quad Pres 14.78 % Gbdt Pres 16.21 %
Shopping 0.11 %
0.03 % 23.71 % 18.51 % 30.39 % 36.73 % 13.57 % 23.39 % 40.83 % 33.18 %
S . Local M . Local 2.3 % 8.92 % 23.11 % 27.53 % 35.23 %
Table 5 : CTR , match until Web3
News Coverage 3.8 % Logit Rank 14.52 % Gbdt Rank 12.51 % Quad Pres 11.45 % Gbdt Pres 14.11 %
Shopping 0.18 %
0.11 % 21.48 % 13.80 % 42.96 % 24.93 % 12.88 % 15.47 % 36.00 % 24.72 %
S . Local M . Local 3.4 % 9.65 % 22.42 % 24.38 % 30.66 %
7 . RELATED WORK
As a general framework for search result presentation , this work draws on various aspects in IR research . It is inspired by the task of federated search result presentation . It extends traditional “ ranked list ” formulation to more a general notion of “ presentation ” . Finally , it delivers optimal presentation by learning from interactive search logs . 7.1 Document Ranking in Retrieval
Document ranking has long been the core problem of ad hoc retrieval . Given a query , the retrieval system returns a list of documents ranked by decreasing probability of relevance . The presentation is optimal with respect to the user ’s effort when she sequentially and independently examines results from top to bottom [ 36 ] . Design and evaluation of document ranking functions are at the central stage of IR research , dating from vector space models [ 37 ] and language modeling ranking functions [ 40 ] to more recent machine learning ranking [ 29 ] and top document reranking [ 11 , 22 , 35 ] .
Our framework extends homogeneous document ranking to heterogeneous content presentation . Document ranking is a special case when presentation is a ranked list . From an algorithmic perspective , we use all documents on SERP
110 to determine the optimal presentation , which is in the same spirit of reranking/global ranking [ 35 , 22 ] . The difference is that our framework allows much more general notion of presentation than a list . In fact , global ranking algorithms , and more broadly , structured prediction algorithms in machine learning literature [ 10 ] , can be readily plugged into our framework as the user response model . 7.2 Federated Search
Federated search ( or aggregated search ) refers to searching through a collection of specialized search engines , verticals , and aggregating the results on SERP . Usually , contents from different verticals are heterogeneous and visually rich . Federated search has two sub tasks : vertical selection and result presentation [ 32 , 3 ] . Given a query , the task of vertical selection is to accurately determine which candidate verticals provide potentially relevant results [ 7 , 8 ] . After getting results from candidate verticals , the task of result presentation is to merge vertical results with general webpage results on the same page [ 5 ] .
This paper is concerned with result presentation . Previous approaches formulate it as a ranking problem [ 5 , 34 , 23 ] . Specifically , [ 5 , 23 ] employ pointwise ranking functions to rank results and blocks , while [ 5 , 34 ] also construct pairwise preference judgments to train a ranking function . [ 14 ] considers 2 D grid presentation for image and shopping results . Our framework allows more flexible definition of presentation than ranked list and 2 D grid , eg arbitrary frames , image sizes , and text fonts .
Federated search results significantly change the landscape of SERP , which in turn calls for changes in evaluation methodology . Bailey et al . [ 9 ] propose the notion of whole page relevance . They argue that the Cranfield style evaluation is inadequate to quantify user ’s holistic experience on modern SERP , such as overall presentation and coherence . It proposes to evaluate whole page relevance by assigning grades to various SERP elements . Our framework incorporates this idea by defining an appropriate user satisfaction metric that guides the search of optimal presentation .
Our work is related to the whole page optimization for sponsored search [ 16 ] or online ads bidding [ 33 ] , where the focus is to optimize revenue of the search service provider . Our framework is more general and can be applied in these problems by changing the optimization objective . 7.3 Search Behavior Modeling
To deliver good search experience , understanding user behavior on SERP is critical . Eye tracking experiments [ 20 , 19 ] and click log analyses [ 24 , 15 ] observe that users follow sequential order in browsing blue link only SERPs . Lowerranked results are examined with lower probability . On the one hand , these results confirm the probability ranking principle , encouraging search engines to put more relevant results on top . On the other hand , the position bias has to be handled with care when using click through data as relevance judgments to train ranking functions [ 24 ] .
As heterogeneous results appear on modern SERP , users do not necessarily follow sequential order in browsing search results . Studies observe more interesting user interaction patterns , notably vertical bias [ 12 , 26 , 31 ] and presentation bias [ 39 ] . These patterns not only make it questionable to stick to the probability ranking principle , but also complicates the inference of relevance judgments from click through data . As a result , global ranking that takes into account relations between documents is developed [ 32 ] , and click modeling of federated search becomes even more elaborate [ 12 , 17 ] .
Instead of building click models , our framework uses randomized experiments to understand user ’s presentation preference . Using presentation exploration buckets , we collect user responses on almost identical contents rendered with different presentations . Interaction logs collected in this way are almost free from presentational bias and can be used to estimate user response to a specific presentation given the search content3 . This enables automatic generation of presentations for different contents , which is more scalable than hand crafted presentation rules .
8 . CONCLUSION
This paper proposes a new problem in web search : wholepage presentation optimization . Modern retrieval systems return a collection of heterogeneous results , calling for more flexible presentation strategies than the conventional ranked list . This paper formulates the problem as a mathematical optimization problem , and proposes a framework that solves the problem in a data driven fashion . This general framework is instantiated to enable more flexible and expressive definition of page presentation than ranked list . Simple instantiations of the framework are shown to outperform ranking based methods in satisfying federated search users . This study opens up many interesting avenues for the future work . We can instantiate the general presentation optimization framework properly on other heterogeneous content presentation scenarios , such as mobile and tablet search , where user ’s attention bias may be different from that on large screens . User response models can be instantiated in more sophisticated ways , eg , modeling cursor position as conditional random fields over the SERP canvas . Finally , defining a proper quantitative metric for user satisfaction based on SERP level , fine grained user behaviors can be explored in human computer interaction research .
9 . ACKNOWLEDGEMENTS
We sincerely thank the anonymous reviewers for their useful comments . This work is partially supported by the National Science Foundation under grant number IIS 1054199 .
10 . REFERENCES [ 1 ] Eye tracking study : Google results with videos . https://wwwlooktrackercom/blog/eye tracking casestudy/google results with videos eye tracking study
[ 2 ] Google ads in second position get more attention . http://miratechcom/blog/eye tracking googlehtml
[ 3 ] Trec federated web search track . https://sitesgooglecom/site/trecfedweb
[ 4 ] J . Arguello and F . Diaz . Vertical selection and aggregation .
In B . Long and Y . Chang , editors , Relevance Ranking for Vertical Search Engines . Elsevier , 2013 .
[ 5 ] J . Arguello , F . Diaz , and J . Callan . Learning to aggregate vertical results into web search results . In Proceedings of the 20th ACM conference on Information and knowledge management , pages 201–210 , 2011 .
[ 6 ] J . Arguello , F . Diaz , J . Callan , and B . Carterette . A methodology for evaluating aggregated search results . In
3The presentation can never be completely random due to constraints of business and SERP template itself .
111 Advances in information retrieval , pages 141–152 . Springer , 2011 .
[ 7 ] J . Arguello , F . Diaz , J . Callan , and J F Crespo . Sources of evidence for vertical selection . In Proceedings of the 32nd ACM SIGIR conference on Research and development in information retrieval , pages 315–322 , 2009 .
[ 8 ] J . Arguello , F . Diaz , and J F Paiement . Vertical selection in the presence of unlabeled verticals . In Proceedings of the 33rd ACM SIGIR conference on Research and development in information retrieval , pages 691–698 , 2010 . [ 9 ] P . Bailey , N . Craswell , R . W . White , L . Chen ,
A . Satyanarayana , and S . M . Tahaghoghi . Evaluating whole page relevance . In Proceedings of the 33rd ACM SIGIR conference on Research and development in information retrieval , pages 767–768 , 2010 .
[ 10 ] G . Bakir , T . Hofmann , B . Sch¨olkopf , A . Smola , B . Taskar , and S . Vishwanathan . Predicting structured data . 2007 .
[ 24 ] T . Joachims , L . Granka , B . Pan , H . Hembrooke , and G . Gay . Accurately interpreting clickthrough data as implicit feedback . In Proceedings of the 28th ACM SIGIR conference on Research and development in information retrieval , pages 154–161 , 2005 .
[ 25 ] H . W . Kuhn . The hungarian method for the assignment problem . Naval Research Logistics Quarterly , 2(1 2):83–97 , 1955 .
[ 26 ] D . Lagun and E . Agichtein . Effects of task and domain on searcher attention . In Proceedings of the 37th ACM SIGIR conference on Research and development in information retrieval , pages 1087–1090 , 2014 .
[ 27 ] J . Langford . Vowpal wabbit . http://hunch.net/ vw . [ 28 ] L . Li , W . Chu , J . Langford , and R . E . Schapire . A contextual bandit approach to personalized news article recommendation . In Proceedings of the 19th conference on World Wide Web , pages 661–670 , 2010 .
[ 11 ] J . Carbonell and J . Goldstein . The use of mmr ,
[ 29 ] T Y Liu . Learning to rank for information retrieval . diversity based reranking for reordering documents and producing summaries . In Proceedings of the 21st ACM SIGIR conference on Research and development in information retrieval , pages 335–336 , 1998 .
[ 12 ] D . Chen , W . Chen , H . Wang , Z . Chen , and Q . Yang .
Beyond ten blue links : enabling user click modeling in federated web search . In Proceedings of the fifth ACM conference on Web search and data mining , pages 463–472 , 2012 .
[ 13 ] T . Chen . extreme gradient boosting library . https://githubcom/tqchen/xgboost
Foundations and Trends in Information Retrieval , 3(3):225–331 , 2009 .
[ 30 ] Y . Liu , Y . Chen , J . Tang , J . Sun , M . Zhang , S . Ma , and
X . Zhu . Different users , different opinions : Predicting search satisfaction with mouse movement information . In Proceedings of the 38th ACM SIGIR conference on Research and development in information retrieval , 2015 .
[ 31 ] Z . Liu , Y . Liu , K . Zhou , M . Zhang , and S . Ma . Influence of vertical result in web search examination . In Proceedings of the 38th ACM SIGIR conference on Research and development in information retrieval , 2015 .
[ 14 ] F . Chierichetti , R . Kumar , and P . Raghavan . Optimizing
[ 32 ] B . Long and Y . Chang . Relevance Ranking for Vertical two dimensional search results presentation . In Proceedings of the fourth ACM conference on Web search and data mining , pages 257–266 , 2011 .
[ 15 ] N . Craswell , O . Zoeter , M . Taylor , and B . Ramsey . An experimental comparison of click position bias models . In Proceedings of the 2008 Conference on Web search and data mining , pages 87–94 , 2008 .
[ 16 ] N . R . Devanur , Z . Huang , N . Korula , V . S . Mirrokni , and Q . Yan . Whole page optimization and submodular welfare maximization with online bidders . In Proceedings of the fourteenth ACM conference on Electronic commerce , pages 305–322 , 2013 .
[ 17 ] F . Diaz , R . W . White , G . Buscher , and D . Liebling . Robust models of mouse movement on dynamic web search results pages . In Proceedings of the 22nd ACM conference on information and knowledge management , pages 1451–1460 , 2013 .
[ 18 ] J . H . Friedman . Stochastic gradient boosting .
Computational Statistics & Data Analysis , 38(4):367–378 , 2002 .
Search Engines . Elsevier , 2014 .
[ 33 ] P . Metrikov , F . Diaz , S . Lahaie , and J . Rao . Whole page optimization : how page elements interact with the position auction . In Proceedings of the fifteenth ACM conference on Economics and computation , pages 583–600 , 2014 .
[ 34 ] A . K . Ponnuswami , K . Pattabiraman , Q . Wu ,
R . Gilad Bachrach , and T . Kanungo . On composition of a federated web search result page : using online users to provide pairwise preference for heterogeneous verticals . In Proceedings of the fourth ACM conference on Web search and data mining , pages 715–724 , 2011 .
[ 35 ] T . Qin , T Y Liu , X D Zhang , D S Wang , and H . Li .
Global ranking using continuous conditional random fields . In Advances in neural information processing systems , pages 1281–1288 , 2009 .
[ 36 ] S . E . Robertson . The probability ranking principle in ir . In
Journal of Documentation , pages 294–304 , 1977 .
[ 37 ] G . Salton and C . Buckley . Term weighting approaches in automatic text retrieval . Information processing & management , 24(5):513–523 , 1988 .
[ 19 ] H . Gord . Eye tracking on universal and personalized search .
[ 38 ] A . Schuth , K . Hofmann , and F . Radlinski . Predicting http://searchenginelandcom/eye tracking on universaland personalized search 12233 Technical report , Enquiro Research .
[ 20 ] L . A . Granka , T . Joachims , and G . Gay . Eye tracking analysis of user behavior in www search . In Proceedings of the 27th ACM SIGIR conference on Research and development in information retrieval , pages 478–479 , 2004 .
[ 21 ] M . Hearst . The evaluation of search user interfaces . In
Search User Interfaces . Cambridge University Press , 2019 . [ 22 ] S . Ji , K . Zhou , C . Liao , Z . Zheng , G R Xue , O . Chapelle ,
G . Sun , and H . Zha . Global ranking by exploiting user clicks . In Proceedings of the 32nd ACM SIGIR conference on Research and development in information retrieval , pages 35–42 , 2009 .
[ 23 ] L . Jie , S . Lamkhede , R . Sapra , E . Hsu , H . Song , and
Y . Chang . A unified search federation system based on online user feedback . In Proceedings of the 19th ACM SIGKDD conference on Knowledge discovery and data mining , pages 1195–1203 . ACM , 2013 . search satisfaction metrics with interleaved comparisons . In Proceedings of the 38th ACM SIGIR conference on Research and development in information retrieval , 2015 .
[ 39 ] Y . Yue , R . Patel , and H . Roehrig . Beyond position bias :
Examining result attractiveness as a source of presentation bias in clickthrough data . In Proceedings of the 19th conference on World Wide Web , pages 1011–1018 , 2010 .
[ 40 ] C . Zhai and J . Lafferty . A study of smoothing methods for language models applied to ad hoc information retrieval . In Proceedings of the 24th ACM SIGIR conference on Research and development in information retrieval , pages 334–342 , 2001 .
112
