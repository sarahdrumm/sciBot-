To Suggest , or Not to Suggest for Queries with Diverse
Intents : Optimizing Search Result Presentation
Makoto P . Kato kato@dlkuiskyoto uacjp
Katsumi Tanaka tanaka@dlkuiskyoto uacjp
Yoshida Honmachi , Sakyo , Kyoto , Japan 6068501
Kyoto University
ABSTRACT We propose a method of optimizing search result presentation for queries with diverse intents , by selectively presenting query suggestions for leading users to more relevant search results . The optimization is based on a probabilistic model of users who click on query suggestions in accordance with their intents , and modified versions of intent aware evaluation metrics that take into account the co occurrence between intents . Showing many query suggestions simply increases a chance to satisfy users with diverse intents in this model , while it in fact requires users to spend additional time for scanning and selecting suggestions , and may result in low satisfaction for some users . Therefore , we measured the loss of time caused by query suggestion presentation by conducting a user study in different settings , and included its negative effects in our optimization problem . Our experiments revealed that the optimization of search result presentation significantly improved that of a single ranked list , and was beneficial especially for patient users . Moreover , experimental results showed that our optimization was effective particularly when intents of a query often co occur with a small subset of intents .
Categories and Subject Descriptors H33 [ Information Search and Retrieval ]
Keywords search result diversification ; query suggestion ; optimization
1 .
INTRODUCTION
Web search queries are often ambiguous and/or underspecified , as indicated by their word lengths : 3.1 for desktop searches and 2.9 for typed mobile searches [ 13 ] . This problem of Web search queries will not be resolved in the near future , since the number of mobile users is still increasing as of December 2014 [ 1 ] , and so the average length of Web search queries is accordingly expected to decrease . Therefore , search result diversification has been considered as an important challenge for Web search engines , and extensively
Figure 1 : Search result diversification ( i ) without query suggestions , and ( ii ) with query suggestions . studied to satisfy as many intents as possible behind an ambiguous or underspecified query .
Although most of the existing work has focused on diversification in a single ranked list , query response paradigm is not always satisfactory for diverse intents . Figure 1 ( i ) shows an example of a search engine result page ( SERP ) for query “ Transformers ” . Assume that all the documents are relevant to a single intent at the same level . Despite high diversity in SERP ( i ) , this SERP cannot present many relevant search results in the top three for any intent behind this query : only a relevant result for movie , DVD , and game intents , and no relevant one for the others . In addition , users interested in games have to skip two irrelevant search results for reaching a relevant search result . Therefore , diversification on a single ranked list cannot fully satisfy many users , especially when a given query has a lot of ambiguity . In fact , the average number of intents is 7.92 in the NTCIR INTENT tasks [ 26 , 29 ] , and may overabound for a single SERP including ten search results .
We can overcome the limitation by taking into account the interaction between searchers and search engines . Figure 1 ( ii ) shows a SERP for query “ Transformers ” presenting query suggestion “ Transformers game ” , and one for this query suggestion . Users interested in movies may browse the SERP returned in response to the original query and find two relevant results , while users interested in games may click on the query suggestion and find two relevant results . As seen in this example , presenting query suggestions can lead users to their desired search results and increase the chance to satisfy many users at a high level . Whereas , it in fact requires users to spend additional time for scanning and selecting suggestions , and may result in lower satisfaction for some users . To suggest , or not to suggest : that is the question .
The goal of this paper is to develop a method of optimizing search result presentation , ie selecting a set of query suggestions to be presented , and ranked list of documents for each of the input query and suggestions . There are three main challenges in this problem . First , search results have been usually optimized by max
Movie DVD Movie Transformers game Game Game Toy ( ii)Transformers Movie DVD Game ( i)Transformers game Transformers 133 Implicit approaches are based on an assumption that similar documents are likely to be redundant as they cover similar interest . One early work on search result diversification was an implicit approach presented by Carbonell and Goldstein [ 7 ] . They balanced the relevance of documents to a query and dissimilarity among retrieved documents by maximal marginal relevance . Zhai , Cohen , and Lafferty proposed some evaluation metrics for taking into account the redundancy of documents that generalizes the precision and recall [ 31 ] . They also extended the MRR based approach by using several types of novelty measures . Chen and Karger proposed an implicit approach for search result diversification based on optimizing objective functions , which are defined as the probability of at least k relevant documents for all users [ 12 ] . Radlinski , Kleinberg , and Joachims proposed methods of learning diverse rankings based on multi armed bandits [ 25 ] . Their algorithms maximize the expected total reward based on feedback by users with different interests . As a result , their algorithm produces diverse search results to satisfy different types of users .
In contrast , explicit approaches directly model user intents , and try to maximize the coverage of documents relevant to each intent . Agrawal et al . proposed IA select algorithm for search result diversification and intent aware evaluation metrics [ 3 ] . They took an explicit approach that modeled intents of a given query , and formalized a search result diversification problem as a maximization of the probability of at least one relevant document for all the intents . Carterette and Chandar proposed a probabilistic model for faceted topic retrieval , and methods of retrieving a set of documents that maximizes the probability of including all facets for a given query [ 8 ] . Their problem is slightly different from the search result diversification problem , while their approach can be regarded as an explicit one for search result diversification . Santos , Macdonald , and Ounis proposed a probabilistic model for search result diversification called xQuAD , which explicitly models an ambiguous or underspecified query as a set of subqueries [ 27 ] . While their model was similar to Agrawal et al . ’s IA select , they also proposed methods of generating subqueries and estimating their importance , and established a practical approach to search result diversification . Dou et al . proposed a method of mining intents from various types of sources such as anchor texts , query logs , clusters of search results [ 14 ] . Recent search result diversification methods were developed on the basis of the combination of implicit and explicit approaches [ 16 ] , learning to rank [ 32 ] , rank aggregation [ 23 ] , and multiple knapsack problem [ 30 ] . We also take an explicit approach in our search result presentation optimization .
Although , to the best of our knowledge , our work is the first trial to optimize search results including query suggestions , some closely related work exists outside the search result diversification . One is dynamic ranked retrieval proposed by Brandt et al . [ 6 ] . Their dynamic rankings provide search results that can change based on user feedback ( eg clicks on search results ) . Their approach is to optimize the expected utility of users with different interests , and similar to ours expect that i ) we allow users to have multiple intents , ii ) we take into account the effects of user interaction on their satisfaction , and iii ) we do not only rank search results but also select the optimal query suggestion set . Another closely related work is query suggestion optimization proposed by Anagnostopoulos et al . [ 4 ] . Their approach is based on optimization of the expected utility of query reformulations , while search result optimization and diverse intents of a query have not been considered together with query suggestions .
Some work modeled user effort for measuring the utility of search results , or developing a new evaluation metric . Dunlop proposed time to view graphs that measure the time to find a given number
Figure 2 : Joint intent probability for query “ Transformers ” . imizing a certain evaluation metric , while our problem includes uncertain factors such as intent and query suggestion click probabilities . In this paper , we formalize the optimization of search result presentation as a maximization problem of expected utility of users who click on query suggestions in accordance with their intents . Second , users have been assumed to have a single intent in conventional search result diversification , while , in reality , they do have multiple intents in a session . Figure 2 shows intents behind query “ Transformers ” , and their joint intent probability P ( x , y ∈ X|q ) , the probability of users who input query q and have intents including both x and y . For example , ( Movie , DVD ) cells represent the probability of users interested in topics including “ Movie ” and “ DVD ” . Appropriate search result presentation highly relies on the co occurrence of intents , as search results relevant to tightly coupled intents should not be included in different SERPs ( eg “ Movie ” and “ DVD ” ) . Therefore , we propose joint intent aware evaluation metrics to measure the expected utility of users with multiple intents . Third , effects of query suggestions on user search behaviors need to be calibrated . Although the query suggestion effects on search behaviors have been studied in the literature ( eg [ 20] ) , it is still unclear how much time users have to spend for scanning and selecting from query suggestions . To answer the question , we conducted a user study in desktop and mobile environments , and incorporated findings into the optimization of search result presentation .
Experiments were carried out to demonstrate the effectiveness of the search result presentation optimization . We utilized two test collections developed in the NTCIR INTENT tasks [ 26 , 29 ] , and found that our optimization could significantly improve that of a single ranked list . Our contributions can be summarized as follows : ( 1 ) we proposed a searcher model interacting with query suggestions in accordance with their multiple intents , and a method of optimizing search result presentation for an ambiguous or underspecified query ; ( 2 ) we conducted a user study to examine the effects of query suggestions on search behaviors , and calibrated some parameters used in the searcher model ; and ( 3 ) we demonstrated the effectiveness of the search result presentation optimization , and confirmed that our optimization was effective especially for patient users and queries with a limited number of intents .
The rest of the paper is organized as follows . Section 2 surveys related work on search result diversification . Section 3 introduces our model for optimizing search result presentation . Section 4 describes a user study we conducted for calibrating effects of query suggestions . Section 5 presents experimental results . Finally , Section 6 concludes by outlining future work .
2 . RELATED WORK
The literature contains many studies on search result diversification , which can be classified into implicit and explicit approaches .
MovieDVDGameToyAnimePictureMovieDVDGameToyAnimePicture006012018024030036042048054060134 of relevant documents [ 15 ] . Järvelin et al . proposed an extension to the discounted cumulated gain for evaluating multiple query sessions , by including a discount factor for query reformulations [ 18 ] . Baskaya , Keskustalo , and Järvelin conducted a simulation based experiment to compare search strategies , by taking into account time constraints [ 5 ] . Smucker and Clarke calibrated a discount factor in an evaluation metric based on the time spent reaching a certain rank [ 28 ] . Our user study gives an insight into the cost of query suggestions in this line of work .
3 . METHODOLOGY
This section describes our model for optimizing search result presentation , ie selecting a set of query suggestions and ranked lists of documents . First , we propose a probabilistic model of users interacting with query suggestions , and formalize the problem of search result presentation optimization . Second , we introduce jointintent aware evaluation metrics for measuring the search result utility with multiple intents . Third , we propose methods of estimating probabilities used in the optimization problem . Finally , we describe an algorithm for the optimization problem . 3.1 User Model
We propose a probabilistic model of users interacting with query suggestions , and then formalize the problem of search result presentation optimization as maximization of the expected utility of users following the probabilistic model .
Our user model is described as follows : 1 . A user inputs query q and receives a SERP d(aq ) presenting query suggestions S ;
2 . The user decides either to continue to browse the SERP or to click a presented query suggestion following probability P ( a|q , S ) , where a ∈ A , a set of user actions such as browsing the presented SERP ( denoted by aq ) , and selecting a query suggestion s ∈ S ( denoted by as ) , ie A = {aq}∪AS and AS = {as}s∈S ;
3 . If the user continues to browse d(aq ) , s/he can obtain utility u with probability P ( u|q , S , aq , d(aq ) ) ;
4 . If the user clicks on query suggestion s , s/he can browse another SERP d(as ) and obtain utility u with P ( u|q , S , as , d(s) ) . This probabilistic model enables us to formalize the expected utility of search results with query suggestions as shown below :
E[u|q , S , D ] =
P ( a|q , S)E[u|q , S , a , d(a) ] ,
( 1 ) a∈A where D is a set of SERPs ( ie D = {d(a)}a∈A ) .
The problem of optimizing search result presentation can be formalized as maximization of the expected utility of search results with query suggestions :
∗
( S
, D
∗
) = argmax
( S,D )
E[u|q , S , D ] .
( 2 )
In other words , we assume that search result presentation is optimal if its expected utility is maximized . The expected utility E[u|q , S , a , d(a ) ] in Equation 1 can be approximated by the value of a joint intent aware evaluation metric for users who take a particular action . In the following subsections , we introduce the joint intent aware evaluation metric , and methods of estimating some probabilities in Equation 1 .
3.2 Joint intent aware Evaluation Metrics
Agrawal et al . proposed intent aware metrics for diversity aware evaluation [ 3 ] , and their general form is shown below :
IA(d ) =
P ( i|q)Mi(d ) ,
( 3 ) i∈I where d = ( d1 , . . . , dr , . . . ) is a ranked list of documents , I is a set of intents for a given query q , P ( i|q ) is the probability of intent i of users inputting query q , and Mi(d ) is an evaluation metric for a particular intent i . Note that we use an intent as a user interest in a certain subtopic of a query ( ie topical search intents [ 21] ) , and assume that there is a one to one relationship between intents and subtopics . The evaluation metric Mi(d ) can be instantiated by discounted cumulative gain ( DCG ) [ 17 ] , expected reciprocal rank ( ERR ) [ 11 ] , and even a recently proposed metric such as time biased gain ( TBG ) [ 28 ] .
As we mentioned earlier , we introduce joint intent aware versions to take into account the co occurrence of multiple intents . In Agrawal et al . ’s intent aware metrics , they modeled the intent probability by a multinomial distribution over a set of intents I , and assumed that each user has a single intent . In contrast , we assume that each user can have multiple intents in a search session , and model the intent probability as a probability over a power set of intents 2I . By letting P ( X|q ) be a probability of joint intent X ⊂ I given query q , a joint intent aware metric is defined as follows :
JIAP ( X|q)(d ) =
P ( X|q)MX ( d ) ,
( 4 )
X∈2I i ∈ X , ie MX ( d ) = where MX ( d ) is an evaluation metric for a particular intent set X . We can naturally define join intent aware versions of intent aware evaluation metrics by defining MX ( d ) as the sum of Mi(d ) for i∈X Mi(d ) . The reason why we use the sum of Mi(d ) is that we assume users can obtain gains based on each of their intents , which are accumulated . Although the computation of a joint intent aware evaluation metric by a naïve method requires 2|I| computations of P ( X|q ) and MX ( d ) , we can save on the computational cost by using a basic i ωiXi . By using i∈I 1(i ∈ X)Mi(d ) ( 1(x ) is an indicator function ) , a joint intent aware evaluation metric can be computed by the following equation : theorem : E[Y ] = MX ( d ) = i ωiE[Xi ] where Y =
P ( i ∈ X|q)Mi(d ) ,
( 5 ) q . Formally , P ( i ∈ X|q ) =
JIAP ( X|q)(d ) = i∈I where P ( i ∈ X|q ) is the probability of X including i given query X∈2I 1(i ∈ X)P ( X|q ) . Thus , we only need to estimate P ( i ∈ X|q ) for computing a joint intentaware evaluation metric . mate the expected utility E[u|q , S , a , d(a ) ] in Equation 1 :
The joint intent aware evaluation metric is utilized to approxi
E[u|q , S , a , d(a ) ] = JIAP ( X|q,S,a)(d(a ) )
=
P ( X|q , S , a)MX ( d(a) ) ,
( 6 )
( 7 )
X∈2I where P ( X|q ) is replaced with P ( X|q , S , a ) ( a probability of intent set X for a user who issues q and takes action a for query suggestions S ) , since the distribution of intent sets can be different after taking a particular action a for query suggestions S .
Furthermore , we take into account the negative effect of query suggestions in the expected utility . Presented query suggestions may interrupt browsing of SERPs and cause time loss , probably because of the viewport narrowed by the query suggestion space
135 or time spent assessing query suggestions . Selecting query suggestions also requires some time from the users , which accordingly reduces the utility of SERPs returned in response to clicked query suggestions . Both effects can possibly increase as the number of query suggestions increases , and can be represented by time tn,a , where n is the number of query suggestions ( ie |S| ) , and a is an action taken by the user .
TBG is a time based evaluation metric proposed by Smucker and Clarke [ 28 ] , and discounts the gain at each rank by the expected time to reach the rank . Thus , TBG can naturally include the time loss tn,a as follows :
N r=1
− ln 2 h
M TBG i
( d ) =
Gi(r ) exp
( Ti(r ) + tn,a )
,
( 8 )
N N r=1 where Gi(r ) is the gain at rank r in terms of intent i , Ti(r ) is the time to reach rank r , and h is the half life of users ( h = 224 unless otherwise noted ) . Although the original version of TBG does not include cutoff parameter N , we include it since we can only evaluate a limited number of search results in reality . DCG [ 17 ] and ERR [ 11 ] can also include the time loss by converting it to rank loss R(tn,a ) , which is the rank that users can reach in time tn,a :
M DCG i
( d ) =
M ERR i
( d ) = gi(r ) logβ(1 + r + R(tn,a ) )
,
( 9 )
1 hi(r )
( 1 − hi(r
)),(10 ) r + R(tn,a ) r=1 r−1 r=1 where gi(r ) is a relevance grade for the r th document in terms of intent i , β is the base of the logarithm ( β = 2 in this paper ) , and hi(r ) = ( 2gi(r ) − 1)/2gmax , the probability of relevance of the rth document to intent i ( gmax is the maximum relevance grade ) . In this way , the time loss tn,a and rank loss R(tn,a ) penalize the score of an evaluation metric . Their calibration is explained in Section 4 .
3.3 Estimation of Probabilities
In this subsection , we propose methods of estimating two probabilities in the maximization problem in Equation 2 : the probability of taking action a for a given query q and query suggestions S ( ie P ( a|q , S) ) , and the probability of joint intents for query q and taking action a for query suggestions S ( ie P ( X|q , S , a) ) .
We assume the following settings for the estimations . First , we assume that intents for a query are used as query suggestions , ie S ⊂ I . This is not a strong limitation as intents ( or subtopics ) provided by the NTCIR INTENT tasks are quite similar to ordinary query suggestions [ 26 , 29 ] . The task organizers in fact explicitly mentioned that intents ( or subtopics ) can be used for query suggestions . Second , we assume that the frequency of intent sets is available for each query . If a search engine query log is available , one can obtain it by extracting all the sessions including a query , and regarding co occurring queries within a session as an intent set . In our experiments , we obtained the frequency of intent sets by using a crowd sourcing service and asking workers to vote for intents assuming that they were users who issued a particular query . Estimation of P ( a|q , S ) .
Directly estimating the probability P ( a|q , S ) is not practical since this probability depends on presented query suggestions S , and we may not be able to obtain training data for an arbitrary set of query suggestions . Therefore , we first decompose the probability
P ( a|q , S ) by marginalizing intent set X :
P ( a|q , S ) =
P ( a|X , S)P ( X|q ) .
( 11 )
X∈2I
|S| k=1
It is still hard to estimate P ( a|X , S ) ( the probability of taking action a for users with intent set X given query suggestions S ) , while we can naturally approximate it for the case of a ∈ AS as follows :
P ( as|X , S ) pQS
1(s ∈ X ) |X ∩ S| .
( 12 )
This approximation indicates that users select query suggestions relevant to their intents with the same probability determined by parameter pQS . For example , users click on a query suggestion with a probability pQS/2 if two query suggestions relevant to their intents are presented in a SERP .
The assumption above simplifies Equation 11 as follows :
P ( as|q , S ) = pQS
P ( s ∈ X,|S ∩ X| = k|q ) ,
( 13 )
1 k where P ( s ∈ X,|S ∩ X| = k|q ) is the probability of X including s and k elements of S given query q . By letting fq,s,k be the frequency of intent sets for query q including s and k elements of S , and fq be the frequency of intent sets for query q , P ( s ∈ X,|S ∩ X| = k|q ) can be estimated as fq,s,k/fq . Having estimated P ( as|q , S ) , we can obtain P ( aq|q , S ) as fol lows : P ( aq|q , S ) = 1 − s∈S P ( as|q , S ) .
Estimation of P ( X|q , S , a ) . As we showed in Equation 5 , we do not necessarily estimate P ( X|q , S , a ) for all possible X but do estimate P ( i ∈ X|q , S , a ) for each intent i . The probability of P ( i ∈ X|q , S , a ) is also hard to directly estimate due to the lack of training data , while this probability can be obtained with Bayes’ rule :
P ( a|i ∈ X , S)P ( i ∈ X|q )
P ( i ∈ X|q , S , a ) =
( 14 ) The estimate of P ( i ∈ X|q ) is fq,i/fq where fq,i is the frequency of intent set X including i for query q . The probability P ( a|i ∈ X , S ) can be transformed into the following :
P ( a|q , S )
.
P ( a|i ∈ X , S ) =
P ( a|X , S)P ( X|i ∈ X ) ,
( 15 ) where P ( X|i ∈ X ) is the probability of X when the user is known to have intent i . By using the approximation in Equation 12 , we can obtain the following solution for P ( as|i ∈ X , S ) :
P ( as|i ∈ X , S ) = pQS
P ( s ∈ X,|S ∩ X| = k|i ∈ X ) , ( 16 )
1 k where P ( s ∈ X,|S ∩ X| = k|i ∈ X ) is the probability of X including s and k elements of S conditioned by i ∈ X , which is estimated as fq,i∧s,k/fq,i where fq,i∧s,k is the frequency of intent sets for query q including i , s , and k elements of S . In the same way as P ( a|q , S ) , we can obtainP ( aq|i ∈ X , S ) as follows : P ( aq|i ∈ X , S ) = 1 − s∈S P ( as|i ∈ X , S ) .
3.4 Optimization
We describe a method of selecting the optimal query suggestions and retrieving the optimal ranked lists of documents for a given query and each query suggestion . The optimization algorithm is described in Algorithm 1 .
X∈2I
|S| k=1
136 The input of this algorithm includes a query q , a set of query suggestion candidates S , and a set of documents D . The algorithm outputs the optimal query suggestions S∗ and ranked lists of documents for a given query and each query suggestion D∗ . This algorithm iteratively selects a query suggestion that maximizes the expected utility E[u|q , S , D ] when it is added to the query suggestion set . When no query suggestion can improve the expected utility , this algorithm stops and returns the optimal query suggestions that have been selected .
If we can obtain the optimal query suggestions , the optimization problem can be reduced to a problem of finding the optimal ranked list of documents for each action . The greedy algorithm in lines 5 7 can provide the optimal result for DCG . The theoretical lowerbound of a greedy algorithm for ERR IA , which is ( 1 − 1/e ) times the optimal value , was suggested by Chapelle et al . [ 10 ] . They used a real dataset to demonstrate that a greedy algorithm can yield results almost equivalent to the optimal results for many cases . This property is also inherited by the joint intent aware version of ERR , as both have the same form . set of documents D . for each s ∈ S/S∗ do
/* a greedy algorithm for ranking documents */ for r = 1 to N do
Algorithm 1 Optimization of query suggestions and ranked list of documents Input : A query q , a set of query suggestion candidates S , and a Output : Query suggestions S∗ , and ranked lists of documents D∗ . 1 : Emax = maxd(aq ) E[u|q,∅ , aq , d(aq ) ] 2 : for k = 1 to |S| do 3 : S = S∗ ∪ {s} 4 : A = {aq} ∪ AS 5 : for each a ∈ A do 6 : 7 : 8 : 9 : 10 : 11 : 12 : 13 : 14 : 15 : 16 : 17 : 18 : 19 : 20 : 21 : 22 : 23 : end if 24 : 25 : end for
D = D/d(a ) r = argmaxd∈D E[u|q , S , a , ( d(a ) d(a ) end for d(a ) = ( d(a ) end for D = {d(a)}a∈A if E[u|q , S , D ] > Emax then end if end for /* break if none of query suggestion candidates is added */ if k < |S∗| then
D∗ = D S∗ = S Emax = E[u|q , S∗ , D∗ ]
1 , . . . , d(a ) r−1 , d ) ]
1 , . . . , d(a )
N−1 , d(a ) N ) break
The complexity of the optimization problem in Equation 2 is at least NP hard if ERR is used for E[u|q , S , a , d(a) ] , since the optimization of ERR IA is NP hard [ 10 ] . Although the greedy algorithm does not guarantee the optimal solution , the worst case bound of our algorithm is maxd(aq ) E[u|q,∅ , aq , d(aq ) ] , which is the maximum of the expected utility of a single ranked list . Thus , one can use our algorithm ensuring that resultant SERPs including query suggestions are at least better than the optimized SERP without query suggestions . The time complexity of our algorithm is O(|S|3N|D|TJIA ) , where TJIA is the number of computations for JIA(d ) . The computation for estimating probabilities P ( a|q , S ) and P ( X|q , S , a ) is assumed to be smaller than that for selecting
|S|2 < N|D|TJIA and |S||I| < N|D|TJIA . N documents , ie The time complexity can be reduced by setting the maximum number of query suggestions to be presented in a SERP . By letting M be the maximum number of query suggestions ( M = 5 in our experiment ) , the reduced time complexity is O(|S|M 2N|D|TJIA ) . In our experiments , there are few cases where more than four query suggestions are estimated to be optimal .
4 . USER STUDY
We conducted a user study to calibrate the time loss tn,a and rank loss R(tn,a ) used in joint intent aware evaluation metrics . Questions we address here are summarized as follows :
1 . When n query suggestions are presented in a SERP , how much additional time do users have to spend to click on the first search result in the SERP ?
2 . When n query suggestions are presented in a SERP , how much additional time do users have to spend to click on the first search result in another SERP returned in response to a query suggestion ?
3 . Which rank could users reach in time equivalent to the esti mated time loss ?
The first question corresponds to calibrating tn,aq , ie the time loss for browsing the SERP for the input query when n query suggestions are presented , while the second question corresponds to calibrating tn,as , ie the time loss for browsing a SERP for a query suggestion s ∈ S when n query suggestions are presented . The last question corresponds to calibrating the rank loss R(tn,aq ) . 4.1 Settings
We developed a search system that resembles modern commercial search engines , asked experimental participants to complete given tasks , and measured task completion time in different situations . As devices used for searching might affect search completion time , this study was carried out in both desktop and mobile environments . Figures 3(a ) and 3(b ) show screenshots of our search systems for different device types . They are conventional ten blue link SERPs including a search box , titles , URLs and summaries of Web pages . Query suggestions were placed under the search box and links to verticals . The font size of query suggestions was the same for more than one query suggestion , while a single query suggestion was presented in a similar way as the “ Did you mean ? ” suggestion , and slightly bigger than multiple query suggestions as can be seen in Figure 3(a ) . Participants were allowed to click on titles of Web pages and query suggestions , but not to type queries or use the other vertical during a task . To measure time spent in SERPs and monitor actions taken during the search , we used a Javascript code used in a study on Web search success prediction [ 2]1 .
Tasks given to participants were to click on a title of a Web page relevant to a specific topic . Eight topics ( ID : 105 , 108 , 116 , 137 , 377 , 303 , 338 , and 312 ) were selected from queries used in the NTCIR INTENT 1 and 2 Japanese subtasks [ 26 , 29 ] . Our topic selection criteria were i ) the topic is famous enough to be known by most ordinary users , and ii ) the topic is ambiguous or underspecified enough to include at least five subtopics . In addition to the topics , we selected one of the subtopics for each topic , and created an information need for each subtopic . For example , the information need “ a recipe using red wine and apples ” was created for topic 105 ( “ red wine ” ) and its subtopic ( “ red wine cooking ” ) . As another example , the information need “ a toy dinosaur museum ” was created for topic 312 ( “ dinosaur ” ) and its subtopic ( “ dinosaur 1http://ir ubmathcsemoryedu/uFindIt/
137 could monitor their search task accuracy by checking whether they clicked on a relevant document , we rejected participants who clicked on more than two irrelevant documents . Having excluded participants on the basis of their search task accuracy and lack of Javascript support , we obtained user behavior data of 128 participants for each device type ( 256 in total)3 . 4.2 Calibration
This subsection describes our model of time spent reaching a certain rank in a SERP , and calibration of the time and rank loss .
We assumed that there were two factors affected by the number of query suggestions . One is the time spent assessing query suggestions , scan time , while the other is the time spent clicking on one of the query suggestions , click time . Thus , we used the following models of time spent before reaching the top ranked search result after taking action a when n query suggestions are presented : tn,a =
Tscan(n ) + Taq Tscan(n ) + Tclick(n ) + Taq + Tas
( a = aq ) ( a ∈ AS )
,
( 17 ) where Tscan(n ) is the scan time for n query suggestions , Tclick(n ) is the click time for n query suggestions , Taq is constant time before reaching the top ranked document in the first SERP , and Tas is constant time before reaching the top ranked document in a SERP returned in response to a query suggestion . We explored the shape of the functions Tscan(n ) and Tclick(n ) in our preliminary analysis , and found that a linear function ( Tscan(n ) = αn ) or an exponential function ( Tscan(n ) = α exp(n ) ) fitted the observed data well .
To calibrate each parameter in the models of time spent before reaching the top ranked search result , we also modeled the time spent in a search task in our user study . One possible approach is similar to that used in Smucker and Clarke ’s work , where ranks of clicked search results are included in Equation 17 [ 28 ] . We compared several alternatives to their approach and found that ranks of clicked search results could not explain a large amount of the variance in the spent time , while the length of page scrolls could explain it far better . Below , we show rank based and scroll based models for the time spent doing the search tasks :
Trank(n , a , r ) = tn,a + Trr , Tscroll(n , a , l ) = tn,a + Tll ,
( 18 ) ( 19 ) where r is the clicked rank , Tr is the time spent per search result , l is the length of page scrolls , and Tl is the time per scroll length .
Table 1 shows the R squared of least squares fittings . R squared is a metric indicating how much variance of an objective variable can be explained by independent variables . The scroll based models are more appropriate for both desktop and mobile devices than the ranked based models . In addition , exponential functions for scan and click times were slightly better than linear functions in terms of R squared . Figure 4 shows the average time spent on the first SERP as a function of the number of query suggestions presented , and also supports that exponential functions are suitable at least for five or fewer query suggestions . Therefore , we employed exponential functions for modeling the scan and click times .
Parameters estimated by using the scroll based model are listed in Table 2 . The estimated coefficient for the scan time was higher than that for the click time in desktop , while the estimated coefficient for the scan time was lower than that for the click time in mobile . These results indicate that presenting query suggestions gives a smaller effect on SERP browsing for mobile users than desktop ones , while clicking on a query suggestion requires more time from 3Data http://wwwmpkatonet/projects/ optimizing search result presentation/ available at
( a ) Desktop
( b ) Mobile
Figure 3 : Screenshots of the search system . museum ” ) . We used information needs so specific that there is only one relevant document in the document collection .
The participants were given an instruction “ Please search with search keywords ( topic ) and find a document that you expect to contain information on ( information need ) as fast as possible from search results for ( topic ) or suggested search keywords ” , where variables ( eg ( topic ) ) were replaced with specific phrases we had prepared . After reading the instruction for a topic , participants could start the search task by clicking on a search button next to a text form containing a topic as a search query . Participants were then shown a SERP for the topic ( eg “ red wine ” ) as well as query suggestions including a selected subtopic for the topic ( eg “ red wine cooking ” ) . Query suggestions were randomly selected from subtopics of a topic , and always included a subtopic used for creating an information need for the topic . When participants clicked on one of the search results and pressed “ OK ” for a confirmation dialog box , they could read the instruction for the next topic .
Variables we considered in the user study are i ) the number of query suggestions ( 0 , 1 , 3 , and 5 ) , ii ) topics ( eight topics explained earlier ) , and iii ) position of relevant documents . We placed relevant documents within one of the four types of positions : top top ( the top three ranks in the SERP for a topic as well as the top three ranks in the SERP for the subtopic relevant to a presented information need ) , bottom top ( the bottom three ranks in the SERP for a topic as well as the top three ranks in the SERP for the subtopic relevant to a presented information need ) , nowhere top ( only the top three ranks in the SERP for the subtopic relevant to a presented information need ) , and nowhere bottom ( only the bottom three ranks in the SERP for the subtopic relevant to a presented information need ) . Note that we did not include top bottom and bottom bottom cases simply because a more specific subtopic relevant to a document is unlikely to give a lower score to the relevant document than its topic . Ranks of a relevant document were randomized within a specified rank range ( ie the top three or bottom three ranks ) so that participants did not learn the exact position of relevant documents . We used a within subject design for the user study , and every participant was subjected to every treatment for each variable . The combinations of treatments were counterbalanced by the GraecoLatin square . Thus , all the combinations were examined exactly the same number of times , and the number of required participants were 4× 8× 4 = 128 for four query suggestions , eight topics , and four types of relevant document positions .
We recruited experiment participants through Lancers2 , one of the most popular crowd sourcing services in Japan . Each participant was required to complete eight search tasks for eight topics in different settings , and paid 100 JPY for the tasks . As we
2http://wwwlancersjp/
138 Table 1 : R squared of least squares fittings for different models .
R squared
Model ( Eq No . )
Rank based ( 18 )
Scroll based ( 19 )
αn
Scan / Click time Desktop Mobile 0.126 0.126 0.440 0.442
0.130 0.133 0.445 0.448
α exp(n )
α exp(n )
αn
Table 2 : Parameters estimated by using the scroll based model . Note that αscan and αclick are parameters in Tscan(n ) = αscan exp(n ) and Tclick(n ) = αclick exp(n ) .
Parameter
αscan αclick Taq Tas Tl
Desktop
0.0238 ± 0.0132 0.0190 ± 0.0180
8.34 ± 1.03 1.54 ± 1.43
0.00936 ± 0.000385
Mobile
0.00191 ± 0.0120 0.0284 ± 0.0158 5.95 ± 0.908 2.47 ± 1.25
0.0101 ± 0.000406
Figure 4 : Average time spent at the first SERP as a function of the number of query suggestions presented . mobile users than desktop ones . They could be explained by studies on desktop and mobile searchers by using eye tracking devices . Joachims , Granka , and Pan showed that the fixation ratio on summaries of search results decreased in ascending order of ranks [ 19 ] . In contrast , Lagun et al . reported that mobile users gazed at the second and third search results for a significantly longer time than the first search result [ 22 ] . Based on their findings , we could hypothesize that mobile users are more tolerant to query suggestions presented at the top of SERPs than desktop users .
Finally , we calibrate the rank loss R(tn,a ) by using estimated time spent before reaching the top ranked search result after taking an action , and estimated time spent for reaching a search result at a certain rank without query suggestions presented . The former time could be estimated by using Equation 17 , while the latter time could be estimated by using the rank based model in Equation 18 . Figures 5(a ) and 5(b ) show the estimated time for reaching the top ranked search result after taking actions aq and as as a function of the number of presented query suggestions in desktop and mobile environments , respectively . The horizontal dotted lines represent the estimated time for reaching a certain rank r without query suggestions presented . For desktop users , for example , the time spent for reaching the top ranked search result in the first SERP with four query suggestions presented equals to that for reaching the third search result in the first SERP without query suggestions . We could see large effect of query suggestions on the first SERP browsing ( aq ) for desktop users , but little effect for mobile users . We determined the rank loss so that the decay by R(tn,a ) at the first rank in evaluation metrics became equivalent to that requiring the same time without query suggestions , ie tn,a = Trank(0 , aq , R(tn,a) ) . Table 3 summarizes the time and rank loss estimated on the basis of the user study .
5 . EXPERIMENTS
In this section , we answer the following research questions based on our experimental results : RQ1 In which search scenario can the optimization of search result presentation achieve significant improvement ?
( a ) Desktop
( b ) Mobile
Figure 5 : Estimated time for reaching the top ranked search result after taking actions aq and as as a function of the number of presented query suggestions . The horizontal dotted lines represent the estimated time for reaching a certain rank r .
RQ2 Can the optimization of search result presentation make steady improvement in a practical setting ?
RQ3 What kind of queries can benefit from the optimization of search result presentation ?
5.1 Settings
We utilized two test collections developed in the NTCIR INTENT1 and 2 Japanese tasks [ 26 , 29 ] . The NTCIR INTENT 1 Japanese test collection includes 100 ambiguous or underspecified queries , while the INTENT 2 Japanese test collection includes 66 ambiguous or underspecified queries . These test collections also contain subtopics for each query , and per subtopic graded relevance for a subset of ClueWeb09 JA .
Although the intent probability for each subtopic was available in both of the test collections , we could not obtain the frequency of intent sets from each assessor in these two test collections . Thus , we used the crowd sourcing service , Lancers , to obtain the frequency of intent sets . We showed workers a topic and its subtopics in random order , and asked them to vote for multiple subtopics assuming that they input the topic as a query to a search engine . We gave the workers an instruction “ Please vote for multiple topics you would be especially interested in if you input ( query ) as
12345Numberofquerysuggestions11121314151617Averagespenttime(sec)DesktopMobile012345Numberofquerysuggestions9101112131415161718Estimatedtime(sec)r=1r=3r=5r=7r=9r=11aqas012345Numberofquerysuggestions6789101112131415Estimatedtime(sec)r=1r=3r=5r=7r=9r=11aqas139 Table 3 : Time and rank loss for each action with n query suggestions .
Time tn,aq ( Rank R(tn,aq ) ) Desktop
Mobile
Time tn,as ( Rank R(tn,as ) )
Desktop
Mobile
8.41 ( 0.0624 ) 8.51 ( 0.232 ) 8.82 ( 0.693 ) 9.64 ( 1.95 ) 11.9 ( 5.36 )
5.95 ( 0.00431 ) 5.96 ( 0.0160 ) 5.99 ( 0.0479 ) 6.05 ( 0.135 ) 6.23 ( 0.370 )
10.0 ( 2.50 ) 10.2 ( 2.81 ) 10.8 ( 3.64 ) 12.2 ( 5.89 ) 16.2 ( 12.0 )
8.50 ( 3.35 ) 8.64 ( 3.53 ) 9.03 ( 4.05 ) 10.1 ( 5.41 ) 12.9 ( 9.15 ) n 1 2 3 4 5 a search keyword ” , where “ ( query ) ” was replaced with an actual query in the test collections . We assigned 20 workers for each topic , and used resultant data for estimating probabilities P ( a|q , S ) and P ( X|q , S , a ) as we explained in Section 33 . The only parameter that we have in our optimization method but did not estimate from the user study was pQS , a probability of clicking on a query suggestion when it is only an intent relevant to the user . Thus , we tested our method with different values for pQS in our experiments .
We used DCG , ERR , and TBG with cutoff parameter N = 10 for the optimization and evaluation . The expected time used in TBG is defined as follows : r−1 r=1
Ti(r ) =
TS + TD(r
)P ( C = 1|R = gi(r
) ) .
( 20 )
Time to evaluate the snippet of a search result , TS = 4.4 , and time to judge document at rank r , TD(r ) = 0.018· len(r ) + 7.8 ( len(r ) is the number of words in the document ) , were calibrated on the basis of desktop search tasks in English [ 28 ] . As we used Japanese documents in our experiments , TD(r ) was slightly modified by reference to statistics that the average reading speed is 300 words per minute in English and 500 words per minute in Japanese . Therefore , we used the following settings in a desktop environment , ie TS = 4.4 and TD(r ) = 0.011 · len(r ) + 78 We changed the settings in a mobile environment based on the time spent per search result in the user study : Tr = 0.66 for desktop and Tr = 0.76 for mobile . We simply multiplied the time parameters by the ratio 076/066 , ie TS = 5.1 and TD(r ) = 0.013 · len(r ) + 91 The document length len(r ) was set to 1,500 for convenience , which was the average document length in the test collections . We also made small changes in TBG to deal with graded relevance . The gain in TBG was defined as Gi(r ) = 0.493gi(r ) , and probability of click on a relevant snippet , P ( C = 1|R = gi(r) ) , was set to 0.64 if gi(r ) > 0 ; otherwise , 039 Thus , we assumed that the gain is proportional to the relevance grade , and probability of click on a snippet is constant for non zero relevance grades .
The three evaluation metrics , DCG , ERR , and TBG , have different characteristics , and may be suitable for evaluation in different scenarios . When we use β = 2 for DCG ( the base of the logarithm ) and h = 224 for TBG ( the half life of users ) , ERR exhibits the highest top heaviness , while TBG exhibits the lowest top heaviness . The top heaviness is possibly measured by the rank at which the evaluation metric reduces the gain to half in the case where no relevant search results appears : 2 for ERR , 3 for DCG , and approximately 18/15 in desktop/mobile scenarios for TBG . Thus , ERR may reflect very impatient users , while TBG may represent patient users who can continue search for a long time . In our experiments , we report results with these three evaluation metrics , and discuss scenarios where the optimization of search result presentation was effective .
We tested our optimization algorithm with two systems . One is oracle system in which we utilized the ground truth of joint intent probability and per subtopic graded relevance for demonstrating the characteristics of our optimization method . Another is practical system where we estimated the joint intent probability and per subtopic graded relevance . To estimate the joint intent probability , we used Bing Search API4 to retrieve top 100 Web documents returned in response to each query , regarded the appearance of a term included in a subtopic as a vote , and obtained an intent set from each document . The rest of the estimation was the same as that based on a crowd sourcing service . We trained a regressor by using the per subtopic graded relevance of 66 queries used in NTCIR INTENT 2 , and predicted the per subtopic graded relevance of 100 queries used in NTCIR INTENT 1 . Thus , only 100 queries were used in the experiments with the practical system . We used gradient boosted regression trees ( GBRT ) as a regressor , since it has been shown to be reasonably effective in a learning to rank challenge [ 9 ] . We extracted seven types of features that have been frequently used for learning to rank [ 24 ] , from each pair of ( subtopic , title ) , ( subtopic , body ) , and ( subtopic , title and body ) . The features included the length of text , number of subtopic terms included , sum of the frequency of subtopic terms , TF IDF score , BM25 score , and language model scores with Dirichlet and Jelinek Mercer smoothing methods . The details of the features are described in our dataset3 . The best parameters for GBRT were determined by 3 fold cross validation with the NTCIR INTENT 2 test collection . We regarded subtopics as queries and evaluated the accuracy of the predicted per subtopic relevance grades by normalized DCG . The best single feature , a TF IDF score for the subtopicbody pair , achieve 0.189 , while GBRT achieved 0297 5.2 Experimental Results RQ1 In which search scenario can the optimization of search result presentation achieve significant improvement ?
Table 4 shows the improvement by the optimization of search result presentation with the oracle system , and percentage of queries for which n query suggestions were estimated to be the best . The improvement was defined as the percent increase from the optimized single ranked list to those with query suggestions in terms of a certain evaluation metric , ie 100·(E[u|q , S∗ , D∗]−E[u|q,∅ , aq , d(aq )∗ ] ) /E[u|q,∅ , aq , d(aq )∗ ] , where asterisks indicate that the search results and query suggestions were optimized by Algorithm 1 . Note that the improvement was averaged over queries for which our optimization algorithm decided to present query suggestions . When we used DCG or TBG as an evaluation metric , our optimization could achieve significant improvement in both of the devices , and even with a small probability of clicking on a query suggestion ( eg PQS = 025 ) While one or two query suggestions were estimated to be the best for DCG , two or more query suggestions were the best for TBG . Moreover , query suggestions were presented for many queries when TBG was used . On the other hand , presenting query suggestions was estimated to be ineffective when ERR was used : very small or no improvement can be seen in any settings . As we discussed earlier , ERR is suitable for measuring the satisfaction of impatient users , while TBG is suitable for patient , careful users . Therefore , we can hypothesize that the optimization of search result presentation was helpful for users who want to carefully gather information and can spend a long time , while it might be disrupting for users who want to quickly find a single answer .
Figure 6 shows the effect of the half life in TBG with pQS = 1 . The x axis indicates ranks at which the gain was reduced to half by used half life parameters when no relevant result appears , and y axis indicates the improvement by the search result presentation 4https://datamarketazurecom/dataset/bing/search
140 Table 4 : Improvement by the optimization of search result presentation for different values for pQS . Significant differences at α = 0.05 are indicated by bold font and asterisks ( * ) .
Device ( Metric )
Desktop ( ERR )
Desktop ( DCG )
Desktop ( TBG )
Mobile ( ERR )
Mobile ( DCG )
Mobile ( TBG ) pQS
0.25 0.50 0.75 1.00 0.25 0.50 0.75 1.00 0.25 0.50 0.75 1.00
0.25 0.50 0.75 1.00 0.25 0.50 0.75 1.00 0.25 0.50 0.75 1.00
Gain ( % )
Cohen ’s d
N/A N/A 0.21 2.51 1.18* 2.02* 3.77* 4.66* 1.84* 3.81* 5.95* 9.23*
0.33 1.18 0.49 0.56* 0.77* 1.66* 2.72* 3.84* 1.90* 3.86* 5.91* 9.19*
N/A N/A N/A N/A 2.44 0.97 1.21 1.11 1.34 1.37 1.39 1.51
3.20 2.96 0.90 0.79 0.88 0.92 0.99 1.02 1.36 1.38 1.38 1.50
% of queries for which n QSs were the best
1 0 0
0.60 0.60 4.20 13.3 15.1 18.7 7.20 8.40 10.8 12.7
1.20 1.20 3.60 6.60 21.1 21.7 21.1 21.7 6.60 6.60 11.4 12.0
2 0 0 0 0 0 0 0
1.20 39.8 34.3 33.1 30.1
0 0 0 0
3.00 4.20 4.80 5.40 20.5 23.5 28.3 26.5
3 0 0 0 0 0 0 0 0
4 0 0 0 0 0 0 0 0
38.6 36.1 32.5 36.1
7.80 14.5 17.5 15.7
0 0 0 0 0 0 0 0
0 0 0 0 0 0
0.60 0.60 39.8 37.3 29.5 33.1
5 0 0 0 0 0 0 0 0 0
0.60 0.60 0.60
0 0 0 0 0 0 0 0
24.1 22.9 23.5 21.1
2.40 3.00 1.80 1.80 optimization with 95 % confidence intervals . For example , x = 3 indicates h = 27.5 ( desktop ) and 31.7 ( mobile ) , by which the gain at the third rank was reduced to half . As we expected , the improvement decreased as the half life decreased , yet it was still large even at x = 3 . This suggests that our optimization is effective in desktop or mobile environments even if we assume that half of the users stop reading a SERP after 27.5 or 31.7 ( sec ) .
In summary , our optimization could achieve significant improvement when we assume that users are moderately patient ( ie ones expected in DCG and TBG ) . The more patient users are , the more effectively search engines can improve the expected utility of such users by several query suggestions . RQ2 Can the optimization of search result presentation make steady improvement in a practical setting ?
Figure 6 shows the improvement by the oracle and practical systems . Each of six areas in the figure shows results with different metrics and devices . Significant improvement by the practical system was indicated by an asterisk ( α = 005 ) While the optimization with ERR ( left ) failed to improve the expected utility , the optimization with DCG ( center ) and TBG ( right ) could achieve steady improvement . Although the TBG based optimization could achieve limited improvement , the DCG based optimization seems robust to the error in the intent probability and per subtopic relevance grades . One possible explanation is the risk of presenting search results relevant to a specific intent . The relevance estimation possibly failed for a certain type of intents . For example , documents relevant to a certain type of intents may not include terms in their subtopic , and vice versa . The risk of presenting many irrelevant search results in a SERP , therefore , can increase as the number of presented query suggestions increases . Thus , it is possible that a method likely to suggest many queries , ie the TBG based optimization , shows lower performance when the estimation of intent probability and relevance grades is not accurate .
Figure 6 : Improvement by the optimization with TBG as a function of the rank at which the gain is reduced to half .
Figure 7 : Improvement by the oracle and practical systems . Significant improvement by the practical system was indicated by asterisks ( α = 005 )
RQ3 What kind of queries can benefit from the optimization of search result presentation ?
Intuitively , the optimization with query suggestions should work effectively if we could decrease the uncertainty of intents by presenting query suggestions . To investigate this intuition , we measured expected relative entropy of intents , and examined the correlation between the improvement and entropy .
Figure 8 shows the improvement by the oracle system using TBG and pQS = 1 in the desktop setting as a function of the expected relative entropy , which was computed as follows :
P ( i ∈ X|q)(H[X ] − H[X|i ∈ X ] )
( 21 )
E[H ] = where H[X ] = − X ] = − i∈I i∈I P ( i ∈ X ) log P ( i ∈ X ) and H[X|i ∈ j∈I P ( j ∈ X|i ∈ X ) log P ( j ∈ X|i ∈ X ) . The expected relative entropy reflects the expectation of the amount of information that can be obtained when one knows a user has an intent . Thus , high expected relative entropy indicates that an intent often co occurs with not all , but a small subset of intents . We can see medium positive correlation between the performance improvement and expected relative entropy ( r = 0.528 , p < 001 ) Some of the most improved topics were indicated in Figure 8 : ID : 159 “ Iseya ” a popular and traditional name for kabuki actors , restaurants , or confectionery shops including 18 diverse subtopics .
036912151821Rankreducinggaintohalf567891011Improvement(%)with95DesktopMobile−8−6−4−20246810Improvement(%)Device=Desktop—Metric=ERRDevice=Desktop—Metric=DCG**Device=Desktop—Metric=TBG0250507510pQS−8−6−4−20246810Improvement(%)Device=Mobile—Metric=ERR0250507510pQS**Device=Mobile—Metric=DCG0250507510pQS*Device=Mobile—Metric=TBGPracticalOracle141 [ 7 ] J . Carbonell and J . Goldstein . The use of mmr , diversity based reranking for reordering documents and producing summaries . In SIGIR , pages 335–336 , 1998 .
[ 8 ] B . Carterette and P . Chandar . Probabilistic models of ranking novel documents for faceted topic retrieval . In CIKM , pages 1287–1296 , 2009 .
[ 9 ] O . Chapelle and Y . Chang . Yahoo! learning to rank challenge overview . In Yahoo! Learning to Rank Challenge , pages 1–24 , 2011 .
[ 10 ] O . Chapelle , S . Ji , C . Liao , E . Velipasaoglu , L . Lai , and S L Wu .
Intent based diversification of web search results : metrics and algorithms . Information Retrieval , 14(6):572–592 , 2011 .
[ 11 ] O . Chapelle , D . Metlzer , Y . Zhang , and P . Grinspan . Expected reciprocal rank for graded relevance . In CIKM , pages 621–630 , 2009 .
[ 12 ] H . Chen and D . R . Karger . Less is more : probabilistic models for retrieving fewer relevant documents . In SIGIR , pages 429–436 , 2006 .
[ 13 ] H . Cui , J R Wen , J Y Nie , and W Y Ma . Query expansion by mining user logs . IEEE TKDE , 15(4):829–839 , 2003 .
[ 14 ] Z . Dou , S . Hu , K . Chen , R . Song , and J R Wen . Multi dimensional search result diversification . In WSDM , pages 475–484 , 2011 . [ 15 ] M . D . Dunlop . Time , relevance and interaction modelling for information retrieval . In SIGIR Forum , volume 31 , pages 206–213 , 1997 .
[ 16 ] J . He , V . Hollink , and A . de Vries . Combining implicit and explicit topic representations for result diversification . In SIGIR , pages 851–860 , 2012 .
[ 17 ] K . Järvelin and J . Kekäläinen . Cumulated gain based evaluation of ir techniques . ACM TOIS , 20(4):422–446 , 2002 .
[ 18 ] K . Järvelin , S . L . Price , L . M . L . Delcambre , and M . L . Nielsen . Discounted cumulated gain based evaluation of multiple query ir sessions . In ECIR , pages 4–15 , 2008 .
[ 19 ] T . Joachims , L . Granka , B . Pan , H . Hembrooke , and G . Gay .
Accurately interpreting clickthrough data as implicit feedback . In SIGIR , pages 154–161 , 2005 .
[ 20 ] M . P . Kato , T . Sakai , and K . Tanaka . Structured query suggestion for specialization and parallel movement : effect on search behaviors . In WWW , pages 389–398 , 2012 .
[ 21 ] M . P . Kato , T . Yamamoto , H . Ohshima , and K . Tanaka . Investigating users’ query formulations for cognitive search intents . In SIGIR , pages 577–586 , 2014 .
[ 22 ] D . Lagun , C H Hsieh , D . Webster , and V . Navalpakkam . Towards better measurement of attention and satisfaction in mobile search . In SIGIR , pages 113–122 , 2014 .
[ 23 ] S . Liang , Z . Ren , and M . de Rijke . Fusion helps diversification . In
SIGIR , pages 303–312 , 2014 .
[ 24 ] T Y Liu , J . Xu , T . Qin , W . Xiong , and H . Li . Letor : Benchmark dataset for research on learning to rank for information retrieval . In SIGIR 2007 workshop on learning to rank for information retrieval , pages 3–10 , 2007 .
[ 25 ] F . Radlinski , R . Kleinberg , and T . Joachims . Learning diverse rankings with multi armed bandits . In ICML , pages 784–791 , 2008 .
[ 26 ] T . Sakai , Z . Dou , T . Yamamoto , Y . Liu , M . Zhang , R . Song , M . P .
Kato , and M . Iwata . Overview of the NTCIR 10 INTENT 2 Task . In NTCIR 10 , 2013 .
[ 27 ] R . L . Santos , C . Macdonald , and I . Ounis . Exploiting query reformulations for web search result diversification . In WWW , pages 881–890 , 2010 .
[ 28 ] M . D . Smucker and C . L . Clarke . Time based calibration of effectiveness measures . In SIGIR , pages 95–104 , 2012 .
[ 29 ] R . Song , M . Zhang , T . Sakai , M . P . Kato , Y . Liu , M . Sugimoto ,
Q . Wang , and N . Orii . Overview of the NTCIR 9 INTENT Task . In the 9th NTCIR Workshop Meeting on Evaluation of Information Access Technologies ( NTCIR 9 ) , pages 82–105 , 2011 .
[ 30 ] H T Yu and F . Ren . Search result diversification via filling up multiple knapsacks . In CIKM , pages 609–618 , 2014 .
[ 31 ] C . X . Zhai , W . W . Cohen , and J . Lafferty . Beyond independent relevance : methods and evaluation metrics for subtopic retrieval . In SIGIR , pages 10–17 , 2003 .
[ 32 ] Y . Zhu , Y . Lan , J . Guo , X . Cheng , and S . Niu . Learning for search result diversification . In SIGIR , pages 293–302 , 2014 .
Figure 8 : Improvement by the optimization with TBG as a function of the expected relative entropy .
ID : 180 “ Tachibana high school ” a name for totally different high schools in Japan .
ID : 337 “ kiwi ” an underspecified query including “ lose weight ” , “ buy ” , “ cultivation ” , and “ alcohol ” intents , all of which do not often co occur with the others .
As indicated by the correlation coefficient and examples , our optimization can improve the expected utility especially for queries whose intents do not often co occur with the others , or co occur with a small subset of intents .
6 . CONCLUSIONS
We proposed a method of optimizing search result presentation for an ambiguous or underspecified query . We developed a searcher model interacting with query suggestions in accordance with their multiple intents , conducted a user study to examine the effects of query suggestions on search behaviors , and calibrated parameters used in the searcher model . Our experiments demonstrated the effectiveness of the search result presentation optimization , and showed that the optimization was effective especially for patient users and queries with a limited number of intents . Our future work includes experiments with real users , and integration of query suggestion generation and search result presentation optimization .
7 . ACKNOWLEDGEMENTS
This work was supported in part by the following projects : Grantsin Aid for Scientific Research ( Nos . 25240050 , 15H01718 , and 26700009 ) from MEXT of Japan , and Microsoft Research CORE Project .
8 . REFERENCES [ 1 ] Statcounter global stats . Retrieved January 14 , 2015 , from http://gsstatcountercom/#desktop+mobile comparison ww monthly 201312 201412 .
[ 2 ] M . Ageev , Q . Guo , D . Lagun , and E . Agichtein . Find it if you can : a game for modeling different types of web search success using interaction data . In SIGIR , pages 345–354 , 2011 .
[ 3 ] R . Agrawal , S . Gollapudi , A . Halverson , and S . Ieong . Diversifying search results . In WSDM , pages 5–14 , 2009 .
[ 4 ] A . Anagnostopoulos , L . Becchetti , C . Castillo , and A . Gionis . An optimization framework for query recommendation . In WSDM , pages 161–170 , 2010 .
[ 5 ] F . Baskaya , H . Keskustalo , and K . Järvelin . Time drives interaction : simulating sessions in diverse searching environments . In SIGIR , pages 105–114 , 2012 .
[ 6 ] C . Brandt , T . Joachims , Y . Yue , and J . Bank . Dynamic ranked retrieval . In WSDM , pages 247–256 , 2011 .
−01000102030405Expectedrelativeentropy−10010203040506070Improvement(%)y=84541x+6234(r=0528,p<001)ID:180ID:159ID:337142
