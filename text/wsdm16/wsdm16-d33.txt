Discriminative Learning of Infection Models
Nir Rosenfeld
School of Computer Science
The Hebrew University of
Jerusalem nirrosenfeld@mailhujiacil
Mor Nitzan
Racah Institute of Physics and
Faculty of Medicine
The Hebrew University of
Jerusalem mornitzan@mailhujiacil
Amir Globerson
School of Computer Science
Tel Aviv University gamir@posttauacil
ABSTRACT Infection and diffusion processes over networks arise in many domains . These introduce many challenging prediction tasks , such as influence estimation , trend prediction , and epidemic source localization . The standard approach to such problems is generative : assume an underlying infection model , learn its parameters , and infer the required output . In order to learn efficiently , the chosen infection models are often simple , and learning is focused on inferring the parameters of the model rather than on optimizing prediction accuracy . Here we argue that for prediction tasks , a discriminative approach is more adequate . We introduce DIMPLE , a novel discriminative learning framework for training classifiers based on dynamic infection models . We show how highly non linear predictors based on infection models can be “ linearized ” by considering a larger class of prediction functions . Efficient learning over this class is performed by constructing “ infection kernels ” based on the outputs of infection models , and can be plugged into any kernelsupporting framework . DIMPLE can be applied to virtually any infection related prediction task and any infection model for which the desired output can be calculated or simulated . For influence estimation in well known infection models , we show that the kernel can either be computed in closed form , or reduces to estimating co influence of seed pairs . We apply DIMPLE to the tasks of influence estimation on synthetic and real data from Digg , and to predicting customer network value in Polly , a viral phone based development related service deployed in low literate communities . Our results show that DIMPLE outperforms strong baselines .
1 .
INTRODUCTION
Infection processes arise in many different contexts and are very diverse in nature , extending from the spread of diseases , rumors and ideas , to the propagation of information and the adoption of new products . Such processes are often modeled as stochastic processes over networks . In social contexts , the network edges represent the medium over which “ infections ” occur , such as friendship relations , physical proximity , or communicational links . Infection processes introduce challenging analysis and prediction problems , with many practical applications . Recent years have seen an abundance of work in this domain , including conditions for epidemic outbreaks [ 30 ] , outbreak detection [ 23 ] , epidemic source localization [ 39 ] , optimal immunization strategies [ 12 ] , influence and trend prediction [ 7 , 10 ] , and influence maximization [ 9 , 19 ] , to name a few .
Our focus in the current work is on predicting various outcomes of diffusion processes . There are many possible outcomes of interest , such as the number of people infected , whether a particular area of a network is infected , or the overall cost of the infection . For such tasks , a straightforward generative approach is to assume a simple infection model , estimate its parameters , and infer the desired outputs from the learned model . This approach is appealingly simple , and is indeed used by the majority of works in this domain ( eg [ 2 , 10 , 15 , 26 , 35 , 36] ) . Nonetheless , such a generative approach has several drawbacks . First , the general hardness of parameter estimation and inference often leads to the a priori selection of simple infection models , which offer computational benefits , but lack explanatory power.1 Second , algorithms for learning and inference are usually custom made for a specific task under a specific infection model , and do not generalize to other tasks and models . Third , if the task at hand is to predict some quantity related to the infection process , learning the model parameters may not be optimal with respect to the prediction task . This is especially true if the assumed model is only a coarse approximation of the true dynamics .
An alternative to the above is to take a discriminative approach and train a predictor to maximize the accuracy on the task of interest . While this is generally an effective approach , in infection related problems it is not clear how to extract informative features , what the proper representation might be , or even what hypothesis classes would contain good predictors . Moreover , different tasks may require different representations , even though they might share the same underlying dynamics .
In this work we introduce DIMPLE ( Discriminative Infection Model Prediction and LEarning ) , a novel discriminative learning framework for general prediction tasks in the domain of infection processes over networks . By introducing
1For instance , in the independent cascade model , the assumed independence between infections leads to decomposable objectives [ 25 , 26 ] , or to tractable combinatorial problems over trees and DAGs [ 35 ] .
563 infection kernels , we show how predictors based on generative infection models can be learned discriminatively within standard kernel supporting methods . DIMPLE fuses the generative and discriminative approaches : it enjoys the benefits of each , and overcomes several of the aforementioned drawbacks . Specifically , it enjoys the following properties :
Generality : DIMPLE can be used for learning to predict potentially any property related to an infection process . The only requirement is that the property can be calculated , approximated , or estimated ( eg by simulating the dynamics ) . This is a relatively weak condition since even complex infection models which are hard to learn are usually straightforward to simulate . DIMPLE can therefore be applied to various tasks such as influence estimation , user activity prediction , epidemic source localization , and others .
Prediction oriented approach : By learning an infection model in a discriminative fashion , we are able to produce predictions with lower error compared to generative methods . Instead of trying to learn the “ true ” parameters of a simple infection model , we learn an optimal weighting of parameters with respect to the given prediction task .
Convex optimization : Our approach employs a convex optimization problem , and is therefore free of local optima issues , which are sometimes an issue when maximizing likelihood . Specifically , our learner uses a kernel based Support Vector Regression ( SVR ) optimizer .
The remainder of the paper is organized as follows .
In Sections 2 and 3 we provide the problem definition and solution framework . In sections 4 and 5 we show how to compute infection kernels for both known and general infection models . In Section 6 we review related work on infection models . Section 7 contains experiments on the tasks of influence estimation and predicting customer network value for both synthetic and real data . Conclusions are summarized in Section 8 . 2 . PROBLEM DEFINITION
Consider a network over which some infection process takes place . An infection process can be thought of as any process where the state or action of a node in the network can ( potentially ) affect the future states or actions of its neighboring nodes . Such processes , although often governed by simple local rules , can give rise to complex , global phenomena .
Given a ( possibly partial ) history of network states , our task is to predict some property of future outcomes . As a working example in this section we consider the task of influence estimation : given a network G = ( V , E ) and a set of initially infected “ seed ” nodes x ⊆ V , predict the expected number of eventually infected nodes y ∈ R+ [ 19 ] . setting , we are given a sample set((xm , ym))M
We take a probabilistic perspective and assume the process is stochastic in nature , namely that there exists some joint probability distribution P ( x , y ) . In our discriminative sampled from P ( x , y ) , where xm and ym denote a seed and its influence , respectively . Our goal is to learn a low error prediction function f : X → Y , which will generalize well to unseen samples . Formally , we would like f to have a low generalization error EP [ (f ( x ) , y ) ] for some loss function : Y × Y → R+ . Here we set to be the squared loss , but our method easily applies to the absolute loss , the 0 1 loss , and others . m=1
3 . SOLUTION FRAMEWORK DIMPLE expected squared error E.(f ( x ) − y)2fi over functions f in
In regression , where performance is measured by mean squared error ( MSE ) , the learning task is to minimize the some hypothesis class F . Since the task at hand is predicting outcomes of infection processes , our choice of hypothesis class is inspired by models of such processes . For instance , our hypothesis class may contain functions based on different parameterizations of the independent cascade model , where each parameterization θ corresponds to a different set of edge infection probabilities {θij ∈ [ 0 , 1]}(i,j)∈E . 3.1 Model Based Regression Functions
When the distribution P ( x , y ) is known , the predictor minimizing the MSE can be evaluated , and is given by the conditional mean [ 3 ] , namely the expectation of y given x : EP [ y|x].2 Since in general P is unknown , we do not have access to the true conditional mean . However , for any stochastic dynamical infection model parameterized by θ ∈ Θ , we can consider the hypothesis class :
FΘ =(f : f ( x ; θ ) = Eθ [ y|x ] , θ ∈ Θ )
( 1 ) where the expectation is taken wrt the conditional probability P ( y|x ; θ ) corresponding to the infection model .
Intuitively our hypothesis class contains the conditional means of all distributions generated by some parametrization of our model ; if our model captures the true dynamics , FΘ will contain the true conditional mean . 3.2 Learning After choosing an infection model , our next step is to learn the optimal regression function f ∈ FΘ , or equivalently , to find the θ ∈ Θ which minimizes the expected loss . In the Empirical Risk Minimization paradigm [ 42 ] , the goal of minimizing the expected loss is replaced with the goal of minimizing the empirical loss over the training samples . In our case , the learning objective is given by :
,f ( xm ; θ ) − ym 2 ,
M m=1 min θ∈Θ
1 M f ( x ; θ ) = Eθ [ y|x ]
( 2 )
Given the nature of the regression functions we consider , this objective may be hard to optimize since most infection dynamics have a complex , highly non linear and non convex dependence on θ . Non linear effects such as phase transitions and tipping points are notorious hallmarks of processes resulting from such complex dependencies ( eg , see [ 11] ) .
Since f ( x ; θ ) can be computed or estimated , one alternative for solving Eq ( 2 ) is to apply non convex optimization techniques such as gradient descent or local search methods . When θ is high dimensional , this approach suffers from several drawbacks . First , it often gets stuck in suboptimal local minima , leading to poor performance . Second , it can be computationally costly , since in general the gradient of Eq ( 2 ) cannot be computed analytically and must be estimated empirically . Third , constraining our regression function to a single model may not be expressive enough for prediction tasks where the real dynamics are complex . In the next section we propose extending FΘ to a larger hypothesis class , which contains linear predictors and is
2For the absolute loss , the optimal prediction is the median of P ( y|x ) , and for the 0 1 loss it is argmaxy P ( y|x ) [ 3 ] .
564 more expressive . Its linearity allows for efficient optimization , and its expressiveness can benefit accuracy . 3.3 Extending the Hypothesis Class
Consider the following identity : f ( x ; θ ) = f ( x ; θ
)dθ
)δθ(θ
Θ
( 3 ) where δθ(θ ) = δ(θ − θ ) is the Dirac delta function centered at θ . Thus , the set of functions FΘ is isomorphic to the set of delta functions δθ(θ ) . This of course does not make learning any easier . It will however be worthwhile to now consider arbitrary functions w(θ ) rather than just δ functions . Consider the set of functions of the form : g(x ; w ) = f ( x ; θ)w(θ)dθ
( 4 )
Θ
Assuming f and w are square integrable over Θ , we can view g as an inner product over functions in L2 , denoted : g(x ; w ) = f ( x ) , w
( 5 )
In what follows we will switch from optimizing over functions in FΘ to optimizing over the set of possible g(x ; w ) functions , namely :
G =(g : g(x ; w ) = f ( x ) , w , w ∈ L2 )
( 6 ) The set G includes functions which are weighted linear combinations of functions in FΘ , with weights given by w(θ ) ( see Figure 1 ) . The set G is more expressive than FΘ in the following sense : if we were allowed to choose w(θ ) = δθ(θ ) then clearly any function in FΘ would have been in G ( due to Eq ( 3) ) . This is not formally the case , since δ are not square integrable . However , we can find a square integrable function w that is arbitrarily close to δ and thus G is a superset of F up to arbitrary precision .
Plugging back into the learning objective and adding regularization produces a convex optimization problem over the space of square integrable functions w ∈ L2 :
M m=1 min w∈L2
1 M
,f ( xm ) , w − ym 2 + w2
2
λ 2
( 7 )
Although the objective is now convex , it is not clear how the function w can be represented or handled . Luckily , by applying the Representer Theorem [ 38 , 20 ] , a predictor with the optimal weight function w∗ can be represented as : g(x ; w
∗
) =
αmK(x , xm )
( 8 ) for some coefficients α , where K(· ,· ) is the kernel function :
M m=1
Figure 1 : An illustration of learning a single δθ over FΘ ( such as in maximum likelihood ) versus learning a w weighted mixture of parameters over G .
4 . COMPUTING THE KERNEL MATRIX
In order to compute K(x , x ) , the product f ( x ; θ)f ( x ; θ ) needs to be integrated over the whole range of θ . In general this can be a hard task since θ may be high dimensional . In Sec 4.1 , 4.2 we show that in some important cases the computational difficulty can be reduced to the complexity of simulating the model . In Sec 4.3 we discuss other cases where the integral is estimated via sampling .
4.1 Independent Cascade Kernel
One of the most studied infection models is the independent cascade model [ 13 , 19 ] . In its simplest discrete time variant , if node i is infected at time t , it attempts to infect each of its uninfected neighbors j independently with probability of success θij . If i succeeds in infecting j , j becomes infected at time t + 1 . After its infection attempts , i becomes inactive . In our setting , a network is initialized with a set of infected seed nodes . This triggers a cascade of infections , which carries on until all nodes are infected or until the process dies out .
Calculating the kernel for this process can be significantly simplified by turning to the following equivalent process , introduced in [ 19 ] . First , sample each edge e ∈ E to be active with probability θe . Then , node i is infected if it is reachable from some node in the seed set x ⊆ V over the graph with active edges A ⊆ E . We denote by Pθ(A ) the probability of A being the active edge set under θ .
To see how this formulation allows us to compute the kernel , consider again the task of influence estimation , where the goal is to predict the expected cascade size ( or number of ultimately infected nodes ) given a set of initially infected seed nodes . In our discriminative setup , given a network G , an example x contains the seed set and a label y is the seed ’s influence . Hence , our predictor for the influence of seed x is f ( x ; θ ) = Eθ [ r(x , A) ] , where r(x , A ) is the number of nodes reachable from x under A . Plugging this into Eq ( 9 ) yields :
) =

Θ
A⊆E
Θ
A,A⊆E
)fi dθ
, A
.r(x 
A⊆E
Θ
Pθ(A)r(x , A )
Pθ(A
)r(x
)
, A r(x , A)r(x
)
, A
Pθ(A)Pθ(A
) dθ
( 10 )
 dθ
K(x , x
) = f ( x ) , f ( x
) = f ( x ; θ)f ( x
; θ)dθ
( 9 )
K(x , x
Eθ [ r(x , A ) ] Eθ
Intuitively , the infection kernel K(x , x ) measures the similarity in infections between two seeds x , x over all θ ∈ Θ . Given the kernel matrix of the data , the above problem can be solved by standard tools such as SVRs [ 42 ] . The difficulty now reduces to computing the kernel matrix , which we address in the following section .
=
=
θ⋆MLω(θ)δθ⋆ML(θ)θ565 Note that now the integral is only over the distribution of active edge sets . Luckily this integral can be calculated in closed form , as shown next . For simplicity assume a uniform prior P ( θ ) over θ . The integral is then equal to a joint distribution QIC over active edge set pairs ( A , A ) :
QIC ( A , A
) =
P ( A , A
, θ ) dθ =
Θ
Pθ(A)Pθ(A
)P ( θ ) dθ ∝
=
Θ
Θ
( 11 )
Pθ(A)Pθ(A
) dθ
θji , or no incoming edges with probability 1−
The linear threshold model also has an equivalent formulation using sampled active edge sets [ 19 ] . First , for each node i we sample either one incoming edge ( j , i ) with probability j θji . Then , the set of infected nodes are those reachable from some seed node in the active subgraph . Let θ·i and A·i be the parameters and active state of all incoming edges of node i , respectively . Since all θji ∈ θ·i must sum to no more than one , they are now dependent . Hence , while Eq ( 10 ) still holds , the joint distribution QLT ( A , A ) now factorizes only over nodes :
Since edges are sampled independently as Bernoulli variables , the probability QIC also factorizes over the edges of the graph , simplifying into :
QLT ( A , A
Pθ(Ae)Pθ(A e ) dθe
=
) ∝ i∈V
Θi
Pθ(A)Pθ(A
Θ
Pθ(A·i)Pθ(A
) dθ
·i ) dθ·i ≡
( 14 )
QLT i
( A·i , A
·i ) i∈V
QIC ( A , A
) =
=
=
Θ e∈E e∈E e∈E e ( 1 − θe)1−Ae θA θAe e ( 1 − θe)1−A e e dθe where Θi is the di dimensional simplex . This gives us:3
Θe
QIC e ( Ae , A e )
QLT i
( A·i , A
·i ) =
2
( di+1)(di+2 )
1
( di+1)(di+2 )
A·i = A ·i A·i = A ·i
( 15 ) where Ae = 1 if e ∈ A and 0 otherwise . The integral can then be calculated , resulting in:3
Ae = A Ae = A Ae = A e = 1 e e = 0
1/6 1/3
1/3 .r(x , A)r(x
)fi
, A
QIC e ( Ae , A e ) =
) ∝
A,A
= E
QIC for Θe = [ 0 , 1].4 Hence , the kernel can be written as :
K(x , x
QIC ( A , A
)r(x , A)r(x
)
, A
( 12 )
( 13 )
An interesting observation is that under this representation , the infection kernel K(x , x ) is in fact the co influence of seeds x , x under a bi dependent cascade model , where pairs of ( dependent ) cascades are generated from the joint infection dynamics governed by QIC . Even though we started out considering influence under all of the possible parameterizations θ , our final kernel entries correspond to co influence under a single parameterization defined by QIC .
Although computing influence is known to be #P hard under the independent cascade ( and linear threshold ) model [ 6 ] , it can be efficiently estimated up to arbitrary precision using sampling [ 8 ] . Similarly K(x , x ) can be estimated by sampling active subgraphs from QIC ( A , A ) , computing r(x , A ) and r(x , A),5 and averaging . 4.2 Linear Threshold Kernel and each of its neighbors j has weight θji , where
Another popular model in the influence literature is the In its weighted formulation , each linear threshold model . node i is associated with a random threshold τi ∼ U ( 0 , 1 ) , j θji ≤ 1 . At each time step , node i becomes infected if the sum of weights of its infected neighbors exceeds τi .
3For details , see the supplementary material . 4For Θe = [ a , b ] , the corresponding probabilities are b3−a3 3(b−a ) , 2(b−a ) − b3−a3 b2−a2 5The estimation of K can be significantly sped up by approximating r(x , A ) using the sketching method of [ 8 ] .
3(b−a ) , and b3−a3
3(b−a ) − b2−a2 b−a + 1 .
·i with probability 2/3 and A·i = A
To sample from QLT , for each node i we first sample the event A·i = A ·i with probability 1/3 . Then , in the first case we uniformly sample a joint edge ( or no edge ) , and in the second case we uniformly sample different edges ( or a single edge in either A·i or A ·i ) . 4.3 General Infection Kernel Computation
In the above two cases we used certain properties of the infection models that facilitated factorization of the kernel integral and its closed form evaluation . For more complex processes such structure may not be available . Note that in the general case there are two sources of difficulty . First , the regression function f ( x ; θ ) = Eθ [ y|x ] may be hard to calculate exactly ( as in the independent cascade model above ) . Second , the kernel integral requires integration over a high dimensional θ space . For the first problem , we assume there exists a black box that yields exact or approximate values of f ( x ; θ ) ( eg with simulation as in [ 19] ) . For the θ integration problem , we resort to computing the integral numerically using Monte Carlo integration [ 27 ] . To illustrate this approach , assume that θ is integrated over some domain set Θ with finite volume C , and define : h(θ ) = f ( xi ; θ)f ( xj ; θ )
( 16 )
We can re write the integral of Eq ( 9 ) as :
K(xi , xj ) = h(θ)dθ = C p(θ)h(θ)dθ
( 17 )
Θ
Θ where p(θ ) = 1/C is the uniform density over Θ . The rightmost term in Eq ( 17 ) is proportional to the expectation of h over p(θ ) , and can be estimated empirically :
EU ( Θ ) [ h(θ ) ] 1 N h(θk ) ,
θk ∼ U ( Θ ) ∀k
( 18 )
Furthermore , the standard error of this estimator decreases asymptomatically as Cσf√ ( where σf is the standard deviation of f wrt the density p ) . There exist much more elaborate sampling schemes , with better control for variance ( eg , Quasi Monte Carlo [ 24 ] , Recursive Stratified Sampling [ 31] ) , though for our experiments uniform sampling
N
N k=1
566 converged fairly quickly . Finally , we note that the process can be significantly sped up by estimating all entries of f over a single set of sampled θ values ( eg , see [ 32] ) .
5 . BRANCHING PROCESSES
In this section we discuss a diffusion process that is com patible with the data used in the experiments in Sec 72
Consider a service that spreads by word of mouth ( WoM ) , where new users join the service based on referrals from friends . In such settings , an important task is to predict the future contribution of new users to the spread of the service , as this can serve as a basis for targeted actions . In marketing , customers are evaluated by their customer lifetime value ( CLV ) the predicted future net profit associated with them . In referral settings , CLV is broken down into a customer ’s intrinsic value and network value the expected profit attributed to her via WoM referrals [ 17 , 9 ] .
An effective way to measure network value is to allocate credit to existing users for recruiting new members . Given a user who recently joined the service , our task is to predict the future referral credit which will be attributed to her . In our discriminative setup , an example x may include features describing a user i and her initial actions , while the label y will contain the credit allocated to her up to some horizon . Our predictor f ( x ; θ ) will be the credit expected to be allocated to user i .
By modeling referral recruitments as infections , we can harness the power of infection models to predict future credit . However , both independent cascade and linear threshold models are inadequate for this task since they operate on a given , complete underlying network . In this setting , credit originates from future parts of the network , which are unobserved at sample time .
A suitable choice for modeling infections when the network is latent is a branching process [ 28 ] . In this model , an infection tree is generated by recursively sampling the number of ( infected ) children a node in the tree has from some distribution . While a branching processes is usually used to model population growth , as an infection model it can be used to predict referral credit in evolving networks .
In a multi level marketing credit scheme , each time a new user i joins the service , one credit point is recursively distributed along a “ recruitment tree ” . Credit from i ’s joining is attributed not only to her parent π(i ) , but also to her grandparent , π2(i ) = π(π(i) ) , and so forth . Quantitatively , out of the one credit point due to i ’s joining , 1/2 is credited to π(i ) , ( 1/2)2 is credited to π2(i ) , ( 1/2)3 to π3(i ) , and in general , πk(i ) gets ( 1/2)k credit points from i . The recursion is bounded by some predetermined depth δ in order to control the bias introduced by the users’ time of joining .
Branching processes are natural candidates as infection models for such recursive credit schemes . To apply a branching process to our task , we set the distribution governing the number of children to be a function of the features x and a weight vector θ . For instance , for a geometric distribution , we can set the success probability p to be a scaled sigmoid : b − a 1 + e−z + a p(x ; θ ) = sig(θ , x ) , sig ( z ) =
( 19 ) where 0 ≤ a < b ≤ 1 . The expected number of offsprings µ(x ; θ ) at each depth in trees generated by such a process is easy to compute analytically . By setting the root to be p(x;θ ) − 1 , the µ0(x ; θ ) = 1 and using µ1(x ; θ ) = µ(x ; θ ) = 1 expected number of nodes at depth is :
µ(x ; θ ) = µ−1(x ; θ)· µ(x ; θ ) =,µ(x ; θ ) and the expected credit of user i is therefore :
δ
=1 f ( x ; θ ) =
1 2 µ(x ; θ )
( 20 )
( 21 )
When Eq ( 19 ) is unscaled ( b = 1 , a = 0 ) , the resulting kernel can be computed in closed form.3 It turns out that 0 < a , b < 1 leads to more stable kernels , and we thus use Sec 4.3 for kernel estimation .
6 . RELATED WORK
Current methods for solving infection related tasks over networks typically assume an underlying infection model . The two most common models are the linear threshold model [ 16 ] and the independent cascade model [ 13 ] . Variations and generalizations of these models have also been studied ( eg [ 19 , 15 , 37] ) . Various methods have been proposed for learning the parameters of these models from data . In [ 15 ] , given a social graph and an action log , the infection probabilities over the edges are inferred using different statistical models under the assumption that the infections are independent . Other works use a maximum likelihood approach for estimating the infection probabilities for various models [ 2 , 36 ] . In [ 26 ] , a convex formulation is provided for maximizing the likelihood of the independent cascade model .
Several works aim at inferring a latent underlying network by estimating the edge infection probabilities . These include NetInf [ 14 ] which infers network connectivity , and ConNIe [ 25 ] and NetRate [ 35 ] , which infer connectivity and infection rates . The models are learned from sample cascades ( who got infected by whom and at what time ) , and used within a maximum likelihood approach to infer the model parameters and network structure .
Another class of methods focus on a specific property of the infection process . A main task in the field of diffusion networks concerns the problem of influence maximization , namely , choosing a seed of nodes which , if infected , will result in the largest expected number of overall infected nodes . The authors in [ 19 ] show how influence can be maximized efficiently in the linear threshold and independent cascade models . In [ 10 ] , the authors use a generative approach to learn an influence predictor over a large class of coverage functions , which generalize several infection models .
Several works take a discriminative approach for predicting various infection related quantities . In [ 7 ] , the authors extract features out of cascades and use various classifiers to predict whether the final size of a given cascade will be above or below the median . In [ 41 ] , the authors extract topological , temporal , and content related features and use linear regression to predict the propagation of information . In [ 43 ] , the authors suggest a Linear Influence Model for estimating node specific non parametric influence functions , which are then used to predict the overall future number of infections . In [ 1 ] regression trees are used to investigate and predict the influence of nodes given their attributes and history .
The interplay between generative and discriminative approaches has been studied in several works . In [ 29 ] the authors analyze settings in which generative classifiers are expected to outperform discriminative ones and vice versa . However , the generative results were under the assumption
567 that the generative model is correct , which rarely holds . In general , the discriminative framework is well supported via the empirical risk minimization framework of Vapnik [ 42 ] and others , which provides sample complexity guarantees for algorithms minimizing empirical classification error .
A very different approach to combine generative and discriminative schemes was suggested in [ 18 ] . They use a learned generative model to construct a kernel matrix that measures the “ distance ” between instances according to the sensitivity of their likelihood to changes in the parameters of the model . In contrast to this , we propose an approach that , like logistic regression , involves full discriminative training while still building on the generative form .
7 . EXPERIMENTAL EVALUATION
We evaluate the performance of DIMPLE on the tasks of influence estimation and customer network value prediction , and compare it to standard feature based regression methods and to relevant generative approaches found in the literature when applicable . Our results show that DIMPLE outperforms other methods in various settings . 7.1 Influence Estimation
We perform experiments on influence estimation on both synthetic and real data . Our synthetic experiments include two different settings , both of which generalize the independent cascade model to more realistic diffusion mechanisms . In our real data experiment we predict the number of votes a Digg6 article receives given a seed of initial voters .
We compared DIMPLE to baselines of two types : discriminative feature based regression methods , and prediction based on generative models . For discriminative methods we used Linear Regression with 2 regularization and Support Vector Regression ( SVR ) [ 42 ] with both a linear ( SVR LIN ) and an RBF kernel ( SVR RBF ) . As features we used structural and topological properties , centrality measures , and statistics computed over the activity log of the infection process ( see Table 1 ) . The generative methods we compared to are NetRate [ 35 ] , InfluLearner [ 10 ] , and the maximum likelihood approach for the discrete time independent cascade model ( DIC ML ) suggested in [ 26].7 The input to DIMPLE includes samples ( x , y ) , where x contains a seed and y is influence . It is important to note that while all other methods require as input the full cascade data ( who was infected , when , and sometimes by whom ) , DIMPLE requires only the network , the seeds , and the number of infected nodes per cascade .
In all synthetic settings , we used networks generated with R MAT [ 4 ] , with parameters that have been shown to generate synthetic networks which mimic real world networks in several aspects.8 Using synthetic networks allows us to average over a large set of diverse instances of both networks and dynamics . 711 Within Cascade Dependencies Simple infection models often assume that when a node is infected , it attempts to infect each of its neighbors independently . However , this assumption does not hold in many 6http://digg.com 7For training SVRs we used LIBSVM [ 5 ] . For NetRate and Influlearner we used code provided by the respective authors . 8Parameters were set to [ 0.5 , 0.1 ; 0.1 , 03 ] These set the recursive block wise structure of the network . real world settings . For instance , if a Toronto resident receives an article concerning upcoming events in Toronto , she might be more likely to forward it to her friends who live in Toronto than to friends who live elsewhere . In general , knowing that node i infected a neighbor j which belongs to some group k can affect the probability of i infecting other neighbors in that group .
In such cases , learning the parameters of a simple , independent model may result in poor performance . To demonstrate this , we first generalize the independent cascade model to allow for “ group dependencies ” and then evaluate different methods for influence estimation under these dynamics .
In our experiment , each node in the network is assigned to one of K groups . When node i is infected , it infects all of its neighbors j who are in group k with probability θik . Hence , in a single infection attempt , either all neighbors in a certain group are infected , or none are . This introduces within cascade dependencies . Notice that when each neighbor belongs to a different group , this model is equivalent to the independent cascade model .
Each instance in the experiment was carried out as follows . First , we generated a network of 1,000 nodes , randomly allocated nodes to K groups , and sampled node group infection probabilities θik uniformly from [ 0 , 025 ] Next , we created 500 train and test samples by randomly choosing seeds of five nodes and simulating the group dependent dynamics . Labels were set by averaging the number of infected nodes over multiple cascades ( 10 for training , 10,000 for test ) . We repeated this procedure for multiple networks and sample sets , and for each instance for several values of K .
We compared the performance of DIMPLE to that of other methods , both discriminative and generative.9 We experimented with two variants of DIMPLE , one with a kernel based on the grouped dynamics ( DIMPLE G ) , and the other based on the standard independent cascade model ( DIMPLE IC ) . The grouped cascade kernel can be computed in a similar fashion to Sec 4.1 by sampling edges in groups.3 As shown in Figure 2 ( left ) , when infections are independent , DIMPLE performs well , though not as well as DICML . However , the performance of DIC ML and other generative methods deteriorates quickly when dependencies are introduced . Furthermore , while DIMPLE only uses seeds and cascade sizes as input , the generative methods require the infection times of all nodes in each cascade . When there are few groups , the prediction task becomes harder due to the high degree of dependence . While feature based methods and NetRate perform better here than with weak dependencies , DIMPLE retains its advantage . It is interesting to note that the DIMPLE variant with the independent kernel performs as well as the group dependent variant across all values of K , even when the data contains dependencies .
712 Between Cascade Dependencies Another assumption in simple infection models is that edge probabilities are constant across cascades , and that cascades are carried out independently of one another . This assumption is not always realistic . For instance , if cascades correspond to different types of content and if users have different contextual preferences , infection probabilities will vary and cascades will become dependent .
9We are not aware of any closed form maximum likelihood estimators for the generalized infection dynamics we discuss .
568 Figure 2 : Influence estimation results . Left : An infected node infects either all or none of its neighbors in each group ( with enough groups , infections become independent ) . Middle : As λ grows , infection probabilities increasingly depend on the cascade features ( at λ = 0 they are independent ) . Right : For Digg , bars show percentage of explained variance ( higher is better ) , and numbers in brackets show RMSE ( lower is better ) .
To explore this setting , we present a generalization of the independent cascade model in which infection probabilities depend on features of nodes and cascades . Given node feature vectors xi , xj and a feature vector c describing the contents of a cascade , we set the edge infection probabilities to : pc ij = sig
( 1 − λ )
αkxi kxj k + λ
βc
( 22 ) k where α , β are weight vectors and λ is a scalar interpolating between fully node defined and fully content defined infection probabilities . Note that when λ = 0 , this reduces to an independent cascade model . Setting λ > 0 introduces between cascade dependencies .
Each instance in the experiment was generated as follows . First , we generated a network of 300 nodes and sampled features and weights for both nodes and cascades uniformly from [ −1 , 1]3 . Next , we created 250 train and test samples by randomly choosing five node seeds and simulating infections using the standard independent cascade model dynamics with edge probabilities pc ij for a given λ . Labels were generated as in Sec 711 We repeated this procedure for multiple networks and sample sets , over a range of λ values . We compared the performance of DIMPLE with a featurebased cascade kernel ( DIMPLE FC ) to other discriminative and generative methods.9 DIMPLE was trained by simulating the dynamics over 1,000 randomly drawn values of ( α , β , λ ) . The kernel was constructed from influence estimations over these simulations . Figure 2 ( middle ) displays the average MAE for a range of λ s . For λ = 0 , where the dynamics are equivalent to the standard independent cascade model , DIMPLE attains comparable results to DICML which learns the ( true in this case ) model parameters using an exact maximum likelihood approach . As λ grows and the cascade features have a higher effect on infection probabilities , the performance of all other methods quickly deteriorates , while DIMPLE maintains a low error . 713 Predicting Digg Votes Digg is a popular social news aggregation platform , where users submit links to news articles and vote on articles submitted by fellow users . Digg also facilitates a social network where users can “ follow ” other users . Followers receive updates on the actions of their followees , such as article submissions and votes ; hence , the submission of an article can trigger a cascade of votes across the network . Since users are exposed to articles either by an update from a followee or via an external source , we consider as the seed of an article both the article ’s submitter and all users who voted on it prior to having any of their followees do so . Our task is to predict the number of votes of an article given its seed .
We use the data provided in [ 21 ] which includes the social network as well as all votes and articles that appeared on Digg ’s front page in June 2009 . As we are interested in the network effect of the spread of votes , we focus on votes made before the article was promoted to the front page , and on voters who are members of the social network . Our sample set includes 3,301 articles submitted by 429 distinct users , with 191,880 votes by a total of 3,767 voting users , of which 1,919 acted as seed nodes . We consider articles with at least one vote ( other than the submitter ’s ) , and edges for which both users co voted on at least 5 articles . Results are averaged over several 75:25 train test splits . As can be seen in Figure 2 ( right ) , DIMPLE with a linear threshold kernel ( DIMPLE LT ) outperforms all other methods on this task , while generative methods preform worse than predicting the mean .
7.2 Predicting Customer Network Value
In this section we demonstrate the versatility of our approach by applying it to the prediction of customer network value in a unique real world dataset . Our goal is to predict the amount of credit attributed to users in a referral based development related phone service . The characteristics of our data and the goal of our prediction task do not fit standard generative methods . As we show , DIMPLE with a branching process kernel outperforms other methods .
Data .
Polly [ 33 , 34 ] is a phone based system , developed by researchers at Carnegie Mellon University and Lahore University of Management Sciences , whose goal is to provide development related services to low literate communities . In its basic form , Polly is a viral voice based game , where users can record messages and forward funny voice altered variations of the original message to friends . On top of this entertainment platform , Polly includes a ‘payload’ : when a user interacts with the system , she is offered to listen to targeted development related content , such as job ads ap
124816indep051015202530num groupsMAENode Groups DIMPLE−ICDIMPLE−GLinRegSVR−LINSVR−RBFNetRateInfluLearnerDIC−ML00204060810510152025λMAENode and Cascade Features DIMPLE−FCLinRegSVR−LINSVR−RBFNetRateInfluLearnerDIC−ML001020304(1820)(1965)(2015)(2039)(4919)(4862)(5842 ) % explained var ( RMSE ) Digg Data DIMPLE−LTLinRegSVR−LINSVR−RBFNetRateInfluLearnerDIC−ML569 Figure 3 : Polly results . Left and middle : MSE and MAE of predicting network value over various train test splits . Bars show percentage of explained variance or absolute error ( higher is better ) , numbers in brackets show MSE or MAE ( lower is better ) . Right : Convergence of the Monte Carlo kernel estimation process , showing the 2 distance between consecutive kernels ( lower blue line ) and the final MAE ( top green line ) . propriate for low skilled workers . A user can then choose to listen to these ads , and also forward them to friends .
In rural areas where Polly was deployed , phone based cellular services are abundant , but literacy is often scarce . The high unemployment rate is attributed not to a lack of jobs , but rather to a lack of information regarding job opportunities , which is often textual . This signifies the importance of a person to person voice based system as a platform for spreading important content such as job ads . The system exploits the high phone based connectivity of its users and the active role they take to make job ads widely accessible . In our experiments , we use data collected from a study carried out with Polly in Pakistan in 2012 [ 34 ] . Polly was seeded to five select individuals , which quickly triggered an exponential rise in activity . For our analysis , we used data collected during the first four months following seeding . During this time , Polly has spread to 21,786 users , who engaged in 46,675 interactions . For generality , the only information we use in our experiments is the message log which consists of who sent a message to whom and when .
Prediction task .
One of the main considerations in Polly is the spread of the service to new users . Since Polly relies solely on wordof mouth , predicting the future contribution of new users to the spread of the service is crucial . A natural way to quantify the contribution of users is to measure their network value using a referral credit scheme like the one described in Sec 5 , where the credit for recruiting a new user is recursively distributed across the edges of a “ recruiting tree ” . Under this setting , our goal is to predict the future credit of new users who performed k interactions with the system up to some time horizon T .
Since the referral process in Polly is infectious in nature , applying methods based on infection processes seems advantageous . However , since the task is to predict infections of latent nodes in an unknown part of the network , standard infection models such as the independent cascade and linear threshold models do not apply here . In contrast , the branching process described in Sec 5 is a natural choice , as it can describe how a credit tree “ grows ” from a single node .
We introduce a small variation on the model in Sec 5 . The Polly data contains many users who do not spread the service . We thus consider two types of nodes : active nodes which can have offsprings , and passive nodes which cannot . At any point in the branching process we assume that the number of active and passive offsprings is geometrically distributed with parameters pact(x ; θa ) and ppas(x ; θp ) , respectively . Denote the corresponding expected values by µact(x ; θa ) and µpas(x ; θp ) . Then , assuming the process starts with a single active node , the expected overall number of nodes at depth is :
µ( , x ; θa , θp ) = µact
( x ; θa ) + µact
−1(x ; θa)µpas(x ; θp )
( 23 ) and the predictor is : f ( x ; θa , θp ) =
δ
=1
1 2 µ( , x ; θa , θp )
( 24 )
To calculate the kernel , we use the approach in Sec 43
Evaluation details .
In our experiments we created a sample x for each new user at the time she sent her k = 3rd message , and set the label y to be the credit she accumulated up to T = 7 days from sample time , while restricting the depth of credit trees to δ = 5 . This resulted in 12,494 samples , with about 500 1,500 samples per week . For each sampled user , we computed a set of features based on the system ’s log up to that time . We used both topological and activity related features of focal nodes , their parents , and their children ( see Table 1 ) .
We split the sample set into a series of training and test sets , with splits taken at one week intervals , and compared DIMPLE to other discriminative methods . Selection of metaparameters was performed by training on the early part of the train set and validating on the later part , with a split proportional to that of the train and test sets . For the branching process in DIMPLE , we chose the geometric distribution for both active and passive nodes , and used the expected credit ( for MSE ) and median credit ( for MAE ) as our predictors . Figure 3 displays the accuracy of DIMPLE and other feature based methods . DIMPLE attains superior performance across all splits . Figure 3 ( right ) presents the convergence rate of the kernel and the reduction in error as a function of the number of Monte Carlo steps . As can be seen , the error drops considerably even after as few as 100 steps , and at 1,000 steps the error is very close to that of 100,000 .
48120010203040506(448)(326)(157)(470)(347)(175)(527)(356)(180)(537)(364)(184)split@week % explained var ( MSE ) DIMPLELinRegSVR−LINSVR−RBF48120010203040506(119)(104)(076)(136)(119)(091)(130)(111)(089)(133)(113)(090)split@week % explained abs err ( MAE)100101102103104105051015Monte−Carlo stepskernel distance0811214MAE570 8 . CONCLUSIONS
In this paper we introduced DIMPLE , a discriminative method for learning predictors for dynamic infection processes over networks . Our predictors are inspired by generative processes but are trained to achieve optimal accuracy . The prediction functions are highly non linear in their original formulation . However , we show that by switching to a larger hypothesis space , the problem is effectively “ linearized ” and one can use kernel methods for optimization .
Switching to the larger hypothesis space G has an advantage in terms of expressive power , but may run the risk of overfitting . As with any kernel based method , it turns out that generalization is actually not dependent on the dimension of the feature space ( which in our case , as in RBF kernels , is infinite since w(θ ) is a function of θ ) , but rather on the margin with which the data can be separated ( eg , see [ 40] ) . As with any other kernel method , whether or not such a large margin exists is problem dependent . The empirical success of DIMPLE in our experiments demonstrates that this is likely to be the case in practice .
Our experiments clearly demonstrate that DIMPLE can be applied to a wide range of tasks and dynamics . In particular , we apply it to the tasks of influence estimation and customer network value prediction .
The method as presented is very general , and is not restricted to infection processes . It can in fact be applied to other tasks involving general dynamic processes over networks , such as link prediction with triadic closure based network evolution models [ 22 ] . We leave this for future work .
Acknowledgments : We would like to thank Jacob Gold enberg and Lev Muchnik for valuable discussions , Roni Rosenfeld for motivation and data , and Elad Eban for inspiration . Mor Nitzan is grateful to the Azrieli Foundation for the award of an Azrieli Fellowship . This work was supported by the Intel Collaborative Research Institute for Computational Intelligence ( ICRI CI ) , and by the Israel Ministry of Science and Technology Knowledge Center in Machine Learning and Artificial Intelligence .
9 . REFERENCES outgoing degree incoming degree # out neighbors # in neighbors mean neis.’ deg . log betweenness eigencentrality clustering coeff . distance histogram # incoming msgs # parents time of rcvd msgs hour of sent msgs self msg indicator rank amongst sons time of first msg time of last msg time to first msg time to last msg
( Polly )
Network value
Influence estimation
F i r s t p a r e n t
F o c a l n o d e
S e e d
S e e d n e i g h b o r s
W i i t h n s e e d
L a s t p a r e n t
S o n s
( fi r s t
3 )
Table 1 : Features used in the experiments . In the influence estimation experiments for each feature we used the mean , min . , and max . over all seed nodes . conf . on Knowledge discovery and data mining , pages 1029–1038 , 2010 .
[ 7 ] J . Cheng , L . Adamic , P . A . Dow , J . M . Kleinberg , and
J . Leskovec . Can cascades be predicted ? In Proc . of the 23rd int . conf . on World wide web , pages 925–936 , 2014 .
[ 8 ] E . Cohen , D . Delling , T . Pajor , and R . F . Werneck .
Sketch based influence maximization and computation : Scaling up with guarantees . In Proc . of the 23rd int . conf . on Conference on Information and Knowledge Management , pages 629–638 , 2014 .
[ 1 ] E . Bakshy , J . M . Hofman , W . A . Mason , and D . J .
[ 9 ] P . Domingos and M . Richardson . Mining the network
Watts . Everyone ’s an influencer : quantifying influence on twitter . In Proc . of the 4th int . conf . on Web search and data mining , pages 65–74 . ACM , 2011 . value of customers . In Proc . of the 7th int . conf . on Knowledge discovery and data mining , pages 57–66 . ACM , 2001 .
[ 2 ] E . Bakshy , B . Karrer , and L . A . Adamic . Social
[ 10 ] N . Du , Y . Liang , E . M F Balcan , and G . EDU . influence and the diffusion of user created content . In Proc . of the 10th ACM conference on Electronic commerce , pages 325–334 . ACM , 2009 .
Influence function learning in information diffusion networks . In Proc . of The 31st int . conf . on Machine Learning , pages 2016–2024 , 2014 .
[ 3 ] J . O . Berger . Statistical decision theory and Bayesian
[ 11 ] V . M . Eguiluz and K . Klemm . Epidemic threshold in analysis . Springer , 1985 .
[ 4 ] D . Chakrabarti , Y . Zhan , and C . Faloutsos . R mat : A recursive model for graph mining . In SDM , volume 4 , pages 442–446 . SIAM , 2004 .
[ 5 ] C C Chang and C J Lin . LIBSVM : A library for support vector machines . ACM Transactions on Intelligent Systems and Technology , 2:27:1–27:27 , 2011 .
[ 6 ] W . Chen , C . Wang , and Y . Wang . Scalable influence maximization for prevalent viral marketing in large scale social networks . In Proc . of the 16th int . structured scale free networks . Phys . Rev . Letters , 89(10):108701 , 2002 .
[ 12 ] G . Giakkoupis , A . Gionis , E . Terzi , and P . Tsaparas .
Models and algorithms for network immunization . Technical report , University of Helsinki , 2005 .
[ 13 ] J . Goldenberg , B . Libai , and E . Muller . Talk of the network : A complex systems look at the underlying process of word of mouth . Marketing letters , 12(3):211–223 , 2001 .
[ 14 ] M . Gomez Rodriguez , J . Leskovec , and A . Krause .
Inferring networks of diffusion and influence . In Proc .
571 [ 32 ] A . Rahimi and B . Recht . Weighted sums of random kitchen sinks : Replacing minimization with randomization in learning . In Advances in neural inf . processing systems , pages 1313–1320 , 2009 .
[ 33 ] A . A . Raza , M . Pervaiz , C . Milo , S . Razaq , G . Alster ,
J . Sherwani , U . Saif , and R . Rosenfeld . Viral entertainment as a vehicle for disseminating speech based services to low literate users . In Proc . of the Fifth int . conf . on Information and Communication Technologies and Development , pages 350–359 . ACM , 2012 .
[ 34 ] A . A . Raza , F . Ul Haq , Z . Tariq , M . Pervaiz ,
S . Razaq , U . Saif , and R . Rosenfeld . Job opportunities through entertainment : Virally spread speech based services for low literate users . In Proc . of the SIGCHI Conference on Human Factors in Computing Systems , pages 2803–2812 . ACM , 2013 .
[ 35 ] M . G . Rodriguez , D . Balduzzi , and B . Sch¨olkopf .
Uncovering the temporal dynamics of diffusion networks . arXiv preprint arXiv:1105.0697 , 2011 .
[ 36 ] K . Saito , R . Nakano , and M . Kimura . Prediction of information diffusion probabilities for independent cascade model . In Knowledge Based Intelligent Information and Engineering Systems , pages 67–75 . Springer , 2008 .
[ 37 ] K . Saito , K . Ohara , Y . Yamagishi , M . Kimura , and H . Motoda . Learning diffusion probability based on node attributes in social networks . In Foundations of Intelligent Systems , pages 153–162 . Springer , 2011 .
[ 38 ] B . Sch¨olkopf , R . Herbrich , and A . J . Smola . A generalized representer theorem . In Computational learning theory , pages 416–426 . Springer , 2001 .
[ 39 ] D . Shah and T . Zaman . Rumors in a network : Who ’s the culprit ? Information Theory , IEEE Transactions on , 57(8):5163–5181 , 2011 .
[ 40 ] J . Shawe Taylor , P . L . Bartlett , R . C . Williamson , and
M . Anthony . Structural risk minimization over data dependent hierarchies . Information Theory , IEEE Transactions on , 44(5):1926–1940 , 1998 .
[ 41 ] O . Tsur and A . Rappoport . What ’s in a hashtag ? content based prediction of the spread of ideas in microblogging communities . In Proc . of the 5th int . conf . on web search and data mining , pages 643–652 , 2012 .
[ 42 ] V . Vapnik . The nature of statistical learning theory .
Springer , 2000 .
[ 43 ] J . Yang and J . Leskovec . Modeling information diffusion in implicit networks . In 10th int . conf . on Data Mining , pages 599–608 . IEEE , 2010 . of the 16th int . conf . on Knowledge discovery and data mining , pages 1019–1028 . ACM , 2010 .
[ 15 ] A . Goyal , F . Bonchi , and L . V . Lakshmanan . Learning influence probabilities in social networks . In Proc . of the third ACM int . conf . on Web search and data mining , pages 241–250 . ACM , 2010 .
[ 16 ] M . Granovetter . Threshold models of collective behavior . American journal of sociology , pages 1420–1443 , 1978 .
[ 17 ] J . E . Hogan , K . N . Lemon , and B . Libai . Quantifying the ripple : Word of mouth and advertising effectiveness . Journal of Advertising Research , 44(03):271–280 , 2004 .
[ 18 ] T . Jaakkola and D . Haussler . Exploiting generative models in discriminative classifiers . In Advances in Neural Inf . Processing Systems , pages 487–493 , 1998 . [ 19 ] D . Kempe , J . Kleinberg , and ´E . Tardos . Maximizing the spread of influence through a social network . In Proc . of the 9th int . conf . on Knowledge discovery and data mining , pages 137–146 . ACM , 2003 .
[ 20 ] G . S . Kimeldorf and G . Wahba . A correspondence between bayesian estimation on stochastic processes and smoothing by splines . The Annals of Math . Stat . , pages 495–502 , 1970 .
[ 21 ] K . Lerman and R . Ghosh . Information contagion : An empirical study of the spread of news on digg and twitter social networks . ICWSM , 10:90–97 , 2010 .
[ 22 ] J . Leskovec , L . Backstrom , R . Kumar , and
A . Tomkins . Microscopic evolution of social networks . In Proc . of the 14th int . conf . on Knowledge discovery and data mining , pages 462–470 . ACM , 2008 .
[ 23 ] J . Leskovec , A . Krause , C . Guestrin , C . Faloutsos ,
J . VanBriesen , and N . Glance . Cost effective outbreak detection in networks . In Proc . of the 13th int . conf . on Knowledge discovery and data mining , pages 420–429 . ACM , 2007 .
[ 24 ] W . J . Morokoff and R . E . Caflisch . Quasi monte carlo integration . Journal of computational physics , 122(2):218–230 , 1995 .
[ 25 ] S . Myers and J . Leskovec . On the convexity of latent social network inference . In Advances in Neural Inf . Processing Systems , pages 1741–1749 , 2010 .
[ 26 ] P . Netrapalli and S . Sanghavi . Learning the graph of epidemic cascades . In ACM SIGMETRICS Performance Evaluation Review , volume 40 , pages 211–222 . ACM , 2012 .
[ 27 ] M . E . Newman , G . T . Barkema , and M . Newman .
Monte Carlo methods in statistical physics , volume 13 . Clarendon Press Oxford , 1999 .
[ 28 ] P . Ney and A . Joffe . Branching processes . Springer ,
1978 .
[ 29 ] A . Y . Ng and M . I . Jordan . On discriminative vs . generative classifiers : A comparison of logistic regression and naive bayes . Advances in neural inf . processing systems , 2:841–848 , 2002 .
[ 30 ] R . Pastor Satorras and A . Vespignani . Epidemic spreading in scale free networks . Phys . rev . letters , 86(14):3200 , 2001 .
[ 31 ] W . H . Press and G . R . Farrar . Recursive stratified sampling for multidimensional monte carlo integration . Computers in Physics , 4(2):190–195 , 1990 .
572
