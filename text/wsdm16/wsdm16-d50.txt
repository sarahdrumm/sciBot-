An Information Theoretic Approach to Individual
Sequential Data Sanitization
Luca Bonomi
Liyue Fan
University of California , San
University of Southern
Diego
La Jolla , CA lbonomi@ucsd.edu
California
Los Angeles , CA liyuefan@usc.edu
Hongxia Jin
Samsung Research America hongxiajin@samsungcom
San Jose , CA
ABSTRACT Fine grained , personal data has been largely , continuously generated nowadays , such as location check ins , web histories , physical activities , etc . Those data sequences are typically shared with untrusted parties for data analysis and promotional services . However , the individually generated sequential data contains behavior patterns and may disclose sensitive information if not properly sanitized . Furthermore , the utility of the released sequence can be adversely affected by sanitization techniques . In this paper , we study the problem of individual sequence data sanitization with minimum utility loss , given user specified sensitive patterns . We propose a privacy notion based on information theory and sanitize sequence data via generalization . We show the optimization problem is hard and develop two efficient heuristic solutions . Extensive experimental evaluations are conducted on real world datasets and the results demonstrate the efficiency and effectiveness of our solutions .
CCS Concepts •Information systems → Data mining ; •Security and privacy → Data anonymization and sanitization ;
Keywords Data Sanitization ; Sequential Patterns ; Mutual Information
1 .
INTRODUCTION
The increasing popularity of GPS enabled mobile devices , apps , and wearable sensors greatly facilitates the vast generation of sequential data of various types . For example , such sequential data generated by a particular user might be a sequence of locations he visited , a sequence of physical activities he performed , or a series of purchase orders he made over time . Clearly , sharing these data sequences with untrusted parties would enable them to perform aggregation and mine patterns over a larger number of individuals , and ultimately to aid business decision making . In return , individual data contributors will benefit from the outcome of such analysis , such as fitness ranking among the user community , restaurant recommendation based on location visited , and coupons based on items bought before .
However , individually generated data sequences contain sequential patterns , ie , a set of atomic actions in a particular order , which may disclose sensitive information if not properly sanitized . Consider the following scenarios , Scenario 1 . A female customer who buys unscented lotions and zinc/magnesium supplements first , and unscented soaps and cotton balls in extra large bags later , is probably pregnant and very close to her delivery date [ 7 ] . Scenario 2 . A doctor leaves a patient ’s room and immediately enters a psychiatrist ’s office [ 29 ] , might indicate that the patient is experiencing psychiatric problems . Scenario 3 . Sensitive physical activities can be inferred by a sequence of simple activities [ 20 ] . For example , going to toilet can be recognized by the following sequence : hand motions , sit , stand , hand motions . To prevent the privacy breaches caused by such patterns , rather than single events or actions , data sequences must be sanitized to hide those sequential patterns prior to release . A plethora of works have been developed to hide association rules [ 28 , 3 ] , frequent itemsets [ 25 ] , or individual event [ 13 ] . However , they cannot be applied to sequential patterns which contain multiple atomic actions in a specific order within certain duration . A few works have been proposed to “ hide ” sequential patterns in a sequence database [ 1 , 11 , 14 ] or individual sequences [ 29 ] , by reducing the occurrences of each sensitive pattern to a given threshold in the sanitized data . Among them , works in [ 1 , 11 , 29 ] adopt a suppression approach by removing individual symbols , eg , events , when a sensitive pattern occurs . The authors of [ 14 ] adopt a permutation approach which re arranges the ordering of symbols in each sensitive pattern in the released sequence . The drawback of suppression based approaches is the data loss inflicted by deleting symbols in the released data , especially when sensitive patterns are frequent patterns . The permutation based approaches break the inherent ordering in the sequential data and create “ ghost ” patterns that do not exist in the original database .
In this paper , we propose to sanitize individual sequence data for user specified sequential patterns via generalization , which has been extensively applied to publishing and mining non sequential data [ 26 , 16 , 15 ] . The key idea is to replace the fine grained symbols , eg items , in the sensitive patterns with coarser data , eg categories , in order to
337 Figure 1 : Setting
Figure 2 : Taxonomy guarantee privacy and to preserve the utility of the sanitized sequence . We propose a novel privacy model which measures the amount of information that an adversary could learn by observing the sanitized sequence . Our framework allows the data owners to specify sensitive patterns , privacy levels , and customized utility metrics . By enabling individuals to have full control over their data , we believe that our solutions could greatly improve current data sharing paradigm . The proposed sanitization algorithms can be easily embedded on mobile devices , as in Figure 1 , where the sequence data generated from various apps is sanitized as specified by the user , prior to release to untrusted parties . Specifically , our major contributions are : • We propose a novel notion , ie , mutual information privacy for hiding sequential patterns and sanitize individual sequences via generalization . Our privacy model based on information theory does not impose constraints on the adversary ’s prior knowledge , and provides privacy guarantee without deleting data or creating “ ghost ” patterns . • We formally define an optimization problem for sequence sanitization , which satisfies the user defined privacy constraint while minimizing the utility loss caused by generalization . We identify four variants of the problem : global vs . local generalization , and offline vs . online scenario . In this paper , we focus on the global generalization strategy and show that the problem is hard even in the offline case . • We propose two algorithmic solutions to the proposed problem . The former exploits the monotonicity property of the privacy notion and adopts a top down approach to refine the symbols generalization . The latter exploits the similarity between sensitive patterns and produces a generalization by clustering them in a bottom up fashion . • We perform an extensive experimental evaluation with real world datasets . It shows that the bottom up algorithm is more efficient , while the top down approach provides better utility in the released data . We demonstrate the usability of our techniques in various settings and we believe our solutions can enable a wide range of sequential data sharing applications .
In the remaining of the paper , Section 2 reviews recent related works ; Section 3 formally defines the sanitization problem and provides hardness results . Our proposed solutions are described in Section 4 and Section 5 . Section 6 provides experimental results . Section 7 concludes the paper and states future research directions .
2 . RELATED WORKS
Hiding Set Patterns . Here we use the term “ set pattern ” to represent static , non sequential patterns . A plethora of works have been proposed to hide association rules [ 28 , 3 ] and frequent itemsets [ 25 , 12 ] from transaction data . Given a set of sensitive itemsets or rules , the goal is to reduce the support of itemsets or the confidence of rules in the released datasets . Recently , several approaches have been developed to anonymize set valued data [ 18 , 27 ] using a taxonomy tree . Although extensively studied , solutions for these problems are not applicable in our setting , as set patterns do not consider the sequential order between symbols .
Hiding Sequential Patterns . Existing techniques for sequential pattern hiding fall into two categories , suppression as in [ 1 , 11 , 29 ] and permutation as in [ 14 ] . Essentially , sequential patterns are “ hidden ” in the sanitized data by reducing the occurrences of each sensitive pattern to a given threshold . Suppression based techniques [ 1 , 11 , 29 ] remove symbols from the sequence in order to reduce the exact matching of sensitive patterns . The drawback of those techniques is the data loss inflicted by deletion , especially when sensitive patterns are frequent patterns . To avoid data deletion , the permutation based technique [ 14 ] re arranges the ordering of symbols when a sensitive pattern occurs , thus reducing the matchings . However , permutation breaks the inherent ordering in the sequence data and creates “ ghost ” patterns that do not exist originally . From the privacy aspect , the support based notion [ 1 , 11 , 14 ] is ad hoc and only applicable to releasing sequence databases . Furthermore , it adopts a weaker adversary model where the sensitive patterns are unknown to the adversary . The work of [ 29 ] investigated the trade off between public patterns and private ( sensitive ) patterns in a event stream , and allowed the disclosure of private patterns to preserve high utility public patterns . As a result , the utility oriented mechanism does not provide a privacy guarantee . The work in [ 13 ] hides sensitive contexts , ie , individual events , in individual sequence data and guarantees privacy by bounding the difference between the adversary ’s prior and posterior belief , assuming that the adversary has background knowledge of a user ’s mobility patterns . While this work provides a nice privacy guarantee , its solution cannot be applied in our setting where sensitive patterns contain multiple events .
Privacy for Data Sequences . Numerous privacy notions have been adopted to sanitize data sequences . Previously k anonymity and l diversity have been adopted in [ 21 , 24 ] for sanitizing sequence databases . When applied to protect patterns , k anonymity and l diversity treat each sensitive pattern equally , without considering frequent patterns are more susceptible for privacy breach . The rigorous notion of differential privacy [ 8 ] has been widely adopted for releasing aggregate queries [ 10 ] and for mining sequential patterns [ 2 , 4 ] . Recently local differential privacy [ 6 ] has been proposed when the data aggregator is untrusted . However , it is not straightforward to apply to individual sequence sanitization . On the other hand , mutual information has been widely used to quantify information leakage in literature [ 5 , 22 , 23 ] . In particular , Calmon and Fawaz [ 5 ] adopted mutual information to introduce the information privacy notion which has been shown to provide stronger guarantee in term of information leakage than differential privacy . Based on these considerations , we adopt the mutual information notion to protect user specified sensitive patterns .
3 . PRELIMINARIES 3.1 Definitions
An alphabet Σ is a finite set of literals , ie symbols , where
338 each literal represents an atomic event . A user generated sequence S = {S[1 ] , S[2 ] , . . . , S[n]} of length n , ie |S| = n , is a set of events taken place in a sequential order , where S[i ] ∈ Σ for i = 1 , . . . , n . A sequential pattern contains multiple symbols in a specific order . Formally , we define a sequential pattern p of length m as p = a1 , a2 , . . . , am , where ai ∈ Σ for i = 1 , . . . , m .
Definition 1
( Pattern Occurrence ) . Given an input sequence S and a time window w , we said that a pattern p = a1 , a2 , . . . , am occurs at position i1 in S if there exists a subsequence of S with indices i1 < i2 < ··· < im such that : ( 1 ) im − i1 ≤ w and ( 2 ) S[ik ] = ak , for k = 1 , . . . , m .
Let Σe = Σ Σg denote the extended alphabet induced by
Furthermore , we define the frequency of a pattern p in S , denoted with fS(p ) , the number of occurrences of p in a user sequence S . For example , if S = {a , a , b} and p = a , b and w = 2 , fS(p ) = 2 because there are two subsequences of S , ie S[1 ] , S[3 ] and S[2 ] , S[3 ] , that match p . Taxonomy Tree Based Generalization . Similar to [ 18 , 27 ] , we consult a taxonomy tree for generalizing the symbols to sanitize the input sequence . The benefit of using a taxonomy tree is that the generalized symbols , ie intermediate nodes , preserve the semantic meaning of the original data , ie leaf nodes , providing a trade off between privacy and utility . Formally , let Σg be the alphabet of generalized symbols in the data domain , such as categories and sub categories of products . A taxonomy tree T is a hierarchical structure between the most fine grained events , ie symbols in Σ , and more general events , ie symbols in Σg . Intuitively , original data symbols in the input sequence appear in the leaves of T , while the internal nodes are potential options for generalization . As in Figure 2 , Wine is a fine grained symbol and Alcohol is a general symbol . T . Therefore , a generalization strategy can be defined as a function h : Σ → Σe that maps a fine grained symbol to its generalized form . A sequential pattern is transformed to a generalized pattern after one or more symbols in it have been generalized . In our work , we aim to design a sequence sanitization strategy to hide user specified sensitive patterns by generalizing individual symbols . Privacy Definition . In our problem setting , a user specifies a set of sensitive patterns S ( ie , patterns to hide ) in a input sequence S and wants to release a sanitized sequence ˆS , to receive some services . We denote G as the set of generalized patterns observed in ˆS . Assuming an adversary has prior knowledge about the frequency distributions of the sensitive patterns , he aims to discover the positions of those patterns in S by observing ˆS . The rationale of this adversary model is two fold : 1 ) The content of sensitive patterns are usually public , as in Scenario 1 3 , and even the expected frequencies of sensitive patterns are publicly available statistics . For example , the frequency information can be derived by domain knowledge , eg , how many times one goes to toilet per day , or released by mining aggregated data [ 2 , 4 ] . 2 ) We consider the context of a pattern as sensitive information and thus should be protected . For example , the pattern leave work , go to liqueur store is not surprising at night . However , if the same pattern occurs frequently during the day , it might indicate an addiction problem .
Given a generalized pattern , the adversary can identify a set of sensitive patterns mapped to it using the publicly available taxonomy tree .
Example 1 . Assume a user specifies three sensitive patterns , p1 = Beer , Chips , p2 = W ine , Cheese , and p3 = Cookies , M ilk . Upon observing an occurrence of generalized pattern ˆp = Alcohol , All , an adversary can infer a possible occurrence of p1 or p2 , according to the taxonomy tree in Figure 2 .
Therefore , the positions of a generalized pattern ˆp in ˆS may enable the adversary to infer where the sensitive patterns mapped to it occur in the original sequence . Our goal is to bound the inference gain of the adversary by observing ˆS . We consider the standard statistical inference threat model in [ 5 ] , where the inference gain is measured using mutual information , which has been widely used to quantify information leakage/privacy risk in literature [ 22 , 23 ] .
Let S denote the frequency distributions of sensitive patterns in S , which may be known to the adversary . Let G denote the frequency distributions of generalized patters in ˆS . As shown in [ 5 ] , the average statistical information gain of the adversary by observing ˆS is the mutual information between S and G :
I(S ; G ) = H(S ) − H(S|G ) ,
( 1 ) where H denotes the Shannon entropy . I(S ; G ) is bounded between 0 and H(S ) . Using the mutual information , we define the following privacy notion for sequence sanitization :
Definition 2
( Mutual Information Privacy ) . Given an input sequence S , a set of sensitive patterns S , and ≥ 0 , a sanitized sequence ˆS satisfies mutual information privacy if and only if I(S ; G ) ≤ .
By upper bounding the mutual information , our privacy notion imposes a lower bound on the conditional entropy H(S|G ) in Equation 1 . Intuitively , given a generalized pattern ˆp , a sanitization strategy yields higher conditional entropy by mapping a diverse set of sensitive patterns to ˆp , thus providing higher privacy . This observation also inspires us when designing our algorithmic solutions . The privacy parameter can be specified by individual data providers . A lower value indicates a higher privacy level , where perfect privacy is achieved by setting = 0 . Note that mutual information privacy ( ∀ ) is always feasible , by generalizing all symbols to “ All ” , ie , root of taxonomy tree , where the released sequence is independent of the input , ie , no additional information is gained by the adversary . In our approaches , we use an empirical form of Equation 1 . For each p ∈ S and ˆp ∈ G , ˜P [ p ] , ˜P [ ˆp ] , and ˜P [ p , ˆp ] denote the empirical distributions of p and ˆp and their joint distribution , respectively . These distributions are computed by counting the frequency of the patterns in the data sequences . Thus , the empirical form of Equation 1 is :
˜P [ p , ˆp ] log
˜P [ ˆp ] ˜P [ p , ˆp ]
( 2 ) p∈S p∈S , ˆp∈G
˜I(S ; G ) =
=
˜P [ p ] log
1
˜P [ p ]
˜P [ p , ˆp ] log
− p∈S , ˆp∈G ˜P [ p , ˆp ] ˜P [ p ] ˜P [ ˆp ]
In general , mutual information bounds the average information leakage ; therefore , when value is high , not all sensitive patterns need to be generalized . To ensure that each sensitive pattern p ∈ S is sanitized , a lower value must be specified by the data provider , ie , < minp∈S ˜P [ p ] log 1 . ˜P [ p ] However , we notice in our empirical evaluation that when
339 the alphabet is small , each sensitive pattern will be sanitized even with higher values , eg , 90 % of ˜H(S ) . Utility Metric . To quantify the utility loss inflicted by generalizing the original data symbols , we define a cost function c : Σ× Σe → + , such that for a pair of symbols ( a , ˆa ) , c(a , ˆa ) returns the cost of generalizing a to ˆa . Note that prior to the generalization process , each user could specify or customize the cost for any particular symbol , according to its importance or the user ’s preferences . A formal definition for the cost function is as follows .
Definition 3
( Cost Function ) . For any a ∈ Σ , let πa = ˆa1 , . . . , ˆak be the root to leaf path in T , ie ˆa1 = root and ˆak = a . A function c : Σ× Σe → + is a generalization cost function over T , if ∀a ∈ Σ the following properties hold :
1 . c(a , ˆai ) ≥ c(a , ˆai−1 ) , for i = 1 , . . . , k 2 . c(a , ˆa ) = ∞ if ˆa /∈ πa 3 . c(a , ˆa ) = 0 ⇐⇒ a = ˆa
These properties guarantee : the cost monotonically increases when generalizing a leaf node to a higher level node ; the cost of generalizing a leaf node to its non ancestor is prohibitive ; and there ’s no cost of generalizing a leaf node to itself .
Example 2 . Consider the taxonomy tree in Figure 2 . Each internal node ν is associated with an array where each entry represents the cost of replacing a leaf node with ν . Suppose the scale of cost is [ 0 , 1 ] , replacing any product with the root “ All ” incurs cost 1 , while the cost of replacing “ Cookies ” with “ Snack ” is 08
In practice , the release of each data symbol , generalized or original , allows the user to receive certain services , such as fitness analysis or a grocery item coupon . Intuitively , sharing a generalized symbol would receive less utility than the original symbol . We define the overall utility loss for a generalized sequence as follows .
Definition 4
( Utility Loss ) . The utility loss for a generalized sequence ˆS obtained from a sequence S is :
|S|
U L(S , ˆS ) = c(S[i ] , ˆS[i ] )
( 3 ) i=1
The overall utility loss sums up the user specified cost of generalizing each symbol in the input sequence . In future works , a weight can be introduced to indicate the importance of event time , ie i , which is out of the scope of this paper . 3.2 Problem Variants
Based on the notions above , we formally define the Minimum Utility Loss Generalization problem ( MULG ) to sanitize individual sequence data .
Problem 1
( Minimum Utility Loss Generalization ) .
Given an input sequence S , a set of sensitive patterns S , a cost function c , and ≥ 0 , construct a sanitized version ˆS such that ( 1 ) ˆS satisfies mutual information privacy , and ( 2 ) ˆS incurs minimum utility loss .
We identify several variants of the sequence sanitization problem and provide a brief discussion below . Global vs . Local . The global variant generalizes all occurrences of one symbol in the same way , irrespective of when they occur in the sequence . On the other hand , the local generalization problem treats each occurrence of a symbol differently based on its context , ie the neighboring symbols in the sequence . In comparison , the local variant offers more flexibility in generalization options and more opportunities for utility optimization . However , it also presents a harder optimization problem than the global variant [ 29 ] . Furthermore , the highly locally customized generalization solutions may open up opportunities for adversary to make inference attacks , as discussed in [ 30 ] .
Offline vs . Online . The offline variant makes generalization decisions after the entire sequence is generated , while in the online variant , decisions have to be made in real time as each symbol is generated without knowledge of the future . Despite the importance of online sanitization in many real scenarios , achieving minimum utility loss in this setting is particulary hard due to the uncertainty for the future patterns . For this reason , we focus our study on the offline variant where the knowledge about the entire user sequence can be used to reach the optimality .
Based on the above considerations , we will focus on the offline sequence sanitization problem with global generalization solutions , ie g MULG , in the rest of the paper . In particular , our goal is to identify a generalization function h : Σ → Σe that maps each a ∈ Σ into a unique symbol ˆa ∈ Σe ( ie h(a ) = ˆa ) . Note that due to the unique solution for each symbol , the joint distribution ˜P [ p , ˆp ] is simplified to ˜P [ p ] in Equation 2 . Hardness Results . The following theorem states the hardness of the sanitization problem .
Theorem 1 . The g MULG problem where all the sensi tive patterns are generalized is NP hard .
The result follows from similar arguments as in [ 1 , 29 , 17 ] , and due to the limited space the proof is omitted . We observe that g MULG problem does not enforce the generalization of all sensitive patterns to achieve mutual information privacy . It is possible that some patterns are preserved while others are generalized . However , this does not change the hardness of the problem . In fact , we can always construct a solution for plain g MULG by preserving some patterns and generalizing the rest as long as privacy is achieved .
4 . TOP DOWN APPROACH
Our first approach proceeds in a top down fashion by progressively improving the utility of the symbols generalization without breaching privacy . The intuition is to start from the highest level of generalization for all the symbols , ie , root , and then refine each symbol to enhance the utility as much as possible . A taxonomy tree constructed in the data domain is used to refine the symbol generalization . Among all the possible levels of generalization for a symbol , we choose the one that achieves highest utility while satisfying the privacy constraint . Before describing the proposed algorithm in detail , we first present some useful concepts and the monotonicity property of the solution space .
Refinement Process . The top down approach starts with the highest level of generalization possible gf = [ h(a1 ) = ˆa1 , . . . , h(a|Σ| ) = ˆa|Σ| ] , where each symbol aj is mapped to the root node of the taxonomy tree ( ie ˆaj = root ) . At each iteration i , the current generalization gi is refined to improve the utility of the released sequence . This process is guided by the taxonomy tree T , where given gi = [ ˆai 1 , . . . , ˆai|Σ| ] for j is refined to ˆai+1 a symbol aj , its current representation ˆai by selecting the successive node in T on the path from the j
340 root to aj . Furthermore , symbols at the leaves of the subtree rooted by ˆai j other than aj will be also refined to this new representation , ˆai+1
. j
Example 3 . Consider the tree in Figure 2 and a generalization g = [ W ine = All , Beer = All , Cookies = All , Chips = All , M ilk = All , Cheese = All ] . Furthermore , let Cookies be the symbol chosen to refine in the next iteration . Then the resulting generalization is g = [ W ine = All , Beer = All , Cookies = Snack , Chips = Snack , M ilk = All , Cheese = All ] , where both Cookies and Chips are refined to Snack since are leaves of the same subtree .
As we progressively refine the symbol generalization to lower level nodes in T , the mutual information for the generalized sequence will never decreases . The following lemma states this important monotonicity property .
Lemma 1
( Monotonicity ) . Let gi be the current generalization for the symbols in Σ . Consider gi+1 be the generalization obtained from gi by further refining any symbols in Σ by using the taxonomy tree T , then the following result holds ˜I(S ; Gi+1 ) ≥ ˜I(S ; Gi ) . j j j
1 , . . . , ˆai|Σ| ] chosen to be refined from ˆai
Proof . Let aj be the symbol in the current generalizaj to ˆai+1 tion gi = [ ˆai . ) = {ai1 , . . . , aik} be the set of Furthermore , let LT ( ˆai+1 according the taxonomy tree T and leaves refined to ˆai+1 let ˆp = hi(p ) be the generalized pattern obtained from any p ∈ S applying gi . Then we observe that for each p ∈ S , the following inequality holds : P [ hi(p ) ] ≥ P [ hi+1(p) ] . In fact , let assume that there exists a pattern p ∈ S such that P [ hi(p ) ] < P [ hi+1(p) ] . Thus , there exists at least one occurrences of the symbols in LT ( ˆai+1 ) that is not generalized . However , this is not possible since LT ( ˆai+1 ) are mapped to ˆaj only with the generalization gi+1 , hence this proves that P [ hi(p ) ] ≥ P [ hi+1(p) ] . We use this fact to prove the monotonicity property as follows : ˜I(S ; Gi+1 ) − ˜I(S ; Gi ) =
˜P [ p ] log
( 4 ) j j
˜P [ hi(p ) ] ˜P [ hi+1(p ) ] p∈S
Since P [ hi(p ) ] ≥ P [ hi+1(p) ] , from the last equality we have that ˜I(S ; Gi+1 ) ≥ ˜I(S ; Gi ) .
It is important to notice that such monotonicity property does not hold in general but it is a consequence of using a refinement procedure based on subtrees . Although in this way , we pay an additional cost in utility loss with respect to the optimal solution , the use of the monotonicity property allows us to greatly speed up our algorithm . In fact , if a generalization g violates the privacy level , then all the generalizations that can be obtained refining g will also violate the privacy . Hence , all these possible generalizations can be discarded in our process . This allows us to use the information in early steps of the algorithm to prune a large number of generalizations in the later steps . In particular , if a refinement operation on a symbol ai ∈ Σ fails at an early stage of the algorithm ( ie violates the privacy ) then such operation will also fail on any generalizations in later stages of the algorithm . This result is formalized below .
Corollary 1
( Pruning ) . Let gi be the current generalization which satisfies privacy and gi+1 refine gi by considering symbol aj from ˆai . If gi+1 violates privacy , aj j to ˆai+1 j
Algorithm 1 Top Down Algorithm 1 : procedure Top Down(S , S , , T )
Input : sensitive patterns S , user ’s input sequence S , privacy level , taxonomy tree T ; Output : sanitized sequence ˆS
Σ ← sort by freq . the symbols in Σ gf = [ ˆa1 , . . . , ˆa|Σ| ] with ˆai ← T .root for all i . while ( Σ is not empty ) do
U L ← ∞ for ( ai ∈ Σ ) do li ← current level of generalization for ai in T i ← refined symbol at level li + 1 ˆa g ← gf i ) ← leaves of the subtree rooted by ˆa LT ( ˆa replace in g the generalization of aj ∈ LT ( ˆa if ( U LS ( g ) > U L ) then i i ) with ˆa i continue end if ˜I(S ; G ) ← mutual information using g if ( ˜I(S ; G ) ≤ ) then
U L ← U LS ( g ) g ← g remove aj ∈ LT ( ˆa i ) from Σ
2 : 3 : 4 : 5 : 6 : 7 : 8 : 9 : 10 : 11 : 12 : 13 : 14 : 15 : 16 : 17 : 18 : 19 : 20 : 21 : 22 : 23 : 24 : 25 : 26 : 27 : end procedure else end if end for gf ← g end while ˆS ← sanitize S using gf return ˆS as well as other symbols at leaf nodes in the subtree rooted by ˆai+1 can be pruned for further refinement . j
Proof . It follows directly from the result in Lemma 1 . In fact , since gi+1 fails to meet the privacy requirement , any refinement of gi+1 would also violate privacy . Furthermore , refinement of gi at any leaf node symbol in the subtree rooted by ˆai+1 will yield gi+1 or refinement of gi+1 , which would violate privacy and can be safely pruned . j
These observations allows us to greatly reduce the number of generalizations considered . In fact , given hmax the maximum height of the taxonomy tree , we reduce the number of possible generalizations from O(|Σ|hmax ) to only a small polynomial function of |Σ| and hmax . Algorithm Description . Our procedure is reported in Algorithm 1 . We start with a generalization gf , where each symbol is generalized to the root node ( ie maximum generalization level ) and we sort all the symbols in Σ by decreasing order of frequency . In this way , we select first those symbols that potentially increase more the utility during our refinement process . Although the procedure in Algorithm 1 is defined on the entire alphabet Σ , we note that : 1 ) symbols that never occur in the input sequence can be discarded ; 2 ) in some cases it is sufficient to generalize only the symbols in the sensitive patterns ΣS to achieve the desired level of privacy . In fact , if the generalization gf , where only the symbols in ΣS are generalized to the root , achieves privacy , then we run our refinement process on ΣS rather than consider the entire alphabet .
In the main loop ( lines 4 24 ) , we iteratively refine the current generalization . In particular , given the current generalized symbol ˆai for ai associated with a node ν at level li in the taxonomy tree , we select ˆa i as the symbol represented by its corresponding child node at level li + 1 ( lines 7 8 ) . We then retrieve the leaves LT ( ˆa i ) of the subtree rooted by ν and we refine the representation for the symbols in LT ( ˆa i ( lines 9 10 ) . At line 11 , we compute the i ) with ˆa
341 utility U LS(g ) for the generalization g and at line 15 we test its privacy level . If the privacy is violated , the selected symbols in LT ( ˆa i ) are discarded for further refinement according to our pruning criteria . After iterating among all the possible symbols , we pick the generalization option with minimum utility loss ( line 23 ) . The while loop terminates either when the possible refinement operations violate the privacy or the maximum level of refinement for each symbol is achieved . Finally , using the final generalization gf we produce the sanitized sequence ˆS which is returned as result . Algorithm Analysis . In our implementation , we compute the frequency of all the symbols in the pre processing step so that evaluating U LS(g ) requires only O(|Σ| ) . The privacy evaluation associated with ˜I(S ; G ) requires to compute the frequency information about the current set of generalized patterns . In principle , we can use any mining algorithm as a black box to compute such information . In our implementation , we use the mining algorithm in [ 11 ] which requires O(|S| × w × m ) time , where |S| is the length of the sequence , w is the windows length , and m is the pattern length . Both the utility loss computation and privacy evaluation are performed in the most inner loop . Such loop is executed a number of time proportional to the number of internal nodes in the taxonomy tree . Given a taxonomy tree T , where each internal node has at least two children , then we have that the number of internal nodes is O(|Σ| − 1 ) . Therefore , the worst case computational complexity for Algorithm 1 is O(|Σ|3 × |S| × w × m ) . However , we note that individually generated sequences in practice contain a very small portion of the alphabet Σ and our solution is shown to be very efficient in the experiments .
5 . BOTTOM UP APPROACH
The previous solution greedily picks the symbol which yields minimum utility loss to refine and proceeds until privacy is breached . While designed to minimize the utility loss , the computational cost could be very high for large alphabets . Therefore , we propose an alternative algorithm that produces a generalization by directly exploiting the similarity between sensitive patterns . The key idea is to cluster sensitive patterns together and hide them with a generalization that inflicts minimum utility loss .
This algorithm takes a bottom up approach and follows the idea of hierarchical clustering : starting from each sensitive pattern being in its own cluster , we iteratively merge the closest pair of clusters . In this way , we associate a group sensitive patterns to a common generalized pattern representing the centroid of the cluster . Intuitively , in each iteration we construct a generalization function h(· ) that maps all the sensitive patterns within the same cluster to the same generalized pattern ( ie centroid of the cluster ) . Initially , each pattern p is a cluster C(p ) on its own and each symbol is represented with the finest granularity . Then , when pairs of clusters are merged a new generalization is produced . In our solution , we select the clusters to be merged in a way that minimizes the utility loss of the new generalization . This process ends when the specified privacy level is achieved . Before presenting our algorithm in details , we illustrate the key steps of our clustering process . We first show how to select a new representative when clusters are merged and define a distance metric that achieves minimum utility loss .
Cluster Representative .
In principle , there are several
Algorithm 2 Least Common Generalized Pattern 1 : procedure LCGP(p1 , p2 , T )
Input : patterns p1 = a1 , a2 , . . . , am , p2 = b1 , b2 , . . . , bm ; taxonomy tree T ; Output : least common generalized pattern LCGP ( p1 , p2 )
For each unique symbol a in p1 , p2 set h0(a ) ← a j ← 1 for j ≤ m do i ← 1 for i ≤ m do
2 : 3 : 4 : 5 : 6 : 7 : 8 : 9 : 10 : 11 : 12 : 13 : 14 : 15 : 16 : end procedure
( ai , bi ) symbols appearing at same position i hj ( ai ) = hj ( bi ) ← lca(hj ( ai ) , hj ( bi ) ) i ← i + 1 end for j ← j + 1 hj ( a ) ← hj−1(a ) for every unique symbol a in p1 and p2 end for LCGP ( p1 , p2 ) ← hm(a1 ) , hm(a2 ) , . . . , hm(am ) return LCGP ( p1 , p2 ) ways to merge two clusters and define a common representative for their patterns . For example , one could define a generalization function that sets the non matching symbols to the root . Although this map produces a new representative , it may not be optimal . Therefore , we are interested in finding a valid generalization function h(· ) that leads to minimum utility loss in merging two clusters of patterns .
Problem 2
( Minimum Generalization ) . Given two gen eralized patterns p1 and p2 representing the centroids for the clusters C(p1 ) and C(p2 ) respectively . Let Σ(p1 , p2 ) be the set of unique symbols for the sensitive patterns in the clusters and fS(a ) denote the frequency of a ∈ Σ in S , then the new representative for the patterns in the union of these clusters is p = h∗(p1 ) = h∗(p2 ) , where h∗(· ) is computed as follows .
∗ h
( · ) := arg min h(· ) such that h(· ) is a valid generalization a∈Σ(p1,p2 ) c(a , h(a))fS(a )
To solve this problem , we introduce the notion of least common generalized pattern which defines h∗(· ) exploiting the taxonomy tree structure .
Definition 5
( Least Common Generalized Pattern ) . Given two patterns p1 = a1 , a2 , . . . , am and p2 = b1 , b2 , . . . , bm defined over Σe . Their least common generalized pattern LCGP ( p1 , p2 ) = c1 , c2 , . . . , cm is determined by generalizing each unique symbol a both in p1 and p2 to c = LCAp1,p2 ( a ) . Where LCAp1,p2 ( a ) represents the highest generalization of a among the least common ancestor of a and the symbols related to it , both in p1 and p2 .
The LCGP aims to find a generalization hierarchy between symbols . Since we are looking for a valid generalization h(· ) , we need to take into consideration the symbol tosymbol dependency . In principle , a generalization of a symbol may have implications on others . For example , consider p1 = Beer , Chips and p2 = W ine , Beer . Even though p1[1 ] = “ Beer ” and p2[1 ] = “ W ine ” have least common ancestor “ Alcohol ” , due to the generalization of p1[2 ] = “ Chips ” and p2[2 ] = “ Beer ” to “ All ” , also the symbol “ W ine ” is generalized to “ All ” .
Algorithm 2 illustrates the computation of the Least Common Generalized Pattern for two patterns . We compute LCGP ( p1 , p2 ) by iteratively generalizing each symbol in both p1 and p2 . We observe that such computation can be performed efficiently by only processing each symbol O(m ) times ,
342 where m is the length of the patterns . At each iteration j , for each unique symbol a we construct a map hj(a ) . We start by setting h0(a ) = a and at the end of the algorithm hm(a ) will represent the value LCAp1,p2 ( a ) . In the inner loop at lines 7 10 , we update the map iteratively as follows . For each position i , we select the pair of symbols ( ai , bi ) appearing in both p1 and p2 ( ie ai = p1[i ] and bi = p2[i ] ) and we update their current map as hj(ai ) = hj(bi ) = lca(hj(ai ) , hj(bi) ) , where lca is the least common ancestor between the two symbols in the taxonomy tree T . Since the computation of lca requires O(hmax ) time , the overall complexity for Algorithm 2 is O(hmax × m2 ) . Below , we illustrate a running example for our algorithm .
Example 4 . Consider the taxonomy tree in Figure 2 , where each leaf represents a product , and let p1 = Beer , Chips and p2 = M ilk , Cookies be two patterns . Then their least common generalized pattern is LCGP ( p1 , p2)= All , Snack . Since lca(Beer , M ilk ) = All and lca(Chips , Cookies ) = Snack , at the end of the first iteration we have : h1(Beer ) = h1(M ilk ) = All and h1(Chips ) = h1(Cereal ) = Snack , where “ All ” is the highest among all the least common ancestors . In the successive iteration there are no updates , and therefore LCGP ( p1 , p2 ) = All , Snack . Consider now the pattern p3 = W ine , Beer . In this case the least common generalized pattern for p1 and p3 is LCGP ( p1 , p3 ) = All , All . At the end of the first iteration , we have that h1(W ine ) = Alcohol and h1(Beer ) = h1(Chips ) = All , where “ All ” is the highest among all the least common ancestors . In the next iteration , the procedure updates the map for “ W ine ” leading to h2(W ine ) = lca(Alcohol , All ) = All , while there is no updates for the other symbols .
In our complete solution , given two clusters C(p1 ) and C(p2 ) , we define the new representative pattern for their union as p = LCGP ( p1 , p2 ) . As we showed above , the selection of such a new representative for the clusters induces a new generalization map on each individual symbol a , where ˆa = hm(a ) = LCAp1,p2 ( a ) . Due to the nature of the least common ancestor we can show that such generalization is optimal according to Problem 2 .
Theorem 2 . Given two patterns p1 and p2 associated with the clusters C(p1 ) and C(p2 ) respectively . Then the LCGP ( p1 , p2 ) minimizes the utility loss for representing the patterns both in C(p1 ) and C(p2 ) .
Proof . The proof proceeds by induction on the number of different symbols k in p1 = a1 , a2 . . . , am and p2 = b1 , b2 . . . , bm . Base Case . Consider k = 1 , then there exists i such that ai = bi . Therefore , the minimum utility loss is achieved by generalizing both ai and bi to their least common ancestor . Inductive Step . We assume that the statement holds for k−1 and we prove it for k . Let ( ai , bi ) be the pair of symbols in p1 and p2 with the highest least common ancestor . Then any generalization function h(· ) that generalizes both p1 and p2 has h(ai ) = h(bi ) = lca(ai , bi ) . Therefore the utility loss is at least c(ai , lca(ai , bi))fS(ai ) + c(bi , lca(ai , bi))fS(bi ) plus the generalization cost for the remaining k− 1 different symbols . Among them , some may be replaced with lca(ai , bi ) if they are equal either to ai or bi . Then , let {(aij , bij )}k−1 j=1 be the set of these different symbols left to be generalized . By the induction hypothesis on these k−1 symbols , we have that the
Algorithm 3 Bottom Up Algorithm 1 : procedure Clustering(S , S , , T )
Input : sensitive patterns S , user ’s input sequence S , privacy level , taxonomy tree T ; Output : sanitized sequence ˆS
2 : 3 : 4 : 5 : 6 : 7 : 8 : 9 : 10 : 11 : 12 : 13 : 14 :
C ← set of centroids ( one for each sensitive pattern ) gf = [ ˆa1 , . . . , ˆa|Σ| ] with ˆai = ai for all i . repeat if ( |C| == 1 ) then
Greedily generalize the symbols in gf end if Compute distance between pair of centroids ( p1 , p2 ) ← the closest pair of centroids C ← C − {p1 , p2} p3 ← LCGP(p1 , p2 , T ) Add p3 to the set C Update gf according to LCGP ( p1 , p2 )
C(p3 ) ← C(p1 ) C(p2 )
Updates the centroids accordingly until ( ˜I(S : Gf ) ≤ ) ˆS ← sanitized S using gf return ˆS
15 : 16 : 17 : 18 : end procedure map induced by the least common generalized patterns provides minimum utility loss . Then the final map produced by this procedure is the same as LCGP ( p1 , p2 ) and it achieves minimum utility loss for all the k different symbols .
Cluster Distance . Given two clusters , the least common generalized pattern defines a new representative which incurs minimum utility loss . To minimize the utility loss at each iteration , we select the pair of clusters that are closest to their least common generalized pattern . Formally , the distance between a pair of clusters dg(·,· ) is defined below .
Definition 6
( Generalization Distance ) . Given two centroids p1 and p2 associated with the clusters C(p1 ) and C(p2 ) respectively . The generalization distance between p1 and p2 is defined as : dg(p1 , p2 ) := c(a , LCAp1,p2 ( a))fS(a )
( 5 ) a∈Σ(p1,p2 )
Due to the taxonomy tree structure , we can show that the proposed generalization distance function is also metric . Algorithm Description . Algorithm 3 iteratively constructs a generalization by merging clusters of patterns . The procedure starts with singleton clusters , one for each sensitive pattern . In the main loop ( lines 4 15 ) , at each iteration the closest pair of centroids ( p1 , p2 ) is merged . The new centroid is defined using the procedure in Algorithm 2 , by computing LCGP ( p1 , p2 ) ( lines 11 12 ) . Then , the current generalization for the symbols is updated according to the map induced by LCGP ( p1 , p2 ) . Since our generalization is a global generalization , this step requires also to update the other centroids to guarantee consistency . Finally , if the current generalization achieves the desired level of privacy we terminate the loop and return the corresponding sanitized sequence . Since the entire algorithm merges clusters of patterns , it may terminate with a single cluster which may not satisfy the privacy constraint . In such situation , we greedily further generalize the current symbols using the taxonomy tree till the privacy is achieved ( lines 5 7 ) . Algorithm Analysis . Since we start with singleton clusters containing a sensitive pattern each , the total number of merging operations is O(|S| ) . In the merging step , we have to compute the new representative for the selected clusters .
343 Figure 3 : Taxonomy Tree for MSNBC
Table 1 : Default Parameter Setting
( a ) Top Down : Time vs
( b ) Bottom UP : Time vs
Parameter m w fmin |S|
Description
Pattern Length Pattern Window
Patterns Minimum Frequency Number of Sensitive Patterns
( Relative ) Privacy Level
Value
2 10 10 4 0.5
Such operation requires O(hmax × m2 ) using Algorithm 2 . The distance computation for the pairs of clusters requires the evaluation of the utility loss for the merged clusters , which can be performed in O(|ΣS| ) . The privacy evaluation for the current generalization gf is computed as in Algorithm 1 . Since these operations are performed at each iteration of our solution , the total running time of our algorithm is O(|S|3 × hmax × |S| × w × m3 × |ΣS| ) .
6 . EXPERIMENTS
Here , we present a set of empirical evaluations for our solutions . Due to limited space and exponential run time , we omit the results obtained by the brute force approach for finding the optimal solution . 6.1 Settings Datasets . We consider two real world sequence datasets : MSNBC1 and RM2 . MSNBC contains nearly 1 million sequences with maximum sequence length of almost 15,000 symbols , each of which corresponds to web pages visited by a user within 24 hour period . The alphabet size is 17 , where each symbol represents a page category , such as “ frontpage ” . An example taxonomy tree constructed for this dataset in Figure 3 is used in the evaluation . We discard those user sequences that contain less than 500 symbols and 36 users are preserved . RM dataset contains daily activities of 100 individuals and each user has a sequence of locations recorded by a smartphone over a year [ 9 ] . For evaluation , we keep the locations at area id level and the alphabet size is 69 . Since the spatial information about each area id is absent , we construct a taxonomy tree by Equi Width Binning where each bin contains 10 consecutive area id ’s . We preserve 32 users in this data set where the maximum sequence length is 20,000 symbols . Note that since the computation complexity of both our solutions is linear with the sequence length and the patterns are defined within a time window , increasing the length of input sequence does not necessarily make the problem more difficult .
Algorithm Setup . The default parameter values are presented in Table 1 unless specified otherwise . Since users behave differently and their sequences exhibit high variance , we specify a relative privacy level , as a percentage of the entropy of input sequence , instead of an absolute value . For instance , when = 0.5 , we bound the final mutual information for the released sequence to be at most 50 % of the
1https://archiveicsuciedu/ml/machine learningdatabases/msnbc mld/msnbcdatahtml 2http://realitycommonsmediamitedu/realitymininghtml
( c ) Top Down : UL vs ( d ) Bottom Up : UL vs Figure 4 : Privacy and Cost evaluation on MSNBC
( a ) Top Down : Time vs
( b ) Bottom UP : Time vs
( c ) Top Down : UL vs
( d ) Bottom Up : UL vs
Figure 5 : Privacy and Cost evaluation on RM entropy the input sequence . Assuming the maximum generalization cost , ie , leaf to root , is 1 , we consider two customized cost functions based on the taxonomy tree : linear and exponential . In the former , the cost for generalizing a leaf node is uniformly divided among all levels of the tree , while in the latter the cost decay is exponential . In addition , a standard cost function based on the information loss , as defined in [ 19 ] , is also evaluated . In this case , we measure the cost of generalizing a leaf node to an internal node ν as the ratio between the number of leaves in the subtree rooted by ν and the total number of leaves in T . We observe that all three cost functions satisfy Definition 3 . Evaluation Metrics . We evaluate the utility loss and the run time of our proposed solutions . In particular , for each user in the datasets , we randomly sample a pool of sensitive patterns of frequency at least fmin which are used as a input for our algorithms . For consistency , 25 runs are performed for each user and the average result is used . Since each user ’s sequence has different length , we normalize the utility loss with respect to the sequence length . Finally , we report the average among all the users in each dataset . 6.2 Results Impact of Privacy Level and Cost Functions . Figure 4 and 5 present the run time and utility results for the MSNBC and RM dataset respectively . Small value in
344 ( a ) MSNBC : Time vs P . Len .
( b ) RM : Time vs P . Len .
( a ) MSNBC : Time vs P . Num .
( b ) RM : Time vs P . Num .
( c ) MSNBC : UL vs P . Len .
( d ) RM : UL vs P . Len .
Figure 6 : Impact of the Length of Sensitive Patterns
( c ) MSNBC : UL vs P . Num .
( d ) RM : UL vs P . Num .
Figure 7 : Impact of the Number of Sensitive Patterns dicates a high level of privacy and as increases , the privacy level becomes lower . When decreasing the privacy level , the Bottom Up approach results in less run time in both datasets ( Figure 4(b ) , 5(b) ) , as the algorithm may achieve the privacy level sooner by merging clusters . As the Top Down approach starts with a feasible solution and performs refinement , its run time should increase as the privacy level decreases ( Figure 4(a) ) . However , for RM dataset ( Figure 5(a) ) , it shows a decreasing trend when ≥ 05 The reason is RM has a relatively large alphabet but a taxonomy tree with 3 levels only . When the privacy level is high , the entire alphabet are explored to satisfy the privacy constraint , although many symbols may be pruned at early stages . When decreasing the privacy level , only the symbols in sensitive patterns need to be generalized and the extra refinement allowed by the privacy relaxation is limited due to the height of the tree . As a result , we can only observe the dominant decrease caused by alphabet size in the overall run time . Comparing the two approaches , Bottom Up is typically faster than TopDown . Especially for the RM dataset we observe one order of magnitude speedup . From the utility perspective ( Figure 4(c),4(d),5(c ) , 5(d) ) , as the privacy level decreases both solutions are able to reduce the utility loss for the released sequence . Top Down preserves the utility better due to the independent generalization of symbols . Regarding the cost functions , we observe that the information loss , ie iloss , provides the least utility loss in both datasets and linear and exponential decay functions have similar performance . Therefore , for the rest of the experiments we only present results for iloss and linear as cost function .
Impact of the Length of Sensitive Patterns m . For simplicity , we denote the top down approach and the bottomup approach as T and C respectively . In Figure 6 , we report the performance of our solutions when varying the length of sensitive patterns , ie the number of symbols in each pattern . As the pattern length increases , the run time for our solutions increases simultaneously because more symbols need to be considered and the matching of patterns for privacy evaluation becomes more expensive . We can observe that our approaches provide robust utility results with respect to m , since in both datasets ( Figure 6(c ) , 6(d ) ) the utility loss for the released sequence does not present substantial changes .
Impact of the Number of Sensitive Patterns . Figure 7
( a ) Time vs Freq . ( α )
( b ) UL vs Freq . ( α )
Figure 8 : Impact of the Frequency of Sensitive Patterns reports the impact of the number of sensitive patterns . We observe that the run time for our top down approaches in MSNBC increases with the number of sensitive patterns ( Figure 7(a) ) , while in RM the run time tends to be stable ( Figure 7(b) ) . We believe that this phenomenon is due to the nature of these datasets . In general , we could expect that an increment of the number of sensitive patterns increases the number sensitive symbols in ΣS . For the RM dataset , our strategies may take advantage of the patterns distribution by limiting the generalization only on ΣS rather than considering the entire Σ , leading to more stable performance on RM . The utility loss increases with the number of sensitive patterns since more symbols need to be generalized . The utility results for the two datasets are reported in Figure 7(c),7(d ) .
Impact of the Frequency of Sensitive Patterns . Figure 8 illustrates the impact of the frequency of sensitive patterns on the proposed solutions . To illustrate this dependency , we consider an absolute value of mutual information privacy = 01 We select a single user sequence in the MSNBC dataset and we sample two sensitive patterns p1 , p2 such that f ( p1)/f ( p2 ) ≥ α where α ∈ {2 , 4 , 8 , 16 , 32} . Intuitively , for small values of α the sensitive patterns have similar frequency and therefore are easy to protect . On the other hand , when α increases the patterns’ frequency largely differs and the sanitization becomes hard . As expected , the run time for our solutions increases with α ( Figure 8(a) ) , as well as the utility loss ( Figure 8(b) ) , since more symbols are generalized in order to guarantee privacy .
Side Effects of Hiding Patterns . Figure 9 illustrates the side effects on the frequent patterns for a randomly selected user ’s sequence in the MSNBC dataset when the sensitive patterns are generalized . Comparing the original top10 frequent patterns against those mined from the sanitized
345 Figure 9 : Case Study : top 10 patterns mined on original sequence ( Top ) and sanitized sequence ( Bottom ) . sequence , we observe that a large number of patterns are preserved and the two frequency distributions are very similar . The sanitization produced by the top down approach generalizes the web pages in subtrees associated with the categories Social ( So . ) and Overview ( Ov ) This leads to an increment of the absolute frequency value for some patterns in the released sequence . For example , the patterns 10 , 10 and 14 , 14 are both generalized to So . , So . , which amplifies its occurrences . Furthermore , we notice that the semantic meaning of the frequent patterns is preserved at a coarser granularity enabling data mining applications to privately extract information about user ’s specific patterns . In this example , we can still infer that this user is interested in web pages under the social and overview categories .
7 . CONCLUSIONS
In this paper , we studied the problem of sanitizing individual data sequence to protect user specified sensitive patterns . We introduced the notion of mutual information privacy and proposed to generalize data symbols in the input sequence to satisfy the privacy definition . We showed that the problem of minimum utility loss sequence sanitization is hard and we designed two efficient solutions . The top down approach constructs a generalization by refining each symbol according to the taxonomy tree . The bottom up approach clusters sensitive patterns by exploiting pattern similarity . Our experiment results on real world datasets showed that the bottom up approach achieves similar utility results as the top down approach while incurring considerably lower running time . Future research directions include the study of the utility guarantees of our solutions and the extension of the privacy model to provide worst case guarantees . 8 . REFERENCES
[ 1 ] O . Abul , F . Bonchi , and F . Giannotti . Hiding sequential and spatiotemporal patterns . IEEE Trans . on Knowl . and Data Eng . , 22(12):1709–1723 , Dec . 2010 .
[ 2 ] L . Bonomi and L . Xiong . A two phase algorithm for mining sequential patterns with differential privacy . In CIKM ’13 , pages 269–278 , New York , NY , USA , 2013 . ACM .
[ 3 ] J . Cao , P . Karras , C . Ra¨ıssi , and K L Tan . ρ uncertaintyy :
Inference proof transaction anonymization . Proc . VLDB Endow . , 3(1 2):1033–1044 , Sept . 2010 .
[ 4 ] R . Chen , B . C . Fung , B . C . Desai , and N . M . Sossou .
Differentially private transit data publication : A case study on the montreal transportation system . In KDD ’12 , pages 213–221 , New York , NY , USA , 2012 . ACM .
[ 5 ] F . du Pin Calmon and N . Fawaz . Privacy against statistical inference . In Communication , Control , and Computing ( Allerton ) , 2012 50th Annual Allerton Conference on , pages 1401–1408 , Oct 2012 .
[ 6 ] J . Duchi , M . Jordan , and M . Wainwright . Local privacy and statistical minimax rates . In Foundations of Computer Science ( FOCS ) , 2013 IEEE 54th Annual Symposium on , pages 429–438 , Oct 2013 .
[ 7 ] C . Duhigg . How companies learn your secrets . The New York
Times , 2012 .
[ 8 ] C . Dwork . Differential privacy . In M . Bugliesi , B . Preneel ,
V . Sassone , and I . Wegener , editors , Automata , Languages and Programming , volume 4052 of Lecture Notes in Computer Science , pages 1–12 . Springer Berlin Heidelberg , 2006 .
[ 9 ] N . Eagle and A . ( Sandy ) Pentland . Reality mining : Sensing complex social systems . Personal Ubiquitous Comput . , 10(4):255–268 , Mar . 2006 .
[ 10 ] L . Fan and H . Jin . A practical framework for privacy preserving data analytics . In WWW ’15 , pages 311–321 , Republic and Canton of Geneva , Switzerland , 2015 . International World Wide Web Conferences Steering Committee .
[ 11 ] A . Gkoulalas Divanis and G . Loukides . Revisiting sequential pattern hiding to enhance utility . In KDD ’11 , pages 1316–1324 , New York , NY , USA , 2011 . ACM .
[ 12 ] A . Gkoulalas Divanis and V . S . Verykios . An integer programming approach for frequent itemset hiding . In Proceedings of the 15th ACM International Conference on Information and Knowledge Management , CIKM ’06 , pages 748–757 , New York , NY , USA , 2006 . ACM .
[ 13 ] M . G¨otz , S . Nath , and J . Gehrke . Maskit : Privately releasing user context streams for personalized mobile applications . In SIGMOD ’12 , pages 289–300 , New York , NY , USA , 2012 . ACM .
[ 14 ] R . Gwadera , A . Gkoulalas Divanis , and G . Loukides .
Permutation based sequential pattern hiding . 2013 IEEE 13th International Conference on Data Mining , 0:241–250 , 2013 .
[ 15 ] S . Hajian , J . Domingo Ferrer , and O . Farr ˜A˘as .
Generalization based privacy preservation and discrimination prevention in data publishing and mining . Data Mining and Knowledge Discovery , 28(5 6):1158–1188 , 2014 .
[ 16 ] X . He , Y . Xiao , Y . Li , Q . Wang , W . Wang , and B . Shi .
Permutation anonymization : Improving anatomy for privacy preservation in data publication . In L . Cao , J . Huang , J . Bailey , Y . Koh , and J . Luo , editors , New Frontiers in Applied Data Mining , volume 7104 of Lecture Notes in Computer Science , pages 111–123 . Springer Berlin Heidelberg , 2012 .
[ 17 ] Y . He , S . Barman , D . Wang , and J . F . Naughton . On the complexity of privacy preserving complex event processing . In Proceedings of the 30th ACM SIGMOD SIGACT SIGART Symposium on Principles of Database Systems , PODS 2011 , June 12 16 , 2011 , Athens , Greece , pages 165–174 , 2011 .
[ 18 ] Y . He and J . F . Naughton . Anonymization of set valued data via top down , local generalization . Proc . VLDB Endow . , 2(1):934–945 , Aug . 2009 .
[ 19 ] V . S . Iyengar . Transforming data to satisfy privacy constraints . In KDD ’02 , pages 279–288 , New York , NY , USA , 2002 . ACM .
[ 20 ] O . Lara and M . Labrador . A survey on human activity recognition using wearable sensors . Communications Surveys Tutorials , IEEE , 15(3):1192–1209 , Third 2013 .
[ 21 ] B . Malin . Protecting dna sequence anonymity with generalization lattices . Carnegie Mellon University , School of Computer Science , 2004 .
[ 22 ] A . McGregor , I . Mironov , T . Pitassi , O . Reingold , K . Talwar , and S . Vadhan . The limits of two party differential privacy . In Foundations of Computer Science ( FOCS ) , 2010 51st Annual IEEE Symposium on , pages 81–90 , Oct 2010 .
[ 23 ] L . Sankar , S . Rajagopalan , and H . Poor . Utility privacy tradeoffs in databases : An information theoretic approach . Information Forensics and Security , IEEE Transactions on , 8(6):838–852 , June 2013 .
[ 24 ] R . Sherkat , J . Li , and N . Mamoulis . Efficient time stamped event sequence anonymization . ACM Trans . Web , 8(1):4:1–4:53 , Dec . 2013 .
[ 25 ] X . Sun and P . S . Yu . A border based approach for hiding sensitive frequent itemsets . In Proceedings of the Fifth IEEE International Conference on Data Mining , ICDM ’05 , pages 426–433 , Washington , DC , USA , 2005 . IEEE Computer Society .
[ 26 ] Y . Tao , H . Chen , X . Xiao , S . Zhou , and D . Zhang . Angel :
Enhancing the utility of generalization for privacy preserving publication . Knowledge and Data Engineering , IEEE Transactions on , 21(7):1073–1087 , July 2009 .
[ 27 ] M . Terrovitis , N . Mamoulis , and P . Kalnis . Local and global recoding methods for anonymizing set valued data . The VLDB Journal , 20(1):83–106 , Feb . 2011 .
[ 28 ] V . Verykios , A . Elmagarmid , E . Bertino , Y . Saygin , and
E . Dasseni . Association rule hiding . Knowledge and Data Engineering , IEEE Transactions on , 16(4):434–447 , April 2004 .
[ 29 ] D . Wang , Y . He , E . Rundensteiner , and J . F . Naughton .
Utility maximizing event stream suppression . In SIGMOD ’13 , pages 589–600 , New York , NY , USA , 2013 . ACM .
[ 30 ] R . C W Wong , A . W C Fu , K . Wang , and J . Pei . Minimality attack in privacy preserving data publishing . In Proceedings of the 33rd International Conference on Very Large Data Bases , VLDB ’07 , pages 543–554 . VLDB Endowment , 2007 .
346
