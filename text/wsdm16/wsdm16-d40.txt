Multi view Machines
Bokai Cao1∗ , Hucheng Zhou2 , Guoqiang Li3† and Philip S . Yu1,4 1Department of Computer Science , University of Illinois at Chicago , IL , USA
2Microsoft Research , Beijing , China
3Huawei Technologies , Shenzhen , China caobokai@uic.edu , huzho@microsoft.com , liguoqiang9@huawei.com , psyu@csuicedu
4Institute for Data Science , Tsinghua University , Beijing , China
ABSTRACT With rapidly growing amount of data available on the web , it becomes increasingly likely to obtain data from different perspectives for multi view learning . Some successive examples of web applications include recommendation and target advertising . Specifically , to predict whether a user will click an ad in a query context , there are available features extracted from user profile , ad information and query description , and each of them can only capture part of the task signals from a particular aspect/view . Different views provide complementary information to learn a practical model for these applications . Therefore , an effective integration of the multi view information is critical to facilitate the learning performance .
In this paper , we propose a general predictor , named multiview machines ( MVMs ) , that can effectively explore the full order interactions between features from multiple views . A joint factorization is applied for the interaction parameters which makes parameter estimation more accurate under sparsity and renders the model with the capacity to avoid overfitting . Moreover , MVMs can work in conjunction with different loss functions for a variety of machine learning tasks . The advantages of MVMs are illustrated through comparison with other methods for multi view prediction , including support vector machines ( SVMs ) , support tensor machines ( STMs ) and factorization machines ( FMs ) .
A stochastic gradient descent method and a distributed implementation on Spark are presented to learn the MVM model . Through empirical studies on two real world web application datasets , we demonstrate the effectiveness of MVMs on modeling feature interactions in multi view data . A 3.51 % accuracy improvement is shown on MVMs over FMs for the problem of movie rating prediction , and 0.57 % for ad click prediction .
∗This work was done while the author was doing internship at Microsoft Research . †This work was done before the author joins Huawei .
CCS Concepts •Information systems → Data mining ; •Computing methodologies → Machine learning ; Factorization methods ;
Keywords multi view learning , feature interaction , factorization
1 .
INTRODUCTION
Web data is available not only in great volume but also in multiple representations/views from a variety of sources or feature subsets . Generally , different views provide complementary information to learn an effective model for webscale applications . Thus , multi view learning can facilitate the learning performance and is prevalent in a wide range of application domains . For example , for the business on the web , it is critical to estimate the probability that the display of an ad to a specific user when s/he searches for a query will lead to a click . This process involves three entities : users , ads , and queries . An effective integration of features describing these different entities is directly related to precise targeting of an advertising system .
One of the key challenges of multi view learning is to model the interactions/correlations between different views , wherein complementary information is contained . Conventionally , multi kernel learning algorithms combine kernels associated with respective views to improve the learning performance [ 9 ] . Basically , coefficients are learned based on the usefulness/informativeness of the corresponding views , and thus inter view correlations are only considered at the viewlevel . These approaches , however , fail to explore the explicit correlations between features across multiple views .
In contrast to modeling on views , another direction for modeling multi view data is to directly consider the abundant correlations between features from different views . Feature interactions with different orders can reflect different but complementary insights . Assume that we have obtained a latent factor representing wealth/price related attributes for each entity ( ie , users , ads and queries ) in the advertising system of a search engine , as illustrated in Table 1 . For example , users who have high purchase power ( ie , a positive latent factor , auser > 0 ) may have interests in luxury products ( aad > 0 ) . However , a thoughtful recommender system should not always recommend luxury products to these users regardless of the query context . In Table 1 , it is unreasonable to recommend a luxury bag ( aad > 0 ) to the user when s/he searches for a disease ( aquery < 0 ) , in which case some
427 Table 1 : An example showing the discrepancy between feature interactions with different orders . #1 = user + ad + query , #2 = user × ad + user × query + ad × query , #3 = user × ad × query . #1
Query
User
#2
Ad
#3
1.20 ( + ) 1.20 ( + ) 1.20 ( + )
1.80 ( + ) 1.80 ( + ) 1.80 ( )
0.50 ( + ) 0.50 ( ) 0.50 ( )
3.50 ( + ) 2.50 ( + ) 1.10 ( )
3.66 ( + ) 0.66 ( + ) 1.86 ( )
1.08 ( + ) 1.08 ( ) 1.08 ( + ) relevant medicines or medical books ( aad < 0 ) would seem better choices . We can observe that , in such scenarios , only the third order interactions contribute positively to the recommendation of medicines and negatively to that of luxury bags , while the first order and the second order interactions insist to recommend something inappropriate in the specific context . Note here that we do not claim the higher order interactions can work as the best indicator by their own in all problems . Nevertheless , integrating their contributions into the decision function in an efficient manner is critical . S . Rendle pioneers the concept of factorization machines ( FMs ) [ 10 ] which are now the state of the art approach to model feature interactions and inspire this work . However , the practical implementations of FMs are usually limited to the second order interactions , ie , pairwise correlations . This is partially due to the fact that a separate set of latent factors ( parameters to be learned ) is introduced for each order of interactions in FMs . That is to say , a feature has a latent representation when it is considered for the secondorder interactions , while the same feature has a different representation for the third order interactions . Moreover , the global bias and the first order interaction terms in FMs are not factorized and independent from the latent factors for higher order interactions . These bias terms and latent factors for different orders altogether compose inconsistent representations of input features and thus compromise the model interpretability . In addition , independent factorization of interactions with different orders results in a large set of model parameters to be learned which makes the training process challenging .
The major challenge of including higher order interactions is that observations with such interactions become sparser with higher orders . Therefore , parameters representing higher order interactions can hardly be learned from their limited observations , especially from the extremely sparse data , eg , recommender systems . We suggest a common latent subspace for all features that is shared by different orders of interactions . In this manner , the full order interactions observed in the data can collectively be used to learn a consistent representation in the latent feature space .
In this paper , we propose a novel model for multi view prediction , called multi view machines ( MVMs ) . The main advantages of MVMs are outlined as follows :
• MVMs include the global bias and the full order interactions between features from multiple views , ranging from the first order interactions ( ie , contributions of single features ) to the highest order interactions ( ie , contributions of combinations of features from each view ) .
• MVMs jointly factorize the interaction parameters for different orders to allow accurate parameter estimation
Table 2 : Symbols .
Symbol Definition and description s v M T · , · ◦ ×k |·| ·F each lowercase letter represents a scale each boldface lowercase letter represents a vector each boldface capital letter represents a matrix each calligraphic letter represents a tensor , set or space denotes inner product denotes tensor product or outer product denotes mode k product denotes absolute value denotes ( Frobenius ) norm of vector , matrix or tensor under sparsity and avoid overfitting via the effect of bias factors .
• MVMs are a general predictor that can work with different loss functions ( eg , square error , hinge loss , logit loss ) for a variety of machine learning tasks .
To empirically analyze and understand these advantages , we have the MVM model and other baselines implemented in a distributed environment , GraphX [ 4 ] , which is a component of Spark [ 17 ] . Extensive experiments are conducted on real world web application datasets , for regression and classification tasks , respectively . A 3.51 % accuracy improvement is shown on MVMs over FMs for the problem of movie rating prediction , and 0.57 % for ad click prediction .
2 . BACKGROUND
In this section , we first state the problem of multi view prediction and briefly review the adaptation of existing methods for multi view prediction , including support vector machines ( SVMs ) , support tensor machines ( STMs ) and factorization machines ( FMs ) . 2.1 Multi view Prediction
, , x(m)T x(1)T
, x(2)T x(1)T views , ie , xT =
, x(3)T the dimensionality of the p th view . Let d =m
Suppose each instance has representations in m different , where x(p ) ∈ RIp , Ip is p=1 Ip , so x ∈ Rd . Considering the problem of click through rate ( CTR ) prediction for advertising display , for example , an instance corresponds to an impression which involves a user , an ad , and a query . Therefore , suppose xT = is an impression , x(1 ) contains information of the user profile , x(2 ) is associated with the ad information , and x(3 ) is the description from the query aspect . The result of an impression is click or non click . Given a training set with n labeled instances represented from m views : D = {(xi , yi)| i = 1 , , n} , in which xT i = and yi ∈ {−1 , 1} is the class label of the x(1 ) i th instance . For CTR prediction problem , y = 1 denotes click and y = −1 denotes non click in an impression . The task of multi view classification is to learn a function f : RI1 × ··· × RIm → {−1 , 1} that correctly predicts the label of a test instance . Alternatively , if yi ∈ R , it is a multi view regression problem , eg , rating prediction .
T
, , x(m )
T i i
Table 2 lists some basic symbols that will be used throughout the paper . In addition , we introduce the concept of tensors which are higher order arrays that generalize the notions of vectors ( the first order tensors ) and matrices ( the
428 second order tensors ) , whose elements are indexed by more than two indexes . Definitions of tensor product and mode k product are given which will be used to formulate our proposed model .
Definition 2.1
( Tensor Product or Outer Product ) .
The tensor product X ◦ Y of a tensor X ∈ RI1×···×Im and another tensor Y ∈ RI ( X ◦ Y)i1,,im,i m is defined by m = xi1,,im yi
1×···×I
1,,i m
1,,i
( 1 ) for all index values .
Definition 2.2
( Mode k Product ) . The mode k product X ×k M of a tensor X ∈ RI1×···×Im and a matrix M ∈ RI k×Ik is defined by
Ik
( X ×k M)i1,,ik−1,j,ik+1,,im = xi1,,im mj,ik
( 2 ) for all index values . ik=1
Basically , through the mode k matrix product , a new tensor of the same order is obtained by applying the matrix to each mode k fiber of the tensor . 2.2 SVM Model
Vapnik introduced support vector machines ( SVMs ) [ 14 ] based on the maximum margin principle . Essentially , SVMs integrate the hinge loss and the L2 norm regularization . The decision function with a linear kernel is as follows1 : d
ˆy = w0 + wixi = w0 + i=1 p=1 m
Ip ip=1 w(p ) ip x(p ) ip
( 3 )
, , x(m)T where x is simply a concatenation of features from different views in the multi view setting , ie , xT = , as shown in Figure 1 . x(1)T iq ip
= 0 or x(q ) iq
= 0 and x(q )
Obviously , no interactions between views are explored in Eq ( 3 ) . Through the employment of a nonlinear kernel , SVMs can implicitly project data from the feature space into a more complex high dimensional space , which allows SVMs to model higher order interactions between features . However , as discussed in [ 10 ] , all interaction parameters of nonlinear SVMs are completely independent . For nonlinear SVMs , there must be enough instances x ∈ D where x(p ) = 0 to reliably estimate the second order interaction parameter w(p,q ) . The instances ip,iq with either x(p ) ip ing w(p,q ) . That is to say , on a sparse dataset where there ip,iq are too few or even no cases for some higher order interactions , nonlinear SVMs are likely to degenerate into linear SVMs . Therefore , factorizing and projecting higher order interactions into a consistent latent space would facilitate parameter estimation under sparsity . 2.3 STM Model Cao et al . investigated multi view classification by modeling features interactions across views as a tensor , ie , X = 1The sign function is omitted , because the analysis and conclusions can easily extend to other generalized linear models , eg , logistic regression .
= 0 cannot be used for estimat
Figure 1 : Related work ( and variations ) on modeling feature interactions in multi view data . x(1 ) ◦ ··· ◦ x(m ) ∈ RI1×···×Im [ 2 ] and solved the problem in the framework of support tensor machines ( STMs ) [ 13 ] . Basically , as shown in Figure 1 , only the highest order interactions are explored : m wi1,,im x(p ) ip
( 4 ) im=1 p=1
··· Im
I1 where wi1,,im =m
ˆy = i1=1 of the weight tensor W ∈ RI1×···×Im [ 2 ] . p=1 w(p ) ip
, ie , a rank one decomposition
However , estimating a lower order interaction ( eg , a pairwise one ) reliably is easier than estimating a higher order one , and lower order interactions can usually explain the data sufficiently [ 12 , 1 ] . Thus , it motivates us to include the lower order interactions into the model . Moreover , instead of a rank one decomposition , it is desirable to apply a higher rank decomposition of the weight tensor to capture more latent factors and thereby achieving a better approximation to the original interaction parameters .
2.4 FM Model
Rendle introduced factorization machines ( FMs ) [ 10 ] that combine the advantages of SVMs with factorization models . The model equation of a 2 way FM is as follows : vi , vj xixj
( 5 ) d d d p=1 Ip and vi , vj =k wixi + i=1 i=1 j=i+1
ˆy = w0 + where d =m f =1 vi,f vj,f .
Note that pairwise interactions between all features are included in FMs without consideration of the view segmentation . In the multi view setting , there can be redundant correlations between features within the same view , ie , intra view correlations , which are thereby unworthy of consideration . Field aware FMs [ 6 ] integrate the field/view concept into the FM model where the extension is limited to the second order feature interactions . The coupled group lasso model [ 16 ] is essentially an application of the 2 way FMs in multi view classification . Let mvFM denote the multi view
View%1View%2View%3View%1View%2View%3secondorder%interac2onsfirstorder%interac2onsthirdorder%interac2onsSVMSTMFM429 variation of FMs with a decision function as follows :
ˆy = w0 +
+ ··· + m Ip Im−1 p=1 ip=1 w(p ) ip x(p ) ip
+
Im
I1 i1=1
I2 i2=1 im−1=1 im=1 v(m−1 ) im−1
, v(m ) im x(m−1 ) im−1 x(m ) im v(1 ) i1
, v(2 ) i2 x(1 ) i1 x(2 ) i2
( 6 )
The pairwise interaction parameter w(p,q ) ip,iq
= v(p ) ip
, v(q ) iq in Eq ( 6 ) indicates that w(p,q ) can be learned from instances ip,iq = 0 and some x(q ) = 0 ( sharing vp ) , or x(q ) = 0 with x(p ) iq ip and some x(p ) It makes mvFM ( and ip FMs ) more effective under sparsity than SVMs where only = 0 can be used to learn instances with x(p ) ip the second order feature interaction w(p,q ) ip,iq
= 0 ( sharing vq ) .
= 0 and x(q ) iq iq
.
However , the interaction parameters for different orders are completely independent in mvFM ( and FMs ) , eg , the first order interaction parameter , w(p ) , and the second order ip interaction parameter , v(p ) , in Eq ( 6 ) . Furthermore , as ip illustrated in Figure 1 , additional sets of model parameters will be introduced when we consider higher order feature interactions in mvFM ( and FMs ) which makes the learning process harder . A more effective strategy is needed when including the higher order interactions . 3 . MULTI VIEW MACHINE MODEL 3.1 Model Formulation
The key challenge of multi view prediction is to model the interactions between features from different views , wherein complementary information is contained . Here , we consider nesting all interactions up to the mth order between m views :
+
ˆy =
+ global bias
β0 I2 I1 i1=1 i2=1
+ · · · + m p=1
Ip ip=1
β(p ) ip x(p ) ip first order interactions
β(1,2 ) i1,i2 x(1 ) ii x(2 ) i2
+ · · · + im=1 im−1=1
Im−1
 m x(p ) ip p=1
Im  second order interactions
· · · Im im=1
I1 i1=1
βi1,,im mth order interactions
β(m−1,m ) im−1 ,im x(m−1 ) im−1 x(m ) im
( 7 )
Let us add an extra feature with constant value 1 to the , 1 ) ∈ RIp+1,∀p = feature vector x(p ) , ie , z(p)T 1 , , m . Then , Eq ( 7 ) can be compactly rewritten as :
= ( x(p)T
I1+1
··· Im+1 i1=1 im=1
ˆy = m p=1 wi1,,im z(p ) ip
( 8 ) where wI1+1,,Im+1 = β0 and wi1,,im = βi1,,im ,∀ip ≤ Ip . For wi1,,im with some indexes satisfying ip = Ip + 1 , it encodes lower order interaction between views whose ip ≤ denote wi1,,im where only ip ≤ Ip Ip . Hereinafter , let w(p ) ip and iq = Iq + 1 , q = p , and let w(p,q ) denote wi1,,im where ip ≤ Ip , iq ≤ Iq and ir = Ir + 1 , r /∈ {p , q} , etc . ip,iq
:,f ◦ a(2 )
:,f ◦ a(3 ) :,f .
Figure 2 : CP factorization . The third order ( m = 3 ) tensor W is approximated by k rank one tensors . The f th factor tensor is the tensor product of three vectors , ie , a(1 )
The number of parameters in Eq ( 8 ) is m p=1(Ip + 1 ) , which can make the model prone to overfitting and ineffective on sparse data . Therefore , we assume that the effect of interactions has a low rank and the mth order weight tensor W = {wi1,,im} ∈ R(I1+1)×···×(Im+1 ) can be factorized into k factors :
W = C ×1 A(1 ) ×2 ··· ×m A(m )
( 9 ) where A(p ) ∈ R(Ip+1)×k , and C ∈ Rk×···×k is the identity tensor , ie , ci1,,im = δ(i1 = ··· = im ) . Basically , Eq ( 9 ) k m is a CANDECOMP/PARAFAC ( CP ) factorization [ 7 ] as duced to km shown in Figure 2 , with element wise notation wi1,,im = ip,f . The number of model parameters is rep=1(Ip + 1 ) = k(m + d ) . It transforms Eq ( 8 ) I1+1 m m p=1 a(p ) into : f =1
··· Im+1
 k

( 10 )
ˆy = a(p ) ip,f z(p ) ip i1=1 im=1 p=1 f =1 p=1
We name this model in Eq ( 10 ) as multi view machines ( MVMs ) . As shown in Figure 3 , the full order interactions between multiple views are modeled in a tensor , and they are factorized collectively . The model parameters that have to be estimated are :
A(p ) ∈ R(Ip+1)×k , p = 1 , , m
( 11 ) where the ip th row a(p ) ip,k ) within A(p ) ip describes the ip th feature in the p the view with k factors . ip,1 , , a(p )
= ( a(p )
T
Definition 3.1
( Bias Factor ) . The bias factor is a collection of bias from each factor . In MVMs , the last row of A(p ) , ie , a(p ) , represents the bias factor of the p th view , and it is always associated with z(p ) Ip+1 = 1 in Eq ( 10 ) .
Ip+1
T
Hence , wI1+1,,Im+1 = k m f =1 p=1 a(p ) Ip+1,f
( 12 ) is the global bias , denoted as w0 hereinafter . 3.2 Time Complexity
Eq ( 10 ) is O(km
Next , we show how to compute the decision function of MVMs efficiently . The straightforward time complexity of p=1(Ip + 1) ) . However , we observe that there is no model parameter which directly depends on feature interactions , due to the joint factorization . Therefore , the time complexity can be largely reduced .
Lemma 31 The model equation of MVMs can be com puted in O(k(m + d) ) .
≈+ !++a:,1(1)a:,1(2)a:,1(3)a:,2(1)a:,2(2)a:,2(3)a:,k(1)a:,k(2)a:,k(3)W430 Figure 3 : Multi view machines . The full order feature interactions in multi view data are modeled in a tensor and jointly factorized into a common latent subspace .
Proof . The feature interactions in Eq ( 10 ) can be refor mulated as : i1=1 m I1+1 ··· Im+1 m ··· Im+1 I1+1 k
I1+1 k im=1 im=1 i1=1 f =1 p=1 z(p ) ip p=1 z(1 ) i1 a(1 ) i1,f
··· m p=1
 k Im+1 a(p ) ip,f z(p ) ip f =1
=
=
 a(p ) ip,f z(m ) im a(m ) im,f
( 13 ) f =1 i1=1 im=1
This equation has only linear complexity in both k and Ip . Thus , its time complexity is O(k(m + d) ) , which is in the same order of the number of model parameters . 3.3 Discussion
The joint factorization of the global bias and the full order interactions is important for MVMs . Thus , dependencies exist when interactions share the same feature . It benefits MVMs for parameter estimation under sparsity , since the latent factor a(p ) can be learned from any instances ip = 0 , which allows the second order interaction whose x(p ) ip = 0 or w(p,q ) ip,iq = 0 x(q ) iq as in nonlinear SVMs . Therefore , the interaction parameters in MVMs can be effectively learned without direct observations of such interactions in a training set of sparse data . can be approximated from instances whose x(p ) ip = 0 rather than instances whose x(p )
= 0 and x(q ) ip iq
The main difference between FMs and MVMs is that the interaction parameters for different orders are completely independent in FMs , eg , the first order interaction w(p ) and ip the second order interaction v(p ) in Eq ( 6 ) . On the conip trary , in MVMs , all orders of interactions share the same
Ip+1+1 , , a(m )
I1+1 , , a(p−1 ) in Eq ( 10 ) . For example , the comand the bias factors from other m− 1 views , Ip−1+1 , a(p+1 ) Im+1 , approximates the . Similarly , we can obtain the set of latent factors a(p ) ip bination of a(p ) ip ie , a(1 ) first order interaction w(p ) ip second order interaction w(p,q ) and ip,iq other m − 2 bias factors . Therefore , compared to MVMs , FMs are partially and independently factorized . Such difference is more significant for higher order FMs . As summarized in Table 3 , assuming the same number of factors for different orders of interactions , the model complexity of an m way FM is O(kmd ) which can be much larger than O(k(m + d ) ) of MVMs . 3.4 Extensions by combining a(p ) ip
, a(q ) iq
MVMs are flexible in the interactions of interests . That is to say , when there are too many views available for a learning task and interactions between some of them may obviously be physically meaningless , or sometimes the very high order interactions may not be intuitively interpretable , it is not desirable to include these potentially redundant interactions in the model . In such scenarios , one can ( 1 ) partition ( overlapping ) groups of views , ( 2 ) construct multiple MVMs on these view groups where the full order interactions within each group are included , and ( 3 ) implement a coupled matrix/tensor factorization [ 5 ] . This strategy excludes the inter group feature interactions .
On the other hand , in scenarios where the view segmentation is not given , one may be aggressive to consider interactions between all features , which becomes the problem setting of the original FMs . To achieve this purpose , we can simply repeat the same feature set in multiple views . Overall , MVMs are applicable with either conservative or radical strategies . Although MVMs can be easily adapted
View%1View%2View%3View%1View%2View%3global%biasthird3order%interac6onssecond3order%interac6onsfirst3order%interac6ons431 Table 3 : Summary of related work . Model complexity refers to both the number of parameters in the model and the time complexity to compute the decision function .
Method
Model complexity Feature interactions
Parameter factorization
Support vector machines ( SVMs ) [ 14 ] Support tensor machines ( STMs ) [ 13 ] Factorization machines ( FMs ) [ 10 ] Multi view machines ( MVMs )
O(d ) O(kd ) O(kmd )
O(k(m + d ) ) first order highest order up to full order full order none factorized ( k = 1 [ 2 ] ) partially and independently factorized fully and jointly factorized to include/exclude interactions between any features , that is outside the scope of this paper ; our focus is on investigating how to effectively explore the full order feature interactions from a given set of views .
4 . LEARNING MULTI VIEW MACHINES To learn model parameters in MVMs , we consider the following regularization framework :
L(ˆy(x|Θ ) , y ) + λΩ(Θ )
( 14 ) argmin
Θ
( x,y)∈D where Θ = {A(p)| p = 1 , , m} represents all model parameters , L(· ) is the loss function , Ω(· ) is the regularization term , and λ is the trade off between the empirical loss and the risk of overfitting .
Importantly , MVMs can be used to perform a variety of machine learning tasks , depending on the choices of the loss function . For example , to conduct regression , one can use the square error :
LS(ˆy(x|Θ ) , y ) = ( ˆy(x|Θ ) − y)2
( 15 ) and for classification problems , the logit loss can be used :
LL(ˆy(x|Θ ) , y ) = log(1 + exp(−y · ˆy(x|Θ) ) )
( 16 ) or the hinge loss . The regularization term is chosen based on our prior knowledge about the model parameters . Typically , we can apply L2 norm . 4.1 Gradient Descent
The model can be learned efficiently by alternating least square ( ALS ) , stochastic gradient descent ( SGD ) , L BFGS , etc . From Eq ( 13 ) , the gradient of the MVM model is : ∂ ˆy(x|Θ )
=z(p ) ip z(1 ) i1 a(1 ) i1,f
∂θ z(p−1 ) ip−1 a(p−1 ) ip−1,f
···
Ip−1+1 ··· Im+1 ip−1=1
I1+1 Ip+1+1 i1=1
 z(p+1 ) ip+1 a(p+1 ) ip+1,f ip+1=1 im=1 z(m ) im a(m ) im,f
( 17 ) ip
= x(p ) ip ip,f , and z(p )
= 1 if ip = Ip + 1 , otherwise where θ = a(p ) z(p ) . It validates that MVMs possess the multilinearip ity property [ 11 ] , because the gradient along θ is independent of the value of θ itself .
Note that in Eq ( 17 ) , the sumIp+1 ip=1 z(p ) a(p ) ip,f and their product can be precomputed for updating the f th factor of all features . Hence , each gradient can be computed in constant time O(1 ) . In an iteration , including the precomputation time , all parameters can be updated in O(k(m + d) ) . It can be even reduced under sparsity , where most of the ip elements in x ( or z ) are 0 and thus , the sums have only to be computed over the non zero elements , and only non zero parameters need to be updated according to Eq ( 17 ) .
It is straightforward to embed Eq ( 17 ) into the gradient of the loss functions eg , Eqs . ( 15) (16 ) , for direct optimization , as follows :
∂LS(ˆy(x|Θ ) , y )
∂θ
= 2(ˆy(x|Θ ) − y ) · ∂ ˆy(x|Θ )
∂θ
∂LL(ˆy(x|Θ ) , y )
−y
=
∂θ
1 + exp(y · ˆy(x|Θ ) ) 4.2 Distributed Implementation
· ∂ ˆy(x|Θ )
∂θ
( 18 )
( 19 )
Web scale applications in the real world always contain a huge number of entities represented in multiple views , eg , users , movies , ads , queries , with millions of instances , eg , ratings , impressions . In this section , we introduce a design for scalable learning and its implementation on top of GraphX [ 4 ] , which is a component of Spark [ 17 ] for graphs and graph parallel computation and provides high performance , scalability and fault tolerance for the learning process .
The training data is represented as a graph that contains two types of vertices , ie , instance vertices and feature vertices . A directed edge from a feature vertex to an instance vertex exists if the feature is non zero in the instance . The graph representation is efficient due to the inherent sparsity of the training data . The factor vector ( or weight coefficient that is not factorized in some baselines ) of a feature is represented as attributes of the corresponding feature vertex , the label information of an instance is represented as the attribute of the corresponding instance vertex , and the feature value is represented as the edge attribute . For distributed learning , the graph is partitioned and scheduled to different computing nodes for execution by the underlying distributed graph framework . In this manner , both data parallelism and model parallelism are achieved . ip
∗ a(p )
Each iteration in the gradient descent algorithm consists of two major steps , ie , feed forward and back propagation . In the feed forward process , messages are sent from feature vertices to instance vertices following the edges which are arrays b = Rk where bf = z(p ) ip,f . An instance vertex receives all messages from its connected feature vertices and sums them in view wise . The predicted value is then computed accordingly based on Eq ( 13 ) . In the backpropagation process , messages are sent from instance vertices to feature vertices which are arrays c = Rk where each element represents a gradient . A feature vertex averages the gradients received from its connected instance vertices and updates the factor vector accordingly based on Eqs . ( 18)(19 ) .
432 5 . EXPERIMENTS 5.1 Experimental Setup Data collections . To evaluate the performance of multiview prediction , we conduct extensive experiments on the MovieLens dataset for movie rating prediction ( regression ) and the BingAds dataset for CTR prediction ( classification ) , respectively .
• MovieLens dataset2 . A regression task for rating prediction is studied on the public dataset , MovieLens . Ratings are made on a 5 star scale , with halfstar increments . Each rating in this dataset has three views , ie , users , movies and implicit user feedback . The user view consists of binary feature vectors for user ids , and thus for each rating there is only one nonzero feature in the user view , ie , the associated user id ; the same for the movie view . The implicit feedback view is constructed following SVD++ [ 8 ] to capture users’ history information . Specifically , it consists of all movies the user has ever rated and it is normalized . Hence , this view makes use of implicit feedback information and indicates users’ preference . For this problem , the performance is measured by root mean square error ( RMSE ) .
• BingAds dataset3 . A classification task for CTR prediction is investigated on a dataset collected from ad impression logs of Bing , comprising three views : queries , ad URLs and impression information . Each instance is labeled as 1 if the impression is clicked and 1 otherwise . The query view consists of unigrams of user query words4 . The ad URL view includes URLs corresponding to the shown ads . The impression view is composed of impression locations and matched types . All features are hashed as integer ids and represented by binary values . There are multiple non zero features in the query view , only one non zero feature in the ad URL view , and 2 non zero features in the impression view . Area under the curve ( AUC ) is used as the evaluation metric .
See Table 4 for more information about the statistics and parameters used for each dataset . Compared models . In order to demonstrate the effectiveness of modeling feature interactions in multi view data , we compare the following models :
• Linear regression/logistic regression ( LR ) . We implement linear regression for regression tasks , eg , rating prediction , and logistic regression for classification tasks , eg , CTR prediction . They are essentially representative linear models ( including linear SVMs ) , but with different loss functions , eg , the square error and the logit loss , respectively . It is discussed in the form of SVMs in detail in Section 22
• Tensor factorization ( TF ) is a generalization of matrix factorization to higher orders . We can directly use
2http://grouplens.org/datasets/movielens 3The dataset is used internally in the Bing Ads team for model experiments rather than training product models . 4Stemming , lemmatization , removing stop words , etc . , are handled beforehand .
Table 4 : The statistics and parameters for each dataset . The number in braces indicates the dimensionality of the corresponding view .
Dataset
Views n η λ k #iterations
MovieLens
BingAds users ( 138,493 ) movies ( 27,278 ) impl . ( 27,278 )
20,000,263 queries ( 958,426 ) ad URLs ( 1,935,510 ) impressions ( 18 )
28,622,281
0.1 0.01 20 200
0.1 0.01 20 200 tensors to model the multi view data and factorize the weight tensor [ 2 ] . When the hinge loss is used , it can be solved in the framework of support tensor machines ( STMs ) [ 13 ] . When there are two views with categorical features , TF is reduced to conventional matrix factorization without bias terms . It is introduced as the STM model in Section 23
• Factorization machine ( FM ) explores pairwise interactions between all features without consideration of the view segmentation [ 10 ] . Its adaptation in the multi view setting , denoted as mvFM , considers feature interactions across views with the decision function in Eq ( 6 ) . This FM variation is specifically reviewed in Section 24 In addition to the popular 2 way FM model , we also implemented 3 way FMs to include higher order interactions , denoted as mvFM 3d , where feature interactions with different orders are modeled but with separate sets of parameters5 . Moreover , we regularized the second order and the third order interactions sharing the same latent factors and assigned the global bias and the first order interactions with independent weights that are not factorized , denoted as mvFM reg .
• Multi view machine ( MVM ) is our proposed model to explore the full order interactions embedded within multi view data , where feature interactions with different orders are jointly factorized and thereby sharing the same set of latent factors .
Configuration . All compared models are implemented on top of GraphX in Spark and trained with iterative forward and backward steps as in introduced in Section 42 The stochastic gradient descent with adaptive ( sub)gradient [ 3 ] is used as the optimization method . The code has been made available at GitHub6 .
For a fair comparison , the same parameter setting in Table 4 is used for all compared models . A deterministic data sampling is applied on both datasets so that 80 % data is used for training and the other 20 % for test . All models are trained with the same hardware configuration , where 10 homogeneous computing nodes are connected via 40Gbps 5In experiments , the rank k in mvFM 3d is set to 10 for both the second order and the third order interactions , so that the number of model parameters stays the same as other factorization baselines . 6https://github.com/cloudml/zen/tree/mvm opt/ml/src/ main/scala/com/github/cloudml/zen/ml/recommendation
433 Table 5 : Prediction accuracy . ↓ indicates the smaller the value the better the performance ; ↑ indicates the larger the value the better the performance .
MovieLens ( RMSE ) ↓ BingAds ( AUC ) ↑
Dataset MVM FM mvFM mvFM 3d mvFM reg TF LR
0.8376 0.8681 0.8447 0.9060 0.9807 0.8572 1.0017
0.7917 0.7872 0.7729 0.7201 0.6947 0.6645 0.7450
Infiniband network and each node has 16 2.40GHz Intel(R ) Xeon(R ) CPU E5 2665 cores and 128GB memory . There are 1 driver configured with 25GB memory and 10 workers configured with 100GB memory . The data is partitioned into 160 partitions based on node degree [ 15 ] to balance the load in each core and reduce the communication among cores .
5.2 Multi view Prediction Accuracy
The experimental results are shown in Table 5 . On the MovieLens dataset , the smaller RMSE , the better an algorithm . We can observe that LR is a simple baseline because as a conventional linear model , it neglects any interactions between features . However , such feature interactions can be critical in the sparse data , which explains much better performance achieved by FM through including pairwise feature interactions . We further find that mvFM is able to outperform FM by excluding intra view correlations . In our case of movie rating prediction on SVD++ data [ 8 ] , intraview correlations indicate interactions between movies the user has rated before which do not have direct influence on the user ’s preference of the current movie . However , interview correlations include interactions between the current movie and those movies rated by the user which are critical by matching the latent factor of the current movie and that of rated movies in the past . It validates the importance of introducing the view concept to learn an effective model in many problems .
The two variations of mvFM , mvFM 3d and mvFM reg , add the third order feature interactions in addition to the 2way mvFM . The difference is that mvFM reg uses the same set of latent factors for the second order and the third order interactions , while mvFM 3d introduces different parameters for interactions with different orders . It seems that the inclusion of higher order interactions fails to bring us any accuracy improvement , but TF manages to perform well by solely relying on the highest order interactions . It might imply that the consensus and complementary information between lower order and higher order interactions need to be better taken care of , which leads to our MVM model . Overall , we can observe from Table 5 that MVM achieves the best performance through joint factorization of feature interactions with different orders .
On the BingAds dataset , FM shows better performance than mvFM implying intra view correlations might be important for this problem . Consider the impression view comprising 2 non zero features for each instance , ie , impression location and matched type . Feature interactions between impression locations and matched types are not included in mvFM , whose variations and TF are even defeated by the linear model , LR . 5.3 Convergence Efficiency
In this section , we show more details about the training procedure of compared models . Figure 4(a ) illustrates the training loss on the MovieLens dataset where results are plotted in a log scale for better resolution on final convergence in the late stage . We observe that several compared models can fit themselves very well on the training data . For example , the final converged training RMSE of mvFM reg is as small as 0.5879 ; however , its test RMSE turns out to be 09807 It is clear that these models are easily prone to overfitting . On the contrary , our MVM model fits the training data with a moderate training RMSE = 0.7855 , and achieves the best test RMSE = 0.8376 , as shown in Table 5 .
Figure 5(a ) shows similar observations on the BingAds dataset where the overfitting problem is more significant . There is a possible reasoning about the capacity of MVMs to avoid overfitting . The joint factorization of the global bias , the first order interactions , the second order and higher order interactions plays a key role through the effect of bias factors a(p ) Ip+1 . The bias factor of each view will be updated by all instances , since it is always associated with a nonzero feature . Considering lower order interactions , other factor vectors contribute to the decision function by combining with bias factors of other views , as shown in Eq ( 10 ) . Therefore , bias factors are frequently retrieved and updated , making themselves relatively sensitive among model parameters . Our initial experiments found that the MVM model would suffer an unstable convergence process without the use of adaptive gradient [ 3 ] . Fortunately , this problem can be greatly alleviated by adaptively choosing an appropriate learning rate , as illustrated by the monotonic convergence process shown in Figure 4(a ) and Figure 5(a ) . With this problem solved , bias factors bring MVMs with the capacity to avoid overfitting by storing and providing the global knowledge , because each training instance will update them and each test instance will be predicted based on them . Such global knowledge is critical to model the bias information per view per factor and thus makes MVMs a robust model . In contrast , other compared methods ( eg , LR , FM ) use a single model parameter , ie , the global bias , for the purpose of the global knowledge , which is insufficient .
Figure 4(b ) and Figure 5(b ) compare the time cost of each model on the MovieLens dataset and the BingAds dataset , respectively . We find that our MVM model has the best system performance among models that consider high order feature interactions without bringing too much system overhead than the linear model , LR . The steep rise of LR and TF in Figure 5(b ) appears because of fault occurrence during training and automatic recovery by Spark . 5.4 Hyperparameter Sensitivity
In all experiments , the parameter η is heuristically set to 0.1 for MVMs and other baseline models , since the performance is insensitive to the initial learning rate by using the adaptive gradient [ 3 ] . In this section , we study the influence of the other two key hyperparameters , k and λ , in our MVM model . Due to the space limit , only results on MovieLens dataset are presented .
Experiments are conducted for different k and the results are shown in Figure 6(a ) . In contrast to findings in other
434 ( a ) Training loss .
( b ) Time cost .
Figure 4 : The training procedure on the MovieLens dataset .
( a ) Training loss .
( b ) Time cost .
Figure 5 : The training procedure on the BingAds dataset . related models based on latent factors [ 16 , 11 ] where accuracy can steadily get improved with larger k , we observe that both the converged training loss and test loss turn out to be better with the increasing of k and reach a peak at k = 40 , after which the accuracy will obviously decrease . It is reasonable in a general sense , because larger k renders the model with greater expressiveness which also leads to higher risk of overfitting . When the expressiveness of the model exceeds the information embedded in data , it is likely that the model will fit the training data very well yet fail on the test data . On the other hand , larger k leads to more model parameters which make it harder to learn an effective model within limited iterations . In general , Figure 6(a ) indicates that the performance of our MVM model in Table 5 can be further improved with k = 40 at the cost of a linear increase in both runtime and memory .
Moreover , we investigate the influence of the regularization parameter λ and present the results in Figure 6(b ) . We observe that our MVM model is insensitive to λ in a relatively large range and performs well and steadily when λ ≤ 01 It makes sense because large λ will let the regularization term override the effect of the loss function and thus dominate the objective . 5.5 Scalability
To investigate the scalability of our distributed learning framework introduced in Section 4.2 , we compute the speedup factor relative to the time cost with 4 nodes by varying the number of computing nodes from 4 to 10 . The number of training data partitions is always configured to be the num ber of cores . Experiments are repeated 10 iterations , and the average speedup factors with standard deviations are reported in Figure 6(c ) . We can observe that the speedup appears to be close to linear and close to the ideal speedup factors . Therefore , our distributed implementation of the MVM model is very scalable for web scale applications .
The gap between the real and ideal speedup may result from the increasing communication cost with the increasing number of computing nodes , since copies of instance and feature vertices would be distributed in more computing nodes which leads to larger state synchronization cost of each instance and feature vertice . We will consider using the parameter server to alleviate such problem .
6 . CONCLUSION AND FUTURE WORK
In this paper , we have proposed a multi view machine ( MVM ) model and presented a stochastic gradient descent ( SGD ) learning method with a distributed implementation on Spark . In general , the model is particularly designed for data that is composed of features from multiple views , between which the full order interactions are effectively explored . In contrast to other models that include only partial feature interactions or factorize different orders of interactions separately , MVMs jointly factorize the full order feature interactions and thereby benefiting parameter estimation under sparsity and rendering the model with the capacity to avoid overfitting . Moreover , MVMs can be applied to a variety of supervised machine learning tasks , including classification and regression . Empirical studies on real world
050100150200−1−05005115Number of iterationsTraining loss ( log scale ) MVMFMMVFMMVFM−3DMVFM−REGTFLR05010015020001234567Number of iterationsTime elapsed ( hr ) MVMFMMVFMMVFM−3DMVFM−REGTFLR050100150200−2−15−1−050Number of iterationsTraining loss ( log scale ) MVMFMMVFMMVFM−3DMVFM−REGTFLR0501001502000051152253Number of iterationsTime elapsed ( hr ) MVMFMMVFMMVFM−3DMVFM−REGTFLR435 ( a ) Influence of k .
( b ) Influence of λ .
( c ) Speedup .
Figure 6 : Sensitivity analysis of hyperparameters and the speedup of the distributed learning framework . web application datasets demonstrate the effectiveness of MVMs on modeling feature interactions in multi view data , which outperform baseline models for multi view prediction . The MVM model can be further investigated in several directions for future work . For example , in addition to SGD , we are interested in implementation of other learning algorithms in a distributed environment to facilitate convergence efficiency , eg , alternating least square ( ALS ) and Markov Chain Monte Carlo ( MCMC ) for MVMs . It is also interesting to have our model applied to other multi view prediction problems . Moreover , defining an evaluation metric for an effective view segmentation would be critical for the subsequent multi view learning . 7 . ACKNOWLEDGEMENTS
We would like to thank the anonymous reviewers for their comments . We also thank Dinggang Shen for his discussions , Chunhui Zhang from the Bing Ads team for providing the dataset and Bo Zhao for helping with the experiments . 8 . REFERENCES [ 1 ] Yuanzhe Cai , Miao Zhang , Dijun Luo , Chris Ding , and Sharma Chakravarthy . Low order tensor decompositions for social tagging recommendation . In WSDM , pages 695–704 . ACM , 2011 .
[ 2 ] Bokai Cao , Lifang He , Xiangnan Kong , Philip S . Yu ,
Zhifeng Hao , and Ann B . Ragin . Tensor based multi view feature selection with applications to brain diseases . In ICDM , pages 40–49 . IEEE , 2014 .
[ 3 ] John Duchi , Elad Hazan , and Yoram Singer . Adaptive subgradient methods for online learning and stochastic optimization . The Journal of Machine Learning Research , 12:2121–2159 , 2011 .
[ 4 ] Joseph E Gonzalez , Reynold S Xin , Ankur Dave ,
Daniel Crankshaw , Michael J Franklin , and Ion Stoica . GraphX : Graph processing in a distributed dataflow framework . In OSDI , pages 599–613 . USENIX , 2014 .
[ 5 ] Liangjie Hong , Aziz S Doumith , and Brian D Davison .
Co factorization machines : modeling user interests and predicting individual decisions in twitter . In WSDM , pages 557–566 . ACM , 2013 .
[ 6 ] Yu Chin Juan , Yong Zhuang , and Wei Sheng Chin . LIBFFM : A Library for Field aware Factorization Machines , 2015 . Software available at http://wwwcsientuedutw/˜cjlin/libffm
[ 7 ] Tamara G Kolda and Brett W Bader . Tensor decompositions and applications . SIAM review , 51(3):455–500 , 2009 .
[ 8 ] Yehuda Koren . Factorization meets the neighborhood : a multifaceted collaborative filtering model . In KDD , pages 426–434 . ACM , 2008 .
[ 9 ] Gert RG Lanckriet , Nello Cristianini , Peter Bartlett , Laurent El Ghaoui , and Michael I Jordan . Learning the kernel matrix with semidefinite programming . The Journal of Machine Learning Research , 5:27–72 , 2004 .
[ 10 ] Steffen Rendle . Factorization machines . In ICDM , pages 995–1000 . IEEE , 2010 .
[ 11 ] Steffen Rendle . Factorization machines with libFM .
Intelligent Systems and Technology , 3(3):57 , 2012 . [ 12 ] Steffen Rendle and Lars Schmidt Thieme . Pairwise interaction tensor factorization for personalized tag recommendation . In WSDM , pages 81–90 . ACM , 2010 .
[ 13 ] Dacheng Tao , Xuelong Li , Weiming Hu , Stephen
Maybank , and Xindong Wu . Supervised tensor learning . In ICDM , pages 8–pp . IEEE , 2005 .
[ 14 ] Vladimir Vapnik . The nature of statistical learning theory . Springer Science & Business Media , 2000 .
[ 15 ] Cong Xie , Ling Yan , Wu Jun Li , and Zhihua Zhang . Distributed power law graph computing : Theoretical and empirical analysis . In NIPS , pages 1673–1681 , 2014 .
[ 16 ] Ling Yan , Wu jun Li , Gui Rong Xue , and Dingyi Han .
Coupled group lasso for web scale CTR prediction in display advertising . In ICML , pages 802–810 , 2014 .
[ 17 ] Matei Zaharia , Mosharaf Chowdhury , Tathagata Das , Ankur Dave , Justin Ma , Murphy McCauley , Michael J Franklin , Scott Shenker , and Ion Stoica . Resilient distributed datasets : A fault tolerant abstraction for in memory cluster computing . In NSDI , pages 2–2 . USENIX , 2012 .
0102030405008208408608809kRMSE TrainingTest−4−3−2−10101234λ ( log scale)RMSE TrainingTest46810115225Number of computing nodesSpeedup IdealActual436
