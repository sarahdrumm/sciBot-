Dynamic Cost Per Action Mechanisms and Applications to Online Advertising
Hamid Nazerzadeh∗
Amin Saberi∗
Rakesh Vohra†
Abstract
We examine the problem of allocating a resource repeatedly over time amongst a set of agents . The utility that each agent derives from consumption of the resource is private information to that agent , and prior to consumption , may be unknown to that agent . We describe a mechanism based on a sampling based learning algorithm that under suitable assumptions is asymptotically individually rational , asymptotically Bayesian incentive compatible and asymptotically ex ante efficient .
Our mechanism can be interpreted as a Cost Per Action or Cost Per Acquisition ( CPA ) charging scheme in online advertising . In this scheme , instead of paying per click , the advertisers pay only when a user takes a specific action ( eg fills out a form ) or completes a transaction on their websites .
∗Management Science and Engineering Department , Stanford University , Stanford , CA 94305 , email :
{hamidnz,saberi@stanford.edu} versity , Evanston , IL 60208 , email : {r vohra@kelloggnorthwesternedu}
†Department of Managerial Economics and Decision Sciences , Kellogg School of Management , Northwestern Uni
1
1
Introduction
We study the following problem : there are a number of self interested agents competing for identical items sold repeatedly at times t = 1 , 2,··· . At each time t , a mechanism allocates the item to one of the agents . Agents discover their utility for the good only if it is allocated to them . If agent i receives the good at time t , she realizes utility uit ( denominated in money ) for it and reports ( not necessarily truthfully ) the realized utility to the mechanism . Then , the mechanism determines how much the agent has to pay for receiving the item . We allow the utility of an agent to change over time . For this environment , we are interested in auction mechanisms which have the following four properties .
1 . The mechanism is individually rational in each period .
2 . Agents have an incentive to truthfully report their realized utilities .
3 . The efficiency ( and revenue ) is , in an appropriate sense , not too small compared to a second price auction .
4 . The correctness of the mechanism does not depend on an a priori knowledge of the distribution of uit ’s . This feature is motivated by the Wilson doctrine [ 27 ] . 1
We propose simple mechanisms that under minimal assumptions satisfy all the properties above .
The precise manner in which these properties are formalized is described in section 2 .
We will build our mechanisms on a sampling based learning algorithm . The learning algorithm is used to estimate the expected utility of the agents . The mechanism takes two randomly alternating actions : exploration and exploitation . During exploration , the item is allocated for free to a randomly chosen agent . During exploitation , the mechanism allocates the item to the agent with the highest estimated expected utility . After each allocation , the agent who has received the item reports its realized utility . Subsequently , the mechanism updates the estimate of utilities and determines the payment .
We characterize a class of learning algorithms that ensure that the corresponding mechanism has the four desired properties . The main difficulty in obtaining this result is the following : since there is uncertainty about the utilities , it is possible that in some periods the item is allocated to an agent who does not have the highest utility in that period . Hence , the natural second highest price payment rule would violate individual rationality . On the other hand , if the mechanism does not charge an agent because her reported utility after the allocation is low , it gives her an incentive to shade her reported utility down . Our mechanism solves these problems by using an adaptive second pricing scheme .
We illustrate our results by identifying simple mechanisms that have the desired properties . We demonstrate these mechanisms for the case in which the uit ’s are independent and identicallydistributed random variables as well as the case where their expected values evolve like independent reflected Brownian motions . In these cases the mechanism is actually ex post individually rational . In our proposed mechanism , the agents do not have to bid for the items . This is advantageous when the bidders themselves are unaware of their utility values . However , in some cases , an agent might have a better estimate of her utility for the item than our mechanism . For this reason , we describe how we can slightly modify our mechanism to allow those agents to bid directly .
1Wilson criticizes relying too much on common knowledge assumptions .
1
In the next section , we will motivate our work in the context of online advertising . However , the motivation for our mechanism is not limited to such applications .
1.1 Applications to Online Advertising
Currently , the main two charging models in the online advertising industry are cost per impression ( CPM ) and cost per click ( CPC ) . In the CPM model , the advertisers pay the publisher for the impression of their ads . CPM is commonly used in traditional media ( eg magazines and television ) or banner advertising and is more suitable when the goal of the advertiser is to increase brand awareness .
A more attractive and more popular charging model in online advertising is the CPC model in which the advertisers pay the publisher only when a user clicks on their ads . In the last few years , there has been a tremendous shift towards the CPC charging model . CPC is adopted by search engines such as Google or Yahoo! for the placement of ads next to search results ( also known as sponsored search ) and on the website of third party publishers .
In this paper we will focus on a recently popular and widely advocated charging scheme known as the Cost Per Action or Cost Per Acquisition ( CPA ) model . In this model , instead of paying per click , the advertiser pays only when a user takes a specific action ( eg fills out a form ) or completes a transaction . Recently , several companies like Google , eBay , Amazon , Advertising.com , and Snap.com have started to sell advertising in this way .
CPA models can be the ideal charging scheme , especially for small and risk averse advertisers . We will briefly describe a few advantages of this charging scheme over CPC and refer the reader to [ 20 ] for a more detailed discussion .
One of the drawbacks of the CPC scheme is that it requires the advertisers to submit their bids before observing the profits generated by the users clicking on their ads . Learning the expected value of each click , and therefore the right bid for the ad , is a prohibitively difficult task especially in the context of sponsored search in which the advertisers typically bid for thousands of keywords . CPA eliminates this problem because it allows the advertisers to report their payoff after observing the user ’s action .
Another drawback of the CPC scheme is its vulnerability to click fraud . Click fraud refers to clicks generated by someone or something with no genuine interest in the advertisement . Such clicks can be generated by the publisher of the content who has an interest in receiving a share of the revenue of the ad or by a rival who wishes to increase the cost of advertising for the advertiser . Click fraud is considered by many experts to be the biggest challenge facing the online advertising industry [ 16 , 12 , 26 , 22 ] . CPA schemes are less vulnerable because generating a fraudulent action is typically more costly than generating a fraudulent click . For example , an advertiser can define the action as a sale and pay the publisher only when the ad yields profit2 .
On the other hand , there is a fundamental difference between CPA and CPC charging models . A click on the ad can be observed by both advertiser and publisher . However , the action of the user is hidden from the publisher and is observable only by the advertiser . Although the publisher can require the advertisers to install a software that will monitor actions that take place on their web site , even moderately sophisticated advertisers can find a way to manipulate the software if they find it sufficiently profitable . It is this difference that motivates the present paper .
2Of course in this case , CPA makes generating a fraudulent action a more costly enterprize , but not impossible
( one could use a stolen credit card number for example )
2
In our setting , the item being allocated is a search query for a keyword . An advertiser obtains a payoff when the user clicks on her advertisement and takes a specific action . Since the payoff is uncertain , she cannot know what it will be unless her ad is displayed . For simplicity of exposition only , we assume one keyword and one advertisement slot . In Section 6 we outline how to extend our results to the case where more than one advertisement can be displayed for each query .
1.2 Related Work
There are a number of interesting results on using machine learning techniques in mechanism design . We only briefly survey the main techniques and ideas and compare them with the approach of this paper .
Most of these works , like [ 7 , 10 , 19 ] , consider one shot games or repeated auctions in which the agents leave the environment after receiving an item . In our setting we may allocate the items to an agent several times and hence , we need to consider the strategic behavior of the agents over time . There is also a big literature on regret minimization or expert algorithms . In our context , these algorithms are applicable even if the utilities of the agents are changing arbitrarily . However , the efficiency ( and therefore the revenue ) of these algorithms is comparable to the mechanisms that allocate the item to the single best agent ( expert ) ( eg see [ 18] ) . Our goal is more ambitious : our efficiency is close the most efficient allocation that may allocate the item to different agents at different times . On the other hand , we focus on utility values that change smoothly ( eg like a Brownian motion ) .
In a finitely repeated version of the environment considered here , Athey and Segal [ 3 ] construct an efficient , budget balanced , mechanism where truthful revelation in each period is Bayesian incentive compatible . Similar results are obtained , by Bergemann and V¨alim¨aki [ 8 ] , Cavallo et al . [ 11 ] , and Bapna and Weber [ 6 ] , for classes of incentive compatible multiarmed bandit mechanisms ( see [ 14] ) . All these mechanisms need the exact solution of the underlying optimization problems , and therefore require complete information about the prior of the utilities of the agents . This violates the last of our desiderata . For a survey in dynamic mechanism design literature see [ 24 ] . In the context of sponsored search , attention has focused on ways of estimating click through rates . Different ( approximate ) truthful mechanisms have been proposed for learning the clickthrough rates [ 15 , 28 ] . From a different direction , Immorlica et al . [ 17 ] , examine the vulnerability of various procedures for estimating click through , and identify a class of click through learning algorithms in which fraudulent clicks cannot increase the expected payment per impression by more than a factor of o(1 ) . These works commonly assume that utilities are remain constant overtime . In contrast , we study conditions which guarantee incentive compatibility and efficiency , while the utility of ( all ) agents may evolve over time .
Recently , Babaioff et al . [ 5 ] , and Devanur and Kakade [ 13 ] give a characterization of incentive compatible multi armed bandit mechanisms . They show that if one requires truthfulness as a weakly dominant strategy then the the exploration and exploitation phases should be completely separated .
2 Definitions and Notation
Suppose n agents competing in each period for a single item . The item is sold repeatedly at times t = 1 , 2,··· . Denote by uit the nonnegative utility of agent i for the item at time t . Utilities are
3 denominated in a common monetary scale . The utilities of agents may evolve over time according to a stochastic process . We assume that for i = j , the evolution of uit and ujt are independent stochastic processes . We also define µit = E[uit|ui1,··· , ui,t−1 ] . Throughout this paper , expectations are taken conditioned on the complete history . For simplicity of notation , we now omit those terms that denote such a conditioning . Let M be a mechanism . At each time , M allocates the item to one of the agents . Define xit to be the variable indicating the allocation of the item to i at time t . If the item is allocated to agent i , she can observe uit , her utility for the item . Then she reports rit as her utility to the mechanism . The mechanism then determines the payment , denoted by pit . Note that we do not require an agent to know her utility for the item before acquiring it . She may also misreport her utility to the mechanism after the allocation .
Definition 1 An agent i is truthful if rit = uit , for all time xit = 1 , t > 0 .
Our goal is to design a mechanism which has the following properties . We assume n , the number of agents , is constant .
Individual Rationality : A mechanism is ex post individually rational if for any time T > 0 and any agent 1 ≤ i ≤ n , the total payment of agent i does not exceed the sum of her reports : xitrit − pit > 0 .
T t=1
T t=1
M is asymptotically ex ante individually rational if : lim inf T→∞ E[ xitrit − pit ] ≥ 0 .
Incentive Compatibility : This property implies that truthfulness defines an asymptotic Bayesian Nash equilibrium . Consider agent i and suppose all agents except i are truthful . Let Ui(T ) be the expected total profit of agent i , if agent i is truthful between time 1 and T . Also , let
Ui(T ) be the maximum of expected profit of agent i under any other strategy . Asymptotic incentive compatibility requires thatUi(T ) − Ui(T ) = o(Ui(T ) ) . mechanism up to time T is E[T Then , the expected revenue of a second price mechanism up to time T is equal to E[T
Efficiency and Revenue : Call a mechanism ex ante efficient if at each time t it allocates the item to an agent in argmaxi{µit} . The total social welfare obtained by an ex ante efficient t=1 maxi{µit} ] . Let γt be the second highest µit at time t > 0 . t=1 γt ] . We will measure the efficiency and the revenue of M by comparing it to the second price mechanism that allocates the item to an agent in argmaxi{µit} and charges her the second highest µit . Let W ( T ) and R(T ) be the expected welfare and the expected revenue of
4 mechanism M between time 1 and T , when all agents are truthful , ie
W ( T ) = E[ xitµit ] n T n T t=1 i=1 t=1 i=1
R(T ) = E[ pit ]
{µit} ] max i
T t=1
T
Then , M is asymptotically ex ante efficient if :
W ( T ) = ( 1 − o(1))E[
Also , the revenue of M is asymptotically equivalent to the revenue of the second price auction if :
R(T ) = ( 1 − o(1))E[
γt ]
3 Proposed Mechanism t=1
We build our mechanism on top of a learning algorithm that estimates the expected utility of the agents . We refrain from an explicit description of the learning algorithm . Rather , we describe sufficient conditions for a learning algorithm that can be extended to a mechanism with all the properties we seek ( see section 41 ) In section 5.1 and 5.2 we give two examples of environments where learning algorithms satisfying these sufficient conditions exist . The mechanism randomly alternates between two actions : exploration and exploitation . At time t , with probability η(t ) , η : N → [ 0 , 1 ] , the mechanism explores ie it allocates the item for free to an agent chosen uniformly at random . With the remaining probability , the mechanism exploits . During exploitation , the item is allocated to the agent with the highest estimated expected utility . Then , the agent reports her utility to the mechanism and the mechanism determines the payment . We first formalize our assumptions about the learning algorithm and then we discuss the payment scheme . The mechanism is given in Figure 1 .
The learning algorithm uses the history of the reports of agent i to give an estimate of µit . Let
µit(T ) be the estimate of the learning algorithm for µit conditional on the history of the reports We now describe the payment scheme . Letγt(T ) = maxj=i{µjt(T )} , where i is the agent who up to but not including time T . Note that T can be bigger than t . In other words , we allow the learning algorithm to refine its earlier estimates using more recent history . receives the item at time t . We define yit to be the indicator variable of the allocation of the item to agent i during exploitation . The payment of agent i at time t , denoted pit , is equal to :
Therefore , we have : pit = t−1 t k=1 yik min{γk(k),γk(t)} − t−1 t−1 yik min{γk(k),γk(t)} . k=1 pik = pik k=1 k=1
5
For t = 1 , 2 , . . .
With probability η(t ) , explore :
Uniformly at random , allocate the item to an agent i , 1 ≤ i ≤ n . pit ← 0
With probability 1 − η(t ) , exploit :
Randomly allocate the item to an agent i ∈ argmaxi{µit(t)} . pit ←t−1 k=1 yik min{γk(k),γk(t)} −t−1 k=1 pik rit ← the report of agent i . pjt ← 0 , j = i
Figure 1 : Mechanism M
In this payment scheme , an agent only pays for the items that are allocated to her during exploitation , up to but not including time t . The payments emulate the second pricing scheme with the difference that the second highest utility γt is replaced with its estimation . The minimum is taken to ensure individual rationality , see Theorem 1 .
Another important feature of our payment mechanism is that it is adaptive and cumulative . We allow the mechanism to correct the payments of the agents if the items allocated to them were overpriced or underpriced earlier .
4 Analysis
We start this section by defining ∆t . Assume all agents are truthful up to time t . Let ∆t be the maximum over all agents i , the difference between µit and its estimation using only reports taken during exploration . We assume the accuracy of the learning algorithm is monotone . Ie at time T ≥ t we have :
E[|µit(T ) − µit| ] ≤ E[|µit(t ) − µit| ] ≤ E[∆t ] evolution of uit ’s and the random choices of the mechanism . For simplicity of notation , we omit those terms that denote such a conditioning .
In the inequality above , and in the rest of the paper , the expectations ofµit are taken over the show that if ∆t is small , then agents cannot gain much by deviating from the truthful strategy . We also bound the efficiency loss in terms of ∆t .
In this section , we will relate the performance of the mechanism to the estimation error of the learning algorithm . We start with the individual rationality aspects of the mechanism . Then we
( 1 )
Theorem 1 For a truthful agent i up to time T , the expected amount that i may be overcharged
6
E[ yituit ] − E[ pit ] ≥ −E[
∆t ] t=1 t=1 t=1
T
T
T
T
T−1
T
Proof : We prove a stronger result by showing :
E[ pit ] − E[ yituit ] ≤ E[ t=1 t=1 t=1 where ( z)+ = max{0 , z} . yit(γt(t ) − µit)+ ] ≤ E[
T t=1
∆t ]
( 2 )
If yit = 0 then pit = 0 . Also , recall that the expectations are taken conditioned on the previous
E[
T t=1
T t=1 pit ] − E[ yituit ] ≤ E[ history . Therefore , for every time t , E[uit ] = E[µit ] . By the payment rule we have :
T−1 yit(min{γt(t),γt(T )} − µit ) ] T−1 yit(γt(t ) − µit ) ] T−1 ( γt(t ) − µit)+ ] impliesγt(t ) ≤µit(t ) . Plugging that into the above inequality , we get :
≤ E[
≤ E[ t=1 t=1 t=1 yit indicates whether the mechanism allocated the item to agent i at time t . Therefore , yit = 1 for the items she receives is bounded by the total estimation error of the algorithm , ie ,
T
T
E[ pit ] − E[ yituit ] ≤ E[ t=1 t=1
T−1 ( µit(t ) − µit)+ ] T−1 t=1
∆t ]
≤ E[
The last inequality follows from equation ( 1 ) . t=1 fi
Now we study the incentive compatibility aspects of the mechanism .
Theorem 2 If all other agents are truthful , agent i cannot increase her expected utility up to time T by the quantity below if she deviates from the truthful strategy :
T t=1
8E[
∆t ] + E[ max 1≤t≤T
{µit} ]
( 3 )
Let us bound the expected profit of i for deviating from the truthful strategy . The term Proof : E[max1≤t≤T{µit} ] in the expression above bounds the outstanding payment of agent i up to time T . Recall that agents do not pay for the last item they receive during exploitation .
7
Let S be the strategy that i deviates to . Fixing the evolution of all ujt ’s , 1 ≤ j ≤ n , and all random choices of the mechanism . Let DT be the set of times that i receives the item under strategy S during exploitation and before time T . Formally , DT = {t < T|yit = 1 , if the strategy of i is S} . t correspond to the estimates of the mechanism when the strategy of i is S . The expected profit i could obtain under strategy S from the items she received during exploitation , up to time T − 1 , is equal to :
Similarly , let CT = {t < T|yit = 1 , if i is truthful} . Also , letµ it andγ µit − min{γ µit − min{γ t(t),γ t(t),γ t(T )} ] + t(T )} ]
( 4 )
E[ t∈DT
µit − min{γ t(t),γ t(T )} ] = E[ E[ t∈DT ∩CT t∈DT \CT
For times t ≥ 1 , we examine two cases : 1 . The first case is when t ∈ DT ∩ CT . We will show that in those cases the “ current price ” t(t),γ min{γ given to agent i under the two scenarios are close . To this aim , we first observe : t(T )} = γt(t ) + min{γ t(T ) −γt(t)} Let j = i be the agent with the highest µjt(t ) , ie , µjt(t ) = argmaxj=i{µjt(t)} = γt(t ) . Because i is the winner both in DT and CT , by definition of γt we have µ jt(T ) ≤γ µ min{γ t(t),γ t(t ) −γt(t),γ t(T ) . Plugging into the above inequality we get : t(T )} ≥ γt(t ) + min{µ jt(t ) −µjt(t),µ ≥ γt(t ) − |µjt(t ) −µ jt(t)| − |µjt(t ) −µ = γt(t ) − |µjt(t ) − µjt + µjt −µ ≥ γt(t ) − 2|µjt(t ) − µjt| − |µjt −µ jt(T ) −µjt(t)} jt(t)| − |µjt(t ) − µjt + µjt −µ jt(T )| jt(t)| − |µjt −µ jt(t ) ≤ γ t(t ) and jt(T )| jt(T )|
By taking expectation from both sides we have :
E[min{γ t(t),γ t(T )}I(t ∈ DT ∩ CT ) ] ≥ E[γt(t)I(t ∈ DT ∩ CT ) ] − E[2|µjt(t ) − µjt|I(t ∈ DT ∩ CT ) ] − E[,|µjt −µ jt(t)| − |µjt −µ ≥ E[γt(t)I(t ∈ DT ∩ CT ) ] − E[2|µjt(t ) − µjt| + |µjt −µ jt(T )| I(t ∈ DT ∩ CT ) ] jt(t)| + |µjt −µ t(T )}I(t ∈ DT ∩ CT ) ] ≥ E[γt(t)I(t ∈ DT ∩ CT ) ] − 4E[∆t ] t(t),γ jt(T )| ]
( 5 )
E[min{γ
Because agent j is truthful , by ( 1 ) , we get :
2 . The second case is when t ∈ DT \ CT . We show in those cases agent i cannot increase her
“ profit ” by much .
8
Let j be the agent who would receive the item at time t when agent i is truthful . Therefore t(t),γ
µit − min{γ
µjt(t ) ≥µit(t ) . Also , µjt(t ) ≤ maxj=i{µjt} = γt(t ) . Similarly , µjt(T ) ≤ γt(T ) . t(t),γ t(T )} = µit(t ) + |µit −µit(t)| − min{γ ≤ µjt(t ) + |µit −µit(t)| − min{µjt(t),µjt(T )} ] t(T )} = max{0,µjt(t ) −µjt(T )} + |µit −µit(t)| ≤ |µjt(t ) −µjt(T )| + |µit −µit(t)| ≤ |µjt(t ) − µjt| + |µjt −µjt(T )| + |µit −µit(t)|
We sum up the inequality above over all t ∈ DT \ CT . Because all agents are truthful , taking expectation from both sides , , by ( 1 ) , we get :
E[ t∈DT \CT
µit − min{γ t(t),γ
T t(T )} ] ≤ 3E[ t=1
∆t ]
( 6 )
T−1 t=1
E[
Substituting inequalities ( 5 ) and ( 6 ) into ( 4 ) : yituit − pit ] ≤ E[ = E[
∆t ] t∈DT ∩CT
T µit −γt(t ) ] + 7E[ µit −γt(t ) ] − E[ µit −γt(t ) ] + 7E[ µit − γt ] ≤ E[T yituit − pit ] − E[ uit − pit ] ≤ 8E[ t=1 ∆t ] . Therefore :
T t∈CT \DT t∈CT
∆t ] t∈CT \DT t=1
By inequality ( 2 ) , −E[ T−1
E[
T t=1
∆t ] which completes the proof . t=1 t∈CT t=1 fi during exploration is equal to E[T
We now compare the welfare of our mechanism to the efficient mechanism that allocates the item to the agent with the highest expected utility every time . The expected loss of efficiency t=1 η(t ) maxi{µit} ] . In the next theorem , we show that in the equilibrium , the efficiency loss during exploitation is bounded by a factor of the total estimation error of the learning algorithm . Theorem 3 Let W ( T ) denote the expected welfare of mechanism M between time 1 and T . If all the agents are truthful , we have :
W ( T ) ≥ E[
T t=1
T t=1
{µit} ] − E[ max i
9
η(t ) max i
{µit} + 2∆t ]
Our mechanism can lose efficiency in two ways . First , we can lose efficiency during Proof : exploration when we allocate the item to one of the agents chosen uniformly at random . The expected loss in this case is at most E[T the rule of the mechanism we haveµit(t ) ≤µjt(t ) . By subtracting this inequality from µjt − µit we
The mechanism can also make a mistake during exploitation : the error in the estimations may lead to allocating the item to an agent who does not value the item the most . Suppose at time t , during exploitation , the mechanism allocated the item to agent j instead of i , ie , µit > µjt . By t=1 η(t ) maxi{µit} ] . get :
µjt − µit ≥ µjt − µit − ( µjt(t ) −µit )
= ( µjt −µjt(t ) ) + ( µit(t ) − µit ) loss during exploration is bounded by 2E[T
We sum up this inequality over all such time t , and by inequality ( 1 ) , the expected efficiency Therefore , for the expected welfare of M between time 1 and T we have : t=1 ∆t ] .
T t=1
E[ max i
{µit} ] − W ( T ) ≤ E[
T t=1
η(t ) max i
{µit} + 2∆t ] fi
4.1 Sufficient Conditions for the Learning Algorithm
In this section , we give sufficient conditions on the learning algorithm which guarantee asymptotic ex ante individual rationality , incentive compatibility and efficiency of our mechanism . Theorem 4 If for the learning algorithm , for all 1 ≤ i ≤ n , and T > 0 : then mechanism M is asymptotically ex ante individually rational and asymptotically incentive compatible . Also , if in addition to ( C1 ) , the following condition holds
( C1 ) E[max1≤t≤T{µit} +T ( C2 ) E[T t=1 ∆t ] = o(E[T t=1 η(t ) maxi{µit} ] = o(E[T t=1 maxi{µit} ] ) t=1 η(t)µit ] ) then , M is asymptotically ex ante efficient .
Before stating the proof , note that the above theorem suggests a trade off between exploitation and exploration rates in our context : higher exploration rates lead to more accurate estimates of the utilities of the agents but they decrease the efficiency . So it is natural that condition ( C1 ) gives a lower bound on the exploration rate and condition ( C2 ) gives an upper bound . In the following sections , we will show with two examples how conditions ( C1 ) and ( C2 ) can be used to adjust the exploration rate of a learning algorithm in order to obtain asymptotic efficiency and incentive compatibility . Proof :
The expected utility of a truthful agent i up to time T is equal to :
T t=1
E[ xituit ] =
1 n
E[
η(t)µit ] + E[ yitµit ]
T
T t=1 t=1
10
Subtracting E[T T
E[
T xituit ] − E[ t=1 t=1
T yitµit ] − E[ t=1 pit ] ) t=1 pit ] from both sides , by Theorem 1 we get :
T T t=1
E[
η(t)µit ] + ( E[
E[
η(t)µit ] − E[
∆t ] t=1 t=1
T T t=1
T pit ] =
1 n ≥ 1 n
T
Plugging condition ( C1 ) into the equation above yields :
E[ xituit ] − E[ pit ] ≥ (
− o(1))E[
η(t)µit ]
( 7 ) ity ( 7 ) implies that the utility of the agent i is Ω(E[T
Therefore , the mechanism is asymptotically ex ante individually rational . Moreover , inequalt=1 η(t)µit] ) . Thus , by Theorem 2 , if ( C1 ) t=1 t=1 t=1 holds , then the mechanism is asymptotically incentive compatible .
To prove the claim about the efficiency of the mechanism , we invoke Theorem 3 . By this
1 n theorem and condition ( C1 ) we have :
T
T t=1
W ( T ) ≥ E[
η(t ) max
{µit} + 2∆t ]
≥ E[
{µit} ] − ( 1 + o(1))E[
η(t ) max i
{µit} ]
T
{µit} ] − E[ t=1
T T t=1 t=1 max i max i
Plugging condition ( C2 ) into the equation above we get :
E[ max i
{µit} ] − W ( T ) = O(E[
η(t ) max i
{µit} ] ) i
T t=1
T T t=1 which implies asymptotic ex ante efficiency . t=1
= o(E[ max i
{µit} ] ) fi
The above theorem shows that under some assumptions , the welfare obtained by the mechanism is asymptotically equivalent to efficient mechanism that every time allocates the item to the agent with the highest expected utility . We give similar conditions on the revenue guarantee of the mechanism .
Theorem 5 If in addition to ( C1 ) , the following condition holds
( C3 ) E[T t=1 η(t ) maxi{µit} ] = o(E[T t=1 γit ] ) then the revenue of the mechanism is asymptotically equivalent to the revenue of the efficient mechanism that at every time allocates the item to the agent with the highest expected utility and charges the winning agent the second highest expected utility .
11 first one is the loss during the exploration which is at most E[T
Proof : Using a similar argument as before , our mechanism can lose revenue in three ways . The t=1 η(t ) maxi{µit} ] . There is also an estimation error of γt . Let i be the agent who has received the item at time t . We consider two cases : t=1 η(t)γt ] ≤ E[T
1 . If i is the agent with the second highest expected utility , then let j be the agent with the highest expected utility . The estimation error of γt is equal to γt − min{γt(t),γt(T )} .
γt − min{γt(t),γt(T )} ≤ µit − min{µjt(t ) −µjt(T )} ≤ µjt − min{µjt(t ) −µjt(T )} ≤ max{µjt −µjt(t ) , µjt −µjt(T )} ≤ |µjt −µjt(t)| + |µjt −µjt(T )|
Therefore in this case , by inequality ( 1 ) , the expected estimation error of γt is bounded by 2∆t .
2 . Otherwise , let j be the agent with the second highest expected utility .
Similar to the previous case , we have :
γt − min{γt(t),γt(T )} ≤ µjt − min{µjt(t ) −µjt(T )} γt − min{γt(t),γt(T )} ≤ |µjt −µjt(t)| + |µjt −µjt(T )| which bounds the expected estimation error of γt by 2∆t .
There third factor contributing to loss of revenue is the outstanding payment of the agents . Agents do not pay for the last item they have received during exploitation . These outstanding payments attribute to a loss that is bounded by n · E[max1≤t≤T γt ] .
For the expected revenue of the mechanism we have : pit ] t=1 i=1
T n T ( 1 − η(t ) ) min{γt(T ),γt(t)} ] − n · E[max T T
( 1 − η(t))(γt − 4∆t ) ] − n · E[max t≤T
T
T
γt ]
γt ] − E[
η(t)γt ] − E[ t=1 t=1
γt ] t≤T
R(T ) = E[
≥ E[
≥ E[
≥ E[ t=1 t=1 t=1
Plugging condition ( C1 ) we get :
R(T ) ≥ E[
T t=1
T
γt ] − ( 1 + o(1))E[ t=1
12
4∆t ] − n · E[max t≤T
γt ]
η(t ) max i
{µit} ]
Therefore , by condition ( C3 )
T
E[ t=1
γt ] − R(T ) = o(E[
T t=1
γt ] ) fi
4.2 Allowing the Agents to Bid In mechanism M no agent explicitly bids for an item . Whether an agent receives an item or not depends on the history of their reported utilities and the estimates that the learning algorithm computes from them . This may be advantageous when the bidders themselves are unaware of their expected utilities . However , sometimes the agents have a better estimate of their utilities than the mechanism . For this reason we describe how to modify M so as to allow the agents to bid for the items . Suppose M is doing exploitation at time t and let Bt be the set of agents who are bidding at this time . The mechanism bids on the behalf of all agent i /∈ Bt . Denote by bit the bid of agent i ∈ Bt for the item at time t . The modification of M sets bit =µit(t ) , for i /∈ B . Then , the item is Let i be the agent who received the item at time t . Also , letγt(T ) to be equal to maxj=i{bjk} . allocated at random to one of the agents in argmaxibit .
The payment of agent i will be pit ← t−1 yik min{γk(t ) , bik} − t−1 pik . k=1 k=1
We also call agent i truthful if : 1 . rit = uit , for all time xit = 1 , t ≥ 1 .
2 . If i bids at time t , then E[|bit − µit| ] ≤ E[|µit(t ) − µit| ] .
Note that the second condition does not require that agents bid their actual utility , only that their bids are closer to their actual utilities than our estimates . With these modifications , the theorems in the previous section continue to hold .
5 Examples
In this section , we study two models for the utilities of the agents . In the first model , the utilities of the agents are independent and identically distributed . In the second , the utility of each agent evolves independently like a reflected Browning motion . In both of these examples , we give simple sampling based algorithms for learning the utilities of the agents . We show how we can use Theorems 4 and 5 to adjust the exploration rate of a simple learning algorithm to satisfy the conditions of our theorem . We also show that these mechanisms satisfy a stronger notion of individual rationality ie ex post individual rationality . A mechanism is ex post individually rational if for any agent i and for all T ≥ 1 :
T pit ≤ T t=1 t=1
13 xitrit
5.1 Independent and Identically Distributed Utilities Assume that for each i , uit ’s are independent and identically distributed random variables . For simplicity , we define µi = E[uit ] , t > 0 . Without loss of generality , assume 0 < µi ≤ 1 . bandit problem3 . For ∈ ( 0 , 1 ) , we define :
In this environment , the learning algorithm we use is an ε greedy algorithm for the multi armed t−1 ( T k=1
0 , xit nit = η ( t ) = min{1 , nt− ln1+ t} k=1 xikrik)/niT ,
µit(T ) = niT > 0 niT = 0
Call the mechanism based on this learning algorithm M ( iid ) . Lemma 6 If all agents are truthful , then , under M ( iid )
E[∆t ] = O(
1√ t1−
) .
The proof of this lemma is given in appendix A1 3 , M ( iid ) is asympTheorem 7 M ( iid ) is ex post individually rational . Also , for 0 ≤ ≤ 1 totically incentive compatible , ex ante efficient , and has a revenue asymptotically equivalent to the revenue of the efficient second price auction .
Proof : We first prove ex post individual rationality . It is sufficient to prove it only for the periods that agent i has received the item during exploitation . For T , such that yiT = 1 , we have
Because yiT = 1 we haveγt(T ) ≤µit(T ) . Plugging into the inequality above we get t=1 t=1 pit = yit min{γt(T ),γit(t)} ≤ T−1 T−1 T yitγt(T ) pit ≤ T−1 yitµit(T ) ≤ T−1 T−1 xitµit(T ) =
T t=1 xitrit t=1 t=1 t=1 t=1
Therefore the mechanism is ex post individually rational . We complete the proof by showing that conditions ( C1 ) , ( C2 ) , and ( C3 ) hold . Note that µi ≤ 1 . By Lemma 6 , for ≤ 1
3 , we have
E[1 +
∆t ] = O(T
1+
2 ) = o(T 1− ln1+ T ) = O(
η ( t)µi ) . t=1 t=1
Therefore , ( C1 ) holds .
E[1 +T−1
The welfare and revenue of the mechanism between time 1 and T of is θ(T ) . For any > 0 , fi t=1 ∆t + ηt ] = o(T ) which satisfies ( C2 ) and ( C3 ) .
3 See [ 4 ] for a similar algorithm .
14
T−1
T
5.2 Brownian Motion In this section , we assume for each i , 1 ≤ i ≤ n , the evolution of µit is a reflected Brownian motion with mean zero and variance σ2 i . The reflection barrier is 0 . In addition , we assume µi0 = 0 , and i ≤ σ2 , for some constant σ . σ2 In this environment our learning algorithm estimates the reflected Brownian motion using a mean zero martingale . We define lit as the last time up to time t that the item is allocated to agent i . This includes both exploration and exploitation actions . If i has not been allocated any item yet , lit is zero .
η ( t ) = min{1 , nt− ln2+2 t}
µit(T ) =
rilit rili,t−1 rili,T t < T t = T t > T
Call this mechanism M ( B ) . It is not difficult to verify that the results in this section hold as long 6 ) . However , for simplicity of as the expected value of the error of these estimates at time t is o(t exposition , we assume that the advertiser reports the exact value of µit .
We begin analyzing the mechanism by stating some of the well known properties of reflected
1
Brownian motions ( see [ 9] ) . Proposition 8 Let [ Wt , t ≥ 0 ] be a reflected Brownian motion with mean zero and variance σ2 ; the reflection barrier is 0 . Assume the value of Wt at time t is equal to y :
For T > 0 , let z = Wt+T . For the probability density function of z − y we have :
Corollary 9 The expected value of the maximum of µiT , 1 ≤ i ≤ n , is θ(
√
T ) .
Note that in the corollary above n and σ are constant . Now , similar to Lemma 6 , we bound
E[∆T ] . The proof is given in appendix A2 Lemma 10 Suppose under M ( B ) all agents are truthful until time T , then , E[∆T ] = O(T
2 ) .
Now we are ready to prove the main theorem of this section :
3 , M ( B ) is asymptotically Theorem 11 M ( B ) is ex post individually rational . Also , for 0 ≤ ≤ 1 incentive compatible , ex ante efficient , and has a revenue asymptotically equivalent to the revenue of the efficient second price auction .
15
√ tσ2 )
E[y ] = θ(
Pr[(z − y ) ∈ dx ] ≤
Pr[|z − y| ≥ x ] ≤
E[|z − y|I(|z − y| ≥ x ) ] ≤
2 8T σ2 8T σ2
πT σ2 e 1 x
π
−x2 2T σ2
−x2 2T σ2 e
−x2 2T σ2 e
π
( 8 )
( 9 )
( 10 )
( 11 )
Proof : We first prove ex post individual rationality . Note that the agents only pay for the items they receive during exploitation . Assume agent i is the person who has received the item at time T and it was during exploitation . ie , yiT = 1 . By the payment rule of the mechanism we have :
Because yiT = 1 we haveγit(t ) ≤µit(t ) . Therefore we get : t=1 t=1
T
T pit =
T−1 pit ≤ T−1 yit min{γt(t),γit(T )} ≤ T−1 yitrili,t−1 ≤ T yitµit(t ) =
T−1 t=1 t=1 t=1 t=1 t=1 yitγit(t ) xitrit .
We complete the proof by showing that conditions ( C1 ) , ( C2 ) , and ( C3 ) hold . By ( 8 ) , the expected utility of each agent at time t from random exploration is
η ( t)µit = θ(t− ln1+ t tσ2 ) = θ(t
1
2− ln1+ t )
√
Therefore , the expected utility up to time T from exploration is θ(T
3
2− ln1+ T ) .
Also , by Lemma 10 and Corollary 9 :
{µiT} +
E[max t≤T
T−1 t=1
∆t ] = O(T 1+ 2 )
( 12 )
( 13 )
3 , we have 3 tion ( C1 ) is met .
2 − ≥ 1 +
For ≤ 1 √ By Corollary 9 , the expected value of maxi{µiT} and γT are of θ(
2 . Therefore , by equations ( 12 ) and ( 13 ) , for ≤ 1
T ) . Therefore up to time T , both the expected welfare and the expected revenue of the efficient second price mechanism are of θ(T
2 ) . For any 0 < < 1 , we have :
3 , condi
3
Therefore , conditions ( C2 ) and ( C3 ) are satisfied , and M ( B ) is asymptotically ex ante efficient fi and it has a revenue asymptotically equivalent to the efficient second price auction .
θ(T
3
2 ) = ω(T 1+ 2 )
6 Discussion and Open Problems
In this section we discuss a few extensions of our mechanism .
Multiple Slots : We can modify mechanism M when there are multiple slots for showing an ad , under the common assumption of separability of the click through rates . Namely , we assume that there exists a set of conditional distributions which determine the probability that the ad in slot j1 is clicked on conditioned on the event that there was a click on the ad in slot j2 . During exploitation , M allocates the slots to the advertisers with the highest expected utility , and the prices are determined according to Holmstrom ’s lemma ( [21 ] , see also [ 2] ) . The estimates of the utilities are updated based on the reports , using the conditional distribution .
16
Creating Multiple Identities : During exploration , our mechanism gives the item for free to one of the agents chosen uniformly at random . Therefore , it is easy to see that an agent can benefit from participating in the mechanism with multiple identities . This may not be cheap or easy for all advertisers . After all , the traffic should be eventually routed to a legitimate business . Still , a possible solution is increasing the cost of creating new identities by charging advertisers a fixed premium for entering the system , or for the initial impressions . A similar problem emerge even if an advertiser is allowed to define multiple actions . Agarwal et al . [ 1 ] showed that multiple actions provide incentive for the advertiser to skew the bidding . Finding a more natural solution for these problems remains as a challenging open problem .
Characterizing Dynamic Mechanisms : In this paper , we presented a simple dynamic mechanism that satisfy desirable economic properties under a limited set of assumptions . An interesting line of research that is gaining a lot of attention is to characterize such mechanisms . Pavan et al . [ 25 ] study incentive compatible mechanisms in a Bayesian setting . On the flip side , Babaioff et al . [ 5 ] , and Devanur and Kakade [ 13 ] study the problem in a prior free setting , but for a rather strong notion of incentive compatibility . Characterizing incentive compatible dynamic mechanisms in prior free setting remains as an interesting open problem .
References
[ 1 ] N . Agarwal , S . Athey , and D . Yang Skewed Bidding in Pay Per Action Auctions for Online
Advertising . American Economic Review Papers and Proceedings , May 2009 .
[ 2 ] G . Aggarwal , A . Goel , and R . Motwani . Truthful auctions for pricing search keywords . Pro ceedings of ACM conference on Electronic Commerce , 2006 .
[ 3 ] S . Athey , and I . Segal . An Efficient Dynamic Mechanism . manuscript , 2007 .
[ 4 ] P . Auer , N . Cesa Bianchi , and P . Fischer . Finite time Analysis of the Multiarmed Bandit
Problem . Machine Learning archive , Volume 47 , Issue 2 3 , 235 256 , 2002 .
[ 5 ] M . Babaioff , Y . Sharma , and A . Slivkins . Characterizing Truthful Multi Armed Bandit Mech anisms . Proceeding of the 10th ACM Conference on Electronic Commerce , 2009 .
[ 6 ] A . Bapna , and T . Weber . Efficient Dynamic Allocation with Uncertain Valuations . Journal of Mathematical Economics , Vol . 44 , No . 3 4 , pp . 394403 , 2008 .
[ 7 ] M . Balcan , A . Blum , J . Hartline , and Y . Mansour . Mechanism Design via Machine Learning .
Proceedings of 46th Annual IEEE Symposium on Foundations of Computer Science , 2005 .
[ 8 ] D . Bergemann , and J . V¨alim¨aki . Efficient Dynamic Auctions . Proceedings of Third Workshop on Sponsored Search Auctions , 2007 .
[ 9 ] A . Borodin , and P . Salminen . Handbook of Brownian Motion : Facts and Formulae . Springer ,
2002 .
[ 10 ] A . Blum , V . Kumar , A . Rudra , and F . Wu . Online Learning in Online Auctions . Proceedings of the fourteenth annual ACM SIAM symposium on Discrete Algorithms , 2003 .
17
[ 11 ] R . Cavallo , D . Parkes , and S . Singh , Efficient Online Mechanism for Persistent , Periodically
Inaccessible Self Interested Agents . Working Paper , 2007 .
[ 12 ] K . Crawford . Google CFO : Fraud A Big Threat . CNN/Money , December 2 , 2004 .
[ 13 ] N . Devanur and S . Kakade . The Price of Truthfulness for Pay Per Click Auctions . Proceeding of the 10th ACM Conference on Electronic Commerce , 2009 .
[ 14 ] J . Gittins . Multi Armed Bandit Allocation Indices . Wiley , New York , NY , 1989 .
[ 15 ] R . Gonen , and E . Pavlov . An Incentive Compatible Multi Armed Bandit Mechanism . Proceedings of the Twenty Sixth Annual ACM Symposium on Principles of Distributed Computing , 2007 .
[ 16 ] B . Grow , B . Elgin , and M . Herbst . Click Fraud : The dark side of online advertising . Busi nessWeek . Cover Story , October 2 , 2006 .
[ 17 ] N . Immorlica , K . Jain , M . Mahdian , and K . Talwar . Click Fraud Resistant Methods for Learning Click Through Rates . Proceedings of the 1st Workshop on Internet and Network Economics , 2005 .
[ 18 ] R . Kleinberg . Online Decision Problems With Large Strategy Sets . PhD Thesis , MIT , 2005 .
[ 19 ] S . Lahaie , and D . Parkes . Applying Learning Algorithms to Preference Elicitation . Proceedings of the 5th ACM conference on Electronic Commerce , 2004 .
[ 20 ] M . Mahdian and K . Tomak . Pay per action model for online advertising . Proceedings of the
3rd International Workshop on Internet and Network Economics , 549 557 , 2007 .
[ 21 ] P . Milgrom , Putting Auction Theory to Work . Cambridge University Press , 2004 .
[ 22 ] D . Mitchell . Click Fraud and Halli bloggers . New York Times , July 16 , 2005 .
[ 23 ] N . Nisan , T . Roughgarden , E . Tardos , and V . Vazirani , editors . Algorithmic Game Theory ,
Cambridge University Press , 2007 .
[ 24 ] D . Parkes . Online Mechanisms Algorithmic Game Theory ( Nisan et al . eds. ) , 2007 .
[ 25 ] A . Pavan , I . Segal , and J . Toikka . Dynamic Mechanism Design : Revenue Equivalence , Profit
Maximization , and Information Disclosure , Working Paper , 2008 .
[ 26 ] B . Stone . When Mice Attack : Internet Scammers Steal Money with “ Click Fraud ” . Newsweek ,
January 24 , 2005 .
[ 27 ] R . Wilson . Game Theoretic Approaches to Trading Processes . Economic Theory : Fifth World Congress , ed . by T . Bewley , chap . 2 , pp . 33 77 , Cambridge University Press , Cambridge , 1987 .
[ 28 ] J . Wortman , Y . Vorobeychik , L . Li , and J . Langford . Maintaining Equilibria During Exploration in Sponsored Search Auctions . Proceedings of the 3rd International Workshop on Internet and Network Economics , 2007 .
18
A Skipped Proofs from Section 5
A.1 Proof of Lemma 6
Proof : We prove the lemma by showing that for any agent i ,
Pr[|µi −µit(t)| ≥ 1√
µi ] = o(
1 tc ),∀c > 0 . t1−
First , we estimate E[nit ] . There exists a constant d such that :
E[nit ] ≥ t−1 k=1 t−1 k=1
η ( k ) n
= min{ 1 n
, k− ln1+ k} > t1− ln1+ t
1 d
By the Chernoff Hoeffding bound :
Pr[nit ≤ E[nit ]
2
] ≤ e
−t1− ln1+ t
8d
.
Inequality ( 1 ) and the Chernoff Hoeffding bound imply :
Pr[|µi −µit(t)| ≥ 1√
µi ] t1−
= Pr[|µi −µit(t)| ≥ 1√ + Pr[|µi −µit(t)| ≥ 1√ t1− t1−
2
µi ∧ nit ≥ E[nit ] E[nit ] µi ∧ nit < −t1− ln1+ t
2
]
]
≤ 2e = o( Therefore , with probability 1 − o( 1 1 , E[∆t ] = O(
1√ t1− ) .
2d
+ e t1− t1− ln1+ t µi − 1 1 tc ),∀c > 0 t ) , for all agents , ∆t ≤ 1√
8d t1− . Since the maximum value of uit is fi
A.2 Proof of Lemma 10 Proof : Define Xit = |µi,T − µi,T−t| . We first prove Pr[Xit > T T c ),∀c > 0 . There exists a constant Td such that for any time T ≥ Td , the probability that i has not been randomly allocated the item in the last t < Td step is at most :
2 ] = o( 1
Pr[T − li,T−1 > t ] < ( 1 − T − ln2+2 T )t ≤ e
−t ln2+2 T
T
.
( 14 )
Let t = 1 ln1+ T
T . By equation ( 10 ) and ( 14 ) ,
Pr[Xit > T
2 ] = Pr[Xit > T
2 ∧ T − li,T−1 ≤ t ]
2 ∧ T − li,T−1 > t ]
+ Pr[Xit > T 1 T c ),∀c > 0 .
= o(
19
Hence , with high probability , for all the n agents , Xit ≤ T √ 2 , then , by Corollary 9 , the expected value of the maximum of µit over these agents is θ( T Therefore , E[maxi{Xit} ] = O(T
2 . If for some of the agents Xit ≥ T ) . fi
2 ) . The lemma follows because E[∆T ] ≤ E[maxi{Xit} ] .
20
