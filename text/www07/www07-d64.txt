Random Web Crawls
Toufik Bennouas
LIRMM , Universit´e Montpellier 2 , France bennouas@lirmm.fr
Fabien de Montgolfier
LIAFA , Univerist´e Paris 7 , France fm@liafajussieufr
Abstract— This paper proposes a random Web crawl model . A Web crawl is a ( biased and partial ) image of the Web . This paper deals with the hyperlink structure , ie a Web crawl is a graph , whose vertices are the pages and whose edges are the hypertextual links . Of course a Web crawl has a very particular structure ; that is investigated . We then propose a model generating similar structures , explaining how and why they occur .
I . INTRODUCTION
The Web is a fascinating object that is studied very extensively since a few years . Among the many research problems it opens , are the topological issues , ie describing the shape of the Web [ 10 ] , [ 12 ] . Understanding the hyperlink structure has allowed the design of the most powerful search engines like Google , famous because it uses the PageRank algorithm from Brin and Page [ 19 ] , or HITS from Kleinberg [ 13 ] .
We know , however , only parts of the Web . The crawlers are software that automatically browse the Web and cache the “ most relevant ” informations , especially the URLs of existing documents and the hyperlinks between them . This is recursively performed , the analysis of crawled pages allowing to get new valid URLs . But bandwidth limitations , HTML errors , unreferenced URLs , removed pages and the existence of dynamic pages ( generated from requests in URL or from a session mechanism ) make very hard , if not impossible , to output “ the ” Web : crawlers instead produce partial and biased images .
Many models describe random graphs and some of them ( see Section III ) are designed to model Web Graphs , ie the hyperlink structure of the Web itself . However , the graphs they produce can not be compared with the reality ( that is beyond observation as we said ) but only with crawls outputs . The bias has then to be discussed by the authors .
Our approach here is quite different . We try to model the structure not of the Web itself , but of Web crawls , because it can positively be compared to crawls and no bias analysis has to be done . Furthermore , it can explain why some biases and artifacts occur . For example to explain why the “ Bow Tie ” structure appears [ 7 ] or why the breadth fırst search crawling downloads the most popular pages fırst [ 18 ] . In this paper we fırst present some known properties of Web crawls , then our model , and discuss how it mimics genuine crawls .
II . WEB CRAWL PROPERTIES
Web crawls can be quite large objects ( for instance Google currently claims more than 8 billion pages in database ) but are very sparse graphs , since the average degree is around 7 links per page [ 7 ] . The number of pages is denoted n and number of links is m . Here crawls are directed graphs . They are not necessarily strongly connected , since a connecting page may be removed from the Web and then deleted from the crawl . They have very few sources ( pages with no incoming links , either submitted by peoples or unlinked after a while ) and a lot of sinks ( pages not crawled yet or with no hyperlink ) . A . Connectivity and Clustering
Small World graphs , defıned by Watts and Strogatz [ 22 ] and studied by many authors since , are graphs that fulfıll the following two properties :
1 ) the characteristic path length ( average distance between two vertices ) is small : O(log n ) or O(log log n )
2 ) the clustering coeffıcient ( probability that two vertices sharing a common neighbor are linked ) is high : O(1 ) . The Web ( in fact the crawls ) characteristic path length seems small ( about 16 clicks [ 7 ] or 19 [ 3] ) , logarithmic in n . Its clustering coeffıcient is high , but authors differ on the exact fıgure . It seems it is above 0:1 while random graphs ( Erd¨osR´enyi , see Section III A ) with the same average degree have almost null clustering .
Crawls diameter ( maximum distance between two pages ) is potentially infınite because a dynamic page labeled by n in URL may refer to a dynamic page labeled by n + 1 , but since Web crawlers usually perform BFS ( see Section IV A ) the diameter of crawls may be actually small . B . Degree distribution
Zipf laws ( aka power laws ) are probability laws such that log(P rob(X = d ) ) = ff , log(d )
If the in degree ( respectively out degree ) distribution of a graph follows a Zipf law , P rob(X = d ) is the probability for a vertex to have in ( resp . out ) degree d . In other words , the number of vertices with degree d is k:d , ( k depends from the number of vertices n ) . A graph class such that the degree of almost all graphs follow a Zipf law is called scale free because some parameters like are scale invariant . Scale free graphs have been extensively studied recently [ 4 ] , [ 5 ] , [ 15 ] , [ 16 ] . Many graphs modeling social networks , interaction between objects ( proteins , peoples , neurons ) or other network properties seem to have the scale free property .
For Web crawls , a measure from Broder & al . [ 7 ] on a 200 000 000 pages crawl show that the in and out degrees follow Zipf law . The exponents are in = 2:1 for in degree and out = 2:72 for out degree .
C . Strongly connected components and the Bow Tie structure According to Broder , Kumar et al [ 7 ] the Web has a Bow Tie structure : a quarter of the page are in a Giant Strongly Connected Component ( GSCC ) , a quarter are the “ in ” page , leading to the GSCC but not linked from there , another quarter are the “ out ” pages reachable from the GSCC but not linking to it , and the last quarter is not related to the GSCC . This famous assertion was reported even by Nature [ 21 ] but , since four years , an increasing number of people suspects it is a crawling artifact . According to the same survey , the distribution of the size of strongly connected components follows a Zipf law with exponent roughly 25
D . Cores
Another well known property of crawls is the existence of cores . A core is a dense directed bipartite subgraph , consisting in many hub pages ( or fans ) pointing many authorities . It is supposed [ 17 ] , [ 14 ] that such cores are the central structure of cybercommunities , set of pages about the same topics . The authorities are the most relevant pages , but they do not necessarily point one each other ( because competition , for instance ) but the hubs list most of them . Starting from this assumption , HITS algorithm [ 13 ] ranks the pages containing a given keyword according to an hub factor and an authority factor . Recent works [ 17 ] , [ 16 ] enumerate over 200,000 bipartite cores from a 200,000,000 pages crawl of the Web . Cores sizes ( counting hubs , authorities , or both ) follow Zipf laws of exponent between 1:09 and 1:4 .
E . Spectral properties and PageRank factor
Another ranking method , the most popular since it does not depends from given keywords , is Google ’s PageRank factor [ 19 ] . It is an accessibility measure of the page . Briefly , the PageRank of a page is the probability for a random surfer to be present on this page after a very long surf . It can be computed by basic linear algebra algorithms . PageRank distribution also follows a Zipf law with the same exponent as the in degree distribution [ 20 ] . Pages with high PageRank are very visible , since they are effectively popular on the Web and are linked from other pages with high PageRank . A crawler therefore easily fınds them , while it may miss low ranked pages . This is indeed an useful bias for search engine crawlers !
III . RANDOM GRAPHS MODELS
A . Basic models : Erd¨os R´enyi
For a long time the most used random graph model was Erd¨os Renyi model [ 11 ] . The random graph depends on two parameters , the number of vertices n and the probability p for two vertices to be linked . The existence of each edge is a random variable independent from others . For suitable values ( p = d=n ) , E R graphs indeed have characteristic path length of O(log n ) but very small clustering ( p = o(1 ) ) and degree distribution following a Poisson law and not a Zipf law . Therefore they do not accurately describe the crawls . Other models have then be proposed where attachment is not independent .
B . Incremental generation models
Most random Web graph models [ 4 ] , [ 5 ] , [ 15 ] , [ 16 ] , [ 6 ] propose an incremental construction of the graph . When the existence of a link is probed , it depends from the existing links . That process models the creation of the Web across time . In some models all the link going from a page are inserted at once , and in others it is incremental .
C . Preferential Attachment models
The fırst evolving graph model ( BA ) was given by Barabasi and Albert [ 4 ] . The main idea is that new nodes are more likely to join to existing nodes with high degrees . This model is now referred to as an example of a preferential attachment model . [ 4 ] concluded that the model generates graphs whose in degree distribution follows a Zipf law with exponent = 3 .
Another preferential attachment model , called the Linearized Chord Diagram ( LCD ) , was given in [ 5 ] . In this model a new vertex is introduced in the graph at each step , and connects to existing vertices with a constant number of edges . A vertex is selected as the end point of the an edge with probability proportional to its in degree , with an appropriate normalization factor . In degrees follow a Zipf law with exponent roughly 2 when out degrees are 7 ( constant ) .
In the ACL [ 2 ] model , each vertex is associated a in weight ( respectively out weight ) dependent of in degree ( respectively out degree ) . A vertex is selected as the end point of the an edge with probability proportional to its weight .
In these models edges are added but never deleted . The CLdel model [ 8 ] and CFV model [ 9 ] incorporate in their design both the addition and deletion of nodes and edges .
D . Copy models
A model was proposed by [ 15 ] to explain other relevant properties of the Web , especially the great number of cores , since in [ 15 ] was demonstrated that ACL model generates graphs which on average contain few cores .
The linear growth coping model from Kumar& al . [ 16 ] postulates that a Web page author shall copy an existing page when writing its own , including the hyperlinks . In this model ( KRRT ) , each new page has a master page from which it copies a given amount of links . The master page is chosen proportionally to their in degree . Other links from the new are then added following uniform or preferential attachment . The result is a graph with all properties of previous models , plus the existence of many cores .
All these models are complicated and use many parameters . It is necessary to fınd a simple and practical model which approaches as well as possible with the Web . It is the aim of this work .
IV . A WEB CRAWL MODEL
It this section we present our Web crawl model . It aims to mimic the crawling process itself , rather than the page writing process as web graph models do .
A . Web crawls strategies
We suppose the crawler visits pages only once . The benefıt is to avoid modeling the disappearance of pages or links across time , because the law it follows is still debatable ( is the pages lifetime related to their popularity or degree properties ? ) When scanning a page , the crawler gets at once the set of its outcoming links .
A crawl is a graph traversal . At any time the ( potentially infınite ) set of valid URL is divided into
1 ) Crawled or Discovered : the corresponding pages exist and its outcoming links are known
2 ) Unvisited : a link to this URL has been found but not probed
3 ) Erroneous : the URL was probed but points a nonexisting or non HTML fıle ( some search engine index them , but they do not contain URL and are not interesting for our purposes )
4 ) Unknown : the URL was never encountered The crawling algorithm basically chose and remove from its Unvisited set an URL to crawl , and then add the outcoming unprobed links of the page , if any , to the Unvisited set . The crawling strategy is the way the Unvisited set is managed . It may be : ffl DFS ( depth fırst search ) The strategy is FIFO and the data structure is a stack . ffl BFS ( breadth fırst search ) The strategy is LIFO and the data structure is a queue . ffl DEG ( higher degree ) The most pointed URL is chosen .
The data structure is a dynamic heap . ffl RND ( random ) An uniform random URL is chosen . In the model we do not manage the ( uninteresting ) erroneous or unknown URLs . The crawled pages are ordered by their discovery date .
B . Model description
We postulate the out degree and in degree of a page follows Zipf laws of given parameters . It is easy to pre compute the out degree of a page since all its links are scanned at once . But how to pre compute the in degree of a page , since some incoming links may be discovered after the page itself ? We choose to solve the problem using a Discovery list .
All potential pages ( they may be not eventually discovered if the crawl halts before ) receive an ID p , an in degree din(p ) and an out degree dout(p ) ( in degree seems independent from out degree in real crawls ) . Then for each page , its ID p is put in the discovery list din(p ) times , and the discovery list is then shuffled at random . Notice that any subsequence of the discovery list follows a Zipf law ( it is a uniformly randomly chosen subset ) .
The generating algorithm is then simply : 1 ) Remove a page p from the unvisited set according to the strategy ( BFS,DFS , DEG or RAND )
2 ) Remove dout(p ) IDs from the discovery set and add the unvisited to the unvisited set
3 ) Go to 1
The unvisited set is seeded with one or more IDs . Because the average out degree of a page is big enough , the crawling process won’t stop unless almost all IDs have been discovered ( but the crawl should be halted before , as we do not need to exhaust the list ) .
Our model differs radically from preferential attachment or copy model because of the existence of this Discovery list . It is a pre computation of the extremities of the links , and the crawling process just assigns to each link an origin .
V . RESULTS
We present here simulation results using the different strategies and showing how the measurements evolve across time . Because the scale free effect , the actual number of pages does not matter , since it is big enough . We have used several graphs of different sizes but with the same exponents in = 2:1 and out = 2:72 ( experimental values from [ 7] ) . And unless otherwise specifıed , we present results from BFS , the most used crawling strategy , and simulations up to 20,000,000 crawled pages .
) g o l ( l d e w a r c s e c i t r e v f o r e b m u N
) g o l ( l d e w a r c s e c i t r e v f o r e b m u N
20
18
16
14
12
10
8
6
4
2
0
18
16
14
12
10
8
6
4
2
0
16/x**2.6 BFS , 5 % 1805/x**262 BFS , 25 % 1926/x**2615 BFS , 75 %
0
2
4
6
8
10
12
Out degree ( log )
1377/x**26 BFS , 1 % 154/x**213 BFS , 25 % 1648/x**21 BFS , 75 %
0
2
4
6 8 In degree ( log )
10
12
14
Fig 1Out degreedistribution(top)andin degreedistribution
( bottom)atthreestepsofaBFS
At any step of the crawl , the actual degree distribution follows a Zipf law of the given parameters ( 2.1 and 2.72 ) with very small deviation ( see Fig 1 ) . This result is independent from the crawl strategy ( BFS , etc . ) It demonstrates that our generated crawls really are scale free graphs .
2.5e+06
2e+06
1.5e+06
1e+06
500000
0
0
BFS DEG RAND
5
10
15
20
25
Path length t i n e c i f f e o c g n i r e t s u C l
0.4
0.35
0.3
0.25
0.2
0.15
0750029 00454094*log(x ) 0546027 00293349*log(x ) BFS DEG RAND
0
50000 100000 150000 200000 250000 300000 350000 400000 450000 500000
Number of vertices crawled n o i t l a u p o P
) g o l ( n o i t l a u p o P
11
10
9
8
7
6
5
4
3
2
0
1
2
3
4
5
6
7
8
9
10
Path length ( log )
Fig 4Evolutionofclusteringcoeffıcient acrosstime
The clustering ( Fig 4 , computed on 500,000 pages simulation ) is high and do not decrease too much as the crawl goes bigger . Our crawls defınitely are small world graphs .
BFS RAND DEG DFS
0.7
0.65
0.6
0.55
0.5
0.45
0.4
0
100000
200000 300000 Number of vertices crawled
400000
500000
BFS RAND DEG DFS
0.65
0.6
0.55
0.5
0.45
0.4
0.35
) n o i t r o p o r p ( i e z s
C C S
) n o i t r o p o r p ( i e z s
T U O
0.3
0
100000
200000 300000 Number of vertices crawled
400000
500000
Fig 5EvolutionofthesizeofthelargestSCC(top)andofthe
OUTpages(bottom)acrosstime(Oneseed,upto500,000 pagesinthecrawl )
The relative size of the four bow tie compoents ( SCC , IN , OUT and OTHER ) are roughly the same for BFS , DEG and even RAND ( but not DFS ) strategies ( Fig 5 ) . When using only one seed , the size of the largest SCC converges toward two thirds of the size of the graph . These proportions thus differ
Fig 2DistributionofpathlengthforBFSandDEGandRAND
( top)andDFS(bottom )
The distribution of path lengths ( Fig 2 , top ) clearly follows a Gaussian law for BFS , DEG and RAND strategies . This distribution is plotted at 30 % of the crawl but it does not change a lot accros time , as shown in Fig 3 . DFS produces far more greater distances between vertices , and the distribution follows an unknown law ( Fig 2 , bottom ) . DFS crawls diameter is about 10 % of the number of vertices! This is because DFS crawls are like long tight trees . It is why DFS is not used by real crawlers , and we will investigate it no more .
40
35
30
25
Diameter
BFS , apl DEG , apl RAND , apl BFS , d DEG , d RAND , d log(N)/log(average degree ) i t r e e m a D h g n e
/ t l h t a p e g a r e v A
20
15
10
5
0
Average path length
50000
100000 150000 200000 250000 300000 350000 400000 450000 500000
Number of vertices crawled
Fig 3Evolutionofdiameterandaveragepathlengthacross timeforBFS,DEGandRAND from [ 7 ] crawl observations since the “ in ” and “ others ” parts are smaller . But with many seeds ( it may be seen as many pages submitted to the crawler portal ) the size of the “ in ” component is larger and can be up to one quarter of the pages . Our model replicates indeed very well genuine crawls bow tie topology . that our simulations also produces ,
Fig 9 focuses on a well known topological property of the very high crawls , number of sinks regardless of crawl size . Notice that their existence is a problem for practical PageRank computation [ 19 ] . In other words , the large “ out ” component of the bowtie is very broad and short
2275*x**( 21 ) BFS
18
16
14
12
10
8
6
4
PageRank values(log )
Fig 6PageRankdistribution
168*10**( 8)*x+0062*log(x) 039 BFS DFS RAND DEG
16
14
12
10
8
6
4
2
0
20
1.2
1
0.8
0.6
0.4
0.2
) g o l ( l d e w a r c s e c i t r e v f o r e b m u N k n a R e g a P l a t o T
0
0
2e+06
4e+06
6e+06
8e+06 Number of vertices crawled
1e+07 1.2e+07 1.4e+07 1.6e+07 1.8e+07 2e+07
Fig 7PageRankcapture .
Fig 6 shows the PageRank distribution ( PageRank is normalized to 1 and logarithms are therefore negative ) . We have found result similar to [ 20 ] observations : the distribution is a Zipf law with exponent 21 In Fig 7 shows the sum of the PageRank of the crawled pages across time ( the PageRank computed at the end of the crawl , so that it must vary from 0 at beginning to 1 when crawl stops ) . In a very few steps , BFS and DEG strategies fınd the very small amount of pages that contains most of the total PageRank . At any time the PageRank distribution follows a Zipf law of exponent 21
Fig 8 shows another dynamical property : the discovery rate . It is the probability for the extremity of a link of being already crawled . It converges toward 40 % for all strategies . This is an interesting scale free property : after a while , the probability for a URL to point a new page is very high , about 60 % . This “ expander ” property is very usefull for true crawlers . This simulation shows it does not depends only from the dynamical nature of the web , but also from the crawling process itself .
)
%
( x e t r e v d o l n a i g n e e s f o y t i l i b a b o r p s e c i t r e v s k n s f i o n o i t c a r F
55
50
45
40
35
30
25
20
15
10
0
0.5
0.45
0.4
0.35
0.3
0.25
0.2
44*log(x) 3146 BFS DFS 559*log(x) 5181 RAND 44*log(x) 3146 DEG
5e+06
1e+07
1.5e+07
2e+07
Number of vertices crawled
Fig 8Evolutionofthediscoveryrate .
0644592+ 00235404*log(x ) RAND DFS 0791703 00323677*log(x ) BFS DEG
0
2e+06
4e+06
6e+06
8e+06 Number of vertices crawled
1e+07 1.2e+07 1.4e+07 1.6e+07 1.8e+07 2e+07
Fig 9Evolutionoftheproportionofsinks(pageswithno crawledsuccessors)amongcrawledpages
Cores creation
We used Agrawal practical algorithm [ 1 ] for cores enumeration ( notice that the maximal core problem is NP complete ) . Figure 10 gives the number of core of a given minimal size for a crawl up to 25 000 vertices . As shown , the number of cores is very dependent from exponents of Zipf laws , since high exponents means sparser graphs .
Fig 11 shows that the number of ( 4 ; 4) cores ( at least four hubs and four authorities ) in proportional with n and after a while stays between n=100 and n=50 .
Crawl of 1000 vertices
Hubs Auth .
# cores
2 2 2 2 2 2 3 3 3
2 3 4 5 6 7 3 4 5
220 83 37 14 14 14 84 37 14
Crawl of 1000 vertices Hubs Auth . # cores
3 3 4 4 4 4 5 5 6
6 7 4 5 6 7 5 6 6
5 2 40 14 5 2 12 3 3
Fig 10Numberofsmallcoresseenona1000verticesgraph.It meansthatoursimulatedcrawlscontainmanycore,asreal
) 4
,
4 ( s e r o c f o n o i t c a r F
0.035
0.03
0.025
0.02
0.015
0.01 crawlsdo .
BFS
0
1000
2000
3000
4000 6000 Number of vertices crawled
5000
7000
8000
9000
10000
Fig 11Cores(4,4)distribution
VI . CONCLUSION
As said in Section II , a good crawl model should output graphs with the following properties :
1 ) highly clustered 2 ) with a short characteristic path length 3 ) in and out degree distributions follow Zipf laws 4 ) with many sinks 5 ) such that high PageRank vertices ( computed in the fınal graph ) are discovered early
6 ) with a bow tie structure
As shown in Section V , our model meets all these objectives . Property 3 of course is ensured by the model , but the other ones are results of the generating process . The basic assumption of degree distribution , together with the crawling strategy , is enough to mimic the properties observed in large real crawls . This is conceptually simpler than other model that also have the same properties like the Copy model [ 15 ] .
The Bow Tie structure we observe differs from [ 7 ] since the largest strongly connected component is larger . But together with the other topological properties measured , it prove that we reproduce quite well the topology of real crawls with our very simple model . It is nice , because we have far more assumption than [ 5 ] or [ 16 ] . Our approach is different from the Web graph models , that mimic the page writing strategy instead of the page crawling , but give similar result . It points out that we need more numerical or other measures of graph in order to analyze their structure .
BFS , RAND and DEG strategies are the most used in crawlers . We show that they produce very similar results for topological aspects . For dynamical aspects ( PageRank capture for instance ) BFS and DEG seems better , but are harder to implement in a real crawler . DFS is defınitely bad , and for this reason is not used by crawlers .
REFERENCES
[ 1 ] Rakesh Agrawal and Ramakrishnan Srikant . Fast algorithms for mining association rules in large databases . In VLDB ’94 : Proceedings of the 20th International Conference on Very Large Data Bases , pages 487– 499 . Morgan Kaufmann Publishers Inc . , 1994 .
[ 2 ] William Aiello , Fan Chung , and Linyuan Lu . A random graph model for massive graphs . In Proceedings of the thirty second annual ACM symposium on Theory of computing , pages 171–180 . ACM Press , 2000 .
[ 3 ] R´eka Albert , Hawoong Jeong , and Albert L´aszl´o Barabasi . Diameter of the world wide web . Nature , 401:130–131 , September 1999 .
[ 4 ] Albert L´aszl´o Barabasi and R´eka Albert . Emergence of scaling in random networks . Science , 286:509–512 , 1999 .
[ 5 ] B´ela Bollob´as , Oliver Riordan , Joel Spencer , and G´abor E . Tusn´ady .
The degree sequence of a scale free random graph process . Random Structures and Algorithms , 18(3):279–290 , May 2001 .
[ 6 ] Anthony Bonato . A survey of models of the web graph .
In The Proceedings of Combinatorial and Algorithmic Aspects of Networking , August 2004 .
[ 7 ] Andrei Broder , Ravi Kumar , Farzin Maghoul , Prabhakar Raghavan , Sridhar Rajagopalan , Raymie Stata , Andrew Tomkins , Janet Wiener , Allan Borodin , Gareth O . Roberts , Jeffrey S . Rosenthal , and Panayiotis Tsaparas . Graph structure of the web . In Proceedings of the ninth international conference on World Wide Web , pages 309–320 . Foretec Seminars , Inc . , 2000 .
[ 8 ] Fan Chung and Linyan Lu . Coupling on line and off line analyses for random power graphs . Internet Mathematics , 1(4):409–461 , 2003 .
[ 9 ] Alan Frieze Colin Cooper and Juan Vera . Random deletion in a scale free random graph process . Internet Mathematics , 1(4):463–483 , 2003 . [ 10 ] Kemal Efe , Vijay Raghavan , C . Henry Chu , Adrienne L . Broadwater , Levent Bolelli , and Seyda Ertekin . The shape of the Web and its implications for searching the Web , 31 –6 2000 .
[ 11 ] P . Erd¨os and A . R´enyi . On random graphs . Mathematicae , volume 6 , pages 290–297 , 1959 .
In Publicationes of the
[ 12 ] Jean Loup Guillaume and Matthieu Latapy . The web graph : an overview . In 4mes rencontres francophones sur les Aspects Algorithmiques des Telecommunications ( AlgoTel ) . INRIA , 2002 .
[ 13 ] Jon M . Kleinberg . Authoritative sources in a hyperlinked environment .
J . ACM , 46(5):604–632 , 1999 .
[ 14 ] Jon M . Kleinberg . Hubs , authorities , and communities . ACM Computing
Surveys ( CSUR ) , 31(4es):5 , 1999 .
[ 15 ] R . Kumar , P . Raghavan , S . Rajagopalan , D . Sivakumar , A . Tomkins , and E . Upfal . Stochastic models for the web graph . In Proceedings of the 41st Annual Symposium on Foundations of Computer Science , page 57 . IEEE Computer Society , 2000 .
[ 16 ] Ravi Kumar , Prabhakar Raghavan , Sridhar Rajagopalan , and Andrew Tomkins . Extracting large scale knowledge bases from the web . In Proceedings of the 25th International Conference on Very Large Data Bases , pages 639–650 . Morgan Kaufmann Publishers Inc . , 1999 .
[ 17 ] Ravi Kumar , Prabhakar Raghavan , Sridhar Rajagopalan , and Andrew Tomkins . Trawling the web for emerging cyber communities . In Proceeding of the eighth international conference on World Wide Web , pages 1481–1493 . Elsevier North Holland , Inc . , 1999 .
[ 18 ] Marc Najork and Janet L . Wiener . Breadth first crawling yields highquality pages . In Proceedings of the tenth international conference on World Wide Web , pages 114–118 . ACM Press , 2001 .
[ 19 ] L . Page , S . Brin , R . Motwani , and T . Winograd . The PageRank Citation Ranking : Bringing Order to the Web . Technical report , Computer Science Department , Stanford University , 1998 .
[ 20 ] Gopal Pandurangan , Prabhakar Raghavan , and Eli Upfal . Using pagerank to characterize web structure . In Proceedings of the 8th Annual International Conference on Computing and Combinatorics , pages 330– 339 . Springer Verlag , 2002 .
[ 21 ] Nature . 405:113 , 11 May 2000 . [ 22 ] D . J . Watts and S . H . Strogatz . Collective dynamics of small world networks . Nature , 393(1–7):440–442 , 1998 .
