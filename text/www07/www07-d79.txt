A Unified Platform for Data Driven Web Applications with
Automatic Client Server Partitioning
Fan Yang1 , Nitin Gupta1 , Nicholas Gerner1 , Xin Qi1 , Alan Demers1 , Johannes Gehrke1 ,
Jayavel Shanmugasundaram2 1 Cornell University Ithaca , NY
2 Yahoo! Santa Clara , CA
{yangf , niting , nsg7 , qixin , ademers , johannes}@cscornelledu , jaishan@yahoo inc.com
ABSTRACT Data driven web applications are usually structured in three tiers with different programming models at each tier . This division forces developers to manually partition application functionality across the tiers , resulting in complex logic , suboptimal partitioning , and expensive re partitioning of applications .
In this paper , we introduce a unified platform for automatic partitioning of data driven web applications . Our approach is based on Hilda [ 14 , 26 ] , a high level declarative programming language with a unified data and programming model for all the layers of the application . Based on run time properties of the application , Hilda ’s run time system automatically partitions the application between the tiers to improve response time while adhering to memory and/or processing constraints at the clients . We evaluate our methodology with traces from a real application and with TPC W , and our results show that automatic partitioning outperforms manual partitioning without the associated development overhead .
Categories and Subject Descriptors H4m [ Information Systems ] : Miscellaneous ; D1m [ Programming Techniques ] : Miscellaneous ; D211 [ Software Engineering ] : Software Architectures ; D28 [ Software Engineering ] : Metrics—performance measures
General Terms Design Languages Performance
Keywords Hilda , Client Server Partitioning , Declarative Language , Web 2.0 , Data Driven Application
1 .
INTRODUCTION
An important class of applications is data driven web applications , ie , web applications that run on top of a backend database system . Examples of such applications are b2c portals such as online shopping sites and online auctions ,
Copyright is held by the International World Wide Web Conference Committee ( IW3C2 ) . Distribution of these papers is limited to classroom use , and personal use by others . WWW 2007 , May 8–12 , 2007 , Banff , Alberta , Canada . ACM 978 1 59593 654 7/07/0005 .
Figure 1 : Tiers in a Data Driven Web Application and various b2b portals . Data driven web applications are usually structured in three tiers : a database system that stores persistent data as the lowest tier , an application server that contains most of the application logic as the middle tier , and the client web browser that contains some client specific application logic and presentation as the top tier ( see Figure 1 ) .
Current development platforms use different programming models at each tier . For example , server side application development frameworks such as J2EE and Enterprise Java Beans ( EJBs ) wrap relational data as Java objects . PHP and ASP.NET bridge the difference between the data model at the lowest tier and the middle tier in similar ways . The top tier usually uses a different programming model , such as AJAX or FLASH [ 1 ] , which allows the developer to build rich clients . The difference in programming model between the different tiers forces the developer to decide manually how to partition application functionality across the tiers , and to implement functionality at each tier separately using different programming languages and models .
Exposing the boundaries between tiers to the programmer in this way has four significant drawbacks .
Increased Development Time . Having different programming models in different tiers makes it hard to develop , maintain , and optimize applications , as the developer must manually bridge the differences between the individual models ( for example , the relational model , EJBs , and HTML forms ) .
Complex Logic due to Partitioning . Partitioning application logic across the tiers requires complex logic to synchronize the state of the application . For example , in order to enable partial updates ( a well known strategy in AJAX [ 2] ) , data can be cached at the client side . However , client side caching can allow content shown in the browser to become outdated due to concurrent updates to the application state from different users . Such application level conflicts are difficult to detect , and existing systems do not provide automatic support for conflict detection .
Suboptimal Partitioning . Since the decision of how to partition the application is left to the developer ( who may have little data on which to base her decisions ) , the resulting division of the application may be be suboptimal in terms of system performance .
Expensive Re Partitioning . Once a partitioning of the application has been implemented , moving functionality between layers is complex . For example , consider an application that allows a user to sort on a column of a table : this may initially be implemented in the application server as an SQL order by query issued over a relational database . If the developer later decides to move sorting to the client side to improve responsiveness , the sort functionality must be reimplemented in a different programming model such as JavaScript .
In this paper , we introduce a unified platform for automatic partitioning of data driven web applications . Our approach is based on Hilda , a high level declarative programming language with a unified data and programming model for all layers of the application [ 26 ] . In particular , we show how to automatically partition a Hilda application between the client and middle tier based on run time behavior of the application — all of this completely transparent to the developer . Our way of partitioning automatically synchronizes state between client and server without the developer having to write any additional code to achieve this . A web application developer thus can focus on the core application logic without worrying about the partitioning of the application or changes to the partitions . The Hilda system is available as open source software at http://wwwcscornelledu/database/hilda
In summary , this paper makes the following contributions . • We have developed a run time environment for Hilda that allows us to automatically partition a data driven web application dynamically between client and middle tier in a way that is completely transparent to the developer . ( Section 2 )
• We model client middle tier partitioning as an optimization problem . The resulting problem is NP hard with the client side space constraint , but we give an approximation algorithm that is provably within a factor of three of the optimal solution of the problem . ( Section 3 )
• We show how we can use trace data to instantiate the optimization model and how to derive practical decisions about client middle tier partitioning . In a thorough experimental evaluation using a technical benchmark and a real application , we show the efficacy of our techniques . ( Section 4 )
We discuss related work in Section 5 , and we conclude in Section 6 .
2 . CLIENT SERVER PARTITIONING
We first review Hilda , a high level language for data driven applications on which our model is based . We then come to the first contribution of the paper , the Hilda Runtime System that enables automatic client server partitioning .
2.1 Hilda
Hilda is a high level declarative language designed for developing data driven web applications [ 26 ] . It is based on UML [ 5 ] and the relational data model [ 19 ] . Instead of using different data models and languages for different layers of the application stack , Hilda presents a unified programming model for all layers ( see Figure 1 ) . Our discussion in this section summarizes the aspects of Hilda that are important for this paper ; a full discussion of Hilda can be found in our prior work , which is the source of some of the material in this subsection [ 26 ] .
A Hilda program consists of building blocks called AUnits ( for Application Units ) , analogous to UML classes . Each AUnit models a functional component of the application and encapsulates the operations and the data associated with ( one or more ) web pages , subpages , or a frame of a webpages . An AUnit is a single entry single exit programming construct that has an ( optional ) input schema and an ( optional ) output schema ( similar to arguments and return values of functions ) . The input and output schemas are both relational schemas [ 19 ] . The developer organizes AUnits into parent child relationships , resulting in a hierarchical structure that models the hierarchical structure of a website . The leaf level AUnits represent basic components such as HTML forms that allow user interaction . For example , the Navigation Bar AUnit represents a form containing options , one of which can be selected by a user . One of the AUnits in a Hilda program is designated as the root AUnit , which intuitively corresponds to the “ main ” function in a program .
A running Hilda program consists of instances of AUnits ; we call the construction of an instance of an AUnit activation , and the destruction of an instance of an AUnit deactivation . An AUnit instance A is activated from its parent AUnit instance ; the parent passes input data through the input schema ; when A gets deactivated it returns output data back to the parent through its output schema .
The activation of child AUnits is controlled through a part of the parent AUnit called its activators . Each activator specifies the following information : ( i ) the child AUnit of which it creates instances ; ( ii ) an activation condition , which defines when and how instances of the child AUnit should be activated ; ( iii ) an activation ID that uniquely identifies each child AUnit instance , ( iv ) input data for instances of the child AUnit ; and ( v ) optional operations triggered by the outputs of the child AUnit instances . An activator can activate multiple instances of the same child AUnit ; the ID serves as the primary key to distinguish between different instances of the same child AUnit . Thus within an AUnit , we can distinguish child AUnits by their IDs ; this also allows us to globally identify AUnit instances by through a concatenation of its ID with the IDs of its ancestor AUnit instances . In Hilda , all conditions , activators , etc . of an AUnit are written as declarative SQL queries . Rendering logic is defined for every AUnit to specify the visual appearance of the AUnit at the client . Example : A Course Management System . Figure 2
Figure 2 : A part of the AUnit hierarchy that models the Course Management System
Figure 3 : Activation tree and the corresponding webpage shows part of the AUnit hierarchy for a Course Management System ( CMS ) we developed at Cornell . The CMS AUnit represents the application , and contains AUnits for faculty , students and system administrators , which are modelled by child AUnits Faculty , Student and Sys Admin , respectively . A faculty member can view students , add and remove staff , edit assignments and perform other course related operations , each of which is implemented as a child of the Faculty AUnit . For example , the StaffList AUnit encapsulates the data corresponding to the list of staff members associated with the current course . This AUnit allows the faculty member to view and update , in a browser , the complete list of staff members . The ID of a Faculty AUnit instance is the faculty ’s NetID ; the ID of a FacultyCourse AUnit instance is the name of the course ; its KEY is a tuple consisting of the course ID and the faculty ’s NetID . The activation condition for Faculty is that the current user logs in as a course faculty member
To start a Hilda program , the system creates an instance of the root AUnit of the Hilda program . The system then recursively activates children of the root AUnit according to their activation conditions , and constructs a tree of AUnit instances called the activation tree . The system maintains this activation tree for each user session . At any time , the activation tree represents the part of the application currently available to the user through a web browser .
When a user performs an operation , such as submitting
Figure 4 : Activation tree and the corresponding webpage after selecting editing staff
Figure 5 : System Architecture a form , the leaf level AUnit corresponding to that operation returns data to its parent AUnit , which then performs operations to update the application state , and/or returns data to its parent , and so forth . AUnit instances can only get data from and return data to their parents , which is analogous to the data flow in function invocations . After the return chain terminates , a new activation tree is constructed by reevaluating every activation condition based on the updated state.1 Thus in Hilda a web application is no longer considered as a connected graph of individual web pages that allows users to navigate from any page to any other page . Instead , execution of a web application is modelled as a sequence of transitions from one activation tree to another , where the transition is triggered by the users’ interaction with a leaf level AUnit . Example : CMS ( Continued ) . The CMS AUnit is the root AUnit of the application in Figure 2 . A new instance of this root AUnit is activated each time a new user connects to the application , and this instance is deactivated when the user disconnects . Consider a case in the CMS , when a user logs in as course faculty and navigates to a course page . Figure 3 shows the current activation and the page in the browser . Each activated AUnit instance corresponds to a sub page of the content shown in the browser . NavigationBar 1 corresponds to the navigation bar at the top of the page , and NavigationBar 2 corresponds to the one at the left . Suppose the user wants to view and edit the list of staff members in the current course and selects that option from the navigation bar . NavigationBar 2 returns the selected option to its parent FacultyCourse and updates the local state2 of its parent . When the return chain finishes ( in this case , the chain only has one step ) , the new activation tree is constructed based on the updated state . One StaffList instance will be activated based on its activation condition . Figure 4 shows the resulting activation tree and its appearance in a web browser . Again if the user wants to view the student list for this course and chooses “ Student ” from the navigation bar , the NavigationBar 2 AUnit instance returns to the FacultyCourse instance and updates the state , which then deactivates the StaffList instance and activates a new instance of the StudentList AUnit . The webpage gets updated and will show student information 2.2 The Hilda Runtime System
1Reconstruction of the whole activation tree after each return chain terminates is only the semantics of the execution model ; more efficient implementations are possible [ 26 ] . 2We omit the details of local states of AUnit here , which are also expressed as tables . In this case , we have a CurrentChoice table which keeps track of the options users selected . Please refer to [ 26 ] for more details the same activation tree . In the first case , we keep the complete activation tree at the server side , while in the second case , we keep part of it at the client side . The main drawback of running everything at the server side is that the client must contact the server for every operation the user performs . The server then resends the entire refreshed page in HTML format to the client . However , if the navigation bar and Stafflist instances are executed at the client , the run time system can cache the data needed by them . Then , if the user adds or removes staff , the list of staff members is updated locally in the client , and the server is contacted only when the Submit button is clicked by the user . Only after this step are the updates in the staff list sent to the server , which updates the database . Note that the client already has all the data and code for the new staff list , unless it has been updated by other users — which is detected by the run time synchronization algorithm . This allows the RTSC to create HTML at the client without waiting for the server to respond . Other parts of the page , such as navigation bars , are normally unaffected by the transmitted data , and therefore keep their place in the page .
Maintaining AUnit instances at the client can therefore result in better system response time and a better user experience . This can also be seen from our experiments , discussed in Section 4 . Similar caching logic for partial updating of pages can be implemented in frameworks like AJAX only by extensive client side coding . In our framework , such partitions are automatic .
3 . MODEL OF CLIENT SERVER
PARTITIONING
In this section , we present a cost model for client server partitioning . We first define the problem and formulate it as an optimization problem . We show that the problem is NP hard . We then give an algorithm that approximates the optimum partition , and prove a bound on the approximation error .
3.1 Partitioning Philosophy
A plausible method of solving the client server partitioning problem would be partition at the granularity of AUnit definitions ; ie , to partition the set of AUnit definitions into two sets , one corresponding to AUnits whose instances will run on the server , and the other corresponding to AUnits whose instances will run on the client . However , this method would not capture the fact that different instances of the same AUnit may require very different amounts of computation and data transfer . For example , in the Course Management System , the EditCourse AUnit provides the functionality for course staff to edit courses . The amount course related data , such as the number of students enrolled , the number of assignments , etc . , can differ substantially between courses . We may want to ship the data and computation to the client for small courses while keeping big courses at the server side to save bandwidth . This motivates our decision to group similar instances together , profile their execution and then partition programs at the level of AUnit instances based on the profiles .
Different types of clients can have very different computing and storage resources . For example , moving computing and data to a powerful desktop client may be desirable , while doing the same for a PDA client may adversely affect its re
Figure 6 : Activation tree with different partitions
The Hilda RunTime Systems , for both the server and the client , are evaluation engines for Hilda programs . They execute the application logic specified in a program by maintaining the activation trees , and maintain consistency between the client and server states . We use RTSS to refer to the run time system residing at the server , and RTSC for the system residing at the client . The RTSS is a Java servlet running in some application server ( such as JBOSS or Weblogic ) , and has connections to the back end database . It communicates with the RTSC . The RTSC runs as a “ sticky ” applet , which resides in the secondary cache of the client and is available for quick loading by browsers [ 16 ] .
Based on the semantics of Hilda , the RTSS and RTSC coordinate with each other to maintain the activation tree . An AUnit instance is activated when its activation condition is satisfied and is deactivated when the conditions fails to be satisfied . The location ( client or server ) where an instance is activated is not predefined by the application developer ; instead , it is determined and can be changed at run time by the run time system .
The RTSC caches local data at the client .
It uses this data to generate webpages dynamically ( eg student info list ) , and to store a user ’s temporary input ( eg items in a shopping cart ) . This temporary data is stored in main memory , and reused by the RTSC . The RTSC contacts the RTSS to check for updates to its input data . The system imposes an upper bound on how out of date the client state can be by periodically contacting the server using heartbeat messages . To avoid sending the cached data back and forth between the client and the server , the RTSS maintains a copy of the data sent to each client . On receiving an update request , the server checks for updates to the client input data , and responds with only the updated data . To limit the amount of server side data required for each client session , the developer can specify a maximum life span for the data in the server cache , as well as the heartbeat frequency of the client . Our cache consistency strategy is similar to a detection based approach for transactional client server caching [ 12 , 25 , 18 ] , although the system allows for the integration of other strategies in the future .
Client server partitioning is done based on the activation profile of an application . The activation profile specifies which AUnit instances , identified by their unique key , should be activated in the client . When the run time system activates an instance of some AUnit , it refers to the configuration profile to determine whether the RTSS or the RTSC should activate the instance . The activation profile is generated automatically , based on the observed workload of the system . We will discuss its generation in the next section . CMS example : Figure 6 , shows two different partitions of sponse time . This motivates us to partition the application based on the types of clients . We make the assumption that the cost associated with a partition is independent of the load on the server , so that partitions corresponding to different clients do not interfere with each other ’s performance . Essentially , we are assuming that server load can be managed using existing load balancing techniques [ 8 ] ; improving server system scalability is outside the scope of the paper . The solutions for each client type thus obtained can be combined to yield an overall optimal solution for the application . Therefore , we describe the cost model for only a single client type . 3.2 Terminology
Recall that Hilda models an application in a hierarchical manner , where each AUnit contains other AUnits . Let aid be a unique identifier associated with each AUnit definition . Then , we define the class graph of a Hilda program P as : Definition 1 : ClassGraph(P ) = ( V , E ) where V = {v|v is an AUnit definition in P} , and E = {(v , w)|v , w ∈ V and w is a child AUnit of v} .
For a valid Hilda program , the class graph must be a DAG . However , instances of an AUnit may be activated for different keys . These instances can be uniquely identified by the pair ( aid , key ) , where key captures the path from a root AUnit instance to the current AUnit instance in the activation tree . This leads us to the definition of the key tree of a given Hilda program P : Definition 2 : KeyT ree(P ) = ( V , E ) where V = {(aid , key)| aid is the identifier of some AUnit definition in P} , and E = {(v , w)|v , w ∈ V and w.aid corresponds to a child AUnit of the AUnit corresponding to v.aid} .
Note that each AUnit corresponds to a different key , where the key includes the key of the AUnit ’s parent node . Therefore , an instance of any AUnit , except the root , is activated by exactly one parent . Thus the key tree must be a tree . Note that the class graph of a Hilda program is effectively an aggregated version of the key tree , obtained by merging nodes that have the same aid . In order to estimate the response time of a system , we require for each node of a key tree , various annotations such as the expected time for processing the AUnit instance and expected data to be processed for that instance . We next define an annotation function for the key tree of a Hilda program P as : Definition 3 : The annotation function A ( P ) : V → R 4 for the key tree ( V , E ) of a hilda program P is a function that maps each node v of the key tree onto a 4 tuple , where the fields correspond to the following : the probability pv that a randomly chosen activation over the space of all executions of P is on the AUnit that v is associated with ; the expected time tv for processing queries of the node , the expected sum dv of the size of input and output data , and the expected number lv of connections3 established , respectively . By the definition of the probabilities pv we also have
. pv = 1 . v
. . 3One HTTP connection is established for every client server communication .
We describe here the partitioning of a Hilda program into the client part and the server part , at the granularity of its key tree . Whether an AUnit instance is activated and evaluated at the client or the server depends on how the partitioning is done . Let ζ : V ∈ KeyT ree(P ) → {client , server} be a function specifying where AUnit instances in the key tree of program P are located . Then , we define a parameter α , which is the ratio of the client computation time to the server computation time for a given operation , as :
α = tu tv where ζ(u ) = server and ζ(v ) = client .
The data size of any given AUnit instance is assumed to be independent of ζ . This is because the input and output data of any instance remains the same , regardless of whether the instance is located at the server or the client . The partition of a hilda program P , then , is defined by a cut C as : Definition 4 : P artition(P ) ≡ C = ( Gs , Gc ) a cut in the tree KeyT ree(P ) = ( V , E ) st Gs and Gc are disjoint , Gs is connected , the root node belongs to Gs , and Vs ∪ Vc = V 4 . .
In this definition , Gs = ( Vs , Es ) is the part that runs on the server and Gc = ( Vc , Ec ) is the part that runs on the client , ie an AUnit instance a will be activated and maintained at client side iff ∃v ∈ Vc ( a.key = vkey We denote the set of edges between the two sides of the partition by Ecut = E − ( Es ∪ Ec ) . 3.3 Cost Model
In this paper , our goal is to optimize the average response time for users . Optimizing other goals , such as system throughput , would involve a similar analysis but a different cost model . We leave this as future work . Recall that we assume that the key trees corresponding to different types of clients are independent of each other , and do not affect the cost model for any given tree . We therefore consider the key tree for only a single type of client .
Before formalizing our cost model , we want to justify several implications that make the model more tractable . First , we ignore the cost at server side for synchronization and processing heart beat messages , because it is done asynchronously and thus does not noticeably affect users’ response time . Second , we do not consider the cost of transferring the run time system and Hilda code to the client side . These are implemented as sticky applets and can be reloaded from the client machine at low cost after being downloaded for the first time . Finally , we ignore web browser rendering time , which should be the same across different partitioning scenarios . Given the key tree KeyT ree(P ) = ( V , E ) of a program P , the annotation function A , and the cut C = ( Gs , Gc ) that partitions the tree into server and client subgraphs , we define the expected user response time as : costC ( P ) = v∈KeyT ree(P )
. pv × tC v
The time to perform AUnit instance processing , given a partition , includes the time to process the AUnit instance at the client ( return queries and later reactivation queries ) , the time to send query results to and from the server and the time to process the queries at the server : 4Vs and Vc are the nodes on server and client respectively tC v = tclient v
+ tdata v + tserver v v v where tclient is the expected time for processing v at the client , tdata is the expected time for sending result sets to and from the server , including the time for preparation of the data , and tserver is the expected time for processing queries at the server . Based on our earlier assumption that the time to process an AUnit instance at the client is proportional to the time to process the same instance at the server , and assuming that the data transmission time for transferring a result set between client and server is proportional to the size of that result set , we have : fi tclient v
= tserver v
=
0 α × tv fi tv 0 if ζ(v ) = server if ζ(v ) = client if ζ(v ) = server if ζ(v ) = client
We also have fi tdata v =
γ × dv + L × lv + dv/β if ∃u st ( u , v ) ∈ Ecut 0 otherwise v
Here , the data transmission cost tdata consists of three parts : the expected time for preparing the data to transfer , expected overhead of the handshaking process for establishing TCP connections , and the expected time for transferring the data . We assume that the expected time for preparing and transferring data is proportional to the expected amount of data transferred , with proportionality constants γ and β , respectively . L is the expected overhead for the handshaking process(initial round trip time ) , which allows us to take into account the number of connections .
These definitions yield the following optimization problem to choose a cut C for a program P : arg min
C costC ( P )
We define an additional constraint to take into account client memory limitations . Let MC ( T ) , the memory usage at the client given the cut C , be given by :
.
MC ( P ) = v∈KeyT ree(P ),ζ(v)=client mv where mv is the maximum memory that is used by any query of the AUnit instance v . Then , if ˆM is the maximum memory available for the application at the client , we have the constraint MC ( T ) ≤ ˆM . 3.4 Solution For Partitioning
The problem of finding an optimal partition with constraints for a given key tree has been proven to be NPhard [ 27 ] . Therefore , we design an approximation algorithm , which guarantees to give a result which is within three times of the optimal in the worst case . The technique we use is Randomized Rounding [ 21 ] : we first formulate the problem as an Integer Programming ( IP ) problem , relax it to a Linear Programming ( LP ) problem , solve it , and use a randomized algorithm , similar to that in [ 15 ] , to round the solution to an integral one that is not much worse . Given a key tree KeyT ree(P ) = ( V , E ) , for every node v ∈ V , we define a variable xv and for every edge e ∈ E , we define a variable ye . The optimal partition problem for a given Hilda program P , with the annotation function A can then be formulated as the following IP problem :
( 1 − xv ) ∗ M ( v ) ≤ ˆM
Constraints : • xroot = 1 , root is the root of KT • ∀v ∈ V , xv ∈ {0 , 1} • ∀e ∈ E , ye ∈ {0 , 1} • ∀e(v1 , v2 ) ∈ E xv1 ≥ xv2 and ye ≥ xv1 − xv2 • ' • ' ye ∗ n(e ) For each node v ∈ V and edge e = ( u , v ) ∈ Ecut , c(v ) = α × tv is the computing cost at client side s(v ) = tv is the computing cost at server side M ( v ) = mv is the memory cost at client side n(e ) = ( 1/β + γ ) × dv + L × lv is the data transfer cost
( 1 − xv ) ∗ c(v ) +
' e∈E
' v∈V
Minimizing function : xv ∗ s(v ) + v∈V v∈V fi xv =
The optimal solution for above integer programming will give us an optimal partition c = ( Gs , Gc ) , Ecut in the following way : fi
0 if v ∈ Vs 1 if v ∈ Vc and ye =
0 if e ∈ Ecut 1 if e ∈ Ecut
We can relax the above problem , by allowing xv ∈ [ 0 , 1 ] and ye ∈ [ 0 , 1 ] , and get an LP problem that is solvable in polynomial time , with solution X∗ . We can then round each x∗ v to 0 or 1 with a threshold uniformly randomly chosen from [ 1/3 , 2/3 ] . This special rounding technique guarantees that the objective function and constraints are still within a reasonable bound .
It is provable that the response time of the partition generated by our algorithm is at most three times as much as the optimal response time obtained using LP , which is no more than the optimal response time . The proof is omitted due to space constraints . Please refer to the Technical Report [ 27 ] for more details on the algorithm and the proof . Note that the theoretical bound given here is a theoretical worst case bound . In practice , we have found that the response time obtained using our algorithm is very close to the optimal response time for the applications that we considered in our experimental evaluation .
4 . EXPERIMENTAL EVALUATION
In this section , we first describe the setup for the experiments we performed to evaluate the performance our Hilda system ( Section 41 ) We then compare the performance of a Hilda and a J2EE implementations of a real world application ( CMS ) and a technical benchmark ( TPC W ) . These comparisons show the benefits of automatic clientserver partitioning ( Section 42 ) 4.1 Experimental Setup
We first discuss how we estimate the annotation of a key tree using a trace of the running application . We then describe how we apply the result of the optimization problem to achieve a partition of the application , and we give an overview of the physical setup for the experiments .
411 Parameter Estimation
413 Physical Setup
A trace consists of a sequence of AUnit activations , along with meta data for the time , data and number of connections associated with each activation . Definition 5 : Let P be a Hilda program . A trace T race(P ) = i )|1 ≤ i ≤ nfi of P is a sequence of five tuples ff(i , vi , ti , di , li , tγ called events . The number i is the sequence number of the event , vi = ( aid , key ) uniquely identifies an AUnit instance in P , ti is the time taken to process the queries in this instance , di is sum of the size of the input and output data for the instance , li is the number of connections established between the client and the server , and tγ is the time spent to prepare the data by this instance . . i
Given the above definition , the annotation function of the keytree of program P can be estimated through an aggregated version of the trace . Since multiple events in the trace may be associated with the same node v of the key tree , we can estimate the value of v ’s annotation by counting and aggregating the trace data for each node . More precisely , we estimate the annotation function for a node v ∈ KeyT ree(P ) as follows . Let A ( v ) = ( p , t , d , l ) . Then we can estimate ( p , t , d , l ) with ( ˆp , ˆt , ˆd , ˆl ) as follows : ) ∈ T race(P )}| ) ∈ T race(P )} , ) ∈ T race(P )} ) ∈ T race(P )}
|{i|∃(i , v' , t' , d' , l' , t'' '{t|∃(i' , t , d' , l' , t'' n p × n '{d|∃(i' , t' , d , l' , t'' p × n '{l|∃(i' , t' , d' , l , t'' p × n
ˆd =
ˆp =
ˆl =
ˆt =
,
,
.
The other parameters for optimization were specified according to the physical setup . We ran the experiments on the PlanetLab network . Given that only powerful desktop clients are used in PlanetLab , we assumed that the client and the server have similar computing power . Therefore , we set parameter α = 1 , and no bound was imposed for the memory available at the client . The bandwidth ( β ) of the network was roughly 300KB , and the round trip time L was approximated as 10ms . We could also have estimated these parameters automatically at runtime ; this is left as future work . We also estimated γ as follows : tγ i di
.
γ =
1 n
. i≤n
412 Partitioning Logic
The client server partitioning for a program P is done at the granularity of key trees . Given a cut C = ( Gs , Gc ) in the key tree , we ship the data of the AUnit instances in Vc to the client . However , note that our constructed annotation function assumes that the future workload is very similar to the one seen before . In practice , the future workload can contain AUnit instances that have never been encountered before . Therefore , the partitioning is also done at the class graph level , using nodes from the class graph as representatives for instances not yet seen in the trace . For unseen instances , we will position the instance based on the computed partitions for the class graph .
We illustrate the benefits of Hilda using a Course Management System and an Online Book Store application that is based on the TPC W benchmark . We compare the average users’ response time , which we refer to as responsiveness , of a Hilda implementation and a J2EE implementation of the two applications . The applications were deployed in a JBOSS application server setup on a 2.66Ghz machine having 4GB of RAM , and used MS SQL 2005 as the backend database management server . The client simulators were deployed on the PlanetLab network , and included the Hilda RTSC .
We measured the response time for each operation , ie the time taken to submit a request , process it at the server/client and receive the resulting page from the server . Therefore , this measure includes the time spent on the server to process the request , the time spent at the client and the network transmission time . However , we did not take into account the time taken by the web browsers to render the resulting HTML pages . Also , in order to reduce the error due to the erratic nature of the PlanetLab network , the experiments were conducted twice . The values we present in the next section are therefore averages over two runs of the simulation . 4.2 Experimental Results
We now present experimental results from two applica tions : a CMS and an Online Book Store .
421 Course Management System
Our first experiments were performed on CMS , a Course Management System developed at the Cornell Computer Science Department which is currently in use by more than 2000 students , staff and faculty [ 6 ] . The original version of CMS was developed using traditional application development tools such as J2EE/EJB , JavaScript and HTML , while a new version has been developed using Hilda .
The J2EE version of the CMS was developed by experienced programmers , and therefore included extensive clientserver partitioning that was done manually . Most of the client side application logic was implemented using JavaScript , which allowed for web pages to be updated dynamically . For example , features such as sorting tables based on selected column values , showing or hiding portions of a web page , and caching users’ input temporarily in the browser were already implemented at the client side .
To calculate the average response time , we emulated the operations performed on the CMS in one semester . A usage log consisting of 60000 operations was collected from the J2EE version of the system , along with the necessary parameters . Table 1 lists the operations that the users performed . The first three thousand operations from this log were used as a trace to construct the annotation function , which was then used to compute a partition for the application . The remaining operations were then evaluated based on the computed partition . The average response time shown in Table 2 does not include the time to collect the trace . Table 2 also presents the performance measure of the application when it is deployed at the server without any partitioning .
It is evident from Table 2 that the Hilda version of CMS with automatic partitioning is comparable to the J2EE version in average response time . The automatically partitioned version , however , reduces the average data trans
Operation Description O1 O2
O3 O4
O5
O6 O7
O8
O9 O10 O11 O12 O13 O14 O15 O16 O17 O18 O19 O20 O21 O22
O23
View CMS homepage View course management system summary Add/remove courses View course property page(as instructor ) View course property page(as admin ) Edit course property View course homepage(as student ) View course homepage(as instructor ) View student list page View add students page Add/edit students Drop students Update students final grades View adding assignment page View editing assignment page view assignment list View assignment details Editing assignment View adding category page View edit category schema page View edit category content page Add/remove/edit columns in category schema Add/remove/edit rows of category content
Number 24994 244
18 219
83
91 7912
1858 1858 9 133 867 48 25 158 841 846 20923 497 205 120 150 103
16
Table 1 : Operations in the CMS Application
System
Average Response Average Data Time(ms ) 278.80 312.64 270.01
J2EE Server Only Client Server Table 2 : CMS : Response Time and Data Transmission
Transmission(KB ) 17.99 19.09 12.74 ferred between the client and the server by roughly 30 % .
Figure 8 shows the average data transfer for each operation . Owing to the fact that caching user input at the client reduces the amount of data transferred between the client and the server , operations such as O3 , O11 , O12 , O13 , O14 , O19 , O22 , O23 that involve updates result in comparatively less data transfer . For example , consider O3 – after the system administrator creates a new course , the page is refreshed with a new list of courses . However , if the AUnit for the course list gets pushed to the client , the page generated at client side is able use locally cached data .
The J2EE version of the CMS allows a web browser to cache webpages for later visit , at the page level , while the Hilda run time system caches data at the AUnit ( subpage ) level . For example , a navigation bar that is present on most pages includes the list of available courses , and contains the assignment and category list corresponding to each course . After partitioning , the Hilda run time system keeps the AUnit instances for the navigation bar at the client , including the data and the logic to generate HTML segments for navigation bars . Such partial updating yields benefits in the response time for the operations O1 , O7 , O8 , O16 , O17 , O19 and O20 . The Hilda run time system also makes sure that the data for a navigation bar ( list of assignments , courses and categories ) is up to date , by periodically checking with the server for any changes .
Bad design decisions may sometime result in suboptimal In the J2EE version of the CMS , the logic performance .
Figure 7 : Average Response Time for Different Operations in CMS
Figure 8 : Average Amount of Data Transmitted for Different Operations in CMS for users to sort tables based on different columns is always pushed to the client . However , all pages in the system are assembled dynamically , and the Javascript generated on the fly is embedded in the HTML pages . This Javascript makes the size of pages with sortable tables very large ( 600K on average ) . It increases the network transmission time and results in poor response time even compared to the Hilda version without any partitions ( Figure 7 : O10 , O11 , O12 and O13 ) .
422 Online Book Store
The TPC W [ 10 ] benchmark specifies an online book shop application as the test case for evaluating application server performance . In this application , users can register , view book details , manage their shopping carts and check out , while managers can add new book details into their inventory . We implemented the application using both J2EE and Hilda , and evaluated the average response time of the two systems using a trace synthesized according to the specifications in the benchmark . In the J2EE version , we did not implement any application logic at the client side except for the basic HTML presentations . We took the first 5 percent of the workload as training set for the system to collect statistics , and then measured the response time after the application ran with the computed optimal partition for the Hilda version with partitioning enabled .
Table 4 , Figure 9 and Figure 10 show the average response time and the average data transmission for each operation of the application , in the J2EE version and the Hilda versions with and without automatic partitioning . The Hilda system
Operation Description O1 O2 O3 O4 O5 O6 O7 O8 O9 O10
View website homepage Register as new user Add a book to product list Register an author of a book View book details Add a book into shopping cart View shopping cart details View checkout page Checkout View order status
Number 118 999 2098 970 1542 4593 814 918 1799 920
Table 3 : Operations in TCP W Online Bookstore Application
System
J2EE Server Only Client Server
Average Response Average Data Time(ms ) 221.80 231.88 143.48
Transmission(KB ) 21.7 21.9 3.3
Table 4 : Average Response Time and Data Transmission for TPC W benefits from activating instances of shopping cart AUnit at the client side . A user can add the book she viewed ( O5 ) into the shopping cart ( O6 ) and view the details at a later time ( O7 ) , possibly before checkout . The shopping cart and the details about the books in the shopping cart are cached along with the AUnit instance , which make the add to the cart ( O6 ) and view detail ( O7 ) operations locally executable , resulting in a much better response time .
5 . RELATED WORK
In recent years , many programming models and frameworks [ 4 , 7 , 9 , 11 , 13 ] have been proposed for designing and developing web applications . A few of these frameworks also propose a high level programming model to develop application logic for different tiers of an application . Hilda takes the further step of automating the process of client server partitioning using a quantitative approach .
Caching data and query results at clients is a concept that has been studied in relational and object oriented database systems . Work in this area has focused on Transactional Client Server Cache Consistency [ 12 , 18 , 25 ] , a technique that evaluates part of a transaction at the client by shipping it the required data . This work is concerned with guaranteeing the ACID properties of a transaction , and proposes many different approaches such as the Avoidance Based Approach ( Adaptive CallBack Locking ) and the Detection Based Ap
Figure 9 : Average Response Yime for Different Operations in TPC W
Figure 10 : Average Amount of Data Transmitted for Different Operations in TPC W proach ( Adaptive Optimistic Concurrency Control ) . However , the work assumes a predefined partition of transactions across the server and the client , and thus is complementary to what Hilda achieves .
Hybrid Shipping Architectures have been proposed to run queries in a distributed setting [ 22 , 23 ] . The motivation behind these systems is that data shipping ( query execution at clients ) and query shipping ( query execution at servers ) can be done together . However , these architectures only consider the partitioning of a single read only query . They decompose each query into operators such as join , scan and display etc . and then distribute these operations across different sites , taking into account the parallelism and communication costs . They use standard optimization techniques to achieve this . Our goal , on the other hand , is to partition queries in one transaction across the server and the client and to cache data for multiple queries .
Another related area of research is Mobile Code , which aims at transforming a centralized program into a distributed architecture and utilizing resources in distributed systems [ 3 , 17 , 20 , 24 ] . The system in [ 17 ] takes the binary code of a program and distributes the components and the procedures among a cluster in order to optimize the communication cost . Wang et al . address the problem of partitioning programs in the context of mobile devices [ 24 ] . They represent a program in the form of a Task Control Flow Graph ( TCFG ) , ie a directed graph , where each node represents a task , and each edge represents data transfer between the tasks . Their cost model includes computation time , communication time , scheduling time , and data registration time . They formulate the optimization problem as a parameterized min cut/max flow problem , where common parameters include buffer size , input size , command line options , etc . The Abacus system [ 3 ] consists of a programming model and a run time system . The proposed programming model encourages the programmer to develop data intensive applications using small , functionally independent components or objects . The run time system automates the placement of the objects in data intensive applications and file systems among the nodes of a cluster . The J Orchestra [ 20 ] system partitions Java applications into distributed ones using Java RMI . By rewriting the code using Java RMI , their system can distribute components which share data in memory and thus result in finer granularity for partitioning . However , none of this work consider the concepts of consistency and conflicts for the cached table data between client and server sides . Another drawback is that the language model used by all of this work is not declarative , and therefore the efficacy of the system is limited by how programmers code the components and the procedures .
[ 9 ] S . Ceri , P . Fraternali , and A . Bongio . Web modeling language ( webml ) : a modeling language for designing web sites . In Proc . the ninth International World Wide Web Conference , 2000 .
6 . CONCLUSION AND FUTURE WORK
In this paper , we introduced a unified platform for data driven web applications . The platform is based on Hilda , a high level declarative language that allows dynamic partitioning of the web application between the client and the server in a manner that is completely transparent to the developer . This automatic partitioning helps in avoiding manual application partitioning decisions , which can be ad hoc and suboptimal . Based on the observed workload , the Hilda run time system determines a client server partition of the application , which is close to the optimal partition , using a quantitative method . We also illustrated the benefits of Hilda and automatic client server partitioning by comparing it with J2EE , using two web applications — a Course Management System with a real workload and an Online Book Store with a benchmark workload . We showed that the performance of the CMS is comparable for both Hilda and J2EE , and that Hilda gains on the amount of data transferred between the client and the server . The TPC W benchmark Online Book Store illustrated a 35 percent improvement in response time for Hilda over a corresponding J2EE implementation .
There are several avenues for future work . The current Hilda optimization model treats each user operation independently , but does not take into account the client side operations performed by the users . Interesting techniques such as asynchronous prefetching and anticipating user actions to prefetch data are not supported . The optimization goal currently focuses only on improving a user ’s experience and the system ’s response time . It would be interesting to consider other goals for optimization , such as system throughput by automating load balancing at server side .
7 . REFERENCES [ 1 ] Adobe flash . http://enwikipediaorg/wiki/Macromedia Flash .
[ 2 ] Asynchronous javascript and xml . http://enwikipediaorg/wiki/Ajax ( programming ) .
[ 3 ] K . Amiri et al . Dynamic function placement for data intensive cluster computing . In USENIX 2000 Annual Technical Conference , San Diego , CA , June 2000 . , pages 307–322 , 2000 .
[ 4 ] A . Bongio , S . Ceri , P . Fraternali , and A . Maurino .
Modeling data entry and operations in webml . In The World Wide Web and Databases ( WebDB , Selected Papers ) , pages 201–214 , 2000 .
[ 5 ] G . Booch et al . The Unified Modeling Language User Guide,The Addison Wesley Object Technology Series . Addison Wesley , 1998 .
[ 6 ] C . Botev et al . Supporting workflow in a course management system . In Proc . SIGCSE , 2005 .
[ 7 ] M . Brambilla et al . Declarative specification of web applications exploiting web services and workflows . In Proc . SIGMOD , pages 909–910 , 2004 .
[ 8 ] V . Cardellini , M . Colajanni , and P . S . Yu . Dynamic load balancing on web server systems . IEEE Internet Computing , 3(3):28–39 , 1999 .
[ 10 ] T . W . Commerce . Tpc benchmark http://wwwtpcorg/tpcw/
[ 11 ] E . Cooper , S . Lindley , P . Wadler , and J . Yallop .
Links : Web programming without tiers . In Submitted to ESOP 2007 .
[ 12 ] M . J . Franklin , M . J . Carey , and M . Livny .
Transactional client server cache consistency : alternatives and performance . ACM Trans . Database Syst . , 22(3 ) , 1997 .
[ 13 ] P . Fraternali . Tools and approaches for developing data intensive web applications : A survey . ACM Computing Surveys , 31(3):227–263 , 1999 .
[ 14 ] N . Gerner et al . Automatic clientserver partitioning of data driven web applications . In Proc . SIGMOD , 2006 .
[ 15 ] A . Hayrapetyan , D . Kempe , M . P´al , and Z . Svitkina .
Unbalanced graph cuts . In European Symposium on Algorithms ( ESA ) , Mallorca , Spain , 2005 .
[ 16 ] http://javasuncom/j2se/142/docs
/guide/plugin/developer guide/applet cachinghtml
[ 17 ] G . C . Hunt and M . L . Scott . The coign automatic distributed partitioning system . In Operating Systems Design and Implementation , pages 187–200 , 1999 .
[ 18 ] M . Ozsu , K . Voruganti , and R . Unrau . An asynchronous avoidance based cache consistency algorithm for client caching dbmss , 1998 .
[ 19 ] R . Ramakrishnan and J . Gehrke . Database
Management Systems . McGraw Hill , 3 edition , 2003 .
[ 20 ] E . Tilevich and Y . Smaragdakis . J orchestra :
Automatic java application partitioning . European Conference on Object Oriented Programming ( ECOOP ) , Malaga , June 2002 .
[ 21 ] V . V . Vazirani . Approximation Algorithms .
Springer Verlag , Berlin , 2001 .
[ 22 ] K . Voruganti , M . T . Ozsu , and R . C . Unrau . An adaptive hybrid server architecture for client caching ODBMSs . In The VLDB Journal , pages 150–161 , 1999 .
[ 23 ] K . Voruganti , M . T . ¨Ozsu , and R . C . Unrau . An adaptive data shipping architecture for client caching data management systems . Distrib . Parallel Databases , 15(2):137–177 , 2004 .
[ 24 ] C . Wang and Z . Li . Parametric analysis for adaptive computation offloading . In PLDI ’04 : Proceedings of the ACM SIGPLAN 2004 conference on Programming language design and implementation , 2004 .
[ 25 ] K . Wu , P . fei Chuang , and D . J . Lilja . An active data aware cache consistency protocol for highly scalable data shipping dbms architectures . In CF ’04 : Proceedings of the 1st conference on Computing frontiers , 2004 .
[ 26 ] F . Yang et al . Hilda : A high level language for data driven web applications . In Proc . ICDE , 2006 .
[ 27 ] F . Yang et al . A unified platform for data driven web applictions with automatic client server partitioning . Technical report , Cornell University , 2007 . http://techreportslibrarycornelledu
