Supervised Rank Aggregation
Yu Ting Liu1,2* , Tie Yan Liu1 , Tao Qin1,3* , Zhi Ming Ma4 , and Hang Li1
1Microsoft Research Asia 4F , Sigma Center , No . 49 ,
Zhichun Road , Haidian District ,
Beijing , 100080 , China
{tyliu , hangli}@microsoft.com
2 School of Science ,
Beijing Jiaotong University ,
Beijing 100044 , China liuyt_njtu@hotmail.com
3Department of Electronic
Engineering ,
Tsinghua University , Beijing 100084 , China qinshitao99@mailsthueducn
4Academy of Math and
Systems Science , Chinese
Academy of Science , Beijing 100080 , China mazm@amtaccn
ABSTRACT
This paper is concerned with rank aggregation , the task of combining the ranking results of individual rankers at meta search . Previously , rank aggregation was performed mainly by means of unsupervised learning . To further enhance ranking accuracies , we propose employing supervised learning to perform the task , using labeled data . We refer to the approach as â€˜Supervised Rank Aggregationâ€™ . We set up a general framework for conducting Supervised Rank Aggregation , in which learning is formalized an optimization which minimizes disagreements between ranking results and the labeled data . As case study , we focus on Markov Chain based rank aggregation in this paper . The optimization for Markov Chain based methods is not a convex optimization problem , however , and thus is hard to solve . We prove that we can transform the optimization problem into that of Semidefinite Programming and solve it efficiently . Experimental results on meta searches show that Supervised Rank Aggregation can significantly outperform existing unsupervised methods .
Categories and Subject Descriptors H33 [ Information Storage and Retrieval ] : Information Search and Retrieval â€“ Retrieval models . H34 [ Information Systems Application ] : Systems and Software performance evaluation ( efficiency and effectiveness ) .
General Terms Algorithms , Experimentation , Theory
Keywords Rank Semidefinite programming aggregation , supervised learning , Markov Chain ,
1 . INTRODUCTION Rank aggregation is to combine ranking results of entities from multiple ranking functions in order to generate a better one . The individual ranking functions are referred to as base rankers , or simply rankers , hereafter .
Rank aggregation can be classified into two categories [ 2 ] . In the first category , the entities in individual ranking lists are assigned scores and the rank aggregation function is assumed to use the scores ( denoted as score based aggregation ) [ 11][18][28 ] . In the second category , only the orders of the entities in individual ranking lists are used by the aggregation function ( denoted as
*This work was conducted when the first and the third authors were interns at Microsoft Research Asia
Copyright is held by the International World Wide Web Conference Committee ( IW3C2 ) . Distribution of these papers is limited to classroom use , and personal use by others . WWW 2007 , May 8 12 , 2007 , Banff , Alberta , Canada . ACM 978 1 59593 654 7/07/0005 . order based aggregation ) . We focus on order based aggregation in this paper . Order based aggregation is employed at meta search , for example , in which only order ( rank ) information from individual search engines is available .
Previously order based aggregation was mainly addressed with the unsupervised learning approach , in the sense that no training data is utilized ; methods like Borda Count [ 2][7][27 ] , median rank aggregation [ 9 ] , genetic algorithm [ 4 ] , fuzzy logic based rank aggregation [ 1 ] , Markov Chain based rank aggregation [ 7 ] and so on were proposed . One exception is Borda Fuse [ 2 ] which also makes use of training data . However , it is different from the supervised learning method we propose in this paper .
We argue that in order to improve the accuracy of rank aggregation , it is better to employ a supervised learning approach in which we train an order based aggregation function within an optimization framework using labeled data . At meta search , for example , labeled data can be documents and their relevancies to given queries . The key factors , thus , are ( a ) to assume that only order information from individual rankers is available , ( b ) to use labeled data , and ( c ) to train the aggregation function within an optimization framework . In this paper , we refer to the approach as â€˜Supervised Rank Aggregationâ€™ . labeled
There are several advantages for taking the supervised learning approach . First , we can leverage the use of information existing in training data . Second , we can apply existing optimization techniques to the problem . Third , it becomes easier to make domain or user adaptation . Certainly , it also has a disadvantage , that is , labeled data is needed and creating such data can be costly . This is , however , a shortcoming for any supervised learning method and we can leave it as future research topic .
In this paper , we first give a general framework for conducting Supervised Rank Aggregation . We show that we can define supervised the existing unsupervised methods , such as Borda Count and Markov Chain based methods by exploiting the framework . learning methods corresponding to
Then we mainly investigate the supervised versions of Markov Chain based methods in this paper , because previous work shows that their unsupervised counterparts are superior [ 24 ] . It turns out , however , that the optimization problems for the Markov Chain based methods are hard , because they are not convex optimization problems . We are able to develop a method for the optimization of one Markov Chain based method , called Supervised MC2 . Specifically , we prove that we can transform the optimization problem into that of Semidefinite Programming . As a result , we can efficiently solve the issue . ( We plan to apply the same technique to the other Markov Chain methods in the future . )
Experimental results on meta searches show that Supervised Rank Aggregation ( ie , Supervised MC2 ) can achieve better performances than existing methods .
WWW 2007 / Track : SearchSession : Search Quality and Precision481 The rest of this paper is organized as follows . In Section 2 , we introduce related work . In Section 3 , we propose a general framework and for Supervised Rank Aggregation . In Section 4 , we propose an optimization algorithm for the method of Supervised MC2 . Experimental results are reported in Section 5 . Conclusions and future work are given in the last section . specific methods
2 . RELATED WORK The origin of research on rank aggregation can be traced back to the eighteenth century , when it was studied in social choice theory and applied into political elections [ 5 ] . In recent years , rank aggregation gets spotlight again in many new applications , such as genome database construction [ 26 ] , document filtering [ 13 ] , database middleware construction [ 10 ] , spam webpage detection [ 7 ] , meta search [ 2][7][17][24 ] , word association finding [ 7 ] , multiple search [ 11 ] , and similarity search [ 9 ] .
There are two types of rank aggregation : score based and orderbased . In the former the aggregation function takes score information from the individual base rankers as input , while in the latter it only utilizes order information . Order based aggregation fits well with meta search , as in meta search only order information from base rankers is available ; this is also the main focus of the research in this paper .
Existing methods for order based aggregation includes , for example , Borda Count [ 2][7][27 ] , median rank aggregation [ 9 ] , genetic algorithm [ 4 ] , fuzzy logic based rank aggregation method [ 1 ] and Markov Chain based rank aggregation [ 7 ] . Borda Count ranks entities based on their positions in the ranking lists . For example , the entities are sorted according to the number of entities that are ranked below them in all the ranking lists . Median rank aggregation sorts the entities based on the medians of their ranks in all the ranking lists . Markov Chain based rank aggregation assumes that there exists a Markov Chain on the entities and the order relations between entities in the ranking lists represents the transitions in Markov Chain . The stationary distribution of the Markov Chain is utilized to rank the entities . Dwork et al [ 7 ] proposed four methods ( denoted as MC1 , MC2 , MC3 , and MC4 ) to construct the transition probability matrix of the Markov Chain .
The unsupervised methods described above implicitly conduct majority voting in their final ranking decisions . That is to say , these methods treat all the ranking lists equally and give high ranks to those entities ranked high by most of the rankers . This assumption may not hold in practice , however . For example , in meta search , ranking lists are generated by different search engines with different capacities and accuracies . It is not reasonable to treat the results of the search engines equally .
To deal with the problem , Aslam et al [ 2 ] proposed Borda Fuse , which can be viewed as weighted Borda Count for meta search . Specifically , different rankers are assigned different weights , while the weights are trained separately by using labeled training data . For example , the weights can be calculated based on the MAP ( Mean Average Precision ) scores of the base rankers . Experimental results show that Borda Fuse indeed improves upon Borda Count . The problem with Borda Fuse is that the weights of the ranking list are calculated independently and by using heuristics . It is also not clear whether the same idea can be applied to other methods .
We note that order based rank aggregation in meta search is similar to relevance ranking in document retrieval , but there are some clear differences . Therefore , the methods proposed for relevance ranking may not be directly applicable to order based rank aggregation . In relevance ranking , a typical approach is to employ a linear combination model of the features to rank documents . One can also employ a supervised learning method to train the model . Each feature can be viewed as a ranker and the final ranking model can be viewed as an aggregation function . However , this final ranking model is more close to that of scorebased aggregation , not that of order based aggregation . How to apply a score based method to order based aggregation is still an open problem , and is out of the scope of this paper .
3 . SUPERVISED RANK AGGREGATION In this section , we first introduce a general optimization framework for order based rank aggregation . We then define Supervised Rank Aggregation methods within the framework .
We first give some definitions and notations . Given a set of entities S , let V be a subset of S and assume that there is a total order among the entities in V . ğœ is called a ranking list with respect to S , if ğœ is a list of the entities in V maintaining the same total order relation , ie,ğœ = ğ‘‘1 , â‹¯ , ğ‘‘ğ‘š , if ğ‘‘1 > â‹¯ > ğ‘‘ğ‘š , ğ‘‘i âˆˆ ğ‘‰,ğ‘– = 1 , â‹¯ , ğ‘š , where > denotes the relation and m denotes the size of V . If V equals S , Ï„ is called a full list , otherwise , it is called a partial list . A special case of partial list is a top t list , for which the first tth entities are ordered in the list .
3.1 Optimization Framework The goal of rank aggregation is to assign a real valued score to each of the entities by aggregating all the ranking lists given by the base rankers , and then sort the entities according to their scores . Without loss of generality , hereafter we assume that it is in the descending order .
Let ğœ1 , â‹¯ , ğœğ‘™ denote the ranking lists with respect to ğ‘† and ğ‘› denotes the number of entities in S . We define the aggregation function as ğ›¹ : ğœ1 , â‹¯ , ğœğ‘™ â†¦ ğ‘¥ , where ğ‘¥ denotes the final score vector of all entities . That is , if ğ‘¥ = ğ›¹ ğœ1 , â‹¯ , ğœğ‘™ , then all the entities are ranked by the scores in x .
For example in Borda Count , ğ‘¥ is called Borda score , which is calculated as ,
ğ‘™ ğ‘¥ = ğ›¹ ğœ1 , â‹¯ , ğœğ‘™ = ğ‘˜=1 ğ‘‡
ğ‘¥(ğ‘˜ )
( 311 )
( ğ‘˜ )
ğ‘–=1,â‹¯,ğ‘›
, ğ‘¥ğ‘–
( ğ‘˜ ) = # ğ‘—|ğ‘– >ğœğ‘˜ ğ‘— , and ğ‘– >ğœğ‘˜ ğ‘— where ğ‘¥(ğ‘˜ ) â‰œ ğ‘¥ğ‘– means that entity i is ranked higher than entity j in ranking list ğœğ‘˜ . We assume that the aggregation function ğ›¹ is parameterized by a parameter vector ğ›¼ . In a supervised learning approach to rank aggregation , we try to learn the optimal values of the parameters by using labeled training data . Typically training data may include ground truth indicating pairwise preferences of which entities should be ranked higher than the others . In the learning , we actually manage to find the aggregation function that minimizes the disagreements between the ground truth and the output of the aggregation function .
We represent the agreement between the output list of an aggregation function and the ground truth by using an inequality
ğ»ğ‘¥ < 0 where ğ‘¥ denotes the output of the function and H denotes a matrix representing the pairwise preference relationship between entities . For example , suppose that the scores produced by the aggregation function are ğ‘¥ = ğ‘¥1 , ğ‘¥2 , ğ‘¥3 , ğ‘¥4 ğ‘‡ , and the ground truth indicates
WWW 2007 / Track : SearchSession : Search Quality and Precision482 that entity 1 should be ranked higher than entity 2 , and entity 4 should be ranked higher than entity 3 . Then , the inequality becomes :
ğ»ğ‘¥ < 0 , where ğ» =
âˆ’1 0
1 0 0 0 1 âˆ’1
By using such a matrix , we can bring any form of ground truth into our framework , and do not need assume a total order existing over all the entities in the training set .
There is no guarantee that there exists a parameter vector ğ›¼ that satisfies all the pairwise constraints in the ground truth . That is , disagreements may exist . We introduce â€˜slack variableâ€™ ğ‘¡ to represent the differences ( errors ) ,
ğ»ğ‘¥ < ğ‘¡ , ğ‘¡ â‰¥ 0
To reduce training errors is equivalent to minimize the norm of t . Thus we can formalize Supervised Rank Aggregation as the following optimization problem . minğ‘¥,ğ›¼,ğ‘¡ ğ‘¡ğ‘‡ğ‘¡
ğ‘  . ğ‘¡ . ğ‘¥ = ğ›¹ ğœ1 , â‹¯ , ğœğ‘™ ; ğ›¼
ğ›¼ âˆˆ ğ¶ ğ»ğ‘¥ < ğ‘¡ , ğ‘¡ â‰¥ 0
( 312 ) where Î± denotes the parameter vector and C denotes a feasible region for Î± . The dimension of matrix H equals the number of pairs indicating pairwise preferences in the training data . The objective ğ‘¡ğ‘‡ğ‘¡ actually denotes the empirical loss in the training data . When empirical loss is 0 , the aggregation function ğ›¹ satisfies all the pairwise constraints .
With different ways of the aggregation function , we come to different methods for rank aggregation . instantiating and optimizing
3.2 Methods We show that we can define Supervised Rank Aggregation methods within the framework . In this paper we only consider the case in which the aggregation function is defined as a linear model of base rankers . Even the model is simple ; it is powerful enough for accomplishing the tasks in this paper .
( 1 ) Borda Fuse
Many other rank aggregation methods are based on Markov Chain . It is advantageous to employ the Markov Chain model in rank aggregation , particularly when the base rankers only output partial lists [ 8 ] . Experimental results show that the Markov Chain based methods outperform other methods [ 24 ] . That is why we focus on Markov Chain based approach in this paper .
Dwork et al [ 7 ] proposed four Markov Chain based models for rank aggregation , referred to as MC1 , MC2 , MC3 , and MC4 . The four models correspond to four different heuristic rules for constructing the transition probability matrix in Markov Chain .
Let us take MC2 as example . The transitions in Markov Chain are defined as follows . If the current state is i , then we first select a ranking list ğœğ‘˜ uniformly randomly from the ranking lists ğœ1 , â‹¯ , ğœğ‘™ that contain state i , then select state j uniformly randomly from the set of states that are ranked not lower than state i in ğœğ‘˜ , and define j as the next state .
For a full list or top t list , it is not difficult to verify that the transition matrix is arithmetic mean of transition probability matrices produced from individual ranking lists , referred to as denote the kth base base transition matrices . Let ğ‘ƒğ‘˜ â‰œ ğ‘ğ‘–ğ‘— transition matrix produced by ranking list ğœğ‘˜ , in which each element ğ‘ğ‘–ğ‘— corresponds to the conditional probability of state j given state i in ranking list ğœğ‘˜ . The final transition matrix P is defined as
( ğ‘˜ )
ğ‘›Ã—ğ‘›
ğ‘˜
ğ‘ƒ =
ğ‘™ ğ‘˜=1
ğ‘ƒğ‘˜
1
ğ‘™ 1
, ğ‘— >ğœğ‘˜ ğ‘– or ğ‘— = ğ‘– ğ‘˜ = ğ‘ğ‘–ğ‘— ğ‘š 0 , otherwise
( 321 ) where ğ‘š = # ğ‘—|ğ‘— >ğœğ‘˜ ğ‘– or ğ‘— = ğ‘– . The score vector x can then be computed by solving ğ‘¥ = ğ‘ƒğ‘‡ğ‘¥ , with constraints ğ‘¥ğ‘– In Supervised MC2 we assign weighting coefficients to the base matrices Pk :
ğ‘› ğ‘–=1 = 1 , ğ‘¥ğ‘– > 0 , ğ‘– = 1 , â‹¯ , ğ‘› .
ğ‘™ ğ‘ƒ = ğ‘˜=1
ğ›¼ğ‘˜
ğ‘ƒğ‘˜
Formally , Supervised MC2 is defined as follows .
Many rank aggregation methods are in fact based on majority voting . Borda Count [ 2][7][27 ] is such a method and the major assumption within it is that all the base rankers are equally important . As discussed above , it is more reasonable to give different weights to different rankers . In other words , we can consider using Borda Fuse minğ‘¥,ğ›¼ ,ğ‘¡ ğ‘¡ğ‘‡ğ‘¡ ğ‘™ ğ‘  . ğ‘¡ . ğ‘¥ = ğ‘¥ ğ‘˜=1 ğ‘¥ğ‘– ğ›¼ğ‘˜
ğ‘› ğ‘–=1 = 1 , ğ‘¥ğ‘– > 0 , ğ‘– = 1 , â€¦ , ğ‘› ğ‘™
ğ‘˜=1
= 1 , ğ›¼ğ‘˜ â‰¥ 0 , ğ‘˜ = 1 , â€¦ , ğ‘™
ğ‘ƒğ‘˜
ğ›¼ğ‘˜
ğ‘‡
ğ»ğ‘¥ < ğ‘¡ , ğ‘¡ â‰¥ 0
( 322 )
ğ‘¥ = ğ›¹ ğœ1 , â‹¯ , ğœğ‘™ =
ğ‘™ ğ‘˜=1
ğ›¼ğ‘˜
ğ‘¥(ğ‘˜ )
Note that Borda Fuse contains Borda Count as its special case .
With the optimization framework in ( 312 ) , we can define Supervised Borda Fuse . Specifically we formalize it as the following optimization problem : minğ‘¥,ğ›¼,ğ‘¡ ğ‘¡ğ‘‡ğ‘¡ ğ‘™ ğ‘¥(ğ‘˜ ) ğ‘  . ğ‘¡ . ğ‘¥ = ğ‘˜=1 ğ›¼ğ‘˜
ğ›¼ğ‘˜ = 1 , ğ›¼ğ‘˜ â‰¥ 0 , ğ‘˜ = 1 , â€¦ , ğ‘™
ğ‘™
ğ‘˜=1
ğ»ğ‘¥ < ğ‘¡ , ğ‘¡ â‰¥ 0 where ğ‘¥(ğ‘˜ ) is the same as that in ( 311 ) Note that the parameter vector is comprised of weights of ranking lists and is to be optimized as well .
( 2 ) Markov Chain based methods
Similarly , we can construct the supervised versions of MC1 , MC3 , and MC4 . The only differences lie in the structures of the transition probability matrices . a )
Supervised MC1 :
The transition matrix of MC1 can be written as
ğ‘ƒ = ğ‘‘ğ‘–ğ‘ğ‘”
1
, â‹¯ ,
ğ‘1ğ‘—
1
ğ‘›
ğ‘— =1
ğ‘ğ‘›ğ‘—
ğ‘„
ğ‘›
ğ‘— =1 where ğ‘„ â‰œ ğ‘ğ‘–ğ‘—
=
1
ğ‘™
ğ‘›Ã—ğ‘›
ğ‘™
ğ‘˜=1
ğ‘„ğ‘˜
, ğ‘„ğ‘˜ â‰œ ğ‘ğ‘–ğ‘—
( ğ‘˜ ) with
ğ‘›Ã—ğ‘›
( ğ‘˜ ) = ğ‘ğ‘–ğ‘—
1 , ğ‘— >ğœğ‘˜ ğ‘– or ğ‘— = ğ‘– 0 , otherwise
.
We can derive Supervised MC1 by assigning weighting coefficients to ğ‘„ , and obtain the following optimization problem .
WWW 2007 / Track : SearchSession : Search Quality and Precision483 minğ‘¥,ğ›¼ ,ğ‘¡ ğ‘¡ğ‘‡ğ‘¡
ğ‘™ ğ‘  . ğ‘¡ . ğ‘¥ = ğ‘˜=1
ğ›¼ğ‘˜
ğ‘„ğ‘˜
ğ‘‘ğ‘–ğ‘ğ‘”
ğ‘‡
1
ğ‘›
ğ‘— =1
, â‹¯ ,
1
ğ‘›
ğ‘— =1
ğ‘ğ‘›ğ‘— ğ‘› ğ‘¥ğ‘– ğ‘–=1 = 1 , ğ‘¥ğ‘– > 0 , ğ‘– = 1 , â€¦ , ğ‘› ğ‘™
ğ›¼ğ‘˜ = 1 , ğ›¼ğ‘˜ â‰¥ 0 , ğ‘˜ = 1 , â€¦ , ğ‘™ ğ‘˜=1 ğ»ğ‘¥ < ğ‘¡ , ğ‘¡ â‰¥ 0
ğ‘1ğ‘—
ğ‘¥ b )
Supervised MC3 :
The formulation of MC3 is similar to that of MC2 , except the definition of ğ‘ğ‘–ğ‘—
ğ‘˜
:
ğ‘˜ = ğ‘ğ‘–ğ‘—
1
ğ‘› âˆ’ğ‘š
ğ‘› , ğ‘— >ğœğ‘˜ ğ‘– ğ‘› , ğ‘— = ğ‘– 0 , otherwise
, and ğ‘š = # ğ‘—| ğ‘— >ğœğ‘˜ ğ‘– .
Therefore , we can define Supervised MC3 in a similar way as we define Supervised MC2 . c )
Supervised MC4 :
MC4 is similar to MC1 , except that the following two facts differ .
( i )
The definition of ğ‘„ : ğ‘„ â‰œ ğ‘ğ‘–ğ‘— with ğ‘ğ‘–ğ‘—
( ğ‘˜ ) =
1 , ğ‘— >ğœğ‘˜ ğ‘– . 0 , otherwise
=
1
ğ‘™
ğ‘›Ã—ğ‘›
ğ‘™
ğ‘˜=1
ğ‘„ğ‘˜
,
( ii )
The definition of P : ğ‘ƒ â‰œ ğ‘ğ‘–ğ‘—
, with
ğ‘›Ã—ğ‘›
ğ‘˜ = ğ‘ğ‘–ğ‘—
1
ğ‘› âˆ’ğ‘š
ğ‘› , ğ‘ğ‘–ğ‘— > 1 2 ğ‘› , ğ‘— = ğ‘– 0 , otherwise
, and ğ‘š = # ğ‘—|ğ‘ğ‘–ğ‘— > 1
2 .
Therefore , we can obtain Supervised MC4 , similar to Supervised MC1 .
In summary , with the use of the optimization framework , we can introduce new supervised aggregation methods , corresponding to most of the existing unsupervised rank aggregation methods . The key factor is that weights are assigned to the ranking lists and they are also trained within the optimization framework .
The question next is how to conduct the optimizations . For some forms of function ğ›¹ in ( 312 ) , the optimization is hard to solve , such as those in the Markov chain based methods . We know of no existing optimization techniques which can be straightforwardly applied , because they are not convex optimization problems . In our work we are able to find an optimization solution for Supervised MC2 on the basis of Semidefinite Programming ( SDP ) , as will be explained below .
4 . AN OPTIMIZATION SOLUTION In this section , we describe our solution to the optimization problem for Supervised MC2 as in ( 322 ) We think that similar techniques can also be applied to other Markov Chain based methods , but leave it as future work .
Our method for Supervised MC2 consists of three steps :
1 ) We modify the objective and constraints in ( 322 ) to make the feasible region convex .
2 ) We further transform the optimization problem into a quadratic optimization problem by employing the bound optimization technique .
3 ) Finally , we transform the quadratic optimization problem into a Semidefinite Programming problem .
Let us elaborate on the three steps in more details . Theoretical justifications of the transformations are given in a lemma and a proposition .
The first constraint in ( 322 ) represents an eigenvector problem . One can easily verify that the feasible region of the optimization problem is not convex . In general such a problem is hard to solve . We reformulate the original optimization problem by putting the first constraint into the objective function :
ğ‘™ minğ‘¥,ğ›¼ ,ğ‘¡ ğ‘¡ğ‘‡ğ‘¡ + ğ‘˜=1
ğ›¼ğ‘˜ ğ‘ƒğ‘˜
ğ‘‡
ğ‘¥ âˆ’ ğ‘¥ 1
ğ‘› ğ‘–=1 = 1 , ğ‘¥ğ‘– > 0 , ğ‘– = 1 , â€¦ , ğ‘› ğ‘™
ğ‘˜=1
ğ‘  . ğ‘¡ . ğ‘¥ğ‘– ğ›¼ğ‘˜
= 1 , ğ›¼ğ‘˜ â‰¥ 0 , ğ‘˜ = 1 , â€¦ , ğ‘™
( 4.1 )
ğ»ğ‘¥ < ğ‘¡ , ğ‘¡ â‰¥ 0 where âˆ™ 1denote the â„“1 norm of a vector . Then , the feasible region becomes convex and the objective function becomes one consisting of two parts . The first part ğ‘¡ğ‘‡ğ‘¡ corresponds second part ğ‘™
ğ›¼ğ‘˜ ğ‘ƒğ‘˜ ğ‘˜=1 corresponds to an approximation of the training errors , and the to ğ‘‡ ğ‘¥ âˆ’ ğ‘¥ 1 stationary distribution . The second part of the objective function is not convex . We try to minimize a differentiable and convex upper bound of it . Lemma 1 gives the upper bound using the properties of â„“1 norm .
Lemma 1 : Let ğ›¯ = ğœ‰ğ‘– ğ‘‡
ğ‘™ ğ‘–=1,â‹¯,ğ‘› = ğ‘˜=1
ğ›¼ğ‘˜ ğ‘ƒğ‘˜
ğ‘‡
ğ‘¥ âˆ’ ğ‘¥ , we have
ğœ© 1 â‰¤ 2 âˆ’ 2ğ›¼ğ‘‡ğ´ğ‘¥ , where ğ´ =
( 1 )
( 1 ) â‹¯ ğ‘ğ‘›ğ‘› ğ‘11 â‹® â‹® ( ğ‘™ ) ( ğ‘™ ) â‹¯ ğ‘ğ‘›ğ‘› ğ‘11
â‹±
.
Proof : See Appendix .
The optimization problem then becomes minğ‘¥,ğ›¼ ,ğ‘¡ ğ‘¡ğ‘‡ğ‘¡ + 2 âˆ’ 2ğ›¼ğ‘‡ğ´ğ‘¥ ğ‘› ğ‘  . ğ‘¡ . ğ‘¥ğ‘– ğ‘–=1 = 1 , ğ‘¥ğ‘– > 0 , ğ‘– = 1 , â€¦ , ğ‘› ğ‘™
ğ›¼ğ‘˜ ğ‘˜=1
= 1 , ğ›¼ğ‘˜ â‰¥ 0 , ğ‘˜ = 1 , â€¦ , ğ‘™
ğ»ğ‘¥ < ğ‘¡ , ğ‘¡ â‰¥ 0
( 4.2 )
By defining ğ›½ = ( ğ›¼1 , â€¦ , ğ›¼ğ‘™ , ğ‘¥1 , â€¦ , ğ‘¥ğ‘› , ğ‘¡1 , â€¦ , ğ‘¡ğ‘š )ğ‘‡ , where m is the number of rows in matrix H , and omitting the constant in the objective function which is irrelevant to the optimization , problem ( 4.2 ) becomes minğ›½ ğ›½ğ‘‡ğ»0ğ›½
( 4.3 ) with ğ»0 =
âˆ’ğ´ğ‘‡
âˆˆ ğ‘…(ğ‘™+ğ‘›+ğ‘š )Ã—(ğ‘™+ğ‘›+ğ‘š )
ğ»1 =
âˆˆ ğ‘… ğ‘™+ğ‘š Ã— ğ‘™+ğ‘›+ğ‘š ( 4.4 )
ğ‘  . ğ‘¡ . ğ»1ğ›½ â‰¤ 0 ğ»2ğ›½ = ğ‘’2 ğ»3ğ›½ < 0 0
âˆ’ğ´ 0 0 0 ğ¼ 0 0 0 âˆ’ğ¼ğ‘™ 0 0 0 âˆ’ğ¼ğ‘š ğ‘‡ ğ‘’ğ‘™ 0 0 ğ‘‡ 0 0 ğ‘’ğ‘› 0 âˆ’ğ¼ğ‘› 0 0 ğ» âˆ’ğ¼ğ‘š
ğ»2 =
âˆˆ ğ‘…2Ã— ğ‘™+ğ‘›+ğ‘š
ğ»3 =
âˆˆ ğ‘… ğ‘› +ğ‘š Ã— ğ‘™+ğ‘›+ğ‘š where ğ¼ğ‘– is identity matrix of size i , and ğ‘’ğ‘– is vector with size i in which all the elements are one .
The optimization in ( 4.3 ) is an optimization problem with quadratic objective function and linear constraints . The remaining issue is that the Hessian matrix H0 is not positive definite and thus the objective function is not convex . In this situation , if we employ a method like Gradient Decent , the solution will be
WWW 2007 / Track : SearchSession : Search Quality and Precision484 sensitive to the initial values , and will likely to become locally optimal . To cope with it , we further transform the optimization problem into a Semidefinite Programming ( SDP ) , with the theoretical support from Proposition 2 .
In our experiment , we used 30 ranking models ( features ) [ 22 ] as base rankers . These include term frequency , inverse document frequency , document their combinations . length , BM25 score [ 25 ] , and
ğ»0 + ğœ†0ğ·
1
2ğ‘ˆ ğ‘‡ğ»2 + ğ›¬3
1
2ğ‘ˆğ‘‡
âˆ’ğ›¬2
ğ‘‡ğ‘’2 âˆ’ 2ğœ†0 âˆ’ ğ›¾
â‰½ 0
( 4.5 )
Proposition 2 : Optimization problem ( 4.3 ) is equivalent to the following Semidefinite Programming problem , maxğœ†,ğ›¾ ğ›¾ ğ‘  . ğ‘¡ . ğœ† â‰¥ 0
Where ğ‘ˆ = ğ›¬1
ğ‘‡ğ»1 + ğ›¬2
ğ‘‡ğ»3 , and ğœ† = ( ğœ†0 , ğ›¬1
ğ‘‡ , ğ›¬2
ğ‘‡ , ğ›¬3
ğ‘‡)ğ‘‡ .
Proof : See Appendix .
Finally we can solve the optimization problem using the techniques of SDP1 , for example , the interior point method SDPA [ 30 ] proposed in [ 12 ] .
Table 1 . Results of different methods for meta search with
OHSUMED data
Supervised
P@1
P@2
P@3
P@4
P@5
P@6
P@7
P@8
P@9
P@10
MC2
0.483
0.384
0.363
0.352
0.329
0.331
0.324
0.316
0.312
0.302
MC1 MC2 MC3 MC4
BordaCount
Borda Fuse
0.337 0.376 0.357
0.308
0.349
0.349
0.324 0.345 0.316
0.294
0.310
0.320
0.290 0.325 0.280
0.289
0.310
0.306
0.284 0.312 0.285
0.275
0.280
0.295
0.266 0.312 0.268
0.280
0.276
0.292
0.265 0.298 0.265
0.276
0.265
0.272
0.269 0.296 0.263
0.276
0.266
0.264
0.269 0.292 0.263
0.274
0.265
0.265
0.266 0.287 0.260
0.270
0.264
0.261
0.264 0.288 0.255
0.267
0.254
0.258
Our Supervised MC2 algorithm can be summarized as follows .
MAP
0.302
0.275 0.286 0.271
0.269
0.267
0.272
Supervised MC2 :
Input : ranking lists ğœ1 , â‹¯ , ğœğ‘™ Output : weighting parameter Î±
Algorithm : a ) Construct base transition matrices ğ‘ƒ1 , â‹¯ , ğ‘ƒğ‘™ according to equation ( 321 ) b ) Create matrix ğ´ as shown in Lemma 1 . c ) Create matrices H0 , H1 , H2 , H3 as shown in equation ( 44 ) d ) Construct matrix U as shown in Proposition 2 . e ) Call SDP tool [ 30 ] to solve problem ( 4.5 ) and get solution Î» . f )
Compute Î² by equation ( 823 ) g ) Output the first l elements of Î² as parameter Î± .
5 . EXPERIMENTS In this section , we report the experimental results on meta search using our method based on Supervised Rank Aggregation and existing methods . Our first experiment was conducted with TREC dataset , and the second was with data from real web search engines .
5.1 TREC Data TREC datasets were used in many previous works on rank aggregation [ 2][18][19][20][24 ] , in which heuristic models were used as base rankers . This motivated us to conduct our experiments with TREC dataset as well . We selected the OHSUMED dataset used in the filtering track of TREC 2000 .
The OHSUMED dataset is a collection of 348,566 documents and 106 queries . The ground truth is provided by the TREC committee with three levels of relevance judgments : â€˜definitely relevantâ€™ , â€˜possibly relevantâ€™ , and â€˜not relevantâ€™ to the query . Based on these judgments , we can construct pairwise constraints for the training of Supervised MC2 .
1 SDP is a hot research field in recent years [ 29 ] , and many fast iterative algorithms have been developed [ 16][21][23][30 ] .
Table 2 . Results of different methods for meta search with
OHSUMED data
Supervised
N@1
N@2
N@3
N@4
N@5
N@6
N@7
N@8
N@9
N@10
MC2
0.651
0.595
0.586
0.581
0.575
0.579
0.583
0.586
0.590
0.589
MC1 MC2 MC3 MC4
BordaCount
Borda Fuse
0.534 0.573
0.553 0.516
0.544
0.544
0.530 0.553
0.533 0.513
0.525
0.531
0.517 0.546
0.515 0.513
0.526
0.525
0.519 0.546
0.518 0.512
0.514
0.523
0.514 0.549
0.514 0.517
0.518
0.525
0.517 0.548
0.518 0.520
0.515
0.520
0.523 0.554
0.521 0.527
0.522
0.522
0.530 0.558
0.527 0.533
0.525
0.528
0.536 0.562
0.532 0.539
0.530
0.533
0.541 0.568
0.534 0.542
0.531
0.537 implemented and
Next , we conducted rank aggregation using our method . For comparison , we also tested other rank aggregation methods , including MC1 , MC2 , MC3 , MC4 , Borda Count , and Borda Fuse . The experiments were performed through 4 fold cross validation . We randomly split the query set into four subsets , used the first two of them for training , the third for validation , and the fourth for testing , and rotated this process four times to create four data sets . Then we took the average performance over the four trials as the final result for each method .
We used three measures in our experiments for ranking accuracy evaluations : Precision [ 3 ] , Mean Average Precision ( MAP ) [ 3 ] and Normalized Discount Cumulative Gain ( NDCG ) [ 14][15 ] . When evaluating the performances in terms of precision , we regarded both â€˜definitely relevantâ€™ and â€˜possible relevantâ€™ as positive , and â€˜not relevantâ€™ as negative .
Table 1 shows the results in terms of precision at n ( P@n ) and MAP , and Table 2 shows the results in terms of DNCG at n ( N@n ) for all the methods .
From the results , we can see that Supervised MC2 outperforms all the other methods , suggesting that it is better to employ Supervised Rank Aggregation proposed in this paper .
WWW 2007 / Track : SearchSession : Search Quality and Precision485 5.2 Web Search Data We also tried to apply the rank aggregation methods directly to meta search on the web .
521 Experimental Results We randomly sampled 500 queries from the query log of a commercial search engine , as query set . Table 3 shows some example queries .
Table 3 . Sample queries used in meta search
Queries
Altavista , Astronomy Picture of the day , BBC , cadillac , daily nation , delta dental , family guy , fox theater , Google , group health , habitat for humanity , hotmail , Image Entertainment , imdb , jacksonville news , jetblue , kofax , laredo morning times , liberty university , michael Jordan , Microsoft , national zoo , NCAA football , ohio department of education , philips , prime outlets , southern baptist convention , Superbowl , tacoma news tribune , texas department of public safety , Tuesday Morning , ucla , university of Tennessee , venetian , etc .
Table 4 . Results of different methods for meta search with data from web search engines
Supervised
P@1
P@2
P@3
P@4
P@5
P@6
P@7
P@8
P@9
P@10
MC2
0.864
0.692
0.620
0.560
0.525
0.496
0.471
0.447
0.426
0.407
MC1 MC2 MC3 MC4
BordaCount
Borda Fuse
0.738 0.837 0.734 0.818 0.712
0.709
0.606 0.664 0.611 0.668 0.579
0.581
0.547 0.575 0.550 0.586 0.505
0.501
0.505 0.520 0.502 0.529 0.457
0.544
0.466 0.476 0.469 0.484 0.426
0.423
0.436 0.446 0.434 0.446 0.394
0.394
0.408 0.419 0.409 0.413 0.372
0.370
0.387 0.400 0.388 0.385 0.349
0.350
0.367 0.383 0.367 0.359 0.332
0.332
0.350 0.366 0.352 0.340 0.315
0.313
MAP
0.410
0.333 0.374 0.333 0.340 0.296
0.292
Next , we submitted the queries to six commercial web search engines , and collected the top 100 ranking lists of the queries returned by the search engines . We combined the results together and eliminated the duplicate pages . On average there were 362 unique pages per query . The overlap among the ranking lists of the search engines was small : there were on average 4 pages per query occurring in all the ranking lists . Then we asked human annotators to make relevance judgments on the pages . The relevance judgments were binary : relevant or irrelevant . Three annotators made judgments , and majority voting was finally conducted on the results .
We then conducted meta search on the data through 4 fold cross validation ( in the same way as in Section 51 We applied our proposed method , and used MC1 , MC2 , MC3 , MC4 , Borda Count , and Borda Fuse as baselines . Table 4 shows the results in terms of P@n and MAP . From the results , we can see that our proposed method achieves the best results in terms of both MAP and P@n . Again this verifies the effectiveness of our proposed method for rank aggregation .
Table 5 shows the experiment results in terms of NDCG@n .
Table 5 . Results of different methods for meta search with
OHSUMED data
Supervised
N@1
N@2
N@3
N@4
N@5
N@6
N@7
N@8
N@9
MC2
0.741
0.631
0.605
0.592
0.585
0.582
0.579
0.575
0.572
MC1 MC2 MC3 MC4
BordaCount
Borda Fuse
0.554
0.722
0.552
0.690
0.533
0.531
0.495
0.609
0.485
0.589
0.459
0.460
0.484
0.576
0.475
0.561
0.440
0.438
0.483
0.562
0.473
0.545
0.435
0.433
0.482
0.553
0.476
0.534
0.433
0.431
0.480
0.548
0.474
0.525
0.432
0.430
0.477
0.543
0.473
0.517
0.430
0.429
0.475
0.538
0.471
0.510
0.428
0.428
0.472
0.536
0.468
0.503
0.427
0.426
N@10
0.569
0.469
0.533
0.467
0.498
0.425
0.423
522 Discussions We ( Supervised MC2 ) outperforms the baseline methods . investigated why our proposed supervised method
Figure 1 shows MAP and the weight to each search engine assigned by our method in the first trial of the cross validation experiment . ( The results from the other trials have the same tendencies ) .
Figure 1 . MAP and weights of search engines
From Figure 1 , we have the following observations .
( a ) The weights of search engines are different from each other .
This validates the correctness of our assumption that rankers should have different weights .
( b ) The weights of search engines do not necessarily correlate with their MAP values .
Although the fourth search engine achieves the best MAP and obtains the largest weight at the same time , for the other engines , MAP and weight do not correlate . For example , the first search engine has a higher MAP than the second , but it has much smaller weight than the second . Our explanation to this is as follows . The weights of search engines not only depend on their performances , but also depend on the correlations among search engines . If a search engine highly correlates to the others , its weight ( influence ) will be reduced within the general optimization framework .
To verify the correctness of this explanation , we calculate the correlation coefficient between each pair of the six engines using the following formula , and present the results in Table 6 . ( Note that the correlation is symmetric . )
0005010150202503035SE1SE2SE3SE4SE5SE6MAPWeightWWW 2007 / Track : SearchSession : Search Quality and Precision486 cor SEğ‘– , SEğ‘— =
1
# query query
# ğ‘¢,ğ‘£ ğ‘¢>SE ğ‘–ğ‘£ and ğ‘¢>SE ğ‘— ğ‘£ or ( ğ‘¢ <SE ğ‘–ğ‘£ and ğ‘¢<SE ğ‘— ğ‘£ )
# ğ‘¢ ,ğ‘£ ğ‘¢ ,ğ‘£âˆˆSE ğ‘– and ( ğ‘¢,ğ‘£âˆˆSE ğ‘— ) where SEi denotes the i th search engine , ğ‘¢ >SE ğ‘– ğ‘£ means that document u is ranked higher than v by SEi for a given query , and ğ‘¢ , ğ‘£ âˆˆ SEğ‘– means that documents u and v are returned by search engine SEi .
SE1 SE2 SE3 SE4 SE5 SE6
Table 6 . Correlation among search engines
SE1
1
SE2 0.502
1
SE3 0.524 0.335
1
SE4 0.681 0.437 0.661
1
SE5 0.615 0.433 0.530 0.617
1
SE6 0.675 0.502 0.597 0.696 0.613
1
From Table 6 , we can see that the first search engine highly correlates to the forth and the sixth search engines , and therefore its weight is suppressed by the large weights of the two engines . In contrast , the second search engine only weakly correlates to the other engines , and thus it retains a large weight .
The observation can also give explanation to other results in the experiments . From Table 5 , one may see an interesting phenomenon . Borda Fuse , as a supervised method , performs even worse than the unsupervised methods . As explained , Borda Fuse assumes that the weight of each base ranker only depends on its accuracy , and it neglects the correlation among base rankers . It seems that this is not appropriate anyway . Therefore , it appears better to perform rank aggregation using an optimization framework as we do .
6 . CONCLUSIONS
In this paper , we have proposed a new approach to rank aggregation : Supervised Rank Aggregation . Our method is mainly designed for meta search and is unique in that ( a ) takes order information from base rankers , ( b ) it makes use of labeled training data , and ( c ) it trains the final ranking function within a single optimization framework . We have set up a general framework for employing the approach . Specifically , we have formalized the learning problem as that of optimization . We propose an efficient algorithm to solve the optimization for one of the typical rank aggregation settings , namely the Markov chain based method . We have compared the performances of our proposed method with those of existing methods on meta search . The results show that the proposed method can outperform the existing methods . The contributions of this paper include
1 ) proposal on employing the supervised learning approach for rank aggregation ;
2 ) formulation of the supervised learning approach as an optimization problem ;
3 ) development of an optimization algorithm form the Markov
Chain based learning method ; and
4 ) empirical verification of the effectiveness of the proposed approach .
As future work , we plan to apply the techniques used in this paper to other supervised learning methods , and to apply the methods to other applications such as similarity search and genome informatics .
ACKNOWLEDGEMENTS
The authors would like to thank Wei Ying Ma at MSRA for his suggestions and comments on this work . They are also grateful to Shisheng Li at USTC for his helps in the experiments . The authors would also like to thank the anonymous reviewers for their valuable comments on the paper .
REFERENCES [ 1 ] Ahmad N . and Beg M . M . S . Fuzzy Logic Based Rank Aggregation Methods for the World Wide Web , In Proceedings of the International Conference on Artificial Intelligence in Engineering and Technology , Malaysia , 2002 , 363 368 .
[ 2 ] Aslam , J . A . and Montague , M . Models for Metasearch . In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval . ACM Press , New York , 2001 , 276 284 .
[ 3 ] Baeza Yates , R . and Ribeiro Neto , B . Modern Information
Retrieval . Addison Wesley , 1999 .
[ 4 ] Beg , M . M . S . Parallel Rank Aggregation for the World Wide Web . World Wide Web . Kluwer Academic Publishers , vol 6 , issue 1 , 5 22 . March 2004 .
[ 5 ] Borda , J . C . MÃ©moire sur les Ã©lections au scrutin . Histoire de lâ€™AcadÂ´emie Royale des Sciences , 1781
[ 6 ] Boyd , S . and Vendenberghe , L . Convex Optimization .
Cambridge , U . K . Cambridge Univ . Press 2003 .
[ 7 ] Dwork , C . , Kumar , R . , Naor , M . , and Sivakumar , D . Rank Aggregation Methods for the Web . In Proceedings of the 10th International World Wide Web Conference . 2001 , 613622 .
[ 8 ] Dwork , C . , Kumar , R . , Naor , M . , and Sivakumar , D . Rank
Aggregation revisited . 2001 . Manuscript .
[ 9 ] Fagin , R . , Kumar , R . , and Sivakumar , D . Efficient Similarity Search and Classification via Rank Aggregation . In Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data . San Diego , 2003 , 301312 .
[ 10 ] Fagin , R . , Lotem , A . , and Naor , M . Optimal Aggregation Algorithm for Middleware . In Proceedings of the Twentieth ACM SIGMOD SIGACT SIGART Symposium on Principles of Database Systems . Santa Barbara , California , United States , 2001 , 102 113 .
[ 11 ] Fox , E . A . and Shaw , J . A . Combination of Multiple Searches . In Proceedings of the Second Text Retrieval Conference , 1994 .
[ 12 ] Fujisawa , K . , Fukuda , M . , Kojima , M . , and Nakata , K . Numerical Evaluation of ( SemiDefinite Programming Algorithm ) . High Performance Optimization , Kluwer Academic Press , 267 301 , 2000 . the SDPA
[ 13 ] Hull , D . A . , Pedersen , J . O . , and SchÃ¼tze , H . Method Combination for Document Filtering . In Proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval . ACM Press , 1996 , 279 287 .
WWW 2007 / Track : SearchSession : Search Quality and Precision487 [ 14 ] Jarvelin , K.and Kekalainen , J . IR Evaluation Methods for Retrieving Highly Relevant Documents . In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval . ACM Press , 2000 , 41 48 .
[ 15 ] Jarvelin , K and Kekalainen , J . Cumulated Gain Based Evaluation of IR Techniques . ACM Transactions on Information Systems , 2002 .
[ 16 ] Klerk , E . Aspects of Semidefinite Programming : Interior Point Algorithms and Selected Applications . Applied Optimization Series , Volume 65 . Kluwer Academic Publishers , March 2002 , 300 pp .
[ 17 ] Lam , K . W . and Leung , C . H . Rank Aggregation for Metasearch Engines . In Proceedings of the 13th International World Wide Web Conference . 2004 .
[ 18 ] Manmatha , R , Rath , T . , and Feng , F . Modeling Score Distributions for Combining the Outputs of Search Engines . In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval . ACM Press , 2001 , New York .
[ 19 ] Manmatha , R . and Sever , H . A Formal Approach to Score Normalization for Meta search . In Proceedings of HLTâ€™02 , 2002 , 88 93 .
[ 20 ] Montague , M . and Aslam , J . A . Relevance Score Normalization for Meta search . In Proceedings of the 10th Conference on Information and Knowledge Management . Atlanta , GA , 2001 , 427 433 .
[ 21 ] Monteiro , R . D . C . First and Second Order Methods for
Semidefinite Programming . Georgia Tech , January 2003 .
[ 22 ] Nallapati , R . Discriminative Models
Information Retrieval . In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information retrieval . ACM Press , 2004 , 64 71 . for
[ 23 ] Pardalos , PM and Wolkowicz , H . Topics in Semidefinite and Interior Point Methods . Fields Institute Communications 18 , AMS , Providence , Rhode Island , 1998 .
[ 24 ] Randa , M . E . and Straccia , U . Web metasearch : Rank vs . Score based Rank Aggregation Methods . In Proceedings of the 2003 ACM Symposium on Applied Computing , March 0912 , 2003 , Melbourne , Florida .
[ 25 ] Robertson , S . E . Overview of the Okapi Projects . Journal of
Documentation , Vol . 53 , No . 1 , 1997 , pp . 3 7 .
APPENDIX
Proof of Lemma 1
Lemma 1 : Let ğ›¯ = ğœ‰ğ‘– ğ‘‡
ğ‘™ ğ‘–=1,â‹¯,ğ‘› = ğ‘˜=1
ğ›¼ğ‘˜ ğ‘ƒğ‘˜
ğ‘‡
ğ‘¥ âˆ’ ğ‘¥ , we have
ğœ© 1 â‰¤ 2 âˆ’ 2ğ›¼ğ‘‡ğ´ğ‘¥ , where ğ´ =
( 1 )
( 1 ) â‹¯ ğ‘ğ‘›ğ‘› ğ‘11 â‹® â‹® ( ğ‘™ ) ( ğ‘™ ) â‹¯ ğ‘ğ‘›ğ‘› ğ‘11
â‹±
.
Proof :
ğ‘™ Define ğ‘ƒ = ğ‘˜=1 ( ğ‘˜ ) ğ›¼ğ‘˜ ğ‘ğ‘—ğ‘–
ğ‘™ ğ‘“ğ‘–ğ‘— = ğ‘˜=1 rewrite Î as and ğ¹ = ğ‘“ğ‘–ğ‘—
= ğ‘ƒğ‘‡ . It is clear that ğ›¼ğ‘˜ ğ‘ƒğ‘˜ . Using fi to denote the ith row of F , we can
ğ‘›Ã—ğ‘›
ğ›¯ = ğœ‰ğ‘– ğ‘‡
ğ‘–=1,â‹¯,ğ‘› = ğ¹ğ‘¥ âˆ’ ğ‘¥ or ,
ğ‘› ğœ‰ğ‘– = ğ‘“ğ‘– ğ‘¥ âˆ’ ğ‘¥ğ‘– = ğ‘“ğ‘–ğ‘– âˆ’ 1 ğ‘¥ğ‘– + ğ‘— =1,ğ‘— â‰ ğ‘–
ğ‘“ğ‘–ğ‘— ğ‘¥ğ‘—
( 811 )
Because the transition probability matrix has identical rows , for the right hand side of equation ( 811 ) , ğ‘“ğ‘–ğ‘– âˆ’ 1 ğ‘¥ğ‘– is non positive and the others are non negative . Therefore , we can get an upper bound of |ğœ‰ğ‘–| as follows by using the properties of â„“1 norm ğ‘› |ğœ‰ğ‘–| â‰¤ 1 âˆ’ ğ‘“ğ‘–ğ‘– ğ‘¥ğ‘– + ğ‘— =1,ğ‘— â‰ ğ‘–
ğ‘› = 1 âˆ’ 2ğ‘“ğ‘–ğ‘– ğ‘¥ğ‘– + ğ‘— =1
ğ‘“ğ‘–ğ‘— ğ‘¥ğ‘—
ğ‘“ğ‘–ğ‘— ğ‘¥ğ‘—
Applying the result to each element in Î yields
ğœ© 1 â‰¤ 1 âˆ’ 2ğ‘“ğ‘–ğ‘– ğ‘¥ğ‘–
ğ‘› + ğ‘— =1
ğ‘“ğ‘–ğ‘— ğ‘¥ğ‘—
ğ‘› ğ‘› ğ‘–=1 ğ‘–=1 ğ‘› ğ‘–=1 = 1 and ğ‘“ğ‘–ğ‘— ğ‘¥ğ‘— ğ‘“ğ‘–ğ‘—
ğ‘› = ğ‘— =1
ğ‘› ğ‘–=1
ğ‘› ğ‘–=1
ğ‘“ğ‘–ğ‘— ğ‘¥ğ‘—
= 1 , we obtain
ğ‘› = ğ‘— =1 = 1
ğ‘¥ğ‘—
Considering that ğ‘¥ğ‘–
ğ‘›
ğ‘— =1
ğ‘› ğ‘–=1 and
ğ‘› ğ‘–=1
1 âˆ’ 2ğ‘“ğ‘–ğ‘– ğ‘¥ğ‘–
= ğ‘¥ğ‘– ğ‘™ If further considering ğ‘“ğ‘–ğ‘– = ğ‘˜=1
( ğ‘˜ )
ğ›¼ğ‘˜ ğ‘ğ‘–ğ‘–
, we obtain
ğ‘› ğ‘–=1 âˆ’ 2 ğ‘“ğ‘–ğ‘– ğ‘¥ğ‘–
ğ‘› ğ‘–=1
= 1 âˆ’ 2 ğ‘“ğ‘–ğ‘– ğ‘¥ğ‘–
ğ‘› ğ‘–=1
ğœ© 1 â‰¤ 2 âˆ’ 2 ğ‘“ğ‘–ğ‘– ğ‘¥ğ‘–
ğ‘› ğ‘–=1
ğ‘™ = 2 âˆ’ 2 ğ‘˜=1
ğ‘› ğ‘–=1
( ğ‘˜ )
ğ›¼ğ‘˜ ğ‘ğ‘–ğ‘–
ğ‘¥ğ‘–
By using matrix form to represent this inequality , we eventually have
ğœ© 1 â‰¤ 2 âˆ’ 2ğ›¼ğ‘‡
( 1 )
( 1 ) â‹¯ ğ‘ğ‘›ğ‘› ğ‘11 â‹® â‹® ( ğ‘™ ) ( ğ‘™ ) â‹¯ ğ‘ğ‘›ğ‘› ğ‘11
â‹±
ğ‘¥ = 2 âˆ’ 2ğ›¼ğ‘‡ğ´ğ‘¥ .
[ 26 ] Sese , J . and Morishita , S . Rank Aggregation Method for Biological Databases . Genome Informatics , 12 : 506 507 , 2001 .
[ 27 ] Van Erp M . and Schomaker , L . Variants of the Borda Count Method for Combining Ranked Classifier Hypotheses . In Proceedings of the 7th International Workshop on Frontiers in Handwriting Recognition . Amsterdam , 2000 , 443 452 .
[ 28 ] Vogt , C . and Cottrel , G . W . Fusion via a Linear Combination of Scores . Information Retrieval , v.1 n.3 , p.151 173 , October 1999 .
[ 29 ] Semidefinite Programming . http://www usertu chemnitzde
/~helmberg/semidefhtml
[ 30 ] SDPA Online for Your Future . http://gridrdendaiacjp/sdpa/
.
Proof of Proposition 2
Proposition 2 : The optimization problem in ( 4.3 ) is equivalent to the following Semidefinite Programming problem , maxğœ†,Î³ Î³ ğ‘  . ğ‘¡ . ğœ† â‰¥ 0
1
2ğ‘ˆğ‘‡
âˆ’ğ›¬2
ğ‘‡ğ‘’2 âˆ’ 2ğœ†0 âˆ’ ğ›¾
â‰½ 0
( 821 )
ğ»0 + ğœ†0ğ·
1
2ğ‘ˆ ğ‘‡ğ»1 + ğ›¬2 where ğ‘ˆ = ğ›¬1
ğ‘‡ğ»2 + ğ›¬3
ğ‘‡ğ»3 , and ğœ† = ( ğœ†0 , ğ›¬1
ğ‘‡ , ğ›¬2
ğ‘‡ , ğ›¬3
ğ‘‡)ğ‘‡ .
Proof :
Considering that ğ›½ = ( ğ›¼1 , â€¦ , ğ›¼ğ‘™ , ğ‘¥1 , â€¦ , ğ‘¥ğ‘› , ğ‘¡1 , â€¦ , ğ‘¡ğ‘š )ğ‘‡ , we always have
ğ‘™ ğ›½ğ‘‡ğ·ğ›½ = ğ›¼ğ‘‡ğ›¼ + ğ‘¥ğ‘‡ğ‘¥ â‰¤ ğ‘˜ =1
ğ›¼ğ‘˜
2
+ ğ‘¥ğ‘–
ğ‘› ğ‘–=1
2 = 2 where ğ· = ğ‘‘ğ‘–ğ‘ğ‘”(ğ‘’ğ‘™+ğ‘›
ğ‘‡ , 0ğ‘š
ğ‘‡ ) .
WWW 2007 / Track : SearchSession : Search Quality and Precision488 It is clear that if we add this redundant constraint to the optimization problem ( 4.3 ) , its optimal solution will not change because the feasible region has not changed . In this way , we can transform the optimization problem ( 4.3 ) into the following quadratically constrained quadratic optimization ( QCQP ) problem . minğ›½ ğ›½ğ‘‡ğ»0ğ›½ ğ‘  . ğ‘¡ . ğ›½ğ‘‡ğ·ğ›½ â‰¤ 2
( 822 )
ğ»1ğ›½ â‰¤ 0 ğ»2ğ›½ = ğ‘’2 ğ»3ğ›½ < 0
The Lagrangian of ( 822 ) is
ğ¿ ğœ† , ğ›½
ğ‘‡ğ»1ğ›½ + ğ›¬2
ğ‘‡(ğ»2ğ›½ âˆ’ ğ‘’2 ) + ğ›¬3
= ğ›½ğ‘‡ğ»0ğ›½ + ğœ†0 ğ›½ğ‘‡ğ·ğ›½ âˆ’ 2 + ğ›¬1 = ğ›½ğ‘‡(ğ»0 + ğœ†0ğ·)ğ›½ + ğ‘ˆğ›½ âˆ’ ğ›¬2 where ğ›¬1 = ğœ†1 , ğœ†2 , â€¦ , ğœ†ğ‘™+ğ‘š ğ‘‡ , ğ›¬2 = ğœ†ğ‘™+ğ‘š +1 , ğœ†ğ‘™+ğ‘š +2 ğ‘‡ , ğ‘‡ , ğ›¬3 ğ›¬3 = ğœ†ğ‘™+ğ‘š +3 , ğœ†ğ‘™+ğ‘š +4 , â€¦ , ğœ†ğ‘™+ğ‘›+2ğ‘š +2 ğ‘‡ , ğœ† = ( ğœ†0 , ğ›¬1 and ğ‘ˆ = ğ›¬1
ğ‘‡ğ‘’2 âˆ’ 2ğœ†0
ğ‘‡ğ»3 .
ğ‘‡ğ»1 + ğ›¬2
ğ‘‡ğ»2 + ğ›¬3
ğ‘‡ , ğ›¬2
ğ‘‡)ğ‘‡ ,
ğ‘‡ğ»3ğ›½ to Î² exists , one can
According to the optimization theory [ 6 ] , if the infimum of ğ¿ ğœ† , ğ›½ with respect the minimization of the objective function in the primal problem the dual function ğ‘” ğœ† = ( 822 ) infğ›½ ğ¿ ğœ† , ğ›½ . The condition for this transformation is existence of the infimum of ğ¿ ğœ† , ğ›½ . We will discuss this condition in the following three cases . the maximization of transform to
1 )
If ( ğ»0 + ğœ†0ğ· ) is positive definite , then function ğ¿ ğœ† , ğ›½ is a convex quadratic function of Î² . Therefore , we can find the infimum from the optimality condition :
âˆ‡ğ›½ ğ¿ ğœ† , ğ›½ = 2(ğ»0 + ğœ†0ğ·)ğ›½ + ğ‘ˆğ‘‡ = 0 which yields
ğ›½ = âˆ’
1
2
( ğ»0 + ğœ†0ğ·)âˆ’1ğ‘ˆğ‘‡ ( 823 )
If ( ğ»0 + ğœ†0ğ· ) is strict positive semidefinite , using the pseudo inverse in [ 6 ] , we can get a relaxation on the above condition . That is if ğ‘ˆ âˆˆ ğ‘Ÿğ‘ğ‘›(ğ»0 + ğœ†0ğ· ) , we can get the dual function as follows2 ,
ğ‘” ğœ† = âˆ’ğ›¬2
ğ‘‡ğ‘’2 âˆ’ 2ğœ†0 âˆ’
1 4
ğ‘ˆ(ğ»0 + ğœ†0ğ·)â€ ğ‘ˆğ‘‡
2 ) Otherwise function ğ¿ ğœ† , ğ›½ has no lower bound , thus the problem ( 822 ) has no solution .
With the above discussions , we conclude if and only if ( ğ»0 + ğœ†0ğ· ) â‰½ 0 , ğ¿ ğœ† , ğ›½ has an infimum and the corresponding optimization problem ( 822 ) can be transformed to its dual problem3 . As a result , we can solve the dual problem in ( 824 ) and get the solution for ( 822 ) maxğœ† ğ‘”(ğœ† ) ğ‘  . ğ‘¡ . ğœ† â‰¥ 0
( 824 )
( ğ»0 + ğœ†0ğ· ) â‰½ 0
One can also find that ğ‘” ğœ† is the Schur complement [ 6 ] of
( ğ»0 + ğœ†0ğ· ) in the matrix
ğ»0 + ğœ†0ğ·
1
2ğ‘ˆ
1
2ğ‘ˆğ‘‡ ğ‘‡ğ‘’2 âˆ’ 2ğœ†0
âˆ’ğ›¬2
. In this situation , ( 824 ) can be further formulated as a Semidefinite Programming ( SDP ) problem with respect to variables ğ›¾ and ğœ† . maxğœ†,Î³ ğ›¾ ğ‘  . ğ‘¡ . ğœ† â‰¥ 0
( 825 )
ğ»0 + ğœ†0ğ·
1
2ğ‘ˆğ‘‡
1
2ğ‘ˆ
âˆ’ğ›¬2
ğ‘‡ğ‘’2 âˆ’ 2ğœ†0 âˆ’ ğ›¾
â‰½ 0
For problem ( 822 ) , there exists a Î² that makes the following two inequalities true : ğ›½ğ‘‡ğ·ğ›½ < 2 , ğ»1ğ›½ < 0 , and ğ»3ğ›½ < 0 . That is , problem ( 822 ) is strictly feasible , and thus the optimal values of ( 822 ) and its Lagrange dual problem ( 824 ) are equivalent [ 6 ] .
Recall that optimization problem ( 4.3 ) and ( 822 ) are equivalent ; according to the strong duality theorem [ 6 ] , problem ( 4.3 ) is equivalent to the SDP problem ( 825 )
Accordingly , we get the dual function
ğ‘” ğœ† = âˆ’ğ›¬2
ğ‘‡ğ‘’2 âˆ’ 2ğœ†0 âˆ’
1 4
ğ‘ˆ(ğ»0 + ğœ†0ğ·)âˆ’1ğ‘ˆğ‘‡ which is a concave quadratic function of
.
2 ğ‘€â€  is the pseudo inverse of matrix M [ 6 ] . 3 ğ‘€ â‰½ 0 means that matrix M is semi positive definite [ 6 ] .
ï¬WWW 2007 / Track : SearchSession : Search Quality and Precision489
