From : KDD 96 Proceedings . Copyright © 1996 , AAAI ( wwwaaaiorg ) All rights reserved .
A Comparison of Approaches For Maximizing Business Payoff of Prediction Models
Brij Masand and Gregory Piatetsky Shapiro
GTE Laboratories ,
40 Sylvan Rd . Waltham MA 02254 , USA email : brij@gte.com , gps@gte.com
Abstract
In many database marketing applications the goal is to predict the customer behavior based on their previous actions . A usual approach is to develop models which maximize accuracy on the training and test sets and then apply these models on the unseen data . We show that in order to maximize business payoffs , accuracy optimization is insufficient by itself , and explore different strategies to take the customer value into account . We propose a framework for comparing payoffs of different models and use it to compare a number of different approaches for selecting the most valuable subset of customers . For the two datasets that we consider , we find that explicit use of value information during the training process and stratified modelling based on value both perform better than post processing strategies .
4 % base rate ) , giving the model a Zifc of 30/4 75 For a large customer base , even small improvements in prediction accuracy can yield large improvements in lift .
In this paper we argue that lift measure by itself is not sufficient and that we should take the customer value into account in order to determine the model payoff . Using the predicted behavior and a simple business model we estimate the payoffs from different models and examine different strategies to arrive at an optimal model that maximizes overall business value rather than just accuracy or lift .
In the rest of this paper we explain the business problem and model of business payoffs , present the main experimental hypotheses , results and conclusions .
1 Introduction
2 Motivation
The rapidly growing business databases contain much potentially valuable knowledge that could be extracted by Data Mining and Knowledge Discovery Techniques ( Piatetsky Shapiro and Frawley 1991 , Fayyad et al . 1996 ) . One of the most widespread applications of data mining is targeted database marketing , which is the use of historical customer records to predict customer behavior .
In our application , historical customer records are used to group the customers into two classes those who respond to special offers and those who don’t . Using past information collected over several months on usage of telephone services and responses to past offers , our task is to build a model for predicting the customer class in the next month and apply it to several hundred thousand customers . The prediction model is used to rank the customers according to their likelihood of response . Although the response rate for such applications is often low {eg 4% ) , and it is difficult or impossible to predict the class with high accuracy for all customers , a good model can concentrate the likely responders near the top of the list . For example , the top 5 % of the list ( sorted by the model prediction ) may contain 30 % of responders ( compared to
When using a predictive model to assign likelihood of response , typically overall accuracy or lift is maximized . To maximize business value however , we need to maximize not only prediction accuracy but identify a group of customers that are not only highly likely to respond but are going to be “ high value ” customers . How might one arrive at such a model ? Is it enough to just select the estimated high value customers from the group of predicted reponders as a post processing step or might it be beneficial to have the predictive model itself take the value into account while building the model ? We examine these questions by conducting experiments that contrast different strategies to take value into account .
3 Description of the business problem
Given billing information on customers for a given month , we want to predict the customer behavior for the next month
Prediction &I Deviation
195
Table 1 : Example data fields from Customer Billing Record ( Aug 95 ) nor mnmth yu
AA . “ IIU.* and arrive at a ranking of all subscribers for the purpose of extending offers to the top few percent from such a ranked list , The billing data ( see Table 1 ) includes such information as total amount billed , unpaid balances , use of specific ( telephone ) services , customer service calls plus information on length of service and past marketing behavior eg whether they accepted certain offers or not . The sample database we use includes information for 100,000 customers with about two hundred fields per rmntrrmnr ic LlI,tUC ” .ll ” I S ” to regard it as a classification problem and use one of the machine learning methods such as neural networks ( Rumelhart 1986 ) , decision tree induction ( Quinlan 1993 ) or nearest neighbor classifiers ( Dasarathy 1991 ) and build a predictive model that maximizes some criteria such as overall accuracy , lift in the top 5 % .
A etmvlcerl nnnrrmr;h rl ULullUuAU uyya ” u ” AA
+n thic nrnhlem L ” U ” p ” “ lrrxl
For the experiments described in this paper , we use a commercial neural network software package from Neuralware . The following steps describe how we prepare the data for modelling . 3.1 Pre processing and sampling data for analysis
Our first sample database comprises of 25,000 records from a particular market . Based on historical data , typical response rate for this application ( responding to offers ) are in the vicinity of 7 % . For the purpose of building a model we need a more concentrated representation of responders . This is because with a sparse response rate , a learning method can achieve high accuracy by always predicting everyone to be a non responder .
We create a random sample where we include about 2000 responders and then add enough non responders to make a dataset of 50 50 concentration of responders . For each one of these individuals we include information from one previous month .
We divide this dataset ( about 4000 records ) into approximately 2/3 and l/3 size training and test sets with a 50 50 concentration of responders . A separate random
196
KDD 96 service length
25.0 12.0 30.0 optional Feature Charge 4.0
6.0
2.0
1dCarrier customer service calls response to offers
280
280
280
2 0 0
0 1
0 sample of 6000 records ( with no overlap with the train , test sets ) is held aside for evaluation purposes .
A second , larger sample is drawn from a different market with 100,000 records and a much lower response rate of about 2 % . Following similar steps training and test sets of size about 4000 each are created and a non overlapping heldaside set of 24,000 records is kept separately for evaluation . 3.2 Reducing the number of data fields for model development
As reported in ( Kohavi & Sommerfield 1995 ) and ( Almuallim & Dietterich 1991 ) , reducing or eliminating irrelevant data fields can result in improved classification accuracy . We first excluded fields that have constant values as well 1 as dependent fields such as tax charges . In order to prune the data fields further , we correlated each field with the target field and arrived at a smaller list of fields per month ( about 40 ) . While in general it may not be desirable to exclude fields before the application of a learning procedure , in this case our goal is to compare different learning strategies to maximize business payoff from the models , therefore just including the “ best n ” fields still serves the purpose of contrasting different modelling strategies . 3.3 Methodology of testing
In order to estimate the accuracy and payoff of different models their ability to predict needs to be tested on a representation of “ unseen ” data . For our experiments the neural network training was done using the 50 50 train and the test sets , using the test set to prevent overtraining . Once the models were developed , they were applied to the heldaside set to compute the ranked list of subscribers and to estimate model payoffs on unseen data . c cI _ = _ the benefit of nntential na&:hifiv non resnnnflern ~yeri&s response . There is an estimated “ optimal ” number of people to call , in this case about 15 % giving an estimated optimal payoff of about $72 k for this group . If this was linearly scaled to 100,000 customers this payoff would represent about $1 .l million . ( In practice we have observed a nonlinear scaling , giving a higher lift and payoff for larger samples ) .
‘lhble 3 : Example lift table for ranked output log for 6,300 customers
I
I
I
I segment hit rate per segment
1 15 % 1 15.24
I 20 % 1 7.30
30 % t 50 % 1 70 %
80 %
90 % /ioo+4g
6.35
5.71
3.02
2.54 1.75
CUlll abs hits
CUIll % of all hits caphued
144
206
30.00 42.92 lift payoff per seg ( x loW cum payoff
( x low
6.00
61.13
61.13
4.29
8.02
69.15 1 72.29
254 1 52.92
1 3.53 1 3.14
1 277 1 57.71
1 2.89 1 0.48
1 71.81
67.08
81.67
89.79
93.12
95.42
322
392
431
447 458 ,^^ 480
1
2.24
0.15
1.63
2.08
1.28
4.54
1.16
4.63
71.40
67.97
58.99
54.36
I _^^^ 1 1uu.o
I _^ 1 1.0
1.06
5.05
I 1 3.37
49.31
I 1 45.Y4
The figure below ( based on slightly different data from the above table ) shows an optimal cutoff point from the ranked prediction log for an optimal payoff . Here the optimal point is about 11 % for a payoff of $82k .
. . . . , . ,
1 7coca I f
. . . . . . . . . . . . . . . . . . . . . . I . . . . . . . . . . . . . . . / ~C e L A . ‘ i~ 4 ; ‘ “ “ \._ 1 5 . I
198
KDD 96
4 Main _ . I exnerimental v&e ‘ = Itraining vs . post processing hvnnthecew J =
L Li hmed
Given the above definition of business payoff in this domain , we now examine different strategies for the task of optimizing the payoff . Essentially we would like to rank the subscriber base in such a way that the people who are most likely to respond are at the top of the ranked list . In addition , we would like to identify a group of customers that are likely to respond to high value offers . For example , it might be better to extend offers to a smaller group of people with a moderate likelihood of response but with the high expected revenue , rather than a group of customers that with a higher likelihood of response but a lower expected revenue .
We ask the question : Is it enough to use a predictive model to arrive at ranked list of subscribers , and then , as a post processing step select the “ high value ” customers ( as estimated from their past behavior ) or is it necessary to somehow include the notion of value in the training process itself7 Should we stratify customers into high value customers and low value customers and build different models for them ? We examine and contrast the following four approaches : 4.1 Baseline payoff calculation
Using a basic predictive model we establish a baseline payoff against which all improvements can be measured . The payoff calculations are done as explained in sections 3.4 and 35 It is important to note that what we call “ value ” ( various sources of revenue ) are present as some of the inputs , although their use is to predict a likelihood of any response at all ( a classification problem ) rather than an absolute magnitude of the response ( a regression problem ) . 4.2 Post Processing
r,l,~ Ia ” * fwh:A . \nuru nnA thn sr\t,,nl cuw a&mot,.4 “ “ LILueuu
;n n&;mnt.4 10 ruuAuaI*u
We examine a simple strategy to re rank the basic ranked list using a criteria that takes both likelihood of ~oaw.n,,~~ ‘ “ VyvAAuv UAAU UI ” from the input revenue variables ) . We use a product of the two factors as a composite rank factor . This simulates “ expected value ” even though the likelihood as estimated from the neural net prediction is not a strict probability . Thus subscribers with a high likelihood but low estimated value may not be near the top of the list because another subscriber with a moderate likelihood but high value might have a higher composite score . merging the entire output logs from the two stratified models while the last row describes the results obtained by merging only optimum subsets from the output of the two stratified models . .Tabie 6 : Dataset 2 , Comparison of Optfmum pay&& ( x %lOOO ) experiment
1 basic opt
Payoff input 25 input I y ; ; 35 1 : dev )
5.4 Comparison of optimal payoffs with best accuracy and lift
More details related to the experiments for Dataset 1 in Table 5 can be found in Table 7 where the first column :,A ^__^_ AL __I A Luiwm~ uw ~qmrrnc;nw mm mule 3 ana me remammg
IL m,ct z a r* ~ ~ + * lltble 7 : Comparkm of OptSmum payoffs vs . accuracy and lift
2 basic resorted opt payoff t 3 value mining opt payoff stratl merge opt payoff f 5 atrat2opt
58*6
59.3
5%3
54.7
53.3
54.0
58.3
58.1 55.6 merge opt payoff
5.1 Improvements using post processing
We found that re ranking based on the product of the likelihood of response and estimated revenue does not results in a significant change for Dataset 1 while for Dataset 2 there is an improvement ( 95 % confidence level for a test comparing the difference between means of two populations ) 5.2 Improvements using vaiue based training
As can be seen by comparing the third and the first row of Tables 5 and 6 , the results from the value based training are significantly better than the payoffs from the basic model and also consistently better than the post processing strategy 5.3 Improvements using training based on stratified models we expected the resuits to improve significantly using the straight merge from the stratified models but the best comparable results to value based training were obtained by merging the optimum subsets from the stratified model outputs .
200
KDD 96
. 5 basic opbnerge strat2 35
‘WW 77.22
JO.,& 36.12
34.02
2.33 columns describe parameters such as accuracy and lift . It is generally expected that high accuracy and high lift will be correlated with a high payoff model , however as cau be seen from Table 7 , the best payoffs are not correlated with in snnnisferrf . with the th.e best a_rrugq or hiphenf explmation that when we rank subscribers with just the fikelihaod of response9 there is no necessary correlation between high likelihood of response and high magnitude ( high value ) of response , Thus the strategy which achieves high lift in predicting subscribers may not have tht ? highest payoff value .
. _ _I lift This a,*
Another dimension of comparison can be the optimal percent of subscribers selected fur the optimal payoff< For a comparable payoff , a smaller set of selected subscribers woura ue prereraote .
* .*
We address the problem of identifying an optimal subset of customers with highest estimated payoff for extending offers to them , We find that for our domain , using neural network models on two datasets , different ln size and response rates , the value based training strategies and the stratified optimal merge approach outperform the simple post processing strategy based on re ranking using expected value estimates .
6,I . Discussion and AnaIysis One might ask whether the high valne stratified group ( top 25 % ) is not sufficient by itself to produce the highest payoffs . We found that there are enough moderate value *‘L+ a* , : , AL , ,:I: , , J7ct K ,_^ I . A‘ d.^ 43?&,& lGbp ” IkuP18 11 , “ lci IGllMlI1111~ 13 m ~ “ c;lI LIISLL “ IF yayuu lt ” tll the top 25 % alone cannot match the highest payoffs from the value based training .
While new m ranking factors for post processing can perhaps be discovered by methods such as Genetic Programming or manual heuristics , we show 8 definite improvement using a value based response variable for training across a range of model complexity as measured by different number of inputs . It ’s not clear yet if this approach would apply to different domains with a similar sparse response rate and value criteria ( e,g , a domain such as credit card attrition ) . 6.2 Extensions
Methods such C4.5 aud KNN classification can also be modified for value based training . We are adding bootstrap error estimates for the optimum payoffs to better assess the statistical significanoe of the relative ranking of different strategies . We are also experimenting With variations of value based training such as re ranking the output logs of the value based model and also doing value based training on stratified sets ,
7 AchnowIs?dgmeats
We would like to thank John Vittal for his support of this pfoject ,
8 References techtziques , LOS Alamkx , CA : IEEE Press .
Arimuailim W . and Dietterich T . 1991 , Learning with Many hrelevant Features . In Proceedin;gs of AAAI 9f,547 552 . Menlo Park , cpr : AAL4.l Press . I)astitfkyf 3.Y 1991 . iVeare#t Neighbor Norms : NN Pattern Clarsifiatioti Fayyad , I& Piatetsky&hapiraI G . , P . Smyth , and Uthurusamy , R . Discovery and Data Mining , Cam1996 . Advarices irr KY&~& . bridge , Iv& AAMMIT Kohavi , R . and Sommerfleld , D . 1995 . Feature Subset Selection Using the Wrapper M&ho& Gvt?rtltting and Dynamic Search Space Tepology . In Proceedings of KDD 95 : First International Conference on Knowledge Discovery and Data Mining , 192 197 . Menlo Park , CA N Plaf.et&y Shapiro , G . and Frawley , W . 1991 . Knowledge Discovery in Databases , Cambtidgt ? , MA : AAAIIMIT Press . Rumelhart , D . E . , Hinton , G , E . & Williams , R . J . 1986 . Leaming internal representations by error propagation , In Parallel Distributed Processi@ : Explorations in the microstructure of cognition . Volume I : Fotou&ations . Cambridge , MA : MIT Press/Bradford Books , pp 328 362 . QtWti , Qatos , CA : Morgan Kmffman~ j , R . 1993 . C4.4 : Prograitu for Machine Learning . Los
Press .
Prediction Es Deviation
201
