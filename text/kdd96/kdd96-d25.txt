From : KDD 96 Proceedings . Copyright © 1996 , AAAI ( wwwaaaiorg ) All rights reserved .
Extraction of Spatial Proximity
Patterns by Concept Generalization
Edwin M . Knorr and Raymond
T . Ng
Department of Computer Science University of British Columbia
Vancouver , BC , V6T 124 , Canada
{knorr,rng}@csubcca
Abstract
We study the spatial data mining problem of how to extract a special type of proximity relationship namely that of distinguishing two clusters of points based on the types of their neighbouring features . The points in the clusters may represent houses on a map , and the features may represent spatial entities such as schools , parks , golf courses , etc . Classes of features are organized into concept hierarchies . We develop algorithm GenDis which uses concept generalization to identify the distinguishing features or concepts which serve as discriminators . Furthermore , we study the issue of which discriminators axe “ better ” than others by introducing the notion of maximal discriminators , and by using a ranking system weigh maximal discriminators from different concept hierarchies . to quantitatively
Introduction
In recent years , there has been considerable research in detecting patterns hidden in data ( Agrawal et al . 1992 ; Agrawal , Imielinski , & Swami 1993 ; Borgida & Brachman 1993 ) . A reasonable and rather popular approach to spatial data mining is the use of clustering techniques to analyze the spatial distribution of data ( Ng & Han 1994 ; Ester , Kriegel , & Xu 1995 ; Zhang , Ramakrishnan , & Livny 1996 ) . While such techniques are effective and efficient in identifying spatial clusters , they do not support further analysis and discovery of the properties of the clusters . To this end , we have developed an approximate , but efficient , algorithm ( Knorr 1995 ) to discover knowledge about the clusters by analyzing the features that are in close proximity to the clusters . More specifically , given a spatial cluster GE , the algorithm finds the top L features that are closest to Cl in an aggregate sense . An aggregate notion of proximity is needed because the distribution of points in a cluster may not be uniform . For example , a particular golf course may appear in a cluster ’s top 10 list if the golf course is relatively close to many of the houses in the cluster . On the other hand , a particular shopping centre which is actually closer to the cluster ( in terms of feature boundary to cluster boundary distance ) may not appear in the top10 list if few houses are relatively close to the shopping centre . extraction
It is also important to identify common classes of features which are in close proximity to most ( or all ) of the input clusters ( Knorr & Ng 1996 ) . This nois important because , tion of commonality for example , it is often unlikely that one particular golf course is close to every cluster , even though each cluster may have some golf course close to it though not necessarily the same one . If such is the case , then a generalized statement can be made concerning the fact that the clusters tend to be near golf courses . Such statements can be useful in terms of knowledge discovery because they describe generic types of features common to multiple clusters .
While aggregate proximity relationships and commonality extraction can be quite valuable , we are also interested in determining the distinguishing features ( or classes of features ) between two clusters . For example , if an expensive housing cluster and a poor housing cluster are given as input , we may find that concepts such as “ golf courses ” , “ private schools ” , and “ social services centres ” are discriminators , in which the expensive housing cluster is close to a golf course or a private school , but the poor housing cluster is not ; and , the poor housing cluster is close to a social services centre , whereas the expensive housing cluster is not . this paper , we describe an algorithm called GenDis , which finds “ discriminating ” classes of features that serve to distinguish one cluster from another . We use concept generalization to extract the discriminators . Attribute oriented concept generalization ( Han , Cai , & Cercone 1992 ; Lu , Han , & Ooi 1993 ) has been shown to be quite useful in guiding the discovery of general patterns . The work in this paper differs from the attribute oriented approach in a number of
In
Spatial , Temporal , Csr Multimedia Data Mining
347 concepts are also listed . In our example , there are 175 educational institutions , 150 of which are grade schools . Of those 150 grade schools , 14 are private grade schools and of those 14 , 5 are exclusively for boys , 6 are exclusively for girls , and 3 are co ed ( not shown ) . For simplicity , only those concepts that are relevant to our discussion are shown namely those concepts relating to specific features which appear in at least one of the original top lc lists . Recall that a top lc list for a given cluster contains the Ic features “ nearest ” the cluster nearest in an aggregate sense .
Algorithm
GenDis : Extraction of
Maximal Discriminators and Definition
Motivation Due to limited space , we limit our discussion to the extraction of patterns for “ dissimilar ” clusters . More specifically , given two clusters as input , we aim to find discriminating features , or simply discrima’nators , that distinguish one cluster from the other .
A natural way of detecting discriminators is to use set differences on the two lists . ( This is the underlying principle used in the attribute oriented approach . ) Consider the concept hierarchies in Figures 1 and 2 , and : suppose the top /c list associated with an expensive housing cluster CZ , contains e&l , esrgl , and ppl ( plus a number of features that are found in the topk list of a poor housing cluster Cl , ) suppose the top k list associated with Cl , contains ptl and esuj , ( plus a number of features common to CL> Set differences on these two lists yield all 5 mentioned features as discriminators . The presence of es& , esrgl , or ppl distinguishes CE , from Cl, and the presence of ptl or esu j2 distinguishes Cl , from Cl , . While this approach of using set differences is easy to compute , drawing distinctions based solely on individual features can be somewhat limiting . For example , closer scrutiny of these 5 features and the clusters to which they belong reveals that private schools are close to the expensive cluster , but not to the poor cluster ; and that a public school is close to the poor cluster , but not to the expensive cluster . Thus , while es& and esujz are both educational institutions , and ppl and ptl are both parks , a key question is : how diffrom ptl ? This ferent question motivates our study of maximal discriminators , defined below . is esrbl from esujz , and ppl
Concept generalization can help answer the “ how different ” question by highlighting the differences between features . For example , the difference between esrb 1 esrb 2 esuj 1 esuj 2
Figure 1 : Educational Institutions Concept Hierarchy
Parks ( P>
75
Playgrounds ( p )
25
5
Trails ( t ) ppl
PP2 P$
Pt2
Figure 2 : Parks Concept Hierarchy ways . First , to identify discriminating patterns , we do not use set differences and thresholds . Second , our notion of maximal discriminators is unique . Finally , we introduce a way of ranking the discriminating concepts .
Concept Hierarchies
In a GIS context , we define a feature as a natural or man made place of interest . Natural features may include mountains , lakes , islands , etc . , and man made features may be schools , parks , golf courses , shopping centres , etc . We define a concept to be a class of features . Each concept is part of some hierarchy . The trees shown in Figures 1 and 2 are two concept hierarchies , to which we will refer throughout this paper . In the educational institutions hierarchy , one subclass of educational institutions ( shorthand “ e ” ) is grade schools ( “ s ” ) . Grade schools in turn are classified into private ( “ r ” ) and public ( “ u ” ) schools , which can be further sub classified into less general concepts . Specific instances of schools appear at the leaf level . The leaves are considered to be trivial concepts . We use shorthand notation to identify any node in a tree . For example , in Figure 1 , the feature esrbl is a boys’ private grade school , and the feature el is an educational institution that is not a grade school ( eg , art school , university , technical college ) . The cardinalities of the
348
Technology Spotlight esujz and esrbr is that the former is a junior high public school , whereas the latter is a boys’ private school . This observation can be obtained by generalizing esujs to esuj , and by generalizing esrbr to esrb . This leads to two questions . First , in ascending the concept hierarchy , how many levels of generalization are most appropriate ? We defer the answer to the next paragraph . Second , is this kind of highlighting by generalization always possible ? To answer this question , suppose in our example that esujz were esrbz instead . Although esrbr and esrbz are distinct entities , generalizing both features yields the same class of boys’ private schools . In effect , rather than highlighting the differences , generalization in this case underscores the similarities or the lack of differences between the features . Thus , in evaluating the differences between features , concept generalization is useful in both highlighting the differences and identifying the lack of differences , whatever the case may be .
To capture the essence of the above discussion , and to determine the appropriate number of levels of generalization , we use the notion of the smallest common ancestor of a set of nodes in a tree . More formally , if Fr , , F , are all features in the same concept hierarchy , the smallest common ancestor of Fl , . . . , FU , is the root of the smalldenoted by scu({F~ , est subtree containing FI , . . . , F , . Now , suppose F and G are two sets of features from the same concept hierarchy . We define the maximal discrime’nator of F and G , denoted by md(F , G ) , as follows :
. . . , F,} ) ,
If the subtree rooted at sea(F ) contains sea(G ) , or if the subtree rooted at sea(G ) contains sea(F ) , then md(F , G ) is NULL .
Otherwise , let F’ be the child of sca(FUG ) such that the subtree rooted at that child contains scu(F ) , and let G’ be the child of scu(F U G ) such that the subtree rooted at that child contains scu(G ) . Then , md(F , G ) is the pair ( F’ , G’ ) .
For example , consider Figure 1 and the sets F = {esrbl , esrgr ) and G = {esujr,esuj2 ) . By definition , is esr , scu(G ) is esuj , and scu(F U G ) is es . scu(F ) Furthermore , F’ is esr and G’ is esu . Thus , the maximal discriminator is ( esr , esu ) , which corresponds to private schools and public schools the observation we want , as discussed above .
Consider md({esrbl ) , as another ex{esujl,esrbz} ) is es , whose subample . This time scu({esujr , esrba} ) tree contains esrbl = scu({esrbl} ) . Thus , the maximal discriminator in this case is NULL , indicating that the sets {esrbl ) and {esujr,esrbs} are not considered to be sufficiently different .
I 2 2.1
2.2
2.3 231 2.4 241
2.5 251 252 2521 2.6 3 answer set S to empty set
Initialize For each concept hierarchy Let F be the set of this hierarchy
Let G be the set of this hierarchy
If both F and G are empty from features for one cluster features from for the other cluster goto 2.6
If either
F or G is empty add <C,nil> the root of to S , where C is the concept hierarchy else compute md(F,G ) as defined if md(F,G ) is not null add md(F.G ) to S
End for Compute and report the discriminators final the in S rankings of
Figure 3 : Algorithm GenDis for Extracting Maximal
One may wonder what the word “ maximal ” in maximal discriminator means . It is used to describe the situation in which the sets F and G are generalized to the fullest possible extent . Any further generalization will render the sets identical ( corresponding to scu(F U G) ) . Thus , the maximal discriminator reports the broadest difference between two sets . In our first example , the broadest difference is simply the distinction between private and public schools as indicated by md(F , G ) = ( esr , esu ) .
A useful by product of the notion of smallest common ancestors and maximal discriminators is the ability to report more specific information . In general , by following the path from F’ to scu(F ) , and by following the path from G’ to scu(G ) , we get more specific levels of distinction . The most specific level of distinction occurs with the pair ( scu(F ) , scu(G) ) ; however , in practice , if scu(F ) ( or scu(G ) ) is a leaf , then it may make more sense to report the parent of the leaf . For example , suppose F = {esrbl} and G = {esujs ) , and suppose “ St . George ’s School ” is the name of feature esrbl and “ Pierre Elliot Trudeau School ” is the name of feature esujs . A user who is unfamiliar with these feature names may prefer to see a more general level of distinction namely , the distinction of a boys’ private school versus a junior high public school . We leave the desired level of distinction as an application issue , but mention it for completeness .
GenDis
Algorithm Figure 3 presents the outline of Algorithm GenDis for extracting maximal discriminators for two clusters , using multiple concept hierarchies . Let us apply the
Spatial , Temporal , d Multimedia Data Mining
349
{esrbl , esrgl,ppl} and G = {esujs} .
Like the situation algorithm on clusters CZ , = and Cl , = {ptl , esujs} . Suppose the first iteration of the for loop considers the educational institutions hierarchy , in which F = {esrbl,esrgl} In Step 2521 , md(F,G ) is the pair ( esr,esu ) , which is added to the answer set S . The pair corresponds to private schools and public schools , which highlight the distinction ( in terms of kinds of features ) between the two clusters . In the next iteration , the parks hierarchy is used , in which F = {ppr} and G = {ptl} . From Figure 2 , md(F , G ) is the pair ( pp,pt ) , which corresponds to playgrounds and trails . Thus , ( pp,pt ) is added to S , yielding a second discriminating class of features . for identifying and quantifying commonalities ( Knorr & Ng 1996 ) , maximal discriminators from different concept hierarchies should be ranked ( i ) to give an idea of how strong the discriminators are , and ( ii ) to take into account the varying cardinalities of different concepts and hierarchies . This can be done as follows . Given the pair ( F’ , G’ ) as a maximal discriminator , the score is defined by the maximum of the cardinalities of F’ and G’ , normalized by the total cardinality of the concept hierarchy . These scores are then ranked . For example , from Figures 1 and 2 , we see that the score for ( esr,esu ) is 136/175 ( approximately 0.78 ) , and the score for ( pp,pt ) is 25/75 ( approximately 033 ) Although the scores depend on the cardinalities and granularities of the various concept hierarchies , smaller scores are generally favoured since they often reflect the fact that different types of discriminating features having relatively low probabilities of occurrence appear in both clusters .
The score for ( C , niE ) , where C is the root of a concept hierarchy ( eg , ( e,niZ ) ) is 1 because , as shown in Step 241 of Algorithm GenDis , this corresponds to a situation where concepts from the hierarchy rooted at lists , but not both . C appear in one of the two top& is NULL are Of course , those cases where md(F,G ) not included in the list of discriminators and hence , the rankings since F and G are not considered to be sufficiently different .
From a complexity standpoint , smallest common ancestors can be computed in O(1 ) time , with 0(n ) pre processing time , where n is the total number of nodes ( Hare1 & Tarjan 1984 ) . It is easy to see that the complexity of computing maximal discriminators is equally low .
Future Work
In future work , we will investigate how to generalize the extraction of maximal discriminators from two clusters to n clusters , in an efficient manner . In other words , given n clusters and their n top k lists , we aim
350
Technology Spotlight to distinguish cluster Cli clusters , for all i E ( 1 , . . . , n} . from the remaining n 1
Acknowledgments
This research has been partially sponsored by NSERC Grants OGF’0138055 and STRO134419 , IRIS 2 Grants HMI 5 and IC 5 , and a CITR Grant on “ Distributed Continuous Media File Systems . ”
References
Agrawal , R . ; Ghosh , S . ; Imielinski , T . ; Iyer , B . ; and Swami , A . 1992 . An interval classifier for database mining applications . In Proc . 18th VLDB , 560 573 . Agrawal , R . ; Imielinski , T . ; and Swami , A . 1993 . Mining association rules between sets of items in large databases . In Proc . ACM SIGMOD , 207 216 . Borgida , A . , and Brachman , R . 1993 . Loading data into description reasoners . In Proc . ACM SIGMOD , 217 226 . Ester , M . ; Kriegel , H . ; and Xu , X . 1995 . Knowledge discovery in large spatial databases : Focusing techniques for efficient class identification . In Proc . 4th Int’l Symposium on Large Spatial Databases , 67 82 . Han , J . ; Cai , Y . ; and Cercone , N . 1992 . Knowledge discovery in databases : an attribute oriented approach . In Proc . 18th VLDB , 547 559 . Harel , D . , and Tarjan , R . 1984 . Fast algorithms for finding nearest common ancestors . SIAM Journal on Computing 13~338 355 . Knorr , E . M . , and Ng , R . T . 1996 . Finding aggregate proximity relationships and commonalities in spatial IEEE Transactions on Knowledge and data mining . ( Special Issue on Database MinData Engineering ing ) . Forthcoming . Knorr , E . M . 1995 . Efficiently determining aggregate proximity relationships in spatial data mining . Master ’s thesis , Dept . of Computer Science , Univ . British Columbia . Lu , W . ; Han , J . ; and Ooi , B . 1993 . Discovery of general knowledge in large spatial databases . In Proc . Fur East Workshop on Geographic Information Systems , 275 289 . Ng , R . , and Han , J . 1994 . Efficient and effective clustering methods for spatial data mining . In Proc . 20th VLDB , 144 155 . Zhang , T . ; Ramakrishnan , R . ; and Livny , M . 1996 . Birch : An efficient data clustering method for very large databases . In Proc . ACM SIGMOD , Forthcoming .
