From : KDD 95 Proceedings . Copyright © 1995 , AAAI ( wwwaaaiorg ) All rights reserved .
Accelerated Quantification of Bayesian Networks with Incomplete
Data
Bo Thiesson
Aalborg University
Fredrik Bajers Vej 7E
DK 9220 Aalborg O , Denmark thiesson@iesdaucdk
Abstract specification
Probabilistic expert systems based on Bayesian networks ( BNs ) require initial both a qualitative graphical structure and quantitative assessment of conditional probability tables . This paper considers statistical batch learning of the probability tables on the basis of incomplete data and expert knowledge . The EM algorithm with a generalized conjugate gradient acceleration method has been dedicated to quantification of BNs by maximum posterior likelihood estimation for a super class of the recursive graphical models . This new class of models allows a great variety of local functional restrictions to be imposed on the statistical model , which hereby extents the control and applicability of the constructed method for quantifying BNs .
Introduction
The construction of probabilistic expert systems ( Pearl 1988 , Andreassen et al . 1989 ) based on Bayesian networks ( BNs ) is often a challenging process . It is typically divided into two parts : First the construction of a graphical structure which defines relations between variables in a model , and second the quantitative assessment of the strength of these relations as defined by tables of conditional distributions .
Both aspects of this process can be eased by applying automated methods for from a database of observations or from a combination of a database and expert knowledge . See Buntine ( 1995 ) for a literature review on different learning methods . learning assessment of relations
This paper considers statistical batch learning for the quantitative in a given structural model . In this scheme a BN resembles a quantified statistical model , that is , a particular distribution belonging to the set of distributions as defined by the model . Usually , the recursive graphical °The author is grateful to Steffen L . Lauritzen for help ful discussions . Grant support was provided by ESPRIT Basic Research Action , project no . 6156 ( DRUMS the PIFT programme of the Danish Research Councils . lI ) and
306 KDD 95 the models of Wermuth & Lauritzen ( 1983 ) underlie statistical modelling for BNs . We investigate a superclass for these models , denoted as recursive exponential models ( REMs ) , which have evolved by the desire impose functional restrictions onto local components of the model . One may visualize a local component as the part of a model which defines the functional structure of a particular conditional distribution in a quantification table for the BN . Hence , the REMs extends the recursive graphical models by the possibility to functionally control the quantification of these tables .
Given a database of observations , the maximum likelihood estimate ( MLE ) is the usual candidate for quantification of a model . If also prior knowledge on parameters is available , the largest posterior mode is a natural alternative . This mode will be denoted as the maximum posterior likelihood estimate ( MPLE ) .
In situations of incomplete observations the deterfor numermination of the MLE or MPLE may call is shown how ical techniques . In Lauritzen ( 1995 ) it to exploit the EM algorithm , as formally defined in Dempster et al . ( 1977 ) , for maximization within the framework of recursive graphical models . Unlike various other numerical techniques for optimization , the EM algorithm converges reliably even when started in a distant point from the maximum . A common criticism , however , is that convergence can be painfully slow if high accuracy of the estimate is necessary . See eg Louis ( 1982 ) , Meilijson ( 1989 ) , Jamshidian & nrich ( 1993 ) , and several discussants of Dempster et al . ( 1977 ) . For this reason we investigate an acceleration of the EM algorithm by a generalised conjugate gradient algorithm which has a superior rate of convergence when started close enough to the maximum to ensure convergence . By using the EM algorithm for early iterations and the acceleration algorithm for the final iterations , we have constructed a hybrid method which preserves the global convergence of the EM algorithm but has a higher rate of convergence .
The idea of accelerating the EM algorithm is not new in general . See Jamshidian & Jennrich ( 1993 ) for a recent survey . In the context of quantifying BNs it is new , probably due to the lack of publications on the analytical evaluation of derivatives as needed for most accelerations . Lauritzen ( 1995 ) and Spiegelhalter et al . ( 1993 ) do , however , mention the possibility of calculating the gradient for recursive graphical models , and in fact Russell et al . ( 1995 ) covers gradient descent methods for MLE quantification based on these models .
Section 2 and 3 review the EM algorithm and the generalized conjugate gradient algorithm used for acceleration . Section 4 gives a concise description of REMs and account for the simplification into the wellknown recursive graphical models . In Section 5 the algorithms are specialized for these models .
The MLE method
Suppose that x is only observed indirectly
Given a conceptual model , yielding the vector of random variables X = ( X~)~v , with a family of distributions p(X[8 ) parameterised by the vector 0 E O and denote by/(9[x ) , the associated log likelihood function . through the actually observed , possible incomplete , sample of data y . The observed data may be incomplete in two ways : Observations may be missing according to a full instantiation of X so that individual cases only hold observed values according to a sub vector XA , A C V . This accounts for both situations of latent ( or hidden ) variables and situations of randomly missing values among variables . Observations may also be imprecise if there are variables for which the collector of data cannot distinguish between a set of possible values and therefore reports this set instead of just a single value . By this scheme , incomplete data associates a set of possible completions denoted X(Y ) . Under the condition that the observed data is incomplete in an uninformative way according to these possible completions , the incomplete data distribution satisfies p(yl e ) = ~ p(xl8
) . ~Ex(y )
( 1 )
In case of incomplete data the MLE is typically too to calculate analytically . Here , the general difficult idea of the numerical approaches considered in this paper is described .
The EM algorithm easy interpretaThe EM algorithm has an intuitively tion of converting the ML estimation into a sequence of "pseudo estimations" with respect to the conceptual model for complete data . Let 0"‘ denote the current value of 0 after n iterations . Each iteration of the EM algorithm can then be described in two steps :
E step : Determine the conditional expectation of the log likelihood function given the observed data
Q(OIO" , y ) = E0 [ t(OlX ) ] .
Determine 0"‘+x by maximizing
M step : Q(olo" , u ) in Generalizations of the EM algorithm appear by increasing Q(8"‘+lla’‘,y ) over Q.(O’‘lS",y than maximising it . The generalized EM algo strictly rather rithms may be favourable in situations where the MLE of Q(818"‘ , y ) has to be calculated iteratively .
) for when to "retire"
There is no clear criterion the EM algorithm in favour of the acceleration algorithm . However , if acceleration is started too early , divergence will reveal by a sudden decrease in likelihood value . If this happens the EM algorithm must take over a few iterations from the point previous to the decrease in likelihood before the faster method is started again . gradient acceleration
Conjugate On the ground that appropriate generalised gradients can significantly improve the performance of algorithms that use gradients , Jamshidian & Jennrich ( 1993 ) proposed an acceleration of the EM algorithm based on a generalised conjugate ~radient method .
They showed that if the MLE 8 is an interior point of O , then
8"+1 0"‘ = ((2(glg , y)) li(o’ly ) + g ) , n n+l is the Hessian Q(OlO , y ) where i(Only ) is the gradient of the log likelihood function at 0"‘ and ( ~(/~l/~,y ) ) evaluated at 0 . As the key to the acceleration method they observed that in the neighbourhood of /~ , the EM step 0 0 approximates the generalised graB dient l(0 [ y ) = (Q(O[O,y))I[ 0 II ( 0’( ~)(018 , y))0)½ , where’ denotes transpose . The obvious advantage of this approximation is that the evaluation of a generalized gradient does only require an EM step . Hence , by the assumption that the EM step qualifies as an appropriate generalised gradient , Jamshidian & Jennrich ( 1993 ) proposed a generalised conjugate gradient acceleration , which for each iteration operates as follows : l(0 with the n orm
1
B
LS step ( Line search ) : Determine "+1 =8"+ ad’‘ , where a is a step length which ( approximately ) maximizes 1(8"‘ + ad,,[y ) . DU step ( Direction update ) : Determine the next conjugate direction as dn+l ~ /(0 n’l 1
[ y ) /3d’‘ , where
=
[ (8,,+11y),(i(o +11y ) dk(l(0"‘+11y ) Re"In ) )
i(o"ly ) )
Thiesson
307
The algorithm is initialised by do =/(0°[y ) .
The algorithm is motivated as an acceleration of the EM algorithm as follows . If the length of an EM step is not optimal , a line search in this direction will improve the rate of convergence . This is actually the acceleration algorithm with/3 0 . The rate of convergence can also be improved by ensuring that moving along in a direction will not cancel out traversals of previous steps . Hence , instead of moving in the direction of the EM step , we proceed in a direction which is conjugate to the previous direction , and , insofar as possible , to all previous directions traversed . This is accomplished by/3 with the evaluation of gradients as the cost .
The MPLE method
Traditional ML estimation may be compromised when dealing with ill posed problems like latent structure models or situations of sparse data , where small changes in the data can have a large impact on the estimate . In these situations we may resort to a Bayesian interpretation given to the estimation problem . Instead of just maximizing the likelihood we may incorporate prior information about the parameters by finding the largest posterior mode , the MPLE . Dempster et al . ( 1977 ) briefly describe how to modify the EM algorithm to produce the MPLE . This has been further entertained in Green ( 1990 ) . A specialization for recursive graphical models is found in Lauritzen ( 1995 ) . Suppose we have information about 0 in the form of a prior distribution ~r(0 ) , then
P(01y ) o(p(y[O)1r(O ) .
By considering the posterior distribution as a posterior likelihood , maximized by the EM algorithm by simply replacing the E step with the expectation of the posterior log likelihood , Q*(0[0" ) , the E step becomes
Q*(OIO" ) = Q(oIo" ) + logTr(0 ) .
( 2 )
In the M step , Q* is maximized instead of Q .
The Bayesian interpretation of the EM algorithm can be projected directly onto the gradient , as additionally needed for the acceleration method .
Analogous to the notation of the gradient for the log likelihood by i(0[y ) ,
( traditional ) l*(Oiy ) denote the gradients for the logarithm of prior and posterior distributions , respectively . The gradient of the posterior log likelihood is then given by let i(0 ) i*(Oly ) = i(Oly ) + i(O ) .
( 3 )
In effect , each of the expressions which goes into the MPLE method is made up by two terms , which describe and amount of data against prior knowledge for modelling an acceptable solution to the problem . the game between fidelity
308 KDD 95
The statistical modelling
Here , we introduce the REMs , which have evolved from the recursive graphical models by the desire to impose functional restrictions onto local components of a model . We also investigate appropriate priors . models
Recursive exponential represented by a directed A REM can be graphically acyclic graph . That is , the variables X can be arranged by a response structure , where the set of nodes represents variables and directed edges signify for each variable Xr E X the existence of direct causal influence from variables represented by the parents Xp~(~ ) .
According to this graphical structure , a REM holds assumptions of variation independence between parameters in different local components of the model , to be described below . Readers familiar with Spiegelhalter & Lauritzen ( 1990 ) and the line of work reported Heckerman et al . ( 1995 ) may recognize these assumptions as used in these papers but not explicitly named .
By global variation independence , p(XlO ) factorises into a product of mutually independent components given by the recursive response structure of the graph . That is ,
’Or)’ P(XlO ) = ]’I P(XrlXP°(r ) vEV where O = xrevOr , and 0r E Or completely specifies the relatidnship between the variable X~ and its conditional set of variables Xp~(~ ) .
In some applications , particularly pedigree analysis , it is typical to restrain the tables of conditional distributions by using the knowledge that some of the tables are equal . Equal tables must be parametrized by the same parameters . Let ~ C V specify a set of variables that associates equal tables and denote by V the total set of these equivalence classes . Then p(xlo ) = II fief7 vE~ where 0~ E ®~ specifies the relationship between Xv and Xv,,( , ) for any v E ~ . Hence , the global parameter independence is relaxed a bit . If equal tables are represented by a single generic table , as assumed from now on , this representation more directly illustrates the reduction of the parameter space into O = x~eg®~ .
By local variation independence each ( generic ) table is additionally broken into local components of as defined for each parent conditional distributions configuration . index a parent configuration in the generic table , and let s = 0 , , S~ index a particular value of the variable .
For v E ~ , let r = 1,,R~
Hence , Oo xrO~lr and conditional probabilities given by p(slr , O~lr ) , where 0~1~ E ®~1~"
By these simplifying assumptions the quantification of local of a REM is broken into the quantification models , which comply with the typical scenario of breaking down the quantification of a BN into tables of independently quantified conditional distributions . are
The statistical modelling by REMs does not stop at this point . To completely qualify as a REM each local model must be structurally defined by a regular exponential model . As any exponential model is allowed , the REMs become a very extensive class of models , which allows sophisticated functional restrictions to be placed on each local model , if necessary .
Disregarding the possibility of specifying equal tathe local exponential modelling makes the dif bles , ference from the recursive graphical models for which each local model cannot be restricted beyond the fact that it is a model of probability distributions . We do not account this as a functional restriction .
A recursive graphical model is defined in the framework of REMs as follows ( positivety are applied for simplicity ) . Consider the local model that structurally defines the conditional distribution p( Ir ) . Let so denote an index of reference , say So 0 , and let for s+ = 1,,S~ constraints
= log[p(s+lr)/p(solr ) ] and t s+ ( s ) = 0 otherwise .
1 for s = s+
The local model is then defined by the exponential model having probabilities of the form p(sl~ , O ) = b(s , r ) exp[O’t(s ) ¢(0)1 ,
( 4 )
¢(8 ) = log exp(8 s+ , where O = ( 81 , , & ) defines t he parameters , t ( s ) = ( tl(s),,t function , and b(s , r ) = 1 the carrying density . s~ ( s ) ) the statistics ,
¢(0 ) the normalizing for parameters
Prior distributions The construction of a prior distribution for parameters is simplified considerably by matching the assumptions of variation independence with assumptions of relaxed global and local independence of parameters considered as random variables . By these assumptions , the distribution for parameters factorises as
R~
.(o ) = 17[ II (eolr ) vEV r=l
Hence , each local component of the prior can be considered independently .
The notion of global and local independence is also nicely covered within the line of work reported in Heckerman et aI . ( 1995 ) . It is inspired by similar assumptions in Spiegelhalter & Lauritzen ( 1990 ) , which introduced a method for sequential updating a Bayesian network as new observations eventuate . To prepare the quantification methods for the possibility of future sequential updating by this method , we are especially interested in ( approximately ) conjugate priors .
If functional restrictions are not specified for the local model , the natural conjugate prior on probabilities is given by a S~ dimensional Dirichlet distribution with parameters a(s , r ) associated for each probability . That is , p(.Ir , 8~lr ) , , , 79(a(0 , r) , , a(S~,r) ) . of parameters as given by the By a transformation exponential representation of probabilities in ( 4 ) , the prior distribution for 8~1~ is defined by ( noting that dp(’lr , 8~l~)/dS~lr = 1 Is=op(slr,89l~ ) gives the Jacobian of the transformation )
S~ s~ II p(slr , s=0
( 5 )
Given a general exponential local model , the construction of a conjugate prior becomes more complicated . Denote by 8" the value that maximizes the local prior rr(8~l~ ) . By a Taylor series expansion around O* Thiesson ( 1995 ) shows that a conjugate distribution can be approximated by a distribution proportional to the multivariate normal distribution
.A]’(8* , ~I(8") 1 ) , where/~ and the maximizing value 8* are unknown parameters to be assessed by experts , and I(8" ) denotes the observed information at the value 8" .
In practice though , it seems unreasonable to request domain experts for a parametrization of any of these priors . To overcome this problem Thiesson ( 1995 ) also shows how to assess the parametrization from a specification of a "best guess" distribution with a judgment of imprecision ( or confidence ) on each of the probabilities in the form of an upper and lower boundary . Assessment of Dirichlet priors can also be studied in Spiegelhalter et al . ( 1993 ) and Heckerman et al . ( 1995 ) .
Specialization
The maximization algorithms are specialiT.ed for the REMs . We consider computation of the MPLE , but the MLE is easily obtained by inserting non informative priors in the following . It turns out that maximization can be accomplished by local computations .
Thiesson 309
The EM algorithm To identify the E step , we will first consider the likelihood function for a sample of independent complete observations , x = ( xl,,xL ) tion of the probability for a single observation
Due to the factoriza p(x’lO ) = 17[ 1 [ p(x~lx~o(o ) ,
0~14o,o
/ 1
, )
T3EI~" rE9 the likelihood factorises as
R~ S~ r)a(s’~)’ L(Olx ) ~ 1 I p(~’lO ) = l’I II p(slr’O~l
L
1=1
9E~z r=l s=0 where h(s , r ) = ~oe~ n(s , r ) , and n(s , r ) denotes the marginal count in the configuration ( s , r ) for a family variables ( X , , Xp~(,) ) . The marginal count is obtained by adding up the qualifying observations as
L n(s , r ) = ~ ",r)(~’~ , ’Xp~(, ) )
I 1 where
( , ~ ) , t t , { 1 for ( xto , Xt~p ( )v )
X ’ ( x~,Xp~(, ) )
0 otherwise .
~
( 8 , r )
( 7 )
For a sample of independent , possibly incomplete , observations y = ( yl , , the conditional expectation of the likelihood function is obtained by replacing the marginal counts by expected marginal counts yL )
~*(s,r )
=
En*(s,r ) vE,3
L veil 1=1
As pointed out in Lauritzen ( 1995 ) , the posterior probabilities in ( 8 ) can be efficiently calculated by the procedure of Lauritzen & Spiegelhalter ( 1988 ) for probability propagation .
The Frstep ( 2 ) can now be identified
Q*(OIO",y )
= E E h*(s’r)l°gp(slr’O~lr )
+l°gTr(0~l~ ) s=O
9EQ r=l Ro
= ~ ~Q*(O~IrI°",Y)’ where p(slr , 0~1~ ) is of the exponential form ( 4 ) .
By this , the M step is completed by maximizing ( or increasing ) each local part of the expected loglikelihood independently .
310 KDD 95
If the local model does not hold functional restric tions , the local prior is given by ( 5 ) , and the maximum for Q*(O~I~IOn , y ) can be found analytically as the value 0~1~ E O~1~ which obeys
~* ( s , r ) + a(s , p(slr’g~l~ ) = h*(r ) + a(r )
= ~ oS
E,=ofi*(s,r ) and a(r )
For situations of functional restrictions where fi* ( r)= a(s,r ) . A similar result is found in Lauritzen ( 1995 ) . Recall that a local model without functional restrictions complies with a local part of a recursive graphical model . in a local model , we typically have to carry out the maximization by an iterative method . Being able to calculate both first and second order derivatives for Q* ( 0~1~ 0" , y ) , t he globally convergent algorithm for maximization , as described in Jensen et al . ( 1991 , Theorem 3 ) , can be applied . The overall computational efficiency of the EM algorithm can be improved if only the first most influential iterations are considered here . algorithm
The acceleration Recall that the generalized gradient for the posterior likelihood is approximated by an EM step . Hence , to accomplish the specialization of the acceleration algorithm we only need to derive the gradient . It can be divided as specified in ( 3 ) .
First consider the derivation of the gradient for the log likelihood of a single incomplete ob
( traditional ) servation , denoted by y . From ( 1 ) we see that
By global and local variation ing the chain rule for differentiation independence and by us
8 p(zl0 ) 00~1~
= ~ p(x~lxp~(~),O~l~ ) p(x’lxp~(v)’O~l~ ) p(xlO )
O
] vG9
= p(xlO ) ~_ , x~(xpo(~))~ logp(x~lr , veil where X~(xpo(, ) ) is defined similarly to ( 7 ) .
Let T(0~lr ) denote the expected value of the statistic for the local exponential model defining p(x , lr , O~l~ By inserting the exponential representation we get
) .
8
Finally , by inserting ( 10 ) into ( 9 ) , the local components of the gradient for a single observation are derived as
) logp(yl0 xex(y ) p(y]0 )
=
S~ vG~ s=O rly , e ) ( t(s )
For a sample of independent observations the gradients for each observation simply add up . Hence , if y denotes a sample , then the gradient for the log likelihood is given by the local components derived as s~ 0 ro0 l : l(°ly ) =
$ 0
( tCs ) ( 0 ) .
( 11 )
When functional restrictions are not specified in a local model , the gradient for the associated local logprior is found by straightforward differentiation of the logarithm to the prior in ( 5 ) , whereby
0 l(e ) s~ ¯ =0 r ) ( tCs )
)
( 12 )
Similarly , when restrictions are specified , the local gradient is derived by differentiation of the logarithm to the normal prior distribution in ( 6 ) oe l l(e ) = e*iJ .
( 13 ) Local computation of the posterior gradient , with components composed of ( 11 ) and one of ( 12 ) and ( 13 ) , hereby implies that also the acceleration algorithm can be evaluated locally .
References
Andreassen , S . , Jensen , F . V . , Andersen , S . K . , Falck , B . , Kj~erulff , U . , Woldbye , M . , Sorensen , A . , Rosenfalck , A . & Jensen , F . ( 1989 ) . MUNIN an expert EMG assistant , in J . E . Desmedt ( ed. ) , ComputerAided Electromyography and Expert Systems , Elsevier Science Publishers , chapter 21 , pp . 255 277 . Buntine , W . L . ( 1995 ) . A guide to the literature on learning graphical models , IEEE Transactions on Knowledge and Data Engineering . Submitted . Dempster , A . P . , Laird , N . & Rubin , D . B . ( 1977 ) . Maximum likelihood from incomplete data via the EM algorithm ( with discussion ) , Journal of the Royal Statistical Society , Series B 39 : 1 38 .
Green , P . J . ( 1990 ) . On use of the EM algorithm for penalized likelihood estimation , Journal of the Royal Statistical Society , Series B 52 : 443 452 . Heckerman , D . , Geiger , D . & Chickering , D . M . ( 1995 ) . Learning Bayesian networks : The combination of knowledge and statistical data , Machine Learning . To appear . Jamshidian , M . & Jennrich , R . I . ( 1993 ) . Conjugate gradient acceleration of the EM algorithm , Journal of the American Statistical Association 88(421 ) : 221228 .
Jensen , S . T . , Johansen , S . & Lauritzen , S . L . ( 1991 ) . Globally convergent algorithms for maximizing a likelihood function , Biometrika 78(4 ) : 867 877 . Lauritzen , S . L . ( 1995 ) . The EM algorithm for graphical association models with missing data , Computational Statistics ~ Data Analysis 19 : 191 201 .
I . ( 1989 ) . A fast
Lanritzen , S . L . & Spiegelhalter , D . J . ( 1988 ) . Local computations with probabilities on graphical structo expert systems ( with tures and their application discussion ) , Journal of the Royal Statistical Society , Series B 50 : 157 224 . Louis , T . A . ( 1982 ) . Finding the observed information matrix when using the EM algorithm , Journal of the Royal Statistical Society , Series B 44(2 ) : 226 233 . Meilijson , improvement to the algorithm on its own terms , Journal of the Royal Statistical Society , Series B 51(1 ) : 127 138 . Pearl , J . ( 1988 ) . Probabilistic Reasoning in Intelligent Systems : Networks of Plausible Inference , Series in Representation Publishers , Inc . Russell , S . , Binder , J . , Koller , D . & Kanazawa , K . ( 1995 ) . Local learning in probabilistic networks with hidden variables , Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence . To appear . and Reasoning , Morgan Kanfmann
Spiegelhalter , D . J . , Dawid , A . P . , Lanritzen , S . L . & in expert
Cowell , R . G . ( 1993 ) . Bayesian analysis systems , Statistical Science 8(3 ) : 219 247 . Spiegelhalter , D . J . & Lauritzen , S . L . ( 1990 ) . Sequential updating of conditional probabilities on directed graphical structures , Networks 20 : 579 605 . Thiesson , B . ( 1995 ) . Score and information for recursive exponential models with incomplete data . Manuscript . Wermuth , N . & Lauritzen , S . L . ( 1983 ) . Graphical and recursive models for contingency tables , Biometrika 70 : 537 552 .
Thiesson 311
