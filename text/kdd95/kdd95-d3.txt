From : KDD 95 Proceedings . Copyright © 1995 , AAAI ( wwwaaaiorg ) All rights reserved .
, ,
/ ” 8’ /‘/
I , ,
An iterative improvement approach for the discretization of numeric attributes in Bayesian classifiers
0
”
‘?J/ ,‘ :
;,,rh ‘/ ‘&;!11/
,i ;
”
Department of Information and Computer Science
University of California , Irvine
Michael J . Pazzani
Irvine , CA 92717 pazzani@icsuciedu phone : ( 714 ) 824 5888 fax ( 714 ) 824 4056
Abstract
The Bayesian classifier is a simple approach to classification that produces results that are easy for people to interpret . In many cases , the Bayesian classifier is at least as accurate as much more sophisticated learning algorithms that produce results that are more difficult for people to interpret . To use numeric attributes with Bayesian classifier often requires the attribute to be discretized into a number of values that the discretization of intervals . We show numeric attributes is critical to successful application of the Bayesian classifier and propose a new method based on iterative improvement search . We compare this method to previous approaches and show that it results in significant reductions in misclassification error and costs on an industrial problem of troubleshooting the local loop in a telephone network . The approach can take prior knowledge into account by improving upon a user provided set of boundary points , or can operate autonomously .
Introduction
This research was motivated by a problem of discovering how to troubleshoot a telephone network using a data base of repair records provided by NYNEX ( the primary local phone company for New York and New England ) . When NYNEX customers have problems with their lines , they tail a special number to report the problem . A phone company representative takes information from the customer about the symptoms of the trouble and creates a trouble report . At the same time , the representative initiates electrical tests on the line the Mechanized Loop Test , or Iv&T . The data gathered by the MLT , which include voltage readings , for example , are attached to the trouble report , which is then sent to a
228
KDD 95
Maintenance Administrator ( MA ) for diagnosis . The MA uses the information from the trouble report , MLT , and Screening Decision Unit to make a high level diagnosis of the trouble . Based on this diagnosis , the MA determines the part of the local loop to which a repair technician should be dispatched : the central office ( PDI ) , the cable ( PDF ) , or the customer ’s home ( PDO ) . The MA can also specify that the trouble should be held for additional testing ( PDT ) . they would recommend given
In this paper , we explore the use of Bayesian classifiers for dispatching the repair technician . The Bayesian classifier is trained on a database of repair records . Each repair record contains 21 variables encoding information on the type of switching equipment and various voltages and resistance measurements . Each repair record was reviewed by between 2 and 4 experts who indicated what type of dispatch this information.’ For each repair record , we constructed a set of acceptable recommendations where an acceptable recommendation is defined to be an answer that any of the experts gave . However , if three experts gave the same recommendation and one expert gave a different recommendation , we used the diagnosis proposed by the majority . In all other cases , we consider it acceptable for an automated system to react like any of the experts would on a particular case . Using this definition of acceptable , there are an average of 1.84 acceptable diagnoses for each case . We selected Bayesian classifiers for this task for four reasons :
1 . One might initially believe that the technician who fixed the problem would be able to indicate the location to which the technician should have been dispatched . However , for a variety of reasons , ( Danyluk & Provost , 1993 ) , this information is not very reliable .
1 . It has been found that they can be at least as learning accurate as more “ sophisticated ” methods ( eg , Kononenko , 1990 ) .
2 . Bayesian classifiers produce an estimate of the probability that an example belongs to each class . This probability estimate may be used to determine the most likely class of a training example , or the class that will have the least expected cost of misclassification errors . While other learners ( eg , decision trees , Quinlan , 1986 ) may be extended to produce probability estimates , in practice , these estimates do not vary continuously and do not provide fine grained information to perform well at determining the least expected cost of misclassification errors ( eg , Pazzani , Merz , Murphy , Ali , Hume , and Brunk , 1994 ) .
3 . The Bayesian classifier reveals information that some experts find more useful than other models such as decision trees ( Kononenko , 1991 ) . The results of the Bayesian classifier are conditional probabilities that may be viewed as simple , one attribute rules . is possible to take advantage of expert a l 1 _ ALKuowieoge on me criiicai vaiues of continuous variables , and revise this information , as we shall describe .
4 . It
However , on some problems , the simple Bayesinn classifier is much less accurate than other learners . In Pazzani ( 1995 ) , we addressed one possible problem with Bayesian classifiers in that they assume attribute values are independent within each class . However , the methods used to detect and correct for violations of this assumption did not have an effect on the accuracy or misclassification cost of the Bayesinn classifier on the telephone troubleshooting database . In this paper , we address another problem with using Bayesian classifiers on practical problems : the treatment of numeric data . We also address issues that occur when learning from data in which there is disagreement among exploiting information from experts on how numeric data may be discretized . experts and
Background
The Bayesian classifier ( Duda Jc Hart , 1973 ) is a probabilistic method for classification . It can be used to determine the probability that an example j belongs to class Ci given values of attributes of the example : P(CitAt=VI , & . & An=V, , ) the attribute values are independent , lhis probability is proportional to : P(C , ) JJ P(Ak=Vk,lCi )
If
. k
Both P(Ak=Vk,lCi ) and P(C ) may be estimated from training data . This classifier , which assumes attribute independence , has been called the simple Bayesian classifier , the naive Bayesian classifier , the idiot Bayesian classifier , and the first order Bayesian classifier . To determine the most likely class of the example , the probability of each class may be computed and the example assigned to the class with the highest probability . The probability of each class may also be used to determine the class with the least expected cost of misclassification errors .
NYNEX has implemented a rule based expert system , MAX ( Rabinowitz , et al . , 1991 ) , that is used for to determine the location of a malfunction customer reported telephone troubles . MAX is a rule based system that makes its diagnosis based upon the results of the MLT as well as other general information , such as the type of switching equipment through which the customer ’s line goes . Since its deployment in 1990 . it has been modified and expanded for other related tasks in NYNEX as well . Among its benefits are that it is fast , consistent , and reduces the number of incorrect dispatches . One of MAX ’s limitations is that it is not always correct . On the database we anniyzed , MAX gives a diagnosis that does not match one of the acceptable answers on 33.8 % of the 500 examples . Although this error rate may seem high , MAX generally performs at least as In this work , we’ll well as experienced MA ’s . compare the classification rate of the Bayesian classifier to the classification rate of MAX .
Numeric Values in Bayesian classifiers .
To classify an example with value vk , for attribute Ak , Bayesian classifiers need to use P(Ak=Vk]Ci ) . If attributes have nominal values , this can be easily determined by finding the proportion of examples in the training set of class i that have the value vk , for If two experts disagree about the attribute Ak . classification of an example , the example is split among classes in proportion to the opinions of experts . For example , if two experts call an example a PDT and one calls the example a PDI , the example is considered 2/3 PDT and l/3 PDI when updating the counts of examples of each class and the counts of examples with each attribute value within in each class .
When attributes have numeric values , a number of approaches may be used . The Bayesian classifier used in Pazzani et al . ( 1994 ) was derived from the Bayeshan classific ? used in Langley & Sage ( 1994 ) . for numeric data by This computed qAk=vk,icl ) assuming that within each class , the values for an attribute are normally distributed about some mean . It found the mean and standard deviation for each
Pazzani
229
120 100 80 60 h 3 “ 1 e .
Figure
1 . Frequency of values of resistances within some classes do not appear to be normally distributed . class of a numeric attribute , and used this information to determine the probability that an attribute had a given value . This Bayesian classifier was much less accurate than decision trees or other learners on the telephone troubleshooting problem . At first , we ____ msuiiled that ibe independence ws ” mpiion Of ihe Bayesian classifier was to blame : but recently , we have found that the normality assumption of numeric attributes was a major contributor to the poor performance . For example , Figure 1 plots the frequency of given ranges of resistance for one attribute for examples of the class PDT ( ie , require retesting ) . Clearly , this is not normally distributed . Discretizing numeric variables The more typical way of dealing with numeric variables in Bayesian classifiers is to discretize the variables into a small number of partitions , and treat these partitions as nominal values . For example , one could use for values as first quartile , second quartile , third quartile , our experimentation with an approach in which we find the minimum and maximum value in the training sets and divide the data into P partitions of equal size . Figure 2 shows the Error ( proportion of examples for which the classifier ’s answer does not match an acceptable answer ) for training sets of size 200 , 300 , and 400 and for values of P equal to 2,3,4 , 5,6,8 , 10 , 12 , and 15 . Each point is the average of 20 trials of randomly choosing a training set of the specified size and using the remaining examples in the databnse as test examples . Several things are apparent from Figure 2 . First , the choice of P has a major effect on the error rate . too small a value is chosen , important distinctions are missed , and if too large a value is chosen , the data is so overly partitioned that the probability estimates become unreliable . Second , the best value for P depends upon the size of the training set . fourth quartile . We start
If
230
KDD 95
[ O 199 ] ,
[ O 1199 ] ,
There are seveml problems with the straightforward approach to partitioning data used above . First , it ’s not clear how to choose the best value for P ( although cross validation on the training set is likely to find a good value ) . Second , it ’s not clear that the same for P shouid be used for every atttbute . This vdue particular problem has 21 attributes and it might be best to divide some into 2 groups and others 10 . Third , the approach doesn’t look for critical values of the variable , but just divides the variable into evenly spaced partitions . For example , if one were dividing the variable graphed in Figure 1 into three regions , [ 200 33991 and [ 3400 36001 the mnges might be preferred [ 1120 23991 and[2400 36001 . In spite of its problems , this simple approach to discretion is much better than assuming normality . The error rate assuming normality with 400 examples exceeds 40 % . Furthermore , some values of P result in a more accurate classifier than a manually constructed rule based expert system and a classifier less accurate than other approaches we have tried , such as decision trees ( Quinlan , 1986 ) and rule learners ( Pazzani & Kibler , 1992 ) . Searching for boundaries The problem of finding a good set of boundary points to discretize values for numeric attributes can be viewed as a search problem . In particular , one approach would be to generate possible boundaty points and estimate the error ( or misclassification cost ) of the Bayesian classifier with these boundary points cross vauaauon ( LOOCV ) . Unfortunately , generating and testing all such boundary points is imppgtical . In the worst case , there are at most 0(2 ) possible boundary points , where A is the number of numeric attributes and N is the number of examples ( which is an upper bound on the number of values of each attribute ) , ieave one out that is not to using
> ,
0.5
0.4 k t ” W
0.3
0.2 0
7
200 300 400
5
10
15
Number of Partitions
Figure 2 . Error rate as a function of the number of partitions for training sets of size 200,300 and 400 ‘L , &^l L^ , . & _^ hl, l r:
Ll
Ull
LIIG rcltyllullG uuuulGsl1uuLlllg pluulr;lll . since each value of each attribute of each example is a potential boundary point . For the Bayesian classifier finding the optimal boundary points for each attribute individually need not result in an overall optimal set of boundary points . One approach , that we evaluate in the next section is to avoid the search problem and use a statistical or heuristic approach for finding boundary points . In this section , we propose a search procedure to find a “ good ” set of boundary points . The search procedure starts with an initial seed set of boundary points ( eg , by discretizing each attribute into 5 partitions ) and has two operators that adjust boundary points : 1 . Merge two contiguous intervals . two 2 . Split an intervals by considering introducing a new boundary point that is midway between every pair of contiguous attribute values within that interval . interval into
The adjust process uses an iterative improvement search strategy as follows :
1 . Estimate the error ( or misclassification cost ) of each adjustment using LOOCV of the current set of boundary points , and reorder the examples such that those that are misclassified occur before those that are correctly classified .
2 . Reorder the attributes randomly 3 . For each attribute in the set of attributes
A . Apply all operators in all possible ways to the the current boundary points of attribute .
B . Estimate the error ( or misclassification cost ) of each adjustment using LOOCV
C . If the error of any adjustment is less than the error of the current boundary points , then make that adjustment with the lowest error
4 . If no boundnry point was adjusted in Step 3
Then return the current boundary points Else Go to step 1
Oil i~~i ?
nn ~36
~‘AAILLU fOi . ,*:,A1 implemented that example to
There are several efficiency issues that are needed to *n Lo Lllc(hG this ccl~ “ l1111111 nln, .,Gthm databases . First , we may m,ake one change to each attribute in Step 3 , rather than making the single change to a single attribute that results in the lowest overall all error . This reduces the number of times that the loop needs to be executed . The attributes are reordered randomly before the loop is executed so that the order of attributes does not have a major effect on the search process . Second , leave one out cross validation is used to estimate error because it may be efficiently for Bayesian classifiers . A classifier can be built on the entire training set and when leaving an example out , the contribution of the probability estimates is subtracted out before classifying the example . A further optimization greatly speeds up the leave one out cross validation . The examples are ordered in Step 2 such that the examples misclassified by the classifier using the current boundary points are tested first . When calculating the error of a classifier with a new boundary point , we stop the leave one out testing as soon as it is certain that it will have at least as many errors as the current boundary points . On this problem , we have found that this reduces the number of examples classified during error estimation by approximately 70 % . The iterative improvement algorithm is sensitive to Tn lZ:iomwe ‘4 the d( I L .bYL ” .*a ” L . . “ “ I we report on an experiment in which we start by discretizing the numeric attributes into 5 pxtitions , and then adjust these partitions by adding or deleting boundary points . The results displayed are the average error of 20 trials using 200 , 300 and 400 training examples , as well as the standard error around the mean value . In all cases , the error is th+ rmrrh . ” “ I “
CPP~ rhnwn tn ctnrt b ” “ a
dV ” “ initinl
Pazzani
231
II m
5 Partitions I 5 Partitions , Adjusted 1 b 0.25
0.35
0.30 b t : 0.25 !a
0.20
0.20 I 100
300
200
400 Number of examples
500
Number of Examples
3 . Error rate as a function of the number of Figure training examples with and without using boundary point adjustment when starting with discretizing each numeric attribute into 5 intervals . computed on a test set consisting of all examples not used as training examples . The results show lhat adjusting the boundary points significantly reduces the error as measured by a paired two tailed t test at least at the .05 level adjusted for the fact that we are ~rcjrig We Wiii use this same experimental methodology in all of our later experiments .
3 Comp&Oiis*
4 . Error rate as a function of the number of Figure training examples with and without using boundary point adjustment when starting with boundary points derived from an expert system . examples . In fact , adjusting the expert boundary points results in the lowest classification error rate on this problem that we have observed with any learning method . The boundary points after adjustment are avaiiabie for inspection and an expert examining these boundary points could gain useful information from the boundary points .
J and information based approach
An Fayyad & Irani ( 1993 ) have developed an approach for finding a good set of boundary points for discretizing a numeric attributes to be used as a test in a decision tree . The approach is based on information . thnnr . , criteria to LIIb ” I determining when to stop subdividing intervals . Figure 5 this method of discretizntion to using this method to create the initial set of boundary points for adjusting . This method also works well , but adjustment results in a slight but statistically significant improvements at each level of training examples . compares using
StOppig iises
XIX an
. nc uu
For in . . . p&g the . I rnlm l ” . “ ” hnndnrw uuum.uw , defined thresholds
Expert An alternative way of discretizing numeric data is to allow an expert to select the boundary points . Since NYNEX has built an expert system to perform this task we could use the threshold values for each ntfrihnte UU “ “ . ” example , the rules might say that if the voltage between ring and ground is 0 , one sort of repair is needed , while if the voltage is between 0 and 3 , another repair is needed , and if the voltage is above 3 , a third repair is needed . In this case 0 and 3 would serve as boundary points . Figure 4 compares using the expert boundary points directly in the Bayesian classifier and using the expert boundary points after adjusting . We see that the expert boundary points perform well , especially when there are a large number of training example : but the significant adjustment process improvement at each level of the number of training results in a
232
KDD 95
Fayyad and Irani Fayyad and Irani , Adjusted I
Acknowledgements
The research reported here was supported in part by NSF grant IRI 93 104 13 .
I 0.35 1 1
References
9,309 347 .
Irani , K .
Cooper , G . & Herskovits , E . ( 1992 ) . A Bayesian method for the induction of probabilistic networks from data . Machine Learning , Danyluk , A . & Provost , F . ( 1993 ) . Small disjuncts in action : Leaming the telephone network local loop . Machine Learning Conference , pp 81 88 . to diagnose errors in
Duda , R . & Hart , P . ( 1973 ) . Pattern classificarion and scene analysis . New York : John Wiley & Sons . Fayyad , U . & ( 1993 ) . Multi interval discretization of continuous valued attributes for classification learning . The National Conference on Artificial Intelligence ( pp . 328 334 ) . Washington , D.C : AAAI Press . Kononenko , I . ( 1990 ) . Comparison of inductive and naive Bayesian learning approaches to automatic knowledge acquisition . ( Ed. ) , acquisition . in Current Amsterdam : 10s Press . Kononenko , of classifier . Working Session on Learning .
( 199 1 ) . Semi naive Bayesian the Sixth European ( pp . 206 219 ) .
In B . Wielinga knowledge
I . Proceedings
Langley , P . & Sage , S . ( 1994 ) . Induction of selective Bayesian classifiers . Proceedings the Tenth Conference on Uncertainty
Intelligence in Artificial trends of in
C . and
Brunk ,
( 1994 ) . inductive
Moore , A . W . , and Lee , M . S . ( 1994 ) . Efficient algorithms for minimizing cross validation error . In Machine Learning : Proceedings of fhe Eleventh International Conference . Morgan Kaufmann . Pazzani , M . , & Kibler , D . ( 1992 ) . The role of prior learning . Machine knowledge Learning , 9 , 54 97 Pazzani , M . , Merz , C . , Murphy , P . , Ali , K . , Hume , Reducing T . , Misclassification Costs . Proceedings of the Eleventh International Conference on Machine Learning . Pazzani , M . ( 1995 ) . Searching for dependencies in Bayesian classifiers . Artflcial and Stntistics Workshop . Quinlan , JR ( 1986 ) . Machine Learning , 1 , 8 l 106 . Rabinowitz , H . , Flamholz , J . , Wolin , E.,& Euchner , J . telephone trouble ( 1991 ) . screening expert system . In R . Smith & C . Scott /7TdT \ ( CUS . ) m.rfi ’ ’ UrrlJlcrar intelligence , 3,213 230 . Rachlin , Kasif , Salzberg , & Aha , D . ( 1994 ) . Towards a better understanding of memory based reasoning systems . Proceedings of the Eleventh Conference on Machine Learning .
Nynex MAX : A ,* _ .
Induction of decision trees .
International appflcanons
Intelligence innovaiive
Of
Pazzani
233
0.15
100
300
200
400 Number of Examples
500
5 . Error rate as a function of the number of Figure training examples with and wilhout using boundary point adjustment when starting with boundary points derived from an information based method ( Fayyad & Irani , 1993 ) .
Conclusions the
We have investigated an iterative improvement approach to discretizing a set of numeric attributes for The successful use in a Bayesian classifier . application of the Bayesian classifier depended critically on finding a good method for discretizing numeric variables . Although we have developed the method in the context of Bayesian classifiers , it might be used in other leaners that usually discretize of numeric data such as Bayesian networks ( eg Cooper & Herskovits . 1992 ) . The approach proposed here makes adjustments to a user defined or automatically created set of initial intervals in an attempt to improve upon ( or there is no misclassification cost ) . guarantee of finding an optimal set of intervals , in practice it has resulted in improvement over other approaches . The process is relatively efficient . In our experiments it required no more than 5 minutes of PDT1 L . ” points . Furthermore , the approach may be interrupted at anytime to produce its current best set of boundnry points . In addition to the NYNEX troubleshooting problem , we have tested this algorithm on several databases from the UC1 archive of databases ( eg , glass and breast cancer diagnosis ) and found similar decreases in error rates compared to other methods . current misclassification error
Although tn .wl;,,rt I ” U ” JtmL hnnnrlv . , “ “ U~muulJ
. , nkrcm u p.v’ ”
. , Cnerr u uplu
CP~ nf .a ” I “ L
Inn \ “ ,a
WI\ L ” ,
