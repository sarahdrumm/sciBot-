From : KDD 95 Proceedings . Copyright © 1995 , AAAI ( wwwaaaiorg ) All rights reserved .
Feature Subset Selection Using the Wrapper Method :
Overfitting and Dynamic Search Space Topology
Ron Kohavi and Dan Sommerfield
Computer Science Department
Stanford University Stanford , CA . 94305
{ronnyk , sommda}©CS . St axtf ord . EDU
Abstract
In the wrapper approach to feature subset selection , a search for an optimal set of features is made using the induction algorithm as a black box . The estimated future performance of the algorithm is the heuristic guiding the search . Statistical methods for feature subset selection including forward selection , backward elimination , and their stepwise variants can be viewed as simple hill climbing techniques in the space of feature subsets . We utilize best first search to find a good feature subset and discuss overfitting problems that may be associated with searching too many feature subsets . We introduce compound operators that dynamically change the topology of the search space to better utilize the information available from the evaluation of feature subsets . We show that compound operators unify previous approaches that deal with relevant and irrelevant features . The improved feature subset selection yields significant improvements for real world datasets when using the ID3 and the Naive Bayes induction algorithms .
1 Introduction
Practical algorithms in supervised machine learning accuracy ) when degrade in performance ( prediction that are not necessary for faced with many features predicting the desired output . An important question in the fields of machine learning , knowledge discovery , statistics , and pattern recognition is how to select a good subset of features . The problem is especially severe when large databases , with many features , are searched for patterns without filtering of important features by human experts or when no such experts exist .
Common machine learning algorithms , including such as CART , top down induction of decision trees , ID3 , and C4.5 ( Breiman , Friedman , Olshen g~ Stone 1984 , Quinlan 1993 ) , and nearest neighbor algorithms , such as IB1 , are known to suffer from irrelevant features . Naive Bayes classifiers , which assume independence of features given the instance label , suffer from correlated and redundant features . A good choice of
192 KDD 95 features may not only help improve performance accuracy , but also aid in finding smaller models for the data , resulting in better understanding and interpretation of the data .
In the filter approach to feature subset selection , a feature subset is selected as a preprocessing step where features are selected based on properties of the data itself and independent of the induction algorithm . In the wrapper approach , the feature subset selection is found using the induction algorithm as a black box . The feature subset selection algorithm conducts a search for a good feature subset using the induction algorithm itself as part of the evaluation function .
John , Kohavi & Pfleger
( 1994 ) used the wrapper search . Kohavi search improves the ac method coupled with a hill climbing ( 1994 ) showed that best first curacy . One problem with expanding the search ( ie , using best first search and not hill climbing ) is that of overfitting : the accuracy estimation ( cross validation in both papers ) guides the search toward feature subsets that will be good for the specific cross validation folds ; however , overusing the estimate can lead to overfitting , a problem we discuss in Section 4 . of the state
In the common organization space search , each node represents a feature subset , and each operator represents the addition or deletion of a feature . The main problem with this organization is that the search must expand ( ie , generate successors of ) every node from the empty subset or from the full subset on the path to the best feature subset , which is very expensive . In Section 5 we introduce a way to change the search space topology by creating dynamic operators that directly connect to nodes considered promising given the evaluation of the children . These operators better utilize the information available in all the evaluated children . Our experimental results , shown in Sections 5 and 6 , indicate that compound operators help identify better feature subsets faster and that feature subset selection can significantly improve the performance of induction algorithms .
2 Relevant and Optimal
Features instances
The input to a supervised learning algorithm is a training set D of m labelled independently and from an unknown distriidentically distributed ( iid ) instance space . An unlabution ~D over the labelled belled instance X is an element of the set F1 x F2 x ¯ x Fn , where Fi is the domain of the ith feature . Labelled instances are tuples ( X , Y ) where Y is the label , or output .
Let Z be an induction algorithm using a hypothesis space 7/ ; thus Z maps D to h E 7/ and h E 7/ maps an unlabelled instance to a label . The prediction accuracy of a hypothesis h is the probability of correctly classifying the label of a randomly selected instance from the instance space according to the probability distribution ~D . The task of the induction algorithm is to choose a hypothesis with the highest prediction accuracy .
We now define relevance of features in terms of a Bayes classifier the optimal classifier for a given problem . A feature X is strongly relevant if removal of X alone will result in performance deterioration of an optimal Bayes classifier . A feature X is weakly relevant if it is not strongly relevant and there exists a subset of features , S , such that the performance of a Bayes classifter on S is worse than the performance on SO{f} . A feature is irrelevant if it is not strongly or weakly relevant . The set of strongly relevant features is called the core . Formalized versions of the above definitions can be found in John et al . ( 1994 ) .
There are three main problems with these definitions that make them hard to use in practice . First , many hypothesis spaces are parametric ( eg , perceptrons , monomials ) and the best hypothesis approximating the target concept from the family may not even use all the strongly relevant features . Second , practical learning algorithms are not always consistent : even with an infinite amount of data they might not converge to the best hypothesis . Third , even consistent learning procedures may be improved for finite samples by ignoring relevant features . These reasons motivated us to define the optimal features , which depend not only on the data , but also on the specific induction algorithm . An optimal feature subset , 8" , for a given induction algorithm and a given training set is a subset of the features , ~q° , such that the induction algorithm generates a hypothesis with the highest prediction accuracy . The feature subset need not be unique .
The relation between relevant and optimal features is not obvious . In Section 5 , we show how compound operators improve the search for optimal features using the ideas motivated by the above definitions of relevance .
3 Feature
Subset Heuristic
Selection as
Search and pattern recognition literature
The statistical on feature subset selection dates back a few decades , but the research deals mostly with linear regression . We refer the reader to the related work section in John et al . ( 1994 ) for key references . Langley ( 1994 ) provides a survey of recent feature subset selection algorithms , mostly in machine learning .
Most criteria for feature subset selection from the statistics and pattern recognition communities are algorithm independent and do not take into account the differences between the different induction algorithms . For example , as was shown in John et al . ( 1994 ) , features with high predictive power may impair the overall accuracy of the induced decision trees .
The task of finding a feature subset that satisfies a given criteria can be described as a state space search . Each state represents a feature subset with the given criteria used to evaluate it . Operators determine the partial ordering between the states .
In this paper , we use the wrapper method wherein the criteria to optimize is the estimated prediction accuracy . Methods that wrap around the induction algorithm , such as holdout , bootstrap , and cross validation ( Weiss & Kulikowski 1991 ) are used to estimate the prediction accuracy . To conduct a search , one needs to define the following : Search Space Operators The operators the search space are usually either "add feature" or "delete feature" or both . In the statistics literature , the term forward selection refers to a space containing only the "add feature" operator ; the term backward elimination refers to a space containing only the "delete feature" operator . The stepwise methods use both operators . In our experiments , we used both operators . in
Accuracy Estimation The heuristic in the wrapper approach is the estimated prediction accuracy . In our experiments , we used ten fold crossvalidation as the accuracy estimation function . function
Search Algorithm Any heuristic search algorithm can be used to conduct the search . In our experiments , we used best first search , which at every iteration generates the successors of the the best unexpanded node ( the node with the highest estimated accuracy ) . The termination condition was five consecutive non improving nodes . The initial node determines the general direction of the search . One typically starts forward selection from the empty set of features and backward elimination from the full set of features .
Kohavi
193
Accuracy
Rand forward selection
/ °
8O
75
65 60 55
Accuracy
Breast Cancer forward selection 80 78
/ ’
"
76 74 72 70
" .
,
"
"
Accuracy
Glass2 backward elimination
80
75
70
,
\
"
""
~’\~ Nodes
0
20 40 60 80 I00
50 i00 200 3bo 400 sbo sdo Nodes
0
20
40
60
"" Nodes 80
Figure 1 : Overfitting in feature subset selection using ID3 . The left graph shows accuracies for a random dataset The solid line represents the estimated accuracy for a training set of 100 instances , the thick grey line for a training set of 500 instances , and the dotted line shows the real accuracy . The middle and right graphs show the accuracy for real world datasets . The solid line is the estimated accuracy and the dotted line is the accuracy on an independent test set .
4 Overfitting
An induction algorithm overfits the dataset if it models the given data too well and its predictions are poor . An example of an over specialized hypothesis , or classifter , is a lookup table on all the features . Overfitting is closely related to the bias variance tradeoff ( Geman & Bienenstock 1992 , Breiman et al . 1984 ) : if the algorithm fits the data too well , the variance term is large , and hence the overall error is increased .
Most accuracy estimation methods , including crossvalidation , evaluate the predictive power of a given hypothesis over a feature subset by setting aside instances ( holdout sets ) that are not shown to the induction algorithm and using them to assess the predictive ability of the induced hypothesis . A search algorithm that explores a large portion of the space and that is guided by the accuracy estimates can choose a bad feature subset : a subset with a high accuracy estimate but poor predictive power .
If the search for the feature subset is viewed as part of the induction algorithm , then overuse of the accuracy estimates may cause overfitting in the featuresubset space . Because there are so many feature subsets , it is likely that one of them leads to a hypothesis that has high predictive accuracy for the holdout sets . A good example of overfitting can be shown using a no information dataset ( Rand ) where the features and the label are completely random . Figure 1 ( left ) shows the estimated accuracy versus the true accuracy for the best node the search has found after expanding k nodes . One can see that especially for the small sample of size 100 , the estimate is extremely poor ( 26 % optimistic ) , indicative of overfitting . The middle and right graphs in the figure show overfitting in small real world datasets .
Recently , a few machine learning researchers have reported the cross validation estimates that were used to guide the search as a final estimate of performance , thus achieving overly optimistic results . Experiments
194 KDD 95 using cross validation to guide the search must report the accuracy of the selected feature subset on a separate test set or on holdout sets generated by an external loop of cross validation that were never used during the feature subset selection process .
The problem of overfitting raised has been previously community by Wolpert ( 1992 ) and Schaffer ( 1993 ) , the subject has received much attention in the statistics community ( cf . Miller ( 1990) ) . in feature subset space in the machine learning that overfitting
Although the theoretical problem exists , our experis mainly a problem iments indicate when the number of instances is small . For our experiments , we chose reasonably large datasets and our accuracies are estimated on unseen instances . In our reported experiments , there were 70 searches for feature subsets . Ten searches were optimistically biased by more than two standard deviations and one was pessimistically biased by more than two standard deviations .
5 Compound Operators
In this section we introduce compound operators , a method that utilizes the accuracy estimation computed for the children of a node to change the topology of the search space .
The motivation for compound operators comes from Figure 2 that partitions the feature subsets into core features ( strongly relevant ) , weakly relevant features , and irrelevant features . An optimal feature subset for a hypothesis space must be from the relevant feature subset ( strongly and weakly relevant features ) . A backward elimination search starting from the full set of features ( as depicted in Figure 2 ) that removes one feature at a time , will have to expand all the children of each node before removing a single feature . If there are i irrelevant features and f features , ( i ¯ f ) nodes must be evaluated . In domains where feature subset selection might be most useful , there are many features real1 ic,a c aus]:rallan backward elimination
/
./f~"
"
86}
84
81[ /,."/"
Il" real acc crx backward elimination
I J’ .* ’ .o t/(
~ea~ large forward selection
80~
"
50
100 150 200 250 300
Nodes
~6
166 260 360 4~°des 2of i00 200 300 400
Nodes
Figure 3 : Comparison of compound ( dotted line ) and non compound ( solid line ) searches . The accuracy ( y axis ) is that the best node on an independent test set Mter a given number of node evaluations ( x axis ) .
No features yealdy
D¢2 am ¢ithersiongly~levant~ or weakly relevmat
’:~
)ll////
All feaures
Figure 2 : The state space . If a feature subset contMn.q an irrelevant feature , it is in the irrelevant area ; if it conrains only strongly relevant features it is in the core region ; otherwise , it is in the relevant region . The dotted arrows indicate compound operators . but such a search may be prohibitively expensive . than just the node with the maximum
Compound operators are operators that are dynamically created after the standard set of children , created by the add and delete operators , have been evaluated . Intuitively , there is more information in the evaluation of the children evaluation . Compound operators that led to the best children into a single dynamic operator . If we rank the operators by the estimated accuracy of the children , operator ci to be the combination of the best i + 1 operators . For example , the first compound operator will combine the best two operators . combine operators then we can define compound
The compound operators are applied to the parent , thus creating children nodes that are farther away in the state space . Each compound node is evaluated and the generation of compound operators continues as long as the estimated accuracy of the compound nodes improves .
Compound operators generalize a few suggestions the previously made . Kohavi ( 1994 ) suggested that from the set of strongly relevant search might start features ( the core ) . If one starts from the full set features , removal of any single strongly relevant feature will cause a degradation in performance , while removal of any irrelevant or weakly relevant feature will not . Since the last compound operator connects the full feature subset to the core , the compound operators from the full feature subset plot a path leading to the core . The path is explored by removing one feature at a time until estimated accuracy deteriorates . Caruana ~ Freitag ( 1994 ) implemented a SLASH version of feature subset selection that eliminates the features not used in the derived decision tree . If there are no features then ( ignoring orderings due to ties ) one of the compound operators will lead to the same node that slash would take the search to . While the SLASH approach is only applicable for backward elimination , compound operators are also applicable to forward selection . that improve the performance when deleted ,
In order to compare the performance of the feature subset selection algorithm with and without compound nodes , we ran experiments comparing them on different datasets . Figure 3 compares a search with and without compound operators . improve the search by finding nodes with higher accuracy faster ; however , whenever it is easy to overfit , they cause overfitting earlier .
Compound operators
6 Experimental
Results both
In order to compare the feature subset selection , we used ID3 and Naive Bayes , implemented in A//£C++ ( Kohavi , John , Long , Manley & Pfleger 1994 ) . The ID3 version does no pruning by itself ; pruning is thus achieved by the feature subset selection mechanism . The Naive Bayes algorithm assumes the features are independent given the instance label . The use of feature subset selection first for Naive Bayes was discretized using the discretization algorithm presented in Fayyad & Irani ( 1993 ) and implemented in A~LC++ . in Naive Bayes was suggested in Langley ~ Sage ( 1994 ) . The data
Kohavi
195
Dataset anneal breast ( L ) chess crx heart hypothyroid pima soybean lrg vote
Majority Accuracy 76174 14 70284 27 1066 5222 t 09 55514 19 55564 30 1055 95234 04 sizes
Feat Train Test ures 24 9 36 15 13 25 8 35 16
898 CV 5 286 CV 5 2130 690 CV 5 270 CV 5 2108 768 CV 5 683 CV 5 435 CV 5
6510=t=17 13474 13 61384 23
Dataset
( W ) australian breast cleve DNA horse colic mushroom sick euthyroid vehicle votel sizes
Feat Train Test ures 14 10 13 180 22 22 25 18 15
Majority Accuracy 55514 19 690 CV 5 65524 18 699 CV 5 303 CV 5 54464 29 51914 09 3186 2000 63044 25 368 CV 5 2708 5416 51804 06 1055 90744 05 2108 25774 15 846 CV 5 435 CV 5 61384 23
Table h Datasets and baseline accuracy ( majority ) . CV 5 indicates accuracy estimation by 5 fold cross validation . number after the 4 denotes one standard deviation of the accuracy .
The
( W )
Dataset anneal australian breast ( L ) breast chess cleve crx DNA heart horse colic hypothyroid mushroom pima sick euth soybean lrg vehicle vote votel Average
ID3
99554 02 80434 10 68204 29 9442 4 08 98694 03 71994 32 79864 17 90394 09 7222 t 30 75324 38 98584 04 100004 00 71754 21 96494 06 91944 10 73764 20 94024 04 8460+12
85.68
ID3 FSS 99334 02 85944 17 73434 23 94284 08 98874 03 77874 20 84354 16 92504 08 81484 28 84794 20 98774 03 100004 00 68364 30 9583 t 06 9327=t=13 69864 09 95634 08 86444 12
87.83 p val 0.23 1.00 0.92 0.45 0.65 0.94 0.97 0.97 0.99 0.99 0.65 0.50 0.18 0.22 0.80 0.04 0.97 0.87
C4.5
91654 16 85364 07 71004 23 94714 04 99504 03 73624 23 85804 10 92704 08 77044 28 84784 13 99204 03 100004 00 72654 18 9770+05 88284 20 69864 18 95634 04 86674 11
87.01
NB FSS Naive Bayes 96664 10 97664 04 85904 16 86094 11 70634 21 70994 23 96574 04 97144 05 94284 07 87154 10 83204 26 82874 31 85074 08 86964 12 93424 07 93344 07 84074 20 81484 33 8096=1=25 83704 12 99244 03 98584 04 99704 01 96604 03 73564 22 75514 16 95644 06 97354 05 9341 1 08 9136 1 20 59224 16 61234 13 9034=t=09 94714 06 87364 21 90804 20 86.63
87.97 p val 0.18 0.47 0.45 0.19 1.00 0.53 0.08 0.53 0.75 0.84 0.93 1.00 0.24 0.98 0.83 0.84 1.00 0.88
Table 2 : The accuracies The numbers after probability p values were computed using a one tailed the 4 indicate t test . that FSS improves ID3 and the second column indicates for ID3 , ID3 with feature subset selection the standard deviation of the reported accuracy . The first p val column indicates
( FSS ) , C4.5 , Naive Bayes , and Naive Bayes with FSS . the that FSS improves Naive Bayes . The the probability datasets to overfit are easier
Because small datasets repository least 250 instances . our approach , we chose real world UC Irvine at instances , a separate stances was used ; instances , describes general using from the ( Murphy & Aha 1994 ) that had For datasets with over 1000 test set with one third of the infor datasets with fewer than 1000 was used . Table 1 information about the datasets used .
5 fold cross validation
The initial node for our search was the empty set faster domains one would expect of features mainly because the search progresses and because many features best first caused by interacting cannot . or weakly relevant . The search is able to overcome small local maxima in real world to be irrelevant features , whereas hill climbing
Table 2 shows that feature subset selection signif
196 KDD 95
( over 90 % confidence ) improves ID3 on eight icantly degrades out of the eighteen domains and significantly the performance only on one domain . Performance of Naive Bayes significantly improves on five domains and significantly degrades on one domain . The average error rate for the datasets by 15 % for ID3 and Naive Bayes were inferior outperformed C4.5 after
Both to C4.5 , but both
ID3 and by 10 % for Naive Bayes . tested decreased ( relatively ) feature subset selection .
A similar experiment that C4.5 with feature proved C4.5 : 87.01 % to 87.60 % , a 4.5 % reduction the average
( not shown ) with C4.5 showed imsubset accuracy went up from selection in error . slightly
The execution time on a Sparc20 for feature subset selection using ID3 ranged from under five minutes for breast cancer cleve , heart ,
( Wisconsin ) , and vote about an hour for most datasets DNA took 29 hours , followed by chess at four hours . The DNA run took so long because of ever increasing estimates that did not really improve the test set accuracy .
7 Conclusions
We reviewed the wrapper method and discussed the problem of overfitting when the search through the state space is enlarged through the use of best first search . While overfitting can occur , the problem is less severe for large datasets , so we have restricted our experiments to such datasets . One possible way to deal with overfitting is to reevaluate the best nodes using different cross validation folds ( ie , shuffle the data ) . Initial experiments indicate that re evaluation of the best nodes indeed leads to lower estimates for those nodes , partially overcoming the overfitting problem . that change the search topology based on information available from the evaluation of children nodes . The approach generalizes previous suggestions and was shown to speed up discovery of good feature subsets . Our results indicated significant improvement both for ID3 and NaiveBayes and some improvement for C45 The average error rate for the datasets tested decreased ( relatively ) by 15 % for ID3 , by 10 % for Naive Bayes , and by 4.5 % for C45
We introduced compound operators
An issue that has not been addressed in the literature is whether we can determine a better starting point for the search . For example , one might start with the feature subset used by a learning algorithm when the subset is easy to identify , such as when using decision trees . this paper was
The work in
Acknowledgments done using the A//£g++ library , partly funded by ONR grant N00014 94 1 0448 and NSF grants IPd 9116399 and IRI 9411306 . We thank George John and Pat Langley for their comments . The reviewers comments were excellent , but many are not addressed due to lack of space .
References
Breiman , L . , Friedman , J . H . , Olshen , R . A . & Stone , C . J . ( 1984 ) , Classification and Regression Trees , Wadsworth International Group .
Caruana , R . & Freitag , D . ( 1994 ) , Greedy attribute selection , in W . W . Cohen & H . Hirsh , eds , "Machine Learning : Proceedings of the Eleventh International Conference" , Morgan Kaufmann .
Fayyad , U . M . & Irani , K . B . ( 1993 ) , Multi interval discretization of continuous valued attributes for classification International Joint Conference on Artificial ligence" , Morgan Kaufmann , pp . 1022 1027 . learning , in "Proceedings of the 13th Intel
Geman , S . & Bienenstock , E . ( 1992 ) , "Neural networks dilemma" , Neural Compu and the bias/variance tation 4 , 1 48 .
John , G . , Kohavi , 1 % . & Pfleger , K . ( 1994 ) , Irrele vant features and the subset selection problem , in "Machine Learning : Proceedings of the Eleventh International Conference" , Morgan Kaufmann , pp . 121 129 . Available by anonymous ftp from : starry . Stanford . EDU : pub/ronnyk/m194 , ps .
Kohavi , R . ( 1994 ) , Feature subset selection as search with probabilistic posium on Relevance" , pp . 122 126 . estimates , in "AAAI Fall Sym
Kohavi , R . ,
John , G . , Long , R . , Manley , D . &
Pfleger , K . ( 1994 ) , MLC++ : A machine learning library in C++ , in "Tools with Artificial Intelligence" , IEEE Computer Society Press , pp . 740 743 . Available by anonymous ftp from : starry . Stanford . EDU : pub/ronnyk/mlc/ toolsmlc , ps .
Langley , P . ( 1994 ) , Selection of relevant features in main "AAAI Fall Symposium on Rel chine learning , evance" , pp . 140 144 .
Langley , P . & Sage , S . ( 1994 ) , Induction of selective in "Proceedings of the Tenth bayesian classifiers , Conference on Uncertainty in Artificial gence" , Morgan Kaufmann , Seattle , WA , pp . 399406 .
Intelli
Miller , A . J . ( 1990 ) , Subset Selection in Regression ,
Chapman and Hall .
Murphy , P . M . & Aha , D . W . ( 1994 ) , UCI repository machine learning databases , For information contact ml repository@icsuciedu
Quinlan , J . 1 % . ( 1993 ) , C~.5 : Programs for Machine
Learning , Morgan Kaufmann , Los Altos , California .
Schaffer , C . ( 1993 ) , "Selecting method by cross validation" , 13(1 ) , 135 143 . a classification Machine Learning
Weiss , S . M . & Kulikowski , C . A . ( 1991 ) , Computer Systems that Learn , Morgan Kaufmann , San Mateo , CA .
Wolpert , D . H . ( 1992 ) , "On the connection between in sample testing and generalization error" , Complex Systems 6 , 47 94 .
Kohavi 197
