From : KDD 95 Proceedings . Copyright © 1995 , AAAI ( wwwaaaiorg ) All rights reserved .
Compression Based
Evaluation of Partial Determinations
Bernhard Austrian Research Institute
Pfahringer and Stefan Kramer for Artificial Intelligence
Schottengasse 3 , A 1010 Vienna , Austria
{bernhard,stefan}@aiunivieacat
Abstract
Our work tackles the problem of finding partial determinations in databases and proposes a compressionbased measure to evaluate them . Partial determinations can be viewed as generalizations of both functional dependencies and association rules , in that they are relationalin nature and may have exceptions . Extending the meazures used for evaluating association rules , namely support and conjidetace , to partial determinations leads to a few problems . We therefore propose a measure based on the minimum description this problem . We length assume a given database as efficiently as possible . The new measure estimates the compression achievable by transmitting partial determinations It and the cortakes the complexity rectness of a given partial determination , thus avoidin the presence of noise . We ing overfitting the also describe new measure . Preliminary in a few boolean domains are favorable . especially three different kinds of search using to remedy task of transmitting
( MDL ) principle the hypothetical instead of the original data . into account both empirical results
1
Introduction dependencies to the search for functional researchers in KDD have paid much attenRecently , ( Mantion rules ( Agrawal & nila & Raih& , 1994 ) and association deSrikant , 1994 ; Mannila et al . , 1994 ) . Functional pendencies are essentially relational and do not allow for the possibility of exceptions . Therefore , algorithms searching for functional dependencies would not find a dependency even if there was only one exception to the rule . instance , an algorithm might functional dependency folfrom a train schedule induce the
For lowing database :
{Direction}
+
{Minutes}
This means that the time when the train was a different schedule at night , would usually not find such a dependency . the direction of a train determines If there this kind of algorithm is leaving every hour .
On the contrary , associataon rules not only allow for of exceptions , but are essentially prob the possibility
234
KDD 95 in large databases where exceptions are likely is a useful feature for the search espeto and rules are propositional
This abilistic . cially arise . However , association therefore limited For instance , ation rule in their expressiveness . in a supermarket database the associ
{Bread , Butter}
*
{Milk ) that satisfy for all customers
It says , that customers might be found . that purchase bread and butter also purchase milk . Clearly , the statement will not be true that the information purchase bread and butter . Anyway , is useful even if the association rule holds only for 80 % of the customers the left hand side . Our work deals with a form of knowledge that association the propositional than is more expressive for real world data , since exrules , but also well suited ( Russell , 1989 ) , ceptions are possible . According dependencies we will call partial determanations . Partial determinations can be viewed as generalizations of both functional dependencies and association rules , in that in nature and may have exceptions . they are relational those non strict functional
Unfortunately , the measures evaluating association rules , support and confidence , are not suited for partial determinations . We therefore propose an alternative , compression based measure for evaluating partial determinations its use in search . in databases , and describe to
In section 2 we define partial determinations , tion 3 we introduce section 4 we report on empirical survey related work , and finally conclusions . in secthe compression based measure , in results , in section 5 we in section 6 we draw
2 Partial Determinations
Before we define partial determinations , we will present the definition of functional to ( Mannila & Riiha , 1994 ) : dependencies according
Given a relation schema ( ie a set of attributes ) R , a functional dependency over R is an expression X + Y , where X , Y C R . To define the semantics of such expressions , ( a table ) over R , let T be a relation for the ie a set of tuples over R . We write Dam(A ) domain of an attribute A E R and oom(X ) for the domain of a set of attributes X s R . The projection of a tuple is defined as the restriction oft on X ( here X C R ) . Now X + Y holds in T , denoted T b X ) Y , if all tuples U , v E T with u[X ] = v[X ] also satisfy u[Y ] = v[Y ] . t on a set of attributes X , denoted t[X ] ,
Partial determinatzons are generalizations of functional dependencies . They are expressions of the form index d is a number . The set of atX +d Y . The the left hand tributes X will be referred side of the partial determination , and Y will be referred to as RHS , the right hand side . to as LHS ,
Semantically , X fd Y ,
fd Y holds if d is the determination
X defined by ( Russell , 1989 ) . The determination the probability the same values of Y , provided ues of X . Note that d(X,Y ) to a particular mapping in r , denoted T b factor d(X , Y ) as factor is two randomly chosen tuples have they have the same valregard is defined without from oom(X ) to Dam(Y ) . that the
‘The tuples
Corresponding in T determine to a partial determination , we define following way . The doin a mapping pdxddy is defined by the LHS and the main of the mapping is defined by the RHS of the partial determirange itnation . self : For all tuples u that are equal under the projection X , we call the most frequently occurring u[Y ] the to majority tuple . pdx+dy maps tuples of oom(X ) respective majority their A tuple u E T is called an exception to the mapping pdxbay , if u[y1 # pdx ,dy(u[X] ) . Depending on the number of exceptions , is more or less ‘functional ” . the partial determination tuples in Dam(Y ) . the mapping the notion of a partial determinathe statement X ,d Y and the is not problemis a correspondence between the ref function pdx+6y This
In the following , is used for both tion corresponding atic , since there erents . for partial determinations
Partial Determinations
3 Evaluating in databases reSearching quires a function measuring their goodness . Next , we will show why support and confidence , the measures of association rules , can not be generalized for this purpose .
Support
{11,12! . . . . ms and Confidence
3.1 For simplicity , we will define support and confidence the same notation as for functional dependenusing I J be a relation schema cies . Let R = with attributes over a binary domain ( 0 , 1 ) . ( Agrawal Let & Srikant , be a relation over the relation schema T = {tl,tz R . An association rule is an expression X =+ Y , where = 0 . If W E R and t[A ] = 1 X c R , Y c R , and XnY = 1 . The rule X 3 Y for all A E IV , we write t[W ] holds in T with confidence c if c % of the tuples in T those attributes
1994 ) call atems . )
, . . . , tn ) for which t[X ] = i also t[YJ = i . The rule X + Y has support s in T if t[XY ] = 1 for 5 % of the tuples in T . those measures for partial determina
Generalizing
2 .
3 .
4 . tions leads to four different problems : 1 . that satisfy in T . So support and confidence the tuthe rule is the rule , and is 100 % minus the percentage of excepare no difto partial determine
When we consider partial determinations , ples can be divided into those which satisfy and those which are exceptions . Here , support the percentage of tuples confidence tions ferent measures when applied tions . is added to the LHS of the When another attribute gets “ betdependency , the evaluation automatically ter ” or , in the worst case , remains the same . The reason is that the number of exceptions will decrease in most cases , and at worst will remain constant . Conversely , when adding another RHS attribute , the evaluation automatically gets “ worse ” or , in the best case , remains the same . The reason for this is that the number of exceptions will increase in most cases , and at best will remain constant . The measure does not plexity of needed to encode the relationship between main and the range . Thus , the partial determination just needs to have enough attributes order to obtain a high support . Obviously , cause overfitting of the data . In order in the LHS in this may the cominformation take into account the mapping , the do that the is , to avoid these problems , we propose a the so called compression based measure based on ( RissaMinimum Description Length ( MDL ) principle nen , 1978 ) . This principle allows taking into account a theory ’s accuracy and its complexity simultaneously . The key idea is to measure the joint cost of coding a thetheory and of coding ory . A good introduction in this paper we only deal with ( Pednault , In partial determinations ranging over boolean attributes and having a single consequent . to MDL can be found in terms of that the data
1991 ) .
3.2
A New Compression Based for Partial Determinations
Measure task of transmitting a We assume the hypothetical If we can find a given database as efficiently as possible . transgood partial determination instead of the raw mitting data values can improve efficiency considerably . The MDL principle allows us to estimate the degree of compression achievable by using a given partial determination . this partial determination for a given attribute ,
The MDL principle tries to measure both the sim(in our in a common currency , in terms of the number of bits needed for entheory and data . ( Cheeseman 90 ) defines the plicity and the accuracy of a particular setting : partial determination ) namely coding theory
Pfahringer
235 message length of a theory ( called model in his article ) aS :
3.3
Bounds Measure for the Compression Based
Total message length
=
Message Message length length to describe to describe the model + the data , given the model .
This way a more complex theory will need more bits to be encoded , but might save bits when encoding more data correctly . The theory with the minimal total mesis also the most probable theory explaining sage length ( Rissanen , 1978 ) . Actually we don’t really the data need to encode! we just need a formula estimating the number of bits needed if we were to encode a theory and data in terms of that theory . to reconstruct the original attribute that will allow the hypothetical
So we have to define an encoding for partial deterreceiver of a minations transmission values . Therefore we have to specify which attributes are is used to compute used in the LHS , which mapping the value of the RHS attribute , and which in ( the excepthe relation contradict The exact definition of the coding cost of a tions ) . partial determination is given in figure 1 . for boolean attributes the determination tuples
Total cost ( 1 ) the attributes ping and for encoding ( encoding is the sum of the cost for selecting ( the theory ) and for defining their mapto the mapping the exceptions the data in terms of the theory ) .
The selection of attributes
( 2 ) is defined by specify ing which of all possible attributes are used .
For encoding the mapping only the 21usedAttrsl different mapping values need to be encoded . This can be done by specifying which of all of these values are “ true ”
( 3 ) .
Additionally we have to “ correct ” specifying which examples ( tuples assigned a wrong value by the mapping the mapping by in the relation ) are
( 4 ) .
For estimating the cost of encoding the selection of E elements out of N possible elements ( 5,6,7 ) we just use the theoretical entropy based bound provided by ( Shannon & Weaver , 1949 ) . As we do not have to really encode the data , using this theoretical bound makes selection could also be coded sense.’ Alternatively , in ( Quinlan 1993 ) . The and estimated as described advantage of the entropy based estimate is its efficient computability ( only 2 logarithms need to be computed versus 2 * E logarithms ) .
Now according to the MDL principle tial determination which minimizes function , most compressive determination ) determination the one with ie given the tuples of the relation . the one parthe above cost(the is the most probable the smallest bit cost
‘Especially as there are coding methods known ( eg arithmetic coding ) , that can actually achieve this bound .
236
KDD 95
We can deduce two bounds useful for searching for partial determinations . First of all the maximal number of LHS attributes is a function of the number of tuples in a given relation ( BOUNDS ) : of a useful partial determination
ILHS~D 1 < log2 ITuples(
This follows from the fact that encoding the raw values of the RHS attribute ( without using a partial determination PD ) would yield a bitstring of an approximate size of ITuplesl . Now a partial determination PD using 1.rogalr upresl Ln3 abLrruubes WOUIU uenne d rnapp~ng with a truth table ITuplesl entries . Additionally we have to encode the selected attributes . Therefore we would get : containing 21°galTuplesl =
Irn . f I TTTC( LI IL. L
.,A J L! L I
Using such a partial determination would be more costly than directly encoding the data . is a lower bound
The second bound
( ie adding attributes the cost achievable when specializing to the LHS of ) a particular determination PDN . Assume PPthcat thin UllcuU “ Ill ” I.2 sulting mapping has p bits “ on ” and n bits “ off ” . If we add an attribute AN+~ we get BOUNDS : to this partial determination ,
~tnacl N attr;hntan U ” b ”
Aatnrm;ncxt;nn u~u~Illllllcuul ” ll
1 . ul ” “ IIVUU~ ”
~.nrl thn UllU UIL ” for
> +
COStm@&+1 ) costehoose(N + 1 , IAllAttrsl ) c&hoose(~qP , 4 , 2N+1 ) This is because we have to encode one more attribute , in the regarding coding the resulting mapping will cost best case still have only min(p,n ) bits “ on ” or respectively , and in the best case the number of “ off ” _____ L1 excepblorls
.:I , 1 . ^ ^ ztxu . Will ve the gain larger .
( a determination
This BOUND:! also tells us that specializing a total already having zero as determination exceptions ) will not yield a better determination , its respective cost will be strictly 3.4 Rule Interest Using in COS~MDL ( PD ) ) as the actual evaluation isfies the stated by ( Piatetsky Shapiro , nally formulated be reformulated 1 . If the attributes of the left hand side are statistically function satrule interest measures 1991 ) . They were origirules and can easily for propositional for partial determinations :
Measures cost coding three principles
( costMDL(@ ) independent of the attributes of the right hand side , function shall be zero . the value of the rule interest 2 . If the support gets bigger , the value of the rule in for terest function shall increase .
3 . If the number of tuples in and the support stays constant , interest function shall decrease . the database increases the value of the rule
COS~MDL(PD ) cost(LHS ) cost(Mapping ) cost(Ezceptions )
=
=
=
= cost(LHS ) + cost(Mapping )
+ cost(Ezceptions ) cost,hoose(lUsecZAttrsl , IAllAttrsl ) cost,hooJe(lTTUeEIZtTieSI , IAllEntriesl ) cost,hoose(lEzxeptionsl ,
IAlIEzamplesl )
~ost,~~~~~(E , N )
= N * entropy(E , N ) entropy(E , N )
=
(plog(E/N )
+ plog(1 E/N ) )
‘log(‘ )
=
1 “ P * logz(P ) ftLeT:ise
Figure 1 : The Definition of the Coding Cost of a Partial Determination
( 1 )
( 2 )
( 3 )
( 4 )
( 5 )
( 6 )
( 7 )
_
_
The first condition theory ( Rissanen , 1978 ) . Empirically , is satisfied by MUL based measures because of the way they are derived from probability this is also in experiments on random data . The second evident condition true for our measure , since the number of exceptions decreases . The third condition is trivially satisfied as well , because the tuples that are added can only be exceptions if the support stays the same . is obviously
4 Empirical
Results may fail for evaluation .
For a preliminary empirical evaluation of the above formula we have done a few experiments on mostly artificial boolean data sets with different levels of attribute noise . As enumerating and evaluating all possible subsets of attributes up to a size given by BOUNDS is clearly out of the question for realistically sized data sets , one must reasonable candidates For this purpose we have implemented 1 . three different search strategies : rely on search to provide to find a good partial deespecially if the first attributes chosen
Hill Climbing : termination , during search are irrelevant . Depth Bounded Backtracking : prune some of the search space , but unfortunately for such small sets of attributes BOUND2 is rarely ever effective , because for small sets exception costs the total coding costs . Bounded tend to dominate depth first to find partial search will of course fail determinations more complex than the bound . On the other hand , if it fails to find any partial determination , we know that no small determination exists . And if we can afford to search up to a depth equal
BOUND2 allows to
2 .
3 . find tries the best partiai de to BouNDi , we wiii either termination or prove its non existence . to be Stochastic Search : This third strategy both more complete than hill climbing and to be less costly than bounded depth first search . This strategy works as follows : N times do randomly generate attribute subsets of a size not exceeding BOUNDS . If such a random subset has a smaller bit cost than the empty set , this subset is already a useful partial determination by itself . But due to its random generation it might contain more attributes than necessary . Therefore this partial determination , we perform an unbounded depth first search on this subset , which will return the optimal partial determination possible for this subset . The global effect of this search strategy is that especially in the presence of attribute noise it tends to find better partial determinations at still reasonable runtime costs . We have conducted a few experiments using purely artificial boolean domains to investigate the effects of attribute noise . Table 1 summarizes search results for a particular boolean relation involving 15 attributes : to further improve a2 = a7 = a10 = a15 = or(a0 , al ) or(a3 , ~4 , a5 , a6 ) parity(a8 , a9 ) parity(al1 , a12 , a13 , a14 )
This particular experiment used 5000 randomly drawn ie with a examples . Attribute probability value was switched from 0 to 1 or vice versa . Due to space reasons we noise was set to lo % , of 10 % each attribute
Pfahringer
237
I
T 20 23 70 20 31 70 20 31 70 20 20 70
Det
0,l 0,l
8,9 8,9
2,3,4,5,6,9,10,11,13
6,10,11,12,13,14
Attr a2 a7 a10 al5
S s ” s ”
D&z ifs ”
DB3 HC ss DBs HC ss DBa
Gain 0 1099 1099 0 116 0 0 939 939 0 317 0
Exe 30 17 17 15 13 15 50 25 25 49 34 49 found for selected
Table 1 : Partial determinations boolean attributes . Search methods are hill climbing ( HC ) , stochastic search ( SS ) , and depth bounded backtracking to level three ( DBs ) . Gain is the gain in number of bits relative ( Exe ) ( T ) are in seconds , and Det are percentages , runtimes are the LHS attributes of the respective determination . to the empty set , exceptions
P found found the gain ( ie costing the determination ’s mapping results for the RHS attributes . only report minations really have less exceptions empty set . For each type of search we list mination COStMDL(PD ) ) ber of exceptions of the determination when applying ples , and the runtimes fails
All deterthan the the deter(if any ) , ex ressed in number of bits , the numin percentages to the tu any determinations . for all attributes Stochastic search finds determinations search takes in reasonable time . Bounded depth first more finding better solutions . For OR4 and PARITY4 , and a depth bound of three , this was to be expected . Only one out of all results possibly shows signs of overfitting ( SS for A7 ) . than both other methods without in seconds . to
Hill climbing time find in the “ natural ” boolean attribute
Table 2 summarizes the results of searching for parvoting domain’ . tial determinations This domain is in principle a boolean domain , but does have missing values . Therefore we introduced an additional for each original atthe value is known or not . tribute specifying whether ( democrat or republican ) The binary class information is provided as attribute 0 . So we have a total of 33 boolean attributes and 300 examples . Due to space reasons we only report for a few prototypical RHS attributes . From the table we see that gain and exceptions correlate as expected , namely that for larger gains we also get fewer exceptions . Complete search finds slightly better solutions at a pronouncedly only larger time cost . Both incomplete methods sometimes that fail the fol(see eg A20 or A22 ) . But note lowing well known simple determination between atZVOTING is one of the databases available at the UC results
Irvine Machine Learning Repository
238
KDD 95 tributes A0 and A8 is found by all three methods :
5 Related Work
In this section we briefly describe how our work relates to the closest work found in the literature . l l l the that is bigger than the data . Furthermore , the proportion the determination left hand side . Since Shen is not and described a method respect the counter support test suggests their significance . that searches for in a large knowledge base . and the term “ partial deterfor evaluating facts . to given of tuples holds In other words , he estimates the confidence of the dependency the same argument
( Russell , 1989 ) introduced mination ” partial determinations with Briefly , Russell estimates in the database for which through sampling . nothing else but given a set of tuples . Therefore as for support and confidence applies . ( Shen , 1991 ) describes an algorithm three kinds of regularities One of those regularities are determinations , to those having a single attribute they are restricted in looking for more complex dependencies , there is no need to avoid overfitting the determinations of interest are like association rules in that The algorithm generthey have binary attributes . ates such simple determinations and returns them if the support and if a statistical ( Schlimmer , 1993 ) proposes an algorithm returns every “ reliable ” partial determination with a complexity lower than a user defined threshold . The reliability measure is supposed to measure the “ functional degree ” of the map given subsequent data . Schlimmer argues that than functions that measure the functional degree of the map given data . The reliability measure is used the current If a determination for pruning : is not reliable , one can conclude that all determinations with a more complex domain will not be reliable and need not be considered . The algorithm performs a complete search in the space of partial determinations up to threshold , but it does not a user defined complexity avoid overfitting the data , since it does not have a penalty We recognize two problems with ability measure : 1 . The reliability measure only evaluates the domain it of the partial determinations . does not consider how the tuples from the domain map to the tuples from the range . Clearly , not every two functions with the same domain and the same range , but different mappings , should have the same values of the evaluation function . As a take further into account how the projections of the tuples to the attributes side are distributed . consequence , the measure does not for overly complex dependencies . the right hand
Consequently , this particular this is better from reli
0
Search Attr BC ss D& HC ss D& HC ss D&
12
20
Gain 1 Exe 5 194 5 194 194 5 14 87 16 79 14 87 37 1 0 50 37 1
T 6 15 57 6 14 52 8 14 52
Det 8 8 8 10,24,26 lo,26 10,24,26 4,32
4,32
Attr 2
14
22
Gain 19 18 19 104 104 106 0 11 20
Exe 28 30 28 15 15 13 35 27 24
T 8 14 52 7 15 52 12 14 52
Det 6,?2,23 6,16 6,12,23 16 16 0,10,16
0,14 0,4,18
Table 2 : Partial determinations found for some attributes of the voting domain .
References
2 . The algorithm does not avoid overfitting the data , since it does not have a penalty for overly complex dependencies . We believe that algorithms searching for more complex dependencies have to cope with the problem of overfitting .
6
Further Work and Conclusion
We plan to extend this work by experiments with other search strategies , eg genetic algorithms and combinations of search algorithms . Experiments with large real world databases will have to be the next step . This will require generalizing the measure for multi valued attributes . More importantly , we will investigate how knowledge can be included to obtain meaningful dependencies . et al . , 19941 show how hierarchies of attributes can be used to select rules from large sets of discovered association rules . Alternatively , hierarchies could be utilized by operators dtiring ihe search for dependencies . ~urt~ierrIiore , we will investigate determinations having more than one RHSattribute .
[ Klemettinen in order
In contrast the XDi based
In summary , we presented a new compression based measure to evaluate partial determinations and its use for search . Partial determinations are a useful form of knowledge since they are more expressive than association rules but also allow for exceptions . The possibility of exceptions makes this kind of dependency interesting to other to search for in real world data . to the aumeasures of partial determinations thors , the data . The usefulness of the approach is supported by experimental evidence . Acknowledgements sponsored by der Wissenschaftlichen Forschung ( F WF ) under grant number P10489 MAT . Financial support for the Austrian Reis provided search by the Austrian Federal Ministry of Science , Research , and Arts . We would like to thank Gerhard Widmer for valuable discussions . is Fends zur Fiirderung function avoids Wdiiiiig the Austrian for Artificial
Intelligence
Institute research known
This
M . , Ma&la
Finding
_
Inference .
Algorithms for
Interesting Rules
International Conference on Information
Agrawal R . and Srikant R . : Fast Algorithms for Mining Association Rules . Proceedings of the 20th VLDB Conference , Santiago , Chile , 1994 . Cheeseman P . : On Finding the Most Probable Model , in Shrager J . , Langley P(eds ) : Computational Models of Discovery and Theory Formation , Morgan Kaufmann , Los Altos , CA , 1990 . H . , Ronkainen P . , Toivonen Klemettinen H . , and Verkamo AI : from Large Sets of Discovered Association Rules . Proceedings of the Third and Knowledge Management , ACM Press , 1994 . Efficient Mannila H . , Toivonen H . , and Verkamo AI : Algorithms for Discovering Association Rules . Proceedings of the AAAI 94 Workshop on Knowledge Discovery in Databases , 1994 . Mannila H . and R%ihi K J : Inferring Functional Dependencies From Relations . Data d Knowledge Engineering 12 ( 1994 ) 83 99 , 1994 . Pednault EPD : Minimal tive Piatetsky Shapiro 1991 . Piatetsky Shapiro tation In : Databases , G . Piatetsky Shapiro AAAI Press , 1991 . Quinlan Morgan Kaufmann , San Mateo , CA , 1993 . J . Rissanen : Modeling by Shortest Data Description . Automatica , Russell SJ : The Use of Knowledge T ,.,,I , , J . C1 “ . lx& D,l,:,L,: , , UUG‘z . “ I‘ . I ICIIIalI I U ” UUILIIL~ , UVILUVIL , 1989 . Schlimmer JC : Efficiently Inducing Determinations : Complete and Systematic Search Algorithm timal Pruning . Proceedings of the 10th International ference on Machine Learning , 1993 . Shannon CE and Weaver W . : The Mathematical of Communication , Shen W M : Discovering Regularities edge Bases . Proceedings of the 8th shop on Machine Learning , 1991 .
Inducin Databases , G . ( eds. ) , AAAI Press ,
JR : C4.5 : Programs for Machine Learning ,
Knowledge Discovery and WJ Frawley
A that Uses OpCon from Large KnowlInternational Work and Presenin ( eds. ) ,
In : Knowledge Discovery
G . : Discovery , Analysis , of Illinois Press , 1949 .
Length Encoding and in Analogy and In of Strong Rules . and WJ Frawley
14:465 471 , 1978 .
In :
Theory
University
Pfahringer
239
