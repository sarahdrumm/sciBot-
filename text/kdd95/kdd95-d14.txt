From : KDD 95 Proceedings . Copyright © 1995 , AAAI ( wwwaaaiorg ) All rights reserved .
Decision Tree Induction : How Effective is the Greedy Heuristic ?
Sreerama
Murthy and Steven
Salzberg
Department of Computer Science
Johns Hopkins University Baltimore , Maryland 21218 lastname@csjhuedu
Abstract
Most existing decision tree systems use a greedy approach to induce trees locally optimal splits are induced at every node of the tree . Although the greedy approach is suboptimal , it is believed to produce reasonably good trees . In the current work , we attempt to verify this belief . We quantify the goodness of greedy tree induction empirically , using the popular decision tree algorithms , C4.5 and CART . We induce decision trees on thousands of synthetic data sets and compare them to the corresponding optimal trees , which in turn are found using a novel map coloring idea . We measure the effect on greedy induction of variables such as the underlying concept complexity , training set size , noise and dimensionality . Our experiments show , among other things , that the expected classification cost of a greedily induced tree is consistently very close to that of the optimal tree .
Introduction
Decision trees are known to be effective classifiers in a variety of domains . Most of the methods developed have used a standard top down , greeay approach to building trees , which can be summarized as follows . Recursively do the following until no more nodes can be split : choose the best possible test at the current node according to some goodness measure and split the current node using that test ; after a complete tree is grown , prune it back to avoid overfitting the training data ( Breiman et al . 1984 ; Quinlan 1993 ) . The choice of a "best" test is what makes this algorithm greedy . The best test at a given internal node of the tree is only a locally optimal choice ; and a strategy choosing locally optimal splits necessarily produces suboptimal trees
( Goodman & Smyth 1988 ) .
Optimality of a decision tree may be measured in terms of prediction accuracy , size or depth . It should be clear that it is desirable to build optimal trees in terms of one or more of these criteria . Maximizing classification accuracy on unseen data ( within the constraints imposed by the training data ) is obviously desirable . Smaller , shallower decision trees imply better comprehensibility and computational efficiency . Shallow trees are also more cost effective , as the depth of
222 KDD 95 a tree is a measure of its classification ever , because the problem of building optimal trees is known to be intractable ( Hyafil & Rivest 1976 ; Murphy & McCraw 1991 ) , a greedy heuristic might be wise given realistic computational constraints . cost . How
The goal of this paper is to examine closely the consequences of adopting a greedy strategy . We ask the question , if we had unlimited resources and could compute the optimal tree , how much better should we expect to perform ? An alternative way of asking the same question is , what is the penalty that decision tree algorithms pay in return for the speed gained by the greedy heuristic ?
Setting up the Experiments
Our experimental framework is quite simple we use C4.5 ( Quinlan 1993 ) and CART ( Breiman et al . 1984 ) to induce decision trees on a large number of random data sets , and in each case we compare the greedily induced tree to the optimal tree . The implementation of this framework raises some interesting issues .
Optimal Decision Tree for a Training Set . The problem of computing the shallowest or smallest decision tree for a given data set is NP complete ( Hyafil Rivest 1976 ; Murphy & McCraw 1991 ) , meaning that it is highly unlikely that a polynomial solution will be found . Previous studies that attempted comparisons to optimal trees ( eg , ( Cox , Qiu , & Kuehner 1989 ) ) used approaches like dynamic programming to generate the optimal trees . Because it is slow , this option is impractical for our study , in which we use hundreds of thousands of artificial data sets . Our solution is to first generate a random decision tree D , and then generate data sets for which D is guaranteed to be the optimal tree . The main idea behind ensuring the optimality of a random decision tree is coloring its leaf nodes with appropriate class labels . is a real valued vector Xi = An instance plus a class label c~ . xis are the at(x~l,zi2,,xia ) tributes of Xi , and d is its dimensionality . Consider a binary decision tree D in two attributes . ( The ensuing argument applies to arbitrary dimensions . ) D induces a hierarchical partitioning of the attribute space , which can be drawn as a map M . The boundaries of M are the splits ( test nodes ) in D , and the regions of M are the leaf nodes in D . Assuming that each leaf node of D contains instances of only one class , we can color M by assigning a distinct color to each class in D . Now consider a data set S consistent with D , which has the additional property that S requires every leaf node of D , ie , every leaf node of D contains at least one instance of S .
It should be clear that D is the smallest binary decision tree consistent with S , provided no two neighboring regions of M have the same color . Informally , any decision tree that has fewer leaves than D needs to either ignore some decision regions of D , or merge ( parts of ) two or more regions into one . The former is ruled out because S requires all decision possibility regions in D . The latter is impossible because no decision regions of the same color are adjacent , so no two regions can be merged . Hence , any decision tree consistent with S has to have at least as many leaf nodes as D . Moreover , if D was a perfectly balanced tree to start with , then any decision tree consistent with S has to be at least as deep as D .
In our experiments , we start with perfectly balanced , empty trees . We then generate random tests at the decision nodes , ensuring that no leaf region is empty . Finally we color the leaves to ensure optimality with respect to size , using the following procedure . We first compute the adjacency information of the leaf nodes . After initializing the class labels at all leaf nodes to k ( > number of leaves ) , we go back and change the label of each leaf to be the smallest number in [ 1 , k ] that is not yet assigned to any neighbor . This heuristic procedure worked quite well in all our experiments . ( For instance , decision trees of 64 leaf nodes in the plane were colored with 5 classes on average . ) A sample random decision tree in 2 D , along with the class labels assigned by the above coloring procedure , is shown in Fig 1 .
Tree Quality Measures . In all our experiments , we report tree quality using six measures :
¯ Classification data ; accuracy : accuracy on the training
¯ Prediction accuracy : accuracy on an independent , noise free testing set ;
¯ Tree size : number of leaf nodes ; ¯ Maximum depth : distance from the root to the far thest leaf node ; ( distance from A to B is the number of nodes between , and including , A and B )
¯ Average depth : mean distance leaf node in the tree ; from the root to a
¯ Expected depth : number of tests needed to classify an unseen example . We compute expected depth by averaging , over all the examples in the testing set ,
2
2
1
1
2
Figure 1 : The partitioning induced by a random decision tree of 32 leaf nodes . Class labels assigned by our coloring procedure are shown ( for most nodes ) . the length of the path that the example followed in the tree .
Control Variables . The effectiveness of greedy induction can not be measured independently of training data characteristics . For instance , if the training data is very noisy , it is likely that no induction method will be able to generate accurate trees . In this paper , we study the effectiveness of greedy induction in controlled settings with respect to the following parameters : ¯ concept complexity ( measured as the size of the op timal decision tree ) ,
¯ size of the training set , ¯ amount and nature of noise in the training data ( noise in class labels versus noise in attributes ) , and induction
¯ dimensionality ( number of attributes ) . Tree Induction Methods Used . The tree methods we use are C4.5 ( Quinlan 1993 ) and CART ( Breiman et al . 1984 ) . One main difference between C4.5 and CART is the goodness criterion , the criterion used to choose the best split at each node . C4.5 uses the information gain 1 criterion , whereas CART uses either the Gini index of diversity or the twoing rule . All the experiments in this paper were repeated using information gain , Gini index and twoing rule . In no case significant differences did the results show statistically between goodness measures the differences in accuracies , sizes and measurements of depth were always much less than one standard deviation . For brevity , we report only the results with information gain ( ie , C4.5 ) in the rest of this paper . We implemented all the goodness measures using the OC1 system ( Murthy ,
1Quinlan suggested gain ratio as an improvement over information gain . However the two measures are equivalent in our experiments as all our decision trees are binary .
Murthy
223
Optimal
Size
8 16 32 64
Training
Set 1000 1000 1000 1000
Classification
Accuracy
100.0 100.0 100.0 100.0
Prediction Accuracy 9954 01 9874 03 9724 06 9434 09
Tree Size
984 17 2074 33 404 t 68 7174 103
Maximum 484 07 ( 3 ) 724 10 ( 4 ) 93=t=10 ( 5 ) 1154 12 ( 6 )
Depth Average
Expected 294 03 ( 3 ) 36=1=03 ( 3 ) 394 04 ( 4 ) 50=t=04 ( 4 ) 50+05 ( 5 ) 634 05 ( 5 ) 74 t 05 ( 6 ) 584 05 ( 6 )
Table 1 : Effects of concept complexity . No noise in data . Numbers in parentheses are for the optimal trees .
Kasif , & Salzberg 1994 ) . Although C4.5 and CART differ in respects other than the goodness measures , we have not implemented these differences . In the experiments in which the training data is noise free , no pruning was used with either method . In the experiments using noisy training sets , we augment both methods with cost complexity pruning ( Breiman et al . 1984 ) , reserving 10 % of the training data for pruning .
Experiments
This section describes five experiments , each of which is intended to measure the effectiveness of greedy induction as a function of one or more control variables described earlier . The procedure is more or less the same for all experiments . For each setting of the control variables : generate 100 random trees with no class labels ; for each tree Dom generated in the above step : color Dopt with class labels ; generate a large , noise free testing set for which Dom is optimal ; generate 50 training sets using Dopt ; for each training set T : greedily record D and Dopt ; induce a tree D on T ; report the mean and std . dev . of the quality measures for the 5000 trees .
The instances in the training and testing sets are always generated uniformly randomly , and are labeled using the optimal decision tree . The size of the testing set is linearly dependent on the complexity of the optimal tree and the dimensionality of the data , whereas the size of the training set is a control variable . More precisely , [ T[ = C * ( D 1 ) * 500 , where [ T[ is the size of the testing set , C is the size of the optimal tree and D is the number of attributes . For instance , for a size 16 concept in 4 dimensions , we use a testing set of size 16 ¯ ( 4 1 ) * 500 = 24,000 . We ensure that no subtree of the optimal decision tree is consistent with the testing set .
In all the tables in this paper , each entry comprises of the average value of a tree quality measure over 5000 trees and the standard deviation ( one a ) . Numbers parentheses correspond to the optimal trees . The a ’s are omitted when they are zero . Optimal values are omitted when their values are obvious . The optimal trees always give 100 % prediction accuracy in our ex
224 KDD 95 periments , because the testing addition , the training data is noise free . they give 100 % classification set has no noise . In accuracy when
Experiment 1 : The purpose of this experiment is to evaluate the effectiveness of greedy induction as a function of the size of the optimal tree . All training sets comprise of 1000 random 2 D instances . There is no noise in the training data . Table 1 summarizes the results . Observations : The prediction accuracy of greedily induced trees decreases as the size of the optimal tree for the data increases . This can be either be due to the inadequacy of greedy search or due to inadequate training data . ( The training set size remained at 1000 though the concept complexity increased from 8 to 64 . ) In Experiment 2 , we increase the size of the training set in proportion with the size of the optimal tree , in order to better isolate the effects due to greedy search . The difference between the sizes of greedily induced and optimal trees increases with the size of the optimal tree in Table 1 . However , it can be seen on closer observation that the variances , not just the differences in size , are increasing . Greedily induced tree sizes are just more than one a away from the optimal in 3 out of 4 rows , and less than one std . dev . away for concepts of size 64 .
The maximum depth measurements in Table 1 show that greedily induced trees can have decision paths which are about twice as long as those in the optimal trees , even for moderately complex concepts . However , the average depth measurements show that the decision paths in greedily induced trees only have about one test more than those in the optimal trees . In terms of the third depth measurement , the expected depth , greedily induced trees are almost identical to the optimal ones , for all the concept sizes considered in this experiment . This is a very desirable , although sometrend which is seen consistently what counterintuitive , throughout our experiments . ( Note that no pruning was used in this experiment . )
Experiment 2 : The purpose of this experiment is to isolate the effects of concept complexity , from those of the training set size . The size of the training sets now grows linearly with the concept complexity 25 training points on average are used per each leaf of the optimal tree . There is no noise . Table 2 summarizes
Optimal
Size 8 16 32 64
Training
Set 200 4OO 800 1600
Classification
Accuracy
100.0 100.0 100.0 100.0
Prediction Accuracy 975+07 971±07 9664 07 964±06
Tree Size
85±15 175±33 3804 71 763±123
Maximum 44=1=06 ( 3 ) 66±09 ( 4 ) 91±10 ( 5 ) 116=t=12 ( 6 )
Depth Average
34+03 ( 3 ) 47±05 ( 4 ) 62±05 ( 5 ) 75±06 ( 6 )
Expected 284 03 ( 3 ) 38±05 ( 4 ) 4S±05 ( 5 ) 58±06 ( 6 )
Table 2 : Effects of concept complexity and training set size . No noise . Numbers in parentheses are for the optimal trees .
Accuracy
Classification loo.o ( loo.o )
Class Noise o % 5 % 921±13 ( 951±001 ) lO % 877±13 ( 905 4 002 ) 15 % 835+13 ( 861±005 ) 20 % 797±14 ( 819±005 ) 25 % 7614 14 ( 778+003 )
Prediction Accuracy 939±14 895±24 882 1 26 866±29 849±31 8314 34
Tree Size
3114 62 219±51 2224 51 224±54 227±52 2334 57
Depth
Maximum Average I Expected 82±09 70±08 70±08 70+08 71±08 714 08
56±04I 49±05 49+04 49 t 05 49±05 48±05
48±05 44±04 44±04 44±04 44±04 44±04
Table 3 : Effects of noise in class labels . Numbers in parentheses are for the optimal trees . Optimal trees are of size 32 . the results . Observations : It is interesting to note that the prediction accuracy does not drop as much with increase in optimal tree size here as it does in Experiment 1 . In fact , when the optimal trees grew 8 times as large ( from 8 to 64 ) , the accuracy went down by just more than one standard deviation . In addition , none of the in tree size between greedily induced and differences optimal trees in Table 2 are more than one a . This is surprising , considering no pruning was used in this experiment . In terms of the three depth measures , the observations made in Experiment 1 hold here also .
Comparing the entries of Tables 1 and 2 , line by line , one can see the effect of the training set size on prediction accuracy . When the training set size increases , the prediction accuracy increases and its variance goes down . In other words , the more ( noise free ) data there is , the more accurately and reliably greedy induction can learn the underlying concept . training
Experiment 3 : This experiment is intended to evaluate the effectiveness of greedy induction in the presence of noise in class labels . The training sets are all in 2 D , and consist of 100 instances per class , uniformly randomly distributed in each class , k % noise is added into each training set by incrementing by 1 the class labels of a random k % of the training points . All concepts are of size 32 , so all optimal tree depth values are equal to 50 Pruning was used when noise level is greater than 0 % . Table 3 summarizes the results . and Observations : As is expected , the classification prediction accuracies decrease when the amount of noise is increased . The tree size and and depth measurements vary significantly when the first 5 % of noise is introduced ( obviously because pruning is started ) , and remain steady thereafter .
One needs to be careful in analyzing the results of experiments 3 and 4 , in order to separate out the effects of noise and the effect of the greedy search . What we want to investigate is whether the greedy heuristic becomes less and less effective as the noise levels increase , or if it is robust . For instance , the fact that the classification accuracy decreases linearly with increase in noise in Table 3 is perhaps not as significant as the fact that the prediction accuracy decreases more slowly than classification accuracy . This is because the former is an obvious effect of noise whereas the later indicates for the noise . that greedy induction might be compensating
Several patterns in Table 3 argue in favor of the effectiveness of pruning , which has come to be an essential part of greedy tree induction . Classification accuracies of the greedy trees are close to , and less than , those of the optimal trees for all the noise levels , so overfitting is not a problem . Prediction accuracies of greedily induced trees with pruning are better than their classification accuracies , again indicating that there is no strong overfitting . Tree size and depth measurements remained virtually unchanged in the presence of increasing noise , certifying to the robustness of pruning . is similar
Experiment 4 : This experiment to the previous one , in that we measure the effectiveness of greedy induction as a function of noise in the training data . However , this time we consider noise in attribute values . The training sets again comprise 100 2 D instances per class , uniformly randomly distributed in each class , k % noise is introduced into each training
Murthy 225
Accuracy
Classification lOO.O ( loo.o )
Prediction Attribute Accuracy Noise 9394 14 0 % 9004 23 5 % 9524 13 ( 0804 04 ) 8874 26 10 % 935+14 ( 9604 07 ) 15 % 9214 16 ( 9414 10 ) 8744 28 8624 31 20 % 907+18 ( 9225=13 ) 25 % 8945=20 ( 9o6+1~ ) 8504 34
Tree Size
3114 62 2224 53 2265=55 2335=56 2374 56 2375=55
Depth
Maximum [ Average Expected 484 05 824 09 444 04 704 08 444 04 70±08 445=04 705=08 445=04 704 08 435=04 705=08
564 04 494 05 494 05 505=05 494 05 494 05
Table 4 : Effects of noise in attribute values . Numbers in parentheses are for optimal trees . Optimal trees are of size 32 .
#Dim .
Classification
Accuracy
2 4 8 12
I00.0 I00.0 100.0 100.0
Tree Size
Prediction Accuracy 9875=032075=33 2395=60 9834 07 2454 65 9805=08 9794 09 2545=68
724 10
50+04
Depth
665=09 I 504 05
Maximum I Average [ Expected . 394 04 405=04 415=02 415=02
634 09 634 09
494 05 494 05
Table 5 : Effects of dimensionality . Training set size=1000 . No noise . Optimal trees are of size 16 . set by choosing a random k % of the instances , and by adding an e E [ 01,01 ] to each attribute . All the concepts are of size 32 , so all the optimal depth measurements are equal to 50 Cost complexity pruning was used in cases where the noise level was greater than 0 % . The results are summarized in Table 4 . Observations : There results with attribute noise ( Table 4 ) and noise in class labels ( Table 3 ) are very similar , except for the classification accuracies . The values for prediction accuracy , tree size and depth measurements in the presence of k % noise are almost the same whether the noise is in attribute values or class labels . The classification and prediction accuracies decrease with increasing noise . The tree size and depth measurements decrease when the first 5 % of the noise is introduced ( due to pruning ) and remain steady thereafter .
However , introducing k % attribute noise is not equivalent to introducing k % class noise . Changing the attributes of an instance by a small amount affects the classification of only those instances lying near the borders of decision regions , whereas changing the class labels affects the classification of all the instances involved . This can be seen from the classification accuracies of the optimal trees in Tables 3 and 4 . The classification accuracy of the greedy trees is quite close to , and less than that of the optimal trees in both tables . All the prediction accuracy values in Table 4 , unlike those in Table 3 , are less than the corresponding classification accuracies .
Experiment 5 : Our final to quantify the effect of dimensionality on the greedy heuristic . All the training sets consist of 1090 uniformly randomly generated instances , wish no noise , experiment attempts
226 KDD 95 as in Experiment 1 . No pruning was used . All concepts are of size 16 , so the optimal tree depths are 40 Table 5 summarizes the results . Observations : The changes in all tree quality measures are quite small when dimensionality is increased from 2 to 12 . This result is surprising because , intuhigher dimensional concepts should be much itively , to learn than lower dimensional ones , more difficult when the amount of available training data does not change . Our experiments indicate that the effects due to dimensionality do not seem to be as pronounced as the effects due to concept complexity ( Table 1 ) noise . The quantity that does increase with increasing dimensionality is the variance . Both prediction accuracy and tree size fluctuate significantly more in higher dimensions than in the plane . This result suggests that methods that help decrease variance , such as combining the classifications of multiple decision trees ( see ( Murthy 1995 ) for a survey ) , may be useful in higher dimensions .
Discussion and Conclusions
In this paper , we presented five experiments for evaluating the effectiveness of the greedy heuristic for decision tree induction . In each experiment , we generated thousands of random training sets , and compared the decision trees induced by C4.5 and CART to the corresponding optimal trees . The optimal trees were found using a novel graph coloring idea .
We summarize the main observations from our experiments below . Where relevant , we briefly mention related work in the literature . ¯ The expected depth of greedily induced decision trees very close to the optimal . Garey was consistently and Graham ( 1974 ) showed that a recursive greedy splitting algorithm using information gain ( not using pruning ) can be made to perform arbitrarily worse than the optimal in terms of expected tree depth . Goodman and Smyth ( 1988 ) argued , by establishing the equivalence of decision tree induction and a form of Shannon Fano prefix coding , that the average depth of trees induced by greedy one paas ( ie , no pruning ) algorithms is nearly optimal .
¯ Cost complexity pruning ( Breiman et 02 . 1984 ) dealt effectively with both attribute and class noise . However , the accuracies on the training set were overly optimistic in the presence of attribute noise .
¯ Greedily induced trees became less accurate as the concepts became harder , ie , as the optimal tree size increased . However , increasing the training data size linearly with concept complexity helped keep the accuracy stable .
¯ Greedily induced trees were not much larger than the optimal , even for complex concepts . However , the variance in tree size was more for higher dimension02 and more complex concepts . Dietterieh and Kong ( 1995 ) empirically argued that even in terms of prediction accuracy , variance is the main cause for the failure of decision trees in some domains .
¯ For a f~ed training set size ,
( Fukanaga & Hayes 1989 ) ) have increasing the dimension02ity did not affect greedy induction as much as increasing concept complexity or noise did . Several authors ( eg , gued that for a finite sized data with no a priori probabilistie information , the ratio of training sample size to the dimensionality must be aa large aa possible . Our results are consistent with these studies . However , with a reasonably large training set ( 1000 instances ) , the drop in tree quality was quite small in our experiments , even for a 6 fold ( 2 to 12 ) inereaae in dimensionality . that trees , these measures consistently
¯ The goodness measures of CART and C4.5 were identical in terms of the quality of trees they generatetL It has been observed earlier ( eg,(Breiman et 02 . 1984 ; Mingers 1989 ) ) that the differences between these goodness criteria are not pronounced . Our observation produced identical in terms of six tree quality measures , in a large scale experiment ( involving more than 130,000 s~rnthetic data sets ) strengthens the existing results . Many researchers have studied ways to improve upon greedy induction , by using techniques such aa limited lookahead search and more elaborate classifier representations ( eg , decision graphs instead of trees ) . ( See in the cur(Murthy 1995 ) for a survey . ) The results rent paper throw light on why it might be difficult to improve upon the simple greedy algorithm for decision tree induction .
References tree algorithms .
Inc . ,
Breiman , L . ; Friedman , J . ; Olshen , R . ; and Stone , C . 1984 . Classification and Regression Trees . Wadsworth International Group . Cox , L . A . ; Qiu , Y . ; and Kuehner , W . 1989 . Heuristic leaat cost computation of discrete classification functions with uncertain argument values . Annals of Operations Research 21(1):1 30 . Dietterieh , T . G . , and Kong , E . B . 1995 . Machine learning bias , statistical bias and statistical variance of decision In Machine Learning : Proceedings of the Twelfth International Conference . Tahoe City , CA : Morgan Kanfmann Publishers San Mateo , CA . to appear . Fukanaga , K . , and Hayes , tLA . 1989 . Effect of sample size in elaasifier design . IEEE Transactions on Pattern Analysis and Machine Intelligence 11:873 885 . Garey , M . R . , and Graham , R . L . 1974 . Performance bounds on the splitting algorithm for binary testing . Acta Informatica 3(Fase . 4):347 355 . Goodman , R . M . , and Smyth , P . J . 1988 . Decision tree design from a communication theory standpoint . on Information Theory 34(5):979 994 . Hyafil , L . , and Rivest , 1~ L . 1976 . Constructing optimal binary decision trees is NP complete . Information Processing Letters 5(1):15 17 . Mingers , J . 1989 . An empirical comparison of selection measures for decision Learning 3:319 342 . Murphy , O . J . , and McCraw , R . L . 1991 . Designing storage efficient decision trees . IEEE Transactions on Computers 40(3):315 319 . Murthy , S . K . ; Kasif , S . ; and Salzberg , S . 1994 . A system for induction of oblique decision trees . Journal of Artificial Intelligence Research 2:1 33 . Murthy , SK ing decision trees : A survey . http://wwwesjhuedu/grad/murthy Quinlan , J . R . 1993 . C4.5 : Programs for Machine Learning . San Mateo , CA : Morgan Kaufmann Publishers .
1995 . Data exploration tree induction . Machine
IEEE Transactions us
In preparation .
2The fact that we only used binary splits in reabvalued domains may be one reason why information gain , Gini index and twoing rule behaved similarly .
Murthy
227
