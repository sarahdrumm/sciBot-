Answering Approximate Queries over Autonomous Web Databases
Xiangfu Meng
Z . M . Ma
College of Information Science and Engineering , Northeastern University
College of Information Science and Engineering , Northeastern University
Shenyang , China , 110004
Shenyang , China , 110004 marxi@126.com mazongmin@iseneueducn
Li Yan
School of Software
Northeastern University Shenyang , China , 110004 yanlilyjiaji@163.com
ABSTRACT To deal with the problem of empty or too little answers returned from a Web database in response to a user query , this paper proposes a novel approach to provide relevant and ranked query results . Based on the user original query , we speculate how much the user cares about each specified attribute and assign a corresponding weight to it . This original query is then rewritten as an approximate query by relaxing the query criteria range . The relaxation order of all specified attributes and the relaxed degree on each specified attribute are varied with the attribute weights . For the approximate query results , we generate users’ contextual preferences from database workload and use them to create a priori orders of tuples in an off line preprocessing step . Only a few representative orders are saved , each corresponding to a set of contexts . Then , these orders and associated contexts are used at query time to expeditiously provide ranked answers . Results of a preliminary user study demonstrate that our query relaxation and results ranking methods can capture the user ’s preferences effectively . The efficiency and effectiveness of our approach is also demonstrated by experimental result .
Categories and Subject Descriptors H23 [ Database Management ] : Languages Query languages ; H24 [ Database Management ] : Systems Query processing
General Terms Algorithms , Performance , Experimentation , Human Factors .
Keywords Web database , query relaxation , query results ranking , top k .
1 . INTRODUCTION Nowadays , there is more and more interest in using the World Wide Web , especially , for searching and retrieving information over Web database1 that are available “ on line ” . Exploiting Webbased information sources is non trivial because the user has no direct access to the data . Users in general accomplish their search
1 We use the term “ Web database ” to refer to a non local autonomous database that is accessible only via a Web ( form ) based interface .
Copyright is held by the International World Wide Web Conference Committee ( IW3C2 ) . Distribution of these papers is limited to classroom use , and personal use by others . WWW’09 , April 20–24 , 2009 , Madrid , Spain . ACM 978 1 60558 487 4/09/04 . using Boolean queries and an item from the database simply either matches or it does not . In such context , users may be confronted with the following two problems : 1 . Empty answers : When the query is too selective , the answer may be empty or too little . In that case , it is desirable to have the option of relaxing the original query for presenting more relevant items that can meet user ’s needs and preferences closely . 2 . Many answers : When the query is not too selective , too many tuples may be in the answer . In such a case , it will be desirable to have the option of order the matches automatically that ranks more “ globally important ” answer tuples higher and returning only the best matches . In the first case , several approaches have been proposed to deal with this issue [ 4 , 16 , 19 ] . The basic idea of these approaches is based on reducing the constraints on the original query in order to expand the scope of the query . However , most existing work does not consider the user preferences when relaxing the original query . But in real applications the efficiency of query relaxation is affected greatly by the user preferences . In this paper , we tackle the empty answers problem for Web database by proposing an automatic relaxing and ranking approach , AQRR ( approximate query2 & results ranking ) , which can relax the original query and rank the approximate query results from a Web database in a domain and user independent way . We will use the illustrative examples below to motivate and provide an overview of our approach . Example 1 . Consider a used car selling Web database CarDB ( Make , Model , Price , Color , Engine , Year , Mileage ) . Each tuple in CarDB represents a used car for sale . Based on the used car database the user may issue the following query : Q : CarDB ( Model = Camry and Price < 10000 ) On receiving the query , CarDB will provide a list of a few Camrys that is priced below $10000 ( since there are very few Camrys priced below $10000 ) . In such a case , the traditional query relaxation approaches will expand each conditions of the original query with the same relaxation degree to provide the relevant answers . However , in the real world the user who submitted this query may care more about Price than Model ( because there are few used cars priced below $10000 and then we can speculate that the user concerns more about the Price ) and the relaxation degree of the query criteria on Price should be relaxed smaller than that on Model . Hence , we need to surmise
2 A user query that requires a close but not necessarily exact match is an approximate query .
WWW 2009 MADRID!Track : XML and Web Data / Session : XML Querying1021 to specified contexts , the user ’s preferences on different attributes when relaxing the scope of the original query . In this paper , our solution for relaxing a given query Q is to generate an approximate query Q by reducing the constraints of the original query . The underlying motivation is that the tuples most similar to the original query will have differences only in the least important attribute specified by the query and the relaxation degree for each specified attribute should be different according to the importance of the attribute to the specific user . The first attribute to be relaxed must be the least important specified attribute and has the maximum relaxation degree . The intuition of our attribute weight measuring approach is that the importance of specified attribute for the user can be reflected by the distribution of the specified attribute value in the database . However , after relaxing the original query , another problem faced by users will be that there are usually many answers returned for an approximate query . To resolve the many answers problem , two types of solutions have been proposed . The first type [ 5 , 9 ] categorizes the query results into a navigational tree , and the second type [ 3 ] , [ 6 ] , [ 12 ] ranks the results . The success of both approaches depends on the utilization of user preferences . But these approaches do not consider the contexts in which the preferences appear . However , in real applications the preferences are often associated ie contextual preferences , which take the form i1 ( cid:59 ) i2 , d | X , meaning that item i1 is preferred to item i2 with the interest degree d in the context of X . Example 2 . Consider the used car relation CarDB mentioned above . Assume that we have the following contextual preferences : Model=Camry ( cid:59 ) Model=Accord , 0.6 | Price = 20000 Model=Accord ( cid:59 ) Model=Camry , 0.8 | Price = 25000 These preferences illustrate that the ranking of the tuples of a relation is subjective . In the context of a used car that is priced around $20,000 , people may prefer Camry to Accord with interest degree 06 In contrast , in the context of a used car that is priced around $25,000 , people may prefer Accord to Camry with interest degree 08 This example shows that one can not rank objects independently of the context in which they appear . In this paper , we propose a contextual preference model of the form {i1 ( cid:59 ) i2 , d | X} , meaning that item i1 is preferred to item i2 with the interest degree d in the context of X . Based on contextual preferences , we incorporate the set of contextual preferences P into the query results ranking mechanism . Specifically , for a given approximate query , the contextual preferences that are related to it are taken into account to provide ranked top k results . We collect the contextual preferences from the database workload by using association rules . These contextual preferences are used to create a priori orders of tuples in an offline processing step . Only a few representative orders are saved , each corresponding to a set of contexts . For an incoming approximate query Q , we first evaluate the similarity between Q and contexts , and then quickly provide the ranked results that agree with the orders as much as possible . Our contributions are summarized as follows : We propose a query relaxation method to solve the empty answers problem and provide relevant answers for a user query over the autonomous Web database . This method considers both the data distribution and user preferences when relaxing the original query .
We propose a ranking method for the approximate query results . This method uses pre computation , clustering and top k algorithm to deal with the large result tuples .
The rest of this paper is organized as follows . Section 2 reviews some related work . Section 3 proposes our query relaxation method . Section 4 gives the definition of contextual preferences and outlines the ranking method of approximate query results while the algorithmic solutions for it are discussed in Section 5 . The experiment results are presented in Section 6 . The paper is concluded in Section 7 .
2 . RELATED WORK Several researches have been proposed to deal with the empty answers problem . These researches can be classified into two main categories . The first one is based on fuzzy set theory such as [ 4 ] and [ 15 ] , which relaxes the query criteria by using membership functions , domain knowledge and α cut operation of fuzzy number . The second category focuses on the development of cooperative database systems such as [ 16 , 19 ] which handle the query relaxation based on distance notion , data distribution , etc . However , it should be noted that the approaches based on fuzzy sets are highly dependent on the domain knowledge and it is mainly useful in expanding the numerical query criteria range while the cooperative database system usually requires the user feedback . Furthermore , both of these two types of approaches seldom consider the user preferences when relaxing the query and ranking the query results . Compared with the above work , our approach is fully automatic and does not require the domain knowledge . Our approach also takes the user preferences into consideration when relaxing the query and ranking the answers . There is also some work on ranked retrieval from a database . In [ 20 ] and [ 23 ] , user relevance feedback is employed to learn the similarity between a result tuple and the query . In [ 14 ] and [ 17 ] , the SQL query language is extended to allow the user to specify the ranking function according to their preference for the attributes . In [ 3 ] and [ 8 ] , the importance scores of tuples in a relation are extracted automatically by analyzing the workloads , which can reveal what users are looking for and what they consider as important . In [ 10 ] and [ 14 ] , a quantitative and a qualitative preference model were proposed , respectively . In the first , preferences are specified indirectly using scoring functions that associate a numeric score with every tuple of the query answer . While in the second , preferences among tuples are specified directly using binary preference relations . Recently , several works started to consider the contextual preferences for ranking database query results , such as [ 1 ] , [ 21 ] and [ 22 ] . We make use of some of these ideas , but enhance the contextual preferences with the interest degrees and focus on how the preferences associated with different contexts and interest degrees have impact on the query results . The work that is most similar to ours is the AIMQ in [ 16 ] , which addresses the problem of answering imprecise queries over autonomous Web databases by using approximate functional dependencies and approximate keys . Our approach differs from that in [ 16 ] in the following aspects : 1 . AIMQ learns the attribute importance based on pre extracted data and it can only determine the attribute importance sequence without the specific weights . The attribute importance sequence is also invariant to the different user queries . In contrast , our approach speculates how much the user cares about each specified
WWW 2009 MADRID!Track : XML and Web Data / Session : XML Querying1022 attribute according to the user ’s query and thus the attribute importance can be tailored to the user preferences . 2 . AIMQ only takes the similarities between an imprecise query and answer tuples into consideration for ranking the relevant answer tuples while it ignores the impact of user ’s preferences on ranking . On the contrary , AQRR considers both the similarities of the query and answer tuples and the user preferences .
3 . APPROXIMATE QUERY 3.1 Problem Definition Definition 1 . ( approximate query ) Consider an autonomous Web database R with categorical and numerical attributes A = {A1 , A2,… , Am} and a query Q over R with a conjunctive selection condition of the form Q = ∧i∈{1,… , k}(Ai θ ai ) , where k ≤ m and θ ∈ {> , < , = , ≠ , ≥ , ≤ , between} . Note that , if θ is the operator between and ai is an interval which is represented by [ ai1 , ai2 ] , Ai θ ai has the form of “ Ai between ai1 AND ai2 ” . Each Ai in the query condition is an attribute from A and ai is a value ( or interval ) in its domain . By relaxing Q , an approximate query Q which is used to find all tuples of R that show similarity to Q above a threshold Tsim∈(0 , 1 ) is obtained . Specifically ,
Q(R ) = {t | t∈R , Similarity ( Q , t ) > Tsim} .
3.2 Attribute Weight Assignment In the real world , different users have different preferences , thus the attribute importance is usually different for different types of users . The query criteria user specified can reflect the user ’s preferences on the attribute . For instance , for a query with condition “ Year = 2007 and Price < 10000 ” , the specified attribute Year is less important for user ( there may be many used cars have the date of shipment in 2007 ) than the attribute Price ( relatively fewer used cars priced below $10,000 ) . Hence , we will assign the weight for each specified attribute according to the distribution of its value specified by the user in the database .
321 Importance of Specified Categorical Attributes The well known IDF method has been used extensively in IR to suggest that commonly occurring words convey less information about user ’s needs than rarely occurring words , and thus should be weighted less . IDF(w ) of a word w is defined as log(n/F(w ) ) where n is the number of documents , and F(w ) is the number of document in which w appears . If the database only had categorical attributes , each tuple can be treated as a small document . Thus , we can mimic these techniques for our problem . For a point query “ Ai = v ” , we define IDFi(v ) as log(n/Fi(v ) ) which represents the importance of attribute value v in the database , where n is the number of tuples in the database and Fi(v ) is the frequency of tuples in the database of Ai = v . As mentioned above , the importance of specified categorical attribute value is treated as the importance of its corresponding attribute .
322 Importance of Specified Numerical Attributes For evaluating the importance of numerical attribute values , it is inappropriate to adopt the definition of traditional IDF as above mentioned because of their binary nature ( where if u and v are arbitrarily close to each other yet distinct ) . Moreover , the “ IDF ” of a numeric value should depend on nearby values .
In this paper , we adopt the definition given in [ 3 ] to measure the similarity of numeric attribute values . Let {v1 , v2 , … , vn} be the values of attribute A that occur in the database . For specified attribute value v in the query , it defined IDF(v ) as shown in Equation ( 1 ) , where h is the bandwidth parameter .
IDF(v ) = log
⎛ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝ n 1⎛ ⎜ 2 ⎝ e
2 iv v h
⎞ ⎟ ⎠
⎞ ⎟ ⎟ ⎟ ⎟ ⎟ ⎠
( 1 ) n
∑ i
A popular estimate for the bandwidth is h = 1.06σn−1/5 , where σ is the standard deviation of {v1 , v2 , … , vn} . Intuitively , the denominator in Equation ( 1 ) represents the sum of “ contributions ” to v from every the other point vi in the database . These contributions are modeled as ( scaled ) Gaussian distributions , so that the further v is from vi , the smaller is the contribution from vi . For example , “ Price = 10000 ” is sparse in its domain and other values are far from it , thus the value “ 10000 ” will get a large IDF . The importance of specified numerical attribute value is also treated as the importance of its corresponding attribute . Moreover , if query condition is generalized as “ Ai IN Qi ” , where Qi is a set of values for categorical attributes , or a range [ lb , ub ] for numeric attributes , We define the maximum log(n/Fi(v ) ) of each different value v in Qi . The generalized importance measuring function is shown in Equation ( 2 ) . IDF v ( )
( 2 )
IDFi ( v ) = max v Q ∈ i
By normalized processing , the weight wi of attribute Ai specified by the query can be calculated by wi = i
IDF v ( ) k ∑ IDF v ( ) i i
1 =
( 3 ) in which , k is the number of attributes specified by the query .
3.3 Attribute Values Similarity Assessment 331 Similarity of Categorical Attribute Values We discuss an approach for deriving the similarity coefficient between two categorical attribute values . It is an adaptation of the method proposed in [ 16 ] . The similarity between two values binding a categorical attribute is measured as the percentage of common AV pairs that are associated to them . Given a categorical value , all the AV pairs associated to the value can be seen as the features describing the value . The similarity between two values can be estimated by the commonality in the features ( AV pairs ) describing them . For example , given tuple t ={Toyota , Camry , 15k , 2008} , the AV pair Model=Camry is associated to the AVpairs Make = Toyota , Price = 15000 and Year = 2008 . An AV pair can be visualized as a selection query that binds only a single attribute . By issuing an AV pair query ( such as “ Model = Camry ” ) over the extracted database , a set of tuples all containing the AV pair can be identified . The answer set containing each AV pair as a structure is called the supertuple . The supertuple contains a Set of keywords for each attribute in the relation not bound by the AV pair . Table 1 shows the supertuple for “ Model = Camry ” over the relation CarDB as a 2 column tabular structure .
WWW 2009 MADRID!Track : XML and Web Data / Session : XML Querying1023 Table 1 . Supertuple for Model = “ Camry ”
Make Price Mileage Color Year
Toyota : 112 5000 10000 : 15 , 10000 30000 : 40,… 10000 20000 : 12 , 20000 40000 : 25,… Black : 46 , Silver : 15 , … 2008 : 17 , 2007 : 37,…
The similarity between two AV pairs can be measured as the similarity shown by their supertuples . The supertuples contain sets of keywords for each attribute in the relation and the Jaccard Coefficient is used to determine the similarity between two supertuples . Thus , in this paper the similarity coefficient between two categorical values is then calculated as a sum of the Set similarity on each attribute ,
VSim(C1 , C2 ) =
=∑ m i
1J(
C A C A . i 1
.
,
2 i
)
( 4 ) where C1 , C2 are supertuples with m attributes , A is the Set corresponding to the i th attribute , J( , ) is the Jaccard Coefficient and is computed as J(A , B ) = |A∩B|/|A∪B| .
332 Similarity of Numerical Attribute Values Because of the continuity of numerical data , we propose an approach to estimate the similarity coefficient between two numerical values . Let {v1 , v2 , … , vn} be the values of numerical attribute A occurring in the database . Then the similarity coefficient NSim(v , q ) between v and q can be defined by Equation ( 5 ) , where h is the same as mentioned in Equation ( 1 ) .
NSim(v , q ) =
1 v q h
2
⎞ ⎟ ⎠
1
⎛ + ⎜ ⎝
( 5 )
For a numerical condition Ai = q of Q , let ψi be a sub threshold for Ai , according to Equation ( 5 ) , we can then get the relaxation range of numerical attribute Ai as follows :
[ q – h i
ψ1− ψ i
, q + h i
ψ1− ψ i
] ( 6 )
3.4 Query Relaxation The sub threshold for each specified attribute should be computed according to the attribute weights and the threshold for the query . Given a conjunctive selection query Q to be executed over database table R , we assume wi is the weight of specified attribute Ai , k is the number of specified attributes , Tsim is the given threshold by the user . Then , the sub threshold ψi for each specified attribute in Q can be calculated as follows : relaxed condition Ci . By join all the relaxed conditions , the approximate query Q is formed .
Algorithm 1 The query rewriting algorithm Input : Original query Q= {C1 , …,Ck} , sub threshold {ψ1,… , ψk} . Output : An approximate query Q = {C1 , … , Ck} . 1 . For i = 1 , … , k 2 . Ci ← Ci 3 . If Ai is a categorical attribute 4 . ∀v ∈ Dom(Ai ) 5 . If VSim(ai , v ) > ψi 6 . Add v into the query range of C i 7 . End If 8 . End If 9 . If Ai is a numerical attribute 10 . Replace query range of Ci with [ q–h
, q+h i i
ψ1− ψ i
ψ1− ψ i
]
11 . End If 12 . Q = Q ∪ Ci 13 . End For 14 . Return Q
4 . APPROXIMATE QUERY RESULTS RANKING This section firstly gives a definition of the contextual preferences , and then discusses the method of preferences processing . Finally , the approximate query results ranking problem is defined and its solution is presented based on contextual preferences . 4.1 Contextual Preferences 411 Definition of Contextual Preferences Definition 2 ( contextual preferences ) Contextual preferences are of the form {Ai = ai1 ( cid:59 ) Ai = ai2 , d | X} , where X is ∧j∈l(Aj θ aj ) , with ai1 , ai2 ∈ Dom(Ai ) , l ⊆ {1,… , k} , θ ∈ {> , < , = , ≠ , ≥ , ≤ , between} and aj ∈ Dom(Aj ) , d is the interest degree of preference ( ie , compared to ai2 , the interest degree of ai1 is d , where 0.5 ≤ d ≤ 1 , and it can be learned from the database workload ) . The lefthand side of a preference specifies the choice and the interest degree while the right hand side is the context . For collecting contextual preferences , we automatically generate preferences by using association rules mining [ 2 ] on the database workload log of past users queries . The workload is represented as a set of “ tuples ” , where each tuple represents a query and is a vector containing the corresponding values of the specified attributes [ 8 ] . The rationale behind this automatic generation of preferences is the following . We say that {A1 = a ( cid:59 ) A1 = b , d | X} if conf(X →a ) > conf(X →b ) , where conf(X →a ) is the confidence of the association rule X → a in the workload , ie ,
ψi ( i = 1,… , k ) =
⎧ ⎪⎪ ⎨ ⎪ ⎪⎩ w 1 ψ 1
, , =
= w k ψ k
T sim
= k
∑ i
1 = w i
⋅
ψ i
The query rewriting algorithm ( Algorithm 1 ) is shown as follows . For each condition Ci in the original query Q , by extracting values of its corresponding attribute Ai having similarity above the subthreshold ψi and adding them into its query range , we can get the
( 7 ) conf (
X
→ = a
) frequency( frequency(
X a ∧ X )
) d
=
X conf ( a ) → + a ) → conf (
X
X conf (
→ ( 8 ) where , conf(X →a ) ( resp . conf(X →b ) ) is the frequency of the value a ( resp . b ) occurring together with context X in the workload , then we can obtain the value of d by using Equation ( 8 ) . The intuition is that when an attribute value a occurs together
) b
WWW 2009 MADRID!Track : XML and Web Data / Session : XML Querying1024 with context X in the workload more often than the attribute value b , then this implies that a is also preferred to b over X .
412 Contextual Preferences Processing Example 3 . Consider the used car relation CarDB of Table 2 and the following preferences for tuples in this relation . the context . All tuples that are not indifferent are asserted . This paper only considers the asserted tuples . Let ’s consider the preferences of the same class and defines the effective preference ( Peff p ) for ordered pairs of tuples ( t , t’ ) : for any ordered pair of asserted tuples ( t , t’ ) such that there exists a p∈PX for which dpref(t , t’ , p ) + dpref(t’ , t , p ) = 1 , we then have :
Table 2 . CarDB table
Model Accord Accord CR V Camry Matrix
Color Silver Blue Black Blue Gray
Engine Make 2.4L Honda Honda 3.5L Honda 3.0L Toyota 3.5L 3.3L Toyota
Price 30999 31999 32500 22999 23999
Year 2008 2007 2007 2007 2007 p1 = {Model = Accord ( cid:59 ) Model = CR V , 0.7 | Make = Honda ∧ Price between 30000 and 33000} p2 = {Color = Sliver ( cid:59 ) Color = Black , 0.6 | Make = Honda ∧ Price between 30000 and 33000} p3 = {Engine = 3.0L ( cid:59 ) Engine = 2.4L , 0.9 | Make = Honda ∧ Price between 30000 and 33000} p4 = {Model = Camry ( cid:59 ) Model = Matrix , 0.8 | Make = Toyota ∧ Price between 22000 and 25000} Under the assumption above , preference p1 suggests that in the context of “ Make=Honda ∧Price between 30000 and 33000 ” , tuples t1 and t2 are preferred to tuple t3 with the interest degree 07 In the same context , from p2 , tuple t1 is preferred to t3 with the interest degree 0.6 , and from p3 tuple t3 is preferred to t1 with the interest degree 09 Finally , from p4 , tuple t4 is preferred to tuple t5 in the context of “ Make = Toyota ∧ Price between 22000 and 25000 ” with the interest degree 08 For any single preference p and any pair of tuples ( ti , tj ) , p either prefers ti to tj ( denoted by ti ( cid:59 ) p tj ) or tj to ti ( denoted by tj ( cid:59 ) p ti ) or it is inapplicable with respect to ti and tj ( denoted by ti~ptj ) . Thus , every preference p defines a preference degree ( dpref ) over any pair of tuples t , t’ that evaluates as follows : dpref ( t , t’ , p ) = d , ⎧ ⎪ 1 d , ⎨ ⎪ ⊥⎩ , if t if t ' if t ~ t ' ( cid:59 ) p ( cid:59 ) t t ' p p where , d ( 0.5 ≤ d ≤ 1 ) is the interest degree of the preference p . For any two contexts X1=∧j∈l1(Ajθaj ) and X2 = ∧j∈l2(Ajθbj ) with l1 , l2 ⊆ {1 , 2 , … , k} , we say that they are equal if and only if l1=l2=l and θaj is the same as θbj for all j ∈ l . We say that two preferences {Ai=ai1 ( cid:59 ) Ai=ai2 , d1 | X1} and {Aj= aj1 ( cid:59 ) Aj = aj2 , d2 | X2} belong to the same preference class if X1 = X2 = X . PX is used to denote the set of all preferences in the same class defined by context X . For example , preferences p1 , p2 and p3 from Example 3 belong to the same class defined by “ Make=Honda ∧ Price between 30000 and 33000 ” , while p4 belongs to a different class . The set of preferences PX in a class , partitions the tuples in the relation in two sets , the set of indifferent tuples and the set of asserted tuples . A tuple is indifferent with respect to a context if no explicit preferences that involve it have been expressed within
P eff p − t t P ( , X
' ,
)
=
∑ p P ∈ X
∑ ( d d pref p P ∈ X t t p ( , ' , t t p ( , ' , ) d + pref pref
) t ( ' , , t p
.
) )
If for a pair of asserted tuples ( t , t’ ) there does not exist a p∈PX for which dpref ( t , t’ , p ) + dpref ( t’ , t , p ) = 1 , then 1 2 t t P ( , X
P eff p −
) P = t ( ' , , t P X eff p −
.
=
' ,
)
If for a pair of tuples ( t , t’ ) , t or t’ ( or both ) are indifferent with respect to context X , then : t t P ( ,
=⊥ .
' ,
P eff p −
)X
Let ’s consider the preference class P Make = Honda ∧ Price between 30000 and 33000 from Example 3 to illustrate the definition mentioned above . For this preference class ( denote by PX for brevity ) , the effective preferences for the tuples are as follows : ( 1 ) Peff p ( t1 , t2 , PX ) = Peff p ( t2 , t1 , PX ) = 1/2 . ( 2 ) Peff p ( t1 , t3 , PX ) = ( 07+06+01)/3 = 7/15 , Peff p ( t3 , t1 , PX ) =
( 03+04+09)/3 = 8/15 .
( 3 ) Peff p ( t2 , t3 , PX ) = 7/10 , Peff p ( t3 , t2 , PX ) = 3/10 . ( 4 ) Peff p ( . , . , PX ) = ⊥ . For a given class of preferences PX and a relation R , the Xpreference graph GX ( VX , EX ) is defined as : the set of nodes is the set of all asserted tuples in R . For every ordered pair of nodes ( t , t’ ) there exists a directed edge e(t , t’ ) ∈ EX , with weight : wX ( t → t’ ) = peff p(t , t’ , PX ) , and wX ( t → t’ ) + wX ( t’ → t ) = 1 . Figure 1 shows the preference graph for P Make = Honda ∧ Price between 30000 and 33000 for the used car relation and preferences p1 , p2 and p3 .
Figure 1 . Preference graph for context “ Make = Honda ∧ Price between 30000 and 33000 ”
3/10
8/15
7/10
7/15
1/2
1/2 t3 t2 t1
4.2 The Approximate Query Results Ranking Problem Consider a conjunctive selection query Q over relation R = {t1,… , tn} with schema ( A1,…,Am ) . Let Q is an approximate query generated based on Q . The approximate query Q is of the form Q = σ∧j∈{1,…,k} Cj ,where k ≤ m . Let Q(R ) ⊆ R is the subset of tuples in R that are in the answer of Q . The goal is to address the approximate query results ranking problem defined as follows :
WWW 2009 MADRID!Track : XML and Web Data / Session : XML Querying1025 Problem 1 . ( approximate query results ranking problem ) : Assume mXP } and an approximate query a set of preferences P ={ Q . The approximate query results ranking problem asks for an order of τ of Q(R ) such that
1XP ,… ,
τ
= arg max
' τ m
∑ i
1 = sim(
' Q X
,
)Agree( ' , τ i
P X
)i where ,
Agree( , τ
P X
)
= ∑ t t ( , t ' ) : ( ) τ τ
< t ( ' )
P eff p t t P ( , X
' ,
)
The objective of the problem is to find the order τ over the set of tuples in Q(R ) that agrees as much as possible with the input preferences . Additionally , the degree of agreement with a class of preferences is weighted by the similarity between the contexts of those preferences and the approximate query Q . We adopt cosine similarity to quantify the similarity between an approximate query Q and a context X . Firstly , we form the set representations of a context and an approximate query . Consider the set D of all distinct 〈attribute , value〉 pairs appearing in the R , that is , D = {〈Ai , a〉 | ∀i∈{1,…,k} and ∀a∈Dom(Ai)} . Since Dom(Ai ) is the active domain of attribute Ai the cardinality of this set is finite . Let N = |D| and let OR be an arbitrary but fixed order on the pairs appearing in D . D[i ] refers to the i th element of D based on the ordering OR . A vector representation of a context X = ∧j ( Aj= aj ) is a binary vector VX of size N . The i th element of the vector corresponds to pair D[i ] . If D[i ] satisfies one of the conjunctions of X then VX[i ] = 1 , otherwise it is 0 . The vector representation of an approximate query Q is a vector VQ of size N . The i th element of the vector corresponds to pair D[i ] . If D[i ] satisfies one of the conditions of Q , then VQ[i ] can be computed as follows : j
=
C C
D D
V i w [ ] ' j Q
.value ) , .value ) ,
C if Domain( j C if Domain(
VSim( [ i].value , ⎧⎪ ×⎨ NSim( [ i].value , ⎪⎩
.attribute)=categorical .attribute)=numerical where , wj is the weight of attribute specified by the condition Cj of original query Q . Otherwise , the VQ[i ] = 0 . Now we can define the similarity between context X and approximate query Q using their vector representations as follows : j j sim(
' Q X
,
)
= cos(
V V , ' Q
X
)
=
|
|
D
∑ i
1 =
V i V i [ ] ' Q
[ ]
⋅
X
|
|
D
∑ i
1 =
2 V i [ ] ' Q
|
|
D
∑ i
1 =
2 V i [ ] X
So , we can additionally define the similarity between an approximate query Q and a set of context χ as follows : sim(
' Q
,
) χ
= ∑
X sim( χ
∈
' Q X
,
)
After this , the approximate query results ranking problem is fully defined . According to [ 11 ] , this problem is NP hard .
4.3 Approach Given Problem 1 is NP hard , we have to think of approximation algorithms for solving it . In this paper , we adopt the idea of solution presented in [ 1 ] and propose the improved algorithms for solving it . The solution consists of three processing steps : create orders of tuples , find representative orders and rank the top k answer tuples . The first and second steps are processed during offline time and the third step is processed during online time . Step 1 ( create orders ) : For each preference class PXi create a order iτ of the tuples in R such that
τ i
= arg max Agree(
τ i
' ,
XP
)i
( 9 )
τ i
'
The output of this step is a set of m 〈context , order〉 pairs of the form 〈Xi , iτ 〉 , where Xi is the context for the preferences in PXi and iτ is the order of the tuples in R that ( approximately ) satisfies Equation ( 9 ) . According to the output order of tuples , each tuple t has a score that is associated with the position of t in each order iτ . The score of tuple t in iτ that corresponds to Xi is : iτ ( t ) represents the position of s t X + , where ( | ( ) 1 i tuple t in order iτ . tτ= − n
) i
1τ ,… , lτ for m initial pairs 〈Xi ,
Step 2 ( find representative orders ) : In order to reduce the number of 〈context , order〉 pairs , we need to find representative iτ 〉 , where , l < m . These orders orders partition the space of the m initial 〈context , order〉 pairs into l groups . Each group i is characterized by order iτ and a such that for each disjunction of contexts X iτ is a representative order for the initial order jτ . Finding representative orders can be considered as an orders clustering problem . The score of tuple t in 〈 iX τ 〉 is given by :
X∈ order
X 1{
, ,
⊆
X
X
} m
, j i i i s t X ( |
) i tτ= − n
( ) 1
+ ( 10 ) i i i i
X
⋅
,
)
)
. sim( iτ .
' Q X s t X ( | iτ ( t ) represents the position of tuple t in order where Given the offline computations , the only online task is to appropriately combine the priori formed rankings of the tuples to return a ranked answer to the given approximate query . Step 3 ( rank the top k answer tuples ) For an approximate query Q over relation R , using the output of step 2 , compute the set Qk(R ) ⊆ Q(R ) ⊆ R with | Qk(R)| = k , such that ∀t ∈ Qk(R ) and t’∈{RQk(R)} it holds that score(t , Q ) > score(t’ , Q ) , with score(t , Q ) = ∑ 5 . ALGORITHM In this section , we discuss the complexity of the problem in each step and present the algorithm for solving it . 5.1 Orders Creating Algorithm Maximum Acyclic Sub graph problem is known to be NP Hard . We describe it in brief as follows : for an input directed weighted graph G find the maximum weight sub graph of G that is acyclic . From this , the orders creating problem is as hard as the Maximum Acyclic Sub graph problem , and the connection between the Maximum Acyclic Sub graph and the orders creating problem becomes intuitively clear via the X preference graph . We next give an algorithm for the orders creating problem . The Greedy algorithm for creating orders ( Algorithm 2 ) is an adaptation of the algorithm proposed in [ 1 ] and [ 11 ] . The algorithm is operated on the X preference graph . At every step a greedy selection is made .
WWW 2009 MADRID!Track : XML and Web Data / Session : XML Querying1026 Algorithm 2 The Greedy algorithm for creating orders Input : Relation R = {t1,… , tn} , a set of preferences from a single class XP .
Output : A pair 〈X , τ〉 where is an order of the tuples in R such that as many of the preferences in XP are satisfied . j
X t
)
1 = n j
∑ w t ( i
1 . S = {t1,…,tn} 2 . rank = 0 3 . For all i∈{1,… , n} Do p t 4 . ( ) = → i 5 . End For 6 . While S ≠ ∅ Do 7 . rank + = 1 8 . arg max t u 9 . τ(tv ) = rank 10 . S = S – {tv} 11 . For all t∈S Do 12 . p(t ) = p(t ) –wX ( t → tv ) 13 . End For 14 . End While p t (
=
S ∈
) t u v
5.2 Orders Clustering Algorithm 521 Orders Clustering Problem In order to quantify how well an order τ of the tuples in R is represented by another order ρ we need to define a distance measure between orders over the same set of tuples . The wellknown Euclidean distance is employed in our paper : d ( E
, ) ρτ
=
(
−∑
( τ ρ i i
1 2 2 ) ) n i
1 =
1τ ,
2τ and
In Euclidean distance , dE satisfies the triangle inequality . That is , 3τ are any three permutations over a set of n if objects . Then , the following inequality is true : 3τ ) ≤ dE ( 1τ ,
2τ ) + dE ( 2τ , dE ( 1τ ,
3τ )
Based on the Euclidean distance , we can reformulate the orders clustering problem in step 2 : assume an input consisting of m context order pairs 〈Xi , iτ 〉 and let Tm be the set of the m orders over the tuples of relation R : Tm = { 1τ ,… , mτ } . Find a set of l < m orders Tl = { 1τ ,… , lτ } such that : = ∑
T cos t( l
)
T τ ∈ m d( , τ
T l
)
( 11 ) is minimized . The distance of a single order τ from a set of orders T is defined as d( , τ
) min d( , =
) τρ
T
.
T ρ ∈
We call the orders in set Tl representative orders and associate with each representative order iτ a set of contexts
X j
=
{
X τ j
| i
= arg min d( j
'
τ τ j i
,
)}
'
.
As we know , the k median problem is to be NP hard and the orders clustering problem can be treated as the k median problem [ 7 ] . We next propose an algorithm for dealing with the orders clustering problem . j s
,
) c
Tτ ∈ i d ( E'
τ τ i j iT 〉 |
= ∑ iτ ∈ Tm ,
. Let rs = cs / | iT ⊆ Tm} . The cost of each Star s = 〈 iτ ,
522 Algorithm Observing the solution of the orders clustering , we can find that every representative order connects with some other orders of Tm and these connections are like star structures . Here , we call a connection as a Star . Then we can re define the orders clustering problem as follows : Let U be the set of all Stars , ie , U = iT 〉 {〈 iτ , ∈ U can be denoted as : iT | be the performance price ratio . Our objective is to find a set of Star S , such that S ⊆ U , which minimizes the cost and enables that there are l representative orders in S and any initial order jτ ∈ Tm appears at least one time at Star s ∈ S . For solving this problem , our approach consists of two steps , which are a pre processing step and a processing step . In the preprocessing step , we build a sequential permutation li = { 1iτ , iτ ∈ Tm , where the orders in li 2iτ , , are arranged non decreasing according to their cost corresponding imτ ) . Such to permutations can help the algorithm to find the near globally optimal solution . Note that , the time complexity of the preprocessing phrase is O(m2logm ) , where |Tm| = m . The task of processing step is to find the l representative orders by using the Greedy refinement algorithm ( Algorithm 3 ) based on the Stars formed in pre processing step . The time complexity of the processing phrase is O(ml ) . imτ } over Tm for each order
2iτ ) ≤ ≤ dE ( iτ , iτ , that is , dE ( iτ ,
1iτ ) ≤ dE ( iτ , iT 〉 |
Algorithm 3 The Greedy RF algorithm for clustering orders Input : Tm = { 1τ ,… , mτ } , U = {〈 iτ , iT ⊆ Tm} , l iτ ∈ Tm , Output : A set of representative l orders Tl ={〈 1T 〉〈 1τ , 1 . Let B = {} be a buffer that can hold m 〈 iτ , iT 〉 pairs 2 . While Tm ≠ ∅ and l > 0 Do 3 . B ← ∅ 4 . For each 5 . Pick si = 〈 iτ , iT 〉 with minimum rs from Ui = {〈 iτ , iτ ∈ Tm Do lτ , iT 〉 | lT 〉} iT
⊆ Tm , | iT | = [ 2 , |Tm| l + 1]}
6 . B ←B +{si} 7 . End For iT 〉 with minimum rs from B 8 . Pick s = 〈 iτ , 9 . Tm← Tm – iT { iτ } , Tl ←Tl + s , l← l – 1 10 . End While 11 . Return T 5.3 Top k Ranking Algorithm We describe a solution of ranking top k answers problem , which employs the computations made in the offline steps , to provide ranked top k answers for an approximate query . As discussed above , there are l different orders of all the tuples of relation R . iX ⊆ {X1,…,Xm} Each order forming l pairs 〈 iτ 〉 . Each tuple t ∈ R in each such pair i is associated with the s ( t | iX ) as defined in Equation ( 10 ) . We adapt Fagin ’s Threshold Algorithm [ 13 ] to retrieve the top k answers . The top k algorithm ( Algorithm 4 ) works as follows . iτ is associated with a set of contexts iX ,
WWW 2009 MADRID!Track : XML and Web Data / Session : XML Querying1027 Algorithm 4 The top k ranking algorithm Input : Representative orders Tl = { 1τ ,… , lτ } , associated contexts iX ⊆ {X1,…,Xm} , an approximate query Q .
Output : top k answer tuples . 1 . Let B ={} be a buffer that can hold k tuples ordered by score 2 . Let L be an array of size l storing the last score from each order 3 . Repeat 4 . For all i ∈ {1,…,l} Do 5 . Retrieve next tuple t from 6 . Compute score( , sim( 7 . Update L with score of t in 8 . If t ∈ Q ( R ) 9 . Get score of t from other orders { iτ | iτ ' Q X s t X , iτ as the score of t jτ ∈ Tl and j ≠ i}
' t Q
( |
=
)
⋅
)
) i i via random access
10 . score(t , Q ) ← summing up of all the retrieved scores 11 . Insert 〈t , score(t , Q)〉 in the correct position in B 12 . End If 13 . End For
14 . Until
B K [
].score
15 . Return B l
≥ ∑ L i [ ] i
1 =
6 . EXPERIMENTS 6.1 Experimental Setup For our evaluation , we set up a used car database CarDB ( Make , Model , Year , Color , Engine , Price , Mileage ) containing 100,000 tuples extracted from Yahoo! Autos . The attributes Make , Model , Year , Color and Engine are categorical attributes and the attributes Price and Mileage are numerical attributes . We used Microsoft SQL Server 2005 RDBMS on a P4 3.2 GHz PC with 1 GB of RAM for our experiments . We implemented all algorithms in C# and connected to the RDBMS through ADO . We employed the approach discussed in Section 4.1 to generate preferences from the workload of internet used car database . Using a confidence level of 0.2 , we obtained 529 different classes of preferences , which are used in our experiments .
6.2 Relaxation and Ranking Experiments To verify the efficiency of the query relaxation and results ranking methods of AQRR , we requested 5 subjects to behave as different kinds of buyers , such as rich people , clerks , students , women , etc . and each subject was asked to submit three queries for CarDB according to their preference . Each query had on average 2.8 specified attributes for CarDB . Since it is not practical to ask the people to find all relevant answers in the database and rank the whole query results for a given approximate query , we adopt the following strategy . For each test query Qi , a set Hi of 30 tuples , which likely to contain a good mix of relevant and irrelevant tuples to the query , is generated ( We did this by mixing the Top 10 results of each ranking algorithm of AQRR , AIMQ and RANDOM , removing ties , and adding a few randomly selected tuples . These ranking algorithms will be described in the following ) Finally , we presented the queries along with their corresponding Hi ’s to each user in our study . Each subject ’s responsibility was to mark the tuples in Hi relevant to the query Qi and mark the Top 10 tuples that they preferred most . We then measure the efficiency of query relaxation and results ranking methods of AQRR .
621 Query Relaxation Experiment We use the Recall metrics to evaluate the efficiency of our query relaxation method . Recall is the ratio of the number of relevant tuples retrieved to the total number of relevant tuples . To evaluate the Recall of answers we provide , we also set up another approximate query answering system that uses the AIMQ algorithm proposed in [ 16 ] , since AIMQ addresses the similar problem ( answering imprecise queries over autonomous database ) as AQRR does . AIMQ makes use of the AFDs ( approximate function dependencies ) and approximate keys to decide the attribute relaxation order , and then AIMQ relaxes the original query according to the relaxation order and the threshold for the query . The Recall of answers for AQRR and AIMQ is shown in Figure 2 ( where Tsim=0.7 for both AQRR and AIMQ ) .
AQRR
AIMQ l l a c e R
1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0
1
2 3
4 5
6 7
8
9 10 11 12 13 14 15
Queries
Figure 2 . Recall of answers for AQRR and AIMQ
It can be seen that the Recall of AQRR consistently higher than AIMQ . The average Recall of AQRR is 0.92 while the AIMQ is 077 This is because AIMQ learns the attribute importance based on some pre extracted data and the importance of attributes are invariant to the different user queries . It is also incapable of giving a specific weight to show how important each attribute is . In contrast , AQRR speculates how much the user cares about each specified attribute according to the user ’s query and assigns a specific weight to each specified attribute . Thus , the attribute importance can be tailored to the user preferences . Additionally , the similarities between different attribute values are reasonable . Hence , the result tuples for the approximate query can meet the user ’s needs and preferences more closely .
622 Ranking Experiment This experiment aims at evaluating the ranking precision of AQRR . Besides AQRR described above , we implemented RANDOM and AIMQ algorithms , to compare with AQRR . RANDOM ranking model : In the RANDOM ranking model , the tuples in the query results are presented to the user in a random order . The RANDOM model provides a base line to show how well AQRR can capture the user behaviour over a random method . AIMQ ranking model : AIMQ measures the similarity between an imprecise query Q and an answer tuple t as
Sim(Q,t )
= n
∑ i 1 =
W ( A ) i imp
⎧ ⎪ × ⎨ ⎪⎩
VSim(Q.A ,t.A ) , i i if Domain(A )=Categorical i
1
Q.A ,t.A i
Q.A i i
, if Domain(A )=Numerical i where n = Count(boundattributes(Q) ) , Wimp( = 1 ) is the importance weight of each attribute , and VSim measures the similarity between the categorical values . The similarities between the Q and answer tuples are used to rank the relevant query results . i 1W=∑ imp n
WWW 2009 MADRID!Track : XML and Web Data / Session : XML Querying1028 For formally comparing the ranking precision of the various ranking functions , we used a standard collaborative filtering metric R proposed in paper [ 3 ] to measure ranking quality ( Equation ( 12) ) . In the equation , ri is the subject ’s preference for the ith tuple in the ranked list returned by the ranking function ( 1 if it is marked relevant , and 0 otherwise ) .
R
= ∑ i r i i 1 −⎛ ⎜ 92 ⎝
⎞ ⎟ ⎠
( 12 )
In order to make sure that the same results without the same order will be retrieved by using each ranking algorithm , we use the same relaxed queries ( where Tsim = 0.7 for each original query ) as the input for RANDOM , AIMQ and AQRR . Figure 3 shows the ranking precision of different ranking algorithms for each query .
AQRR
AIMQ
RANDOM c i r t e
M R
1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0
1
2 3
4 5
6 7
8 9 10 11 12 13 14 15
Queries
Figure 3 . Precision for different ranking methods for CarDB
It can be seen that AQRR greatly outperform AIMQ and RANDOM . The average ranking precision of AQRR and AIMQ were 0.83 and 0.55 , respectively . The reason is that AIMQ can only rank the relevant answers of the relaxed query by using the similarities while the exact answers are ranked randomly . In contrast , AQRR can rank both the exact answers and relevant answers according to the user ’s needs and preferences . Moreover , the ranking method of AQRR takes both the similarities between tuples and the query and the user ’s preferences into consideration , so that the ranking results can capture the user ’s needs and preferences more efficiently . Also , our method computes the similarity of numerical attribute values factoring in the distribution of attribute values while AIMQ only computed the distance of two numerical attribute values .
6.3 Orders Creating&Clustering Experiments 631 Orders Creating Experiment This experiment aims at evaluating the accuracy of the orders of tuples created by Greedy algorithm based on the contextual preferences with interest degrees ( henceforth referred to as CPDG algorithm ) . In [ 1 ] , Agrawal presents a contextual preference model of the form “ A1= a1 ( cid:59 ) A1= a2 | X ” , while our contextual preference model has the form “ A1= a1 ( cid:59 ) A1= a2 , d | X ” . Obviously , the critical difference between the two preference models is that our contextual preference model involves the interest degree of the preference while the contextual preference model in [ 1 ] does not . We also build system by employing the Greedy algorithm to create orders based on the contextual preferences without interest degrees ( henceforth referred to as CPG algorithm ) . Next , we conduct an experiment to compare the accuracy of orders created by CPDG and CPG , respectively . We collect 50 tuples asserted by 100 preferences in the same preference class . We then use the metric P ( Equation ( 13 ) ) to evaluate the accuracy of orders
P = Order ( A ) ∩ Γ / | T | ( 13 ) to where A = {CPDG , CPG} , Γ represents the correct order of test tuples ranked according the input preferences , Order(A ) represents the order of tuples created by algorithm A . Order(A ) ∩ Γ represents the number of tuples locating in the same position in the orders of Order(A ) and Γ , respectively ; |T | is the total number of test tuples . Figure 4 shows the accuracy of the orders created by CPDG and CPG , resp .
CPDG
CPG c i r t e
M P
1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0
10
20
30
40
50
60
Number of Preferences
70
80
90 100
Figure 4 . Accuracy of orders for CPDG and CPG
It can be seen that the accuracy of the order created by CPDG is steadily better than the order created by CPG . Additionally , the accuracy of the order created by CPG degrades as the amount of preferences increases . This is because the contextual preference model does not consider the interest degree of preference so that it can not reconcile the contradictions of too many preferences .
632 Orders Clustering Experiment This experiment aims at testing the quality of the algorithm for the orders clustering . For this experiment we assume there are no indifferent tuples . Every dataset is characterized by 4 parameters : n , m , l , noise . Here n is the number of tuples in each of the orders , m is the number of input orders , and l is the number of true underlying clusters . We generate l random orders by sampling uniformly at random the space of all possible permutations of n elements . These initial orders form the centers around which we build each one of the clusters . The task of the algorithms is to rediscover the clustering model used for the data generation . Given a cluster center , each order from the same cluster is generated by adding to the center a specified amount of noise of swaps . The swap means that tuples from the initial order are picked and their positions in the order are exchanged . The amount of noise is the number of swaps we make . We experiment with datasets generated for the following parameters : n = 300 , m = 600 , l = {8 , 16} , noise = {2 , 4 , 8,…,128} for swaps . Figure 5 shows the performance of the algorithms as a function of the amount of noise . The y axis is ratio : F(A)/F(INP ) , for A = {Greedy , Furthest , Greedy RF} , where the Greedy RF algorithm is proposed in Algorithm 3 , while the Greedy and Furthest algorithm are presented in [ 1 ] . We compare them here since they all aim at solving the orders clustering problem . The F(A ) is the total cost of the solution provided by algorithm A when Euclidean distance is used as a distance measure between orders . The F(INP ) corresponds to the cost of the clustering structure ( Equation(11 ) ) used in the data generation process . l = 8 3 2.75 2.5 2.25 2 1.75 1.5 1.25 1 0.75
) P N
I ( F
/ )
A
( F
Greedy
Furthest
Greedy RF l = 16 3 2.75 2.5 2.25 2 1.75 1.5 1.25 1 0.75
) P N
I ( F
/ )
A
( F
Greedy
Furthest
Greedy RF
1
2
3
5 Number of Clusters
4
6
7
1
2
3
5 Number of Clusters
4
6
7
Figure 5 . Algorithms’ performance for orders clustering
WWW 2009 MADRID!Track : XML and Web Data / Session : XML Querying1029 From Figure 5 we can see that : Greedy RF algorithm performs greatly better than Greedy algorithm and slightly better than Furthest algorithm . The reason is that : the Greedy RF is executed on the orders which were arranged according to their cost in preprocessing step and makes twice greedy selection in processing step , so that it can obtain the near globally optimization solution .
6.4 Performance Report Figure 6 shows the online execution time of the queries over CarDB as a function of the number of representative orders . It can be seen that the execution time of AQRR grows almost linearly with the number of representative orders . This is because most of the computations have been accomplished at pre processing time .
) s m i
( e m T n o i t u c e x E
25000 22500 20000 17500 15000 12500 10000 7500 5000 2500 0
0
50
150
100 200 Number of Clusters
250
300
Figure 6 . Execution representative orders . times for different numbers of
7 . CONCLUSIONS In this paper we first motivated the need for supporting approximate queries & results ranking over databases in a domain independent way . Then we presented AQRR , a domain independent approach for answering approximate queries over autonomous Web databases . Starting from the user query , AQRR assigned the weights of specified attributes according to the distribution of values of specified attributes in the database . Then , according to the similarities of different attribute values , AQRR relaxed the original query by adding the most similar categorical values or nearby numerical values into the query criteria range . For ranking the approximate query results , AQRR took advantage of the user ’s contextual preferences to pre compute a few representative orders of tuples and used them to quickly provide the ranked query results . The experiments on real dataset identified that the query relaxation method of AQRR can find more relevant tuples for the user . Rather , the top k ranked answers can be returned fast and achieve high accuracy as well . It would be interesting to investigate how to minimize the updating cost when the database and preferences are varied .
8 . ACKNOWLEDGMENTS Work is supported by the National Natural Science Foundation of China ( 60873010 ) and Program for New Century Excellent Talents in University ( NCET 05 0288 ) .
9 . REFERENCES [ 1 ] Agrawal , R . and Rantzau , R . Context sensitive ranking . In Proceedings of the SIGMOD Conference , 383 394 , 2006 .
[ 2 ] Agrawal , R . , Imielinski , T . , and Swami , A . N . Mining association rules between sets of items in large databases . In Proceedings of the SIGMOD Conference , 207 216 . 1993 .
[ 3 ] Agrawal , S . , Chaudhuri , S . , Das , G . , and Gionis , A . Automated ranking of database query results . ACM Trans . Database Syst . , 28(2 ) : 140 174 , 2003 .
[ 4 ] Bosc , P . , Hadjali , A . , and Pivert , O . Empty versus overabundant answers to flexible relational queries . Fuzzy Sets and Systems , 159 , 1450 1467 , 2007 .
[ 5 ] Chen , Z . Y . and Li , T . Addressing diverse user preferences in
SQL Query Result navigation . In Proceedings of the SIGMOD Conference , 641 652 , 2007 .
[ 6 ] Chakrabarti , K . , Ganti , V . , Han , J . , and Xin , D . Ranking objects based on relationships . In Proceedings of the SIGMOD Conference , 371–382 , 2006 .
[ 7 ] Chrobak , M . , Keynon , C . , and Young , N . The reverse greedy algorithm for the metric k median problem . Information Processing Letters 97 , 68 72 , 2005 .
[ 8 ] Chaudhuri , S . , Das , G . , and Hristidis , V . Probabilistic information retrieval approach for ranking of database query results . ACM Trans . Database Syst . , 31(3):1134–1168 , 2006 .
[ 9 ] Chakrabarti , K . , Chaudhuri , S . , and Hwang , S . Automatic categorization of query results . In Proceedings of the SIGMOD Conference , 755–766 , 2004 .
[ 10 ] Chomicki , J . Preference formulas in relational queries . ACM
Trans . Database Syst . , 28(4 ) : 427 466 , 2003 .
[ 11 ] Cohen , W . W . , Schapire , R . E . Learning to order things .
Journal of Artificial Intelligence Research , 10 , 243–270 , 1999 . [ 12 ] Das , G . , Hristidis , V . , Kapoor , N . , and Sudarshan , S . Ordering the attributes of query results . In Proceedings of the SIGMOD Conference , 395 406 , 2006 .
[ 13 ] Fagin , R . , Lotem , A . , and Naor , M . Optimal aggregation algorithms for middleware . In Proceedings of the PODS Conference , 102 113 , 2001 .
[ 14 ] Kieβling , W . Foundations of preferences in database systems .
In Proceedings of the VLDB Conference , 311 322 , 2002 .
[ 15 ] Ma , Z . M . and Yan , L . Generalization of strategies for fuzzy query translation in classical relational databases . Information and Software Technology , 49(2):172 180 , 2007 .
[ 16 ] Nambiar , U . and Kambhampati , S . Answering imprecise queries over web databases . In Proceedings of the ICDE Conference , 45 54 , 2006 .
[ 17 ] Ortega , B . M . , Chakrabarti , K . , and Mehrotra , S . An approach to integrating query refinement in SQL . In Proceedings of the EDBT Conference , 15 33 , 2002 .
[ 18 ] Ortega , B . M . Integrating similarity based retrieval and query refinement in databases . Ph.D Dissertation , UIUC , 2003 .
[ 19 ] Muslea , I . Machine learning for online query relaxation . In
Proceedings of the KDD Conference , 246 255 , 2004 .
[ 20 ] Rui , Y . , Huang , T . S . , and Merhotra , S . Content based image retrieval with relevance feedback in MARS . In Proceedings of the ICIP Conference , 815 818 , 1997 .
[ 21 ] Stefanidis , K . and Pitoura , E . Adding context to preferences . In
Proceedings of the ICDE Conference , 846 855 , 2007 .
[ 22 ] Su , W . , Wang , J . , Huang , Q . , and Lochovsky , F . Query result ranking over e commerce web databases . In Proceedings of the CIKM Conference , 575 584 , 2006 .
[ 23 ] Wu , L . , Faloutsos , C . , Sycara , K . , and Payne , T . FALCON : feedback adaptive loop for content based retrieval . In Proceedings of the VLDB Conference , 297 306 , 2000 .
WWW 2009 MADRID!Track : XML and Web Data / Session : XML Querying1030
