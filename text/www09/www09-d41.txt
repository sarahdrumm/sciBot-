Latent Space Domain Transfer between High Dimensional
Overlapping Distributions
Sihong Xie† Wei Fan‡
Jing Peng∗ Olivier Verscheure‡
Jiangtao Ren†
∗
†Sun Yat Sen University , Guangzhou , China {mc04xsh@mail2 , issrjt@mail}sysueducn
‡IBM T . J . Watson Research Center , New York , USA {weifan , ov1}@usibmcom
§Montclair State University , Montclair , New Jersey , USA pengj@mailmontclairedu
ABSTRACT Transferring knowledge from one domain to another is challenging due to a number of reasons . Since both conditional and marginal distribution of the training data and test data are non identical , model trained in one domain , when directly applied to a different domain , is usually low in accuracy . For many applications with large feature sets , such as text document , sequence data , medical data , image data of different resolutions , etc . two domains usually do not contain exactly the same features , thus introducing large numbers of “ missing values ” when considered over the union of features from both domains . In other words , its marginal distributions are at most overlapping . In the same time , these problems are usually high dimensional , such as , several thousands of features . Thus , the combination of high dimensionality and missing values make the relationship in conditional probabilities between two domains hard to measure and model . To address these challenges , we propose a framework that first brings the marginal distributions of two domains closer by “ filling up ” those missing values of disjoint features . Afterwards , it looks for those comparable sub structures in the “ latent space ” as mapped from the expanded feature vector , where both marginal and conditional distribution are similar . With these sub structures in latent space , the proposed approach then find common concepts that are transferable across domains with high probability . During prediction , unlabeled instances are treated as “ queries ” , the mostly related labeled instances from outdomain are retrieved , and the classification is made by weighted voting using retrieved out domain examples . We formally show that importing feature values across domains and latentsemantic index can jointly make the distributions of two related domains easier to measure than in original feature space , the nearest neighbor method employed to retrieve related out domain examples is bounded in error when predicting in domain examples . Software and datasets are available for download .
Categories and Subject Descriptors H33 [ Information Storage and Retrieval ] : Information Search and Retrieval ∗The author is supported by the National Natural Science Foundation of China under Grant No . 60703110 Copyright is held by the International World Wide Web Conference Committee ( IW3C2 ) . Distribution of these papers is limited to classroom use , and personal use by others . WWW 2009 , April 20–24 , 2009 , Madrid , Spain . ACM 978 1 60558 487 4/09/04 .
General Terms Algorithms
1 .
INTRODUCTION
Domain transfer for high dimensional datasets , such as microarray data , text data , web log data , is a challenging problem . When the number of features is in thousands , indomain and out domain data rarely share the exact set of feature . In the same time , there may not be any labeled example from in domain . When one uses the union of the overlapping and non overlapping features and leave the missing values as “ zero ” , the distance of two marginal distributions p(x ) can become asymptotically very large . Otherwise , if one only considers those common features , such information may be limited in its predictability , and many algorithms may have difficulties to find transferable structures across the two domains . Therefore , one main challenge to transfer high dimensional overlapping distribution is on how to effectively use those large number of features that are nonoverlapping or present in one domain but not in the other . Nonetheless , the main task for inductive learning is to identify “ sub structures ” between the two domains where it is transferable or the conditional probabilities p(y|x ) across these structures are similar . This is particularly difficult under the given scenario . First , there are no labeled data from in domain , hence the relationship of p(y|x ) across the two domains are not directly measurable . Second , the problems are high dimensional with missing values . Thus the second main challenge is on how to look for transferable substructures ( or considering subset of features in conditional probability ) in this space .
In order to resolve these two main challenges , we first bring the marginal distribution of the overlapping distributions asymptotically closer by “ filling up ” the missing values in some reasonable way to be discussed . Then to resolve the high dimensional problem , we map the original feature space into a low dimensional “ latent space ” where each feature is a linear combination of high dimensional features . We show that in this low dimensional space , transferable concepts are easier to discover and their prediction error can be bounded . By default , most inductive learner usually makes the cluster assumption indicating that two nearby points are likely to have the same class label . This is more likely to hold true in low dimensional space . Specifically , it has been shown formally that as the dimensionality of the space increases , the Euclidean distance between any points in the high dimensional space is getting asymptotically closer . In other words , distance based classification is unreliable in high dimensional space . For example , in Figure 1(a ) , we plot two domains’ data in 3 D space , where the pluses(+ ) and crosses(× ) are labeled out domain positive and negative instances , respectively . The stars( ? ) and squares(fi ) are the unlabeled in domain positive and negative instances , respectively . The two domains’ data are related since the positives from both domains lie on the x y plane while all the negatives lie on y z plane . However , two domains are different since they have quite different distribution . When classifying the stars using cosine similarity , because they are closer to some of the crosses than to the pluses , they are classified incorrectly , The cluster assumption is obviously violated in this space . Next , we will briefly summarize the proposed approach and come back to this example to see how the problem is being resolved .
Given these challenges , we propose a new latent space based method for domain transfer where there is no labeled in domain data and both out domain and in domain are high dimensional . Briefly , the proposed approach has three main steps , as depicted in Figure 2 . We first employ a “ multiple regression method ” to fill up those “ missing values ” across the two domains in order to draw the marginal distributions closer , here we assume that the discrepency between distributions of two domains over these overlapping features are reasonably small ( these features are shared by two domains ) . Then , both the high dimensional out domain and in domain data are mapped into a low dimensional latentspace . In order to classify an unlabeled in domain example , we retrieve the closest neighbors in the latent space to the in domain example and use weighted voting as the predicted class label . To be exact , regression models are built using out domain data , taking overlapping features as independent variables and non overlapping features as dependent variables . Missing values in in domain are filled up by these models . Intuitively , these uniform transferable models describe feature dependency in both domains , thus data will lie in the same space and marginal distribution p(x ) become closer ( see Section 32 ) Second , we propose to use SVD ( Singular Value Decomposition ) for dimension reduction and similar structure discovery . SVD first maps the high dimensional out domain and in domain data to a latent space with lower dimension . In this space , data giving the same concept will lie nearby[6 ] ( see Section 3.3 ) , ie the closer two instances are , the more likely they are having the same label , thus p(y|x ) across domains will be similar within cluster where instances are coherently nearby . To exploit this similar conditional distribution , given an in domain instance x0 , those out domain instances which are most close to x0 are retrieved and x0 is classified by similarity weighted voting . Now , let ’s re visit the previous example . We apply SVD on two domains’ data ( step 3 in Figure 2 ) and the lower dimensional representation in latent space are obtained , as plotted in Figure 1(b ) . For each in domain point x(star or square ) , we retrieve p nearest out domain points ( plus or cross ) and classify x according to the labels of the retrieved points and the corresponding similarity ( step 4 in Figure 2 ) . Note that the cluster assumption holds in this space . We can see clearly that the stars/squares are now approximately in the same direction of the pluses/crosses , all retrieved outdomain points will be pluses/crosses .
Our contributions are as follow : ( 1 ) We propose a transfer learning framework which make joint distributions of two
10 8 6 4 2 0 z z
0
2 y out domain positive out domain negative in domain positive in domain negative
4
6
8
10
8
10
6
4 x
0
2
( a ) High dimensional space
0.25
0.2
0.15 x
0.1
0.05
0 positive out domain negative out domain positive in domain negative in domain
0.05
0.05
0
0.05
0.15
0.2
0.25
0.1 z
( b ) Low dimensional space
Figure 1 : Illustrating Example high dimensional domains with overlapping features easier to measure , and thus identify transferable concepts is straightforward . Multiple regression and SVD are used to resolve the difficulties to measure and identify structures with similar p(x ) and p(y|x ) respectively . The experiments results demonstrate the proposed framework outperforms traditional learning algorithms including SVM . ( 2 ) Formal analysis demonstrates that missing values importation via regression asymptotically reduces the difference between two marginal distributions p(x ) than one without importation . In the low dimensional latent space , we give the upper bound of nearest neighbor classifier used in the given transfer learning scenario . This condition is guaranteed by the clusters
Figure 2 : Flow Chart of the Proposed Framework
Table 1 : Symbols Definition
Symbol ( X ` , Y ` )
( X u ) ( ˆX u )
` u p(x ) p(y|x )
F+ Fc W S A U Σ V k
Definition Labeled out domain data Unlabeled in domain data X u with predicted missing values Number of out domain Number of in domain points marginal distribution conditional distribution Features exist only in out domain Features shared by two domains input high dimensional space low dimensional latent space Data matrix combining X ` and ˆX u Matrix of principle direction Matrix of singular values Matrix of principle component Dimensionality of latent space
( namely , features in F+ ) , thus Ml gives the estimated functional relationship between Fc and f l + . Ml is then applied on H u to predict the values of all in domain points on f l + , and thus missing values are filled up .
Various regression models can fit into the proposed framework , since they are basically seeking a function such that the predictions are expected to have deviation from the actual values within a given small tolerance for all the training data . For implementation , Support Vector Regression ( SVR)[14 , 15 ] is employed to fill up missing values , it can be formulated as : minimize st kwk2 + λ
1 2 n
Xi=1
( ξi + ξ∗ i ) yli + − hw , hii − b ≤ + ξi + ≤ + ξ∗ hw , hii + b − yli ξi , ξ∗ i ≥ 0 i recovery process[6 ] . ( 3 ) The proposed framework can be generalized to include various regression and cluster methods , not limited to the experimental choices .
2 . LATENTMAP : MEASURE AND TRANSFER OVERLAPPING DISTRIBUTIONS We introduce latent space based transfer learning framework between two domains that lie on two different spaces that are at most overlapping . We summarize symbols and their definitions in Table 1 . Suppose we have ` labeled outdomain instances ( X ` , Y ` ) = {(x1 , y1 ) , . . . , ( x` , y`)} and u unlabeled in domain instances X u = {x`+1 , . . . , x`+u} . Let X ` and X u also denote matrices , each row represents an instance while each column represents a feature . We assume that two domains’ data fall in two categories ( y ∈ {0 , 1} ) . As an important step , we first discuss how to perform multiple regression to fill up the missing values , then SVD based dimension reduction and clustering is briefly discussed . 2.1 Missing Values Importation
Since two spaces are overlapping or they only share some small number of features , when a classification model is trained on out domain data using all its features then subsequently use to prediction in domain documents , one need to consider how to properly handle those “ missing values ” or features in F+ that exist only in out domain but not in in domain . As we shall see in Section 3 , if we leave those missing values as “ zeros ” , the distance between any two points can be asymptotically very large and order of difference between different points are difficult to distinguish . Nonetheless , if we fill up the missing values in some reasonable way , the order of the difference between closer points and further points is more measurable . We propose to use “ multiple regression ” to fill up those missing values across the two domains . Let Fc denote the set of features overlap across domains , and H ` and H u denote the outdomain and in domain data X ` and X u projected on Fc |F+| respectively . Let F+ = {f 1 + } . The values of outdomain points on F+ can be seen as a series of column vector yl + , l = 1 , . . . , |F+| ( not class label ) while the values of in domain points on F+ are missing . |F+| regression models M = {M1 , . . . , M|F+|} can be built using H ` as observation of independent variables ( namely , features in Fc ) and yl + , l = 1 , . . . , |F+| as observation of dependent variables
+ , . . . , f where hi is the i th row in H ` and yli + is the i th entry of + . ξi , ξ∗ yl i are slack variables , w is the model parameter and λ determines the trade off between the generalization ability ( measure by kwk2 ) and the amount up to which deviations larger than are tolerated . The functional relationship learned using H ` and yl + , ie Ml , is then applied on H u ( can be seen as new coming examples in the same space with H ` ) and give prediction on yl + . The aim of multiple regression is to minimize the discrepency between the marginal distributions of two domains . Admittedly , H ` and H u come from different distributioins , however generalization error bound is given in [ 18 ] when training and test data are from different distribution , and this justifies our regression strategy . 2.2 Dimensionality Reduction
The marginal distributions p(x ) of two domains are made easier to measure and quantify in the input space W via missing values importation . However , the dimension of the vector space is still so high that makes it hard to identify similar structures across domains . Given a point x ∈ Rn , as n grows , the distance between the “ nearest ” neighbor to x will approximate to the distance from the “ farthest ” neighbor to x . Thus , the distance in high dimensional space is meaningless . Since for any reasonable problem , one makes the the clustering manifold assumption that nearby points will have similar label , the Euclidean distance in the high dimensional space is no longer a good measure to even apply this assumption . In addition , there is no labeled in domain data , no information of p(y|x ) is given , thus the relationship of p(y|x ) across domains are not directly measurable . To solve this problem , one ought to map the data to a low dimensional space . In this space , sub structures are discovered in places where in domain and out domain points will have similar labels and the cluster assumption holds across true with high probability . We propose to use SVD for latent space mapping and sub structures discovery .
In short , SVD first maps the data to a lower dimensional space , this mapping has been proved to be consistent with k means clustering ’s objective function , namely , the low dimensional representation of data are cluster membership indicators from which clusters structure can be reconstructed [ 6 ] . By cluster assumption [ 1 ] , points in the same cluster has similar conditional distributions regardless of whether the points are from the same domain or not , thus we have identify transferable sub structure in latent space . Specif ically , let A ∈ Rt×d be the feature instance matrix of two domains ( see Table 1 ) , then each row of A is an instance and each column of A is one dimension of the union of two space . We further assume that the the first ` rows are out domain instances and the next u rows are in domain instances and the first |F+| columns are features exist only in out domain and the next |Fc| columns represent those overlapping dimensions of two spaces , thus d = ` + u and t = |F+| + |Fc| . SVD is applied on A
A = U ΣV T
( 1 ) where Σ is a t × d matrix with diagonal entries ( σi , i ∈ {1 , . . . , min{t , d} ) being the singular values
σ1 ≥ σ2 ≥ · · · ≥ σmin{t,d} and off diagonal entries being zero . The t × t matrix U and the d × d matrix V are the left and right eigenvector matrices , respectively . Note that both U and V are orthogonal matrices : U TU = It , V TV = Id . Then SVD can be seen as a solution of PCA by the following equation :
ATA = ( V ΣU TU )(ΣV T ) = V Σ2V T where V = {v1 , . . . , vd} is the eigenvectors . For dimension reduction , we obtain the top k − 1 eigenvectors with largest eigenvalues ( σ2 k−1 ) as new data representation . That is let Vk−1 ∈ Rd×k−1 contains ( v1 , . . . , vk−1 ) as columns , then each row υi , i = 1 , . . . , d in Vk−1 is a new representation of xi , i = 1 , . . . , d in the k − 1 dimensional space , ie
1 , . . . , σ2
Vk−1 = ATUk−1Σ−1 k−1
( 2 ) k−1Uk−1Σ−1 where Uk−1 is the first k − 1 columns of U and Σk−1 is a k − 1 × k − 1 matrix keeping the first k − 1 columns and rows of Σ . Note that we use ATUk−1Σ−1 k−1 to approximate the top k − 1 columns of the principle component matrix instead of truncating the V to k − 1 columns which equals AT k−1 . This is common in information retrieval and justified by the fact that Ak−1 is the best rank k − 1 approximation of A . Uk−1Σ−1 k−1 can be considered as a mapping T : W → S which maps data in the space W ( space where each dimension represents one feature ) to a new lower dimensional space S ( called latent space ) . For clustering , [ 6 ] has proved that the above dimension reduction also reveals the information of the clusters structure which k means seeks to discover . Specifically , k means uses k centroids to represent k clusters which are determined by minimizing the sum of squared error
Jk = k
Xl=1 Xi∈Cl
( xi − ml)2
( 3 ) where ml = Pi∈Cl xi/nl is the centroid of cluster Cl and nl is the number of points in Cl . Let Qk−1 = ( q1 , . . . , qk−1 ) be cluster membership indicator vectors ( we’ll show how to reconstruct cluster structure later ) such that QT k−1Qk−1 = Ik−1 ∈ R(k−1)×(k−1 ) and qT l e = 0 , l = 1 , . . . , k − 1 . Then the k means objective can be written as
Jk = Tr(ATA ) − eTATAe/n − Tr(QT k−1ATAQk−1 )
( 4 ) since the first two terms in Equation ( 4 ) have nothing to do with Qk−1 , then the k means objective becomes
Tr(QT k−1ATAQk−1 ) max Qk−1
( 5 )
The objective ( 5 ) has a closed form and global optimal solution which is the eigenvectors Vk−1 = ( v1 , . . . , vk−1 ) of ATA . The clusters structure can be recovered by constructing a connectivity matrix :
Sim = Vk−1V T k−1
( 6 )
The entry Simij can be interpreted as connectivity between xi and xj , we can associate a connectivity probability be tween xi and xj : cij = Simij/Sim jj . Finally , the clusters structure is determined such that xi and xj are in the same cluster if and only if cij > β . ii Sim
1 2
1 2
The effect of SVD is three fold . First , further bring the marginal distributions p(x ) in S close given p(x ) is sufficient close in W , this is achieved by the mapping given by Uk−1ΣT k−1 as we will see in Section 3 . Second , identify transferable sub structure through which out domain knowledge can be used for in domain learning . Given an unlabeled indomain instance , we exploit the cluster structure by using top p nearest labeled instances for weighted voting .
Label0(xi ) = Xxj ∈N(xi )
Label(xj)Sim(xi , xj )
( 7 ) where Label0(x ) is the prediction and Label(x ) is the true label of x and N ( xi ) is the set of p nearest neighbors of xi according to Sim . Since given xi , those xj with highest cij must be in the same cluster of Xi , given the size of the cluster is large enough , thus points in N ( xi ) must be those out domain points which have the most similar conditional distribution with xi by the cluster assumption[1 ] . Finally , we see that the dimension is simultaneously reduced to k−1 . 2.3 LatentMap for Domain Transfer
The above discussion provides us some insights to solve the problems raised in Section 1 . We propose to use both regression and latent space mapping to bring the distributions p(x , y ) of in domain and out domain close and at the same time reduce the dimensions . We call the proposed framework LatentMap in the sequel . As shown in Algorithm 1 , LatentMap consists of two key steps to deal with distributional difference between domains . First , X ` is used to give models which describe the relationship between features Fc and each feature f l + , l = 1 , . . . , |F+| , then we apply these models on X u to fill up the missing values , the resulting data is denoted by ˆX u . This step ensure we can bring the in domain and out domain marginal distributions in W close , as shown in Section 3 . Then these data is used to construct the data matrix A , which incorporates X ` as its first ` rows and ˆX u as its next u rows . A is decomposed into three matrices using SVD and new representation of two domains’ data are obtained in the latent space S . It has to be noted that this representation is indicators of clusters structure , instances having the same concept will be close in the latent space S . In other words , points in the same cluster express similar concepts and their labels are expected to be the same , thus we bring the conditional distributions p(y|x ) of two domains close in S . Note also that this new representation of data have much lower dimensions than the data have in W and dimension reduction is fulfilled . What follows is instance retrieval step , given an in domain instance xj , j = ` + 1 , . . . , ` + u , the labels of p out domain instances which are most close to xj are use to vote for the label of xj . This retrieval process may use instances from other clusters which xj does not belong to according to the cluster recovery process , nonetheless , the way that points are clustered depends on β ( see the previous section ) : if β is small , size of clusters will become large and otherwise small , the clustering result may not be perfect . By using similarity weighted voting , we can ensure that the nearest neighbors ( which are most likely to have the same conditional probability ) have the most significant effect on deciding xj ’s label while those points with less similarity to xj are not excluded entirely but have a weaker effect on labelling xj .
Algorithm 1 LatentMap : Transfer Learning between High Dimensional Overlapping Distributions 1 : Input : ( X ` , Y ` ) , X u 2 : Output : Labels of X u 3 : Build regression models M = {M1 , . . . , M|F+|} using X ` . 4 : Use M to predict missing values in X u and obtain ˆX u 5 : Construct A ∈ Rt×d by taking X ` as A ’s first ` rows and ˆX u
6 : Apply SVD on A , A = U ΣV T . 7 : Present the out domain data in latent space , Vk−1 = as next u rows .
ATUk−1Σ−1 k−1 space do
8 : for Each in domain instance υi , i = ` + 1 , . . . , ` + u in latent
9 :
10 :
Calculate the similarity between υi and all the out domain instance υj , j = 1 , . . . , ` Retrieve top p out domain instances based on the calculated similarity . Label xi by voting using the retrieved instances’ labels .
11 : 12 : end for
3 . FORMAL ANALYSIS
In this section , we provide the theoretical basis for LatentMap . In transfer learning , the distributions of the two domains are different . For convenience , let 0 and 1 be the subscripts denoting out domain and in domain , respectively . We assume data from two domains are generated according to two unknown distributions p0(x , y ) = p0(x)p0(y|x ) and p1(x , y ) = p1(x)p1(y|x ) , where pi(x ) , i = 0 , 1 are the marginals , pi(y|x ) , i = 0 , 1 are the conditionals , and p0(x ) 6= p1(x ) ,p0(y|x ) 6= p1(y|x ) . It is difficult for learning algorithms to learn effectively since they usually assume p0(x , y ) = p1(x , y ) . The pivot is to find ways to mitigate the problem arising from this difference .
The outline of the analysis is as follows . First , we show that missing value prediction and participation of the indomain data in SVD computation allow us to establish a bound for |p0(x)− ˆp1(x)| in latent space , where ˆp1 represents the induced in domain marginal in latent space . Next , under the clustering assumption [ 1 ] and by the bounded difference between the two marginals , we can further assume that the difference between the two conditionals is also bounded or at least similar across the two domains . Thus we can show that SVD not only brings two marginal distributions closer but also helps us discover clustering structures , where outdomain instances have their conditionals similar to a given in domain instance . Note that we measure marginal distribution discrepency using Euclidean distance , and the Parzen windows method justifies this : where pn(x ) is the estimated density at x , given instances xi , i = 1 , · · · , n . Thus pn(x ) is the sum of n Gaussians centered at xi . pn(x ) is a function of the distance between x and the instances xi . In the sequel , we shall consider the marginal distribution p(x ) as the empirical estimated marginal distribution pn(x ) given by Equation ( 8 ) . 3.1 Generalization Error with different Train ing and Test Distributions
As stated before , the challenge of transfer learning is that the joint distributions p(x , y ) of the two domains are different . LatentMap aims at bridging this difference by bringing ˆp1(x ) closer to p0(x ) . This strategy is justified by a theorem given in [ 18 ] that states that when the joint probabilities p(x , y ) of the training and test data are different , the generalization error can be bounded asymptotically and the bound has two terms : one bounds the out domain generalization error and the other bounds the difference between the two distributions . Assume that from the training data X ` , Y ` we can obtain a Bayesian predictive model p(y|x , X ` , Y ` ) = Z p(y|x , ω)p(ω|X ` , Y `)dω
( 9 ) where ω is the parameter of the model . For i = 0 , 1 , let
Gi(` ) = Ei x,yE0
X `,Y ` [ log pi(y|x ) p(y|x , X ` , Y ` )
]
( 10 )
Note that the generalization error is measured in KL divergence . Also notice that the expectation Ei x,y is over x and y with distribution pi(y|x)pi(x ) . Thus G0(` ) and G1(` ) correspond to the generalization error with and without domain distributional difference , respectively . To give the generalization bound , two assumptions are made in [ 18 ] : ( A ) Gi(` ) has an asymptotic expansion and Gi(` ) → Bi as ` → ∞ , where Bi is a constant . ( B ) The largest difference between the training and test distributions is finite , ie
M0 = max x,y∼p0(y|x)p0(x )
[ p1(y|x)p1(x ) p0(y|x)p0(x )
] < ∞
( 11 )
Theorem 3.1 Under the assumptions ( A ) and ( B ) , the generalization error G1(` ) asymptotically has an upper bound ,
G1(` ) ≤ M0G0(` ) + D1 + D2 ,
( 12 ) where
D1 = Z p1(y|x)p1(x ) log p1(y|x ) p0(y|x ) dxdy
D2 = 0 if p1(y|x ) = p0(y|x ) and 1 otherwise .
The detail of the proof can be found in [ 18 ] . G0(` ) represents the out domain generalization bound . D2 is 1 since p0(y|x ) 6= p1(y|x ) . To minimize the asymptotically generalization error upper bound in transfer learning , we want to minimize the rest two terms : D1 and M0 relying on how close the two domain distributions are . First , D1 is KL divergence between the two conditionals p0(y|x ) and p1(y|x ) . Second , since p0(x ) is fixed , maxx,y∼p0(y|x)p0(x ) p1(x)/p0(x ) will be minimized when the difference between the induced in domain marginal ˆp1(x ) and p0(x ) can be minimized . pn(x ) =
1 αn n
Xi=1 exp−
( x−xi )2
2σ2
( 8 )
Empirically , given a finite number of in domain points {x`+1 , . . . , x`+u} generated according to p1(x , y ) , if the two conditional distributions are similar , then for all i = ` +
1 , . . . , ` + u , p(y|xi ) will deviate from p(y|xi ) only by a small amount . Thus the estimated D1 will be small . In addition , given out domain points X ` = {x1 , . . . , x`} , if in domain points X u are close to X ` , according to the Parzen windows method , the estimated marginals p1(xi ) and p0(xi ) at xi , i = 1 , . . . , ` as a function of the distance between xi and points in X ` and X u , respectively , will also be close . We conclude that if the difference between the two distributions can be minimized , then the upper bound of generalization error can be minimized . 3.2 Missing Value Prediction via Regression
In this section , we analyze the effect of missing value prediction via regression . Given out domain samples xi ∈ X ` and an in domain instance xj ∈ X u , xj ’s values along features F+ are missing ( treated as zeros ) , while xi have values along features F+ ∪ Fc . Assume that ˆxj is the same as xj except having its values along F+ predicted via regression models trained using X ` . x can be projected onto F+ and Fc , respectively and be written as x = ( (xa)t , ( xb)t)t , where xa is the projection of x on F+ and xb is the projection on Fc . The squared distance between x and xj is given by
D(x , xj ) = kx − xjk2 = kxa − xa j k2 + kxb − xb jk2
( 13 ) i − xb
We assume that the difference between the two marginals over Fc is bounded , ie ∃M1 > 0 such that ∀xi ∈ X ` , xj ∈ jk2 < M1 . Thus the second term in Equation X u , kxb ( 13 ) is the same for both D(xi , xj ) and D(xi , ˆxj ) . Consider the first term in D(xi , ˆxj ) . When ylj + are predicted with SVRs , we have + − ylj kyli
+ k2 = k < w , xb ≤ kwk2kxb i > − < w , xb i − xb j > k2 jk2 < kwk2M1 where kwk2 is minimized when training the SVR model . + , · · · , y|F+|i )t , Since xa j k2 is also bounded and minimized . That is , thus kxa ∃δ > 0 such that i = ( y1i i − ˆxa
+ , · · · , y|F+|j j = ( y1j
)t and ˆxa
+
+ kxa i − ˆxa j k2 =
|F+|
Xl=1
( yli
+ − ylj
+ )2 < δ and δ is minimized through the regression model . On the other hand , kxa i − xa j k2 =
F+
Xl=1
( yli
+ − 0)2 = kxa i k2 which is fixed depending on xi . Compared with the minimized D(xi , ˆxj ) , D(xi , xj ) is bounded but not minimized . Thus we conclude here that the marginal distribution is minimized via regression . 3.3 Bound for Distributional Difference
Projecting the data onto a lower dimensional space ( latent space S ) is necessary since the dimensionality of W is high . In this section , we first prove that the difference between p0(x ) and p1(x ) can be bounded in S , given it is bounded in W . Then by the clustering assumption , we bound the difference between p0(y|x ) and p1(y|x ) in S . Once these two distributional differences are bounded , then by applying Theorem 3.1 we claim that the generalization error in transfer learning is bounded through the proposed regression and SVD based dimension reduction strategy .
Bound the marginal distributions in latent space In the previous section , we have discussed how the marginal distribution difference can be bounded in the space W . In this section , we further investigate properties of SVD and show how the difference between the two marginals can be bounded in latent space S , given this difference is bounded in W . In step 7 in Algorithm 1 , the data matrix A in W is mapped to S by Vk−1 = ATUk−1Σ−1 k−1 where Uk−1 = ( u1 , . . . , uk−1 ) . The mapping is T = Uk−1Σ−1 k−1 : W → S , a point x ∈ W is mapped to T x ∈ S that is called x ’s image in S . S can be seen as a space spanned by basis ( u1/σ1 , . . . , uk−1/σk−1 ) . The following theorem concludes that we can further bound the marginal distributions of two domains by the mapping T .
Theorem 3.2 Let x , x0 be two vectors in W and T x , T x0 be their images in the latent space S under the mapping T = Uk−1Σ−1 k−1 : W → S . If kx − x0k2 < δ , δ > 0 , then kT x −
1 σ2 k−1 j=1
T x0k2 < δrPk−1 norm kTj k2 = qPt
Proof . First , each column of T , Tj , j = 1 , . . . , k − 1 has j = 1/σj . Second , the Frobenius norm the the linear transform T can be expressed as follow : i=1 |(T )ij|2 = qkuj k2
2/σ2 kT k2
2 = kUk−1Σ−1 k−1k2
2 = k−1
Xj=1 t
Xi=1
|(T )ij|2
= k−1
Xj=1 t
(
Xi=1
|(T )ij |2 ) = k−1
Xj=1 kTik2
2 = k−1
Xj=1
1 σ2 i
Now we are ready to bound the distance of two images T x , T x0 in S . kT x − T x0k2
2 = kT ( x − x0)k2
2 ≤ kT k2
2kx − x0k2
2 < δ2 k−1
Xj=1
1 σ2 j thus we have kT x − T x0k2 < δrPk−1 j=1
1 σ2 j
Basically , since we only choose the top k − 1(k <= 10 ) eigenvectors , we can use only those eigenvectors whose corresponding eigenvalues are larger than 1 . Thus the distance of marginal distributions of two domains can be bounded in latent space .
Bound the conditional distributions in latent space In this section , we show that under the clustering assumption [ 13 ] , the proposed retrieval strategy is optimal in making the conditional distributions p0(y|x ) and p1(y|x ) similar . Then in the next section , we derive the Bayes risk of this retrieval process . Following [ 13 ] , the clustering assumption states that nearby points tend to have the same label . More precisely , let η(x ) be a regression function of y on x , η(x ) = p(y = 1|x ) , and I(· ) be the indicator function . Then cluster assumption can be written as ( C ) Let Ci , i = 1 , . . . , k be clusters , then the function x ∈ X → I(η(x ) ≥ 1/2 ) takes a constant value on each of Ci , i = 1 , . . . , k . Alternatively , the above assumption is equivalent to p(y = y0|x , x0 ∈ Ci ) ≥ p(y 6= y0|x , x0 ∈ Ci ) . So points in each cluster have the same p(y|x ) . This is similar to the manifold assumption made in [ 1 ] . The assumption requires that η(x ) vary smoothly on the support of p(x ) which is a compact manifold , ie η(x ) should not vary significantly in a small enough area on the manifold . LatentMap follows these assumptions . In reality , however , the assumptions may not hold exactly , so we employ an instance retrieval strategy to approximate the cluster structures . For each in domain instance x , p nearest neighbors from the out domain are retrieved . These neighbors are most likely to be in the same cluster as x . Weighted voting is used to predict the label of x , where a closer neighbor has more impact on deciding the label of x . This is consistent with the manifold assumption that η(x ) of two points should be close when they are nearby . We conclude that the conditional distributions p(y|x ) of two domains are brought close within each cluster . 3.4 Upbound the Risk of Nearest Neighbor Clas sifier across Domains
In previous section , we show that the generalization error of transfer learning can be bounded . Since k nn is employed as classifier in latent space S , we further analyze the Bayesian risk of k nn in LatentMap . We conclude that the risk can be bounded and the upper bound can be minimized when two conditional distributions of the two domains are positive correlated . Assume that the marginal distributions pi(x ) , i = 0 , 1 are continuous and are measurable with respect to a σ finite measure ν . Next we show that for any in domain instance x , the nearest neighbor of x in the outdomain converges to x with probability one . We need some assumptions : ( D ) Both in domain and out domain data lie in the same space . This is true since both in domain and outdomain data are in the latent space S . ( E ) Let Bx(r ) , r > 0 be the ball {ˆx ∈ X : d(x , ˆx ) ≤ r} centered at x with radius r . Bx(r ) has non negative probability measure , ∀r > 0 , with respect to the in domain marginal probability .
Lemma 3.1 Let in domain instance x be drawn according to p1(x ) and out domain instances x1 , x2 , . . . be drawn according to p0(x ) . These instances are independent . Let x0 ` be the nearest neighbor to x from the set {x1 , . . . , x`} . Then x0
` → x with probability one . Proof . From the second assumption , for a fixed in domain if the distance between point x ∈ X , for any δ > 0 , x and the nearest neighbor x0 ` from the out domain samples {x1 , · · · , x`} is larger than δ , then all the points in {x1 , · · · , x`} are outside the sphere Bx(δ ) , ie , p1{d(x , x0
` ) ≥ δ} = ( 1 − p1(Bx(δ)))` → 0
Consider a series of point sets : P` = {x1 , · · · , x`} , with increasing ` , d(x , x0 ` ) is monotonically decreasing . So the nearest neighbor of x converges to x with probability one .
To find out the bound , we investigate the Bayes decision risk in the transfer learning setting . We define the loss function as 0 1 loss : L(i , j ) = 1 , if i = j and 0 otherwise . Under such settings , the Bayes decision rule is r∗ = min j
1
Xi=0 p(y|x)L(i , j ) = min{p(y = 0|x ) , p(y = 1|x)}
= min{p(y = 0|x ) , 1 − p(y = 0|x)}
The Bayes decision rule minimizes the Bayes risk R∗ , defined as
R∗ = E[r∗ ] = Z r∗f ( x)dx where f ( x ) = P1 i=0 p(y = i)p(x|y = i ) . We have the following theorem that is the counterpart of the analysis of k nn in [ 4 ] in the transfer learning setting .
Theorem 3.3 Let p(·|y = i ) , i = 0 , 1 be such that with probability one , x is either 1 ) a continuity point of p(·|y = i ) , or 2 ) a point of non zero probability measure . Then the risk R ( probability of error ) is bounded as max{R∗
1 , R∗
0} ≤ R ≤ R∗
1 + R∗
0 − 2R∗
1R∗
0 − 2cov(r∗
1 , r∗
0 ) . where R∗ risk , respectively . i , i = 0 , 1 is the out domain and in domain Bayes
Proof . For a fixed in domain instance ( x , y ) , let ( x0
` , y0 ` ) be the nearest neighbor of x in the out domain , where y and y0 ` are independent . Then the risk of misclassifying x is given by
` are the labels of x and x0
` , respectively . y and y0 r(x , x0
` ) = E[L(y , y0
`)|x , x0 = p1(y = 0|x)p0(y0
+p1(y = 1|x)p0(y0
`|x , x0 ` )
` ] = p(y 6= y0 ` = 1|x0 ` ) ` = 0|x0 ` )
Similar to [ 4 ] , here we wish to show that r(x , x0 to the random variable r∗
` ) converges 0 with probabilities 1 . By Lemma 3.1 and the continuity of p(·|y = i ) , with ` ) → p(·|x ) for both in domain and out probability 1 , p(·|x0 domain posterior probabilities . Thus
0 − 2r∗
1 + r∗
1 r∗ r(x , x0
` ) → r∗(x )
= p1(y = 0|x)p0(y = 1|x )
+p1(y = 1|x)p0(y = 0|x )
= p1(y = 0|x)(1 − p0(y = 0|x ) )
+(1 − p1(y = 0|x))p0(y = 0|x )
1 = min{p1(y = 0|x ) , 1 − p1(y = 0|x)} and r∗
Since r∗ 0 = min{p0(y = 0|x ) , 1−p0(y = 0|x)} , r∗(x ) can be expressed as 1(1−r∗ r(x ) = r∗ Because r(x , x0 nated convergence theorem , r(x , x0
0 )+(1−r∗ ` ) is bounded below 1 , applying the domi
0 . Overall risk R = lim`→∞ E[r(x , x0
` ) ] = E[r(x ) ]
1 )r∗
( 14 )
`) ] .
R = E[ lim `→∞ 1 ( x ) ] + E[r∗ 0 − 2E[r∗ 0 − 2R∗ 1R∗
= E[r∗ = R∗ = R∗
1 + R∗ 1 + R∗
0 ( x ) ] − 2E[r∗ 1 ( x)r∗ 0 − 2cov(r∗
0(x ) ]
1 , r∗ 0 )
1 ( x)r∗
0(x ) ]
( 15 ) ( 16 ) ( 17 ) where R∗ i Rewriting Equation ( 14 ) , we have is the Bayes risk that is the expectation of r∗ i .
R = E[r∗
1 ( x ) + r∗
= R∗
1 + E[r∗
0(x ) − 2r∗ 0 ( x)(1 − 2r∗
1 ( x)r∗ 0(x ) ] 1 ( x) ) ] ≥ R∗ 1 similarly , we have R ≥ R∗ max{R∗
0} ≤ R ≤ R∗
1 , R∗
0 thus we have 1R∗
0 − 2R∗
1 + R∗
0 − 2cov(r∗
1 , r∗
0 ) .
Since r∗ = min{p(y1|x ) , 1 − p(y1|x)} , r∗ itive correlated , giving a positive cov(r∗
1 and r∗ 1 ) , r∗ 0 ) .
0 can be pos
Remark ( 1 ) If the two conditional distributions p0(y|x ) and p1(y|x ) are identical , the lower and upper bounds are the same as k nn ’s ( see [ 4] ) . Note that the upper bound can not be better than k nn ’s upper bound . ( 2 ) In transfer learning , p0(y|x ) 6= p1(y|x ) , the lower bound is the larger one of {R∗ 1} , which indicates k nn can not perform better when
0 , R∗
Table 2 : Data Summary
Instances ` u
2020 2005 2431 2007 2218 1963 1885 1663 1239 1016 1079
2008 1980 1951 2373 1837 1992 1761 1939 1210 1046 1080
Features
|F+| 1081 810 791 682 1007 1017 615 490 230 178 233
|Fc| 4172 4165 4345 5072 5017 4956 4677 5104 4091 3892 3834
|F+|/|Fc|
0.2591 0.1945 0.1820 0.1345 0.2007 0.2052 0.1315 0.0960 0.0562 0.0457 0.0608
Data Sets
Re vs Si Au vs Av
C vs R C vs S C vs T R vs S R vs T S vs T O vs Pe O vs Pl Pe vs Pl training and test data are from different domains than from a single domain . The upper bound says that if p0(y|x ) and p1(y|x ) are positively correlated , then the upper bound will be lower . However , when p0(y|x ) and y1(y|x ) negatively correlates , ie two domains’ concepts contradict , then the upper bound grows . This is consistent with our intuition : transfer learning will benefit from two domains’ similarity . 3.5 Scalability Issues
One of the crux in LatentMap is to compute the SVD of a large matrix A . We don’t have compute the exact SVD which requires O(m2n + mn2 ) computational complexity where m and n are number of rows and columns respectively . Iterative algorithms for computing the first k − 1 eigenvectors ( and the corresponding eigenvalues ) exist such as Lanczos method . Recently , randomized SVD are proposed , such as the method in [ 7 ] , it sample columns of a large matrix according to a suitable probability distribution then a smaller matrix is constructed on which SVD is applied . This method provides a good approximation of the exact SVD and its running time is O(mn + n ) . Another issue is find the top p nearest neighbor in ` out domain instances for u in domain data in the k dimension space , the computational complexity is O(k ∗ |`| ∗ |u| ) .
4 . EXPERIMENT
To demonstrate the effectiveness of the proposed framework , we carried out experiments on several data sets frequently used in transfer learning . Results show that LatentMap can map the data to a significant low dimension space where the distributions of two domains are similar . Missing values are dealt with properly , which improve the performance when the missing values are relatively copious . 4.1 Data Sets and Experiment Setup
We conducted experiments on three text data sets , all of which have different in and out domain distributions . We used SRAA ( Simulated Real Auto Aviation ) , 20 newsgroups and Reuters 21758 as three main document classification tasks in this experiment . The SRAA corpus contains 73,218 UseNet articles from four discussion groups : simulated auto racing , simulated aviation , real autos , and real aviation . The 20 newsgroups corpus contains approximately 20,000 newsgroup documents , while Reuters 21758 contains 21758 Reuters news articles in 1987 . Corpora are organized in a hierarchical manner . Our task is to classify documents into one of the top level categories in the hierarchy . For example , in one of the 20 newsgroups task , we want to tell whether a document comes from category comp or rec . Since the distributions of in domain and out domain data are different in the transfer learning setting , we split documents from each top category into two sub categories , one as the in domain category and the other as the outdomain category . For example , the out domain data consists of documents from compwindowsx and rec.autos , while the in domain data contains documents from comp.graphics and recmotorcycles All three main data sets are such organized that there are totally 11 transfer learning tasks in the experiment .
Raw text files are transformed into word vectors . All letters are turned into lower case and IDF TF is used to produce term values . We discard terms whose occurrences are less than 2 . Each word vector is normalized such that the length of the vector equals one . The Lovins stemming scheme is used to stem words appearing in the text . Simple tokening and stop word processing are used according to weka ’s default setting .
In cross domain text classification problems , some features ( terms ) appear in both domains , while others appear only in the out domain and missing in the in domain , and vice verse . Table 2 shows the statistics of the features of all 11 tasks . As we can see , all the cross domain tasks have missing values , ie , F+ is not empty . The last column in Table 2 shows the ratio |F+|/Fc . In some tasks , this ratio is quite significant . For example , in the task of Real vs Simulated , |F+| is over one forth of |Fc| . In other tasks such as Orgs vs Places , the ratio is less than 1/20 .
Baseline methods We compare LatentMap with various learning algorithms , including naive Bayes ( NB ) , Logistic regression ( LR ) , decision trees ( C4.5 ) and SVM . For the implementation of naive Bayes , Logistic regression and decision trees , we use the Weka package[16 ] . SVMlight is used as SVM classifier . Procedural parameters are kept as default for all the classifiers . To predict missing values , we use SVR as our predictor and the implementation is provided by libSVM[3 ] . The traditional learning algorithms assume that the training and test data are governed by an identical distribution p(x , y ) = p(x)p(y|x ) . For this reason , we provide these learning methods with the original high dimensional data . In particular , the union of F+ and Fc are used as a whole and missing values are set to zero . We note that LatentMap has two key steps . First , the missing values in the in domain data are predicted from the out domain data . And as such , the induced in domain marginal is closer to the out domain marginal in input space W . Second , projecting the data onto a lower dimensional latent space S built using both the in and out domain data not only reveals cluster structures but also provides tight bounds for the two conditionals in the latent space . To see the effectiveness of these two steps , we include the following two methods in our comparison : 1 ) Running k nn on data with regression but without dimension reduction ( called k nnReg , short for k nn after Regression ) 2 ) Mapping the data with missing values set to zero to latent space ( called pLatentMap , short for partial LatentMap ) . 4.2 Performance Evaluation
In this section , the experiment results of LatentMap against the baseline methods are provided . The results show clearly that LatentMap is able to bring two domains’ joint distri butions p(x , y ) closer via regression and latent space projection , giving rise to a effective transfer learning framework .
Overall Performance Study The results of LatentMap and other traditional methods on three data sets are summarized in Table 3 with the best results in bold font . It can be seen that in most of the tasks ( 10 out of 11 ) the performance is improved significantly . One exception is in task Comp vs Sci where the accuracy is slightly lower ( within 3 % ) than the best of the baseline methods . Since all the baseline learning algorithms assume that the underlying distributions p(x , y ) of training data and test data are identical , they perform poorly on most of the transfer tasks . For example , in the task Rec vs Talk from the 20 newsgroups data set , the lowest accuracy ( around 60 % ) is achieved by Logistic regression and the decision tree , while the best learner ( naive Bayes and SVMs ) make correct predictions around 72 % . In this situation , LatentMap outperforms the best baseline method with an improvement of near 20 % . Over all , the smallest margin of improvement is around 2 % on the task Orgs vs People .
Comparison of LatentMap and two simpler implementations is depicted in Figure 3 . In Figure 3(a ) , we compare LatentMap and k nnReg . It is clearly shown that by filling up the missing values , either LatentMap or k nnReg outperforms the best of the baseline methods in 9 out of 11 tasks . Among these 9 tasks , LatentMap greatly outperforms k nnReg in 6 tasks ( task 2,3,5,6,7 and 8 ) and very close to k nnReg in 2 tasks ( task 10 and 11 ) . This confirms that the second step that further discovers cluster structures in the latent space and moves the induced p1(y|x ) closer to p0(y|x ) can greatly improve the accuracy . Notice that while LatentMap has a lower accuracy than k nnReg in tasks 10 and 11 , it is still higher than that of the baseline methods . In Figure 3(b ) , we show the effect of multiple regression . By making the conditional distributions of two domains similar via latent space mapping , LatentMap and pLatentMap together outperform the baseline methods 9 out of 11 tasks ( tasks 1,2,3,5,6,7,8,10 and 11 ) . Furthermore , by predicting missing values through regression analysis , LatentMap outperforms pLatentMap in 5 out of these 9 tasks ( task 2,3,6,7,11 ) with other three close to pLatentMap ( tasks 8,9,10 ) . Note that in Figure 3(b ) , on the last three tasks , LatentMap performs approximately the same as pLatentMap . By examining the last column of Table 2 , we can see that LatentMap can achieve greater improvement on tasks where two domains overlap less or higher |F+|/|Fc| ratio ( tasks Comp vs Talk and Rec vs Sci have the minimal feature set overlapping among six 20 Newsgroups tasks ) . Our results show that LatentMap can effectively lessen the discrepancy of two domains distributions . In particular , latent space mapping that discovers cluster structures can greatly improve the performance while regression analysis guarantees the performance when a large number of values is missing .
Parameter Sensitivity There are two important parameters in the LatentMap algorithm : dimensionality of the latent space k and the number of documents in out domain to retrieve for voting p . We choose one task from each of the three main tasks to examine the sensitivity issue . Since the distributions of in domain and out domain data are different , parameters chosen using cross validation on the out domain data will not work for the in domain data .
11
6 y c a r u c c A
0.98
0.97
0.96
0.95
0.94
0.93
0.92
0.91
0.9
0 y c a r u c c A
0.98
0.97
0.96
0.95
0.94
0.93
2 y c a r u c c A
0.72
0.71
0.7
0.69
0.68
0.67
0.66
0.65
0.64
0.63
0.62
0 y c a r u c c A
0.71
0.7
0.69
0.68
0.67
0.66
0.65
0.64
2
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1 y c a r u c c A
0
1 y c a r u c c A
0.92
0.9
0.88
0.86
0.84
0.82
0.8
0.78
0.76
0.74
0 y c a r u c c A
0.9
0.85
0.8
0.75
0.7
0.65
0.6
0.55
0.5
2
Baseline pLatentMap LatentMap
Baseline knnReg LatentMap
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1 y c a r u c c A
2
3
4
5
6
7
8
9
10
Domain Transfer Tasks ( a ) k nnReg
11
0
1
2
3
4
5
6
7
8
9
10
Domain Transfer Tasks
( b ) pLatentMap
Figure 3 : Effect of Two Key Steps
20 Newsgroups ( Comp_vs_Talk )
Reuters ( Orgs_vs_Places )
SRAA ( Auto_vs_Aviation )
Accuracy wrt p Best Baseline Accuracy
Accuracy wrt p Best Baseline Accuracy
Accuracy wrt p Best Baseline Accuracy
50
100
150
200
50
100
150
200
50
100
150
200
#Retrieved Out−Domain Instances ( a ) Accuracy wrt retrieved out domain instances
#Retrieved Out−Domain Instances
#Retrieved Out−Domain Instances
SRAA ( Auto_vs_Aviation )
20 Newsgroups ( Comp_vs_Talk )
20 Newsgroups ( Comp_vs_Talk )
0.99
Accuracy wrt k Best Baseline Accuracy
Accuracy wrt k Best Baseline Accuracy
Accuracy wrt k Best Baseline Accuracy
2.5
3
3.5
4
4.5
5
5.5
6
2.5
3
3.5
4
4.5
5
5.5
6
2.5
3
3.5
4
4.5
5
5.5
#Dimension of Latent Space ( b ) Accuracy wrt dimensionality of latent space
#Dimension of Latent Space
#Dimension of Latent Space
Figure 4 : Sensitivity Study
In this experiment , parameter p varies from 5 to 200 with increment 10 and k varies from 2 to 6 with increment 1 . When p is changing , k is fixed at 5 . The resulting accuracy curves are depicted in Figure 4(a ) . These accuracies are compared with that obtained from the traditional learning algorithm whose performance is the best . From the figure , it is obvious that the accuracy is improved as the number of retrieved out domain instances increases , yet it remains stable after a certain threshold such as p = 150 . This confirms the cluster assumption . That is , when more nearest neighbors are selected for voting , the effect of minor misclustered out domain instances will be diminished or canceled out , leading to higher accuracy . The accuracy with respect to dimensionality of the latent space is higher than the best baseline classifier . Thus it is not critical which value k takes . When the underlying latent space ’s dimensions are changing , we always retrieve 50 instances from the out domain , the resulting curves are shown in Figure 4(b )
5 . RELATED WORK
One main challenge of transfer learning is how to resolve and , in the same time , take advantage of the difference between two domains . Some are based on instance weighting strategy ( [2 , 5 , 8 , 12] ) . For example , [ 5 ] adopts the boosting weight formula as the re weighting scheme . Some other methods base on dimension reduction , which usually map data to a new representation facilitating domain transfer ( [10] ) . Recently , [ 9 ] proposes a locally weighted ensemble framework to combine multiple models for transfer learning by dynamically assigning weights of a model according to a model ’s predictive power on each test example . [ 11 ]
Table 3 : Comparison of Performance
Methods
NB LR C4.5 SVM
LatentMap
SRAA
Re vs Si Au vs Av 0.6838 0.6863 0.635 0.6877 0.7311
0.6889 0.6768 0.7576 0.7399 0.8929
C vs R 0.8098 0.8467 0.6858 0.8401 0.9421
C vs S 0.7042 0.6195 0.5908 0.6962 0.6890
20 News Groups C vs T R vs S 0.7113 0.7701 0.6391 0.7400 0.9006
0.89 0.933 0.7104 0.9107 0.9777
Reuters
R vs T 0.7189 0.5928 0.5997 0.7269 0.9154
S vs T 0.704 0.6818 0.6266 0.7375 0.8633
O vs Pe O vs Pl Pe vs Pl 0.5769 0.6554 0.5046 0.6471 0.5231 0.5595 0.5250 0.6860 0.7050 0.5852
0.6424 0.6319 0.6195 0.6472 0.7094 proposes a information theory framework to address crosslanguage classification problem . [ 17 ] addressed the problem of cross domain text classification using PLSA ( Probabilistic Latent Semantic Analysis ) to bridge domain transfer .
6 . CONCLUSION
We address transfer learning challenges in text classification and other related problems , where the spaces of two domains are at most overlapping , the marginal and conditional distributions are different , and the dimensionality can be extremely high . We propose a framework ( LatentMap ) which draws joint distributions of two domains closer . The missing values are filled up to minimize the gap between marginal distributions , then the data is mapped to a latent space where both the marginal distribution and the relationship of two conditional distributions become easier to measure . Then , transferable sub structures can be easily identified in the mapped low dimensional latent space . The dimensionality of the latent space is usually below 10 , remarkably smaller as compared to the usual several thousands of the original space . Experiment over 11 text domain transfer tasks shows that LatentMap works as expected and achieves great improvement ( up to around 20 % ) compared to traditional learning algorithms including SVM . Comparison with two simpler strategies ( k nnReg and pLatentMap ) in the same transfer learning scenario shows that LatentMap can combine the advantages of both filling up missing value and latent space mapping . Parameters sensitivity analysis shows that LatentMap works well in very low dimensional spaces and is immune to the variation of the number of retrieved out domain instances .
7 . REFERENCES [ 1 ] Mikhail Belkin , Partha Niyogi , and Vikas Sindhwani .
Manifold regularization : A geometric framework for learning from labeled and unlabeled examples . J . Mach . Learn . Res . , 7:2399–2434 , 2006 .
[ 2 ] Steffen Bickel , Michael Br¨uckner , and Tobias Scheffer . Discriminative learning for differing training and test distributions . In ICML ’07 : Proceedings of the 24th international conference on Machine learning , pages 81–88 , New York , NY , USA , 2007 . ACM .
[ 3 ] Chih Chung Chang and Chih Jen Lin . LIBSVM : a library for support vector machines , 2001 . Software available at http://wwwcsientuedutw/ cjlin/libsvm .
[ 4 ] T . Cover and P . Hart . Nearest neighbor pattern classification . Information Theory , IEEE Transactions on , 13(1):21–27 , 1967 .
[ 5 ] Wenyuan Dai , Qiang Yang , Gui Rong Xue , and Yong
Yu . Boosting for transfer learning . In ICML ’07 : Proceedings of the 24th international conference on Machine learning , pages 193–200 , New York , NY , USA , 2007 . ACM .
[ 6 ] Chris Ding and Xiaofeng He . K means clustering via principal component analysis . In ICML ’04 : Proceedings of the twenty first international conference on Machine learning , page 29 , New York , NY , USA , 2004 . ACM .
[ 7 ] P . Drineas , A . Frieze , R . Kannan , S . Vempala , and V . Vinay . Clustering large graphs via the singular value decomposition . Mach . Learn . , 56(1 3):9–33 .
[ 8 ] Wei Fan and Ian Davidson . On sample selection bias and its efficient correction via model averaging and unlabeled examples . In SDM , 2007 .
[ 9 ] Jing Gao , Wei Fan , Jing Jiang , and Jiawei Han .
Knowledge transfer via multiple model local structure mapping . In Proceedings of the 2008 ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2008 .
[ 10 ] Xiao Ling , Wenyuan Dai , Gui Rong Xue , Qiang Yang , and Yong Yu . Spectral domain transfer learning . In KDD ’08 : Proceeding of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining , pages 488–496 , New York , NY , USA , 2008 . ACM .
[ 11 ] Xiao Ling , Gui Rong Xue , Wenyuan Dai , Yun Jiang , Qiang Yang , and Yong Yu . Can chinese web pages be classified with english data source ? In WWW , pages 969–978 , 2008 .
[ 12 ] Jiangtao Ren , Xiaoxiao Shi , Wei Fan , and Philip S . Yu . Type independent correction of sample selection bias via structural discovery and re balancing . In SDM , pages 565–576 , 2008 .
[ 13 ] Philippe Rigollet . Generalization error bounds in semi supervised classification under the cluster assumption . J . Mach . Learn . Res . , 8:1369–1392 , 2007 . [ 14 ] A . J . Smola and B . Schoelkopf . A tutorial on support vector regression , 1998 .
[ 15 ] Vladimir N . Vapnik . The nature of statistical learning theory . Springer Verlag New York , Inc . , New York , NY , USA , 1995 .
[ 16 ] Ian H . Witten and Eibe Frank . Data Mining : Practical
Machine Learning Tools and Techniques with Java Implementations . Morgan Kaufmann , October 1999 .
[ 17 ] Gui Rong Xue , Wenyuan Dai , Qiang Yang , and Yong
Yu . Topic bridged plsa for cross domain text classification . pages 627–634 . SIGIR , 2008 .
[ 18 ] Keisuke Yamazaki , Motoaki Kawanabe , Sumio
Watanabe , Masashi Sugiyama , and Klaus Robert M¨uller . Asymptotic bayesian generalization error when training and test distributions are different . In ICML ’07 : Proceedings of the 24th international conference on Machine learning , pages 1079–1086 , New York , NY , USA , 2007 . ACM .
