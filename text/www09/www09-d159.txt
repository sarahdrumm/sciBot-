Spatio Temporal Models for Estimating Click through Rate
Deepak Agarwal
Yahoo! Labs 701 First Ave
Sunnyvale , CA 94089 . dagarwal@yahoo inc.com
Bee Chung Chen
Yahoo! Labs 701 First Ave
Sunnyvale , CA 94089 . beechun@yahoo inc.com
Pradheep Elango
Yahoo! Labs 701 First Ave
Sunnyvale , CA 94089 . elango@yahoo inc.com
ABSTRACT We propose novel spatio temporal models to estimate clickthrough rates in the context of content recommendation . We track article CTR at a fixed location over time through a dynamic Gamma Poisson model and combine information from correlated locations through dynamic linear regressions , significantly improving on per location model . Our models adjust for user fatigue through an exponential tilt to the firstview CTR ( probability of click on first article exposure ) that is based only on user specific repeat exposure features . We illustrate our approach on data obtained from a module ( Today Module ) published regularly on Yahoo! Front Page and demonstrate significant improvement over commonly used baseline methods . Large scale simulation experiments to study the performance of our models under different scenarios provide encouraging results . Throughout , all modeling assumptions are validated via rigorous exploratory data analysis .
Categories and Subject Descriptors H35 [ Information Storage and Retrieval ] : Online Information Services ; G.3 [ Probability and Statistics ]
General Terms Algorithms , Design , Experimentation , Measurement
1 .
INTRODUCTION
The web has become an important medium to distribute information from various sources . Web sites like Blogosphere , YouTube , Digg , Yahoo! , Google News , MSN ; news feeds like Associated Press and Washington Post provide users with a wide range of choices to keep up with diverse content in a timely fashion . However , explosion of content may make it difficult for users to select the right content to look at – a phenomenon that is also refereed to as information overload . In fact , content publishers and aggregators select the best and most relevant content to attract and retain users to their site on an ongoing basis .
In general , a publisher page is composed of several sections ; some serve personalized content , some are links to applications ( eg e mail ) , some allow users to add new content of their choices and so on . In fact , several sites also publish a module with most popular content ; Digg , Yahoo! Buzz , Copyright is held by the International World Wide Web Conference Committee ( IW3C2 ) . Distribution of these papers is limited to classroom use , and personal use by others . WWW 2009 , April 20–24 , 2009 , Madrid , Spain . ACM 978 1 60558 487 4/09/04 .
Top Stories on Google News , Top Picks on MSN are some examples . Often , human editors select a set of articles from a diverse content pool . While editorial selection prunes low quality content and ensures various constraints that characterize a site ( eg in terms of topicality ) are met , the quality of a site may improve further if it adapts quickly to show the best articles . One way is to constantly monitor the content quality ( defined through user feedback : clicks , votes , etc ) of articles over time . In this paper , we develop and illustrate novel statistical methods to track article quality for a prominent module , called the Today Module , published on the Yahoo! Front Page .
Today Module Overview : Figure 1 shows the location of the Today Module on the Yahoo! Front Page , that consists of four tabs : Featured , Entertainment , Sports and Video . Each of the four tabs on the panel publish articles at four positions . Unlike the other three tabs , the Featured Tab is not tied to a particular topic and selects articles from a diverse content mix . We label the four positions on the Featured Tab as F1 , F2 , F3 , F4 . Only one of the four footers on a tab is “ active ” as a story ( see Fig 1 ) . The F1 story is displayed by default but a user can switch to another story ( at F2 , F3 , F4 ) by clicking on the corresponding footer . Since the F1 position has maximum exposure , it is the prime spot on the Today Module . At any given time , the system picks the top four articles from an active set ; these are displayed at F1 , F2 , F3 , F4 . New articles created by editors are periodically pushed to replace some old ones ; the procedure helps keep up with novel and important stories ( eg , breaking news ) and eliminate the irrelevant and fading ones .
Several measures based on user feedback are generally used to measure article quality scores ; those that rely on user clicks have strong signal and are readily available . When normalized by amount of exposure that is typically measured by the number of times an article is shown at a location ( e.g , at a certain position ) , we get a measure called click through rate ( CTR ) , ie , number of clicks per display ; throughout we use CTR to select the best articles in our application . Other measures like votes , time spent reading the article , etc can also be used ; the choice depends on the application .
Contributions : We provide a modeling approach to estimate CTR after incorporating spatio temporal correlations and adjusting for user fatigue . In particular , we propose dynamic models to : ( a ) Track article CTR at a single location through a dynamic Gamma Poisson model . ( b ) Combine information from correlated locations ( eg , other positions ) through dynamic linear regressions to improve tracking per
WWW 2009 MADRID!Track : Data Mining / Session : Click Models21 data ( clicks and views ) obtained from Today Module enable us to accurately measure article CTR at fairly fine temporal resolutions . In this paper , we estimate CTR aggregated at 5 minute time intervals .
Serving biased versus completely randomized data : We analyze data obtained from two buckets . A bucket represents a disjoint set of randomly selected users who are served content according to some scheme ( referred to as a serving scheme hereafter ) . Since our goal in this study is to serve the most popular , we consider a bucket ( estimated most popular , EMP hereafter ) that continues to show the best stories at the F1 F4 positions unless article popularity scores change with recent data . In addition , we create another bucket ( random bucket , RND hereafter ) that serves a randomly selected set of four distinct articles to each user visit . Previous studies of content optimization systems [ 15 , 16 ] only report findings based on data obtained from EMP bucket . In fact , the completely randomized nature of data obtained from RND helps us study subtle user content interaction patterns in an unbiased way ; conducting such analysis from EMP data alone requires adjusting for confounding factors ( eg repeat exposure , selection bias in displayed articles ) , which is difficult to carry out in practice . 2.1 Temporal Characteristics
Almost every article CTR shows significant temporal variation and different behavior in the two buckets . Figure 2 illustrates this for a randomly selected article . We observe a sharp F1 CTR decay in the EMP bucket ( discontinuities indicate periods when the article was taken out of F1 ) but a strong time of day , day of week pattern ( TOD hereafter ) in RND with almost no CTR decay ; the diurnal article CTR is several times higher than the nocturnal one ( relative to PST ) . In fact , TOD in RND and decay in EMP are observed for almost all articles .
Next , we explain TOD variations and decay through a comprehensive data analysis . For simplicity , we only analyze article CTR at the F1 position ; we discuss multiple positions later in this section .
Regression analysis : To parsimoniously estimate TOD variations in article CTR , we perform a regression analysis separately for each bucket . Let pitr denote the CTR of article i in time interval t with repeat exposure r , where r is the fraction of users in t who have previously seen article i at F1 . Through extensive offline analysis , we converged on the following CTR model that provides a good explanation of CTR dynamics . log pitr = θi + f ( t ) + g(r )
( 1 ) where θi ’s are main effects that represent the adjusted popularity of article i ; f ( t ) is a periodic ( period of one week ) adaptive regression spline function with the knots selected through the AIC criteria [ 9 ] , and g(r ) is a quadratic function that models article CTR decay . In fact , we plug in the estimate of f ( t ) from RND ( black curve in Figure 3 ) to obtain f ( t ) for EMP.1 The goal of this one time offline regression analysis is not to build an online model that tracks article CTR but to illustrate and estimate the TOD pattern , and provide an explanation of CTR decay as a function of repeat
1This provides a better fit . Model is fitted through a Poisson regression with CTR given by Equation ( 1 ) . It has 707 parameters and provides an excellent fit ( deviance ∼ 0 ) with few parameters .
Figure 1 : The Today Module on Yahoo! Front Page formance . ( c ) Adjust for user fatigue in the recommendation process by modeling CTR drop through repeat exposure features . Our modeling assumptions are validated via extensive exploratory data analysis . In fact , our analysis on completely randomized data provide new insights on various aspects of user content interaction . In particular , we show article CTR decay is explained better through amount of repeat exposure , instead of article lifetime as suggested by [ 15 ] ; article CTR at different positions are correlated but the correlation is article specific and evolves over time – the conventional approach of using article independent factors to translate CTR among positions can lead to inferior system performance ( which will be shown in Section 4 ) ; there is strong time of day and day of week effect in article CTR that can be largely attributed to the location of the user ( US versus International ) . To the best of our knowledge , a large scale study similar to ours on randomized data have not been reported for a web application before .
We note that although we present our models as enhanced methods of selecting popular stories , it is easy to extend our methods to perform personalization for user segments . In particular , article CTR from our models can be used as informative features in regression models ; one can also track article CTR separately in different user segments in order to provide segment specific popular stories . A detailed analysis and illustration of such extensions is beyond the scope of this paper ; we provide further discussion of this issue in section 5 . Our roadmap is as follows : In Section 2 , we report on an extensive exploratory data analysis that motivate our modeling in later sections . In Section 3 , we describe our spatio temporal modeling approach . Section 4 report on extensive experiments ( real and synthetic data ) to show the effectiveness of our methods . We end with a discussion and scope for future work in Section 5 .
2 . EXPLORATORY ANALYSIS OF CTR
We mention two key aspects of our data below . Large volume of click through data : The Yahoo! homepage gets hundreds of millions of daily user visits ; it is the most visited content page on the web . The Today Module is an important component of the homepage that helps in improving user engagement , often reflected through clicks on articles displayed on the Featured Tab . Large amount of
WWW 2009 MADRID!Track : Data Mining / Session : Click Models22 ( a ) CTR in the EMP Bucket
( b ) CTR in the Random Bucket
Figure 2 : Empirical F1 CTR of a random article in EMP and RD
Figure 3 : TOD pattern for overall traffic ( black solid curve ) vs . US only traffic ( red dashed curve )
Figure 5 : View volumes : US traffic ( red solid curve ) vs . international traffic ( blue dotted curve )
TOD . Figure 5 shows the scaled number of page views per hour over a week ( relative to PST ) for US and international users separately . Not surprisingly , the Yahoo! Front Page is mostly visited by US users during the day and international users during the night . We note that the CTR of each article for US users is several times higher than that of international users ( articles are programmed mainly for US audience ) , hence the nocturnal CTR are substantially lower . Re estimating f ( t ) via model in Equation ( 1 ) for US only visits ( red dashed curve in Figure 3 ) still shows a TOD pattern but with significantly smaller variations . The regression model does not provide a good fit to international visit data due to data sparseness ( low click and visit volumes ) .
Further analysis with user and article features failed to explain the TOD pattern in US traffic . In general , it is difficult to construct temporal features related to population changes that can explain away the temporal variations in article CTR ; capturing such latent population changes directly through a dynamic model that tracks article CTR ( instead of population changes ) is the approach we follow in this paper . However , sharp temporal changes in CTR require the dynamic models to adapt more , forcing it to use a small amount of recent data which may increase variance and deteriorate tracking performance . Hence , it is always useful to look for features that explain large temporal fluctuations in CTR . In fact , all our analysis in subsequent sections will be based on US only user visits ( we track international visits separately ) .
Explaining CTR decay through per user features :
Figure 4 : Decay as a function of repeat exposure exposure . The excellent fit of model in Equation ( 1 ) also supports a common TOD pattern and decay curve across articles .
Explaining decay pattern : Estimated decay ( ie , g(r ) ) in Figure 4 is largely due to the amount of repeat exposure and not the article lifetime as some earlier studies had indicated [ 15 ] . In fact , Figure 2(a ) shows an example where article CTR increases with lifetime when the article is put back at F1 after being taken out of EMP for a while . Indeed , the fraction of repeat exposure decreases when the article reappears and hence it gets a higher CTR , relative to when it was shown previously .
Explaining the TOD pattern : To explain estimated TOD pattern ( ig , f ( t ) ) in Figure 3 , we performed an extensive slice and dice analysis of the data that revealed traffic split by US versus international visits best explain
000408Scaled CTR16:3017:0017:3018:0018:3019:0019:3020:0020:3021:00000408Scaled CTR16:0017:0018:0019:0020:0021:0022:0023:0000:0001:0002:0003:0004:0005:0006:0007:0008:0009:0010:0011:0012:0013:0014:0015:00−04−020002Offset in log CTR0123456700010203040506−3−2−10Fraction of repeated usersOffset in log CTR00204060810Hourly Number of Views07/1507/1607/1707/1807/1907/2007/21WWW 2009 MADRID!Track : Data Mining / Session : Click Models23 ( a ) Previous clicks
( b ) Views since last click
Figure 7 : F1 to F2 CTR ratio during a day
Bucket EMP
Random
Table 1 : Percentage of clickers
F1 only F2 F4 only Both F1 & F2 F4
58 % 54 %
13 % 17 %
29 % 29 %
However , feedback received from footers provide additional source of information that is potentially useful in improving CTR estimates . In fact , it is ideal to have a joint model that learns from all positions to improve individual positional estimates . Prior work ( see [ 13 , 12 ] ) assume a model that translates article CTR among positions through a common article independent but position dependent ratio , ie , CT R(position x ) = τx,yCT R(position y ) , where τ ’s are unknown constants independent of articles . In this section , we study correlation among positional CTR for articles over time by using the RND data . We recall that for RND , each article is served at all positions , uniformly at random , in each time interval .
To study the correlation between F1 and Fx article CTR over time ( x = 2 , 3 , 4 ) , we first note that footer CTR are similar ( but significantly lower than the F1 CTR ) ; hence we only look at the relationship between F1 and F2 . In Figure 7 , each curve is the F1 to F2 CTR ratio of an article during its lifetime . As evident , articles have different ratios that change over time , clearly violating the assumption of an article independent ratio used in prior work . Next , Figure 8(a ) shows the F1 and F2 CTRs of a few randomly selected articles over time . Although there is a strong positive linear relationship for each article , there are substantial variations in the straight lines fitted to each individual articles ( both in slope and intercept ) . Figure 8(b ) shows the distribution of slope as function of average F1 CTR across articles . As evident , variation in slopes is not explained by F1 article CTR . ( Intercepts also show a similar behavior . ) In fact , none of the features we analyzed ( eg , article lifetime , time of day , article categories ) were able to explain the article specific F1 to F2 relationship .
We also note that there is a difference in users who click on F1 and footers . Defining a clicker to be a user who clicked the Today Module at least k times in a month , Table 1 shows the percentage of clickers who clicked on F1 only , clickers who clicked one of the footers only , clickers who clicked both , for k = 1 . ( Other values of k provided similar results . ) As seen , overlap in F1 and footer clickers is not large . 2.3 Discussion of Exploratory Analysis
Our exploratory analysis clearly shows that article CTR is dynamic . Although we believe the root cause of dynamic
( c ) Previous F1 Views
( d ) Time since 1st view
Figure 6 : Relative CTR as functions of different user activity features . ( relative to first view CTR )
Although aggregate repeat exposure explains CTR decay adequately , adjusting for user fatigue in the recommendation process requires a model that explains CTR decay through user level repeat exposure features . Prior exposure to articles may occur in several ways and with varying degrees of strength . We consider features that are based on number of previous article views at the story position , at the footer position , number of previous story clicks , number of views since last click and time since first view .
One expects previous clicks to be a strong exposure that should substantially reduce the user ’s propensity to click on the article again . However , this is not true since each article is composed of multiple links in our application ; users who interact with an article generally engage more by exploring multiple links . Figure 6(a ) shows variation in CTR with number of previous clicks , relative to the first view CTR ( probability of a click on first exposure to the article ) ; it shows an increased propensity of repeat click . Number of views since a user ’s last click , shown in Figure 6(b ) , provides a better explanation for decay , but is noisy .
Out of all the features we looked at , number of previous F1 article views , shown in Figure 6(c ) , is the best predictor of decay ; the ones based on time elapsed since first exposure of different types , eg , Figure 6(d ) , were noisy and weakly predictive after the first five minutes .
2.2 Positional Characteristics
Presentation bias affects article CTR , ie , the same article displayed at different positions under identical conditions will have different click rates . In our scenario , the F1 position is prominent and receives more attention than footers .
135790246810Number of previous clicksRelative CTR135790002040608Number of views since last clickRelative CTR135790002040608Number of previous F1 viewsRelative CTR135790002040608Number of previous F1 viewsRelative CTR01530456000051015Minutes since 1st F1 ViewRelative CTR468101214F1 CTR / F2 CTR05:0007:0009:0011:0013:0015:0017:0019:0021:0023:00WWW 2009 MADRID!Track : Data Mining / Session : Click Models24 exposure to article ) and g(Ru ) is a function that depends on the repeat view features Ru of user u . We note that θ0it is independent of the user u ; we adjust for user fatigue by an “ exponential tilt ” to the first view CTR through a function that only depends on the repeat exposure features . Thus , our modeling approach consists of : ( a ) Dynamic model to estimate θ0it for a fixed location over time . ( b ) Regression model to enhance the estimate of θ0it at the target location by combining information from correlated locations . ( c ) Estimating the function g(Ru ) to adjust for repeat exposure to article per user . We note that although we use the scoring function in Equation 2 to select the top k articles in a content recommendation system in this paper , the estimation technique is general and has wider applicability .
We now describe our approach to select the top k articles to be shown in the EMP bucket in the context of our application . We note that locations in our application are the positions at which articles are shown . Since one of the position ( F1 in our case ) is more important than others , we rank articles based on their predicted F1 CTR ( or some monotone transform ) in the next time interval . This is a reasonable strategy if F1 article CTR is independent of CTR of other articles that are shown at footer positions . This is indeed the case in our application as shown empirically in the Appendix . The presence of RND bucket ensures we obtain data on all live articles at all times ; thus we can always predict CTR of a live article in the next time interval . As discussed earlier , we build models to track first view article CTR and adjust for user fatigue through repeat exposure features to provide each user with user specific most popular articles .
In the rest of the paper , we describe candidate models for each component of our unified spatial temporal modeling approach that are motivated by our content optimization problem . More specifically , we address the following .
• Effective tracking of article CTR at a single position : As shown in Section 2 , temporal dynamics of article CTR is difficult to model through known features – models that quickly adapt to temporal changes are attractive . We note that articles that are in the explore mode ( either in RND or some other small bucket ) may receive lower views ( and clicks ) compared to those that are promising . This may imbalance the article view distribution over time ; our tracking model must be robust to such imbalance in sample size . We provide a Bayesian time series model based on a Gamma Poisson assumption ( validated through empirical analysis ) that is both accurate and robust .
• Improved tracking by combining information across positions : With small sample size , estimated article CTR at target position may improve substantially by combining information from other positions . However , variation of position correlations across articles and time makes this a challenging modeling task which , to the best of our knowledge , has not been addressed before . We provide a dynamic regression model to address this issue in Section 32
• Adjusting for user fatigue : As shown in Section 2 , article CTR decay with repeat exposure . Developing userspecific repeat exposure features that can appropriately down weigh article CTR per user is of paramount importance for good performance . In Section 3.3 , we
( a ) F1 vs . F2 over time
( b ) Regression slope
Figure 8 : Relationship between F1 and F2 CTR . In plot ( a ) , each curve starts with ( F2,F1 ) CTR at time 0 at one of the end points and shows the temporal relationship for the article over time . In plot ( b ) , slopes of linear regression fits for around 300 article curves are plotted against mean F1 article CTR .
CTR is user population change over time , it is often hard to identify features that capture such population changes in practice . Thus , the ability of tracking CTR over time at each position ( enhanced by combining estimates from different positions ) is useful for content optimization systems . We also point out the importance of using completely randomized data for understanding the dynamics of such systems in an unbiased way . Analysis based solely on non randomized serving data ( eg , data from the EMP bucket ) may be misleading . For example , Wu and Huberman [ 15 ] analyzed the serving data obtained from digg.com where they empirically show that popularity of an article decays quickly over time and modeled this decay subsequently using article lifetime ( time elapsed since first appearance ) . As in our application , we conjecture that the true reason for decay in their application is not lifetime , but the amount of repeat exposure . Also , since most articles in their context have short lifetime and they were not able to put an article at a prime position for long periods of time , they did not observe the TOD pattern we see in RND . Furthermore , they conducted a follow up analysis [ 16 ] comparing different serving schemes based on their previous model but using a global positional translation factor for each position ( ie , all stories have the same factor ) . Here again , a global translation factor may not be correct as we observe in our application . Finally , we note that some interesting temporal patterns were reported in the context of search in [ 10 ] .
3 . SPATIO TEMPORAL CTR MODELING We describe our spatio temporal approach to modeling CTR of a single article . We shall refer to the spatial coordinates as locations , each location represents an independent source of information that is correlated with the CTR at the target location . For instance , the target location may be the F1 position and the footers ( F2 F4 ) may consist of other correlated locations . We assume the CTR of article i when shown at location to user u at time t is given by θu,it . To account for user fatigue , we assume the following
θu,it = θ0it exp{g(Ru)}
( 2 ) where θ0it is the first view CTR ( probability of click on first
00501502502040608Scaled F2 CTRScaled F1 CTR0206−50510Mean item F1 CTRSlopeWWW 2009 MADRID!Track : Data Mining / Session : Click Models25 present a novel method that provides recommendation after adjusting for fatigue in a model based way .
Before we proceed further , a word about our notations . Since decay is modeled using features , we drop the user index . Throughout , θixt , cixt and vixt denotes the true article CTR , the number of clicks and the number of views ( respectively ) obtained by showing article i at location x in time interval t , for users who see the article for the first time . Some or all subscripts are dropped when not needed . Unless otherwise mentioned , CTR would always mean first view CTR in the rest of this section .
3.1 Dynamic Models for a Single Location
We begin with CTR tracking for a single article at a fixed location , and drop the article and location indices . At any given time interval t , we have a prior probability distribution for the CTR θt that is based on data until time interval t−1 ; this is combined with the observed clicks and views ( ct and vt respectively ) at time t , to obtain the posterior probability distribution of CTR θt at time t .
Gamma Poisson and Beta Binomial : For occurrence ( or count ) data , Beta Binomial and Gamma Poisson are attractive choices [ 4 ] . For a Beta Binomial model , the conditional distribution of clicks c given views v and CTR θ is a binomial with mean vθ , and the prior for θ is Beta(α,γ ) that has mean α/γ and variance α(γ−α ) γ2(1+γ ) . The posterior of θ is Beta(α + c , γ + v ) . For the Gamma Poisson model , the prior of θ is Gamma(α , γ ) with mean α/γ and variance α/γ2 , while the observation distribution of c given v and θ is Poisson with mean vθ ; the posterior of θ in this case is Gamma(α + c , γ + v ) . We work with the Gamma Poisson model throughout the paper . In fact , we validate the modeling assumption on our data in the Appendix .
Dynamic Gamma Poisson ( DGP ) model : We now describe our dynamic Bayesian model that is based on a Gamma Poisson assumption and fitted through the “ discounting ” concept pioneered by [ 11 ] . More specifically , consider obtaining CTR posterior θt at time t . Assume the posterior of θt−1 ( CTR at time t− 1 ) is Gamma(αt−1 , γt−1 ) with mean µt−1 and variance σ2 t−1 . Note that this posterior captures all information relevant to CTR based on observed data until time t − 1 . The conditional distribution of ct for known θt is Poisson(mean=vtθt ) . Now , to compute the posterior of θt , we need to decide on a prior for θt ; it is natural to use the posterior of θt−1 as the prior . However , such an approach gives equal weight to all previous data points and adapts slowly to CTR changes over time . The discounting concept solves this problem by using a dilated version of θt−1 as a prior ( dilation increases variance ; mean is unchanged. ) , ie , prior for θt is now Gamma(δαt−1 , δγt−1 ) t−1/δ , 0 < δ ≤ 1 . The posterior of θt is with variance σ2 given by Gamma(αt = δαt−1 + ct , γt = δγt−1 + vt ) , where δ determines the rate of adaptation of the tracker ; it is estimated using a tuning dataset . Note that small values of δ adapt faster and may lead to high variance ; large values of δ adapt slowly and may lead to high bias in predictions . In our application , since the TOD pattern for US traffic is not too sharp , values of δ in the range of [ .95 , 1 ) worked well .
This model is simple to implement . Starting with α0 and γ0 as our initial prior parameters ( which can be estimated based on historical data analysis ) , for each interval t , the update formula for ( αt , γt ) is given by
αt = δαt−1 + ct γt = δγt−1 + vt
( 3 ) where vt = ct = 0 if the article is not shown in some interval t . The predicted CTR for the next interval has mean αt/γt and variance αt/γ2 In fact , the posterior mean αt/γt is t . the predicted CTR for next interval t + 1 . A closer look k=0 ct−kδk + δtα0 and at Equation 3 shows that αt = k=0 vt−kδk + δtβ0 ; thus the posterior mean is a ratio βt = of exponentially weighted sums ( clicks and views ) . t−1 t−1
Alternative models : Our DGP model is different from a commonly used model that tracks CTR through a exponentially weighted sum of per interval click to view ratio ( ct/vt ) ; DGP smooths the clicks and views separately instead and provides a more robust model , especially for imbalanced data . We note that the above update rule can also be applied to the Beta Binomial model , but the variance dilation would be approximate , not exact . In prior work [ 1 ] , we applied the Gaussian Kalman filter with logit transformation to dynamic CTR tracking . This model is sensitive to noisy or sparse data . Kalman filters can also be defined based on more appropriate distributions , eg , Poisson observations with Gamma states . However , the update formula is not closed form and our update rule can be thought of as an approximation to that . 3.2 Spatial Model
Our exploratory analysis in Section 2 clearly shows strong article specific spatial ( more specifically , positional in this case ) correlation that evolves through time . With small sample size , estimated article CTR at target location may improve substantially by combining information from other correlated locations . We propose a dynamic linear regression model ( DLR ) to accomplish this .
Dynamic Linear Regression ( DLR ) : Let θxt denote the CTR of an article at location x in time interval t , for x = 1 , , K . Without loss of generality , assume our goal is to estimate the CTR θ1t at the target location by using data from all correlated locations θxt . Let Dxt denote aggregated clicks ( cx1 , , cx,t−1 ) and views ( vx1 , , vx,t−1 ) for the article at x observed before time t . Our goal is to estimate the posterior distribution of θ1t combining information from D1t , , DKt ,
The basic idea of dynamic linear regression ( DLR ) model is to use a base model ( eg , the Gamma Poisson model ) to separately track the article CTR θxt for each location x and obtain additional information for the target from location x through a dynamic linear regression ( fitted using a Kalman filter ) of the target θ1t on the location θxt . Finally , information from all locations are combined to obtain a more informed posterior for the target .
Motivated by our exploratory analysis shown in Figure 8(a ) , we model the relationship between θ1t and θxt by a linear regression . Specifically , we assume the following linear model :
θ1t = αxt + βxt θxt + xt , xt ∼ N ( 0 , s2 x ) ,
( 4 ) where αxt and βxt are time varying parameters of the linear model , and xt is the error term with variance s2 x . To obtain information on θ1t from location x , we first use a base model to track the distribution of ( θxt | Dxt ) for each location x separately . Each base model outputs the CTR mean ˆθxt and variance s2 xt . Then , for a location x , we apply
WWW 2009 MADRID!Track : Data Mining / Session : Click Models26 the above linear model to predict θ1t using θxt , this gives the distribution of ( θ1t | Dxt ) . It can be shown that ( θ1t | Dxt ) has the following mean and variance :
µxt = E[θ1t | Dxt ] = ˆαxt + ˆβxt ˆθxt xt = V[θ1t | Dxt ] = σ2
V[αxt ] + V[βxt]s2 2C[αxt , βxt]ˆθxt + s2 x xt + ˆβ2 xts2 xt + ˆθ2 xtV[βxt]+
Finally , we combine each individual ( θ1t | Dxt ) through the following standard proposition[6 ] .
Proposition 1 . Assume θ1t has an uninformative prior . xt ) , for x = 1 , , K , and D1 , ,
If ( θ1t | Dxt ) ∼ N ( µxt , σ2 DK are mutually independent given θ1t , then ( θ1t | D1t , , DKt ) is normally distributed with
E[θ1t | D1t , , DKt ] = V[θ1t | D1t , , DKt ] =
1 x(1/σ2 xt)µxt x(1/σ2 xt ) x(1/σ2 xt )
The revised mean and variance formulae are used to get an updated CTR posterior for the base model through method of moments . We now discuss estimation of time varying regression parameters in Equation 4 through a Kalman filter that uses output of the base model . Specifically , for each position x , we use the following state space model :
( cid:183 )
( cid:184 )
( cid:183 )
ˆθ1t = αxt + ˆθxtβxt + xt ,
αxt βxt
=
αx,t−1 βx,t−1
+ ,
( cid:184 ) xt ∼ N ( 0 , s2 ∼ N ( 0 , Σ ) xs2
1t + β2 xts2 xt )
1t and s2 x is the model error variance ; s2 where s2 xt are the variances of ( θ1t|D1t ) and ( θxt|Dxt ) that are outputs from the base models . In fact , the variance formula for xt above can be derived using the formula of iterated variance calculation : V ar(ˆθ1t ) = Eθ2t ( V ar(ˆθ1t|θ2t ) ) + V arθ2t ( E(ˆθ1t|θ2t) ) . Note that we assume V ar(ˆθ1t|θ2t ) = s2 1t ; this incorporates the uncertainty involved in θ1t . αx,t−1 and βx,t−1 are the regression parameters from the previous interval ; and Σ is the variance covariance matrix controlling how the regression relationship evolves over time . Note that s2 x and Σ are article independent constants that are estimated using historical data in an offline manner . xs2
We apply the standard Kalman filter update rule [ 11 ] to track αxt and βxt over time . The Kalman filter outputs ˆαxt , ˆβxt , V[αxt ] , V[βxt ] and C[αxt , βxt ] are used in computing µxt and σ2 xt in Proposition 1 .
Alternate Models : Our spatial modeling approach is simple , scalable and is motivated by methods used in metaanalysis[7 ] that improve estimate of a quantity by combining information available from different independent sources . An alternate approach would be to use a multivariate Kalman filter that assumes CTR from different locations are correlated and evolves over time . One such model would be assuming that clicks are generated according to Poisson and the state ( a vector of the true CTR ’s of L locations ) evolve over time based on a L × L covariance matrix , which is difficult to estimate when L is large . 3.3 User Fatigue Model
As shown in Section 2 , article CTR decays with repeat exposure . Developing user specific repeat exposure features that can appropriately down weight article CTR per user is of paramount importance for good performance . In fact , Figure 6(c ) clearly shows an exponential drop off in overall
CTR with increase in amount of repeat exposure . To model user fatigue , it is important to incorporate exposure features per user while estimating overall CTR ; especially in EMP . We present a novel dynamic model to incorporate fatigue that provides good performance in our application .
Let θRt denote article CTR at time t corresponding to repeat exposure feature vector R . We assume the following factorization : θRt = θ0t exp{Rbt} , where θ0t is the first view CTR at time t ( probability of click on first article exposure ) . Comparing this with Equation ( 2 ) , we see this is a special case of our general spatial temporal approach with g(Ru ) = R If cRt and vRt denote clicks and views at time t corresponding to feature vector R , then the distribution of our observations conditional on the state at time t is given by bt . cRt | θ0t , bt ∼ Poisson(vRt θ0t exp{R bt} )
( 5 )
Indeed , Equation ( 5 ) provides the likelihood of the state vector ( θ0t , bt ) at time t . We assume θ0t is known and equals the posterior mean obtained as ouput of our spatial model or Dynamic Gamma Poisson model at a single position . We now focus on updating the posterior of bt . The prior for bt is assumed to be Gaussian with mean equal to the posterior mean ˆbt−1 of bt−1 , and variance obtained by dilating the posterior variance At−1 of bt−1 to At−1/δ . Combining the likelihood in Equation ( 5 ) with the prior provides the posterior of bt by Bayes theorem . Since the posterior is not available in closed form , we apply Laplace approximation to obtain a Gaussian posterior . In fact , the posterior is approximately Gaussian with mean given by the mode ˆbt of log posterior and variance At given by inverse of negative hessian computed at mode of log posterior . 4 . EXPERIMENTAL RESULTS
In this section , we present results on experiments with both real and simulated data . First , we show that the dynamic Gamma Poisson model that tracks article CTR at a single position significantly outperforms several alternative models . We then show that our spatial DLR model significantly improves CTR tracking when the target position suffers from data sparsity . In fact , we find methods that use article independent translation ratios to estimate positional effects may hurt performance in our application . Finally , we report analysis of our fatigue model . 4.1 Replay Experiments
We first report experimental results on “ replay ” experiments that are based on log data collected from the Today Module . In fact , we use US traffic data from RND bucket aggregated at five minute intervals collected over a month , which contains about 180 million user visits . Parameters of all models are estimated using training set ( first 15 days ) and results are reported on the test set ( last 15 days ) . We subsampled our test set to simulate performance for different traffic volumes .
Replay methodology : To evaluate the performance of an online model , we “ replay ” articles retrospectively using the model predicted F1 CTR at the next time interval , and aggregate the total number of clicks that is received by the highest ranked articles over a 15 day test period . We only report the F1 position performance since it accounts for the majority of click traffic and hence a good approximation to the system performance ( results for other positions were
WWW 2009 MADRID!Track : Data Mining / Session : Click Models27 cles . In fact , we see that using data from all positions may be worse than using data from a single position if the model does not track non stationary correlations properly .
Why DGP F1 is good ? In the RND data used by our replay experiments , each article receives roughly the same volume of traffic at each position . Since footers receive much fewer clicks than F1 , the noise in data from footers is higher than that from F1 . It turns out that after having the less noisy F1 traffic for every article at all times , it is difficult to squeeze additional information from the weaker F2 F4 signal even if the model that predicts F1 CTR using correlation from F2 F4 is accurate . In fact , when the traffic volume of the target position is sufficiently large at all time , there is no need to use correlation among multiple positions .
However , in many scenarios , it is not feasible to have a constant stream of completely randomized data for each article at all times . When the pool of live articles is large , we may only be able to explore a small fraction of articles at a time . For some web sites , human editors may want to have full control over some important positions for some periods of time . Also , to fully optimize the CTR of a system , complete randomization is not an optimal exploration scheme . Thus , we conduct a series of simulation experiments to understand how multi positional models would perform with different traffic patterns .
4.2 Simulation Study
In this section , we investigate the behavior of DLR with different traffic patterns . We apply the loess method described in the Appendix to smooth the CTR time series of each article at each position using the RND data , and treat each smoothed CTR curve as the true CTR θixt of article i at position x in interval t . A simulation run is similar to a replay run . However , in each interval , instead of using the observed clicks and views , we control the number of views vixt to be generated and simulate the number of clicks according to P oisson(θixt vixt ) . Since we know the true CTRs , we can measure the predictive error exactly . Also , because only DLR is competitive to the single positional DGP F1 , we only report results for these two models .
Constant F1 traffic : We first simulate a scenario in which the F2 F4 traffic is constant ( with 100 views per article , per position , per interval ) , and the F1 traffic is also constant but may be relatively small . We vary the ratio between the F1 traffic volume and F2 F4 traffic volume , and show the mean absolute errors of DLR and DGP F1 in Figure 10(a ) . As seen from the figure , DLR outperforms DGP F1 when the F1 traffic is less than 10 % of F2 F4 traffic . Larger than that , there is almost no difference in performance .
Periodic F1 traffic : Next , we simulate scenarios in which the F2 F4 traffic is still constant ( with the same volume as before ) , but the F1 traffic is observed only periodically . We vary the fraction of time intervals in which F1 data is observed , and show the mean absolute errors of DLR and DGP F1 in Figure 10(b ) . As seen from the figure , DLR outperforms DGP F1 when the F1 observed fraction is less than 30 % .
Simulation with the UCB1 serving scheme : Finally , we try to understand the performance of DLR in a system that seeks to maximize CTR by allocating different amounts of exploration traffic to articles ( instead of using a small fixed size random bucket with uniform randomization ) . We implemented the UCB1 scheme [ 2 ] and use it to control the
( a ) Replay
( b ) UCB1 simulation
Figure 9 : Model performance in the replay experiment and simulation based on UCB1 serving scheme qualitatively similar ) . The target position here is the F1 position .
Models : We report results for a range of models . DLR is the dynamic linear regression model described in Section 3.2 that tracks F1 CTR using data from all positions . Several variations of the dynamic Gamma Poission models ( DGP ) are tested : DGP F1 uses data only from F1 ; DGPNaive assumes all positions are exchangeable and aggregates clicks and views from all positions ; DGP Ratio aggregates across positions but downweighs footer views by a constant ratio ( overall F1 to footer CTR ratio ) . We also fitted two Gaussian Kalman filter ( GKF ) models to the empirical CTR and a logistic transformation of the empirical CTR . GKF F1 is a univariate Gaussian model that only uses the F1 data , while GKF Multi is a standard multivariate Gaussian Kalman filter model . We also consider two other baseline models : EWMA F1 ( exponential weighted moving average ) that tracks the F1 CTR θ1t for an article by using exponentially weighted moving average of empirical CTR ratios : θ1t = wθ1,t−1 + ( 1 − w)(ct/vt ) where , ct and vt are the observed F1 clicks and views at t , and w is tuned using the training data . Cumu F1 ( cumulative F1 CTR ) simply divides the total number of F1 clicks by F1 views observed up to a given time point . In fact , this is a special case of DGP obtained by setting discounting factor to unity .
Experimental Results : Figure 9(a ) shows the results . For CTR tracking using data from a single positon , DGP F1 significantly outperforms alternatives GKF F1 and EWMAF1 , that use instant CTR ( ct/vt ) in model updates . For CTR tracking using multi positional data , DLR performs significantly better than the alternatives : GKF Multivar , DGP Ratio and DGP Naive . To our surprise , the singlepositional DGP F1 is hard to beat , only DLR can provide a slight improvement over it . The other models that are based on a fixed positional correlation assumption have inferior performance compared to DGP F1 since the positional correlations are not constant and vary across time and arti
80859095100Avg Views Per IntervalPercentage Lift in F1 CTR2505001000150025005000 DLRDGP−F1GKF−MultiCumu−F1DGP−RatioDGP−NaiveGKF−F1EWMA5060708090Avg Views Per Interval2505001000150025005000 DLRDGP−F1WWW 2009 MADRID!Track : Data Mining / Session : Click Models28 ( a ) Constant F1 traffic
( b ) Periodic F1 traffic
Figure 10 : Behavior of DLR with different traffic patterns ( x axis is log scaled ) amount of F1 exploration traffic to be given to each article , while still keeping F2 F4 traffic the same as the RND data . Figure 9(b ) shows the CTR lifts of DLR and DGP F1 over a model that predicts article CTR randomly for different traffic volumes . As seen , DLR provides better lifts especially when the traffic volume is small . This result is encouraging , but future work is needed to understand the interaction between DLR and an exploration scheme . 4.3 User Fatigue Analysis
In this section , we provide results to show the effectiveness of our user fatigue model discussed in Section 33 In particular , we show the predictive accuracy measured in terms of mean absolution error ( MAE ) and predictive log likelihood for models with and without previous user exposure features . After conducting preliminary exploratory analysis , we converged on the following repeat exposure feature vector R = ( R1 , R2 ) where , R1 is number of previous F1 article views and R2 is number of previous story views through footer clicks on Featured tab or other tab click or other tab click followed by a footer click . Other features ( number of previous footer exposures , time since first exposures of various types ) were noisy and did not look promising . We tested the following models : m0 : Null model which assumes no repeat exposure features . m1 : which assumes R = R1 , m2 : which assumes R = ( R1 , R2 ) and m3 : which assumes R = ( R1 , R2 , R1R2 ) . m3 does not converge on a number of articles for which it provided poor performance , hence we only report results for m0 , m1 and m2 . Define M1 = M AE(m1)/M AE(m0 ) and M2 = M AE(m2)/M AE(m0 ) . Figure 11 shows the plot of MAE ratio of M2/M1 ( representing error reduction of m2 over m1 ) versus M1 ( representing error reduction of m1 over m0 ) . First , both m1 and m2 are significantly better than the null model m0 on all articles , m2 is marginally better than m1 for a large number of articles . Results for predictive log likelihood showed similar patterns .
5 . DISCUSSION
We have discussed dynamic models for estimating CTR of articles that incorporate information from correlated locations and decay caused by repeated exposure . Although we demonstrated the utility of the models using a content recommendation application based on overall popularity , the models can also be used to perform personalized recommen
Figure 11 : Mean Absolute Error ( MAE ) of user fatigue models . dation . One approach to personalization is to create user segments based on analysis on a large amount of historical data ( using clustering or other techniques ) , track article CTR for each segment separately and serve the segmentspecific most popular articles . This is a reasonable solution as our experiments in Section 4 showed , spatial modeling performs well even with extremely sparse data . Another approach is to build personalization models ( eg , regression models ) that are based on both user features and dynamic article CTR estimates obtained from our models to combine relevance and popularity . In our preliminary study , those dynamic CTR features are the most important features in such models .
Our models are motivated by rigorous exploratory analysis . All the modeling assumptions have been verified . Experimental results confirmed good performance of our models .
6 . REFERENCES [ 1 ] D . Agarwal , B C Chen , P . Elango , and et al . Online models for content optimization . In NIPS , 2008 .
[ 2 ] P . Auer , N . Cesa Bianchi , and P . Fischer . Finite time analysis of the multiarmed bandit problem . Machine Learning , 2002 .
[ 3 ] Y . Benjamini and Y . Hochberg . Controlling the false discovery rate : a practical and powerful approach to multiple testing . Journal of the Royal Statistical Society B , 1995 .
[ 4 ] CAColin and PKTrivedi Regression Analysis of
Count Data . Cambridge University Press , 1998 .
[ 5 ] J . M . Chambers . Software for Data Analysis :
Programming with R . Springer , 2008 .
[ 6 ] CRRao Linear Statistical Inference and Its
Applications . Wiley , 2002 .
[ 7 ] DKStangl and DABerry Meta analysis in Medicine and Health Policy . CRC Press , 2000 .
[ 8 ] D.Lambert and CLiu Adaptive thresholds :
Monitoring streams of network counts . Journal of the American Statistical Association , 2006 .
[ 9 ] C . Kaufman , V . Ventura , and R . Kass . Spline based non parametric regression for periodic functions and its application to directional tuning of neurons . Statistics in Medicine , 2005 .
[ 10 ] Q . Mei and K . W . Church . Entropy of search logs : how hard is search ? with personalization ? with backoff ? In WSDM , 2008 .
00100501005010050010000002000600100014F1−to−others traffic ratioMean absolute errorDGP−F1DLR002005010020050000300050007Fraction of time with F1 dataMean absolute errorDGP−F1DLR0204060810090094098M1M2/M1WWW 2009 MADRID!Track : Data Mining / Session : Click Models29 [ 11 ] M.West and JHarrison Bayesian Forecasting and
Dynamic Models . Springer Verlag , 1997 .
[ 12 ] F . Radlinski and T . Joachims . Active exploration for learning rankings from clickthrough data . In KDD , 2007 .
[ 13 ] M . Richardson , E . Dominowska , and R . Ragno .
Predicting clicks : estimating the click through rate for new ads . In WWW , 2007 .
[ 14 ] SPEllner and YSeifu Using spatial statistics to select model complexity . Journal of Computational and Graphical Statistics , 2002 .
[ 15 ] F . Wu and B . A . Huberman . Novelty and collective attention . In Proceedings of National Academy of Sciences , 2007 .
[ 16 ] F . Wu and B . A . Huberman . Popularity , novelty and attention . In EC . ACM , 2008 .
APPENDIX Effect of Concurrent articles : Ranking of articles at different slots depends on correlation in article CTR that are shown concurrently to users . If CTR of an article is influenced by other concurrent articles in the content pool , it is important to adjust for such effects in ranking articles . In general , it is hard to conduct such correlation studies from serving bucket data ; the availability of random bucket data in our scenario makes this study feasible .
To measure such correlations , we compare the observed number of clicks cij for article i displayed at F1 when shown concurrently with article j shown at Fx ( x= 2 , 3 , 4 ) with baseline Eij , expected number of clicks under the null hypothesis of no correlation . Large deviations of cij ’s from the corresponding Eij ’s would be indicative of correlation .
If h Nij,h
Computing baseline frequency Eij : To avoid data sparsity , we only considered pairs that were shown together at least 1000 times during their lifetimes . To adjust for temporal variations , we partitioned the data into several 2 hour ˆCT Ri,h denote the empirical CTR of article windows . i in time window h and Nij,h is the number of joint occurences of article i at F1 with article j at Fx in time winˆCT Ri,h . We then compute p values dow h , Eij = ( Pij,low,Pij,high ) that measures extremeness of cij under a Poisson model with mean Eij . In particular , Pij,low = P r(X ≤ cij ) ; Pij,high = P r(X > cij ) , where X follows P oisson(Eij ) . Low values of Pij,low(Pij,high ) are indicative of strong negative(positive ) correlations . Figure 5 shows the quantile quantile plot of normal scores of the p values against a standard normal distribution . The close agreement indicates no significant correlations in our data . We also conducted formal hypothesis testing by adjusting the p values for multiple testing using the FDR procedure([3] ) . The test flagged only 3 pairs as significant ( out of a total of about 26K ) at 5 % error rate . This further shows there is no need to incorporate the effect of other articles while selecting the set of top 4 , perhaps the editors are already ensuring a diverse pool of articles through their programming .
Validating Gamma Poisson Assumption To validate the Gamma Poisson assumption empirically , we consider click and view time series of all articles appearing in a month in RND . Our method is motivated by techniques proposed in [ 8 ] . Recall the Gamma Poisson model : if the CTR or state θt at each time interval is known , then clicks ct ∼ P oisson(vtθt ) and ct ’s are independent of each other . Of
Figure 12 : Q Q plot of normal scores for Pij,high . Similar patterns observed for normal scores of Pij,low
( a ) Poisson Q Q
( b ) Gamma Q Q
Figure 13 : Validating Gamma Poisson assumption course the states are unknown and estimated from data , which introduces uncertainty modeled through a Gamma distribution . We first fit a non parametric curve to the time series using loess ( local polynomial regression ) available in software R [ 5 ] ; the neighborhood size ( span ) is selected such that the autocorrelations in the residuals is close to zero as suggested in [ 8 ] . In fact , this ensures the assumption of independence of clicks in different intervals is approximately true . The autocorrelation measure we use is based on Moran I statistic as recommended in [ 14 ] .
Assuming the estimated loess curve ˜θt for an article in time t is the truth , we draw observations from P oisson(˜θtvt ) . Figure 13(a ) shows a quantile quantile plot of observed clicks versus simulated clicks from fitted Poisson distribution . The close agreement among the quantiles clearly shows the Poisson assumption is reasonable .
To validate the Gamma assumption , we assume CTR of an article in a given hour changes very slowly and can be assumed to be almost a constant . We fit a Gamma distribution to each of the 12 loess fitted article CTR in a given hour by matching moments and draw 12 samples from the fitted Gamma distribution . The quantile quantile plot of estimated loess CTR ’s and Gamma samples shown in figure 13(b ) shows the Gamma assumption is reasonable for our data . ( Similar patterns observed over large number of simulations . ) This one time offline analysis clearly shows the Gamma Poisson assumption is appropriate for tracking article CTR in our data . A similar analysis showed BetaBinomial is also reasonable but slightly worse than GammaPoisson .
−4−2024−6−2024Standard normalnormal scores0408012004080120Obs ClicksExpected clicks005015005015True CTRExpected CTRWWW 2009 MADRID!Track : Data Mining / Session : Click Models30
