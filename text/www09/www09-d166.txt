Web Scale Classification with Naive Bayes
Congle Zhang , Gui Rong Xue , Yong Yu
Shanghai Jiao Tong University
Shanghai , 200240 , China {zhangcongle , grxue , yyu}@apexsjtueducn
ABSTRACT Traditional Naive Bayes Classifier performs miserably on web scale taxonomies . In this paper , we investigate the reasons behind such bad performance . We discover that the low performance are not completely caused by the intrinsic limitations of Naive Bayes , but mainly comes from two largely ignored problems : contradiction pair problem and discriminative evidence cancelation problem . We propose modifications that can alleviate the two problems while preserving the advantages of Naive Bayes . The experimental results show our modified Naive Bayes can significantly improve the performance on real web scale taxonomies .
Categories and Subject Descriptors : Intelligence ] : Learning
I26 [ Artificial
General Terms : Algorithms , Experimentation
Keywords : Naive Bayes , Web scale Taxonomy
1 .
INTRODUCTION
Text classification is widely used in information retrieval area and text mining area , especially in the web environment . Web scale taxonomy classification entails high efficiency of the classifier , and Naive Bayes Classifier ( NBC ) naturally lends itself to such tasks because it is simple , fast , easy to implement and relatively effective . NBC has been studied for a long time in small datasets[2 ] . Unfortunately , when employing NBC on tasks with thousands classes , we found that it achieves extremely bad performance which we will discuss in detail later . One may think this poor performance must come from some intrinsic limitations of NBC ( say , the independent assumption ) . And this gives the motivation in some works to try to use extra information ( eg , the hierarchy structure of the taxonomy)[3 ] or other complicated classification algorithms to handle the case of a large number of classes [ 1 ] .
In this paper , we aim to maintain the advantage of NBC ( eg simple algorithm , easy implementation and fast computation ) , and to achieve good performance at the same time . We discover that two largely ignored problems of NBC can severely hurt its classification performance . We call them contradiction pair problem and discriminative evidence cancelation problem , which will be discussed in Section 2 . These two problems have the characteristic that they are
Copyright is held by the author/owner(s ) . WWW 2009 , April 20–24 , 2009 , Madrid , Spain . ACM 978 1 60558 487 4/09/04 .
Hongyuan Zha
College of Computing Georgia Institute of
Technology
Atlanta , GA 30332 zha@ccgatechedu rather benign for small number of classes but manifest themselves notably in the presence of a large number of classes . To deal with the two problems , we propose two different modifications on NBC : Weight Manipulation Naive Bayes and Parametric Smoothing Naive Bayes . Our modified NBC can improve the accuracy from 9 % to about 50 % for a 1000class case , and from 20 % to about 70 % for a 200 class case , while maintaining all the advantages of NBC . Moreover , each of them is designed to fix the two problems without making NBC much slower or significantly more difficult to implement .
2 . TWO PROBLEMS OF NAIVE BAYES
For word wi and class c , standard NBC based on multinomial model and Laplacian smoothing yields maximum likelihood estimation of the class conditional probability pml(wi|c ) = c/Nc , and smoothed estimate ps(wi|c ) = N i N i Nc+V . To classify a test document d , NBC assigns d by the label .(d ) = arg maxc{log p(c ) + c is the frequency count of the word wi appearing in training set of class c , Nc = c , fi is the occurrence of wi in the test document d . wi∈d fi log p(wi|c)} . Here N i
.
. i N i c+1
The first problem is what we call the contradiction pair problem . It means , for a word and the classes , the smoothed estimates do not preserve the order of the maximum likelihood estimates ( MLE ) , ie for a word w and two classes c1 , c2 , we may have MLEs with pml(w|c1 ) > pml(w|c2 ) but the smoothed estimates with ps(w|c2 ) < ps(w|c2 ) . Figure 2 provides an illustration of this issue . Since NBC relies on smoothed estimates to make class predictions , the consequence of contradiction pair problem is that NBC loses some evidences carried by the maximum likelihood estimates .
Maximum Likelihood Estimation pml(w|c5 ) pml(w|c2 ) pml(w|c1 ) pml(w|c9 ) e g r a L l l a m S
Smoothed Estimation ps(w|c3 ) ps(w|c1 ) ps(w|c2 ) ps(w|c7 ) e g r a L l l a m S
Smoothing
Contradition
Pair
Figure 1 : Illustration of contradiction pair problem .
We call the second problem discriminative evidence cancelation problem which can be illustrated using a simple example in Figure 2 . Assume a document with three words d = {w1 , w2 , w3} and class c is one of the candidate classes
WWW 2009 MADRID!Poster Sessions : Wednesday , April 22 , 20091083 Naïve Bayes
Training
Naïve Bayes
Testing ps(w1|c(cid:397))=0.1 ps(w2|c(cid:397))=0.1 ps(w3|c(cid:397))=0.1 and d={w1,w2,w3} p(d|c(cid:397 ) ) =1.0e 3 p(d|c ) =0.24e 3 ps(w1|c)=0.49 ps(w2|c)=0.49 ps(w3|c)=0.001 ( pml(w3|c)=0 )
Discriminative
Evidence p(c|d ) < p(c(cid:397)|d ) c(cid:397 ) beats c fi
Figure 2 : Illustration of discriminative evidence cancelation problem where we assumed p(c ) = p(c ) . Besides , there is p(c|d ) = p(d|c)p(c)/p(d ) to label d , suppose {(w1 , c ) , ( w2 , c)} are discriminative evidence . The smoothed estimation ps(wi|c ) is discriminative evidence when ps(w1|c ) is far larger than the average of ps(w1|cj ) , ie ps(w1|c ) >> Avgj{ps(w1|cj)} . We assume the same is true for ps(w2|c ) . In this case , for d , it is natufi ral to prefer c over c ) is discriminative evifi dence for class c . Unfortunately , standard NBC does not do this : when p(w3|c ) is very small , class c will not be able to fi compete against c ) are only moderately large . That is to say , the effects of discriminative evidences are overwhelmed by low probability estimates of other words in NBC . unless ps(w3|c fi if ps(w1|c fi
) , ps(w2|c fi
) , ps(w3|c fi
Contradiction pair problem and discriminative evidence cancelation problem manifest themselves notably in the presence of a large number of classes . It is because : ( i)More classes mean more chances of contradiction pairs . Average to every prediction , the number of possible contradiction pairs is linear to the number of class . ( ii ) there is a large amount of zero frequency in word class frequency table . They may get small and unreliable values in smoothing and then easily overwhelm those discriminative evidence . ( iii)the values of Nc may vary more greatly , which largely randomize the order of ps(w|cj ) . The above claims can be verified by some careful calculations .
3 . MODIFICATIONS
We propose two different modifications in this section . The first is Weight Manipulation Naive Bayes ( WMNB ) . We regard estimations p(wi|c ) as the weight instead of probabili p(wi|c ) = ity , which means they are free from the constraint 1 . For word wi and cu , we use zi u to denote the weight . Our idea is that we could directly use maximum likelihood estimations as the weight for non zero frequencies of wi in cu , and use moderate small weight for zero frequencies . The modified training process of Naive Bayes is presented as :
. fi i u = z
. log pml(wi|cu ) = log N i γ/ j:N j u=0 1 u − log Nu . u .= 0 if N i otherwise
( 1 ) j:N j
. u} . wi∈d fi · zi
WMNB can alleviate contradiction pair problem : where γ < 0 is a constant value , u=0 1 counts the number of zero freqency word for class c . To classify d = {f1 , f2 . . . fV } , the modified testing process of Naive Bayes is : .(d ) = arg maxu{log p(cu ) + if we have pml(wi|cu ) − pml(wi|cv ) > 0 , there are two possibilities : ( i ) pml(wi|cu ) > pml(wi|cv ) > 0 : then we have zi u − v = log pml(wi|cu ) − log pml(wi|cv ) > 0 ( ii ) pml(wi|cu ) > zi pml(wi|cv ) = 0 : then since γ is the constant value to turn , we can always take one small enough γ that meets zi v . For discriminative evidence cancelation problem , WMNB can alleviate it by controlling the sum of those small weights . u > zi
We call the second modification Parametric Smoothing
Naive Bayes . It tries to modify traditional smoothing methods to alleviate the two problems in this paper . Besides Laplacian method employed by standard Naive Bayes , we can also borrow other smoothing methods from language model . We modify four well known smoothing methods and lead to four variants : PNBC Laplace , PNBC Absolute , PNBCLinear and PNBC WittenBell . The basic idea is that NBC should be able to control the extent of reducing the probability of non zero frequency words . For example , in PNBCLaplace , we take ps(wi|c ) = w α , with a moderate samll α decided by cross validation , the smoothed estimations are close to maximum likelihood estimations and avoid discriminative evidence cancelation problem as well . Table 1 : Performance Comparison on three datasets
. N i c+α Nc+
Standard NB
WMNB
PNBC Laplace PNBC Absolute
PNBC Linear
PNBC Wittenbell
20 NG ODP YahooQA 0.8771 0.9151 0.9136 0.9109 0.9120 0.9104
0.0946 0.4915 0.4925 0.4986 0.4940 0.4987
0.3637 0.5241 0.5069 0.5023 0.5031 0.5023
4 . EVALUATION
Table 1 shows the overall performance of WMNB and the variants of PNBC on a 1,048 classes and 185,728 documents Open Directory Project(ODP ) dataset ; 505 classes and 1,910,741 documents Yahoo Question Answer ( YahooQA ) dataset ; and 20 Newgroups as well . We use standard NBC as the comparisons . It can be seen clearly that our modified Naive Bayes results in remarkable improvements on ODP and YahooQA by alleviating the contradiction pair problem and discriminative evidence cancelation problem . Moreover , the performances of our modified algorithms are similar to each other . In other words , not a specific kind of variants , but their shared underlying principles proposed before count . Figure 3 plots the performance curve for standard NBC , WMNB and PNBC Laplace on YahooQA and ODP datasets with different number of classes . We can see that WMNB and PNBC Laplace beat Standard NBC at every points . These evaluations proves our modified Naive Bayes algorithms are effective on web scale taxonomies . y c a r u c c A
0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0
10
WMNB PNBC LAPLACE Standard NBC
30
20 90 Percentage of the classes in class subset ( % )
40
50
60
70
80
( a ) ODP
0.9
0.8
0.7
0.6
0.5
0.4
0.3
100
10
WMNB PNBC LAPLACE Standard NBC
70
60
50
40
30
20 90 Percentage of the classes in class subset( % ) ( b ) YahooQA
80
100
Figure 3 : Standard NB , WMNB and PNBC Laplace with different number of classes 5 . REFERENCES
[ 1 ] L . Cai and T . Hofmann . Hierarchical document categorization with support vector machines . In CIKM 2004 , pages 78 87 , 2004 .
[ 2 ] J . Rennie , L . Shih , J . Teevan , and D . Karger . Tackling the poor assumptions of naive bayes text classifiers . In ICML 2003 , pages 285 295 , 2003 .
[ 3 ] G R Xue , D . Xing , Q . Yang , and Y . Yu . Deep classification in large scale text hierarchies . In SIGIR 2008 , pages 100 107 , 2008 .
WWW 2009 MADRID!Poster Sessions : Wednesday , April 22 , 20091084
