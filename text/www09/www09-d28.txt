Modeling Semantics and Structure of Discussion Threads∗
Chen Lin† , Jiang Ming Yang‡ , Rui Cai‡ , Xin Jing Wang‡ , Wei Wang† , Lei Zhang‡
†School of Computer Science , Fudan University . {chen_lin , weiwang1}@fudaneducn
‡Microsoft Research , Asia . {jmyang , ruicai , xjwang , leizhang}@microsoft.com
ABSTRACT The abundant knowledge in web communities has motivated the research interests in discussion threads . The dynamic nature of discussion threads poses interesting and challenging problems for computer scientists . Although techniques such as semantic models or structural models have been shown to be useful in a number of areas , they are inefficient in understanding discussion threads due to the temporal dependence among posts in a discussion thread . Such dependence causes that semantics and structure coupled with each other in discussion threads . In this paper , we propose a sparse coding based model named SMSS to Simultaneously Model Semantic and Structure of discussion threads . Categories and Subject Descriptors I51 [ Pattern Recognition ] : Models Statistical General Terms Algorithms , Experimentation Keywords Threaded discussion , sparse coding , reply reconstruction 1 .
INTRODUCTION
Discussion threads have long been a popular option for web users to exchange opinions and share knowledge , eg thousands of web forum sites , mailing lists , chat rooms , and so on . A discussion thread usually originated from a root post by the thread starter . Fig 1 gives an intuitive description of a thread1 . It contains 7 posts . The first post is a piece of news about the release of “ SilverLight 2.0 ” . Some users comment on this post , ie , the 2nd and 3rd posts are about the “ update time ” ; some users have further questions and initiate sub discussions , ie , the 5th , 6th , and 7th posts are about “ Javascript communication ” ; others troll or complain , ie , the 4th post . As more users joining in and making comments , the thread grows , forming a nested dialogue structure as shown in the left part of Fig 1 . Furthermore , discussion threads show rich complexity in the semantics . Since users always response to others , previous posts affect later posts and cause the topic to drift in a thread . This is shown in the right part of Fig 1 . The goal of this paper is to model both the structure and semantics of a discussion thread in a simultaneous way .
∗This work was done when the first author visiting Microsoft Research , Asia . 1http://developersslashdotorg/commentspl?sid=1000769 Copyright is held by the author/owner(s ) . WWW 2009 , April 20–24 , 2009 , Madrid , Spain . ACM 978 1 60558 487 4/09/04 .
Figure 1 : An example of the structure and semantics of a discussion thread from Slashdot . 2 . THE SMSS MODEL expressed as a mixture of topics , as d(i ) T
A discussion thread has the following four characteristics . A discussion thread has several topics . Suppose there are T topics and V words , the jth topic is described as a distribution over the word space RV , as R is real numbers and x(j ) ∈ RV , 1 ≤ j ≤ T . Then , each post d(i ) is · x(j ) , where θ(i ) is the coefficient of d(i ) on topic x(j ) . To estimate the topic space X = {x(1 ) , . . . , x(T )} , in SMSS we minimize the loss function D − XΘ2 F . Here the thread contains L posts as D = { d(1 ) , . . . , d(L)} ; and the coefficient matrix Θ = {θ(1 ) . . . θ(L)} . j=1 θ(i ) j j
An individual post is related to a few topics . Although one thread may contain several semantic topics , each individual post usually concentrates on a limited number of topics . Therefore , we assume θ(i ) of each post is sparse and introduce a regularizer θ(i)1 in SMSS . larizer θ(i ) −i−1
A post is related to its previous posts . Users usually read current posts in a thread before they reply . Thus the semantics of a reply post is related to its previous posts . In SMSS we formally describe such reply structure as a reguk is the structural coefficient between the ith and kth posts . In other words , θ(i ) can be expressed as a linear combination of θ(k ) . k · θ(k)2
F , where b(i ) k=1 b(i )
The reply relations are sparse .
In most situations , users only intend to comment on one or two previous posts . Again , in SMSS we introduce a regularizer to favor such sparse structural coefficients b(i)1 .
Based on the above observations , the SMSS model is to estimate the value of topic matrix X , the coefficient matrix Θ , and the structural coefficients b for each post , by mini junkSilverlight 1.0JavascriptSilverlight 2.0 ReleasedAbout timeAs I still haven't installed Silverlight 1.0 or seen a site that requires it.Re:About timeSilverlight 1.0 should never have come out . Silverlight 1.0 vs Silverlight 2.0 is like comparing Flash to FlexAnd nothing of valueJavascript communicationwas gained.We're looking for a replacement for canvas in IE . excanvas sucks . We could use flash , but the Javascriptflash interface is very slowRe:Javascript communicationuse SVG , it IS XMLRe:Javascript communicationNo . SVG is no good for what we need . Also , its cross browser support is actually poorer , and performance is abysmal.StructureSemanticsWWW 2009 MADRID!Poster Sessions : Wednesday , April 22 , 20091103 L mizing the following loss function f : θ(i)1 f = D − XΘ2
L
F + λ1
θ(i ) − i−1 i=1 k=1
+λ2 i=1 k · θ(k)2 b(i )
F + λ3
L i=1 b(i)1
( 1 )
M
Here , the optimization objective balances the four terms by parameter λ1 , λ2 , and λ3 . In this way , both the semantics and the structure information are estimated simultaneously . Furthermore , for a collection of M threads which shared the same topics matrix X , we can optimize them together by minimizing : minimizeX,{Θ(t)},{bi} f ( n)(· )
( 2 )
The optimization problem is not jointly convex but can be solved by iteratively minimizing the convex sub problems . n=1
3 . APPLICATIONS
To demonstrate the efficiency of the proposed SMSS model , we reconstruct the reply relationships among posts using the semantics and structures estimated by SMSS . 3.1 Reply Reconstruction
Intuitively , posts with reply relations should have similar terms . However , the term similarity is unreliable as posts in discussion threads are usually very short . Our idea is to integrate the revealed semantic topics and structure as additional information in the similarity measure . Formally the similarity of a given post j and a previous post i is the combination of all the features , as : sim(i , j ) = sim( d(i ) , d(j ) ) + w1 · sim(b(i),b(j ) ) +w2 · sim(θ(i ) , θ(j ) )
( 3 )
Based on the similarity , we propose an approach to analyze a thread with L posts . That is , for a new post we compute the similarity between itself and all previous posts , rank the similarity , choose the post with highest score as a candidate parent . In case that the candidate parent is not similar enough to the new post , we assume this post initializes a new discussion branch of the thread . 3.2 Experiment
In experiment , we adopt two forums , Apple Discussion2 and Slashdot3 , as our data sources . These two forums are carefully selected because they have provided clear reply relations for evaluation . Since threads in Apple Discussion are much shorter , we sample 2000 threads including 20000 posts . For Slashdot , we sample 100 threads which also contains about 20000 posts . We manually write a wrapper to parse these pages and extract the exact reply relations as the ground truth . The evaluation metric is precision .
For comparison , we also adopt some naive methods such as Nearest Previous ( NP ) , Reply Root ( RR ) , and Only Document Similarity ( DS ) . NP assigns each post to the nearest previous post as the reply target ; RR assigns each post to the root post as the reply target ; and DS assigns each post to the post has most similar terms .
Moreover , we also compared our SMSS model with some state of the art models which can provide semantic topic analysis , such as latent dirichlet allocation ( LDA ) [ 1 ] and
2http://discussionsapplecom/ 3http://wwwslashdotorg/
Table 1 : Performance of reply reconstruction in all posts vs high quality posts
Method
Slashdot
Apple
All Posts Good Posts All posts Good Posts
NP RR DS LDA SWB SMSS
0.021 0.183 0.463 0.465 0.463 0.524
0.012 0.319 0.643 0.644 0.644 0.737
0.289 0.269 0.409 0.410 0.410 0.517
0.239 0.474 0.628 0.648 0.641 0.772
LDA , θ(j ) the special words with background model ( SWB ) [ 2 ] . Similar to Eq 3 , we compute the post similarity by sim(i , j ) = sim( d(i ) , d(j))+w1·sim(θ(i ) LDA ) from LDA . While SWB is an extension of LDA and allows words in documents to be modeled as either originating from general topics , or from post specific “ special ” word distributions , or from a threadwide background distribution . We leverage both its topic distribution θ(i ) SW B for similarity computing , as sim(i , j ) = sim( d(i ) , d(j ) ) + w1 · sim(θ(i )
SW B and special words distribution ψ(i ) SW B ) + w2 · sim( ψ(i )
SW B , ψ(j )
SW B , θ(j )
SW B ) .
All the three methods ( LDA , SWB , and SMSS ) achieve best average performance at w1 = 0.9 while the w2 is tuned for SWB and SMSS respectively . The experiment results are shown in Table 1 .
From Table 1 , we have four observations : ( I ) in Slashdot , a certain number of posts reply to the thread root , few to the nearest previous post ; while in Apple Discussion , there are almost equal number of posts replying to the nearest previous post and the root . This is because : discussion threads in Apple Discussion follow a Question Answering style . New solutions and fresh questions in replies invoke a serial of discussions . However , threads in Slashdot are usually initialized by a piece of news . Interesting aspects of the news and brilliant replies arise branches of discussions . ( II ) SWB and LDA show slight improvements to the baseline DS . This has verified our assumption that topics are robust in modeling the semantics ; but topics are not capable enough to extract reply relations . ( III ) In our experiment SWB achieves best performance when w2 is very small . This is because posts in threaded discussions are usually short . It is very difficult to estimate a sound coefficient for document specific word distribution . ( IV ) SMSS demonstrates significant improvement . The major difference between SMSS and former approaches is that SMSS resolves the structure representation b(i ) for post pi in each discussion thread . The best parameter of structural similarity is w2 = 09 This indicates that , besides of semantic similarities , structure similarities are more distinguishing in identifying reply relations . Furthermore , we also analyze the performance for the posts of different quality . We define posts whose score is larger than 3 in Slashdot and the posts marked as “ Helpful ” or “ Solved ” in Apple Discussion as good posts . The similarity based methods have better performance for these posts with high quality . It makes sense since the posts with high quality may cause more significative replies .
4 . REFERENCES [ 1 ] D . M . Blei , A . Y . Ng , and M . I . Jordan . Latent dirichlet allocation . Journal of Machine Learning Research , 3(6):993–1022 , 2003 .
[ 2 ] C . Chemudugunta , P . Smyth , and M . Steyvers .
Modeling general and specific aspects of documents with a probabilistic topic model . Advances in newral information processing systems , 41(6):391–407 , 1990 .
WWW 2009 MADRID!Poster Sessions : Wednesday , April 22 , 20091104
