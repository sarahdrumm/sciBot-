Efficient Interactive Fuzzy Keyword Search
Shengyue Ji
University of California , Irvine Irvine , California 92697 , USA shengyuj@icsuciedu
Guoliang Li
Tsinghua University Beijing 100084 , China liguoliang@tsinghuaeducn
Chen Li
University of California , Irvine Irvine , California 92697 , USA chenli@icsuciedu
Jianhua Feng
Tsinghua University Beijing 100084 , China fengjh@tsinghuaeducn
ABSTRACT Traditional information systems return answers after a user submits a complete query . Users often feel “ left in the dark ” when they have limited knowledge about the underlying data , and have to use a try and see approach for finding information . A recent trend of supporting autocomplete in these systems is a first step towards solving this problem . In this paper , we study a new information access paradigm , called “ interactive , fuzzy search , ” in which the system searches the underlying data “ on the fly ” as the user types in query keywords . It extends autocomplete interfaces by ( 1 ) allowing keywords to appear in multiple attributes ( in an arbitrary order ) of the underlying data ; and ( 2 ) finding relevant records that have keywords matching query keywords approximately . This framework allows users to explore data as they type , even in the presence of minor errors . We study research challenges in this framework for large amounts of data . Since each keystroke of the user could invoke a query on the backend , we need efficient algorithms to process each query within milliseconds . We develop various incrementalsearch algorithms using previously computed and cached results in order to achieve an interactive speed . We have deployed several real prototypes using these techniques . One of them has been deployed to support interactive search on the UC Irvine people directory , which has been used regularly and well received by users due to its friendly interface and high efficiency .
Categories and Subject Descriptors H33 [ Information Storage and Retrieval ] : Information Search and Retrieval—query formulation , search process
General Terms Algorithms , Experimentation , Performance
Keywords Interactive Search , Fuzzy Search , Autocomplete
1 .
INTRODUCTION
In a traditional information system , a user composes a query , submits it to the system , which retrieves relevant
Copyright is held by the International World Wide Web Conference Committee ( IW3C2 ) . Distribution of these papers is limited to classroom use , and personal use by others . WWW 2009 , April 20–24 , 2009 , Madrid , Spain . ACM 978 1 60558 487 4/09/04 . answers . This information access paradigm requires the user to have certain knowledge about the structure and content of the underlying data repository . In the case where the user has limited knowledge about the data , often the user feels “ left in the dark ” when issuing queries , and has to use a tryand see approach for finding information , as illustrated by the following example .
At a conference venue , an attendee named John met a person from a university . After the conference he wanted to get more information about this person , such as his research projects . All John knows about the person is that he is a professor from that university , and he only remembers the name roughly . In order to search for this person , John goes to the directory page of the university . Figure 1 shows such an interface . John needs to fill in the form by providing information for multiple attributes , such as name , phone , department , and title . Given his limited information about the person , especially since he does not know the exact spelling of the person ’s name , John needs to try a few possible keywords , go through the returned results , modify the keywords , and reissue a new query . He needs to repeat this step multiple times to find the person , if lucky enough . This search interface is neither efficient nor user friendly .
Figure 1 : A typical directory search form .
Many systems are introducing various features to solve this problem . One of the commonly used methods is autocomplete , which predicts a word or phrase that the user may type based on the partial query the user has entered . As an example , almost all the major search engines nowadays automatically suggest possible keyword queries as a user types in partial keywords . Both Google Finance and Yahoo! Finance support searching for stock information interactively as users type in keywords . More and more Web sites are having this feature , due to recent advances in highspeed networks and browser based programming languages and tools such as JavaScript and AJAX .
In this paper , we study a new computing paradigm , called “ interactive , fuzzy search . ” It has two unique features : ( 1 )
WWW 2009 MADRID!Track : Search / Session : Search UI371 Figure 2 : Interactive fuzzy search on the UC Irvine people directory ( http://psearchicsuciedu )
Interactive : The system searches for the best answers “ on the fly ” as the user types in a keyword query ; ( 2 ) Fuzzy : When searching for relevant records , the system also tries to find those records that include words similar to the keywords in the query , even if they do not match exactly .
We have developed several prototypes using this paradigm . The first one supports search on the UC Irvine people directory . A screenshot is shown in Figure 2 . In the figure , a user has typed in a query string “ professor smyt ” . Even though the user has not typed in the second keyword completely , the system can already find person records that might be of interest to the user . Notice that the two keywords in the query string ( including a partial keyword “ smyt ” ) can appear in different attributes of the records . In particular , in the first record , the keyword “ professor ” appears in the “ title ” attribute , and the partial keyword “ smyt ” appears in the “ name ” attribute . The matched prefixes are highlighted . The system can also find records with words that are similar to the query keywords , such as a person name “ smith ” . The feature of supporting fuzzy search is especially important when the user has limited knowledge about the underlying data or the entities he or she is looking for . As the user types in more letters , the system interactively searches on the data , and updates the list of relevant records . The system also utilizes a priori knowledge such as synonyms . For instance , given the fact that “ william ” and “ bill ” are synonyms , the system can find a person called “ William Kropp ” when the user has typed in “ bill crop ” . This search prototype has been used regularly by many people at UCI , and received positive feedback due to the friendly user interface and high efficiency . Another prototype , available at http://dblpicsuciedu , supports search on a DBLP dataset ( http//wwwinformatikuni trierde/∼ley/db/ ) with about 1 million publication records . A third prototype , available at http://pubmedicsuciedu , supports search on 3.95 million MEDLINE records ( http://wwwncbinlmnihgov/pubmed ) In this paper we study research challenges that arise naturally in this computing paradigm . The main challenge is the requirement of a high efficiency . To make search really interactive , for each keystroke on the client browser , from the time the user presses the key to the time the results computed from the server are displayed on the browser , the delay should be as small as possible . An interactive speed requires this delay be within milliseconds . Notice that this time includes the network transfer delay , execution time on the server , and the time for the browser to execute its javascript ( which tends to be slow ) . Providing a high effi ciency on a large amount of data is especially challenging because of two reasons . First , we allow the query keywords to appear in different attributes with an arbitrary order , and the “ on the fly join ” nature of the problem can be computationally expensive . Second , we want to support fuzzy search by finding records with keywords that match query keywords approximately .
We develop novel solutions to these problems . We present several incremental search algorithms for answering a query by using cached results of earlier queries . In this way , the computation of the answers to a query can spread across multiple keystrokes of the user , thus we can achieve a high speed . Specifically , we make the following contributions . ( 1 ) We first study the case of queries with a single keyword , and present an incremental algorithm for computing keyword prefixes similar to a prefix keyword ( Section 3 ) . ( 2 ) For queries with multiple keywords , we study various techniques for computing the intersection of the inverted lists of query keywords , and develop a novel algorithm for computing the results efficiently ( Section 41 ) Its main idea is to use forward lists of keyword IDs for checking whether a record matches query keyword conditions ( even approximately ) . ( 3 ) We develop a novel on demand caching technique for incremental search . Its idea is to cache only part of the results of a query . For subsequent queries , unfinished computation will be resumed if the previously cached results are not sufficient . In this way , we can efficiently compute and cache a small amount of results ( Section 42 ) ( 4 ) We study various features in this paradigm , such as how to rank results properly , how to highlight keywords in the results , and how to utilize domain specific information such as synonyms to improve search ( Section 5 ) . ( 5 ) In addition to deploying several real prototypes , we conducted a thorough experimental evaluation of the developed techniques on real data sets , and show the practicality of this new computing paradigm . All experiments were done using a single desktop machine , which can still achieve response times of milliseconds on millions of records .
1.1 Related Work Prediction and Autocomplete:1 There have been many studies on predicting queries and user actions [ 17 , 14 , 9 , 19 , 18 ] . With these techniques , a system predicts a word or
1The word “ autocomplete ” could have different meanings . Here we use it to refer to the case where a query ( possibly with multiple keywords ) is treated as a single prefix .
WWW 2009 MADRID!Track : Search / Session : Search UI372 a phrase the user may type in next based on the sequence of partial input the user has already typed . Many prediction and autocomplete systems treat a query with multiple keywords as a single string , thus they do not allow these keywords to appear at different places . For instance , consider the search box on the home page of Apple.com , which allows autocomplete search on Apple products . Although a keyword query “ itunes ” can find a record “ itunes wi fi music store , ” a query with keywords “ itunes music ” cannot find this record ( as of November 2008 ) , simply because these two keywords appear at different places in the record . The techniques presented in this paper focus on “ search on the fly , ” and they allow query keywords to appear at different places . As a consequence , we cannot answer a query by simply traversing a trie index . Instead , the backend intersection ( or “ join ” ) operation of multiple lists requires more efficient indexes and algorithms .
CompleteSearch : Bast et al . proposed techniques to support “ CompleteSearch , ” in which a user types in keywords letter by letter , and the system finds records that include these keywords ( possibly at different places ) [ 4 , 5 , 2 , 3 ] . Our work differs from theirs as follows . ( 1 ) CompleteSearch mainly focused on compression of index structures , especially in disk based settings . Our work focuses on efficient query processing using in memory indexes in order to achieve a high interactive speed . ( 2 ) Our work allows fuzzy search , making the computation more challenging . ( 3 ) For a query with multiple keywords , CompleteSearch mainly caches the results of the query excluding the last keyword , which may require computing and caching a large amount of intermediate results . Our incremental caching technique described in Section 4.2 achieves a higher efficiency .
Gram Based Fuzzy Search : There have been recent studies to support efficient fuzzy string search using grams [ 7 , 1 , 8 , 10 , 15 , 16 , 13 , 12 , 11 , 20 , 6 ] . A gram of a string is a substring that can be used as a signature for efficient search . These algorithms answer a fuzzy query on a collection of strings using the following observation : if a string r in the collection is similar to the query string , then r should share a certain number of common grams with the query string . This “ count filter ” can be used to construct gram inverted lists for string ids to support efficient search . In Section 6 we evaluated some of the representative algorithms . The results showed that , not surprisingly , they are not as efficient as trie based incremental search algorithms , mainly because it is not easy to do incremental computation on gram lists , especially when a user types in a relatively short prefix , and count filtering does not give enough pruning power to eliminate false positives .
2 . PRELIMINARIES
Problem Formulation : We formalize the problem of interactive , fuzzy search on a relational table , and our method can be adapted to textual documents , XML documents , and relational databases . Consider a relational table T with m attributes and n records . Let A = {a1 , a2 , . . . , am} denote the attribute set , R = {r1 , r2 , . . . , rn} denote the record set , and W = {w1 , w2 , . . . , wp} denote the distinct word set in T . Given two words wi and wj , “ wi fi wj ” denotes that wi is a prefix string of wj .
A query consists of a set of prefixes Q = {p1 , p2 , . . . , pl} . For each prefix pi , we want to find the set of prefixes from the data set that are similar to pi.2 In this work we use edit distance to measure the similarity between two strings . The edit distance between two strings s1 and s2 , denoted by ed(s1 , s2 ) , is the minimum number of edit operations ( ie , insertion , deletion , and substitution ) of single characters needed to transform the first one to the second . For example , ed(smith , smyth ) = 1 .
Definition 1
( Interactive Fuzzy Search ) . Given a set of records R , let W be the set of words in R . Consider a query Q = {p1 , p2 , . . . , p.} and an edit distance threshold δ . For each pi , let Pi be {p i fi w and . i , pi ) ≤ δ} . Let the set of candidate records RQ be . ed(p {r|r ∈ R , ∀1 ≤ i ≤ fi , ∃p i ∈ Pi and wi appears in r , . i fi wi} . The problem is to compute the best records in . p RQ ranked by their relevancy to Q . These records are computed incrementally as the user modifies the query , eg , by typing in more letters . i|∃w ∈ W , p .
Indexing : We use a trie to index the words in the relational table . Each word w in the table corresponds to a unique path from the root of the trie to a leaf node . Each node on the path has a label of a character in w . For simplicity , a node is mentioned interchangeably with its corresponding string in the remainder of the paper . Each leaf node has an inverted list of IDs of records that contain the corresponding word , with additional information such as the attribute in which the keyword appears and its position . For instance , Figure 3 shows a partial index structure for publication records . The word “ vldb ” has a trie node ID of 15 , and its inverted list includes record IDs 6 , 7 , and 8 . For simplicity , the figure only shows the record ids , without showing the additional information about attributes and positions . d a t a l u
5 i
1 n
3 4 6
1 2 3 6 u i s
4
4 v l d b
6 7 8
Figure 3 : Trie with inverted lists at leaf nodes .
3 . SINGLE KEYWORD
In this section we study interactive , fuzzy search on a single keyword prefix . For the case of exact prefix search using the trie index , the approach is straightforward : for each prefix , there exists only one corresponding trie node ( if any ) . The inverted lists of its descendant leaf nodes are accessed to retrieve candidate records .
The solution to the problem of fuzzy search is trickier since one prefix can have multiple similar prefixes ( called “ active 2Clearly our techniques can be used to answer queries when only the last keyword is treated as a partial prefix , and the other are treated as completed keywords .
WWW 2009 MADRID!Track : Search / Session : Search UI373 ED = 0
ED = 1
ED = 2
0
10 l
0
10 l
0
10 l
0
10 l
0
10 l
11 i
14 u
4
11 i
14 u
4
11 i
14 u
4
11 i
14 u
4
11 i
14 u
4
1 n u
12
13
3 4
5
15
16 i s
7
1 n u
12
13
3 4
5
15
16 i s
7
1 n u
12
13
3 4
5
15
16 i s
7
1 n u
12
13
3 4
5
15
16 i s
7
1 n u
12
13
3 4
5
15
16 i s
7
( a ) Initialize
( b ) query “ n ”
( c ) query “ nl ”
( d ) query “ nli ”
( e ) query “ nlis ”
Figure 4 : Fuzzy search of prefix queries of “ nlis ” ( threshold δ = 2 ) . nodes ” ) , and we need to compute them efficiently . The leaf descendants of the active nodes are called the predicted keywords of the prefix . For example , consider the trie in Figure 4 . Suppose the edit distance threshold δ = 2 , and a user types in a prefix p = “ nlis ” . Prefixes “ li ” , “ lin ” , “ liu ” , and “ luis ” are all similar to p , since their edit distances to “ nlis ” are within δ . Thus nodes 11 , 12 , 13 , and 16 are active nodes for p ( Figure 4 ( e) ) . The predicted keywords for the prefix are “ li ” , “ lin ” , “ liu ” , and “ luis ” .
We develop a caching based algorithm for incrementally computing active nodes for a keyword as the user types it in letter by letter . Given an input prefix p , we compute and store the set of active nodes Φp = {(n , ξn)} , in which n is an active node , and ξn = ed(p , n ) ≤ δ . The idea behind our algorithm is to use prefix filtering : when the user types in one more letter after p , the active nodes of p can be used to compute the active nodes of the new query . The algorithm works as follows . First , an active node set is initialized for the empty query , ie , Φ = {(n , ξn ) | ξn = |n| ≤ δ} . That is , it includes all trie nodes n whose corresponding string has a length |n| within the edit distance threshold δ . These nodes are active nodes for since their edit distances to are within δ .
As the user types in a query string px = c1c2 . . . cx letter by letter , the active node set Φpx is computed and cached for px . When the user types in a new character cx+1 and submits a new query px+1 , the server computes the activenode set Φpx+1 for px+1 by using Φpx . For each ( n , ξn ) in Φpx , the descendants of n are examined as active node candidates for px+1 , as illustrated in Figure 5 . For the node n , if ξn + 1 ≤ δ , then n is an active node for px+1 , and ( n , ξn + 1 ) is added into Φpx+1 . This case corresponds to deleting the last character cx+1 from the new query string px+1 . Notice even if ξn + 1 ≤ δ is not true , node n could still potentially become an active node of the new string , due to operations described below on other active nodes in Φpx .
For each child nc of node n , there are two possible cases . Case 1 : the child node nc has a character different from cx+1 . Figure 5 shows a node ns for such a child node , where “ s ” stands for “ substitution , ” the meaning of which will become clear shortly . We have ed(ns , px+1 ) ≤ ed(n , px ) + 1 = If ξn + 1 ≤ δ , then ns is an active node for the ξn + 1 . new string , and ( ns , ξn + 1 ) is added into Φpx+1 . This case corresponds to substituting the label of ns for the letter cx+1 . Case 2 : the child node nc has a label cx+1 . Figure 5 shows the node nm for such a child node , where “ m ” stands for “ matching . ” In this case , ed(nm , px+1 ) ≤ ed(n , px ) = ξn ≤ δ . Thus , nm is an active node of the new string , so we add ( nm , ξn ) into Φpx+1 . This case corresponds to the match between the character cx+1 and the label of nm . One subtlety here is that , if the distance for the node nm is smaller than δ , ie , ξn < δ , the following operation is also required : for each nm ’s descendant d that is at most δ − ξn letters away from nm , we need to add ( d , ξd ) to the active node set for the new string px+1 , where ξd = ξn + |d| − |nm| . This operation corresponds to inserting several letters after node nm.3
… … n , n
… … px+1
… … +1 n , n ns , n+1 nm , n cx+1 px c1 c2 c3 . . . cx cx+1
… … n d , n+|d| |nm|
… …
Deletion
Match
Substitution
Insersion
Figure 5 : Incrementally computing the active node set Φpx+1 from the active node set Φpx . We consider an active node ( n , ξn ) in Φpx .
During the computation of set Φpx+1 , it is possible to add multiple pairs ( v , ξ1 ) , ( v , ξ2 ) , . . . for the same trie node v . In this case , only the one with the smallest edit operation is kept in Φpx+1 . The reason is that , by definition , for the same trie node v , only the pair with the edit distance between the node v and the query string px+1 should be kept in Φpx+1 , which means the minimum number of edit operations to transform the string of v to the string of px+1 . 3Descendants of node ns do not need to be considered for insertions , because if these descendants are active nodes of Φpx+1 , their parents must appear in Φpx , and they can be processed by substitutions on their parents .
WWW 2009 MADRID!Track : Search / Session : Search UI374 The following lemma shows the correctness of this algo
4 . MULTIPLE KEYWORDS rithm .
Lemma 1 . For a query string px = c1c2 . . . cx , let Φpx be its active node set . Consider a new query string px+1 = c1c2 . . . cxcx+1 . ( 1 ) Soundness : Each node computed by the algorithm described above is an active node of the new query string px+1 . ( 2 ) Completeness : Every active node of the new query string px+1 will be computed by the algorithm above .
For example , assume a user types in a query “ nlis ” letter by letter , and the threshold δ is 2 . Figure 4 illustrates how the algorithm processes the prefix queries invoked by keystrokes . Table 1 shows the details of how to compute the active node sets incrementally . The first step is to initialize Φ = {(0 , 0 , ( 10 , 1 ) , ( 11 , 2 ) , ( 14 , 2)} ( Figure 4(a ) and Table 1(a) ) . When the user types in the first character “ n ” , for the string s = “ n ” , its active node set Φs is computed based on Φ as follows . For ( 0 , 0 ) ∈ Φ , we add ( 0 , 1 ) into Φs , since letter “ n ” can be deleted . For node 10 , a child of node 0 with a letter “ l ” , ( 10 , 1 ) is added into Φs , as “ l ” can be substituted for “ n ” . There are no match and insertion operations as node 1 does not have a child with label “ n ” . Φs is computed in this way ( Figure 4 ( b ) and Table 1 ( b) ) . Similarly , prefix queries of “ nlis ” can be answered incrementally .
For each active node , the keywords corresponding to its leaf descendants are predicted keywords . Consider the activenode set for the prefix query “ nl ” as shown in Figure 4(c ) . For ( 11 , 2 ) , “ li , “ lin ” , and “ liu ” are predicted keywords accordingly . We retrieve the records on the inverted lists of predicted words to compute answers to the query .
Table 1 : Active node sets for processing prefix queries of “ nlis ” ( edit distance threshold δ = 2 ) ( a ) Query “ n ”
( b ) Query “ nl ”
( c ) Query “ nli ”
Φ Delete Substitute Match Insert Φn
Φn Delete Substitute Match Insert Φnl
Φnl Delete Substitute Match Insert Φnli
Φnli Delete Substitute Match Insert Φnlis
( d ) Query “ nlis ”
.11,2fi
.14,2fi
.0,0fi .0,1fi .10,1fi
.10,1fi .10,2fi
– –
11,2fi;14,2fi
– – – – – – .0,1fi ; .10,1fi ; .11,2fi ; .12,2fi ; .14,2fi
.12,2fi
– –
–
.0,1fi .10,1fi .0,2fi .10,2fi 11,2fi;14,2fi – .10,1fi – 11,2fi;14,2fi –
– – – – .10,1fi ; .0,2fi ; .11,2fi ; .14,2fi
.11,2fi .12,2fi .14,2fi – – – –
– – – –
.10,1fi .10,2fi .14,2fi .11,1fi
.0,2fi – – – –
– – – –
.11,2fi
.14,2fi
– –
.15,2fi
–
12,2fi;13,2fi .11,1fi ; .10,2fi ; .12,2fi ; .13,2fi ; .14,2fi ; .15,2fi .11,1fi .10,2fi .12,2fi .13,2fi .14,2fi .15,2fi .11,2fi – 12,2fi;13,2fi – – – – –
– – – – .11,2fi ; .12,2fi ; .13,2fi ; .16,2fi
– – .16,2fi –
– – – –
– – – –
In this section we study answering interactive fuzzy search when a user types in multiple keywords . The goal is to efficiently and incrementally compute the records with keywords whose prefixes are similar to those query keywords . ( 1 ) InterWe focus on several challenges in this setting . section of multiple lists of keywords : Each query keyword ( treated as a prefix ) has multiple predicted complete keywords , and the union of the lists of these predicted keywords includes potential answers . The union lists of multiple query keywords need to be intersected in order to compute the answers to the query . These operations can be computationally costly , especially when each query keyword can have multiple similar prefixes . In Section 4.1 we study various algorithms for computing the answers efficiently . ( 2 ) Cachebased incremental intersection : In most cases , the user types the query letter by letter , and subsequent queries append additional letters to previous ones . Based on this observation , we study how to use the cached results of earlier queries to answer a query incrementally ( Section 42 ) 4.1 Intersecting Union Lists of Prefixes
For simplicity , we first consider exact search , and then extend the results to fuzzy search . Given a query Q = {p1 , p2 , . . . , p.} , suppose {ki1 , ki2 , . . .} is the set of keywords that share the prefix pi . Let Lij denote the inverted list of kij , and Ui = j Lij be the union of the lists for pi . We i Ui . study how to compute the answer to the query , ie ,
S
T
P i,j |Lij| ) . The shorter the keyword prefix is , the slower
Simple Methods : One method is the following . For each prefix pi , we compute the corresponding union list Ui onthe fly and intersect the union lists of different keywords . The time complexity for computing the unions could be O( the query could be , as inverted lists of more predicted keywords need to be traversed to generate the union list . This approach only requires the inverted lists of trie leaf nodes , and the space complexity of the inverted lists is O(n × L ) , where n is the number of records and L is the average number of distinct keywords of each record .
Alternatively , we can pre compute and store the union list of each prefix , and intersect the union lists of query keywords when a query comes . The main issue of this approach is that the precomputed union lists require a large amount of space , especially since each record occurrence on an inverted list needs to be stored many times . The space complexity of all the union lists is O(n × L × w ) , where w is the average keyword length . Compression techniques can be used to reduce the space requirement .
There have been other approaches for answering prefix intersection . For instance , Bast et al . [ 4 ] proposed a method that groups ranges of keywords and builds document lists separately for each range . Intersection is performed between an existing document list and several ranges called “ HYB blocks . ” The limitation of this approach is that , for most queries , the ranges can include many irrelevant documents , which require a lot of time to process . We will show our experimental results of this method in Section 6 .
Efficient Prefix Intersection Using Forward Lists : We develop a new solution based on the following ideas . Among the union lists U1,U2 , . . . ,U . , we identify the shortest union list . Each record ID on the shortest list is verified by check
WWW 2009 MADRID!Track : Search / Session : Search UI375 ing if it exists on all the other union lists ( following the ascending order of their lengths ) . Notice that these union lists are not materialized in the computation . The shortest union list can be traversed by accessing the leaf nodes of the corresponding prefix . The length of each union list can be pre computed and stored in the trie , or estimated on the fly . To verify record occurrences efficiently , a forward list can be maintained for each record r , which is a sorted list of IDs of keywords in r , denoted as Fr . A unique property of the keyword IDs is that they are encoded using their alphabetical order . Therefore , each prefix pi has a range of keyword IDs [ M inIdi , M axIdi ] , so that if pi is a prefix of another string s , then the ID of s should be within this range .
An interesting observation is , for a record r on the shortest union list , the problem of verifying whether r appears on ( non materialized ) union list Uk of a query prefix pk , is equivalent to testing if pk appears in the forward list Fr as a prefix . We can do a binary search for M inIdk on the forward list Fr to get a lower bound Idlb , and check if Idlb is no larger than M axIdk . The probing succeeds if the condition ` ´ holds , and fails otherwise . The time complexity for process(fi − 1)logL , where fi is the ing each single record r is O number of keywords in the query , and L is the average number of distinct keywords in each record . A good property of this approach is that the time complexity of each probing does not depend on the lengths of inverted lists , but on the number of unique keywords in a record ( logarithmically ) .
Figure 6 shows an example when a user types in a query “ vldb li ” . The predicted keywords for “ li ” are “ li ” , “ lin ” , “ line ” , and “ linux ” . The keyword ID range of each query keyword is shown in brackets . For instance , the keywordID range for prefix “ li ” is [ 3 , 5 ] , which covers the ranges of “ lin ” and “ liu ” . To intersect the union list of “ vldb ” with that of “ li ” , we first identify “ vldb ” as the one with the shorter union list . The record IDs ( 6 , 7 , 8 , . . . ) on the list are probed one by one . Take record 6 as an example . Its forward list contains keyword IDs 2 , 4 , 8 , . . We use the range of “ li ” to probe the forward list . By doing a binary search for the keyword ID 3 , we find keyword with ID 4 on the forward list , which is then verified to be no larger than MaxID = 6 . Therefore , record 6 is an answer to the query , and the keyword with ID 4 ( which appears in record 6 ) has “ li ” as a prefix . vldb[8,8 ] li[3,6 ] li lin line linux vldb
[ 3,6 ] [ 4,6 ] [ 5,5 ] [ 6,6 ] [ 8,8 ]
6
7
8
…
Shortest Inverted
List
2 4 9 …
Probing
1 2 7 8 …
1 2 3 9 …
Forward Lists
Figure 6 : Prefix intersection using forward lists . ( Numbers with underlines are keyword IDs , and numbers without underlines are record IDs . )
Extension to Fuzzy Search : The algorithm described above naturally extends to the case of fuzzy search . Since each query keyword has multiple active nodes of similar prefixes , instead of considering the union of the leaf nodes of one prefix node , now we need to consider the unions of the leaf nodes for all active nodes of a prefix keyword . The lengths of these union lists can be estimated in order to find a shortest one . For each record r on the shortest union list , for each of the other query keywords , for each of its active nodes , we test if the corresponding similar prefix can appear in the record r as a prefix using the forward list of r .
4.2 Cache Based Intersection of Prefixes
In Section 3 we presented an algorithm for incrementally computing similar prefixes for a query keyword , as the user types the keyword letter by letter . Now we show that prefix intersection can also be performed incrementally using previously cached results .
We use an example to illustrate how to cache query results and use them to answer subsequent queries . Suppose a user types in a keyword query Q1 = “ cs co ” . All the records in the answers to Q1 are computed and cached . For a new query Q2 = “ cs conf ” that appends two letters to the end of Q1 , we can use the cached results of Q1 to answer Q2 , because the second keyword “ conf ” in Q2 is more restrictive than the corresponding keyword “ co ” in Q1 . Each record in the cached results of Q1 is verified to check if “ conf ” can appear in the record as a prefix . In this way , Q2 does not need to be answered from scratch . As in this example , in the following discussion , we use “ Q1 ” to refer to a query whose results have been cached , and “ Q2 ” to refer to a new query whose results we want to compute using those of Q1 .
Cache Miss : Often the more keywords the user types in , the more typos and mismatches the query could have . Thus we may want to dynamically increase the edit distance threshold δ as the query string is getting longer . Then it is possible that the threshold for the new query Q2 is strictly larger than that of the original query Q1 . In this case , the active nodes of keywords in Q1 might not include all those of keywords in Q2 . As a consequence , we cannot use the cached results of Q1 ( active nodes and answers ) to compute those of Q2 . This case is a cache miss , and we need to compute the answers of Q2 from scratch .
Reducing Cached Results : The cached results of query Q1 could be large , which could require a large amount of time to compute and space to store . There are several cases where we can reduce the size . The first case is when we want to use pagination , ie , we show the results in different pages . In this case , we can traverse the shortest list partially , until we have enough results for the first page . As the user browses the results by clicking “ Previous ” and “ Next ” links , we can continue traversing the shortest list to compute more results and cache them .
The second case is when the user is only interested in the best results , say , the top k records according a ranking function , for a predefined constant k . Such a function could allow us to compute the answers to the query Q1 without traversing the entire shortest list , assuming we are sure that all the remaining records on the list cannot be better than the results already computed . In other words , the ranking function allows us to do early termination during the traversal . When using the top k results of Q1 to compute the top k results of Q2 , it is possible that the cached results are not enough , since Q2 has a more restrictive keyword . In this case , we can continue the unfinished traversal on the
WWW 2009 MADRID!Track : Search / Session : Search UI376 shortest list , assuming we have remembered the place where the traversal stopped on the shortest list for query Q1 .
Figure 7 shows an example of incrementally computing top k answers using cached results . A user types in a query “ cs conf vanc ” letter by letter , and the server receives queries “ cs co ” , “ cs conf ” , and “ cs conf vanc ” in order . ( Notice that it is possible that some of the prefix queries were not received by the server due to the network overhead and the server delay . ) The first query “ cs co ” is answered from scratch . Assuming the union list of keyword “ cs ” is the shorter one . The traversal stops at the first vertical bar . Each record accessed in the traversal is verified by probing the keyword range of “ co ” using the forward list of the record . Records that pass the verification are cached . When we want to answer the query “ cs conf ” incrementally , we first verify each record in the cached result of the previous query by probing the keyword range of “ conf ” . Some of these results will become results of the new query . If the results from the cache is insufficient to compute the new top k , we resume the traversal on the list of “ cs ” , starting from the stopping point of the previous query , until we have enough top k results for the new query . The next query “ cs conf vanc ” is answers similarly . cs co cs conf
Stopping point
Compute
Traversal list
Cached results
Verify
Compute
Traversal list cs conf vanc
Cached results
Verify
Compute
Traversal list
Cached results
Figure 7 : Computing top k results using cached answers and resuming unfinished traversal on a list .
In the case of cache miss , ie , earlier cached results cannot be used to compute the answers of a new query , we may need to answer the new query from scratch . We may choose a different list as the shortest one to traverse , and subsequent queries can be computed incrementally similarly .
5 . ADDITIONAL FEATURES
5.1 Ranking
A ranking function considers various factors to compute an overall relevance score of a record to a query . The following are several important factors . ( 1 ) Matching prefixes : We consider the similarity between a query keyword and its best matching prefix . The more similar a record ’s matching keywords are to the query keywords , the higher this record should be ranked . The similarity is also related to keyword length . For example , when a user types in a keyword “ circ ” , the word “ circle ” is closer to the query keyword than “ circumstance ” , therefore records containing the word “ circle ” could be ranked higher . In most cases exact matches on the query should have a higher priority than fuzzy matches . ( 2 ) Predicted keywords : Different predicted keywords for the same prefix can have different weights . One way to assign a score to a keyword is based on its inverted document frequency ( IDF ) . ( 3 ) Record weights : Different records could have different weights . For example , a publication record with many citations could be ranked higher than a less cited publication .
As an example , the following is a scoring function that combines the above factors . Suppose the query is Q = {p1 , p2 , . . .} , p . i is the best matching prefix for pi , and ki . is the best predicted keyword for p i ) be an . i and pi . The score of a record r edit similarity between p for Q can be defined as :
. i . Let sim(pi , p
X
Score(r , Q ) =
[ sim(pi , p i i)+α·(|p . i|−|ki|)+β·score(r , ki) ] , . where α and β are weights ( 0 < β < α < 1 ) , and score(r , ki ) is a score of record r for keyword ki . 5.2 Highlighting Best Prefixes
When displaying records to the user , the most similar prefixes for an input prefix should be highlighted . This highlighting is straightforward for the exact match case . For fuzzy search , a query prefix could be similar to several prefixes of the same predicted keyword . Thus , there could be multiple ways to highlight the predicted keyword . For example , suppose a user types in “ lus ” , and there is a predicted keyword “ luis ” . Both prefixes “ lui ” and “ luis ” are similar to “ lus ” , and there are several ways to highlight them , such as “ luis ” or “ luis ” . To address this issue , we use the concept of normalized edit distance . Formally , given two prefixes pi and pj , their normalized edit distance is : ed(pi , pj ) max(|pi|,|pj| )
, ned(pi , pj ) =
( 1 ) where |pi| denotes the length of pi . Given an input prefix and one of its predicted keywords , the prefix of the predicted keyword with the minimum ned to the query is highlighted . We call such a prefix a best matched prefix , and call the corresponding normalized edit distance the “ minimal normalized edit distance , ” denoted as “ mned ” . This prefix is considered to be most similar to the input . For example , for the keyword “ lus ” and its predicted word “ luis ” , we have ned( “ lus ” , “ l ” ) = 2 3 , ned( “ lus ” , “ lui ” ) = 1 4 . Since mned( “ lus ” , “ luis ” ) = ned( “ lus ” , “ luis ” ) , “ luis ” will be highlighted .
3 , and ned( “ lus ” , “ luis ” ) = 1
3 , ned( “ lus ” , “ lu ” ) = 1
5.3 Using Synonyms
We can utilize a priori knowledge about synonyms to find relevant records . For example , “ William = Bill ” is a common synonym in the domain of person names . Suppose in the underlying data , there is a person called “ Bill Gates ” . If a user types in “ William Gates ” , we can also find this person . To this end , on the trie , the node corresponding to “ Bill ” has a link to the node corresponding to “ William ” , and vise versa . When a user types in “ Bill ” , in addition to retrieving the records for “ Bill ” , we also identify those of “ William ” following the link . In this way , our techniques can be extended to utilize synonyms .
WWW 2009 MADRID!Track : Search / Session : Search UI377 6 . EXPERIMENTS
We deployed several prototypes in different domains to support interactive , fuzzy search . We conducted a thorough experimental evaluation of the developed techniques on real data sets , such as publications and people directories . Here we report the results on the following two data sets mainly because of their relative large sizes . ( 1 ) DBLP : It included about one million computer science publication records , with six attributes : authors , title , conference or journal name , year , page numbers , and URL . ( 2 ) MEDLINE : It had about 4 million latest publication records related to life sciences and biomedical information . We used five attributes : authors , their affiliations , article title , journal name , and journal issue . Table 2 shows the sizes of the data sets , index sizes , and index construction times .
Table 2 : Data sets and index costs
Data Set
DBLP
Record Number
Original Data Size
# of Distinct Keywords Index Construction Time
Trie Size
Inverted List Size Forward List Size
1 million 190 MB
392K 50 secs 36 MB 52 MB 54 MB
MEDLINE 4 million 1.25 GB
1.79 million
588 secs 165 MB 445 MB 454 MB
Two prototypes are available at http://dblpicsuciedu/ and http://pubmedicsuciedu/ For each of them , we set up a Web server using Apache2 on a Linux machine with an Intel Core 2 Quad processor Q6600 ( 2.40GHz , 8M , 1066MHz FSB ) and 8G DDR2 800 memory . We implemented the backend as a FastCGI server process , which was written in C++ , compiled with a GNU compiler . To make sure our experiments did not affect the deployed systems , we did the experiments on another Linux machine with an Intel Core 2 Duo processor E6600 ( 2.40GHz , 4M , 1066MHz FSB ) and 3G DDR2 667 memory . 6.1 Efficiency of Computing Similar Prefixes We evaluated the efficiency of computing the prefixes on the trie that are similar to a query keyword . For each data set , we generated 1,000 single keyword queries by randomly selecting keywords in the data set , and applying a random number of edit changes ( 0 to 2 ) on the keyword . The average length ( number of letters ) of keywords was 9.9 for the DBLP data set , and 10.2 for the MEDLINE data set . For each prefix of each query , we measured the time to find similar prefixes within an edit distance of 2 , not including the time to retrieve records . We computed the average time for the prefix queries with the same length .
We implemented three methods to compute similar prefixes . ( 1 ) Incremental/Cache : We computed the active nodes of a query using the cached active nodes of previous prefix queries , using the incremental algorithm presented in Section 3 . This algorithm is applicable when the user types a query letter by letter . ( 2 ) Incremental/NoCache : We used the incremental algorithm , but assuming no earlier active nodes have been cached , and the computation started from scratch . This case happens when a user copies and pastes a long query , and none of the active nodes of any prefix queries has been computed . It also corresponds to the traditional non interactive search case , where a user submits a query and clicks the “ Search ” button . ( 3 ) Gram Based : We built gram inverted lists on all prefixes with at least three letters using the method described in [ 15 ] . We used the implementation in the Flamingo release4 , using a gram length of 3 and a length filter . The total number of such prefixes was 1.1 millions for the DBLP data set and 5 millions for the MEDLINE data set . The index structure can be used to compute similar prefixes for keywords with at least four letters .
Figure 8 shows the performance results of these three methods . The method Incremental/Cache was most efficient . As the user types in letters , its response time first increased slightly ( less than 5 ms ) , and then started decreasing quickly after the fourth letter . The main reason is that the number of active nodes decreased , and the cached results made the computation efficient . The method Incremental/NoCache required longer time since each query needed to be answered from scratch , without using any cached active nodes . The method Gram Based performed efficiently when the query keyword had at least seven letters . But it had a very poor performance for shorter keywords , since the count filter had a weak power to prune false positives .
) s m
1000
( i e m T h c r a e S g v A
100
10
1
0.1
0.01
Incremental/Cache Incremental/NoCache Gram Based
Incremental/Cache Incremental/NoCache Gram Based
) s m
1000
( i e m T h c r a e S g v A
100
10
1
0.1
0.01
2
4
6
8
10 12 14
Prefix Length
( a ) DBLP
2
4
6
8
10 12 14
Prefix Length ( b ) MEDLINE
Figure 8 : Computing prefixes similar to a keyword .
6.2 Efficiency of List Intersection of Multiple
Keywords
We evaluated several methods for intersecting union lists of multiple keywords , as described in Section 41 For each data set , we generated 1,000 queries by randomly selecting records from the data set , and choosing keywords from each record . We implemented the following methods to intersect the union lists of the keywords . ( 1 ) ForwardLists : It is the algorithm presented in Section 4.1 , which traverses the shortest union list and uses the other query keywords to probe the forward lists of records on the shortest list . The union list was traversed on the fly without being materialized . ( 2 ) HashProbe : The shortest union list was materialized as a hash table at query time . Each record ID on the other union lists was searched on the hash table . ( 3 ) MaterializedUnions : We materialized the union lists of all the query keywords and their prefixes , and computed an intersection by using the record IDs of the shortest list to probe the other union lists . We measured the intersection time only . ( 4 ) HYB : We implemented a structure called “ HYB ” as described in [ 4 ] . We used an in memory implementation , and all IDs were stored without any encoding and compression , since decoding and decompression will introduce additional time overhead during query answering . The number of HYB blocks
4http://flamingoicsuciedu/releases/20
WWW 2009 MADRID!Track : Search / Session : Search UI378 cases , since it can stop early during the traversal of the list , and a new query can be incrementally computed using earlier results . All these methods required a relative long time when the prefix had 6 letters due to the cache miss .
16
12
) s m
( i e m T h c r a e S g v A
68.7
38.7
32.8
NoCache CompleteTraversal PartialTraversal
8
4
0
2
3
4
6
5 7 Prefix Length
8
9
10
Figure 10 : Performance of prefix intersection ( DBLP ) .
6.4 Scalability
We evaluated the scalability of our algorithms . As an example , we used the MEDLINE dataset . Figure 11(a ) shows how the index size increased as the number of records increased . It shows that all the sizes of trie , inverted lists , and forward lists increased linearly .
ForwardLists HashProbe MaterializedUnions HYB
( i e z S x e d n
I
1200
1000
)
B M
Forward Lists Inverted Lists Trie
800
600
400
200
0
0.5 1 1.5 2 2.5 3 3.5 4
Record Number ( million ) ( a ) Index Size
3
4
5
6
7
8
9 10
Prefix Length ( b ) MEDLINE
Queries with errors Queries with no errors was 162 for the DBLP dataset and 285 for the MEDLINE data set , using the parameters recommended in [ 4 ] .
We evaluated these methods on queries with two keywords , assuming no previous query results were cached . Figure 9 shows the average time of each method as the length of the second keyword increased . The intersection operation was very time consuming when the second keyword had no more than two letters , since the union lists of the prefixes were long . The HashProbe method performed relatively poorly due to the cost of building the hash table for the shorter list and traversing the other list . The HYB method ’s performance was even worse for most cases . The main reason is that this method was designed to have a prefix overlapping with few HYB blocks ( usually one or two ) to avoid merging too many answer lists . However , it has a side effect that an HYB block could include too many keywords compared to the query prefix , forcing the method to process long lists . This drawback was not a main issue in a disk based setting , as the algorithm can benefit from list compression and sequential disk IOs time . The MaterializedUnions method performed well , but with a high memory cost as discussed in Section 41 The ForwardLists algorithm achieved an excellent performance , at the cost of storing the forward lists . An interesting finding in the results is that ForwardLists even outperformed MaterializedUnions on the MEDLINE data set . This is because as the data set became larger , the average time of each binary search on the union lists increased , while the average time of each binary probe on the forward lists did not change much . i
(
4 ) s m 3 e m T 2 h c r a e 1 S g v A 0
2
3
ForwardLists HashProbe MaterializedUnions HYB
4
5
6
7
8
9
10
Prefix Length ( a ) DBLP
) s m
120
( i e m T h c r a e S g v A
90
60
30
0
2
Figure 9 : List intersection of multiple keywords .
6.3 Performance of Cache Based Intersection We evaluated the performance of different methods of cache based prefix intersection , as described in Section 42 We allowed at most one typo for each prefix with at most five letters , and two errors for prefixes with more than five letters . As a consequence , for a query with two keywords , when the sixth letter of the second keyword was typed in , a cache miss occurred . We implemented the following methods . ( 1 ) NoCache : No query results are cached . A query is computed without using any cached query results . Early termination is enabled . ( 2 ) CompleteTraversal : It traverses the shortest union list completely to compute the results of the current query . ( 3 ) PartialTraversal : It traverses the shortest union list partially until it finds the top 10 results for the current query ( as discussed in Section 42 )
Figure 10 shows the query time of the methods on the DBLP data set . CompleteTraversal outperformed the NoCache method for relatively long prefixes ( with more than 6 letters ) mainly due to the smaller set of cached results . The PartialTraversal method was the most efficient one in most
) s m
( i e m T h c r a e S g v A
4 3.5 3 2.5 2 1.5 1 0.5 0
0.5
Queries with errors Queries with no errors
1.5
1 3.5 Record Number ( million )
2.5
2
3
4
( b ) Single Keyword
50
40
30
20
10
) s m
( i e m T h c r a e S g v A
0
0.5
2
2.5
1.5
1 3.5 Record Number ( million ) ( c ) Multiple Keywords
3
4
Figure 11 : Scalability ( MEDLINE ) .
We measured the query performance as the data size increased . We first evaluated queries with a single keyword . We considered two types of queries : the first type was generated by randomly selecting keywords in the data set ; the second type was obtained by modifying the first type by adding edit errors ( 0 to 2 ) . Each query asked for the 10 best records . For each type , we measured the query response time for each keystroke . Figure 11(b ) shows the results for the MEDLINE data set as we increased the number of records . It shows that the algorithms can answer a single keyword query efficiently ( within 3 ms ) , for both types of queries .
We next evaluated the algorithms for queries with multiple keywords , which asked for 10 best records . We generated queries with two keywords , and measured the average
WWW 2009 MADRID!Track : Search / Session : Search UI379 query time of each keystroke on the second keyword . Figure 11(c ) shows that our algorithms can process such queries very efficiently . For instance , when the data set had 4 million records , a query without errors was processed within 20 ms , while a query with errors was answered within 55 ms . 6.5 Round Trip Time
The round trip time of interactive fuzzy search consists of three components : server processing time , network time , and JavaScript running time . Different locations in the world could have different network delays to our servers in southern California . We measured the time cost for a sample query “ divsh srivstava search ” on the DBLP prototype from five locations around the world : US west , US east , China , Israel , and Australia . Figure 12 shows the results . We can see that the server running time was less than 1/5 of the total round trip time . JavaScript took around 40 to 60 ms to execute . The relative low speed at some locations was mainly due to the network delay . For example , it took about 4/5 of the total round trip time when our system was tested from China . For all the users from different locations , the total round trip time for a query was always below 300 ms , and all of them experienced an interactive interface . For large scale systems processing queries from different countries , we can solve the possible network delay problem by using distributed servers .
) s m
( i e m T p i r T d n u o R
Network Javascript Server
300
200
100
0 US/West US/East China
Israel Australia
Location
Figure 12 : Round trip time for different locations .
6.6 Saved Typing Effort
Interactive search can also save user typing efforts , since results can be found before the user types in complete keywords . To evaluate the saving of typing effort , we constructed six queries on the DBLP data set as shown in Table 3 . The keywords in italic font are mistyped keywords . Each query was typed in letter by letter , until the system found the expected records . We measured how much lettertyping effort the system can save for the user . For each query Qi , let N ( Qi ) be the number of letters the user typed before the relevant answers are found . We use 1 − N(Qi ) |Qi| to quantify the relative saved effort . For example , for query Q6 , the user could find relevant answers right after typing in “ divsh sri sea ” . The saved effort of Q6 is 1− 13 22 = 41 % , as the user only needed to type in 13 letters , instead of 22 letters in the full query . Table 3 shows that this paradigm can save the user 40 % to 60 % typing effort on average .
7 . CONCLUSIONS
We studied a new information access paradigm that supports interactive , fuzzy search . We proposed an efficient incremental algorithm to answer single keyword fuzzy queries .
Table 3 : Queries and saved typing effort .
Query
Saved typing effort sunta sarawgi surajit chuardhuri nick kodas approxmate flostsos icde similarity similarty search icde divsh srivstava search
42 % 50 % 41 % 38 % 55 % 41 %
ID Q1 Q2 Q3 Q4 Q5 Q6
We studied various algorithms for computing the answers to a query with multiple keywords that are treated as fuzzy , prefix conditions . We developed efficient algorithms for incrementally computing answers to queries by using cached results of previous queries in order to achieve an interactive speed on large data sets . We studied several useful features such as ranking answers , highlighting results , and utilizing synonyms . We deployed several real systems to test the techniques , and conducted an thorough experimental study of the algorithms . The results proved the practicality of this new computing paradigm .
8 . REFERENCES [ 1 ] A . Arasu , V . Ganti , and R . Kaushik . Efficient exact set similarity joins . In VLDB , pages 918–929 , 2006 .
[ 2 ] H . Bast , A . Chitea , F . M . Suchanek , and I . Weber . Ester : efficient search on text , entities , and relations . In SIGIR , pages 671–678 , 2007 .
[ 3 ] H . Bast , C . W . Mortensen , and I . Weber . Output sensitive autocompletion search . In SPIRE , pages 150–162 , 2006 .
[ 4 ] H . Bast and I . Weber . Type less , find more : fast autocompletion search with a succinct index . In SIGIR , pages 364–371 , 2006 .
[ 5 ] H . Bast and I . Weber . The completesearch engine : Interactive , efficient , and towards ir& db integration . In CIDR , pages 88–95 , 2007 .
[ 6 ] A . Behm , S . Ji , C . Li , and J . Lu . Space constrained gram based indexing for efficient approximate string search . In ICDE , 2009 .
[ 7 ] K . Chakrabarti , S . Chaudhuri , V . Ganti , and D . Xin . An efficient filter for approximate membership checking . In SIGMOD Conference , pages 805–818 , 2008 .
[ 8 ] S . Chaudhuri , V . Ganti , and R . Motwani . Robust identification of fuzzy duplicates . In ICDE , pages 865–876 , 2005 .
[ 9 ] K . Grabski and T . Scheffer . Sentence completion . In SIGIR , pages 433–439 , 2004 .
[ 10 ] M . Hadjieleftheriou , A . Chandel , N . Koudas , and D . Srivastava . Fast indexes and algorithms for set similarity selection queries . In ICDE , pages 267–276 , 2008 .
[ 11 ] L . Jin , N . Koudas , C . Li , and A . K . H . Tung . Indexing mixed types for approximate retrieval . In VLDB , pages 793–804 , 2005 .
[ 12 ] L . Jin and C . Li . Selectivity estimation for fuzzy string predicates in large data sets . In VLDB , pages 397–408 , 2005 .
[ 13 ] N . Koudas , C . Li , A . K . H . Tung , and R . Vernica . Relaxing join and selection queries . In VLDB , pages 199–210 , 2006 .
[ 14 ] K . Kukich . Techniques for automatically correcting words in text . ACM Comput . Surv . , 24(4):377–439 , 1992 .
[ 15 ] C . Li , J . Lu , and Y . Lu . Efficient merging and filtering algorithms for approximate string searches . In ICDE , pages 257–266 , 2008 .
[ 16 ] C . Li , B . Wang , and X . Yang . VGRAM : Improving performance of approximate queries on string collections using variable length grams . In VLDB , pages 303–314 , 2007 .
[ 17 ] H . Motoda and K . Yoshida . Machine learning techniques to make computers easier to use . Artif . Intell . , 103(1 2):295–321 , 1998 .
[ 18 ] A . Nandi and H . V . Jagadish . Effective phrase prediction . In
VLDB , pages 219–230 , 2007 .
[ 19 ] H . E . Williams , J . Zobel , and D . Bahle . Fast phrase querying with combined indexes . ACM Trans . Inf . Syst . , 22(4):573–594 , 2004 .
[ 20 ] C . Xiao , W . Wang , X . Lin , and J . X . Yu . Efficient similarity joins for near duplicate detection . In WWW , pages 131–140 , 2008 .
WWW 2009 MADRID!Track : Search / Session : Search UI380
