Compressed Web Indexes
Flavio Chierichetti∗
Dipartimento di Informatica Sapienza University of Rome
Via Salaria , 113
Roma , 00198 , Italy chierichetti@diuniroma1it
Ravi Kumar Yahoo! Research 701 First Avenue
Sunnyvale , CA 94089 , USA ravikumar@yahoo inc.com
Prabhakar Raghavan
Yahoo! Research 701 First Avenue
Sunnyvale , CA 94089 , USA pragh@yahoo inc.com
ABSTRACT Web search engines use indexes to efficiently retrieve pages containing specified query terms , as well as pages linking to specified pages . The problem of compressed indexes that permit such fast retrieval has a long history . We consider the problem : assuming that the terms in ( or links to ) a page are generated from a probability distribution , how well compactly can we build such indexes that allow fast retrieval ? Of particular interest is the case when the probability distribution is Zipfian ( or a similar power law ) , since these are the distributions that arise on the web .
We obtain sharp bounds on the space requirement of Boolean indexes for text documents that follow Zipf ’s law . In the process we develop a general technique that applies to any probability distribution , not necessarily a power law ; this is the first analysis of compression in indexes under arbitrary distributions . Our bounds lead to quantitative versions of rules of thumb that are folklore in indexing . Our experiments on several document collections show that the distribution of terms appears to follow a double Pareto law rather than Zipf ’s law . Despite widely varying sets of documents , the index sizes observed in the experiments conform well to our theoretical predictions .
Categories and Subject Descriptors H33 [ Information Storage and Retrieval ] : Information Search and Retrieval
General Terms Algorithms , Experimentation , Measurements
Keywords Power law , double Pareto , index size , compression
1 .
INTRODUCTION
We study the following setting : suppose that we have n web pages in our search engine , each having an integer ID in {1 , . . . , n} . We have a collection of adjacency lists each consisting of a set of IDs : the adjacency list may denote the ∗Most of this work was done while the author was visiting Yahoo! Research .
Copyright is held by the International World Wide Web Conference Committee ( IW3C2 ) . Distribution of these papers is limited to classroom use , and personal use by others . WWW 2009 , April 20–24 , 2009 , Madrid , Spain . ACM 978 1 60558 487 4/09/04 . pages containing a term ( thus we have one list per term ) , or the pages linking to a page ( in this case , one list per page ) . Suppose further that the members of each list are drawn independently from a probability distribution . We wish to build a data structure that compactly stores all the adjacency lists so as to efficiently respond to a query of the form “ enumerate the members of a specified list . ” The trivial solution of a membership vector ( a 0/1 n vector with a position for each integer in {1 , . . . , n} , with a 1 denoting that the corresponding integer is in the list and 0 otherwise ) is prohibitively expensive since most adjacency lists in practice are extremely sparse . The solution is to use a standard inverted index , that writes down the integers in each list , using space Θ(k log n ) to store a list with k elements . The literature ( eg , [ 27 ] ) has developed sophisticated coding schemes for encoding the integers in a list to beat the space used by the standard inverted index . How well do these schemes perform , in theory and in practice ?
The frequency of occurrence of terms in documents ( aka pages ) is the subject of Zipf ’s law [ 28 ] , which states that the frequency of the ith most frequent term in a page collection is proportional to 1/i . It is the best known of the class of so called power law distributions , in which the ith most frequent item in a set occurs with probability proportional to 1/iα , where α is a positive real number . Such power laws have been observed in a number of natural , physical , sociological , and computational phenomena [ 24 , 4 , 18 ] . For instance , the number of links to a web page has been observed [ 11 ] to follow a power law with α = 21 Text indexing has a long history of folklore rules ( for instance , that inverted indexes require about one third of the storage of the text being indexed ) . Can a rigorous analysis based on Zipf justify such folklore rules ?
We develop a general analysis technique that applies to lists generated by any distribution ( not necessarily Zipfian ) ; of course , the theoretical bounds for index size depend on the specific distribution . This is the first analysis of compression in indexes under arbitrary distributions . Experiments with text corpora ( Section 3.2 ) suggest that empirical term distributions are in fact somewhat more intricate than Zipfian distributions . The generality of our analytical technique makes it applicable nevertheless , while simultaneously raising the question of how we can better exploit specific properties of Zipfian distributions . 1.1 Scope and outline of this paper
We consider a collection of n pages , each of which is a sequence of terms . The set of all terms in a collection is known as its vocabulary and we denote its cardinality by m . An in
WWW 2009 MADRID!Track : Search / Session : Caching and Indices451 verted index is a data structure built from a collection that facilitates query processing in a search engine for the collection . For each term t in the vocabulary , it maintains the set of pages from the collection that contain t ; this set is known as the posting list ( a DB/IR nomenclature for the adjacency list ) for t . From such a representation the search engine can efficiently identify , for instance , all pages containing both of two query terms t1 and t2 . The reader is referred to [ 3 , 14 , 27 ] for introductory material on inverted indexes .
The space used by an inverted index is a major concern in search engine design , especially in web scale engines where it is commonplace for the index to reside in memory . Accordingly , the postings lists ( which account for the bulk of the space used by an inverted index ) are typically stored in a compressed form , albeit one that permits quick decompression at query time . In Section 1.3 below we cover the principal forms of such compression . The central question we study : what is the space used by such a compressed inverted index if the terms in a page are stochastically generated ? Manning et al . [ 14 ] give a heuristic calculation for estimating the space used by an inverted index for pages that follow Zipf ’s law ; however ( as they point out ) their calculation makes several assumptions that are technically invalid . Our main results ( Theorems 2 and 3 ) give rigorous and tight stochastic bounds on inverted index space for a variety of compression schemes , when the terms in pages follow an arbitrary distribution ( with Double Pareto as a special case ) . We thus significantly generalize , and make rigorous , the calculation in Manning et al . ( Our analysis could also be applied to study the compressibility of web graphs , if the links are generated by a process similar to the terms in our model ; we will not focus on web graph compressibility in this paper . ) We then study the actual term distributions in several benchmark page collections including news articles and wikipedia ( Section 3 ) . We observe that these collections follow double Pareto distributions , rather than simple power laws . We confirm the fit between our analytical bounds and the actual space used by compressed inverted indexes for these collections ( Section 6 ) . Despite a widely varying set of documents , the index sizes observed in the experiments conform well to our theoretical predictions .
1.2 Inverted indexes
As common in inverted indexes , we assume each page has a unique page identifier ( ID ) that is an integer in [ 1 , n ] . The assignment of IDs to pages can depend on a variety of factors ranging from age to some quality measure . Then , the postings list for a term t is a set of integers denoting the pages containing t . For efficient query processing [ 14 ] , it is customary for the postings list to be sorted in increasing order of page ID . Given such an increasing sequence of integers d1 , d2 , d3,··· a common technique is to store instead a compressed version of the sequence of gaps : d1 , d2 − d1 , d3 − d2,··· ; below , we mention some codes commonly used to compress these gaps . Then , at query time , the original postings list can be reconstructed by traversing and decompressing the sequence of gaps while adding them up .
The storage requirement for inverted indexes stems from two sources : ( 1 ) storing the terms in the vocabulary and ( 2 ) storing the postings . It is known that the latter component is the dominant use of storage ; henceforth we focus on the postings alone .
1.3 Codes for compressing the gaps
For concreteness , we focus on the γ and δ codes for compressing the index ; see [ 27 ] for more background on this . For the rest of the paper , we use lg to denote log to the base 2 . The γ code of x ∈ Z + is obtained by representing x− 2bxc in binary , but prefixing this by a unary representation of the binary length of x . Thus we need blg xc for the prefix and blg xc bits for representing x − 2blg xc . Let Sγ(x ) denote the number of bits used by the γ code ; we have
Sγ(x ) = 1 + 2blg xc .
The δ code of x ∈ Z + is obtained by representing x− 2bxc in binary , but prefixing this by a γ code of its length . If Sδ(x ) denotes the number of bits used by this encoding scheme , we have
Sδ(x ) = 1 + blg(x)c + 2blg(1 + blg(x)c)c .
1.4 Notation
We will also use the following notation . H(x ) will denote the binary entropy of x :
H(x ) = −x lg x − ( 1 − x ) lg(1 − x ) .
Hα,n will denote the nth generalized harmonic number : nX
Hα,n =
−α . i
2 . RELATED WORK i=1
Several theoretical explanations have been proposed to explain Zipf ’s law [ 28 ] , most notably by Mandelbrot [ 13 ] and Simon [ 22 ] . Witten and Bell [ 26 ] investigate the quality of the fit obtained by the law in natural language text . Li [ 12 ] showed that random texts exhibit Zipf ’s law like word frequency distribution . Ha et al . [ 9 ] study the extension of Zipf ’s law to word and character n grams .
An observation known as Heaps’ law [ 10 ] estimates vocabulary size m as a function of collection size n : m = m(n ) = K(Ln)θ = Θ(L(n)n)θ , where L(n ) is the average page length , 0 < θ < 1 and K = O(1 ) . Typical values for the parameters K and θ are : 30 ≤ K ≤ 100 and θ ≈ 05
Other term distribution models , including the K mixture and two Poisson model , are discussed by Manning and Sch¨utze [ 15 ] . Williams and Zobel present a detailed study of vocabulary growth in large web collections [ 25 ] . van Leijenhorst and van der Weide formally derive Heaps’ law from a generalized version of Zipf ’s law [ 23 ] ; for a more heuristic derivation , see [ 2 ] . Gelbukh and Sidorov [ 8 ] observe that the coefficients of Zipf ’s and Heaps’ law are language dependent .
Double Pareto distributions can be viewed as the juxtaposition of two power laws with differing exponents , for different ranges of ordinate values . The use of double Pareto distributions to model file size distributions is developed by Reed and Jorgensen [ 19 ] and Mitzenmacher [ 16 ] . For an extensive use of γ and δ codes to represent a web graph , see the work of Boldi and Vigna [ 6 , 7 ] .
While there have been several works on improving the compression on inverted indexes ( eg , [ 5 , 20 , 21] ) , to the best of our knowledge , there has been neither theoretical nor empirical study on the compression of indexes generated by a simple page model . The heuristic calculation in the [ 14 ] , which inspired our work , is the closest . Our work can be thought of as making this calculation rigorous .
WWW 2009 MADRID!Track : Search / Session : Caching and Indices452 3 . TERM DISTRIBUTION IN REAL DATA First we study the empirical distribution of terms in real data . We will argue that empirical term distributions are in fact somewhat more intricate than Zipfian distributions . 3.1 Data
For our experiments , we use five collections , namely , the TREC data corresponding to news articles from the Associated Press ( trec.ap ) , the TREC Wall Street Journal collection ( trec.wsj ) , the Reuters collection ( reuters ) , Wikipedia ( wiki ) , and random web pages ( web ) . We chose these collection for repeatability reasons .
The trec.ap collection consists of articles from the Associated Press . It has 316,504 pages and 299,702 terms . The average page length is 338.55 terms . The sum of the posting list lengths is 55,678,330 .
The trec.wsj collection consists of articles from the Wall Street Journal . It has 173,232 pages and 208,620 terms . The average page length is 387.91 terms . The sum of the posting list lengths is 32,495,850 .
The reuters collection consists of articles from Reuters . It has 806,791 pages and 399,990 terms . The average page length is 218.51 terms . The sum of the posting list lengths is 96,638,730 .
Note that the pages in the above three collections are created by professional writers . For comparison purposes , our last two collections are wiki and web . The first is wiki , the collection of all articles in Wikipedia ( crawled in October 2007 ) ; these articles are edited by online users . It has 2,373,481 pages and 3,962,599 terms . The average page length is 347.48 terms . The sum of the posting list lengths is 369,622,681 .
The second is web , which is a set of 1,179,206 web pages . These web pages were sampled at random from a repository at Yahoo! . Terms were extracted from these web pages after parsing them to discard HTML elements . The total number of terms is 2,880,011 and the sum of the posting list lengths is 250,093,064 . The average page length is 474.64 terms . 3.2 Empirical term distribution
We first study the size of the vocabulary as a function of the size of the collection . Figure 1 ( left ) shows this plot for trecap We then fit a power law to this plot and observe that Heaps’ law holds in trec.ap with θ = 0508 We then study the frequency distribution of this collection . We compute the probability of each term and plot the rank of the term vs the probability . Figure 1 ( right ) shows this plot for trecap We see that the term distribution is clearly not a power law since the plot is not a straight line . We observe a similar phenomenon for the other three collections as well ( Figure 2 ) .
Based on this empirical observation , we postulate that the term distribution is a double Pareto distribution . While it is easy to rule out possibilities such as power law with exponential cutoff or log normal distribution , it is possible that there could be other candidate distributions that fit the observation . We chose double Pareto mainly for sake of mathematical tractability ; moreover , our experimental results seem to endorse this choice . 3.3 Double Pareto distribution three parameters ( α , β , C ) , where 0 ≤ α < 1 < β and C = ω(1 ) . For 1 ≤ k ≤ C , the kth term has probability proportional to
´−α . For k ≥ C , the kth term has prob´−β . Double Pareto distributions ` k
` k ability proportional to offer a neat trade off between the frequent words ( exponent α ) , the rare words ( exponent β ) , and the ratio of their masses ( C ) , expressed below ( proved in Appendix B ) .
C
Theorem 1 . The probability that a randomly drawn term
C
( t has rank r(t ) = k is
Pr[r(t ) = k ] =
( 1 ± o(1 ) ) ( 1−α)(β−1 ) ( 1 ± o(1 ) ) ( 1−α)(β−1 )
β−α β−α
C α−1k−α if k ≤ C , C β−1k−β if k > C .
Also , Pr[r(t ) ≤ C ] = β−1 β−α ± o(1 ) . 1−α
β−α ± o(1 ) , and Pr[r(t ) > C ] =
Fitting a double Pareto distribution to the trec.ap ( also shown in Figure 1 ( right) ) , we observe that α = 0.911 , β = 2.014 , and C = 1900 . This fit was obtained by choosing the parameters of the double Pareto distribution that minimize the sum of squares error . The mass of the distribution until the cut off point C is 0777 The corresponding fits for the other four collections is given in Table 1 . It is clear that web based collections wiki and web are closer to each other and differ from the other non web collections , in terms of many of the parameters . It is also interesting to note that the double Pareto law parameter β is away from 2 and the Heaps’ law parameter θ is away from 0.5 for both wiki and web . This suggests the vocabulary growth in wiki and web follows a different pattern than usual text collections by having a flatter tail .
Collection trec.wsj reuters wiki web
α
0.896 0.869 0.900 0.935
β
2.025 1.963 1.500 1.325
C 700 700 2400 1100 cdf@C 0.672 0.666 0.719 0.684
θ
0.467 0.504 0.725 0.689
Table 1 : Heaps’ ( θ ) and double Pareto parameters ( α , β , C ) for other collections . The β and θ of the web collections ( wiki , web ) are significantly different from the others .
4 . A PAGE GENERATION MODEL
We now state a simple page generation model . We assume each of the n pages to have the same number of terms L = L(n ) ; we will relax this in Section 72 Each page in the collection is independently generated by the following process . Each of the L terms in the page is picked independently according to some fixed probability distribution1 on the vocabulary . For instance , in the power law distribution with exponent α > 1 , the ith term in the vocabulary is chosen with probability pi ∝ i−α . Since the sampling is done with replacement , the number of distinct terms in a page can be less than L . Let T be the m × n Boolean term–document matrix that has a 1 in entry ( i , j ) if the term i is present in the page j ; all other entries are 0 . Let Ri be the random variable
A double Pareto distribution is made up of two power laws stitched together at a cut off point C . Specifically , it has
1We are mostly interested in power law and double Pareto distributions , but our results hold for arbitrary ones .
WWW 2009 MADRID!Track : Search / Session : Caching and Indices453 Figure 1 : Heaps’ law and double Pareto law for trecap The two curves virtually coincide and hence may be hard to distinguish . representing the ith row of this matrix , ie , the row corresponding to the ith most frequent term . Thus the rows of T are in order of decreasing term frequency . Note that row i of T is a bit vector representation of the postings list for term i . When i is a frequent term , this is a space efficient representation of the postings list ; when i is infrequent , the sequence of page IDs ( or equivalently , gaps ) is more efficient . However , these two representations are equivalent .
5 .
INDEX SIZE IN THE MODEL
We begin with the main theoretical result of this paper : the δ compressed index size of pages generated according to the model described in Section 4 is tightly concentrated around its expectation , which is computed in Section 5.1 ; the concentration is computed in Section 52 5.1 Expected size
We first assume that n pages have been generated by the model described in Section 4 . We then measure the compressibility of the term–document matrix of these pages using δ codes . ( Our results apply to γ codes as well , but for simplicity of exposition , we do not present these results . )
Let S be the random variable corresponding to size of the δ compressed term–document matrix of these pages . For a term 1 ≤ i ≤ m , let Si denote the random variable corresponding to the size of row i in the term–document matrix after using δ codes for compression . Let Pi = 1 − ( 1 − pi)L denote the probability that term i is contained in a given page . For 1 ≤ g ≤ n , let Xg,i be the random variable denoting the number of gaps of length g in row i .
Theorem 2 . We have the following : E[Xg,i ] = Pi(1 − Pi)g−1 + P 2 i ( 1 − Pi)g−1(n − g ) , nX mX g=1
E[Si ] =
E[S ] =
( Sδ(g ) · E[Xg,i ] ) , and
E[Si ] . i=1
Proof . The expressions for E[Si ] and E[S ] can be easily proved by the linearity of expectation . Now , we prove the expression for E[Xg,i ] . Consider a generic g and i . Note that none of the first g−1 pages can be the terminal endpoint of a length g gap . The gth page is such an endpoint if and only if none of the previous pages was in row i , ie , if none of them contained the term i . Any other page j > g is an endpoint if and only if it is in the row , along with the ( j − g)th page , and no other page is in between . Thus the statement follows once again from the linearity of expectation .
Notice that the above expressions are purely in terms of Pi ’s , which themselves are determined by the term distribution given by pi ’s and the page size L . Also notice that a simple closed form expression for S in terms of pi ’s does not appear plausible ; this is true even if pi ’s were to follow a power law . Things are made more complicated by the fact that the functions Sδ(x ) and Sγ(x ) have highly discontinuous behavior when x is small . 5.2 Concentration
While the expectation was computed “ row by row ” of the term–document matrix , for obtaining the concentration bounds , we use a more holistic analysis ( using Theorem 7 ; see Appendix A ) .
To be able to do this , we first need to make the following very mild assumption . We assume that the number of terms per page is sub exponential in the number of pages . This is reasonable since in practice , the number of terms per page is much less than the number of pages .
“
“
” ”
Theorem 3 . If L = exp size S is concentrated , ie ,
O n log4 n
, the total compressed
Pr[|S − E[S]| = o(E[S] ) ] = 1 − O(n
−c ) , for each c > 0 .
Proof . Recall the page generation process . For each page , we draw L terms ( with replacement ) from the term distribution . Thus , the random variable S is completely determined by n · L random trials . Note that , no matter how the trials turn out to be , we have S = Ω(n ) , as each page contains at least one term ( by L ≥ 1 ) and the minimum gap cost is 1 .
We split the analysis into the following two cases .
( 1 ) Case L = o(n/ log3 n ) , ie , the page is small . We apply Theorem 7 , letting its X be our total index size S and its Xi , i = 1 , . . . , n · L , be the draw of the ith term ; we choose bi = O(log n ) and D = O(n · L · log2 n ) . The crucial
WWW 2009 MADRID!Track : Search / Session : Caching and Indices454 Figure 2 : Term distribution for trec.wsj , reuters , wiki , web , suggesting double Pareto . observation is that changing a trial could change S by at most a value of the order of the maximum encoding length of a gap , ie , by at most O(log n ) . This follows because a change may only add a new gap and/or remove a gap induced by the previous outcome of the trial . In any case , the maximum difference between the old and the new S is bounded by O(log n ) . Let t0 = n/(L log3 n ) . Since L = o(n/ log3 n ) , we have E[S ] · n/t0 . Using S = Ω(n ) , we have t0 = ω(1 ) . Let t = t = o(E[S] ) . Then , the statement follows from Theorem 7 by choosing F to be the whole probability space so that Pr[F ] = 1 . ( This case could have been proved with the method of bounded differences , but we use Talagrand ’s inequality to have a uniform presentation . ) p
( 2 ) Case L = ω(n/ log4 n ) , ie , the page is large . Call a term i “ frequent ” if pi > 100 log L and “ rare ” otherwise . L Let Prare be the sum of the pi ’s of rare terms . Let Sfreq , Srare be the random variables representing the encoding size of , respectively , the frequent and rare terms’ rows . We have S = Sfreq + Srare . Note that by the Chernoff bound , with probability at least 1−O(n−2 ) , each frequent term will be chosen in every page.2 The expected number of times each page chooses every such term is at least 100 log L = ( 1 − o(1 ) ) · 100 log n . So Sfreq is concentrated .
Let us now consider rare terms and show that Srare is con centrated .
2The −2 exponent can be decreased and depends on the lower bound on the pi ’s of the frequent terms . We are not interested in optimizing this constant , however .
( 2a ) Case Prare ≤ 100 log L
L . Our plan is to bound the deviation of Srare via Theorem 7 , as before ( this time , though , we choose X = Srare ) . We do so by first proving the following , where the weights bi refer to those in Theorem 7 .
Lemma 4 . With probability 1 − O(n−2 ) , only O(n log L ) of the n · L independent random variables making up the term–document matrix will require weights bi = O(log n ) ; all the other weights can be set to 0 .
Proof . Let ξ be the random variable denoting a term in the document that is generated according our model . Suppose ξ corresponds to a frequent term . Then ξ makes zero contribution to Srare and so modifying ξ cannot decrease the value of Srare . On the other hand , if ξ was for a rare term , then we just apply the trivial bound O(log n ) , ie , modifying ξ cannot cost more than O(log n ) towards the compressed index size .
By the Chernoff bound , and the union bound , each single page will choose at most O(log L ) rare terms , with probability at least 1 − O(n−2 ) . Thus the number of rare terms is at most O(n log L ) and the proof is complete .
Given Lemma 4 , we apply Theorem 7 with
D = O(n log L log2 n ) = O
„
«
„ n2 log2 n
« n√ log n
, and t = Θ( p
D log n ) = O to show that with probability 1 − n−2 , the deviation of Srare from E[Srare ] is at most by o(n ) . ( It might very well be that
WWW 2009 MADRID!Track : Search / Session : Caching and Indices455 E[Srare ] is asymptotically much smaller than this deviation , but since S = Ω(n ) and we interested in showing the concentration of S , the above error term suffices . )
( 2b ) Case Prare > 100 log L L . Note that , by the Chernoff bound , each single page will make Θ(Prare · L ) choices among the rare terms with probability at least 1 − O(n−2 ) . Thus , in Theorem 7 we will have at most O(n · Prare · L ) different bi ’s upper bounded by O(log n ) and the rest are set to 0 . Now , choosing D = O(n · Prare · L · log n ) and t = √ Θ( D log n ) will then give us a high probability statement with error term = O( n · Prare · L · log n ) .
√
It only remains to be shown = o(E[Srare] ) . Again by domination and the Chernoff bound , with probability at least 1−O(n−2 ) , there is no page that chooses any single rare term more than O(log L ) times . ( Note that for the union bound on the pages to work out , we need log L = Ω(log n ) , which is ensured by the assumption L = ω(n/ log4 n) . ) Thus , with probability at least 1 − O(n−2 ) , no page will have less log L ) rare terms . This implies that E[Srare ] ≥ than Ω( Prare·L n · Prare · L log L , which is asymptotically larger than the error term .
The reader may wonder why we had to use this particular version of Talagrand ’s inequality to prove concentration . The difficulty is caused by Case ( 2a ) of the previous proof , ie , we needed to bound the error term of a possibly small quantity that depended on many random variables . With Lemma 4 we showed that , while the number of random variables was large , with high probability only few of them had a role in determining our quantity — the version of Talagrand ’s inequality we used allowed us to prove concentration from that observation .
6 . EXPERIMENTAL RESULTS
In this section we present the results of our experiments on the five data sets . We present three sets of the numbers corresponding to various compression schemes . For each collection , we first randomly permute the page ids in order to remove any biases associated with the page numbering .
Predicted and actual estimates . The specific compression schemes we consider are the γ and δ codes , with and without byte alignment.3 The byte aligned versions are denoted with a suffix 8 . The first set of numbers ( called Pareto fit ) are the theoretical estimates , using our estimation of the parameters of the double Pareto law , namely , α , β , and C . We compute the estimates by applying Theorem 2 ; recall that Section 5.2 shows that the actual bounds are tightly concentrated around the expectation . The second set of numbers ( called frequency fit ) are the theoretical estimates using the observed term frequencies . Recall that our analysis is general enough to handle any distribution , not just power laws . We set up a distribution using the term frequencies and once again apply Theorem 2 to estimate the compressed index size . For both these predicted values , for computational ease , we assume that all pages have the same length . The third set of numbers are the actual values , obtained by indexing the collection . 3Byte aligning a bit string of size k means padding ( 8 − k mod 8 ) zero bits to the end ; see [ 27 ] for the practical benefits of byte alignment .
Measures . We consider the following measures that we use to compare the theoretical and empirical estimates . In the following tables , S denotes compressed index size and U denotes uncompressed index size ; thus S/U is the compression ratio . The next two rows give the compression ratios for the index before and after the cutoff C . The final row gives the fraction of the index size used by the top 100 terms ( most probably , the stop words ) .
Results . The results are presented in Table 2 . From the entries in the tables , it can be seen that overall there is a reasonable agreement between both the analytical predictions and actual index metrics . Even though there is arguably a little discrepancy between the predicted values and the actual values , the closeness of the values themselves should be viewed as quite striking . This is because the actual values are computed for the pages in trec.ap , trec.wsj , reuters , wiki , and web collections , not for the models derived from them . The discrepancy is mainly due to two simplifications in our analysis , namely , that all the pages are of the same length ( for computational ease ) and the pages and the terms in a page are generated independently ( model assumption for analytical tractability ) . Clearly these assumptions do not hold in practice .
Nevertheless , in most cases , the frequency fit estimates are closer to the actual values than the Pareto fit estimates ; this is not surprising given that the double Pareto fit is only meant to be an approximation to the term distribution . So , even though the wiki and web pages seemed to have different vocabulary growth mechanisms , they all are comparable in terms of compression ratios . Notice that the compression ratios are worse than what is usually observed in practice . This is because the page ids were randomly permuted ; to achieve the best compression often requires a careful page ordering [ 5 , 20 , 21 ] . In fact , our experiments suggest that without a good page ordering , it is hard to obtain good compression ratios ; this is similar in spirit to the work on web graph compression [ 6 , 7 ] , where locality in url ordering is exploited for improving the compression .
The values in the tables also show that compression is significant below the cutoff C , and further that there is fairly good agreement between prediction and reality . The compression is dramatic below the top 100 terms ( perhaps the stop words ) . It is interesting to note that wiki and web achieve far more compression in this regime than the other three non web collections .
Figure 3 shows the compression obtained for each posting list using Pareto fit , frequency fit , and the actual values . Once again the actual values are more or less aligned with the predicted values .
7 . DISCUSSIONS 7.1 When is the entropy bound achieved ?
In this section we analyze the conditions under which the compressed size of a row of the term–document matrix approaches the entropy bound . We prove that δ codes achieve the entropy bound for row i for ω = Pi = o(1 ) . In practice , this means that we can approximate the expected size of those4 rows by
` log n
´ n
( 1 − o(1))n · H(Pi ) ≤ E[Si ] ≤ ( 1 + o(1))n · H(Pi ) . 4For other rows , we can use the formulas of Theorem 2 .
WWW 2009 MADRID!Track : Search / Session : Caching and Indices456 γ
Pareto fit δ
γ 8
δ 8
γ
Frequency fit δ γ 8
δ 8
γ
δ
γ 8
δ 8
Actual
S ( MB )
61.851
59.906
103.79
99.912
75.368
S/U
S≤C /U≤C S>C /U>C S≤100/S
.360 .251 .762 .070
.349 .263 .663 .083
.478 .399 .770 .177
.460 .399 .686 .184
.424 .261 .774 .056
S ( MB )
26.777
26.777
50.793
49.366
43.243
S/U
S≤C /U≤C S>C /U>C S≤100/S
.299 .181 .620 .091
.299 .201 .567 .103
.426 .349 .636 .229
.414 .349 .591 .235
.417 .197 .664 .055
S ( MB )
95.073
93.311
165.95
160.17
139.79
S/U
S≤C /U≤C S>C /U>C S≤100/S
.313 .198 .637 .113
.307 .215 .567 .131
.455 .368 .701 .242
.440 .368 .640 .251
.422 .214 .687 .073
S ( MB )
731.264
662.282
1075.9
992.58
754.87
S/U
S≤C /U≤C S>C /U>C S≤100/S
.431 .239 .803 .044
.391 .247 .669 .055
.582 .417 .901 .118
.537 .416 .769 .128
.455 .249 .789 .041 trec.ap 71.120
.399 .272 .673 .068 trec.wsj 41.160
.397 .216 .599 .066 reuters 130.43
.394 .230 .603 .088 wiki 680.68
.410 .256 .661 .051 web
117.75
111.45
58.520
54.626
87.723
82.467
.524 .406 .778 .147
.496 .406 .689 .155
.489 .313 .857 .065
.457 .321 .739 .079
.550 .427 .807 .140
.517 .427 .706 .149
69.574
66.188
33.181
31.217
50.077
47.256
.503 .356 .668 .151
.479 .356 .616 .159
.503 .259 .751 .063
.474 .280 .670 .077
.538 .377 .701 .132
.508 .377 .640 .140
214.06
201.15
107.227
98.967
157.464
146.815
.539 .380 .741 .164
.506 .380 .667 .175
.465 .249 .730 .081
.429 .262 .634 .100
.569 .402 .775 .155
.531 .402 .689 .167
1085.8
996.73
505.863
448.814
692.737
627.040
.600 .422 .888 .105
.551 .422 .759 .114
.546 .320 .889 .049
.485 .319 .735 .063
.655 .463 .944 .096
.592 .462 .791 .106
S ( MB )
466.60
416.47
683.54
622.94
404.05
373.82
621.855
581.89
290.626
265.198
420.073
389.012
S/U
S≤C /U≤C S>C /U>C S≤100/S
.464 .186 .781 .035
.414 .201 .657 .045
.595 .369 .852 .110
.542 .369 .739 .121
.403 .184 .661 .041
.373 .199 .577 .050
.543 .369 .747 .126
.508 .369 .671 .135
.487 .243 .747 .049
.444 .257 .644 .062
.587 .398 .788 .112
.543 .398 .699 .121
Table 2 : Predicted and actual index sizes for trec.ap , trec.wsj , reuters , wiki , and web . pc + d 1
Let p = Pi and let ‘ = b 1 us to use the previous asymptotic formula for the remaining gaps’ lengths . pe , and let k be the maximum integer such that ‘k ≤ n , that is n ‘ . Subdivide the sequence ( 0 , . . . , n−1 ) in k +1 subsequences , each of length ‘ , with the possible exception of the last ( having length n− ‘k ≤ ‘ ) . Then , for 0 ≤ i < k , the ith subsequence will be σi = ( i · ‘ , . . . , ( i + 1 ) · ‘ − 1 ) . Subdivide it again in i = ( i·‘ , . . . , i·‘+b ‘ 2c , ( i+1)·‘−1 ) , σ0 of length b ‘ i = ( i·‘+b ‘ pe respectively . The probability that no element is chosen in σ0
2c−1 ) and σ00 2e = d 1
‘ − 1 ≤ k ≤ n pc and d ‘
2c = b 1 i is
( 1 − p ) p c ≥ ( 1 − p ) b 1
1 p = q
0
,
Figure 3 : Term by term compression for trec.wsj using γ codes . and for large n , q0 ≈ e−1 . The probability that at least an element is chosen in σ00 is i 1 − ( 1 − p ) p e ≥ 1 − ( 1 − p ) d 1
1 p = q
00
,
The lower bound is trivial , since each row is compressed by itself . The entropy of the random variable representing the row is exactly n · H(Pi ) ( as each row i is completely determined by a sequence of n independent Pi biased coin flips ) , and the entropy is a lower bound for the expected compression size of any code .
` log n
´
Lemma 5 . Let i be a row such that ω
= Pi = o(1 ) . Then , for all c > 0 , Pr[Si ≤ ( 1+o(1))nH(Pi ) ] = 1−O(n−c ) . n
Proof . If the length g of a gap is ω(1 ) , then its δ compressed size is lg g + o(log g ) . We will start by proving that at least a constant fraction of the gaps will have a size ω(1 ) whp This will let us disregard constant length gaps and will allow
Pk and for large n , q00 ≈ 1 − e−1 . Since these two events are independent , the probability of them happening together is q = q0 · q00 = Θ(1 ) ; let Xi be the indicator variable of their intersection . If Xi = 1 , then at least a gap of length Ω( 1 p ) will end in some element of i=1 Xi is a lower bound for the number of σi . Thus , X = gaps of length Ω( 1 p ) . ‘ − 1 = Θ(np ) . Thus E[X ] = Θ(np ) = ω(log n ) . As the Xi are binary iid random variables , we can apply the Chernoff bound to their sum X , Pr[X ≤ E[X ]
] = exp ( −Ω(E[x] ) ) = exp(−ω(log n ) ) = O(n
‘ − 1 different σi ’s , with n
There exists at least n
−c ) ,
2
WWW 2009 MADRID!Track : Search / Session : Caching and Indices457 for c > 0 . The number of gaps of length Ω(1/p ) will be , whp , at least Ω(np ) . This will allow us to disregard , in the total compression size , the contribution of constant length gaps . p
Let Y denote the random variable counting the number of gaps ( or pages ) in the row . Then , E[Y ] = np . By the Chernoff bound , Pr[|Y − np| >
−c . As p = ω(log n/n ) , we have that , whp , Y = np ± o(np ) . In the rest of the proof we assume that both Y = np ± o(np ) and X = Ω(np ) . The intersection of these two events has probability at least 1 − O(n−c ) , for all c > 0 .
3c n p log n ] ≤ 2 exp(−c log n ) = 2n
PY Consider the gap lengths g1 , . . . , gY ( recall that they are random variables ) induced by these Y pages . Obviously i=1 gi ≤ n . In particular let L be the set of indices of the “ little ” gaps and B the set of indices of the “ big ” gaps — say , for each ‘ ∈ L , g‘ < log 1 p , while for each b ∈ B , gb >
Note that X = Ω(np ) implies |B| = Ω(np ) . Also , by Y = np ± o(np ) , we obtain |L| = O(np ) . The total compression size of the little gaps is asymptotically smaller than the total compression size of the big gaps , as |B| /|L| = Ω(1 ) , and each of the big gaps has a ω(log 1 p ) compression size , while each of the little gaps has a O(log log 1 p ) compression size . q
1 p .
That is,X
‘∈L
X b∈B
Sδ(g‘ ) < o
Sδ(gb )
≤ o
= i=1
YX 0@X X ≤ YX YY t6∈L t6∈L i=1
=
= lg lg gt + o(log gt ) lg gt + o(Si ) lg gi + o(Si ) gi + o(Si ) .
The total space needed by the coding will thus be
Si =
Sδ(gi ) =
Sδ(gt )
Sδ(g‘ )
! 0@X t6∈L i=1
Sδ(gi )
! YX 1A + X 1A + o(Si )
‘∈L
.
! i=1
The arithmetic mean geometric mean inequality states , given a sequence x1 , . . . , xt of non negative reals ,
By applying the inequality to the gi ’s we get t
√ x1 · x2 ··· xt ≤ x1 + x2 + ··· + xt ” Y YY
YX
!Y
“ n t
. gi
≤
,
Y gi ≤
1 Y i=1 i=1 thus , we can obtain Si ≤ Y lg n Y
≤ ( 1 + o(1))nH(p ) .
+ o(Si ) = ( np ± o(np ) ) lg
1 p ± o(p )
+ o(Si ) which proves our claim .
We now address the question : how tight is Lemma 5 , ie , do δ codes achieve the entropy bound outside the range of Pi ’s given by Lemma 5 ? Notice that the upper bound is strict : if Pi = 1 , then the entropy of the row is 0 while we use Ω(n ) bits to represent it . ( If Pi = c , for some constant c < 1 , we would use a number of bits that is larger than the entropy bound by some function of c ) . The lower bound can possibly be improved a little , but it cannot be entirely removed . If Pi ≤ n−c , for c > 1 , then nH(Pi ) ≥ c · n1−c log n ; on the other hand , E[Si ] = ( 1 + o(1 ) ) · n1−c log n , since we pay at most ( 1+o(1))·log n bits per page , and the expected number of pages is ≤ n1−c ) . Thus the ratio between the expected length of our encoding of the row and its entropy is ≤ 1/c . The above might appear to contradict the entropy lower bound . However , this is not the case . Indeed , if a term is not chosen by any page , we completely ignore that term while building the index , we don’t encode its row at all ( instead of encoding an empty row ) . So if Pi ≤ n−c , for c > 1 , with probability 1−o(1 ) , row i will be empty and we won’t need to encode it . On the other hand , if Pi = ω(log n/n ) , with high probability the row will be non empty , ie , we will have to encode it and the entropy bound can be used to lower bound the encoding size . 7.2 Varying length pages
So far , we have assumed that the length of each page is fixed to be some integer L . We remark here that the results hold even if we allow pages to have varying sizes , under mild assumptions . Suppose pages are generated as follows . For each i = 1 , . . . , n , let Li be a fixed integer . Page i is generated by sampling Li terms from the term distribution . Suppose each Li = O(n1− ) ( or even i=1 Li = o(n2/ log3 n) ) , then we can obtain the same concentration result of Theorem 3 : the index size is concentrated around its expectation . We omit the details in this version .
Pn
The expression for E[Xg,i ] would become significantly more complicated , though . Indeed , two different pages may have different probabilities of containing the same term . So E[Xg,i ] would become
24Pi,k−g · Pi,k g−1Y
( 1 − Pi,j ) , nX k=g+1
+ Pi,g k−1Y j=k−g+1
35 +
( 1 − Pi,j )
E[Xg,i ] = j=1 where Pi,j is the probability that page j contains term i : Pi,j = 1 − ( 1 − pi)Lj . 7.3 Computational costs
We compare the computational costs of answering a query ( ie , a conjunction or disjunction of terms ) using an uncompressed vs a compressed index . There are ( at least ) two ways of assessing the computational efficiency : the number of bits read and the number of basic operations performed by the CPU ( say , arithmetic , logical , shift operations ) . It is easy to see that for a k term query , where the terms are generated independently from the term distribution , we have :
Lemma 6 . The expected number of bits read is
( 1 − pi)k)E[Si ] . Here , Si is the size of the posting list for term i . This follows from the independence assumption and concrete bounds for
Pn i=1(1 −
WWW 2009 MADRID!Track : Search / Session : Caching and Indices458 compressed indexes can be obtained using Theorem 2 . ( Unfortunately , one can show that this random variable is not concentrated . ) This brings us to an interesting question : how to achieve the best computation compression trade off ? Suppose the term–document matrix is stored in a sparse , but uncompressed form ( each gap is encoded using dlg ne bits ) . Then , reading a word would require dlg ne operations . Suppose we compress the gaps using δ codes . Decoding a gap of length ‘ uses Θ(log ‘ ) operations . Asymptotically , this could be smaller than the cost paid for uncompressed gaps — except when the gaps are very large , Ω(n ) say : in this case leaving them uncompressed is a better choice .
To summarize , to achieve a good computational compression trade off , a reasonable strategy could be : ( i ) either remove very frequent terms ( stop words ) or store their rows uncompressed in term–document matrix , ( ii ) store the rows corresponding to rare terms without compression , and ( iii ) δ compress the gaps of the posting lists of the other terms ( that , overall , make up the largest part of the corpus ) .
8 . CONCLUSIONS
In this paper we analyze the compressibility of a term– document matrix , where the pages are generated by a simple model in which the terms are chosen independently from a given distribution . We show that the size of the index is tightly concentrated around its expectation . We then analyze five data sets and use a double Pareto law to model the distribution of terms in the pages . The parameters of the double Pareto distribution are used to predicate various aspects of the size of the compressed index . We show that the predicted index sizes closely match the actual index sizes . It will be interesting to consider more nuanced page generation models and carry out similar analysis .
Although our work was primarily motivated by compressibility of web indexes under the Zipfian term distribution , our analysis did not use any specific properties of the Zipf distribution . The general problem of analyzing algorithms and data structures under power law inputs is , however , an important area that demands further study . First , it appears that in a wide variety of practical settings , power laws are quite common . Second , analyzing algorithms and data structures under power law inputs demands the development of new tools , beyond what is available in the classical probabilistic analysis of algorithms . This is because classical probabilistic analysis depends substantially on inputs drawn identically and independently from a common generative process . On the other hand , such iid inputs do not yield power laws . Accordingly , the analysis of algorithms and data structures with power law inputs demands new tools in probabilistic analysis .
9 . REFERENCES [ 1 ] T . M . Apostol . Introduction to Analytic Number Theory .
Springer Verlag , 1976 .
[ 2 ] R . Baeza Yates and G . Navarro . Block addressing indices for approximate text retrieval . Journal of the American Society for Information Science , 51(1):69–82 , 2000 .
[ 3 ] R . Baeza Yates and B . Ribeiro Neto . Modern Information
Retrieval . Addison Wesley , 1999 .
[ 4 ] A L Barabasi . Linked : How Everything is Connected to
Everything Else and What It Means . Penguin Group , 2003 .
[ 5 ] D . Bladford and G . Blelloch . Index compression through document reordering . In Proceedings of the Data Compression Conference , pages 342–351 , 2002 .
[ 6 ] P . Boldi and S . Vigna . The Webgraph framework i : Compression techniques . In Proceedings of the 13th International Conference on World Wide Web , pages 595–602 , 2004 .
[ 7 ] P . Boldi and S . Vigna . The Webgraph framework ii : Codes for the world wide web . In Data Compression Conference , 2004 .
[ 8 ] A . Gelbukh and G . Sidorov . Zipf and Heaps laws’ coefficients depend on language . In Proceedings of the 2nd International Conference on Computational Linguistics and Intelligent Text Processing , pages 332–335 , 2001 .
[ 9 ] L . Q . Ha , E . I . Sicilia Garcia , J . Ming , and F . J . Smith .
Extension of Zipf ’s law to word and character n grams for English and Chinese . Computational Linguistics and Chinese Language Processing , 8(1):77–102 , 2003 .
[ 10 ] H . S . Heaps . Information Retrieval : Computational and
Theoretical Aspects . Academic Press , New York , 1978 .
[ 11 ] R . Kumar , P . Raghavan , S . Rajagopalan , and A . Tomkins .
Trawling the Web for emerging cyber communities . Computer Networks , 31(11–16):1481–1493 , 1999 .
[ 12 ] W . Li . Random texts exhibit Zipf ’s law like word frequency distribution . IEEE Transactions on Information Theory , 38(6):1842–1845 , 1992 .
[ 13 ] B . Mandelbrot . An information theory of the statistical structure of language . In W . Jackson , editor , Communication Theory , pages 486–502 . Academic Press , 1953 .
[ 14 ] C . D . Manning , P . Raghavan , and H . Sch¨utze . Introduction to Information Retrieval . Cambridge University Press , 2008 .
[ 15 ] C . D . Manning and H . Sch¨utze . Foundations of Statistical Natural Language Processing . MIT Press , Cambridge , MA , 1999 .
[ 16 ] M . Mitzenmacher . Dynamic models for file sizes and double
Pareto distributions . Internet Mathematics , 1(3):305–333 , 2003 .
[ 17 ] M . Molloy and B . Reed . Graph Coloring and the
Probabilistic Method . Springer Verlag , 2002 .
[ 18 ] M . Newman , A L Barabasi , and D . J . Watts . The
Structure and Dynamics of Networks . Princeton University Press , 2006 .
[ 19 ] W . J . Reed and M . Jorgensen . The double
Pareto lognormal distribution A new parametric model for size distributions . Communications in Statistics : Theory and Methods , 33(8):1733–1753 , 2004 .
[ 20 ] W Y Shieh , T F Chen , J . J J Shann , and C P Chung .
Inverted file compression through document identifier reassignment . Information Processing and Management , 39(1):117–131 , 2003 .
[ 21 ] F . Silvestri , R . Perego , and S . Orlando . Assigning document identifiers to enhance compressibility of web search indexes . In Proceedings of the Symposium on Applied Computing , pages 600–605 , 2004 .
[ 22 ] H . A . Simon . On a class of skew distribution functions .
Biometrika , 42:425–440 , 1955 .
[ 23 ] D . C . van Leijenhorst and T . P . van der Weide . A formal derivation of Heap ’s law . Information Sciences , 170:263–272 , 2005 .
[ 24 ] D . Watts . Six Degrees : The Science of a Connected Age .
W . W . Norton , 2003 .
[ 25 ] H . E . Williams and J . Zobel . Searchable words on the web .
International Journal on Digital Libraries , 5(2):99–105 , 2005 .
[ 26 ] I . H . Witten and T . C . Bell . Source models for natural language text . International Journal Man Machine Studies , 32(5):545–579 , 1990 .
[ 27 ] I . H . Witten , A . Moffat , and T . C . Bell . Managing
Gigabytes : Compressing and Indexing Documents and Images . Morgan Kaufmann , 1999 .
[ 28 ] G . K . Zipf . Human Behavior and the Principle of Least
Effort . Addison Wesley , Cambridge MA , 1949 .
WWW 2009 MADRID!Track : Search / Session : Caching and Indices459 That is ,
+ 2(1− Pr[F] ) .
( T + H )
−1 =
APPENDIX A . TAIL BOUNDS
To prove concentration bounds ( ie , to prove that random variables are strongly concentrated around their expectation ) , we use the following version of Talagrand ’s inequality [ 17 ] .
Theorem 7
( Talagrand ’s inequality ) . Let X be a non negative random variable , not identically 0 , determined by n independent random variables X1 , . . . , Xn such that X = f ( X1 , . . . , Xn ) . Fix some D > 0 , and let F be the event that for the outcome x = ( x1 , . . . , xn ) of the trials , there exists a list of non negative weights b1 , . . . , bn such that
Pn 2 . for any outcome y , it holds X(y ) ≥ X(x ) −P i ≤ D , and i=1 b2
1 . bi . xi6=yi
Then , for any 0 ≤ t ≤ E[X ] , Pr[|X − E[X]| > t + 60
√
D ] ≤ 4 exp
„
«
− t2 8D
We will also use the well known Chernoff bound to prove the concentration bounds .
Theorem 8
( Chernoff bound ) . Let X1 , . . . , Xn be iid
Bernoulli random variables , 1 with probability p and 0 otherwise . Let X = i=1 Xi . Then
Pn
Pr[|X − np| > t ] < 2 exp
„
«
− t2 3np
.
B . PROOF OF THEOREM 1
` k
´−α .
PC PC Z C
C k=1 bution , H =
Proof . Let us use H to denote the “ head ” of the distrik=1 k−α , then H = C αh . By applying simple −αdx ≤ C 1−α 1 − α
−αdx ≤ h ≤ 1 +
If h = calculus , C 1−α − 1
Z C
1 − α
( x + 1 )
≤ x
1
0 we obtain thus
Pm h =
C 1−α 1 − α
± O(1 ) ,
H =
C 1 − α
± O(C α ) =
C 1 − α
Pm
± o(C ) .
` k
´−β . Let
Now let us consider the “ tail ” T = t =
It is known [ 1 ] that , for β > 1 , k=C+1 k−β . sX
−β = ζ(β ) − s1−β β − 1 k k=1 k=C+1
C
± O(s
−β ) .
This implies t =
−β − CX
−β k k mX k=1 k=1
= ζ(β ) − m1−β β − 1 ± O(m1−β + C
± O(m
=
C 1−β β − 1
−β ) ,
−β ) − ζ(β ) +
C 1−β β − 1
± O(C
−β ) and
T = C βt =
C β − 1
± O(m1−βC β + 1 ) =
C β − 1
± o(C ) . where the third equality is justified by m1−β = o(C 1−β ) , which holds as C = o(m ) . The normalizing factor of the probabilities is ( T + H)−1 .
„ „
+
C β − 1
C 1 − α C(β − α ) ± o(C ) ( 1 − α)(β − 1 ) ( 1 − α)(β − 1 ) C(β − α ) ± o(C ) · ( 1 − α)(β − 1 ) 1 C
β − α
=
=
=
«−1
± o(C )
«−1
± o(C
−1 ) .
This proves the first part of the theorem .
The probability that a newly drawn term t is chosen within the first c terms ( that is , the probability that it happens to be in the “ head ” ) is Pr[r(t ) ≤ C ] =
H
T + H C
1−α ± o(C )
C
1−α + C
β−1 ± o(C )
1−α ± o(1 )
1
β−1 ± o(1 ) ·
1
1−α + 1 1 ± o(1 ) 1 − α β − 1 ± o(1 ) β − α ± o(1 ) β − 1 ± o(1 ) . β − α
=
=
=
=
=
The claim follows .
( 1 − α)(β − 1 )
( β − 1 ) + ( 1 − α ) ± o(1 )
WWW 2009 MADRID!Track : Search / Session : Caching and Indices460
