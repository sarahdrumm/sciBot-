Learning to Recognize Reliable Users and Content in
Social Media with Coupled Mutual Reinforcement
Jiang Bian
College of Computing
Georgia Institute of Technology jbian3@mailgatechedu
Yandong Liu
Math & Computer Science
Emory University yliu49@emory.edu
Ding Zhou Facebook Inc . dzhou@facebook.com
Eugene Agichtein
Math & Computer Science
Emory University eugene@mathcsemoryedu
Hongyuan Zha
College of Computing Georgia Institute of
Technology zha@ccgatechedu
ABSTRACT Community Question Answering ( CQA ) has emerged as a popular forum for users to pose questions for other users to answer . Over the last few years , CQA portals such as Naver and Yahoo! Answers have exploded in popularity , and now provide a viable alternative to general purpose Web search . At the same time , the answers to past questions submitted in CQA sites comprise a valuable knowledge repository which could be a gold mine for information retrieval and automatic question answering . Unfortunately , the quality of the submitted questions and answers varies widely increasingly so that a large fraction of the content is not usable for answering queries . Previous approaches for retrieving relevant and high quality content have been proposed , but they require large amounts of manually labeled data – which limits the applicability of the supervised approaches to new sites and domains . In this paper we address this problem by developing a semi supervised coupled mutual reinforcement framework for simultaneously calculating content quality and user reputation , that requires relatively few labeled examples to initialize the training process . Results of a large scale evaluation demonstrate that our methods are more effective than previous approaches for finding high quality answers , questions , and users . More importantly , our quality estimation significantly improves the accuracy of search over CQA archives over the state of the art methods .
Categories and Subject Descriptors H33 [ Information Systems ] : Information Search and Retrieval ; H35 [ Information Systems ] : On line Information Services ; G22 [ Discrete Mathematics ] : Graph Theory General Terms Algorithms , Measurement , Experimentation Keywords Community Question Answering , Authority and Expertise in Online Communities Copyright is held by the International World Wide Web Conference Committee ( IW3C2 ) . Distribution of these papers is limited to classroom use , and personal use by others . WWW 2009 , April 20–24 , 2009 , Madrid , Spain . ACM 978 1 60558 487 4/09/04 .
1 .
INTRODUCTION
Many information needs are better served by explicit , selfcontained answers than by lists of results or documents . Recently , Community Question Answering ( CQA ) portals emerged that allow users to ask natural language questions that are answered by other users of the system . For millions of users , and tens of millions of questions posted on popular CQA sites such as Yahoo! Answers1 , and Naver2 , this method of information seeking has proven to be more successful than general purpose Web search . For example , Yahoo! Answers already has tens of millions of users , and stores hundreds of millions of answers to previously asked questions , and serves millions of visits each day . These databases of past questions and respective answers are proving to be a valuable resource for specific information needs not well served by general purpose Web search engines . Unfortunately , the quality , accuracy , and comprehensiveness of the content in the CQA archives varies drastically , and a large portion of the content is not useful for answering user queries . Not surprisingly , the reputation and expertise of the contributors can provide crucial indicators into the quality and the reliability of the content . The reputation of the contributor could also be a valuable factor for ranking search results from CQA repositories , as well as for improving the system interface and incentive mechanisms .
In a CQA environment , schematically shown in Figure 1 , there are three sets of connected entities : users , answers and questions . In addition to the intuitive connection between questions and answers , users are also connected with two other sets of entities by both expressing specific information needs via posting questions , and by responding to existing question via posting their answers to questions .
Unfortunately , existing methods for estimating content quality in CQA either require large amounts of supervision ( eg , [ 2 ] ) or focus on the network properties of the CQA without considering the actual content of the information exchanged ( eg , [ 17] ) . We observe that user reputation and the quality of the content they produce are often tied together in a mutually reinforcing relationship . Building on this observation , we propose a general framework for simultaneously calculating the answer and question quality and
1http://answersyahoocom/ 2http://wwwnavercom/
WWW 2009 MADRID!Track : Data Mining / Session : Graph Algorithms51 features to identify high quality content in CQA . Recently , Bian et al . [ 3 ] developed a ranking system to retrieve relevant and high quality answers , but they did not explicitly integrate content quality and user reputation information into the ranking process . While these models have shown to be quite effective for finding high quality [ 2 ] and relevant [ 3 ] content , they do not explicitly model user reputation , and require substantial amounts of manual supervision .
At the same time , there has been a long standing interest in modeling authority , reputation and expertise in social networks and communities . Link based ranking algorithms have been shown to be successful in the context of evaluating quality of Web pages . Two of the most prominent link analysis algorithms are PageRank [ 12 ] and HITS [ 11 ] . Variations of PageRank and HITS have already been applied in many contexts , especially for propagating reputation and finding experts in the mutual reinforcement process . Guha et al . [ 7 ] and Ziegler [ 20 ] study the problem of propagating trust and distrust among users in social media , while considering trust as a transitive property in network relationships . Expert finding is also an active area of research , where researchers also take advantage of mutual reinforcement principle . Zhang et al . [ 17 ] analyze data from an on line forum , seeking to identify users with high expertise . They apply both ExpertiseRank and HITS to identify users with high expertise . Jurczyk and Agichtein [ 10 ] show an application of the HITS algorithm to a CQA portal , especially the user interactions graph , and show a positive correlation between authority calculated with the HITS algorithm and answer quality . Campbell et al . [ 5 ] compute the score of HITS over the user user graph in a network of e mail exchanges , showing that it is more correlated to quality than other metrics . Zhou et al . [ 19 ] propose a method for co ranking authors and their publications using their networks . Dom et al . [ 6 ] also study the performance of several link based algorithms to rank people by expertise on a network of e mail exchanges . Although link based and probabilistic approaches have been shown to be successful in ranking entities on the graph , most of them focus on ranking only one type of entity , and few of them utilize other properties of the entities except with link structure . Building on the previous work , our framework is based on the model of network relationships in social media [ 13 ] and exploits the mutual reinforcement principle [ 16 ] . We propose a mutual reinforcement framework for ranking sets of entities , specifically applied to the CQA network that connects users , questions , and answers . Our approach take advantage of mutually reinforcing relationship to rank various sets of entities simultaneously , and in this approach , many other features are used besides link structure .
3 . LEARNING CONTENT QUALITY AND USER REPUTATION IN CQA
Answer and question quality are crucial to information retrieval in community question answering . It has been noted in previous work that user reputation is expected to correlate with answer and question quality but the relationship between user reputation and content quality is not straightforward . “ Authoritative ” users may provide poor answers , and “ poor ” users may occasionally provide excellent answers [ 2 , 1 ] . In this section , we will explore a learning based approach to calculate answer and question quality as well as user rep
Figure 1 : Network of interactions in CQA connecting users , questions and answers user reputation based on their network relationships , coupled with the individual quality/reputation related features . In our framework , the CQA interactions are viewed as composite bipartite graphs where each pair of entity types ( eg , users and the answers they generate ) can form one bipartite graph . We develop a machine learning approach that starts with a set of known labels for users and answers , and exploits the mutual reinforcement between the connected entities in each bipartite graph to compute the respective quality and reputation scores simultaneously , iteratively refining the labels for the unlabeled entities . Our specific contributions include :
• A mutual reinforcement framework to calculate the quality and reputation scores of multiple sets of entities in a network relationship , simultaneously .
• A semi supervised learning method to identify highquality content and users in CQA that dramatically reduces the required amount of manually labeled data for training , while outperforming state of the art supervised methods .
• An important practical application of our framework to enhance search over CQA archives , by incorporating the predicted content quality into the ranking .
Next we review related work , which sets the context for our new method that we introduce in Section 3 .
2 . RELATED WORK
Our work builds on the research in traditional automatic question answering ( QA ) [ 15 ] , and on the more recent research area of QA over the Web [ 4 ] . However , question answering over Community QA archives is different from traditional QA . The most significant difference is that we are attempting to retrieve answers from a social media archive with a large amount of associated user information [ 2 ] . This kind of information ( such as user reputation in a QA community ) can be crucial to the answer quality and retrieval accuracy .
Due to the explosive rise in popularity of Yahoo! Answers and similar sites , CQA has become an active area of research . Jeon et al . [ 8 ] presented a machine translation model to find similar questions from a community QA service , but did not take quality of answers into consideration . Su et al . [ 14 ] analyzed the quality of answers in QA portals . Jeon et al . [ 9 ] built a model for answer quality based on features derived from the specific answer being analyzed . Agichtein et al . [ 2 ] presented a supervised approach to mining user interaction and content based lexical
WWW 2009 MADRID!Track : Data Mining / Session : Graph Algorithms52 utation in CQA , simultaneously . We will focus on the specific characteristics of Yahoo! Answers and discuss how to employ coupled mutual reinforcement principle to learn answer and question quality and user reputation . We will start with a more precise definition of the problem of calculating answer and question quality and user reputation , and then describe the mutual reinforcement principle between these three types of entities in CQA . Then we present a coupled mutual reinforcement framework to model answer and question quality and user reputation . Based on mutual reinforcement in CQA network , we apply a semi supervised regression based approach to the problem of learning answer and question quality and user reputation . 3.1 Problem Statement In a CQA system , there are three distinct types of entities : users U , answers A , and questions Q . Questions and answers are posted by a diverse community of users . And one question can solicit several answers from a number of different users . We can further categorize users into two subsets : askers Uq and answerers Ua . Note that there can be an overlap between askers and answerers that is , a user may post both questions and answers . Before proceeding , we define question and answer quality and user reputation more precisely :
Definition 1 . Question Quality : a score between 0 and 1 indicating a question ’s effectiveness at attracting highquality answers .
Definition 2 . Answer Quality : a score between 0 and 1 indicating the responsiveness , accuracy , and comprehensiveness of the answer to a question .
In previous work , question and answer quality were defined in terms of content , form , and style , as manually labeled by paid editors [ 2 ] . In contrast , our definitions focus on question effectiveness , and the answer accuracy – both quantities that can be measured automatically and do not necessarily require human judgments .
Definition 3 . Answer reputation : a score between 0 and 1 , indicating the expected quality of the answers posted by a user .
Definition 4 . Question reputation : a score between 0 and 1 , indicating the expected quality of the questions posted by a user .
Clearly , the definitions above are somewhat “ circular ” in that the reputation of the user depends on the quality of the questions or answers they post–where quality , in turn , can be influenced by the user reputation . In fact , we will soon show how to exploit this relationship in our mutual reinforcement framework . We now state our problem more formally : Problem : Predicting Content and Contributor Quality
Given a CQA archive , determine the quality of each question and answer and the answer reputation and question reputation of each user , simultaneously , with minimal manual labeling .
In the rest of this section we will first introduce the “ coupled mutual reinforcement principle ” for content quality and user reputation in community question answering . We will then present our novel semi supervised , regression based approach , based on the mutual reinforcement idea .
3.2 Coupled Mutual Reinforcement Principle Recall that our goal is to identify high quality questions and answers , and high reputation users , simultaneously . We now state the mutual reinforcement principle that underlies our approach to solving this problem with the minimum of manual labeling :
An answer is likely to be of high quality if the content is responsive and well formed , the question has high quality , and the answerer is of high answer reputation . At the same time , a user will have high answer reputation if she posts highquality answers , and high question reputation if she tends to post high quality questions . Finally , a question is likely to be of high quality if it is well stated , is posted by a user with high question reputation , and attracts high quality answers .
Before we can turn this idea into an algorithm , we need to represent our setting more precisely . Recall that CQA systems are centered around three entities and their relationships : Users ( U ) , questions ( Q ) , and answers ( A ) . The relationships between these entities are illustrated in Figure 2 . In particular , to represent the relationships between answers and their authors , we can use a bipartite graph with an edge between each user and the answers that they post . Note that it is convenient to partition these graphs according to the question thread – that is , to consider subgraphs that involve answers to a particular question . Similarly , we can represent the relationship between askers and the questions they post by a bipartite graph with an edge connecting an asker to their posted question(s ) . We consider the sets of bipartite graphs that share the same question to be coupled .
We denote the whole graph in Figure 2 as G(U,A,Q , MUA , MUQ , MQA ) where MUA = [ mij ] is the |U| by |A| matrix containing all the pairwise edges , ie , mij = 1 if there is an edge between user ui and answer aj . Similarly , MUQ and MQA are the matrices containing pairwise edges representing the association between users and questions they post , and question and the posted answers , respectively . Note that users may appear in both the asker and the answerer sets ; however , we purposefully remove this additional coupling by separating the “ asker ” from the “ answerer ” personas of each user and modeling the reputation of each persona separately .
Now we can state the mutual reinforcement principle introduced earlier more precisely , as the set of four simultaneous equations governing the answer reputation ya u and question reputation yq u of a user u , and the corresponding answer quality ya and question quality yq respectively : muaya u ∝ ya ya ∝ α muaya u + ( 1 − α)yq(∼a ) u∼a u∼q a∼u q∼a
( 1 )
( 2 )
( 3 )
( 4 ) u ∝ yq muqyq yq ∝ γ maqya + ( 1 − γ)yq u(∼q ) where u ∼ a or u ∼ q represents an edge between a user and her answers , or user and her questions ; yq(∼a ) denotes the
WWW 2009 MADRID!Track : Data Mining / Session : Graph Algorithms53 is also incorporated into the log likelihood function ( Section 332 ) Finally , we summarize a CQA MR algorithm which can both fit the model and learn on answer and question quality and users reputation ( Section 333 )
331 Features In a CQA system , there are several complementary feature sets for answers , questions and users , respectively . Table 1 shows a list of features for answers , questions and users , which form the feature space of answers , X(A ) , questions X(Q ) and users , X(U ) . We denote one answer as xa in answer feature space X(A ) , one question as xq in X(Q ) and one user as xu in X(U ) .
332 Learning Answer and Question Quality and User
Reputation Using Coupled Mutual Reinforcement
Given an answer a , a question q and a user u described by feature vectors xa , xq and xu , let the probability of them being a good answer , good question , good asker or good answerer be P ( xa ) , P ( xq ) , Pqst(xu ) and Pans(xu ) , respectively . In the following , we will describe a generic approach to learning all these probabilities following the same way . We use P to denote any of P ( xa ) , P ( xq ) , Pqst(xu ) or Pans(xu ) and use x to represent the corresponding feature vector .
Using logistic regression , we model the log odds of P ( x ) by the following linear models : log
P ( x )
1 − P ( x )
= βT x
( 9 )
Figure 2 : with user question bipartite graph .
|Q| coupled bipartite graphs connecting quality of answer a ’s question ; yq u(∼q ) denotes the questionreputation of the user who ask question q ; the symbol ∝ stands for “ proportional to ” ; and α and γ are proportionality constants .
To simplify the notation , we collect answer reputation and question reputation scores of all users into vectors ya u and yq u respectively , and collect all answer and question quality scores into vector ya and yq , resulting in the simplified form of the same principle :
UAya ya u = M ya = αM TUAya yq UQyq u = M yq = γM TUQyq u + ( 1 − α)M TQAyq u + ( 1 − γ)MQAya
( 5 )
( 6 ) ( 7 )
( 8 ) where M T stands for the matrix transpose of M ; M M UQ is derived from MUA and MUQ as for each m mij|A| ( mij ∈ MUA ) and for each m m ij = ij = mij|Q| ( mij ∈ MUQ ) . m j=1 mij ij ∈ M ij ∈ M
UA and UA , UQ , j=1 mij
We can now turn the mutual reinforcement principle into a semi supervised algorithm to estimates content quality and user reputation , as we describe next . 3.3 CQA MR : Coupled Semi Supervised Mu tual Reinforcement
Due to the tight correlation and connection between those three sets of entities in CQA ( questions , answers and users ) , we apply a mutually reinforcing approach to learn the questionreputation and answer reputation of users as well as the quality of questions and answers , simultaneously . In the following , we first describe the features for learning question and answer quality and user reputation ( Section 331 ) Then , we present a logistic regression approach for learning question and answer quality and user reputation ( Section 332 ) However , we are given very few labels on answers and questions quality and users reputation in CQA . Thus , we apply the discussed coupled mutual reinforcement relationship for semi supervised learning on answers and questions quality and users reputation , and such relationship where β are coefficients of the linear models . When given sufficient labeled instances , one can compute those coefficients by maximizing the corresponding log likelihoods , say LL(X ) for Equation 9 :
LL(X ) = yβT x − log(1 + eβT x )
( 10 ) x∈X where y ∈ {0 , 1} are the label of instance vector x ; X denotes the any of U , Q or A , which corresponds to the type of instance x .
We can see that the above learning model depends exclusively on the corresponding feature space of the specific type of instances , ie,answers , questions or users . Thus the quality of questions and answers are learned only based on answer related or question related features while the reputation of users is estimated based on user related features . After adapting the coupled mutual reinforcement principle between the answer and question quality and the user reputation , showed in Equation 5 , 6 , 7 and 8 , we are able to measure the conditional log likelihood of observing one label set given some others belonging to different kinds of but associated entities . We use y to denote the current labels for x and use y to denote new expected labels given the other kinds of old labels are known . We represent Yd as the set of different types of entity associated with y . For instance , based on Equation 6 , the set Yd of answer entity u , yq} , and in Equation 7 , the set Yd of users for ya is {ya questions yq u is {yq} .
We use KL divergence to measure the conditional log
WWW 2009 MADRID!Track : Data Mining / Session : Graph Algorithms54 Table 1 : Features Spaces : X(Q ) , X(A ) and X(U )
Words shared between question and answer
Number of words of question subject Number of words of question detail Date and time when the question was posted Number of stars received earned for this question Number of answers received for this question
Question Feature Space X(Q ) Q : subject length Q : detail length Q : posting time Q : question stars Q : number of answers Answer Feature Space X(A ) A : overlap A : number of comments Number of comments added by other participants A : total thumbs up A : total thumbs down User Feature SPace X(U ) U : total points U : questions asked U : questions resolved U : total answers U : best answer U : stars U : thumbs up ratio U : thumbs down ratio U : indegree U : outdegree U : hub score U : authority score
Total points earned over lifetime community Number of questions asked Number of questions resolved Number of posted answers Number of answers that were selected as “ best answer ” Number of stars the user receive The ratio of thumbs up votes the user posted before The ratio of thumbs down votes the user posted before number of other users whose questions are answered by the user number of other users who answer the questions posted by the user the hub score of the user computed by HITS the authority score of the user computed by HITS
Total number of thumb up votes for the answers Total number of negative votes for the answers as βa u ) for the logistic regression model :
( cid:181 ) u− a u = βa
β
∂2L(U ) ∂βa u∂βa u
T
( cid:182)−1 ∂L(U )
∂βa u likelihood of y given associated Yd :
|X|
LL(y|Yd ) = −
1 − y(i ) 1 − y(i ) ( 11 ) And now we can extend the objective function from the
− ( 1 − y(i ) ) log y(i ) y(i ) y(i ) log i=1 original log likelihood in Eq 10 to the following :
L(X ) = LL(X ) + σLL(y|Yd )
( 12 ) where σ is a prescribed weight parameter . This equation represents the combined log likelihood for learning the probability of each type of entity . Note that the KL divergence can be combined with the original log likelihood naturally because both are log likelihood measured on probability distributions so are of the same units .
333 Fitting Models–Algorithm Next , we show how to fit the above models and how to solve the parameter estimation problem . The idea is to start with uniform distributions for P ( xa ) , P ( xq ) , Pqst(xu ) and Pans(xu ) , and then iteratively update them to increase the likelihood based on their coupled mutually reinforcing relationship . In the following , we will first describe a generic approach to fitting any of the four logistic regression models for P ( xa ) , P ( xq ) , Pqst(xu ) or Pans(xu ) in the mutual reinforcement framework . Then , we will describe an algorithm to learn answer and question quality and question reputation and answer reputation of user simultaneously .
We now describe how to fit the logistic regression model in more detail . As an example , consider fitting the model for the answer reputation of users . From the current answer quality ya , we use Equation 5 to calculate ya u . Note that we will keep the given labeled scores to the corresponding users . Then , given user related features X(U ) and ya u , we use the Newton Raphson update to compute the new βa u ( denoted
( 13 )
( 14 )
( 15 )
Let Xu denote the matrix of xu values , pa fitted probabilities of users and Wa u(xi)(1 − pa element ( i , i ) equal to pa Raphson step is thus : u the vector of the u a diagonal matrix with u(xi) ) , then the Newton a u = βa u + ( XT u Wa uXu )
β
We rewrite this equation as :
−1XT u ( ya u − pa u ) a u = ( XT
β
−1XT u Wa uza u u Wa −1(ya uXu ) u − pa u + Wa u ) is the residual . Using u u , we are able to calculate the new answer where za u = Xuβa the new value of βa reputation of users ya u .
Then we can apply the same approach to fit the logistic regression model for the answer and question quality and the question reputation of users ( denoted as βa , βq and βq u , respectively ) .
Based on the proposed method of fitting logistic regression models , we present the following algorithm CQA MR ( Alg . 1 ) for simultaneously learning answer and question quality and user ask and answer reputation , where the Forward phase carries out the mutual reinforcement from the left to the right while the Backward phase from the right to the left as shown in Figure 2 . Since we generate combined log likelihood for learning the probability of each type of entity , the mutually reinforcing approach of our algorithm should cause the successive estimates of content quality and user reputation to converge . We will empirically demonstrate convergence in Section 5 .
In this section , we have defined the problem of calculating content quality and user reputation in CQA . Then , We present coupled mutual reinforcement framework and a semi supervised regression based approach to solve the
WWW 2009 MADRID!Track : Data Mining / Session : Graph Algorithms55 Algorithm 1 : CQA MR input : questions , answers and users and their connection from CQA network . output : answer quality ya ; answer reputation of user ya u ; question quality yq ; question reputation of user yq u
Start with an initial guess , eg uniform values , for ya , ya u , yq and yq u ; begin while ya , ya u , yq , yq u not converge do
Forward fit the logistic regression models and calculate new values for ya , yq and yq sequence ; Backward fit the logistic regression models and calculate new values for yq , yq and yq sequence u in u in end problem . In the following sections , we will setup and carry on a large scale evaluation on the framework and our new method .
4 . EXPERIMENTAL SETUP
This section presents our evaluation setup . First , we describe our dataset including corpus of questions , answers and the corresponding users . Then , we describe our evaluation metrics and some methods for computing answer quality and user reputation used for comparison in the experimental results reported in Section 5 . We also describe several ranking methods to illustrate the effects of user reputation and answer and question quality on general QA retrieval . 4.1 Data Collection
Yahoo! Answers Web service supplies an API to allow Web users to crawl existing question answering archives and the corresponding user information from the website . In order to ensure that our experiments are representative and repeatable , we use the TREC QA benchmarks to crawl QA archives and related user information . This is done by submitting TREC QA queries into the CQA site and retrieving the returned questions , answers and related users . ( All the data in this paper can be found at http://irmathcsemoryedu/shared/ ) Factoid questions from the TREC QA benchmarks We use factoid questions from seven years of the TREC QA track evaluations ( years 1999–2006)3 for our experiments . It is worth noting that TREC questions from the years 1999 to 2003 are independent of each other : each question is selfcontained and is submitted directly as a query . Starting from 2004 , however , the questions are organized in groups with a ‘target’ . For those questions , we submit their ‘target’ as well as the questions themselves . In total , approximately 3,000 factoid TREC questions were compiled as the initial set of queries . Since we need some candidate answers from Yahoo! Answers to estimate how well different ranking functions perform , we select the 1250 TREC factoid questions that have at least one similar question in the Yahoo! Answers archive .
The CQA dataset Our dataset was collected in order to mimic a user ’s experience with a CQA site . We submit each TREC query to the Yahoo! Answers Web service4 and retrieve up to 10 top ranked related questions according to the Yahoo! Answers ranking . For each of these Yahoo! questions , we retrieve as many answers as there are available for each question thread . There are , in total , 107293 users , 27354 questions and 224617 answers . And after automatically judging 224617 query , question , answer tuples by TREC answer pattern , 19409 tuples are labeled as “ relevant ” while 182308 are labeled as “ non relevant ” , and there is no answer pattern provided from TREC for the other 22900 tuples . Note that , although the proportion of factoid questions in Yahoo! Answers may not be large , we use them in order to have objective metric of correctness , and extrapolate performance to whole QA archives . Relevance Judgments In our experiment , the data are labeled for evaluating QA general retrieval in the following two ways : by using the TREC factoid answer patterns , and , independently , manually in order to validate the pattern based automatic labels . For automatic relevance labels we use the available regular expression answer patterns for the TREC factoid questions . We check every answer ’s text body , and if the text matches one of the answer patterns , we consider the answer text to be relevant , and non relevant otherwise . We manually validated the accuracy of our automatically assigned relevance labels in our previous work [ 3 ] , and we found 90 % agreement of automatic labels with manual relevance judgments . Data Labeling We use a set of labels for good users and good answers from Yahoo! Answers directly . For some question threads in Yahoo! Answers , there is one “ best answer ” which is selected by the asker . These “ best answers ” can be viewed as highquality answers . In addition , Yahoo! Answers selects some users as “ top contributors ” based on those users’ answering history . These “ top contributors ” can also be viewed as users with high answer reputation . In our data , there are 4000 “ top contributors ” and 18000 “ best answers ” .
In order to evaluate the effectiveness of our algorithm for calculating answer quality and answer reputation of users , we utilize a portion of these labels for users and answers . The other labels are used for testing by comparing with corresponding results of CQA MR . In our experiments , we will keep 3600 top contributors’ labels and 16000 best answers’ labels for training our model , and then use the rest 400 top contributors and 2000 best answers to test the performance of our algorithm for learning answer quality and user answer reputation . More importantly , we will evaluate the improvements to search , as described next .
In order to evaluate the effectiveness of our algorithm for computing question quality , we manually label a portion of the data . We randomly chose 250 resolved questions from Yahoo! Answers website which have received at least 5 answers . Two annotators were given 150 questions with 50 in common , and asked to label the quality of those 250 questions independently . The instructions for this labeling task were to consider both question subject and detail when examining question quality , and to consider answers when there is difficulty to understand the question . Questions
3http://trecnistgov/data/qahtml
4http://developeryahoocom/answers/
WWW 2009 MADRID!Track : Data Mining / Session : Graph Algorithms56 were labeled as “ good ” , “ fair ” and “ bad ” , according to special guidelines share by annotators .
Table 2 reports the agreement between the two raters on the 50 common questions . Since sometimes it is very hard to distinguish between ‘good’ and ‘fair’ questions , we also combined ‘good’ with ‘fair’ to form a binary labeling . Both agreements are reported in Table 2 . As we can see that we can get moderate agreement for both methods . As we can see , the binary labeling results in higher agreement ; hence , we will use the binary “ Good ” / “ Bad ” labels to evaluate question quality .
Table 2 : Inter annotator agreement and Kappa for question quality
3 categories
2 categories
Agreement
Kappa coefficient
64 % 0.41
80 % 0.46
4.2 Evaluation Metrics
We adapt the following information retrieval metrics to evaluate the performance of the our algorithm for learning answer quality as well as the performance of general QA retrieval .
• Mean Reciprocal Rank(MRR ) : The MRR of each individual query is the reciprocal of the rank at which the first relevant answer was returned , or 0 if none of the top N results contained a relevant answer.The score for a sequence of queries is the mean of the individual query ’s reciprocal ranks . Thus , MRR is calculated as q∈Qr
1 rq
M RR =
1 |Qr| where Qr is a set of test queries , rq is the rank of the first relevant document for q .
• Precision at K : for a given query , P ( K ) reports the fraction of answers ranked in the top K results that are labeled as relevant . In our setting , we require a relevant answer to be labeled “ matched ” for TREC pattern . For this metric , the position of relevant answers within the top K is irrelevant , while it measures overall user potential satisfaction with the top K results .
• Mean Average of Precision(MAP ) : Average precision for each query is defined as the mean of the precision at K values calculated after each relevant answer was retrieved . The final MAP value is defined as the mean of average precisions of all queries in the test set . This metric is the most commonly used singlevalue summary of a run over a set of queries . Thus , MAP is calculated as
N r=1(P ( r ) × rel(r ) )
M AP =
1 |Qr|
|Rq| q∈Qr where Qr is a set of test queries , Rq is the set of relevant document for q , r is the rank , N is the number retrieved , rel( ) is a binary function on the relevance of a given rank , and P ( ) is precision at a given cut off rank .
4.3 Methods Compared
We now describe the methods used to compute user reputation , which we use for our main task of improving CQA retrieval . Specifically , we compare the following methods :
• Baseline : users are ranked by “ indegree ” ( number of answers posted ) , an effective baseline estimate of user authority in CQA according to reference [ 17 ] .
• HITS : we calculate the user reputation based on HITS algorithm . Users are ranked based on their authority scores .
• CQA Supervised : we classify users into those with ” high ” and ” low ” reputation using a supervised classifier , namely SVM ( SMO implementation ) , trained over the features in Table 1 . Then user are ranked based on their reputation scores .
• CQA MR : predict user reputation based on our mutual reinforcement algorithm ( Section 33 )
Unfortunately , a direct experimental comparison with reference [ 2 ] , which is most closely related to our work , is impossible as neither the dataset or the truth labels used for the experiments in [ 2 ] are available . However , CQA Supervised is a similar approach and uses similar features to those described in [ 2 ] , thereby providing a realistic state of the art content quality classifier comparable to reference [ 2 ] .
Our main task is to improve CQA retrieval by incorporating content quality and user reputation . We compare the following ranking methods :
• Baseline :
In this method , the answers are ranked by the score computed as the difference of thumbsup votes and thumbs down votes received for each answer . This ranking closely approximates the ranking obtained when a user clicks “ order by votes ” option on the Yahoo! Answers site . The detail of this method and how to compute MRR and MAP under this setting is discussed in [ 3 ] .
• GBrank : In this method , we apply the ranking method proposed in our previous work [ 3 ] , which did not include answer and question quality and user reputation into ranking function . This method has been showed in [ 18 ] to have better performance than many state of the art supervised ranking methods , such as RankSVM .
• GBrank HITS : In this method , we optimize GBrank by adding user reputation calculated by HITS algorithm as extra features for learning the ranking function .
• GBrank Supervised : In this method , we first apply a supervised method ( SVM ) to learn the answer and question quality and user reputation based on their individual feature set independently . Then , we optimize GBrank by adding obtained quality and reputation as extra features for learning the ranking function .
• GBrank MR : In this method , we optimize GBrank by adding answer and question quality and user reputation calculated by CQA MR as extra features for learning the ranking function .
Note that , GBrank MR and GBrank Supervised , we use the same set of labels in learning .
WWW 2009 MADRID!Track : Data Mining / Session : Graph Algorithms57 5 . EXPERIMENTAL RESULTS
In this section , we will present several large scale experiments . These experiments are used to demonstrate that ( 1 ) the CQA MR algorithm exhibits good convergence behavior ; ( 2 ) CQA MR is effective for computing the question quality ; ( 3 ) the performance of general QA retrieval can be improved by incorporating predicted quality features calculated by CQA MR ; ( 4 ) user reputation from CQA MR tends to be better than those computed by other state ofthe art methods ; ( 5 ) the amount of supervision in CQAMR affects the quality of predicted quality features . 5.1 Predicting Content Quality and User Rep utation
511 Mutual Reinforcement Convergence We first perform training using Algorithm CQA MR introduced above . We examine the convergence behavior of CQA MR by calculating the log likelihood function ( Eq 10 ) over the iterations . We find that the log likelihood values increase and converge at around 40 iterations , when computing content quality and user reputation . u , yq ) and LL(ya
We also calculate the log likelihood for different values of σ . We are able to find that the log likelihood has much smaller values when σ is bigger , especially in the initial few iterations , which means that the conditional log likelihood u|ya ) ) is very small initially . There(LL(ya|ya fore , the difference in the labels between successive iterations is big , which implies labels’ inconsistency , in the early stage of the algorithm . However , we can also find that when we take more than ten iterations , the log likelihood is almost the same regardless of the σ values . Thus , at later stage of our algorithm , the log likelihood values are more sensitive to the objective function of the logistic regression while the labels remain consistent across iterations , stabilizing at around 30 iterations . 512 Predicting Answer Reputation We now compare the effectiveness of CQA MR with other methods for predicting answer reputation of users . For this task , we use the hold out set of 3600 users , with 678 of them labeled as “ top contributors ” . Figure 3 reports the fraction of “ top contributor ” users included in the top K users ranked by answer reputation , for varying K . As we can see , CQA MR exhibits significantly higher precision than CQA Supervised , which , in turn outperforms HITS and the simple “ in degree ” count baseline . 513 Predicting Question Quality We now compare the effectiveness of CQA MR with other methods for predicting question quality . For this task , we use the set of 250 questions with manually labeled quality , which is described in section 41 We train CQA MR as described above ( that is , no additional question quality labels provided for training ) and randomly select 100 labeled questions for evaluating the performance of predicting question quality . In order to compare with existing methods for predicting question quality , we also apply a supervised classifier , namely SVM ( SMO implementation ) , trained over the features in Table 1 . And the testing set is the same 100 labeled questions used above while the other 150 labeled questions are used for training SVM .
Figure 3 : Precision at K for the status of top contributors in testing data
Figure 4 : Precision Recall curves for predicting question quality of CQA MR and Supervised method . while that value is 0.890 for the supervised method ( SVM ) . Figure 4 shows the precision recall curves for both methods , it is clear that CQA MR gives good performance on predicting question quality and exhibits significantly higher precision than supervised method . In addition , we also try to add 150 labeled question as seeds in training CQA MR . Interestingly , adding the question labels as additional seeds for training CQA MR does not significantly improve performance . 5.2 Quality aware CQA Retrieval
The answer and question quality and user reputation computed by CQA MR , CQA Supervised and HITS can be viewed as prior “ static ” features for QA retrieval since they are independent of queries . This complements “ classical ” information retrieval and QA retrieval , which primarily focused on query dependent relevance features .
In this experiment , we seek to enhance the performance of general QA retrieval by incorporating predicted quality features ( answer and question quality and user reputation ) . We use GBrank [ 18 ] as the ranking function and apply the same framework in our previous work [ 3 ] . For the training data we use 800 TREC queries and the associated community QA pairs , and we use another 450 queries and the associated community QA pairs for testing data . The set of features used to train the ranking function is described in detail in [ 3 ] .
The mean average precision ( MAP ) for CQA MR is 0.940
We train four ranking functions , GBrank MR , GBrank
02040608010002030405060708091KPrecision@K BaselineHITS : authorityCQA−MRSupervised002040608107508085090951105RecallPrecision CQA−MRSupervisedWWW 2009 MADRID!Track : Data Mining / Session : Graph Algorithms58 method and supervised learning method for computing user reputation , ie,HITS and CQA Supervised .
Figure 5 demonstrates the Precision at K of GBrank
MR compared with methods of GBrank HITS and GBrankSupervised . The GBrank Supervised method replaces QA quality and user reputation calculated by a supervised learning method . Note that we use the same set of labels in learning for GBrank MR and GBrank Supervised . In order to compare the two methods GBrank MR and GBrank Supervised , we apply t test based on their precision and the p value of significance test is 002 The GBrankHITS method replaces user reputation by those calculated by the HITS algorithm .
Figure 5 and Table 3 indicate that GBrank MR achieves much better performance than GBrank HITS , which implies that the user reputation calculated by CQA MR gives more contribution than user ’s authority scores computed by HITS . However , GBrank HITS outperforms GBrank which does not contain QA quality and user reputation features . It shows that user ’s authority scores from HITS are still useful to enhance the performance of QA retrieval . Our conjecture is that for user ’s answer reputation , it is much more important because CQA MR not only utilizes network relationship but also individual reputation related features while the authority scores in HITS only relies on the graph structure of CQA systems .
From Figure 5 and Table 3 , we can also find that GBrank
MR performs significantly better than GBrank Supervised ( p < 003 ) After analyzing information gain of features , we find that GBrank MR assigns higher weights on QA quality and user reputation features . All of these imply that the QA quality and user reputation calculated by CQA MR gives more contribution than those calculated by supervised method with limited amount of training data . GBrankSupervised also outperforms GBrank which shows that QA quality and user reputation obtained by supervised method are still useful to enhance the performance of QA retrieval .
5.4 Effects of the Amount of Supervision
As mentioned before , we utilize a set of training labels for users and answers in the algorithms CQA MR and CQASupervised to learn predicted quality features . In this experiment , we show the influence of the amount of training labels ( ie,degree of supervision ) on the performance of CQA MR and CQA Supervised . The labeled set contains 3600 good users and 16000 good answers . We vary the size of training labeled set used in the learning process by randomly selecting a certain number of labels . For example , choosing 40 % labels means to use 2400 good users and 10800 good answers in the learning process .
Figures 6 and 7 report the MRR and MAP scores for the hold out validation data against varying amount of labeled training data for high quality question and answer retrieval . We can see that MRR and MAP scores increase when there are more labels in CQA MR . CQA MR can achieve same accuracy as CQA Supervised with about half of the required training data . Therefore , CQA MR can improve QA retrieval much more with less supervised learning compared to CQA Supervised . We also find that GBrankHITS have higher accuracy than GBrank MR when the amount of supervision is less than 1200 examples , suggesting that HITS indeed identifies high quality content/users , but can be improved on by our method .
Figure 5 : Precision at K for Baseline , GBrank , GBrank HITS , GBrank Supervised and GBrankMR for various K
Supervised , GBrank HITS and GBrank , on training data ( ie,the 850 TREC queries ) with predicted quality features added in the first three methods and training data without these features in the last one , respectively . Then , we test on the remainder hold out testing data of 450 TREC queries and the associated community QA pairs .
Figure 5 illustrates the Precision at K of GBrank MR , GBrank Supervised and GBrank HITS compared with GBrank and the baseline method . The figure shows that all of the four ranking functions out perform the baseline method . Furthermore , after incorporating predicted quality features , ie,answer and question quality and user reputation , GBrank MR , GBrank Supervised and GBrankHITS give better precision than GBrank without these features . In particular , the Precision at 1 of GBrank MR is about 79 % compared to 76 % Precision at 1 exhibited by GBrank .
Table 3 : Accuracy of GBRank MR , GBRankSupervised , GBRank HITS , GBRank , and Baseline ( TREC 1999 2006 questions )
Baseline GBrank GBrank HITS GBrank Supervised GBrank MR
Gain
MRR 0.664 0.782 +0.045(6 % ) 0.827 0.833 +0.051(7 % ) 0.865 +0.083(11 % )
Gain
MAP 0.443 0.465 0.473 +0.008(2 % ) 0.477 +0.012(3 % ) 0.483 +0.018(4 % )
Table 3 reports the MAP and MRR scores for GBrankMR , GBrank Supervised , GBrank HITS , GBrank as well as the baseline method . Table 3 indicates that GBrankMR , GBrank Supervised and GBrank HITS achieve much better performance than GBrank and the baseline method . In particular , for MRR scores , GBrank MR achieves a gain of about 11 % relative to GBrank ; and GBRankMR obtains double the gains of GBrank HITS for both MRR and MAP scores . The above experiments illustrate the usefulness of the extracted static features in improving answer relevance . 5.3 Effects of the QA quality and User Repu tation Features
We now explore the effects of the QA quality and user reputation , which is calculated by CQA MR , on learning the ranking function . To this end , we perform a study on its influence on QA retrieval compared with existing graph based
1234505055060650707508085KPrecision@K GBrank−MRGBrank−SupervisedGBrank−HITSGBrankBaselineWWW 2009 MADRID!Track : Data Mining / Session : Graph Algorithms59 searching the CQA archives .
In the future , we will explore the performance of our approach across different question domains , and take into account varying expertise ( authority ) of users for different domains . In addition , we also plan to focus on exploring probabilistic models of content quality and user authority , and on applying these techniques to other online communities .
7 . REFERENCES [ 1 ] L . A . Adamic , J . Zhang , E . Bakshy , and M . S . Ackerman .
Knowledge sharing and yahoo answers : everyone knows something . In Proc . of WWW , 2008 .
[ 2 ] E . Agichtein , C . Castillo , D . Donato , A . Gionis , and
G . Mishne . Finding high quality content in social media with an application to community based question answering . In Proceedings of WSDM , 2008 .
[ 3 ] J . Bian , Y . Liu , E . Agichtein , and H . Zha . Finding the right facts in the crowd : Factoid question answering over social media . In Proc . of WWW2008 , 2008 .
[ 4 ] E . Brill , S . Dumais , and M . Banko . An analysis of the askmsr question answering system . In Proceedings of EMNLP , 2002 .
[ 5 ] C . Campbell , P . Maglio , A . Cozzi , and B . Dom . Expertise identification using email communications . In Proc . of CIKM , 2003 .
[ 6 ] B . Dom , I . Eiron , A . Cozzi , and Y . Zhang . Graph based ranking algorithms for e mail expertise analysis . In Proc . of workshop on Data Mining and Knowledge Discovery , 2003 .
[ 7 ] R . Guha , R . Kumar , P . Raghavan , and A . Tomkins .
Propagation of trust and distrust . In Proc . of 13th International World Wide Web Conference ( WWW2004 ) , 2004 .
[ 8 ] J . Jeon , W . Croft , and J . Lee . Finding similar questions in large question and answer archives . In Proceedings of CIKM , 2005 .
[ 9 ] J . Jeon , W . Croft , J . Lee , and S . Park . A framework to predict the quality of answers with non textual features . In Proceedings of SIGIR , 2006 .
[ 10 ] P . Jurczyk and E . Agichtein . Discovering authorities in question answer communities using link analysis . In Proc . of CIKM , 2007 .
[ 11 ] J . Kleinberg . Authoritative sources in a hyperlinked environment . Journal of the ACM , 46(5):604–632 , 1999 .
[ 12 ] L . Page , S . Brin , R . Motwani , and T . Winograd . The pagerank citation ranking : Bringing order to the web . In Technical report , Stanford Digital Library Technologies Project , 1998 .
[ 13 ] J . Scott . Social Network Analysis : A Handbook . SAGE
Publications , January 2000 .
[ 14 ] Q . Su , D . Pavlov , J . Chow , and W . Baker . Internet scale collection of human reviewed data . In Proc . of WWW2007 , 2007 .
[ 15 ] E . M . Voorhees . Overview of the TREC 2003 question answering track . In Text REtrieval Conference , 2003 .
[ 16 ] H . Zha . Generic summarization and keyphrase extraction using mutual reinforcement principle and sentence clustering . In Proc . of SIGIR , 2002 .
[ 17 ] J . Zhang , M . Ackerman , and L . Adamic . Expertise networks in online communities : structure and algorithms . In Proc . of ( WWW ) , 2007 .
[ 18 ] Z . Zheng , H . Zha , K . Chen , and G . Sun . A regression framework for learning ranking functions using relative relevance judgments . In Proc . of SIGIR , 2007 .
[ 19 ] D . Zhou , S . Orshanskiy , H . Zha , and L . Giles . Co ranking authors and documents in a heterogeneous network . In Proc . of ( ICDM ) , 2007 .
[ 20 ] C . Ziegler and G . Lausen . Propagation models for trust and distrust in social networks . Information Systems Frontiers , 7(4 5):337–358 , 2005 .
Figure 6 : MRR of GBrank , GRrank HITS , GBrankMR , and GBrank Supervised for varying fraction of training labels used in CQA MR
Figure 7 : MAP of GBrank , GRrank HITS , GBrankMR , GBrank Supervised for varying fraction of training labels used in CQA MR
In summary , our experimental results show that CQAMR is an effective method for identifying high quality content and highly reputable users in CQA , particularly when training data is limited . More importantly , we have shown that the predicted quality and reputation features , when modeled explicitly , significantly improve the accuracy of CQA retrieval . Finally , our experiments show the effectiveness of our mutual reinforcement algorithm , as GBRank MR significantly outperforms a state of the art supervised method ( GBRank Supervised , implemented using SVM ) operating on the same features .
6 . CONCLUSIONS
We presented CQA MR , a framework for semi supervised quality and reputation estimation of content and users in Community Question Answering . We have demonstrated the effectiveness of CQA MR in large scale experiments of a CQA dataset comprising over 100,000 users , 27,000 questions , and more than 200,000 answers . Our experiments demonstrate significant improvements over the supervised method , both in accuracy and in reducing the required amount of training data . Interestingly , for the task of predicting question quality , CQA MR significantly outperforms a supervised method while requiring no question quality training labels . Furthermore , we demonstrated a significant improvement that CQA MR provides for the practical task of
060012001800240036000750808509Number of training labelsMRR score GBrank−MRGBrank−SupervisedGBrankGBrank−HITS060012001800240036000455046046504704750480485049Number of training labelsMAP score GBrank−MRGBrank−SupervisedGBrank−HITSGBrankWWW 2009 MADRID!Track : Data Mining / Session : Graph Algorithms60
