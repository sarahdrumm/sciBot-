Large Scale Text Categorization by
Batch Mode Active Learning
Steven C . H . Hoi† Rong Jin‡ Michael R . Lyu†
†Department of Computer Science and Engineering ‡Department of Computer Science and Engineering
The Chinese University of Hong Kong
Shatin , NT , Hong Kong
{chhoi , lyu}@csecuhkeduhk
ABSTRACT Large scale text categorization is an important research topic for Web data mining . One of the challenges in large scale text categorization is how to reduce the amount of human efforts in labeling text documents for building reliable classification models . In the past , there have been many studies on applying active learning methods to automatic text categorization , which try to select the most informative documents for manually labeling . Most of these studies focused on selecting a single unlabeled document in each iteration . As a result , the text categorization model has to be retrained after each labeled document is solicited . In this paper , we present a novel active learning algorithm that selects a batch of text documents for manually labeling in each iteration . The key of the batch mode active learning is how to reduce the redundancy among the selected examples such that each example provides unique information for model updating . To this end , we use the Fisher information matrix as the measurement of model uncertainty and choose the set of documents that can efficiently minimize the Fisher information matrix of a classification model . Extensive experiments with three different datasets have shown that our algorithm is more effective than the state of the art active learning techniques for text categorization and can be a promising tool toward large scale text categorization on World Wide Web .
Categories and Subject Descriptors H33 [ Information Systems ] : Information Search and Retrieval ; I52 [ Design Methodology ] : Classifier Design and Evaluation
General Terms Algorithms , Performance , Experimentation
Keywords text categorization , active learning , logistic regression , Fisher information matrix , bound optimization
1 .
INTRODUCTION
The goal of text categorization is to automatically assign text documents to a set of predefined categories . With the Copyright is held by the author/owner(s ) . WWW2006 , May 22–26 , 2006 , Edinburgh , UK . .
Michigan State University
East Lansing , MI 48824 , USA rongjin@csemsuedu rapid growth of Web pages on World Wide Web ( WWW ) , text categorization has become more and more important in both the world of research and applications . Usually , text categorization is regarded as a supervised learning problem . In order to build a reliable model for text categorization , we need to first of all manually label a number of documents using the predefined categories . Then , a statistical machine learning algorithm is engaged to learn a text classification model from the labeled documents . Two critical challenges are posed when we develop an effective scheme of large scale text categorization for WWW documents :
• Reducing the number of labeled documents . The first challenge is how to reduce the number of manually labeled documents that are required for building reliable text classification models . This is particularly important for text categorization of WWW documents given its large size .
• Improving learning efficiency . The second challenge is how to efficiently learn a classification model for automatic text categorization . This is because text documents of WWW often change rapidly from time to time . It is critical to choose an efficient machine learning algorithm that can be trained and employed efficiently .
In this paper , we focus on the first challenge of text categorization . In the past , there have been a number of studies on applying active learning to text categorization . The main idea is to only select the most informative documents for manually labeling . Most active learning algorithms are conducted in the iterative fashion . In each iteration , the example with the highest classification uncertainty is chosen for manually labeling . Then , the classification model is retrained with the additional labeled example . The step of training a classification model and the step of soliciting a labeled example are iterated alternatively until most of the examples can be classified with reasonably high confidence . One of the main problems with such a scheme is that only a single example is selected for labeling . As a result , the classification model has to be retrained after each labeled example is solicited . In the paper , we propose a novel active learning algorithm that is able to select a batch of unlabeled examples in each iteration . A simple strategy toward the batch mode active learning is to select the top k most informative examples . The problem with such an approach is that some of the selected examples could be similar , or even identical , to each other , and therefore do not provide additional information for model updating . In general , the key of the batch mode active learning is to ensure the small redundancy among the selected examples such that each example provides unique information for model updating . To this end , we use the Fisher information matrix , which represents the overall uncertainty of a classification model . We choose the set of examples such that the Fisher information matrix of a classification model can be effectively reduced . The rest of this paper is organized as follows . Section 2 reviews the related work on text categorization and active learning algorithms . Section 3 briefly introduces the concept of logistic regression , which is used as the classification model in our study for text categorization . Section 4 presents the batch mode active learning algorithm and an efficient learning algorithm based on the bound optimization algorithm . Section 5 presents the results of our empirical study . Section 6 sets out our conclusions .
2 . RELATED WORK
Text categorization is a long term research topic which has been actively studied in the communities of Web data mining , information retrieval and statistical learning [ 15 , 35 , 36 ] . Essentially the text categorization techniques have been the key toward automated categorization of large scale Web pages and Web sites [ 18 , 27 ] , which is further applied to improve Web searching engines in finding relevant documents and facilitate users in browsing Web pages or Web sites .
In the past decade , a number of statistical learning techniques have been applied to text categorization [ 34 ] , including the K Nearest Neighbor approaches [ 20 ] , decision trees [ 2 ] , Bayesian classifiers [ 32 ] , inductive rule learning [ 5 ] , neural networks [ 23 ] , and support vector machines ( SVM ) [ 9 ] . Empirical studies in recent years [ 9 , 35 ] have shown that SVM is the state of the art technique among all the methods mentioned above .
Recently , logistic regression , a traditional statistical tool , has attracted considerable attention for text categorization and high dimension data mining [ 12 ] . Several recent studies have shown that the logistic regression model can achieve comparable classification accuracy as SVMs in text categorization . Compared to SVMs , the logistic regression model has the advantage in that it is usually more efficient than SVMs in model training , especially when the number of training documents is large [ 13 , 37 ] . This motivates us to choose logistic regression as the basis classifier for large scale text categorization .
The other critical issue for large scale text document categorization is how to reduce the number of labeled documents that are required for building reliable text classification models . Given the limited amount of labeled documents , the key is to exploit the unlabeled documents . One solution is the semi supervised learning , which tries to learn a classification model from the mixture of labeled and unlabeled examples [ 30 ] . A comprehensive study of semi supervised learning techniques can be found in [ 25 ] and [ 39 ] . Another solution is active learning [ 19 , 26 ] that tries to choose the most informative unlabeled examples for manually labeling . Although previous studies have shown the promising performance of semi supervised learning for text categorization [ 11 ] , the high computation cost has limited its application [ 39 ] , particularly when the number of unlabeled documents is large . In this paper , we focus our discussion on the active learning .
Active learning , or called pool based active learning , has been extensively studied in machine learning for many years and has already been employed for text categorization in the past [ 16 , 17 , 21 , 22 ] . Most active learning algorithms are conducted in the iterative fashion . In each iteration , the example with the highest classification uncertainty is chosen for manually labeling . Then , the classification model is retrained with the additional labeled example . The step of training a classification model and the step of soliciting a labeled example are iterated alternatively until most of the examples can be classified with reasonably high confidence . One of the key issues in active learning is how to measure the classification uncertainty of unlabeled examples . In [ 6 , 7 , 8 , 14 , 21 , 26 ] , a number of distinct classification models are first generated . Then , the classification uncertainty of a test example is measured by the amount of disagreement among the ensemble of classification models in predicting the labels for the test example . Another group of approaches measure the classification uncertainty of a test example by how far the example is away from the classification boundary ( ie , classification margin ) [ 4 , 24 , 31 ] . One of the most well known approaches within this group is Support Vector Machine Active Learning developed by Tong and Koller [ 31 ] . Due to its popularity and success in the previous studies , it is used as the baseline approach in our study .
One of the main problems with most existing active learning algorithm is that only a single example is selected for labeling . As a result , the classification model has to be retrained after each labeled example is solicited . In this paper , we focus on the batch mode active learning that selects a batch of unlabeled examples in each iteration . A simple strategy is to choose the top k most uncertain examples . However , it is likely that some of the most uncertain examples are strongly correlated and therefore will provide similar information to the classification model . In general , the challenge in choosing a batch of unlabeled examples is twofold : on one hand the examples in the selected batch should be informative to the classification model ; on the other hand the examples should be diverse enough such that information provided by different examples does not overlap with each other . To address this challenge , we employ the Fisher information matrix as the measurement of model uncertainty , and choose the set of examples that efficiently reduce the Fisher information matrix . Fisher information matrix has been used widely in statistics for measuring model uncertainty [ 28 ] . For example , in the Cramer Rao bound , Fisher information matrix provides the low bound for the variance of a statistical model . In this study , we choose the set of examples that can well represent the structure of the Fisher information matrix .
3 . LOGISTIC REGRESSION
In this section , we give a brief background review of logistic regression , which has been a well known and mature statistical model suitable for probabilistic binary classification . Recently , logistic regression has been actively studied in data mining and machine learning communities due to its close relations to Support Vector Machines and Adaboost , which are usually categorized as “ large margin classifiers ” [ 33 , 37 ] . Compared with many other statistical learning methods , such as support vector machine , the logistic regression model has the following advantages :
• It is a high performance classifier that can be efficiently trained with a large number of labeled examples . Previous studies have show that the logistic regression model is able to achieve the similar performance of text categorization as SVMs [ 13 , 37 ] . These studies also showed that the logistic regression model can be trained significantly more efficiently than SVMs , particularly when the number of labeled documents is large .
• It is a robust classifier that does not have any configuration parameters to tune . In contrast , some state ofthe art classifiers , such as support vector machines and AdaBoost , are sensitive to the setup of the configuration parameters . Although this problem can be partially solved by the cross validation method , it usually introduces a significant amount of overhead in computation .
Logistic regression can be applied to both real and binary data . It outputs the posterior probabilities for test examples that can be conveniently processed and engaged in other systems . In theory , given the input features x = ( x1 , x2 , . . . , xd ) of a test example where d is the number of features , logistic regression models the conditional probability of assigning a class label y to the example by p(y|x ) =
1
1 + exp(−y(wT x + b ) )
( 1 ) where y ∈ {+1,−1} is the class label . w = ( w1 , w2 , . . . , wd ) are the weights assigned to different features , and b is the bias term . In general , logistic regression is a linear classifier that is sufficient to classify text documents that are usually in the high dimensional data space . For the implementation of logistic regression , a number of efficient algorithms have been developed in the recent literature [ 13 ] .
4 . BATCH MODE ACTIVE LEARNING
In this section , we present a batch mode active learning algorithm for large scale text categorization . In our proposed scheme , logistic regression is used as the base classifier for binary classification . In the following , we first introduce the theoretical foundation of our active learning algorithm . Based on the theoretical framework , we then formulate the active learning problem into a semi definite programming ( SDP ) problem [ 3 ] . Finally , we present an efficient learning algorithm for the related optimization problem based on the eigen space simplification and a bound optimization strategy . 4.1 Theoretical Foundation
Our active learning methodology is motivated by the work in [ 38 ] , in which the author presented a theoretical framework of active learning based on the minimization of Fisher information matrix . Given the Fisher information matrix represents the overall uncertainty of a classification model , our goal is to search for a set of examples that can most efficiently reduce the Fisher information matrix . As showed in [ 38 ] , this goal can be formulated into the following optimization problem :
Let p(x ) be the distribution of all unlabeled examples , and q(x ) be the distribution of unlabeled examples that are chosen for manually labeling . Let α denote the parameters of the classification model . Let Ip(α ) and Iq(α ) denote the Fisher information matrix of the classification model for the distribution p(x ) and q(x ) , respectively . Then , the set of examples that can most efficiently reduce the uncertainty of classification model is found by minimizing the ratio of the two Fisher information matrix Ip(α ) and Iq(α ) , ie ,
∗ q
= arg min q tr(Iq(α )
−1Ip(α ) )
( 2 )
For the logistic regression model , the Fisher information
Iq(α ) is attained as :
Iq(α ) = −
= y=±1 1 q(x ) p(y|x )
∂2
∂α2 log p(y|x)dx
( 3 )
1 + exp(αT x )
1
1 + exp(−αT x ) xxT q(x)dx
In order to estimate the optimal distribution q(x ) , we replace the integration in the above equation with the summation over the unlabeled data , and the model parameter α with the empirically estimated ˆα . Let D = ( x1 , . . . , xn ) be the unlabeled data . We can now rewrite the above expression for Fisher information matrix as : πi(1 − πi)xixT n
Iq( ˆα ) = i qi + δId
( 4 ) where i=1
πi = p(−|xi ) =
1 + exp( ˆαT xi )
1 n
( 5 )
In the above , qi stands for the probability of selecting the i th example and is subjected to i=1 qi = 1 . Id is the identity matrix of d dimension . δ is the smoothing parameter . δId is added to the estimation of Iq( ˆα ) to prevent it from being a singular matrix . Similarly , for Ip( ˆα ) , the Fisher information matrix for all the unlabeled examples , we have it expressed as follows : n i=1
Ip( ˆα ) =
1 n
πi(1 − πi)xixT i + δId
( 6 )
4.2 Why Using Fisher Information Matrix ?
In this section , we will qualitatively justify the theory of minimizing the Fisher information for batch mode active learning . In particular , we consider two cases , the case of selecting a single unlabeled example and the case of selecting two unlabeled examples simultaneously . To simplify our discussion , let ’s assume xi2 2 = 1 for all unlabeled examples .
Selecting a single unlabeled example . The Fisher information matrix Iq is simplified into the following form when the i th example is selected :
Iq( ˆα ; xi ) = πi(1 − πi)xixT i + δId
Then , the objective function tr(Iq( ˆα)−1Ip( ˆα ) ) becomes : tr(Iq( ˆα )
−1Ip( ˆα ) ) ≈ 1 n nπi(1 − πi ) j=1 n j=1
+
1 nδ
πj(1 − πj)(xT i xj)2
πj(1 − πj)(1 − ( xT i xj)2 )
To minimize the above expression , we need to maximize the term πi(1 − πi ) , which reaches its maximum value at πi = 05 Since πi = p(−|xi ) , the value of πi(1 − πi ) can be regarded as the measurement of classification uncertainty for the i th unlabeled example . Thus , the optimal example chosen by minimizing the Fisher information matrix in the above expression tends to be the one with a high classification uncertainty .
Selecting two unlabeled examples simultaneously . To simplify our discussion , we assume that the three examples , x1 , x2 , and x3 , have the largest classification uncertainty . Let ’s further assume that x1 ≈ x2 , and meanwhile x3 is far away from x1 and x2 . Then , if we follow the simple greedy approach , the two example x1 and x2 will selected given their largest classification uncertainty . Apparently , this is not an optimal strategy given both examples provide almost identical information for updating the classification model . Now , if we follow the criterion of minimizing Fisher information matrix , this mistake could be prevented because
Iq( ˆα ; x1 , x2 ) =
( x1xT
1 + x2xT
2 ) + δId
≈ x1xT
1 + δId = Iq( ˆα ; x1 )
1 2
As indicated in the above equation , by including the second example x2 , we did not change the expression of Iq , the Fisher information matrix for the selected examples . As a result , there will be no reduction in the objective function tr(Iq( ˆα)−1Ip( ˆα ) ) when including the example x2 . Instead , we may want to choose x3 that is more likely to decrease the objective function even though its classification uncertainty is smaller than that of x2 . 4.3 Optimization Formulation
It is not easy to find an appropriate distribution q(x ) that minimizes tr(I−1 q Ip ) . In the following , we present the semidefinite programming ( SDP ) approach for optimizing tr(I−1 q Ip ) .
Given the optimization problem in ( 2 ) , we can rewrite the objective function tr(I−1 ) . We then introduce a slack matrix M ∈ Rn×n such that M ( cid:186 ) I 1/2 . Then original optimization problem can be p rewritten as follows : q Ip ) as tr(I 1/2
I−1 q I 1/2
I−1 q I 1/2 p p p tr(M ) min q,M s . t . M ( cid:186 ) I 1/2 p n i=1
−1 q I 1/2 p
I
( 7 ) qi = 1 , qi ≥ 0 , i = 1 , . . . , n
( cid:181 )
In the above , we use the property tr(A ) ≥ tr(B ) if A ( cid:186 ) B . Furthermore , we use the Schur complementary , ie ,
D ( cid:186 ) AB
−1AT ⇔
( 8 ) if B ( cid:186 ) 0 . This will lead to the following formulism for the problem in ( 7 )
B AT A D
( cid:186 ) 0
( cid:182 ) or more specifically min q,M s . t . tr(M ) n n i=1
( cid:195 ) xixT i I 1/2 p qiπi(1 − πi )
I 1/2 p M qi = 1 , qi ≥ 0 , i = 1 , . . . , n
( cid:186 ) 0
( 10 ) i=1
The above problem belongs to the family of Semi definite programming and can be solved by the standard convex optimization packages such as SeDuMi [ 29 ] . 4.4 Eigen Space Simplification
Although the formulation in ( 10 ) is mathematically sound , directly solving the optimization problem could be computationally expensive due to the large size of matrix M , ie , d × d , where d is the dimension of data . In order to reduce the computational complexity , we assume that M is only expanded in the eigen space of matrix Ip . Let {(λ1 , v1 ) , . . . , ( λs , vs)} be the top s eigen vectors of matrix Ip where λ1 ≥ λ2 ≥ . . . ≥ λs . We assume matrix M has the following form :
M =
γkvkvT k
( 11 ) where the combination parameters γk ≥ 0 , k = 1 , . . . , s . as Iq ( cid:186 ) We rewrite the inequality for M ( cid:186 ) I 1/2 p M−1I 1/2 I 1/2 . Using the expression for M in ( 11 ) , we have
I−1 q I 1/2 p p p
I 1/2 p M
−1I 1/2 p =
−1 k λkvkvT γ k
( 12 ) s k=1 s k=1
Given that the necessary condition for Iq ( cid:186 ) I 1/2 p v , ∀v ∈ Rd , vT Iqv ≥ vT I 1/2 p M k Iqvk ≥ γ −1 k λk for k = 1 , . . . , s . This necessary we have vT condition leads to following constraints for γk : p M−1I 1/2
−1I 1/2 is p
, k = 1 , . . . , s
( 13 )
γk ≥
λk n i=1 qiπi(1 − πi)(xT s i vk)2
Meanwhile , the objective function in ( 10 ) can be expressed as tr(M ) =
γk
( 14 ) k=1
By putting the above two expressions together , we transform the SDP problem in ( 10 ) into the following optimization problem : s n k=1
λk n i=1 qiπi(1 − πi)(xT i vk)2 qi = 1 , qi ≥ 0 , i = 1 , . . . , n min q∈Rn st
( 15 ) min q,M s . t . tr(M )
( cid:195 ) n i=1
( cid:186 ) 0
I 1/2 p M
Iq I 1/2 p qi = 1 , qi ≥ 0 , i = 1 , . . . , n i=1
( 9 )
Note that the above optimization problem is a convex optimization problem since f ( x ) = 1/x is convex when x ≥ 0 . Given this formulation in ( 15 ) , we present a bound optimization algorithm for solving the above optimization problem .
4.5 Bound Optimization Algorithm
Category # of total samples
The main idea of bound optimization algorithm is to update the solution iteratively . In each iteration , we will first calculate the difference between the objective function of the current iteration and the objective function of the previous iteration . Then , by minimizing the upper bound of the difference , we find the solution of the current iteration . Let q and q denote the solution obtained in two consecutive iterations , and let L(q ) be the objective function in ( 15 ) . Based on the proof given in Appendix A , we have the following expression :
L(q ) = s ≤ n k=1 i=1
λk n s i=1 qiπi(1 − πi)(xT ( q i)2 qi
πi(1 − πi ) k=1 i vk)2
( cid:179)n i vk)2λk
( xT jπj(1 − πj)(xT j=1 q
( cid:180)2 j vk)2 ( 16 )
Now , instead of optimizing the original objective function L(q ) , we can optimize its upper bound , which leads to the following simple updating equation : s k=1
( cid:179)n
( xT i vk)2λk j=1 qjπj(1 − πj)(xT j vk)2
( cid:180)2 qi ←−q2 i πi(1 − πi ) qi ←− qin j=1 qj
( 17 )
Similar to all bound optimization algorithms [ 3 ] , this algorithm will guarantee to converge to a local maximum . Since the original optimization problem in ( 15 ) is a convex optimization problem , the above updating procedure will guarantee to converge to a global optimal .
Remark : It is interesting to examine the property of the solution obtained by the updating equation in ( 17 ) . First , according to ( 17 ) , the example with a large classification uncertainty will be assigned with a large probability . This is because qi is proportional to πi(1 − πi ) , the classification uncertainty of the i the unlabeled example . Second , according to ( 17 ) , the example that are similar to many unlabeled examples is more likely to be selected . This is because probability qi is proportional to the term ( xT i v)2 , the similarity of the i th example to the principle eigenvectors . This is consistent with our intuition that we should select the most informative and representative examples for active learning .
5 . EXPERIMENTAL RESULTS 5.1 Experimental Testbeds
In this section we discuss the experimental evaluation of our active learning algorithm in comparison to the state ofthe art approaches . For a consistent evaluation , we conduct our empirical comparisons on three standard datasets for text document categorization . For all three datasets , the same pre processing procedure is applied : the stopwords and the numbers are removed from the documents , and all the words are converted into the low cases without stemmming . The first dataset is the Reuters 21578 Corpus dataset , which has been widely used as a testbed for evaluating algorithms for text categorization [ 35 ] . In our experiments , the ModApte split of the Reuters 21578 is used . There are earn acq money fx grain crude trade interest wheat ship corn
3964 2369 717 582 578 485 478 283 286 237
Table 1 : A list of 10 major categories of the Reuters21578 dataset in our experiments .
Category
# of total samples course department faculty project staff student
930 182 1124 504 137 1641
Table 2 : A list of 6 categories of the WebKB dataset in our experiments . totally 10788 text documents in this collection . Table 1 shows a list of the 10 most frequent categories contained in the dataset . Since each document in the dataset can be assigned to multiple categories , we treat the text categorization problem as a set of binary classification problems , ie , a different binary classification problem for each category . In total , 26 , 299 word features are extracted and used to represent the text documents .
The other two datasets are Web related : the WebKB data collection and the Newsgroup data collection . The WebKB dataset comprises of the WWW pages collected from computer science departments of various universities in January 1997 by the World Wide Knowledge Base ( Web >Kb ) project of the CMU text learning group . All the Web pages are classified into seven categories : student , faculty , staff , department , course , project , and other . In this study , we ignore the category of others due to its unclear definition . In total , there are 4518 data samples in the selected dataset , and 19 , 686 word features are extracted to represent the text documents . Table 2 shows the details of this dataset . The newsgroup dataset includes 20 , 000 messages from 20 different newsgroups . Each newsgroup contains roughly about 1000 messages . In this study , we randomly select 11 out of 20 newsgroups for evaluation . In total , there are 10 , 996 data samples in the selected dataset , and 47 , 410 word features are extracted to represent the text documents . Table 3 shows the details of the engaged dataset .
Comparing to the Reuter 21578 dataset , the two Webrelated data collections are different in that more unique words are found in the Web related datasets . For example , for both the Reuter 21578 dataset and the Newsgroup dataset , they both contain roughly 10 , 000 documents . But , the number of unique words for the Newgroups dataset is close to 50 , 000 , which is about twice as the number of unique words found in the Reuter 21578 . It is this feature that
Category # of total samples boundary wT x + b = 0 , ie ,
0 1 2 3 4 5 6 7 8 9 10
1000 1000 1000 1000 1000 1000 999 1000 1000 1000 997
Table 3 : A list of 11 categories of the Newsgroup dataset in our experiments . makes the text categorization of WWW documents more challenging than the categorization of normal text documents because more feature weights need to be decided for the WWW documents than the normal documents . It is also this feature that makes the active learning algorithms more valuable for text categorization of WWW documents than the normal documents since by selecting the informative documents for manually labeling , we are able to decide the appropriate weights for more words than by a randomly chosen document . 5.2 Experimental Settings
In order to remove the uninformative word features , feature selection is conducted using the Information Gain [ 36 ] criterion . In particular , 500 of the most informative features are selected for each category in each of the three datasets above .
For performance measures , the F 1 metric is adopted as our evaluation metric , which has been shown to be more reliable metric than other metrics such as the classification accuracy [ 36 ] . More specifically , the F 1 is defined as
F 1 =
2 ∗ p ∗ r p + r
( 18 ) where p and r are precision and recall . Note that the F 1 metric takes into account both the precision and the recall , thus is a more comprehensive metric than both the precision and the recall .
To examine the effectiveness of the proposed active learning algorithm , two reference models are used in our experiment . The first reference model is the logistic regression active learning algorithm that measures the classification uncertainty based on the entropy of the distribution p(y|x ) . In particular , for a given test example x and a logistic regression model with the weight vector w and the bias term b , the entropy of the distribution p(y|x ) is calculated as :
H(y|x ) = −p(−|x ) log p(−|x ) − p(+|x ) log p(+|x )
The larger the entropy is , the more uncertain we are about the class labels of the example x . We refer to this baseline model as the logistic regression active learning , or LR AL for short . The second reference model is based on support vector machine [ 31 ] that is already discussed in the section of related work . In this method , the classification uncertainty of an example x is determined by its distance to the decision d(x ; w , b ) =
|wT x + b| w2
The smaller the distance d(x ; w , b ) is , the more the classification uncertainty will be . We refer to this approach as support vector machine active learning , or SVM AL for short . Finally , both the logistic regression model and the support vector machine that are only trained over the labeled examples are used in our experiments as the baseline models . By comparing to these two baseline models , we are able to determine the amount of benefits that are brought by different active learning algorithms .
To evaluate the performance of the proposed active learning algorithms , we first pick 100 training samples , which include 50 positive examples and 50 negative examples , randomly from dataset for each category . Both the logistic regression model and the SVM classifier are trained on the labeled data . For the active learning methods , 100 unlabeled data samples are chosen for labeling and their performances are evaluated after rebuilding the classifiers respectively . Each experiment is carried out 40 times and the averaged F 1 with its variance is calculated and used for final evaluation .
To deploy efficient implementations of our scheme toward large scale text categorization tasks , all the algorithms used in this study are programmed in the C language . The testing hardware environment is on a Linux workstation with 3.2GHz CPU and 2GB physical memory . To implement the logistic regression algorithm for our text categorization tasks , we employ the implementation of the logistic regression tool developed by Komarek and Moore recently [ 13 ] . To implement our active learning algorithm based on the bound optimization approach , we employ a standard math package , ie , LAPACK [ 1 ] , to solve the eigen decomposition in our algorithm efficiently . The SVMlight package [ 10 ] is used in our experiments for the implementation of SVM , which has been considered as the state of the art tool for text categorization . Since SVM is not parameter free and can be very sensitive to the capacity parameter , a separate validation set is used to determine the optimal parameters for configuration . 5.3 Empirical Evaluation
In this subsection , we will first describe the results for the Reuter 21578 dataset since this dataset has been most extensively studied for text categorization . We will then provide the empirical results for the two Web related datasets . 531 Experimental Results with Reuter 21578 Table 4 shows the experimental results of F 1 performance averaging over 40 executions on 10 major categories in the dataset .
First , as listed in the first and the second columns of Table 4 , we observe that the performance of the two classifiers , logistic regression and SVM , are comparable when only the 100 initially labeled examples are used for training . For categories , such as “ grade ” and ” interest ” , SVM achieves noticeably better performance than the logistic regression model . Second , we compare the performance of the two classifiers for active learning , ie , LogReg AL and SVM AL , that use the margin as the selection criterion . The results are listed in the third and the fourth columns of Table 4 . We
Category earn acq money fx grain crude trade interest wheat ship corn
SVM
92.12 ± 0.22 83.56 ± 0.26 64.06 ± 0.60 60.87 ± 1.04 67.78 ± 0.39 52.64 ± 0.46 56.80 ± 0.60 62.71 ± 0.72 67.11 ± 1.59 44.39 ± 0.84
LogReg
92.47 ± 0.13 83.35 ± 0.26 63.71 ± 0.63 58.97 ± 0.91 67.32 ± 0.48 48.93 ± 0.55 53.59 ± 0.60 57.38 ± 0.79 64.91 ± 1.75 41.15 ± 0.69
SVM AL 93.30 ± 0.28 85.96 ± 0.34 73.32 ± 0.38 74.95 ± 0.42 75.72 ± 0.24 66.41 ± 0.33 67.20 ± 0.39 86.01 ± 1.04 75.86 ± 0.53 71.27 ± 0.62
LogReg AL 93.40 ± 0.14 86.57 ± 0.32 71.21 ± 0.61 74.82 ± 0.53 74.97 ± 0.44 66.31 ± 0.33 66.15 ± 0.49 86.49 ± 0.27 72.82 ± 0.46 71.61 ± 0.60
LogReg BMAL 94.00 ± 0.09 88.07 ± 0.17 75.54 ± 0.26 77.77 ± 0.27 78.04 ± 0.14 69.29 ± 0.34 68.71 ± 0.37 88.15 ± 0.21 76.82 ± 0.34 74.35 ± 0.47
Table 4 : Experimental results of F1 performance on the Reuters 21578 dataset using 100 training samples ( % ) .
( a ) “ earn ”
( b ) “ acq ”
( c ) “ money fx ”
Figure 1 : Experimental results of F1 performance on the “ earn ” , “ acq ” and “ money fx ” categories find that the performance of these two active learning methods becomes closer than the case when no actively labeled examples are used for training . For example , for category “ trade ” , SVM performs substantially better than the logistic regression model when only 100 labeled examples are used . The difference in F 1 measurement between LogReg AL and SVM AL almost diminishes when both classifiers use the 100 actively labeled examples for training . Finally , we compare the performance of the proposed active learning algorithm , ie , LogReg BMAL , to the margin based active learning approaches LogReg AL and SVM AL . It is evident that the proposed batch mode active learning algorithm outperform the margin based active learning algorithms . For categories , such as “ corn ” and “ wheat ” , where the two margin based active learning algorithms achieve similar performance , the proposed algorithm LogReg BMAL is able to achieve substantially better F 1 scores . Even for the categories where the SVM performs substantially better than the logistic regression model , the proposed algorithm is able to outperform the SVM based active learning algorithm noticeably . For example , for category “ ship ” where SVM performs noticeably better than the logistic regression , the proposed active learning method is able to achieve better performance than the margin based active learning based on the SVM classifier .
In order to evaluate the performance in more details , we conduct the evaluation on each category by varying the num ber of initially labeled instances for each classifier . Fig 1 , Fig 2 and Fig 3 show the experimental results of the mean F 1 measurement on 9 major categories . From the experimental results , we can see that our active learning algorithm outperforms other two active learning algorithm in most of the cases while the SVM AL method is better than the LogReg AL method . We also found that the improvement of our active learning method is more evident compared to the other two approaches when the number of labeled instances is smaller . This is because the smaller the number of initially labeled examples used for training , the larger the improvement we would expect . When more labeled examples are used for training , the gap for future improvement begins to decrease . As a result , the three methods start to behavior similarly . This result also indicates that the proposed active learning algorithm is robust even when the number of labeled examples is small while the other two active learning approaches may suffer importantly when the margin criterion is not very accurate for the small sample case .
532 Experimental Results with Web Related Datasets The classification results of the WebKB dataset and the
Newsgroup dataset are listed in Table 5 and 6 .
First , notice that for the two Web related datasets , there are a few categories whose F 1 measurements are extremely low . For example , for the category “ staff ” of the WebKB
2040608010012014016008909091092093094095Number of Labeled SamplesF1SVMLogRegSVM−ALLogReg−ALLogReg−BMAL204060801001201401600760780808208408608809Number of Labeled SamplesF1SVMLogRegSVM−ALLogReg−ALLogReg−BMAL2040608010012014016005055060650707508085Number of Labeled SamplesF1SVMLogRegSVM−ALLogReg−ALLogReg−BMAL ( a ) “ grain ”
( b ) “ crude ”
( c ) “ trade ”
Figure 2 : Experimental results of F1 performance on the “ grain ” , “ crude ” and “ trade ” categories
( a ) “ interest ”
( b ) “ wheat ”
( c ) “ ship ”
Figure 3 : Experimental results of F1 performance on the “ interest ” , “ wheat ” and “ ship ” categories dataset , the F 1 measurement is only about 12 % for all methods . This fact indicates that the text categorization of WWW documents can be more difficult than the categorization of normal documents . Second , we observe that the difference in the F 1 measurement between the logistic regression model and the SVM is smaller for both the WebKB dataset and the Newsgroup dataset than for the Reuters21578 dataset . In fact , there are a few categories in WebKB and Newsgroup that the logistic regression model performs slightly better than the SVM . Third , by comparing the two margin based approaches for active learning , namely , LogReg AL and SVM AL , we observe that , for a number of categories , LogReg AL achieves substantially better performance than SVM AL . The most noticeable case is the category 4 of the Newsgroup dataset where the SVM AL algorithm is unable to improve the F 1 measurement than the SVM even with the additional labeled examples . In contrast , the LogReg AL algorithm is able to improve the F 1 measurement from 56.09 % to 6187 % Finally , comparing the LogReg BMAL algorithm to the LogReg AL algorithm , we observe that the proposed algorithm is able to improve the F 1 measurement substantially over the margin based approach . For example , for the category 1 of the Newsgroup dataset , the active learning algorithm LogReg AL only make a slightly improvement in the F 1 measurement with the additional 100 labeled examples . The improvement for the same category by the proposed batch active learning algorithm is much more significant , increasing from 83.12 % to 9112 % Compared to all the learning algorithms , the proposed learning algorithm achieves the best or close to the best performance for almost all categories . This observation indicates that the proposed active learning algorithm is effective and robust for large scale text categorization of WWW documents .
6 . CONCLUSIONS
This paper presents a novel active learning algorithm that is able to select a batch of informative and diverse examples for manually labeling . This is different from traditional active learning algorithms that focus on selecting the most informative examples for manually labeling . We use the Fisher information matrix for the measurement of model uncertainty and choose the set of examples that will effectively reduce the norm of the Fisher information matrix . We conducted extensive experimental evaluations on three standard data collections for text categorization . The promis
2040608010012014016005055060650707508Number of Labeled SamplesF1SVMLogRegSVM−ALLogReg−ALLogReg−BMAL204060801001201401600620640660680707207407607808Number of Labeled SamplesF1SVMLogRegSVM−ALLogReg−ALLogReg−BMAL2040608010012014016003504045050550606507075Number of Labeled SamplesF1SVMLogRegSVM−ALLogReg−ALLogReg−BMAL2040608010012014016004045050550606507075Number of Labeled SamplesF1SVMLogRegSVM−ALLogReg−ALLogReg−BMAL204060801001201401600450505506065070750808509Number of Labeled SamplesF1SVMLogRegSVM−ALLogReg−ALLogReg−BMAL2040608010012014016004505055060650707508085Number of Labeled SamplesF1SVMLogRegSVM−ALLogReg−ALLogReg−BMAL Category course department faculty project staff student
SVM
87.11 ± 0.51 67.45 ± 1.36 70.84 ± 0.76 54.06 ± 0.82 12.73 ± 0.44 74.05 ± 0.51
LogReg
89.16 ± 0.45 68.92 ± 1.39 71.50 ± 0.59 56.74 ± 0.57 12.73 ± 0.28 76.04 ± 0.49
SVM AL 88.55 ± 0.48 82.02 ± 0.47 75.59 ± 0.65 57.67 ± 0.98 19.48 ± 1.07 77.03 ± 0.95
LogReg AL 89.37 ± 0.65 79.22 ± 1.14 73.66 ± 1.23 56.90 ± 1.01 24.84 ± 0.58 80.40 ± 1.16
LogReg BMAL 90.99 ± 0.39 81.52 ± 0.46 76.81 ± 0.51 59.71 ± 0.82 21.08 ± 0.73 81.50 ± 0.44
Table 5 : Experimental results of F1 performance on the WebKB dataset using 40 training samples ( % ) .
Category
0 1 2 3 4 5 6 7 8 9 10
SVM
96.44 ± 0.35 83.38 ± 1.01 61.03 ± 1.51 72.36 ± 1.90 55.61 ± 1.06 70.58 ± 0.51 85.25 ± 0.45 39.07 ± 0.90 58.67 ± 1.21 69.35 ± 0.82 99.76 ± 0.10
LogReg
95.02 ± 0.45 83.12 ± 0.96 59.01 ± 1.39 71.96 ± 1.67 56.09 ±1.21 72.47 ±0.40 86.30 ±0.45 40.22 ±0.90 59.14 ±1.25 70.82 ±0.92 99.40 ±0.21
SVM AL 97.37 ± 0.52 91.61 ± 0.57 61.15 ± 2.08 73.15 ± 2.71 56.05 ±2.18 71.69 ±1.11 89.54 ±1.09 42.19 ±1.13 63.77 ±2.05 74.34 ±1.79 99.95 ± 0.02
LogReg AL 95.66 ± 1.01 85.07 ± 1.51 64.91 ± 2.52 75.88 ± 3.13 61.87 ±2.25 72.99 ±1.46 89.14 ±0.89 46.72 ±1.61 66.57 ±1.24 77.17 ±1.06 99.85 ±0.06
LogReg BMAL 98.73 ± 0.11 91.12 ± 0.36 66.13 ± 1.32 78.47 ± 1.95 61.91 ± 1.03 76.54 ± 0.43 92.07 ± 0.26 47.58 ± 0.76 67.07 ± 1.34 77.48 ± 1.20 99.90 ± 0.06
Table 6 : Experimental results of F1 performance on the Newsgroup dataset using 40 training samples ( % ) . ing results demonstrated that our method is more effective than the margin based active learning approaches , which have been the dominate method for active learning . We believe our scheme is essential to perform large scale categorization of text documents especially for the rapid growth of Web documents on World Wide Web .
7 . ACKNOWLEDGMENTS
We would like to thank Dr . Paul Komarek in CMU for sharing the text dataset and his kindly discussion in helping us deploy the logistic regression package correctly .
8 . REFERENCES [ 1 ] E . Z . B . Anderson . LAPACK user ’s guide ( 3rd ed )
Philadelphia , PA , SIAM , 1999 .
[ 2 ] C . Apte , F . Damerau , and S . Weiss . Automated learning of decision rulesfor text categorization . ACM Trans . on Information Systems , 12(3):233–251 , 1994 . [ 3 ] S . Boyd and L . Vandenberghe . Convex Optimization .
Cambridge University Press , 2003 .
[ 4 ] C . Campbell , N . Cristianini , and A . J . Smola . Query learning with large margin classifiers . In 17th International Conference on Machine Learning ( ICML ) , pages 111–118 , San Francisco , CA , USA , 2000 .
[ 5 ] W . W . Cohen . Text categorization and relational learning . In 12th International Conference on Machine Learning ( ICML ) , pages 124–132 , 1995 .
[ 6 ] S . Fine , R . Gilad Bachrach , and E . Shamir . Query by committee , linear separation and random walks . Theor . Comput . Sci . , 284(1):25–51 , 2002 .
[ 7 ] Y . Freund , H . S . Seung , E . Shamir , and N . Tishby .
Selective sampling using the query by committee algorithm . Mach . Learn . , 28(2 3):133–168 , 1997 .
[ 8 ] T . Graepel and R . Herbrich . The kernel gibbs sampler . In Advances in Neural Information Processing Systems 13 , pages 514–520 , 2000 .
[ 9 ] T . Joachims . Text categorization with support vector machines : learning with many relevant features . In Proc . 10th European Conference on Machine Learning ( ECML ) , number 1398 , pages 137–142 , 1998 . [ 10 ] T . Joachims . Making large scale svm learning practical . In Advances in Kernel Methods Support Vector Learning , MIT Press , 1999 .
[ 11 ] T . Joachims . Transductive inference for text classification using support vector machines . In Proc . 16th International Conference on Machine Learning ( ICML ) , pages 200–209 , San Francisco , CA , USA , 1999 .
[ 12 ] P . Komarek and A . Moore . Fast robust logistic regression for large sparse datasets with binary outputs . In Artificial Intelligence and Statistics ( AISTAT ) , 2003 .
[ 13 ] P . Komarek and A . Moore . Making logistic regression a core data mining tool : A practical investigation of accuracy , speed , and simplicity . In Technical Report TR 05 27 at the Robotics Institute , Carnegie Mellon University , May 2005 .
[ 14 ] A . Krogh and J . Vedelsby . Neural network ensembles , cross validation , and active learning . In Advances in Neural Information Processing Systems , volume 7 , pages 231–238 . The MIT Press , 1995 .
[ 15 ] M . Lan , C . L . Tan , H B Low , and S . Y . Sung . A comprehensive comparative study on term weighting schemes for text categorization with support vector machines . In Posters Proc . 14th International World Wide Web Conference , pages 1032–1033 , 2005 .
[ 16 ] D . D . Lewis and W . A . Gale . A sequential algorithm for training text classifiers . In Proc.17th ACM International SIGIR Conference , pages 3–12 , 1994 .
[ 17 ] R . Liere and P . Tadepalli . Active learning with committees for text categorization . In Proceedings 14th Conference of the American Association for Artificial Intelligence ( AAAI ) , pages 591–596 , MIT Press , 1997 .
[ 36 ] Y . Yang and J . O . Pedersen . A comparative study on feature selection in text categorization . In Proceedings 14th International Conference on Machine Learning ( ICML ) , pages 412–420 , Nashville , US , 1997 .
[ 18 ] T Y Liu , Y . Yang , H . Wan , Q . Zhou , B . Gao ,
[ 37 ] J . Zhang , R . Jin , Y . Yang , and A . Hauptmann .
H . Zeng , Z . Chen , , and W Y Ma . An experimental study on large scale web categorization . In Posters Proceedings of the 14th International World Wide Web Conference , pages 1106–1107 , 2005 .
Modified logistic regression : An approximation to svm and its applications in large scale text categorization . In Proc . 20th International Conference on Machine Learning ( ICML ) , Washington , DC , USA , 2003 .
[ 19 ] D . MacKay . Information based objective functions for
[ 38 ] T . Zhang and F . J . Oles . A probability analysis on the active data selection . Neural Computation , 4(4):590–604 , 1992 .
[ 20 ] B . Masand , G . Lino , and D . Waltz . Classifying news stories using memory based reasoning . In 15th ACM SIGIR Conference , pages 59–65 , 1992 .
[ 21 ] A . K . McCallum and K . Nigam . Employing EM and pool based active learning for text classification . In Proc.15th International Conference on Machine Learning , pages 350–358 . San Francisco , CA , 1998 .
[ 22 ] N . Roy and A . McCallum . Toward optimal active learning through sampling estimation of error reduction . In 18th International Conference on Machine Learning ( ICML ) , pages 441–448 , 2001 .
[ 23 ] M . E . Ruiz and P . Srinivasan . Hierarchical text categorization using neural networks . Information Retrieval , 5(1):87–118 , 2002 .
[ 24 ] G . Schohn and D . Cohn . Less is more : Active learning with support vector machines . In Proc . 17th International Conference on Machine Learning , pages 839–846 , 2000 .
[ 25 ] M . Seeger . Learning with labeled and unlabeled data .
Technical report , University of Edinburgh , 2001 .
[ 26 ] H . S . Seung , M . Opper , and H . Sompolinsky . Query by committee . In Computational Learning Theory , pages 287–294 , 1992 .
[ 27 ] L . K . Shih and D . R . Karger . Using urls and table layout for web classification tasks . In Proc . International World Wide Web Conference , pages 193–202 , 2004 .
[ 28 ] S . D . Silvey . Statistical Inference . 1974 . [ 29 ] J . Sturm . Using sedumi : a matlab toolbox for optimization over symmetric cones . Optimization Methods and Software , 11–12:625–653 , 1999 .
[ 30 ] M . Szummer and T . Jaakkola . Partially labeled classification with markov random walks . In Advances in Neural Information Processing Systems , 2001 .
[ 31 ] S . Tong and D . Koller . Support vector machine active learning with applications to text classification . In Proc . 17th International Conference on Machine Learning ( ICML ) , pages 999–1006 , Stanford , US , 2000 .
[ 32 ] K . Tzeras and S . Hartmann . Automatic indexing based on Bayesian inference networks . In Proc . 16th ACM Int . SIGIR Conference , pages 22–34 , 1993 .
[ 33 ] V . N . Vapnik . Statistical Learning Theory . John Wiley
& Sons , 1998 .
[ 34 ] Y . Yang . An evaluation of statistical approaches to text categorization . Journal of Information Retrieval , 1(1/2):67–88 , 1999 .
[ 35 ] Y . Yang and X . Liu . A re examination of text categorization methods . In 22nd ACM SIGIR Conference , pages 42–49 , Berkley , 1999 . value of unlabeled data for classification problems . In 17th International Conference on Machine Learning ( ICML ) , 2000 .
[ 39 ] J . Zhu . Semi supervised learning literature survey .
Technical report , Carnegie Mellon University , 2005 .
Let L(q ) be the objective function in ( 15 ) . We then have
APPENDIX A . PROOF OF INEQUATION i vk)2 i vk)2 i vk)2 i vk)2 qi q i
= k=1 k=1
λk
λk
×
L(q ) = s n s i=1 qiπi(1 − πi)(xT n n i=1 qiπi(1 − πi)(xT n iπi(1 − πi)(xT i=1 q iπi(1 − πi)(xT i=1 q n i=1 pix ≤ n n n ≤ n n iπi(1 − πi)(xT q i vk)2 jπj(1 − πj)(xT j=1 q i)2πi(1 − πi)(xT ( q j=1 q iπi(1 − πi)(xT iπi(1 − πi)(xT i vk)2 jπj(1 − πj)(xT n n i vk)2 i vk)2 qi q i=1 q i=1 q i=1
= qi i i=1 j vk)2 j vk)2 k=1
×
L(q ) ≤ s n ( cid:195 ) n s ( cid:179)n × n n
( q k=1 i=1 i=1
=
( q2 i ) qi
= i=1
λk iπi(1 − πi)(xT i=1 q i vk)2 i)2πi(1 − πi)(xT ( q j=1 q n i vk)2 jπj(1 − πj)(xT λk qi j=1 q i)2(xT jπj(1 − πj)(xT i vk)2πi(1 − πi ) j vk)2 j vk)2
( cid:180)2 qi s
( k=1 n
πi(1 − πi )
Substituting the above inequation back into ( 19 ) , we can achieve the following inequality :
Using the convexity property of reciprocal function , namely 1/ i=1 , we can arrive at the following deduction : x for x ≥ 0 and pdf {pi}n i=1 pi
( 19 )
1 qi q i
( 20 )
( xivk)2λk jπj(1 − πj)(xT j=1 q
. j vk)2)2
This finishes the proof of the inequality mentioned above .
