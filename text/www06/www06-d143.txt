Using Proportional Transportation Similarity with Learned
Element Semantics for XML Document Clustering
Institute of Computer Science and Technology , Peking University , Beijing 100871 , China
{wanxiaojun , yangjianwu}@icstpkueducn
Xiaojun Wan , Jianwu Yang of the the problems document
“ under contribution ” clustering , Proportional
ABSTRACT This paper proposes a novel approach to measuring XML document similarity by taking into account the semantics between XML elements . The motivation of the proposed approach is to overcome and “ over contribution ” existing in previous work . The element semantics are learned in an unsupervised way and the Proportional Transportation Similarity is proposed to evaluate XML document similarity by modeling similarity calculation as a transportation problem . Experiments of clustering are performed on three ACM SIGMOD data sets and results show the favorable performance of the proposed approach . Categories and Subject Descriptors : H33 [ Information Storage and Retrieval ] : Information Search and Retrieval – clustering General Terms : Theory , Experimentation Keywords : XML Transportation Similarity 1 . INTRODUCTION XML document clustering aims to group XML documents into different clusters with a standard of document similarity . Much previous work has explored clustering methods for grouping structurally similar XML documents so as to find common structures or DTDs for various purposes . While in this study we focus on grouping topically similar XML documents , ie , the text contents of the XML documents within a cluster express a similar topic or subject . Based on the traditional Vector Space Model ( VSM ) , XML documents are processed as ordinary unstructured documents by removing all element tags and thus the structural information is totally lost . Another intuitive method takes into account the structural information by computing the similarity of texts within each element respectively and then combining these similarities linearly , which is called C VSM . Much work [ 1 , 4 ] explores this problem by extending VSM in order to incorporate the element tag information . For SLVM proposed in [ 4 ] , each document , docx , is represented as a matrix , given as x=<dx(1),dx(2),…,dx(n)>T , dx(i)=<dx(i,1),dx(i,2),…,dx(i,m)> , where m is d the number of XML elements , and n is the number of terms . is a feature vector related to the term ti for all the d elements , dx(i,j ) is a feature related to the term ti and specific to the element ej , given as d x(i,j)= TF(ti,docx.ej)*IDF(ti ) and TF(ti,docx.ej ) is the frequency of the term ti in the element ej of the documents docx . And each dx(i,j ) is normalized by ∑ . The similarity between two documents docx and docy is then defined with an
Copyright is held by the International World Wide Web Conference Committee ( IW3C2 ) . Distribution of these papers is limited to classroom use , and personal use by others . WWW 2006 , May 23–26 , 2006 , Edinburgh , Scotland . ACM 1 59593 323 9/06/0005 . x R mn ×∈
∈)( ix jixd
R d
),( m i element semantic matrix M doc doc sim
(
, e introduced , given as dM ) •
= d
•
T n
SLVM x
∑ i
1 = y ix )( e iy )(
( 1 ) where Me is an m*m element semantic matrix which captures both the similarity between a pair of XML elements as well as the contribution of the pair to the overall document similarity . The popular way to determine the value of Me is based on the edit distance [ 5 ] . In order to acquire a more appropriate element semantic matrix , with the notion that term similarity should be affecting document similarity and vice versa , we propose an iterative algorithm for learning the element semantic matrix Me , given as
S d
= n
∑ ( λ i i
1 =
T
⋅
B i )(
•
BM
• e
/ ) n i )(
M e
= n
∑ ( λ i i
1 =
⋅
B i )(
•
S d
•
T
B i )(
/ ) n
( 2 )
( 3 )
T
,
,
=
)2(
B i
B i
λ i
|,| 1
=<
)| ∞
/9.0
LL max(| nB ( R
BB )1( represents a
> ) B pm ×∈)( i r B , set of XML is a matrix with its kth column documents , where corresponding to dk(i ) of the k th document and p is the number of . All the entries’ values documents . of matrix Sd are normalized between zero and one . Two totally different documents have a similarity value of zero and two identical documents have a similarity value of one . An additional constraint for getting a non trivial solution of having both matrices with all zero elements is required to force the diagonal elements of Sd ( ie , the similarity of identical documents ) to take the value of one . VSM does not consider structural information and C VSM only allows the text within an element of one document to correspond to the text within the same element of the other document . The two cases ignore the semantic relationship between different elements and have “ under contribution ” problem . SLVM overcomes the above “ under contribution ” problem by allowing the text within an element of one document to correspond to the text within any element of the other document . However , the corresponding relation between elements is loose , ie the text with a weight within an element of one document can always use its total weight to correspond to the text within any element of the other document , which the so called “ over contribution ” problem . In order to address the above problems of “ under contribution ” the Proportional and “ over contribution ” , illuminated by Transportation Distances [ 2 ] , we propose the Proportional Transportation Similarity ( PTS ) the document similarity . PTS allows the text within an element of one document to correspond to the text within any element of the other document to measure is believed to have two under a few strict constraints by modeling a transportation problem in the linear programming field . The document similarity over one single term is computed as follows : feature vectors dx(i)=<dx(i,1),dx(i,2),…,dx(i,m)> and Given dy(i)=<dy(i,1),dy(i,2),…,dy(i,m)> , related to the particular term ti for all the elements in documents docx and docy respectively , where m is the number of elements , a weighted graph G is constructed as follows : Let dx(i)=<dx(i,1),dx(i,2),…,dx(i,m)> as the weighted point set for the term ti in document docx , dx(i,j ) is a feature related to the term ti and specific to the element ej given as the TF(ti,docx.ej)*IDF(ti ) value . Let dy(i)=<dy(i,1),dy(i,2),…,dy(i,m)> as the weighted point set for the term ti in document docy , dy(i,j ) is a feature related to the term ti and specific to the element ej given as the TF(ti,docy.ej)*IDF(ti ) value Let G={dx(i ) , dy(i ) , Me} as a weighted graph constructed by dx(i ) , dy(i ) , and Me . V=dx(i)∪dy(i ) is the vertex set while Me is the edge , matrix , either learned or obtained based on edit distance . )i(xdW x(i ) , dy(i ) , ie the sums of all weights )i(ydW of points within dx(i ) , dy(i ) , respectively . Based on the weighted graph G , the possible flows ζ = [ fuv ] , with fuv the flow from dx(i,u ) to dy(i,v ) , are defined by the following constraints : are the total weights of d
0≥uvf m f =∑ uv v
1 = mu ≤≤1 mv ≤≤1 d uix ),( mu ≤≤1 m f =∑ uv u
1 =
Wd viy d ),( W d iy )( ix )( mv ≤≤1 m m
∑ ∑ u
1 = v
1 = f uv
= ixW d
)(
( 4 )
( 5 )
( 6 )
( 7 )
Constraint ( 4 ) allows moving weights from dx(i ) to dy(i ) and not vice versa . Constraint ( 5 ) and ( 7 ) force all of dx(i ) ’s weight to move to the positions of points in dy(i ) . Constraint ( 6 ) ensures that this is done in a way that preserves the old percentages of weight in dy(i ) . And PTS is given by
PTS
( d
, d
)i(x
)i(y
)
= max
F
∈
ζ m m
∑ ∑
1u =
1v =
Mf uv
(
) e uv
( 8 )
The overall document similarity based on PTS is obtained as sim
( doc x
, doc y
)
=
PTS
PTS
( d
, d ix )(
) iy )(
( 9 ) n
∑ i
1 =
The above transportation problem can be efficiently solved by interior point algorithms [ 3 ] which have polynomial time complexity . 2 . EXPERIMENTAL RESULTS In the experiments , we use the agglomerative hierarchical clustering ( AHC ) algorithm as the cluster engine . Three benchmarking datasets with different sizes extracted from ACM
SIGMOD Record 19991 , which is composed of hundreds of documents from past issues of SIGMOD Record , are used for evaluation . The class each record belonging to is given by the “ category ” tag2 . The weighted F measure is used as the evaluation metric . Table 1 gives the results . For SLVM and PTS , either the edit distance approach or the learning method can be employed to acquire element semantics , the corresponding results are given in different columns respectively .
Table 1 . F measure ( % ) comparison results VSM C VSM
SLVM
PTS
Data Set
Learned Semantics
Learned Semantics
Edit
Distance 43.3 22.9 35.1
Edit
Distance 46.0 24.2 38.8
36.9 22.4 47.7
40.0 21.2 42.4
58.9 46.6 69.9
53.0 43.0 58.8
ACM 8 ACM 12 ACM 16
Seen from the above table , the proposed PTS with learned element semantics performs best over all three data sets . The approaches with the learned element semantics , ie SLVM and PTS , both significantly outperform the traditional approaches not considering the element semantics , ie VSM and C VSM , which shows the importance of incorporating the element semantics for XML document clustering . For SLVM and PTS , we find that the performance based on the learning method for acquiring the element semantics is significantly better than that based on the edit distance approach , which demonstrates that the learned element semantics can reflect the true underlying semantics between XML elements . The experimental results demonstrate that with the appropriate element semantics , PTS can the performance by circumventing both the “ over contribution ” problem and the “ under contribution ” problem . 3 . REFERENCES [ 1 ] A . Doucet , H . A . Myka . Naive Clustering of a Large XML improve
Document Collection . In Proceedings of the 1st INEX , Germany , 2002 .
[ 2 ] P . Giannopoulos and R . C . Veltkamp . A Pseudo Metric for
Weighted Point Sets . In Proceedings of the 7th European Conference on Computer Vision ( ECCV ) , 715–730 , 2002 . [ 3 ] N . Karmarkar . A new polynomial time algorithm for linear programming . In Proceedings of the Sixteenth Annual ACM Symposium on Theory of Computing , 302 311 , 1984 .
[ 4 ] JW Yang and XO Chen . A semi structured document model for text mining . Journal of Computer Science and Technology , 17(5 ) : 603 610 , 2002 .
[ 5 ] K . Zhang and D . Shasha . Simple fast algorithms for the editing distance between trees and related problems . SIAM J . Comput . , 18(6):1245–1262 , 1989 .
1http://wwwacmorg/sigs/sigmod/record/xml/XMLSigmodRecord Mar1999.zip
2All the “ category ” tags as well as the inner texts and the corresponding descriptions in the record are removed to make the answer class blind to the clustering algorithm .
