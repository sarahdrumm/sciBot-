Co optimization of Multiple Relevance Metrics in Web
Search
Dong Wang1,2,* , Chenguang Zhu1,2,* , Weizhu Chen2 , Gang Wang2 , Zheng Chen2
1Institute for Theoretical
Computer Science Tsinghua University Beijing , China , 100084
{wd890415 , zcgcs60}@gmailcom
2Microsoft Research Asia
No . 49 Zhichun Road
Haidian District
Beijing , China , 100080
{v dongmw , v chezhu , wzchen , gawa , zhengc}@microsoft.com that find
ABSTRACT Several relevance metrics , such as NDCG , precision and pSkip , are proposed to measure search relevance , where different metrics try to characterize search relevance from different perspectives . Yet we empirically the direct optimization of one metric cannot always achieve the optimal ranking of another metric . In this paper , we propose two novel relevance optimization approaches , which take different metrics into a global consideration where the objective is to achieve an ideal tradeoff between different metrics . To achieve this objective , we propose to co optimize multiple relevance metrics and show their effectiveness . Categories and Subject Descriptors H33 [ Information Systems ] : Information Search and Retrieval ; General Terms Algorithms , Design , Experimentation , Theory . Keywords Learning to Rank , User Feedback , LambdaRank . 1 . INTRODUCTION Recent advances in search relevance have positioned it as a very important aspect of information retrieval ( IR ) , and traditional works to improve search relevance can be grouped into two different categories based on the kinds of metrics used for optimization . The first one aims to improve relevance from explicitly judged labeled data by learning a ranking model to optimize a metric , like NDCG [ 4 ] . We call this kind of metric an explicit relevance metric since it ’s based on the explicit data . The other category looks for ways to improve search relevance by leveraging large scale implicit user behavior log data from commercial search engines , and optimize another kind of metric , like CTR [ 2 ] , pSkip [ 5 ] . We call this kind of metric an implicit relevance metric since it ’s based on implicit data . However , to the best of our knowledge , previous works mostly focus on optimizing one metric to improve search relevance , though both the explicit relevance metric and implicit metric have their own merits [ 3 ] . Yet , we empirically observe that the exclusive optimization of one metric cannot always achieve the optimal ranking of another metric . For example , directly
Copyright is held by the author/owner(s ) . WWW 2010 , April 26–30 , 2010 , Raleigh , North Carolina , USA . ACM 978 1 60558 799 8/10/04 . optimizing NDCG on the explicit data often results in a nonoptimal relevance for pSkip on the implicit data , and vice versa . We may see this conflict from a lot of real examples . As an instance , for a query 𝑞 , we will only consider its three URLs : 𝑢1 , 𝑢2 and 𝑢3 . For a case that 𝑢1 and 𝑢2 are both rated as Excellent while 𝑢2 has a higher click frequency than 𝑢1 , if we only optimize NDCG , the NDCG is maximized if we put 𝑢1 > 𝑢2 , where > means the right part is put below the left part in the search result ; however , the pSkip doesn’t achieve the optimal result since we put 𝑢2 with higher click frequency below 𝑢1 . In this extreme case , if we can optimize NDCG and pSkip simultaneously , we may put 𝑢2 > 𝑢1 , so NDCG and pSkip can both achieve the optimal result . For another case : 𝑢2 is a duplicate of 𝑢1 , so most users won’t click 𝑢2 and will likely jump to 𝑢3 if they are unsatisfied with 𝑢1 . So if 𝑢1 and 𝑢2 are more relevant than 𝑢3 , maximizing NDCG will rank them as 𝑢1 > 𝑢2 > 𝑢3 , while optimizing pSkip will rank them as 𝑢1 > 𝑢3 > 𝑢2 based on the click frequency . All of these real cases illustrate that we cannot solve this kind of conflict if we only consider one metric in optimization . Conversely , if we can take both metrics into consideration , it ’s possible for us to find an ideal tradeoff to optimize both metrics simultaneously . In this paper , we propose to co optimize the explicit relevance metric and implicit relevance metric simultaneously with our objective being to find an ideal co optimization approach . Especially , we aim to answer the question : how can we maximize one metric without even slightly sacrificing another metric ? For example , we aim to find a ranking function that optimizes pSkip with the constraint that the decrease of the NDCG score is less than 0.1 percent . To achieve this objective , we propose two novel methods from different machine learning approaches to co optimize multiple relevancies . ,fi 2 . LEARNING MODELS Exclusive optimization for explicit metric cannot always achieve the optimal value for implicit metric , and verse vice . Here we propose two combination models . 2.1 Indirect Optimization Model Firstly , we propose indirect optimization model . In this model , we try to integrate CTR into the calculation of NDCG . In order
,fi*This work was done when the first and second authors were visiting Microsoft Research Asia .
WWW 2010 • PosterApril 26 30 • Raleigh • NC • USA1199 to balance two measurements , we add a tradeoff parameter 𝛼 into our optimization function as ( 1 ) :
𝑓𝐼𝑂 = 1 𝑓𝑚𝑎𝑥
𝑖
2𝑟𝑞 𝑖 ∙ 𝛼𝐶𝑇𝑅 𝑑𝑞 𝑖 +1−𝛼
𝑙𝑜𝑔 1+𝑖
( 1 ) where 𝑓𝑚𝑎𝑥 is the normalizing factor being the ideal evaluation score , 𝑟𝑞 𝑖 is the rating for document ranked at position 𝑖 . 𝐶𝑇𝑅 𝑑𝑞 𝑖 is the click through rate for document ranked at position 𝑖 . Here , we use LambdaRank[1 ] to optimize the evaluation function . The 𝜆𝑖𝑗 here is as ( 2 ) : 𝜕𝐶 𝜕 𝑜𝑖 ,𝑗
𝑓𝐼𝑂 ≡ 𝑆𝑖𝑗 |∆𝑓𝐼𝑂 𝑖𝑗 𝜆𝑖𝑗
Here 𝑆𝑖𝑗 equals 1 when 𝑑𝑞 𝑖 is more valuable than 𝑑𝑞 ( 𝑗 ) and 1 otherwise . 2.2 Direct Optimization Model Moreover , we propose direct optimization model . For direct optimization we built the optimization function as ( 3 ) :
( 2 )
|
𝑓𝐷𝑂 = 𝛼𝑓 + 1 − 𝛼 𝑁𝐷𝐶𝐺
( 3 ) Here 𝑓 is an implicit evaluation function like CTR or pSkip . We can generate two 𝜆 gradients for each pair of training documents during the training process . One is generated by document ’s label in order to optimize NDCG and the other is generated by user implicit feedback in order to optimize 𝑓 . So that the total 𝜆gradient for each pair of search result is ( 4 ) : 𝑓 + 1 − 𝛼 𝜆𝑖𝑗
𝜆𝑖𝑗 ≡ 𝛼𝜆𝑖𝑗
𝑁𝐷𝐶𝐺
( 4 )
|(5 )
|(6 )
𝜕𝐶 𝜕𝑜𝑖 ,𝑗
More specially , 𝜆𝑖𝑗 for optimize NDCG and 𝑓𝑝𝑆𝑘𝑖𝑝 is as ( 5 ) : 𝜕𝐶 𝜕𝑜𝑖 ,𝑗
𝜆𝑖𝑗 ≡ 𝛼𝑆𝑖𝑗 |∆𝑓𝑝𝑆𝑘𝑖𝑝 𝑖𝑗
| + 1 − 𝛼 𝑆′
𝑖𝑗 |∆𝑁𝐷𝐶𝐺𝑖𝑗
𝜕𝐶 𝜕 𝑜𝑖,𝑗
And 𝜆𝑖𝑗 for optimize NDCG and 𝑓𝐶𝑇𝑅 as ( 6 ) :
𝜆𝑖𝑗 ≡ 𝛼𝑆𝑖𝑗 |∆𝑓𝐶𝑇𝑅@𝑝 𝑖𝑗
𝜕𝐶 𝜕 𝑜𝑖,𝑗
| + 1 − 𝛼 𝑆′
𝑖𝑗 |∆𝑁𝐷𝐶𝐺𝑖𝑗
𝑖𝑗 may be different since they get their
Notice that 𝑆𝑖𝑗 and 𝑆′ value by different evaluation function . 3 . EXPERIMENTAL RESULTS We set two experiments to show the performance of our learning models . More specifically , our experiments show that we can improve implicit relevance such as CTR , pSkip with explicit relevance NDCG no significant drop , and vice versa . We compare different learning models on a large real dataset . In the following diagram , IO : Stand for indirect optimization model . DO : Stand for direct optimization model . pSkip vs . NDCG@10 DO_pSkip
IO_pSkip
0.64
0.62
0 1 @ G C D N
0.6
0.58
0.56
0.54 pSkip
0.82
0.825
0.83
0.835
0.84
0.845
0.85
0.855
Figure 1 : curve generated by pSkip and NDCG@10
In Figure 1 , we show the performance of direct optimization model and indirect optimization model are almost the same when pSkip is high , but direct optimization model will get a higher NDCG score when pSkip score is low . Moreover , we get the same NDCG score and decrease pSkip score by 2 % in our new learning models .
CTR@10 vs . NDCG@10
DO_CTR@10
IO_CTR@10
0.63
0 1 @ G C D N
0.61
0.59
0.57
0.55
0.74
0.76
0.78
CTR@10
0.8
0.82
Figure 2 : curve generated by CTR@10 and NDCG@10
In Figure 2 , we show the performance of combing 𝑓𝐶𝑇𝑅@10 with NDCG by our learning models . We see indirect optimization model is more sensitive than direct optimization model . Both two models increase CTR score by 4 % with NDCG score remains the same . Overall , Indirect optimization model always treat explicit relevance as important metric . Direct optimization model can achieve the optimal point for any tradeoff parameter . 4 . CONCLUSION In this paper we investigate two novel approaches to cooptimize implicit relevance metric and explicit relevance metric , and evaluate our learning models’ performance by the curve generated by NDCG , CTR and pSkip as entity metrics . By optimizing the combination function of these metrics , we can reach an ideal balance between explicit relevance metric and implicit metric . Especially , we achieve a better pSkip or CTR score without drop of NDCG score . 5 . REFERENCES [ 1 ] Burges CJC , Ragno R . , and Le QV Learning to rank with non smooth cost function . Proceedings of NIPS , 2006 . [ 2 ] Fox S . , Karnawat K . , Mydland M . , Dumais ST , and White T . Evaluating implicit measures to improve the search experience . ACM Transactions on Information Systems , 23:147–168 , 2005 .
[ 3 ] Huffman SB , and Hochster M . How well does result relevance predict session satisfaction ? In Proc . of SIGIR , 2007 .
[ 4 ] Jarvelin , K . , and Kekanainen , J . ( 2000 ) . Ir evaluation relevant documents . methods retrieving highly Proceedings of SIGIR 2000 , 41–48 . for
[ 5 ] Wang K . , Walker T . , and Zheng Z . PSkip : Estimating relevance ranking quality from web search clickthrough data . Proceedings of KDD , 2009 .
WWW 2010 • PosterApril 26 30 • Raleigh • NC • USA1200
