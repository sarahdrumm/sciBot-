Co optimization of Multiple Relevance Metrics in Web
Search
Dong Wang1,2,* , Chenguang Zhu1,2,* , Weizhu Chen2 , Gang Wang2 , Zheng Chen2
1Institute for Theoretical
Computer Science Tsinghua University Beijing , China , 100084
{wd890415 , zcgcs60}@gmailcom
2Microsoft Research Asia
No . 49 Zhichun Road
Haidian District
Beijing , China , 100080
{v dongmw , v chezhu , wzchen , gawa , zhengc}@microsoft.com that find
ABSTRACT Several relevance metrics , such as NDCG , precision and pSkip , are proposed to measure search relevance , where different metrics try to characterize search relevance from different perspectives . Yet we empirically the direct optimization of one metric cannot always achieve the optimal ranking of another metric . In this paper , we propose two novel relevance optimization approaches , which take different metrics into a global consideration where the objective is to achieve an ideal tradeoff between different metrics . To achieve this objective , we propose to co optimize multiple relevance metrics and show their effectiveness . Categories and Subject Descriptors H33 [ Information Systems ] : Information Search and Retrieval ; General Terms Algorithms , Design , Experimentation , Theory . Keywords Learning to Rank , User Feedback , LambdaRank . 1 . INTRODUCTION Recent advances in search relevance have positioned it as a very important aspect of information retrieval ( IR ) , and traditional works to improve search relevance can be grouped into two different categories based on the kinds of metrics used for optimization . The first one aims to improve relevance from explicitly judged labeled data by learning a ranking model to optimize a metric , like NDCG [ 4 ] . We call this kind of metric an explicit relevance metric since it â€™s based on the explicit data . The other category looks for ways to improve search relevance by leveraging large scale implicit user behavior log data from commercial search engines , and optimize another kind of metric , like CTR [ 2 ] , pSkip [ 5 ] . We call this kind of metric an implicit relevance metric since it â€™s based on implicit data . However , to the best of our knowledge , previous works mostly focus on optimizing one metric to improve search relevance , though both the explicit relevance metric and implicit metric have their own merits [ 3 ] . Yet , we empirically observe that the exclusive optimization of one metric cannot always achieve the optimal ranking of another metric . For example , directly
Copyright is held by the author/owner(s ) . WWW 2010 , April 26â€“30 , 2010 , Raleigh , North Carolina , USA . ACM 978 1 60558 799 8/10/04 . optimizing NDCG on the explicit data often results in a nonoptimal relevance for pSkip on the implicit data , and vice versa . We may see this conflict from a lot of real examples . As an instance , for a query ğ‘ , we will only consider its three URLs : ğ‘¢1 , ğ‘¢2 and ğ‘¢3 . For a case that ğ‘¢1 and ğ‘¢2 are both rated as Excellent while ğ‘¢2 has a higher click frequency than ğ‘¢1 , if we only optimize NDCG , the NDCG is maximized if we put ğ‘¢1 > ğ‘¢2 , where > means the right part is put below the left part in the search result ; however , the pSkip doesnâ€™t achieve the optimal result since we put ğ‘¢2 with higher click frequency below ğ‘¢1 . In this extreme case , if we can optimize NDCG and pSkip simultaneously , we may put ğ‘¢2 > ğ‘¢1 , so NDCG and pSkip can both achieve the optimal result . For another case : ğ‘¢2 is a duplicate of ğ‘¢1 , so most users wonâ€™t click ğ‘¢2 and will likely jump to ğ‘¢3 if they are unsatisfied with ğ‘¢1 . So if ğ‘¢1 and ğ‘¢2 are more relevant than ğ‘¢3 , maximizing NDCG will rank them as ğ‘¢1 > ğ‘¢2 > ğ‘¢3 , while optimizing pSkip will rank them as ğ‘¢1 > ğ‘¢3 > ğ‘¢2 based on the click frequency . All of these real cases illustrate that we cannot solve this kind of conflict if we only consider one metric in optimization . Conversely , if we can take both metrics into consideration , it â€™s possible for us to find an ideal tradeoff to optimize both metrics simultaneously . In this paper , we propose to co optimize the explicit relevance metric and implicit relevance metric simultaneously with our objective being to find an ideal co optimization approach . Especially , we aim to answer the question : how can we maximize one metric without even slightly sacrificing another metric ? For example , we aim to find a ranking function that optimizes pSkip with the constraint that the decrease of the NDCG score is less than 0.1 percent . To achieve this objective , we propose two novel methods from different machine learning approaches to co optimize multiple relevancies . ,fi 2 . LEARNING MODELS Exclusive optimization for explicit metric cannot always achieve the optimal value for implicit metric , and verse vice . Here we propose two combination models . 2.1 Indirect Optimization Model Firstly , we propose indirect optimization model . In this model , we try to integrate CTR into the calculation of NDCG . In order
,fi*This work was done when the first and second authors were visiting Microsoft Research Asia .
WWW 2010 â€¢ PosterApril 26 30 â€¢ Raleigh â€¢ NC â€¢ USA1199 to balance two measurements , we add a tradeoff parameter ğ›¼ into our optimization function as ( 1 ) :
ğ‘“ğ¼ğ‘‚ = 1 ğ‘“ğ‘šğ‘ğ‘¥
ğ‘–
2ğ‘Ÿğ‘ ğ‘– âˆ™ ğ›¼ğ¶ğ‘‡ğ‘… ğ‘‘ğ‘ ğ‘– +1âˆ’ğ›¼
ğ‘™ğ‘œğ‘” 1+ğ‘–
( 1 ) where ğ‘“ğ‘šğ‘ğ‘¥ is the normalizing factor being the ideal evaluation score , ğ‘Ÿğ‘ ğ‘– is the rating for document ranked at position ğ‘– . ğ¶ğ‘‡ğ‘… ğ‘‘ğ‘ ğ‘– is the click through rate for document ranked at position ğ‘– . Here , we use LambdaRank[1 ] to optimize the evaluation function . The ğœ†ğ‘–ğ‘— here is as ( 2 ) : ğœ•ğ¶ ğœ• ğ‘œğ‘– ,ğ‘—
ğ‘“ğ¼ğ‘‚ â‰¡ ğ‘†ğ‘–ğ‘— |âˆ†ğ‘“ğ¼ğ‘‚ ğ‘–ğ‘— ğœ†ğ‘–ğ‘—
Here ğ‘†ğ‘–ğ‘— equals 1 when ğ‘‘ğ‘ ğ‘– is more valuable than ğ‘‘ğ‘ ( ğ‘— ) and 1 otherwise . 2.2 Direct Optimization Model Moreover , we propose direct optimization model . For direct optimization we built the optimization function as ( 3 ) :
( 2 )
|
ğ‘“ğ·ğ‘‚ = ğ›¼ğ‘“ + 1 âˆ’ ğ›¼ ğ‘ğ·ğ¶ğº
( 3 ) Here ğ‘“ is an implicit evaluation function like CTR or pSkip . We can generate two ğœ† gradients for each pair of training documents during the training process . One is generated by document â€™s label in order to optimize NDCG and the other is generated by user implicit feedback in order to optimize ğ‘“ . So that the total ğœ†gradient for each pair of search result is ( 4 ) : ğ‘“ + 1 âˆ’ ğ›¼ ğœ†ğ‘–ğ‘—
ğœ†ğ‘–ğ‘— â‰¡ ğ›¼ğœ†ğ‘–ğ‘—
ğ‘ğ·ğ¶ğº
( 4 )
|(5 )
|(6 )
ğœ•ğ¶ ğœ•ğ‘œğ‘– ,ğ‘—
More specially , ğœ†ğ‘–ğ‘— for optimize NDCG and ğ‘“ğ‘ğ‘†ğ‘˜ğ‘–ğ‘ is as ( 5 ) : ğœ•ğ¶ ğœ•ğ‘œğ‘– ,ğ‘—
ğœ†ğ‘–ğ‘— â‰¡ ğ›¼ğ‘†ğ‘–ğ‘— |âˆ†ğ‘“ğ‘ğ‘†ğ‘˜ğ‘–ğ‘ ğ‘–ğ‘—
| + 1 âˆ’ ğ›¼ ğ‘†â€²
ğ‘–ğ‘— |âˆ†ğ‘ğ·ğ¶ğºğ‘–ğ‘—
ğœ•ğ¶ ğœ• ğ‘œğ‘–,ğ‘—
And ğœ†ğ‘–ğ‘— for optimize NDCG and ğ‘“ğ¶ğ‘‡ğ‘… as ( 6 ) :
ğœ†ğ‘–ğ‘— â‰¡ ğ›¼ğ‘†ğ‘–ğ‘— |âˆ†ğ‘“ğ¶ğ‘‡ğ‘…@ğ‘ ğ‘–ğ‘—
ğœ•ğ¶ ğœ• ğ‘œğ‘–,ğ‘—
| + 1 âˆ’ ğ›¼ ğ‘†â€²
ğ‘–ğ‘— |âˆ†ğ‘ğ·ğ¶ğºğ‘–ğ‘—
ğ‘–ğ‘— may be different since they get their
Notice that ğ‘†ğ‘–ğ‘— and ğ‘†â€² value by different evaluation function . 3 . EXPERIMENTAL RESULTS We set two experiments to show the performance of our learning models . More specifically , our experiments show that we can improve implicit relevance such as CTR , pSkip with explicit relevance NDCG no significant drop , and vice versa . We compare different learning models on a large real dataset . In the following diagram , IO : Stand for indirect optimization model . DO : Stand for direct optimization model . pSkip vs . NDCG@10 DO_pSkip
IO_pSkip
0.64
0.62
0 1 @ G C D N
0.6
0.58
0.56
0.54 pSkip
0.82
0.825
0.83
0.835
0.84
0.845
0.85
0.855
Figure 1 : curve generated by pSkip and NDCG@10
In Figure 1 , we show the performance of direct optimization model and indirect optimization model are almost the same when pSkip is high , but direct optimization model will get a higher NDCG score when pSkip score is low . Moreover , we get the same NDCG score and decrease pSkip score by 2 % in our new learning models .
CTR@10 vs . NDCG@10
DO_CTR@10
IO_CTR@10
0.63
0 1 @ G C D N
0.61
0.59
0.57
0.55
0.74
0.76
0.78
CTR@10
0.8
0.82
Figure 2 : curve generated by CTR@10 and NDCG@10
In Figure 2 , we show the performance of combing ğ‘“ğ¶ğ‘‡ğ‘…@10 with NDCG by our learning models . We see indirect optimization model is more sensitive than direct optimization model . Both two models increase CTR score by 4 % with NDCG score remains the same . Overall , Indirect optimization model always treat explicit relevance as important metric . Direct optimization model can achieve the optimal point for any tradeoff parameter . 4 . CONCLUSION In this paper we investigate two novel approaches to cooptimize implicit relevance metric and explicit relevance metric , and evaluate our learning modelsâ€™ performance by the curve generated by NDCG , CTR and pSkip as entity metrics . By optimizing the combination function of these metrics , we can reach an ideal balance between explicit relevance metric and implicit metric . Especially , we achieve a better pSkip or CTR score without drop of NDCG score . 5 . REFERENCES [ 1 ] Burges CJC , Ragno R . , and Le QV Learning to rank with non smooth cost function . Proceedings of NIPS , 2006 . [ 2 ] Fox S . , Karnawat K . , Mydland M . , Dumais ST , and White T . Evaluating implicit measures to improve the search experience . ACM Transactions on Information Systems , 23:147â€“168 , 2005 .
[ 3 ] Huffman SB , and Hochster M . How well does result relevance predict session satisfaction ? In Proc . of SIGIR , 2007 .
[ 4 ] Jarvelin , K . , and Kekanainen , J . ( 2000 ) . Ir evaluation relevant documents . methods retrieving highly Proceedings of SIGIR 2000 , 41â€“48 . for
[ 5 ] Wang K . , Walker T . , and Zheng Z . PSkip : Estimating relevance ranking quality from web search clickthrough data . Proceedings of KDD , 2009 .
WWW 2010 â€¢ PosterApril 26 30 â€¢ Raleigh â€¢ NC â€¢ USA1200
