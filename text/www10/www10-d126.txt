Building Taxonomy of Web Search Intents for Name Entity
Queries
Xiaoxin Yin
Microsoft Research One Microsoft Way
Redmond , WA 98052 xyin@microsoft.com
Sarthak Shah Microsoft Corp .
One Microsoft Way
Redmond , WA 98052 sarthaks@microsoft.com
ABSTRACT A significant portion of web search queries are name entity queries . The major search engines have been exploring various ways to provide better user experiences for name entity queries , such as showing “ search tasks ” ( Bing search ) and showing direct answers ( Yahoo! , Kosmix ) . In order to provide the search tasks or direct answers that can satisfy most popular user intents , we need to capture these intents , together with relationships between them .
In this paper we propose an approach for building a hierarchical taxonomy of the generic search intents for a class of name entities ( eg , musicians or cities ) . The proposed approach can find phrases representing generic intents from user queries , and organize these phrases into a tree , so that phrases indicating equivalent or similar meanings are on the same node , and the parent child relationships of tree nodes represent the relationships between search intents and their sub intents . Three different methods are proposed for tree building , which are based on directed maximum spanning tree , hierarchical agglomerative clustering , and pachinko allocation model . Our approaches are purely based on search logs , and do not utilize any existing taxonomies such as Wikipedia . With the evaluation by human judges ( via Mechanical Turk ) , it is shown that our approaches can build trees of phrases that capture the relationships between important search intents .
Categories and Subject Descriptors H33 [ Information Storage and Retrieval ] : Information Search and Retrieval – search process . it is reported 71 % of queries contain name entities [ 14 ] . For simplicity we will use the word “ entity ” to refer to name entity .
Recently there are some new methods used by different search engines to help users find the right information faster and more effectively for entity queries . The first method is to provide relevant “ search tasks ” ( Bing search ) . For example , when the user searches for an entity , Bing usually shows several popular tasks for this entity . As shown in Figure 1(a ) , for query {Britney Spears}1 Bing shows five tasks : Songs , Tickets , Tour , Albums , and Biography , together with three result snippets for each task . The tasks are all generic ones for a certain class of entities ( eg , musicians ) , which are found through mining the search logs . The second method is to show “ direct answers ” for what the user might be searching for , which saves the efforts of inspecting result snippets . Kosmix and WolframAlpha show only such “ direct answers ” , while Yahoo! and Google mix such information with regular results . Given the query {Britney Spears} , Kosmix shows her biography , similar artists , albums , genres , etc . ( Figure 1(b) ) , and Yahoo! shows her image , videos , official site , songs , and links to albums , lyrics , photos and videos ( Figure 1(c) ) .
Although these methods bring more convenience to users , there exist two limitations . First , the direct answers only work well for certain classes of entities . For example , for query {university of washington} , Yahoo! provides no direct answer , while Kosmix shows a simple list of direct answers without most popular query intents such as sports , admissions , etc .
Second , these approaches put the major generic search intents
General Terms Algorithms , Measurement , Experimentation .
Keywords Web search intent , query clustering .
1 . INTRODUCTION
Until a few years ago , most major search engines return only ten result snippets to user , and let the user find useful results by reading the snippets . Although this has been very successful , it costs much user effort in reading snippets , and it is not always possible for the search engine to accurately capture the user ’s intent when it is not clearly indicated in the query . This problem is more obvious for name entity queries , since different users may search for different aspects of a name entity using the same query , and it is very difficult for the search engine to infer the exact search intent . Name entity queries are a most popular type of queries . According to an internal study of Microsoft , at least 20 30 % of queries submitted to Bing search are simply name entities , and
Copyright is held by the International World Wide Web Conference Committee ( IW3C2 ) . Distribution of these papers is limited to classroom use , and personal use by others . WWW 2010 , April 26 30 , 2010 , Raleigh , North Carolina , USA . ACM 978 1 60558 799 8/10/04 .
( a ) Search tasks of Bing ( left pane ) ( b ) Top part of Kosmix result page
( c ) Top part of Yahoo! result page
Figure 1 : The result pages containing search tasks or different aspects of information by Bing , Kosmix , and Yahoo!
1 We use “ {x} ” to represent a web search query x . for a class of entities into a simple flat list . But these intents actually form a taxonomy , as some intents are related and some are sub concepts of others . Take Britney Spears ( or any musician ) as an example . “ Albums ” , “ songs ” , “ lyrics ” , and “ music videos ” are all about her music , “ biography ” and “ profile ” are about information of her life and career , and “ tours ” and “ tickets ” are about her concerts . Showing a flat list of search tasks or direct answers that mix up different aspects of information makes it difficult for users to find what he wants or get an overview of the entity .
The purpose of our study is to organize the generic search intents for a class of entities into a taxonomy according to the relationship between different intents . A class of entities is a set of entities that are usually considered to be of the same type , such as musicians , movies , car models , cities , etc . We work on classes of entities instead of individual ones , because the sparseness and noises in the data prevent us from accurately inferring the relationships between intents of different queries involving a single entity . By aggregating data involving many entities we can better capture the relationships between intents . An example taxonomy of major generic search intents for musicians is shown in Figure 2 , which is generated by our approach .
Our work can help search engines in three ways . First , it can help to organize the search tasks by selecting tasks for representative intents and avoiding redundancy . With the taxonomy of intents we can also easily create a “ tree of search tasks ” , so that a user can navigate in the tree to find the right task . Second , our work can help to better organize the direct answers , so that their layout is consistent with the taxonomy of user intents . The direct answers can also be organized into a tree according to the taxonomy , whose nodes can be dynamically shown to users when necessary . Third , our work can help web developers and users better understand queries containing name entities , and provide an overview of different search intents for a class of entities .
Outline and contributions
Baeza Yates et al . propose the concept of query relationships , including equivalence , IS A , and overlapping relationships [ 2 ] . However , only equivalence relationship is studied in [ 2 ] , which is also studied in [ 3][5][24 ] . In this paper we study both equivalence and IS A relationships between search intents , and how to build taxonomy of intents based on such relationships .
The first challenge is how to identify generic search intents for a class of entities . For each class of entities , we can find the generic search intents from user queries . For example , if many users search for {[musician ] lyrics} for many different musicians , we can know “ lyrics ” is an important search intent for musicians . root song , albums song lyrics lyrics for youtube discography cd hits lyrics music , videos music videos tv show downloads videos de mp3 listen to pics , pictures of movie tickets tour band tour dates concert schedule , fan club fan concert dates wikipedia , wiki singer who is death pictures , photos , images concert biography , bio
Figure 2 : Result intent tree of musicians ( most popular phrases only )
There are usually many users explicitly indicating their search intents in queries , and thus we can capture almost every major generic search intent for a class of entities . We represent such generic intents using words and phrases co appearing with entities in user queries . If a word or phrase co appears with a number of entities in user queries ( eg , “ lyrics ” , “ biography ” ) , it usually represents a generic intent , and we call it an “ intent phrase ” .
The second challenge is how to infer the relationship between two intent phrases . Some intent phrases carry the same intent , such as “ pictures ” , “ pics ” , “ photos ” , “ images ” in queries for musicians . Some intents are sub concepts of some other intents . For example , “ wallpapers ” are a type of “ pictures ” , and “ wikipedia ” is source of “ biography ” . We propose a method for inferring the relationship between two intent phrases based on user clicks , which indicate their intents . Although it is often difficult to accurately infer the relationship between two queries , there are usually many entities that co appear with two intent phrases in queries , and we can aggregate the relationships between these queries to infer the relationship between the two intent phrases . Our experiments show such aggregation improves accuracy significantly .
The third challenge is how to organize intent phrases into a tree , so that each node contains intent phrases corresponding to the same intent , and the child nodes of a node correspond to subconcepts of this node . We propose three algorithms for this task . The first algorithm is based on Directed Maximum Spanning Tree [ 6][10 ] . The second algorithm is adapted from Hierarchical Agglomerative Clustering [ 8][21 ] . The third algorithm is based on Pachinko allocation models [ 18 ] for building hierarchical topic models for documents . These algorithms are evaluated with human labeled data on ten classes of entities .
The rest of this paper is organized as follows . We discuss related work in Section 2 . Section 3 presents the problem formulation , and Section 4 describes how to infer the relationships between intent phrases . We present approaches for building intent trees in Section 5 , and empirical study in Section 6 . This study is concluded in Section 7 .
2 . RELATED WORK
There are some studies using existing taxonomy to generate related queries , or organize queries or information into an existing taxonomy . Ariel et al . [ 11 ] utilizes query click logs and the taxonomy of Open Directory Project ( ODP , wwwdmozorg ) to generate related keywords for advertisers . In [ 20 ] Paşca and Alfonseca utilize WordNet and Wikipedia to organize the information extracted from Web into a hierarchy . Hu et al . [ 16 ] identify search intents of queries by considering Wikipedia categories and articles as possible search intents . Our approach is different from above approaches as we do not utilize any existing , manually edited taxonomies . This is because these taxonomies are very different from user intents . For example , the ODP taxonomy for “ Music ” is shown in Figure 3 . Neither this taxonomy , nor the taxonomies from Wikipedia or WordNet , can be used to represent the search intents for musicians . Therefore , we purely rely on user behaviors to build hierarchies of generic search intents .
Baeza Yates et al . study relationships between queries [ 2 ] , in which “ IS A ” relationship is mentioned but not studied or computed . Wen et al . [ 24 ] study query clustering by their similarities . In [ 4 ] Chuang et al . use hierarchical agglomerative clustering to organize queries into hierarchical clusters . These papers study how to organize queries using their similarities . But they do not infer the “ IS A ” relationships between queries or search intents , and do not build taxonomies that reflect such relationships , which are the goals of our study .
Music
Bands and artists
Instruments
People
Regional
 
By instruments
By style
Cover bands
  Early
Electronic Human voice  
Agents and managers
Bands and artists
Big bands   Africa
Asia  
Figure 3 : ODP taxonomy for music
Another related category of studies is how to find related queries as query suggestions , which has been studied from may perspectives . Cui et al . [ 7 ] uses the clicked URLs of queries to find related queries . In [ 5 ] Chien and Immorlica find related queries using the temporal query frequency information . Cao et al . [ 3 ] first group queries into clusters in an offline process , and then utilize query sequences to find related queries . In [ 22 ] Wang et al . propose an approach for generating the broad intent aspects for query suggestions , based on search session data . In [ 19 ] Paşca studies extracting different classes of entities , and top attributes for each class . Our study is different from these studies because we study a different problem of building taxonomy of query intents . We infer relationships between generic intents for a class of entities , which is more feasible than inferring relationships between queries , and does not suffer from ambiguity of keywords .
3 . PROBLEM FORMULATION
Our study is about the generic intents on certain classes of entities . A class of entities is a set of entities that people usually consider them to be of the same type . Examples include musicians , soccer athletes , presidents of US , movies , car manufacturers , cell phone models , superstores , TV networks , cities , etc . Almost every entity can be categorized into some classes . It is usually easy to get a class of entities . For most classes we can think about , there is corresponding categories or lists on Wikipedia , such as enwikipediaorg/wiki/Category:Lists_of_musicians for musicians . Such lists can also be gathered by approaches such as [ 19 ] .
Because a large portion of web search queries are entity queries and many others contain names of entities , improving user experiences for entity queries is very important for search engines . The goal of our study is to find generic search intents associated with a class of entities , identify relationships between different generic intents , and organize them into a hierarchical structure . Similar to studies on query suggestion [ 5][7][22 ] , we represent a specific search intent by a query , such as {britney spears biography} .
We represent generic intents by intent phrases . For a query containing an entity name as a substring , we define the query excluding the entity name as an intent phrase . For example , for query {britney spears biography} with “ britney spears ” being an entity , “ biography ” is an intent phrase . We consider an intent phrase carrying the same generic intent no matter whether it appears before or after the entities in queries .
Different intent phrases may represent the same generic intent . Take intent phrases for musicians as an example . “ Biography ” , “ bio ” , “ biography of ” all represent the same generic intent . Some intent phrases may indicate generic intents that are sub concepts of others . For example , “ history ” , “ life ” , and “ death ” represent sub concepts of “ biography ” .
For each class of entities , our approach performs three tasks : ( 1 ) Find all important intent phrases . ( 2 ) Through the query click behaviors of users , determine the relationships between intent phrases , ie , whether one intent phrase is equivalent to , unrelated to , or represents a sub concept of another intent phrase . ( 3 ) Based on the relationships , organize the intent phrases into a hierarchical structure ( like the one in Figure 2 ) , so that each node represents a generic intent and contains all intent phrases for it , and the child nodes of each node represent sub concepts of that node . The correctness of the intent trees will be evaluated by human judges . Non Goals
In this paper we do not study the following problems : ( 1 ) How to find a class of entities , which can usually be done through parsing Wikipedia categories/lists , or using automatic approaches such as that proposed by Paşca [ 19 ] . ( 2 ) Ranking search tasks or generic intents for a specific entity , which can be done by analyzing query frequencies , query refinements , and clicked URLs of queries involving a specific entity . Many existing approaches on ranking query suggestions can be applied here .
4 . INTENT PHRASES AND THEIR RELATIONSHIPS 4.1 Intent Phrases
We hope to find all intent phrases that represent generic search intents involving entities of a certain class . For each generic intent ( eg , lyrics of a musician ) , there are usually many users expressing the intent explicitly in their queries , and thus we can capture most generic intents using all intent phrases . On the other hand , to make sure an intent phrase represents a generic intent for different entities , the intent phrase should appear with at least θ entities in queries ( θ = 5 in our study ) . We exclude a small set of stop phrases that do not indicate clear intents , such as “ web ” , “ online ” , “ 2009 ” , and inappropriate phrases such as “ nude ” and “ sex ” . There are 185 stop phrases , including misspelled terms . Table 1 shows the top intent phrases for some classes of entities , sorted by the number of entities they co appear within queries . We can see they represent very important intents for each class of entities . Some of them are redundant ( eg , “ jobs ” and “ employment ” ) , and we will study how to infer relationships between intent phrases .
4.2 Relationships between Intent Phrases
Before defining the relationships between intent phrases , we need to first define the relationships between the intents of different queries . We say the intents of query q1 belong to those of q2 , if the information ( or web pages ) that can satisfy the needs of q1 can also satisfy the needs of q2 . For example , a user searching for {Seattle} is usually looking for tourism , general information , or government of Seattle . Therefore , the intents of queries like {Seattle tourism} and {Seattle government} belong to those of {Seattle} . Queries q1 and q2 are likely to be equivalent if the intents of each of them belong to those of the other .
The search intent of a query is inherently subjective and varies
Table 1 : Top intent phrases for certain classes of entities
Musicians
Universities library employment jobs bookstore address athletics alumni tuition
Actors actor photos biography pictures imdb bio
Cities city city of news real estate hospital apartments lyrics music youtube wikipedia songs Wiki wikipedia movies jobs map discography biography for different users . As shown by Lee et al . [ 17 ] , the intent of a query can usually be indicated by clicks of the user . It is also proposed by Baeza Yates et al . [ 2 ] that relationships between queries can be defined with clicked URLs . Therefore , we propose an approach for deciding the relationship between intents of two queries by their clicked URLs .
For a query q submitted by a user , a search engine returns a ranked list of result URLs . There are three possibilities for each result URL : ( 1 ) Clicked , ( 2 ) skipped , ie , a URL ranked below it is clicked , and ( 3 ) neither clicked nor skipped . According to the studies of Joachims et al . [ 15 ] , the assumption that a clicked URL is more relevant than a skipped URL can provide a large number of preferences between pairs of URLs with reasonable accuracy .
In order to determine the relationships between intents of different queries through their clicked URLs , we need to be able to estimate the relevance of a URL for a query based on the search logs . We adopt a model similar to the one proposed in [ 13 ] . Suppose we are given the query click logs of a search engine in a period of time . We say a URL is viewed for a query if it is either clicked or skipped . Consider a query q in the logs , and a URL u that is viewed for q . Let click(q , u ) be the number of times a user clicks u for q , and skip(q , u ) be the number of times a user skips u for q . We define the relevance of u for q as ( ) uq ( skip uq
. ( 1 ) click +
) ε+
( uq click uq rel
=
)
)
(
,
,
,
, rel(q , u ) is the estimated probability for u being clicked when it is viewed for q . ε is a smoothing factor , which reflects the fact that we are not confident about the estimated relevance when we do not see many clicks/skips for a URL . ( ε is set to 1.0 in our study . ) As discussed in [ 13][23 ] , the measure based on probability of being clicked for viewed URLs is quite resistant to positional bias . For two queries q1 and q2 , we want to estimate the degree of the search intents of q1 being included in those of q2 . This is a challenging problem as search intent is implicitly implied by user behaviors . Because clicked URLs indicate query intents , distribution of clicks on the clicked URLs of a query is a good indicator of distribution of query intents . For example , the major clicked URLs of four queries involving “ Seattle ” are shown in Figure 4 , together with the percentage of Bing search users clicking on each URL for each query . We can see the major user intents for query {Seattle} are about tourism , official site of city , and hotels/attractions/restaurants . Most users querying for {city of Seattle} and {Seattle tourism} click on URLs frequently clicked for query {Seattle} , which indicate the intents of {city of Seattle} and {Seattle tourism} are included in that of {Seattle} .
DEFINITION 1 ( Relationship between intents of queries ) . Let U(q ) be the set of URLs clicked for q . For two queries q1 and q2 , the degree of q1 ’s intents being included in q2 ’s is defined as
( qd 1
⊆ q
2
)
=
∑ ( ∈ qUu 1 click )
( uq
,
1
∑ ( ∈ qUu 1 click )
)
⋅ rel
( uq
,
2
)
. ( 2 )
( uq
,
1
)
݀ሺݍଵ ⊆ ݍଶሻ is the average relevance to q2 of each clicked URL of q1 , and ݀ሺݍଵ ⊆ ݍଶሻ ∈ [ 0,1ሻ . If we could assume a user only clicks on URLs that satisfy his search intent and only skips URLs that do not , then rel(q , u ) is the estimated probability of a user who submits query q considers URL u to satisfy his intent . In this way ݀ሺݍଵ ⊆ ݍଶሻ is the estimated probability that a clicked URL for q1 can satisfy the search intent of a user querying for q2 . This will be a good estimation of the probability that q1 ’s intents are included in q2 ’s intents , if each URL does not satisfy multiple different intents . In reality some URLs can satisfy different ( but city of Seattle
75 % wwwseattlegov ( official site of city )
Seattle tourism
Seattle hotels
18 %
58 %
18 %
1.4 % enwikipediaorg/wiki/seattle wwwvisitseattleorg
( convention and visitor ’s bureau ) wwwseattlegov/html/visitor
( visiting seattle ) wwwseattlecom
( hotels , attractions , restaurants ) wwwseattle downtowncom
( Seattle hotels )
13 %
3.4 %
6 %
14.9 %
1.5 %
Seattle
Figure 4 : Percentage of users clicking each URL for four queries involving “ Seattle ” usually related ) intents . For example , wwwseattlecom contains information about Seattle ’s hotels , restaurants , attractions , etc . , which may confuse our algorithm by mixing up two weakly related or even unrelated intents .
Fortunately our goal is to identify the relationships between different intent phrases , and each intent phrase usually co appears with many different entities in many queries . We can achieve higher accuracy by aggregating the relationships between many pairs of queries involving different entities .
DEFINITION 2 ( Relationship between intent phrases ) . Let f(q ) be the query frequency of q , and let e+w be the query consisted of entity e and intent phrase w ( for simplicity we do not distinguish the queries in which w appears before e or after e ) . For a class of entities E , and two intent phrases w1 and w2 , the degree that the search intents of w1 are included in those of w2 is defined as )
+⊆+
( wef 1 we 2
+
)
⋅
∑ (
+
( wed 1 ) >
0
∈ wefEe 1
,
. ( 3 )
( wd 1
⊆ w 2
)
=
∑ (
+
+
( wef 1 ) >
0
)
∈ wefEe 1
,
݀ሺݓଵ ⊆ ݓଶሻ is the weighted average degree of the intents of a query containing w1 being included in those of a query containing w2 , if these two queries contain the same entity . ݀ሺݓଵ ⊆ ݓଶሻ ∈ [ 0,1ሻ for any w1 and w2 . If both ݀ሺݓଵ ⊆ ݓଶሻ and ݀ሺݓଶ ⊆ ݓଵሻ are high , then the intents of w1 and w2 should be very similar , and w1 and w2 are likely to be equivalent . If ݀ሺݓଵ ⊆ ݓଶሻ is high but ݀ሺݓଶ ⊆ ݓଵሻ is low , then w1 should correspond to a sub concept of w2 . We will use such relationships between intent phrases to organize the intent phrases into a tree .
5 . ORGANIZING INTENT PHRASES
In this section we describe our approaches for organizing intent phrases into intent trees . Each node of the intent tree contains one or more intent phrases with same or similar search intents . The child nodes of a node n should contain intent phrases that represent sub concepts of the intent phrase(s ) of n . There is a root node that contains no intent phrase . An example intent tree has been shown in Figure 2 .
We will present three approaches that solve this problem from different angles . The first approach is based on directed maximum spanning tree [ 10 ] . The second one is based on hierarchical agglomerative clustering [ 8 ] . The third one is a baseline approach based on Pachinko Allocation models [ 18 ] . 5.1 Method Based on Directed MST
Given the relationships between different intent phrases for a class of entities , a good intent tree should have the following property : For all pairs of intent phrases ( w1 , w2 ) that are in the same node or w1 in a child node of w2 , ∑ is high . This problem is similar to finding a maximum spanning tree
݀ሺݓଵ ⊆ ݓଶሻ
ሺ௪భ,௪మሻ
( MST ) in the directed graph containing a node for each intent phrase and an edge between each pair of related intent phrases . Directed MST has been studied in [ 6][10 ] . ( These papers study how to find Minimum Spanning Tree , which is equivalent to finding Maximum Spanning Tree . ) In order to apply such algorithms , we first define the “ goodness ” of an intent tree .
DEFINITION 3 ( Score of intent tree ) . Given the intent phrases w1,…,wK for a class of entities , and an intent tree T with nodes n1,…,nL containing w1,…,wK , we define the score of T as
( Ts
)
=
∑
∀ nji , i
,
∈ child max ( ∈
( nWwnWw
) ,
∈ k l i
( wd k
⊆ w l
)
+
) j
( n j
)
∑
β ( root
∈∀ ni , i child
)
L
+
∑∑ ( i
= 1
) ∈
( nWMST i
(
( wd k
⊆ w l
)
+
( wd l
⊆ w k
)
( 4 )
)
) ww l
, k child(n ) is the child nodes of n . W(n ) is the intent phrases of node n . β is the base weight of each node . For a node n , if ݀൫ݓ௜ ⊆ ݓ௝ሻ < ߚ for any wi of n and wj of some other node , then we do not consider n represent a sub concept of any other node , and n should be a child of the root . MST(W(n ) ) is the undirected maximum spanning tree constructed for a graph built from W(n ) , with a node for each phrase wk and an edge with weight ݀ሺݓ௞ ⊆ ݓ௟ሻ + ݀ሺݓ௟ ⊆ ݓ௞ሻ between phrases wk and wl . □ s(T ) contains three terms . The first and second terms are for relationships between all pairs of parent and child nodes , and the third term is for the inherent consistency of phrases within each node . If we consider the graph with each intent phrase as a node , and the relationship between each two intent phrases as an edge , then s(T ) is defined based on a spanning tree on this graph . We will show that the intent tree with maximum score can be built by finding the directed maximum spanning tree in a graph .
We hope to build an intent tree from the graph of intent phrases , so that each tree node may contain multiple equivalent intent phrases . However , this is not allowed in the tree generated by a maximum spanning tree ( MST ) algorithm . To use a MST algorithm , we define a semi intent tree , which can be generated by an MST algorithm and can be converted into an intent tree . Each non roof node in a semi intent tree contains a single intent phrase . Between each pair of nodes n1 and n2 , there is zero or one directed edge . Each edge ( n1 , n2 ) is either a belonging edge , which indicates n1 is a child of n2 ; or an equivalence edge , which indicates n1 and n2 contain equivalent intent phrases . Each non roof node has a single parent node . To convert a semi intent tree into an intent tree , we simply merge all nodes connected by equivalent edges into a single node .
Given the intent phrases w1,…,wK for a class of entities , we build a semi intent tree T’ of nodes n1,…,nK , with ni containing phrase wi . The score of T’ is measured as ( Ts
⊆ w w
+
+
=
)
'
) j
∑
β ( root
∈∀ ni , i child
)
( 5 )
∑
( wd i ) ( n j
∀ nji , i
,
∈ child
∑
( wd i ( equivalent n j
⊆ )
) j
∀ nji , i
,
∈
, where equivalent(n ) is the equivalent nodes of n .
Building Optimal Semi Intent Tree
In order to build an optimal semi intent tree , we first convert the intent phrases and their relationships into a directed graph G . For each phrase wi , we create a node ni containing wi . For each two phrases wi and wj with ݀൫ݓ௜ ⊆ ݓ௝൯ > 0 , we add a belonging edge from ni to nj with weight ݀൫ݓ௜ ⊆ ݓ௝൯ . We add two equivalent edges between ni and nj ( in both directions ) with weight ݀൫ݓ௜ ⊆ ݓ௝൯ + ݀൫ݓ௝ ⊆ ݓ௜൯ , if ݀ሺݓଵ ⊆ ݓଶሻ > ߚ , ݀ሺݓଶ ⊆ ݓଵሻ > ߚ , ݀ሺݓଵ ⊆ ݓଶሻ > ߙ ∙ ݀ሺݓଶ ⊆ ݓଵሻ , and ݀ሺݓଶ ⊆ ݓଵሻ > ߙ ∙
݀ሺݓଵ ⊆ ݓଶሻ . ߙ is a parameter between 0 and 1 , and a larger ߙ indicates more strict criterion for considering two nodes as equivalent . If ߙ = 1 , then no nodes are considered equivalent .
Given the graph G , we apply Edmond ’s algorithm [ 10 ] to build a directed maximum spanning tree . Edmond ’s algorithm is for building directed minimum spanning tree , which is equivalent to building directed maximum spanning tree . The main idea of Edmond ’s algorithm is as follows : ( 1 ) Select the edge with smallest weight that points to each node . ( 2 ) If no cycle formed , stop . ( 3 ) For each cycle formed , contract the nodes on the cycle into a pseudo node , and modify the weight of each edge pointing to this pseudo node according to weights of edges in the cycle . ( 4 ) Select the edge with smallest weight pointing to the pseudo node to replace another edge . ( 5 ) Go to step ( 2 ) with the new graph .
For a graph with n nodes and m edges , Edmond ’s algorithm takes O(mn ) time . A more efficient algorithm is presented in [ 12 ] , which takes O(m + n·logn ) time . Since we only care about intent phrases that co appear with at least a certain number of entities , the number of intent phrases is usually limited , and Edmond ’s algorithm is efficient enough . The maximum spanning tree on graph G is a semi intent tree , which can be easily converted into an intent tree by merging equivalent nodes .
Proof of Optimality
Here we give the sketch of the proof that the above algorithm can generate the optimal intent tree T* , so that s(T* ) ≥ s(T ) for any valid intent tree T .
LEMMA 1 . For any intent tree T , there exists a semi intent tree T’ so that s(T ) = s(T’ ) , and T’ can be converted into T .
PROOF SKETCH . Select all pairs of phrases wk and wl so that ݀ሺݓ௞ ⊆ ݓ௟ሻ is involved in computing s(T ) , and add edge between their corresponding nodes to construct a semi intent tree T’ ( adding equivalent edges for phrases of same node in T ) . In this way s(T ) = s(T’ ) and T’ can be converted into T . □
THEOREM 1 . The intent tree T* converted from the semi intent tree T’* built by directed maximum spanning tree algorithm is the intent tree with maximum score .
PROOF . If T* is not optimal , then there exists intent tree T so that s(T ) > s(T* ) . By Lemma 1 there exists semi intent tree T’ so that s(T’ ) > s(T’* ) , which contradicts with T’* being the maximum semi spanning tree . □
5.2 Method Based on Hierarchical Agglomerative Clustering
The above algorithm can find the optimal intent tree defined based on the maximum spanning tree . Its disadvantage is that the algorithm cannot merge nodes or edges during the procedure for finding maximum spanning tree . When we merge multiple equivalent intent phrases into a single node , or put one node as a child of another node , we should consider all intent phrases in the merged node and descendant nodes as a whole , and measure its overall relationships with other nodes . To accomplish this task , we modify the hierarchical agglomerative clustering ( average link ) algorithm [ 8 ] , so that it can run on a directed graph and perform two types of merging operations : ( 1 ) Putting one node as a child of another node , and ( 2 ) merging two nodes into one . Other hierarchical agglomerative clustering algorithms ( Single link and complete link ) are not used because they are highly vulnerable to noises , and the addition or removal of one edge can often significantly change the result clusters .
Our algorithm is described in Algorithm 1 . Lines 1 6 are for initializing a graph whose nodes represent the intent phrases and edges represent relationships between intent phrases . There are two types of edges . A type 1 edge ( ni , nj ) indicates putting node ni as a child of nj . A type 2 edge ( ni , nj ) indicates merging ni as an equivalent node of nj , which means merging the phrases and child nodes of ni into those of nj , and removing ni . Let ݀൫݊௜ ⊆ ݊௝൯ represent the average degree of any intent phrase of ni belongs to that of nj . We add a type 1 edge from ni to nj if ݀൫݊௜ ⊆ ݊௝൯ > 0 , if ݎ ∙ ቀ݀൫݊௜ ⊆ ݊௝൯ + ݀൫݊௝ ⊆ ݊௜൯ቁ > and add a maxቀ݀൫݊௜ ⊆ ݊௝൯,݀൫݊௝ ⊆ ݊௜൯ቁ , where ݎ is a parameter between 0.5 and 1 controlling whether to merge two nodes into one or put one as a child of another . If ݎ=0.5 , we only put nodes as children of other nodes ; if ݎ=1 , we only merge two nodes into one . type 2 edge
Lines 7 16 are for repeatedly merging nodes . In each iteration the edge with highest weight is selected . If it is a type 1 edge , we put one node as a child of the other ; otherwise we merge the two nodes into one . After merging node ni into nj in either way , we need to update the weights of edges involving nj . For every other node nk , suppose the original weight of edge from nj ( or ni ) to nk is djk ( or dik ) . Let f(w ) be the total frequency of all queries consisted of intent phrase w and an entity in the specific class , and ݂ሺ݊ሻ = ∑ , where W(n ) is the set of phrases of n . The new ௪∈ௐሺ௡ሻ weight from merged node nj to nk is an average of dik and djk , weighted by f(ni ) and f(nj ) , as follows
݂ሺݓሻ
݀′௝௞ = ௙ሺ௡೔ሻ∙ௗ೔ೖା௙൫௡ೕ൯∙ௗೕೖ
௙ሺ௡೔ሻା௙൫௡ೕ൯
. ( 6 )
It can be easily proved that if djk ( or dik ) is the weighted average degree of any intent phrase in nj ( or ni ) belonging to that of nk , then d’jk is that for the newly merged node nj . After updating the edges , the algorithm extracts the next edge with highest weight , and performs the merging .
Lines 17 19 create the final intent tree and output it . This algorithm is different from traditional average link algorithm in two aspects . First , in the input of our algorithm there can be two directed edges between each two objects , indicating bidirectional relationships . In contrast , average link algorithm only
Algorithm 1 . Average link for intent trees
Input : Intent phrases w1,…,wK for a certain class of entities .
݀൫ݓ௜ ⊆ ݓ௝൯ for 1≤ i , j ≤ K .
Output : An intent tree whose nodes contain w1,…,wK . Procedure : 1 : build graph G with a node ni containing each phrase wi 2 : for each ݀൫ݓ௜ ⊆ ݓ௝൯ > 0 3 : add edge ( ni , nj ) with weight=݀൫ݓ௜ ⊆ ݓ௝൯ and type=1 4 : if ݎ ∙ ቀ݀൫ݓ௜ ⊆ ݓ௝൯ + ݀൫ݓ௝ ⊆ ݓ௜൯ቁ > maxቀ݀൫ݓ௜ ⊆ ݓ௝൯,݀൫ݓ௝ ⊆ ݓ௜൯ቁ 5 : add edge ( ni , nj ) with weight=ݎ ∙ ቀ݀൫ݓ௜ ⊆ ݓ௝൯ + ݀൫ݓ௝ ⊆ ݓ௜൯ቁ and type=2 6 : E ← set of all edges in G 7 : while max௘∈ா ݁ . weight > min_sim 8 : ( ni , nj ) ← edge in E with maximum weight 9 : if ni or nj has been merged with other nodes then continue 10 : if ( ni , nj).type=1 11 : merge ni as a child of nj 12 : if ( ni , nj).type=2 13 : merge the phrases and children of ni as into those of nj 14 : remove ni 15 : for each node nk with no parent 16 : update ( nj , nk).weight and ( nk , nj).weight 17 : create intent tree T with T.root being an empty node 18 : put every node n with no parent as child of T.root 19 : output T handles simple , undirected similarity between objects . Second , our algorithm can put one node as a child node of another , besides merging two nodes into one as in average link algorithm . Comparing to the DMST based algorithm described in Section 5.1 , this algorithm updates the relationships between different nodes after any node is modified . When deciding which nodes to be merged , it considers the relationship between all descendants of each node , and all intent phrases on these descendants . This makes it more accurate than the DMST based algorithm according to our experiments , although it is a greedy algorithm .
It is sometimes necessary to select a representative intent phrase within a node . We define the query frequency of an intent phrase w as the sum of frequency of all queries consisted of w and an entity in the corresponding class . We find the intent phrase with highest query frequency is usually a good representative for a node , which is shown in the example intent trees in Section 64
5.3 Baseline Method Based on Pachinko Allocation models
The problem of generating hierarchical topic models has been studied in language modeling , and [ 18 ] presents a most popular approach called Pachinko Allocation models ( PAM ) , which builds a hierarchy of topics from a set of documents , with each topic being a distribution of words . We adapt this approach for building intent trees , which serves as our baseline approach .
To apply topic modeling on intent phrases for a class of entities , we consider all intent phrases from queries that lead to clicks on a certain URL u to be “ intent phrases of u ” , represented by W(u ) . W(u ) is a multi set , and the count of intent phrase w in W(u ) is the sum of click(q , u ) for any query q consisted of w and an entity in the specific class . In language modeling it is assumed that keywords appearing in the same document are generated from the topic distribution this document . We make a similar assumption that each intent phrase in W(u ) is generated from the topic distribution of URL u . In this way we can convert each URL into a pseudo document that contains a bag of intent phrases , and can apply PAM on these pseudo documents to generate hierarchical topic models .
The output of PAM is a four level DAG of topics , with root at the top level , super topics at the second level , sub topics at the third level , and intent phrases at the bottom level . Each intent phrase has a certain probability to belong to each sub topic , and so does each sub topic to each super topic . To build an intent tree , we put each intent phrase as a child of the sub topic that it belongs to with highest probability , and put each sub topic as a child of a super topic in the same way . A tree can be built in this way .
This tree is not an intent tree yet , because the super topics in the output of PAM are not associated with any intent phrases . We assign an intent phrase to a super topic if its probability of belonging to this super topic is at least η times its total probability of belonging to any sub topic , where η > 1 . In this way an intent tree can be built using PAM , which has three levels of nodes : The root , the super topics , and the sub topics .
6 . EXPERIMENTS
We now present the experimental evaluation of our approaches . All experiments are run on a Windows server with dual 2.66GHz Intel quad core CPU and 32GB main memory , or on the PC cluster of Microsoft . All experiments on the Windows server are run with a single core . 6.1 Data Collection
We test the approaches on ten datasets , each containing a certain class of entities , as shown in Table 2 . Nine of them are the set
Table 2 : Data sources for ten classes of entities
Class of entity car models
US clothing stores film actors musicians restaurants universities / colleges
US cities
US presidents
US retail companies
US TV networks
Num . Entity
859 103
19432 21091
694 7191 246 57 180 276
Wikipedia categories or Web source
2000s_automobiles clothing_retailers_of_the_united_states
*_film_actors
*_female_singers , *_male_singers , music_groups *_restaurants universities_and_colleges_* wwwmongabaycom/igapo/UShtm presidents_of_the_united_states retail_companies_of_the_united_states american_television_networks of entities from one or multiple Wikipedia categories or their descendent categories ( * represent wildcard match ) , and one is from a web page . For the nine classes from Wikipedia , we ignore all entities with a Wikipedia disambiguation page .
We use the query click logs of Bing search ( formerly as Live search ) from 2008/01/01 to 2008/12/31 . For each query that appears at least 6 times in the logs , we get the numbers of clicks and skips of each URL for this query . For each pair of queries with common clicked URL(s ) , we compute the degree of the intent of one query being included in the other , using MapReduce on a PC cluster . Based on this , we compute the relationships between the intents of each pair of intent phrases as in Section 42
We compare the performances of three approaches : ( 1 ) DMST , ‒ Method based on directed maximum spanning tree , ( 2 ) HAC , ‒ method based on hierarchical agglomerative clustering , and ( 3 ) PAM , ‒ method based on pachinko allocation model ( baseline approach ) . For each class of entities , we use each approach to build an intent tree . The top 400 intent phrases that co appear with most entities in search queries are considered for each class of entities . We ignore remaining intent phrases as they are usually obscure or do not carry generic intents .
6.2 Measurements by Mechanical Turk
Based on an intent tree , we say two intent phrases are either equivalent ( if they belong to the same node ) , one belonging to another ( if the node of one phrase is a descendent of the node of the other ) , or unrelated ( if none of the above ) . In order to measure the accuracy of each approach , we test how accurate it is in judging the relationship between two queries containing the same entity and different intent phrases . We use Amazon Mechanical Turk [ 1 ] to judge the relationships between queries . For each pair of queries q1 and q2 , a Mechanical Turk worker is asked to figure out their intents ( we provide buttons for showing Google ’s results of each query and ask them to search on Google if they are not sure ) , and select one of the following four options about the intents of q1 and q2 : ( 1 ) Unrelated , ( 2 ) q1 ’s intents belongs to those of q2 , ( 3 ) q1 ’s intents contain those of q2 , and ( 4 ) equivalent . We provide four to six examples for each type of cases , such as {tom hanks movies} and {how to contact tom hanks} ( unrelated ) , {dayton ohio furnitures} and {furniture in ohio} ( belongs ) , {go transit} and {go transit schedule} ( contains ) , and {track income tax return} and {track tax return} ( equivalent ) .
For each class of entities , we randomly select 250 query pairs , so that each pair of queries contain same entity and different intent phrases . The chance of a query being selected is proportional to the frequency of this query . To balance the number of query pairs of each type of relationship , we select one third of query pairs being unrelated , one third being equivalent , and one third in which one query belongs to another . In this step the relationship between a pair of queries is predicted by the intent tree generated either by DMST or HAC ( one of them is randomly selected to make each prediction ) . This will not lead to biased results because the predicted relationship is not indicated to the worker in any manner .
We have 2500 Mechanical Turk questions for the 10 classes of entities , and assign each question to three workers ( paying 1 cent to each ) . We find the chance that a worker agrees with another one is 698 % We only consider the questions on which at least two workers agree , and there are 2327 of such questions . The distribution of answers to these questions is as follows : 930 “ unrelated ” , 384 “ belongs ” , 288 “ contains ” , and 725 “ equivalent ” .
In order to see whether the answers by Mechanical Turk are accurate , we randomly choose 100 query pairs with Mechanical Turk judgments ( ten from each class ) , and manually identify the relationships between each pair of queries . Unless the relationship is obvious by common sense , we figure out the intents of each query according to the most clicked URL on Bing . The average accuracy of Mechanical Turk judgments is 083 The precision and recall for query pairs with each type of relationships is shown in Table 3 . Query pairs with “ contains ” relationship is converted into those of “ belongs ” by switching the order . In general Mechanical Turk judgments are quite accurate . Please note although there are only three categories of results in Table 3 , for each question there are four possible answers , because “ belongs ” relationship can happen in both directions .
Table 3 : Precision/Recall of Mechanical Turk
Precision
Unrelated Belongs
Equivalent
1.000 0.680 0.944
Recall 0.727 0.895 0.919
F1
0.842 0.773 0.931
In general the judgments by Mechanical Turk are reasonably accurate . There are some errors , such as {qvc handbags} is considered to belong to {qvc outlet} , while “ qvc ” is mainly an online store and these two queries have very different intents .
6.3 Query and Intent Phrase Relationship
We first check whether the query relationship defined in Def.1 in Section 4.2 is consistent with the judgments by Mechanical Turk . For each query pair q1 , q2 with judgments by Mechanical Turk , we get the degree of intents of one query being included in those of the other , ie , ݀ሺݍଵ ⊆ ݍଶሻ and ݀ሺݍଶ ⊆ ݍଵሻ . The distributions are shown in Figure 5 . We can see the distribution of relationships between unrelated query pairs and equivalent ones are mostly separated , while query pairs with one belonging to the other are less separated from other types .
P r o p o r t i o n
0.25
0.2
0.15
0.1
0.05
0
0
0.1
Unrelated Belongs Belongs ( other direction ) Equivalent
0.2
0.3
0.4
0.5
0.6
0.7
݀ሺݍଵ ⊆ ݍଶሻ or ݀ሺݍଶ ⊆ ݍଵሻ
0.8
0.9
1
Figure 5 : Distribution of relationships between queries for each type of query pairs
Then we use our manually labeled examples Then we use our manually labeled examples and Mechanical Turk data to measure the accuracy of query relationships . We set a to measure the accuracy of query relationships . We set a threshold τ . For each query pair q1 , q2 , we predict they are ( 1 ) , we predict they are ( 1 ) unrelated if ݀ሺݍଵ ⊆ ݍଶሻ ൑ ߬ and ݀ሺݍଶ ⊆ ݍଵሻ ൑ ߬ , ( 2 ) q1 belongs to q2 if ݀ሺݍଵ ⊆ ݍଶሻ > ߬ and ݀ሺݍଵ ⊆ ݍଶሻ > 2 ∙ ݀ ݀ሺݍଶ ⊆ ݍଵሻ , ( 3 ) q2 belongs to q1 likewise , and ( 4 ) equivalent if none of above . likewise , and ( 4 ) equivalent if none of above . We found τ = 0.09 leads to highest accuracy wrt manually labeled wrt manually labeled examples . The accuracy , precision , recall and F accuracy , precision , recall and F1 measure are shown in Table 4 .
ሻ
Table 4 : Precision/Recall of Inferred Query Relationship
: Precision/Recall of Inferred Query Relationships
By manually labeled data
By Mechanical Turk d By Mechanical Turk data a ) accuracy wrt γ b ) accuracy wrt min_sim b ) accuracy wrt min_sim
Accuracy unrelated belongs equivalent prec 0.763 0.125 0.700
0.540 rec'l 0.659 0.211 0.568
F1
0.707 0.157 0.627 prec . 0.698 0.195 0.623
0.543 rec'l 0.789 0.180 0.564
F1
0.741 0.187 0.592
We also measure the accuracy of relationships between intent We also measure the accuracy of relationships between intent phrases as defined in Def2 The same method is used to predict The same method is used to predict the relationship between two queries as mentioned the relationship between two queries as mentioned above , except τ = 0.26 which leads to optimal results , as shown in s shown in Table 5 .
Table 5 : Precision/Recall of Intent Phrase
Relationships
By manually labeled data
By Mechanical Turk data By Mechanical Turk data
Accuracy unrelated belongs equivalent prec 0.612 0.333 0.769
0.630 rec'l 0.932 0.105 0.541
F1
0.739 0.160 0.635 prec . 0.544 0.810 0.652
0.578 rec'l 0.939 0.111 0.598
F1
0.689 0.195 0.624
6.4 Accuracy of Intent Trees
Here we measure the accuracy of intent trees generated by intent trees generated by PAM , DMST , and HAC . For each class of entities , we use the For each class of entities , we use the intent tree generated by each approach to predict the relationship intent tree generated by each approach to predict the relationship between each pair of queries . The accuracy is defined as The accuracy is defined as percen a ) accuracy wrt η b ) accuracy wrt #super b ) accuracy wrt #super topic
: Accuracy of PAM Figure 6 : Accuracy of PAM
Figure 8 : Accuracy of
: Accuracy of HAC tage of predictions being correct .
Figure 6(a ) shows the average accuracy of PAM accuracy of PAM on all classes of entities wrt η , which controls when to put an intent phrase on , which controls when to put an intent phrase on topic ( top level node ) or a sub topic ( 2nd level node ) . Figa super topic ( top level node ) or a sub wrt number of super topics . We always ure 6 ( b ) shows accuracy wrt number of super topics to be four times that of super topics . set the number of sub topics to be four times that of super PAM achieves highest accuracy with η η = 1.8 and 10 super topics , which are used in all experiments ( including this one ) ( including this one ) unless specified otherwise . PAM fails to run on three classes of entities ( film PAM fails to run on three classes of entities ( film actors , musicians , and US cities ) be actors , musicians , and US cities ) because the input size is too large . PAM is also very slow and for Figure Figure 6(b ) we only test on three classes on which PAM finishes in 24 hours ( US clothing on which PAM finishes in 24 hours ( US clothing stores , US presidents and US TV networks ) . US TV networks ) .
Figure 7 ( a ) shows the average accuracy accuracy on ten classes of enticontrols when to merge intent phras(no such merging is done when α = 1 ) . Figure 7 controls whether to put a node child of root or that of another node . A smaller β means more DMST achieves highest ac= 0.08 , which are used in all experi ties by DMST wrt α , which controls when to merge in es into one node ( no such merging is done when ( b ) shows accuracy wrt β , which cont as a child of root or that of another node . nodes are put as child nodes of root . DMST achieves highest a curacy when α = 0.95 and β = 0.08 , which are used in ments ( including this one ) unless specified otherwise unless specified otherwise .
Figure 8(a ) shows the average accuracy of accuracy of HAC wrt γ , which controls when to merge intent phrases into one node . controls when to merge intent phrases into one node . When γ = 0.5 , no two intent phrases are merged into one node and the result 0.5 , no two intent phrases are merged into one node is a tree with a phrase on each node . When . When γ = 1 , no node is a child of another node , and the result is a set of clusters child of another node , and the result is a set of clusters generated by traditional hierarchical agglomerative clustering by traditional hierarchical agglomerative clustering . None of these two cases leads to best accuracy . Figure Figure 8(b ) shows the accuracy wrt min_sim , which is the similarity threshold for merging wrt min_sim , which is the similarity threshold for merging nodes . Highest accuracy is achieved when Highest accuracy is achieved when γ = 0.9 and min_sim = 0.03 , which are used in all experiments ( including this one ) . l experiments ( including this one ) .
Table 6 shows the accuracy of PAM , DMST , and shows the accuracy of PAM , DMST , and HAC on ten classes of entities . HAC achieves highest accuracy on most achieves highest accuracy on most classes , and DMST is slightly more accurate than PAM ( whose classes , and DMST is slightly more accurate than PAM ( whose
Table 6 : Accuracy on ten classes of entities : Accuracy on ten classes of entities a ) accuracy wrt α b ) accuracy wrt b ) accuracy wrt β
: Accuracy of DMST Figure 7 : Accuracy of DMST
Class of entities car models
US clothing stores film actors musicians restaurants universities / colleges
US cities
US presidents
US retail companies
US TV networks
Average
PAM(baseline )
0.5219 0.4758
N/A
N/A N/A
0.5771 0.4360
( baseline ) DMST 0.5020 0.5118 0.60000 0.6640 0.4542 0.6008 0.5696 0.6296 0.5000 0.5724 0.5604
0.6173 0.4333 0.6645 0.5323
HAC 0.7075 0.4976 0.7055 0.7652 0.4781 0.6613 0.8101 0.7613 0.6429 0.7171 0.6747
Table 7 : Precision , recall and F1 of each type of query pairs
( wrt Mechanical Turk judgments )
PAM ( baseline )
DMST
Accuracy unrelated belongs equivalent
.532 rec'l .924 .050 .549
.560 prec . rec'l .817 .678 .405 .308 .854 .379
F1 .646 .082 .653 prec .497 .220 .807
F1 .741
.350 .525
HAC
.675 prec rec’l F1 .727 .791 .389 .262 .723
.867 .198 .873
.791 accuracy is computed on seven classes ) . Considering there are four possible types of relationship for each query pair , HAC achieves reasonably high prediction accuracy of 06747
We also study the precision , recall and F1 for each type of query pairs judged by Mechanical Turk . As shown in Table 7 , HAC achieves best precision and recall for unrelated and equivalent query pairs , while DMST is the best for “ belongs ” relationship .
We also use the 100 manually labeled query pairs to measure the accuracy , precision , and recall or each approach , which is shown in Table 8 . The accuracy is higher than when measured with Mechanical Turk data , which contains noises . The relative performances of different approaches remain the same .
Table 8 : Precision , recall and F1 of each type of query pairs
( wrt manually labeled data )
PAM ( baseline )
Accuracy prec
.586 rec'l
DMST
.610
HAC
.760
F1 prec . rec'l F1 prec rec’l F1 unrelated
.609
.824
.700
.854
.796
.824
.848
.886
.867 belongs
0
0
0
.500
.737
.596
.625
.263
.370 equivalent
.684
.542
.605
.857
.324
.470
.762
.865
.810
Table 9 summarizes the accuracy of different approaches . Considering this is a four class classification problem ( “ unrelated ” , “ belongs ” in either direction , or “ equivalent ” ) , our approaches achieve reasonably high accuracy . It is shown that aggregating query relationships involving different entities is helpful , as intent phrase relationships are more accurate than query relationships . HAC achieves significantly higher accuracy , because clustering helps to better capture the relationships between intent phrases . For example , even if the inferred relationship between “ bio ” and “ wikipedia ” is very weak for a class of entities such as musicians , HAC can probably group “ bio ” and “ biography ” into a node , and identify “ wikipedia ” as a child node of that node , and thus infers the intents of “ wikipedia ” belongs to those of “ bio ” .
Table 9 : Summary of accuracy of different approaches
Approach MTurk query relation intent phrase relation
PAM
( baseline )
DMST HAC
Accuracy by labeled examples
Accuracy by MTurk data
0.830
0.540
0.630
0.586
0.610
0.760
N/A
0.543
0.578
0.532
0.560
0.675
Our approaches are less accurate in predicting the belonging relationships between intent phrases . There are two reasons . First , user clicks do not always represent user intents . There are many other factors such as positional bias , URLs containing many different contents , etc . , which may cause our approaches to make mistakes . For example , HAC assigns “ quotations ” as a child of “ quotes ” for US presidents , probably because “ quotations ” appears in much fewer queries than “ quotes ” . But they are actually equivalent . Second , user intents are often different from meanings of words . For example , most users searching for {[car model ] transmission} are interested in the transmission problems of the root parts , accessories new , car used , for sale manual , repair manual , service manual performance parts aftermarket parts body parts auto parts miles per gallon sedan used cars owners manual maintenance troubleshooting
2 door hybrid problems , recalls transmission , transmission problems
Figure 9 : Intent tree of car models by HAC car model , and thus HAC puts “ transmission ” as a child of “ problems ” , which is considered to be incorrect because “ transmission ” may have other intents . These incorrect relationships still capture the user behaviors , and the intent trees can still be reasonable and useful with these errors .
The intent tree for musicians generated by DMST is shown in Figure 2 . Here we show two other intent trees by HAC . Figure 9 shows the intent tree of car models , and Figure 10 shows that of US Presidents . Because of space limitation we only show nodes with most popular intent phrases . The query frequency of a node n is defined as the sum of frequencies of queries consisting of an entity and an intent phrase in n or any descendants of n . We show all nodes with query frequency higher than 0.5 % of the query frequency of root node , and ignore all intent phrases that are only slightly different with another intent phrase in the same node with higher frequency ( eg , we ignore “ biography of ” if “ biography ” is shown ) . The intent phrases within each node and child nodes of each node are ordered by their query frequencies .
From the intent trees we can see the hierarchical structures of user intents being captured . For example , in the intent tree for musicians the node “ music , videos ” has child nodes of “ song , albums ” , “ cd ” , “ music videos ” , “ mp3 ” , etc . , which are different types of music . In the intent tree for car models the node “ parts / accessories ” has child nodes of “ performance parts ” , “ aftermarket parts ” , “ body parts ” , and “ auto parts ” , which are popular types of parts . In the intent tree for US presidents the node containing “ biography ” has child nodes like “ presidency ” , “ wikipedia ” , “ education ” , and “ history ” . Such intent trees are very useful in providing the search tasks for search engines like Bing , and helping to organize the result pages for search engines like Kosmix .
6.5 Run time and Scalability
The first two steps of our approach are : ( 1 ) Computing the relationships between queries , and ( 2 ) computing the intent phrases for each class of entities , and relationships between them . We use president , biography , bio root pictures facts , facts about high school , middle school children , wife presidency wikipedia education history photos , images pictures president interesting facts fun facts for kids trivia school elementary school administration school district
Figure 10 : Intent tree of US Presidents by HAC
Table 10 : Run time ( seconds ) of each approach
Class of entities
#URL
PAM
DMST car_model
442721
203640 clothing_store
39965
56160 film_actors musician restaurants
1386353
1720721
N/A
N/A
71723
155280 universities_and_colleges
405994
91980 us_cities
382015
N/A us_president
21647
21480 us_retail_companies
138258
253140 us_tv_networks
38770
Average
251040
147531
0.109
0.265
0.250
0.374
0.125
0.062
0.078
1.342
0.484
0.062
0.315
HAC
0.218
0.265
0.203
0.171
0.234
0.140
0.125
0.390
0.328
0.203
0.228 a PC cluster supporting MapReduce [ 9 ] to perform these two tasks . The query relationships are computed from aggregated data of queries and clicked/skipped URLs of the year 2008 . The job runs on hundreds of CPUs ( we cannot disclose the exact number ) and takes 7 hours 10 minutes . The output contains about 5B pairs of queries that share clicked URLs . The job for the second step takes 2 hours 26 minutes , and outputs from 2K to 657K intent phrases for different classes of entities , with relationships between them . Table 10 shows the run time on each class of entities by each approach with optimal parameters . The top 400 intent phrases are used for each class . DMST and HAC are much more efficient than PAM because they use the pre computed relationships between intent phrases .
We also test the scalability of DMST and HAC wrt number of intent phrases , as shown in Figure 11 . Both approaches are super linear in time complexity : O(mn ) for DMST and O(n2 ) for HAC , for n intent phrases and m relationships between pairs of phrases . Their run time grows slower than the theoretical bound : 106 times for DMST and 46 times for HAC when number of phrases grows 16 times . Usually hundreds of intent phrases are sufficient for capturing all important search intents , and both DMST and HAC can generate the intent tree in no more than a few seconds .
10
1
0.1
) c e s ( e m i t n u R
0.01
DMST AHC
100
200
400
800
1600
#intent phrases
Figure 11 : Scalability of DMST and HAC
7 . Conclusions and Future Work
In this paper we study the problem of building taxonomies of search intents for entity queries . We use intent phrases to represent the important generic search intents , and propose an approach based on user logs for measuring the belonging relationships between intents of queries . We propose three approaches for building hierarchies of intent phrases based on such relationships . It is shown by experiments that these approaches can generate intent trees that capture the hierarchical structure of user intents .
An important direction of future work is to study how to rank the child nodes of each node in the intent tree . There have been many mature studies on ranking query suggestions , which can be adapted for ranking intent phrases .
8 . REFERENCES [ 1 ] Amazon Mechanical Turk . https://wwwmturkcom/mturk/ [ 2 ] R . Baeza Yates , A . Tiberi . Extracting semantic relations from query logs . KDD’07 .
[ 3 ] H . Cao , D . Jiang , J . Pei , Q . He , Z . Liao , E . Chen , and H . Li .
Context aware query suggestion by mining click through and session data . KDD’08 .
[ 4 ] S L Chuang , L F Chien . Automatic query taxonomy generation for information retrieval applications . Online Information Review 27(4 ) , 2003 .
[ 5 ] S . Chien and N . Immorlica . Semantic similarity between search engine queries using temporal correlation . WWW’05 .
[ 6 ] Y . J . Chu and T . H . Liu . On the shortest arborescence of a directed graph . Science Sinica , vol.14 , pp.1396 1400 , 1965 .
[ 7 ] H . Cui , J R Wen , J Y Nie , and W Y Ma . Probabilistic query expansion using query logs . WWW’02 .
[ 8 ] WHE Day and H . Edelsbrunner . Efficient algorihtms for agglomerative hierarchical clustering methods . Journal of Classification , 1(1):7 24 , 1984 .
[ 9 ] J . Dean and S . Chemawat . MapReduce : Simplified data processing on large clusters . OSDI’04 .
[ 10 ] J . Edmonds . Optimum branching . J . Research of the National
Bureau of Standards , 71(B ) , pp.233 240 , 1967 .
[ 11 ] A . Fuxman , P . Tsaparas , K . Achan , and R . Agrawal . Using the wisdom of the crowds for keyword generation . WWW’08 . [ 12 ] H . N . Gabow , Z . Galil , T . Spencer , and R . E . Tarjan . Efficient algorithms for finding minimum spanning trees in undirected and directed graphs , Combinatorica 6 , pp . 109 122 , 1986 .
[ 13 ] F . Guo , C . Liu , and Y M Wang . Efficient multiple click models in web search . WSDM’09 .
[ 14 ] J . Guo , G . Xu , X . Cheng , and H . Li . Named entity recogni tion in query . SIGIR’09 .
[ 15 ] T . Joachims , L . Granka , B . Pan , H . Hembrooke , F . Radlinski , and G . Gay . Evaluating the accuracy of implicit feedback from clicks and query reformulations in web search . In ACM Trans . Information Systems , 25(2 ) , 2007 .
[ 16 ] J . Hu , G . Wang , F . Lochovsky , J T Sun , and Z . Chen . Un derstanding User ’s Query Intent with Wikipedia . WWW’09 . [ 17 ] U . Lee , Z . Liu , and J . Cho . Automatic identification of user goals in web search . WWW’05 .
[ 18 ] W . Li and A . McCallum . Pachinko Allocation : DAG structured mixture models of topic correlations . ICML’06 . [ 19 ] M . Paşca . Organizing and searching the world wide web of facts step two : harnessing the wisdom of the crowds . WWW’07 .
[ 20 ] M . Paşca and E . Alfonseca . Web derived resources for Web information retrieval : From conceptual hierarchies to attribute hierarchies . SIGIR’09 .
[ 21 ] R . Sibson . SLINK : An optimally efficient algorithm for the single link cluster method . Computer Journal , 16:30 34 , 1973 .
[ 22 ] X . Wang , D . Chakrabarti , K . Punera . Mining broad latent query aspects from search sessions . KDD’09 .
[ 23 ] K . Wang , T . Walker , and Z . Zheng . PSkip : estimating relev ance ranking quality from web search clickthrough data . KDD’09 .
[ 24 ] J R Wen , J Y Nie , H J Zhang . Clustering user queries of a search engine . WWW’01 .
