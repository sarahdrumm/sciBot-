Actively Predicting Diverse Search Intent from User
Browsing Behaviors∗
Zhicong Cheng
School of Software and
Microelectronics Peking University
Beijing , 100871 , P . R . China czc0316@live.com
Bin Gao
Microsoft Research Asia 4F , Sigma Center , No . 49 ,
Zhichun Road
Beijing , 100190 , P . R . China bingao@microsoft.com
Microsoft Research Asia 4F , Sigma Center , No . 49 ,
Zhichun Road
Tie Yan Liu
Beijing , 100190 , P . R . China tyliu@microsoft.com
ABSTRACT This paper is concerned with actively predicting search intent from user browsing behavior data . In recent years , great attention has been paid to predicting user search intent . However , the prediction was mostly passive because it was performed only after users submitted their queries to search engines . It is not considered why users issued these queries , and what triggered their information needs . According to our study , many information needs of users were actually triggered by what they have browsed . That is , after reading a page , if a user found something interesting or unclear , he/she might have the intent to obtain further information and accordingly formulate a search query . Actively predicting such search intent can benefit both search engines and their users . In this paper , we propose a series of technologies to fulfill this task . First , we extract all the queries that users issued after reading a given page from user browsing behavior data . Second , we learn a model to effectively rank these queries according to their likelihoods of being triggered by the page . Third , since search intents can be quite diverse even if triggered by the same page , we propose an optimization algorithm to diversify the ranked list of queries obtained in the second step , and then suggest the list to users . We have tested our approach on large scale user browsing behavior data obtained from a commercial search engine . The experimental results have shown that our approach can predict meaningful queries for a given page , and the search performance for these queries can be significantly improved by using the triggering page as contextual information . Categories and Subject Descriptors H.3 [ Information Storage and Retrieval ] : Information Search and Retrieval
General Terms Algorithms , Experimentation
Keywords SearchTrigger , search intent , diversification , log mining , contextual information ∗ intern at Microsoft Research Asia .
This work was performed when the first author was an
Copyright is held by the International World Wide Web Conference Committee ( IW3C2 ) . Distribution of these papers is limited to classroom use , and personal use by others . WWW 2010 , April 26–30 , 2010 , Raleigh , North Carolina , USA . ACM 978 1 60558 799 8/10/04 .
1 .
INTRODUCTION
User intent understanding has become a hot topic in Web search and data mining , and commercial search engines have realized its importance for providing better search experiences . For example , Google , Yahoo! , Bing1 , and Ask all provide query suggestion [ 1 , 3 , 13 ] as a prediction of user search intent . While such a technique has achieved certain success , it also has obvious limitations , some of which are listed as below .
• The prediction was conducted in a passive manner , in the sense that it was performed only after users submitted their queries to search engines . Note that Web users spend most of their time not on search , but instead on browsing , authoring , and so on . Therefore in most cases , when users have latent search intents , search engines cannot make meaningful predictions . In this regard , the impact of passive intent prediction on Web users will not be sufficiently significant .
• The prediction was usually based on historical queries issued by the user , or similar queries issued by other users . Search engines are not aware how the information need of a user was originally generated and what motivated him/her to issue the query . As a result , the contextual information that search engines can obtain will be insufficient to produce high quality and personalized search results .
In order to overcome the aforementioned limitations , it is desired to predict users’ search intents in an active manner , based on their behaviors even beyond search ( eg , browsing , authoring , etc. ) , and leverage information related to these behaviors to improve search quality . This is exactly the motivation of our work .
For this purpose , first of all , we should understand the factors that may trigger users’ information needs . According to our study , in a significant proportion of cases , information needs are generated when users browse the Web . Our analysis ( see Section 2 ) on user browsing behavior data shows that about 19.3 % browsing sessions contain “ browse → search ” patterns ( ie , the user searched something right after he/she browsed a page ) . In 66 % of such sessions , some search queries were almost certainly triggered by the contents of the pages that users browsed before their search actions . That is , when browsing a page , the user might find something interesting or unclear in the page , or be reminded 1The new generation of Windows Live Search .
WWW 2010 • Full PaperApril 26 30 • Raleigh • NC • USA221 of something related to the page . Then he/she might want to conduct a search immediately after browsing the page , to learn more about all of this . For example , Linda is browsing the news of Michael Jackson ’s death and she wants to listen to some songs of Jackson to honor the memory of him . So she comes up with the search query Music of Michael Jackson . For ease of reference , we call this scenario SearchTrigger . More accurately , SearchTrigger refers to a “ browse → search ” pattern , in which the search query is triggered by the content of the browsed page . We call the query a SearchTrigger query , and the pattern a SearchTrigger pattern . Our further analysis shows that the SearchTrigger queries are very diverse . For 92 % pages , the corresponding queries fall into at least two dissimilar topical clusters .
A real example of SearchTrigger that we found during the study is given as follows .
Example 1 . ( SearchTrigger . ) A user is a fan of Shakespeare . He/she opened a webpage about Shakespeare ’s FAQ ( http://absoluteshakespearecom/trivia/faq/faqhtm ) There are tens of FAQs in the page about Shakespeare . For instance , there is a question “ Is it true nobody knows Shakespeare ’s birthday ? ” at the top of the page . The answer is : “ It is true we don’t know Shakespeare ’s date of birth . We know it was in 1564 but our only record at this time was of his baptism at the Holy Trinity Church on April the 26th . By convention and some guesswork , Shakespeare ’s birthday is by tradition celebrated three days earlier on April the 23rd . ” After reading the notes , the user felt interested in the birthday of Shakespeare . To satisfy his/her curiosity , the user raised a query “ when was Shakespeare born ” to a search engine for more answers . Note that the SearchTrigger queries are those queries triggered by the browsed page , but usually NOT the key phrases of the page or the queries that have the page as its top search result . For instance , in the above example , “ when was Shakespeare born ” does not appear in the page of Shakespeare ’s FAQ , and therefore is not one of the key phrases . Also if we search “ when was Shakespeare born ” using major search engines , the page of Shakespeare ’s FAQ even does not appear in the top 100 results . This is actually reasonable . If the query is a key phrase of the page or the page is in the top search results for the query , it is very likely that the page has already contained the answer to the query and it is unnecessary for the user to issue the query to search engines . The query is submitted usually because the user wants to obtain some novel information that is not covered by the triggering page .
If we can predict the latent search intent in the SearchTrigger scenario , and suggest meaningful queries to users when they are browsing , we will be able to help both search engines and their users . On one hand , by providing search shortcuts corresponding to these suggested queries , we actually extend the search function to outside the search box , and provide more opportunities for users to use search engines . On the other hand , by using the historical browsing behaviors ( including the content of the pages browsed by the user ) as contextual information , we can improve search accuracy for these suggested queries and provide much better user experiences . This task is , however , non trivial due to the following reasons : ( i ) not all “ browse → search ” patterns correspond to real SearchTrigger ; ( ii ) the search intents of users can be very diverse even if they read the same page , according to the statistics given in Section 2 .
To tackle the challenging task , in this work , we propose a series of technologies .
1 . Given a page , we extract all the queries that users searched right after reading the page from user browsing behavior data .
2 . We learn a ranker to effectively sort these queries ac cording to their likelihoods of being SearchTrigger queries .
3 . We propose an optimization framework to diversify the ranked list of queries obtained in the second step , and present the diversified ranked list of queries to users .
We have tested our proposed approach on large scale user browsing behavior data , and develop a contextual retrieval algorithm to leverage the page that triggers a query to improve search accuracy for the query . The experimental results have shown that the approach can improve user experience and enhance search accuracy .
To sum up , the contributions of this work are as below . • We have proposed the concept of active prediction of users’ search intents , which extends the functionality of search engines beyond their original boundaries .
• By mining user browsing behavior data , we have discovered a special pattern called SearchTrigger , in which the search intent is triggered by the page that a user visits right before his/her search action .
• We have found that SearchTrigger queries for the same page are usually very diverse . Accordingly , we have proposed a method to suggest a diversified query list for a given page and demonstrated its effectiveness through a contextual retrieval algorithm .
The rest of the paper is organized as follows . In Section 2 , we present the study on user browsing behavior data , and show that SearchTrigger is a popular pattern in such data . The algorithms to effectively predict SearchTrigger queries for a given page are introduced in Section 3 . Experimental results are discussed in Section 4 . Conclusions and future work are presented in the last section .
2 . ANALYSIS ON USER BEHAVIOR DATA In order to better understand how what users browsed triggered their search intents , we have conducted an extensive study on user browsing behavior data , as reported in this section .
The primary source of data for this study was the anonymous logs of URLs visited by users who opted in to provide data through a widely distributed toolbar on Internet browsers . Each log entry is a tuple of {user ID , timestamp , URL} , meaning a user visited a URL at some time . User ID was encrypted by an irreversible hash function . Intranet and secure ( eg , https ) URL visits were all removed from the source . To minimize the influence caused by linguistic and regional variations , we only kept the records generated in the United States . As a result , the data consist of about 3 billion anonymous page views in 32 successive days during May and June in 2007 .
WWW 2010 • Full PaperApril 26 30 • Raleigh • NC • USA222 2.1 “ Browse → Search ” Pattern Extraction We extracted “ browse → search ” patterns from all sessions in the user browsing behavior data . Here we define a session as a logical unit of time ordered user browsing activities . For each user ’s data , we start a new session if there is more than 30 minutes of inactivity between the current page view event and its immediate preceding event [ 17 ] . For the page view events and sessions , we have the following definitions .
Definition 1 . Search Portal Event / Search Event / Browse Event . If a page view event contains the URL of a ( general or vertical ) search engine portal2 but does not contain any search query , we call the event a search portal event ; if the page view event contains the URL of a search engine and a query as its argument , we call the event a search event ; otherwise , we call the event a browse event .
For example , in the following session , the first URL corresponds to a browse event , the second a search portal event , and the third a search event .
Example 2 . ( Three successive URLs in a session . ) http://wwwapplecom/iphone/ http://wwwgooglecom/ http://wwwgooglecom/search?q=iphone+release+date
Definition 2 . Search Session / Non search Session . If a session contains at least one search event , it is called a search session . Otherwise , it is a non search session .
Definition 3 . “ Browse → Search ” Pattern . After excluding all search portal events from a search session , if there is a search event immediately after a browse event , we call the tuple {URL , query} a “ browse → search ” pattern where URL is the page visited in the browse event and query is extracted from the search event .
For instance , from Example 2 , one can extract the following “ browse → search ” pattern , {http://wwwapplecom/ iphone/ , iphone release date} . Note that there is only one case that a search session does not contain any “ browse → search ” pattern : all search events happened prior to the browse events in the session . In this case , this is no evidence that the search events are triggered by the content of previously visited pages . Therefore , they are not in the interested scope of our study .
Definition 4 . “ Browse → Search ” Session . If a search session contains at least one “ browse → search ” pattern , it is called a “ browse → search ” session .
We processed the entire user browsing behavior data in our study , and obtained the statistics as shown in Table 1 . From the table , we can see that about 19.3 % sessions are “ browse → search ” sessions and the average number of “ browse → search ” patterns per such session is 56 As will be seen in the next subsection , these “ browse → search ” patterns potentially correspond to SearchTrigger , but not necessarily all qualified SearchTrigger . 2Note that in our study , we only included the search events from Google , Yahoo! , Windows Live Search , AOL , and Ask , because the majority of the search market share in the United States ( around 90 % according to [ 24 , 25 , 26 ] ) comes from these five search engines .
Table 1 : Statistics on sessions and “ browse → search ” patterns .
Entity
Log entries
Unique URLs
Sessions
Non search sessions
Search sessions
“ Browse → search ” sessions “ Browse → search ” patterns Average entries per session
Percentage of non search sessions
Percentage of search sessions
Percentage of “ browse → search ” sessions
Average “ browse → search ” patterns per “ browse → search ” session
Quantity 2,998,754,253 940,555,664 153,663,449 116,399,857 37,263,592 29,695,191 167,570,019 19.5 75.7 % 24.3 % 19.3 %
5.6
2.2 Identification of SearchTrigger Queries
We randomly sampled 200 “ browse → search ” sessions and asked experienced human analysts to perform further analysis on the data . As a result , we found 849 “ browse → search ” patterns , the queries in which can be classified into seven categories .
1 . Key phrase of the page The query is a key phrase of the browsed page . For example , after visiting a page about Shakespeare ’s FAQ ( http : //absoluteshakespearecom/trivia/faq/faqhtm ) , a user issued the query Shakespeare ’s play , which is a key phrase in the page due to its high relevance with the main topic of the page and its high frequency .
2 . Information in the page but not key phrases The query describes some interesting part of the browsed page , but it is not a key phrase . The query given in Example 1 belongs to this category . Furthermore , information in the page such as images and flashes may also trigger queries belonging to this category . We sent the queries in this category to major search engines , and found that their corresponding top 10 search results do not contain the browsed pages .
3 . Famous site Many users have the experiences that they wanted to visit a famous website such as facebook , youtube , myspaces , but they forgot its exact URL . They then issued a query like facebook to a search engine to get a shortcut to the portal of the website . Such queries are usually not triggered by the page that a user previously browsed .
4 . Unrelated topic Sometimes , a user searched a query absolutely unrelated to the content of the previous page . There might be various reasons for this situation . For example , the user changed his/her interest to another totally different topic , or it is some information need from his/her daily life that triggered the query ( eg , the user felt hungry and searched for the phone number of a restaurant ) .
5 . Repeated search Sometimes a user first submitted a query to a search engine , and browsed a page in the search results . How
WWW 2010 • Full PaperApril 26 30 • Raleigh • NC • USA223 Table 2 : Statistics of “ browse → search ” patterns . Percentage
Number of Queries
Category
Average
Pattern Frequency
SearchTrigger
Non SearchTrigger
Cannot judge
Key phrase of the page Information in the page but not key phrases
Famous site
Unrelated topic Repeated search Query refinement
Cannot judge
202
604
23
179
15 121 350 118
23.8 %
71.1 %
2.7 %
21.1 %
1.8 % 14.2 % 41.2 % 13.9 %
43
5.1 %
26.8
2.5
92.9 1.6 52.0 1.8 1.0 ever , he/she was not satisfied with the page . Then he/she clicked the back button in the browser to try another page in the search results , or changed to another search engine and searched the query again . As a result , we can extract a sequence of events like query → URL1 → query → URL2 . Here the second query is not triggered by the content of URL1 since it is simply a repeated search .
6 . Query refinement Sometimes a user first submitted a query to a search engine , and browsed a page in the search results . However , he/she was not satisfied with the page and he/she refined the query and resubmitted it to the search engine . As a result , we can obverse the following sequence of events , query 1 → URL1 → query 2 → URL2 . In this case , query 2 is not triggered by the content of URL1 either .
7 . Cannot judge It was difficult for the analysts to categorize all the “ browse → search ” patterns accurately . In some cases , the analysts could not make sure which category a pattern should belong to . In some other cases , the page in a pattern has been expired or needs login ( eg , forums ) to view its content . We regard all these cases as cannot judge .
The statistics of the above categories are summarized in Table 2 . In addition to the number of queries and percentage in the 849 patterns , we also count the frequency of a “ browse → search ” pattern in the entire user browsing behavior data ( see the last column in Table 2 ) , which can reflect whether a particular pattern is popular or not .
According to the definition of SearchTrigger given in the introduction , the human analysts thought that both the first and the second categories in the study correspond to SearchTrigger , categories 3 to 6 are not SearchTrigger , and category 7 is not judgeable . From this result , we can come to the following conclusions .
• About a quarter ( 23.8 % ) of “ browse → search ” patterns contain SearchTrigger queries . Further analysis shows that 132 of the sampled 200 sessions ( 66 % ) contain at least one SearchTrigger query . All these numbers show that SearchTrigger is a frequent pattern of user behaviors and it is worthy of further investigation .
• Among the SearchTrigger queries , the proportion of queries belonging to category 2 is significantly larger than that belonging to category 1 . This coincides with our discussions in the introduction : most SearchTrigger queries ( in our study , 88.6 % ) are not key phrases of the browsed page .
Table 3 : Statistics on diverse SearchTrigger queries .
Number of Search Intents Number of Pages
≤ 1 2 ∼ 5 6 ∼ 9 ≥ 10
8 12 68 12
• According to the average pattern frequency in each category , we can see that not all high frequency patterns correspond to SearchTrigger ( eg , famous site and repeated search also have very high frequencies ) .
2.3 Diversity in SearchTrigger Queries
We randomly sampled 100 pages from the user browsing behavior data . Then we collected the queries in all the “ browse → search ” patterns containing the page . We asked human analysts to judge whether these queries are SearchTrigger queries . For the queries that are judged as SearchTrigger queries , human analysts further grouped them into several clusters according to their corresponding search intents . For example , after reading a page about rabbits ( http://exoticpetsaboutcom/od/rabbits/Rabbitshtm ) , users issued 11 different queries , 7 of which were identified as SearchTrigger queries . These queries were organized into three groups , ie , {rabbits , pet rabbit , wild rabbits} , {rabbits pictures , pet pictures} , and {rabbit care guide , rabbits breeds} , indicating three different search intents . The statistics of this study are summarized in Table 3 . From the table we can see that about 92 % pages triggered at least two different search intents , showing that SearchTrigger queries are often diverse . Our explanation to this observation is that a page may contain several different ( but correlated ) topics and each of them can motivate users to search something .
3 . PREDICTING DIVERSE SEARCH INTENT
The statistics in Section 2 show that SearchTrigger is a frequent pattern of user behaviors . Then if search engines can accurately predict users’ intents ahead of time and suggest SearchTrigger queries while users are browsing a page , they can provide timely search function when users need it , and thus greatly improve user experiences . This task is , however , non trivial , as discussed in the introduction . To tackle the challenges , we propose a set of techniques . First , we extract the “ browse → search ” patterns from user browsing behavior data , and build a candidate query set for each page . Then , given a page and its candidate queries , we extract various features and learn a ranking model to sort these queries according to their likelihoods of being SearchTrigger
WWW 2010 • Full PaperApril 26 30 • Raleigh • NC • USA224 queries . After that , we adopt an optimization algorithm to diverse the ranked list of the SearchTrigger queries . This diversified query list will be presented to users as suggestions .
3.1 Problem Definition
Given a page , the task is to predict a ranked list of SearchTrig ger queries that a random user may want to issue after reading the page , based on historical user browsing behavior data . To this end , one can segment user browsing behavior data into sessions , and extract all “ browse → search ” patterns . For each page p , a candidate query set can be generated by aggregating all its co occurrence queries in the patterns . Suppose the candidate query set is Sp = {q m } . Then a straightforward solution to the task is to count the frequency of each query in Sp , and suggest the most frequentlyasked queries to users . However , this naive method would not work well , because 71.1 % candidate queries are nonSearchTrigger , and many of them are of high frequencies , according to the analysis in Section 2 .
2 ,··· , q
( p ) 1 , q
( p )
( p )
To solve the aforementioned problem , we propose extracting multiple features for each candidate query and adopting machine learning technologies to rank these candidate queries according to their likelihoods of being SearchTrigger queries based on the features .
3.2 Query Features
On one hand , the task of query ranking can be regarded as a dual problem of document ranking in search . Therefore , it is straightforward to also extract query document matching features for the task , which are widely used in the literature of document ranking . For example , we extract the following features to describe the matching between a query and its preceding page : term frequency ( TF)[2 ] , inverse document frequency ( IDF)[2 ] , TF *IDF[2 ] , LMIR with ABS smoothing ( LMIR.ABS)[20 ] , LMIR with DIR smoothing ( LMIR.DIR)[20 ] , and LMIR with JM smoothing ( LMIR . JM)[20 ] . If we consider that each page contains three parts , ie , url , title , and body , we will have 18 query document matching features in total . In addition , we also extract the length of query , unique word count of following features : query , and maximum word length of query . In many cases , we had better not suggest long queries ( or query words ) to users , because most of them are rarely asked by real users . These features can help avoid such cases in the suggested queries .
On the other hand , however , there is also difference between the task of query ranking and that of document ranking . In the former case , each candidate query is represented by features while in the latter case each candidate document is represented by a set of features . This difference actually poses a challenge to us : queries are usually much shorter than documents , which makes the above content matching features not informative enough to describe queries . For example , only from the query word , it is difficult to judge whether a query reflects the interesting part of a page that can attract users’ attention . Such information , which is important for identifying SearchTrigger queries , need to be extracted from other information sources , eg , the bipartite graph as described below . In our work , we extract all “ browse → search ” patterns from user browsing behavior data , and build a page query bipartite graph . In this bipartite graph , a page node is cre
Figure 1 : An example of page query bipartite graph . ated for each unique page , while a query node is created for each unique query in the patterns . An edge eij is generated between page pi and query qj if they co occur in a “ browse → search ” pattern . The weight wij of edge eij is the frequency of such patterns . An example page query bipartite graph is shown in Figure 1 . From the bipartite graph , we extract the following features , in hope to describe users’ interests :
• Query Visibility . We call the number of edges linking to a query query visibility . If a query has large query visibility , it means that users would ask the query after visiting many different pages .
• Query Popularity . We call the sum of weights of all the edges linking to a query query popularity . If a query has large query popularity , its total number of occurrences in the extracted patterns is large .
• Pattern Frequency . We call the weight of the edge between a query and the given page pattern frequency . This feature reflects whether the same query is issued by many different people after reading the page .
3.3 Learning to Rank Candidate Queries
( p )
Previous work [ 4 , 9 , 12 ] has shown the advantage of using a learning to rank approach over using heuristic rules , especially when there are multiple evidences of ranking to be considered . Given the query features as described in Section 3.2 , we also adopt a learning to rank technique to rank the candidate queries . Given page p and its candidate query set Sp = {q m } , where m is the number of queries . Let X ⊂ Rd be the q feature space of queries , where d is the number of features . ( p ) Then x i with respect to p , i = 1 , 2,·· · , m . Suppose Y = {l1 , l2,··· , lK} is the set of labels representing the likelihood that a query is a SearchTrigger query for the document . Assume that there is a total order between the labels , ie , l1 > l2 > ··· > lK . In our study , K is set to 3 , and l1 , l2 , and l3 represent the labels of SearchTrigger , cannot judge , and non SearchTrigger respectively . i ∈ X denotes the feature vector of q
( p ) 1 , q
( p )
( p )
2 ,··· ,
In the training process , there is a set of n pages , their candidate queries , and the corresponding labels ( given by human annotators ) , in which zp = {(x m )} . Here ( p ) ( p ) 1 , y 2 , y i ∈ Y is its label . i ∈ X is the feature vector of q x If we have y ie , Z = {z1 , z2,··· , zp,· ·· , zn} , ( p ) 1 ) , ( x
2 ),··· , ( x and y ( p ) , then we can say q i
( p ) i > y
( p ) m , y
( p ) i
( p ) j
( p )
( p )
( p )
( p )
( p ) higher than q j
, denoted as the partial order q should be ranked ( p ) j i ) q
( p )
.
WWW 2010 • Full PaperApril 26 30 • Raleigh • NC • USA225 ( p )
( p )
( p ) i
) > f ( x i ) q j ⇔ f ( x
Suppose F is the set of ranking functions , then each instance of it f ∈ F can rank the pair q ( p ) j ) . The training process aims to find the optimal f that can fit as many pairs of partial orders in the training set as possible . Any pairwise3 learning to rank algorithms , such as Ranking SVM[12 ] , RankBoost[9 ] , and RankNet[4 ] , can be adopted to learn the ranking function f , in the above setting . For example , when using Ranking SVM , we assume f to be a linear combination of features f ( x ) = ωT x ( where ω is the parameter vector representing the weights of the features ) , and use the following optimization problem to learn the parameter ω , ffωff2
1 2 min ω,ξ
+ C
.
.
( p ) ij
ξ p x
( p ) i
( p ) j
.x ij ,∀x
( p )
T
( p )
( p )
( x i − x j ) > 1 − ξ ij ≥ 0 . ( 1 ) st ω Here ff · ff is the L2 norm , ξ is a set of slack variables , and C is a trade off coefficient . If the solution to ( 1 ) is ¯ω , then the ranking function can be written as i ) x
( p ) j
, ξ
( p )
( p ) f ( x ) = ¯ω
T x .
( 2 )
This ranking function will be used to rank the candidate queries for a new page in the user browsing behavior data . Then the top ranked queries can be presented to users as SearchTrigger suggestions . 3.4 Diversification of SearchTrigger Queries As shown in Section 2.3 , SearchTrigger queries can be very diverse even if they are triggered by the same page . In order to minimize the dissatisfaction of a random user after seeing the suggested SearchTrigger queries , one needs to diversify the queries before presenting them to users4 . p ⊆ Sp ) , fi which contains SearchTrigger queries that are diverse in their topics . We formulate this task as a set selection problem inspired by [ 10].5 In particular , we define an objective g(· ) , as a function of ranking model f ( · ) ( eg , learned in the previous subsection ) and a query dissimilarity measure δ(· , · ) . The goal is to select a set of queries , S p ⊆ Sp , such fi that the objective function g(· ) can be maximized , ie ,
To this end , our task is to select a subset S fi p ( S p , f ( · ) , δ(·,·) ) . fi
( 3 ) fi where m fi S p = arg = |S max p⊆Sp,|S . S . p|=m . g(S fi p . p| is the size of S fi .
A simple yet reasonable objective function is given as fol lows ( λ > 0 is a trade off coefficient ) ,
. p , f ( · ) , δ(· , · ) ) = fi g(S
δ(i , j ) + λ f ( i ) .
( 4 )
( p ) q i
( p ) j ∈S . p
,q
( p ) i ∈S . p q
It is clear that the maximization of this objective function will guarantee that the queries selected will have a large ranking score ( since the sum of the ranking scores has been maximized ) , and each two queries will be different ( since the average pairwise dissimilarity has been maximized ) . 3Besides pairwise learning to rank algorithms , one can also choose pointwise [ 8 ] and listwise [ 5 ] learning to rank algorithms . 4This can be regarded as a dual problem of search result diversification . 5Note that one can use other diversification formulations like those discussed in [ 7 , 15 ] .
To solve the above optimization problem , one needs to address two technical challenges . First , since queries are usually very short , it is non trivial to define an effective query dissimilarity measure . Second , the problem is a typical NPhard problem and thus the efficient optimization of it is nontrivial . We will present our solutions to these two challenges in the following subsections .
341 Query Dissimilarity Measure
To compute effective query dissimilarity measure , we propose using the page query bipartite graph built in Section 3.2 , since it contains rich information of query relationships . However , this graph is not fully reliable since many edges in the bipartite graph do not correspond to SearchTrigger patterns . This might not be a big issue for ranking model learning since it is a supervised process and we can leverage other features to avoid the negative influence of this graph . However , it may become a problem when we use the graph for query diversification , since this is an unsupervised optimization process . If the graph is unreliable , the optimization results will accordingly become unreliable .
To tackle the problem , we clean the graph before using it to compute query dissimilarity . For each page in the graph , we extract features for its co occurrence queries and compute the ranking scores of these queries using the model learned in Section 33 After that , we normalize the scores to interval [ 0 , 1 ] and use the normalized scores to re weight the corresponding edges in the bipartite graph . That is , for edge eij with original weight wij , if the normalized ranking score of query qj with regards to page pi is vij , we will change the edge weight of eij to wij vij . In this way , the bipartite graph is cleaned because the weights of the SearchTrigger patterns are enlarged and those non SearchTrigger patterns are reduced .
We then calculate query dissimilarity by Jensen Shannon divergence ( JSD ) [ 23 ] on the cleaned graph.6 The basic idea is that if two queries share many pages with high weights , they should be similar to each other ; otherwise dissimilar . According to the bipartite graph , we can represent each ( p ) as a vector βi . Each dimension of the vector query q i βij = wijvij , where j is the index of a page in the graph . ( p ) ( p ) Given two queries q k , their dissimilarity in terms i of JSD is calculated as below , and q
δ(i , k ) = ( D(βiffα ) + D(βkffα))/2 ,
( 5 ) where D(·ff· ) is the Kullback Leibler divergence [ 22 ] and α = ( βi + βk)/2 .
342 Efficient Optimization fi
( p )
ηi =
Let η be an indicator vector defined as below , ( i = 1 , 2,··· , m ) . . Then the objective function g(· ) can be written as , p , f ( · ) , δ(·,· ) ) = fi i ∈ S /∈ S .
1 , q ( p ) 0 , q i
δ(i , j)ηiηj + λ fi p fi p g(S
( 6 ) f ( i)ηi .
( p ) q i
( p ) j ∈Sp
,q
( p ) i ∈Sp q
6Note that one can use other forms of query dissimilarity , such as those based on cosine similarity and Pearson correlation [ 21 ] . Here the use of JSD is just an example .
( 7 )
WWW 2010 • Full PaperApril 26 30 • Raleigh • NC • USA226 Suppose Δ = {δ(i , j)} represents the query dissimilarity matrix , then we can get the following equivalent form of the original optimization problem , max
1 2
T
η
T
Δη + λf η ; η = {ηi} , ηi = 0 , 1 ; ; f = {fi} .
η = m e = ( 1 , 1,·· · , 1 )
T fi st e
( 8 )
The above optimization problem is a typical 0 1 integer programming problem , which is NP hard . We propose relaxing the values of η to be continuous ( ie , ηi ∈ [ 0 , 1] ) , and converting ( 8 ) to the following quadratic optimization problem . Note that the same trick has been widely used in semi supervised learning and spectral clustering [ 6][16 ] . min
T
η
1 2
Lη − λf η ; η = {ηi} , 0 . ηi . 1 ; fi
T st e
η = m e = ( 1 , 1,··· , 1 ) − 1
− 1
T
; f = {fi} .
( 9 )
Here L = I−D
2 ΔD
2 , in which I is the identity matrix and D is a diagonal matrix with its diagonal elements equal to the sum of all the elements in the corresponding rows of Δ . Suppose the solution to the above optimization problem is η . Then we can select the queries corresponding to the largest m to form the suggested query set . Actually , the above optimization problem has the following properties . elements in η
∗
∗ fi
• It is not difficult to verify that L is a positive semidefinite matrix and thus ( 9 ) is a convex optimization problem . As a result , η is the global optimal solution to ( 9 ) . In contrast , in some previous work like [ 10 ] , a greedy method was used to solve similar set selection problem , which is not guaranteed to result in an optimal solution .
∗
• The problem can be solved in a time complexity of O(m3 ) . For each page , the number of candidate queries is usually not very large ( eg , less than 100 ) . Therefore , the computational complexity turns out to be affordable .
Note that when users go to a search engine with the suggested SearchTrigger queries , the page that they previously browsed can serve as an informative context for the search engine . There is a rich literature of contextual information retrieval , which basically leverages various contextual information to improve search quality [ 18 ] . Many ideas in the previous work can be used directly or indirectly . In Section 4.3 , we tested a simple contextual retrieval algorithm and the experimental results clearly demonstrated the benefit of using the aforementioned contextual information to answer SearchTrigger queries .
4 . EXPERIMENTAL EVALUATION
In this section , we presented our experimental study on the proposed approach . 4.1 Datasets
We used the user browsing behavior data as mentioned in Section 2 for our experiments . After partitioning the data to
700000
600000
500000
400000
300000
200000
100000 i s n o s s e s f o r e b m u N
0
0
20
40
60
80
100
Number of pages in a session
Figure 2 : The distribution of page numbers in the sessions . i s n o s s e s f o r e b m u N
9000000
8000000
7000000
6000000
5000000
4000000
3000000
2000000
1000000
0
0
5
10
15
20
25
30
35
40
Number of "browse−search" patterns in a session
Figure 3 : The distribution of the numbers of “ browse → search ” patterns in the sessions . sessions , we extracted 167,570,019 “ browse → search ” patterns from them . The distribution of page numbers and the numbers of “ browse → search ” patterns in the sessions are shown in Figure 2 and Figure 3 . We filtered out the patterns whose queries contain non alphanumeric terms like Chinese or Arabic words . Then we removed those pages whose number of occurrences in the data is less than 5 , for most of them correspond to rare patterns . After the cleaning , we obtained 56,929,950 patterns left and built a page query bipartite graph from these patterns . The graph contains 3,529,910 unique pages and 25,677,960 unique queries . The degree distribution of queries in the graph is shown in Figure 4 . This bipartite graph was used to extract query features and compute query dissimilarities .
4.2 Performance of SearchTrigger Query Sug gestion
We compared our proposed approach with some baseline methods , and investigated the benefit of diversifying suggested queries . All the methods under comparison are listed as below .
Key Phrase Extraction ( KPE ) . KPE is a technique to extract important keywords or phrases from a given text document [ 11 , 14 , 19 ] . We use the method described in [ 19 ] to
WWW 2010 • Full PaperApril 26 30 • Raleigh • NC • USA227 108
107
106
105
104
103
102
101 s e d o n y r e u q f o r e b m u N
100
100
101
102
103
104
105
106
Degree of query node
Figure 4 : The degree distribution of the queries in the page query bipartite graph . extract key phrases in a page and regard them as triggered queries . Pattern Frequency ( PF ) . After extracting all “ browse → search ” patterns from user browsing behavior data , a straightforward solution is to count the frequencies of all queries that co occur with a page , and suggest the most frequently asked queries to users .
Query Ranking based on Query Features ( QRQF ) . This method uses the features introduced in Section 3.2 , employs Ranking SVM to combine them for query ranking . No diversification is introduced , and the ranking result given by Ranking SVM is directly suggested to users . The trade off coefficient C is empirically set to 5 .
SearchTrigger Queries Diversification ( SQD ) . Based on the ranking result given by QRQF , the diversification method described in Section 3.4 is used to obtain a refined query set . The trade off coefficients C and λ are empirically set to 5 and 2 . This algorithm is exactly our proposed approach . As mentioned in Section 2 , we have two labeled datasets . The first dataset contains the labels of 849 “ browse → search ” patterns , and the second one contains the labels of all the queries with regards to 100 pages . We used the first dataset to train the models of QRQF and SQD , and then used the second one to test the performance of all the algorithms . Table 4 shows some examples of the suggested query sets produced by different algorithms . Pages No . 1 , 2 , and 3 correspond to http://moviesaboutcom/od/currentfilms/ , http://nationalprioritiesorg/indexphp?option=com_ wrapper&Itemid=182 , and http://pdsjplnasagov/planets/ welcome.htm , respectively . Due to space restrictions , for each page , only the top 5 ranked queries are shown . From the examples , we have the following observations :
1 . The key phrases extracted from page content are very different from the queries issued by users . Some key phrases like planetary exploration can be well understood by viewing the page content and thus users may not want to learn more about them ; some key phrases like Hollywood are well known words and users seldom issue them as search queries ; some extracted key phrases like links within this site seem to be neither a good summary of the page nor a possible triggered query .
5 @ n o s c e r P i i
1
0.8
0.6
0.4
0.2
0
( 1 ) Average precisions @5
KPE
PF
QRQF SQD
5 @ s t n e t n i h c r a e s f o r e b m u N
( 2 ) Average search intent hitting numbers @5 4
3
2
1
0
KPE
PF
QRQF SQD
Figure 5 : Average precisions and average search intent hitting numbers for different algorithms .
2 . PF sometimes produces unexpected queries . For example , rick maze was suggested for page No2 This query is the name of an editor in the magazine of Army Times . We searched within the whole site of page No.2 and did not find any convincing evidence to support that this query was triggered by the page . By further investigation on the browsing behavior data , we found the query corresponds to typical repeated search in several sessions .
3 . The queries suggested by QRQF look more reasonable than those suggested by KPE and PF . However , many of the queries suggested by QRQF are very similar to each other , eg , recent movies , recent movie releases , and new movies releases . By using SQD , we obtained even better results which are both reasonable and diverse . For example , The results produced by SQD for the planet page No.3 is a good example to demonstrate this .
To make a statistical comparison among these algorithms , for each page , we computed the precision [ 2 ] of the query set produced by each algorithm according to the ground truth set labeled by human annotators ; we also counted the number of search intents that the query set hit . After that , we computed the average precision and average hitting number for each algorithm . The results are shown in Figure 5 , where the two measures are computed with respect to the suggested sets of the top 5 queries . We can see that QRQF and SQD correspond to the largest average precisions and average search intent hitting numbers , which are significantly better than the other two algorithms . Compared with QRQF , SQD performed significantly better in hitting more search intents , with only a small loss of precision . Therefore , we say that SQD is able to satisfy more users’ information needs .
To better understand the benefit of diversifying the sugInstead gested query set , let us have a look at Figure 6 . of presenting the average results as in Figure 5 , in Figure 6 we plot the distribution of pages with regards to different precisions and hitting numbers . In particular , each page pi is represented by a two dimensional vector ( φi , ϕi ) , where φi is the precision and ϕi is the hitting number of the top 5 queries produced by an algorithm . In each sub figure , X axis corresponds to φi , Y axis corresponds to ϕi , and Z axis corresponds to the frequency of ( φi , ϕi ) ( denoted as ρ(φi,ϕi) ) . The curved surface is fitted upon the tuples ( φi , ϕi , ρ(φi,ϕi) ) . From the sub figures , we can clearly see the advantage of SQD over QRQF : its peak lies in the area with larger hitting number than that of QRQF , while their precisions are similar to each other .
WWW 2010 • Full PaperApril 26 30 • Raleigh • NC • USA228 Table 4 : Examples of suggested query sets by different algorithms ( top 5 results ) . No .
2 NPP Cost of War Obama Afghanistan Billion iraq war rick maze positives of iraq war iraq war cost war in iraq deaths iraq war iraq war cost information on the war in iraq cost of iraq war war in iraq cost cost of iraq war chart iraq war coalition of the willing members war in iraq deaths current iraq war debt
3 planetary planetary exploration planetary exploration program Welcome to the Planets links within this site planets pictures of the planets pictures of space shuttle photos of planets saturn photos planets 9 planets photos of planets planets solar system pictures of the planets planets nasa kids pictures saturn photos pictures of space shuttle ufos
KPE
PF
QRQF
SQD
1 Movies Movie News Hollywood Movies Hollywood New on Video recent movies new movies releases new movies new movies released sports movies recent movies movies recent movie releases dancing movies new movies releases new movies releases recent movies dancing movies sports movies movies of 2006
( 1 ) QRQF s e g a p l e p m a s f o r e b m u N
40
30
20
10
0 5
4
3
Number of SearchTrigger queries
2
1
1
5
4
3
2
Number of search intents
( 2 ) SQD s e g a p l e p m a s f o r e b m u N
40
30
20
10
0 5
4
3
Number of SearchTrigger queries
2
1
1
5
4
3
2
Number of search intents
Figure 6 : The performance comparison of QRQF and SQD .
4.3 Contextual Re ranking
To verify whether it is beneficial to use the triggering pages as contextual information for search , we tested a simple contextual retrieval algorithm in this subsection .
The basic idea is to extract some additional features to represent the content similarity between the triggering page and the documents to rank , and use these features to rerank the original search results . Specifically , suppose a user selects a suggested SearchTrigger query q for page p , then both q and p will be sent to the search engine.7 Given query q , the search engine can retrieve top k relevant documents , ie , D = {t1 , t2,··· , tk} , using its default retrieval function . After that , the content similarities between page p and these documents are calculated . According to the similarities , the similar documents to p will be promoted in the search result , as compared to those equally relevant but dissimilar documents . This heuristic is designed by considering that query q is triggered by page p and thus the user might be willing to see documents sharing similar content or topic with the triggering page p . The re ranking algorithm is described in Table 5 , where sim(· ) is the cosine similarity function and γ is a parameter to set the weight of contextual information ( in our experiments , we empirically set γ = 04 )
To test the above algorithm , we designed the following experiment . We used the model learned by SQD to test unlabeled pages in the cleaned bipartite graph . If a “ browse → search ” pattern is predicted as SearchTrigger and the user did click a URL in the search result given by a search engine SE for the query ( which can be observed in user browsing behavior data ) , we will regard it as a “ browse → search → click ” pattern . We sampled 500 such patterns from the “ browse → search ” sessions . For each of these patterns , we submitted that query to search engine SE and got the top 50 pages in its search result . We crawled the content 7Actually only the URL of the page needs to be sent to the search engine . With the URL , one can quickly retrieve the content of the page from the index of the search engine .
WWW 2010 • Full PaperApril 26 30 • Raleigh • NC • USA229 Table 5 : Contextual re ranking for search .
Input : triggering page p , SearchTrigger query q , and weighting parameter γ ; Output : re ranked documents {t 1 . Retrieve the documents D = {t1 , t2 , ··· , tk} of q
2,· ·· , t fi k} ; fi fi 1 , t using search engine ’s default retrieval function . The documents are sorted in the descending order of their relevance scores {r1 , r2,··· , rk} ;
2 . For each document ti ∈ D do si = sim(p , ti ) ; fi s i = ri + γsi ;
3 . Sort documents in D to {t fi 1 , t
2,··· , t fi k} in the fi descending order of s fi i ; of these pages and used the algorithm in Table 5 to rerank these pages . We regard the clicked page in the session as ground truth . If the reranking algorithm really boosted this clicked page , we say there is gain for this pattern . Among the 500 patterns , our experimental results show that there are 337 patterns ( or 67.4 % ) with gains , 22 patterns ( or 4.4 % ) without any position change , and 141 patterns ( or 28.2 % ) with losses . This demonstrates that the use of contextual information can improve search quality and user satisfaction .
5 . CONCLUSIONS AND FUTURE WORK
In this paper we have proposed the concept of actively predicting users’ search intents based on their browsing behaviors . Our analysis on large scale user browsing behavior data indicates that many search intents are triggered by the pages that a user browses right before his/her search actions . In order to suggest meaningful queries to satisfy such intents of users when they browse the Web , we have proposed a machine learning method and demonstrated its effectiveness in the scenario of contextual retrieval . Our experimental results have shown that the proposed approach can predict meaningful queries to users for a given page , and can significantly improve the search quality with regards to these queries .
For future work , we would like to study the case that a query is triggered by a sequence of successively browsed pages , and the case that a page triggers several queries with different intents of a user in the same session . We believe that the deep understanding of users’ search intents when they are browsing can help extend the functionality of search engines beyond their current boundaries , and can also provide users with a much better experience on the Web .
6 . REFERENCES [ 1 ] R . A . Baeza Yates , C . A . Hurtado , and M . Mendoza .
Query recommendation using query logs in search engines . In EDBT Workshops , pages 588–596 , 2004 .
[ 2 ] R . A . Baeza Yates and B . Ribeiro Neto . Modern
Information Retrieval . Addison Wesley , 1999 .
[ 3 ] H . Cao , D . Jiang , J . Pei , Q . He , Z . Liao , E . Chen , and
H . Li . Context aware query suggestion by mining click through and session data . In KDD , pages 875–883 , 2008 .
[ 4 ] CJC Burges , T . Shaked , E . Renshaw , A . Lazier , M . Deeds , N . Hamilton , G . Hullender . Learning to Rank using Gradient Descent . In ICML , pages 89–96 , 2005 .
[ 5 ] Z . Cao , T . Qin , T Y Liu , M F Tsai , and H . Li .
Learning to rank : From pairwise approach to listwise approach . In ICML , pages 129–136 , 2007 .
[ 6 ] O . Chapelle , B . Scholkopf , and A . Zien .
Semi Supervised Learning . MIT Press , Cambridge , MA , 2006 .
[ 7 ] C . LA Clarke , M . Kolla , G . V . Cormack ,
O . Vechtomova , A . Ashkan , S . Buttcher , and I . MacKinnon . Novelty and diversity in information retrieval evaluation . In SIGIR , pages 659–666 , 2008 .
[ 8 ] D . Cossock and T . Zhang . Subset ranking using regression . In COLT , pages 605–619 , 2006 .
[ 9 ] Y . Freund , R . Iyer , R . Schapire , and Y . Singer . An efficient boosting algorithm for combining preferences . Journal of Machine Learning Research , 2003 ( 4 ) .
[ 10 ] S . Gollapudi and A . Sharma . An Axiomatic Approach for Result Diversification . In WWW , pages 381–390 , 2009 .
[ 11 ] M . P . Grineva , M . N . Grinev , and D . Lizorkin .
Extracting key terms from noisy and multitheme documents . In WWW , pages 661–670 , 2009 .
[ 12 ] T . Joachims . Optimizing search engines using clickthrough data . In KDD , pages 133–142 , 2002 .
[ 13 ] R . Jones , B . Rey , O . Madani , and W . Greiner .
Generating query substitutions . In WWW , pages 387–396 , 2006 .
[ 14 ] R . Mihalcea and A . Csomai . Wikify! : linking documents to encyclopedic knowledge . In CIKM , pages 233–242 , 2007 .
[ 15 ] F . Radlinski , R . Kleinberg , and T . Joachims . Learning
Diverse Rankings with Multi Armed Bandits . In ICML , 2008 .
[ 16 ] J . Shi and J . Malik . Normalized cuts and image segmentation . In IEEE Transactions on Pattern Analysis and Machine Intelligence , 22:888 905 , 2000 . [ 17 ] R . W . White , M . Bilenko , and S . Cucerzan . Studying the use of popular destinations to enhance web search interaction . In SIGIR , pages 159–166 , 2007 .
[ 18 ] R . W . White , P . Bailey , and L . Chen . Predicting user interests from contextual information . In SIGIR , pages 363–370 , 2009 .
[ 19 ] I . H . Witten , G . W . Paynter , E . Frank , C . Gutwin , and C . G . Nevill Manning . Kea : Practical automatic keyphrase extraction . In ACM DL , pages 254–255 , 1999 .
[ 20 ] C . Zhai and J . Lafferty . A study of smoothing methods for language models applied to Ad Hoc information retrieval . In SIGIR , pages 334 342 , 2001 .
[ 21 ] http://enwikipediaorg/wiki/Correlation [ 22 ] http://enwikipediaorg/wiki/Kullback
Leibler divergence
[ 23 ] http://enwikipediaorg/wiki/Jensen
Shannon divergence
[ 24 ] http://searchengineland.com/nielsen netratings august 2007 search share puts google on topmicrosoft holding gains 12243
[ 25 ] http://wwwaccuracastcom/search daily news/seo7471/us search engine market share data jan 2009/
[ 26 ] http://wwwcomscorecom/Press Events/
Comunicados de prensa/2007/node 1285/Top US Search Engines
WWW 2010 • Full PaperApril 26 30 • Raleigh • NC • USA230
