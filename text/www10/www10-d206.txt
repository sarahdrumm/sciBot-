Monitoring Algorithms for Negative Feedback Systems
Mark Sandler
Google Inc
76 9th Avenue New York , NY ,
USA sandler@google.com
S . Muthukrishnan
Google , Inc
76 9th Avenue New York , NY ,
USA muthu@google.com
ABSTRACT There are many online systems where millions of users post original content such as videos , reviews of items such as products , services and businesses , etc . While there are general rules for good behavior or even formal Terms of Service , there are still users who post content that is not suitable . Increasingly , online systems rely on other users who view the posted content to provide feedback .
We study online systems where users report negative feedback , ie , report abuse ; these systems are quite distinct from much studied , traditional reputation systems that focus on eliciting popularity of content by various voting methods . The central problem that we study here is how to monitor the quality of negative feedback , that is , detect negative feedback which is incorrect , or perhaps even malicious . Systems address this problem by testing flags manually , which is an expensive operation . As a result , there is a tradeoff between the number of manual tests and the number of errors defined as the number of incorrect flags the monitoring system misses .
Our contributions are as follows :
• We initiate a systematic study of negative feedbacks systems . Our framework is general enough to be applicable for a variety of systems . In this framework , the number of errors the system admits is bounded over the worst case of adversarial users while simultaneously the system performs only small amount of manual testing for multitude of standard users who might still err while reporting .
• Our main contribution is a randomized monitoring algorithm that we call Adaptive Probabilistic Testing ( APT ) , that is simple to implement and has guarantees on expected number of errors . Even for adversarial users , the total expected error is bounded by εN over N flags for a given ε > 0 . Simultaneously , the number of tests performed by the algorithm is within a constant factor of the best possible algorithm for standard users .
• Finally , we present empirical study of our algorithm that shows its performance on both synthetic data and real
Copyright is held by the International World Wide Web Conference Com(cid:173 ) mittee ( IW3C2 ) . Distribution of these papers is limited to classroom use , and personal use by others . WWW 2010 , April 26–30 , 2010 , Raleigh , North Carolina , USA . . ACM 978(cid:173)1(cid:173)60558(cid:173)799(cid:173)8/10/04 . data accumulated from a variety of negative feedback systems at Google . Our study indicates that the algorithm performs better than the analysis above shows .
Categories and Subject Descriptors H12 [ Information Systems ] : Models and Principles— User/Machine Systems ; I20 [ Computing Methodologies ] : Artificial Intelligence—General
General Terms Human Factors,Algorithms,Experimentation
Keywords Negative Feedback Systems , User Reputation , Probabilistic Analysis
1 .
INTRODUCTION
The World Wide Web made information dissemination much easier and more efficient , from communication and commerce to content . In particular , not only did more offline content come online and access to it made easier , but generation and publication of content has become easier . Millions of users publish blogs , self made videos , and post comments or reviews of products and businesses such as hotels , restaurants , and others . Users also post and answer questions , tag pictures and maps , form and nurture social networks , etc . This giant publication system works by implicit understanding that content should be appropriate ( eg , avoid porn ) , legal ( eg , no illegally copied content ) , believed to be correct ( eg , when tagging maps and answering questions factually ) , or respectful of others privacy ( eg , with blogging and social networking ) , etc . ; explicitly , these are enforced by Terms of Service that gives the platform provider the right to remove content , cancel access , report to police , etc . However , for a variety of reasons — non professional users , monetary incentives , lack of cost to action , etc . — such user generated content is always suspect . The challenge is how to identify inappropriate content at the Internet scale where millions of pieces of content being generated every day . While sophisticated algorithms that explicitly identify inappropriate content are used in cases ( eg , with finding copyright violations ) , a common solution has been to rely on the community , ie , other users , to identify and “ flag ” inappropriate content .
We refer to such systems where users flag or report inappropriate content as negative feedback systems . Exam
WWW 2010 • Full PaperApril 26 30 • Raleigh • NC • USA871 ples of negative feedback systems include Amazon reviews1 , or YouTube comments2 . There are many other examples . These systems should be distinguished from others that rely on user participation , in particular those that identify popular items based on user feedback . These use various voting schemes to tap into the wisdom of crowds and aggregate positive feedback , for say ranking content.3 Such ratings systems rely on the fact that single popular item can score thousands of ratings . In contrast , negative feedback systems are expected to take action quickly , in some cases even after a single flag on certain content .
The community based policing in negative feedback systems faces a different challenge . How do the systems know when a user ’s flag is genuine and something to be acted upon ? Flags may be simply incorrect due to an error on the user ’s part ( mistaken about an answer to a query ) , or maliciousness ( flag and remove a good review for a competitor ) , or even cluelessness ; in some cases , there may be revenge flags . Known systems solve this problem by manually evaluating some or all the flags . There are two underlying assumptions here . First , humans can identify a correct flag . Second , human testing of flags is more scalable than human testing of the original contents . These are reasonable assumptions in many applications . As an example , consider YouTube comments : humans can readily spot incorrect flags of spam , and the number of flags is several orders of magnitude smaller than the number of either user generated videos or comments.4 However , as the systems grow in size , even testing of all flags becomes prohibitively expensive and becomes prone to denial of service style attacks . The challenge is then reduced to a tradeoff : number of flags tested by humans versus the number of incorrect flags the system misses . Testing all the flags by humans will be prohibitive but detect all incorrect flags , and testing none will allow far too many incorrect flags . In this paper , we study this tradeoff .
1.1 Our Formulation
Consider a set of items I , and a user u who generates a sequence of flags i1 , i2 , . . . , iN . These flags correspond to the items ij that the user u deems abusive . The flag could either be true or false . A flag ij is True means that the item ij violates Terms of Service ; a false flag indicates that the item is not abusive and user committed an error ( whether honestly or maliciously ) when reporting it . For each flag , the monitoring algorithm performs one of the three actions , A = {accept , reject , test} , and the outcome is S = {accepted , rejected , positive , negative} . The first two states correspond to the case when algorithm accepted the report and took appropriate action against the item ( say , removed or demoted it ) or rejected the flag ( did not perform any action against content ) without any further testing . The last two correspond to the case where the algorithm chooses to perform external , human based testing and discovered 1See views . Introduction Algorithms Third Thomas Cormen/dp/ 0262033844/ 2See where reported spam comments are hidden by default . 3For example , see http://diggcom/about/ 4Of course , since humans do not test content that are not flagged by any user , there could potentially be false negatives . This , however , is a reasonable outcome , since it indicates that the content is not seen by too many users . rehttp://wwwamazoncom/ http://wwwyoutubecom/watch?v=4TpRAp0WWLs
This ” eg ,
“ Report link in product the true state of the flag . If the algorithm chooses to test , depending on the outcome of the test the system will either accept or reject the item . We further assume that an item is not flagged multiple times ; in fact , multiple flags are seldom seen in our applications and can be handled as a sequence of independent , single flags . In fact as we discuss later , very few flags per item , is a crucial difference between our system and traditional reputation systems . Formally ,
• User strategy is a function U : A∗ × r → {true , false} that takes as an input vector A∗ of past actions of the monitoring algorithm , a random vector r , and generates the next flag .
• The monitoring algorithm R : S∗×r → A is a function that takes as an input the vector S∗ of past outcomes , a random vector r and returns the action for the current flag .
Interestingly , we do not assume any structure between items ( such as , two videos were posted by the same user ) or model the correlation between items with flags ( such as , reviews in poor language tend to get flags , or videos with DJ mixes get fewer flags ) . Our approach here is to focus on users alone , and ignore the signals from the content . This may be seen as analogous to web search where link analysis , without content analysis , gives lot of information [ 5 , 9 ] . The additional motivation for us is that focusing on user analysis makes our model applicable across content types ( be they video , review text or others ) and general . Finally , notice that the monitoring algorithm has no access to user strategy and it only learns something about user when it decides to test the flag , and not in other actions . This is in contrast with standard learning with experts framework where algorithm learns something on each action or step .
Say user u places N flags . Given a randomized monitoring algorithm A we measure :
• tA u ( N ) , the expected number of tests performed by algorithm A on the sequence of N flags by user u , and
• eA u ( N ) , the expected number of errors — an incorrect flag that is accepted or a correct flag that is rejected — by the algorithm for user u .
1.2 Our Contributions
To the best of our knowledge this is the first effort to formalize the problem of monitoring negative feedback . More formally , we study the tradeoff between tA u ( N ) . • Our main contribution is the design of a randomized algorithm called Adaptive Probabilistic Testing ( AP T ) to process flags in real time as they are being submitted by users , and a detailed theoretical analysis of it . We prove that AP T satisfies the following : u ( N ) and eA u
– [ Adversarial Users ] eAP T
( N ) ≤ εN in expectation against any user u and any fixed ε , 0 ≤ ε ≤ 1 . The user strategy could be adversarial and user can observe actions of the monitoring algorithms once they are performed and adjust his strategy arbitrarily .
– [ Standard Users ] We denote by STD(p ) a standard user who errs with some probability p independently on each flag . Let OPT be the optimal algorithm for monitoring such a user with
WWW 2010 • Full PaperApril 26 30 • Raleigh • NC • USA872 eOPT STD(p)(N ) ≤ εN . We show that for APT algorithm we have : tAP T STD(p)(N ) ≤ 4 tOPT
STD(p)(N ) + o(N ) . in other words , the adaptive testing algorithm performs within constant factor of the best possible algorithm for that particular type of user .
• We present an experimental study of our algorithm with synthetic and real data collected from various Google systems . Our algorithm satisfies eu(N ) ≤ εN almost always ( not only in expectation ) , and at the same time , behaves significantly better than our theoretical analysis , more like tAP T STD(p)(N ) + o(N ) .
STD(p)(N ) ≤ tOPT
Thus the framework we use for evaluating any monitoring algorithm involves two properties : ( 1 ) the number of errors should be bounded in all cases including adversarial users , and simultaneously , ( 2 ) in a system where overwhelming majority of the users are nonmalicious , the monitoring algorithm should perform almost as well as the best possible algorithm that satisfies ( 1).5 It is our experience that practitioners do need both of the properties above . Systems need to be robust when it comes to spammers , but also graceful for majority of users who tend to be honest . Notice that it is not trivial for a monitoring algorithm to satisfy these two properties simultaneously . For example , a naive approach would be to test a user at the beginning to see if the user is standard , determine p and then thereafter run OPT for user STD(p ) . This however will not satisfy the first property because a strategy available to a user is to pretend to be STD(0 ) at the beginning ( eg never lie ) and then switch to STD(1 ) . In fact , our algorithm is far more graceful : if an adversarial user becomes standard only for certain consecutive number of flags and is arbitrary elsewhere , our algorithm will automatically behave like OPT for most of that portion when user is standard . We do not formally abstract this property for the various portions , and only focus on the two properties above over the entire sequence of flags .
1.3 Related Work and Technical Overview
Our model is very general and can be applied to many real world negative feedback systems . Because of its generality however it is reminiscent of many other problems . In particular it is reminiscent of “ online label prediction problems ” , where the task is to predict the label on each item . But typically in such problems , there is an underlying wellstructured class of hypotheses and errors are measured with respect to best in this class . In contrast , users may have arbitrary strategy in our problems , and no structured class of hypotheses may fit their behavior . In multiarmed bandit [ 2 ] , expert learning setting with limited feedback [ 6 ] , apple tasting [ 7 ] , and other learning problems , for each online item , one is given the correct label for all experts or arms after the action is performed , but in our problem , we obtain the correct label only when we test an item . Hence our monitoring algorithms have to work more agnostically . Our approach is also reminiscent of large class of “ reputation problems ” where a user ’s reputation is measured based on his agreement with other users or item quality and is used as weight to measure how much the system values user feedback . Reputation problems arise in user driven systems such as Wikipedia [ 1 , 4 ] and others [ 3 , 8 ] . Such reputation schemes do not apply directly to negative feedback systems , since our systems do not have opportunity to adjust a users’ errors based on other users.6
Despite the plethora of work in related areas , and practical motivations , to the best of our knowledge there is very little work done in the area of negative feedback systems . In fact , the only work we are aware of is empirical paper by Zheleva et al [ 10 ] that considers a problem of computing trust score of e mail users reporting spam/non spam . Their results considers a community based scoring and a fixed scoring function and present empirical results , however , their algorithm provides no guarantees against malicious users .
Approaching our problem from the first principles , it is immediately clear that the monitoring algorithm has to use randomization to determine what flags to test . The algorithm we design is simple and natural , keeping a testing probability pi that is adjusted based on feedback from tests , up or down based on whether tests reveal correct or incorrect flags . The main difficulty is its analysis because of the dependence of the state of the algorithm to the entire trajectory of testing probabilities and user strategies . In particular , the analysis of expected behavior relies on careful upper bounding the hitting times of sequences of Bernoulli trials with stopping probabilities pi1 ≥ pi2 ≥ pil . . . . The stopping probability itself changes non deterministically , in fact influenced by user strategy , over time in such a way that there is no explicit upper bound on expectation . Instead , we rely on expectations of hitting times conditioned on the fact that stopping probability stays above some fixed value , and then generalize the results .
2 . ALGORITHM AND INTUITION
In this section we describe our algorithm and provide in tuition why it works ; we defer full analysis to Section 3 .
We begin by presenting a monitoring algorithm which only performs two actions : test and accept . This algorithm has applications of its own such as when leaving abusive content after it was reported is unacceptable for legal reasons . Similarly , a monitoring algorithm which either rejects or tests has applications of its own in systems where accidentally removing content bears high cost . Later we generalize these two algorithms into an universal monitoring algorithm with at most ε1N false positive ( accepts erroneous flags ) and at most ε2N false negative errors ( ignores correct flags ) .
The high level idea behind the algorithm is as follows . When flag i arrives , the algorithm flips a pi biased coin and tests the flag with probability pi and accepts it otherwise . After each flag , the new probability pi+1 is computed . The crux is to determine the update rule for the consequent of pi ’s . A naive approach would be to set p1 = ··· = pk = 1 for some k , and use the fraction of incorrect flags out of k as pj for j > k . However , if user changes strategy after k flags , this method will fail . A different approach is to fix some window size w and test a fraction in each window i to estimate the number ni of incorrect flags , and use ni/w
5For calibration , tOPT STD(p)(N ) = c(ε , p)N tests if p > ε and O(1 ) otherwise . This is far fewer than what an optimal algorithm needs for an adversarial user .
6If some item was demoted or deleted on request of a user , it will no longer be equally visible to other users , and thus likely to become uncorrectable .
WWW 2010 • Full PaperApril 26 30 • Raleigh • NC • USA873 as testing probability for subsequent window . Again , if user changes strategy between windows , this method will fail and make far too many errors . Thinking about this further , one will realize that we need a more flexible way to combine testing probability , and knowledge from each testing . Our approach is based on the following simple observation – if we test flags with probability p , then upon discovery of a false flag , the expected number of missed false flags is 1−p p , independently of user strategy . Indeed , since each false flag is tested with probability p , the expected stopping time on a sequence of false flags is 1−p p . Thus , intuitively if between two consecutive discoveries of false flags the testing probability was in the range [ p′ , p′′ ] then we have both lower and upper bound on the expected number of missed flags in between as 1−p′ . The formal proof of this statement is not obvious since the actual probability p′ is changing nondeterministically and is in fact not bounded away from zero in advance . A technical achievement in our paper is to indeed develop this idea formally as in Theorem 3.1 , but this intuition suffices . Using this , we can keep track of approximately how many false items we have accepted ( eg number of false positive errors ) , and thus can chose new pi in such a way so that we satisfy the constraint on the number of false positives in expectation . See Algorithm 1 for the full details . p′ and 1−p′′ p
Algorithm 1 Test Accept Algorithm Input : A stream of flags . Output : For each flag we either accept or test it . If the flag is tested the algorithm learns its true state Description :
1 . Set testing probability pi = 1 , estimated number of false skipped flags at L = 0 ;
2 . For each flag i ,
( a ) Test flag with probability pi and accept otherwise . ( b ) If flag is tested and the flag is false , set L ←
L + 1−pi pi
( c ) Set the new testing probability pi+1 ← 1
εi+1−L .
Test accept and test reject cases are symmetric ( with default action accept action being replaced by reject , and in step 2b , L increases if the flag is true ) . For completeness Algorithm 2 , provides full details on test reject algorithm .
Combining Two Cases . The idea behind test accept reject algorithm is that we just run test accept and test reject algorithms in parallel , with only one of them being active and producing the next action . After every step , both algorithms advance one step , and we re set active algorithm to the one which has lower probability of testing . The complete algorithm is given on Figure 3 .
3 . ANALYSIS
We first analyze errors by the monitoring algorithm against an adversarial user ; later , we analyze the number of tests it performs against a standard user .
Algorithm 2 Test Reject Algorithm Input : A stream of flags . Output : For each flag we either reject or test it . If the flag is tested the algorithm learns its true state Description :
1 . Set testing probability pi = 1 , estimated number of true skipped flags at L = 0 ;
2 . For each flag i ,
( a ) Test flag with probability pi and reject otherwise . ( b ) If flag is tested and it is true , set L ← L + 1−pi ( c ) Set the new testing probability pi+1 ← 1 εi+1−L . pi
Algorithm 3 Adaptive Probabilistic Testing algorithm Input : Stream of flags , constants ε1 , ε2 Output : For each flag , output test , accept or reject Description :
Let A and B be the test accept and test reject algorithms respectively . For each flag , the algorithms A and B are run in parallel to produce probabilities pAi and pBi . If pAi < pBi , set algorithm A as active , B as passive . Else , set B as active and A as passive . Active algorithm flips a coin performs its action and updates its state . Passive algorithm is executed to update its pi , but the suggested action is not performed .
3.1 Adversarial Users
We begin by analyzing the test accept algorithm . For each flag this algorithm tests each flag with certain probability and accepts it otherwise . Thus the only type of error admitted is false positives , where algorithm accepts a false flag . Intuitively , how many undetected false flags there are between two detected ones ? We begin by estimating the run length until the first detected flag , if the testing probabilities is some non increasing sequence {pi} .
Lemma 31 Let {ri} be a sequence of Bernoulli trials with parameter pi , where {pi} is monotonically nonfor increasing sequence , and pi itself can depend on rj , j < i . Let Q ∈ [ 0,∞ ] be the hitting time for the sequence {r0 , r1 , . . } In other words random variable Q is equal to the first index i , such that ri = 1 . Then for any γ , we have the expectation bound : E [ Q|pQ ≥ γ ] ≤ ( 1− γ)/γ and E [ Q|pQ ] ≤ ( 1− pQ)/pQ ( 1 ) and further the realizations are concentrated around the expectation : Pr [ Q > c/γ |pQ ≥ γ ] ≤ e−c and Pr [ Q > c/pQ ] ≤ e−c ( 2 ) Proof . Consider sequence r(γ ) i = ri if pi ≥ γ and is Bernoulli trial with probability γ otherwise . And suppose Q(γ ) is a hitting time for r(γ )
, such that r(γ ) i
. Then i
EhQ(γ)i = EhQ(γ)|pQ ≥ γi Pr [ pQ ≥ γ ]
+ EhQ(γ)|pQ < γi Pr [ pQ < γ ] ≥ EhQ(γ)|pQ ≥ γi
WWW 2010 • Full PaperApril 26 30 • Raleigh • NC • USA874 f1 = 2 f2 = 3 f3 = 4 f4 = 6 f5 = 7
True
False
False
False
True
False
False
Tested
Accepted
Accepted
R1 = 2 ( 2 missed false flags )
Tested g1 = 4
Accepted
Accepted
R2 = 1 ( 1 missed flag )
Tested g2 = 7 f6 = 9
False
Tested g3 = 9
True
Tested
R3 = 0
Figure 1 : A sequence of flags . Gray flag indicates that the flag was not tested and its true state is unavailable to the algorithm . fi indicates indices of all false flags ( both discovered and not ) , gi indicates realization of indices of discovered false flags . Ri is a realization of random variable “ the number of undiscovered flags between two sequential gi ’s ” . where in the first transition we used the linearity of expectation and in the second we used the fact that for any fixed se quence P = {pi} , EhQ(γ)|pQ ≥ γ , Pi < EhQ(γ)|pQ ≤ γ , Pi . On the other hand Q(γ ) and Q are equal to each other if pQ ≥ γ . Thus , we have similarly for probabilities we have :
E [ Q|pQ ≥ γ ] = EhQ(γ)|pQ ≥ γi ≤ EhQ(γ)i Pr»Q ≥ γ |pQ ≥ γ– γ–
γ |pQ ≥ γ– = Pr»Q(γ ) ≥ ≤ Pr»Q(γ ) ≥ c c c
Now we just need to show EhQ(γ)i ≤ ( 1 − γ)/γ and PrhQ(γ ) ≥ cγi ≤ e−c . Observe that Q(γ ) can be upper bounded by geometric random variable G ≥ 0 with parameter γ . Indeed , let us suppose gi is 1 with probability min{1 , γ/pi} if r(γ ) i = 1 , and is 0 otherwise . Unconditionally each gi is 1 with probability γ . Thus , hitting time G for {gi} is a geometric random variable , and by definition G ≥ Q . Since expectation of G is ( 1− γ)/γ we have the first part of our lemma . The second part of equation ( 1 ) follows from the definition of conditional expectation . To prove the equation ( 2 ) , we note that
Pr [ G > c/γ ] = ( 1 − γ ) c
γ +1 ≤ e−c since it is exactly the probability that a sequence of Bernoulli trials with identical probability γ does not hit 1 after c γ steps . Since Q(γ ) ≥ G in the entire space , we have the desired bound .
Theorem 32 For the test accept algorithm , the expected number of errors eu(N ) ≤ εN for an adversarial user u . Proof . We count the expected number of undetected false positives so far after we test the ith flag . The crux is to consider the underlying sequence of false flags and corresponding testing probability , and hide all the true flags inside the probability changes pi and apply lemma 31
Suppose the false flags have occurred at positions f1 , f2 . . . fl . We do not know what those fi are , but our goal is to show that for any sequence the desired lower bounds holds . Denote ri a random variable that indicates whether i th false flag has been accepted without testing . In other words ri is a sequence of bernoulli trials each occurring with probability 1 − pfi .
Consider g0 , g1 , gl′ where g0 = 0 , and gi is an index of the ith detected false flag . In other words {gi} is a random subsequence of {fi} where algorithm detected false flag . Note that while fi are unknown , gi are the steps of the algorithm where we test and discover false flags and thus are known . Let Ri denote a random variable that is equal to the number of false flags between flags gi−1 and gi . We illustrate all the notation we used with an example on Figure rfj . And thus i=1 rfi , therefore it is sufficient for us to esti
31 It is easy to see that Ri =Pj:gi−1≤fj <gi Pl′ i=1 Ri = Pl mate EhPl′ i=1 Rii . Note that Ri is a hitting time for the sequence of pgi−1 , . . . pgi , where the sequence if over hidden false flags pgi is not bounded a priori . Since our algorithm does not increase testing probability if it does not detect false flags by Lemma 3.1
E [ Ri|pgi ] ≤
1 − pgi pgi
.
Further , note that for fixed pgi the expectation is bounded independently of all earlier probabilities and therefore : l′
Xi=1
E [ Ri|pg0 . . . pgi ] =
≤ l′ l′
Xi=1 Xi=1
EˆE [ Ri|pgi ]˛˛pg1 , . . . pgi−1˜ 1 − pgi pgi ≤ εN . where the last transition follows from the step 2c of Algorithm 1 , where we have pgi = ε(gi−1)+1−L and thus 1−pgi . Hence
1
1−pgj pgj j=1 pgi ≤ εgi − Lgi , where Lgi = Pi Pl′ i=1 To finish the proof :
1−pgi pgi ≤ εN . l′
Xi=1 l′
E [ Ri ] = E2 Xi=1 4
E [ Ri|pg1 ]3 5
WWW 2010 • Full PaperApril 26 30 • Raleigh • NC • USA875 Expanding the right hand part , we have : l′
Xi=2
E [ Ri|pg1 ]3 5
E2 E [ R1|pg1 ] + 4 = E2 4E [ R1|pg1 ] + E [ R2|pg1 , pg2 ] + = ·· · = E2 E [ Ri|g1 , . . . gi]3 Xi=1 4 5 pgi 3 ≤ E2 1 − pgi 4
5 ≤ εN
Xi l′ l′ l′
Xi=3
E [ Ri|pg1 , pg2 ]3 5
Similarly to above , the same results apply to the testreject algorithm . Combining these two results together we have :
Theorem 33 For AP T monitoring algorithm the expected number of false positives is at most ε2N and the expected number of false negatives is at most ε2N .
Indeed , lets A denote the set of items where testProof . accept algorithm was active , and let B denote the set of items where test reject algorithm was active . During the B phase , the test accept algorithm did not produce any mistake ( since no item was accepted ) , thus the expected number of errors still test accept is ε1N . More formally , we define Ri as aa hitting times , of detecting false flags , with an extra constraint that the algorithm must have been active , and the analysis carries through .
3.2 Standard Users
In this section , we consider standard users . Recall that for a standard user , each flag is incorrect with some unknown probability pu . This models two dimensions about users in negative feedback systems . First , even genuine users err sometimes , but it is not correlated across items , and hence we assume it is with some fixed , independent , but unknown probability pu . Second , some of non malicious users might be clueless and err ; in such cases , the error probability pu is again considered independently random , not correlated with the items . We abstract these as STD(p ) users . Note that p may be small as in the first case or large as in the second case . What is the minimum number of tests we need to perform to guarantee at most ε1N of false positive ? Since the user is random the only parameters we can tune are the number of tests T , the number of accepts A and the number of rejects R with the goal of minimizing T , since it does not matter which flags got tested :
T + A + R = N , Ap ≤ ε1N , R(1 − p ) ≤ ε2N , min T
Thus if p ≤ ε1 then we can accept all the flags and not do any testing . On the other hand if p ≥ 1 − ε2 , then we can reject all flags and again not perform any testing . In the general case it can be shown that the total fraction of tested flags will be at least 1 − ε1 1−p . In the case when ε2 = 0 we get the total fraction of flags that needs to be tested is at least p−ε1 and if ε1 = 0 it becomes 1−p−ε2 p − ε2
. p
We now analyze the behavior of our algorithm and show that for a standard user the algorithm is competitive with
1−p respect to the optimal algorithm described above . Equivalently , we prove that if p ≤ ε then the expected number of tests is o(N ) and if p ≥ ε then it is bounded by 4 OPT . As empirical evaluation in Section 4 shows , the analysis below is very likely to be not tight , and providing a tighter constant is an interesting open problem . p
Theorem 34 For a STD(p ) user with N flags , each false with probability p , the test accept algorithm performs ε tests if p ≤ ε , and γN + c tests othin expectation βN erwise , where γ = 4 p−ε and c and β are O(1 ) . Similarly p if p ≥ 1 − ε test reject algorithm performs at most βN and 4 1−p−ε
1−p
ε
1−p N + c otherwise .
Proof . Suppose our target is ε fraction of errors . It is easy to see that the algorithm can be reformulated as follows . At step i test with probability 1+εi , and every time the item is tested , the probability of testing resets back to 1 with probability p . The question then becomes what is the expected number of tests we will perform ? The full proof is given in the appendix .
1
Finally , we analyze the performance of the AP T algorithm .
Theorem 35 Consider STD(p ) a user u with N flags . The number of tests performed by the AP T algorithm is at most 4 OP T + 2 max(ε1 , ε2)N + o(N ) .
Proof .
The total number of tests is the lesser of the number of tests performed by either of the test accept and test reject algorithms in isolation . Thus it is sufficient to only consider ε1 ≤ p ≤ 1 − ε2 , otherwise , by Theorem 3.4 the expected number of tests is o(N ) . For the latter case we have , the expected number of tests is : tAP T ST D(p)(N ) ≤ 4n min(1 −
ε1 p
, 1 −
ε2 1 − p
) .
( 3 )
ST D(p)(N ) ≥ N ( 1− ε1
If p ≥ 1/2 , the number of tests performed by the optimal 1−p ) ≥ N ( 1− ε1 algorithm is tOP T p −2ε2 ) . Similarly , for p ≤ 1/2 the number of tests is bounded by : ST D(p(N ) ≥ N ( 1− ε1 tOP T 1−p −2ε1 ) , combining these two inequalities with equation ( 3 ) we have the desired result . p − ε2 1−p ) ≥ n(1− ε1 p − ε2
4 . EXPERIMENTS
In this section we perform two kinds of experiments . First , on synthetic data , we compare our algorithm with the optimal algorithm which knows user strategy in advance . The primary goal here is to show that not only the algorithm performs only constant times as optimal , but to further demonstrate that the constant is very close to 1 .
Second , we present results from running our algorithm on real data consisting of abuse reports submitted to several Google properties . An important observation here is that since our algorithms are not using the actual content in any way , the only important quantity is the number of reports per user . In all our experiments we assume that acceptable error level for either false positive or false negative type of errors is 01
Synthetic data . We first demonstrate the performance ( number of tests and number of errors ) of the algorithm against standard users . To achieve this we plot the optimal
WWW 2010 • Full PaperApril 26 30 • Raleigh • NC • USA876 Figure 2 : Experiments on synthetic data s t s e t / s r o r r e f o r e b m u N
600
500
400
300
200
100
0
Number of tests performed by the algortihm Minimum number of tests required False negatives False positives
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9 y t i l i b a b o r P
1
0.8
0.6
0.4
0.2
0
0
Testing probability User probability of error OPT False positive error rate False negative error rate
1000
2000
3000
4000
5000
The probability of error by the user number of flags reported
( a ) The number of tests and the number of errors admitted by AP T algorithm when user happens to be STD(p ) vs that by the optimal algorithm that knows p .
( b ) Performance of the AP T algorithm for STD(p ) user that changes p over time . number of tests for fixed acceptable error ε = 0.1 and p changing from 0.01 to 1 for STD(p ) against average number of tests performed by AP T . For each p we assume 1000 flags and run the experiments 30 times , to get accurate estimate of the expected number of tests performed . The results are presented in Figure 3(a ) . It is clear that our algorithm is in fact much closer to the optimal than the theoretical analysis above suggests , and tAP T STD(p)(N ) + o(N ) .
STD(p)(N ) is more like tOPT
On the Figure 3(b ) we consider a user who changes his error probability over time . The step function with extreme values between 0 and 1 is user real error rate . The best optimal test rate ( if the algorithm knew the underlying constant ) , is step function bounded by 05 The line , closely following the latter , shows the testing probability for the AP T algorithm when STD(p ) user keeps changing p . It is clear that AP T algorithm automatically adjusts its testing rate nicely to be nearly close to the best testing for STD(p ) for whatever p the user uses for a period of time .
Experiments with real data . In this section we present experimental results that we perform using real data , which contains a subset of abuse reports accumulated by various Google services over the period of time of about 2 years . The dataset contained roughly about 650 randomly selected anonymized users who submitted at least 50 reports to the system ( some submitted considerably more ) . Their total contribution was about 230,000 flags . The algorithm computed testing probability independently for every user , and thus our guarantees apply for every user independently . Our goal was to measure to average testing rate as a function of the total number of flags , since it translates to immediate reduction of amount of manual labor required . On Figure 3(c ) we plot the number of total flags arrived into system ( blue curve ) , vs the total number of tested flags ( green curve ) . The bottom three curves show the actual fraction of admitted errors vs the acceptable error levels .
To illustrate the actual fraction of tested flags we refer to Figure 3(d ) . As one can see the testing ratio in general hovers around 0.35 , which means that only roughly 1 in every 3 user flags needs to get tested , and the remainder can be acted on automatically .
5 . FUTURE WORK AND CONCLUSIONS
We described a simple model for monitoring negative feedback systems , and presented the AP T algorithm with expected number of errors ≤ εN for even adversarial users ; for a standard user STD(p ) , the expected number of tests is close to that of the optimal algorithm for STD(p ) . Practitioners look for such algorithms that are resistant to adversary users , while still being gracefully efficient to standard users . We have found these algorithms useful for some of the Google systems . u
From a technical point of view , the immediate open question is if our analysis of AP T algorithm can be improved since our experiments indicate tAP T STD(p)(N ) behaves more like tOPT STD(p)(N ) + o(N ) . Further , could we extend the expected case analysis in this paper to high concentration bounds ? We are able to analyze the AP T algorithm and show that the eAP T ( N ) is within twice the expectation with overwhelming probability for any user , under certain mild conditions . This result appears in Appendix B . We need to use martingale inequalities to prove these bounds , but they require bounded difference , whereas hitting times of our algorithm are not bounded . We overcome this problem by suitably modifying Azuma ’s inequality . This analysis may be of independent interest . Similar high concentration bounds for tAP T
STD(p)(N ) would be of interest .
From a conceptual point of view , negative feedback systems are abundant on the Internet , and we need more research on useful monitoring algorithms . For example , an extended model is to consider items having attributes and typical user error as some function of those attributes that we need to learn . Similarly considering standard ( eg non malicious ) user whose behavior evolves over time is also important . A potential first step in this direction is to compare the performance of a monitoring algorithm with the optimal ( but not necessarily spam resistant ) algorithm that is required to have
WWW 2010 • Full PaperApril 26 30 • Raleigh • NC • USA877 s g a l f
#
180000
160000
140000
120000
100000
80000
60000
40000
20000
0
0
Total flags Flags tested Missed Positives Missed Negatives Acceptable Error Level
1
0.9
0.8
0.7
0.6
0.5
0.4 l a t o t / d e t s e t
100
200
300
400
500
600
700
0
100
200
300
400
500
600
700 days days
( c ) The number of flags that the system tested vs . number of flags vs . the number of errors the total
( d ) The ratio between number of tested flags and the total number of flags as a function of time
Figure 3 : Performance of the algorithm on flags received from real users . bounded number of error on every prefix sequence of user flags for users that change their failure probability gracefully over time . Another potentially interesting direction which has practical justification is to allow monitoring algorithms to take retroactive actions such as deciding to test an item which was accepted or rejected earlier . Finally , a rich direction is to not consider each user individually , but group them according to their past history of reports . This allows a reduction in the amount of testing for users who provide only a few flags since such users can contribute a significant fraction of flags in many real world systems . Such a grouping can be dynamic and depend on users’ strategies as well as other properties , and guarantees will be relative to the group . We hope our work here spurs principled research on monitoring algorithms for negative feedback systems that are much needed .
5.1 Acknowledgements
Authors would like to thank Nir Ailon , Raoul Sam Daruwala , Yishay Mansour and Ruoming Pang and anonymous reviewers for interesting discussion and good feedback on the paper .
6 . REFERENCES [ 1 ] B . Adler and L . de Alfaro . A content driven reputation system for the Wikipedia . In Proc . of the 16th Intl . World Wide Web Conf.(WWW 2007 ) .
[ 2 ] D . Berry and B . Fristedt . Bandit Problems . Chapman and Hall , 1985 .
[ 3 ] R . Bhattacharjee and A . Goel . Avoiding ballot stuffing in eBay like reputation systems . In Proceedings of the 2005 ACM SIGCOMM workshop on Economics of peer to peer systems , pages 133–137 . ACM New York , NY , USA , 2005 .
[ 4 ] R . Bhattacharjee and A . Goel . Algorithms and incentives for robust ranking . In SODA , pages 425–433 . Society for Industrial and Applied Mathematics Philadelphia , PA , USA , 2007 .
[ 5 ] S . Brin and L . Page . The anatomy of a large scale hypertextual Web search engine . Computer networks and ISDN systems , 30(1 7):107–117 , 1998 .
[ 6 ] N . Cesa Bianchi and G . Lugosi . Prediction , learning , and games . Cambridge University Press , 2006 .
[ 7 ] D . Helmbold , N . Littlestone , and P . Long . Apple tasting . Information and Computation , 161(2):85–139 , 2000 .
[ 8 ] S . D . Kamvar , M . T . Schlosser , and H . Garcia Molina .
The eigentrust algorithm for reputation management in p2p networks . In WWW ’03 : Proceedings of the 12th international conference on World Wide Web , pages 640–651 , New York , NY , USA , 2003 . ACM .
[ 9 ] J . Kleinberg . Authoritative sources in a hyperlinked environment . Journal of the ACM , 46(5):604–632 , 1999 .
[ 10 ] E . Zheleva , A . Kolcz , and L . Getoor . Trusting spam reporters : A reporter based reputation system for email filtering . ACM Trans . Inf . Syst . , 27(1):1–27 , 2008 .
APPENDIX
A . PROOF OF 3.4
1
Proof of Theorem 34 We reformulate the algorithm equivalently : at step i test with probability 1+εi , and every time we reset the probability p . The question then becomes what is the expected number of tests we will perform ? Let Xi be the random variable that is 1 if there were no reset until step i , and ith flag was tested ( whether or not the probability was reset on flag i . ) Further let Pi be the probability that there was no reset until step i ( whether or not reset happened on step i ) . Obviously 1+εi . The number of tests that happened before and including the first reset is Pi 1+εi . Let Fn denotes the random variable indicating how many tests were performed during the first n steps . Let Fn,i denote random variables indicating how many tests were performed by the
Pi = Qi−1 the random variable : EˆPn
1+εj ) and E [ Xi ] = Pi j=1(1 − p i=1 Xi˜ =Pn i=1
WWW 2010 • Full PaperApril 26 30 • Raleigh • NC • USA878 algorithm after i th step respectively , provided that the reset occurred at position i , and is 0 if the reset hasn’t occurred . Also let Gn denote the number of tests performed by the algorithm before and including the first reset . Note that the 1+εi Fn−i . On algorithm has no memory and thus Fn,i = Pi the other hand since each test causes a reset with probability p the expected number of tests before the first reset can be expressed as p
Substituting that into ( 4 ) we have :
E [ Fn ] ≤
=
1 p
1 p n−1
+
1 + εi pPiγ(n + c − i )
Xi=1 + ( 1 − Pn)(γn + c ) −
=
1 p
( 1 − Pn+1 )
≤ γn + c +
1 p − Pnc − γp
Thus we need to prove : n−1 pPiγi 1 + εi
Xi=1 Xi=1 ( 1 + ε)(1 − p + εi)α+1 i(1 − p + ε)α+1 n−1
.
1 p − Pncγ − γp n−1
Xi=1 i(1 − p + ε)α+1
( 1 + ε)(1 − p + εi)α+1 ≤ 0
( 6 ) x
( q + εx)α+1 = n−1
Let q = 1 − p : i Xi=1 ( q + ε)α+1 +
( q + εi)α+1 ≥
=
1
=
=
1
( q + ε)α+1 +
1
( q + ε)α+1 + n
1 x=2
( q + ε)α+1 +Z n ( −1 + α)αε2(q + εx)α˛˛˛˛ −(q + αεx ) ( −ε + p)p(q + εx)α˛˛˛˛
( p − ε)p(q + 2ε)α −
−(q + px )
1 + p n x=2 x=2 q + pn
( p − ε)p(q + εn)α
E [ Gn ] =
Pi
1 + εi n
Xi=1 ≤ min(
1 p
) ≤ Where in the first upper bound , we used
ε
,
ε ln(1 + εn ) ln(1 + εn )
1
1 + εi ≤Z n
1
1
εx + 1 dx ≤ ln(1 + εn )
ε
.
Thus :
X i = 1n E [ Fn ] = E" n Xi=1 Xi=1 Now we estimate Pn ln[Pi ] = ln(1 − have : i=1
= n
Fn,i + Gn#
( 4 )
( 5 )
Pi
1 + εi pE [ Fn−i ] + ln(1 + εn )
ε
Pi 1+εi [ 1 + pE [ Fn−i] ] . First of all we p
1 + ε
) + i−1
Xj=2 ln(1 − p
1 + εj
)
After some algebra , where we approximate the sum and integrating , we have e and β = 2 ln(1 + εn ) 1+ε
Case p ≤ ε . We need to show that E [ Fn ] ≤ βnα where α = p ε . The proof is by induction . The base is obvious , to prove the induction hypothesis we have : n
ε
ε
ε
+
≤
≤
≤ ln(1 + εn )
Pi × p × E [ Fn−i ]
1 + εi ln(1 + εn ) ln(1 + εn ) ln(1 + εn )
E [ Fn ] ≤
β(1 − p + ε)α+1nα ( 1 + ε)(1 − p + εn)α
Xi=1 + ( 1 − Pn+1)Fn−1 + βnα − + βnα − β + βnα − where first we used Pn−1 1+εi = 1 − Pn , then that ( 1− p + ε ) ≥ 1 and ( 1 − p + εn)α ≤ ( 1 + εn)α ≤ 1 + ( εn)α α < 1 , 1+εαn ≥ 1/2 . and finally , Case p ≥ ε . We have to prove E [ Fn ] ≤ γn + c . If p ≥ 4/3ε the result is obvious since 4 p−ε p ≥ 1 . We consider p ≤ 4/3ε only . After some algebra , we get the following bound :
( 1 + ε)(1 + εαnα ) 2(1 + ε ) ≤ βnα ln(1 + εn )
≤ nα pPi i=1 nα
β
ε
ε
Pi 1 + εi ≥
( 1 − p + ε ) p ε +1 ( 1 + ε)(1 − p + iε ) p ε +1
Pi ≥
( 1 − p + ε ) p ε +1
( 1 + ε)(1 − p + ( i − 1)ε ) p ε
≥
1
( q + ε)α+1 +
1 + p
( p − ε)p(q + 2ε)α − substituting we have : p
1 + ε n
Xi=1 i(q + ε)α+1 ( q + εi)α+1 ≥ p(q + ε)α+1
1 + ε
`
− q + pn
( p − ε)p(q + εn)α´ p
+
1 + ε
( 1 + p)(q + ε)α+1
( 1 + ε)(p − ε)(q + 2ε)α −
( q + pN )(q + ε)α+1 ≥ ( 1 + ε)(p − ε)(q + nε)α Note term asymptotically behaves as O(n1−α ) = o(1 ) , and on the other hand we have Pn ≥ ( 1+ε)(q+(n−1)ε)α , thus by adjusting c independently of n we can guarantee :
( q+ε)α+1 that last the cPn −
( q + pN )(q + ε)α+1
( 1 + ε)(p − ε)(q + N ε)α ≥ 1 thus we have : cPn + p
1 + ε p
1 + ε
+ n i(q + ε)α+1 ( q + εi)α+1 ≥ ( 1 + p)(q + ε)α+1
Xi=1 ( 1 + ε)(p − ε)(q + 2ε)α − 1 p
( 1 + p)(q + ε ) ( 1 + ε)(p − ε )
1 + ε − 1 + ( p − 1 − ε)(p − ε ) + ( 1 − p2)(1 − p + ε )
( 1 −
1 − p + 2ε p
)
( 1 + ε)(p − ε )
( q + ε)2(q − p2 + ε )
( 1 + ε)(p − ε )
≥
1
4(p − ε )
≥
≥
≥
≥
WWW 2010 • Full PaperApril 26 30 • Raleigh • NC • USA879 Using this bound and substituting γ = 4(p−ε ) ately have ( 6 ) satisfied , as needed . p we immedi
B . CONCENTRATION BOUNDS ON NUM(cid:173 )
BER OF ERRORS
In this section we show that under certain conditions the number of missed false flags is also tightly concentrated around its expectation . In particular we introduce an extra constraint to algorithm 1 , that requires that each pi ≥q 1 i . Curbing at this probability adds O(√N ) extra tests , the adversarial case is not affected , and the standard user expectation only adds o(N ) , thus the analysis carries through . The proof is based on application of Azuma inequality to the sequence of Ri with a twist that since Ri are unbounded we need to bound each Ri with high probability .
Lemma B1 Suppose X1 . . . Xn be a submartingale such that Pr [ Xi+1 − Xi ≥ ci ] ≤ δ n , then
Pr [ Xn ≥ λ ] ≤ exp[
] + δ
λ i
2P c2
Proof . In order to prove the inequality we introduce bounded random variables that are almost always equal to Xi , and then use Azuma inequality combined with union bound . Let Yi = Xi − Xi−1 , and let Y ′i = min(Yi , ci ) , and X′i = Pn i=1 Y ′i , then using Azuma inequality we have Pr [ X′m ≥ λ ] ≤ exp[− λ ] , Consider the underlying probability space and let Ω( . ) denotes the subspace where condition ( . ) is satisfied .
2 P c2 i
We have Ω(X′n 6= Xn ) ⊆ Ω(Y ≥ ci , for some i ) , thus Pr [ X′n 6= Xn ] ≤ nδ n = δ . On the other hand we have Ω(Xn ≥ γ ) ⊆ Ω(X′n 6= Xn ) ∪ ( Ω(X′n ≥ γ ) ∩ Ω(X′n = Xn) ) . Thus :
Pr [ Xn ≥ λ ] ≤ PrˆX′n 6= Xn˜ + PrˆX′n 6= Xn˜
≤ δ + exp[−
λ i=1 c2 i
]
2Pn as needed . have
Theorem B.2
( Concentration ) . If all pi ≥ q 1 Pr [ (e(N ) − εN ) ≥ εN ] ≤ 2 exp[−ε2/3N 1/6 + log N ] i we
.
Note that the constraint on pi is on the algorithm execution ( eg we control all pi ) , and not on the stopping times outcomes , thus we are not introducing any hidden conditioning and do not change distributions . Proof . The idea of the proof is to apply Azuma inequality and use lemma B.1 to get the desired bound . c
1−pgi pgi
Fix δ = exp[−ε2/3N 1/6 + log N ] and denote c = log N/δ , By using lemma 3.1 we and consider Ti = have E [ Ri − Ti|Ti , R0 . . . Ri−1 ] ≤ 0 for any Ti , and thus E [ Ri − Ti|R0 . . . Ri−1 ] < 0 thereforePi i=0(Ri−Ti ) is a submartingale . On the other hand , we have : PrhRi − Ti ≥ c√Ni ≤ Pr»Ri − Ti ≥ pgi– ≤ exp[−c ] ≤ δ N Where we have used that gi ≤ N , constraint on pgi ≥ i √gi and the lemma 3.1 to bound PrhRi − Ti ≥ c pgii . Thus we can use lemma B.1 with bound ci ≤ log(N/δ)√N . For any l′ and λ = εN we have : Pr2 4
5 ≤ δ + exp[−
Ri − Ti ≥ εN3
2l′√N log2(N/δ ) ε2√N
Xi
ε2N 2 l′
]
.
,
≤ exp[−
( log N − log δ)2 ] + δ
Now we substitute for log δ into the exponent and we get as needed :
Ri − Ti ≥ εN3 l′
Pr2 Xi 4
ε2√N
5 ≤ δ + exp[−
( log N − log δ)2 ] ε2√N ( ε2/3N 1/6)2 ] + δ ≤ 2δ .
≤ exp[−
WWW 2010 • Full PaperApril 26 30 • Raleigh • NC • USA880
