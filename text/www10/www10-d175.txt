Hierarchical Feature Selection for Ranking
Guichun Hua , Min Zhang , Yiqun Liu , Shaoping Ma , Liyun Ru
State Key Laboratory of Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Science and Technology , Tsinghua University , Beijing 100084 , China huaguichun@gmail.com , {z m , msp , yiqunliu}@tsinghuaeducn , lyru@vipsohucom
ABSTRACT Ranking is an essential part of information retrieval(IR ) tasks such as Web search . Nowadays there are hundreds of features for ranking . So learning to rank(LTR ) , an interdisciplinary field of IR and machine learning(ML ) , has attracted increasing attention . Those features used in the IR are not always independent from each other , hence the feature selection , an important issue in ML , should be paid attention to for LTR . However , the state of the art LTR approaches merely analyze the connection among the features from the aspects of feature selection . In this paper , we propose a hierarchical feature selection strategy containing 2 phases for ranking and learn ranking functions . The experimental results show that ranking functions based on the selected feature subset significantly outperform the ones based on all features .
Categories and Subject Descriptors : H33 [ Information Search and Retrieval ] : Retrieval models
General Terms : Algorithms , Experimentation
Keywords : Learning to Rank , Feature Selection
1 .
INTRODUCTION
Web search engines are often referred to when people are requiring some information from Internet , and ranking is an essential part in the structure of search engines . Nowadays , hundreds of features for ranking have been proposed eg content based features such as 𝑇 𝐹 𝐼𝐷𝐹 , 𝐵𝑀 25 ; link based features such as 𝑃 𝑎𝑔𝑒𝑅𝑎𝑛𝑘 , 𝐻𝐼𝑇 𝑆 ; user behavior features based on click through data . It is a hot research field to construct more efficient ranking functions based on these features , so LTR , an interdisciplinary field of IR and ML , has gained increasing attention for a few recent years .
The conventional ML research shows that the features and the composition of the features affect the performance of learning methods , and the construction methods of ranking functions for IR show that the features are not independent from each other . For example , the features of 𝑇 𝐹 ( Term Frequency ) and 𝐼𝐷𝐹 ( Inverse Document Frequency ) are elements to construct the feature 𝐵𝑀 25 . However , the stateof the art LTR approaches merely analyze the connection among the features from the aspects of feature selection except [ 6 , 2 ] to the best of our knowledge . [ 6 ] applies the boosted regression trees to select the proper feature subset .
Copyright is held by the author/owner(s ) . WWW 2010 , April 26–30 , 2010 , Raleigh , North Carolina , USA . ACM 978 1 60558 799 8/10/04 . where 𝑛 is the number of clusters . The clustering method we choose is K Means . By this way , the number of clusters can be decided with the quality value .
𝑓𝑣 ,𝑓𝑢∈𝐹𝑖
𝑠𝑖𝑚(𝑓𝑣 , 𝑓𝑢 )
)
𝑁𝑖 = 1 𝑁𝑖 > 1
[ 2 ] considers the feature importance and similarity between two features , and proposes an efficient greedy feature selection method . However , they are both flat feature selection methods which may be biased , and they could not decide which number of features selected is proper .
The main contributions of this paper are that : ( 1 ) we propose a hierarchical feature selection strategy containing 2 phases to make the selected features not biased . ( 2 ) design a quality measure to decide the proper number of selected features . We use Ranking SVM(RankSVM ) [ 3 , 4 ] and ListNet [ 1 ] to verity the strategy because they are powerful and commonly used approaches in LTR [ 7 , 5 ] . The experimental results show that our feature selection methods do significantly improve the performance of the ranking functions .
2 . FEATURE SELECTION STRATEGY
The process of the hierarchical feature selection strategy contains 2 phases : ( 1 ) the similarity between any two features is measured , and the similar features are aggregated into groups ; ( 2 ) the representative feature in each group is selected through either delegation method . By this way , the selected features are not biased to a group of features which are more representative than the ones in other group . 2.1 Cluster based feature similarity analysis The Kendall ’s 𝜏 is chosen as the feature similarity measure and the similarity between features 𝑓𝑖 and 𝑓𝑗 : 𝑠𝑖𝑚(𝑓𝑖 , 𝑓𝑗 ) is calculated as follows :
}
( 𝑑𝑠 , 𝑑𝑡 ) ∈ 𝐷𝑞∣𝑑𝑠 ≺𝑓𝑖 𝑑𝑡 𝑎𝑛𝑑 𝑑𝑠 ≺𝑓𝑗 𝑑𝑡
{
}
( 𝑑𝑠 , 𝑑𝑡 ) ∈ 𝐷𝑞
#
{
𝜏𝑞(𝑓𝑖 , 𝑓𝑗 ) =
# where 𝑑𝑠 ≺𝑓𝑖 𝑑𝑡 denotes 𝑑𝑡 ranks higher than 𝑑𝑠 based on 𝑓𝑖 for document pair ( 𝑑𝑠 , 𝑑𝑡 ) in the set 𝐷𝑞 wrt a query 𝑞 , and #{.} denotes the number of elements in the set {}
Features are clustered according to their similarities . We define a measure based on the intra cluster similarities to estimate the quality of clustering results . But the intracluster similarity would be maximum if the cluster have only one element . Therefore the 𝑃 𝑒𝑛𝑎𝑙𝑡𝑦 is defined to reduce such effect , and it is the average of all similarities as : where 𝑁 is the number of 𝑁∗(𝑁−1 ) features in feature set 𝐹 . The quality measure is as follows :
( ∑
)
2
𝑓𝑣 ,𝑓𝑢∈𝐹 𝑠𝑖𝑚(𝑓𝑣 , 𝑓𝑢 ) 𝑛∑ ( ∑
{
𝑃 𝑒𝑛𝑎𝑙𝑡𝑦 𝑁𝑖∗(𝑁𝑖−1 )
2
𝑖=1
𝑄𝑢𝑎𝑙𝑖𝑡𝑦𝑛 =
WWW 2010 • PosterApril 26 30 • Raleigh • NC • USA1113 y t i l a u Q
0.7
0.6
0.5
0.4
2
10
18
26
34
42
Number of Clusters
Figure 1 : Quality of all clustering results
2.2 Delegation methods
We proposed two delegation methods to choose features from each cluster .
First is the Delegation Method Based on Evaluation Measure ( 𝐵𝐸𝑀 ) . In each cluster , every feature is used solely for ranking on training . Both normal and inverse value of the feature are applied respectively . The ranking results are measured with most commonly used criteria in IR such as MAP and 𝑁 𝐷𝐶𝐺@𝑛 . The feature ( or the inverse of the feature ) leading to the best performance is selected from the cluster .
Second is Delegation Method Implied by LTR Method ∑𝑁 ( 𝐼𝐿𝑇 𝑅 ) . Most of the LTR algorithms generate the final 𝑖=1 𝜔𝑖 ∗ 𝑓𝑖 , where 𝜔𝑖 is ranking functions as linear mode : the weight of the feature 𝑓𝑖 . The feature leading to the highest weight is selected from the cluster .
3 . EXPERIMENT
3.1 Experiment Settings
The experiment dataset is LETOR which is broadly used in LTR research . The LETOR4.0 is released in July 2009 with 25,205,179 web pages and two query sets from Million Query Track of TREC2007(1692 queries ) and TREC2008(784 queries ) marked as MQ2007(MQ7 ) and MQ2008(MQ8 ) respectively in the following . 5 fold cross validation has been made in the experiments : three for training , one for validation and one for test . And experimental results are analyzed in terms of NDCG@n .
3.2 Experimental Results and Analysis
The quality measure of each clustering result shows in
Fig 1 , then the number of clusters is decided as 26 .
The ranking function name is shown as
𝐷𝑎𝑡𝑎𝑠𝑒𝑡 𝐿𝑇 𝑅 𝐴𝑙𝑔𝑜𝑟𝑖𝑡ℎ𝑚 𝐹 𝑒𝑎𝑡𝑢𝑟𝑒 𝑆𝑒𝑙𝑒𝑐𝑡𝑖𝑜𝑛 𝐴𝑙𝑔𝑜𝑟𝑖𝑡ℎ𝑚 . 𝐷𝑎𝑡𝑎𝑠𝑒𝑡 is MQ7 or MQ8 . 𝐿𝑇 𝑅 𝐴𝑙𝑔𝑜𝑟𝑖𝑡ℎ𝑚 is RankSVM(RS ) or ListNet(LN ) . 𝐹 𝑒𝑎𝑡𝑢𝑟𝑒 𝑆𝑒𝑙𝑒𝑐𝑡𝑖𝑜𝑛 𝐴𝑙𝑔𝑜𝑟𝑖𝑡ℎ𝑚 is 𝑀 𝐴𝑃 denoting 𝐵𝐸𝑀 with MAP , 𝑟𝑠 denoting 𝐼𝐿𝑇 𝑅 with RankSVM , 𝑙𝑛 denoting 𝐼𝐿𝑇 𝑅 with ListNet , or 𝐴𝑙𝑙 denoting method based on all features(the baseline of our work that do not use feature selection ) .
Comparative results of feature selection on MQ7 are shown in Fig 2(a ) . Features selected in MQ7 are applied directly in MQ8 , whose performance is shown in Fig 2(b ) . The paired T Tests are conducted on the improvements of NDCG@n(pvalue<0.05 means significant improvements ; p value<0.01 means very significant improvements ) .
The Fig 2 shows that :
( 1 ) the feature selection using 𝐵𝐸𝑀 with MAP consistently achieves significant performance for ranking : 𝑀 𝑄7 𝑅𝑆 𝑀 𝐴𝑃 , 𝑀 𝑄7 𝐿𝑁 𝑀 𝐴𝑃 , 𝑀 𝑄8 𝑅𝑆 𝑀 𝐴𝑃 and 𝑀 𝑄8 𝐿𝑁 𝑀 𝐴𝑃 outperform the baselines with p value= 0.0249 , 0.0008 , 0.0002 and 0.0004 inde
0.45
0.44
0.43
0.42
0.41
0.4
0.39
0.49
0.47
0.45
0.43
0.41
0.39
0.37
0.35
( a )
MQ7_RS_All MQ7_RS_rs MQ7_LN_All MQ7_LN_rs
MQ7_RS_MAP MQ7_RS_ln MQ7_LN_MAP MQ7_LN_ln
1
2
3
4
5
6
7
8
9
10
( b )
0.5
0.4
0.3
0.2
1 3 5 7 9
MQ8_RS_All
MQ8_RS_MAP
MQ8_RS_rs
MQ8_RS_ln
MQ8_LN_All
MQ8_LN_MAP
MQ8_LN_rs
MQ8_LN_ln
1
2
3
5 4 NDCG@
6
7
8
Figure 2 : Comparison Results with NDCG@1∼10 on MQ7 and MQ8 pendently . ( 2 ) in MQ7 , the best performance is obtained by 𝑀 𝑄7 𝑅𝑆 𝑟𝑠 with p value= 0.0006 vs the baseline 𝑀 𝑄7 𝑅𝑆 𝐴𝑙𝑙 . In MQ8 , all ranking functions outperform the baseline ones , and 𝑀 𝑄8 𝐿𝑁 𝑀 𝐴𝑃 obtains the best performance with p value= 0.0004 vs the baseline 𝑀 𝑄8 𝐿𝑁 𝐴𝑙𝑙 . ( 3 ) the feature selection using 𝐼𝐿𝑇 𝑅 with RankSVM does improve the performance for ranking , while the one using 𝐼𝐿𝑇 𝑅 with ListNet gains poor performance in MQ7 .
4 . CONCLUSIONS
In this paper , we propose a hierarchical feature selection strategy containing 2 phases and design a quality measure with which the number of clusters can be decided . The experimental results show that our methods could achieve significant improvement for ranking .
5 . ACKNOWLEDGMENTS
This work is supported by Natural Science Foundation ( 60736044 , 60903107 ) and Research Fund for the Doctoral Program of Higher Education of China ( 20090002120005 ) .
6 . REFERENCES [ 1 ] Z . Cao and etal Learning to rank : from pairwise approach to listwise approach . In ICML 2007 , pages 129–136 .
[ 2 ] X . Geng and etal Feature selection for ranking . In SIGIR
2007 , pages 407–414 .
[ 3 ] R . Herbrich and etal Large margin rank boundaries for ordinal regression . In Advances in Large Margin Classifiers , pages 115–132 , 2000 .
[ 4 ] T . Joachims . Optimizing search engines using clickthrough data . In KDD 2002 , pages 133–142 .
[ 5 ] T Y Liu . Learning to rank for information retrieval . In
Foundation and Trends on Information Retrieval , pages 641–647 , 2009 .
[ 6 ] F . Pan and etal Feature selection for ranking using boosted trees . In CIKM 2009 , pages 2025–2028 .
[ 7 ] M . Zhang and etal Is learning to rank effective for web search . In SIGIR 2009 workshop : Learning to Rank for Information Retrieval , pages 641–647 .
WWW 2010 • PosterApril 26 30 • Raleigh • NC • USA1114
