Hierarchical Feature Selection for Ranking
Guichun Hua , Min Zhang , Yiqun Liu , Shaoping Ma , Liyun Ru
State Key Laboratory of Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Science and Technology , Tsinghua University , Beijing 100084 , China huaguichun@gmail.com , {z m , msp , yiqunliu}@tsinghuaeducn , lyru@vipsohucom
ABSTRACT Ranking is an essential part of information retrieval(IR ) tasks such as Web search . Nowadays there are hundreds of features for ranking . So learning to rank(LTR ) , an interdisciplinary field of IR and machine learning(ML ) , has attracted increasing attention . Those features used in the IR are not always independent from each other , hence the feature selection , an important issue in ML , should be paid attention to for LTR . However , the state of the art LTR approaches merely analyze the connection among the features from the aspects of feature selection . In this paper , we propose a hierarchical feature selection strategy containing 2 phases for ranking and learn ranking functions . The experimental results show that ranking functions based on the selected feature subset significantly outperform the ones based on all features .
Categories and Subject Descriptors : H33 [ Information Search and Retrieval ] : Retrieval models
General Terms : Algorithms , Experimentation
Keywords : Learning to Rank , Feature Selection
1 .
INTRODUCTION
Web search engines are often referred to when people are requiring some information from Internet , and ranking is an essential part in the structure of search engines . Nowadays , hundreds of features for ranking have been proposed eg content based features such as ğ‘‡ ğ¹ ğ¼ğ·ğ¹ , ğµğ‘€ 25 ; link based features such as ğ‘ƒ ğ‘ğ‘”ğ‘’ğ‘…ğ‘ğ‘›ğ‘˜ , ğ»ğ¼ğ‘‡ ğ‘† ; user behavior features based on click through data . It is a hot research field to construct more efficient ranking functions based on these features , so LTR , an interdisciplinary field of IR and ML , has gained increasing attention for a few recent years .
The conventional ML research shows that the features and the composition of the features affect the performance of learning methods , and the construction methods of ranking functions for IR show that the features are not independent from each other . For example , the features of ğ‘‡ ğ¹ ( Term Frequency ) and ğ¼ğ·ğ¹ ( Inverse Document Frequency ) are elements to construct the feature ğµğ‘€ 25 . However , the stateof the art LTR approaches merely analyze the connection among the features from the aspects of feature selection except [ 6 , 2 ] to the best of our knowledge . [ 6 ] applies the boosted regression trees to select the proper feature subset .
Copyright is held by the author/owner(s ) . WWW 2010 , April 26â€“30 , 2010 , Raleigh , North Carolina , USA . ACM 978 1 60558 799 8/10/04 . where ğ‘› is the number of clusters . The clustering method we choose is K Means . By this way , the number of clusters can be decided with the quality value .
ğ‘“ğ‘£ ,ğ‘“ğ‘¢âˆˆğ¹ğ‘–
ğ‘ ğ‘–ğ‘š(ğ‘“ğ‘£ , ğ‘“ğ‘¢ )
)
ğ‘ğ‘– = 1 ğ‘ğ‘– > 1
[ 2 ] considers the feature importance and similarity between two features , and proposes an efficient greedy feature selection method . However , they are both flat feature selection methods which may be biased , and they could not decide which number of features selected is proper .
The main contributions of this paper are that : ( 1 ) we propose a hierarchical feature selection strategy containing 2 phases to make the selected features not biased . ( 2 ) design a quality measure to decide the proper number of selected features . We use Ranking SVM(RankSVM ) [ 3 , 4 ] and ListNet [ 1 ] to verity the strategy because they are powerful and commonly used approaches in LTR [ 7 , 5 ] . The experimental results show that our feature selection methods do significantly improve the performance of the ranking functions .
2 . FEATURE SELECTION STRATEGY
The process of the hierarchical feature selection strategy contains 2 phases : ( 1 ) the similarity between any two features is measured , and the similar features are aggregated into groups ; ( 2 ) the representative feature in each group is selected through either delegation method . By this way , the selected features are not biased to a group of features which are more representative than the ones in other group . 2.1 Cluster based feature similarity analysis The Kendall â€™s ğœ is chosen as the feature similarity measure and the similarity between features ğ‘“ğ‘– and ğ‘“ğ‘— : ğ‘ ğ‘–ğ‘š(ğ‘“ğ‘– , ğ‘“ğ‘— ) is calculated as follows :
}
( ğ‘‘ğ‘  , ğ‘‘ğ‘¡ ) âˆˆ ğ·ğ‘âˆ£ğ‘‘ğ‘  â‰ºğ‘“ğ‘– ğ‘‘ğ‘¡ ğ‘ğ‘›ğ‘‘ ğ‘‘ğ‘  â‰ºğ‘“ğ‘— ğ‘‘ğ‘¡
{
}
( ğ‘‘ğ‘  , ğ‘‘ğ‘¡ ) âˆˆ ğ·ğ‘
#
{
ğœğ‘(ğ‘“ğ‘– , ğ‘“ğ‘— ) =
# where ğ‘‘ğ‘  â‰ºğ‘“ğ‘– ğ‘‘ğ‘¡ denotes ğ‘‘ğ‘¡ ranks higher than ğ‘‘ğ‘  based on ğ‘“ğ‘– for document pair ( ğ‘‘ğ‘  , ğ‘‘ğ‘¡ ) in the set ğ·ğ‘ wrt a query ğ‘ , and #{.} denotes the number of elements in the set {}
Features are clustered according to their similarities . We define a measure based on the intra cluster similarities to estimate the quality of clustering results . But the intracluster similarity would be maximum if the cluster have only one element . Therefore the ğ‘ƒ ğ‘’ğ‘›ğ‘ğ‘™ğ‘¡ğ‘¦ is defined to reduce such effect , and it is the average of all similarities as : where ğ‘ is the number of ğ‘âˆ—(ğ‘âˆ’1 ) features in feature set ğ¹ . The quality measure is as follows :
( âˆ‘
)
2
ğ‘“ğ‘£ ,ğ‘“ğ‘¢âˆˆğ¹ ğ‘ ğ‘–ğ‘š(ğ‘“ğ‘£ , ğ‘“ğ‘¢ ) ğ‘›âˆ‘ ( âˆ‘
{
ğ‘ƒ ğ‘’ğ‘›ğ‘ğ‘™ğ‘¡ğ‘¦ ğ‘ğ‘–âˆ—(ğ‘ğ‘–âˆ’1 )
2
ğ‘–=1
ğ‘„ğ‘¢ğ‘ğ‘™ğ‘–ğ‘¡ğ‘¦ğ‘› =
WWW 2010 â€¢ PosterApril 26 30 â€¢ Raleigh â€¢ NC â€¢ USA1113 y t i l a u Q
0.7
0.6
0.5
0.4
2
10
18
26
34
42
Number of Clusters
Figure 1 : Quality of all clustering results
2.2 Delegation methods
We proposed two delegation methods to choose features from each cluster .
First is the Delegation Method Based on Evaluation Measure ( ğµğ¸ğ‘€ ) . In each cluster , every feature is used solely for ranking on training . Both normal and inverse value of the feature are applied respectively . The ranking results are measured with most commonly used criteria in IR such as MAP and ğ‘ ğ·ğ¶ğº@ğ‘› . The feature ( or the inverse of the feature ) leading to the best performance is selected from the cluster .
Second is Delegation Method Implied by LTR Method âˆ‘ğ‘ ( ğ¼ğ¿ğ‘‡ ğ‘… ) . Most of the LTR algorithms generate the final ğ‘–=1 ğœ”ğ‘– âˆ— ğ‘“ğ‘– , where ğœ”ğ‘– is ranking functions as linear mode : the weight of the feature ğ‘“ğ‘– . The feature leading to the highest weight is selected from the cluster .
3 . EXPERIMENT
3.1 Experiment Settings
The experiment dataset is LETOR which is broadly used in LTR research . The LETOR4.0 is released in July 2009 with 25,205,179 web pages and two query sets from Million Query Track of TREC2007(1692 queries ) and TREC2008(784 queries ) marked as MQ2007(MQ7 ) and MQ2008(MQ8 ) respectively in the following . 5 fold cross validation has been made in the experiments : three for training , one for validation and one for test . And experimental results are analyzed in terms of NDCG@n .
3.2 Experimental Results and Analysis
The quality measure of each clustering result shows in
Fig 1 , then the number of clusters is decided as 26 .
The ranking function name is shown as
ğ·ğ‘ğ‘¡ğ‘ğ‘ ğ‘’ğ‘¡ ğ¿ğ‘‡ ğ‘… ğ´ğ‘™ğ‘”ğ‘œğ‘Ÿğ‘–ğ‘¡â„ğ‘š ğ¹ ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ ğ‘†ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘–ğ‘œğ‘› ğ´ğ‘™ğ‘”ğ‘œğ‘Ÿğ‘–ğ‘¡â„ğ‘š . ğ·ğ‘ğ‘¡ğ‘ğ‘ ğ‘’ğ‘¡ is MQ7 or MQ8 . ğ¿ğ‘‡ ğ‘… ğ´ğ‘™ğ‘”ğ‘œğ‘Ÿğ‘–ğ‘¡â„ğ‘š is RankSVM(RS ) or ListNet(LN ) . ğ¹ ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ ğ‘†ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘–ğ‘œğ‘› ğ´ğ‘™ğ‘”ğ‘œğ‘Ÿğ‘–ğ‘¡â„ğ‘š is ğ‘€ ğ´ğ‘ƒ denoting ğµğ¸ğ‘€ with MAP , ğ‘Ÿğ‘  denoting ğ¼ğ¿ğ‘‡ ğ‘… with RankSVM , ğ‘™ğ‘› denoting ğ¼ğ¿ğ‘‡ ğ‘… with ListNet , or ğ´ğ‘™ğ‘™ denoting method based on all features(the baseline of our work that do not use feature selection ) .
Comparative results of feature selection on MQ7 are shown in Fig 2(a ) . Features selected in MQ7 are applied directly in MQ8 , whose performance is shown in Fig 2(b ) . The paired T Tests are conducted on the improvements of NDCG@n(pvalue<0.05 means significant improvements ; p value<0.01 means very significant improvements ) .
The Fig 2 shows that :
( 1 ) the feature selection using ğµğ¸ğ‘€ with MAP consistently achieves significant performance for ranking : ğ‘€ ğ‘„7 ğ‘…ğ‘† ğ‘€ ğ´ğ‘ƒ , ğ‘€ ğ‘„7 ğ¿ğ‘ ğ‘€ ğ´ğ‘ƒ , ğ‘€ ğ‘„8 ğ‘…ğ‘† ğ‘€ ğ´ğ‘ƒ and ğ‘€ ğ‘„8 ğ¿ğ‘ ğ‘€ ğ´ğ‘ƒ outperform the baselines with p value= 0.0249 , 0.0008 , 0.0002 and 0.0004 inde
0.45
0.44
0.43
0.42
0.41
0.4
0.39
0.49
0.47
0.45
0.43
0.41
0.39
0.37
0.35
( a )
MQ7_RS_All MQ7_RS_rs MQ7_LN_All MQ7_LN_rs
MQ7_RS_MAP MQ7_RS_ln MQ7_LN_MAP MQ7_LN_ln
1
2
3
4
5
6
7
8
9
10
( b )
0.5
0.4
0.3
0.2
1 3 5 7 9
MQ8_RS_All
MQ8_RS_MAP
MQ8_RS_rs
MQ8_RS_ln
MQ8_LN_All
MQ8_LN_MAP
MQ8_LN_rs
MQ8_LN_ln
1
2
3
5 4 NDCG@
6
7
8
Figure 2 : Comparison Results with NDCG@1âˆ¼10 on MQ7 and MQ8 pendently . ( 2 ) in MQ7 , the best performance is obtained by ğ‘€ ğ‘„7 ğ‘…ğ‘† ğ‘Ÿğ‘  with p value= 0.0006 vs the baseline ğ‘€ ğ‘„7 ğ‘…ğ‘† ğ´ğ‘™ğ‘™ . In MQ8 , all ranking functions outperform the baseline ones , and ğ‘€ ğ‘„8 ğ¿ğ‘ ğ‘€ ğ´ğ‘ƒ obtains the best performance with p value= 0.0004 vs the baseline ğ‘€ ğ‘„8 ğ¿ğ‘ ğ´ğ‘™ğ‘™ . ( 3 ) the feature selection using ğ¼ğ¿ğ‘‡ ğ‘… with RankSVM does improve the performance for ranking , while the one using ğ¼ğ¿ğ‘‡ ğ‘… with ListNet gains poor performance in MQ7 .
4 . CONCLUSIONS
In this paper , we propose a hierarchical feature selection strategy containing 2 phases and design a quality measure with which the number of clusters can be decided . The experimental results show that our methods could achieve significant improvement for ranking .
5 . ACKNOWLEDGMENTS
This work is supported by Natural Science Foundation ( 60736044 , 60903107 ) and Research Fund for the Doctoral Program of Higher Education of China ( 20090002120005 ) .
6 . REFERENCES [ 1 ] Z . Cao and etal Learning to rank : from pairwise approach to listwise approach . In ICML 2007 , pages 129â€“136 .
[ 2 ] X . Geng and etal Feature selection for ranking . In SIGIR
2007 , pages 407â€“414 .
[ 3 ] R . Herbrich and etal Large margin rank boundaries for ordinal regression . In Advances in Large Margin Classifiers , pages 115â€“132 , 2000 .
[ 4 ] T . Joachims . Optimizing search engines using clickthrough data . In KDD 2002 , pages 133â€“142 .
[ 5 ] T Y Liu . Learning to rank for information retrieval . In
Foundation and Trends on Information Retrieval , pages 641â€“647 , 2009 .
[ 6 ] F . Pan and etal Feature selection for ranking using boosted trees . In CIKM 2009 , pages 2025â€“2028 .
[ 7 ] M . Zhang and etal Is learning to rank effective for web search . In SIGIR 2009 workshop : Learning to Rank for Information Retrieval , pages 641â€“647 .
WWW 2010 â€¢ PosterApril 26 30 â€¢ Raleigh â€¢ NC â€¢ USA1114
