Trend Detection Model
NTT Communication Science Laboratories 2 4 Hikaridai , Seika cho , Soraku gun , Kyoto ,
NTT Communication Science Laboratories 2 4 Hikaridai , Seika cho , Soraku gun , Kyoto ,
Ryuichiro Higashinaka†
Noriaki Kawamae∗
Japan 619 0237 kawamae@gmail.com
Japan 619 0237 higashinakaryuichiro@labnttcojp
ABSTRACT This paper presents a topic model that detects topic distributions over time . Our proposed model , Trend Detection Model ( TDM ) introduces a latent trend class variable into each document . The trend class has a probability distribution over topics and a continuous distribution over time . Experiments using our data set show that TDM is useful as a generative model in the analysis of the evolution of trends .
Categories and Subject Descriptors G.3 [ PROBABILITY AND STATISTICS ] : Time series analysis
General Terms Algorithms , experimentation
Keywords Topic Model , Trend Model , Dynamic Topic Model , Latent Variable Modeling , Timestamps , Trend Analysis
1 .
INTRODUCTION
Modeling the evolution of trends over time is important in the analysis of user behavioral data and large document collections such as mails , news , and blogs .
This paper presents Trend Detection Model(TDM ) ; it models trends over continuous time . In this model , we suppose that each trend can be presented as a set of ( 1 ) the mixture of topics and ( 2 ) localization over time . Following this assumption , we introduce a latent variable , called trend class , that has both a probability distribution over both topics and a beta distribution over time , into each document .
A key advantage of TDM is that it can capture trends of different spans at the same time in the low dimensionality set of topics and timestamps . TDM captures the topic evolution over time by the trend class , while Dynamic Topic Models ( DTMs ) [ 1 ] captures the word evolution of each topic over time . Simultaneously , this class predicts absolute time values given an unstamped document , and predicts topic ∗ †
Currently with NTT Comware Corporation . Currently with NTT Cyber Space Laboratories , NTT Cor poration .
Copyright is held by the author/owner(s ) . WWW 2010 , April 26–30 , 2010 , Raleigh , North Carolina , USA . ACM 978 1 60558 799 8/10/04 .
Figure 1 : Graphical Models : TOT and TDM : In this figure , shaded and unshaded variables indicate observed and latent variables , respectively . An arrow indicates a conditional dependency between variables and stacked panes indicate a repeated sampling with the iteration number shown . distributions given the words in document as Topics Over Time ( TOT ) [ 2 ] can , since this class is associated with a continuous distribution over time . Consequently , this model incorporates two characteristics for modeling trends in documents and realizes the functionalities of both DTMs and TOT at the same time .
2 . TREND DETECTION MODEL
2.1 The trend class
In this subsection , we describe our model . Table 1 shows the notations used in this paper ; Figure 1 shows the generative process using graphical models of ( a ) TOT and ( b ) TDM . Before introducing our model , let us review the concept of Topics Over Time ( TOT ) model . TOT explicitly models absolute timestamp values by parameterizing the continuous distribution over time associated with each topic . In this model , topics are responsible for generating both observed timestamps as well as words .
Our proposed model , basic TDM models time jointly with topic co occurrence patterns rather than word co occurrence patterns . A novel feature of this model is the inclusion of trend class c ; it is responsible for generating both observed timestamps and topics in each document . The trend class allows TDM to represent trends by topic distribution associated with time θT DM rather than word distribution associated with time φT OT . Therefore , TDM assigns the same trend class to documents if they have almost identical timestamps as well as a similar set of topics , otherwise they must
WWW 2010 • PosterApril 26 30 • Raleigh • NC • USA1129 Table 1 : Notation used in this paper
SYMBOL DESCRIPTION C Z D V Nd td cd zdi wdi ψ number of trend classes number of topics number of documents number of unique words number of tokens in d timestamps associated with d trend associated with d topic associated with the ith token in d ith token in document d multinomial distribution of trend classes ( ψ|α ∼ Dirichlet(α ) ) beta distribution associated with z , c multinomial distribution of topics specific to c ( θc|β ∼ Dirichlet(β ) ) multinomial distribution of words specific to z(φz|γ ∼ Dirichlet(γ ) )
λz,c θc
φz be assigned to different trends .
2.2 Inference and Learning
The generative model for TDM can be described by the Bayesian hierarchical model . We employ Gibbs sampling to perform approximate inference in TDM . In the Gibbs sampling procedure , we need to calculate the conditional distributions . We use the chain rule and can then obtain the conditional distribution P ( cd = j|c\d , z , t , α , β , λ ) as
PC nj\d + αj c ( nc\d + αc )
P ( j| · ·· ) ∝ × ( 1 − td)λj1−1tλj2−1 d B(λj1 , λj2 )
PZ
Γ( QZ z njz\d + βz ) z Γ(njz\d + βz )
QZ z Γ(njz + βz ) PZ z njz + βz )
Γ( where nj\d represents the number of documents ( except for d ) that have been assigned to j , njz\d represents the number of tokens assigned to topic z in the documents ( except for d ) associated with j , and B is the beta function . to topic k is P ( zdi = k|j , z\di , w , β , γ ) and is written as
Likewise , the predictive distribution of adding word wdi
P ( k| · ·· ) ∝ nkwdi\di + γwdi v(nkv\di + γv )
P
P njk\di + βk z(njz\di + βz )
,
( 2 ) where nkv\di represents the number of tokens assigned to word v in topic k , except di .
3 . EXPERIMENTS
We present the quantitative evaluations of the proposed models , where we used a data set : 8 years ( 2001 2008 ) of research papers in the proceedings of ACM CIKM , SIGIR , KDD , and WWW . After removing stop words , numbers , and the words that appeared less than five times in the corpus from this data , we yielded a total set of 3078 documents and 20286 unique words from 2204 authors . In our evaluation , the smoothing parameters α,β , and γ were set to {1/Z(TOT ) , 1/C(TDM)} , {0.1(TOT ) , 1/Z(TDM)} , and 0.1 , respectively .
Table 2 : Perplexity comparison : All models are learned with the number of topics Z set at 200 . The number in the second row for TDM is the number of trend classes . These results are averaged over fivefold cross validation . Results that differ significantly by parametric non paired t test p < 0.01 from TOT are marked with ’*’ .
DTMs TOT
TDM
1587
1543
25 ∗ 1488
50 ∗ 1457
75 ∗ 1441
100 ∗ 1436
To measure the ability of the proposed model to act as generative model , we computed test set perplexity under the estimated parameters and compared the resulting values , and show the results in Table 2 . From this table , we observe that the trend class allows TDM to group documents under the various topic distributions rather than permitting various topic distributions on each document . This implies that clustered documents contain less noise than otherwise , and reduce the perplexity over all .
One interesting common feature of both TOT and TDM is the ability to predict the timestamp given the words in a document . This functionality also provides another opportunity to quantitatively compare TDM against TOT . On the corpus , we measure the ability to predict the published year given paper and show the results in Table 3 .
Table 3 : L1 error comparison : The number in the first row is the number of topics . The number in parentheses for TDM is the number of trend classes . Results that differ significantly by parametric nonpaired t test p < 0.01 , p < 0.05 from other methods are marked with ’**’ and ’*’ respectively .
( 1 )
Model TOT
TDM(50 ) TDM(100 )
50 2.44 ∗ 2.11 ∗∗ 2.03
100 2.25 ∗∗ 1.93 ∗∗ 1.85
150 2.11 ∗∗ 1.88 ∗∗ 1.71
200 1.97 ∗∗ 1.76 ∗∗ 1.65
Since TOT would generate different time stamps within the same document , this generation is overwhelmed by the plurality of words generated under the bag of words assumption . This defect dampens the predictive performance .
From these results , we can say that TDM attains lower perplexity than DTMs and TOT , and can predict what topic will rise more accurately .
4 . CONCLUSION
In this paper , we proposed a model that takes time jointly with topic co occurrence . Experiments using various data sets showed that TDM captures the trends of different spans at the same time . In future work , we will extend TDM by considering other metadata .
5 . REFERENCES [ 1 ] D . Blei and J . Lafferty . Dynamic topic models . In
ICML , pages 113–120 , 2006 .
[ 2 ] X . Wang and A . McCallum . Topics over time : a non markov continuous time model of topical trends . In KDD , pages 424–433 , 2006 .
WWW 2010 • PosterApril 26 30 • Raleigh • NC • USA1130
