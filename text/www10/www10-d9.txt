b Bit Minwise Hashing
Ping Li∗
Department of Statistical Science
Faculty of Computing and Information Science
Cornell University ,
Ithaca , NY 14853 pingli@cornell.edu
ABSTRACT This paper establishes the theoretical framework of b bit minwise hashing . The original minwise hashing method has become a standard technique for estimating set similarity ( eg , resemblance ) with applications in information retrieval , data management , computational advertising , etc .
By only storing b bits of each hashed value ( eg , b = 1 or 2 ) , we gain substantial advantages in terms of storage space . We prove the basic theoretical results and provide an unbiased estimator of the resemblance for any b . We demonstrate that , even in the least favorable scenario , using b = 1 may reduce the storage space at least by a factor of 21.3 ( or 10.7 ) compared to b = 64 ( or b = 32 ) , if one is interested in resemblance ≥ 05 Categories and Subject Descriptors H28 [ Database Applications ] : Data Mining General Terms Algorithms , Performance , Theory INTRODUCTION 1 .
Computing the size of set intersections is a fundamental problem in information retrieval , databases , and machine learning . Given two sets , S1 and S2 , where
S1 , S2 ⊆ Ω = {0 , 1 , 2 , , D − 1} , a basic task is to compute the joint size a = |S1 ∩ S2| , which measures the ( un normalized ) similarity between S1 and S2 . The resemblance , denoted by R , is a normalized similarity measure : , where f1 = |S1| , f2 = |S2| .
R = a
|S1 ∩ S2| |S1 ∪ S2| = f1 + f2 − a
In large datasets encountered in information retrieval and databases , efficiently computing the joint sizes is often highly challenging [ 3,18 ] . Detecting duplicate web pages is a classical example [ 4,6 ] . Typically , each Web document can be processed as “ a bag of shingles , ” where a shingle consists of w contiguous words in a document . Here w is a tuning parameter and was set to be w = 5 in several studies [ 4 , 6 , 12 ] . Clearly , the total number of possible shingles is huge . Considering merely 105 unique English words , the total number of possible 5 shingles should be D = ( 105)5 = O(1025 ) . Prior studies used D = 264 [ 12 ] and D = 240 [ 4 , 6 ] . 1.1 Minwise Hashing
In their seminal work , Broder and his colleagues developed minwise hashing and successfully applied the technique to duplicate ∗Supported by Microsoft , NSF DMS and ONR YIP . Copyright is held by the International World Wide Web Conference Committee ( IW3C2 ) . Distribution of these papers is limited to classroom use , and personal use by others . WWW 2010 , April 26–30 , 2010 , Raleigh , North Carolina , USA . ACM 978 1 60558 799 8/10/04 .
Arnd Christian König
Microsoft Research Microsoft Corporation Redmond , WA 98052 chrisko@microsoft.com
Web page removal [ 4 , 6 ] . Since then , there have been considerable theoretical and methodological developments [ 5 , 8 , 19 , 21–23 , 26 ] . As a general technique for estimating set similarity , minwise hashing has been applied to a wide range of applications , for example , content matching for online advertising [ 30 ] , detection of large scale redundancy in enterprise file systems [ 14 ] , syntactic similarity algorithms for enterprise information management [ 27 ] , compressing social networks [ 9 ] , advertising diversification [ 17 ] , community extraction and classification in the Web graph [ 11 ] , graph sampling [ 29 ] , wireless sensor networks [ 25 ] , Web spam [ 24,33 ] , Web graph compression [ 7 ] , and text reuse in the Web [ 2 ] . Here , we give a brief introduction to this algorithm . Suppose a random permutation π is performed on Ω , ie ,
π : Ω −→ Ω , where Ω = {0 , 1 , , D − 1} .
An elementary probability argument shows that
Pr ( min(π(S1 ) ) = min(π(S2) ) ) =
|S1 ∩ S2| |S1 ∪ S2| = R .
( 1 )
After k minwise independent permutations , π1 , π2 , , πk , one can estimate R without bias , as a binomial : k ( cid:180 ) j=1
=
1 k
ˆRM =
( cid:179 )
Var
ˆRM
1{min(πj ( S1 ) ) = min(πj ( S2))} ,
R(1 − R ) .
1 k
( 2 )
( 3 )
Throughout the paper , we frequently use the terms “ sample ” and “ sample size ” ( ie , k ) . In minwise hashing , a sample is a hashed value , min(πj(Si) ) , which may require eg , 64 bits to store [ 12 ] . 1.2 Our Main Contributions
In this paper , we establish a unified theoretical framework for b bit minwise hashing . Instead of using b = 64 bits [ 12 ] or 40 bits [ 4 , 6 ] , our theoretical results suggest using as few as b = 1 or b = 2 bits can yield significant improvements .
In b bit minwise hashing , a sample consists of b bits only , as opposed to eg , 64 bits in the original minwise hashing . Intuitively , using fewer bits per sample will increase the estimation variance , compared to ( 3 ) , at the same sample size k . Thus , we will have to increase k to maintain the same accuracy . Interestingly , our theoretical results will demonstrate that , when resemblance is not too small ( eg , R ≥ 0.5 , the threshold used in [ 4 , 6] ) , we do not have to increase k much . This means our proposed b bit minwise hashing can be used to improve estimation accuracy and significantly reduce storage requirements at the same time .
For example , when b = 1 and R = 0.5 , the estimation variance will increase at most by a factor of 3 . In this case , in order not to lose accuracy , we have to increase the sample size by a factor of
3 . If we originally stored each hashed value using 64 bits [ 12 ] , the improvement by using b = 1 will be 64/3 = 213
Algorithm 1 illustrates the procedure of b bit minwise hashing , based on the theoretical results in Sec 2 .
Algorithm 1 The b bit minwise hashing algorithm , applied to estimating pairwise resemblances in a collection of N sets . Input : Sets Sn ∈ Ω = {0 , 1 , , D − 1} , n = 1 to N . Pre processing : 1 ) : Generate k random permutations πj : Ω → Ω , j = 1 to k . 2 ) : For each set Sn and each permutation πj , store the lowest b bits of min ( πj ( Sn) ) , denoted by en,i,j , i = 1 to b . Estimation : ( Use two sets S1 and S2 as an example . ) 1 ) : Compute ˆEb = 1 k i=1 1{e1,i,πj = e2,i,πj} = 1 b k j=1
.
ˆEb−C1,b 1−C2,b
2 ) : Estimate the resemblance by ˆRb = are from Theorem 1 in Sec 2 . 1.3 Comparisons with LSH Algorithms
, where C1,b and C2,b
Locality Sensitive Hashing ( LSH ) [ 8,20 ] is a set of techniques for performing approximate search in high dimensions . In the context of estimating set intersections , there exist LSH families for estimating the resemblance , the arccosine and the Hamming distance [ 1 ] . In [ 8 , 16 ] , the authors describe LSH hashing schemes that map objects to {0 , 1} ( ie , 1 bit schemes ) . The algorithms for the construction , however , are problem specific . Two discovered 1 bit schemes are the sign random projections ( also known as simhash ) [ 8 ] and the Hamming distance LSH algorithm proposed by [ 20 ] . Our b bit minwise hashing proposes a new construction , which maps objects to {0 , 1 , , 2b − 1} instead of just {0 , 1} . While our major focus is to compare with the original minwise hashing , we also conduct comparisons with the other two known 1 bit schemes . 131 The method of sign ( 1 bit ) random projections estimates the arccosine , which is cos−1 , using our notation for sets S1 and S2 . A separate technical report is devoted to comparing b bit minwise hashing with sign ( 1 bit ) random projections . See wwwstatcornelledu/~li/hashing/RP_minwisepdf That report demonstrates that , unless the similarity level is very low , b bit minwise hashing outperforms sign random projections .
Sign Random Projections
( cid:179 )
( cid:180 ) a√ f1f2
The method of sign random projections has received significant attention in the context of duplicate detection . According to [ 28 ] , a great advantage of simhash over minwise hashing is the smaller size of the fingerprints required for duplicate detection . The spacereduction of b bit minwise hashing overcomes this issue . 132 The Hamming Distance LSH Algorithm Sec 4 will compare b bit minwise hashing with the Hamming distance LSH algorithm developed in [ 20 ] ( and surveyed in [ 1] ) :
• When the Hamming distance LSH algorithm is implemented naively , to achieve the same level of accuracy , its required storage space will be many magnitudes larger than that of b bit minwise hashing in sparse data ( ie , |Si|/D is small ) . • If we only store the non zero locations in the Hamming distance LSH algorithm , then its required storage space will be about one magnitude larger ( eg , 10 to 30 times ) . 2 . THE FUNDAMENTAL RESULTS
Consider two sets , S1 and S2 ,
S1 , S2 ⊆ Ω = {0 , 1 , 2 , , D − 1} , f1 = |S1| , f2 = |S2| , a = |S1 ∩ S2|
Apply a random permutation π on S1 and S2 : π : Ω −→ Ω . Define the minimum values under π to be z1 and z2 : z1 = min ( π ( S1 ) ) , z2 = min ( π ( S2 ) ) .
Define e1,i = ith lowest bit of z1 , and e2,i = ith lowest bit of z2 . Theorem 1 derives the main probability formula .
( cid:195 ) b i=1
THEOREM 1 . Assume D is large . 1{e1,i = e2,i} = 1
Eb = Pr where r1 = f1 D
,
C1,b = A1,b
C2,b = A1,b f2 D
, r2 = r2 r1 + r2 r1 r1 + r2
+ A2,b
+ A2,b r1 r1 + r2 r2 r1 + r2
,
,
= C1,b + ( 1 − C2,b ) R
( 4 )
( 5 )
( 6 ) r1 [ 1 − r1]2b−1 1 − [ 1 − r1]2b ,
A2,b =
A1,b = For a fixed rj ( where j ∈ {1 , 2} ) , Aj,b is a monotonically deFor a fixed b , Aj,b is a monotonically decreasing function of rj ∈ creasing function of b = 1 , 2 , 3 ,
( 7 ) r2 [ 1 − r2]2b−1 1 − [ 1 − r2]2b .
[ 0 , 1 ] , reaching a limit : lim rj→0
Aj,b =
1 2b .
( 8 )
Proof : See Appendix A.2
D and r2 = f2
Theorem 1 says that , for a given b , the desired probability ( 4 ) is determined by R and the ratios , r1 = f1 D . The only assumption needed in the proof of Theorem 1 is that D should be large , which is always satisfied in practice . Aj,b ( j ∈ {1 , 2} ) is a decreasing function of rj and Aj,b ≤ 1 2b . As b increases , Aj,b converges to zero very quickly . In fact , when b ≥ 32 , one can essentially view Aj,b = 0 . 2.1 An Intuitive ( Heuristic ) Explanation
A simple heuristic argument may provide a more intuitive expla nation of Theorem 1 . Consider b = 1 . One might expect that
Pr ( e1,1 = e2,1 ) =Pr ( e1,1 = e2,1|z1 = z2 ) Pr ( z1 = z2 ) +Pr ( e1,1 = e2,1|z1 = z2 ) Pr ( z1 = z2 ) ??≈R +
( 1 − R ) =
1 + R
,
1 2
2 because when z1 and z2 are not equal , the chance that their last bits 2 . This heuristic argument is are equal “ may be ” approximately 1 actually consistent with Theorem 1 when r1 , r2 → 0 . According to ( 8 ) , as r1 , r2 → 0 , we have A1,1 , A2,1 → 1 2 , and C1,1 , C2,1 → 1 also ; and hence the probability ( 4 ) approaches 1+R 2 .
In practice , when a very accurate estimate is not necessary , one might actually use this approximate formula to simplify the estimator . The errors , however , could be quite noticeable when r1 , r2 are not negligible ; see Sec 52 2.2 The Unbiased Estimator
2
Theorem 1 suggests an unbiased estimator ˆRb for R :
ˆRb =
ˆEb =
, k
ˆEb − C1,b 1 − C2,b 1 k b j=1 i=1
( 9 )
,
( 10 )
1{e1,i,πj = e2,i,πj} = 1 where e1,i,πj ( e2,i,πj ) denotes the ith lowest bit of z1 ( z2 ) , under the permutation πj . Following property of binomial distribution ,
( cid:179 )
( cid:180 )
( cid:179 )
( cid:180 )
Var
ˆRb
=
=
1 k
ˆEb
Var [ 1 − C2,b]2 =
Eb(1 − Eb ) [ 1 − C2,b]2
1 k
[ C1,b + ( 1 − C2,b)R ] [ 1 − C1,b − ( 1 − C2,b)R ]
[ 1 − C2,b]2
( 11 )
( cid:179 )
( cid:180 ) ( cid:179 ) converges to the variance of ˆRM , the esti
ˆRb
For large b , Var mator for the original minwise hashing : R(1 − R )
( cid:180 ) b→∞ Var lim
ˆRb
=
( cid:179 )
( cid:180 ) k
( cid:180 )
( cid:179 ) ( cid:180 )
( cid:179 )
= Var
ˆRM
.
In fact , when b ≥ 32 , Var indistinguishable for practical purposes . 2.3 The Variance Space Trade off and Var
ˆRM
ˆRb are numerically
As we decrease b , the space needed for storing each “ sample ” will be smaller ; the estimation variance ( 11 ) at the same sample size k , however , will increase . This variance space trade off can be precisely quantified by the storage factor B(b ; R , r1 , r2 ) :
( cid:179 )
( cid:180 )
B(b ; R , r1 , r2 ) = b × Var b [ C1,b + ( 1 − C2,b)R ] [ 1 − C1,b − ( 1 − C2,b)R ]
× k
ˆRb
=
[ 1 − C2,b]2
.
( 12 )
Lower B(b ) is better . The ratio , B(b1;R,r1,r2 ) B(b2;R,r1,r2 ) , measures the improvement of using b = b2 ( eg , b2 = 1 ) over using b = b1 ( eg , b1 = 64 ) . Some algebra yields the following Theorem .
THEOREM 2 . If r1 = r2 and b1 > b2 , then A1,b1 ( 1 − R ) + R A1,b2 ( 1 − R ) + R
B(b1 ; R , r1 , r2 ) B(b2 ; R , r1 , r2 ) b1 b2
=
1 − A1,b2 1 − A1,b1
,
( 13 ) is a monotonically increasing function of R ∈ [ 0 , 1 ] .
If R → 1 ( which implies r1 → r2 ) , then
B(b1 ; R , r1 , r2 ) B(b2 ; R , r1 , r2 )
→ b1 b2
1 − A1,b2 1 − A1,b1
.
( 14 )
If r1 = r2 , b2 = 1 , b1 ≥ 32 ( hence we treat A1,b = 0 ) , then
B(b1 ; R , r1 , r2 ) B(1 ; R , r1 , r2 )
= b1
R
R + 1 − r1
( 15 )
Proof : We omit the proof due to its simplicity.2
Suppose the original minwise hashing used 64 bits to store each sample , then the maximum improvement of b bit minwise hashing would be 64 fold , attained when r1 = r2 = 1 and R = 1 , according to ( 15 ) . In the least favorable situation , ie , r1 , r2 → 0 , the improvement will still be 64
3 = 21.3 fold when R = 05
Fig 1 plots B(64 )
B(b ) , to directly visualize the relative improvements , which are consistent with what Theorem 2 predicts . The plots show that , when R is very large ( which is the case in many practical applications ) , it is always good to use b = 1 . However , when R is small , using larger b may be better . The cut off point depends on r1 , r2 , R . For example , when r1 = r2 and both are small , it would be better to use b = 2 than b = 1 if R < 0.4 , as shown in Fig 1 .
Figure 1 : B(64 ) B(b ) , the relative storage improvement of using b = 1 , 2 , 3 , 4 bits , compared to using 64 bits . B(b ) is defined in ( 12 ) .
3 . EXPERIMENTS
Experiment 1 is a sanity check , to verify : ( A ) our proposed estimator ˆRb in ( 9 ) , is indeed unbiased ; and ( B ) its variance follows the prediction by our formula in ( 11 ) .
Experiment 2 is a duplicate detection task using a Microsoft proprietary collection of 1,000,000 news articles .
Experiment 3 is another duplicate detection task using 300,000
UCI NYTimes news articles . 3.1 Experiment 1
The data , extracted from Microsoft Web crawls , consists of 10 pairs of sets ( ie , total 20 words ) . Each set consists of the document IDs which contain the word at least once . Thus , this experiment is for estimating word associations .
Table 1 : Ten pairs of words used in Experiment 1 . For example , “ KONG ” and “ HONG ” correspond to the two sets of document IDs which contained word “ KONG ” and word “ HONG ” respectively .
Word 1 KONG RIGHTS OF GAMBIA UNITED SAN CREDIT TIME LOW A
Word 2 HONG RESERVED AND KIRIBATI STATES FRANCISCO CARD JOB PAY TEST r1 0.0145 0.187 0.570 0.0031 0.062 0.049 0.046 0.189 0.045 0.596 r2 0.0143 0.172 0.554 0.0028 0.061 0.025 0.041 0.05 0.043 0.035
R 0.925 0.877 0.771 0.712 0.591 0.476 0.285 0.128 0.112 0.052
B(32 ) B(1 ) 15.5 16.6 20.4 13.3 12.4 10.7 7.3 4.3 3.4 3.1
B(64 ) B(1 ) 31.0 32.2 40.8 26.6 24.8 21.4 14.6 8.6 6.8 6.2
B(1 ) and B(64 )
Table 1 summarizes the data and also provides the theoretical improvements , B(32 ) B(1 ) . The words were selected to include highly frequent word pairs ( eg , “ OF AND ” ) , highly rare word pairs ( eg , “ GAMBIA KIRIBATI ” ) , highly unbalanced pairs ( eg , ” A Test ” ) , highly similar pairs ( e.g , “ KONG HONG ” ) , as well as word pairs that are not quite similar ( eg , “ LOW PAY ” ) .
We estimate the resemblance using the original minwise hashing estimator ˆRM and the b bit estimator ˆRb ( b = 1 , 2 , 3 ) . 311 Validating the Unbiasedness Figure 2 presents the estimation biases for the selected 2 word pairs . Theoretically , both estimators , ˆRM and ˆRb , are unbiased ( ie , the y axis in Figure 2 should be zero , after an infinite number of repetitions ) . Figure 2 verifies this fact because the empirical biases are all very small and no systematic biases can be observed .
002040608105101520253035Resemblance ( R)Improvementr1 = r2 = 10−10b = 1b = 2b = 3b = 4002040608105101520253035Resemblance ( R)Improvementr1 = r2 = 0.1b = 1b = 2b = 3b = 400204060810102030405060Resemblance ( R)Improvementr1 = r2 = 0.5b = 1b = 2b = 3b = 400204060810102030405060Resemblance ( R)Improvementr1 = r2 = 0.9b = 1b = 2b = 3b = 4 the original minwise hashing ( using 32 bits ) . Figure 4 presents the precision & recall curves . The recall values ( bottom two panels in Figure 4 ) are all very high and do not differentiate the estimators .
Figure 4 : Microsoft collection of news data . The task is to retrieve news article pairs with resemblance R ≥ R0 . The recall curves ( bottom two panels ) indicate all estimators are equally good ( in recalls ) . The precision curves are more interesting for differentiating estimators . For example , when R0 = 0.4 ( top right panel ) , in order to achieve a precision = 0.80 , the estimators ˆRM , ˆR4 , ˆR2 , and ˆR1 require k = 50 , 50 , 75 , 145 samples , respectively , indicating ˆR4 , ˆR2 , and ˆR1 respectively improve ˆRM by 8 fold , 10.7 fold , and 11 fold .
The precision curves for ˆR4 ( using 4 bits per sample ) and ˆRM ( using 32 bits per sample ) are almost indistinguishable , suggesting a 8 fold improvement in space using b = 4 .
When using b = 1 or 2 , the space improvements are normally around 10 fold to 20 fold , compared to ˆRM , especially for achieving high precisions ( eg , ≥ 09 ) This experiment again confirms the significant improvement of the b bit minwise hashing using b = 1 ( or 2 ) . Table 2 summarizes the relative improvements .
In this experiment , ˆRM only used 32 bits per sample . For even larger applications , however , 64 bits per sample may be necessary [ 12 ] ; and the improvements of ˆRb will be even more significant .
Note that in the context of ( Web ) document duplicate detection , in addition to shingling , a number of specialized hash signatures have been proposed , which leverage properties of natural language
Figure 2 : Empirical biases from 25000 simulations at each sample size k . “ M ” denotes the original minwise hashing .
312 Validating the Variance Formula Figure 3 plots the empirical mean square errors ( MSE = variance + bias2 ) in solid lines , and the theoretical variances ( 11 ) in dashed lines , for 6 word pairs ( instead of 10 pairs , due to the space limit ) . All dashed lines are invisible because they overlap with the corresponding solid curves . Thus , this experiment validates that the variance formula ( 11 ) is accurate and ˆRb is indeed unbiased ( otherwise , MSE will differ from the variance ) .
Figure 3 : Mean square errors ( MSEs ) . “ M ” denotes the original minwise hashing . “ Theor . ” denotes the theoretical variances of Var( ˆRb)(11 ) and Var( ˆRM )(3 ) . The dashed curves , however , are invisible because the empirical MSEs overlapped the theoretical variances . At the same k , Var( ˆR1 ) > Var( ˆR2 ) > Var( ˆR3 ) > Var( ˆRM ) . However , ˆR1 ( ˆR2 ) only requires 1 bit ( 2 bits ) per sample , while ˆRM requires 32 or 64 bits . 3.2 Experiment 2 : Microsoft News Data
To illustrate the improvements by the use of b bit minwise hashing on a real life application , we conducted a duplicate detection experiment using a corpus of 106 news documents . The dataset was crawled as part of the BLEWS project at Microsoft [ 15 ] . We computed pairwise resemblances for all documents and retrieved documents pairs with resemblance R larger than a threshold R0 .
We estimate the resemblances using ˆRb with b = 1 , 2 , 4 bits , and
101102103−8−6−4−2024x 10−4Sample size kBias KONG − HONGMb=1b=1b=1Mb=2b=3b = 1b = 2b = 3M101102103−10−505x 10−4Sample size kBiasA − TEST b=3b=2b=2MMb=1b = 1b = 2b = 3M10110210310−410−310−2Sample size kMean square error ( MSE)KONG − HONG b=1M23b = 1b = 2b = 3MTheor.10110210310−410−310−2Sample size kMean square error ( MSE)RIGHTS − RESERVED b=12Mb = 1b = 2b = 3MTheor.10110210310−410−310−2Sample size kMean square error ( MSE)OF − AND b=1M2b = 1b = 2b = 3MTheor.10110210310−410−310−210−1Sample size kMean square error ( MSE)GAMBIA − KIRIBATI b=1M23b = 1b = 2b = 3MTheor.10110210310−410−310−210−1Sample size kMean square error ( MSE)LOW − PAY b=1Mb=23b = 1b = 2b = 3MTheor.10110210310−410−310−210−1Sample size kMean square error ( MSE)A − TEST b=1Mb=2b=3b = 1b = 2b = 3MTheor010020030040050000102030405060708091R0 = 0.3Sample size ( k)Precision b=1b=2b=1b=2b=4M010020030040050000102030405060708091R0 = 0.4Sample size ( k)Precision 2b=1b=1b=2b=4M010020030040050000102030405060708091R0 = 0.5Sample size ( k)Precision b=12b=1b=2b=4M010020030040050000102030405060708091R0 = 0.6Sample size ( k)Precision 2b=1b=1b=2b=4M010020030040050000102030405060708091R0 = 0.7Sample size ( k)Precision b=12b=1b=2b=4M010020030040050000102030405060708091R0 = 0.8Sample size ( k)Precision 2b=1b=1b=2b=4M010020030040050000102030405060708091R0 = 0.3Sample size ( k)Recall Recallb=1b=2b=4M010020030040050000102030405060708091Sample size ( k)Recall RecallR0 = 0.8b=1b=2b=4M text ( such as the placement of stopwords [ 31] ) . However , our approach is not aimed at any specific type of data , but is a general , domain independent technique . Also , to the extent that other approaches rely on minwise hashing for signature computation , these may be combined with our techniques .
Table 2 : Relative improvement ( in space ) of ˆRb ( using b bits per sample ) over ˆRM ( 32 bits per sample ) . For precision = 0.9 , 0.95 , we find the required sample sizes ( from Figure 4 ) for ˆRM and ˆRb and use them to estimate the required storage in bits . The values in the table are the ratios of the storage costs . The improvements are consistent with the theoretical predictions in Figure 1 .
R0
Precision = 0.9 4 b = 1 2 8.8 0.3 — 5.7 8.3 10.0 0.4 0.5 12.7 8.4 8.6 11.7 0.6 9.6 14.8 0.7 8.0 10.3 0.8 0.9 14.0 10.7
9.2 10.8 12.9 16.0 17.4 16.6
Precision = 0.95 b = 1 2 4 — — 7.1 8.2 — 10.0 8.2 10.1 7.7 8.5 12.4 10.5 7.6 12.7 15.4 7.7 14.2 18.7 23.0 17.6 9.7
3.3 Experiment 3 : UCI NYTimes Data
We conducted another duplicate detection experiment on a public ( UCI ) collection of 300,000 NYTimes articles . The purpose is to ensure that our experiment will be repeatable by those who can not access the proprietary data in Experiment 2 .
Figure 5 presents the precision curves for representative threshold R0 ’s . The recall curves are not shown because they could not differentiate estimators , just like in Experiment 1 . The curves confirm again that using b = 1 or b = 2 bits , ˆRb could improve the original minwise hashing ( using 32 bits per sample ) by a factor of 10 or more . The curves for ˆRb with b = 4 almost always overlap with the curves for ˆRM , verifying an expected 8 fold improvement .
Figure 5 : UCI collection of NYTimes data . The task is to retrieve news article pairs with resemblance R ≥ R0 . 4 . COMPARISONS WITH THE HAMMING
DISTANCE LSH ALGORITHM
The Hamming distance LSH algorithm proposed in [ 20 ] is an influential 1 bit LSH scheme . In this algorithm , a set Si , is mapped into a D dimensional binary vector , yi : yit = 1 , if t ∈ Si ; yit = 0 , otherwise . k j=1
ˆH =
D k k coordinates are randomly sampled from Ω = {0 , 1 , , D − 1} . We denote the samples of yi by hi , where hi = {hij , j = 1 to k} is a k dimensional vector . These samples will be used to estimate the Hamming distance H ( using S1 , S2 as an example ) :
H =
[ y1i = y2i ] = |S1 ∪ S2| − |S1 ∩ S2| = f1 + f2 − 2a .
D−1 i=0
Using the samples h1 and h2 , an unbiased estimator of H is simply
[ h1j = h2j ] ,
( 16 )
D − H 2 D2
D2 k
H D whose variance would be
( cid:179 )
( cid:180 )
Var
ˆH
E
( cid:163 ) ( cid:161 ) [ h1j = h2j]2(cid:162 ) − E2 ( [h1j = h2j ] ) D−1 i=0 [ y1i = y2i]2 ( cid:184 ) ( cid:183 )
( cid:164 ) i=0 [ y1i = y2i ]
( cid:195)D−1
−
D
=
=
D2 k2 k D2 k
2
.
=
( cid:180 )
( cid:179 )
( 17 ) The above analysis assumes k ( cid:191 ) D ( which is satisfied in pracin ( 17 ) by D−k tice ) ; otherwise one should multiply the Var D−1 , the “ finite sample correction factor . ” It would be interesting to compare ˆH with b bit minwise hashing . In order to estimate H , we need to convert the resemblance estimator ˆRb ( 9 ) to ˆHb : ˆHb = f1 + f2 − 2
( f1 + f2 ) . ( 18 )
( f1 + f2 ) =
ˆRb
ˆH
1 + ˆRb
The variance of ˆHb can be computed from Var
( 11 ) using the
1 − ˆRb 1 + ˆRb
( cid:179 )
( cid:181 ) −2
1−x 1+x
( 1 + R)2
( cid:180 ) ( cid:182)2 ( cid:181 )
1 k2
ˆRb = −2 ( 1+x)2 ) :
( cid:181 )
( cid:182 )
1 k2
+ O
( cid:182 )
.
( 19 )
“ delta method ” in statistics ( note that
( cid:179 )
( cid:180 )
Var
ˆHb
( cid:179 ) ( cid:179 )
( cid:180 ) ( cid:180 ) 4(r1 + r2)2
( f1 + f2)2
=Var
=Var
ˆRb
ˆRb
( 1 + R)4 D2 + O
Recall ri = fi/D . To verify the variances in ( 17 ) and ( 19 ) , we conduct experiments using the same data as in Experiment 1 . This time , we estimate H instead of R , using both ˆH ( 16 ) and ˆHb ( 18 ) . Figure 6 reports the mean square errors , together with the theoretical variances ( 17 ) and ( 19 ) . We can see that the theoretical variance formulas are accurate . When the data is not dense , the estimator ˆHb ( 18 ) given by b bit minwise hashing is much more accurate than the estimator ˆH ( 16 ) . However , when the data is dense ( eg , “ OF AND ” ) , ˆH could still outperform ˆHb .
We now compare the actual storage needed by ˆHb and ˆH . We define the following two ratios to make fair comparisons :
( cid:179 ) ( cid:179 )
( cid:180 ) ( cid:180 )
Var
ˆH
Wb =
Var
ˆHb
× k × bk
Var
, Gb =
ˆH
Var
( cid:179 )
( cid:180 ) ( cid:179 )
( cid:180 )
× r1+r2 × bk ˆHb
2
64k
.
( 20 )
Wb and Gb are defined in the same spirit as the ratio of the storage factors introduced in Sec 23 Recall each sample of b bit minwise hashing requires b bits ( ie , bk bits per set ) . If we assume each sample in the Hamming distance LSH requires 1 bit , then Wb in ( 20 ) is a fair indicator and Wb > 1 means ˆHb outperforms ˆH .
However , as can be verified in Fig 6 and Fig 7 , when r1 and r2 are small ( which is usually the case in practice ) , Wb tends to be very
010020030040050000102030405060708091R0 = 0.5Sample size ( k)Precision b=12b=1b=2b=4M010020030040050000102030405060708091R0 = 0.6Sample size ( k)Precision 2b=1b=1b=2b=4M010020030040050000102030405060708091R0 = 0.7Sample size ( k)Precision b=12b=1b=2b=4M010020030040050000102030405060708091R0 = 0.8Sample size ( k)Precision 2b=1b=1b=2b=4M large , when r1 , r2 are small . However , when r1 is very large ( eg , 0.9 ) , it is possible that W1 < 1 , meaning that the Hamming distance LSH could still outperform b bit minwise in dense data .
By only storing the non zero locations , Figure 7 illustrates that b bit minwise hashing will outperform the Hamming distance LSH algorithm , usually by a factor of 10 ( for small R ) to 30 ( for large R and r1 ≈ r2 ) . 5 . DISCUSSIONS 5.1 Computational Overhead
The previous results establish the significant reduction in storage requirements possible using b bit minwise hashing . This section demonstrates that these also translate into significant improvements in computational overhead in the estimation phrase . The computational cost in the preprocessing phrase , however , will increase . 511 Preprocessing Phrase In the preprocessing phrase , we need to generate minwise hashing functions and apply them to all the sets for creating fingerprints . This phrase is actually fairly fast [ 4 ] and is usually done off line , incurring a one time cost . Also , sets can be individually processed , meaning that this step is easy to parallelize .
The computation required for b bit minwise hashes differs from the computation of traditional minwise hashes in two respects : ( A ) we require a larger number of ( smaller sized ) samples , in turn requiring more hashing and ( B ) the packing of b bit samples into 64bit ( or 32 bit ) words requires additional bit manipulation .
It turns out the overhead for ( B ) is small and the overall computation time scales nearly linearly with k ; see Fig 8 . As we have analyzed , b bit minwise hashing only requires increasing k by a small factor such as 3 . Therefore , we consider the overhead in the preprocessing stage not to be a major issue . Also , it is important to note that b bit minwise hashing provides the flexibility of trading storage with preprocessing time by using b > 1 .
Figure 6 : MSEs ( normalized by H 2 ) , for comparing ˆH ( 16 ) with ˆHb ( 18 ) . In each panel , three solid curves stand for ˆH ( labeled by “ H ” ) , ˆH1 ( by ” b=1 ” ) , and ˆH2 ( by ” b=2 ” ) , respectively . The dashed lines are the corresponding theoretical variances ( 17 ) and ( 19 ) , which are largely overlapped by the solid lines . When the sample size k is not large , the empirical MSEs of ˆHb deviate from the theoretical variances , due to the bias caused by the nonlinear transformation of ˆHb from ˆRb in ( 18 ) . large , indicating a highly significant improvement of b bit minwise hashing over the Hamming distance LSH algorithm in [ 20 ] .
We consider in practice one will most likely implement the algorithm by only storing non zero locations . In other words , for set Si , only ri × k locations need to be stored ( each is assumed to use 64 bits ) . Thus , the total bits on average will be r1+r2 64k ( per set ) . In fact , we have the following Theorem for Gb when r1 , r2 → 0 . THEOREM 3 . Consider r1 , r2 → 0 , and Gb as defined in ( 20 ) .
2
If R → 0 ,
If R → 1 , then Gb → 8 b then Gb → 64 b
2b − 1 2b − 1
2b Proof : We omit the proof due to its simplicity . 2
( cid:179 )
( cid:180 )
.
.
( 21 )
( 22 )
Figure 7 plots W1 and G1 , for r1 = r2 = 10−6 , 10−4 , 0.001 , 0.01 , 0,1 ( which are probably reasonable in practice ) , as well as r1 = r2 = 0.9 ( as a sanity check ) . Note that , not all combinations of r1 , r2 , R are possible . For example , when r1 = r2 = 1 , then R has to be 1 .
Figure 7 : W1 and G1 as defined in ( 20 ) . We consider r1 = 10−6 , 10−4 , 0.001 , 0.01 , 0.1 , 09 Note that not all combinations of ( r1 , r2 , R ) are possible . The plot for G1 also verifies the theoretical limits proved in Theorem 3 .
Figure 7 confirms our theoretical results . W1 will be extremely
Figure 8 : Running time in the preprocessing phrase on 100K news articles . 3 hashing functions were used : 2 universal hashing ( labeled by “ 2 U ” ) , 4 universal hashing ( labeled by “ 4 U ” ) , and full permutations ( labeled by “ FP ” ) . Experiments with 1 bit hashing are reported in 3 dashed lines , which are only slightly higher ( due to additional bit packing ) than their corresponding solid lines ( the original minwise hashing using 32 bit ) .
The experiment in Fig 8 was conducted on 100K articles from the BLEWS project [ 15 ] . We considered 3 hashing functions : first , 2 universal hash functions ( computed using the fast universal hashing scheme described [ 10] ) ; second , 4 universal hash functions ( computed using the CWtrick algorithm of [ 32] ) ; and finally full random permutations ( computed using the Fisher Yates shuffle [ 13] ) . 512 Estimation Phrase We have shown earlier that , when R ≥ 0.5 and b = 1 , we expect a storage reduction of at least a factor of 21.3 , compared to using
10110210310410−210−1100101102Sample size kMean square error ( MSE)KONG − HONG b=1b=2H10110210310410−310−210−1100Sample size kMean square error ( MSE)OF − AND b=1b=2H10110210310410−310−210−1100101Sample size kMean square error ( MSE ) UNITED−STATESb=2b=1H10110210310410−310−210−1100101Sample size kMean square error ( MSE)LOW − PAY b=1Hb=2002040608110−1100101102103104105106Resemblance ( R)W1W1 , r2 = r1r1 = 1e−61e−4000100101r1 = 0900204060810102030405060Resemblance ( R)G1r1 = 0.9G1 , r2 = r1r1 = 1e−6 to 0150100150200250300005115225335445x 104Sample size k ( # hashing)Time ( sec ) 2−U4−UFP32 bits1 bit 64 bits . computational overhead of the estimation .
In the following , we will analyze how this impacts the
Here , the key operation is the computation of the number of identical b bit samples . While standard hash signatures that are multiples of 16 bit can easily be compared using a single machine instruction , efficiently computing the overlap between b bit samples for small b is less straightforward . In the following , we will describe techniques for computing the number of identical b bit samples when these are stored in a compact manner , meaning that individual b bit samples e1,i,j and e2,i,j , i = 1 , . . . , b , j = 1 , . . . k are packed into arrays Al[1 , . . . , k·b w ] , l = 1 , 2 of w bit words . To compute the number of identical b bit samples , we iterate through the arrays ; for an each offset h , we first compute v = A1[h ] ⊕ A2[h ] , where ⊕ denotes the bitwise XOR . Subsequently , the h th bit of v will be set if and only if the h th bits in A1[h ] and A2[h ] are different . Hence , to compute the number of overlapping b bit samples encoded in A1[h ] and A2[h ] , we need to compute the number of b bit blocks ending at offsets divisible by b that only contain 0s . The case of b = 1 corresponds to the problem of counting the number of 0 bits in a word . We tested different methods suggested in [ 34 ] and found the fastest approach to be pre computing an array bits[1 , . . . , 216 ] , such that bits[t ] corresponds to the number of 0bits in the binary representation of t . Then we can compute the number of 0 bits in v ( in case of w = 32 ) as c = bits[v & 0xffffu ] + bits[(v ( cid:192 ) 16 ) & 0xffffu ] .
Interestingly , we can use the same method for the cases where b > 1 , as we only need to modify the values stored in bits , setting bits[i ] to the number of b bit blocks that only contain 0 bits in the binary representation of i .
We evaluated this approach using a loop computing the number of identical samples in two signatures covering a total of 1.8 billion 32 bit words ( using a 64 bit Intel 6600 Processor ) . Here , the 1bit hashing requires 1.67x the time that the 32 bit minwise hashing requires.The results were essentially identical for b = 2 .
Combined with the reduction in overall storage ( for a given accuracy level ) , this means a significant speed improvement in the estimation phase : suppose in the original minwise hashing , each sample is stored using 64 bits . If we use 1 bit minwise hashing and consider R > 0.5 , our previous analysis has shown that we could gain a storage reduction at least by a factor of 64/3 = 21.3 fold . The improvement in computational efficiency would be 213/167 = 12.8 fold , which is still significant . 5.2 Reducing Storage Overhead for r1 and r2 D and D . The storage cost could be a concern if r1 ( r2 ) must be
The unbiased estimator ˆRb ( 9 ) requires knowing r1 = f1 r2 = f1 represented with a high accuracy ( eg , 64 bits ) .
This section illustrates that we only need to quantize r1 and r2 into Q levels , where Q = 24 is probably good enough and Q = 28 is more than sufficient . In other words , for each set , we only need to increase the total storage by 4 bits or 8 bits , which are negligible . For simplicity , we carry out the analysis for b = 1 and r1 = r2 = r . In this case , A1,1 = A2,1 = C1,1 = C2,1 = 1−r 2−r , and the correct estimator , denoted by ˆR1,r would be ˆR1,r = ( 2 − r ) ˆE1 − ( 1 − r ) ,
( cid:179 )
( cid:179 )
( cid:180 )
( cid:180 )
Bias
ˆR1,r
Var
ˆR1,r
=
( cid:179 )
( cid:180 ) k
ˆR1,r
− R = 0 , = E ( 1 − r + R)(1 − R )
.
See the definition of ˆE1 in ( 10 ) . Now , suppose we only store an approximate value of r , denoted by ˜r . The corresponding ( approximate ) estimator is denoted by ˆR1,˜r : ˆR1,˜r = ( 2 − ˜r ) ˆE1 − ( 1 − ˜r ) ,
( cid:179 )
( cid:179 )
( cid:180 )
( cid:180 )
Bias
ˆR1,˜r
Var
ˆR1,˜r
=
( cid:179 )
( cid:180 ) k
ˆR1,˜r
− R = = E ( 1 − r + R)(1 − R )
( ˜r − r)(1 − R )
,
2 − r ( 2 − ˜r)2 ( 2 − r)2 .
Thus , the ( absolute ) bias is upper bounded by |˜r−r| ( in the worst case , ie , R = 0 and r = 1 ) . Using Q = 24 levels of quantization , the bias is bounded by 1/16 = 00625 In a reasonable situation , eg , R ≥ 0.5 , the bias will be much smaller than 00625 Of course , if we increase the quantization levels to Q = 28 , the bias ( < 1/256 = 0.0039 ) will be negligible , even in the worst case .
Similarly , by examining the difference of the variances ,
( cid:179 )
( cid:175)(cid:175)(cid:175)Var
( cid:180 )
( cid:179 )
( cid:180)(cid:175)(cid:175)(cid:175 )
− Var
ˆR1,˜r
ˆR1,r ( 1 − r + R)(1 − R )
=
|˜r − r|
( 4 − ˜r − r ) ( 2 − r)2 we can see that Q = 28 would be more than sufficient . 5.3 Combining Bits for Enhancing Performance k
,
Our theoretical and empirical results have confirmed that , when the resemblance R is reasonably high , each bit per sample may contain strong information for estimating the similarity . This naturally leads to the conjecture that , when R is close to 1 , one might further improve the performance by looking at a combination of multiple bits ( ie , “ b < 1 ” ) . One simple approach is to combine two bits from two permutations using XOR ( ⊕ ) operations .
Recall e1,1,π denotes the lowest bit of the hashed value under π .
Theorem 1 has proved that
E1 = Pr ( e1,1,π = e2,1,π ) = C1,1 + ( 1 − C2,1 ) R
Consider two permutations π1 and π2 . We store x1 = e1,1,π1 ⊕ e1,1,π2 , x2 = e2,1,π1 ⊕ e2,1,π2
Then x1 = x2 either when e1,1,π1 = e2,1,π1 and e1,1,π2 = e2,1,π2 , or , when e1,1,π1 = e2,1,π1 and e1,1,π2 = e2,1,π2 . Thus 1 + ( 1 − E1)2 ,
T = Pr ( x1 = x2 ) = E2
( 23 ) which is a quadratic equation with a solution
2T − 1 + 1 − 2C1,1
2 − 2C2,1
.
( 24 )
√
R =
We can estimate T without bias as a binomial . The resultant estimator for R will be biased , at small sample size k , due to the nonlinearity . We will recommend the following estimator max{2 ˆT − 1 , 0} + 1 − 2C1,1
ˆR1/2 =
2 − 2C2,1
( 25 ) The truncation max{ . , 0} will introduce further bias ; but it is necessary and is usually a good bias variance trade off . We use ˆR1/2 to indicate that two bits are combined into one . The asymptotic variance of ˆR1/2 can be derived using the “ delta method ”
.
( cid:181 )
( cid:182 )
T ( 1 − T )
4(1 − C2,1)2(2T − 1 )
=
1 k
+ O
1 k2
.
( 26 )
( cid:179 )
( cid:180 )
Var
ˆR1/2
Note that each sample is still stored using 1 bit , despite that we use “ b = 1/2 ” to denote this estimator .
Interestingly , as R → 1 , ˆR1/2 does twice as well as ˆR1 :
( cid:179 ) ( cid:179 )
( cid:180 ) ( cid:180 ) = lim
R→1
Var
ˆR1 lim R→1
Var
ˆR1/2
2(1 − 2E1)2 ( 1 − E1)2 + E2
1
= 2 .
( 27 )
( Recall , if R = 1 , then r1 = r2 , C1,1 = C2,1 , and E1 = C1,1 + 1 − C2,1 = 1 . ) On the other hand , ˆR1/2 may not be good when R is not too large . For example , one can numerically show that
( cid:179 )
( cid:180 )
( cid:179 )
( cid:180 )
Var
ˆR1
< Var
ˆR1/2 if R < 0.5774 , r1 , r2 → 0
,
Figure 9 plots the empirical MSEs for four word pairs in Experiment 1 , for ˆR1/2 , ˆR1 , and ˆRM . For the highly similar pair , “ KONG HONG , ” ˆR1/2 exhibits superior performance compared to ˆR1 . For the fairly similar pair , “ OF AND , ” ˆR1/2 is still considerably better . For “ UNITED STATES , ” whose R = 0.591 , ˆR1/2 performs similarly to ˆR1 . For “ LOW PAY , ” whose R = 0.112 only , the theoretical variance of ˆR1/2 is very large . However , owing to the truncation in ( 25 ) ( ie , the variance bias trade off ) , the empirical performance of ˆR1/2 is not too bad .
Figure 9 : MSEs for comparing ˆR1/2 ( 25 ) with ˆR1 and ˆRM . Due to the bias of ˆR1/2 , the theoretical variances Var , ie , ˆR1/2 ( 26 ) , deviate from the empirical MSEs when k is small .
( cid:179 )
( cid:180 )
In a summary , for applications which care about very high simi larities , combining bits can reduce storage even further .
6 . CONCLUSION
The minwise hashing technique has been widely used as a standard duplicate detection approach in the context of information retrieval , for efficiently computing set similarity in massive data sets . Prior studies commonly used 64 bits to store each hashed value .
This study proposes b bit minwise hashing , by only storing the lowest b bits of each hashed value . We theoretically prove that , when the similarity is reasonably high ( eg , resemblance ≥ 0.5 ) , using b = 1 bit per hashed value can , even in the worst case , gain a 21.3 fold improvement in storage space , compared to storing each hashed value using 64 bits . We also discussed the idea of combining 2 bits from different hashed values , to further enhance the improvement , when the target similarity is very high .
Our proposed method is simple and requires only minimal modification to the original minwise hashing algorithm . We expect our method will be adopted in practice .
7 . REFERENCES [ 1 ] Alexandr Andoni and Piotr Indyk . Near optimal hashing algorithms for approximate nearest neighbor in high dimensions . In Commun . ACM , volume 51 , pages 117–122 , 2009 .
[ 2 ] Michael Bendersky and W . Bruce Croft . Finding text reuse on the web . In WSDM , pages 262–271 , 2009 .
[ 3 ] Sergey Brin , James Davis , and Hector Garcia Molina . Copy detection mechanisms for digital documents . In SIGMOD , pages 398–409 , 1995 .
[ 4 ] Andrei Z . Broder . On the resemblance and containment of documents . In Sequences , pages 21–29 , 1997 .
[ 5 ] Andrei Z . Broder , Moses Charikar , Alan M . Frieze , and Michael Mitzenmacher . Min wise independent permutations . Journal of Computer Systems and Sciences , 60(3):630–659 , 2000 .
[ 6 ] Andrei Z . Broder , Steven C . Glassman , Mark S . Manasse , and
Geoffrey Zweig . Syntactic clustering of the web . In WWW , pages 1157 – 1166 , 1997 .
[ 7 ] Gregory Buehrer and Kumar Chellapilla . A scalable pattern mining approach to web graph compression with communities . In WSDM , pages 95–106 , 2008 .
[ 8 ] Moses S . Charikar . Similarity estimation techniques from rounding algorithms . In STOC , pages 380–388 , 2002 .
[ 9 ] Flavio Chierichetti , Ravi Kumar , Silvio Lattanzi , Michael
Mitzenmacher , Alessandro Panconesi , and Prabhakar Raghavan . On compressing social networks . In KDD , pages 219–228 , 2009 .
[ 10 ] Dietzfelbinger , Martin and Hagerup , Torben and Katajainen , Jyrki and Penttonen , Martti A reliable randomized algorithm for the closest pair problem . Journal of Algorithms , 25(1):19–51 , 1997 .
[ 11 ] Yon Dourisboure , Filippo Geraci , and Marco Pellegrini . Extraction and classification of dense implicit communities in the web graph . ACM Trans . Web , 3(2):1–36 , 2009 .
[ 12 ] D . Fetterly , M . Manasse , M . Najork , and J . Wiener . A large scale study of the evolution of web pages . In WWW , pages 669–678 , 2003 .
[ 13 ] RA Fisher and F . Yates . Statistical Tables for Biological , Agricultural and Medical Research . Oliver & Boyd , 1948 .
[ 14 ] George Forman , Kave Eshghi , and Jaap Suermondt . Efficient detection of large scale redundancy in enterprise file systems . SIGOPS Oper . Syst . Rev . , 43(1):84–91 , 2009 .
[ 15 ] Michael Gamon , Sumit Basu , Dmitriy Belenko , Danyel Fisher ,
Matthew Hurst , and Arnd Christian König . Blews : Using blogs to provide context for news articles . In AAAI , 2008 .
[ 16 ] Aristides Gionis and Dimitrios Gunopulos and Nick Koudas .
Efficient and Tunable Similar Set Retrieval . In SIGMOD , pages 247 258 , 2001 .
[ 17 ] Sreenivas Gollapudi and Aneesh Sharma . An axiomatic approach for result diversification . In WWW , pages 381–390 , 2009 .
[ 18 ] Monika R Henzinge . Algorithmic challenges in web search engines .
Internet Mathematics , 1(1):115–123 , 2004 .
[ 19 ] Piotr Indyk . A small approximately min wise independent family of hash functions . Journal of Algorithm , 38(1):84–90 , 2001 .
[ 20 ] Piotr Indyk and Rajeev Motwani . Approximate nearest neighbors :
Towards removing the curse of dimensionality . In STOC , 1998 .
[ 21 ] Toshiya Itoh , Yoshinori Takei , and Jun Tarui . On the sample size of k restricted min wise independent permutations and other k wise distributions . In STOC , pages 710–718 , 2003 .
[ 22 ] P . Li and K . Church . A Sketch Algorithm for Estimating Two way and Multi way Associations Computational Linguistics , pages 305–354 , 2007 . ( Preliminary results appeared in HLT/EMNLP 2005 . )
[ 23 ] P . Li , K . Church and T . Hastie . One Sketch For All : Theory and Applications of Conditional Random Sampling . In NIPS , 2008 .
[ 24 ] Nitin Jindal and Bing Liu . Opinion spam and analysis . In WSDM , pages 219–230 , 2008 .
[ 25 ] Konstantinos Kalpakis and Shilang Tang . Collaborative data gathering in wireless sensor networks using measurement co occurrence . Computer Commu . , 31(10):1979–1992 , 2008 . [ 26 ] Eyal Kaplan , Moni Naor , and Omer Reingold . Derandomized constructions of k wise ( almost ) independent permutations . Algorithmica , 55(1):113–133 , 2009 .
[ 27 ] Ludmila , Kave Eshghi , Charles B . Morrey III , Joseph Tucek , and Alistair Veitch . Probabilistic frequent itemset mining in uncertain databases . In KDD , pages 1087–1096 , 2009 .
10110210310−410−310−2Sample size kMean square error ( MSE ) KONG − HONGb=1M1/2b = 1b = 1/2MTheor.10110210310−410−310−2Sample size kMean square error ( MSE ) OF − ANDb = 1/2Mb = 11/2b = 1b = 1/2MTheor.10110210310−410−310−210−1Sample size kMean square error ( MSE)UNITED − STATES Mb = 1/2b=1b = 1b = 1/2MTheor.10110210310−410−2100Sample size kMean square error ( MSE ) LOW − PAYb = 1/2b = 1/2b=1Mb = 1b = 1/2MTheor . The expressions for P1 , P2 , and P3 can be understood by the experiment of randomly throwing f1+f2−a balls into D locations , labeled 0 , 1 , 2 , , D − 1 . Those f1 + f2 − a balls belong to three disjoint sets : S1 − S1 ∩ S2 , S2 − S1 ∩ S2 , and S1 ∩ S2 . Without any restriction , the total number of combinations should be P3 .
To understand P1 and P2 , we need to consider two cases : 1 . The jth element is not in S1 ∩ S2 : =⇒ P1 .
We first allocate the a = |S1 ∩ S2| overlapping elements randomly in [ j + 1 , D − 1 ] , resulting in combinations . Then we allocate the remaining f2−a−1 elements in S2 also randomly in the unoccupied locations in [ j + 1 , D − 1 ] , resulting in combinations . Finally , we allocate the remaining elements in S1 randomly in the unoccupied locations in [ i + 1 , D − 1 ] , which has combinations .
( cid:161)D−j−1−a
( cid:161)D−j−1 ( cid:162 ) ( cid:161)D−i−1−f2 f2−a−1
( cid:162 )
( cid:162 ) a f1−a−1
2 . The jth element is in S1 ∩ S2 : =⇒ P2 .
After conducing expansions and cancelations , we obtain
( cid:179 )
=
=
=
Pr ( z1 = i , z2 = j , i < j ) =
P1 + P2
P3
( cid:180 )
1 a + 1 f2−a
( a−1)!(f1−a−1)!(f2−a−1)!(D−j−f2)!(D−i−f1−f2+a)!
( D−j−1)!(D−i−1−f2)! a!(f1−a)!(f2−a)!(D−f1−f2+a)!
D! f2(f1 − a)(D − j − 1)!(D − f2 − i − 1)!(D − f1 − f2 + a)!
D!(D − f2 − j)!(D − f1 − f2 + a − i)! i−1 t=0(D − f1 − f2 + a − t ) f2(f1 − a ) j−i−2 j−i−2 t=0 t=0 j ( D − f2 − i − 1 − t ) i−1 t=0(D − t )
D − f2 − i − 1 − t
D − 2 − t t=0
D − f1 − f2 + a − t D + i − j − 1 − t
= f2 D f1 − a D − 1
For convenience , we introduce the following notation : r1 = f1 D
, r2 = f2 D
, s = a D
.
Also , we assume D is large ( which is always satisfied in practice ) . Thus , we can obtain a reasonable approximation :
Pr ( z1 = i , z2 = j , i < j ) =r2(r1 − s ) [ 1 − r2]j−i−1 [ 1 − ( r1 + r2 − s)]i
Similarly , we obtain , for large D ,
Pr ( z1 = i , z2 = j , i > j ) =r1(r2 − s ) [ 1 − r1]i−j−1 [ 1 − ( r1 + r2 − s)]j
Now we have the tool to calculate the probability
Pr ( e1,1 = e2,1 , z1 = z2 )
[ 28 ] Gurmeet Singh Manku , Arvind Jain , and Anish Das Sarma .
Detecting Near Duplicates for Web Crawling . In WWW , 2007 . [ 29 ] Marc Najork , Sreenivas Gollapudi , and Rina Panigrahy . Less is more : sampling the neighborhood graph makes salsa better and faster . In WSDM , pages 242–251 , 2009 .
[ 30 ] Sandeep Pandey , Andrei Broder , Flavio Chierichetti , Vanja
Josifovski , Ravi Kumar , and Sergei Vassilvitskii . Nearest neighbor caching for content match applications . In WWW , 441–450 , 2009 .
[ 31 ] Martin Theobald , Jonathan Siddharth , and Andreas Paepcke .
Spotsigs : robust and efficient near duplicate detection in large web collections . In SIGIR , pages 563–570 , 2008 .
[ 32 ] Mikkel Thorup and Yin Zhang . Tabulation based 4 universal hashing with applications to second moment estimation . In SODA , 2004 .
[ 33 ] Tanguy Urvoy , Emmanuel Chauveau , Pascal Filoche , and Thomas
Lavergne . Tracking web spam with html style similarities . ACM Trans . Web , 2(1):1–28 , 2008 .
[ 34 ] Henry S . Warren . Hacker ’s Delight . Addison Wesley , 2002 . APPENDIX A . PROOF OF THEOREM 1 Consider two sets , S1 , S2 ⊆ Ω = {0 , 1 , 2 , , D − 1} . Denote f1 = |S1| , f2 = |S2| , and a = |S1 ∩ S2| . Apply a random permutation π on S1 and S2 : π : Ω −→ Ω . Define the minimum values under π to be z1 and z2 : z1 = min ( π ( S1 ) ) , z2 = min ( π ( S2 ) ) .
Define e1,i = ith lowest bit of z1 , and e2,i = ith lowest bit of z2 . The task is to derive Pr i=1 1{e1,i = e2,i} = 1
, which can be decomposed to be
( cid:179)b
( cid:180 )
( cid:195 ) ( cid:195 ) b b i=1 i=1
Pr
+Pr
1{e1,i = e2,i} = 1 , z1 = z2
1{e1,i = e2,i} = 1 , z1 = z2
( cid:195 ) b
=Pr ( z1 = z2 ) + Pr
1{e1,i = e2,i} = 1 , z1 = z2
=R + Pr i=1
1{e1,i = e2,i} = 1 , z1 = z2
( cid:195 ) b
. where R = i=1 |S1∩S2| |S1∪S2| = Pr ( z1 = z2 ) is the resemblance .
When b = 1 , the task boils down to estimating
Pr ( e1,1 = e2,1 , z1 = z2 )
=
+
  i=0,2,4 , j=i,j=0,2,4 ,
Pr ( z1 = i , z2 = j )
  . i=1,3,5 , j=i,j=1,3,5 ,
Pr ( z1 = i , z2 = j )
Therefore , we need the following basic probability formula :
Pr ( z1 = i , z2 = j , i = j ) .
We start with
P1 + P2
P3
( cid:180)(cid:179)D − a ( cid:179)D ( cid:179)D − j − 1 ( cid:179)D − j − 1
( cid:180)(cid:179)D − f1 ( cid:180 ) ( cid:180)(cid:179)D − j − 1 − a ( cid:180)(cid:179)D − j − a f2 − a − 1 f2 − a f1 − a a a
, a − 1 f2 − a
( cid:180)(cid:179)D − i − 1 − f2 ( cid:180 ) ( cid:180)(cid:179)D − i − 1 − f2 f1 − a − 1
( cid:180 )
, f1 − a − 1
.
P3 =
P1 =
P2 =
Pr ( z1 = i , z2 = j , i < j ) =
, where i=1,3,5 , j=i,j=1,3,5 , i=0,2,4 , j=i,j=0,2,4 ,
Pr ( z1 = i , z2 = j )
Pr ( z1 = i , z2 = j )
=
+
  ( cid:161 )
For example , ( again , assuming D is large )
Pr ( z1 = 0 , z2 = 2 , 4 , 6 , ) =r2(r1 − s ) =r2(r1 − s )
1 − r2
1 − [ 1 − r2]2
[ 1 − r2 ] + [ 1 − r2]3 + [ 1 − r2]5 +
 
( cid:162 )
Pr ( z1 = i , z2 = j )
Pr ( z1 = i , z2 = j )
1 r1 + r2 − s
.
Pr ( z1 = i , z2 = j )
1 + [ 1 − ( r1 + r2 − s ) ] + [ 1 − ( r1 + r2 − s)]2 +
Pr ( z1 = 1 , z2 = 3 , 5 , 7 , ) = r2(r1 − s)[1 − ( r1 + r2 − s ) ] [ 1 − r2 ] + [ 1 − r2]3 + [ 1 − r2]5 +
=r2(r1 − s)[1 − ( r1 + r2 − s ) ]
1 − r2
1 − [ 1 − r2]2 .
( cid:162 )
×(cid:161 )
Therefore ,
+ i=0,2,4 , i<j,j=0,2,4 , i=1,3,5 ,
=r2(r1 − s ) i<j,j=1,3,5 ,
1 − r2
1 − [ 1 − r2]2 ×
( cid:161 )
=r2(r1 − s )
By symmetry , we know
1 − r2
1 − [ 1 − r2]2
+ j=0,2,4 , i>j,i=0,2,4 , j=1,3,5 ,
=r1(r2 − s ) i>j,i=1,3,5 ,
1 − r1
1 − [ 1 − r1]2
Pr ( z1 = i , z2 = j )
1 r1 + r2 − s
.
Combining the probabilities , we obtain
Pr ( e1,1 = e2,1 , z1 = z2 ) r2(1 − r2 ) 1 − [ 1 − r2]2 r2 − s r1 + r2 − s r1 − s
= r1(1 − r1 ) 1 − [ 1 − r1]2 + r1 − s r2 − s r1 + r2 − s
=A1,1 r1 + r2 − s
+ A2,1 r1 + r2 − s
, where
A1,b = r2 [ 1 − r2]2b−1 1 − [ 1 − r2]2b . Therefore , we can obtain the desired probability , for b = 1 , r1 [ 1 − r1]2b−1 1 − [ 1 − r1]2b , b=1
A2,b =
( cid:195 )
1{e1,i = e2,i} = 1
Pr i=1
=R + A1,1
=R + A1,1 r2 − s f2 − a r1 + r2 − s f1 + f2 − a f2 − R
+ A2,1
+ A2,1 r1 − s r1 + r2 − s f1 − a f1 + f2 − a
1+R ( f1 + f2 )
1+R ( f1 + f2 )
+ A2,1 f1 − a f1 + f2 − a
=R + A1,1 f1 + f2 − R f2 − Rf1 =R + A1,1 f1 + f2 =C1,1 + ( 1 − C2,1)R
+ A2,1 f1 − Rf2 f1 + f2 where
C1,b = A1,b
C2,b = A1,b r2 r1 + r2 r1 r1 + r2
+ A2,b
+ A2,b r1 r1 + r2 r2 r1 + r2
.
To this end , we have proved the main result for b = 1 .
Next , we consider b > 1 . Due to the space limit , we only provide a sketch of the proof . When b = 2 , we need
Pr ( e1,1 = e2,1 , e1,2 = e2,2 , z1 = z2 )
    i=0,4,8 , j=i,j=0,4,8 ,
Pr ( z1 = i , z2 = j ) i=1,5,9 , j=i,j=1,5,9 ,
Pr ( z1 = i , z2 = j ) i=2,6,10 , j=i,j=2,6,10 ,
Pr ( z1 = i , z2 = j ) i=3,7,11 , j=i,j=3,7,11 ,
Pr ( z1 = i , z2 = j )
   
=
+
+
+
( cid:162 )
We again use the basic probability formula Pr ( z1 = i , z2 = j , i < j ) and the sum of ( different ) geometric series , for example ,
[ 1 − r2]3 + [ 1 − r2]7 + [ 1 − r2]11 + =
[ 1 − r2]22−1 1 − [ 1 − r2]22 .
Similarly , for general b , we will need
[ 1 − r2]2b−1 + [ 1 − r2]2×2b−1 + [ 1 − r2]3×2b−1 + =
After more algebra , we prove the general case :
[ 1 − r2]2b−1 1 − [ 1 − r2]2b .
( cid:195 ) b i=1
Pr
1{e1,i = e2,i} = 1 r2 − s
+ A2,b r1 − s r1 + r2 − s r1 + r2 − s =R + A1,b =C1,b + ( 1 − C2,b)R ,
1 − [ 1 − r1]2b−1
( cid:180 )
( cid:180 )
=
1 2b ,
∂A1,b
∂b
=
− ≤0
It remains to show some useful properties of A1,b ( same for
A2,b ) . The first derivative of A1,b with respect to b is r1[1 − r1]2b−1 log(1 − r1 ) log 2
1 − [ 1 − r1]2b
−[1 − r1]2b
( cid:179 ) 1 − [ 1 − r1]2b(cid:162)2 ( cid:161 ) ( cid:179 ) ( cid:161 ) 1 − [ 1 − r1]2b(cid:162)2 ( cid:162 ) log(1 − r1 ) log 2 r1
( Note that log(1 − r1 ) ≤ 0 )
2b − 1 2b[1 − r1]2b−1 2b − 1
( cid:161 ) ( cid:161 ) ( cid:162 ) 1 − [ 1 − r1]2b(cid:162 ) ( cid:161 ) 1 − [ 1 − r1]2b(cid:162)2 ( cid:161 ) − 2b[1 − r1]2b−1r1 [ 1 − r1]2b−1 1 − [ 1 − r1]2b(cid:162)2 ( cid:161 ) [ 1 − r1]2b−2
( cid:179 )
=
Thus , A1,b is a monotonically decreasing function of b . Also , [ 1 − r1]2b−2
[ 1 − r1]2b−1 − r1 lim r1→0
A1,b = lim r1→0 [ 1 − r1]2b−1 − r1
∂A1,b ∂r1
=
[ 1 − r1]2b−2
1 − 2br1 − [ 1 − r1]2b(cid:180 )
≤ 0 .
Note that ( 1 − x)c ≥ 1 − cx , for c ≥ 1 and x ≤ 1 . Therefore A1,b is a monotonically decreasing function of r1 . b Bit Minwise Hashing for Estimating Three Way Similarities
Ping Li
Dept . of Statistical Science
Cornell University
Arnd Christian K¨onig
Microsoft Research Microsoft Corporation
Wenhao Gui
Dept . of Statistical Science
Cornell University
Abstract
Computing1 two way and multi way set similarities is a fundamental problem . This study focuses on estimating 3 way resemblance ( Jaccard similarity ) using b bit minwise hashing . While traditional minwise hashing methods store each hashed value using 64 bits , b bit minwise hashing only stores the lowest b bits ( where b ≥ 2 for 3 way ) . The extension to 3 way similarity from the prior work on 2 way similarity is technically non trivial . We develop the precise estimator which is accurate and very complicated ; and we recommend a much simplified estimator suitable for sparse data . Our analysis shows that b bit minwise hashing can normally achieve a 10 to 25 fold improvement in the storage space required for a given estimator accuracy of the 3 way resemblance .
1 Introduction The efficient computation of the similarity ( or overlap ) between sets is a central operation in a variety of applications , such as word associations ( eg , [ 13] ) , data cleaning ( eg , [ 40 , 9] ) , data mining ( eg , [ 14] ) , selectivity estimation ( eg , [ 30 ] ) or duplicate document detection [ 3 , 4 ] . In machine learning applications , binary ( 0/1 ) vectors can be naturally viewed as sets . For scenarios where the underlying data size is sufficiently large to make storing them ( in main memory ) or processing them in their entirety impractical , probabilistic techniques have been proposed for this task . Word associations ( collocations , co occurrences ) If one inputs a query NIPS machine learning , all major search engines will report the number of pagehits ( eg , one reports 829,003 ) , in addition to the top ranked URLs . Although no search engines have revealed how they estimate the numbers of pagehits , one natural approach is to treat this as a set intersection estimation problem . Each word can be represented as a set of document IDs ; and each set belongs to a very large space Ω . It is expected that |Ω| > 1010 . Word associations have many other applications in Computational Linguistics [ 13 , 38 ] , and were recently used for Web search query reformulation and query suggestions [ 42 , 12 ] . Here is another example . Commercial search engines display various form of “ vertical ” content ( eg , images , news , products ) as part of Web search . In order to determine from which “ vertical ” to display information , there exist various techniques to select verticals . Some of these ( eg , [ 29 , 15 ] ) use the number of documents the words in a search query occur in for different text corpora representing various verticals as features . Because this selection is invoked for all search queries ( and the tight latency bounds for search ) , the computation of these features has to be very fast . Moreover , the accuracy of vertical selection depends on the number/size of document corpora that can be processed within the allotted time [ 29 ] , ie , the processing speed can directly impact quality . Now , because of the large number of word combinations in even medium sized text corpora ( eg , the Wikipedia corpus contains > 107 distinct terms ) , it is impossible to pre compute and store the associations for all possible multi term combinations ( eg , > 1014 for 2 way and > 1021 for 3 way ) ; instead the techniques described in this paper can be used for fast estimates of the co occurrences . Database query optimization Set intersection is a routine operation in databases , employed for example during the evaluation of conjunctive selection conditions in the presence of single column indexes . Before conducting intersections , a critical task is to ( quickly ) estimate the sizes of the intermediate results to plan the optimal intersection order [ 20 , 8 , 25 ] . For example , consider the task of intersecting four sets of record identifiers : A ∩ B ∩ C ∩ D . Even though the final outcome will be the same , the order of the join operations , eg , ( A ∩ B ) ∩ ( C ∩ D ) or ( (A ∩ B ) ∩ C ) ∩ D , can significantly affect the performance , in particular if the intermediate results , eg , A∩B∩C , become too large for main memory and need to be spilled to disk . A good query plan aims to minimize
1This work is supported by NSF ( DMS 0808864 ) , ONR ( YIP N000140910911 ) and Microsoft . the total size of intermediate results . Thus , it is highly desirable to have a mechanism which can estimate join sizes very efficiently , especially for the lower order ( 2 way and 3 way ) intersections , which could potentially result in much larger intermediate results than higher order intersections . Duplicate Detection in Data Cleaning : A common task in data cleaning is the identification of duplicates ( eg , duplicate names , organizations , etc . ) among a set of items . Now , despite the fact that there is considerable evidence ( eg , [ 10 ] ) that reliable duplicate detection should be based on local properties of groups of duplicates , most current approaches base their decisions on pairwise similarities between items only . This is in part due to the computational overhead associated with more complex interactions , which our approach may help to overcome . Clustering Most clustering techniques are based on pair wise distances between the items to be clustered . However , there are a number of natural scenarios where the affinity relations are not pairwise , but rather triadic , tetradic or higher ( eg [ 1 , 43] ) . Again , our approach may improve the performance in these scenarios if the distance measures can be expressed in the form of set overlap . Data mining A lot of work in data mining has focused on efficient candidate pruning in the context of pairwise associations ( eg , [ 14] ) , a number of such pruning techniques leverage minwise hashing to prune pairs of items , but in many contexts ( eg , association rules with more than 2 items ) multi way associations are relevant ; here , pruning based on pairwise interactions may perform much less well than multi way pruning . 1.1 Ultra high dimensional data are often binary For duplicate detection in the context of Web crawling/search , each document can be represented as a set of w shingles ( w contiguous words ) ; w = 5 or 7 in several studies [ 3 , 4 , 17 ] . Normally only the abscence/presence ( 0/1 ) information is used , as a w shingle rarely occurs more than once in a page if w ≥ 5 . The total number of shingles is commonly set to be |Ω| = 264 ; and thus the set intersection corresponds to computing the inner product in binary data vectors of 264 dimensions . Interestingly , even when the data are not too high dimensional ( eg , only thousands ) , empirical studies [ 6 , 23 , 26 ] achieved good performance using SVM with binary quantized ( text or image ) data . 1.2 Minwise Hashing and SimHash Two of the most widely adopted approaches for estimating set intersections are minwise hashing [ 3 , 4 ] and sign ( 1 bit ) random projections ( also known as simhash ) [ 7 , 34 ] , which are both special instances of the general techniques proposed in the context of locality sensitive hashing [ 7 , 24 ] . These techniques have been successfully applied to many tasks in machine learning , databases , data mining , and information retrieval [ 18 , 36 , 11 , 22 , 16 , 39 , 28 , 41 , 27 , 5 , 2 , 37 , 7 , 24 , 21 ] . Limitations of random projections The method of random projections ( including simhash ) is limited to estimating pairwise similarities . Random projections convert any data distributions to ( zero mean ) multivariate normals , whose density functions are determined by the covariance matrix which contains only the pairwise information of the original data . This is a serious limitation . 1.3 Prior work on b Bit Minwise Hashing Instead of storing each hashed value using 64 bits as in prior studies , eg , [ 17 ] , [ 35 ] suggested to store only the lowest b bits . [ 35 ] demonstrated that using b = 1 reduces the storage space at least by a factor of 21.3 ( for a given accuracy ) compared to b = 64 , if one is interested in resemblance ≥ 0.5 , the threshold used in prior studies [ 3 , 4 ] . Moreover , by choosing the value b of bits to be retained , it becomes possible to systematically adjust the degree to which the estimator is “ tuned ” towards higher similarities as well as the amount of hashing ( random permutations ) required . [ 35 ] concerned only the pairwise resemblance . To extend it to the multi way case , we have to solve new and challenging probability problems . Compared to the pairwise case , our new estimator is significantly different . In fact , as we will show later , estimating 3 way resemblance requires b ≥ 2 . 1.4 Notation
Figure 1 : Notation for 2 way and 3 way set intersections . a12f1aa23f3a13f2r1r3s12ss23r2s13 Fig 1 describes the notation used in 3 way intersections for three sets S1 , S2 , S3 ∈ Ω , |Ω| = D .
• f1 = |S1| , f2 = |S2| , f3 = |S3| . • a12 = |S1 ∩ S2| , a13 = |S1 ∩ S3| , a23 = |S2 ∩ S3| , a = a123 = |S1 ∩ S2 ∩ S3| . • r1 = f1 D , s = s123 = a D . D . s12 = a12 • u = r1 + r2 + r3 − s12 − s13 − s23 + s .
D , s23 = a23
D , s13 = a13
D , r3 = f3
D , r2 = f2
We define three 2 way resemblances ( R12 , R13 , R23 ) and one 3 way resemblance ( R ) as :
R12 =
|S1 ∩ S2| |S1 ∪ S2| , R13 =
|S2 ∩ S3| |S2 ∪ S3| , which , using our notation , can be expressed in various forms :
|S1 ∩ S3| |S1 ∪ S3| , R23 =
R = R123 =
|S1 ∩ S2 ∩ S3| |S1 ∪ S2 ∪ S3| .
( 1 )
( 2 ) s
Rij = aij sij
, i = j ,
= fi + fj − aij f1 + f2 + f3 − a12 − a23 − a13 + a ri + rj − sij a
R =
( 3 ) Note that , instead of a123 , s123 , R123 , we simply use a , s , R . When the set sizes , fi = |Si| , can be assumed to be known , we can compute resemblances from intersections and vice versa : r1 + r2 + r3 − s12 − s23 − s13 + s
=
=
. s u aij =
Rij
1 + Rij
( fi + fj ) , a =
R
1 − R
( f1 + f2 + f3 − a12 − a13 − a23 ) .
Thus , estimating resemblances and estimating intersection sizes are two closely related problems .
1.5 Our Main Contributions
• We derive the basic probability formula for estimating 3 way resemblance using b bit hashing . The derivation turns out to be significantly much more complex than the 2 way case . This basic probability formula naturally leads to a ( complicated ) estimator of resemblance . D ≈ 0 , but fi/fj = ri/rj may be still significant ) to develop a much simplified estimator , which is desired in practical applications . This assumption of fi/D → 0 significantly simplifies the estimator and frees us from having to know the cardinalities fi .
• We leverage the observation that many real applications involve sparse data ( ie , ri = fi
• We analyze the theoretical variance of the simplified estimator and compare it with the original minwise hashing method ( using 64 bits ) . Our theoretical analysis shows that bbit minwise hashing can normally achieve a 10 to 25 fold improvement in storage space ( for a given estimator accuracy of the 3 way resemblance ) when the set similarities are not extremely low ( eg , when the 3 way resemblance > 002 ) These results are particularly important for applications in which only detecting high resemblance/overlap is relevant , such as many data cleaning scenarios or duplicate detection .
The recommended procedure for estimating 3 way resemblances ( in sparse data ) is shown as Alg . 1 .
Algorithm 1 The b bit minwise hashing algorithm , applied to estimating 3 way resemblances in a collection of N sets . This procedure is suitable for sparse data , ie , ri = fi/D ≈ 0 . Input : Sets Sn ∈ Ω = {0 , 1 , , D − 1} , n = 1 to N . Pre processing phrase : 1 ) Generate k random permutations πj : Ω → Ω , j = 1 to k . 2 ) For each set Sn and permutation πj , store the lowest b bits of min ( πj ( Sn) ) , denoted by en,t,πj , t = 1 to b . Estimation phrase : ( Use three sets S1 , S2 , and S3 as an example . ) 1 ) Compute ˆP12,b = 1 k 2 ) Compute ˆPb = 1 k t=1 1{e1,t,πj = e2,t,πj = e3,t,πj}
. Similarly , compute ˆP13,b and ˆP23,b . t=1 1{e1,t,πj = e2,t,πj} b b k k j=1
. j=1 4b ˆPb−2b( ˆP12,b+ ˆP13,b+ ˆP23,b)+2
3 ) Estimate R by ˆRb = 4 ) If needed , the 2 way resemblances Rij,b can be estimated as ˆRij,b = 2b ˆPij,b−1 2b−1
( 2b−1)(2b−2 )
.
.
2 The Precise Theoretical Probability Analysis Minwise hashing applies k random permutations πj : Ω −→ Ω , Ω = {0 , 1 , , D − 1} , and then estimates R12 ( and similarly other 2 way resemblances ) using the following probability :
Pr ( min(πj(S1 ) ) = min(πj(S2) ) ) =
|S1 ∩ S2| |S1 ∪ S2| = R12 .
This method naturally extends to estimating 3 way resemblances for three sets S1 , S2 , S3 ∈ Ω :
Pr ( min(πj(S1 ) ) = min(πj(S2 ) ) = min(πj(S3) ) ) =
|S1 ∩ S2 ∩ S3| |S1 ∪ S2 ∪ S3| = R .
To describe b bit hashing , we define the minimum values under π and their lowest b bits to be : zi = min ( π ( Si ) ) , ei,t = t th lowest bit of zi .
( cid:195 ) b
To estimate R , we need to computes the empirical estimates of the probabilities Pij,b and Pb , where
Pij,b = Pr
1{ei,t = ej,t} = 1
,
Pb = P123,b = Pr
1{e1,t = e2,t = e3,t} = 1
. t=1 t=1
The main theoretical task is to derive Pb . The prior work[35 ] already derived Pij,b ; see Appendix A . To simplify the algebra , we assume that D is large , which is virtually always satisfied in practice .
( 4 )
( 5 )
( cid:195 ) b
Theorem 1 Assume D is large .
( cid:195 ) b i=1 where u = r1 + r2 + r3 − s12 − s13 − s23 + s , and Z =(s12 − s)A3,b +
( r3 − s13 − s23 + s ) r1 + r2 − s12
( r1 − s12 − s13 + s )
+(s23 − s)A1,b + r2 + r3 − s23 + [ (r1 − s13)A3,b + ( r3 − s13)A1,b ]
+ [ (r1 − s12)A2,b + ( r2 − s12)A1,b ]
( r2 − s12 − s23 + s ) r1 + r3 − s13
( r3 − s13 − s23 + s ) r1 + r2 − s12
G13,b
G12,b ,
Pb = Pr
1{e1,i = e2,i = e3,i} = 1
=
Z u
+ R =
Z + s u
,
( 6 ) s12G12,b + ( s13 − s)A2,b + r1 + r3 − s13 s23G23,b + [ (r2 − s23)A3,b + ( r3 − s23)A2,b ]
( r2 − s12 − s23 + s ) s13G13,b
( r1 − s12 − s13 + s ) r2 + r3 − s23
G23,b
Aj,b = rj ( 1 − rj )2b−1 1 − ( 1 − rj )2b
,
Gij,b =
( ri + rj − sij )(1 − ri − rj + sij )2b−1
1 − ( 1 − ri − rj + sij )2b i , j ∈ {1 , 2 , 3} , i = j .
,
Theorem 1 naturally suggests an iterative estimation procedure , by writing Eq ( 6 ) as s = Pbu− Z .
Figure 2 : Pb , for verifying the probability formula in Theorem 1 . The empirical estimates and the theoretical predictions essentially overlap regardless of the sparsity measure ri = fi/D .
A Simulation Study For the purpose of verifying Theorem 1 , we use three sets corresponding to the occurrences of three common words ( “ OF ” , “ AND ” , and “ OR ” ) in a chunk of real world Web crawl data . Each ( word ) set is a set of document ( Web page ) IDs which contained that word at least once . The three sets are not too sparse and D = 216 suffices to represent their elements . The ri = fi D values are 0.5697 , 0.5537 , and 0.3564 , respectively . The true 3 way resemblance is R = 047
010020030040050004604805052054056058Sample size kPb D = 216b = 2b = 3b = 42 bits3 bits4 bitsTheoretical010020030040050004604805052054056058Sample size kPb D = 220b = 2b = 3b = 42 bits3 bits4 bitsTheoretical We can also increase D by mapping these sets into a larger space using a random mapping , with D = 216 , 218 , 220 , or 222 . When D = 222 , the ri values are 0.0089 , 0.0087 , 00056 Fig 2 presents the empirical estimates of the probability Pb , together with the theoretical predictions by Theorem 1 . The empirical estimates essentially overlap the theoretical predictions . Even though the proof assumes D → ∞ , D does not have to be too large for Theorem 1 to be accurate . 3 The Much Simplified Estimator for Sparse Data The basic probability formula ( Theorem 1 ) we derive could be too complicated for practical use . To D ≈ 0 , obtain a simpler formula , we leverage the observation that in practice we often have ri = fi even though both fi and D can be very large . For example , consider web duplicate detection [ 17 ] . Here , D = 264 , which means that even for a web page with fi = 254 shingles ( corresponding to the D ≈ 0001 Note that , even when ri → 0 , the ratios , eg , r2 text of a small novel ) , we still have fi , can be still large . Recall the resemblances ( 2 ) and ( 3 ) are only determined by these ratios . r1
D using two real life datasets : the UCI dataset containing 3 × 105 We analyzed the distribution of fi NYTimes articles ; and a Microsoft proprietary dataset with 106 news articles [ 19 ] . For the UCINYTimes dataset , each document was already processed as a set of single words . For the anonymous dataset , we report results using three different representations : single words ( 1 shingle ) , 2 shingles ( two contiguous words ) , and 3 shingles . Table 1 reports the summary statistics of the fi
D values .
Table 1 : Summary statistics of the fi
Data 3 × 105 UCI NYTimes articles 106 Microsoft articles ( 1 shingle ) 106 Microsoft articles ( 2 shingle ) 106 Microsoft articles ( 3 shingle )
D values in two datasets Std . 0.0011 0.00023 0.00005 0.00002
Mean 0.0022 0.00032 0.00004 0.00002
Median 0.0021 0.00027 0.00003 0.00002
For truly large scale applications , prior studies [ 3 , 4 , 17 ] commonly used 5 shingles . This means that real world data may be significantly more sparse than the values reported in Table 1 . 3.1 The Simplified Probability Formula and the Practical Estimator Theorem 2 Assume D is large . Let T = R12 + R13 + R23 . As r1 , r2 , r3 → 0 ,
Pb = Pr
1{e1,i = e2,i = e3,i} = 1
( 2b − 1)(2b − 2)R + ( 2b − 1)T + 1
.
( 7 )
=
1 4b
( cid:195 ) b i=1
Interestingly , if b = 1 , then P1 = 1 contained . Hence , it is necessary to use b ≥ 2 to estimate 3 way similarities . Alg . 1 uses ˆPb and ˆPij,b to respectively denote the empirical estimates of the theoretical probabilities Pb and Pij,b . Assuming r1 , r2 , r3 → 0 , the proposed estimator of R , denoted by ˆRb , is
4 ( 1 + T ) , ie , no information about the 3 way resemblance R is
( cid:179 )
( cid:180 )
ˆP12,b + ˆP13,b + ˆP23,b ( 2b − 1)(2b − 2 )
+ 2
.
ˆRb =
4b ˆPb − 2b
( cid:179 )
Theorem 3 Assume D is large and r1 , r2 , r3 → 0 . Then ˆRb in ( 8 ) is unbiased with the variance R − ( 2b − 1)(2b − 2)R2
4b − 6 × 2b + 10
1 + ( 2b − 3)T +
( cid:180 )
( cid:179 )
( cid:180 )
1
V ar
ˆRb
=
1 k
( 2b − 1)(2b − 2 )
( 8 )
.
( 9 )
It is interesting to examine several special cases :
• b = 1 : V ar( ˆR1 ) = ∞ , ie , one must use b ≥ 2 . • b = 2 : V ar( ˆR2 ) = 1 • b = ∞ : V ar( ˆR∞ ) = 1
( cid:161 ) 1 + T + 2R − 6R2 k R(1 − R ) = V ar( ˆRM ) . ˆRM is the original minwise hashing estimator for 3 way resemblance . In principle , the estimator ˆRM requires an infinite precision ( ie , b = ∞ ) . Numerically , V ar( ˆRM ) and V ar( ˆR64 ) are indistinguishable .
( cid:162 )
6k
.
3.2 Simulations for Validating Theorem 3 We now present a simulation study for verifying Theorem 3 , using the same three sets used in Fig 2 . Fig 3 presents the resulting empirical biases : E( ˆRb)−Rb . Fig 4 presents the empirical mean square errors ( MSE = bias2+variance ) together with the theoretical variances V ar( ˆRb ) in Theorem 3 .
Figure 3 : Bias of ˆRb ( 8 ) . We used 3 ( word ) sets : “ OF ” , “ AND ” , and “ OR ” and four D values : 216 , 218 , 220 , and 222 . We conducted experiments using b = 2 , 3 , and 4 as well as the original minwise hashing ( denoted by “ M ” ) . The plots verify that as ri decreases ( to zero ) , the biases vanish . Note that the set sizes fi remain the same , but the relative values ri = fi
D decrease as D increases .
Figure 4 : MSE of ˆRb ( 8 ) . The solid curves are the empirical MSEs ( =var+bias2 ) and the dashed lines are the theoretical variances ( 9 ) , under the assumption of ri → 0 . Ideally , we would like to see the solid and dashed lines overlap . When D = 220 and D = 222 , even though the ri values are not too small , the solid and dashed lines almost overlap . Note that , at the same sample size k , we always have V ar( ˆR2 ) > V ar( ˆR3 ) > V ar( ˆR4 ) > V ar( ˆRM ) , where ˆRM is the original minwise hashing estimator . We can see that , V ar( ˆR3 ) and V ar( ˆR4 ) are very close to V ar( ˆRM ) .
We can summarize the results in Fig 3 and Fig 4 as follows :
• When the ri = fi
D values are large ( eg , ri ≈ 0.5 when D = 216 ) , the estimates using ( 8 ) can be noticeably biased . The estimation biases diminish as the ri values decrease . In fact , even when the ri values are not small ( eg , ri ≈ 0.05 when D = 220 ) , the biases are already very small ( roughly 0.005 when D = 220 ) . • The variance formula ( 9 ) becomes accurate when the ri values are not too large . For example , when D = 218 ( ri ≈ 0.1 ) , the empirical MSEs largely overlap the theoretical variances which assumed ri → 0 , unless the sample size k is large . When D = 220 ( and D = 222 ) , the empirical MSEs and theoretical variances overlap . • For real applications , as we expect D will be very large ( eg , 264 ) and the ri values ( fi/D ) will be very small , our proposed simple estimator ( 8 ) will be very useful in practice , because it becomes unbiased and the variance can be reliably predicted by ( 9 ) .
4 Improving Estimates for Dense Data Using Theorem 1 While we believe the simple estimator in ( 8 ) and Alg . 1 should suffice in most applications , we demonstrate here that the sparsity assumption of ri → 0 is not essential if one is willing to use the more sophisticated estimation procedure provided by Theorem 1 . By Eq ( 6 ) , s = Pbu − Z , where Z contains s , sij , ri etc . We first estimate sij ( from the estimated Rij ) using the precise formula for the two way case ; see Appendix A . We then iteratively solve for s using the initial guess provided by the estimator ˆRb in ( 8 ) . Usually a few iterations suffice . Fig 5 reports the bias ( left most panel , only for D = 216 ) and MSE , corresponding to Fig 3 and Fig 4 . In Fig 5 , the solid curves are obtained using the precise estimation procedure by Theorem 1 . The dashed curves are the estimates using the simplified estimator ˆRb which assumes ri → 0 .
0100200300400500−01−0050005Sample size kBias D = 216b = 2b = 3b = 4M0100200300400500−003−002−0010001Sample size kBias b = 4Mb = 3b = 2D = 2180100200300400500−10−505x 10−3Sample size kBias D = 220MM4b = 230100200300400500−10−505x 10−3Sample size kBias D = 222Mb = 2b = 431010050010−310−210−1Sample size kMean square error ( MSE ) D = 216b = 2b = 3b = 4M2 bits3 bits4 bitsminwiseTheoretical1010050010−310−210−1Sample size kMean square error ( MSE ) D = 218b = 2M3342 bits3 bits4 bitsminwiseTheoretical1010050010−310−210−1Sample size kMean square error ( MSE ) D = 220b = 2M342 bits3 bits4 bitsminwiseTheoretical1010050010−310−210−1Sample size kMean square error ( MSE ) D = 222b = 23M42 bits3 bits4 bitsminwiseTheoretical Even when the data are not sparse , the precise estimation procedure provides unbiased estimates as verified by the leftmost panel of Fig 5 . Using the precise procedure results in noticeably more accurate estimates in non sparse data , as verified by the second panel of Fig 5 . However , as long as the data are reasonably sparse ( the right two panels ) , the simple estimator ˆRb in ( 8 ) is accurate .
Figure 5 : The bias ( leftmost panel ) and MSE of the precise estimation procedure , using the same data used in Fig 3 and Fig 4 . The dashed curves correspond to the estimates using the simplified estimator ˆRb in ( 8 ) which assumes ri → 0 .
5 Quantifying the Improvements Using b Bit Hashing
This section is devoted to analyzing the improvements of b bit minwise hashing , compared to using 64 bits for each hashed value . Throughout the paper , we use the terms “ sample ” and “ sample size ” ( denoted by k ) . The original minwise hashing stores each “ sample ” using 64 bits ( as in [ 17] ) . For b bit minwise hashing , we store each “ sample ” using b bits only . Note that V ar( ˆR64 ) and V ar( ˆRM ) ( the variance of the original minwise hashing ) are numerically indistinguishable . As we decrease b , the space needed for each sample will be smaller ; the estimation variance at the same sample size k , however , will increase . This variance space trade off can be quantified by × k , which is called the storage factor . Lower B(b ) is more desirable . The B(b ) = b × Var ratio B(64 )
B(b ) precisely characterizes the improvements of b bit hashing compared to using 64 bits .
ˆRb
( cid:179 )
( cid:180 )
Fig 6 confirms the substantial improvements of b bit hashing over the original minwise hashing using 64 bits . The improvements in terms of the storage space are usually 10 ( or 15 ) to 25 fold when the sets are reasonably similar ( ie , when the 3 way resemblance > 01 ) When the three sets are very similar ( eg , the top left panel ) , the improvement will be even 25 to 30 fold .
Figure 6 : B(64 ) B(b ) , the relative storage improvement of using b = 2 , 3 , 4 , 6 bits , compared to using 64 bits . Since the variance ( 9 ) contains both R and T = R12 + R13 + R23 , we compare variances using different T /R ratios . As 3R ≤ T always , we let T = αR , for some α ≥ 3 . Since T ≤ 3 , we know R ≤ 3/α . Practical applications are often interested in cases with reasonably large R values .
0100200300400500−1−050051x 10−3Sample size kBias D = 216Biasb = 3b = 21010050010−310−210−1Sample size kMean square error ( MSE ) D = 216b = 2b = 3b = 3b = 21010050010−310−210−1Sample size kMean square error ( MSE ) D = 218b = 2b = 3b = 2b = 31010050010−310−210−1Sample size kMean square error ( MSE ) b = 3b = 2D = 2200020406081051015202530RStorage ratio ( B(64 ) / B(b ) ) T = 3Rb = 2b = 4b = 3b = 60010203040506070805101520RStorage ratio B(64 ) / B(b ) T = 4Rb = 2b = 3b = 4b = 6b = 20010203040505101520RStorage ratio B(64 ) / B(b ) T = 6Rb = 3b = 2b = 44b = 2b = 60005010150202503024681012RStorage ratio B(64 ) / B(b ) T = 10Rb = 3b = 2b = 4b = 60005010150246810RStorage ratio ( B(64 ) / B(b ) ) T = 20Rb = 4b = 3b = 2b = 6000100200300400500601234567RStorage ratio ( B(64 ) / B(b ) ) T = 50Rb = 4b = 3b = 2b = 6 6 Evaluation of Accuracy We conducted a duplicate detection experiment on a public ( UCI ) collection of 300,000 NYTimes news articles . The task is to identify 3 groups with 3 way resemblance R exceeding a threshold R0 . We used a subset of the data ; the total number of 3 groups is about one billion . We experimented with b = 2 , 4 and the original minwise hashing . Fig 7 presents the precision curves for a representative set of thresholds R0 ’s . Just like in [ 35 ] , the recall curves are not shown because they could not differentiate estimators . These curves confirm the significant improvement of using b bit minwise hashing when the threshold R0 is quite high ( eg , 03 ) In fact , when R0 = 0.3 , using b = 4 resulted in similar precisions as using the original minwise hashing ( ie , a 64/4=16 fold reduction in storage ) . Even when R0 = 0.1 , using b = 4 can still achieve similar precisions as using the original minwise hashing by only slightly increasing the sample size k .
Figure 7 : Precision curves on the UCI collection of news data . The task is to retrieve news article 3 groups with resemblance R ≥ R0 . For example , consider R0 = 02 To achieve a precision of at least 0.8 , 2 bit hashing and 4 bit hashing require about k = 500 samples and k = 260 samples respectively , while the original minwise hashing ( denoted by M ) requires about 170 samples . 7 Conclusion In machine learning , highComputing set similarities is fundamental in many applications . dimensional binary data are common and are equivalent to sets . This study is devoted to simultaneously estimating 2 way and 3 way similarities using b bit minwise hashing . Compared to the prior work on estimating 2 way resemblance [ 35 ] , the extension to 3 way is important for many application scenarios ( as described in Sec 1 ) and is technically non trivial . For estimating 3 way resemblance , our analysis shows that b bit minwise hashing can normally achieve a 10 to 25 fold improvement in the storage space required for a given estimator accuracy , when the set similarities are not extremely low ( eg , 3 way resemblance > 002 ) Many applications such as data cleaning and de duplication are mainly concerned with relatively high set similarities . For many practical applications , the reductions in storage directly translate to improvements in processing speed as well , especially when memory latency is the main bottleneck , which , with the advent of many core processors , is more and more common . Future work : We are interested in developing a b bit version for Conditional Random Sampling ( CRS ) [ 31 , 32 , 33 ] , which requires only one permutation ( instead of k permutations ) and naturally extends to non binary data . CRS is also provably more accurate than minwise hashing for binary data . However , the analysis for developing the b bit version of CRS appears to be very difficult . A Review of b Bit Minwise Hashing for 2 Way Resemblance Theorem 4 ( [35 ] ) Assume D is large .
( cid:195 ) b where
P12,b = Pr r2
C1,b = A1,b i=1
+ A2,b r1 r1 + r2 r1 + r2
1{e1,i = e2,i} = 1
= C1,b + ( 1 − C2,b ) R12 r2 r1
, C2,b = A1,b r1 + r2
+ A2,b
, r1 + r2
A1,b = r1 [ 1 − r1]2b−1 1 − [ 1 − r1]2b , If r1 , r2 → 0 , P12,b = 1+(2b−1)R12 , where ˆP12,b is the empirical observation of P12,b . If r1 , r2 are not small , R12 is estimated by ( ˆP12,b−C1,b)/(1−C2,b ) . and one can estimate R12 by 2b ˆP12,b−1 2b−1 r2 [ 1 − r2]2b−1 1 − [ 1 − r2]2b .
A2,b =
2b
01002003004005000020406081Sample size kPrecision R0 = 01b=2b=4M01002003004005000020406081Sample size kPrecision R0 = 02b=2Mb=401002003004005000020406081Sample size kPrecision R0 = 0.3b=2MM4 References [ 1 ] S . Agarwal , J . Lim , L . Zelnik Manor , P . Perona , D . Kriegman , and S . Belongie . Beyond pairwise clustering . In CVPR , 2005 . [ 2 ] M . Bendersky and W . B . Croft . Finding text reuse on the web . In WSDM , pages 262–271 , Barcelona , Spain , 2009 . [ 3 ] A . Z . Broder . On the resemblance and containment of documents . In the Compression and Complexity of Sequences , pages 21–29 ,
Positano , Italy , 1997 .
[ 4 ] A . Z . Broder , S . C . Glassman , M . S . Manasse , and G . Zweig . Syntactic clustering of the web . In WWW , pages 1157 – 1166 , Santa Clara ,
CA , 1997 .
[ 5 ] G . Buehrer and K . Chellapilla . A scalable pattern mining approach to web graph compression with communities . In WSDM , pages
95–106 , Stanford , CA , 2008 .
[ 6 ] O . Chapelle , P . Haffner , and V . N . Vapnik . Support vector machines for histogram based image classification . 10(5):1055–1064 , 1999 . [ 7 ] M . S . Charikar . Similarity estimation techniques from rounding algorithms . In STOC , pages 380–388 , Montreal , Quebec , Canada , 2002 . [ 8 ] S . Chaudhuri . An Overview of Query Optimization in Relational Systems . In PODS , pages 34–43 , 1998 . [ 9 ] S . Chaudhuri , V . Ganti , and R . Kaushik . A primitive operatior for similarity joins in data cleaning . In ICDE , 2006 . [ 10 ] S . Chaudhuri , V . Ganti , and R . Motwani . Robust identification of fuzzy duplicates . In ICDE , pages 865–876 , Tokyo , Japan , 2005 . [ 11 ] F . Chierichetti , R . Kumar , S . Lattanzi , M . Mitzenmacher , A . Panconesi , and P . Raghavan . On compressing social networks . In KDD , pages 219–228 , Paris , France , 2009 .
[ 12 ] K . Church . Approximate lexicography and web search . International Journal of Lexicography , 21(3):325–336 , 2008 . [ 13 ] K . Church and P . Hanks . Word association norms , mutual information and lexicography . Computational Linguistics , 16(1):22–29 , 1991 . [ 14 ] E . Cohen , M . Datar , S . Fujiwara , A . Gionis , P . Indyk , R . Motwani , J . D . Ullman , and C . Yang . Finding interesting associations without support pruning . IEEE Trans . on Knowl . and Data Eng . , 13(1 ) , 2001 .
[ 15 ] F . Diaz . Integration of News Content into Web Results . In WSDM , 2009 . [ 16 ] Y . Dourisboure , F . Geraci , and M . Pellegrini . Extraction and classification of dense implicit communities in the web graph . ACM Trans .
Web , 3(2):1–36 , 2009 .
[ 17 ] D . Fetterly , M . Manasse , M . Najork , and J . L . Wiener . A large scale study of the evolution of web pages . In WWW , pages 669–678 ,
Budapest , Hungary , 2003 .
[ 18 ] G . Forman , K . Eshghi , and J . Suermondt . Efficient detection of large scale redundancy in enterprise file systems . SIGOPS Oper . Syst .
Rev . , 43(1):84–91 , 2009 .
[ 19 ] M . Gamon , S . Basu , D . Belenko , D . Fisher , M . Hurst , and A . C . K¨onig . Blews : Using blogs to provide context for news articles . In AAAI
Conference on Weblogs and Social Media , 2008 .
[ 20 ] H . Garcia Molina , J . D . Ullman , and J . Widom . Database Systems : the Complete Book . Prentice Hall , New York , NY , 2002 . [ 21 ] A . Gionis , D . Gunopulos , and N . Koudas . Efficient and tunable similar set retrieval . In SIGMOD , pages 247–258 , CA , 2001 . [ 22 ] S . Gollapudi and A . Sharma . An axiomatic approach for result diversification . In WWW , pages 381–390 , Madrid , Spain , 2009 . [ 23 ] M . Hein and O . Bousquet . Hilbertian metrics and positive definite kernels on probability measures .
In AISTATS , pages 136–143 ,
Barbados , 2005 .
[ 24 ] P . Indyk and R . Motwani . Approximate nearest neighbors : Towards removing the curse of dimensionality . In STOC , pages 604–613 ,
Dallas , TX , 1998 .
[ 25 ] Y . E . Ioannidis . The history of histograms ( abridged ) . In VLDB , 2003 . [ 26 ] Y . Jiang , C . Ngo , and J . Yang . Towards optimal bag of features for object categorization and semantic video retrieval . In CIVR , pages
494–501 , Amsterdam , Netherlands , 2007 .
[ 27 ] N . Jindal and B . Liu . Opinion spam and analysis . In WSDM , pages 219–230 , Palo Alto , California , USA , 2008 . [ 28 ] K . Kalpakis and S . Tang . Collaborative data gathering in wireless sensor networks using measurement co occurrence . Computer
Communications , 31(10):1979–1992 , 2008 .
[ 29 ] A . C . K¨onig , M . Gamon , and Q . Wu . Click Through Prediction for News Queries . In SIGIR , 2009 . [ 30 ] H . Lee , R . T . Ng , and K . Shim . Power law based estimation of set similarity join size . In PVLDB , 2009 . [ 31 ] P . Li and K . W . Church . A sketch algorithm for estimating two way and multi way associations . Computational Linguistics , 33(3):305–
354 , 2007 ( Preliminary results appeared in HLT/EMNLP 2005 ) .
[ 32 ] P . Li , K . W . Church , and T . J . Hastie . Conditional random sampling : A sketch based sampling technique for sparse data . In NIPS , pages
873–880 , Vancouver , BC , Canada , 2006 .
[ 33 ] P . Li , K . W . Church , and T . J . Hastie . One sketch for all : Theory and applications of conditional random sampling . In NIPS , Vancouver ,
BC , Canada , 2008 .
[ 34 ] P . Li , T . J . Hastie , and K . W . Church . Improving random projections using marginal information . In COLT , pages 635–649 , Pittsburgh ,
PA , 2006 .
[ 35 ] P . Li and A . C . K¨onig . b bit minwise hashing . In WWW , pages 671–680 , Raleigh , NC , 2010 . [ 36 ] Ludmila , K . Eshghi , C . B . M . III , J . Tucek , and A . Veitch . Probabilistic frequent itemset mining in uncertain databases . In KDD , pages
1087–1096 , Paris , France , 2009 .
[ 37 ] G . S . Manku , A . Jain , and A . D . Sarma . Detecting Near Duplicates for Web Crawling . In WWW , Banff , Alberta , Canada , 2007 . [ 38 ] C . D . Manning and H . Schutze . Foundations of Statistical Natural Language Processing . The MIT Press , Cambridge , MA , 1999 . [ 39 ] M . Najork , S . Gollapudi , and R . Panigrahy . Less is more : sampling the neighborhood graph makes salsa better and faster . In WSDM , pages 242–251 , Barcelona , Spain , 2009 .
[ 40 ] S . Sarawagi and A . Kirpal . Efficient set joins on similarity predicates . In SIGMOD , pages 743–754 , 2004 . [ 41 ] T . Urvoy , E . Chauveau , P . Filoche , and T . Lavergne . Tracking web spam with html style similarities . ACM Trans . Web , 2(1):1–28 , 2008 . [ 42 ] X . Wang and C . Zhai . Mining term association patterns from search logs for effective query reformulation . In CIKM , pages 479–488 ,
Napa Valley , California , USA , 2008 .
[ 43 ] D . Zhou , J . Huang , and B . Sch¨olkopf . Beyond pairwise classification and clustering using hypergraphs . 2006 .
Hashing Algorithms for Large Scale Learning
Ping Li
Cornell University pingli@cornell.edu
Anshumali Shrivastava
Cornell University anshu@cscornelledu
Joshua Moore
Cornell University
Arnd Christian K¨onig
Microsoft Research jlmo@cscornelledu chrisko@microsoft.com
Abstract
Minwise hashing is a standard technique in the context of search for efficiently computing set similarities . The recent development of b bit minwise hashing provides a substantial improvement by storing only the lowest b bits of each hashed value . In this paper , we demonstrate that b bit minwise hashing can be naturally integrated with linear learning algorithms such as linear SVM and logistic regression , to solve large scale and high dimensional statistical learning tasks , especially when the data do not fit in memory . We compare b bit minwise hashing with the Count Min ( CM ) and Vowpal Wabbit ( VW ) algorithms , which have essentially the same variances as random projections . Our theoretical and empirical comparisons illustrate that b bit minwise hashing is significantly more accurate ( at the same storage cost ) than VW ( and random projections ) for binary data .
1 Introduction With the advent of the Internet , many machine learning applications are faced with very large and inherently high dimensional datasets , resulting in challenges in scaling up training algorithms and storing the data . Especially in the context of search and machine translation , corpus sizes used in industrial practice have long exceeded the main memory capacity of single machine . For example , [ 33 ] discusses training sets with 1011 items and 109 distinct features , requiring novel algorithmic approaches and architectures . As a consequence , there has been a renewed emphasis on scaling up machine learning techniques by using massively parallel architectures ; however , methods relying solely on parallelism can be expensive ( both with regards to hardware requirements and energy costs ) and often induce significant additional communication and data distribution overhead . This work approaches the challenges posed by large datasets by leveraging techniques from the area of similarity search [ 2 ] , where similar increases in data sizes have made the storage and computational requirements for computing exact distances prohibitive , thus making data representations that allow compact storage and efficient approximate similarity computation necessary . The method of b bit minwise hashing [ 26–28 ] is a recent progress for efficiently ( in both time and space ) computing resemblances among extremely high dimensional ( eg , 264 ) binary vectors . In this paper , we show that b bit minwise hashing can be seamlessly integrated with linear Support Vector Machine ( SVM ) [ 13 , 18 , 20 , 31 , 35 ] and logistic regression solvers . 1.1 Ultra High Dimensional Large Datasets and Memory Bottlenecks In the context of search , a standard procedure to represent documents ( eg , Web pages ) is to use w shingles ( ie , w contiguous words ) , where w ≥ 5 in several studies [ 6 , 7 , 14 ] . This procedure can generate datasets of extremely high dimensions . For example , suppose we only consider 105 common English words . Using w = 5 may require the size of dictionary Ω to be D = |Ω| = 1025 = 283 . In practice , D = 264 often suffices , as the number of available documents may not be large enough to exhaust the dictionary . For w shingle data , normally only abscence/presence ( 0/1 ) information is used , as it is known that word frequency distributions within documents approximately follow a power law [ 3 ] , meaning that most single terms occur rarely , thereby making a w shingle is unlikely to occur more than once in a document . Interestingly , even when the data are not too highdimensional , empirical studies [ 8 , 17 , 19 ] achieved good performance with binary quantized data . When the data can fit in memory , linear SVM training is often extremely efficient after the data are loaded into the memory . It is however often the case that , for very large datasets , the data loading
1 time dominates the computing time for solving the SVM problem [ 35 ] . A more severe problem arises when the data can not fit in memory . This situation can be common in practice . The publicly available webspam dataset ( in LIBSVM format ) needs about 24GB disk space , which exceeds the memory capacity of many desktop PCs . Note that webspam , which contains only 350,000 documents represented by 3 shingles , is still very small compared to industry applications [ 33 ] . 1.2 Our Proposal We propose a solution which leverages b bit minwise hashing . Our approach assumes the data vectors are binary , high dimensional , and relatively sparse , which is generally true of text documents represented via shingles . We apply b bit minwise hashing to obtain a compact representation of the original data . In order to use the technique for efficient learning , we have to address several issues : • We need to prove that the matrices generated by b bit minwise hashing are positive definite , • If we use b bit minwise hashing to estimate the resemblance , which is nonlinear , how can • Compared to other hashing techniques such as random projections , Count Min ( CM ) we effectively convert this nonlinear problem into a linear problem ? which will provide the solid foundation for our proposed solution . sketch [ 11 ] , or Vowpal Wabbit ( VW ) [ 32 , 34 ] , does our approach exhibits advantages ?
It turns out that our proof in the next section that b bit hashing matrices are positive definite naturally provides the construction for converting the otherwise nonlinear SVM problem into linear SVM . 2 Review of Minwise Hashing and b Bit Minwise Hashing Minwise hashing [ 6,7 ] has been successfully applied to a wide range of real world problems [ 4,6,7 , 9 , 10 , 12 , 15 , 16 , 30 ] , for efficiently computing set similarities . Minwise hashing mainly works well with binary data , which can be viewed either as 0/1 vectors or as sets . Given two sets , S1 , S2 ⊆ Ω = {0 , 1 , 2 , , D − 1} , a widely used measure of similarity is the resemblance R :
R =
|S1 ∩ S2| |S1 ∪ S2| = a f1 + f2 − a
, where f1 = |S1| , f2 = |S2| , a = |S1 ∩ S2| .
Applying a random permutation π : Ω → Ω on S1 and S2 , the collision probability is simply
Pr ( min(π(S1 ) ) = min(π(S2) ) ) =
|S1 ∩ S2| |S1 ∪ S2| = R .
( 1 )
( 2 )
One can repeat the permutation k times : π1 , π2 , , πk to estimate R without bias . The common practice is to store each hashed value , eg , min(π(S1 ) ) and min(π(S2) ) , using 64 bits [ 14 ] . The storage ( and computational ) cost will be prohibitive in truly large scale ( industry ) applications [ 29 ] . b bit minwise hashing [ 27 ] provides a strikingly simple solution to this ( storage and computational ) problem by storing only the lowest b bits ( instead of 64 bits ) of each hashed value . ( z(b ) For convenience , denote z1 = min ( π ( S1 ) ) and z2 = min ( π ( S2) ) , and denote z(b ) 2 ) the 1 integer value corresponding to the lowest b bits of of z1 ( z2 ) . For example , if z1 = 7 , then z(2 ) 1 = 3 . Theorem 1 [ 27 ] Assume D is large .
( cid:179 )
( cid:180 )
= C1,b + ( 1 − C2,b ) R
( 3 )
Pb = Pr r1 = f1 D
, z(b ) 1 = z(b )
2 f2 D r2 = r2 r1 + r2
A1,b = r1 [ 1 − r1]2b−1 1 − [ 1 − r1]2b ,
, f1 = |S1| , f2 = |S2|
C1,b = A1,b
+ A2,b
,
C2,b = A1,b
+ A2,b r1 r1 + r2 r2 r1 + r2
,
A2,b = r2 [ 1 − r2]2b−1 1 − [ 1 − r2]2b .(cid:164 ) r1 r1 + r2
( cid:179 )
( cid:180 )
This ( approximate ) formula ( 3 ) is remarkably accurate , even for very small D ; see Figure 1 in [ 25 ] . We can then estimate Pb ( and R ) from k independent permutations :
ˆRb =
ˆPb − C1,b 1 − C2,b
,
Var
ˆPb
Var [ 1 − C2,b]2 =
[ C1,b + ( 1 − C2,b)R ] [ 1 − C1,b − ( 1 − C2,b)R ]
[ 1 − C2,b]2
1 k
( 4 )
( cid:179 )
( cid:180 )
ˆRb
=
It turns out that our method only needs ˆPb for linear learning , ie , no need to explicitly estimate R .
2
3 Kernels from Minwise Hashing b Bit Minwise Hashing Definition : A symmetric n× n matrix K satisfying ij cicjKij ≥ 0 , for all real vectors c is called positive definite ( PD ) . Note that here we do not differentiate PD from nonnegative definite . Theorem 2 Consider n sets S1 , , Sn ⊆ Ω = {0 , 1 , , D − 1} . Apply one permutation π to each set . Define zi = min{π(Si)} and z(b ) the lowest b bits of zi . The following three matrices are PD . 1 . The resemblance matrix R ∈ Rn×n , whose ( i , j) th entry is the resemblance between set i
Si and set Sj : Rij = |Si∩Sj|
|Si∪Sj| =
|Si∩Sj|
|Si|+|Sj|−|Si∩Sj| .
2 . The minwise hashing matrix M ∈ Rn×n : Mij = 1{zi = zj} . 3 . The b bit minwise hashing matrix M(b ) ∈ Rn×n : M ( b ) ij = 1 z(b ) i = z(b ) j
.
Consequently , consider k independent permutations and denote M(b ) matrix generated by the s th permutation . Then the summation
( s ) the b bit minwise hashing s=1 M(b )
( s ) is also PD .
Proof : A matrix A is PD if it can be written as an inner product BTB . Because k
D−1 t=0
Mij = 1{zi = zj} =
1{zi = t} × 1{zj = t} ,
( 5 )
2b−1 t=0 1{z(b ) ij = i = t} × 1{z(b )
Mij is the inner product of two D dim vectors . Thus , M is PD . Similarly , M(b ) is PD because j = t} . R is PD because Rij = Pr{Mij = 1} = E ( Mij ) and M ( b ) Mij is the ( i , j) th element of the PD matrix M . Note that the expectation is a linear operation . ( cid:164 ) 4 Integrating b Bit Minwise Hashing with ( Linear ) Learning Algorithms Linear algorithms such as linear SVM and logistic regression have become very powerful and extremely popular . Representative software packages include SVMperf [ 20 ] , Pegasos [ 31 ] , Bottou ’s i=1 , xi ∈ RD , yi ∈ {−1 , 1} . The SGD SVM [ 5 ] , and LIBLINEAR [ 13 ] . Given a dataset {(xi , yi)}n
L2 regularized linear SVM solves the following optimization problem ) : ( cid:180 ) and the L2 regularized logistic regression solves a similar problem :
1 − yiwTxi , 0 n n i=1
( cid:179 ) wTw + C min w max
( 6 )
1 2
, min w
1 2 wTw + C i=1 log
1 + e
−yiwTxi
.
( 7 )
Here C > 0 is a regularization parameter . Since our purpose is to demonstrate the effectiveness of our proposed scheme using b bit hashing , we simply provide results for a wide range of C values and assume that the best performance is achievable if we conduct cross validations . In our approach , we apply k random permutations on each feature vector xi and store the lowest b bits of each hashed value . This way , we obtain a new dataset which can be stored using merely nbk bits . At run time , we expand each new data point into a 2b × k length vector with exactly k 1 ’s . For example , suppose k = 3 and the hashed values are originally {12013 , 25964 , 20191} , whose binary digits are {010111011101101 , 110010101101100 , 100111011011111} . Consider b = 2 . Then the binary digits are stored as {01 , 00 , 11} ( which corresponds to {1 , 0 , 3} in decimals ) . At run time , we need to expand them into a vector of length 2bk = 12 , to be {0 , 0 , 1 , 0 , 0 , 0 , 0 , 1 , 1 , 0 , 0 , 0} , which will be the new feature vector fed to a solver such as LIBLINEAR . Clearly , this expansion is directly inspired by the proof that the b bit minwise hashing matrix is PD in Theorem 2 . 5 Experimental Results on Webspam Dataset Our experiment settings closely follow the work in [ 35 ] . They conducted experiments on three datasets , of which only the webspam dataset is public and reasonably high dimensional ( n = 350000 , D = 16609143 ) . Therefore , our experiments focus on webspam . Following [ 35 ] , we randomly selected 20 % of samples for testing and used the remaining 80 % samples for training . We chose LIBLINEAR as the workhorse to demonstrate the effectiveness of our algorithm . All experiments were conducted on workstations with Xeon(R ) CPU ( W5590@3.33GHz ) and 48GB
3
RAM , under Windows 7 System . Thus , in our case , the original data ( about 24GB in LIBSVM format ) fit in memory . In applications when the data do not fit in memory , we expect that b bit hashing will be even more substantially advantageous , because the hashed data are relatively very small . In fact , our experimental results will show that for this dataset , using k = 200 and b = 8 can achieve similar testing accuracies as using the original data . The effective storage for the reduced dataset ( with 350K examples , using k = 200 and b = 8 ) would be merely about 70MB .
5.1 Experimental Results on Nonlinear ( Kernel ) SVM We implemented a new resemblance kernel function and tried to use LIBSVM to train an SVM using the webspam dataset . The training time well exceeded 24 hours . Fortunately , using b bit minswise hashing to estimate the resemblance kernels provides a substantial improvement . For example , with k = 150 , b = 4 , and C = 1 , the training time is about 5185 seconds and the testing accuracy is quite close to the best results given by LIBLINEAR on the original webspam data .
5.2 Experimental Results on Linear SVM There is an important tuning parameter C . To capture the best performance and ensure repeatability , we experimented with a wide range of C values ( from 10−3 to 102 ) with fine spacings in [ 0.1 , 10 ] . We experimented with k = 10 to k = 500 , and b = 1 , 2 , 4 , 6 , 8 , 10 , and 16 . Figure 1 ( average ) and Figure 2 ( std , standard deviation ) provide the test accuracies . Figure 1 demonstrates that using b ≥ 8 and k ≥ 200 achieves similar test accuracies as using the original data . Since our method is randomized , we repeated every experiment 50 times . We report both the mean and std values . Figure 2 illustrates that the stds are very small , especially with b ≥ 4 . In other words , our algorithm produces stable predictions . For this dataset , the best performances were usually achieved at C ≥ 1 .
Figure 1 : SVM test accuracy ( averaged over 50 repetitions ) . With k ≥ 200 and b ≥ 8 . b bit hashing achieves very similar accuracies as using the original data ( dashed , red if color is available ) .
Figure 2 : SVM test accuracy ( std ) . The standard deviations are computed from 50 repetitions . When b ≥ 8 , the standard deviations become extremely small ( eg , 002 % ) Compared with the original training time ( about 100 seconds ) , Figure 3 ( upper panels ) shows that our method only needs about 3 seconds ( near C = 1 ) . Note that our reported training time did not include data loading ( about 12 minutes for the original data and 10 seconds for the hashed data ) . Compared with the original testing time ( about 150 seconds ) , Figure 3 ( bottom panels ) shows that our method needs merely about 2 seconds . Note that the testing time includes both the data loading time , as designed by LIBLINEAR . The efficiency of testing may be very important in practice , for example , when the classifier is deployed in a user facing application ( such as search ) , while the cost of training or preprocessing may be less critical and can be conducted off line .
4
10−310−210−110010110280828486889092949698100CAccuracy ( %)b = 1b = 2b = 4b = 68b = 10,16svm : k = 30Spam : Accuracy10−310−210−110010110280828486889092949698100CAccuracy ( %)b = 1b = 2b = 4b = 6b = 8,10,16svm : k = 50Spam : Accuracy10−310−210−110010110280828486889092949698100CAccuracy ( %)Spam : Accuracysvm : k = 100b = 1b = 2b = 4b = 8,10,1646610−310−210−110010110280828486889092949698100CAccuracy ( %)svm : k = 150Spam : Accuracyb = 1b = 2b = 4b = 6,8,10,16410−310−210−110010110280828486889092949698100CAccuracy ( %)b = 6,8,10,16b = 1svm : k = 200b = 24b = 4Spam : Accuracy10−310−210−110010110280828486889092949698100CAccuracy ( %)Spam : Accuracysvm : k = 300b = 1b = 24b = 4b = 6,8,10,1610−310−210−110010110280828486889092949698100CAccuracy ( %)svm : k = 400Spam : Accuracyb = 1b = 24b = 4b = 6,8,10,1610−310−210−110010110280828486889092949698100b = 1b = 24b = 6,8,10,16CAccuracy ( %)svm : k = 500Spam : Accuracyb = 410−310−210−110010110210−210−1100CAccuracy ( std %)b = 1b = 2b = 4b = 6b = 8b = 1610svm : k = 50Spam accuracy ( std)10−310−210−110010110210−210−1100CAccuracy ( std %)b = 1b = 2b = 4b = 6b = 8b = 10,16svm : k = 100Spam accuracy ( std)10−310−210−110010110210−210−1100CAccuracy ( std %)b = 1b = 2b = 4b = 6b = 8,10,16svm : k = 200Spam accuracy ( std)10−310−210−110010110210−210−1100b = 1b = 2b = 4b = 6,8,10,16Spam accuracy ( std)CAccuracy ( std %)svm : k = 500 Figure 3 : SVM training time ( upper panels ) and testing time ( bottom panels ) . The original costs are plotted using dashed ( red , if color is available ) curves . 5.3 Experimental Results on Logistic Regression Figure 4 presents the test accuracies and training time using logistic regression . Again , with k ≥ 200 and b ≥ 8 , b bit minwise hashing can achieve similar test accuracies as using the original data . The training time is substantially reduced , from about 1000 seconds to about 30 seconds only .
Figure 4 : Logistic regression test accuracy ( upper panels ) and training time ( bottom panels ) . In summary , it appears b bit hashing is highly effective in reducing the data size and speeding up the training ( and testing ) , for both SVM and logistic regression . We notice that when using b = 16 , the training time can be much larger than using b ≤ 8 . Interestingly , we find that b bit hashing can be easily combined with Vowpal Wabbit ( VW ) [ 34 ] to further reduce the training time when b is large . 6 Random Projections , Count Min ( CM ) Sketch , and Vowpal Wabbit ( VW ) Random projections [ 1 , 24 ] , Count Min ( CM ) sketch [ 11 ] , and Vowpal Wabbit ( VW ) [ 32 , 34 ] , as popular hashing algorithms for estimating inner products for high dimensional datasets , are naturally applicable in large scale learning . In fact , those methods are not limited to binary data . Interestingly , the three methods all have essentially the same variances . Note that in this paper , we use ” VW “ particularly for the hashing algorithm in [ 34 ] , not the influential “ VW ” online learning platform . 6.1 Random Projections Denote the first two rows of a data matrix by u1 , u2 ∈ RD . The task is to estimate the inner product a = i=1 u1,iu2,i . The general idea is to multiply the data vectors by a random matrix {rij} ∈ RD×k , where rij is sampled iid from the following generic distribution with [ 24 ]
D
E(rij ) = 0 , V ar(rij ) = 1 , E(r3 ij ) = E(r4
( 8 ) ij ) = s − 1 ≥ 0 . This generates two k dim vectors , v1 and v2 : ij ) = 0 , E(r4 ij ) = s , s ≥ 1 .
Note that V ar(r2 v1,j = u1,irij , v2,j = u2,irij , j = 1 , 2 , , k
( 9 )
D ij ) − E2(r2
D i=1 i=1
5
10−310−210−1100101102100101102103CTraining time ( sec)svm : k = 50Spam : Training time10−310−210−1100101102100101102103CTraining time ( sec)svm : k =100Spam : Training time10−310−210−1100101102100101102103CTraining time ( sec)b = 16svm : k = 200Spam : Training time10−310−210−1100101102100101102103Spam : Training timeb = 10b = 16CTraining time ( sec)svm : k = 50010−310−210−110010110212101001000CTesting time ( sec)svm : k = 50Spam : Testing time10−310−210−110010110212101001000CTesting time ( sec)svm : k = 100Spam : Testing time10−310−210−110010110212101001000CTesting time ( sec)svm : k = 200Spam : Testing time10−310−210−110010110212101001000CTesting time ( sec)svm : k = 500Spam : Testing time10−310−210−110010110280828486889092949698100CAccuracy ( %)logit : k = 50Spam : Accuracyb = 1b = 2b = 4b = 6b = 8,10,1610−310−210−110010110280828486889092949698100CAccuracy ( %)logit : k = 100Spam : Accuracyb = 1b = 2b = 4b = 6b = 8,10,1610−310−210−110010110280828486889092949698100CAccuracy ( %)Spam : Accuracylogit : k = 200b = 6,8,10,16b = 1b = 2b = 410−310−210−110010110280828486889092949698100CAccuracy ( %)logit : k = 500Spam : Accuracyb = 1b = 2b = 44b = 6,8,10,1610−310−210−1100101102100101102103CTraining time ( sec)logit : k = 50Spam : Training time10−310−210−1100101102100101102103CTraining time ( sec)logit : k = 100Spam : Training time10−310−210−1100101102100101102103CTraining time ( sec)logit : k = 200Spam : Training time10−310−210−1100101102100101102103CTraining time ( sec)b = 16logit : k = 500Spam : Training time  1 D D i=1
The general family of distributions ( 8 ) includes the standard normal distribution ( in this case , s = 3 ) and the “ sparse projection ” distribution specified as rij =
√ s × with prob . 1 with prob . 1 − 1 2s 0 −1 with prob . 1 s
2s
[ 24 ] provided the following unbiased estimator ˆarp,s of a and the general variance formula : k j=1
ˆarp,s =
1 k v1,jv2,j ,
D
D
E(ˆarp,s ) = a = u1,iu2,i ,
( 10 )
( 11 )
V ar(ˆarp,s ) =
1 k u2 1,i
2,i + a2 + ( s − 3 ) u2 u2 1,iu2 2,i i=1 i=1 i=1 which means s = 1 achieves the smallest variance . The only elementary distribution we know that satisfies ( 8 ) with s = 1 is the two point distribution in {−1 , 1} with equal probabilities . [ 23 ] proposed an improved estimator for random projections as the solution to a cubic equation . Because it can not be written as an inner product , that estimator can not be used for linear learning . 6.2 Count Min ( CM ) Sketch and Vowpal Wabbit ( VW ) Again , in this paper , “ VW ” always refers to the hashing algorithm in [ 34 ] . VW may be viewed as a “ bias corrected ” version of the Count Min ( CM ) sketch [ 11 ] . In the original CM algorithm , the key step is to independently and uniformly hash elements of the data vectors to k buckets and the hashed value is the sum of the elements in the bucket . That is h(i ) = j with probability 1 k , where j ∈ {1 , 2 , , k} . By writing Iij =
, we can write the hashed data as
1 0 if h(i ) = j otherwise w1,j = u1,iIij , w2,j = u2,iIij
( 12 ) i=1 i=1
The estimate ˆacm = j=1 w1,jw2,j is ( severely ) biased for estimating inner products . The original paper [ 11 ] suggested a “ count min ” step for positive data , by generating multiple independent estimates ˆacm and taking the minimum as the final estimate . That step can reduce but can not remove the bias . Note that the bias can be easily removed by using k . k−1
D
D
ˆacm − 1 i=1 u1,i i=1 u2,i
( cid:179 )
( cid:180 ) k
[ 34 ] proposed a creative method for bias correction , which consists of pre multiplying ( elementwise ) the original data vectors with a random vector whose entries are sampled iid from the twopoint distribution in {−1 , 1} with equal probabilities . Here , we consider the general distribution ( 8 ) . After applying multiplication and hashing on u1 and u2 , the resultant vectors g1 and g2 are k
D g1,j = u1,iriIij , g2,j = u2,iriIij , j = 1 , 2 , , k
( 13 ) i=1 i=1 where E(ri ) = 0 , E(r2 i ) = 1 , E(r3 i ) = 0 , E(r4 i ) = s . We have the following Lemma .
( cid:189 ) D
D g1,jg2,j ,
E(ˆavw,s ) = u1,iu2,i = a ,
V ar(ˆavw,s ) = ( s − 1 ) u2 1,iu2
2,i + u2 1,i
2,i + a2 − 2 u2
D i=1 u2 1,iu2 2,i
( cid:164 )
D i=1
( 14 )
( 15 ) i=1 u2
Interestingly , the variance ( 15 ) says we do need s = 1 , otherwise the additional term ( s − 2,i will not vanish even as the sample size k → ∞ . In other words , the choice of 1 ) random distribution in VW is essentially the only option if we want to remove the bias by premultiplying the data vectors ( element wise ) with a vector of random variables . Of course , once we let s = 1 , the variance ( 15 ) becomes identical to the variance of random projections ( 11 ) .
1,iu2
Theorem 3
ˆavw,s = k j=1
D
D
D i=1
1 k
D
D i=1 i=1
6
7 Comparing b Bit Minwise Hashing with VW ( and Random Projections ) We implemented VW and experimented it on the same webspam dataset . Figure 5 shows that b bit minwise hashing is substantially more accurate ( at the same sample size k ) and requires significantly less training time ( to achieve the same accuracy ) . Basically , for 8 bit minwise hashing with k = 200 achieves similar test accuracies as VW with k = 104 ∼ 106 ( note that we only stored the non zeros ) .
Figure 5 : The dashed ( red if color is available ) curves represent b bit minwise hashing results ( only for k ≤ 500 ) while solid curves for VW . We display results for C = 0.01 , 0.1 , 1 , 10 , 100 .
( cid:162 )
( cid:161)− c k k
( cid:161 )
1 − 1
( cid:162)c ≈ exp
This empirical finding is not surprising , because the variance of b bit hashing is usually substantially smaller than the variance of VW ( and random projections ) . In the technical report ( arXiv:1106.0967 , which also includes the complete proofs of the theorems presented in this paper ) , we show that , at the same storage cost , b bit hashing usually improves VW by 10 to 100 fold , by assuming each sample of VW needs 32 bits to store . Of course , even if VW only stores each sample using 16 bits , an improvement of 5 to 50 fold would still be very substantial . There is one interesting issue here . Unlike random projections ( and minwise hashing ) , VW is a sparsity preserving algorithm , meaning that in the resultant sample vector of length k , the number of non zeros will not exceed the number of non zeros in the original vector . In fact , it is easy to see that the fraction of zeros in the resultant vector would be ( at least ) , where c is the number of non zeros in the original data vector . In this paper , we mainly focus on the scenario in which c ( cid:192 ) k , ie , we use b bit minwise hashing or VW for the purpose of data reduction . However , in some cases , we care about c ( cid:191 ) k , because VW is also an excellent tool for compact indexing . In fact , our b bit minwise hashing scheme for linear learning may face such an issue . 8 Combining b Bit Minwise Hashing with VW In Figures 3 and 4 , when b = 16 , the training time becomes substantially larger than b ≤ 8 . Recall that in the run time , we expand the b bit minwise hashed data to sparse binary vectors of length 2bk with exactly k 1 ’s . When b = 16 , the vectors are very sparse . On the other hand , once we have expanded the vectors , the task is merely computing inner products , for which we can use VW . Therefore , in the run time , after we have generated the sparse binary vectors of length 2bk , we hash them using VW with sample size m ( to differentiate from k ) . How large should m be ? Theorem 4 may provide an insight . Recall Section 2 provides the estimator , denoted by ˆRb , of the resemblance R , using b bit minwise hashing . Now , suppose we first apply VW hashing with size m on the binary vector of length 2bk before estimating R , which will introduce some additional randomness . We denote the new estimator by ˆRb,vw . Theorem 4 provides its theoretical variance .
Figure 6 : We apply VW hashing on top of the binary vectors ( of length 2bk ) generated by b bit hashing , with size m = 20k , 21k , 22k , 23k , 28k , for k = 200 and b = 16 . The numbers on the solid curves ( 0 , 1 , 2 , 3 , 8 ) are the exponents . The dashed ( red if color if available ) curves are the results from only using b bit hashing . When m = 28k , this method achieves similar test accuracies ( left panels ) while substantially reducing the training time ( right panels ) .
7
10110210310410510680828486889092949698100C = 0011,10,10001kAccuracy ( %)svm : VW vs b = 8 hashingC = 0.01C = 0.1C = 110,100Spam : Accuracy10110210310410510680828486889092949698100kAccuracy ( %)1C = 0.01C = 0.1C = 110C = 0.01C = 0.110,100logit : VW vs b = 8 hashingSpam : Accuracy100102103104105106100101102103kTraining time ( sec)C = 100C = 10C = 1,01,001C = 100C = 10C = 1,01,001Spam : Training timesvm : VW vs b = 8 hashing102103104105106100101102103kTraining time ( sec)C = 00110,10,01100logit : VW vs b = 8 hashingSpam : Training timeC = 01,001C = 100,10,110−310−210−110010110285909510001238CAccuracy ( %)SVM : 16−bit hashing + VW , k = 200Spam:Accuracy10−310−210−1100101102859095100CAccuracy ( %)12380Logit : 16−bit hashing +VW , k = 200Spam : Accuracy10−310−210−1100101102100101102018CTraining time ( sec)Spam:Training TimeSVM : 16−bit hashing + VW , k = 2001283010−310−210−1100101102100101102CTraining time ( sec)18Logit : 16−bit hashing +VW , k = 200Spam : Training Time800 ( cid:180 )
ˆRb,vw
= V ar
( cid:180 )
( cid:179 )
ˆRb
+
1 m
( cid:179 ) ( cid:180 )
Theorem 4 where V ar
( cid:179 )
Var
ˆRb
( cid:181 )
1 + P 2 b − Pb(1 + Pb ) k
( cid:182 )
,
( 16 )
1
[ 1 − C2,b]2
= 1 k
Pb(1−Pb ) [ 1−C2,b]2 is given by ( 4 ) and C2,b is the constant defined in Theorem 1 . ( cid:164 )
( cid:179 )
( cid:180 )
ˆRb
Compared to the original variance V ar , the additional term in ( 16 ) can be relatively large , if m is small . Therefore , we should choose m ( cid:192 ) k and m ( cid:191 ) 2bk . If b = 16 , then m = 28k may be a good trade off . Figure 8 provides an empirical study to verify this intuition . 9 Limitations While using b bit minwise hashing for training linear algorithms is successful on the webspam dataset , it is important to understand the following three major limitations of the algorithm : ( A ) : Our method is designed for binary ( 0/1 ) sparse data . ( B ) : Our method requires an expensive preprocessing step for generating k permutations of the data . For most applications , we expect the preprocessing cost is not a major issue because the preprocessing can be conducted off line ( or combined with the data collection step ) and is easily parallelizable . However , even if the speed is not a concern , the energy consumption might be an issue , especially considering ( b bit ) minwise hashing is mainly used for industry applications . In addition , testing an new unprocessed data vector ( eg , a new document ) will be expensive . ( C ) : Our method performs only reasonably well in terms of dimension reduction . The processed data need to be mapped into binary vectors in 2b × k dimensions , which is usually not small . ( Note that the storage cost is just bk bits . ) For example , for the webspam dataset , using b = 8 and k = 200 seems to suffice and 28 × 200 = 51200 is quite large , although it is much smaller than the original dimension of 16 million . It would be desirable if we can further reduce the dimension , because the dimension determines the storage cost of the model and ( moderately ) increases the training time for batch learning algorithms such as LIBLINEAR . In hopes of fixing the above limitations , we experimented with an implementation using another hashing technique named Conditional Random Sampling ( CRS ) [ 21 , 22 ] , which is not limited to binary data and requires only one permutation of the original data ( ie , no expensive preprocessing ) . We achieved some limited success . For example , CRS compares favorably to VW in terms of storage ( to achieve the same accuracy ) on the webspam dataset . However , so far CRS can not compete with b bit minwise hashing for linear learning ( in terms of training speed , storage cost , and model size ) . The reason is because even though the estimator of CRS is an inner product , the normalization factors ( i.e , the effective sample size of CRS ) to ensure unbiased estimates substantially differ pairwise ( which is a significant advantage in other applications ) . In our implementation , we could not use fully correct normalization factors , which lead to severe bias of the inner product estimates and less than satisfactory performance of linear learning compared to b bit minwise hashing . 10 Conclusion As data sizes continue to grow faster than the memory and computational power , statistical learning tasks in industrial practice are increasingly faced with training datasets that exceed the resources on a single server . A number of approaches have been proposed that address this by either scaling out the training process or partitioning the data , but both solutions can be expensive . In this paper , we propose a compact representation of sparse , binary data sets based on b bit minwise hashing , which can be naturally integrated with linear learning algorithms such as linear SVM and logistic regression , leading to dramatic improvements in training time and/or resource requirements . We also compare b bit minwise hashing with the Count Min ( CM ) sketch and Vowpal Wabbit ( VW ) algorithms , which , according to our analysis , all have ( essentially ) the same variances as random projections [ 24 ] . Our theoretical and empirical comparisons illustrate that b bit minwise hashing is significantly more accurate ( at the same storage ) for binary data . There are various limitations ( eg , expensive preprocessing ) in our proposed method , leaving ample room for future research . Acknowledgement This work is supported by NSF ( DMS 0808864 ) , ONR ( YIP N000140910911 ) , and a grant from Microsoft . We thank John Langford and Tong Zhang for helping us better understand the VW hashing algorithm , and Chih Jen Lin for his patient explanation of LIBLINEAR package and datasets .
8
References [ 1 ] Dimitris Achlioptas . Database friendly random projections : Johnson Lindenstrauss with binary coins . Journal of Computer and System
Sciences , 66(4):671–687 , 2003 .
[ 2 ] Alexandr Andoni and Piotr Indyk . Near optimal hashing algorithms for approximate nearest neighbor in high dimensions . In Commun .
ACM , volume 51 , pages 117–122 , 2008 .
[ 3 ] Harald Baayen . Word Frequency Distributions , volume 18 of Text , Speech and Language Technology . Kulver Academic Publishers ,
2001 .
[ 4 ] Michael Bendersky and W . Bruce Croft . Finding text reuse on the web . In WSDM , pages 262–271 , Barcelona , Spain , 2009 . [ 5 ] Leon Bottou . http://leonbottouorg/projects/sgd [ 6 ] Andrei Z . Broder . On the resemblance and containment of documents . In the Compression and Complexity of Sequences , pages 21–29 ,
Positano , Italy , 1997 .
[ 7 ] Andrei Z . Broder , Steven C . Glassman , Mark S . Manasse , and Geoffrey Zweig . Syntactic clustering of the web . In WWW , pages 1157 –
1166 , Santa Clara , CA , 1997 .
[ 8 ] Olivier Chapelle , Patrick Haffner , and Vladimir N . Vapnik . Support vector machines for histogram based image classification . IEEE
Trans . Neural Networks , 10(5):1055–1064 , 1999 .
[ 9 ] Ludmila Cherkasova , Kave Eshghi , Charles B . Morrey III , Joseph Tucek , and Alistair C . Veitch . Applying syntactic similarity algorithms for enterprise information management . In KDD , pages 1087–1096 , Paris , France , 2009 .
[ 10 ] Flavio Chierichetti , Ravi Kumar , Silvio Lattanzi , Michael Mitzenmacher , Alessandro Panconesi , and Prabhakar Raghavan . On com pressing social networks . In KDD , pages 219–228 , Paris , France , 2009 .
[ 11 ] Graham Cormode and S . Muthukrishnan . An improved data stream summary : the count min sketch and its applications . Journal of
Algorithm , 55(1):58–75 , 2005 .
[ 12 ] Yon Dourisboure , Filippo Geraci , and Marco Pellegrini . Extraction and classification of dense implicit communities in the web graph .
ACM Trans . Web , 3(2):1–36 , 2009 .
[ 13 ] Rong En Fan , Kai Wei Chang , Cho Jui Hsieh , Xiang Rui Wang , and Chih Jen Lin . Liblinear : A library for large linear classification .
Journal of Machine Learning Research , 9:1871–1874 , 2008 .
[ 14 ] Dennis Fetterly , Mark Manasse , Marc Najork , and Janet L . Wiener . A large scale study of the evolution of web pages . In WWW , pages
669–678 , Budapest , Hungary , 2003 .
[ 15 ] George Forman , Kave Eshghi , and Jaap Suermondt . Efficient detection of large scale redundancy in enterprise file systems . SIGOPS
Oper . Syst . Rev . , 43(1):84–91 , 2009 .
[ 16 ] Sreenivas Gollapudi and Aneesh Sharma . An axiomatic approach for result diversification . In WWW , pages 381–390 , Madrid , Spain ,
2009 .
[ 17 ] Matthias Hein and Olivier Bousquet . Hilbertian metrics and positive definite kernels on probability measures .
136–143 , Barbados , 2005 .
In AISTATS , pages
[ 18 ] Cho Jui Hsieh , Kai Wei Chang , Chih Jen Lin , S . Sathiya Keerthi , and S . Sundararajan . A dual coordinate descent method for large scale linear svm . In Proceedings of the 25th international conference on Machine learning , ICML , pages 408–415 , 2008 .
[ 19 ] Yugang Jiang , Chongwah Ngo , and Jun Yang . Towards optimal bag of features for object categorization and semantic video retrieval . In
CIVR , pages 494–501 , Amsterdam , Netherlands , 2007 .
[ 20 ] Thorsten Joachims . Training linear svms in linear time . In KDD , pages 217–226 , Pittsburgh , PA , 2006 . [ 21 ] Ping Li and Kenneth W . Church . Using sketches to estimate associations . In HLT/EMNLP , pages 708–715 , Vancouver , BC , Canada ,
2005 ( The full paper appeared in Commputational Linguistics in 2007 ) .
[ 22 ] Ping Li , Kenneth W . Church , and Trevor J . Hastie . Conditional random sampling : A sketch based sampling technique for sparse data . In
NIPS , pages 873–880 , Vancouver , BC , Canada , 2006 ( Newer results appeared in NIPS 2008 .
[ 23 ] Ping Li , Trevor J . Hastie , and Kenneth W . Church . Improving random projections using marginal information . In COLT , pages 635–649 ,
Pittsburgh , PA , 2006 .
[ 24 ] Ping Li , Trevor J . Hastie , and Kenneth W . Church . Very sparse random projections . In KDD , pages 287–296 , Philadelphia , PA , 2006 . [ 25 ] Ping Li and Arnd Christian K¨onig . Theory and applications b bit minwise hashing . In Commun . ACM , 2011 . [ 26 ] Ping Li and Arnd Christian K¨onig . Accurate estimators for improving minwise hashing and b bit minwise hashing . Technical report ,
2011 ( arXiv:11080895 )
[ 27 ] Ping Li and Arnd Christian K¨onig . b bit minwise hashing . In WWW , pages 671–680 , Raleigh , NC , 2010 . [ 28 ] Ping Li , Arnd Christian K¨onig , and Wenhao Gui . b bit minwise hashing for estimating three way similarities . In NIPS , Vancouver , BC ,
2010 .
[ 29 ] Gurmeet Singh Manku , Arvind Jain , and Anish Das Sarma . Detecting Near Duplicates for Web Crawling . In WWW , Banff , Alberta ,
Canada , 2007 .
[ 30 ] Marc Najork , Sreenivas Gollapudi , and Rina Panigrahy . Less is more : sampling the neighborhood graph makes salsa better and faster .
In WSDM , pages 242–251 , Barcelona , Spain , 2009 .
[ 31 ] Shai Shalev Shwartz , Yoram Singer , and Nathan Srebro . Pegasos : Primal estimated sub gradient solver for svm .
807–814 , Corvalis , Oregon , 2007 .
In ICML , pages
[ 32 ] Qinfeng Shi , James Petterson , Gideon Dror , John Langford , Alex Smola , and SVN Vishwanathan . Hash kernels for structured data . practical large scale machine learning system .
Journal of Machine Learning Research , 10:2615–2637 , 2009 . developing
Lessons learned
[ 33 ] Simon
Tong . a http://googleresearchblogspotcom/2010/04/lessons learned developing practicalhtml , 2008 .
[ 34 ] Kilian Weinberger , Anirban Dasgupta , John Langford , Alex Smola , and Josh Attenberg . Feature hashing for large scale multitask learning . In ICML , pages 1113–1120 , 2009 .
[ 35 ] Hsiang Fu Yu , Cho Jui Hsieh , Kai Wei Chang , and Chih Jen Lin . Large linear classification when data cannot fit in memory . In KDD , pages 833–842 , 2010 .
9
One Permutation Hashing
Department of Statistical Science
Department of Statistics
Ping Li
Cornell University
Art B Owen
Stanford University Abstract
Cun Hui Zhang
Department of Statistics
Rutgers University
Minwise hashing is a standard procedure in the context of search , for efficiently estimating set similarities in massive binary data such as text . Recently , b bit minwise hashing has been applied to large scale learning and sublinear time nearneighbor search . The major drawback of minwise hashing is the expensive preprocessing , as the method requires applying ( eg , ) k = 200 to 500 permutations on the data . This paper presents a simple solution called one permutation hashing . Conceptually , given a binary data matrix , we permute the columns once and divide the permuted columns evenly into k bins ; and we store , for each data vector , the smallest nonzero location in each bin . The probability analysis illustrates that this one permutation scheme should perform similarly to the original ( k permutation ) minwise hashing . Our experiments with training SVM and logistic regression confirm that one permutation hashing can achieve similar ( or even better ) accuracies compared to the k permutation scheme . See more details in arXiv:12081259
1 Introduction Minwise hashing [ 4 , 3 ] is a standard technique in the context of search , for efficiently computing set similarities . Recently , b bit minwise hashing [ 18 , 19 ] , which stores only the lowest b bits of each hashed value , has been applied to sublinear time near neighbor search [ 22 ] and learning [ 16 ] , on large scale high dimensional binary data ( eg , text ) . A drawback of minwise hashing is that it requires a costly preprocessing step , for conducting ( eg , ) k = 200 ∼ 500 permutations on the data . 1.1 Massive High Dimensional Binary Data In the context of search , text data are often processed to be binary in extremely high dimensions . A standard procedure is to represent documents ( eg , Web pages ) using w shingles ( ie , w contiguous words ) , where w ≥ 5 in several studies [ 4 , 8 ] . This means the size of the dictionary needs to be substantially increased , from ( eg , ) 105 common English words to 105w “ super words ” . In current practice , it appears sufficient to set the total dimensionality to be D = 264 , for convenience . Text data generated by w shingles are often treated as binary . The concept of shingling can be naturally extended to Computer Vision , either at pixel level ( for aligned images ) or at Visual feature level [ 23 ] . In machine learning practice , the use of extremely high dimensional data has become common . For example , [ 24 ] discusses training datasets with ( on average ) n = 1011 items and D = 109 distinct features . [ 25 ] experimented with a dataset of potentially D = 16 trillion ( 1.6×1013 ) unique features . 1.2 Minwise Hashing and b Bit Minwise Hashing Minwise hashing was mainly designed for binary data . A binary ( 0/1 ) data vector can be viewed as a set ( locations of the nonzeros ) . Consider sets Si ⊆ Ω = {0 , 1 , 2 , , D − 1} , where D , the size of the space , is often set as D = 264 in industrial applications . The similarity between two sets , S1 and S2 , is commonly measured by the resemblance , which is a version of the normalized inner product :
R =
|S1 ∩ S2| |S1 ∪ S2| = a f1 + f2 − a where f1 = |S1| , f2 = |S2| , a = |S1 ∩ S2|
,
( 1 )
For large scale applications , the cost of computing resemblances exactly can be prohibitive in time , space , and energy consumption . The minwise hashing method was proposed for efficient computing resemblances . The method requires applying k independent random permutations on the data . Denote π a random permutation : π : Ω → Ω . The hashed values are the two minimums of π(S1 ) and π(S2 ) . The probability at which the two hashed values are equal is |S1 ∩ S2| |S1 ∪ S2| = R
Pr ( min(π(S1 ) ) = min(π(S2) ) ) =
( 2 )
1 k j=1
1 k
D−1
( cid:179 )
( cid:180 )
One can then estimate R from k independent permutations , π1 , , πk :
ˆRM =
1{min(πj(S1 ) ) = min(πj(S2))} ,
( 3 ) Because the indicator function 1{min(πj(S1 ) ) = min(πj(S2))} can be written as an inner product between two binary vectors ( each having only one 1 ) in D dimensions [ 16 ] :
R(1 − R )
Var
ˆRM
=
1 k
1{min(πj(S1 ) ) = min(πj(S2))} =
1{min(πj(S1 ) ) = i} × 1{min(πj(S2 ) ) = i}
( 4 ) i=0 we know that minwise hashing can be potentially used for training linear SVM and logistic regression on high dimensional binary data by converting the permuted data into a new data matrix in D × k dimensions . This of course would not be realistic if D = 264 . The method of b bit minwise hashing [ 18 , 19 ] provides a simple solution by storing only the lowest b bits of each hashed data , reducing the dimensionality of the ( expanded ) hashed data matrix to just 2b × k . [ 16 ] applied this idea to large scale learning on the webspam dataset and demonstrated that using b = 8 and k = 200 to 500 could achieve very similar accuracies as using the original data .
1.3 The Cost of Preprocessing and Testing Clearly , the preprocessing of minwise hashing can be very costly . In our experiments , loading the webspam dataset ( 350,000 samples , about 16 million features , and about 24GB in Libsvm/svmlight ( text ) format ) used in [ 16 ] took about 1000 seconds when the data were stored in text format , and took about 150 seconds after we converted the data into binary . In contrast , the preprocessing cost for k = 500 was about 6000 seconds . Note that , compared to industrial applications [ 24 ] , the webspam dataset is very small . For larger datasets , the preprocessing step will be much more expensive . In the testing phrase ( in search or learning ) , if a new data point ( eg , a new document or a new image ) has not been processed , then the total cost will be expensive if it includes the preprocessing . This may raise significant issues in user facing applications where the testing efficiency is crucial . Intuitively , the standard practice of minwise hashing ought to be very “ wasteful ” in that all the nonzero elements in one set are scanned ( permuted ) but only the smallest one will be used .
1.4 Our Proposal : One Permutation Hashing
Figure 1 : Consider S1 , S2 , S3 ⊆ Ω = {0 , 1 , , 15} ( ie , D = 16 ) . We apply one permutation π on the sets and present π(S1 ) , π(S2 ) , and π(S3 ) as binary ( 0/1 ) vectors , where π(S1 ) = {2 , 4 , 7 , 13} , π(S2 ) = {0 , 6 , 13} , and π(S3 ) = {0 , 1 , 10 , 12} . We divide the space Ω evenly into k = 4 bins , select the smallest nonzero in each bin , and re index the selected elements as : [ 2 , 0 , ∗ , 1 ] , [ 0 , 2 , ∗ , 1 ] , and [ 0 , ∗ , 2 , 0 ] . For now , we use ‘*’ for empty bins , which occur rarely unless the number of nonzeros is small compared to k .
As illustrated in Figure 1 , the idea of one permutation hashing is simple . We view sets as 0/1 vectors in D dimensions so that we can treat a collection of sets as a binary data matrix in D dimensions . After we permute the columns ( features ) of the data matrix , we divide the columns evenly into k parts ( bins ) and we simply take , for each data vector , the smallest nonzero element in each bin . In the example in Figure 1 ( which concerns 3 sets ) , the sample selected from π(S1 ) is [ 2 , 4,∗ , 13 ] , where we use ’*’ to denote an empty bin , for the time being . Since only want to compare elements with the same bin number ( so that we can obtain an inner product ) , we can actually re index the elements of each bin to use the smallest possible representations . For example , for π(S1 ) , after re indexing , the sample [ 2 , 4,∗ , 13 ] becomes [ 2 − 4 × 0 , 4 − 4 × 1,∗ , 13 − 4 × 3 ] = [ 2 , 0,∗ , 1 ] . We will show that empty bins occur rarely unless the total number of nonzeros for some set is small compared to k , and we will present strategies on how to deal with empty bins should they occur .
2
01234567891011121314150110011000101000000101000000000010000011100000001234π(S1):π(S2):π(S3 ) : 1.5 Advantages of One Permutation Hashing Reducing k ( eg , 500 ) permutations to just one permutation ( or a few ) is much more computationally efficient . From the perspective of energy consumption , this scheme is desirable , especially considering that minwise hashing is deployed in the search industry . Parallel solutions ( eg , GPU [ 17] ) , which require additional hardware and software implementation , will not be energy efficient . In the testing phase , if a new data point ( eg , a new document or a new image ) has to be first processed with k permutations , then the testing performance may not meet the demand in , for example , user facing applications such as search or interactive visual analytics . One permutation hashing should be easier to implement , from the perspective of random number generation . For example , if a dataset has one billion features ( D = 109 ) , we can simply generate a “ permutation vector ” of length D = 109 , the memory cost of which ( ie , 4GB ) is not significant . On the other hand , it would not be realistic to store a “ permutation matrix ” of size D× k if D = 109 and k = 500 ; instead , one usually has to resort to approximations such as universal hashing [ 5 ] . Universal hashing often works well in practice although theoretically there are always worst cases . One permutation hashing is a better matrix sparsification scheme . In terms of the original binary data matrix , the one permutation scheme simply makes many nonzero entries be zero , without further “ damaging ” the matrix . Using the k permutation scheme , we store , for each permutation and each row , only the first nonzero and make all the other nonzero entries be zero ; and then we have to concatenate k such data matrices . This significantly changes the structure of the original data matrix .
1.6 Related Work One of the authors worked on another “ one permutation ” scheme named Conditional Random Sampling ( CRS ) [ 13 , 14 ] since 2005 . Basically , CRS continuously takes the bottom k nonzeros after applying one permutation on the data , then it uses a simple “ trick ” to construct a random sample for each pair with the effective sample size determined at the estimation stage . By taking the nonzeros continuously , however , the samples are no longer “ aligned ” and hence we can not write the estimator as an inner product in a unified fashion . [ 16 ] commented that using CRS for linear learning does not produce as good results compared to using b bit minwise hashing . Interestingly , in the original “ minwise hashing ” paper [ 4 ] ( we use quotes because the scheme was not called “ minwise hashing ” at that time ) , only one permutation was used and a sample was the first k nonzeros after the permutation . Then they quickly moved to the k permutation minwise hashing scheme [ 3 ] . We are also inspired by the work on very sparse random projections [ 15 ] and very sparse stable random projections [ 12 ] . The regular random projection method also has the expensive preprocessing cost as it needs a large number of projections . [ 15 , 12 ] showed that one can substantially reduce the preprocessing cost by using an extremely sparse projection matrix . The preprocessing cost of very sparse random projections can be as small as merely doing one projection . See wwwstanfordedu/group/mmds/slides2012/s plipdf for the experimental results on clustering/classification/regression using very sparse random projections . This paper focuses on the “ fixed length ” scheme as shown in Figure 1 . The technical report ( arXiv:1208.1259 ) also describes a “ variable length ” scheme . Two schemes are more or less equivalent , although the fixed length scheme is more convenient to implement ( and it is slightly more accurate ) . The variable length hashing scheme is to some extent related to the Count Min ( CM ) sketch [ 6 ] and the Vowpal Wabbit ( VW ) [ 21 , 25 ] hashing algorithms . 2 Applications of Minwise Hashing on Efficient Search and Learning In this section , we will briefly review two important applications of the k permutation b bit minwise hashing : ( i ) sublinear time near neighbor search [ 22 ] , and ( ii ) large scale linear learning [ 16 ] . 2.1 Sublinear Time Near Neighbor Search The task of near neighbor search is to identify a set of data points which are “ most similar ” to a query data point . Developing efficient algorithms for near neighbor search has been an active research topic since the early days of modern computing ( e.g , [ 9] ) . In current practice , methods for approximate near neighbor search often fall into the general framework of Locality Sensitive Hashing ( LSH ) [ 10 , 1 ] . The performance of LSH largely depends on its underlying implementation . The idea in [ 22 ] is to directly use the bits from b bit minwise hashing to construct hash tables .
3
Specifically , we hash the data points using k random permutations and store each hash value using b bits . For each data point , we concatenate the resultant B = bk bits as a signature ( eg , bk = 16 ) . This way , we create a table of 2B buckets and each bucket stores the pointers of the data points whose signatures match the bucket number . In the testing phrase , we apply the same k permutations to a query data point to generate a bk bit signature and only search data points in the corresponding bucket . Since using only one table will likely miss many true near neighbors , as a remedy , we independently generate L tables . The query result is the union of data points retrieved in L tables .
Figure 2 : An example of hash tables , with b = 2 , k = 2 , and L = 2 .
Figure 2 provides an example with b = 2 bits , k = 2 permutations , and L = 2 tables . The size of each hash table is 24 . Given n data points , we apply k = 2 permutations and store b = 2 bits of each hashed value to generate n ( 4 bit ) signatures L times . Consider data point 6 . For Table 1 ( left panel of Figure 2 ) , the lowest b bits of its two hashed values are 00 and 00 and thus its signature is 0000 in binary ; hence we place a pointer to data point 6 in bucket number 0 . For Table 2 ( right panel of Figure 2 ) , we apply another k = 2 permutations . This time , the signature of data point 6 becomes 1111 in binary and hence we place it in the last bucket . Suppose in the testing phrase , the two ( 4 bit ) signatures of a new data point are 0000 and 1111 , respectively . We then only search for the near neighbors in the set {6 , 15 , 26 , 79 , 110 , 143} , instead of the original set of n data points . 2.2 Large Scale Linear Learning The recent development of highly efficient linear learning algorithms is a major breakthrough . Popular packages include SVMperf [ 11 ] , Pegasos [ 20 ] , Bottou ’s SGD SVM [ 2 ] , and LIBLINEAR [ 7 ] . i=1 , xi ∈ RD , yi ∈ {−1 , 1} , the L2 regularized logistic regression solves Given a dataset {(xi , yi)}n the following optimization problem ( where C > 0 is the regularization parameter ) :
( cid:179 ) ( cid:169 ) n n i=1 i=1
,
( cid:180 ) ( cid:170 )
( 5 )
( 6 )
, min w
1 2 wTw + C log
1 + e−yiwTxi
The L2 regularized linear SVM solves a similar problem : min w
1 2 wTw + C max
1 − yiwTxi , 0
In [ 16 ] , they apply k random permutations on each ( binary ) feature vector xi and store the lowest b bits of each hashed value , to obtain a new dataset which can be stored using merely nbk bits . At run time , each new data point has to be expanded into a 2b × k length vector with exactly k 1 ’s . To illustrate this simple procedure , [ 16 ] provided a toy example with k = 3 permutations . Suppose for one data vector , the hashed values are {12013 , 25964 , 20191} , whose binary digits are respectively {010111011101101 , 110010101101100 , 100111011011111} . Using b = 2 bits , the binary digits are stored as {01 , 00 , 11} ( which corresponds to {1 , 0 , 3} in decimals ) . At run time , the ( b bit ) hashed data are expanded into a new feature vector of length 2bk = 12 : {0 , 0 , 1 , 0 , 0 , 0 , 0 , 1 , 1 , 0 , 0 , 0} . The same procedure is then applied to all n feature vectors . Clearly , in both applications ( near neighbor search and linear learning ) , the hashed data have to be “ aligned ” in that only the hashed data generated from the same permutation are interacted . Note that , with our one permutation scheme as in Figure 1 , the hashed data are indeed aligned . 3 Theoretical Analysis of the One Permutation Scheme
This section presents the probability analysis to provide a rigorous foundation for one permutation hashing as illustrated in Figure 1 . Consider two sets S1 and S2 . We first introduce two definitions ,
4
00 1011 1011 1100 0000 01IndexData Points11 01(empty)6 , 110 , 143 3 , 38 , 217 5 , 14 , 20631 , 74 , 153 21 , 142 , 32900 1011 1011 1100 0000 01IndexData Points11 016,15 , 26 , 7933 , 4897 , 49 , 2083 , 14 , 32 , 9711 , 25 , 998 , 159 , 331 for the number of “ jointly empty bins ” and the number of “ matched bins , ” respectively : k k
Nemp =
Iemp,j ,
Nmat =
Imat,j j=1 j=1 where Iemp,j and Imat,j are defined for the j th bin , as if both π(S1 ) and π(S2 ) are empty in the j th bin otherwise if both π(S1 ) and π(S1 ) are not empty and the smallest element of π(S1 ) matches the smallest element of π(S2 ) , in the j th bin otherwise
( cid:189 ) 1
1 0
0
Iemp,j =
Imat,j =
, 0 ≤ j ≤ k − 1
( 10 )
( cid:162 ) − t
1 − j+s D − t k
Recall the notation : f1 = |S1| , f2 = |S2| , a = |S1 ∩ S2| . We also use f = |S1 ∪ S2| = f1 + f2 − a . Lemma 1
Pr ( Nemp = j ) = j!s!(k − j − s)!
( cid:161 )
Assume D
1 − 1 k s=0 k!
( −1)s k−j ( cid:162 ) ≥ f = f1 + f2 − a . ( cid:162 ) − j f−1 ( cid:182 ) ( cid:181 ) 1 − E ( Nemp )
1 − 1 D − j
= R
( cid:161 )
= k
( cid:161 )
D t=0 f−1 ( cid:182)f 1 − f−1
( cid:181 ) 1 − 1 k k k
E ( Nemp )
E ( Nmat )
( 11 )
( 12 )
Cov ( Nmat , Nemp ) ≤ 0
D j=0
≤
( cid:161 )
 ( cid:162)f is a good approximation to the true value of E(Nemp )
( cid:162 ) − j
1 − 1 D − j
= R j=0
D k k k
( cid:161 )
1 − 1
( cid:162)f ≈ ( cid:161 ) ( cid:162)f ≈ 00067 For practical applications , we would expect that f ( cid:192 ) k ( for most data pairs ) ,
( 13 ) In practical scenarios , the data are often sparse , ie , f = f1 + f2 − a ( cid:191 ) D . In this case , the upper bound ( 11 ) e−f /k , we know that the chance of empty bins is small when f ( cid:192 ) k . For example , if f /k = 5 then 1 − 1 otherwise hashing probably would not be too useful anyway . This is why we do not expect empty bins will significantly impact ( if at all ) the performance in practical settings . Lemma 2 shows the following estimator ˆRmat of the resemblance is unbiased : Lemma 2
1 − 1
. Since
( cid:161 ) k k k
( cid:164 )
( 7 )
( 8 )
( 9 )
( 14 )
( 15 )
( 16 )
( cid:180 )
( cid:179 ) ( cid:181 )
ˆRmat
( cid:181 )
( cid:182)(cid:181 )
= R
1 k − Nemp ≥
ˆRmat = Nmat k − Nemp
,
E
ˆRmat
= R(1 − R )
E
V ar
( cid:181 )
E
( cid:179 )
( cid:179 )
( cid:182 )
( cid:180 )
( cid:180 ) k−1 j=0
( cid:182 )
( cid:182 )
1 +
1 f − 1
− 1 f − 1
1 k − Nemp
=
Pr ( Nemp = j ) k − j
1 k − E(Nemp )
( cid:164 )
ˆRmat
( cid:180 ) = R may seem surprising as in general ratio estimators are not unbiased . The fact that E Note that k− Nemp > 0 , because we assume the original data vectors are not completely empty ( allzero ) . As expected , when k ( cid:191 ) f = f1 + f2 − a , Nemp is essentially zero and hence V ar ≈ R(1−R )
ˆRmat
( cid:179 )
( cid:179 )
( cid:180 )
. In fact , V ar
ˆRmat k is a bit smaller than R(1−R ) k
, especially for large k .
It is probably not surprising that our one permutation scheme ( slightly ) outperforms the original k permutation scheme ( at merely 1/k of the preprocessing cost ) , because one permutation hashing , which is “ sampling without replacement ” , provides a better strategy for matrix sparsification .
5
ˆR(0 ) mat =
Nmat k − N ( 1 ) emp k − N ( 2 ) emp k k emp =
4 Strategies for Dealing with Empty Bins In general , we expect that empty bins should not occur often because E(Nemp)/k ≈ e−f /k , which is very close to zero if f /k > 5 . ( Recall f = |S1 ∪ S2| . ) If the goal of using minwise hashing is for data reduction , ie , reducing the number of nonzeros , then we would expect that f ( cid:192 ) k anyway . Nevertheless , in applications where we need the estimators to be inner products , we need strategies to deal with empty bins in case they occur . Fortunately , we realize a ( in retrospect ) simple strategy which can be nicely integrated with linear learning algorithms and performs well . Figure 3 plots the histogram of the numbers of nonzeros in the webspam dataset , which has 350,000 samples . The average number of nonzeros is about 4000 which should be much larger than k ( eg , 500 ) for the hashing procedure . On the other hand , about 10 % ( or 2.8 % ) of the samples have < 500 ( or < 200 ) nonzeros . Thus , we must deal with empty bins if we do not want to exclude those data points . For example , if f = k = 500 , then Nemp ≈ e−f /k = 0.3679 , which is not small . The strategy we recommend for linear learning is zero coding , which is tightly coupled with the strategy of hashed data expansion [ 16 ] as reviewed in Sec 22 More details will be elaborated in Sec 42 Basically , we can encode “ * ” as “ zero ” in the expanded space , which means Nmat will remain the same ( after taking the inner product in the expanded space ) . This strategy , which is sparsity preserving , essentially corresponds to the following modified estimator :
Figure 3 : Histogram of the numbers of nonzeros in the webspam dataset ( 350,000 samples ) .
( 17 ) j=1 I ( 2 ) k − N ( 2 ) k − N ( 1 ) emp emp = N ( 2 ) j=1 I ( 1 ) emp,j and N ( 2 ) emp = emp and N ( 2 ) emp,j are the numbers of empty bins in π(S1 ) where N ( 1 ) and π(S2 ) , respectively . This modified estimator makes sense for a number of reasons . Basically , since each data vector is processed and coded separately , we actually do not know Nemp ( the number of jointly empty bins ) until we see both π(S1 ) and π(S2 ) . In other words , we can not really compute Nemp if we want to use linear estimators . On the other hand , N ( 1 ) emp are always available . In fact , the use of emp in the denominator corresponds to the normalizing step which is needed before feeding the data to a solver for SVM or logistic regression . emp = Nemp , ( 17 ) is equivalent to the original ˆRmat . When two original vectors When N ( 1 ) are very similar ( eg , large R ) , N ( 1 ) emp will be close to Nemp . When two sets are highly unbalanced , using ( 17 ) will overestimate R ; however , in this case , Nmat will be so small that the absolute error will not be large . 4.1 The m Permutation Scheme with 1 < m ( cid:191 ) k If one would like to further ( significantly ) reduce the chance of the occurrences of empty bins , here we shall mention that one does not really have to strictly follow “ one permutation , ” since one can always conduct m permutations with k = k/m and concatenate the hashed data . Once the preprocessing is no longer the bottleneck , it matters less whether we use 1 permutation or ( eg , ) m = 3 permutations . The chance of having empty bins decreases exponentially with increasing m . emp and N ( 2 )
4.2 An Example of The “ Zero Coding ” Strategy for Linear Learning Sec 2.2 reviewed the data expansion strategy used by [ 16 ] for integrating b bit minwise hashing with linear learning . We will adopt a similar strategy with modifications for considering empty bins . We use a similar example as in Sec 22 Suppose we apply our one permutation hashing scheme and use k = 4 bins . For the first data vector , the hashed values are [ 12013 , 25964 , 20191 , ∗ ] ( ie , the 4 th bin is empty ) . Suppose again we use b = 2 bits . With the “ zero coding ” strategy , our procedure
6
020004000600080001000001234x 104# nonzerosFrequencyWebspam is summarized as follows : Original hashed values ( k = 4 ) : Original binary representations : Lowest b = 2 binary digits : Expanded 2b = 4 binary digits :
New feature vector fed to a solver :
12013 010111011101101 01 0010
25964 110010101101100 00 0001
20191 100111011011111 11 1000
1√ 4 − 1
× [ 0 , 0 , 1 , 0 , 0 , 0 , 0 , 1 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ]
∗ ∗ ∗ 0000
We apply the same procedure to all feature vectors in the data matrix to generate a new data matrix . The normalization factor varies , depending on the number of empty bins in the i th vector .
1 k−N
( i ) emp
5 Experimental Results on the Webspam Dataset The webspam dataset has 350,000 samples and 16,609,143 features . Each feature vector has on average about 4000 nonzeros ; see Figure 3 . Following [ 16 ] , we use 80 % of samples for training and the remaining 20 % for testing . We conduct extensive experiments on linear SVM and logistic regression , using our proposed one permutation hashing scheme with k ∈ {26 , 27 , 28 , 29} and b ∈ {1 , 2 , 4 , 6 , 8} . For convenience , we use D = 224 = 16 , 777 , 216 , which is divisible by k . There is one regularization parameter C in linear SVM and logistic regression . Since our purpose is to demonstrate the effectiveness of our proposed hashing scheme , we simply provide the results for a wide range of C values and assume that the best performance is achievable if we conduct cross validations . This way , interested readers may be able to easily reproduce our experiments . Figure 4 presents the test accuracies for both linear SVM ( upper panels ) and logistic regression ( bottom panels ) . Clearly , when k = 512 ( or even 256 ) and b = 8 , b bit one permutation hashing achieves similar test accuracies as using the original data . Also , compared to the original k permutation scheme as in [ 16 ] , our one permutation scheme achieves similar ( or even slightly better ) accuracies .
Figure 4 : Test accuracies of SVM ( upper panels ) and logistic regression ( bottom panels ) , averaged over 50 repetitions . The accuracies of using the original data are plotted as dashed ( red , if color is available ) curves with “ diamond ” markers . C is the regularization parameter . Compared with the original k permutation minwise hashing ( dashed and blue if color is available ) , the one permutation hashing scheme achieves similar accuracies , or even slightly better accuracies when k is large .
The empirical results on the webspam datasets are encouraging because they verify that our proposed one permutation hashing scheme performs as well as ( or even slightly better than ) the original kpermutation scheme , at merely 1/k of the original preprocessing cost . On the other hand , it would be more interesting , from the perspective of testing the robustness of our algorithm , to conduct experiments on a dataset ( eg , news20 ) where the empty bins will occur much more frequently . 6 Experimental Results on the News20 Dataset The news20 dataset ( with 20,000 samples and 1,355,191 features ) is a very small dataset in not toohigh dimensions . The average number of nonzeros per feature vector is about 500 , which is also small . Therefore , this is more like a contrived example and we use it just to verify that our one permutation scheme ( with the zero coding strategy ) still works very well even when we let k be
7
10−310−210−110010110280828486889092949698100CAccuracy ( % ) b = 1b = 2b = 4b = 6b = 8SVM : k = 64Webspam : Accuracy10−310−210−110010110280828486889092949698100CAccuracy ( % ) b = 1b = 2b = 4b = 6,8SVM : k = 128Webspam : AccuracyOriginal1 Permk Perm10−310−210−110010110280828486889092949698100CAccuracy ( % ) b = 1b = 2b = 4b = 6,8SVM : k = 256Webspam : AccuracyOriginal1 Permk Perm10−310−210−110010110280828486889092949698100CAccuracy ( % ) b = 1b = 2b = 4,6,8SVM : k = 512Webspam : AccuracyOriginal1 Permk Perm10−310−210−110010110280828486889092949698100CAccuracy ( % ) b = 1b = 2b = 4b = 6b = 8logit : k = 64Webspam : Accuracy10−310−210−110010110280828486889092949698100CAccuracy ( % ) b = 1b = 2b = 4b = 6,8logit : k = 128Webspam : Accuracy10−310−210−110010110280828486889092949698100CAccuracy ( % ) b = 1b = 2b = 4b = 6,8logit : k = 256Webspam : AccuracyOriginal1 Permk Perm10−310−210−110010110280828486889092949698100CAccuracy ( % ) b = 1b = 2b = 4,6,8logit : k = 512Webspam : AccuracyOriginal1 Permk Perm as large as 4096 ( ie , most of the bins are empty ) . In fact , the one permutation schemes achieves noticeably better accuracies than the original k permutation scheme . We believe this is because the one permutation scheme is “ sample without replacement ” and provides a better matrix sparsification strategy without “ contaminating ” the original data matrix too much . We experiment with k ∈ {25 , 26 , 27 , 28 , 29 , 210 , 211 , 212} and b ∈ {1 , 2 , 4 , 6 , 8} , for both one permutation scheme and k permutation scheme . We use 10,000 samples for training and the other 10,000 samples for testing . For convenience , we let D = 221 ( which is larger than 1,355,191 ) . Figure 5 and Figure 6 present the test accuracies for linear SVM and logistic regression , respectively . When k is small ( eg , k ≤ 64 ) both the one permutation scheme and the original k permutation scheme perform similarly . For larger k values ( especially as k ≥ 256 ) , however , our one permutation scheme noticeably outperforms the k permutation scheme . Using the original data , the test accuracies are about 98 % . Our one permutation scheme with k ≥ 512 and b = 8 essentially achieves the original test accuracies , while the k permutation scheme could only reach about 975 %
Figure 5 : Test accuracies of linear SVM averaged over 100 repetitions . The one permutation scheme noticeably outperforms the original k permutation scheme especially when k is not small .
Figure 6 : Test accuracies of logistic regression averaged over 100 repetitions . The one permutation scheme noticeably outperforms the original k permutation scheme especially when k is not small .
7 Conclusion A new hashing algorithm is developed for large scale search and learning in massive binary data . Compared with the original k permutation ( eg , k = 500 ) minwise hashing ( which is a standard procedure in the context of search ) , our method requires only one permutation and can achieve similar or even better accuracies at merely 1/k of the original preprocessing cost . We expect that one permutation hashing ( or its variant ) will be adopted in practice . See more details in arXiv:12081259 Acknowledgement : The research of Ping Li is partially supported by NSF IIS 1249316 , NSFDMS 0808864 , NSF SES 1131848 , and ONR YIP N000140910911 . The research of Art B Owen is partially supported by NSF 0906056 . The research of Cun Hui Zhang is partially supported by NSF DMS 0906420 , NSF DMS 1106753 , NSF DMS 1209014 , and NSA H98230 11 1 0205 .
8
10−110010110210350556065707580859095100CAccuracy ( % ) b = 1b = 2b = 4b = 6b = 8SVM : k = 32News20 : Accuracy10−110010110210350556065707580859095100CAccuracy ( % ) b = 1b = 2b = 4b = 6b = 8SVM : k = 64News20 : Accuracy10−110010110210350556065707580859095100CAccuracy ( % ) b = 1b = 2b = 4b = 6b = 8SVM : k = 128News20 : Accuracy10−110010110210365707580859095100CAccuracy ( % ) b = 1b = 2b = 4b = 6b = 8SVM : k = 256News20 : Accuracy10−110010110210365707580859095100CAccuracy ( % ) b = 1b = 2b = 4b = 6b = 8SVM : k = 512News20 : Accuracy10−110010110210365707580859095100CAccuracy ( % ) b = 1b = 2b = 4b = 6,8SVM : k = 1024News20 : Accuracy10−110010110210365707580859095100CAccuracy ( % ) b = 1b = 2b = 4b = 6,8SVM : k = 2048News20 : AccuracyOriginal1 Permk Perm10−110010110210365707580859095100CAccuracy ( % ) b = 1b = 2b = 4,6,8SVM : k = 4096News20 : AccuracyOriginal1 Permk Perm10−110010110210350556065707580859095100CAccuracy ( % ) b = 1b = 2b = 4b = 6b = 8logit : k = 32News20 : Accuracy10−110010110210350556065707580859095100CAccuracy ( % ) b = 1b = 2b = 4b = 6b = 8logit : k = 64News20 : Accuracy10−110010110210350556065707580859095100CAccuracy ( % ) b = 1b = 2b = 4b = 6b = 8logit : k = 128News20 : Accuracy10−110010110210365707580859095100CAccuracy ( % ) b = 1b = 2b = 4b = 6b = 8logit : k = 256News20 : Accuracy10−110010110210365707580859095100CAccuracy ( % ) b = 1b = 2b = 4b = 6b = 8logit : k = 512News20 : Accuracy10−110010110210365707580859095100CAccuracy ( % ) b = 1b = 2b = 4b = 6,8logit : k = 1024News20 : Accuracy10−110010110210365707580859095100CAccuracy ( % ) b = 1b = 2b = 4b = 6,8logit : k = 2048News20 : AccuracyOriginal1 Permk Perm10−110010110210365707580859095100CAccuracy ( % ) b = 1b = 2b = 4,6,8logit : k = 4096News20 : AccuracyOriginal1 Permk Perm References [ 1 ] Alexandr Andoni and Piotr Indyk . Near optimal hashing algorithms for approximate nearest neighbor in high dimensions . In Commun . ACM , volume 51 , pages 117–122 , 2008 .
[ 2 ] Leon Bottou . http://leonbottouorg/projects/sgd [ 3 ] Andrei Z . Broder , Moses Charikar , Alan M . Frieze , and Michael Mitzenmacher . Min wise independent permutations ( extended abstract ) . In STOC , pages 327–336 , Dallas , TX , 1998 .
[ 4 ] Andrei Z . Broder , Steven C . Glassman , Mark S . Manasse , and Geoffrey Zweig . Syntactic clustering of the web . In WWW , pages 1157 – 1166 , Santa Clara , CA , 1997 .
[ 5 ] J . Lawrence Carter and Mark N . Wegman . Universal classes of hash functions ( extended abstract ) . In
STOC , pages 106–112 , 1977 .
[ 6 ] Graham Cormode and S . Muthukrishnan . An improved data stream summary : the count min sketch and its applications . Journal of Algorithm , 55(1):58–75 , 2005 .
[ 7 ] Rong En Fan , Kai Wei Chang , Cho Jui Hsieh , Xiang Rui Wang , and Chih Jen Lin . Liblinear : A library for large linear classification . Journal of Machine Learning Research , 9:1871–1874 , 2008 .
[ 8 ] Dennis Fetterly , Mark Manasse , Marc Najork , and Janet L . Wiener . A large scale study of the evolution of web pages . In WWW , pages 669–678 , Budapest , Hungary , 2003 .
[ 9 ] Jerome H . Friedman , F . Baskett , and L . Shustek . An algorithm for finding nearest neighbors .
Transactions on Computers , 24:1000–1006 , 1975 .
IEEE
[ 10 ] Piotr Indyk and Rajeev Motwani . Approximate nearest neighbors : Towards removing the curse of dimen sionality . In STOC , pages 604–613 , Dallas , TX , 1998 .
[ 11 ] Thorsten Joachims . Training linear svms in linear time . In KDD , pages 217–226 , Pittsburgh , PA , 2006 . [ 12 ] Ping Li . Very sparse stable random projections for dimension reduction in lα ( 0 < α ≤ 2 ) norm . In
KDD , San Jose , CA , 2007 .
[ 13 ] Ping Li and Kenneth W . Church . Using sketches to estimate associations . In HLT/EMNLP , pages 708–
715 , Vancouver , BC , Canada , 2005 ( The full paper appeared in Commputational Linguistics in 2007 ) .
[ 14 ] Ping Li , Kenneth W . Church , and Trevor J . Hastie . One sketch for all : Theory and applications of In NIPS , Vancouver , BC , Canada , 2008 ( Preliminary results appeared conditional random sampling . in NIPS 2006 ) .
[ 15 ] Ping Li , Trevor J . Hastie , and Kenneth W . Church . Very sparse random projections .
287–296 , Philadelphia , PA , 2006 .
In KDD , pages
[ 16 ] Ping Li , Anshumali Shrivastava , Joshua Moore , and Arnd Christian K¨onig . Hashing algorithms for large scale learning . In NIPS , Granada , Spain , 2011 .
[ 17 ] Ping Li , Anshumali Shrivastava , and Arnd Christian K¨onig . b bit minwise hashing in practice : Largescale batch and online learning and using GPUs for fast preprocessing with simple hash functions . Technical report .
[ 18 ] Ping Li and Arnd Christian K¨onig . b bit minwise hashing . In WWW , pages 671–680 , Raleigh , NC , 2010 . [ 19 ] Ping Li , Arnd Christian K¨onig , and Wenhao Gui . b bit minwise hashing for estimating three way simi larities . In NIPS , Vancouver , BC , 2010 .
[ 20 ] Shai Shalev Shwartz , Yoram Singer , and Nathan Srebro . Pegasos : Primal estimated sub gradient solver for svm . In ICML , pages 807–814 , Corvalis , Oregon , 2007 .
[ 21 ] Qinfeng Shi , James Petterson , Gideon Dror , John Langford , Alex Smola , and SVN Vishwanathan . Hash kernels for structured data . Journal of Machine Learning Research , 10:2615–2637 , 2009 .
[ 22 ] Anshumali Shrivastava and Ping Li . Fast near neighbor search in high dimensional binary data . In ECML ,
2012 .
[ 23 ] Josef Sivic and Andrew Zisserman . Video google : a text retrieval approach to object matching in videos .
In ICCV , 2003 . [ 24 ] Simon Tong . http://googleresearchblogspotcom/2010/04/lessons learned developing practicalhtml , 2008 .
Lessons learned developing a practical large scale machine learning system .
[ 25 ] Kilian Weinberger , Anirban Dasgupta , John Langford , Alex Smola , and Josh Attenberg . Feature hashing for large scale multitask learning . In ICML , pages 1113–1120 , 2009 .
9
2 1 0 2 g u A 6
]
G L . s c [
3 0 9 7 2 5 0 / t i m b u s : v i X r a
One Permutation Hashing for Efficient Search and Learning
Ping Li
Dept . of Statistical Science
Cornell University Ithaca , NY 14853 pingli@cornell.edu
Art Owen
Dept . of Statistics Stanford University Stanford , CA 94305 owen@stanford.edu
Cun Hui Zhang Dept . of Statistics Rutgers University
New Brunswick , NJ 08901 czhang@statrutgersedu
Abstract
Minwise hashing is a standard procedure in the context of search , for efficiently estimating set similarities in massive binary data such as text . Recently , the method of b bit minwise hashing has been applied to large scale linear learning ( eg , linear SVM or logistic regression ) and sublinear time near neighbor search . The major drawback of minwise hashing is the expensive preprocessing cost , as the method requires applying ( eg , ) k = 200 to 500 permutations on the data . The testing time can also be expensive if a new data point ( eg , a new document or image ) has not been processed , which might be a significant issue in user facing applications . While it is true that the preprocessing step can be parallelized , it comes at the cost of additional hardware & implementation and is not an energy efficient solution .
We develop a very simple solution based on one permutation hashing . Conceptually , given a massive binary data matrix , we permute the columns only once and divide the permuted columns evenly into k bins ; and we simply store , for each data vector , the smallest nonzero location in each bin . The interesting probability analysis ( which is validated by experiments ) reveals that our one permutation scheme should perform very similarly to the original ( k permutation ) minwise hashing . In fact , the one permutation scheme can be even slightly more accurate , due to the “ sample without replacement ” effect .
Our experiments with training linear SVM and logistic regression on the webspam dataset demonstrate that this one permutation hashing scheme can achieve the same ( or even slightly better ) accuracies compared to the original k permutation scheme . To test the robustness of our method , we also experiment with the small news20 dataset which is very sparse and has merely on average 500 nonzeros in each data vector . Interestingly , our one permutation scheme noticeably outperforms the k permutation scheme when k is not too small on the news20 dataset . In summary , our method can achieve at least the same accuracy as the original k permutation scheme , at merely 1/k of the original preprocessing cost .
1 Introduction
Minwise hashing [ 4 , 3 ] is a standard technique for efficiently computing set similarities , especially in the context of search . Recently , b bit minwise hashing [ 17 ] , which stores only the lowest b bits of each hashed value , has been applied to sublinear time near neighbor search [ 21 ] and linear learning ( linear SVM and logistic regression ) [ 18 ] , on large scale high dimensional binary data ( eg , text ) , which are common in practice . The major drawback of minwise hashing and b bit minwise hashing is that they require an expensive preprocessing step , by conducting k ( eg , 200 to 500 ) permutations on the entire dataset .
1.1 Massive High Dimensional Binary Data
In the context of search , text data are often processed to be binary in extremely high dimensions . A standard procedure is to represent documents ( eg , Web pages ) using w shingles ( ie , w contiguous words ) , where
1 w ≥ 5 in several studies [ 4 , 8 ] . This means the size of the dictionary needs to be substantially increased , from ( eg , ) 105 common English words to 105w “ super words ” . In current practice , it seems sufficient to set the total dimensionality to be D = 264 , for convenience . Text data generated by w shingles are often treated as binary . In fact , for w ≥ 3 , it is expected that most of the w shingles will occur at most one time in a document . Also , note that the idea of shingling can be naturally extended to images in Computer Vision , either at the pixel level ( for simple aligned images ) or at the Vision feature level [ 22 ] .
In machine learning practice , the use of extremely high dimensional data has become common . For example , [ 23 ] discusses training datasets with ( on average ) n = 1011 items and D = 109 distinct features . [ 24 ] experimented with a dataset of potentially D = 16 trillion ( 1.6 × 1013 ) unique features . 1.2 Minwise Hashing
Minwise hashing is mainly designed for binary data . A binary ( 0/1 ) data vector can be equivalently viewed as a set ( locations of the nonzeros ) . Consider sets Si ⊆ Ω = {0 , 1 , 2 , , D − 1} , where D , the size of the space , is often set to be D = 264 in industrial applications . The similarity between two sets S1 and S2 is commonly measured by the resemblance , which is a normalized version of the inner product :
R = |S1 ∩ S2| |S1 ∪ S2|
= a f1 + f2 − a
, where f1 = |S1| , f2 = |S2| , a = |S1 ∩ S2|
( 1 )
For large scale applications , the cost of computing resemblances exactly can be prohibitive in time , space , and energy consumption . The minwise hashing method was proposed for efficient computing resemblances . The method requires applying k independent random permutations on the data .
Denote π a random permutation : π : Ω → Ω . The hashed values are the two minimums of the sets after applying the permutation π on S1 and S2 . The probability at which the two hashed values are equal is
Pr ( min(π(S1 ) ) = min(π(S2) ) ) = |S1 ∩ S2| |S1 ∪ S2|
One can then estimate R from k independent permutations , π1 , , πk :
= R
( 2 )
( 3 )
ˆRM =
1 k k
Xj=1
1{min(πj(S1 ) ) = min(πj(S2))} ,
Var ˆRM =
1 k
R(1 − R )
Because the indicator function 1{min(πj(S1 ) ) = min(πj(S2))} can be written as an inner product between two binary vectors ( each having only one 1 ) in D dimensions [ 18 ] :
D−1
Xi=0
1{min(πj(S1 ) ) = min(πj(S2))} =
1{min(πj(S1 ) ) = i} × 1{min(πj(S2 ) ) = i}
( 4 ) we know that minwise hashing can be potentially used for training linear SVM and logistic regression on high dimensional binary data by converting the permuted data into a new data matrix in D × k dimensions . This of course would not be realistic if D = 264 . The method of b bit minwise hashing [ 17 ] provides a simple solution by storing only the lowest b bits of each hashed data . This way , the dimensionality of the expanded data matrix from the hashed data would be only 2b × k as opposed to 264 × k . [ 18 ] applied this idea to large scale learning on the webspam dataset ( with about 16 million features ) and demonstrated that using b = 8 and k = 200 to 500 could achieve very similar accuracies as using the original data . More recently , [ 21 ] directly used the bits generated by b bit minwise hashing for building hash tables to achieve sublinear time near neighbor search . We will briefly review these two important applications in Sec 2 . Note that both applications require the hashed data to be “ aligned ” in that only the hashed data generated by the same permutation are interacted . For example , when computing the inner products , we simply concatenate the results from k permutations .
2
1.3 The Cost of Preprocessing and Testing
Clearly , the preprocessing step of minwise hashing can be very costly . For example , in our experiments , loading the webspam dataset ( 350,000 samples , about 16 million features , and about 24GB in Libsvm/svmlight format ) used in [ 18 ] took about 1000 seconds when the data are stored in Libsvm/svmlight ( text ) format , and took about 150 seconds after we converted the data into binary . In contrast , the preprocessing cost for k = 500 was about 6000 seconds ( which is ≫ 150 ) . Note that , compared to industrial applications [ 23 ] , the webspam dataset is very small . For larger datasets , the preprocessing step will be much more expensive . In the testing phrase ( in search or learning ) , if a new data point ( eg , a new document or a new image ) has not processed , then the cost will be expensive if it includes the preprocessing cost . This may raise significant issues in user facing applications where the testing efficiency is crucial .
Intuitively , the standard practice of minwise hashing ought to be very “ wasteful ” in that all the nonzero elements in one set are scanned ( permuted ) but only the smallest one will be used .
1.4 Our Proposal : One Permutation Hashing
As illustrated in Figure 1 , the idea of one permutation hashing is very simple . We view sets as 0/1 vectors in D dimensions so that we can treat a collection of sets as a binary data matrix in D dimensions . After we permute the columns ( features ) of the data matrix , we divide the columns evenly into k parts ( bins ) and we simply take , for each data vector , the smallest nonzero element in each bin .
1 2
1
0
0
0
1
0
1
1
0
0
1
2 6
0
1
0
3
4
5
0
1
0
1
0
0
0
0
0
3 10 11 12 13 14 15
4
7
8
9
1
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
1
1
1
0
0
0
0
0
0
0
π(S ) : 1 π(S ) : 2 π(S ) : 3
Figure 1 : Fixed length hashing scheme . Consider S1 , S2 , S3 ⊆ Ω = {0 , 1 , , 15} ( ie , D = 16 ) . We apply one permutation π on the three sets and present π(S1 ) , π(S2 ) , and π(S3 ) as binary ( 0/1 ) vectors , where π(S1 ) = {2 , 4 , 7 , 13} , π(S2 ) = {0 , 6 , 13} , and π(S3 ) = {0 , 1 , 10 , 12} . We divide the space Ω evenly into k = 4 bins , select the smallest nonzero in each bin , and re index the selected elements as three samples : [ 2 , 0 , ∗ , 1 ] , [ 0 , 2 , ∗ , 1 ] , and [ 0 , ∗ , 2 , 0 ] . For now , we use ‘*’ for empty bins , which occur rarely unless the number of nonzeros is small compared to k .
In the example in Figure 1 ( which concerns 3 sets ) , the sample selected from π(S1 ) is [ 2 , 4,∗ , 13 ] , where we use ’*’ to denote an empty bin , for the time being . Since only want to compare elements with the same bin number ( so that we can obtain an inner product ) , we can actually re index the elements of each bin to use the smallest possible representations . For example , for π(S1 ) , after re indexing , the sample [ 2 , 4,∗ , 13 ] becomes [ 2−4×0 , 4−4×1,∗ , 13−4×3 ] = [ 2 , 0,∗ , 1 ] . Similarly , for π(S2 ) , the original sample [ 0 , 6,∗ , 13 ] becomes [ 0 , 6 − 4 × 1,∗ , 13 − 4 × 3 ] = [ 0 , 2,∗ , 1 ] , etc . Note that , when there are no empty bins , similarity estimation is equivalent to computing an inner product , which is crucial for taking advantage of the modern linear learning algorithms [ 13 , 19 , 7 , 11 ] . We will show that empty bins occur rarely unless the total number of nonzeros for some set is small compared to k , and we will present strategies on how to deal with empty bins should they occur .
3
1.5 Summary of the Advantages of One Permutation Hashing
• Reducing k ( eg , 500 ) permutations to just one permutation ( or a few ) is much more computationally efficient . From the perspective of energy consumption , this scheme is highly desirable , especially considering that minwise hashing is deployed in the search industry .
• While it is true that the preprocessing can be parallelized , it comes at the cost of additional hardware and software implementation .
• In the testing phase , if a new data point ( eg , a new document or a new image ) has to be first processed with k permutations , then the testing performance may not meet the demand in for example userfacing applications such as search or interactive visual analytics .
• It should be much easier to implement the one permutation hashing than the original k permutation scheme , from the perspective of random number generation . For example , if a dataset has one billion features ( D = 109 ) , we can simply generate a “ permutation vector ” of length D = 109 , the memory cost of which ( ie , 4GB ) is not significant . On the other hand , it would not be realistic to store a “ permutation matrix ” of size D × k if D = 109 and k = 500 ; instead , one usually has to resort to approximations such as using universal hashing [ 5 ] to approximate permutations . Universal hashing often works well in practice although theoretically there are always worst cases . Of course , when D = 264 , we have to use universal hashing , but it is always much easier to generate just one permutation . • One permutation hashing is a better matrix sparsification scheme than the original k permutation . In terms of the original binary data matrix , the one permutation scheme simply makes many nonzero entries be zero , without further “ damaging ” the original data matrix . With the original k permutation scheme , we store , for each permutation and each row , only the first nonzero and make all the other nonzero entries be zero ; and then we have to concatenate k such data matrices . This will significantly change the structure of the original data matrix . As a consequence , we expect that our one permutation scheme will produce at least the same or even more accurate results , as later verified by experiments .
1.6 Related Work
One of the authors worked on another “ one permutation ” scheme named Conditional Random Sampling ( CRS ) [ 14 , 15 ] since 2005 . Basically , CRS works by continuously taking the first k nonzeros after applying one permutation on the data , then it uses a simple “ trick ” to construct a random sample for each pair with the effective sample size determined at the estimation stage . By taking the nonzeros continuously , however , the samples are no longer “ aligned ” and hence we can not write the estimator as an inner product in a unified fashion . In comparison , our new one permutation scheme works by first breaking the columns evenly into k bins and then taking the first nonzero in each bin , so that the hashed data can be nicely aligned .
Interestingly , in the original “ minwise hashing ” paper [ 4 ] ( we use quotes because the scheme was not called “ minwise hashing ” at that time ) , only one permutation was used and a sample was the first k nonzeros after the permutation . After the authors of [ 4 ] realized that the estimators could not be written as an inner product and hence the scheme was not suitable for many applications such as sublinear time near neighbor search using hash tables , they quickly moved to the k permutation minwise hashing scheme [ 3 ] . In the context of large scale linear learning , the importance of having estimators which are inner products should become more obvious after [ 18 ] introduced the idea of using ( b bit ) minwise hashing for linear learning .
We are also inspired by the work on “ very sparse random projections ” [ 16 ] . The regular random projection method also has the expensive preprocessing cost as it needs k projections . The work of [ 16 ] showed
4 that one can substantially reduce the preprocessing cost by using an extremely sparse projection matrix . The preprocessing cost of “ very sparse random projections ” can be as small as merely doing one projection.1
Figure 1 presents the “ fixed length ” scheme , while in Sec 7 we will also develop a “ variable length ” scheme . Two schemes are more or less equivalent , although we believe the fixed length scheme is more convenient to implement ( and it is slightly more accurate ) . The variable length hashing scheme is to some extent related to the Count Min ( CM ) sketch [ 6 ] and the Vowpal Wabbit ( VW ) [ 20 , 24 ] hashing algorithms .
2 Applications of Minwise Hashing on Efficient Search and Learning
In this section , we will briefly review two important applications of the original ( k permutation ) minwise hashing : ( i ) sublinear time near neighbor search [ 21 ] , and ( ii ) large scale linear learning [ 18 ] .
2.1 Sublinear Time Near Neighbor Search
The task of near neighbor search is to identify a set of data points which are “ most similar ” to a query data point . Efficient algorithms for near neighbor search have numerous applications in the context of search , databases , machine learning , recommending systems , computer vision , etc . It has been an active research topic since the early days of modern computing ( e.g , [ 9] ) .
In current practice , methods for approximate near neighbor search often fall into the general framework of Locality Sensitive Hashing ( LSH ) [ 12 , 1 ] . The performance of LSH solely depends on its underlying implementation . The idea in [ 21 ] is to directly use the bits generated by ( b bit ) minwise hashing to construct hash tables , which allow us to search near neighbors in sublinear time ( ie , no need to scan all data points ) . Specifically , we hash the data points using k random permutations and store each hash value using b bits ( eg , b ≤ 4 ) . For each data point , we concatenate the resultant B = b × k bits as a signature . The size of the space is 2B = 2b×k , which is not too large for small b and k ( eg , bk = 16 ) . This way , we create a table of 2B buckets , numbered from 0 to 2B − 1 ; and each bucket stores the pointers of the data points whose signatures match the bucket number . In the testing phrase , we apply the same k permutations to a query data point to generate a bk bit signature and only search data points in the corresponding bucket . Since using only one hash table will likely miss many true near neighbors , as a remedy , we generate ( using independent random permutations ) L hash tables . The query result is the union of the data points retrieved in L tables .
Index Data Points 00 00 00 01 00 10
6 , 110 , 143 3 , 38 , 217 ( empty )
Index Data Points 00 00 00 01 00 10
8 , 159 , 331 11 , 25 , 99 3 , 14 , 32 , 97
11 01 11 10 11 11
5 , 14 , 206 31 , 74 , 153 21 , 142 , 329
11 01 11 10 11 11
7 , 49 , 208 33 , 489 6 ,15 , 26 , 79
Figure 2 : An example of hash tables , with b = 2 , k = 2 , and L = 2 .
Figure 2 provides an example with b = 2 bits , k = 2 permutations , and L = 2 tables . The size of each hash table is 24 . Given n data points , we apply k = 2 permutations and store b = 2 bits of each hashed value to generate n ( 4 bit ) signatures L times . Consider data point 6 . For Table 1 ( left panel of Figure 2 ) , the lowest b bits of its two hashed values are 00 and 00 and thus its signature is 0000 in binary ; hence we
1See http://wwwstanfordedu/group/mmds/slides2012/s plipdf for the experimental results on cluster ing/classification/regression using very sparse random projections [ 16 ] .
5 place a pointer to data point 6 in bucket number 0 . For Table 2 ( right panel of Figure 2 ) , we apply another k = 2 permutations . This time , the signature of data point 6 becomes 1111 in binary and hence we place it in the last bucket . Suppose in the testing phrase , the two ( 4 bit ) signatures of a new data point are 0000 and 1111 , respectively . We then only search for the near neighbors in the set {6 , 15 , 26 , 79 , 110 , 143} , which is much smaller than the set of n data points .
The experiments in [ 21 ] confirmed that this very simple strategy performed well .
2.2 Large Scale Linear Learning
The recent development of highly efficient linear learning algorithms ( such as linear SVM and logistic regression ) is a major breakthrough in machine learning . Popular software packages include SVMperf [ 13 ] , Pegasos [ 19 ] , Bottou ’s SGD SVM [ 2 ] , and LIBLINEAR [ 7 ] .
Given a dataset {(xi , yi)}n following optimization problem : i=1 , xi ∈ RD , yi ∈ {−1 , 1} , the L2 regularized logistic regression solves the where C > 0 is the regularization parameter . The L2 regularized linear SVM solves a similar problem :
( 5 )
( 6 ) min w
1 2 wTw + C log1 + e−yiwTxi , min w
1 2 wTw + C max(1 − yiwTxi , 0 ) , n
Xi=1 n
Xi=1
To illustrate this simple procedure , [ 18 ] provided a toy example with k = 3 permutations . Suppose for
In their approach [ 18 ] , they apply k random permutations on each ( binary ) feature vector xi and store the lowest b bits of each hashed value , to obtain a new dataset which can be stored using merely nbk bits . At run time , each new data point has to be expanded into a 2b × k length vector with exactly k 1 ’s . one data vector , the hashed values are {12013 , 25964 , 20191} , whose binary digits are respectively {010111011101101 , 110010101101100 , 100111011011111} . Using b = 2 bits , the binary digits are stored as {01 , 00 , 11} ( which corresponds to {1 , 0 , 3} in decimals ) . At run time , the ( b bit ) hashed data are expanded into a vector of length 2bk = 12 , to be {0 , 0 , 1 , 0 , 0 , 0 , 0 , 1 , 1 , 0 , 0 , 0} , which will be the new feature vector fed to a solver such as LIBLINEAR . The procedure for this feature vector is summarized as follows : Original hashed values ( k = 3 ) : Original binary representations : Lowest b = 2 binary digits : Expanded 2b = 4 binary digits : New feature vector fed to a solver :
12013 010111011101101 01 0010
00 0001 [ 0 , 0 , 1 , 0 , 0 , 0 , 0 , 1 , 1 , 0 , 0 , 0 ] × 1√k
25964
20191 110010101101100 100111011011111 11 1000
The same procedure ( with the same k = 3 permutations ) is then applied to all n feature vectors . Very interestingly , we notice that the all zero vector ( 0000 in this example ) is never used when expanding the data . In our one permutation hashing scheme , we will actually take advantage of the all zero vector to conveniently encode empty bins , a strategy which we will later refer to as the “ zero coding ” strategy .
The experiments in [ 18 ] confirmed that this simple procedure performed well .
Clearly , in both applications ( near neighbor search and linear learning ) , the hashed data have to be “ aligned ” in that only the hashed data generated from the same permutation are compared with each other . With our one permutation scheme as presented in Figure 1 , the hashed data are indeed aligned according to the bin numbers . The only caveat is that we need a practical strategy to deal with empty bins , although they occur rarely unless the number of nonzeros in one data vector is small compared to k , the number of bins .
6
3 Theoretical Analysis of the Fixed Length One Permutation Scheme
While the one permutation hashing scheme , as demonstrated in Figure 1 , is intuitive , we present in this section some interesting probability analysis to provide a rigorous theoretical foundation for this method . Without loss of generality , we consider two sets S1 and S2 . We first introduce two definitions , for the number of “ jointly empty bins ” and the number of “ matched bins , ” respectively :
Nemp =
Iemp,j ,
Nmat = k
Xj=1 where Iemp,j and Imat,j are defined for the j th bin , as
Imat,j k
Xj=1
0 otherwise
Iemp,j =fl 1 if both π(S1 ) and π(S2 ) are empty in the j th bin Imat,j = 
0 otherwise matches the smallest element of π(S2 ) , in the j th bin
1 if both π(S1 ) and π(S1 ) are not empty and the smallest element of π(S1 )
Later we will also use I ( 1 ) emp,j ( or I ( 2 ) emp,j ) to indicate whether π(S1 ) ( or π(S2 ) ) is empty in the j th bin .
( 7 )
( 8 )
( 9 )
( 10 )
( 11 )
( 12 )
3.1 Expectation , Variance , and Distribution of the Number of Jointly Empty Bins Recall the notation : f1 = |S1| , f2 = |S2| , a = |S1 ∩ S2| . We also use f = |S1 ∪ S2| = f1 + f2 − a . Lemma 1 Assume D,1 − 1 k ≥ f = f1 + f2 − a ,
E ( Nemp ) k
= f−1
Yj=0
D,1 − 1 D − j k − j
≤1 −
1 kf
Assume D,1 − 2
V ar ( Nemp )
1 k
= k2 k ≥ f = f1 + f2 − a , k E(Nemp ) 1 − k  −1 − Yj=0   k E(Nemp ) 1 − f−1
< k
1
1
E(Nemp )
D,1 − 1 D − j E(Nemp ) k − j k k
Proof : See Appendix A . fi
2
 
− f−1
Yj=0
D,1 − 2 D − j k − j
 
The inequality ( 12 ) says that the variance of Nemp k is smaller than its “ binomial analog . ”
In practical scenarios , the data are often sparse , ie , f = f1 + f2 − a ≪ D . In this case , Lemma 2 illustrates that in ( 10 ) the upper bound,1 − 1 . Since k f ,1 − 1 ≈ e−f /k , we know that the chance of empty bins is small when f ≫ k . For example , if f /k = 5 k f then,1 − 1 ≈ 03679 For practical applications , we would expect that f ≫ k ( for most data pairs ) , otherwise hashing probably would not be too useful anyway . This is why we do not expect empty bins will significantly impact ( if at all ) the performance in practical settings . k f is a good approximation to the true value of E(Nemp ) k f ≈ 0.0067 ; if f /k = 1 , then,1 − 1 k
7
Lemma 2 Assume D,1 − 1
E ( Nemp ) k k ≥ f = f1 + f2 − a . =1 − kf
1 exp 
−D log D+1
D−f +1 + f1 −
1
2(D−f +1 ) k − 1
+  
Under the reasonable assumption that the data are sparse , ie , f1 + f2 − a = f ≪ D , we obtain
E ( Nemp ) k
V ar ( Nemp ) k2
Proof : See Appendix B . fi
1
1
1
=
=1 − k 1 − −1 − kf1 − O f 2 kD kf 1 −1 − kf! kf +1 1 − kf
1
1
1
−1 −
1 k − 1f! + O f 2 kD
In addition to its mean and variance , we can also write down the distribution of Nemp .
Lemma 3
Pr ( Nemp = j ) =
Proof : See Appendix C . fi k−j
Xs=0 ( −1)s k! j!s!(k − j − s)!
D1 − j+s D − t k − t f−1
Yt=0
Because E ( Nemp ) =Pk−1 D,1 − 1 k − j D − j f−1 k
Yj=0 j=0 jPr ( Nemp = j ) , this yields an interesting combinatorial identity :
= j k−1
Xj=0 k−j
Xs=0
( −1)s k! j!s!(k − j − s)!
D1 − j+s D − t k − t f−1
Yt=0
3.2 Expectation and Variance of the Number of Matched Bins
Lemma 4 Assume D,1 − 1
Assume D,1 − 2
V ar(Nmat ) k2
= R 1 − f−1
Yj=0
D,1 − 1 D − j k − j
 
E(Nmat )
1 k k k
=
E ( Nmat )
E ( Nemp ) k ≥ f = f1 + f2 − a . = R1 − k ≥ f = f1 + f2 − a . k E(Nmat ) 1 − f − 1 +1 − k R a − 1 1 − 2 k R2 −1 − Yj=0 1 − 1 − k E(Nmat ) f−1
< k k k
1
1
1
E(Nmat ) f−1
Yj=0 D,1 − 1 D − j
D,1 − 1 k − j D − j  k − j 
2
+ f−1
Yj=0
D,1 − 2 D − j k − j
 
Proof : See Appendix D . fi
8
( 13 )
( 14 )
( 15 )
( 16 )
( 17 )
( 18 )
( 19 )
( 20 )
3.3 Covariance of Nmat and Nemp Intuitively , Nmat and Nemp should be negatively correlated , as confirmed by the following Lemma :
Lemma 5 Assume D,1 − 2
Cov ( Nmat , Nemp ) k2 and
Proof : See Appendix E . fi f−1 f−1 k ≥ f = f1 + f2 − a .   =R D,1 − 1 D,1 − 1 k − j Yj=0 Yj=0    D − j D − j R k − j  f−1 f−1 D,1 − 2 k − j Yj=0 Yj=0 1 − −   D,1 − 1 Cov ( Nmat , Nemp ) ≤ 0
1 k k − j k − j  f−1 k − j Yj=0 − D,1 − 1 k − j D − j
D,1 − 2 D,1 − 1  
3.4 An Unbiased Estimator of R and the Variance
Lemma 6 shows the following estimator ˆRmat of the resemblance is unbiased : Lemma 6
ˆRmat =
Nmat k − Nemp
,
E ˆRmat = R
V ar ˆRmat = R(1 − R)E E k − j k − Nemp = k−1
1
Xj=0
Pr ( Nemp = j )
1 k − Nemp1 +
1 f − 1 −
1 f − 1
≥
1 k − E(Nemp )
( 21 )
( 22 )
( 23 )
( 24 )
( 25 )
Proof : See Appendix F . The right hand side of the inequality ( 25 ) is actually a very good approximation ( see Figure 8 ) . The exact expression for Pr ( Nemp = j ) is already derived in Lemma 3 . fi
The fact that E ˆRmat = R may seem surprising as in general ratio estimators are not unbiased . Note that k − Nemp > 0 always because we assume the original data vectors are not completely empty ( all zero ) . As expected , when k ≪ f = f1 + f2 − a , Nemp is essentially zero and hence V ar ˆRmat ≈ R(1−R ) . In fact , V ar ˆRmat is somewhat smaller than R(1−R ) V ar ˆRmat R(1 − R)/k ≈ g(f ; k ) =
, which can be seen from the approximation : f − 1 − f − 1
( 26 ) k
1
1 k k k f 1 +
1 −,1 − 1 g(f ; k ) ≤ 1
( 27 )
Lemma 7
Proof : See Appendix G . fi
It is probably not surprising that our one permutation scheme may ( slightly ) outperform the original k permutation scheme ( at merely 1/k of its preprocessing cost ) , because one permutation hashing can be viewed as a “ sample without replacement ” scheme .
9
3.5 Experiments for Validating the Theoretical Results
This set of experiments is for validating the theoretical results . The Web crawl dataset ( in Table 1 ) consists of 15 ( essentially randomly selected ) pairs of word vectors ( in D = 216 dimensions ) of a range of similarities and sparsities . For each word vector , the j th element is whether the word appeared in the j th Web page .
Table 1 : 15 pairs of English words . For example , “ RIGHTS ” and “ RESERVED ” correspond to the two sets of document IDs which contained word “ RIGHTS ” and word “ RESERVED ” respectively .
Word 1 RIGHTS OF THIS ALL CONTACT MAY CREDIT SEARCH RESEARCH UNIVERSITY FREE TOP BOOK TIME REVIEW A
Word 2 f1 12234 RESERVED 37339 AND 27695 HAVE MORE 26668 INFORMATION 16836 ONLY 12067 2999 CARD 1402 WEB 4353 12406 9151 5153 12386 3197 39063
USE BUSINESS TRAVEL JOB PAPER TEST f2 11272 36289 17522 17909 16339 11006 2697 12718 4241 11744 8284 4608 3263 1944 2278 f = f1 + f2 − a R 12526 41572 31647 31638 24974 17953 4433 21770 7017 19782 14992 8542 13874 4769 2060
0.877 0.771 0.429 0.409 0.328 0.285 0.285 0.229 0.225 0.221 0.163 0.143 0.128 0.078 0.052
We vary k from 23 to 215 . Although k = 215 is probably way too large in practice , we use it for the purpose of thorough validations . Figures 3 to 8 present the empirical results based on 105 repetitions .
351 E(Nemp ) and V ar(Nemp )
Figure 3 and Figure 4 respectively verify E(Nemp ) and V ar(Nemp ) as derived in Lemma 1 . Clearly , the theoretical curves overlap the empirical curves .
Note that Nemp is essentially 0 when k is not large . Roughly when k/f > 1/5 , the number of empty k f bins becomes noticeable , which is expected because E(Nemp)/k ≈ ,1 − 1 ≈ e−f /k and e−5 = 00067 Practically speaking , as we often use minwise hashing to substantially reduce the number of nonzeros in massive datasets , we would expect that usually f ≫ k anyway . See Sec 4 for more discussion about strategies for dealing with empty bins .
352 E(Nmat ) and V ar(Nmat )
Figure 5 and Figure 6 respectively verify E(Nmat ) and V ar(Nmat ) as derived in Lemma 4 . Again , the theoretical curves match the empirical ones and the curves start to change shapes at the point where the occurrences of empty bins are more noticeable .
10
0.1
0.08
0.06
0.04
0.02
RIGHTS−−RESERVED
Empirical Theoretical
) p m e
N E
(
) p m e
N E
(
0 101
102
103 k
104
105
Empirical Theoretical
MAY−−ONLY
) p m e
N E
(
0.1
0.08
0.06
0.04
0.02
) p m e
N E
(
Empirical Theoretical
0.1
0.08
0.06
0.04
0.02
OF−−AND
0 101
102
103 k
104
105
CREDIT−−CARD
Empirical Theoretical
0.1
0.08
0.06
0.04
0.02
) p m e
N E
(
0.1
0.08
0.06
0.04
0.02
0 101
0.1
0.08
0.06
0.04
0.02
) p m e
N E
(
0 101
102
103 k
104
105
0.1
0.08
0.06
0.04
0.02
) p m e
N E
(
Empirical Theoretical
TOP−−BUSINESS
0 101
102
103 k
104
105
0.1
0.08
0.06
0.04
0.02
) p m e
N E
(
Empirical Theoretical
BOOK−−TRAVEL
0.1
0.08
0.06
0.04
0.02
) p m e
N E
(
Empirical Theoretical
THIS−−HAVE
102
103 k
104
105
Empirical Theoretical
) p m e
N E
(
SEARCH−−WEB
0 101
102
103 k
104
105
Empirical Theoretical
TIME−−JOB
) p m e
N E
(
Empirical Theoretical
0.1
0.08
0.06
0.04
0.02
) p m e
N E
(
0.1
0.08
0.06
0.04
0.02
CONTACT−−INFORMATION
Empirical Theoretical
) p m e
N E
(
ALL−−MORE
0 101
102
103 k
104
105
0.1
0.08
0.06
0.04
0.02
RESEARCH−−UNIVERSITY
Empirical Theoretical
0 101
102
103 k
104
105
REVIEW−−PAPER
Empirical Theoretical
0.1
0.08
0.06
0.04
0.02
0.1
0.08
0.06
0.04
0.02
) p m e
N E
(
0.1
0.08
0.06
0.04
0.02
) p m e
N E
(
0 101
102
103 k
104
105
Empirical Theoretical
FREE−−USE
0 101
102
103 k
104
105
Empirical Theoretical
0 101
102
103 k
104
105
0 101
102
103 k
104
105
0 101
102
103 k
104
105
0 101
102
103 k
104
105
A−−TEST
0 101
102
103 k
104
105
Figure 3 : E(Nemp)/k . The empirical curves essentially overlap the theoretical curves as derived in Lemma 1 , ie , ( 10 ) . The occurrences of empty bins become noticeable only at relatively large sample size k .
100
10−100
Empirical Theoretical
) p m e
N
100
OF−−AND
100
THIS−−HAVE
100
100
10−100
10−100
) p m e
N
) p m e
N
10−100
Empirical Theoretical
) p m e
N
10−100
) p m e
N
Empirical Theoretical
( r a V
10−200
( r a V
10−200
( r a V
10−200
( r a V
10−200
10−300
101
100
10−100
RIGHTS−−RESERVED
102
103
104
105
Empirical Theoretical
) p m e
N
( r a V
10−200
10−300
101
100
10−100
MAY−−ONLY
102
103
104
105
Empirical Theoretical
) p m e
N
Empirical Theoretical
102
103
104
105
Empirical Theoretical
10−300
101
100
10−100
10−200
) p m e
N
( r a V
Empirical Theoretical
102
103
104
105
Empirical Theoretical
10−300
101
100
10−100
) p m e
N
) p m e
N
10−300
101
100
10−100
ALL−−MORE
102
103
104
105
Empirical Theoretical
( r a V
10−200
( r a V
10−200
CREDIT−−CARD
101
102
103 k
104
105
10−300
101
102
SEARCH−−WEB 103 105
104
10−300
101
100
10−100
Empirical Theoretical
) p m e
N
100
100
10−100
Empirical Theoretical
) p m e
N
10−100
) p m e
N
RESEARCH−−UNIVERSITY
102
103
104
105
Empirical Theoretical
( r a V
10−200
( r a V
10−200
( r a V
10−200
( r a V
10−200
10−300
TOP−−BUSINESS
10−300
BOOK−−TRAVEL
10−300
TIME−−JOB
( r a V
10−200
10−300
101
CONTACT−−INFORMATION 105
102
103
104
FREE−−USE
100
10−100
) p m e
N
( r a V
10−200
10−300
101
Empirical Theoretical
102
103
104
105
A−−TEST
100
10−100
) p m e
N
( r a V
10−200
10−300
REVIEW−−PAPER 103 105
104
Empirical Theoretical
103
104
105
104
101
102
103
102 Figure 4 : V ar(Nemp)/k2 . The empirical curves essentially overlap the theoretical curves as derived in Lemma 1 , ie , ( 11 ) .
101
104
105
101
103
105
102
103
105
101
102
101
104
102
11
Empirical Theoretical
1
0.8
0.6
0.4
0.2
) t a m
N E
(
) t a m
N E
(
RIGHTS−−RESERVED
102
103 k
104
105
Empirical Theoretical
) t a m
N E
(
MAY−−ONLY
0 101
102
103 k
104
105
Empirical Theoretical
) t a m
N E
(
0 101
TOP−−BUSINESS 103 k
102
104
105
0 101
1
0.8
0.6
0.4
0.2
) t a m
N E
(
1
0.8
0.6
0.4
0.2
) t a m
N E
(
1
0.8
0.6
0.4
0.2
OF−−AND
0 101
102
103 k
1
0.8
0.6
0.4
0.2
0 101
CREDIT−−CARD 103 k
102
1
0.8
0.6
0.4
0.2
0 101
BOOK−−TRAVEL 103 k
102
Empirical Theoretical
Empirical Theoretical
1
0.8
0.6
0.4
0.2
THIS−−HAVE
) t a m
N E
(
Empirical Theoretical
1
0.8
0.6
0.4
0.2
ALL−−MORE
) t a m
N E
(
Empirical Theoretical
1
0.8
0.6
0.4
0.2
) t a m
N E
(
CONTACT−−INFORMATION
104
105
0 101
102
104
105
0 101
102
103 k
103 k
104
105
Empirical Theoretical
102
103 k
104
105
Empirical Theoretical
0 101
1
0.8
0.6
0.4
0.2
) t a m
N E
(
1
0.8
0.6
0.4
0.2
) t a m
N E
(
Empirical Theoretical
104
105
Empirical Theoretical
1
0.8
0.6
0.4
0.2
0 101
SEARCH−−WEB 103 k
102
1
0.8
0.6
0.4
0.2
) t a m
N E
(
) t a m
N E
(
Empirical Theoretical
104
105
Empirical Theoretical
104
105
RESEARCH−−UNIVERSITY 104
102
0 101
103 k
Empirical Theoretical
1
0.8
0.6
0.4
0.2
REVIEW−−PAPER
) t a m
N E
(
105
FREE−−USE
0 101
102
103 k
104
105
A−−TEST
Empirical Theoretical
1
0.8
0.6
0.4
0.2
) t a m
N E
(
TIME−−JOB
0 101
102
103 k
104
105
0 101
102
104
105
0 101
102
103 k
103 k
104
105
Figure 5 : E(Nmat)/k . The empirical curves essentially overlap the theoretical curves as derived in Lemma 4 , ie , ( 18 ) .
100
10−2
) t a m
N
100
Empirical Theoretical
100
Empirical Theoretical
100
Empirical Theoretical
100
Empirical Theoretical
CONTACT−−INFORMATION
10−2
10−2
) t a m
N
) t a m
N
10−2
) t a m
N
10−2
) t a m
N
( r a V
10−4
( r a V
10−4
( r a V
10−4
( r a V
10−4
( r a V
10−4
10−6
101
RIGHTS−−RESERVED
102
103 k
104
105
10−6
101
OF−−AND
102
103 k
104
105
10−6
101
THIS−−HAVE
102
103 k
104
105
ALL−−MORE
10−6
101
100
10−2
) t a m
N
100
Empirical Theoretical
100
Empirical Theoretical
100
Empirical Theoretical
10−2
10−2
) t a m
N
) t a m
N
10−2
) t a m
N
102
103 k
104
105
RESEARCH−−UNIVERSITY
Empirical Theoretical
102
103 k
104
105
Empirical Theoretical
10−6
101
100
10−2
) t a m
N
( r a V
10−4
( r a V
10−4
( r a V
10−4
( r a V
10−4
( r a V
10−4
MAY−−ONLY
10−6
101
102
103 k
104
105
10−6
101
CREDIT−−CARD 103 k
102
104
105
10−6
101
SEARCH−−WEB 103 k
102
104
105
10−6
101
Empirical Theoretical
102
103 k
104
105
FREE−−USE
10−6
101
100
) t a m
N
( r a V
10−2
10−4
100
Empirical Theoretical
100
Empirical Theoretical
100
Empirical Theoretical
100
Empirical Theoretical
) t a m
N
( r a V
10−2
10−4
) t a m
N
( r a V
10−2
10−4
) t a m
N
( r a V
10−2
10−4
) t a m
N
( r a V
10−2
10−4
10−6
101
TOP−−BUSINESS 103 k
102
104
105
10−6
101
BOOK−−TRAVEL 103 k
102
104
105
10−6
101
TIME−−JOB
102
103 k
104
105
10−6
101
REVIEW−−PAPER 103 k
102
104
105
10−6
101
Figure 6 : V ar(Nmat)/k2 . The empirical curves essentially overlap the theoretical curves as derived in Lemma 4 , ie , ( 19 ) .
12
102
103 k
104
105
Empirical Theoretical
A−−TEST 102
103 k
104
105
353 Cov(Nemp , Nmat )
To verify Lemma 5 , Figure 7 presents the theoretical and empirical covariances of Nemp and Nmat . Note that Cov ( Nemp , Nmat ) ≤ 0 as shown in Lemma 5 . x 10−6 x 10−6 x 10−6 x 10−6
Empirical Theoretical
0
−2
−4
−6
−8
) t a m
N
, p m e
N ( v o C
RIGHTS−−RESERVED
−10
101
102
103 k
104
105
) t a m
N
, p m e
N ( v o C
Empirical Theoretical x 10−6
1
0
−1
OF−−AND
−2 101
102
103 k
104
105
) t a m
N
, p m e
N ( v o C
0
−2
−4
−6
−8
THIS−−HAVE
−10
101
102
103 k
Empirical Theoretical
104
105
) t a m
N
, p m e
N ( v o C
0
−2
−4
−6
−8
ALL−−MORE
−10
101
102
103 k
Empirical Theoretical
104
105
0
−2
−4
−6
−8
) t a m
N
, p m e
N ( v o C
−10
101 x 10−6 x 10−6 x 10−6 x 10−6 x 10−6
CONTACT−−INFORMATION
102
103 k
Empirical Theoretical
104
105
) t a m
N
, p m e
N ( v o C
0
−2
−4
−6
RESEARCH−−UNIVERSITY
−8
−10
101
Empirical Theoretical
104
105
102
103 k
0
−2
−4
−6
−8
) t a m
N
, p m e
N ( v o C
FREE−−USE
−10
101
102
103 k x 10−6 x 10−6
Empirical Theoretical
104
105
Empirical Theoretical
104
105
Empirical Theoretical
0
−2
−4
−6
−8
) t a m
N
, p m e
N ( v o C
REVIEW−−PAPER
0
−2
−4
−6
−8
) t a m
N
, p m e
N ( v o C
A−−TEST 102
−10
101
Empirical Theoretical
104
105
103 k
) t a m
N
, p m e
N ( v o C
0
−2
−4
−6
−8
MAY−−ONLY
−10
101
102
103 k
Empirical Theoretical
104
105
0
−2
−4
−6
−8
) t a m
N
, p m e
N ( v o C
−10
101
TOP−−BUSINESS
Empirical Theoretical
104
105
102
103 k
0
−2
−4
−6
−8
Empirical Theoretical
0
−2
−4
−6
−8
) t a m
N
, p m e
N ( v o C
) t a m
N
, p m e
N ( v o C
−10
101
CREDIT−−CARD 103 k
102
0
−2
−4
−6
−8
) t a m
N
, p m e
N ( v o C
−10
101
BOOK−−TRAVEL 103 k
102
104
105
Empirical Theoretical
104
105
−10
101
SEARCH−−WEB 103 k
102 x 10−6
1
0
−1
) t a m
N
, p m e
N ( v o C x 10−6 x 10−6
TIME−−JOB
−2 101
102
103 k
104
105
−10
101
Empirical Theoretical
104
105
102
103 k
Figure 7 : Cov(Nemp , Nmat)/k2 . The empirical curves essentially overlap the theoretical curves as derived in Lemma 5 , ie , ( 21 ) . The experimental results also confirm that the covariance is non positive as theoretically shown in Lemma 5 .
354 E( ˆRmat ) and V ar( ˆRmat ) Finally , Figure 8 plots the empirical MSEs ( MSE = bias2 + variance ) and the theoretical variances ( 24 ) ,
1 k−Nemp is approximated by where the term E The experimental results confirm Lemma 6 : ( i ) the estimator ˆRmat is unbiased ; ( ii ) the variance formula ( 24 ) and the approximation ( 25 ) are accurate ; ( iii ) the variance of ˆRmat is somewhat smaller than R(1 − R)/k , which is the variance of the original k permutation minwise hashing , due to the “ sample withoutreplacement ” effect . k−E(Nemp ) as in ( 25 ) .
1
Remark : The empirical results presented in Figures 3 to 8 have clearly validated the theoretical results for our one permutation hashing scheme . Note that we did not add the empirical results of the original k permutation minwise hashing scheme because they would simply overlap the theoretical curves . The fact that the original k permutation scheme provides the unbiased estimate of R with variance R(1−R ) has been well validated in prior literature , for example [ 17 ] . k
13
10−1
10−2
10−3
10−4
10−5
) t a m
(
R E S M
10−6
101
10−1
10−2
10−3
10−4
10−5
10−6
101
10−1
10−2
10−3
10−4
10−5
) t a m
(
R E S M
) t a m
(
R E S M
Empirical Theoretical Minwise
OF−−AND
102
103 k
104
105
Empirical Theoretical Minwise
Empirical Theoretical Minwise
RIGHTS−−RESERVED
102
103 k
104
105
Empirical Theoretical Minwise
MAY−−ONLY
102
103 k
104
105
10−1
10−2
10−3
10−4
10−5
) t a m
(
R E S M
10−1
10−2
10−3
10−4
10−5
) t a m
(
R E S M
10−6
101
10−6
101
CREDIT−−CARD 103 k
102
10−1
10−2
10−3
10−4
10−5
10−6
101
BOOK−−TRAVEL 103 k
102
104
105
Empirical Theoretical Minwise
104
105
Empirical Theoretical Minwise
104
105
) t a m
(
R E S M
10−6
101
TOP−−BUSINESS 103 k
102
) t a m
(
R E S M
10−1
10−2
10−3
10−4
10−5
10−6
101
Empirical Theoretical Minwise
THIS−−HAVE
102
103 k
104
105
Empirical Theoretical Minwise
104
105
Empirical Theoretical Minwise
10−1
10−2
10−3
10−4
10−5
) t a m
(
R E S M
10−6
101
SEARCH−−WEB 103 k
102
10−1
10−2
10−3
10−4
10−5
) t a m
(
R E S M
TIME−−JOB
10−6
101
102
103 k
104
105
10−1
10−2
10−3
10−4
10−5
) t a m
(
R E S M
10−1
10−2
10−3
10−4
10−5
) t a m
(
R E S M
10−6
101
10−1
10−2
10−3
10−4
10−5
) t a m
(
R E S M
Empirical Theoretical Minwise
ALL−−MORE
102
103 k
104
105
Empirical Theoretical Minwise
Empirical Theoretical Minwise
CONTACT−−INFORMATION
102
103 k
104
105
Empirical Theoretical Minwise
10−1
10−2
10−3
10−4
10−5
) t a m
(
R E S M
10−1
10−2
10−3
10−4
10−5
) t a m
(
R E S M
10−6
101
10−1
10−2
10−3
10−4
10−5
) t a m
(
R E S M
102
103 k
104
105
Empirical Theoretical Minwise
A−−TEST
102
103 k
104
105
RESEARCH−−UNIVERSITY
10−6
101
FREE−−USE
10−6
101
102
103 k
104
105
Empirical Theoretical Minwise
10−6
101
REVIEW−−PAPER 103 k
102
104
105
10−6
101
Figure 8 : M SE( ˆRmat ) , to verify the theoretical results of Lemma 6 . Note that the theoretical variance curves use the approximation ( 25 ) , for convenience . The experimental results confirm that : ( i ) the estimator ˆRmat is unbiased , ( ii ) the variance formula ( 24 ) and the approximation ( 25 ) are accurate ; ( iii ) the variance of ˆRmat is somewhat smaller than R(1− R)/k , the variance of the original k permutation minwise hashing . 4 Strategies for Dealing with Empty Bins
In general , we expect that empty bins should not occur often because E(Nemp)/k ≈ e−f /k , which is very close to zero if f /k > 5 . ( Recall f = |S1 ∪ S2| . ) If the goal of using minwise hashing is for data reduction , ie , reducing the number of nonzeros , then we would expect that f ≫ k anyway . Nevertheless , in applications where we need the estimators to be inner products , we need strategies to deal with empty bins in case they occur . Fortunately , we realize a ( in retrospect ) simple strategy which can be very nicely integrated with linear learning algorithms and performs very well .
Figure 9 plots the histogram of the numbers of nonzeros in the webspam dataset , which has 350,000 samples . The average number of nonzeros is about 4000 which should be much larger than the k ( eg , 200 to 500 ) for the hashing procedure . On the other hand , about 10 % ( or 2.8 % ) of the samples have < 500 ( or < 200 ) nonzeros . Thus , we must deal with empty bins if we do not want to exclude those data points . For example , if f = k = 500 , then Nemp ≈ e−f /k = 0.3679 , which is not small .
Webspam x 104
4
3
2
1 y c n e u q e r F
0 0
2000 4000 6000 8000 10000
# nonzeros
Figure 9 : Histogram of the numbers of nonzeros in the webspam dataset ( 350,000 samples ) .
The first ( obvious ) idea is random coding . That is , we simply replace an empty bin ( ie , “ * ” as in Figure 1 ) with a random number . In terms of the original unbiased estimator ˆRmat = Nmat , the rank−Nemp dom coding scheme will almost not change the numerator Nmat . The drawback of random coding is that the denominator will effectively become k . Of course , in most practical scenarios , we expect Nemp ≈ 0 anyway .
14
The strategy we recommend for linear learning is zero coding , which is tightly coupled with the strategy of hashed data expansion [ 18 ] as reviewed in Sec 22 More details will be elaborated in Sec 42 Basically , we can encode “ * ” as “ zero ” in the expanded space , which means Nmat will remain the same ( after taking the inner product in the expanded space ) . A very nice property of this strategy is that it is sparsity preserving . This strategy essentially corresponds to the following modified estimator :
ˆR(0 ) mat =
Nmat empqk − N ( 2 ) emp qk − N ( 1 ) j=1 I ( 2 )
( 28 ) emp =Pk emp =Pk where N ( 1 ) respectively . This modified estimator actually makes a lot of sense , after some careful thinking . emp,j are the numbers of empty bins in π(S1 ) and π(S2 ) , emp,j and N ( 2 ) j=1 I ( 1 )
Basically , since each data vector is processed and coded separately , we actually do not know Nemp ( the number of jointly empty bins ) until we see both π(S1 ) and π(S2 ) . In other words , we can not really compute Nemp if we want to use linear estimators . On the other hand , N ( 1 ) emp are always available . emp in the denominator corresponds to the normalizing step which is usually needed before feeding the data to a solver . This point will probably become more clear in Sec 42
In fact , the use ofqk − N ( 1 ) empqk − N ( 2 ) emp and N ( 2 )
When N ( 1 ) emp = N ( 2 ) emp = Nemp , ( 28 ) is equivalent to the original ˆRmat . When two original vectors are very similar ( eg , large R ) , N ( 1 ) emp will be close to Nemp . When two sets are highly unbalanced , using ( 28 ) will likely overestimate R ; however , in this case , Nmat will be so small that the absolute error will not be large . In any case , we do not expect the existence of empty bins will significantly affect the performance in practical settings . emp and N ( 2 )
4.1 The m Permutation Scheme with 1 < m ≪ k In case some readers would like to further ( significantly ) reduce the chance of the occurrences of empty bins , here we shall mention that one does not really have to strictly follow “ one permutation , ” since one can always conduct m permutations with k′ = k/m and concatenate the hashed data . Once the preprocessing is no longer the bottleneck , it matters less whether we use 1 permutation or ( eg , ) m = 3 permutations . The chance of having empty bins decreases exponentially with increasing m .
4.2 An Example of The “ Zero Coding ” Strategy for Linear Learning
Sec 2.2 has already reviewed the data expansion strategy used by [ 18 ] for integrating ( b bit ) minwise hashing with linear learning . We will adopt a similar strategy with modifications for considering empty bins .
We use a similar example as in Sec 22 Suppose we apply our one permutation hashing scheme and use k = 4 bins . For the first data vector , the hashed values are [ 12013 , 25964 , 20191 , ∗ ] ( ie , the 4 th bin is empty ) . Suppose again we use b = 2 bits . With the “ zero coding ” strategy , our procedure is summarized as follows :
Original hashed values ( k = 4 ) : Original binary representations : Lowest b = 2 binary digits : Expanded 2b = 4 binary digits :
New feature vector fed to a solver :
12013
20191 010111011101101 110010101101100 100111011011111 11
25964
01 0010
00 0001
∗ ∗ ∗ 1000 0000
1
√4 − 1 × [ 0 , 0 , 1 , 0 , 0 , 0 , 0 , 1 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ]
15
We apply the same procedure to all feature vectors in the data matrix to generate a new data matrix . The normalization factor
1 varies , depending on the number of empty bins in the i th feature vector . qk−N ( i ) emp
We believe zero coding is an ideal strategy for dealing with empty bins in the context of linear learning as it is very convenient and produces accurate results ( as we will show by experiments ) . If we use the “ random coding ” strategy ( ie , replacing a “ * ” by a random number in [ 0 , 2b − 1] ) , we need to add artificial nonzeros ( in the expanded space ) and the normalizing factor is always 1√k We apply both the zero coding and random coding strategies on the webspam dataset , as presented in Sec 5 Basically , both strategies produce similar results even when k = 512 , although the zero coding strategy is slightly better . We also compare the results with the original k permutation scheme . On the webspam dataset , our one permutation scheme achieves similar ( or even slightly better ) accuracies compared to the k permutation scheme .
( ie , no longer “ sparsity preserving ” ) .
To test the robustness of one permutation hashing , we also experiment with the news20 dataset , which has only 20,000 samples and 1,355,191 features , with merely about 500 nonzeros per feature vector on average . We purposely let k be as large as 4096 . Interestingly , the experimental results show that the zero coding strategy can perform extremely well . The test accuracies consistently improve as k increases . In comparisons , the random coding strategy performs badly unless k is small ( eg , k ≤ 256 ) . On the news20 dataset , our one permutation scheme actually outperforms the original k permutation scheme , quite noticeably when k is large . This should be due to the benefits from the “ sample withoutreplacement ” effect . One permutation hashing provides a good matrix sparsification scheme without “ damaging ” the original data matrix too much .
5 Experimental Results on the Webspam Dataset
The webspam dataset has 350,000 samples and 16,609,143 features . Each feature vector has on average about 4000 nonzeros ; see Figure 9 . Following [ 18 ] , we use 80 % of samples for training and the remaining 20 % for testing . We conduct extensive experiments on linear SVM and logistic regression , using our proposed one permutation hashing scheme with k ∈ {25 , 26 , 27 , 28 , 29} and b ∈ {1 , 2 , 4 , 6 , 8} . For convenience , we use D = 224 , which is divisible by k and is slightly larger than 16,609,143 . There is one regularization parameter C in linear SVM and logistic regression . Since our purpose is to demonstrate the effectiveness of our proposed hashing scheme , we simply provide the results for a wide range of C values and assume that the best performance is achievable if we conduct cross validations . This way , interested readers may be able to easily reproduce our experiments .
5.1 One Permutation vs k Permutation
Figure 10 presents the test accuracies for both linear SVM ( upper panels ) and logistic regression ( bottom panels ) . Clearly , when k = 512 ( or even 256 ) and b = 8 , b bit one permutation hashing achieves similar test accuracies as using the original data . Also , compared to the original k permutation scheme as in [ 18 ] , our one permutation scheme achieves similar ( or even very slightly better ) accuracies .
5.2 Preprocessing Time and Training Time
The preprocessing cost for processing the data using k = 512 independent permutations is about 6,000 seconds . In contrast , the processing cost for the proposed one permutation scheme is only 1/k of the original cost , ie , about 10 seconds . Note that webspam is merely a small dataset compared to industrial applications . We expect the ( absolute ) improvement will be even more substantial in much larger datasets .
16
100 98 96 94 92 90 88 86 84 82 80 10−3 b = 8 b = 6 b = 4 b = 2
SVM : k = 32 Webspam : Accuracy
10−2
10−1
C b = 1
100
101
102
100 98 96 94 92 90 88 86 84 82 80 10−3 b = 8
10−2
)
%
( y c a r u c c A
)
%
( y c a r u c c A
100 98 96 94 92 90 88 86 84 82 80 10−3 b = 8 b = 6 b = 4 b = 2 logit : k = 32 Webspam : Accuracy
)
%
( y c a r u c c A
)
%
( y c a r u c c A
100 98 96 94 92 90 88 86 84 82 80 10−3 b = 6 b = 4 b = 2 b = 1
SVM : k = 64 Webspam : Accuracy 10−1
100
101
102
C b = 8 b = 6 b = 4 b = 2 b = 1
)
%
( y c a r u c c A
)
%
( y c a r u c c A
100 98 96 94 92 90 88 86 84 82 80 10−3 b = 6,8
10−2 b = 4 b = 2 b = 1
Original 1 Perm k Perm
SVM : k = 128 Webspam : Accuracy 10−1
100
101
102
C b = 6,8 b = 4 b = 2 b = 1
100 98 96 94 92 90 88 86 84 82 80 10−3
)
%
( y c a r u c c A
)
%
( y c a r u c c A
100 98 96 94 92 90 88 86 84 82 80 10−3
100 98 96 94 92 90 88 86 84 82 80 10−3 b = 6,8 b = 1 b = 4 b = 2
Original 1 Perm k Perm
SVM : k = 256 Webspam : Accuracy 10−1
100
101
102
C
10−2 b = 6,8 b = 4 b = 2 b = 1
Original 1 Perm k Perm
)
%
( y c a r u c c A
)
%
( y c a r u c c A
100 98 96 94 92 90 88 86 84 82 80 10−3 b = 4,6,8
10−2 b = 2 b = 1
Original 1 Perm k Perm
SVM : k = 512 Webspam : Accuracy 10−1
100
101
102
C b = 4,6,8 b = 2 b = 1
Original 1 Perm k Perm
100 98 96 94 92 90 88 86 84 82 80 10−3
C b = 1
100
102
101
100
10−2
10−1
10−2 logit : k = 64 Webspam : Accuracy 10−1 logit : k = 512 Webspam : Accuracy 10−1 Figure 10 : Test accuracies of SVM ( upper panels ) and logistic regression ( bottom panels ) , averaged over 50 repetitions . The accuracies of using the original data are plotted as dashed ( red , if color is available ) curves with “ diamond ” markers . C is the regularization parameter . Compared with the original k permutation minwise hashing scheme ( dashed and blue if color is available ) , the proposed one permutation hashing scheme achieves very similar accuracies , or even slightly better accuracies when k is large . logit : k = 128 Webspam : Accuracy 10−1
10−2 logit : k = 256 Webspam : Accuracy 10−1
100
10−2
10−2
101
102
100
101
102
101
102
C
100
101
102
C
C
C
The prior work [ 18 ] already presented the training time using the k permutation hashing scheme . With one permutation hashing , the training time remains essentially the same ( for the same k and b ) on the webspam dataset . Note that , with the zero coding strategy , the new data matrix generated by one permutation hashing has potentially less nonzeros than the original minwise hashing scheme , due to the occurrences of empty bins . This phenomenon in theory may bring additional advantages such as slightly reducing the training time . Nevertheless , the most significant advantage of one permutation hashing lies in the dramatic reduction of the preprocessing cost , which is what we focus on in this study .
5.3 Zero Coding vs Random Coding for Empty Bins
The experimental results as shown in Figure 10 are based on the “ zero coding ” strategy for dealing with empty bins . Figure 11 plots the results for comparing zero coding with the random coding . When k is large , zero coding is superior to random coding , although the differences remain small in this dataset . This is not surprising , of course . Random coding adds artificial nonzeros to the new ( expanded ) data matrix , which would not be desirable for learning algorithms .
Remark : The empirical results on the webspam datasets are highly encouraging because they verify that our proposed one permutation hashing scheme works as well as ( or even slightly better than ) the original k permutation scheme , at merely 1/k of the original preprocessing cost . On the other hand , it would be more interesting , from the perspective of testing the robustness of our algorithm , to conduct experiments on a dataset where the empty bins will occur much more frequently .
6 Experimental Results on the News20 Dataset
The news20 dataset ( with 20,000 samples and 1,355,191 features ) is a very small dataset in not too high dimensions . The average number of nonzeros per feature vector is about 500 , which is also small . Therefore , this is more like a contrived example and we use it just to verify that our one permutation scheme ( with the zero coding strategy ) still works very well even when we let k be as large as 4096 ( ie , most of the bins are empty ) . In fact , the one permutation schemes achieves noticeably better accuracies than the
17 b = 4,6,8
)
%
( y c a r u c c A
)
%
( y c a r u c c A
100 98 96 94 92 90 88 86 84 82 80 10−3
100 98 96 94 92 90 88 86 84 82 80 10−3 b = 8 b = 6 b = 4 b = 2
SVM : k = 32 Webspam : Accuracy
10−2
10−1
C b = 1
100
101
102 b = 8 b = 6 b = 4 b = 2 logit : k = 32 Webspam : Accuracy
10−2
10−1 b = 1 100
C
101
102
)
%
( y c a r u c c A
)
%
( y c a r u c c A
100 98 96 94 92 90 88 86 84 82 80 10−3
100 98 96 94 92 90 88 86 84 82 80 10−3 b = 8 b = 6 b = 4 b = 2 b = 1
SVM : k = 64 Webspam : Accuracy 10−1
100
101
102
C b = 8 b = 6 b = 4 b = 2 b = 1 logit : k = 64 Webspam : Accuracy 10−1
100
101
102
C
10−2
10−2
)
%
( y c a r u c c A
)
%
( y c a r u c c A
100 98 96 94 92 90 88 86 84 82 80 10−3
100 98 96 94 92 90 88 86 84 82 80 10−3 b = 6,8
10−2 b = 6,8
10−2 b = 4 b = 2 b = 1
Zero Code Rand Code
SVM : k = 128 Webspam : Accuracy 10−1
100
101
102
C b = 4 b = 2 b = 1
Zero Code Rand Code logit : k = 128 Webspam : Accuracy 10−1
100
101
102
C
)
%
( y c a r u c c A
)
%
( y c a r u c c A
100 98 96 94 92 90 88 86 84 82 80 10−3
100 98 96 94 92 90 88 86 84 82 80 10−3 b = 6,8
10−2 b = 6,8
10−2 b = 4 b = 2 b = 1
Zero Code Rand Code
SVM : k = 256 Webspam : Accuracy 10−1
100
101
102
C b = 4 b = 2 b = 1
Zero Code Rand Code logit : k = 256 Webspam : Accuracy 10−1
100
101
102
C
)
%
( y c a r u c c A
)
%
( y c a r u c c A
100 98 96 94 92 90 88 86 84 82 80 10−3
100 98 96 94 92 90 88 86 84 82 80 10−3
10−2 b = 4,6,8
10−2 b = 2 b = 1
Zero Code Rand Code
SVM : k = 512 Webspam : Accuracy 10−1
100
101
102
C b = 2 b = 1
Zero Code Rand Code logit : k = 512 Webspam : Accuracy 10−1
100
101
102
C
Figure 11 : Test accuracies of SVM ( upper panels ) and logistic regression ( bottom panels ) , averaged over 50 repetitions , for comparing the ( recommended ) zero coding strategy with the random coding strategy to deal with empty bins . We can see that the differences only become noticeable at k = 512 . original k permutation scheme . We believe this is because the one permutation scheme is “ sample withoutreplacement ” and provides a much better matrix sparsification strategy without “ contaminating ” the original data matrix too much .
6.1 One Permutation vs k Permutation We experiment with k ∈ {23 , 24 , 25 , 26 , 27 , 28 , 29 , 210 , 211 , 212} and b ∈ {1 , 2 , 4 , 6 , 8} , for both one permutation scheme and k permutation scheme . We use 10,000 samples for training and the other 10,000 samples for testing . For convenience , we let D = 221 ( which is larger than 1,355,191 ) .
Figure 12 and Figure 13 present the test accuracies for linear SVM and logistic regression , respectively . When k is small ( eg , k ≤ 64 ) both the one permutation scheme and the original k permutation scheme perform similarly . For larger k values ( especially as k ≥ 256 ) , however , our one permutation scheme noticeably outperforms the k permutation scheme . Using the original data , the test accuracies are about 98 % . Our one permutation scheme with k ≥ 512 and b = 8 essentially achieves the original test accuracies , while the k permutation scheme could only reach about 97.5 % even with k = 4096 .
100 95 90 85 80 75 70 65 60 55 50 10−1 b = 8 b = 6 b = 4 b = 2 b = 1
100
SVM : k = 32 News20 : Accuracy 101 C
102
103
100 95 90 85 80 75 70 65 60 55 50 10−1 b = 8 b = 6 b = 4 b = 2 b = 1
100
SVM : k = 64 News20 : Accuracy 101 C
102
103
100 95 90 85 80 75 70 65 60 55 50 10−1 b = 6 b = 4 b = 2 b = 1
100 b = 8
SVM : k = 128 News20 : Accuracy 101 C
102
103 b = 4 b = 1 b = 2 b = 6,8
100 95 90 85 80 75 70 65 10−1 b = 2 b = 1 b = 4
Original 1 Perm k Perm
SVM : k = 2048 News20 : Accuracy 101 103 C
102
100
100 95 90 85 80 75 70 65 10−1 b = 4,6,8 b = 1 b = 2
Original 1 Perm k Perm
SVM : k = 4096 News20 : Accuracy 101 103 C
102
100
)
%
( y c a r u c c A
)
%
( y c a r u c c A
)
%
( y c a r u c c A
)
%
( y c a r u c c A
Original 1 Perm k Perm
SVM : k = 8 News20 : Accuracy b = 8 b = 6 b = 4 b = 2 b = 1
100
101 C
102
103 b = 8
)
%
( y c a r u c c A
)
%
( y c a r u c c A
100 95 90 85 80 75 70 65 60 55 50 10−1
100 95 90 85 80 75 70 65 10−1 b = 6 b = 4 b = 2 b = 1
100
SVM : k = 16 News20 : Accuracy b = 8 b = 6 b = 4 b = 2 b = 1
100
101 C
102
103 b = 8 b = 6
)
%
( y c a r u c c A
)
%
( y c a r u c c A
100 95 90 85 80 75 70 65 60 55 50 10−1
100 95 90 85 80 75 70 65 10−1 b = 4 b = 2 b = 1
100
)
%
( y c a r u c c A
)
%
( y c a r u c c A b = 6,8
100 95 90 85 80 75 70 65 10−1
18
SVM : k = 256 News20 : Accuracy 101 103 C
102
SVM : k = 512 News20 : Accuracy 101 103 C
102
SVM : k = 1024 News20 : Accuracy 101 103 C
102
100
Figure 12 : Test accuracies of linear SVM averaged over 100 repetitions . The proposed one permutation scheme noticeably outperforms the original k permutation scheme especially when k is not small . b = 8 b = 6 logit : k = 128 News20 : Accuracy 101 C
102
103 b = 2
Original 1 Perm k Perm logit : k = 4096 News20 : Accuracy 101 103 C
102
SVM : k = 128 News20 : Accuracy 101 C
102
103
Zero Code Rand Code
SVM : k = 4096 News20 : Accuracy 101 103 C
102 b = 4,6,8 b = 2 b = 1
Original 1 Perm k Perm logit : k = 8 News20 : Accuracy b = 8 b = 6 b = 4 b = 2 b = 1
100
101 C
102
103 b = 8 b = 6
)
%
( y c a r u c c A
)
%
( y c a r u c c A
100 95 90 85 80 75 70 65 60 55 50 10−1
100 95 90 85 80 75 70 65 10−1 b = 4 b = 2 b = 1
100
)
%
( y c a r u c c A
)
%
( y c a r u c c A
100 95 90 85 80 75 70 65 60 55 50 10−1
100 95 90 85 80 75 70 65 10−1 b = 8 b = 6 b = 4 b = 2 b = 1 logit : k = 16 News20 : Accuracy
100
101 C
102
103 b = 8 b = 6 b = 4 b = 2 b = 1
100
)
%
( y c a r u c c A
)
%
( y c a r u c c A
100 95 90 85 80 75 70 65 60 55 50 10−1
100 95 90 85 80 75 70 65 10−1 b = 8 b = 6 b = 4 b = 2 b = 1
100 b = 6,8 logit : k = 32 News20 : Accuracy 101 C
102
103 b = 4 b = 2 b = 1
)
%
( y c a r u c c A
)
%
( y c a r u c c A
100 95 90 85 80 75 70 65 60 55 50 10−1
100 95 90 85 80 75 70 65 10−1 b = 8 b = 6 b = 4 b = 2 b = 1
100 logit : k = 64 News20 : Accuracy 101 C
102
103 b = 6,8 b = 1 b = 2 b = 4
Original 1 Perm k Perm logit : k = 2048 News20 : Accuracy 101 103 C
102
100
)
%
( y c a r u c c A
)
%
( y c a r u c c A
100 95 90 85 80 75 70 65 60 55 50 10−1
100 95 90 85 80 75 70 65 10−1 b = 4 b = 2 b = 1
100 b = 4,6,8 b = 1
100 logit : k = 256 News20 : Accuracy 101 103 C
102 logit : k = 512 News20 : Accuracy 101 103 C
102 logit : k = 1024 News20 : Accuracy 101 103 C
102
100
Figure 13 : Test accuracies of logistic regression averaged over 100 repetitions . The proposed one permutation scheme noticeably outperforms the original k permutation scheme especially when k is not small .
6.2 Zero Coding vs Random Coding for Empty Bins
Figure 14 and Figure 15 plot the results for comparing two coding strategies to deal with empty bins , respectively for linear SVM and logistic regression . Again , when k is small ( eg , k ≤ 64 ) , both strategies perform similarly . However , when k is large , using the random coding scheme may be disastrous , which is of course also expected . When k = 4096 , most of the nonzero entries in the new expanded data matrix fed to the solver are artificial , since the original news20 dataset has merely about 500 nonzero on average .
)
%
( y c a r u c c A
)
%
( y c a r u c c A
100 95 90 85 80 75 70 65 60 55 50 10−1
100 95 90 85 80 75 70 65 10−1 b = 8 b = 6 b = 4 b = 2 b = 1 100 b = 8 b = 6 b = 4 b = 2 b = 1
100
Zero Code Rand Code
SVM : k = 8 News20 : Accuracy
101 C
102
103
SVM : k = 256 News20 : Accuracy 101 C
102
103
)
%
( y c a r u c c A
)
%
( y c a r u c c A
100 95 90 85 80 75 70 65 60 55 50 10−1
100 95 90 85 80 75 70 65 10−1 b = 8 b = 6 b = 4 b = 2 b = 1 100 b = 8 b = 6 b = 4 b = 2 b = 1
100
Zero Code Rand Code
SVM : k = 16 News20 : Accuracy
101 C
102
103
)
%
( y c a r u c c A
)
%
( y c a r u c c A
SVM : k = 512 News20 : Accuracy 101 103 C
102
100 95 90 85 80 75 70 65 60 55 50 10−1
SVM : k = 32 News20 : Accuracy b = 8 b = 6 b = 4 b = 2 b = 1
100
101 C
102
103 b = 6,8 b = 4 b = 2 b = 1
SVM : k = 1024 News20 : Accuracy
100 95 90 85 80 75 70 65 10−1 b = 8 b = 6 b = 4 b = 2 b = 1
100
SVM : k = 64 News20 : Accuracy 101 C
102
103 b = 6,8 b = 1 b = 2 b = 4
SVM : k = 2048
News20 : Accuracy
Zero Code Rand Code
)
%
( y c a r u c c A
)
%
( y c a r u c c A
)
%
( y c a r u c c A
)
%
( y c a r u c c A
100 95 90 85 80 75 70 65 60 55 50 10−1
100 95 90 85 80 75 70 65 10−1
100 95 90 85 80 75 70 65 10−1
100
101 C
102
103
100
101 C
102
103
100
100 95 90 85 80 75 70 65 60 55 50 10−1 b = 8 b = 6 b = 4 b = 2 b = 1
100
Figure 14 : Test accuracies of linear SVM averaged over 100 repetitions , for comparing the ( recommended ) zero coding strategy with the random coding strategy to deal with empty bins . On this dataset , the performance of the random coding strategy can be bad .
Remark : We should re iterate that the news20 dataset is more like a contrived example , merely for testing the robustness of the one permutation scheme with the zero coding strategy . In more realistic industrial applications , we expect that numbers of nonzeros in many datasets should be significantly higher , and hence the performance differences between the one permutation scheme and the k permutation scheme and the differences between the two strategies for empty bins should be small .
19
Zero Code Rand Code logit : k = 8 News20 : Accuracy b = 8 b = 6 b = 4 b = 2 b = 1
100
101 C
102
103 b = 8 b = 6
)
%
( y c a r u c c A
)
%
( y c a r u c c A
100 95 90 85 80 75 70 65 60 55 50 10−1
100 95 90 85 80 75 70 65 10−1 b = 4 b = 2 b = 1
100
Zero Code Rand Code logit : k = 16 News20 : Accuracy b = 8 b = 6 b = 4 b = 2 b = 1
100
101 C
102
103 b = 8 b = 4 b = 6
)
%
( y c a r u c c A
)
%
( y c a r u c c A
100 95 90 85 80 75 70 65 60 55 50 10−1
100 95 90 85 80 75 70 65 10−1 b = 2 b = 1
100 b = 8 b = 6 b = 4 b = 2 b = 1
100 logit : k = 32 News20 : Accuracy 101 C
102
103 b = 4 b = 6,8 b = 2 b = 1 logit : k = 1024 News20 : Accuracy
)
%
( y c a r u c c A
)
%
( y c a r u c c A
100 95 90 85 80 75 70 65 60 55 50 10−1
100 95 90 85 80 75 70 65 10−1
)
%
( y c a r u c c A
)
%
( y c a r u c c A
100 95 90 85 80 75 70 65 60 55 50 10−1
100 95 90 85 80 75 70 65 10−1 b = 8 b = 6 b = 4 b = 2 b = 1
100 logit : k = 64 News20 : Accuracy 101 C
102
103 b = 6,8 b = 1 b = 2 b = 4
Zero Code Rand Code
)
%
( y c a r u c c A
)
%
( y c a r u c c A logit : k = 2048 News20 : Accuracy 101 103 C
102
100 95 90 85 80 75 70 65 60 55 50 10−1 b = 4 b = 2 b = 1
100 b = 4,6,8
100 95 90 85 80 75 70 65 10−1 b = 1
100 b = 8 b = 6 logit : k = 128 News20 : Accuracy 101 C
102
103 b = 2
Zero Code Rand Code logit : k = 4096
News20 : Accuracy 101 C
102
103 logit : k = 256 News20 : Accuracy
101 C
102
103 logit : k = 512 News20 : Accuracy 101 103 C
102
100
101 C
102
103
100
Figure 15 : Test accuracies of logistic regression averaged over 100 repetitions , for comparing the zero coding strategy ( recommended ) with the random coding strategy to deal with empty bins . On this dataset , the performance of the random coding strategy can be bad .
7 The Variable Length One Permutation Hashing Scheme
While the fixed length one permutation scheme we have presented and analyzed should be simple to implement and easy to understand , we would like to present a variable length scheme which may more obviously connect with other known hashing methods such as the Count Min ( CM ) sketch [ 6 ] .
As in the fixed length scheme , we first conduct a permutation π : Ω → Ω . Instead of dividing the space evenly , we vary the bin lengths according to a multinomial distribution mult,D , 1
This variable length scheme is equivalent to first uniformly grouping the original data entries into k bins and then applying permutations independently within each bin . The latter explanation connects our method with the Count Min ( CM ) sketch [ 6 ] ( but without the “ count min ” step ) , which also hashes the elements uniformly to k bins and the final ( stored ) hashed value in each bin is the sum of all the elements in the bin . The bias of the CM estimate can be removed by subtracting a term . [ 20 ] adopted the CM sketch for linear learning . Later , [ 24 ] proposed a novel idea ( named “ VW ” ) to remove the bias , by pre multiplying ( elementwise ) the original data vectors with a random vector whose entries are sampled iid from the two point distribution in {−1 , 1} with equal probabilities . In a recent paper , [ 18 ] showed that the variance of the CM sketch and variants are equivalent to the variance of random projections [ 16 ] , which is substantially larger than the variance of the minwise hashing when the data are binary . k , 1 k , , 1 k .
Since [ 18 ] has already conducted ( theoretical and empirical ) comparisons with CM and VW methods , we do not include more comparisons in this paper . Instead , we have simply showed that with one permutation only , we are able to achieve essentially the same accuracy as using k permutations .
We believe the fixed length scheme is more convenient to implement . Nevertheless , we would like to present some theoretical results for the variable length scheme , for better understanding the differences . The major difference is the distribution of Nemp , the number of jointly empty bins .
Lemma 8 Under the variable length scheme ,
E ( Nemp ) k
=1 −
1 kf1+f2−a
( 29 )
20
V ar ( Nemp ) k2
=
<
Proof : See Appendix H . fi
1
1
1 k k k E(Nemp ) −1 − k E(Nemp )
1 − k 1 − 1 −
E(Nemp ) k2(f1+f2−a )
E(Nemp ) k k
1
−1 −
2 kf1+f2−a!
( 30 )
( 31 )
The other theoretical results for the fixed length scheme which are expressed in terms Nemp essentially is still an unbiased estimator of R and its vari hold for the variable length scheme . For example , Nmat k−Nemp ance is in the same form as ( 24 ) in terms of Nemp .
Remark : The number of empty bins for the variable length scheme as presented in ( 29 ) is actually an upper bound of the number of empty bins for the fixed length scheme as shown in ( 10 ) . The difference between k )−j
D(1− 1 D−j j=0
Qf−1 Lemma 2 , although it is possible thatQf−1 k f ( recall f = f1 + f2 − a ) is small when the data are sparse , as shown in k f in corner cases . Because smaller Nemp implies potentially better performance , we conclude that the fixed length scheme should be sufficient and there are perhaps no practical needs to use the variable length scheme .
D−j ≪,1 − 1 and ,1 − 1
D(1− 1 k )−j j=0
8 Conclusion
A new hashing algorithm is developed for large scale search and learning in massive binary data . Compared with the original k permutation ( eg , k = 500 ) minwise hashing algorithm ( which is the standard procedure in the context of search ) , our method requires only one permutation and can achieve similar or even better accuracies at merely 1/k of the original preprocessing cost . We expect that our proposed algorithm ( or its variant ) will be adopted in practice .
References
[ 1 ] Alexandr Andoni and Piotr Indyk . Near optimal hashing algorithms for approximate nearest neighbor in high dimensions . In Commun . ACM , volume 51 , pages 117–122 , 2008 .
[ 2 ] Leon Bottou . http://leonbottouorg/projects/sgd
[ 3 ] Andrei Z . Broder , Moses Charikar , Alan M . Frieze , and Michael Mitzenmacher . Min wise independent permutations ( extended abstract ) . In STOC , pages 327–336 , Dallas , TX , 1998 .
[ 4 ] Andrei Z . Broder , Steven C . Glassman , Mark S . Manasse , and Geoffrey Zweig . Syntactic clustering of the web . In WWW , pages 1157 – 1166 , Santa Clara , CA , 1997 .
[ 5 ] J . Lawrence Carter and Mark N . Wegman . Universal classes of hash functions ( extended abstract ) . In
STOC , pages 106–112 , 1977 .
[ 6 ] Graham Cormode and S . Muthukrishnan . An improved data stream summary : the count min sketch and its applications . Journal of Algorithm , 55(1):58–75 , 2005 .
[ 7 ] Rong En Fan , Kai Wei Chang , Cho Jui Hsieh , Xiang Rui Wang , and Chih Jen Lin . Liblinear : A library for large linear classification . Journal of Machine Learning Research , 9:1871–1874 , 2008 .
21
[ 8 ] Dennis Fetterly , Mark Manasse , Marc Najork , and Janet L . Wiener . A large scale study of the evolution of web pages . In WWW , pages 669–678 , Budapest , Hungary , 2003 .
[ 9 ] Jerome H . Friedman , F . Baskett , and L . Shustek . An algorithm for finding nearest neighbors . IEEE
Transactions on Computers , 24:1000–1006 , 1975 .
[ 10 ] Izrail S . Gradshteyn and Iosif M . Ryzhik . Table of Integrals , Series , and Products . Academic Press ,
New York , sixth edition , 2000 .
[ 11 ] Cho Jui Hsieh , Kai Wei Chang , Chih Jen Lin , S . Sathiya Keerthi , and S . Sundararajan . A dual coordinate descent method for large scale linear svm . In Proceedings of the 25th international conference on Machine learning , ICML , pages 408–415 , 2008 .
[ 12 ] Piotr Indyk and Rajeev Motwani . Approximate nearest neighbors : Towards removing the curse of dimensionality . In STOC , pages 604–613 , Dallas , TX , 1998 .
[ 13 ] Thorsten Joachims . Training linear svms in linear time . In KDD , pages 217–226 , Pittsburgh , PA , 2006 .
[ 14 ] Ping Li and Kenneth W . Church . Using sketches to estimate associations .
In HLT/EMNLP , pages 708–715 , Vancouver , BC , Canada , 2005 ( The full paper appeared in Commputational Linguistics in 2007 ) .
[ 15 ] Ping Li , Kenneth W . Church , and Trevor J . Hastie . One sketch for all : Theory and applications of conditional random sampling . In NIPS ( Preliminary results appeared in NIPS 2006 ) , Vancouver , BC , Canada , 2008 .
[ 16 ] Ping Li , Trevor J . Hastie , and Kenneth W . Church . Very sparse random projections . In KDD , pages
287–296 , Philadelphia , PA , 2006 .
[ 17 ] Ping Li and Arnd Christian K¨onig . Theory and applications b bit minwise hashing . Commun . ACM ,
2011 .
[ 18 ] Ping Li , Anshumali Shrivastava , Joshua Moore , and Arnd Christian K¨onig . Hashing algorithms for large scale learning . In NIPS , Granada , Spain , 2011 .
[ 19 ] Shai Shalev Shwartz , Yoram Singer , and Nathan Srebro . Pegasos : Primal estimated sub gradient solver for svm . In ICML , pages 807–814 , Corvalis , Oregon , 2007 .
[ 20 ] Qinfeng Shi , James Petterson , Gideon Dror , John Langford , Alex Smola , and SVN Vishwanathan .
Hash kernels for structured data . Journal of Machine Learning Research , 10:2615–2637 , 2009 .
[ 21 ] Anshumali Shrivastava and Ping Li . Fast near neighbor search in high dimensional binary data . In
ECML , 2012 .
[ 22 ] Josef Sivic and Andrew Zisserman . Video google : a text retrieval approach to object matching in videos . In ICCV , 2003 .
[ 23 ] Simon Tong .
Lessons learned developing a practical large scale machine learning system . http://googleresearchblogspotcom/2010/04/lessons learned developing practicalhtml , 2008 .
[ 24 ] Kilian Weinberger , Anirban Dasgupta , John Langford , Alex Smola , and Josh Attenberg . Feature hash ing for large scale multitask learning . In ICML , pages 1113–1120 , 2009 .
22
A Proof of Lemma 1 k k )
E ( Nemp ) =
Recall Nemp = Pk j=1 Iemp,j , where Iemp,j = 1 if , in the j th bin , both π(S1 ) and π(S2 ) are empty , and Iemp,j = 0 otherwise . Also recall D = |Ω| , f1 = |S1| , f2 = |S2| , a = |S1 ∩ S2| . Obviously , if D,1 − 1 k < f1 + f2 − a , then none of the bins will be jointly empty , ie , E(Nemp ) = V ar(Nemp ) = 0 . Next , assume D,1 − 1 k − j Xj=1 k ≥ f1 + f2 − a , then by the linearity of expectation , Pr ( Iemp,j = 1 ) = kPr ( Iemp,1 = 1 ) = k,D(1− 1 f1+f2−a , D f1+f2−ak ≥ f1 + f2 − a . Then Iemp,iIemp,j
To derive the variance , we first assume D,1 − 2 V ar ( Nemp ) =E,N 2 emp − E2 ( Nemp ) =E emp,j +Xi6=j Xj=1  =k(k − 1)Pr ( Iemp,1 = 1 , Iemp,2 = 1 ) + E ( Nemp ) − E2 ( Nemp ) =k(k − 1 ) ×
D,1 − 1 D − j f1+f2−a−1 f1+f2−a−1
Yj=0
= k
I 2 k
 − E2 ( Nemp ) k − j − k ×
Yj=0 f1+f2−a−1
D,1 − 2 D − j k − j
Yj=0 D,1 − 1 D,1 − 1 k − j D − j D − j k , then Pr ( Iemp,1 = 1 , Iemp,2 = 1 ) = 0 and hence Yj=0
D,1 − 1 D − j k − j f1+f2−a−1 f1+f2−a−1
Yj=0
− k ×
2
 
D,1 − 1 D − j k − j
2
  f1+f2−a−1
+ k ×
Yj=0 k < f1 + f2 − a ≤ D,1 − 1
If D,1 − 2 V ar ( Nemp ) = E ( Nemp ) − E2 ( Nemp ) = k ×
Assuming D,1 − 2
V ar ( Nemp ) k2 k − j
1
1
= f1+f2−a−1 k ≥ f1 + f2 − a , we obtain k  D,1 − 1 Yj=0  D − j k  f1+f2−a−1 −1 − Yj=0   1 − k E(Nemp ) k  −1 − Yj=0   1 − k E(Nemp ) f1+f2−a−1
<
= k k
1
1
1 k k
2
2
  Yj=0 1 −   D,1 − 1 k − j  D − j
D,1 − 1 D − j k − j
 
E(Nemp )
E(Nemp )
23 f1+f2−a−1
D,1 − 1 D − j f1+f2−a−1 Yj=0
 k − j  D,1 − 2 D − j k − j f1+f2−a−1
Yj=0
D,1 − 2 D − j k − j
 
 
−
− because
D,1 − 1 D − j ⇐⇒D1 − 1 k2 ⇐⇒1 −
1
> 0 k − j
!2 D,1 − 2 k − j − D − j k − j2 > ( D − j)D1 − 1 k2 > 1 − = 1 −
2 k
2 k
+
This completes the proof .
2 k − j
B Proof of Lemma 2
The following expansions will be useful n−1
1 j
1 2n −
Xj=1 = log n + 0.577216 − x3 log(1 − x ) = −x − 3 − k ≥ f1 + f2 − a . We can write =1 − k − j x2 2 − f1+f2−a−1
Assume D,1 − 1
E ( Nemp )
D,1 − 1 D − j Hence it suffices to study the error term
Yj=0
= k
1 12n2 +
( [10 , 836713 ] )
( |x| < 1 )
( 32 )
( 33 )
1 kf1+f2−a
× f1+f2−a−1
Yj=0
1 − j
( k − 1)(D − j ) f1+f2−a−1
Yj=0
1 − j f1+f2−a−1
( k − 1)(D − j ) . log1 − ( k − 1)(D − j ) ( k − 1)(D − j)3 3 −
Xj=0
( k − 1)(D − j)2
1 j j j
+ ) log f1+f2−a−1
Yj=0 Xj=0 ( − f1+f2−a−1
= j
1 − ( k − 1)(D − j ) −
( k − 1)(D − j ) = 2
1 j
Take the first term , f1+f2−a−1 f1+f2−a−1
1 j
D − j D − j −
D D − j
1
1
=
−
( k − 1)(D − j )
Xj=0 Xj=0 D − j k − 1 f1 + f2 − a − D  f1 + f2 − a − D k − 1 Xj=1  k − 1f1 + f2 − a − Dlog(D + 1 ) − 2(D + 1 ) − log(D − f1 − f2 + a + 1 ) + k − 1 f1+f2−a−1 Xj=0 1 Xj=1 j − j 
D−f1−f2+a
 
1
1
1
1
D
=
=
=
24
1
2(D − f1 − f2 + a + 1 ) +
Thus , we obtain ( by ignoring a term D
D+1 ) f1+f2−a−1 j
1 −
( k − 1)(D − j ) = exp 
Yj=0 Assuming f1 + f2 − a ≪ D , we can further expand log
−D log approximation :
D+1
D−f1−f2+a+1 + ( f1 + f2 − a)1 −
1
2(D−f1−f2+a+1 ) k − 1
+  
D+1
D−f1−f2+a+1 and obtain a more simplified
E ( Nemp ) k
=1 −
1 kf1+f2−a1 − O ( f1 + f2 − a)2 kD
Next , we analyze the approximation of the variance by assuming f1 + f2 − a ≪ D . A similar analysis can show that f1+f2−a−1
Yj=0
D,1 − 2 D − j and hence we obtain , by using 1 − 2 V ar ( Nemp )
2 kf1+f2−a1 − O ( f1 + f2 − a)2 k − j =1 − k 1 − 1 k−1 , k =,1 − 1 kD k2 2
1
1
1
=
−1 − kf1+f2−a kf1+f2−a 1 −1 − kf1+f2−a+1 1 −
=1 − k 1 − −1 − C Proof of Lemma 3
1
1 kf1+f2−a! kf1+f2−a
1 k2(f1+f2−a )
+
1 k 1 −
1 kf1+f2−a
−1 −
2 kf1+f2−a! + O ( f1 + f2 − a)2 kD
−1 −
1 k − 1f1+f2−a! + O ( f1 + f2 − a)2 kD
Let q(D , k , f ) = Pr ( Nemp = 0 ) and Djk = D(1 − j/k ) . Then ,
Pr ( Nemp = j ) =k jP{Iemp,1 = ··· = Iemp,j = 1 , Iemp,j+1 = ··· = Iemp,k = 0} j P =k q(Djk , k − j , f ) .
Djk f P D f where P D f is the “ permutation ” operator : P D f = D(D − 1)(D − 2)(D − f + 1 ) .
Thus , to derive Pr ( Nemp = j ) , we just need to find q(D , k , f ) . By the union intersection formula ,
1 − q(D , k , f ) = k
( −1)j−1k Xj=1 jE
Iemp,i . j
Yi=1
25
From Lemma 1 , we can infer EQj q(D , k , f ) = 1 +
It follows that
Djk i=1 Iemp,i = P f
/P D
. Thus we find t=0 k )−t
D(1− j D−t f =Qf−1 j P ( −1)jk Xj=0
Djk f P D f
= k
Djk f P D f
. k
Xj=1 j P ( −1)jk k−j
Pr ( Nemp = j ) =k ( −1)sk − j j Xs=0 Xs=0 ( −1)s k−j k!
=
P D f s P D(1−j/k−s/k ) D1 − j+s D − t f−1
Yt=0 j!s!(k − j − s)! f k − t
D Proof of Lemma 4
Define
S1 ∪ S2 = {j1 , j2 , , jf1+f2−a} min J = min π(S1 ∪ S2 ) = T = argmin
1≤i≤f1+f2−a π(ji ) , ie , π(jT ) = J i
π(ji )
Because π is a random permutation , we know
Pr ( T = i ) = Pr ( jT = ji ) = Pr ( π(jT ) = π(ji ) ) =
1 f1 + f2 − a
, 1 ≤ i ≤ f1 + f2 − a
Due to symmetry ,
Pr(T = i|J = t ) = Pr(π(ji ) = t| min and hence we know that J and T are independent . Therefore ,
1≤l≤f1+f2−a
π(jl ) = t ) =
1 f1 + f2 − a k
E(Nmat ) =
Pr(Imat,j = 1 ) = kPr(Imat,1 = 1 )
Xj=1 =kPr ( jT ∈ S1 ∩ S2 , 0 ≤ J ≤ D/k − 1 ) =kPr ( jT ∈ S1 ∩ S2 ) Pr ( 0 ≤ J ≤ D/k − 1 ) =kRPr ( Iemp,1 = 0 ) =kR1 − E ( Nemp ) k
E(N 2 mat ) =E    k
Xj=1
=E(Nmat ) + k(k − 1)E(Imat,1Imat,2 )
2 Imat,j  = E   k
Xj=1
26
Imat,j + k
Xi6=j
Imat,iImat,j 
E(Imat,1Imat,2 ) = Pr ( Imat,1 = 1 , Imat,2 = 1 ) D/k−1
Pr ( Imat,1 = 1 , Imat,2 = 1|J = t ) Pr ( J = t )
Pr ( jT ∈ S1 ∩ S2 , Imat,2 = 1|J = t ) Pr ( J = t )
=
=
=
D/k−1
D/k−1
Xt=0 Xt=0 Xt=0 Xt=0
D/k−1
=R
Pr ( Imat,2 = 1|J = t , jT ∈ S1 ∩ S2 ) Pr ( jT ∈ S1 ∩ S2 ) Pr ( J = t )
Pr ( Imat,2 = 1|J = t , jT ∈ S1 ∩ S2 ) Pr ( J = t )
Note that , conditioning on {J = t , jT ∈ S1 ∩ S2} , the problem ( ie , the event {Imat,2 = 1} ) is actually the same as our original problem with f1 + f2 − a − 1 elements whose locations are uniformly random on {t + 1 , t + 2 , , D − 1} . Therefore ,
E(Imat,1Imat,2 )
=R
=R
=R
D/k−1
D/k−1 a − 1 a − 1
Xt=0 f1+f2−a−2 f1 + f2 − a − 1 Yj=0 1 − Pr ( J = t) Xt=0 1 − f1 + f2 − a − 1 Xt=0  f1 + f2 − a − 1
Pr ( J = t ) − a − 1
D/k−1
 
D,1 − 1 k − t − 1 − j D − t − 1 − j D,1 − 1 Yj=0 f1+f2−a−2
D/k−1
Xt=0
Pr ( J = t )
Pr ( J = t )
D − t − 1 − j f1+f2−a−2
 k − t − 1 − j  D,1 − 1 k − t − 1 − j D − t − 1 − j
Yj=0
 
By observing that
= t−1 f1 + f2 − a
Pr(J = t ) = , D−t−1 f1+f2−a−1 , D f1+f2−aXt=0
Yj=0 Pr(J = t ) = 1 − Pr ( Iemp,1 = 1 ) = 1 −
D/k−1
D
D − f1 − f2 + a − j
D − 1 − j
= f1 + f2 − a
D
E ( Nemp ) k
= 1 − f1+f2−a−1
Yj=0
D,1 − 1 D − j f1+f2−a−1
Yj=1 k − j
D − t − j D − j we obtain two interesting ( combinatorial ) identities f1 + f2 − a
D f1 + f2 − a
D
D − f1 − f2 + a − j t−1
Yj=0
D/k−1
Xt=0 Xt=0
Yj=1
D/k−1 f1+f2−a−1
D − 1 − j D − t − j D − j
= 1 − f1+f2−a−1 k − j
Yj=0 D,1 − 1 D − j
D,1 − 1 D − j k − j
Yj=0 f1+f2−a−1
= 1 −
27 which helps us simplify the expression :
D/k−1
=
=
D/k−1
D/k−1
Xt=0 Xt=0 Xt=0 Xt=0 = 1 −
=
= −
2D/k−1
Pr ( J = t ) f1 + f2 − a f1 + f2 − a
D
D f1 + f2 − a
D f1+f2−a−1
Yj=0 f1+f2−a−2 f1+f2−a−1 f1+f2−a−1 f1+f2−a−2 k − t − 1 − j
Yj=0 Yj=1 Yj=1 Yj=1 D,1 − 2 k − j D − j D,1 − 2 k − j D − j
D,1 − 1 D − t − 1 − j D − t − j Yj=0 D − j D,1 − 1 k − t − j D − j D − t − j Xt=0 D − j −  −  Yj=0 1 − D,1 − 1 k − j Yj=0 D − j f1+f2−a−1 f1+f2−a−1 f1+f2−a−1
D/k−1
+ f1+f2−a−1
Yj=0
D,1 − 1 k − t − 1 − j
D − t − 1 − j
D − t − j D − j f1 + f2 − a
D
D,1 − 1 D − j f1+f2−a−1
Yj=1  k − j 
Combining the results , we obtain
E(Imat,1Imat,2 )
=R
=R a − 1 f1 + f2 − a − 1 1 − f1 + f2 − a − 1 1 − 2 a − 1
And hence f1+f2−a−1
Yj=0 Yj=0 f1+f2−a−1
+
D,1 − 1 D − j D,1 − 1 D − j k − j k − j
+ f1+f2−a−1
Yj=0 Yj=0 f1+f2−a−1
D,1 − 2 D − j D,1 − 2 D − j k − j −  k − j  f1+f2−a−1
Yj=0
D,1 − 1 D − j k − j
 
V ar(Nmat ) = k(k − 1)E(Imat,1Imat,2 ) + E(Nmat ) − E2(Nmat ) f1+f2−a−1 a − 1 f1 + f2 − a − 1 1 − 2 =k(k − 1)R + kR D,1 − 1 k − j 1 − D − j f1+f2−a−1
Yj=0
D,1 − 1 Yj=0 D − j   − k2R2 1 − k − j
+ f1+f2−a−1
Yj=0
  f1+f2−a−1
Yj=0 D,1 − 1 D − j
D,1 − 2 k − j D − j  k − j 
2
28
V ar(Nmat ) k2 f1+f2−a−1
Yj=0 k − j
2
 
1
1
1
=
<
= k k
1
1 a − 1
Yj=0
E(Nmat ) f1+f2−a−1 k E(Nmat ) 1 − f1 + f2 − a − 1 k R +1 − 1 − 2 k R2 D,1 − 1 −1 − 1 − D − j k E(Nmat )
1 − k R2 +1 − 1 − 2 k R2 −1 − 1 − k E(Nmat ) 1 −
Yj=0 Yj=0 f1+f2−a−1 f1+f2−a−1
E(Nmat )
E(Nmat ) k k
1
1 k k
D,1 − 1 k − j D − j  D,1 − 1 k − j  D − j f1+f2−a−1 < R = a−1
D,1 − 1 D − j k − j
+ f1+f2−a−1
Yj=0
D,1 − 2 D − j k − j
 
+ 
2 f1+f2−a−1
Yj=0
D,1 − 1 D − j k − j
2   
To see the inequality , note that towards the end of Appendix A . This completes the proof . a f1+f2−a , and D(1− 2 D−j k )−j
< D(1− 1
D−j 2 k )−j as proved
E Proof of Lemma 5 k
E ( NmatNemp ) =E Xj=1  =0 +Xi6=j
Imat,j k
Xj=1
Iemp,j  = k
Xj=1
E ( Imat,j Iemp,j ) +Xi6=j
E ( Imat,iIemp,j )
E ( Imat,iIemp,j ) = k(k − 1)E ( Iemp,1Imat,2 )
E ( Iemp,1Imat,2 ) =Pr ( Iemp,1 = 1 , Imat,2 = 1 ) = Pr ( Imat,2 = 1|Iemp,1 = 1 ) Pr ( Iemp,1 = 1 )
=R 1 − f1+f2−a−1
Yj=0
D,1 − 2 D,1 − 1 k − j k − j  f1+f2−a−1
Yj=0
D,1 − 1 D − j k − j
29
 
  f1+f2−a−1 f1+f2−a−1 f1+f2−a−1 f1+f2−a−1 f1+f2−a−1 f1+f2−a−1 f1+f2−a−1 f1+f2−a−1
Cov ( Nmat , Nemp ) = E ( NmatNemp ) − E ( Nmat ) E ( Nemp ) =k(k − 1)R k − j D,1 − 2 k − j Yj=0 1 −  D,1 − 1  − kR  k D,1 − 1 k − j Yj=0 1 −  D − j   =k2R D,1 − 1 k − j Yj=0 Yj=0    D − j − kR  k − j D,1 − 2 k − j 1 −   D,1 − 1 k − j Yj=0 k − j
D,1 − 1 k − j Yj=0 D − j  D,1 − 1 k − j Yj=0  D − j D,1 − 1 k − j Yj=0 D − j  D,1 − 1  ≤ 0 D − j k − j  − D,1 − 2 k − j Yj=0 1 − − D,1 − 1 k − j  − 1 − ( k − 1)  D,1 − 2 k − j Yj=0   D,1 − 1 Because g(k = ∞ ) = 0 , it suffices to show that g(k ) is increasing in k .
D,1 − 1 D − j D,1 − 1 D − j
− k − j
Yj=0 Yj=0 f1+f2−a−1 f1+f2−a−1 f1+f2−a−1 f1+f2−a−1 f1+f2−a−1
Yj=0
Yj=0 g(k ) =k  =k 
To see the inequality , it suffices to show that g(k ) < 0 , where
D,1 − 2 D,1 − 1 k − j k − j  f1+f2−a−1
D,1 − 2 D,1 − 1 k − j k − j  f−1 f−1 f−1 f−1
Yj=0
Yj=0
Yj=0 k − j
D,1 − 1 D − j
D,1 − 2 D,1 − 1 g(f ; k ) =k 
D,1 − 1 D − j f−1 Yj=0
 − 1 − ( k − 1)   g(f + 1 ; k ) =k   D,1 − 1 k − j k − f  D − f =g(f ; k ) − D − f +  D,1 − 1 k − j  D   D − j  D − f + D,1 − 2 D,1 − 1 k − j  D Yj=0  D,1 − 1 D − j k − j 2  f−1  ,1 − 1 Yj=0 ,D,1 − 2 k − j ( D − j ) D,1 − 1 ,D,1 − 1 h(f ; k ) ≤ 1 holds because one can check that h(1 ; k ) ≤ 1 and ( D(1− 2 ( D(1− 1 k − j k − j  k − j ! − 1 − ( k − 1)  D,1 − 2 D,1 − 2 k − j Yj=0  D,1 − 1 D,1 − 1 k − j k − f!  D,1 − 1 D,1 − 2 k − j kYj=0 D,1 − 1 D,1 − 1 k − j k − f! ≤ 0 D,1 − 1 k − j k D,1 − 1 k − f ! ≤ 1 k ( D − f )
⇐⇒h(f ; k ) =  k )−j)(D−j ) k )−j)2 < 1 .
Thus , it suffices to show
− 
Yj=0 f−1 f−1 f−1
This completes the proof . k − f! k − f
30
F Proof of Lemma 6
We first prove that ˆRmat = Nmat k−Nemp is unbiased ,
P{k − Nemp > 0} = 1
Iemp,j = 1 ⇒ Imat,j = 0 Iemp,j = 0 = R k − Nemp = m = ( m/k)R , m > 0 k − Nemp = R(k − Nemp ) k − Nemp = R
EImat,jfififi EImat,jfififi ENmatfififi ENmat/(k − Nemp)fififi E ˆRmat = R independent of Nemp
Next , we compute the variance . To simplify the notation , denote f = f1 + f2 − a and ˜R = a−1 f−1 . Note that
R2 − R ˜R = R{a(f − 1 ) − f ( a − 1)}/{f ( f − 1)} = R(1 − R)/(f − 1 )
Iemp,1 = Iemp,2 = 0 = R(a − 1)/(f − 1 ) = R ˜R Iemp,1 + Iemp,2 > 0 = 0
EImat,1Imat,2fififi EImat,1Imat,2fififi By conditioning on k − Nemp , we obtain matfififi EN 2 k − Nemp = m = kEImat,1fififi = Rm + k(k − 1)R ˜RPrIemp,1 = Iemp,2 = 0fififi = Rm + k(k − 1)R ˜Rm = Rm + m(m − 1)R ˜R
2.k 2 k − Nemp = m + k(k − 1)EImat,1Imat,2fififi k − Nemp = m k − Nemp = m and matfififi E ˆR2 k − Nemp = m = R ˜R + ( R − R ˜R)/m mat = R ˜R + ( R − R ˜R)E(k − Nemp)−1
E ˆR2
Combining the above results , we obtain
V ar ˆRmat =R ˜R − R2 + ( R − R ˜R)E(k − Nemp)−1
=R(1 − R)E(k − Nemp)−1 − ( R2 − R ˜R)(1 − E(k − Nemp)−1 ) =R(1 − R)E(k − Nemp)−1 − R(1 − R)(f − 1)−1(1 − E(k − Nemp)−1 ) =R(1 − R)(E(k − Nemp)−1 − ( f − 1)−1 + ( f − 1)−1E(k − Nemp)−1 ) )
31
G Proof of Lemma 7 g(f ; k ) =
To show g(f ; , k ) ≤ 1 , it suffices to show h(f ; k ) = ( f + k − 1 ) 1 −1 − 1
1 k f 1 + 1 −,1 − 1 kf! − f ≥ 0
1 f − 1 − k f − 1
( note that h(1 ; k ) = 0 , h(2 ; k ) > 0 ) for which it suffices to show
∂h(f ; k )
∂f
= 1 −1 − and hence it suffices to show −1 − ( f + k − 1 ) log,1 − 1
This completes the proof .
1 kf! + ( f + k − 1 ) −1 −
1
1 kf log1 − k! − 1 ≥ 0 k ≥ 0 , which is true because log,1 − 1 k . k < − 1
H Proof of Lemma 8
Recall we first divide the D elements into k bins whose lengths are multinomial distributed with equal probability 1 k . We denote their lengths by Lj , j = 1 to k . In other words ,
( L1 , L2 , , Lk ) ∼ multinomialD ,
1 k
,
1 k
, ,
1 k and we know
E(Lj ) =
D k
,
V ar(Lj ) = D
1 k 1 −
1 k ,
Cov(Li , Lj ) = −
D k2
Define
We know
Thus
Ii,j =fl 1
0 if the i th element is hashed to the j th bin otherwise
( 34 )
E(Ii,j ) =
1 k
,
E(I 2 i,j ) =
E(1 − Ii,j ) = 1 −
1 k
,
E(Ii,jIi,j ′ ) = 0 ,
,
1 k E(1 − Ii,j)2 = 1 −
E(Ii,jIi′,j ) =
1 k2 ,
1 k
,
E(1 − Ii,j)(1 − Ii,j ′ ) = 1 −
2 k
Nemp = k
Xj=1 Yi∈S1∪S2
( 1 − Ii,j )
32
E ( Nemp ) = k
Xj=1 Yi∈S1∪S2
E ( (1 − Ii,j ) ) = k1 −
1 kf1+f2−a k
( 1 − Ii,j)2 +Xj6=j ′ Yi∈S1∪S2 + k(k − 1)1 − kf1+f2−a
( 1 − Ii,j),1 − Ii,j ′kf1+f2−a
2
E,N 2 emp =
1
Xj=1 Yi∈S1∪S2 =k1 − kf1+f2−a
1
V ar ( Nemp ) =k1 −
+ k(k − 1)1 −
2 kf1+f2−a
− k21 −
1 k2(f1+f2−a )
Therefore ,
V ar ( Nemp ) k2
=
<
1 k 1 − −1 − k 1 −
1
1
1
1 kf1+f2−a 1 −1 − k 1 − k2(f1+f2−a ) kf1+f2−a 1 −1 − kf1+f2−a! −1 − kf1+f2−a!
1
1
1
2 kf1+f2−a!
This completes the proof of Lemma 8 .
33
