Equip Tourists with Knowledge Mined from Travelogues
Qiang Hao†* , Rui Cai‡ , Changhu Wang‡ , Rong Xiao‡ , Jiang Ming Yang‡ , Yanwei Pang† , Lei Zhang‡
†Tianjin University , Tianjin 300072 , PR China
‡Microsoft Research Asia , Beijing 100190 , PR China
†{qhao , pyw}@tjueducn ; ‡{ruicai , chw , rxiao , jmyang , leizhang}@microsoft.com
ABSTRACT With the prosperity of tourism and Web 2.0 technologies , more and more people have willingness to share their travel experiences on the Web ( eg , weblogs , forums , or Web 2.0 communities ) . These so called travelogues contain rich information , particularly including location representative knowledge such as attractions ( eg , Golden Gate Bridge ) , styles ( eg , beach , history ) , and activities ( eg , diving , surfing ) . The location representative information in travelogues can greatly facilitate other tourists’ trip planning , if it can be correctly extracted and summarized . However , since most travelogues are unstructured and contain much noise , it is difficult for common users to utilize such knowledge effectively . In this paper , to mine location representative knowledge from a large collection of travelogues , we propose a probabilistic topic model , named as Location Topic model . This model has the advantages of ( 1 ) differentiability between two kinds of topics , ie , local topics which characterize locations and global topics which represent other common themes shared by various locations , and ( 2 ) representation of locations in the local topic space to encode both location representative knowledge and similarities between locations . Some novel applications are developed based on the proposed model , including ( 1 ) destination recommendation for on flexible queries , ( 2 ) characteristic summarization for a given destination with representative tags and snippets , and ( 3 ) identification of informative parts of a travelogue and enriching such highlights with related images . Based on a large collection of travelogues , the proposed framework is evaluated using both objective and subjective evaluation methods and shows promising results .
Categories and Subject Descriptors I51 [ Pattern Recognition ] : Models – statistical ; H33 [ Information Storage and Retrieval ] : Information Search and Retrieval – information filtering
General Terms Algorithms , Experimentation .
Keywords Travelogue mining , probabilistic topic model , recommendation .
1 . INTRODUCTION Travel , as an integral part of human history , has become more and more popular in people's everyday lives in recent years , partly owing to the increasing amount of travel related information and services on the Web , which provide people with efficient ways to
* This work was performed at Microsoft Research Asia .
Copyright is held by the International World Wide Web Conference Committee ( IW3C2 ) . Distribution of these papers is limited to classroom use , and personal use by others . WWW 2010 , April 26 30 , 2010 , Raleigh , North Carolina , USA . ACM 978 1 60558 799 8/10/04 . online plan and prepare for their trips . Meanwhile , Web 2.0 technologies facilitate and encourage people to contribute rather than just obtain information , leading to a huge amount of usergenerated content ( UGC ) on the Web . In the tourism domain , more and more people have willingness to record and share their travel experiences on weblogs , forums or travel communities , in the form of textual travelogues and photos taken during the trips .
Since travel related UGC not only underlies the communities and social network among travelers but also provides other web users with rich information related to travel , how to leverage it has attracted extensive attention in the literature . For instance , a lot of work has been proposed to mine knowledge from user contributed photos on Flickr [ 6 ] to support various applications such as landmark discovery and recognition [ 22 ] , landmark image selection [ 11][18 ] , location explorer [ 1][5 ] , and image tag suggestion [ 15 ] . By contrast , fewer research efforts have been dedicated to knowledge mining from travelogues . One related work is [ 8 ] , in which the authors proposed to generate overviews for locations by mining representative tags from travelogues . However , to the best of our knowledge , the complete framework of travelogue mining and its applications has not been specially investigated .
We claim that travelogues can serve as a promising resource of travel related knowledge , which is complementary to usergenerated photos because travelogues cover various travel related aspects , including not only landmarks and natural things which correspond to specific visual descriptions in photos , but also abstract aspects ( eg , history , culture , genius loci ) which are informative to tourists but difficult to visualize using photos . With such rich information , travelogues could support more comprehensive descriptions of locations and comparisons between locations than user generated photos , and thus could be leveraged to recommend locations according to various queries . In addition , travelogues contain rich textual contexts of locations to meet various information needs . For example , representative snippets can be extracted to describe a location ’s characteristics and linked to original travelogues as detailed context .
Furthermore , in spite of the access to a great deal of structured travel related information ( eg , vacation packages , flights , hotels ) offered by travel websites and travel agents , many people who are planning a trip prefer to learn experience and guidance from other travelers . Travelogues supplement this structured information with unstructured but personal descriptions of tourist destinations and services . Although the information in a single travelogue is possibly noisy or biased , numerous travelogues as a whole could reflect people ’s overall preference and understanding of travel resources , and thus can serve as a reliable knowledge source .
However , acquiring the knowledge in travelogues is a non trivial task , especially for common users . Actually , there is a gap between raw travelogues and the information needs of tourists due to the data ’s intrinsic limitations listed as follows :
WWW 2010 • Full PaperApril 26 30 • Raleigh • NC • USA401 Travelogues
We went back to the hotel , packed our bags and took the shuttle back to the airport for our flight from Oahu to Kauai…
Since we had a few hours to kill before our big sunset dinner cruise , we went and checked out Spouting Horn near Poipu Beach . It's a pretty cool thing so see because as water rushes under a lava shelf , it spurts through a small opening at the surface . A very unique sight! … .
After a few photos , we continued on our drive . What a scenic ride! We drove along the eastern coastline and up to Kilauea , Princeville and Hanaleijust beautiful . We stopped along the way for lots of pictures , one place had at least a dozen chickens in the parking lot
Oahu , Kauai , Poipu Beach ,
Kilauea , Princeville … locations surface , water … lava coastline , sunset local topics hotel , shuttle airport , flight picture , photo global topics
Figure 1 . Different topics in travelogues , where local topics are shown in italic and blue ; global topics are shown in green ; and locations are shown in underline and red . x Noisy topics : As other UGC , most travelogues are unstructured and contain much noise . For instance , the depictions of destinations and attractions , in which the common tourists are most interested , are usually intertwined with topics common in travelogues related to various locations . x Multiple viewpoints : For each destination , there are usually various descriptions coming from many previous travelers . When trying to comprehensively know about a destination , users usually confront a dilemma that the viewpoint in each single travelogue may be biased , while it is time consuming to read and summarize a number of related travelogues to outline an overview of the destination ’s characteristics . x Lack of destination recommendation : Although a large collection of travelogues can cover most of popular destinations in the world , the depictions in a single travelogue usually focus on only one or a few destinations . Hence , for tourists who have particular travel intentions ( eg , go to a beach , go hiking ) and need to determine where to go , there is no straightforward and effective way to obtain recommended destinations , except for surveying a lot of travelogues . x Lack of destination comparison : In travelogues , besides the explicit comparison made by the authors , there is little information about the similarity between destinations , which is helpful for tourists who need suggestions about destinations similar ( or dissimilar ) to the ones that they are familiar with .
To overcome these limitations of raw travelogue data and bridge the gap to real information needs , several kinds of information processing techniques need to be leveraged . ( 1 ) For the issue of noisy topics , we need to discover topics from travelogues and further distinguish location related topics with other noisy ones . ( 2 ) For the issue of multiple viewpoints , we need to find a representation of locations that summarizes all the useful descriptions of a location to capture its location representative knowledge ( ie , local characteristics such as attractions , activities , styles ) . ( 3 ) To provide destination recommendation , a metric of relevance is necessary to suggest locations most relevant to tourists’ travel intentions . ( 4 ) For destination comparison , a location similarity metric is necessary to compare locations from the perspective of travel . We believe that the first two points should be given primary importance because the location representative knowledge mined from location related topics underlies the ranking and similarity measurement of locations .
In this paper , we consider the above issues and investigate the problem of mining location representative knowledge 1 from a
1 Knowledge useful for tourists ( eg , accommodation , expense ) but not specific to locations is beyond our objective in this paper .
Location Extraction
Travelogue
Modeling
Location
Representative
Knowledge
Destination
Recommendation
Destination
Summarization
Travelogue Enrichment
User
Travelogues
Knowledge Mining
Applications
Figure 2 . The overview of the proposed framework , in which location representative knowledge is first mined from a travelogue corpus , and then used to support three applications . large amount of travelogues to facilitate tourists to fully utilize such knowledge .
In recent years , probabilistic topic models , such as latent Dirichlet allocation ( LDA ) [ 2 ] , have been successfully applied to a variety of text mining tasks [ 3 , 4 , 13 , 14 , 16 , 17 , 19 , 20 ] . This kind of models are suitable for the task of travelogue mining owing to its powerful capability of discovering latent topics from text and representing documents with such topics . However , to the best of our knowledge , the existing models are not applicable for our objective because none of them can be used to address the limitations of travelogue data . Specifically , although documents under these models are represented as mixtures of the discovered latent topics , the entities appearing in the documents ( eg , locations mentioned in travelogues ) either lack of representation in the topic space , or are represented as mixtures of all the topics , rather than the topics appropriate to characterize these entities . Considering the noisy topics in travelogues , the representation of locations using all the topics would be contaminated by the noise and thus is unreliable for further relevance and similarity metrics .
Therefore , we propose a probabilistic topic model , ie , LocationTopic ( LT ) model , to discover topics from travelogues and simultaneously represent locations with appropriate topics . Specifically , we define two different types of topics ( as illustrated in Figure 1 ) , ie , local topics which characterize specific locations from the perspective of travel ( eg , lava , coastline ) , and global topics ( eg , hotel , airport ) which do not characterize certain locations but rather extensively co occur with various locations in travelogues . Since each local topic corresponds to some specific locations and travel related characteristics , a location ’s overall characteristics can be generally represented in the local topic space , as a mixture of ( ie , a multinomial distribution over ) local topics .
Based on the LT model , the aforementioned limitations of travelogue data are handled to some extent because : x By decomposing travelogues into local and global topics , we can obtain location representative knowledge from local topics , with other semantics captured by global topics filtered out . x By representing each location using local topics mined from the entire travelogue collection , multiple viewpoints of each location can be naturally summarized . x Based on the representation of locations in the local topic space , both the relevance of a location to a given travel intention and the similarity between locations can be measured .
As shown in Figure 2 , given a collection of travelogues ( in the implementation , either of two data sets : 100K English travelogues , or 94K Chinese ones ) , we first extract the locations mentioned in the text . Then a LT model is trained on the collection to learn local and global topics , as well as the representation of locations
WWW 2010 • Full PaperApril 26 30 • Raleigh • NC • USA402 Document
Local Topic m r e T
=
λ × m r e T
×
Document
Global Topic
+ ( 1 λ ) × m r e T
×
× n o i t a c o L i c p o T
Document l a b o G l
( cid:1868)(cid:4666)(cid:1875)(cid:513)(cid:1856)(cid:4667)fifi(cid:2019)(cid:3400)(cid:3533 ) ( cid:3533 ) ( cid:1868)(cid:4666)(cid:1875)(cid:513)(cid:1878)(cid:4667)(cid:1868)(cid:4666)(cid:1878)(cid:513)(cid:1864)(cid:4667)(cid:1868)(cid:4666)(cid:1864)(cid:513)(cid:1856)(cid:4667 )
( I )
( cid:1846)(cid:1859)(cid:1864)(cid:1878)(cid:1314)(cid:883 ) fi(cid:3397)fi(cid:4666)(cid:883)(cid:2019)(cid:4667)(cid:3400)(cid:3533 )
( cid:1868)(cid:4666)(cid:1875)(cid:513)(cid:1878)(cid:1314)(cid:4667)(cid:1868)(cid:4666)(cid:1878)(cid:1314)(cid:513)(cid:1856)(cid:4667 )
( II ) i c Location p o T l a c o L
( cid:1846)(cid:1864)(cid:1867)(cid:1855)(cid:1878)(cid:883 )
( cid:1838)(cid:1864)(cid:883 )
Figure 3 . An illustration of the travelogue decomposition with ( I ) Tloc local topics and ( II ) Tgl global topics . It should be noted that this figure mainly serves as an illustrative interpretation of the ideas , but does not exactly accord with the model details . in the local topic space . Based on the learnt knowledge , we can fulfill different application tasks . Specifically , we consider a scenario where a user learns online knowledge to plan a trip in three steps : 1 ) selecting a destination from some recommended ones , 2 ) browsing the characteristics of the selected destination to get an overview , and 3 ) browsing some travelogues to figure out detailed travel route . To facilitate these three steps , the following three applications are implemented , respectively : x Destination Recommendation : We recommend destinations to users , in terms of either similarity to a given destination or relevance to a given travel intention . x Destination Summarization : Each destination is presented as an overview by summarizing its representative aspects with textual tags . Representative snippets are also offered as further descriptions to verify and interpret the relation between each tag and the destination . x Travelogue Enrichment : To help a user better browse and understand travelogues , we identify the informative parts of a travelogue and highlight them with related images .
The paper is organized as follows . In Section 2 , we introduce the proposed Location Topic model . Then we describe three applications of the LT model in Section 3 . Experimental and evaluation results are shown in Section 4 . Section 5 presents the related work . In Section 6 , we give the conclusion and future work .
2 . TRAVELOGUE MODELING In this section , we present the Location Topic ( abbreviated as LT ) model and its usage for further applications . By modeling the generative process of travelogues , the model could discover topics from travelogues and represent locations with the learnt topics .
2.1 Basic Idea Following the existing work on probabilistic topic models , we treat each travelogue document as a mixture of topics , where each topic is a multinomial distribution over terms in vocabulary and corresponds to some specific semantics . As discussed in Section 1 , we further assume that travelogues are composed of local and global topics , and each location is represented by a multinomial distribution over local topics . Thus , the proposed LT model aims at discovering local and global topics , as well as each location ’s distribution over local topics , from a travelogue collection .
We use Figure 3 to provide an illustrative and intuitive explanation how we decompose travelogue documents into local topics and global topics . A travelogue collection can be represented by a
Term Document matrix where the th column encodes the th doc ument ’s distribution over terms , as illustrated at the top left of Figure 3 . Based on this representation , our goal is equivalent to decomposing a given Term Document matrix into multiple ma trices , including Term LocalTopic , Term GlobalTopic , and LocalTopic Location matrices . In Figure 3 , there are another two matrices that we should learn , ie , GlobalTopic Document matrix and Location Document matrix . The former is the same as that of common topic models , whereas the latter is specific and important to our objective , and thus need particular discussion .
For Location Document matrix , we have some observed information , namely the user provided location labels associated with each travelogue . However , such document level labels are not fit for our scenario because they are usually too coarse and incomplete to support knowledge mining for all the locations described in travelogues , and sometimes they are even labeled incorrectly . Hence , we rely on the locations extracted from text instead of these labels . There are several methods for location extraction , eg , looking up a gazetteer , or applying a Web service like Yahoo Placemaker2 . As such pre processing is not our focus , we employ a beforehand implemented location extractor based on a gazetteer and location disambiguation algorithms handling geographic hierarchy and context of locations , which can achieve high accuracy by considering all the candidate locations in a document simultaneously . We will detail this location extractor elsewhere .
Intuitively , the extracted locations can serve as strong indications of locations described in travelogues . However , these extracted locations are improper to be taken as the real Location Document matrix , due to an observed gap between them and the locations actually described . For instance , a series of locations may be mentioned only as a trip summary , but without ( or with quite unequal ) descriptions in the contextual text . Besides , we also observe that in a typical travelogue , the author usually concentrates on depicting some locations in consecutive sentences . That is , consecutive words tend to correspond to the same locations . Considering these observations , we assume that all the words in a text segment ( eg , a document , paragraph , or sentence ) share a multinomial distribution over locations , which is affected by a Dirichlet prior derived from the extracted locations in the segment . In this way , the Location Document matrix is kept variable to better model the data , while also benefiting from the extracted locations as priors .
As the decomposition of likelihood ( cid:1868)(cid:4666)(cid:1875)(cid:513)(cid:1856)(cid:4667 ) shown in Figure 3 , each word in a document is assumed to be “ written ” by making a binary decision between two paths , ie , ( 1 ) selecting a location , a local topic , and a term in sequence , and ( 2 ) selecting a global topic and a term in sequence . Once decomposed as above , a travelogue collection preserves its location representative knowledge in LocalTopic Location matrix , and topics in Term LocalTopic and Term GlobalTopic matrices .
2 http://developeryahoocom/geo/placemaker/
WWW 2010 • Full PaperApril 26 30 • Raleigh • NC • USA403 ( cid:574 )
( cid:142 )
( cid:154 ) ffl ( cid:153 ) ( cid:134 )
( cid:156 )
( cid:588)(cid:134)(cid:481)(cid:149 ) ( cid:590)(cid:134)(cid:481)(cid:149 ) ( cid:134 )
Figure 4 . Graphical representation of the proposed LT model . words , ( 2 ) a binomial distribution over global topics versus local
We extend the extensively used bag of words assumption to treat nomial distribution over local topics , with symmetric Dirichlet a segment could be a sentence , a paragraph , or a sliding window
2.2 Generative Process of Travelogues fi(cid:142)(cid:145)fi(cid:596)(cid:142)(cid:145)fi ( cid:596)(cid:137)(cid:142 ) fi(cid:137)(cid:142 )
( cid:598)(cid:134)(cid:481)(cid:149 ) ( cid:580)(cid:142)(cid:145)fi ( cid:576 ) ( cid:580)(cid:137)(cid:142 ) ( cid:573 ) In the LT model , each location ( cid:1864 ) is represented byfi(cid:2032)(cid:1864 ) , a multipriorfi(cid:2010 ) ; each document ( cid:1856 ) is associated withfi(cid:2016)(cid:1856 ) , a multinomial distribution over global topics , with symmetric Dirichlet priorfi(cid:2009 ) . each document ( cid:1856 ) as a set of ( cid:1856 ) non overlapping segments , where in the document . Each segment is associated with ( 1 ) a bag oftopicsfi(cid:2024)(cid:1856)(cid:481 ) , with Beta priorfi(cid:2011)(cid:4668)(cid:2011)(cid:1859)(cid:1864)(cid:481)(cid:2011)(cid:1864)(cid:1867)(cid:1855)(cid:4669 ) , and ( 3 ) a multinomial distribution ( cid:2022)(cid:1856)(cid:481 ) over segmentfi ’s corresponding location set ( cid:2278)(cid:1856)(cid:481)(cid:1565)(cid:4668)(cid:1864)(cid:513)(cid:1864)fi(cid:131)(cid:146)(cid:146)(cid:135)(cid:131)(cid:148)(cid:149)fi(cid:139)(cid:144)fi(cid:149)(cid:135)(cid:137)(cid:143)(cid:135)(cid:144)(cid:150)fifi(cid:139)(cid:144)fi(cid:1856)(cid:4669 ) , with Dirichlet prior parameterized by ( cid:2031)(cid:1856)(cid:481 ) defined as ( cid:2031)(cid:1856)(cid:481)(cid:1565)(cid:3419)(cid:2012)(cid:1856)(cid:481)(cid:481)(cid:1864)(cid:2020)(cid:942)(cid:851)(cid:4666)(cid:1864)fi(cid:131)(cid:146)(cid:146)(cid:135)(cid:131)(cid:148)(cid:149)fi(cid:139)(cid:144)fi(cid:149)(cid:135)(cid:137)(cid:143)(cid:135)(cid:144)(cid:150)fifi(cid:139)(cid:144)fi(cid:1856)(cid:4667)(cid:3423)(cid:1864)(cid:1488)(cid:2278)(cid:1856)(cid:481 ) , where “ ( cid:851)(cid:4666)(cid:942)(cid:4667 ) ” is short for “ the number of times ” and coefficient ( cid:2020 ) tionfi(cid:1829 ) , which consists of ( cid:1830 ) documents covering ( cid:1838 ) unique locations and ( cid:1849 ) unique terms , is defined as follows : x For each local topicfi(cid:1878)(cid:1488)(cid:4668)(cid:883)(cid:481)(cid:485)(cid:481)(cid:1846)(cid:1864)(cid:1867)(cid:1855)(cid:4669 ) , draw a multinomial distribution over terms , ( cid:2030)(cid:1878)(cid:1864)(cid:1867)(cid:1855)(cid:817)(cid:1830)(cid:1861)(cid:4666)(cid:2015)(cid:1864)(cid:1867)(cid:1855)(cid:4667 ) . x For each global topicfi(cid:1878)(cid:1488)(cid:4668)(cid:883)(cid:481)(cid:485)(cid:481)(cid:1846)(cid:1859)(cid:1864)(cid:4669 ) , draw a multinomial distribution over terms , ( cid:2030)(cid:1878)(cid:1859)(cid:1864)(cid:817)(cid:1830)(cid:1861)(cid:4666)(cid:2015)(cid:1859)(cid:1864)(cid:4667 ) . x For each locationfi(cid:1864)(cid:1488)(cid:4668)(cid:883)(cid:481)(cid:485)(cid:481)(cid:1838)(cid:4669 ) , draw a multinomial distribution over local topics , ( cid:2032)(cid:1864)(cid:817)(cid:1830)(cid:1861)(cid:4666)(cid:2010)(cid:4667 ) . x For each document ( cid:1856)(cid:1488)(cid:4668)(cid:883)(cid:481)(cid:485)(cid:481)(cid:1830)(cid:4669 ) : ( cid:2016)(cid:1856)(cid:817)(cid:1830)(cid:1861)(cid:4666)(cid:2009)(cid:4667 ) . o For each segment of document ( cid:1856 ) : cal topics , ( cid:2024)(cid:1856)(cid:481)(cid:817)(cid:1828)(cid:1857)(cid:1853)(cid:4666)(cid:2011)(cid:4667 ) ;  draw a multinomial distribution over locations in , ( cid:2022)(cid:1856)(cid:481)(cid:817)(cid:1830)(cid:1861)(cid:3435)(cid:2031)(cid:1856)(cid:481)(cid:3439 ) . o For each word ( cid:1875)(cid:1856)(cid:481 ) in segment of document ( cid:1856 ) :  draw a binary switch ( cid:1876)(cid:1856)(cid:481)(cid:817)(cid:1828)(cid:1861)(cid:1867)(cid:1865)(cid:1861)(cid:1853)(cid:1864)(cid:3435)(cid:2024)(cid:1856)(cid:481)(cid:3439 ) ;  if ( cid:1876)(cid:1856)(cid:481)(cid:1864)(cid:1867)(cid:1855 ) , draw a location ( cid:1864)(cid:1856)(cid:481)(cid:817)(cid:1839)(cid:1864)(cid:1861)(cid:1867)(cid:1865)(cid:1861)(cid:1853)(cid:1864)(cid:3435)(cid:2022)(cid:1856)(cid:481)(cid:3439 ) , and then draw a local topic ( cid:1878)(cid:1856)(cid:481)(cid:817)(cid:1839)(cid:1864)(cid:1861)(cid:1867)(cid:1865)(cid:1861)(cid:1853)(cid:1864)(cid:3435)(cid:2032)(cid:1864)(cid:1856)(cid:481)(cid:3439 ) ;  if ( cid:1876)(cid:1856)(cid:481)(cid:1859)(cid:1864 ) , draw a global topic ( cid:1878)(cid:1856)(cid:481)(cid:817)(cid:1839)(cid:1864)(cid:1861)(cid:1867)(cid:1865)(cid:1861)(cid:1853)(cid:1864)(cid:4666)(cid:2016)(cid:1856)(cid:4667 ) ;  draw word ( cid:1875)(cid:1856)(cid:481)(cid:817)(cid:1839)(cid:1864)(cid:1861)(cid:1867)(cid:1865)(cid:1861)(cid:1853)(cid:1864)(cid:4672)(cid:2030)(cid:1878)(cid:1856)(cid:481)(cid:1876)(cid:1856)(cid:481)(cid:4673 ) . denotes the precision of the prior . In the implementation , each paragraph in a travelogue is treated as a raw segment , with further merging to ensure that each segment contains at least one location .
The graphical representation of the LT model is shown in Figure 4 . Accordingly , travelogue
 draw a binomial distribution over global topics versus lo o Draw a multinomial distribution over global topics , the generative process of a lapsed Gibbs sampling [ 7 ] with the following updating formulas . of global/local binary switches , locations , and topics for all the
2.3 Parameter Estimation To estimate the parameters of the LT model , we need to estimate the latent variables conditioned on the observed variables , namely
( cid:1868)(cid:4666)(cid:2206)(cid:481)(cid:2194)(cid:481)(cid:2208)(cid:513)(cid:2205)(cid:481)(cid:2238)(cid:481)(cid:2009)(cid:481)(cid:2010)(cid:481)(cid:2237)(cid:481)(cid:2015)(cid:4667 ) , where ( cid:2206)(cid:481)(cid:2194)(cid:481)(cid:2208 ) are vectors of assignments words in travelogue collectionfi(cid:1829 ) , respectively . We use the colFor global topicfi(cid:1878)(cid:1488)(cid:4668)(cid:883)(cid:481)(cid:485)(cid:481)(cid:1846)(cid:1859)(cid:1864)(cid:4669 ) , ( cid:1868)(cid:3435)(cid:1876)(cid:1861)(cid:1859)(cid:1864)(cid:481)(cid:1878)(cid:1861)(cid:1878)(cid:3627)(cid:1875)(cid:1861)(cid:1875)(cid:481)(cid:2206)(cid:819)(cid:1861)(cid:481)(cid:2208)(cid:819)(cid:1861)(cid:481)(cid:2205)(cid:819)(cid:1861)(cid:481)(cid:2009)(cid:481)(cid:2237)(cid:481)(cid:2015)(cid:1859)(cid:1864)(cid:3439 ) ( cid:3397)(cid:1849)(cid:2015)(cid:1859)(cid:1864)(cid:942 ) ( cid:1856)(cid:481)(cid:819)(cid:1861)(cid:1859)(cid:1864)(cid:481)(cid:1878)(cid:3397)(cid:2009 ) ( cid:1875)(cid:481)(cid:819)(cid:1861)(cid:1859)(cid:1864)(cid:481)(cid:1878)(cid:3397)(cid:2015)(cid:1859)(cid:1864 ) ( cid:1856)(cid:481)(cid:819)(cid:1861)(cid:1859)(cid:1864 ) ( cid:3397)(cid:1846)(cid:1859)(cid:1864)(cid:2009)(cid:942)(cid:4672)(cid:1856)(cid:481)(cid:481)(cid:819)(cid:1861 ) ( cid:1859)(cid:1864 ) ( cid:3397)(cid:2011)(cid:1859)(cid:1864)(cid:4673)(cid:484 ) ( cid:1503 ) ( cid:963 ) ( cid:1875)(cid:1314)(cid:481)(cid:819)(cid:1861 ) ( cid:1859)(cid:1864)(cid:481)(cid:1878 ) ( cid:1875)(cid:1314 ) For local topicfi(cid:1878)(cid:1488)(cid:4668)(cid:883)(cid:481)(cid:485)(cid:481)(cid:1846)(cid:1864)(cid:1867)(cid:1855)(cid:4669 ) and locationfi(cid:1864)(cid:1488)(cid:2278)(cid:1856)(cid:481 ) , ( cid:1868)(cid:3435)(cid:1876)(cid:1861)(cid:1864)(cid:1867)(cid:1855)(cid:481)(cid:1864)(cid:1861)(cid:1864)(cid:481)(cid:1878)(cid:1861)(cid:1878)(cid:3627)(cid:1875)(cid:1861)(cid:1875)(cid:481)(cid:2206)(cid:819)(cid:1861)(cid:481)(cid:2194)(cid:819)(cid:1861)(cid:481)(cid:2208)(cid:819)(cid:1861)(cid:481)(cid:2205)(cid:819)(cid:1861)(cid:481)(cid:2010)(cid:481)(cid:2237)(cid:481)(cid:2015)(cid:1864)(cid:1867)(cid:1855)(cid:3439 ) ( cid:1503 ) ( cid:1875)(cid:481)(cid:819)(cid:1861 ) ( cid:1864)(cid:481)(cid:819)(cid:1861)(cid:3397)(cid:1846)(cid:1864)(cid:1867)(cid:1855)(cid:2010)(cid:942)(cid:1856)(cid:481)(cid:481)(cid:819)(cid:1861 ) ( cid:3397)(cid:1849)(cid:2015)(cid:1864)(cid:1867)(cid:1855)(cid:942 ) ( cid:1864)(cid:481)(cid:819)(cid:1861 ) ( cid:1864)(cid:1867)(cid:1855 ) ( cid:3397)(cid:2031)(cid:1856)(cid:481)(cid:942)(cid:3435)(cid:1856)(cid:481)(cid:481)(cid:819)(cid:1861 ) ( cid:1864)(cid:1867)(cid:1855 ) ( cid:3397)(cid:2011)(cid:1864)(cid:1867)(cid:1855)(cid:3439 ) , ( cid:1864)(cid:1867)(cid:1855)(cid:481)(cid:1878)(cid:3397)(cid:2015)(cid:1864)(cid:1867)(cid:1855 ) ( cid:1864)(cid:1867)(cid:1855)(cid:481)(cid:1878)(cid:3397)(cid:2010 ) ( cid:3397)(cid:2031)(cid:1856)(cid:481)(cid:481)(cid:1864 ) ( cid:1864 ) ( cid:1856)(cid:481)(cid:481)(cid:819)(cid:1861 ) ( cid:963 ) ( cid:1875)(cid:1314)(cid:481)(cid:819)(cid:1861 ) ( cid:1864)(cid:1867)(cid:1855)(cid:481)(cid:1878 ) ( cid:1875)(cid:1314 ) where ( cid:1875)(cid:481)(cid:819)(cid:1861)(cid:1859)(cid:1864)(cid:481)(cid:1878 ) is the number of times term ( cid:1875 ) is assigned to global ( cid:1864)(cid:1867)(cid:1855)(cid:481)(cid:1878 ) is that for local topicfi(cid:1878 ) . ( cid:1856)(cid:481)(cid:819)(cid:1861)(cid:1859)(cid:1864)(cid:481)(cid:1878 ) is the topic ( cid:1878 ) , and similarly ( cid:1875)(cid:481)(cid:819)(cid:1861 ) number of times a word in document ( cid:1856 ) is assigned to global topic is the number of times a word in document ( cid:1856 ) is ( cid:1878 ) , while ( cid:1856)(cid:481)(cid:819)(cid:1861)(cid:1859)(cid:1864 ) assigned to a global topic . ( cid:1864)(cid:481)(cid:819)(cid:1861 ) ( cid:1864)(cid:1867)(cid:1855)(cid:481)(cid:1878 ) is the number of times a word assigned to location ( cid:1864 ) is assigned to local topic ( cid:1878 ) , out of ( cid:1864)(cid:481)(cid:819)(cid:1861 ) words assigned to location ( cid:1864 ) in total . ( cid:1856)(cid:481)(cid:481)(cid:819)(cid:1861 ) ( cid:1864 ) a word in segment of document ( cid:1856 ) is assigned to location ( cid:1864 ) , and and ( cid:1856)(cid:481)(cid:481)(cid:819)(cid:1861 ) consequently to a local topic . ( cid:1856)(cid:481)(cid:481)(cid:819)(cid:1861 ) ( cid:1859)(cid:1864 ) ( cid:1864)(cid:1867)(cid:1855 ) of times a word in segment of document ( cid:1856 ) is assigned to global and to local topics , respectively . For all the counts , subscript ( cid:819)(cid:1861 ) indicates that thefi(cid:1861) th word is excluded from the computation . ( cid:2030)(cid:1878)(cid:481)(cid:1875)(cid:1876 ) ( cid:1503)(cid:1875)(cid:1876)(cid:481)(cid:1878)(cid:3397)(cid:2015)(cid:1876)(cid:481)(cid:1876)(cid:1488)(cid:4668)(cid:1859)(cid:1864)(cid:481)(cid:1864)(cid:1867)(cid:1855)(cid:4669)(cid:481)(cid:1878)(cid:883)(cid:481)(cid:485)(cid:481)(cid:1846)(cid:1876 ) , ( cid:2032)(cid:1864)(cid:481)(cid:1878)(cid:1503)(cid:1864)(cid:1864)(cid:1867)(cid:1855)(cid:481)(cid:1878)(cid:3397)(cid:2010)(cid:481)(cid:1878)(cid:883)(cid:481)(cid:485)(cid:481)(cid:1846)(cid:1864)(cid:1867)(cid:1855 ) .
After such a Gibbs sampler reaches burn in , we can harvest several samples and count the assignments to estimate the parameters :
2.4 Utilizing the Model Once estimated , the parameters of the LT model can support several applications by providing the data representations and similarity metrics for both locations and terms . is the number of times denote the number
241 Location Representation and Similarity Metric ing multinomial distribution over local topics . For the latter , we derive a probability distribution over terms conditioned on loca
Each location ( cid:1864 ) can be represented in either the ( cid:1846)(cid:1864)(cid:1867)(cid:1855 ) dimensional local topic space or the ( cid:1849) dimensional term space . For the former , location ( cid:1864 ) is simply represented byfi(cid:2032)(cid:1864 ) namely its correspondtion ( cid:1864 ) directly from the raw Gibbs samples , by counting the words assigned to locationfi(cid:1864 ) , as ( cid:1868)(cid:4666)(cid:1875)(cid:513)(cid:1864)(cid:4667)(cid:1503)(cid:1864)(cid:1875)(cid:481)fififi(cid:1875)(cid:883)(cid:481)(cid:485)(cid:481)(cid:1849 ) , where ( cid:1864)(cid:1875 ) is the number of times term ( cid:1875 ) is assigned to locationfi(cid:1864 ) . the symmetric similarity between two locations ( cid:1864)(cid:883 ) and ( cid:1864)(cid:884 ) is measdistributions over local topics ( cid:2032)(cid:1864)(cid:883 ) andfi(cid:2032)(cid:1864)(cid:884 ) , as ( cid:1838)(cid:1867)(cid:1855)(cid:1861)(cid:1865)(cid:4666)(cid:1864)(cid:883)(cid:481)(cid:1864)(cid:884)(cid:4667)(cid:1857)(cid:1876)(cid:1868)(cid:3419)(cid:2028)(cid:1830)(cid:1836)(cid:3435)(cid:2032)(cid:1864)(cid:883)(cid:2032)(cid:1864)(cid:884) (cid:3439)(cid:3423 ) ,
According to the location representation in the local topic space , ured by the distance between their corresponding multinomial
WWW 2010 • Full PaperApril 26 30 • Raleigh • NC • USA404 where ( cid:1830)(cid:1836)(cid:4666) (cid:942)(cid:513)(cid:513)(cid:942) (cid:4667 ) denotes the Jensen Shannon ( JS ) divergence defined as ( cid:1830)(cid:1836)(cid:4666)(cid:1868)(cid:1313)(cid:1869) (cid:4667)(cid:883)(cid:884)(cid:1830)(cid:1838)(cid:4672)(cid:1868)(cid:4699)(cid:1868)(cid:3397)(cid:1869)(cid:884) (cid:4673)(cid:3397)(cid:883)(cid:884)(cid:1830)(cid:1838)(cid:4672)(cid:1869)(cid:4699)(cid:1868)(cid:3397)(cid:1869)(cid:884) (cid:4673 ) , while ( cid:1830)(cid:1838)(cid:4666) (cid:942)(cid:513)(cid:513)(cid:942) (cid:4667 ) denotes the Kullback Leibler ( KL ) divergence ; coefficient ( cid:2028)(cid:3408)(cid:882 ) is used to normalize different numbers of local topics .
243 Inference
242 Term Representation and Similarity Metric In addition to that of locations , we also need a representation and corresponding similarity metric of terms , so as to measure the relevance of a location ( or a snippet ) to a given query term in the application of destination recommendation ( or summarization ) .
Hence , we expand each term ( cid:1875 ) in the vocabulary into a probability distribution over the learnt ( cid:1846)(cid:1864)(cid:1867)(cid:1855 ) local topics , denoted byfi(cid:2012)(cid:1875 ) , as ( cid:2012)(cid:1875)(cid:4668)(cid:1868)(cid:4666)(cid:1878)(cid:513)(cid:1875)(cid:4667)(cid:4669)(cid:1878)(cid:883)(cid:1846)(cid:1864)(cid:1867)(cid:1855 ) ( cid:1868)(cid:4666)(cid:1878)(cid:513)(cid:1875)(cid:4667)(cid:1503)(cid:1868)(cid:4666)(cid:1875)(cid:513)(cid:1878)(cid:4667)(cid:1868)(cid:4666)(cid:1878)(cid:4667)(cid:1503)(cid:2030)(cid:1878)(cid:481)(cid:1875)(cid:1864)(cid:1867)(cid:1855)(cid:1878)(cid:1864)(cid:1867)(cid:1855) , ( cid:4682)fifi where ( cid:1878)(cid:1864)(cid:1867)(cid:1855 ) is the total number of words assigned to local topic ( cid:1878 ) . Accordingly , the symmetric similarity between two terms ( cid:1875)(cid:883 ) and ( cid:1875)(cid:884 ) is measured based on their distributions over local topics , as ( cid:1846)(cid:1857)(cid:1865)(cid:1861)(cid:1865)(cid:4666)(cid:1875)(cid:883)(cid:481)(cid:1875)(cid:884)(cid:4667)(cid:1857)(cid:1876)(cid:1868)(cid:3419)(cid:2028)(cid:1830)(cid:1836)(cid:3435)(cid:2012)(cid:1875)(cid:883)(cid:2012)(cid:1875)(cid:884) (cid:3439)(cid:3423 ) . Given the estimated parametersfi(cid:563 ) , we can infer hidden variables unseen document ( cid:1856 ) using the following updating formulas : ( cid:1868)(cid:3435)(cid:1876)(cid:1861)(cid:1859)(cid:1864)(cid:481)(cid:1878)(cid:1861)(cid:1878)(cid:3627)(cid:1875)(cid:1861)(cid:1875)(cid:481)(cid:2206)(cid:819)(cid:1861)(cid:481)(cid:2208)(cid:819)(cid:1861)(cid:482)(cid:563)(cid:3439 ) ( cid:1503)(cid:2030)(cid:1878)(cid:481)(cid:1875)(cid:1859)(cid:1864 ) ( cid:942 ) ( cid:1856)(cid:481)(cid:819)(cid:1861)(cid:1859)(cid:1864)(cid:481)(cid:1878)(cid:3397)(cid:2009 ) ( cid:1859)(cid:1864 ) ( cid:3397)(cid:2011)(cid:1859)(cid:1864)(cid:4673)(cid:481)(cid:1878)(cid:883)(cid:481)(cid:485)(cid:481)(cid:1846)(cid:1859)(cid:1864 ) , ( cid:1856)(cid:481)(cid:819)(cid:1861)(cid:1859)(cid:1864 ) ( cid:3397)(cid:1846)(cid:1859)(cid:1864)(cid:2009)(cid:942)(cid:4672)(cid:1856)(cid:481)(cid:481)(cid:819)(cid:1861 ) ( cid:1868)(cid:3435)(cid:1876)(cid:1861)(cid:1864)(cid:1867)(cid:1855)(cid:481)(cid:1864)(cid:1861)(cid:1864)(cid:481)(cid:1878)(cid:1861)(cid:1878)(cid:3627)(cid:1875)(cid:1861)(cid:1875)(cid:481)(cid:2206)(cid:819)(cid:1861)(cid:481)(cid:2194)(cid:819)(cid:1861)(cid:482)(cid:563)(cid:3439 ) ( cid:1864)(cid:1867)(cid:1855 ) ( cid:3397)(cid:2011)(cid:1864)(cid:1867)(cid:1855)(cid:3439)(cid:481)(cid:1878)(cid:883)(cid:481)(cid:485)(cid:481)(cid:1846)(cid:1864)(cid:1867)(cid:1855 ) . ( cid:1864)(cid:1867)(cid:1855 ) ( cid:3397)(cid:2031)(cid:1856)(cid:481)(cid:942)(cid:3435)(cid:1856)(cid:481)(cid:481)(cid:819)(cid:1861 ) ( cid:1503)(cid:2030)(cid:1878)(cid:481)(cid:1875)(cid:1864)(cid:1867)(cid:1855)(cid:942)(cid:2032)(cid:1864)(cid:481)(cid:1878)(cid:942)(cid:1856)(cid:481)(cid:481)(cid:819)(cid:1861 ) ( cid:3397)(cid:2031)(cid:1856)(cid:481)(cid:481)(cid:1864 ) ( cid:1864 ) ( cid:1856)(cid:481)(cid:481)(cid:819)(cid:1861 ) over locations for each term ( cid:1875 ) appearing in document ( cid:1856 ) by counting the number of times term ( cid:1875 ) is assigned to each location ( cid:1864 ) as ( cid:1868)(cid:4666)(cid:1864)(cid:513)(cid:1875)(cid:4667)(cid:851)(cid:4666)(cid:1875)fi(cid:131)(cid:146)(cid:146)(cid:135)(cid:131)(cid:148)(cid:149)fi(cid:139)(cid:144)fi(cid:1856)fi(cid:131)(cid:144)(cid:134)fi(cid:139)(cid:149)fi(cid:131)(cid:149)(cid:149)(cid:139)(cid:137)(cid:144)(cid:135)(cid:134)fi(cid:150)(cid:145)fi(cid:1864)(cid:4667 ) ( cid:851)(cid:4666)(cid:1875)fi(cid:131)(cid:146)(cid:146)(cid:135)(cid:131)(cid:148)(cid:149)fi(cid:139)(cid:144)fi(cid:1856)(cid:4667 ) for unseen travelogues . Specifically , a Gibbs sampler is run on the
After collecting a number of samples , we can infer a distribution
3 . APPLICATIONS In this section , we introduce how to leverage the learnt LT model to enable three interesting applications : destination recommendation , destination summarization , and travelogue enrichment .
.
3.1 Destination Recommendation The first question raised by a tourist is : where should I go ? Meanwhile , a tourist has some preferences about the travel destinations , which are usually expressed in terms of two criteria : x Being similar to a given location
 “ I quite enjoyed the trip to Honolulu last year . Is there any other destination with similar style ? ” x Being relevant to a given travel intention
 “ I plan to go hiking next month . Could you recommend some destinations good for hiking ? ”
311 Similarity Oriented Recommendation
Given a query location fi(cid:1864)(cid:1869 ) and a candidate destination setfi(cid:2278 ) , each destination ( cid:1864)(cid:1488)(cid:2278 ) has a similarity to ( cid:1864)(cid:1869 ) in the local topic space , defined as ( cid:1838)(cid:1867)(cid:1855)(cid:1861)(cid:1865)fiin Section 241 Besides , each destination has a query independent popularity which should also be considered . The ranking score for recommendation is computed as
.
( cid:963 )
( cid:1864)(cid:1314)(cid:1488)(cid:2278 )
312 Relevance Oriented Recommendation
( cid:1842)(cid:1867)(cid:1868)(cid:4666)(cid:1864)(cid:4667 ) ( cid:851)(cid:4666)(cid:1864)fi(cid:131)(cid:146)(cid:146)(cid:135)(cid:131)(cid:148)(cid:149)fi(cid:139)(cid:144)fi(cid:1829)(cid:4667 ) ( cid:851)(cid:4666)(cid:1864)(cid:1314)fi(cid:131)(cid:146)(cid:146)(cid:135)(cid:131)(cid:148)(cid:149)fi(cid:139)(cid:144)fi(cid:1829)(cid:4667 )
( cid:1855)(cid:1867)(cid:1857)(cid:1864)(cid:1869)(cid:4666)(cid:1864)(cid:4667)(cid:142)(cid:145)(cid:137)(cid:1838)(cid:1867)(cid:1855)(cid:1861)(cid:1865)(cid:3435)(cid:1864)(cid:1869)(cid:481)(cid:1864)(cid:3439)(cid:3397)(cid:2020)(cid:142)(cid:145)(cid:137)(cid:1842)(cid:1867)(cid:1868)(cid:4666)(cid:1864)(cid:4667)(cid:481 ) ( cid:1864)(cid:1488)(cid:2278)(cid:481)(cid:2020)(cid:3410)(cid:882 ) , where coefficient ( cid:2020 ) controls the influence of the static popularity ( cid:1842)(cid:1867)(cid:1868)(cid:4666)(cid:1864)(cid:4667)fiin ranking . Here , ( cid:1842)(cid:1867)(cid:1868)(cid:4666)(cid:1864)(cid:4667 ) is simply defined as the occurrence frequency of location ( cid:1864 ) in the travelogue collectionfi(cid:1829 ) , as Given a travel intention described by a term ( cid:1875)(cid:1869 ) ( eg , “ hiking ” ) , we rank destinations in terms of relevance tofi(cid:1875)(cid:1869 ) . Since a travel single term , we expand ( cid:1875)(cid:1869 ) in the local topic space as fi(cid:2012)(cid:1875)(cid:1869 ) ( a disthis way , the relevance of each location ( cid:1864 ) to the ( cid:1875)(cid:1869 ) can be meas(cid:1855)(cid:1867)(cid:1857)(cid:1875)(cid:1869)(cid:4666)(cid:1864)(cid:4667)(cid:1830)(cid:1838)(cid:4672)(cid:2012)(cid:1875)(cid:1869)(cid:1313)(cid:2032)(cid:1864) (cid:4673)(cid:3397)(cid:2021)(cid:142)(cid:145)(cid:137)(cid:1842)(cid:1867)(cid:1868)(cid:4666)(cid:1864)(cid:4667)(cid:481 ) ( cid:1864)(cid:1488)(cid:2278)(cid:481)(cid:2021)(cid:3410)(cid:882 ) , where ( cid:2032)(cid:1864 ) is location ( cid:1864 ) ’s distribution over the local topics . Actual ly , with the above query expansion strategy , it is straightforward to support multi word queries for more complex travel intentions . intention usually contains more comprehensive semantics than a tribution over the local topics , as introduced in Section 242 ) In ured using KL divergence . The ranking score is thus computed as
3.2 Destination Summarization Once a destination has been determined , a tourist would like to know more details of the destination , like
 “ What are the most representative things in San Francisco ?
Can you tell me with a few words or sentences ? ” described in Section 241 , and simply select those terms with highest probabilities in this distribution as the representative tags .
To summarize the representative aspects of a destination , we first generate a few representative tags , and then identify related snippets for each tag to further describe and interpret the relation be tween the tag and the destination . For a given locationfi(cid:1864)(cid:1869 ) , we can obtain its probability distribution over terms ( cid:3419)(cid:1868)(cid:3435)(cid:1875)(cid:3627)(cid:1864)(cid:1869)(cid:3439)(cid:3423)(cid:1875)(cid:883)(cid:483)(cid:1849 ) as Then , given a representative tagfi(cid:1875)(cid:1869 ) , we generate its corresponding snippets by ranking all the sentences ( cid:4668)(cid:4669 ) in the travelogue collection according to the query “ ( cid:1864)(cid:1869)(cid:3397)(cid:1875)(cid:1869 ) ” . Specifically , a sentencefi consisting of a ( mentioned ) location set ( cid:2278 ) and a term set ( cid:2289 ) is rated in terms of the geographic relevance to location ( cid:1864)(cid:1869 ) and the semantic relevance to tagfi(cid:1875)(cid:1869 ) , as ( cid:1855)(cid:1867)(cid:1857)(cid:1864)(cid:1869)(cid:481)(cid:1875)(cid:1869)(cid:4666)(cid:4667)(cid:1833)(cid:1857)(cid:1867)(cid:1857)(cid:1864)(cid:1857)(cid:1864)(cid:1869)(cid:4666)(cid:4667)(cid:3400)(cid:1857)(cid:1865)(cid:1857)(cid:1864)(cid:1857)(cid:1875)(cid:1869)(cid:4666)(cid:4667 ) , where ( cid:1833)(cid:1857)(cid:1867)(cid:1857)(cid:1864)(cid:1857)(cid:1864)(cid:1869)(cid:4666)(cid:4667)(cid:851)(cid:3435)(cid:1864)(cid:1869)fi(cid:131)(cid:146)(cid:146)(cid:135)(cid:131)(cid:148)(cid:149)fi(cid:139)(cid:144)fi(cid:2278)(cid:3439)(cid:513)(cid:2278)(cid:513 ) ( cid:932 ) ( cid:1857)(cid:1865)(cid:1857)(cid:1864)(cid:1857)(cid:1875)(cid:1869)(cid:4666)(cid:4667)(cid:963 ) ( cid:142)(cid:145)(cid:137)(cid:4666)(cid:883)(cid:3397)(cid:513)(cid:2289)(cid:513)(cid:4667 ) ( cid:932 ) ( cid:1846)(cid:1857)(cid:1865)(cid:1861)(cid:1865)(cid:3435)(cid:1875)(cid:1869)(cid:481)(cid:1875)(cid:3439 ) fi ( cid:1875)(cid:1488)(cid:2289 ) where ( cid:513)(cid:942)(cid:513 ) denotes the cardinality of a set , and ( cid:1846)(cid:1857)(cid:1865)(cid:1861)(cid:1865 ) is the terms in sentence contribute to the semantic relevance more or less , according to their similarities to the query tag . 3.3 Travelogue Enrichment Besides the brief summarization , a tourist would also like to browse through some travelogues written by other tourists . Given a travelogue , a reader is usually interested in the places visited by the author and how these places look like . pair wise term similarity defined in Section 242 Note that all the
, and
,
 “ Where did Jack visit when he was in New York City ? And how do those places look like ? ”
To facilitate browsing , we extract the highlights of a travelogue and enrich them with images to provide additional visual descrip tions . Given a travelogue ( cid:1856 ) which refers to a set of locationsfi(cid:2278)(cid:1856 ) ,
WWW 2010 • Full PaperApril 26 30 • Raleigh • NC • USA405 we treat the informative depictions of locations in ( cid:2278)(cid:1856 ) as the highlights . As described in Section 243 , each term ( cid:1875 ) in travelogue ( cid:1856 ) has a probability ( cid:1868)(cid:4666)(cid:1864)(cid:513)(cid:1875)(cid:4667)fito be assigned to locationfi(cid:1864)(cid:1488)(cid:2278)(cid:1856 ) . Hence , the highlight corresponding to location ( cid:1864 ) is represented as a ( cid:1849)dimensional term vectorfi(cid:2203)(cid:1864)(cid:3435)(cid:1864)(cid:481)(cid:883)(cid:481)(cid:485)(cid:481)(cid:1864)(cid:481)(cid:1849)(cid:3439 ) , where ( cid:1864)(cid:481)(cid:1875)(cid:851)(cid:4666)(cid:1875)fi(cid:1853)(cid:1868)(cid:1868)(cid:1857)(cid:1853)fi(cid:139)(cid:144)fi(cid:1856)(cid:4667)(cid:3400)(cid:1868)(cid:4666)(cid:1864)(cid:513)(cid:1875)(cid:4667)(cid:481)(cid:1875)(cid:883)(cid:481)(cid:485)(cid:481)(cid:1849 ) . To visually enrich every identified highlightfi(cid:2203)(cid:1864 ) , we select images from a candidate image set ( cid:2284)(cid:1864 ) that is geographically relevant to locationfi(cid:1864 ) . Each image ( cid:1488)(cid:2284)(cid:1864 ) is annotated with a set of tagsfi(cid:2286 ) , and is also represented as a ( cid:1849 ) dimensional vectorfi(cid:2204 ) ( cid:3435)(cid:481)(cid:883)(cid:481)(cid:485)(cid:481)(cid:481)(cid:1849)(cid:3439 ) , where ( cid:481)(cid:1875)(cid:963 ) ( cid:1846)(cid:1857)(cid:1865)(cid:1861)(cid:1865)(cid:4666)(cid:481)(cid:1875)(cid:4667)(cid:481)fififi(cid:1875)(cid:883)(cid:481)(cid:485)(cid:481)(cid:1849 ) ( cid:1488)(cid:2286 ) Then , the relevance of image to highlight ( cid:2203)(cid:1864)fiis computed as ( cid:1855)(cid:1867)(cid:1857)(cid:2203)(cid:1864)(cid:4666)(cid:4667)(cid:1731)(cid:2203)(cid:1864)(cid:481)(cid:2204)(cid:1732)(cid:942 ) ( cid:142)(cid:145)(cid:137)(cid:4666)(cid:883)(cid:3397)(cid:513)(cid:2286)(cid:513)(cid:4667)(cid:481)(cid:1488)(cid:2284)(cid:1864 ) , ( cid:883 ) where ( cid:1731)(cid:942)(cid:481)(cid:942)(cid:1732 ) denotes inner product , and the second term is used to the kth image ( cid:1863 ) is chosen , we update ( cid:2203)(cid:1864)(cid:4666)(cid:1863)(cid:4667)(cid:4672)(cid:1864)(cid:481)(cid:883)(cid:4666)(cid:1863)(cid:4667)(cid:481)(cid:485)(cid:481)(cid:1864)(cid:481)(cid:1849)(cid:4666)(cid:1863)(cid:4667)(cid:4673 ) to ( cid:1864)(cid:481)(cid:1875)(cid:4666)(cid:1863)(cid:4667)(cid:4682)(cid:1864)(cid:481)(cid:1875)(cid:4666)(cid:1863)(cid:883)(cid:4667)(cid:3400)(cid:1857)(cid:1876)(cid:1868)(cid:3435)(cid:2028)(cid:942)(cid:1863)(cid:481)(cid:1875)(cid:3439)fi(cid:481)(cid:1863)(cid:3410)(cid:883 ) ( cid:1864)(cid:481)(cid:1875)fifififififififififififififififififififififififififififififififififififififififi(cid:481)(cid:1863)(cid:882) fi , ( cid:1875)(cid:883)(cid:481)(cid:485)(cid:481)(cid:1849 ) , where ( cid:2028)(cid:3408)(cid:882 ) is a coefficient to control the strength of decay . normalize images with different numbers of tags . Moreover , to diversify the resulting images , we select images one by one . Once
4 . EXPERIMENTAL RESULTS In this section , we present experimental results of the LT model and its applications . Both objective and subjective evaluation methods are used to evaluate the effectiveness of the framework .
. decay the semantics already illustrated by the selected images , as
4.1 Data Set There are many sources of travelogues on the Web , either from Weblogs such as Windows Live Spaces , or dedicated travel websites like TravelPod3 , IgoUgo4 , and TravelBlog5 . We collected approximately 100,000 travelogues written in English and related to tourist destinations in the United States , to form an English corpus . A location extractor was applied to extract locations mentioned in these travelogues , yielding 18,000 unique locations . As some subjective evaluations require participators’ knowledge , we also built a Chinese corpus from Ctrip6 , consisting of 94,000 Chinese travelogues related to around 32,000 locations in China .
4.2 Travelogue Modeling After pre processing including stemming and stop word removal , we trained a LT model on each corpus to learn a number of local topics and global topics . The numbers of local and global topics were set empirically to 100 and 50 , respectively . The training procedure for each corpus included 2,000 iterations of Gibbs sampling and lasted for approximately 40 hours on a server with an AMD Opteron quad core 2.4GHz processor .
To illustrate the topics learnt by the LT model , we show the top terms ( ie , terms with the highest probabilities ) of some topics in Table 1 . We can see that local topics characterize some tourism
3 http://wwwtravelpodcom/ 4 http://wwwigougocom/ 5 http://wwwtravelblogorg/ 6 http://wwwctripcom/
Table 1 . Top terms of example local and global topics . local #23 local #57 local #62 local #66 local #69 desert cactus canyon valley hot west heat spring museum art collect gallery exhibit paint work sculpture dive snorkel fish aquarium sea boat whale reef casino gamble play slot table machine game card mountain peak rocky snow high feet lake summit global #8 global #19 global #22 global #26 global #37 flight airport fly plane check bag air travel great best fun kid family old beautiful children enjoy wonderful love amaze fun love young age room hotel bed inn breakfast bathroom night door rain weather wind cold temperature storm sun warm
( a ) Local topic #57 ( museum , … )
( b ) Local topic #62 ( dive , … )
Figure 5 . Geographic distributions of two local topics . The darker a region is , the higher correlation with the topic it has . styles and corresponding locations , including both natural styles like seaside ( local #62 ) and cultural styles like museum ( local #57 ) ; whereas global topics correspond to common themes such as accommodation ( global #26 ) and opinion ( global #19 ) , which tend to appear in travelogues related to almost any destination . ics ( #57 museum and #62 seaside in Table 1 ) on the US state
To exemplify the relationships between local topics and locations , we utilize , following [ 13 ] , the Many Eye visualization service7 to visualize the spatial distribution of some local topics . Based on the LT model , the correlation between a local topic ( cid:1878 ) and a location ( cid:1864 ) is measured by the conditional probabilityfi(cid:1868)(cid:4666)(cid:1878)(cid:513)(cid:1864)(cid:4667 ) , which is equal tofi(cid:2032)(cid:1864)(cid:481)(cid:1878 ) in the LT model . In Figure 5 , we plot two local topmap respectively , where a darker state indicates a higher ( cid:1868)(cid:4666)(cid:1878)(cid:513)(cid:1864)(cid:4667 ) it has . Both maps show uneven geographic distributions of local topics , indicating high dependence between local topics and locations . From Figure 5 ( a ) we see that New York , Illinois , and Oklahoma are famous for {museum , art , …} ; while in Figure 5 ( b ) Hawaii shows the highest correlation with {dive , snorkel , …} . This demonstrates the learnt relationships between local topics and locations are reasonable and consistent with prior knowledge .
4.3 Destination Recommendation
431 Similarity Oriented Recommendation Since the effectiveness of similarity oriented destination recommendation highly relies on the pair wise similarity metric of locations , we directly evaluate this metric ’s capability of discovering similar locations from a given set .
7 http://manyeyesalphaworksibmcom/manyeyes/
WWW 2010 • Full PaperApril 26 30 • Raleigh • NC • USA406 ( a ) LT model
( b ) Baseline method
Figure 6 . Location similarity graphs generated by the LT model and the baseline method , where different colors and shapes stand for different location categories .
We first collected the top destinations recommended by TripAdvisor8 for four travel intentions including Beaches & Sun , Casinos , History & Culture , and Skiing . After filtering out locations not appearing in our corpus , we built a location set consisting of 36 locations , based on which pair wise location similarities were computed ( as describe in Section 241 ) to form a location similarity graph . To demonstrate how well the graph is consistent with the ground truth similarity/dissimilarity between four categories of locations , we use the NetDraw9 software to visualize this graph where similar locations tend to be positioned close to each other , as shown in Figure 6 ( a ) . As a comparison , we implemented a baseline method which formed a pseudo document for each location by concatenating all the travelogues referring to it , and then measured the pair wise location similarity using the common TFIDF based cosine similarity . Comparing the two graphs in Figure 6 ( a ) and ( b ) , we can see that different categories of locations are roughly differentiated by our similarity metric , while under the baseline metric some of them are coupled together . This is owing to one advantage of the LT model , namely preserving the information that characterizes and differentiates locations when projecting the travelogue data into a low dimensional topic space .
432 Relevance Oriented Recommendation To evaluate the relevance oriented recommendation , we collected the top destinations recommended by TripAdvisor for five travel intentions , ie , Beaches & Sun , Casinos , Family Fun , History & Culture and Skiing , as the ground truth for five queries , respectively . For the sake of uniformity , all the queries are truncated into unigrams . Besides the LT model based method presented in Section 312 , we also set up a baseline method , which ranks locations for a query term in decreasing number of travelogues containing both a location and the query term . The resulting location ranking lists of both methods are evaluated by the number of locations , within the top K ones , matching the ground truth locations . The evaluation results are shown in Table 2 , while Table 3 lists some top destinations recommended by our approach .
From Table 2 we observe that the locations recommended by the LT model generally match more ground truth ones than the baseline ; whereas the baseline exceeds our approach at the top 5 and top 10 results for the query family . This observation can be interpreted as the two sides of a coin . On one hand , our method measures each location ’s relevance to the query term in the local topic space to naturally expand the query with similar terms , and thus enable partial match and improve the relevance measurement for queries well captured by local topics ( eg , beach , casino ) . On the
8 http://wwwtripadvisorcom/Inspiration/ 9 http://wwwanalytictechcom/Netdraw/netdrawhtm
Table 2 . Comparison of the relevance oriented destination recommendation results for five queries .
Query
#Ground truth
Methods
#Matches at top K
K=5 K=10 K=15 K=20 beach casino family history skiing
35
6
38
12
20 baseline LT model baseline LT model baseline LT model baseline LT model baseline LT model
1 4 2 4 4 3 4 5 2 3
4 9 2 5 6 5 6 8 4 5
7 12 3 5 8 8 8 9 4 10
9 13 3 5 11 11 8 10 6 12
Table 3 . Top destinations recommended by the LT modelbased method , where those in the ground truth shown in bold .
Query
Top 10 recommended destinations beach
Myrtle Beach , Maui , Miami , Santa Monica , Destin , Hilton Head Island , Virginia Beach , Daytona Beach , Key West , San Diego casino
Las Vegas , Atlantic City , Lake Tahoe , Biloxi , Reno , Deadwood , New Orleans , Detroit , Tunica , New York City family
Orlando , Las Vegas , New York City , Washington , DC , New Orleans , Charleston , Myrtle Beach , Chicago , San Francisco , Walt Disney World history
New Orleans , Charleston , Williamsburg , Washington , DC , New York City , Chicago , Las Vegas , Philadelphia , San Francisco , San Antonio skiing
Lake Tahoe , Park City , South Lake Tahoe , Jackson Hole , Vail , Breckenridge , Winter Park , Salt Lake City , Beaver Creek , Steamboat Springs other hand , for queries mainly captured by global topics ( eg , family , a top term of the global topic #22 shown in Table 1 ) , this query expansion mechanism is less reliable , due to the low confidence of these terms’ distributions over local topics .
4.4 Destination Summarization 441 Representative Tag Generation To compare with the location representative tag generation approach described in Section 3.2 , we implemented three baseline methods . The first one ( “ TF ” ) is to generate a pseudo document for each location by concatenating all the travelogue paragraphs referring to it , and then rank terms in decreasing frequency in the pseudo document . The second one ( “ TF IDF ” ) is to further multiply each term ’s frequency with the Inverse Document Frequency ( IDF ) to penalize common terms . The third baseline is similar to the LT model based approach but disable the global topics .
As there is no existing ground truth of location representative tags , we built one by borrowing people ’s knowledge . For each location , we first formed a tag pool by merging the top tags generated by the proposed method and three baselines , and then asked 20 graduate students to select the top 10 most representative tags . Finally , each tag was rated according to the number of times it was selected , to generate a ranking list of tags as the ground truth . Considering the participators’ background knowledge , we used the Chinese corpus in this experiment and involved 20 popular tourist destinations in China to form the questionnaire .
Based on the ground truth , the tag ranking list generated by each method is evaluated using the Normalized Discounted Cumulative
WWW 2010 • Full PaperApril 26 30 • Raleigh • NC • USA407 K @ G C D N
0.90
0.80
0.70
0.60
0.50
0.40
0.30
TF
TF IDF
LT ( only local topics )
LT ( local + global topics )
K
5.00
4.00
3.00
2.00
1.00
Baseline
LT Model
4.03
4.17
4.09
3.78
3.97
3.64
4.05
3.67
3
5
1
Figure 7 . NDCG@K results of location representative tags generated by ( a ) TF , ( b ) TF IDF , ( c ) LT model with only local topics , and ( d ) LT model with both local and global topics .
10
15
20 geographic relevance semantic relevance
Figure 8 . A subjective evaluation of representative snippets generated by the LT model based method and the baseline . comprehensiveness overall satisfaction
Table 4 . Representative tags generated by the LT model based method for example destinations in the United States .
Destination
Top 10 representative tags
Anchorage
Boston
Chicago
Las Vegas
Los Angeles
Maui
New York
City
Orlando
San Francisco
Washington ,
DC bear , moose , alaskan , glacier , fish , cruise , salmon , wildlife , trail , mountain fenway , whale , historic , sox , cape , england , red , history , revere , church michigan , institute , field , lake , museum , cta , tower , loop , windy , cub strip , casino , show , hotel , bellagio , gamble , fountain , venetian , mgm , slot hollywood , star , studio , universal , movie , boulevard , theatre , china , getty , sunset island , beach , snorkel , whale , ocean , luau , volcano , dive , fish , surf subway , broadway , brooklyn , zero , avenue , island , yorker , manhattan , village , greenwich disney , park , universal , resort , world , theme , studio , kingdom , magic , epcot bay , cable , alcatraz , chinatown , wharf , bridge , prison , bart , fisherman , pier museum , memorial , monument , national , metro , capitol , war , smithsonian , lincoln , president
Gain at top K ( NDCG@K ) [ 9 ] , which is commonly used in the IR area to measure the accuracy of ranking results . The results averaged over all the 20 locations are shown in Figure 7 . It can be seen that our method significantly outperforms the baselines consistently at top K ranking positions . Out of the baselines , the TFIDF method outperforms the TF method consistently , owing to the penalty to noisy tags commonly co occurring with various locations . However , this frequency based penalty mechanism is too coarse to filter out all the noisy tags . Our approach properly filters out these tags using global topics . When global topics are disabled and all the information is modeled by local topics as in the third baseline , the performance is even worse than the TF IDF method .
In addition to the above quantitative evaluation , we also generated representative tags for some US destinations based on the English corpus . As exemplified in Table 4 , the generated tags include not only landmarks ( eg , bellagio , alcatraz ) but also styles ( eg , historic , beach ) and activities ( eg , gamble , dive ) .
442 Representative Snippet Generation As it is quite subjective to evaluate the extent to which a textual snippet is informative for something at somewhere , we resorted to user study to evaluate the generated representative snippets . Based on the Chinese corpus , we prepared 20 groups of data , each consisting of a query in the form of “ location + term ” , 5 snippets generated by the proposed method , and another 5 snippets from a baseline snippet ranking method based on the number of occurrences of the query in a snippet .
Twenty graduate students were asked to assess the two snippet sets ( presented in random order ) in each group using 1 to 5 ratings , from four aspects namely ( 1 ) geographic relevance ( ie , to what extent the snippets are describing the query location ) , ( 2 ) semantic relevance ( ie , describing the query term ) , ( 3 ) comprehensiveness ( ie , providing rich information about the query ) , and ( 4 ) overall satisfaction . Using these aspects we want to demonstrate whether the proposed method can suggest snippets not only relevant to the query but also informative and comprehensive . For each snippet set , we averaged all the users’ evaluations as its ratings on the four aspects . The two methods are compared using pair wise t test on the 20 groups and exhibit significant differences ( p<0.01 ) in all the four aspects . As depicted in Figure 8 , although the difference in the geographic relevance is relatively small due to the straightforward measurement in both methods , our method shows significant advantages in other three aspects due to the query term expansion mechanism .
Besides , some examples generated based on the English corpus are illustrated in Table 5 , where words relevant to the query term ( shown in bold and italic ) provide informative and comprehensive descriptions for the queries .
4.5 Travelogue Enrichment For the evaluation of travelogue enrichment , we conducted a user study based on the Chinese corpus . The materials presented to users consist of 10 travelogue segments , each referring to at least one location and related characteristics . For each segment , there are two image sets ( each with three images ) generated by our method and a baseline method which simply uses the mentioned locations as queries to retrieve geo tagged photos from Flickr .
We asked 20 graduate students to assess both image sets ( presented in random order ) of each segment from four aspects including ( 1 ) geographic relevance ( ie , to what extent the images are depicting the main locations in the segment ) , ( 2 ) semantic relevance ( ie , depicting the objects mentioned in the segment ) , ( 3 ) diversity ( ie , depicting different objects in the segment ) , and ( 4 ) overall satisfaction ( well highlighting and enriching the text ) . The results are depicted in Figure 9 , and pair wise t test on the 10 pairs of image sets exhibits significant differences ( p<0.01 ) in all the four aspects . It indicates that the images selected by our method are more favorable compared with the baseline . Note that in the aspect of geographic relevance , the difference of two methods is small because the baseline is actually geo based , while in other three aspects , our method exhibits larger advantages due to the learnt location representative knowledge and query expansion enabled by the LT model .
Another two English examples are illustrated in Figure 10 , where each travelogue segment is enriched by three images that depict
WWW 2010 • Full PaperApril 26 30 • Raleigh • NC • USA408 Table 5 . Representative snippets generated by the LT model based method for several “ location + term ” queries , where locations are shown in underline and words relevant to the query term are shown in bold and italic .
Query
Alaska
+ wildlife
Las Vegas + casino
Waikiki Beach + beach
Top snippets
1
This time , the bus stopped at the Alaska Wildlife Conservation Center , where we saw musk oxen , bison , brown bears , black bears , eagles , owls , foxes , reindeers , and a few other animals .
2 Along the way we stopped at the Alaska Wildlife Recreation Center to check on the musk ox , Moose , Brown and Black Bears , and many other Arctic animals .
3
1
2
3
1
2
The rest of my week in Alaska was filled with more wildlife sightings , including numerous brown bears and cubs , caribou , and a rare black wolf in Denali ; and a moose with a calf and several wild trumpeter swans .
With the ringing of slot machines it finally hit me , I was rolling into a Las Vegas casino! Sitting at the first row of slot machines we experienced one of the most amazing aspects of common casino etiquette .
Being more inclined to stick my hard earned cash in a money market fund than a slot machine , Las Vegas and casinos have never held much allure for me , but I have to admit that the Las Vegas Strip with all the big casinos seemed pretty glamorous and exciting .
We have played in the Casinos of Las Vegas! They want you to stay in the Casinos so they have waitresses coming around giving you free drinks and all you have to do is tip her $1 or $2 dollars and she just keeps coming back .
The famous Waikiki Beach is very beautiful and is packed with swimmers from the early hours , it has lovely clean sand but the beach is getting smaller by the year and is already at the base of a couple of hotels! The dreaded global warming!
Since Waikiki Beach faces southwest , we were able to enjoy an extraordinary sunset right from the beach in front of our hotel , then we retired to our room where we spent the evening sitting out on the lanai , having some cocktails , and listening to the surf .
3 If you are near the Waikiki Beach area , enjoy the day at the beach , relax , and stay for the beautiful sunset .
4.23
3.77
Baseline
4.07
3.22
LT Model
3.92
4.05
3.18
3.27
5.00
4.00
3.00
2.00
1.00 geographic relevance
Figure 9 . A subjective evaluation of travelogue enrichment by the LT model based method and the baseline method . overall satisfaction semantic relevance diversity its most informative parts . We also present each image ’s original tags and the words in text it corresponds to . For instance , the presented images in Figure 10 ( a ) depict representative and diverse semantics described in the text , ie , ocean , volcano , and beach .
5 . RELATED WORK Some related work has been dedicated to organizing information on the Web to provide online travel assistant services . For instance , Jing et al . [ 10 ] proposed a travel plan assistant system which provided high quality images relevant to given locations based on tourist sight extraction and image retrieval . Wu et al . [ 21 ] proposed a system to generate personalized tourism summary in the form of text , image , video , and news . In [ 12 ] , a trip planning system was presented for place recommendation according to users’ previous choices and tag based place similarity .
Recently , leveraging user contributed photo collections ( eg , Flickr [ 6 ] ) has attracted lots of research efforts . Some of them [ 11][18 ] selected representative photos to visually summarize a given landmark or scene . Ahern et al . [ 1 ] analyzed the tags associated with photos to identify and visualize representative tags for arbitrary areas in the world ; while Crandall et al . [ 5 ] utilized geotagged photos to discover worldwide popular places and their representative images . In [ 22 ] , geo tagged photos were leveraged to discover landmarks and build a world scale landmark recognition engine . Moxley et al . [ 15 ] proposed an image tag suggestion tool based on mining of location tags from Flickr photos .
In [ 8 ] , the authors proposed to generate overviews for locations by mining representative tags from travelogues and retrieving related images . Each travelogue is assumed to be related to only one loca tion ; neither similarity between locations nor the representation of locations in the learnt topic space is considered .
Probabilistic topic models , such as latent Dirichlet allocation ( LDA ) [ 2 ] and its extensions , have been successfully applied to many text mining tasks . Rosen Zvi et al . [ 17 ] extended LDA by incorporating authors of documents as observed variables and representing authors with mixtures of topics . Some models [ 4][19 ] aimed to discover topics in different granularity levels other than document level . In [ 16][20 ] , locations ( entities ) appearing in documents were explicitly modeled as generated by topics , while in [ 13][14 ] locations served as labels associated with documents . In a very recent work [ 3 ] , the model was sensitive to both entities and relationships between entities , given textual data segmented beforehand . In spite of the success in their respective scenarios , all the above models are not applicable to the travelogue mining scenario in this paper , as discussed in the section of Introduction .
6 . CONCLUSION AND FUTURE WORK Travelogues contain abundant location representative knowledge , which is informative for other tourists , but difficult to extract and summarize manually . In this paper , we have investigated the mining of location representative knowledge from travelogues to facilitate tourists to utilize such knowledge . We proposed a probabilistic topic model , ie , Location Topic model , to discover local and global topics from travelogues and characterize locations using local topics . With this model , we could effectively ( 1 ) recommend destinations for flexible queries ; ( 2 ) summarize destinations with representative tags and snippets ; and ( 3 ) enrich the highlights of travelogues with images . The proposed framework was evaluated based on two large travelogue collections , showing promising results on the above tasks .
For the future work , we plan to incorporate prior knowledge of locations to improve the unsupervised knowledge mining . Another direction is to leverage more types of information in travelogues ( eg , opinions , travel routes , and temporal information ) to meet more practical information needs such as itinerary planning .
7 . REFERENCES [ 1 ] S . Ahern , M . Naaman , R . Nair , and J . Yang . World Explorer : visualizing aggregate data from unstructured text in georeferenced collections . In Proc . JCDL , 2007 .
WWW 2010 • Full PaperApril 26 30 • Raleigh • NC • USA409 This was our first trip to Hawaii , let alone Maui . The beaches , activities , types of accommodations , and restaurants make it a great choice for a first visit to the islands . 1 ) The beaches! There are so many all over the island , and all different types : white , black , even red . Large , busy , and with amenities and activities , or small , private , and rustic ( no facilities ) . 2 ) The activities! Go snorkeling , fishing , golfing , hiking up an old volcano , biking down the volcano , four wheeling on unpaved , virtually vacant dirt roads through old lava flows , driving on narrow , curvy , crowded roads through tropical forests , and helicopter rides around the island . 3 ) The restaurants! There are so many fine dining choices with all types of menus , as well as sandwich shops and the more familiar chains . parasailing , diving , surfing , ocean , life , blue , sea , brown , green , beach , water , animal , coral , hawaii , sand , marine , underwater , turtle , shell , diving , maui , snorkeling , travel , vacation , mountain , cold , tourism , island , volcano , hawaii , islands , nationalpark , paradise , pacific , horizon , maui , reef , creature , flipper haleakala , crater , summit , … sky , beach , water , clouds , hawaii , sand , surf , maui , palmtrees
( a ) A segment of a Maui travelogue titled as “ Our Maiden Journey to Magical Maui ”
( http://wwwigougocom/journal j23321 Maui Our_Maiden_Journey_to_Magical_Mauihtml )
Lobster , lobster everywhere , and very reasonably priced . The city was a GREAT place to find seafood . We went to a Red Sox game , which was great fun and we also went to Cape Cod , and Plymouth . This was a highly recommended vacation that we all enjoyed . Take a whale watching tour or visit the aquarium and cruise around the harbor . You can see Old Iron Sides , where the English lost their tea . We also recommend The Ghost and Graves tour which was fun , and a way to get a feel for Boston while getting some local history . park , new , york , red , boston , d50 , nikon , baseball , stadium , sox , fenway , pitcher , yankees , ballpark , blue , boston , aquarium , lobster ocean , boston , island , harbor , boat , whale
( b ) A segment of a Boston travelogue titled as “ The home of the Lobster ”
( http://wwwigougocom/journal j16396 Boston The_home_of_the_Lobsterhtml )
Figure 10 . Two example travelogue segments visually enriched by the proposed method , where each image ’s original tags are shown below the image ; locations are shown in underline and informative words/tags are shown in bold and italic .
[ 2 ] D . M . Blei , A . Y . Ng , and M . I . Jordan . Latent dirichlet allo
[ 13 ] Q . Mei , D . Cai , D . Zhang , and C . Zhai . Topic modeling with cation . J . Mach . Learn . Res . , 3:993 1022 , 2003 . network regularization . In Proc . WWW , 2008 .
[ 3 ] J . Chang , J . Boyd Graber , and D . M . Blei . Connections between the lines : augmenting social networks with text . In Proc . KDD , 2009 .
[ 4 ] C . Chemudugunta , P . Smyth , and M . Steyvers . Modeling general and specific aspects of documents with a probabilistic topic model . In Proc . NIPS , 2006 .
[ 5 ] D . Crandall , L . Backstrom , D . Huttenlocher , and J . Klein berg . Mapping the World ’s Photos . In Proc . WWW , 2009 .
[ 6 ] Flickr . http://wwwflickrcom/ [ 7 ] T . Griffiths and M . Steyvers . Finding scientific topics . In
PNAS , 101:5228–5235 , 2004 .
[ 8 ] Q . Hao , R . Cai , X J Wang , J M Yang , Y . Pang , and L . Zhang . Generating location overviews with images and tags by mining user generated travelogues . In Proc . ACM Multimedia , 2009 .
[ 9 ] K . Jarvelin and J . Kekalainen . IR evaluation methods for retrieving highly relevant documents . In Proc . SIGIR , 2000 .
[ 10 ] F . Jing , L . Zhang , and W Y Ma . VirtualTour : an online travel assistant based on high quality images . In Proc . ACM Multimedia , 2006 .
[ 11 ] L . Kennedy and M . Naaman . Generating diverse and representative image search results for landmarks . In Proc . WWW , 2008 .
[ 12 ] J . Kim , H . Kim , and J . Ryu . TripTip : a trip planning service with tag based recommendation . In Proc . CHI , 2009 .
[ 14 ] Q . Mei , C . Liu , H . Su , and C . Zhai . A probabilistic approach to spatiotemporal theme pattern mining on weblogs . In Proc . WWW , 2006 .
[ 15 ] E . Moxley , J . Kleban , and B . S . Manjunath . SpiritTagger : a geo aware tag suggestion tool mined from Flickr . In Proc . MIR , 2008 .
[ 16 ] D . Newman , C . Chemudugunta , P . Smyth , and M . Steyvers . Statistical entity topic models . In Proceedings of KDD , 2006 . [ 17 ] M . Rosen Zvi , T . Griffiths , M . Steyvers , and P . Smyth . The author topic model for authors and documents . In Proc . UAI , 2004 .
[ 18 ] I . Simon , N . Snavely , and S . M . Seitz . Scene summarization for online image collections . In Proc . ICCV , 2007 .
[ 19 ] I . Titov and R . McDonald . Modeling online reviews with multi grain topic models . In Proc . WWW , 2008 .
[ 20 ] C . Wang , J . Wang , X . Xie , W Y Ma . Mining geographic knowledge using location aware topic model . In Proc . GIR , 2007 .
[ 21 ] X . Wu , J . Li , Y . Zhang , S . Tang , and S Y Neo . Personalized multimedia web summarizer for tourist . In Proc . WWW , 2008 .
[ 22 ] Y T Zheng , M . Zhao , Y . Song , H . Adam , U . Buddemeier , A . Bissacco , F . Brucher , T S Chua , and H . Neven . Tour the world : building a web scale landmark recognition engine . In Proc . CVPR , 2009 .
WWW 2010 • Full PaperApril 26 30 • Raleigh • NC • USA410
