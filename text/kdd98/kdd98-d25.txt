Fast Computation of 2 Dimensional Depth Contours
Ted
Johnson
AT&T Research Center
Florham Park , NJ .
Email : johnsont@researchattcom
Abstract
"One person ’s noise is another person ’s signal." For many applications , including the detection of credit card frauds and the monitoring of criminal activities in electronic commerce , an important knowledge discovery problem is the detection of exceptional/outlying events . In computational statistics , a depth based approach detects outlying data points in a 2 D dataset by , based on some definition of depth , organizing the data points in layers , with the expectation that shallow layers are more likely to contain outlying points than are the deep layers . One robust notion of depth , called depth contours , was introduced by Tukey . ISODEPTH , developed by Ruts and Rousseeuw , is an algorithm that computes 2 D depth contours . In this paper , we give a fast algorithm , FDC , which computes the first k 2 D depth contours by restricting the computation to a small selected subset of data points , instead of examining all data points . Consequently , FDC scales up much better relies on the non existence of collinear points , FDC is robust against collinear points . Keywords : depth contours , statistics , convex hulls computational than ISODEPTH . Also , while ISODEPTH
Introduction
Knowledge discovery tasks fall into four general categories : ( a ) dependency detection , ( b ) class identification , ( c ) class description , and ( d ) exception/outlier detection . The first three categories correspond to patthat apply to many , or a large percentage of , terns in the dataset . The fourth category , objects in contrast , focuses on a very small percentage of data objects , which is often ignored or discarded as noise . Some existing algorithms in machine learning and data mining have considered outliers , but only to the extent of tolerating them in whatever the algorithms are supposed to do ( Angluin & Laird 1988 ; Ester et al . 1996 ;
Person handling correspondence .
Copyright ( ~)1998 , American Association for Artificial Intelligence ( wwwaaaiorg ) All rights reserved .
224 Johnson
Ivy Kwok and Raymond Ng *
Department of Computer Science University of British Columbia Vancouver , BC , V6T 1Z4 Canada
Email : {ikwok,rng}@csubcca
Ng & Han 1994 ; Zhang , Ramakrishnan , & Livny 1996 ) . However , "one person ’s noise is another person ’s signal." Indeed , for some applications , the rare events are often more interesting than the common ones , from a knowledge discovery standpoint . Sample applications include the detection of credit card fraud and the monitoring of criminal activities in electronic commerce . detection
Most of the existing works on outlier
"an outlier and even ( iv ) in the field of statistics
( eg , upper or lower outliers lies ( Barnett & Lewis 1994 ; Hawkins 1980 ) . While there is no single , generally accepted , formal definition of an outlier , Hawkins’ definition captures the spirit : is an observation that deviates so much from other observations as to arouse suspicions that it was generated by a different mechanism" ( Hawkins 1980 ) . Accordingly , over one hundred discordancy/outlier tests have been developed for different circumstances , depending on : ( i ) the data distribution , ( ii ) whether or not the distribution parameters ( eg , mean and variance ) are known , ( iii ) the types number of expected outliers , in of expected outliers an ordered sample ) ( Barnett ~ Lewis 1994 ) . However , all of those tests suffer from the following two problems . First , most of them are univariate ( ie , single attribute ) . This restriction makes them unsuitable for multidimensional datasets . Second , all of them are In numerous situations , we do not distribution based . know the data distribution and have to perform extensive testing to find a distribution the attribute . Although a unified outlier detection system ( Knott & Ng 1997 ) can handle situations where the attribute follows any distribution , like any distance based data mining works , the detection requires the existence of a metric distance function , which is not available for every dataset . In ( Arning , Agrawal , & Raghavan 1996 ) , Arning et al . search a dataset for implicit redundancies , and extract data objects called sequential in Kolmogorov exceptions complexity . This notion of outliers is very different from the aforementioned statistical definitions of outliers . As will be seen shortly , it is also very different from the notion of outliers considered here , primarily because there is not an associated notion of depth . that maximize the reduction that fits
To avoid the aforementioned problems of distribu tion fitting and restriction to univariate datasets , depthbased approaches have been developed . In these approaches , each data point is assigned a depth . Based on the assigned depth , data objects are organized in layers in the data space , with the expectation that shallow layers are more likely to contain outlying data objects than are the deep layers . A key property of depth based approaches is that location depth is scaling invariant
A robust notion of depth , called depth contour was introduced by Tukey ( Tukey 1975 ; 1977 ) . Intuitively , point P in space is of depth k if k is the minimum number of data points that have to be removed to expose P . Here "minimum" is defined across all the half planes passing through P . The k th depth contour marks the boundary between all points with depth k and all those with depth k + 1 . Figure 1 ( a ) shows the first depth contours for a dataset containing 5,000 points . In general , depth contour maps gives a nice picture of the "density" of the outlying regions ( Liu , Parelius , Singh 1997 ) . Rousseeuw develop an algorithm which computes 2 D depth contours .
In ( Ruts & Ronsseeuw 1996 ) , Ruts
ISODEPTH , called
This paper extends the work of Ruts and l~usseeuw in two key areas . First , ISODEPTH relies on the computation of dividers ( to be formally introduced later ) for all n data points in the dataset . Having a complexity at least quadratic in n , ISODEPTH does not scale up well . In contrast , to compute the first k depth contours , our algorithm FDC restricts the computation to a selected , much smaller , subset of points , thanks to the construcconvex hulls . Consequently , tion of the appropriate FDC can scale up much better than ISODEPTH . Second , ISODEPTH relies on the non existence of collinear points . Removing all collinear points can be very time consuming . FDC is robust against collinear points . Figure l(b ) for concentric data points , many of which are collinear . shows the depth contours computed by FDC
Algorithm
FDC
Definition 1 Given a point cloud D consisting of n points , a line L is an e divider of D if there are e points in D to be left of L , and ( n e ) points in D to the right of L .
In the spirit of finding outliers , whenever e < ( n e ) , we say that the e points are to the "outside" of L and the remaining points to the "inside" of L . Just as an e divider L divides the data cloud D into two disjoint subsets , it divides the convex hull of D into two subregions . We call these the "inside region" and the "outside region," denoted as IR(L ) and OR(L ) respectively . Given a collection of e dividers , we refer to the intersection of all their inside regions ( ie , e intersected inside region . r~ IR(L ) ) the
As we shall see later , the only interesting part of an e divider is a finite segment of it . We denote a line segment between points P and Q by ( P , Q ) . The above definition of e dividers can be extended to line segments . More specifically , the depth of a line segment is the i i
( a ) 5,000 Data Points
: 2
: 4
: 6
¯ 8
, 12
: 14
: 18
: 18 io
I0
( b ) Concentric Data Points
Figure 1 : Depth Contours depth of the line that runs through the line segment . A chain of line segments , denoted as ( P1 , P2 , , Pn ) ( where Pi ~ Pj for all i < j ) , is the set of line segments ( Pi , Pi+l ) for 1 < i < ( n 1 ) . For example , IR((P1 , P2 , , P, ) ) denotes the inside region of the convex hull of D , ie , the region of all the points in the convex hull that are to the inside of the chain . As will be obvious later , every line segment in the chain is an edivider . We call the chain an expanded e divider . Given any number of e dividers but at least one expanded edivider , we refer to the intersection of all their inside regions as the expanded e intersected inside region .
Figure 2 shows the pseudo code of Algorithm FDC ( to stand for "Fast Depth Contour" ) , which computes the first k depth contours of a bivariate point cloud . While we shall prove the correctness of the algorithm shortly , the following first gives an example to show how the algorithm works .
An Example depicted
Consider the situation pose that the point cloud contains many points , among which only a few are shown . In particular , in Figure 3 . Sup suppose that
KDD 98 225
Algorithm Input : D the point cloud ; k _ an integer . Output : Contours of depth from 0 to k .
FDC
I G = convex hull of D ; 2 For ( d = 0 ; d < k ; ) { /" new peel
2.1 H= G ; 2.2 Output H as the d th depth contour ; 2.8 If d == k , break ; 2.4 d ffi d+ 1 ; D = D H ; G = convex hull of ( the updated ) D ;
/* Done and Stop "/
If IHI == 3 , continue ; /" degenerate case of having a triangle as the convex hull "/
2.6 For ( e = 1 ; e < IHl/2 ; ) { /" Otherwise : the general case "/ 2e1 For all points P 6 H { 2611 Find the point Pe E H ( if any ) so that ( P , P , ) is e divider of the remaining points in H .
2612 If every point in G it contained in IR((P , Pc ) ) 28121 lt~P ] = IR((P , Pe) ) ; l = ¢;} /" end if "/ 2613 Else { /" form an expanded e divider’/ 26131 Enumerate ( in a clockwise fashion ) all the points in Gthat are
28132 Among them , find Qi that maximizes the angle between the outside ( P , P~ ) as QI
Qm . segment ( Qi , P ) and ( P , P6 ) .
26133 Among them , find Qj that maximizes the angle between the segment ( Qj , Pc ) and ( Pc , P) /* Without Ion of generality , assume that IP,[P ] = IR((P , Qi Qj,P,) ) ; AH[P ] = {Qi end else */ i < j . "[
Qj} ; }/*
28134
} /* end for "/
282 Output the boundary of the region
( f’~PEH II~[P ] ) as the d th depth contour ; If d == k , break ; /* Otherwise */ d= d+ 1 ; e= e+ I ; If __ UpEH AH[P ] ~ ¢ { /" G is not contained
/* Done and Stop */
263 264 265 inside region Np6H I R( ( P , Pc ) ) in the e intersected
2651 2652 G = convex hull of ( the updated ) D ; } /* end if */
H = H U ( UpEH AH[P] ) ; D = D ( UPEH AHiP] ) ;
} /* end for "/
) /* end for "/
Figure 2 : Pseudo Code of Algorithm FDC in Step 1 of Algorithm FDC , the convex hull of the entire point cloud is found to be the polygon with vertices from A to G . In the first iteration of the for loop in Step 2 , this polygon is returned as the zero th depth contour in Step 22 is precisely the polygon with vertices
Suppose that in Step 2.4 , the convex hull of the reall points except maining points in the data cloud ( ie , from A to G ) is found to be the polygon with vertices from H to L . Then in the first iteration of the for loop in Step 2.6 , all the 1 dividers are found . For instance , let us consider one case , say A . The 1 divider of A is <A,C ) . The inside region IR((A,C ) ) the polygon with vertices A , C , D , E , F , G . This polygon completely contains from H to L . Thus , Step 2612 is avoided . In fact , this is the case for every point from A to G and its corresponding 1 divider . Therefore the region ( Np~H IR[P ] ) computed in Step 262 is the 1inside region formed by all the 1 dividers , intersected marked explicitly in Figure 3(a ) . Every point X inside this region has at least two points from A to G that are outside of any line passing through X . And for any point X outside the region ( but inside the polygon with vertices from A to G ) , there exists at least one line passing through X that separates exactly one point is executed , but Step 2613
226 Johnson
C
O
F
( a ) 1 Dividers c
A
A
O
F
( b ) 2 Dividers
Figure 3 : An Example : 1 Dividers and 2 Dividers among A to G from the rest . The boundary of this 1 intersected equal to 1 . inside region gives the contour of depth
Now in the second iteration of the for loop in Step 2.6 , the 2 dividers are considered . As shown in Figure 3(b ) , this time it is no longer the case that every inside region completely contains the polygon with vertices from H to L . In other words , the 2 intersected inside region does not completely contain the latter polygon . As a concrete example , consider A . As shown in Figure 4 , the 2 divider is ( A , D> , and the points H , I , J and are to the outside of ( A , D/ . Thus , in Step 26132 the that maximize the angle formed by the point , point A and D is computed . In this case , the point the angle formed by I , A and D is bigger than ( ie , that formed by H , A and D , and so on ) . Similarly , in Step 26133 , is found that J maximizes the angle formed by the point , D and A . Thus , the original 2divider ( A,D ) is expanded and replaced by the chain <A , I , J , D ) . Accordingly , the original inside region with vertices A , D , E , F , G is expanded to become the polygon with vertices A , I , J , D , E , F , G . Eventually , in Step 262 , the expanded divider ( A , I , J , D ) defines part
I is it is that there is a one to one correspondence between the series HI,1,,HI,,m,H2,1,,H2,m2,Ha,1 , and the contours of depth 0 , 1 , 2 , the list of contours of depth 0 , 1 , 2 ,
In particular , is the list :
For more detail , 1098 ) . please read ( Johnson , Kwok , & Ng
Now we consider complexity/efficiency issue . Let n be the total number points in the data cloud . Let h denote the maximum cardinality of the first k elements in the series HL1 , , Hl,ral , H2,1 , ¯ ¯ . , H2,m2 , Similarly , let g denote the maximum cardinality of the first k instances G . A complexity analysis of FDC is deferred /citeNg98 . The convex hull computation and maintenance takes O(nlogn+ h log 2n ) and the rest of computation , with Step 2611 ( finding the initial e dividers ) dominating other steps , takes O(kh3 ) . This gives an overall complexity of O(n logn + h log 2n + kha ) . the complexity of FDC compare with to a more detailed
How does report
ISODEPTH ’s O(n2logn ) ? This comparison boils down to the relative magnitudes of h , k and n . From the point of view of finding outliers in large datasets , k is typically not large ( say , _< 100 , if not smaller ) and h ( which is also partly dependent on k ) is at least 2 3 orders of magnitude smaller than n . Thus , for the intended uses of the algorithms , we expect FDC to outperform ISODEPTH when n is not too small .
Preliminary
Experimental
Results results experimental comparing Below we present a copy of ISODEPTH with FDC . We obtained the program ISODEPTH from Ruts and Rousseeuw ; in Fortran . We implemented FDC in C++ , was written for most computational geand used the LEDA library such as convex hull computations ometry operations , and polygon intersections . As for datasets , we used both real and generated ones . All graphs shown here are based on generated datasets ; but the conclusions we draw generalize to many real datascts .
Figure 5(a ) shows the computation time taken by the two algorithms to produce depth contours from 0 to k , with k varying between 1 and 21 . The computation time consists of the CPU time taken to produce all k depth contours after the data points are loaded . Figure 5(a ) is based on the dataset consisting of 5,000 points shown in Figure l(a ) . It is clear that FDC outperforms ISODEPTH by at least an order of magnitude . For example , for k = 21 , FDC takes about 25 seconds , whereas ISODEPTH takes at least 1,400 seconds .
Figure 5(b ) shows how the two algorithms scale to the size of the dataset , with k up with respect
KDD 98 227
Figure 4 : An Example ( cont’d ) : Expanding a 2 Divider the boundary of the contour of depth equal to 2 .
To understand why ( A , D ) should be expanded
( A , I , J , D ) in forming part of the boundary of the contour of depth 2 , it is easy to verify from Figure 4 that apart from B and C , there are at least the additional points H to K that are outside of ( A , D ) . In contrast , the chain ( A , I , J , D ) ensures that for any point X the polygon with vertices A , B , C , D , J and I ( and inside the contour of depth 1 ) , there exists at least one line passing through X that separates B and C from the rest .
Note that in Step 265 , I and J are added to the the set of points among which e dividers are to be found in subsequent iterations of the for loop in Step 26 These two points are added because of ( A , D ) . To complete the example shown in Figure 3(b ) , point H is also added because H expands the 2 divider ( F , B ) . Similarly , point K is added because K expands both ( B , E ) and ( C , F ) .
So far the example shown in Figure 3 has illustrated there are 3 extra points the major parts of the algorithm . But one aspect that requires further explanation is the case being dealt with in Step 25 Assume that X , Y , Z that form a triangle encompassing the polygon with vertices from A to G in Figure 3 . In this case , the triangle gives the first convex hull of the dataset , and is therefore the contour of depth 0 . Then it is easy to verify that the next convex hull namely , the polygon with vertices from A to G is the contour of depth 1 . This illustrates why in Step 2.5 , if the convex hull is a triangle , the algorithm can simply proceed to the remaining points .
Correctness and Analysis of FDC
Consider every instance of the set H as Algorithm FDC executes . Each instance of H can be labeled as Hi,e indicating the content of the set at the beginning of the e th iteration of the for loop in Step 2.6 , but during the i th iteration of the for loop in Step 2 . Under this labeling scheme , the set H before entering Step 2.6 ( ie , between Steps 2.1 and 2.5 ) is represented by Hij . Thus , as Algorithm FDC executes , it produces the series HI j , , Hl,,nl , H2,1 , , H2,,n2 , H3j , ¯ ¯ A key result in the many cases we have experimented with , it is conceivable that if k is a large value , then h will be large as well . As factor kh3 corresponds to the finding edividers , we are investigating how to further optimize this step .
There is also no easy way to formulate a pattern for the k values as each dataset has its own characteristics . One approach is to make use of the value h . We can specify the number or percentage of data objects we want , and then compute the depth contour till h meets the required value .
Lastly , we are working on generalizing the 2dimensioanl FDC to 3 dimension . We focus only on the 3 D case because the geometry computation in high dimensions is costly and the results are hard to be visuallized .
References
In Prac . KDD , 164 169 .
Angluin , D . , and Laird , P . 1988 . Learning from noisy examples . Machine Learning 2(4):343 370 . Arning , A . ; Agrawal , R . ; and Raghavan , P . 1996 . A linear method for deviation detection in large databases . Barnett , V . , and Lewis , T . 1994 . Outliers in Statistical Data . John Wiley ~ Sons . Ester , M . ; Kriegel , H P ; Sander , J . ; and Xu , X . 1996 . A density based algorithm for discovering clusters in large spatial databases with noise . In Proc . I(DD , 226231 . Hawkins , D . 1980 . Identification of Outliers . London : Chapman and Hall . Johnson , T . ; Kwok , I . ; and Ng , R . T . 1998 . FDC : Fast computation of 2 dimensional depth contours . Unpublished Manuscript , Dept . of Computer Science , University of British Columbia . Knorr , E . M . , and Ng , R . T . 1997 . A unified notion of outliers : Properties and computation . In Proc . KDD , 219 222 . Liu , R . ; Parelius , J . ; and Singh , K . 1997 . Multivariate analysis by data depth : Descriptive statistics , graphics , inference . In Technical Report , Dept . of Statistics , Rutgers University . Ng , R . , and Han , J . 1994 . Efficient and effective clustering methods for spatial data mining . In Proc . 20th VLDB , 144 155 . Ruts , I . , and Rousseeuw , P . 1996 . Computing depth contours of bivariate point clouds . Computational Statistics Tukey , J . 1975 . Mathmatics and the picturingof data . In Proc . International Congress on Mathmatics , 523531 . 2hkey , J . 1977 . Exploratory Data Analysis . AddisonWesley . Zhang , T . ; Ramakrishnan , R . ; and Livny , M . 1996 . BIRCH : An efficient data clustering method for very large databases .
In Proc . ACM SIGMOD , 103 114 . and Data Analysis 23:153 168 .
§
( a ) Wrt the Number of Depth Contours
FDC ~l IS¢OB~’t~ Otplh . : 1 i i i
+ i oi m,¢i i
""i""
( b ) Wrt the Data~et Size
Figure 5 : FDC vs ISODEPTH to 21 . When there are fewer than 500 points , set the two algorithms are competitive . But with 1,000 points or more , FDC scales up much more nicely than ISODEPTH does . For instance , FDC is 4 times faster than ISODEPTH for 1,000 points , but is more than 50 times faster for 5,000 points .
The version of ISODEPTH we have cannot run when n > 5,000 . This is due to the excessive amount of space used by the program . This is why our head tohead comparisons between the two algorithms stop at 5,000 . The table below shows that FDC is very scalable with respect to n . Although the figures are based on a constant k ( depth = 21 ) , FDC also seems to scale well with respect to k in Figure 5 .
Dataset Size ( n ) Maximum Cardinality ( h ) Computation Time ( sec )
1,000 131 17
10,000 174 27
100,000
199 52
Future Work
In the above analysis , we show a kh3 factor in the complexity figure . Even though h is supposedly very small compared with n and the factor kh3 poses no problem
228 Johnson
