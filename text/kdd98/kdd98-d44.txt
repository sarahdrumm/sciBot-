Online generation of profile association rules
Charu C . Aggarwal
TJ Watson Research Center Yorktown Heights , NY 10598 charu@watsonibmcom
Zheng Sun Duke University ,
Durham , NC 27706 sunz@csdukeedu
Philip
S . Yu
TJ Watson Research Center Yorktown Heights , NY 10598 psyu@watsonibmcom
Abstract
The results discussed in this paper are relevant to a large database consisting of consumer profile information together with behavioral ( transaction ) patterns . The focus of this paper is on the problem of onhne mining of profile association rules in this large database . The profile association rule problem is closely related to the quantitative association rule problem . We show how to use multidimensional indexing structures in order to perform the mining . The use of multidimensional indexing structures to perform profile mining provides considerable advantages in terms of the ability to perform very generic range based onhne queries .
Introduction
Association rules have recently been recognized as an important tool for knowledge discovery in databases . The problem of discovering association rules was first investigated in pioneering work in ( Agrawal , Imielinski , et . al . 1993 ) . Here we examine a database of records which consist of both customer profile ( such as salary and age ) and behavior ( such as buying decision ) information )
The association rule problem was originally proposed for the case of binary itemset data ( Agrawal , Imielinski , et . al . 1993 ) . The intuitive implication of the association rule X ~ Y is that a presence of the set of items X in a transaction also indicates a strong possibility of the presence of the itemset Y . The measures used in order to evaluate the strength of the rule are support and confidence . The support of a rule X ~ Y is the fraction of transactions which contain both X and Y . The confidence of a rule X ~ Y is the fraction of transactions containing X which also contain Y .
A considerable amount of research effort
( Agrawal and Srikant 1994 ) ( Savasere et . al . 1995 ) has been devoted to the problem of speeding up the itemset method for finding association rules from very large sets of transaction data . Several variations of this problem have been discussed in ( Hun and Fu 1995 ) ( Srikant
1Copyright @1998 , American Association for Artificial
Intelligence ( www,aaalorg ) All rights reserved .
Agrawal 1995 ) ( Srikant and Agrawal 1996 ) . The quantitative association rule problem ( Srikant and Agrawal 1996 ) is a generalization of the problem of discovering association rules on sales transaction data , in which both categorical lowed . and quantitative attributes are al
In this paper , we examine association between customer profile information and behavior information , referred to as profile associagion rules . The left hand side of the rules consists of quantitative attributes corresponding to customer profile information . Examples of such attributes are age and salary . The right hand side of the rules consists of binary or categorical attributes corresponding to customer behavior information . Examples of such attributes are buying beer or diapers . The objective of mining profile association rules is to identify customer profile for target marketing . Thus , profile association rules are a special case of quantitative association rules . It is the object of the paper to use this special structure in order to provide online mining capabilities . Furthermore , we will also show how to provide merged rules for different combinations of attributes in the consequent . We shall first describe an operator called ’T’ which indicates a combination of the behavioral attributes . Let C correspond attributes to a customer profile of quanand blb~ correspond to behavtitative The rule ioral attributes C =~ bl = vllb2 = vk is true at a given level of support and confidence if and only if all of the rules C ~ bi = vi for all i E {1,k} are true at that level of support and confidence . Thus , C represents the common profile of people who exhibit all of the behaviors bi = vi for alli E {1 k} . Note that if the set of rules Ci ~ bi = vi holds for all i E {1 , , k} , then the rule f3~=1C’~ =~ bl = vl[b2 = v21lb~ = v~ may no~ necessarily hold . Thus , a straightforward postprocessing after finding rules for individual behavior types does not solve the above problem . assuming values vlv~
= v2l [bk
The rule C ~ bl = vllb2 = v2l
[ b~ = v~ is different from the rule U ~ bl = vl,b2 = v2,,b~ = v~ in that the former tries to find the common profile of people to exhibiting a set of behaviors , while the latter tries
KDD 98 129 of the people such that each of them the profile this the latter find exhibit for sparse behavioral data , while in fact may strongly set of behaviors . Often , rule may not be present imply that set of behaviors . enough support for the case of set C the profile rules which provides the capability
We discuss a framework for online mining of profile to issue such as supranges , and combinations Thus , this method also shows the probrules . We shall see in the times for query responses for association queries with different port/confidence levels , of behavioral how to integrate lem of finding profile association the empirical section are almost instantaneous sets of parameters profile an OLAF environment with large amounts of data . attributes . that
Description of the algorithm retrieval data . The particular index on the profile a multidimensional atof the data . This data structure was proposed
We create tributes by Guttman ( Guttman 1984 ) for efficient variant of index multidimensional the purpose of our algotrees which we shall use for rithm is the S Tree ( Aggarwal , Wolf et . al . 1997 ) . The support of a node is equal to the fraction of data points by the minimum bounding recwhich are encapsulated tangle of that node . We call a node of the index tree to its support is above a preprocessing parabe primary if meter called the primary threshold . Otherwise the node is referred to as secondary . Thus , the upper levels of the tree contain primary nodes , while the lower levels contain secondary nodes . At the lowest level , the secondary nodes point ing structure which the support store is designed in order to answer queries for 2 We the following data at each node : to the raw transactions . is above the primary
This preprocess threshold . i , we store values of behavioral attributes
( 1 ) We store the support of each node . ( 2 ) For each of the behavioral attributes support of particular the corresponding minimum bounding node provided old . keep only a few entries formation at the leaf tions . the in of the threshthen we expect to for each node . No behavioral inin the secondary nodes except transac rectangle they are above the primary
If the data is skewed and sparse , level which contain is maintained the individual that
2The issue of limitation in the number of addressable queries is a pervasive one for most OLAP methods using the preprocess once query many paradigm . In this case , this is not necessarily a major constraint , since online queries are expected to generate only a small number of rules which can be assimilated by an end user . We shall see later that for a primary threshold of pC , there are only at most 2/p~ primary nodes . Thus , low enough primary threshold values can be chosen so that enough storage space would be available to information for these nodes . The use store of a primary threshold for online generation of association rules was first in ( Aggarwal & Yu 1998 ) . the behavioral introduced
130 Aggarwal
Algorithm BasicMining(T , ( bt , v~ ) begin
( bh , m, ) ) while any unvisited large node n exists do begin pick the next unvisited large node n in breadth first search order if node n is concentrated
Generate rule B(n ) ~ bl = vii
Ibk = end ; end
Figure 1 : The basic mining algorithm that since two children . is preserved , a maximum amount of support
An important difference with the traditional multidithe fanout of the nodes are not mensional index is that constant . All primary nodes have just inThis ensures formation the support of each node reduces by a factor of two with depth of the nodes . For they are secondary nodes , the fanouts are chosen so that 3 The binary fanouts packed as compactly as possible . ineffiat the higher level nodes do not lead to storage ciency as in regular R Trees , because of the fact that the overall compactness is decided by the fanouts at the secondary nodes which are more numerous . lower Another issue is the impact of the binary nature of the tree . higher When the primary nature of nodes for point data ( Aggarwal , Wolf et . al . 1997 ) ensures the lowest l/pc . Thus , the total number level of primary nodes is of primary nodes in the binary levels of the index tree is the primary 2/pc . The depth of the subtree nodes is logg . ( 1/pc ) . Since most of the limited number of primary nodes can be main memory resident , does not lead to inefficiency level primary nodes on the depth of the at the time of retrieval . number of nodes at is Pc , the disjoint comprising the total threshold level that this
( b2 , v2 ) to be large with respect
Consider a user query for a set of behavior value ( b~ . , v~ ) . We define a node to the behavior pairs ( bl , vl ) , the index tree value pairs ( bl , vl ) , for each i G {1 k} the number of points satisfying bi = vi in that node is at least a fraction s of the total number of points .
( b~ , vk ) at minimum support to be concentrated to the behavior if for each c of the points in that
A node of the index tree is defined at minimum confidence c with respect value pairs ( bl , Vl ) , ( b2,vz),(b~,v~ ) {1 k} at least node satisfy b . : = vi . A node of the index tree confidence ( b~ , v~ ) , a fraction is said to be diluted at minimum c with respect pairs ( bk , v~ ) if and only if it is not concentrated . the index tree , to the behavior value traverses
The basic mining algorithm
3An exact algorithm which can construct an index structure with differing fanouts at upper and lower level nodes is discussed in ( Aggarwal , Wolf et . al . 1997 ) . The modification to the algorithm in order to incorporate the information about the behavioral attributes is relatively straightforward .
Algorithm Mining WithMerging(LargeNodesTree:T , begin
{ Tree T is a copy of the subtree of the original index tree contaJming only the large nodes } for each large node n in postorder do begin ease ( 1 ) Node n is concentrated :
Generate rule B(n ) ~ bx = vl ]
Ibk =
( 2 ) Node n has no children and is diluted :
Delete node n
( 3 ) Node n has one child p and is diluted :
Delete node n . Set the parent of p to parent of n
( 4 ) Node n has two children pt and p2 and is diluted : n’ =bounding(p1 , p2 ) ; replace node n by n’ . if n’ is concentrated then generate rule S(n’ ) . h = v~ ] Ib~ =
Algorithm bounding(p1 , p2 begin return the minimum bounding rectangle of all tremsactions in pl and p2 , with updated values of support and confidence end ;
Figure 3 : Generating rule trees secondary nodes in order to recalculate the merged regions . the support of to result
The merging algorithm constructs a new rule tree by taking a copy of the index tree of large nodes and performing appropriate modifications on it . The idea is those nodes which do not correspond to any to delete rule , and then restructure the remaining nodes in the in some new merged rules . This rule tree , tree is specific to a user specified set of behavior value pairs ( bl , vl ) , ( bg . , v2) , , vary from 1 to the total number of behavioral attributes . Thus , the rule tree can be built for any subset of behavioral attributes with corresponding values . As in the case of the basic mining algorithm , a node n in the rule tree contains the rule B(n ) =~ bl = vl ] if and only if it is large and concentrated .
( bk , vk ) . The value of
[ b~ =
The algorithm works in a bottom up fashion , by vis iting the large nodes of the index tree in postorder . We assume that the tree T in the description of the algorithm in Figure 3 consists of only those nodes which are large . A node is modified or deleted only if it is diluted . If a node is diluted and has no large children then it is deleted . As a result of such deletions some node p will have only one child . In that case , the single child of that node replaces it . As a result , ths node will no longer be the minimum bounding rectangle of its children . When p is visited , ( note that the the nodes ensures that postorder sequence of visiting
KDD 98 131
Age
6O
4O
30
20
64K end ; end
20K
40K
Salary
( b )
60K
80k
Figure 2 : An example of the rule tree and finds the nodes which are both large and concentrated . The rules from the bounding rectangles of these nodes are generated . Thus , the basic mining algorithm needs to use only the behavioral information at the primary nodes . The algorithm is illustrated in Figure 1 . in Figure 2 . For the Consider the example illustrated purpose of notation , we shall denote the bounding rectangle of node i by B(i ) . We wish to find the profile association rules at a minimum support of 1 % and minimum confidence of 50 % . The behavioral attribute which is investigated is that for first time home buyers . In that case , the rules generated would be as follows : B(2 ) ::~ FirstTimeHomeBuyer B(4 ) :~ FirstTimeHomeBuyer B(5 ) ~ FirstTimeHomeBuyer
As we can see , the contiguous shaded region of Figure 2(b ) gets fragmented into a number of smaller rectangular regions , each of which corresponds to a generated rule . This is because there is no single rectangular region which approximately overlaps the entire shaded region . The purpose of the merging algorithm is to merge these fragmented regions into larger regions , and construct a rule tree which provides the user with a better hierarchical tree like view of the association rules generated . In this case , it will be necessary to use the the if it is readjusted in p changing from being diluted all descendants of p are visited before p is visited ) its minimum bounding rectangle to that of is diluted . This readjustment may its children result to being concentrated . It is precisely these kinds of re adjustments which result ill larger concentrated regions . Thus , the "merging" algorithm proceeds by a series of deletions and re adjustment operations on the minimum bounding rectangles of the nodes . The algorithm is illustrated in Figure 3 . Note that the readjustment operation ( indicated by the algorithm bounding(pl,p2 ) ) requires a recalculation of the support and confidence of the behavioral attributes which are being queried . This requires a range query using the bounding rectangle of Pl and P2 on the original the support may be calculated using the entries for the behavioral attributes at the primary nodes . However , for some of the nodes intersected by this bounding rectangle , the corresponding entries are not present because of the primary threshold criterion . For those branches of the index tree , it may be necessary to access the secondary nodes up to the individual data points in order to recalculate support . The overhead for this operation is relatively small , since such branches are few in number if any , and most support information can be obtained directly index tree . Typically , from Pl and P2 .
Empirical Results the m behavioral attributes
We generated N data points , each having I antecedent and m behavioral attributes . For the purpose of the experiments , we use behavioral attributes , each of which may assume values from the set {1,,k} We shall assume that are denoted by bl , b2 , , bin . Each of the l profile attributes are denoted by C1 , C2 , , Ct . We normalize the profile data in such a way that each of the attributes in the range [ 0 , 1 ] . The individual data points are generated by first deciding the values of the profile attributes , and then deciding the values of the behavioral attributes . lies attribute
The profile values were generated by spreading the data points randomly over the profile subspace . Then , the value of the behavioral attributes are set . For each of the m behavioral attributes ( say the ith behavioral attribute b{ ) , we pick a random number between 1 and k ( say j ) . We generate a hypercube in the profile space , such that each side has a length which is distributed uniformly randomly in the range [ 0 , 02 ] For all points which lie inside this hypercube , a fraction f of points must satisfy the consequent condition bi = j . The actual points which are chosen are arrived at by randomly picking off a fraction f of the points which lie inside the hypercube . Then we set the value of the attribute b{ to j for these points . The remaining points the hypercube may assume any of the values for bi from 1 to k except j . The points the hypercube may assume any value from 1 outside to k . This method of generating hypercubic regions of concentration is useful in evaluating the quality of the inside
132 Aggarwal rules generated . the time it in Figure 4(b ) . These times are
For the purpose of this experiment , we choose l = 2 , m = 4 , and the value of f is chosen uniformly randomly in the range [ 0.7 , 095 ] We ran the experiments for cases when the number of data points N ranged from 5000 to 1,000,000 . We tested both the "basic mining" and the "mining with merging" versions of the algorithm . The following were the performance measures that we used : ( 1 ) Speed of execution : We tested took for the preprocessing and the online processing parts of the algorithm . As we see from Figure 4(a ) , the time for setting up the multidimensional index tree varies linearly with the size of the data . The online response times for a support of 0.8 % on a database with 100,000 points are illustrated small as to be practically instantaneous , and are consistent with the hypothesis that the time required is of the order of 1/s , where s is the minimum support . ( 2 ) Rule Quality : For each behavioral concentration , let us define nearest rule as the rule in the final rule tree which matches most closely with the given concentration in behavior . In order to assess the quality of the rules generated , we tested how well this rule overlapped with the true regions of behavioral concentration . Let us consider N~ctu~t be the number of data points in a rectangular concentration corresponding to the consequent of a given rule . Let Ndiscooer,d be the number of data points in this rectangle which were actually found by the rule , and let Nl~z,e be the number of data points outside the rectangle which are mistakenly found be the rule . Note that Ndi~,oo,~ed is always bounded above by Nacmal . Thus , we have two measures in order to decide the quality of a rule :
Incompleteness Ratio = 1
Ndiscovered
Nactual
Spurious Ratio Nl~ts~
Nac,u,~ ,
( 1 )
( 2 )
The incompleteness and spurious ratios are illustrated in Figures 4 ( c ) and ( d ) . These ratios are small for sonably low support values . Given the quick response times of the algorithm in the range of such support values , it is practically possible to get this high quality of performance for most online queries . The process of merging improved both the incompleteness and spurious ratios . This is because of the better bounding rectangles obtained after the process of deletions and readjustments .
Conclusions and Summary
In this paper , we discussed the problem of online mining of profile association rules . Such rules may be very useful in developing relationships between consumer profiles and behavioral information . We discussed how to use multidimensional indexing to generate profile assocation rules in online fashion . An additional advantage of using indexing structures is that it is possible to specify specific profile ranges and behavioral attributes for which it is desired to find the rules .
References of index
Aggarwal C . C . , and Yu P . S . 1998 . Online Generation of Association Rules . Proceedings of the International Conference on Data Engineering , Orlando , Florida . Aggarwal C . C . , Wolf J . L . , Yu P . S . , and Epelman M . 1997 . The S Tree : An efficient for multi dimensional objects . Proceedings of the International Symposium on Spatial Databases . pages 350370 , Berlin , Germany . Agrawal R . , Imielinski T . , and Swami A . 1993 . Mining association rules between sets of items in very large databases . Proceedings the A CM SIGMOD Conference on Management of data , pages 207 216 , Washington D . C . Agrawal R . , and Srikant R . 1994 . Fast Algorithms for Mining Association Rules in Large Databases . Proceedings of the 20th International Conference on Very Large Data Bases , pages 478 499 . Guttman A . 1984 . R Trees : A Dynamic Index Structure for Spatial Searching . Proceedings of the A CM SIGMOD Conference , Hun J . and Fu Y . 1995 . Discovery of Multiple Level Association Rules from Large Databases . Proceedings of the 21st International Conference on Very Large Data Bases . Zurich , Switzerland , pages 420 431 . Mannila H . , Toivonen H . , and Verkamo A . I . 1994 . Efficient algorithms for discovering association rules . AAAI Workshop on Knowledge Discovery in Databases , pages 181 192 , Seattle , Washington . Savasere A . , Omiecinski E . , and Navathe S . 1995 . An Efficient Algorithm for Mining Association Rules in Large Data Bases . Proceedings of the 21st International Conference on Very Large Data Bases . Zurich , Switzerland , pages 432 444 . Srikant R . , and Agrawal R . 1995 . Mining Generalized Association Rules . Proceedings of the 21st International Conference on Very Large Data Bases , pages 407 419 . Srikant R . , and Agrawal R . Mining quantitative ciation rules in large relational the 1996 ACM SIGMOD Conference of Data . Montreal , Canada . tables . Proceedings of on Management
47 57 . asso
0
2
4
6 S,.ZE OF OXTA
8 o ig
02
04
06 L~%’,B,,Sk~
0$ S~cp pOR T E~l p E RC ENt"
~
~2
~2S
’4
$I o~¸ o!
02
04
06 os
~
12
Su~,O~T e~ pe~:~ce~r
1.4
I~ ,
14
,6
Index Figure 4 : ( a ) Setup Time for Multidimensional Tree ( b ) Variation of online execution time ( c ) missing ratio
( d ) spurious ratio with minimum support
KDD 98 133
