Rule discovery from time series
Gautam Das Department of
King Ip Lin Department of
Mathematical Sciences University Memphis , TN 38152 of Memphis
USA dasg@msci , memphis , edu linki@msci
MathematicM Sciences University Memphis , TN 38152 of Memphis
USA memphis edu
Heikki Mannila Dep~rtment of Computer Science
University of Helsinki
PO Box 26 , FIN 00014 Helsinki ,
Finland mannila@cs.helsinki
.fi
Gopal Renganathan
Autozone Inc . ,
123 So . Front St . , Memphis , TN 38103
USA
Gopal . Renganathan@crv . autozone , corn
Abstract falls , to patterns in one series
We consider the problem of finding rules relating patterns in a time series to other patterns in that series , or patterns in another series . A simple example is a rule such as "a period of low telephone call activity is usually followed by a sharp rise ill call vohune" . Examples of rules relating two or more time series are "if the Microsoft stock price goes up and lntel then IBM goes up the next . day," and "if Microsoft goes up strongly fro" one day , then declines strongly on the next day , and on the same days Intel stays about , level , then IBM stays about level." Our emphasis is in the discovery of local patterns in multivariate time series , in contrast to traditional time series analysis which largely focuses on global models . Thus , we search for rules whose conditions refer to patterns in time series . However , we do not want to define beforehand which patterns are to be used ; rather , we want the patterns to be formed fl’om the data in the context of rule discovery . We describe adaptive methods for finding rules of the above type fi’om time series data . The methods are based on discretizing the sequence hy methods resembling vector quantization . window through the time series , and then cluster these subsequences by using a suitable measure of time series similarity . The discretized version of the time series is obtained by taldng the cluster corresponding to the subsequence . Once tl,e is discretized , we use simple rule finding methods to obtain rifles from the sequence . "vVe present empMcal resuh.s on the behavior of the method . form subsequences by sliding time series identifiers
\,Ve first
Copyright @1998 , American Association Intelligence ( wwwaaaiorg ) All rights for Artificial reserved .
16 Das
Padhraic
Smyth
Department of
Information and Computer Science University of California at Irvine
CA 92697 3425 , USA smyth@ics , uci . edu
Keywords : time series , tion , vector quantization rules , clustering , discretiza
Intro duction frequently at . stock prices and Europe , and daily in business and in science . Well known examples appliTime series data occurs include cations the New York Stock Exchange , daily hourly w)luntes of telephone calls between the United States readings terest larity Shatkay & Zdonik 1996 ; Agrawal el al . 1995 ; Rafiei Mendelzon 1997 ; Yazdani & Ozsoyoglu 1996) ) . temperature in the Pacific . There has been a lot . of intime series on the basis of simi(Agrawal , Faloutsos , & Swami 1993 ; into querying ( see , sea surface eg , between the relationship
In this paper we are interested in finding rules relatthe behavior of patterns within a sequence over ing time , or two or more sequences over time . An example would be a rule such as in the value of sea surface "a period of gradual increase temperature over the South Pacific followed over the Western by sharp in precipitation United States." Rules typically assume an underlying symbolic ( or propositional ) specific novel contribution a discrete time series , basis for exploratory whereas our time series . The in this paper is in the extraction of from the as the interest here is in real valued and then using this data driven pattern rule induction . representation , representation representation is typically increase
A time series can be converted by first sentation ing window ) and then clustering using a suitable measure of pattern into a discrete repreforming subsequences ( using a slidsubsequences these similarity .
The identifiers is obtained by discretized version of the time series corresponding to the subusing the cluster sequence , in a manner which is similar to the data compression technique of vector quantization . Rulefinding algorithms ( such as the "episode rule" methods ) can then be used directly on the discretized sequence to uncover rules relating Thus our rule discovery method aims at finding local relationships from tile series , in the spirit of association rules , sequential patterns , or episode rules ( Agrawal , Imielinski , & Swami 1993 ; Agrawal & Srikant 1995 ; Mannila , Toivonen , & Verkamo 1997 ) . Unlike traditime series modeling , we do not seek a global tional model for the time series , instead searching for local patterns in a relatively non parametric manner . temporal patterns .
As an example of the results of the method , from the daily closing share prices of 10 database cornpahies traded on the NASDAQ we Call find several informative rules . For example consider the rule "if a stock price follows the pattern sl8 in figure 2 ( a ) , then within 20 days it will exhibit th epattern s4 shown in the same figure" This call be interpreted as a pattern of decline . In addition , we can find rules relating the behavior of the stocks of individual companies . For instance , our method discovered a number of rules retwo database companies . Namely , if the stock of lating one company has approximately the pattern 15 shown in Figure 3 ( a ) , then the stock of the other company will exhibit similar behavior within a month . Examining the data , we find that tile two companies are both object oriented database companies . by clustering
Time series discretization Basic method Our method for discretizing a time series by clustering windows is as follows . Suppose we are given a sequence s and a window width w . Given s = ( zl,,z, ) , a window of width w on s is a contiguous quence ( xi,,xi+,o l ) dows ( subsequences ) Sl , , si = ( xi,,xi+to t ) 1,,n subseWe form fl’om s all winsn tv+l of width w , where I i =
Denote W(s ) {si w+ 1} .
Assume we have a distance d(si , sj ) between any two subsequences sl and sj of width w . These distances can be used to cluster the set of all subsequences W(s ) into sets Cl,,Ck For each cluster Ch we introduce a symbol ah , and the discretized version D(s ) of the sequence s will be over the alphabet E = {al,,aa} The sequence D(s ) is obtained by looking for each subsequence si the cluster Cj(i ) such that si E Cj(i ) , and using the corresponding symbol a/(i ) . Thus
D(s ) = aj(1 ) , aj(2) , , aj(n w+l )
Essentially , each symbol an represents a primitive "shape" , and we are interested in discovering rules that involve patterns composed of these basic shapes . An example is shown in Figure 1 .
, ’ ,
’ ,
,
" ; ,
, ,
I
I ,
, ,
"’~ , , ,
, : , , ] 5 "~ , , , , , , ’i r ~ 4 ’ ’ ’ ’ ’ , , , ’ ’ ’ tL_ ;__ ’__’__,_,___,__,_, , , IT:IIII : : :Z Z 7
,’
/~ al= / \
/
Original time series = ( 1,2 , 1 , 2 . 1 , 2 . 3 , 23 4 , 3 . 4 ) Window width = 3
Prhnitive shapes after clustering
Discretized series = ( al , a2 , al , a2 , a3 , al , a2 , a3 , al , a2 )
Figure 1 : Example of rules based on basic shapes
The discretization process described above depends on the choice of w , on the choice of the time series distance function , and on the type of clustering algorithm used . the same is not . a new idea . Essentially
The basic property of this method is that , the alphabet . is derived from the data ; it . is not . provided by a domain expert ( although our methods can be generto allow such inputs ) . Using data derived patalized terns as "primitives" in a more abstract , representation of a signal method is used in the well known vector quantization ( VQ ) method of data compression ( see , eg , ( Gersho & Gray 1992) ) . VQ is based on the notion of replacing local windows ( of size w ) ( of signals or images ) by pattern centroids determined by an algorithm quite similar to k means clustering . For data compression , only the indices of the centroids need to be transmitted , permitting signal compression at . the cost . of some fidelity . Thus , one can view the methods proposed in this paper as the application of VQ ( combined with rule induction ) to signal understanding rather than the more typical VQ task of signal compression .
We are advocating a very general approach tmvards rule discovery in time series databases . The user has the choice of a variety of methods to employ . It cannot be overemphasized that the rule discover3 , process is an iterative activity . Each time the system discovers certain rules , domain experts should analyze and the resulting rules . The discovery algorithms interpret should be run several times with different , parameter settings . Different runs provide different views of the underlying dataset . For example , a small w may produce rules that describe short , term trends , while a large w may produce rules that give a more global view of the dataset . One can also run the method at different scales ( by subsampling the data ) allowing for multi resolution data exploration ( these options are not explored in this paper due to space constraints ) .
The running time of the algorithm is dominated by the time needed to cluster the O(n ) subsequences resulting fi’om the windowing method . It is also possible to consider only every vth windmv for some integer v . In the next subsections we describe the time series
KDD 98 17 similarity notions used and the clustering methods employed . similarity the set W(s ) we need a distance not,ion for
Notions of time series To chlster time series of length w . There are several possibilit.ies , and the specific choice for ally given application should depend on the specific nlanner the application environment is generating In this section we describe some possible distance measures and discuss their use in rule discovery . the observed time series .
( ie ,
The simplest , possibilit.y the L . , metric ) . That is , is to treat the subsequences of length w as elements of R’~ and use the Buclidean distance for 2 y~ ) we define d(x,~ ) ( xl,,x~ ) ( ~i(Xi alternative metric inchldes the general Lp metrics defilled by Lp(’2,~l ) = ( P,i(xiLeo = maxi I xi Yi [ . yi )2)1/2 a8 til e met,ric in
~ ) for p > 1 and and y = ( Yl clustering . Oth
For many applications one would like to see tile shape of t.he subsequence as the main factor in distance deterruination . Thus , two subsequences may have essentially the same shape , although they may differ in their amplitudes and baselines . One way of achieving this is by normalizing tile subsequences and then using the L . , metric on the normalized subsequences . Denoting tile normalized version of sequence :~ by q(5: ) ; we define the distance between .r : and .q by d(Y : , [ I ) = L’,(~I(~ ) , rl(y) ) . is ~/(~)i = xi Ea:’ , ( where EY : is the mean of the value of the sequence ) which makes the mean of the sequence to be 0 . Allother possibility is the standard deviation of the sequence ) forcing the mean to be 0 and the variance 1 . is 7/(2)i = ( x ; Ex)/D~ , ( where D~
One possible normalization
Recently , more sophisticated time series distance measures have been investigated , such as the dynamic time warping ( Berndt & Clifford 1994 ) measure , the longest common , subsequence measure ( Das , Gunopu,~z Mannila 1997 ; Bollobds et al . 1997 ) , and varlos , ious probabilistic distance measures ( Keogh & Smyth 1997 ) . Due to space limitations we omit the details of t.heir use but note that tile results below can be easily generalized to handle any such distance measures . methods
Clustering The first step in the discretization process is the clustering . Recall that w is one of t.he parameters to t.he syst,em ; it is used to define t,he set I’V(s ) . hi principle , any clustering algorithms call be used to cluster the subsequences in W(s ) ; see ( Jain ~ Dubes 1988 ; Kaufinan & Rousseauw 1990 ) lbr overviews . We have experimented with the following two methods .
The first method is a greedy method for producing clusters with at most a given diameter . Treat each subsequence in W(s ) as a point in Rw , and let , us use the L2 metric as distance between the points . Let a small constant d > 0 be another parameter to the clustering algorithm .
18 Das
For each point p ill W(s ) , tile method finds tile cluster center q such that d(p , q ) is minil,mm . If d(p , q ) < then p is added to the cluster whose center is q , otherwise a new cluster with center p is formed . Once the algorithn , has examined all the points in W(s ) , let . the cluster centers be ql,,qk It is easy to see that . the distance bet,ween two chlster centers is at . least d , while the radius of each cluster is at most d .
We also used the traditional k means algorithm , where cluster centers for k clusters are initially chosen at randoln among the points of W’(s ) . hi each iteration , each subsequence of l,V(s ) is assigned to the cluster whose center is nearest to it . After this , for its center is recomputed as the point.wise each cluster average of the sequences contained ill the cluster . The are continued until the process converged . iterations This method is widely used ; one disadvantage is that . the number of clusters has to be known ill advance .
Once tile clustering is complete , each cluster center represents a basic "shape." The alphabet of cluster centers is then used to encode the original time series , as discussed above . of parameters
Remarks on the choice Note that tile entire discretization process del)ends on several parameters . Among them are tile window width ( w ) , the maximal cluster diameter ( d ) or the number ( k ) of clusters . Other parameters may also be used in the preprocessing stages ( like windmv movement v ) . How do we know whether a certain combination of parameters produces a "good" discretization ?
The choice of the window width to ( and window movement v ) depend on the time scale the user is interested in . Thus , no specific guidelines can be provided in the general case , rather the user must . choose his/her values based on their own particular bias and application considerations . One of the useful aspects of our approach is that we can look at the sequence using several granularities .
For the cluster diameter d or the number of chlsters k , there are some intuitive yardsticks that can be used . Tile eventual goal is the discovery of interesting , inrules . Too lnany clusters will terpretable , and useflfl this : it . will be almost impossible to assonot help ill ciate understandable interpretations to so many basic abstract "shapes." Likewise , too few clusters will not help , as each cluster contains subsequences that are too far away from each other .
A simpler strategy is to ignore the issue of whether is good or not until after the rules t.he discretization have been discovered . A good discretization is one that . produces interesting rules . One ( but . not . the only ) criterion for interestingness is estimated informativehess , ie , whether the rule gives additional inlormat.ion about , the sequences . \’Ve can assign a measure of illt’or,nat.iveness to the discovered rules using the ,/ measure ( Smyth ~ Goodman 1991 ; 1992 ) .
One can also run the method for several choices of the rule the user browse the different parameters and let . sets . The running time of the method is small enough so that this is feasible . An extension of this idea is to have the algorithm search over different value of w and d and to return the most informative rules over these values .
Rule discovery from discretized sequences format
Rule In this section we outline algorithms that discover simple rules from a set of discretized sequences . The simplest rule format is : if A occurs , then B occurs within time T .
Here A and B are basic shapes , ie , the are letters from the alphabet produced by the discretization . We write the above rule as A =~ , B .
a,, ) ,
Given a sequence D(S ) = ( al,a2 the frequency F(A ) of the rule A is the number of occurrences of A E D(S ) , and the relative frequency f(A ) of A is F(A)/n . The confidence c(A ~ B ) of the rule A ~ B is the fraction of occurrences of A that are followed by 1 a B within T units , ie , c(m ~ B ) F(A , B , T)F(A ) where
F(A,B,T ) = [ {i lai = AA B e {ai+l,,ai+T 1}}
I is the number of occurrences of A that are followed by a B within T .
A slight modification is , however , often useful . Recall that the two consecutive letters cl and ci+l in the discretized sequence D(s ) come from two windows of width w which have an overlap of w v . Thus , consecutive letters are strongly correlated , and we tend to get rules with high confidence that actually are just a by product of the discretization method . Therefore it typically makes sense to define rule confidence by
F(A , B , T )
[ {i l ai = AA B E {ai+w+l,,ai+w+T 1}}
[ to count only occurrences of B that occur after w ie , units of time .
Computing the frequencies and confidences of such rules is easy , by a simple pass through the sequence . is mk2 , where k is the The number of possible in the alphabet and m is the number number of letters of different possibilities for T . rules
Rules
Informative The above method produces lots of rules , with varying confidences . For interactive knowledge discovery , a good strategy is to allow the user to browse through rule sets and provide tools for the selection of interesting rules ( Kloesgen 1995 ; Klemettinen et al . 1994 ; Brin , Motwani , & Silverstein 1997 ) . Nonetheless , no single significance criterion can probably suffice to select the most valuable rules . Still , the user needs some guidance in determining which rules have a confidence that . differs substantially from the expected .
There are a variety of metrics which can be used to rank rules ( eg , see ( Piatetsky Shapiro 1991 ) for general overview of such methods ) . Here we use the J measure for rule ranking ( Smyth ~ Goodman 1991 ; 1992 ) defined as :
J(BT;A )
=p(A)* p ( BT[A ) g(P ~T ) lo .,p(B IA ) )
+(1 p(BTIA))log( 1iZ~ p(BTIA ) the first where , in the context of sequence rules , p(A ) is the probability of symbol A occurring at a random location in the sequence , p(BT ) is the probability of at least one B occurring in a randomly chosen window of duration t and p(BT[A ) is the probability of at least one B occurring in a randomly chosen window of duration T given that the window is immediately preceded by an A . Intuitively , term in the J measure , namely p(A ) , is a bias towards rules which occur more frequently . The second term is well known as the cross entropy , namely the information gained ( or degree of surprise ) in going from a prior probability p(BT ) to a posterior probability p(BTIA ) . As shown in ( Smyth &~ Goodman 1992 ) the product of the two terms ( the J measure above ) has unique properties as a rule information measure and is in a certain sense a special case of Shannon ’s mutual information . From a practical the measure provides a useful and sound method for ranking rules in a manner which trades off rule frequency and rule confidence . Note that in estimating the probabilities helpful ( "smoothed" counts ) rather estimates discussion of this point ) . estimates than maximum likelihood ( see ( Smyth ~ Goodman 1991 ) for further in the equation for the J measure ( above ) it to use simple maximum a posteriori viewpoint ,
Extensions The basic method can be extended in various ways . We describe briefly some possibilities .
1Note that this differs from the usage of frequency or support for association rules ( Agq’awal , Imielinski , & Swami 1993 ; Agrawal et al . 1996 ) , where frequency is defined as the fraction of objects that satisfy the left and right , hand sides of the nile .
It is straightforward
Multiple time series the previous framework for rules between two series . Given m sequences D(Sh ) = ( Chl , ch2 , , Chn ) for h = a rule still has the form of A ~ B , while A 1,,m , and B might come fi’om different discretizations . The to extend
KDD 98 19 definition of frequency , confidence and significance is the same as the previous definitions .
Extending can he extended to include rules of the form format The rule the rule format above if A1 and A2 and and Ah occur within V uni/s of time , then B occurs within time T , denoted A1 A A Ah ~" B . The frequency of such a rule can be defined as the number of occurrences of.all that are followed by A2 etc . within time V . Rules of this type have been studied under the name sequential ( Agrawal & Srikant 1995 ) and episode rules patterns ( Mannila , Toivonen , & Verkamo 1997 ) , and the algorithms developed there can be used in this context also .
The problem with this extension is that the number of potential rules grows quickly . For rules with h letters oll the left . hand side we have to prune rules on the basis of frequency . That is , in order for tile rule AI A A Ah ~ B t.o be considered , the frequency of the rule has to exceed a given threshold . This technique stems fi’om association rule algorithms ( Agrawal , ,~ Swami 1993 ; Agrawal et aL 1996 ) , and it Imielinski , is very efficient in pruning the search space ; the drawback is that rules with high significance but low fl’equency might go undetected ( Brin , Motwani , & Silverstein 1997 ) . Another possibility here ( which we did not experiment with ) is to use the branch and bound properties of the J measure to prune the search space , as in the ITRULE algorithm of ( Smyth & Goodman 1992 ) .
Experimental results
We used three different data sets in our experiments :
1 . Stuck data : daily closing prices of 10 database companies traded on the Nasdaq stock market for the past 19 months . Each sequence is of length 410 .
2 . Telecommunications data : tratfic volumes oil 34 lines in the Helsinki metropolitan area . The volume is recorded every 15 minutes , and the series have length 478 ( approximately 5 days ) .
3 . Paleoecological data : abundances of 36 different , taxa of diatoms in sediment at the bottom of a lake in northern Finland , at . 147 different , depths . That is , there are 36 different series , each having length 147 . Our experiments focused on finding out whether tile method discovers interesting rules from the sequences , and whether the method is robust enough so that small changes in the values of the parameters do not change the results drastically .
For each of the data sets , we experimented with several different window widths w , rule time lengths T , cluster diameter d , and the number of clusters k . For each experiment , the resulting rules were ranked using the J measure . Due to the lack of space , we present only a small subset of the results , concentrating oil the stock data set .
20 Das
\gTe first ,
Simple rule discovery set up a minimum threshold of 1 % frequency and 50 % confidence . Any rules that do not meet this criteria were discarded . After that we use the J measure to compare the quality of the rules . The top scoring rules are listed in the follmving table . Figure 2 shows the centers of the clusters for each of [ .lie rules .
W
13 15 15 30 d
Rule
3.5 18~4 4.0 4.5 5,5
37:~42 11~9 76~21
Sup . ( % ) 2.8 1.3 3.5 1.2
Fig
Conf .
J Inea .
( % ) 0.0037 2(a ) 59.6 57.37 0,0087 2(b ) 0.0031 2(c ) 66.7 2(d ) 57.3 0.0073
( a )
( c )
( b )
( d )
Figure 2 : Significant rules for stock data
An interpretation of the rule in figure 2 ( a ) is that a stock which follows a 2.5 week declining pattern of sl8 ( sharper decrease and then leveling out ) . will likely incur a short sharp fall within 4 weeks betbre leveling out again ( the shape of s/t ) .
Rules fro’ pairs of sequences We also compare individual sequences to detect applicable rules for each pail’ of sequences . Ill this case , we put . a higher threshold on the minimum support to generate more meaningful rules . We found the top set . of 488 rules ( rules with J measure > 003 ) Figure 3 shows some example rules that describe patterns found from the time series of stock prices of two object oriented database companies . While these time series as a whole are not very similar , there is substantial evidence of links between the two series , as the local patterns demonstrate quite strongly .
Also we discovered that more t.han 27 % of the rules relate 3 pairs of sequences , out . of a total of 45 ( ie 2% ) . This indicates that only a small set of sequences are closely related . w=20 , d=5.5
15(seq10 ) ~ 15(seq6 )
( a ) w=20 , d=5.0
7(seql0 ) ~ 6(seq6 )
( b ) w=30 , d=6.0
8(seqlO ) ~ , 9(seq6 )
( c ) w=20 , d=5.5
15(seq6 ) ~ l(seqlO )
( d )
Figure 3 : Significant rules for sequence 6 and 10 of as a function the clustering Robustness methodology We also investigated the impact of the clustering methodology on the results of the algorithm . We considered some of the rules in the previous section , and tried to find a similar rule using different values of d and/or w . The experimental results showed that as the clustering parameters are perturbed , the effect is to also induce slight changes in the rules which are discovered , ie , the new rules which are discovered are in the neighborhood of the old rules discovered by the previous parameter settings , that the method is reasonably robust . indicating
Another important aspect of the clustering methodIn the previous ology is the clustering algorithm itself . is carried out by using the Kexperiments , clustering means algorithm , which means that the algorithm will iterate until the clusters are stable . Experiment shows that . ranges between 10 and 80 for the stock data . Obviously , tbr large data sets , reading the data multiple times is unrealistic . One alternative is to employ tile greedy method of clustering mentioned previously . the number of iterations
Our experiments discovered many similar rules even using the greedy algorithm . A COUl)le of significant rules are shown in following table and figure 4 :
W
20 30 d
Rule
5.5 5.5 18 21
Sup . ( % ) 8.3 1.3
Conf .
( % ) 73.0 62.7
J mea .
Fig
0.0036 0.0039
4(a ) 4(b )
The two rules in this figure corresponds to rules shown in figure 2 ( c ) and ( d ) respectively . Thus , empirically , we can speed up the clustering process without any significant change in the discovered rules . An inter
( a )
¯
~
~ : ,
( b )
~
~
Figure 4 : Significant rules for stock data ( using only iteration of K means ) esting problem for further research is to more precisely characterize and quantify the trade off between computational costs of the algorithm and the quality of the discovered rules .
Discussion
Extracting rules directly from time series data involves two coupled problems . First , since rules are inherently a symbolic representation , one must transform the lowlevel signal data into a more abstract symbolic alphabet . In this paper this is achieved by data driven clustel’ing of signal windows in a similar way to that used in VQ data compression . and so forth , may well affect
The second problem is that of rule induction from symbolic sequences . Naturally , there is a trade off between the quality of the abstraction and the quality of the rules induced using this abstraction . Parameters such as cluster window width , clustering methodology , number of clusters , the types of rules which are induced . In this context it is important to keep in mind that the proposed technique is essentially intended as an exploratory method and thus , iterative and interactive application of the method coupled with human interpretation of the rules is likely to lead to the most useful results ( rather than any fully automated approach ) . Our methods are first steps , and additional experimentation is needed to estimate the strengths and weaknesses of the method .
( rather in ( Keogh & Smyth 1997 ) may provide
Clearly there are several directions for generalizing abthe concepts introduced here , such as alternative than pattern centroids ) . For examstractions ple , the hierarchical piecewise linear representation introduced computationally efficient way to increase the expressive power of the underlying signal representation . The piecewise linear data structure implicitly handles variability signal peaks which may be amplitude scaled and/or stretched in time ) , a feature which is absent in the "fixed window" method described here . Furthermore , a hierarchical representation may provide a practical way to incorporate scale into the representation in a natural manner , allowing for rules which relate events at different scales in the signal structure . in "warping" of signal structure ( eg , the notion of multi resolution
Also generalizing the rule language is an interesting
KDD 98 21
In Piatetsky Shapiro ,
Third International Conference on Knowledge Discovery and Data Mining ( KDD 97 ) , 24 . AAAI Press . Klemettinen , M . ; Mannila , H . ; Ronkainen , P . ; Toivohen , H . ; and Verkamo , A . I . 1994 . Finding interesting rules from large sets of discovered association rules . In Proceedings of the Third International Conference on Information and l(nowledge Management ( CIEM’g4 ) , 401 407 . Gaithersburg , MD : ACM . Kloesgen , W . 1995 . Efficient discovery of interesting st.atenlents in databases . Journal of Intelligent Information Systems 4(1):53 69 . Mannila , H . ; Toivonen , H . ; and Verkamo , A . I . 1997 . Discovery of fl’equent episodes in event sequences . Data Mining and Knowledge Discovery 1(3):259 289 . Piatetsky Shapiro , G . 1991 . Discovery , analysis , and presentation of strong rules . G . , and Frawley , W . J . , eds . , I(nowledge Discovery in Databases . Menlo Park , CA : AAAI Press . 229 248 . Rafiei , D . , and Mendelzon , A . 1997 . Similarity based queries for time series data . SIGMOD Record ( A CM Special Interest Group on Management of Data ) . Shatkay , H . , and Zdonik , S . B . 1996 . Approximate queries and representations for large data sequences . In Proceedings of the 12th International Conference on Data Engineering , 536 545 . Washington Brussels Tokyo : IEEE Computer Society . Smyth , P . , and Goodman , R . M . 1991 . Rule induction using information theory . In I(nowledge Discovery in Databases . Cambridge : MA : The MIT Press . 159 176 . Smyth , P . , and Goodman , R . M . 1992 . All information theoretic approach to rule induction fl’om databases . IEEE Transactions on I(nowledge and Data Engineering 4(4):301 316 . Yazdani , N . , and Ozsoyoglu , Z . M . 1996 . Sequence matching of images . In Proceedings of 8th International Conference on Scientific and Statistical Database Management ( SSDBM ) , 53 62 . problem , for exainple allowing regular expressions over ( in the spMt tile of ( Agrawal et al . 1995) ) . produced by the clustering patterns
References
Ill Proceedings
1994 . Using dynamic time In Proceedin time series . in
Agrawal , R . , and Srikant , R . 1995 . Mining sequential patterns . In Proceedings of the Eleventh International Conference on Data Engineering ( ICDE’95 ) , 3 14 . Agrawal , R . ; Psaila , G . ; Wimmers , E . b . ; and Zait , M . 1995 . Querying shapes of histories . of VLDB . Agrawal , R . ; Mannila , H . ; Srikant , R . ; Toivonen , H . ; and Verkamo , A . I . 1996 . Fast discovery of association rules . In Eayyad , U . M . ; Piatetsky Shapiro , G . ; Smyth , P . ; and lJthurusamy , R . , eds . , Advances in Knowledge Discovery and Data Mining . Menlo Park , CA : AAAI Press . a07 328 . Agrawal , R . ; Faloutsos , C . ; and Swami , A . 1993 . Efficiency similarity search in sequence databases . In Proceedings of the Conference on Foundations of Data Organization , 22 . IBM Almaden Research Center . Agrawal , R . ; hnieliuski , T . ; and Swami , A . 1993 . Mining association rules between sets of items in large databases . In Buneman , P . , and Jajodia , S . , eds . , Proceedings of A CM SIGMOD Conference on Management of Data ( SIGMOD’93 ) , 207 216 . Washington , DC , USA : ACM . Berndt , and Clifford . warping to find patterns ings of AAAI Workshop on Knowledge Discovery Databases 199~ . Bollob~s , B . ; Das , G . ; Gunopulos , D . ; and Mannila , H . 1997 . Time series similarity problems and wellseparated geometric sets . In 13th Annual ACM Symposium on Computational Geometry , 454 456 . Brin , S . ; Motwani , R . ; and Silverstein , C . 1997 . Beyond market baskets : Generalizing association rules In Peckman , J . M . , ed . , Proceedings of to correlations . ACM SIGMOD Conference on Management of Data ( SIGMOD’97 ) , 265 276 . Tucson , AZ : ACM . Das , G . ; Gunopulos , D . ; and Mannila , H . 1997 . Finding similar In Principles of lfnowledge Discovery and Data Mining ( PKDD ) 1997 . Gersho , A . , and Gray , R . M . 1992 . Vector Quantization and Signal Compression . Boston : Kluwer Academic Publishers . Jain , A . K . , and Dubes , R . C . 1988 . Algorithms for Clustering Data . Englewood Cliffs , N J : Prentice Hall . Kauflnan , L . , and Rousseauw , P . J . 1990 . Finding Groups in Data : An Introduction to Cluster Analysis . John Wiley and Sons . Keogh , E . , and Smyth , P . 1997 . A probabilistic approach to fast . pattern matching in time series databases . In Heckerman , D . ; Mannila , H . ; Pregibon , D . ; and Uthurusamy , R . , eds . , Proceedings of the time series .
22 Das
