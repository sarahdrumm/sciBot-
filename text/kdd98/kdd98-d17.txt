Data reduction based on hyper relations
Hui Wang* , Ivo Diintsch* , David Bell*
School of Information and Software Engineering ,
University of Ulster ,
Newtownabbey , BT 37 0QB , N.Ireland , UK {H . Wang , I . Duent sch , DA . Bell}@ulst . ac . uk
Abstract is that smaller but preserves structures of interest .
Data reduction makes datasets classification In this paper we present a novel approach to data reduction based on lattice and hyper relations . Hyper relations are a generalization of conventional database relations in the sense that we allow sets of values as tuple entries . The advantage of this raw data and reduced data can both be represented by hyper relations . The collection of hyper relations can be naturally made into a complete Boolean algebra , and so for any collection of hyper tuples we can find its unique least upper bound ( lub ) as a reduction of it . We show that the lub may not qualify as a reduced version of the given set of tuples , but the interior cover the subset of internal elements covered by the lub does qualify . We establish the theoretical that such an interior cover exists , and find a way to find it . The proposed method was evaluated using 7 real world datasets . The results were quite remarkable compared with those obtained by C4.5 , and the datasets were reduced with reduction ratios up to 99 % . result
Introduction semantic in a stricter into a more condensed information . is a process which is used to transform form without losing In data mining , data to feature selection
Data reduction raw data significant reduction and data sampling [ (Weiss & Indurkhya 1997) ] . But is regarded as a main a broader sense , data reduction task of data mining [ (Fayyad 1997 ) ] hence any data mining technique can be regarded as a method for data reduction . sense refers
In ( Fayyad 1997 ) , a fundamental question was raised : tech
"Are there meaningful general reduction data
* Equal authorship is implied
Copyright Q1998 , American Association for Artificial ligence ( wwwaaaiorg ) All rights reserved .
Intel a a
R II A , I A2 II 0 1 1 0 1 1 0 0
1 2 3 1 2 3 2 3 a b b b e
C t2 t3
~4 t5
~o tl t6 t7
Table 1 : A relation on the scheme {A1 , A2 , As} where A3 is the classification attribute . IR’II A , I IIA I t~ tl t~
{1} {2,3} {2,3}
{a , b} {a , b} {4
0 1 0
Table 2 : A reduced version of the relation in Table 1 . that can help people effectively niques understand massive datasets ?
"
~dsualize and
In this paper we attempt as a process from an algebraic tion interpreted while preserving classification discussed To understand are pursuing , let ’s look at aa example . to answer the above quesperspective . is to reduce the size of datasets
Data reduction structure .
This will be in the context of algebra and hyper relations . the meaning of data reduction that we the dataset attribute ,
Example 1 . Consider as the classification dicting attributes . Merging the tuples {tl,t2,ta,t5} , tion labels of the original tuples . This leads to a reduced version shown in Table 2 , which clearly agrees with the examples in the original dataset . in Table 1 . Take A3 and A1 and A2 as prein sets {to,t3} , the classifica and {t6,t7} will preserve
KDD 98 349
This type of data reduction is useful in tile worlds of very large databases and data mining for the following reasons . It reduces the storage requirements of data used mainly for classification ; it offers better understandability for the knowledge discovered ; it allows feature selection and continuous attribute discretization to be achieved as by products of data reduction ; and it allows computationally demanding algorithms to become serious contenders in the search of knowledge discovery methods ( eg , Bayesian networks ) .
Definitions and notation
Lattice
]’T= {y E P : ( 3x E T ) x < y} .
Suppose that ~o = ( p , < ) is a partially ordered set , and Tg P . We let If T = {a} , we will write 1" a instead of 1" {a} ; more generally , if no confusion can arise , we shall usually identify singletons with the element they contain . Similarly , we define $ T .
A lattice L is a pm’tially ordered set such that for x , y E L the least upper bound x V y and the greatest lower bound x A y exist , and L has a smallest element 0 and a largest element 1 . An atom of L is an element a > 0 with $ a = {a , 0} .
For a,b E L , if a < b we usually say that a is below b . Given X,Y C_ L , we say X is covered by Y ( or covers X ) , written X ~ Y , if for any x E X there is y E Y such that x _< y ; in particular , if X _C Y , then X #Y .
Conversely , we say X is dense for Y , written X ~ Y , if for any y E Y there is x E X such that x _< y .
The reader is invited to consult ( Gr~itzer 1978 ) for unexplained notation and concepts in lattice theory . an attribute
Hyper relations Suppose that U is a finite set of attributes ; with each A E U we associate domain denoted by DOM(A ) . Denote the power set ( family of all subsets ) of DOM(A ) by VA . \Ve denote by 7 the Cartesian product of all the power sets , ie , HAeU VA . We call T a universal hyper relation over U . A hyper tuple t over U is an element of 7 and its A E U entry is denoted by t(A ) . A hyper relation is a subset of T , and is usually denoted by Rv .
A hyper tuple is called simple , if all its entries have cardinality of 1 ; a hyper relation is called simple , if all its tuples are simple . Simple relations correspond to [ (Ullman 1983) ] . Table conventional database relations
350 Wang
2 is an example of a hyper relation , while Table 1 is an example of simple relation .
It can be shown that the set of all hyper tuples is a lattice under the following ordering : tl < t2 ~ t,(A ) C_ t2(A ) , for all A E U . As a product of Boolean algebras , 7 is a Boolean algebra .
Atoms have entries
( ~ in all but one place ; the nonempty entry has exactly one element .
Given tl,t2 E T , the hyper similarity of tl to t2 , written S(t~,t2 ) , the number of A E U such that tl ( A ) <_ t2(A ) . Clearly , in general , 0 < S(tl , t.2 ) < [ U ] ; if tl < t2 , S(tl,t2 ) = [ U ] . is
Data reduction via interior covers
Data reduction can be achieved with universal hyper relations in the following way . Suppose we have a dataset represented as a simple relation R : some tuples are classified as 0 and others as 1 and no tuple is classified as both 0 and 1 . We want to reduce the size of it while preserving its classification structure .
Let Ri be the set of all hyper tuples classified as being for each in class i E {0,1} , and ri its lub in 7" , ie AEU , ri(A ) = U t(A ) . tERi
We can try to find a set of hyper tuples which , in a sense , is closest to the respective lub but preserves the classification structure . This closest set of hyper tuples will later be called the "interior" contained in the lub . To present our results we need to introduce the following concepts in the context of lattice , not just universal hyper relations .
The classification structure of a dataset in the universal hyper relation can be formally interpreted in general terms as a partial labeling of a lattice . Definition 1 ( Lattice labeling ) . Let G _C L . A labeling o]L is a partial mapping I :$ G ~ B , where B is a finite set , such that l(a ) = l(b ) for any a , b E l g and gEG .
G above can be interpreted as a dataset , and B as the set of classes . The functional nature of l guarantees that no element in L is labeled differently . This amounts to assuming that datasets are consistent .
The preservation of a lattice labeling is characterized as follows . Given a labeled lattice , we are interested in the elements sublattices such that in each sublattice ,
Figure 1 : A labeled la~ice . either have the same label or are unlabeled . The unlabeled elements can assume the same labeling as the labeled elements in the sublattice as a generalization . Then the largest elements of these sublattices used to represent the lattice universal hyper relation , dataset while preserving the classification structure . can be labeling . In the context of this amounts to reducing the
Such sublattices and , in particular , ments in them are what we are interested are characterized through the following concepts . the largest elein and they
Definition 2 . 7~(1 ) is the natural partition of G by the function l , ie two elements are in the same class iff they have the same value under l .
Definition 3 . m E L is equilabeled exactly one class of :P(l ) . if $ m intersects
In sum , the lattice labeling can be represented by a set of equilabeled elements , accompanied by the following simple rule :
Given an equilabeled element , any elements below it will have the same label as the equilabeled element ( if any ) .
However , for a labeled lattice , there are many equilabeled elements . We certainly cannot use all of them . Consider the lattice in Figure 1 . Elements A and B are both equilabeled elements . If we don’t want to keep both of them , which one should be preferred ? Certainly A has greater coverage of unlabeled elements than B , and thus , we can think of B as being of lower complexity in the sense that it is simpler to describe than A , given E and G ; in fact B is the lub of E and G . In the spirit of Occam ’s razor [ (Wolpert 1990) ] , given a set of labeled elements , we prefer a simple generalization . This leads to the following definitions .
Definition 4 . An E cover is a E L such that a is the lub of F C_ G and a is equilabeled . A pair of E covers , m and n , is said to be mergeable if m V n is also an Ecover . An E set is a set of E covers which are pairwise non mergeable .
The set of all E covers is written E . If two E covers are comparable , they are certainly mergeable . Also , each element of G is an E cover , and an E cover is a singleton E set .
Now our focus is on E covers instead of individual is : given a a dataset with known elements in a lattice . Our next question collection S of E covers ( eg , classes ) , what is the expected ( simpler ) representation of the lattice labeling ? Clearly the lub of S is ideal if it is also an E cover because , if so , the labeling of these E covers can be represented by the single lub . Unfortunately this lub may not be an E cover . But instead we can try to find a set of E covers which together is , in a sense , closest to the lub of S . Look at Figure 2 . Consider S de_f {g , I , J , K} . There are two sets of E covers which are below lubX and cover X : II1 def {A , C} and Y2 de f {A , B , C} . We argue that there is no reason not to include B and hence we prefer ]I2 to Y1 In general , we expect a maximal set of such E covers : a collection of all E covers satisfying the above conditions . We call this expected set of E covers the interior cover of S . is a maximal E set B C E such that A ~ B cover of A _C E , written
Definition 5 . The interior E(A ) , lub A and A ,1 B .
Lemma 1 . Let A C_ E and B be an interior cover of A . Then X # B for any X C $ such that A ~ X # lub A and A <~ X . banda_<x ,
~b ~ . Sincea<
Proof . Consider any x E X . Since A _ X , there must be a E A such that a ~ x . Since A ~ B , there must be b E B such that a < b . We need to show that there is b~ EBsuchthatx there are only three possible cases : ¯ x <_ b : obviously b~ = b . ¯ b ~ x : this means that b is mergeable with b~ leading to x , ie , bV b~ = x . This contradicts the assumption that B is an E set . Therefore this case is impossible . ¯ b and x are incomparable : due to the assumption they are non mergeable . Since B that B is an E set , is the maximal set of E covers by definition of interior cover , it follows that x E B . Therefore b~ = x .
Then an important question arises : does the interior cover exist ? Now we set out to answer this question .
[ ]
The following theorem establishes the existence of this interior , and illustrates a way to construct the interior .
KDD 98 351
Data reduction as an approach mining to data
As mentioned in ( Fayyad 1997 ) , in a general sense any data mining algorithms can be regarded as a process of data reduction . What we discussed above is aimed at reducing data while preserving its classification structure . This method can in turn be used as an approach to data mining building models from data .
In this apl)roach , both data and models are reprethough ahnost all datasets sented as hyper relations , we use in data mining m’e usually simple relations a special case of hyper relation . Training process is to find the interior cover of given data in the context of universal hyper relations . Recall that an interior cover is a set of pairwise non mergeable E covers , and an Ecover is a hyper tuple which covers a set of ( simple or hyper ) tuples equally labeled . The procedure has been given in the proof of Theorem 1 . Since a hyper tuple is simply a vector of sets , classification can be done via set inclusion . Specifically , suppose we have a set , M , of hyper tuples with known classes the result of data reduction and a simple tuple , d , the class of which is unknown . The classification of t is done using the ordering < in the universal hyper relation as follows .
¯
¯
¯
If d < m for m E M , then the class of d is identified with that of m . If there are multiple such m , then select the one which has the greatest coverage of simple tuples resulting from the data reduction process , and identify the class of d with that of this hyper tuple .
If there is no such m , then select the one which has the greatest hyper similarity value , and identify the class of d with that of this hyper tuple . resulting
We have implemented a system , called DR , which can reduce a dataset in a model of it , and classify unlabeled data using the model . The data reduction part is a straightforward implementation of the procedure described in the proof of Theorem 1 and the classification part is based on the above procedure . DR was evaluated using real world datasets and is reported in the following section .
Experiment and evaluation
The ultimate goal of data reduction is in improving learning performance . So the objective of the experiment is set to evaluate the proposed data reduction method to see how well it performs in prediction with real world datasets . This is measured in terms of test
Figure 2 : A lattice and its labeling .
Theorem 1 . The interior cover of any set of E covers exists .
Definition 6 . Let A,B C_ L . Define A + B = {aV b : a E A , b E B} , max(A ) be the set of maximal elements of A , and Eq(A ) be the set of all equilabeled elements of A .
Proof . Apply the following procedure to A C_ g :
1 . Mo ,l,f
2 . Co d~f max(Eq(Mo ) U
3 . M,,+t = Cn + Cn , Cn+l def def max(Eq(M,,+l ) U Cn ) .
4 . Continue until Cn = Cn+l .
Let C = C , , = Cn+l . Each c E C is equilabeled , and A 4 C . Assume that s,t E 6’ , , such that s + t is equilabeled . Then , s + t ¢’ C, , , because of maximality , and s + t E Eq(M,~+I ) \ Cn , contradicting that C,~ = Cn t 1 . Thus , C is an E set .
It is ( :lear that A ~ C ~ lubA and A <1 C , and all E covers which densely cover A are included in C hence maximal . Therefore the interior cover exists . [ ]
Theorem 1 indicates a way to construct the interior of any collection of E covers . Algorithms based on this theorem can be designed easily .
We now illustrate the use of the above theorem and its implied algorithm using an abstract lattice . shown in FigThen the parlabeling function is X ~ {darkblack , lightblack} .
Example 2 . Consider the labeled lattice ure 2 . Let X = {H,I,J,K,M,O} . tial The set of E covers is {H , I , J , K , M , O , A , B , C , } . Following the algorithm in Theorem 1 we get the inte~ior Y = {A , t3 , C,M,O} . ClearlyX ~ Y . Any other collection Y’ orE covers such that X ~ Y~ is covered by Y . For example , Y’ = {A , C , M , O} covers X and , clearly , Y~ ~ Y .
352 Wang
CD
14 8 13 4
Dataset NA NN NO NB NE 69O 383 : 307 268 : 500 768 120 : 150 270 150 50 : 50 : 50 I000 700 : 300 332 : 626 958 232 108 : 124
Aust 4 Diab 0 Hear 3 Iris 0 Germ 2O 11 TTT 9 Vote 0
4 0 3 0 2 0 18
6 8 7 4 7 0 0
9 18
Table 3 : General information about the datasets . The acronyms above are : Aust Australian , Diab Diabetes , Hear Heart , Germ German , TTT Tic Tae Toe ; NA Number of attributes , NN Number of Nominal attributes , NO Number of Ordinal attributes , NB Number of Binary attributes , NE Number of Examples , and CD Class Distribution accuracies using cross validation . The results are compared with those by standard data mining methods .
We chose 7 public datasets from the UCI machine repository 1 . Some information about these learning datasets is given in Table 3 .
To achieve our objective , we chose C4.5 for benchmarking , which is implemented in the Clementine package 2 . The evaluation method we used is 5 fold cross validation for both C4.5 and DR . We observed the test accuracies for both methods , as well as the reduction ratio by DR . The reduction ratio we used is defined as ( the number of tuples in the original datasets the number of hyper tuples in the model ) / ( the number of tuples in the original datasets ) . The results are shown in Table 4 . results of the experiment to the cross validated
Discussion From Table 4 we see that our DR algorithm outperforms C4.5 with respect test accuracy for all datasets but Vote . The reason for this is that the Vote dataset is binary , ie , all attributes have only binary values , and there is no reduction possible because the partitions obtained from binary attributes are coatoms in the partition lattice of the object set , see eg ( D/intsch & Gediga 1997 ) .
Conclusion
In this paper we have presented a novel approach to data reduction based on hyper relations . The reduced data can be regarded as a model of the raw data . We have shown that data reduction can be regarded as a process to find a set of interiors contained in the lhttp://wuwicsuciedu/ mlearn/HLRepositoryhtml 2http://uwwislcouk/
Dataset Aust Diab Hear Iris Germ TTT Vote
Average
TA : C4.5
TA : DR RR : DR
85.2 72.9 77.1 94.0 70.5 86.2 96.1 83.1
87.0 78.6 83.3 96.7 78.0 86.9 87.0 85.4
70.6 68.6 74.1 94.0 73.1 81.5 99.1 80.1
Table 4 : Test accuracies with C~.5 and DR , and the reduction ratios obtained by DR . The acronyms are : TA Testing Accuracy , RR Reduction Ratio least upper bounds of individual classes of tuples the dataset . that the interior in which we can find the expected interiors . in In the context of lattice , we have proved exists , and have demonstrated a way that
We illustrated the proposed data reduction method can be regarded as a novel approach to data mining . We also discussed a data mining system , called DR . The training module of DR is a straightforward implementation of our data reduction procedure , and the classification module is simply based on set inclusion .
Results from initial experiments with DR are quite remarkable . DR is a simple algorithm in both learning and classification , while C4.5 is a state of the art algorithm or slightly outperformed C4.5 with respect validated problems . test accuracy for non binary classification for data mining . DR was comparable to to cross
References
Diintsch , I . , and Gediga , G . 1997 . Algebraic aspects of attribute dependencies in information systems . Fundamenta Informaticae 29:119 133 . Fayyad , U . M . 1997 . Editorial . Data Mining and Knowledge Discovery An International Journal 1(3 ) . Grgtzer , G . 1978 . General Lattice Theory . Basel : Birkh~user . Ullman , J . D . 1983 . Principles of Database Systems . Computer Science Press , 2 edition . Weiss , S . M . , and Indurkhya , N . 1997 . Predictive Data Mining : A Practical Guide . Morgan Kaufmann Publishers , Inc . Wolpert , D . H . 1990 . The relationship between Occam ’s Razor and convergent guessing . Complex Systems 4:319 368 .
KDD 98 353
