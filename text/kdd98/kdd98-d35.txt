Large Datasets Lead to Overly Complex Models : an Explanation and a Solution
Tim Oates and David
Jensen
Experimental Knowledge Systems Laboratory
Department of Computer Science
University of Massachusetts Amherst , MA 01003 4610
{oates , jensen } @csuinassedu
Abstract that tie at
This paper explores unexpected results the intersection of two common themes in the KDD community : large datasets and the goal of building compact models . Experiments with many different datasets and several model construction algorithms ( including tree learning algorithms such as c4.5 with three different pruning methods , and rule learning algorithms such as C4.5RULES and RIPPER ) show that increasing the amount of data used to build a model often results in a linear increase in model size , even when that additional complexity results in no significant increase in model accuracy . Despite the promise of better parameter estimation held out by large datasets , as a practical matter , models built with large amounts of data are often needlessly complex and cumbersome . In the case of decision trees , the cause of this pathology is identified as a bias inherent in several common pruning techniques . Pruning errors made low in the tree , where there is insufficient data to make accurate parameter estimates , are propagated and magnified higher in the tree , working against the accurate parameter estimates that are made possible there by abundant data . We propose a general solution to this problem based on a statistical technique known as randomization testing , and empirically evaluate its utility .
Introduction increase increasing
This paper presents striking empirical evidence that , with several popular model construction algorithms , In particular , we show more data is not always better . the amount of data used to build a that in a linear increase in model size , model often results in no sigeven when that additional complexity results nificant large datasets may or may not yield more accurate models than smaller datasets , but they often result in models that are needlessly complex . The scope of this effect is explored with tree learning algorithms such as c4.5 with three different pruning methods and rule learning algorithms such as c4.SRULES and RIPPER . Equally surprising are empirical results showing that some of these algorithms build large models on datasets for in model accuracy . That is ,
Copyright @1998 , Americam Association for Artificial Intelligence ( wwwaaaiorg ) All rights reserved .
294 Oates labels have been randomized . When which the class there is absolutely no structure to be found in the data that would allow one to predict class labels , large models are still constructed , and the size of the model is still strongly dependent on the size of the dataset . Adding more data devoid of structure results in larger models ; in smaller models . removing some of those data results Given the empirical results mentioned above , this paper takes up two challenges : explaining the pathological relationship between dataset size and model size , and finding ways of building models that are not too large while retaining the benefits of large datasets ( eg accurate parameter estimates ) . In the case of decision trees , the cause of the relationship between dataset size and tree size is identified as a bias inherent in several common pruning techniques . Pruning errors made low in the tree , where there is insufficient data to make accurate parameter estimates , are propagated and magnified higher in the tree , working against the accurate there by parameter estinmtes that are made possible abundant data . We propose a general solution to this problem based on a statistical technique known as randomization testing , and empirically evaluate its utility .
The Problem :
Large Datasets and
Excess
Structure
In what way is model size dependent on dataset size ? In particular , when adding more data fails to improve model accuracy , as we expect will happen with moderately large datasets , what does adding more data do to model size ? We explored the answer to this question using three large datasets taken from the UC Irvine repository and three common model construction algorithms . The datasets are census income ( 32,561 instances ) , led24 ( 30,000 instances ) and letter recognition stances ) . The algorithms tree learner , c4.5 ( Quinlan 1993 ) , and two rule learners , C4.5RULES ( Quinlan 1993 ) and RIPPER ( Cohen 1995b ) . Various decision tree pruning techniques have been developed with the explicit goal of eliminating excess structure in trees . Therefore , we ran c4.5 with three different t)runing algorithms : error based pruning ( EBP the c4.5 default ) ( Quinlan 1993 ) , reduced error pruning ( REP ) ( Quinlan 1987 ) , and minimum description include one decision
( 20,000 length pruning ( MDL ) ( Quinlan ~ Rivest 1989 ) .
Plots of model size and accuracy for the led 24 dataset and c4.5 with REP , C4.5RULES ( which takes as input unpruned trees ) and RIPPER are shown below in Figure 1.1 Model sizes reported for c4.5 are numbers of tree nodes , and sizes reported for C4.5RULES and RIPPER are numbers of conditions in rules . The number of instances used with c4.5 and RIPPER ranged from 0 , in which case the best one can do is guess class labels randomly , to 20,000 . The number of instances used with C4.5RULES ranged from 0 to 5,000 because long running times . larger datasets led to prohibitively Note that all of the accuracy curves in Figure 1 reach their maximum value at fewer than 1,000 instances and thereafter remain almost constant . Shockingly , despite the fact that additional data results in no increase in accuracy , model sizes continue to grow dramatically . The ratio of the size of the tree built by c4.5 on the full dataset to the size of the tree built on 1,000 instances is 142 That ratio for RIPPER is 4.6 , and for C4.5RULES it is 39 It is important to remember that C4.5RULES used a much smaller dataset been computationally feasible full dataset there is every indication that its ratio would have been significantly sults is better than the other .
In no way do these reindicate that one of the rule learning algorithms than RIPPER , and had it to run C4.5RULES on the larger .
Another noteworthy feature of the graphs in Figure 1 is the apparently linear nature of the relationship between model size and dataset size beyond the point at which accuracy ceases to increase . For each combination of dataset and algorithm , that point was found by scanning the accuracy curve from left to right , stopping when the mean of three adjacent accuracy estimates was no more than 1 % less than the accuracy of the model based on the full dataset . Running linear regression on the model size curves to the right of this point reveals that the relationship between model size and dataset size is often highly linear . The upper portion of Table 1 shows r 2 , which can be interpreted as the fraction of variance in model size attributable to dataset size . In the majority of cases , more than 90 % of the variance in model size is due to changes in dataset size . The lower portion of Table 1 shows the ratio of the size of the model built on the full dataset to the size of the model built on the smallest amount of data needed to achieve approximately maximum classification accuracy .
One possible explanation for the apparently pathological relationship between dataset size and model size is that as the amount of available data increases the algorithms are able to discern increasingly weak structure in the dataset . An alternative is that the algorithms are simply susceptible to fitting noise , and this problem is exacerbated by large datasets . These two views were explored by building models on datasets for which the
1Details on the experimental method and results for 19 datasets and 4 different pruning techniques can be found in ( Oates & Jensen 1997 ) . r~ census 0.98 0.98 0.99 0.91 0.86 led 24 1.00 0.98 1.00 0.97 0.92
Size Factor census led 24 1.2 14.2 19.7 3.9 4.6
3.8 7.4 4.2 2.1 3.5 letter 1.00 0.99 0.97 0.98 0.83 letter 13.3 1.9 1.3 1.3 1.1
C4.5/EBP C4.5/REP C4.5/MDL C4.5RULES RIPPER
C4.5/EBP C4.5/REP C4.5/MDL C4.5RULES RIPPER
Table 1 : Summaries of the behavior of model size as a function of dataset size over the range of dataset sizes for which accuracy no longer increases . class labels had been randomized , destroying any structure in the data that would allow one to predict class labels .
Figure 2 shows plots of model size as a function of randomized dataset size for each of the algorithms and datasets . Despite the fact that the data are completely devoid of useful structure , EBP and MDL build trees with literally thousands of nodes on the led 24 and letter datasets after as few as 5000 instances . EBP does quite well on the census dataset , constructing very small trees , but MDL and REP perform poorly . Although a direct comparison of the sizes of trees and rule sets is not possible , the rule learning algorithms build very small models on all three datasets , and they appear to remain small as the size of the dataset increases . In fact , RIPPER produces empty rule sets , as it should , for the letter and led 24 datasets , and rule sets with fewer than ten conditions for the census dataset . RIPPER is the only algorithm that behaves as expected when presented with structureless data .
The Cause :
Bias
Propagation exWhy would any of the algorithms explored earlier hibit a pathological relationship between model size and dataset size ? This section answers that question for c4.5 by identifying a bias inherent in all three pruning techniques that leads to a phenomenon that we call bias propagation . Developing an answer for the rule learning algorithms is left as future work . The discussion begins with an analysis of REP for concreteness and clarity , and is then generalized to include EBP and MDL .
REP builds a tree with a set of growing instances , and set of then prunes the tree bottom up with a disjoint errors pruning instances . The number of classification that a subtree rooted at node N makes on the pruning set , ET(N ) , is compared to the number of errors made when the subtree is collapsed to a leaf , EL ( N ) . If ET(N ) >_ EL(N ) , then N is turned into a leaf .
Note that EL(N ) is independent of the structure of
KDD 98 295 i,:~t o
6~ io
Figure 1 : Plots of model size ( upper row ) and accuracy ( lower row ) on the led 24 dataset for c4.5 with REP ( leftmost column ) , RIPPER ( middle column ) and C4.5RULES ( rightmost column ) . Note that the scales of the model size plots are different . in the pruning set that match the attribute the subtree rooted at N . To compute EL ( N ) , all of the instances tests on the path from the root of the tree to N are treated as a set . The number of instances in this set that do not belong to the majority class of the set is the number of errors that the subtree would make as a leaf . For a given pruning set , EL(N ) depends only on the structure of the tree above N , and therefore does not depend on how pruning set instances are partitioned by additional tests below N . As a consequence , EL ( N ) remains constant as the structure beneath N changes due to the effects of bottom up pruning . to EL , ET(N ) is highly dependent on In contrast of the subtree rooted at N . ET(N ) is the structure defined to be the number of errors made by that subtree on the pruning set , and its value can change as pruning takes place beneath N . Consider a subtree rooted at N’ , where N’ is a descendant of N . If ET(N’ ) < EL(N’ ) then N’ is not pruned , and bebeneath N remains unchanged , cause the structure ET(N ) also remains unchanged . The alternative is that ET(N’ ) >_ EL(N’ ) , in which case N’ is turned into a leaf . This structural change either causes ET(N ) to remain unchanged ( when ET(N’ ) = EL(N’ ) ) or to decrease ( when ET(N’ ) > EL(N’) ) .
EL and ET can be used to estimate the error rate of a subtree , as a leaf and as a tree respectively , on the population of instances from which the pruning set was drawn . Each time pruning occurs beneath N , EL(N ) remains invariant and ET(N ) usually decreases . This
296 Oates the probability systematic deflation of ET , a statistical bias inherent in ( 1 ) pruning beneath N inREP , produces two effects : creases that ET(N ) < EL(N ) and that N will therefore not be pruned ; ( 2 ) ET for the final pruned tree tends to be an underestixnate . These effects should be larger for large unpruned trees , because they afford many opportunities to prune and to deflate ET . These effects should also be larger for small pruning sets because they increase the variance in estimates of EL and ET . Even when a node is more accurate as a tree than as a leaf on the population , highly variable that , by random chance , estimates make it more likely ET(N ) > EL(N ) and the subtree rooted at N will be pruned , thereby lowering ET for all parents of N . Likewise , even when a node is more accurate as a leaf than as a tree , it is more likely that , by random chance , ET(N ) < EL(N ) , resulting in no change in ET for the parents of N and the retention of the structure beneath N . In either case , the net result is larger trees , either from the explicit retention of structure or systematic deflation of ET which often leads to the retention of structure higher in the tree .
What are the features , at an abstract level , that lead to bias propagation in REP ? First , each decision node in a tree is assigned two scores for the purpose of pruning : the score of the node as a tree , ST(N ) , and the score of the node as a leaf , SL(N ) . For REP , these scores correspond to errors on the pruning set . Second , the disposition of the node is chosen to either maximize or minimize its score . When scores are equiva
~eP~
/
/
/"
. .~"
~_~ ~
__~~
,~ r~
Figure 2 : Model size as a function of dataset size for datasets with randomized class labels . lower scores are better and pruning ocfunctions lent to errors , curs when SL(]V ) ~_ ST(N ) . Other scoring may treat high scores as better and prune nodes when St(N ) > ST(N ) . ( The effects of consistently choosing the maximum or minimum of a set of values on the sampling distribution is discussed in detail in ( Jensen & Cohen 1998) . ) Third , the score of a node as a tree ( ST ) is directly dependent on the scores of all of that node ’s children . Finally , pruning proceeds bottom up . The net effect is that pruning decisions made below a node in the tree serve to bias the score of that node as a tree , regardless of whether those pruning decisions were correct , making the node ’s subtree look increasingly attractive for retention . to aEP , satisfy
Both MDL and EBB , in addition all for bias propagation . MDL is a of the requirements that chooses to prune bottom up pruning technique when the description length of a node is smaller as a leaf than as a tree , and the number of bits required to encode a tree is directly related to the number of bits required to encode the children of the tree ’s root . EBP is a bottom up pruning technique that chooses to prune when the estimated number of classification errors that a node will commit as a leaf is smaller than as a tree , and the error estimate for a node is directly related to the error estimates for that node ’s children . that ST(N ) can be greater
The Solution : Randomization Pruning For each decision node , N , in a pruned decision tree , ST(N ) > SL(N ) ( assuming that we are maximizing S ) ; otherwise , N would have been pruned back to a leaf . However , experiments with randomized data showed clearly than SL(N ) even when the subtree rooted at N is fitting noise , and the previous section identified bias propagation as the cause of this problem . Ideally , we would like to retain decision nodes only when their scores are high because they root subtrees that represent structure in the data , not inflated due to bias when their scores are artificially propagation . Stated in terms of statistical hypothesis testing , our null hypothesis ( H0 ) is that the class label is independent of the attributes in the data that arraive at a node . We would like the probability of obtaining ST(N ) under Ho to be low for all of the decision nodes in the pruned tree . to determine is a statistical
Randomization testing technique that unconstructs an empirical distribution of a statistic the probder H0 , making it possible than ability of obtaining a value larger ( or smaller ) any particular value for that statistic given that Ho holds ( Cohen 1995a ; Edgington 1995 ; Jensen 1991 ; 1992 ) . For example , consider the null hypothesis stated above . For any node , N , in a decision tree , we want to determine the probability of obtaining a score greater than or equal to ST(N ) given H0 . If that probability is that the subtree rooted at N low , we can be confident has captured structure in the data . If that probability is high , tbe subtree might be fitting noise due to bias propagation .
To construct an empirical distribution of ST ( N ) unthe following procedure K times . der H0 , we repeat Collect all of the data that arrive at node N , randomize the class labels of that data , build and prune a tree on the randomized data , and record ST for that tree . Randomizing the class labels enforces Ho . Given a sample of values of ST of size K obtained in this manner , we can estimate the of obtaining a value at least as high as ST(N ) based on the original ( not randomized ) data by counting the number of values in the sample that are greater than or equal to ST(N ) and dividing by K . This simple application of randomization testing was used as the basis for randomization pruning , aa algorithm for eliminating excess structure from trees built on large datasets . Randomization pruning simply walks over a pruned tree and at each node , N , computes the probability that a score at least as high as ST(N ) could have been obtained under Ho . If is above some threshold , a , then N is pruned back to a if N was retained simply because ST(N ) leaf . That is , was artificially inflated as a result of bias propagation , then we eliminate N from the tree . that probability
Figure 3 shows the results of applying randomization pruning to EBP with a 005 Each of the three columns represents a dataset , the plots in the upper row are size , and the plots in the lower row are accuracy . Each plot shows results for standard EBP and for EBP enhanced with randomization pruning . All curves were generated by randomly selecting 5,000 instances from the dataset , retaining all of the remaining instances for testing accuracy , and building trees on subsets of vari
KDD 98 297
%
Figure 3 : The effects of randomization pruning on tree size and accuracy . ous size of the 5,000 instances . That is , the points in the curves are not means . Notice that randomization pruning produced dramatically smaller trees for all three datasets , with the size of the reduction increasing with the size of the dataset . There was no appreciable impact on accuracy for census and led 24 , and there was a loss of accuracy of 5.4 % on average for letter . Unlike the other two datasets , accuracy on letter increased over all dataset sizes in Figure 3 .
Conclusion increase increasing in no significant algorithms showed that
Experiments with several different datasets and model the construction amount of data used to build models often results in in model size , even when that addia linear tional complexity results increase in model accuracy . Additional experiments produced the that some algorithms ( notably c4.5 ) surprising result construct huge models on datasets that are devoid of structure . Based on this observation , the cause of the pathological relationship between dataset size and tree size was identified as a bias inherent in all three decision tree pruning techniques . We proposed randomization pruning , an algorithm than can be wrapped around existing pruning mechanisms , as a mechanism for counteracting bias propagation , and evaluated its utility .
Future work will involve additional experimentation including extending the with randomization pruning , implementation to wrap around REP and MDL and running it on additional datasets . Also , we want to determine when and why rule learning algorithms fail to appropriately control rule set growth as the size of the dataset increases .
298 Oates
References rule testing . induction .
Cohen , P . R . 1995a . Empirical Methods for Artificial Intelligence . The MIT Press . Cohen , W . W . 1995b . Fast effective In Proceedings of the Twelfth International Conference on Machine Learning . Edgington , E . S . 1995 . Randomization Tests . Marcel Dekker . Jensen , D . , and Cohen , P . R . 1998 . Multiple comparisons in induction algorithms . To appear in Machine Learning . through inJensen , D . 1991 . Knowledge discovery duction with randomization In Proceedings of the 1991 Knowledge Discove~ y in Databases Workshop , 148 159 . Jensen , D . 1992 . Induction with Randomization Testing : Decision Oriented Analysis of La~ye Data Sets . PhD Dissertation , Washington University . Oates , T . , and Jensen , D . 1997 . The effects of training set size on decision tree complexity . In Proceedings of The Fourteenth International Conference on Machine Learning , 254 262 . Quinlan , J . R . , and Rivest , R . 1989 . Inferring decision trees using the minimum description Info~’mation and Computation 80:227 248 . Quinlan , 3 . R . 1987 . Simplifying decision trees . International Journal of Man Machine Studies 27:221 234 . Quinlan , J . R . 1993 . C4.5 : Programs for Machine Learning . Morgan Kaufmann . length principle .
