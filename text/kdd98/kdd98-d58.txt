Toward Scalable Learning with Non uniform Class and Cost Distributions : A Case Study in Credit Card Fraud Detection
Philip Computer Science
K . Chan
Florida Institute of Technolog7
Melbourne , FL 32901 pkc~cs , f it . edu
Abstract
Very large databases with skewed class distributions and non unlform cost per error are not uncommon in real world data mining tasks . We devised a multi classifier meta learning approach to address these three issues . Our empirical results from a credit card fraud detection task indicate that the approach can significantly reduce loss due to illegitimate transactions .
Introduction and other factors ,
Very large databases with skewed class distributions and non uniform cost per error are not uncommon in real world data mining tasks . One such task is credit card fraud detection : the number of fraudulent transactions is small compared to legitimate ones , the amount of financial loss for each fraudulent transaction depends on the amount of transaction and millions of transactions occur each day . A similar task is cellular phone fraud detection ( Fawcett & Provost 1997 ) . Each of these three issues has not been widely studied in the machine learning research community . Fawcett ( 1996 ) summarized the responses to his inquiry on learning with skewed class distributions . The number of responses was few given skewed distributions are not rare in practice . Kubat and Matwin ( 1997 ) acknowledged the performance degradation effects of skewed class distributions and studied techniques for removing unnecessary instances from the majority class . Instances that are in the borderline region , noisy , or redundant are candidates for removal . Cardie and Howie ( 1997 ) stated that skewed class distributions are "the norm for learning problems in natural language processing ( NLP)." In a case based learning framework , they studied techniques to extract relevant features from previously built decision trees and customize local feature weights for each case retrieval .
Error rate is commonly used in evaluating learning algorithms ; cost sensitive learning has not been widely investigated . Assuming the errors can be grouped into a few types and each type incurs the same cost , some Copyright @1998 , American Association for Artificial Intelligence ( wwwaaaiorg ) All rights reserved .
164 Chan
Salvatore
J . Stolfo
Department of Computer Science
Columbia University New York , NY 10027 salOcs , columbia , edu studies ( eg , ( Pazzani et al . 1994 ) ) proposed algorithms that aim to reduce the total cost . Another line of costsensitive work tries to reduce the cost in using a classifier . For instance , some sensing devices are costlier in the robotics domain ( Tan 1993 ) . Fawcett and Provost ( 1997 ) considered non uniform cost per error in timir cellular phone fraud detection task and exhaustively searched ( with a fixed increment ) for the Linear Threshold Unit ’s threshold that minimizes the total cost .
Until recently , researchers in machine learning have been focusing on small data sets . Efficiently learning from large amounts of data has been gaining attention due to the fast growing field of data mining , where data are abundant . Sampling ( eg , parallelism ( eg , main directions in scalable learning . Much of the parallelism work focuses on parallelizing a particular algorithm on a specific parallel architecture . That is , a new algorithm or architecture requires substantial amount of parallel programming work .
( Catlett 1991 ) ) ( Provost & Aronis 1996 ) ) are the and integrating
We devised a multi classifier meta learning approach to address these three issues . Our approach is based on creating data subsets with the appropriate class distribution , applying learning algorithms to the subsets to opindependently and in parallel , timize cost performance of the classifiers by learning ( meta learning ( Chan & Stolfo 1993 ) ) from their classification behavior . That is , our method utilizes all available training examples and does not change the underlying learning algorithms . It also handles non uniform cost per error and is cost sensitive during the learning process . Although our architecture and algorithmindependent approach is not as efficient as the finegrained parallelization approaches , it allows different "off the shelf" learning programs to be "plugged" into a parallel and distributed environment with relative ease . Our empirical results for the credit card fraud problem indicate that our approach can significantly reduce loss due to illegitimate transactions .
This paper is organized as follows . We first describe the credit card fraud detection task . Next we examine the effects of training class distributions on the performance . We then discuss our multi classifier metalearning approach and empirical results . In closing , we s,,mmarize our results and directions .
Credit Card Fraud Detection
When banks lose money because of credit card fraud , ( possibly entirely ) pay for the card holders partially loss through higher interest rates , higher membership fees , and reduced benefits . Hence , it is both the banks’ to reduce illegitimate use of and card holders’ interest credit cards , particularly when plastic is prevalent in today ’s increasingly electronic society . Chase Manhattan Bank provided us with a data set that contains half a million transactions from 10/95 to 9/96 , about 20 % of is much more which are fraudulent ( the real distribution skewed ( fortunately) the is what we were given after the bank ’s filtering ) .
20:80 distribution
Due to the different dollar amount of each credit card the cost of failing to deis not the same . transaction and other factors , tect different Hence we define : fraudulent transactions
AverageAggregateCost =_1 E Cost(i ) n i . n where Cost(i ) is the cost associated with transaction i and n is the total number of transactions . After constilting with a bank representative , we settled on a simplified cost model ( the cost model used by the bank is more complex and is still evolving ) . Since it takes time and personnel to investigate a potential fraudulent transaction , an overhead is incurred for each investigation . That is , if the amount of a transaction is smaller than the overhead , it is not worthwhile to investigate the transaction even if it is suspicious . For example , if it takes ten dollars to investigate a potential loss of one dollar , it is more economical not to investigate . Therefore , assuming a fixed overhead and considering :
Actual Negative
[ Number of instances ]
( fraudulent )
Predicted Positive
Predicted Negative
( fraudulent ) II Actual Positive True Positive
( legitimate ) False Positive ( False Alarm ) [ b ] True Negative ( JVo~na0 [ d ] we devised the following cost model for each transaction :
False Negative
( legitimate )
( Miss ) [ c ]
( Hit ) [ a ]
Cost(FN ) Cost( F P
Cosf(TP )
Cosf(TN ) tranamt overhead if tranamt > overhead or 0 if franamt ~_ overhead overhead if ~ranarnf > overhead or tranamt if tranamt < overhead 0 where ¢ranamt is the amount of a credit card transaction . Furthermore , we define the false negative rate as rate as b ~d" Based on this c and the false positive a+c cost model , we next study the effects of training class distributions on performance .
Effects of Training Distributions
Experiments were performed to study the effects of training class distributions on the credit card cost
Credit Card Fraud ( C4.6 )
BO
Overhead = 25 Overhead 50 ~ Overhead ~ 7S .~ overhead ~ 100 ovo,,o=d . ,5o Ovorhoad = 125
2,~ ~o
20
~ ’
. ~7 ;
2o
1 o go Figure 1 : Training distribution vs . the credit card fraud cost model
Dlatdbutlon of minority olaos ( O/o )
3o
4o
5o
6o
8o
70 model . We use data from the first 10 months ( 10/95 7/96 ) for training and the 12th month ( 9/96 ) for testing . In order to vary the fraud distribution to 90 % for each month , we limit the size of the training sets to 6,400 transactions , which are sampled randomly without replacement . Four learning algorithms ( C4.5 ( Quinlan 1993 ) , CART ( Breiman et al . 1984 ) , RIPPER ( Cohen 1995 ) , and BAYES ( Clark & Niblett 1989 ) ) were used in our experiments . from 10 % only results
The results are plotted in Figure 1 ( due to space limfrom C4.5 are shown the other itations , is an algorithms behave similarly ) . Each data point each of which is generated average of 10 classifiers , from a separate month . Each curve represents a different amount of overhead . Fraud is the minority class . As expected , the larger overhead leads to higher cost . More importantly , we observe that when the overhead is smaller , the cost minimizes at a larger percentage of fraudulent transactions ( minority class ) in the training set . When the overhead is smaller , the bank can afford to send a larger number of transactions for investigathe bank can tolerate more false alarms tion . That is , rate ) and aim for fewer misses ( a higher false positive rate ) , which can be achieved ( a lower false negative by a larger percentage of fraudulent transactions ( positive ’s ) . Conversely , if the overhead is larger , the bank should aim for fewer false alarms ( a lower FP rate ) and tolerate more misses ( a higher FN rate ) , which can obtained by a smaller percentage of positive ’s . that , at some point , making fraud detection economically undesirable . )
( Note the overhead can be large enough
The test set ( from 9/96 ) has 40,038 transactions and 17.5 % of them are fraudulent . If fraud detection is not available , on the average , $36.96 is lost per transaction . Table 1 shows the maximum savings of each algorithm with the most effective fraud percentage during training . Cost is the dollars denotes the most effective ing ; %saved represents the percentage of savings from the average loss of $36.96 ; Ssaved shows the total dollars the month ( 9/96 ) . BAYES performed relatively poor ( we suspect the way we are treating attributes with real values in BAYES is not appropriate for the fraud domain ) and is excluded from the following discussion . Considering the amount of overhead ranged fraud percentage for train lost per transaction ; saved for fraud %
KDD 98 165
Table 1 : Cost and saving in the credit card fraud domain
Learning
Alg . C4.5 CART RIPPER BAYES
Overhead = $50
[
Overhead = 875
Cost [ fraud % [ %saved I Ssaved I[ Cost [ fraud % [ %saved I $saved 27 % 23.85 404K 36 % 534K 20.80 21.16 510K 34 % 39K 3 % 35.23
35 % 5h5K 26.88 44 % ~ % 5 %
30 % 50 % 50 % 20 %
50 % 50 % 50 % 30 %
647K 632K 69K
23.64 24.23 35.99
Overhead $100
Cost I fraud % ] %saved I $saved 28.46 341K 30 % 437K 26.05 26.73 4()9K 28 % 1 % 36.58 15K
30 % 23 % 50 % 50 % 20 %
Figure 2 : Generating four 50:50 data subsets from a 20:80 data set
Font 50:50 lute , ca from $50 to $100 , the learning algorithms we used generally achieved at least 20 % in savings or at least $300K . With an overhead of $75 or less , at least half a million dollars in savings can be attained . More importantly , maximum savings were achieved 7 out of 9 times when the fraud percentage used in training is 50 % , which is not the "natural" 20 % in the original data set . In summary , based on our empirical results in this study , we observe that varying the training class distrithe cost performance . Since the "natubution affects is 20:80 , one way to achieve the "deral" distribution sired" 50:50 distribution is to ignore 75 % of the legitimate transactions we did in the experiments above . The following section discusses an approach that utilizes all the data and improves cost performance and scalability .
( or 60 % of all the transactions ) ,
A Multi classifier Meta learning Approach to Non uniform Distributions As we discussed earlier , using the natural class distribution might not yield the most effective classifiers ( particularly when the distribution is highly skewed ) . Given a skewed distribution , we would like to generate the desired distribution without removing any data . Our approach is to create data subsets with the desired distribution , generate classifiers from the subsets , and integrate from their behavior . In our fraud domain , the natuclassification ral skewed distribution is 20:80 and the desired distribution is 50:50 . We randomly divide the majority instances into 4 partitions and 4 data subsets are formed by merging the minority instances with each of the 4 partitions containing majority instances . That is , the minority instances are replicated across 4 data subsets to generate the desired 50:50 distribution . Figure 2 depicts this process . them by learning ( meta learning )
Formally , let n be the size of the data set with a distribution of x : y ( x is the percentage of the minority class ) and u : v be the desired distribution . The number of minority instances is n × x and the desired number of majority in a subset is nx x ~ . The number of subsets is the number of majority instances ( n x y ) instances
166 Chan instances divided by the number of desired majority in each subset , which is.=~’Y or ~ x ~.u ( When it is not a whole number , we take the ceiling ( [~ x ~ ] ) and replicate some majority instances to ensur~e al~ of the majority instances are in the subsets . ) That is , we have x _u subsets , each of which has nx minority instances u and ~ majority instances .
The next step is to apply a learning algorithm(s ) each of the subsets . Since the subsets are independent , the learning process for each subset can be run in parallel on different processors . For massive amounts of data , substantial improvement in speed can be achieved for super linear time learning algorithms .
The generated classifiers are combined by learning ( meta learning ) from their classification behavior . Several meta learning strategies are described in ( Chan Stolfo 1993 ) . To simplify our discussion , we only describe the class combiner ( or stacking ( Wolpert 1992 ) ) strategy . In this strategy a meta level training set is composed by using the ( base ) classifers’ predictions a validation set as attribute values and the actual classification as the class label . This training set is then used to train a meta classifier . For integrating subsets , classcombiner can be more effective than the voting based techniques ( Chan & Stolfo 1995 ) .
( the and Results for validating , two month gap is chosen
Experiments To evaluate our multi classifier meta learning approach to skewed class distributions , we used transactions from the first 8 months ( 10/95 5/96 ) for training , the ninth month ( 6/96 ) and the twelfth month ( 9/96 ) for testing cording to the amount of time needed to completely determine the legitimacy of transactions and simulates real life ) . Based on the empirical results in the previous section , the desired distribution is 50:50 . Since the natural distribution is 20:80 , four subsets are generated from each month for a total of 32 subsets . We applied four learning algorithms ( C4.5 , CART , RIPPER , and BAYES ) to each subset and generated 128 base classitiers . BAYES ( more effective experience ) was used to train the meta classifier . for meta learning in our
Results from different amounts of overhead are plotted in Figure 3 . Each data point is the average of ten runs using different random seeds . To demonstrate that 50:50 is indeed the desired distribution , we also performed experiments on other distributions and plotted the results in the figure . As expected , the cost is minimized when the fraud percentage is 50 % . Surprisingly ,
Credit Card Fraud ( BAYE$ )
Credit Card Fraud ( BAYES )
I 2~
Overhead Overhead m 50 ~ C~verhead m 76 "~ Ov~l~ d i 100 ~ Overhead ~ 126 Overhesd 150
Overhead = 25 Overhead = 50 ~Overhoad = 75 .~ Overhead = 100 Overhead = 125 Overhead ~ 160
? :::::::::::::::::::::::::::::::::::: : :
70
60 so
3o
20
10 Figure 3 : learning
10
20
30
40 Distribution
50
70 of minority class ( % )
60
80
90
Figure 4 : Distribution meta learning in validation set vs . cost using vious set of experiments ) . The different distributions are generated by keeping all the fraudulent transactions the corresponding number of leand randomly selecting gitimate transactions ( ie , some legitimate records are thrown away ) .
Each data point in Figure 4 is an average of 10 runs using different random seeds ( though the same seeds as in previous experiments ) . To prevent amplifying small differences unnecessarily , we use the same scale in Figure 4 as in the other similar figures . Unexpectedly , the curves are quite flat . That is , changing the class distribution in the validation set does not seem to affect the cost performance ( a subject of further investigation ) .
Table 3 has the same format as the previous cost and savings tables , but the fraudYo columns denote the fraud percentage in the validation set . The first row shows the results from class combiner using the natural distribution in the validation set ( from Table 2 ) . The second row has the lowest cost performance and the corresponding fraud percentage in the validation set . The cost is minimized at around 50 % ( the difference is negligible among different distributions ( Figure 4) ) , it is not much different from the performance obtained from the natural distribution . Note that a 50:50 distribution was obtained by throwing away 60 % of the data from the natural 20:80 distribution in the validation set . That is , comparable performance can be achieved with less data in the validation set , which is particularly beneficial of data . in this domain that has large amounts
Concluding Remarks and the natural distribution
This study demonstrates that the training class distribution affects the performance of the learned clascan be different sifters from the desired training distribution that mazcimizes performance . Moreover , our empirical results indicate that our multi classifier meta learning approach using a 50:50 distribution in the data subsets for training can significantly imate transactions . The subsets are independent and can be processed in parallel . Training time can further be reduced by also using a 50:50 distribution in the validation set without degrading the cost performance . That is , this approach provides a means for efficiently reduce the amount of loss due to illegit
KDD 98 167
20
30
60 Training distribution
40
70
60
90 vs . cost using meta
80
Distribution of minority class ( °/6 )
50 % is the desired distribution for any of the overhead amount . This is different from the results obtained from previous experiments when meta learning was not used . if our approach is indeed
Furthermore , to investigate
( 128 base classifiers ) ,
( 32 base classifiers 8 fruitful , we ran experiments on the class combiner strategy directly applied to the original data sets from the first 8 months ( ie , they have the natural 20:80 distribution ) . We also evaluate how individual classifiers generated from each month perform without class combining . Table 2 shows the cost and savings from class combiner using the 50:50 distribution the average of individual CART classifiers generated using the desired distribution ( from Table 1 ) , class combiner using the natural distribution and the average of months x 4 learning algorithms ) , individual classifiers using the natural distribution ( the ( We did not perform experaverage of 32 classifiers ) . iments on simply replicating the minority instances to achieve 50:50 in one single data set because this approach increases the training set size and is not appropriate in domains with large amounts of data one of the three primary issues we try to address here . ) Compared to the other three methods , class combining on subsets with a 50:50 fraud distribution clearly achieves a significant least $110K for the month ( 6/96 ) . When the overhead is $50 , more than half of the losses were prevented . Surprisingly , we also observe that when the overhead is $50 , a classifter from one ’s month data with the desired 50:50 distribution ( generated by throwing away some data ) achieved significantly more savings trained from all eight months’ than combining classifiers data with the natural distribution . This reaffirms the importance of employing the appropriate training class distribution
( "single CART" ) trained in this domain . in savings at increase set in the vhlidation
Class distribution Thus far we have concentrated on the class distributions in training the base classifiers . We hypothesize that the in the validation set ( and hence the class distribution meta level training set ) affects the overall performance of meta learning . To investigate that hypothesis , in this set of experiments , we fixed the training distribution of the base classifiers to 50:50 and varied the distribution of the validation set from 30:70 to 70:30 ( as in the pre
Table 2 : Cost and savings using meta learning
Overhead = $50
Overhead $75
Overhead $100
Method
Glass combiner Single
CART
( nat . ) Class combiner Avg . single classifier
Cost 17.96 20.80 22.61 27.97
[ fraud % 50 % 50 % natural natural
[ %saved [ $saved 51 % 761K 44 % 647K 39 % 575K 360K 24 %
[
Cost 20.07 23.64 23.99 29.08
[ fraud %
[ %saved
[ Ssaved 50 % 46 % 676K 36 % 50 % 534K 35 % 519K natural 21 % 315K natural
[ Cost 21.87 26.05 25.20 30.02
] fraud %
[ Ssaved
[ %saved 50 % 41 % 604K so % 30 % 437K 471K 32 % 19 % 278K natural natural
Table 3 : Cost and savings using different validation distributions ( base distribution is 50:50 )
Overhead $50
Overhead
$75
Overhead = $100
Method
Class combiner(nat . )
Class combiner
Cost 17.96 17.84 natural 70 %
] fraud % 1%saved
Cost
[ $saved
51%1761K20.07natural I 46%1676KII 21.87 [ natural
1%saved
I Cost fraud %
[
I fraud %
1%saved
[ $saved
766K
19.99
50 %
52 %
46 %
679K
21.81
50 %
[
I $saved I ] 41%1604K I
607K
41 % handling learning tasks with skewed class distributions , non uniform cost per error , and large amounts of data . Not only is our method efficient , it is also scalable to larger amounts of data .
Although downsanlpling instances of the majority is not new for handling skewed distributions class ( Breiman et al . 1984 ) , our approach does not discard any data , allows parallelism for processing large and permits the usage of amounts of data efficiently , learning algorithms to increase multiple "off the shelf" diversity among the learned classifiers . Furthermore , how the data are sampled is based on the cost model , which might dictate downsampling instances of the minority class instead of the majority class .
One limitation of our approach is the need of running preliminary experiments to determine the desired distribution based on a defined cost model . This process can be automated but it is unavoidable since the desired distribution is highly dependent on the cost model and the learning algorithm .
Using four learning algorithms , our approach generfrom a 50:50 class distribution and ates 128 classifiers eight months of data . We might not need to keep all since some of them could be highly cor128 classifiers related and hence redundant . Also , more classifiers are generated when the data set is more skewed or addilearning algorithms are incorporated . Metrics tional for analyzing an ensemble of classifiers ( eg , diversity , correlated error , and coverage ) can be used in pruning unnecessary classifiers . Furthermore , the real disis more skewed than the 20:80 provided to tribution us . We intend to investigate our approach with more skewed distributions . As with a large overhead , a highly skewed distribution can render fraud detection economically undesirable . More importantly , since thieves also learn and fraud patterns evolve over time , some classitiers are more relevant than others at a particular time . Therefore , an adaptive classifier selection method is essential . Unlike a monolithic approach of learning one classifier using incremental learning , our modular multiclassifier approach facilitates adaptation over time and removal of out of date knowledge .
168 Chan
Acknowledgements This work was partially funded by grants from DARPA ( F30602 96 1 0311 ) , NSF ( IRI96 32225 & CDA 96 25374 ) , and NYSSTF ( 423115445 ) . We thank Dave Fan and Andreas Prodromidis for their discussion and the reviewers for their comments .
References
Breiman , L . ; Friedman , J . H . ; Olshen , R . A . ; and Stone , C . J . 1984 . Classification
Trees . Belmont , CA : Wadsworth . and Regression
Cardie , C . , and Howe , N . 1997 . Improving minority class prediction using case specific Learning , 57 65 .
In Proc . 14th Intl . Conf . Mach . feature weights .
Catlett , Intl . Work . Machine Learning ,
J . 1991 . Megainduction : A test
596 599 . flight .
In Proc . Eighth
Chan , P . , and Stolfo , and parallel learning . Learning , 150 165 .
S . 1993 . Meta learning for multistrategy In Proc . Second Intl . Work . Multistrategy
Chan , P . , and Stolfo , and meta learning Machine Learning ,
90 98 .
S . 1995 . A comparative evaluation In Proc . Twelfth on partitioned data .
Clark , Machine Learning 3:261 285 .
P . , and Niblett ,
T . 1989 . The CN2 induction of voting Intl . Conf . algorithm .
Cohen , W . 1995 . Fast effective Conf . Machine Learning ,
115 123 . rule induction .
In Proc . 12th Intl .
Fawcett , T . , and Provost , Mining and Knowledge Discovery
1:291 316 .
F . 1997 . Adaptive fraud detection .
Data
Fawcett , distributionssummary of responses . Machine Learning List , Vol . 8 , No . 20 .
T . 1996 . Learning with skewed class
Kubat , M . , and Matwin , S . 1997 . Addressing anaced training Machine Learning , sets : One sided selection .
179 186 . the curse of imbalIntl . Conf .
In Proc . lJth
Pazzani , M . ; Merz , C . ; Murphy , P . ; All , K . ; Hume , T . ; and Brunk , C . 1994 . Reducing misclassification In Proc . 11th Intl . Conf . Machine Learning ,
217 225 . costs .
Provost , F . , and Aronis , massive parallelism .
Machine Learning 23:33 46 .
J . 1996 . Scaling up inductive learning with
Quinlan , Mated , CA : Morgan Kaufmann .
J . R . 1993 . C4.5 : programs for machine learning .
San
Tan , M . 1993 . Cost sensitive and its applications learning of classification knowledge in robotics . Machine Learning 13:7 .
Wolpert , D . 1992 . Stacked generalization , 259 .
Neural Networks 5:241
