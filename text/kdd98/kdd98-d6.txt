From : KDD 98 Proceedings . Copyright © 1998 , AAAI ( wwwaaaiorg ) All rights reserved . Aggregation of Imprecise and Uncertain Information for Knowledge Discovery in Databases
Sally McClean , Bryan Scotney and Mary Shapcott
School of Information and Software Engineering , University of Ulster
Coleraine , BT52 1SA , Northern Ireland
{si.mcclean , bw.scotney , cmshapcott}@ulstacuk
Abstract them to provide
We consider the problem of aggregation for uncertain and imprecise data . For such data , we define aggregation operators and use information on properties and patterns of data attributes . The aggregates that we define use the Kullback Leibler information divergence between the aggregated probability distribution and the individual tuple data values . We are thus able to provide a probability distribution for the domain values of an attribute or group of attributes using imperfect data .
Information stored in a database is often subject to uncertainty and imprecision . An extended relational data model has previously been proposed for such data which allows us to quantify our uncertainty and imprecision about attribute values by representing them as a probability distribution . Our aggregation operators are defined on such a data model . The provision of such operators is a central requirement in furnishing a database with the capability to perform the operations necessary for Knowledge Discovery in Databases .
Background
Frequently , real life data are uncertain , ie we are not certain about the truth of an attribute value , or imprecise , ie we are not certain about the specific value of an attribute . Such imprecision might occur naturally as a result of data being provided at different levels of the concept hierarchy or from the integration of distributed databases . It is therefore important that appropriate functionality is provided for generalised database systems to provide intelligent ways of handling such imperfect information . A recent survey of various approaches to handling imperfect information in Data and Knowledge Bases has been provided by Parsons ( 1996 ) .
A database model which is based on partial values and partial probabilities ( DeMichiel , 1989 ; Tseng et al . , 1993 ;
Copyright ª ( wwwaaaiorg ) All rights reserved .
1998 , American Association for Artificial Intelligence the partial values . We may
Chen and Tseng , 1996 ) has been proposed to handle both imprecise and uncertain data . Partial values may be thought of as a generalisation of null values : rather than not knowing anything about a particular attribute value , we may identify the attribute value as belonging to a set of possible values . Partial probabilistic values ( Chang and Chen , 1994 ) further generalise by assigning probabilities to the imprecision handling capacity of partial values with the uncertainty handling capabilities of probability theory . This capacity to handle both imprecise and uncertain data is a major strength of our current approach . The facility to aggregate such data is a central requirement in providing a database with the capability to perform the operations necessary for Knowledge Discovery in Databases , where we are frequently concerned with identifying interesting attributes or interesting relationships between attributes . thus combine
Data Models for Imprecise and Uncertain Data
In a generalised Relational Database we consider an attribute A and tuple t of a relation R which has values which are imprecise in that in any particular tuple an attribute value may be a partial value . Definition 1 . A partial value is determined by a set of possible attribute values of tuple t of attribute A of which one and only one is the true value . We denote a partial value h by h = [ a ] corresponding to a set of h a r s , possible values { a } of the same domain , in which exactly one of these values is the true value of h . Here , h is the cardinality of h }is a subset of the domain set 1 , k }of attribute A of relation R and h£ k . {a
, a
; { a a s r
, a s r the definition
We extend to a partial probability distribution , where probabilities are assigned to each partial value .
,p
Definition 2 . A partial probability distribution is a vector of probabilities j ( h ) = ( p e ) which are associated with a partition of partial values h = ( h e ) of tuple t of attribute A such that pi = probability ( the attribute value is a member of partial value set h
, h
=
1 . e
1
1 i ) . Here p i
=
1 i
Example 1 : For a partial probability distribution to be defined on the attribute SMOKING_STATUS , we have a set of partial values which forms a partition of the domain , such as ( {‘heavy’ , ‘light’} , {‘ex heavy’ , ‘ex light’} , {‘never} ) . We then define a partial probability distribution on this partition , eg<{‘heavy’ , ‘light’} , 0.4> , <{‘exheavy’ , ‘ex light’} , 0.35> , <{‘never’} , 025>
This distribution means , for example , that the probability of being either a heavy or a light smoker is 04 An extended relational model in which the tuples consist of partial values was described in DeMichiel ( 1989 ) and Chen and Tseng ( 1996 ) . We extend this data model to an extended relational database model based on a partial probability distribution . An equivalent data model , along with some extended relational operators such as project and join , has been proposed by Barbará et al . ( 1992 ) who introduced the term probability data model ( PDM ) .
ID
01
02
03
04
05
06
07
08
09
10
SEX
<{M},0.0> <{F},1.0>
<{M},1.0> <{F},0.0>
<{M},0.0> <{F},1.0>
<{M},1.0> <{F},0.0>
<{M},0.0> <{F},1.0>
<{M},1.0> <{F},0.0>
<{M},1.0> <{F},0.0>
<{M},1.0> <{F},0.0>
<{M},0.0> <{F},1.0>
<{M},1.0> <{F},0.0>
SMOKING STATUS <{S},1.0> <{E},0.0> <{N},0.0> <{S},0.0> <{E},1.0> <{N},0.0> <{S},0.0> <{E},0.2> <{N},0.8> <{S},0.9> <{E},0.1> <{N},0.0> <{S},0.0> <{E},1.0> <{N},0.0> <{S},1.0> <{E},0.0> <{N},0.0> <{S},0.0> <{E},0.0> <{N},1.0> <{S},1.0> <{E},0.0> <{N},0.0> <{S},0.75> <{E},0.25> <{N},0.0> <{S},1.0> <{E},0.0> <{N},0.0>
HYPER TENSION <{S,M},1.0> <{N},0.0>
<{S,M,N},1.0>
<{S,M},0.0> <{N},1.0>
HEART DISEASE <{S},0.75>
<{M,N},0.25>
<{S},0.75> <{M},0.2> <{N},0.05> <{S,M},0.0> <{N},1.0>
<{S},1.0>
<{M,N},0.0>
<{S},1.0>
<{M,N},0.0>
<{S,M},0.0> <{N},1.0>
<{S,M},1.0> <{N},0.0>
<{S,M},0.0> <{N},1.0>
<{S},1.0>
<{M,N},0.0>
<{S,M},1.0> <{N},0.0>
<{S,M},0.0> <{N},1.0>
<{S},0.2>
<{M,N},0.8>
<{S},1.0>
<{M,N},0.0>
<{S},0.1> <{M},0.2> <{N},0.7> <{S},0.9> <{M},0.05> <{N},0.05> <{S},0.0 > <{M},1.0> <{N},0.0> <{S},0.2>
<{M,N},0.8>
Table 1 . A Probability Data Table
Table Legend SEX : M male , F female ; SMOKING_STATUS:S smoker , E ex smoker , N never ; HYPERTENSION:S severe , M mild , N no hypertension ; HEART_DISEASE:S severe , M mild , N none .
Definition 3 . A probability data model ( or table ) R is a relation based on the probability distribution of partial values for domains D1 , D2,Dn of attributes A1 , A2,An where R ˝ P1 x P2 xPn and Pi is the set of all the probability distributions on the power set of domain Di . Each element of the table is a probability distribution specified on a partition of the appropriate domain into partial values .
An example of a probability data model is presented in Table 1 . This general relational data model allows us to represent a number of other models as special cases . For example , the attribute SEX is crisp with no null values , ie a value is M with probability 1 or F with probability 1 . The attribute SMOKING_STATUS consists entirely of crisp probabilistic values , ie in each case we know probabilities for each of the three possible values smoker , ex smoker attribute HYPERTENSION consists of true partial values : here attribute values are imprecise but not uncertain . The final attribute , HEART_DISEASE , is a true partial probability distribution ; the values are therefore both imprecise and uncertain . smoked . never
The or
Aggregation of Partial Probabilities
In this section we develop an approach which allows us to aggregate attributes of a partial probability distribution relation such as that presented in Table 1 . Thus this general aggregation operator ( gagg ) must include , as special cases , crisp and certain data ( eg SEX in Table 1 ) and crisp and uncertain data ( eg SMOKING_STATUS ) . Notation : As before , we consider an attribute Aj of a partial probability relation R with corresponding domain Dj ={v1,vk} which has tuples t1,tm The value of the rth tuple of Aj is a probability distribution described by vectors j ( ) ) , tr . Aj = ( fr ( j ) where P is a partition of the domain Dj and fr =Prob(tuple value is a member of Sr ) ; gr is the number of sets in the partition for tuple tr . We further define : j ( ) ) and P = ( Sr1
, , Srg
, , f rg j ( ) 1 j( )
( j ) r r
= qir
1 0
( j ) i
S if v otherwise . r
Definition 4 : The general aggregate of a number of probability distribution values on attribute Aj of relation R , denoted gagg ( R . Aj ) , is defined as a vector valued function :
( cid:229 ) ˛ ( cid:236 ) ( cid:237 ) ( cid:239 ) ( cid:238 ) ( cid:239 ) gagg ( R . Aj ) , = ( p 1,p k ) in which the p i ’s are computed from the iterative scheme :
= n ( ) i
( n
1 ) i
( r for i = 1,k g r m ( =1 = 1 j ( ) f r q ir
/ k u
=
1 u
( n
1 ) q ur
) ) / m j)(
Where the data are crisp , ie if we are trying to estimate p i j( )of tuple r , then the from a singleton partition set Sri contribution to p ( j ) . Otherwise we apportion j( ) in the the probability fri proportion p i /( ) . We illustrate this idea in the i is simply fri ( j ) to the p b ’s where vb˛ S bi
S bi following example . Example 2 : Consider the first two tuples of the attribute HEART_DISEASE in Table 1 . Here g1 = 2 ; S11 = {S} ; S12={M , N} ; f11=0.75 ; f12=0.25 ; q111=1 ; q212=1 ; q312=1 ; and g2 = 3 ; S21 = {S} ; S22 = {M} , S23 = {N} ; f21=0.75 ; f22=0.2 ; f23=0.05 ; q121=1 ; q222=1 ; q323=1 . The iteration scheme in Definition 4 is then given by :
+ 1 )
1
( n
1 )
( n
2
/ n (
0 75 . +
3
(
1 1 ) n
)
1 ) +
) /
2 n ( ) n ( )
1
2 0 2 .
/ n ( )
= = ( 2 = n
( n
1 )
1
(
2 1 )
( n
/ 0 75 ( . 1 ) 0 25 ( . / ( ) / 2 n 1 ) 0 25 ( . 2
) /
/ (
( n
1 )
+
( n
(
2
3
1 )
3 n
3 0 05 / . from which we obtain the result that : = n ( )
1 )
1 )
1
3 n n
(
(
( n
( n
1 )
2
3
/ ( / (
2 (
2 n
1 )
+ +
3 ( n
3
0 75 . = 0125 . = 0125 . n ( )
2
3
+
1 )
)
+ +
1 )
1 )
) )
01 . 0 025 . .
3
1 p
=
=
0 2 . ,
0 05 . .
0 75 . , p
Our definition of
By iteration , or simply substitution , we obtain the p result that
= 2 the general aggregate operator provides a general approach which can aggregate both imprecise and uncertain data in a consistent manner . The way the algorithm treats imprecise data is intuitively appealing since imprecise data values are apportioned to the appropriate aggregated probabilities according to the probabilities of their taking each of the possible values . We now show that the general aggregation algorithm also has a strong theoretical underpinning .
The Theoretical Framework The Kullback Leibler information divergence between two distributions p =(p1,…pn ) and q =(q1,…qn ) is defined as
D p
(
|| q
)
= ( cid:229 ) j p j log( p
/ q
) . j j
The iteration equations which define the aggregation in fact minimise information divergence between the aggregated probability distribution {p i} and the data {frs} . This is equivalent to maximising the Kullback Leibler the likelihood of the model given the data . Minimising the Kullback Leibler information divergence , or equivalently maximising the log likelihood , becomes : k t r
Maximise
W m= = r
1
=1 j ( ) log( f r p q ir
) i
=
1 i subject to i k p = = i
1
1 .
Vardi and Lee ( 1993 ) have shown that this problem belongs to a general class which they term Linear Inverse Problems ( LININPOS ) . For such problems they develop an algorithm which converges monotonically to the solution of the minimum information divergence equation . In our case this algorithm is equivalent to Definition 4 , and is an example of the EM ( expectation maximisation ) algorithm ( Dempster et al , 1977 ) , which is widely used for the solution of incomplete data problems . For example , such an approach is used by Scotney and McClean ( 1998 ) to integrate distributed data over attribute domain partitions . In general terms our problem may be regarded as one of graphical modelling , as described by Cox and Wermuth ( 1996 ) , in this case subject to statistical incompleteness . there
Vardi and Lee ( 1993 ) have shown that the iterative scheme always converges , but where is an identifiability problem the solution may not be unique . However , they show that a sufficient condition for uniqueness is that the rows of the matrix Q = { qi } are linearly independent . They also show that a necessary and sufficient condition for uniqueness is that the equation F = .Q does not have multiple solutions , where Q = { qi } and p i} . The vector F consists of elements which are the sums of the probabilities frs aggregated over the set Srs . The vector is then normalised so that the elements sum to 1 . In our case , since it is likely that we will have a large proportion of the data which is crisp , this condition will normally be satisfied .
= {p
In general , convergence of the EM algorithm may be slow ( Wu , 1983 ) . However , if most values are crisp , it is likely that the rate of convergence will not be a problem . Our simulation results support this claim . However , if speed of convergence becomes a problem , then there are a number of recent papers which suggest ways in which convergence can be accelerated , eg Jamshidian and Jennrich ( 1997 ) , Molenberghs and Goetghebeur ( 1997 ) .
In our simulations we have used the scaled cardinalities of all the crisp data as an initial value for the general aggregation operator . This is the answer we would get if we applied the simple aggregation operator and ignored all imprecise data . Such a starting value should therefore prove reasonably close to the final answer and therefore lead to fast convergence . p p p ( cid:229 ) ( cid:229 ) ( cid:229 ) p g g ˛ ( cid:229 ) p p p p p p p p p p p p p p p p p p p p p p p ( cid:229 ) ( cid:229 ) ( cid:229 ) ( cid:229 ) p We now illustrate the approach by applying the algorithm to the attribute HEART_DISEASE in Table 1 . Example 3 : In this case the matrix Q is given by
Q =
1 0 0
0 1 0
0 0 0 1 1 1
, corresponding to {S} , {M} , {N} , and {M , N} respectively , ie we have permuted the rows and columns to reflect a logical ordering of the subsets and have deleted repeated columns . The vector F ,defined earlier , is then given by :
F = ( 0.49 , 0.145 , 0.18 , 0.185 ) .
The iterative scheme for this data is thus :
= = =
( ) n
( ) n
1
2
3
0 . 49 + 0.185 * 0145 . + 0.185 * 018 . =
( ) n
1 ) from which we obtained the result that : 1
0 228 . ,
0 282 .
0 49 .
=
=
1 )
,
2
3
3
2
( n
1 )
2 ( n
/ ( / (
( n
2 ( n
+ 1 ) +
(
3 ( n n
1 )
1 )
) ) ,
3 in one iteration ( to 3 decimal places ) , using a start value of ( 0.601 , 0.178 , 0.221 ) .
Aggregation involving Several Attributes j n
( j )
,…… , v k 1 ( ) x…x va
In the previous section we illustrated the use of a general aggregation operator for imprecise and uncertain data in a single attribute . However , it is often of more interest to derive the joint probabilities of several attributes’ values occurring simultaneously , and our methodology may be extended to cover this situation . Hence we consider the cross product of attributes A1,…An with domains D1,…Dn . We define the cross product = A1x…x An with domain = D1x…x Dn . Let the values of domain Dj be given by j ( ) } . Then the values of are of the form {v1 n ( ) } where ai ˛ {1,…ki} for i =1,n { va 1 The partial probability distribution assigns probabilities {frs} to partitions {Srs} of for each tuple . The algorithm proceeds as in Definition 4 . In Example 4 we joint distribution of the attributes SMOKING_STATUS and HEART_DISEASE in Table 1 . Example 4 : The possible singleton values of the domain = SMOKING_STATUS x HEART_DISEASE are : {S , S} , {S , M} , {S , N} , {E , S} , {E , M} , {E , N},{N , S} , {N , M} , {N , N} with corresponding probabilities p 1 , p 9 . The subsets of which are represented in the data are:{S , S} , {S , M} , {S , N} , {E , S} , {E , M} , {E , N},{N , S} , {N , M} , {N , N},{S,{M , N}},{{E,{M , N}} , and for which the vector F is this approach by deriving illustrate then the
F={0.375 , 0.08 , 0.005 , 0.105 , 0.045 , 0.025 , 0.01 , 0.02 ,
0.15 , 0.105 , 008}
The iterative scheme for this data is given by :
= = = = = = = = =
( ) n
( ) n
( ) n
( ) n
4 ( ) n
( ) n
( ) n
( n
)
( ) n
1
2
3
5
6
7
8
9
0 . 375 + 0.105 * 0 08 . + 0 005 . 0.105 * 0105 . 0 045 . 0 025 . 0 01 . . 0 02 . 015
0.08 * 0.08 *
+ +
2
( n
1 )
( n
1 )
3
/ ( / (
2
( n
( n
2
1 )
+ + 1 )
1 )
) 1 )
3
( n
( n
3
)
( n
1 )
( n
1 )
5
6
/ ( / (
( n
1 )
( n
1 )
5
5
+ +
( n
1 )
( n
1 )
6
6
) )
= = = from which we obtain the result that :
1 5 7
=
. , 0179 2 =
, . 0 054 6 =
. 0 02 9 occurred in one iteration .
. , 0 375 , . 0 096
. , 0 01
. , 0 011
. , 015
3 =
=
,
8
=
4
. , 0105 where convergence
Knowledge Discovery
In order to discover knowledge from databases we are often concerned with inducing rules from database tuples ( Anand et al . , 1997 ) . Rules are often based on sets of attribute values partitioned into an antecedent and a consequent . A typical “ if then ” rule of the form “ if antecedent = true , then consequent = true ” is given by “ if an individual smokes and is hypertensive , then he or she is very likely to suffer from heart disease ” . Support for such a rule is based on the proportion of tuples in the database that have the specified attribute values in both the antecedent and the evaluation of such an association rule often requires only standard database functionality , and may be implemented using embedded SQL ( Anand et al . , 1997 ) . the consequent . Therefore
Using the approach described in previous sections we may use the general aggregation operator to derive joint probabilities and then compute conditional probabilities . Thus , for example , using the data in Table 1 we obtain : probability(smokes and severe heart disease ) = 0.375 probability ( smokes ) = 0565
Therefore , the conditional probability of having severe heart disease given that an individual smokes is given by : probability ( severe heart disease | smokes )
= 0375/0565 = 0664
However , the unconditional probability ( severe heart disease ) = 0.49 , which is smaller . We may therefore possibly induce the rule “ smoking increases the chances of heart disease ” , subject to thresholding parameters .
We may therefore use the general aggregation operator to assess rule validity . Thus , in general , if we wish to examine the rule :
( cid:230 ) Ł ( cid:231 ) ( cid:231 ) ( cid:246 ) ł ( cid:247 ) ( cid:247 ) p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p A1,Ar fi
B1,Bs , where A1Ar , B1,Bs are all attributes of a relation R , then we compute the joint probabilities :
Prob(A1Ar , B1,Bs ) and Prob(A1Ar ) , from which we may assess the validity of the rule by calculating the conditional probability :
Prob(B1,,Bs | A1Ar ) = Prob(A1Ar , B1,Bs ) /
Prob(A1Ar )
We may thus determine the strength and support for various rules under consideration . Thresholding then takes place to decide if the candidate rules are sufficiently likely to merit informed . A probabilistic approach , such as that described , may then be used to represent rules as linguistic summaries ( McClean and Scotney , 1997 ) in order to facilitate a user interface to the KDD process . the user being
Summary and Further Work
We have considered an extended relational data model which handles both imprecise and uncertain data . We have also developed general aggregation operator which allows us to determine a probability distribution for attribute values either for a single attribute or for the cross product of a number of attributes . Such functionality has potential for knowledge discovery in databases where we may use it to assess the validity of association rules . Further work will focus on including domain knowledge , such as that held as integrity constraints , into the model , and on techniques for accelerating the algorithm .
Acknowledgements This work was partially funded by IDARESA ( ESPRIT project no . 20478 ) and partially funded by ADDSIA ( ESPRIT project no . 22950 ) which are both part of EUROSTAT's DOSIS ( Development of Statistical Information Systems ) initiative .
References
Anand SS , Scotney BW , Tan MG , McClean S.I , Bell DA , Hughes JG , and IC Magill ( 1997 ) . Designing a kernel for data mining . IEEE Expert , 12 ( 2 ) , 65 74 .
Barbará D . , Garcia Molina H . and D . Porter ( 1992 ) . The management of probabilistic data . IEEE Transactions on Knowledge and Data Engineering , 4 , 487 501 .
Chang C S and ALP Chen ( 1994 ) . Determining probabilities for probabilistic partial values . Proc . Int . Conf . Data and Knowledge Systems for Manufacturing & Engineering , Hong Kong , 277 284 .
Chen ALP and FSC Tseng ( 1996 ) . Evaluating aggregate operations over IEEE Transactions on Knowledge and Data Engineering , 8 , 273 284 . imprecise data .
Cox DR and N . Wermuth ( 1996 ) . Multivariate Dependencies Models , analysis and interpretation . Chapman and Hall .
LG
( 1989 ) .
DeMichiel database incompatibility : an approach to performing relational operations over mismatched domains . IEEE Transactions on Knowledge and Data Engineering , 4 , 485 493 .
Resolving
Dempster AP , Laird NM and DB Rubin ( 1977 ) . Maximum likelihood from incomplete data via the EM algorithm ( with discussion ) . J . R . Statist . Soc . B , 39 , 1 38 .
Jamshidian JM and RI Jennrich ( 1997 ) . Acceleration of the EM algorithm by using quasi Newton methods . J . R . Statist . Soc . B , 59 , 569 587 .
McClean SI , and BW Scotney ( 1997 ) . Using evidence theory for integration of distributed databases . International Journal of Intelligent Systems , 12 ( 10 ) , 763776 . the
Molenberghs G . and E . Goetghebeur ( 1997 ) . Simple fitting algorithms for incomplete categorical data . J . R . Statist . Soc . B , 59 , 401 414 .
Parsons S . ( 1996 ) . Current approaches to handling imperfect information in data and knowledge bases . IEEE Transactions on Knowledge and Data Engineering , 8 , 353 372 .
Scotney BW , McClean SI , and MC Rodgers ( 1998 ) . Optimal and efficient integration of heterogeneous data in a distributed database . Data and Knowledge Engineering Journal , forthcoming .
Tseng FSC , Chen ALP and W P Yang ( 1993 ) . Answering heterogeneous database queries with degrees of uncertainty . Distributed and Parallel Databases , 1 , 281302 .
Vardi Y . and D . Lee ( 1993 ) . From image deblurring to optimal investments : maximum likelihood solutions for positive linear inverse problems ( with discussion ) . J . R . Statist . Soc . B , 569 612 .
Wu CFJ ( 1983 ) . On the convergence properties of the EM algorithm . The Annals of Statistics , 11 ( 1 ) , 95 103 .
