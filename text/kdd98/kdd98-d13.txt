CLOUDS : A Decision Tree Classifier for Large Datasets
Khaled Alsabti Department of EECS Syracuse University
Sanjay Ranka Department of CISE University of Florida
Vineet
Singh
Information Technology Lab
Hitachi America , Ltd .
Abstract
Classification for very large datasets has many practical applications in data mining . Techniques such as discretization and dataset sampling can be used to scale up decision tree classifiers to large datasets . Unfortunately , both of these techniques can cause a significant loss in accuracy . We present a novel decision tree classifier called CLOUDS , which samples the splitting points for numeric attributes followed by an estimation step to narrow the search space of the best split . CLOUDS reduces computation and I/O complexity substantially compared to state of the art classifters , while maintaining the quality of the generated trees in terms of accuracy and tree size . We provide experimental results with a number of real and synthetic data~ets .
Introduction is used to measure the accuracy of the classifier .
Classification is the process of generating a description or a model for each class of a given dataset . The dataset consists of a number of records , each consisting of sew eral fields called attributes that can either be categorical or numeric . Each record belongs to one of the given ( categorical ) classes . The set of records available for developing classification methods are generally decomposed into two disjoint subsets , training set and test set . The former is used for deriving the classifier , while the latter Decision tree shown to achieve good accuracy , compactness and efficiency for very large datasets ( Agrawal , Mehta , & Rissanen 1996 ; Agrawal , Mehta , & Sharer 1996 ) ; the latter has substantially superior computational characteristics for large datasets . Deriving a typical decision tree classifier from the training set consists of two phases : construction and pruning . Since the construction phase is the computationally more intensive portion ( Agrawal , Mehta , & Sharer 1996 ) , the main focus of this paper is the construction phase .
SLIQ and SPRINT have been classifiers
CART , SLIQ , and SPRINT use the gini to derive the splitting criterion at every internal node of index
Copyright Q1998 , American Association for Artificial Intelligence ( wwwaaaiorg ) All rights reserved .
2 Alsabti larger than the available memory , the tree . The use of the gini index is computationally challenging . Efficient methods such as SPPANT require sorting along each of the numeric attributes , which requires the use of memory resident hash tables to split the sorted attribute lists at every level . For datasets that are significantly this will require multiple passes over the entire dataset . In this paper we present a simple scheme that eliminates the need to sort along each of the numeric attributes by exploiting the following properties of the gini index for a number of real datasets : ( 1 ) The value of the gini index along a given attribute generally increases or decreases slowly . The number of good local minima is small compared to the size of the entire dataset . This is especially true for attributes along which tile best splitting point is obtained . ( 2 ) The minimum value the 9ini index for the splitting point along the splitting attribute is ( relatively ) significantly lower than most the other points along the splitting as well as other attributes . the split
We show that in most cases , the above properties can be used to develop an I/O and computationally efficient method for estimating the split at every internal node . Experimental results on real and synthetic datasets using our method show that , ting point derived has the same 9ini index as the one obtained by sorting along each of the attributes . Using this new method for splitting at every internal node , we have developed a new decision tree classifier called CLOUDS ( Classification DataSets ) . CLOUDS is shown to have substantially lower computation and I/O requirements as compared to SPRINT for a number of real and synthetic datasets . The accuracy and compactness of the decision trees generated the same or comparable to CART , C4 , and SPRINT . for Large or OUt of core is
Estimating the Splitting
Point for
Numeric
Attributes
In this section we present two new methods for estimating the splitting point for the numeric attributes . These methods are used by CLOUDS at each node of the tree . We compare the new methods with a direct method and a simple sampling method . The Direct Method ( DM ) sorts the values of each numeric attribute , and evaluates the gini index at each possible split point to find the splitting point with the minimum gini index . The sorting operation requires O(n lg n ) time , where n is the number of values . It requires several passes of I/O for disk resident data . The Dataset Sampling ( DS ) method draws a random subset of the whole dataset . The DM method is applied to this sample to build the classifier . In order to study the quality of this method , the gini index of the splitter ( the point used for partitioning the data ) is obtained using the whole dataset .
In this paper we present two new methods based on sampling the compu
Sampling the Splitting Points , ie , tation . The first method , Sampling the Splitting points ( SS ) , derives the splitter from a limited number of the points . The second method , SSE ( Sampling splitting the Splitting points with Estimation ) , improves upon SS by estimating the gini values to narrow the search space of the best split , and it is designed to derive the best or near best splitter with respect to the gini index . Both the methods evaluate the gini index at only a subset of splitting points along each numeric attribute .
The gini D at point vl of a numeric attribute dataset S is given by : a for giniD(S,a < vt)= n Ll ( ln ~ ’(~)2)+ c i=1 n nt ( 1 v c~ x~
( 1 )
We use a hill climbing algorithm to estimate the lower bound of the gini index in the given interval . This algorithm makes a series of decisions to estimate the lower bound , with each decision based on the values of gradient along a subset of the c classes . Gradients along a subset of these c classes are calculated and the direction is chosen . The value of the gradient along xi is given by : along the minimum gradient
OginiD(s , a < vt )
Oxi 1 1
2
~l(~ ~l ) n
=i ) a
1 c
( 2 )
( SS ) points the Splitting
In the SS Sampling is dimethod , the range of each numeric attribute vided into q intervals using a quantiling based technique . Each interval contains approximately the same number of points . For each numeric attribute , the gini index is evaluated at the interval boundaries . This is used to determine the minimum gini value ( ginimi,~ ) among all interval boundaries of the numeric attributes and across all the categorical attributes . The split point with ginimi , will be used as a splitter . The SS method requires one pass over the dataset to derive the splitting point . and finds points with Estimation
Sampling the Splitting the range of each ( SSE ) The SSE method divides numeric attribute the minimum gini value ( 9inimi, ) , as in the SS method . Further , it estimates a lower bound gini est of the gini value for each interval . All the intervals with gini est > ginimin are eliminated ( prune ) to derive a list of potential candidate intervals ( alive intervals ) . For an alive interval , we evaluate the gini index at every distinct point in the interval to determine the splitter . This may require another pass over the entire dataset . The SSE method estimates the minimum value of the gini index in a given interval [ vt , vu ] using the statistics of the interval endpoints . In the following we describe our estimation heuristic . Let n and c be the number of records and the number of classes , respectively . We use the following notation : xi ( Yi ) the number of elements of class i that are less than or equal to vt ( vu ) c~ the total number of elements of class i nt in, , )
the number of elements that are less than or equal to vt ( vu )
We calculate giniTM for interval [ vl , vu ] from left to right as well as right gradient at vt . Then , to left . Let class j give the minimum
2 nt
2 nl(n nt ) ( cJ n xj ) < nt(n nt ) nl
Xi )
Yl<i<c From formula ( 3 ) , we obtain
.__ c3ntn xj < Ci n xi Y l < i < c nl
( 3 )
( 4 )
Thus , nt + 1 n e3
( x j+l ) nt + 1 ~ci n xi Vl<i<c
( 5 )
Thus , the class with the minimum gradient will remain the class with the minimum gradient for the next split point . This property can be exploited to reduce the complexity of our hill climbing based estimation algorithm by assuming that all points from the same class appear consecutively . Thus , the time requirements are proportional than the number of split points in the interval . to the number of classes rather
Our hill climbing algorithm for determining gini
TM from the left boundary , and finds the class with starts the minimum gradient . Then , it evaluates the gini index at a new splitting point , which is at the old splitting point Cleft boundary ) plus the number of elements in the interval with class with the minimum gradient . In case the new gini value is less than the current giniest , it becomes the new gini ~t and the process is repeated for the remaining classes ( and smaller interval ) . The same process is applied from right to left .
The SSE method can be modified to further prune as we update
( from the alive intervals ) the intervals
KDD 98 3 the global minimum gini based on computation of alive intervals processed at a given point . Further , it can prioritize the evaluation of the alive intervals such that we start with the intervals with the least estimated gini value . These optimizations have been incorporated in the SSE method .
The performance of the SSE method depends on the quality of the estimated lower bound of the gini index , and on the fraction of elements in the alive intervals ; this fraction is given by the survival ratio . Since the gini index is calculated for each of the splitting points in these intervals , the computational cost of this phase is directly proportional to the survival ratio .
The SS method is simpler to implement and it re less computational and I/O time than the SSE quires method for determining the splitter . On the other hand , the SSE method should generate more accurate splitters with respect to the gini index . The trade off between quality and computational and I/O requirements are described in the next section . evaluthe three methods for a number of real and
Experimental Results We experimentally ated synthetic datasets . Table 1 gives the characteristics four datasets are taken of these datasets . The first ( a widely used benchmark in from STATLOG projects 1 The "Abalone" , "Waveform" and "Isoclassification ) . let" datasets can be found in ( Murphy & Aha 1994 ) . The "Synthl" and "Synth2" datasets have been used in ( Agrawal , Mehta , & Rissanen 1996 ; Agrawal , Mehta , & Shafer 1996 ) for evaluating SLIQ and SPRINT ; they have been referred as "Function2" dataset .
The main parameter of our algorithm is the number of intervals used along each of the numeric attributes . of our algoWe studied the following characteristics rithms for different number of intervals : ( 1 ) We compare the value of gini index of the splitters generated by the SS and SSE methods to the gini index of the splitting point using DS and DM . ( 2 ) For SSE , we obtained the percentage of intervals for which the estimated value was higher than the actual value . These are measured by the number of misses by our method . For the missed intervals , we also measured the maximum and the average difference in the estimated value and the actual minimum for that interval . Further , we obtained the fractional vived for the next pass ( survival ratio ) . number of elements that sur
Table 2 shows the results of the estimated gini index using the DS method . We used datasets for three randora samples chosen from the dataset and have reported the minimum and the maximum gini index obtained . show that the DS method , in the worst These results case , may obtain splitters with relatively larger values of gini index as compared to a direct calculation . This may be partly due to the small size of the datasets . One may expect better accuracy for large datasets . Our re
1For more details about the STATLOG datasets , reader is referred to http://wwwkdnuggetscom/ the
4 Alsabti suits described in the next section show that this can have a negative impact on the overall classification error .
Table 3 shows the estimated gini index using the SS show that method and the DM method . These results index most of the SS method missed the exact gini the times for the test datasets ; though the difference between the exact and estimated gini was not substantial . The "Letter" dataset has 16 distinct values along each of the numeric attributes . The SS method , with number of intervals greater than 16 , should work well for such a dataset . The same holds true for the "Satimage" and "Shuttle" datasets as they have a small number of distinct values . show that
The results in Table 4 show the estimated gini index the SSE method and the DM method . These using results the SSE method missed the exact gini index only for a few cases . Further , the estimated gini generated by the SSE method is more accurate than those of the SS method ( for the missed ones ) . Tables 3 and 4 show that the accuracy of the SS method is more sensitive to the number of interyals which makes the SSE method more scalable . This experiment shows that improvement , more scalable , and robust than the SS method . the SSE method is a definite
Our experimental results
( not reported here ) also show that the number of misses for the SSE method are very small . Further , for the missed intervals , the difference between the actual minimum and estimated minimum was extremely small ( Alsabti , Ranks , & Singh 1998 ) .
The results in Table 5 show that the survival ratio than 10 % for all the test of the SSE method is less and less than a datasets few percent for more than 100 intervals . Further , the survival ratio generally decreased with increase in the number of intervals . for more than 10 intervals ,
The above results clearly demonstrate the effective ness of the SSE method ; it can achieve a very good estimate of the splitter with minimum gini index and require it has a that utilize only better performance than algorithms a small subset of the overall sample for obtaining the splitters . limited amount of memory . Further ,
For each point in an alive interval the class information , as well as the value of the numeric attribute corresponding to the alive interval , needs to be stored and processed . Assuming that all categorical and numeric attributes as well as class field are of the same size ( in the amount of data to be processed for all the bytes ) , alive intervals is proportional to 0(2 x SurvivalRatio x x Numbero f Records ) . NumberO f NumericAttributes This to ( 2 x SurvivalRatio x NurnberO]AllAttributes+l j fraction of the original dataset . We use quantiling based the intervals . Hence , the domain sizes of the numeric attributes do not have any impact on the performance corresponds NurnberO~NumericAttributes technique to generate
"~
Dataset
Letter
Satimage Segment Shuttle Abalone
Waveform 21 Waveform 40
Isolet Synthl Synth2
No of examples 20,000 6,435 2,310 58,000 4,177 5,000 5,000 7,797 400,000 800,000
No . of attributes
No . of numeric ] No . of classes attributes[
16 36 19 9 8 21 4O 617 9 9
16 36 19 9 7 21 4O 617 6 6
26 6 7 7 29 3 3 26 2 2
Table 1 : Description of the datasets
Dataset
Letter
Satimage Segment Shuttle Abalone
Waveform 21 Waveform 40 Isolet
Exact gini
0.940323 0.653167 0.714286 0.175777 0.862017 0.546150 0.541134 0.926344
Min
0.940323 0.653167 0.714286 0.175777 0.862579 0.547439 0.548980 0.926552
10 % [
Max 0.942294 0.659033 0.717774 0.175777 0.866956 0.552724 0.554132 0.932994
15 % [
20 % [
Min
Min
Max 0.942326 0.659033 0.714788
0.940323 0.653167 0.714286 0.175777 0.175777 0.867807 0.862579 0.547083 0.548120 0.548144 0.542480 0.554132 0.543133 0.926344 0.926883 0.926344
Max 0.942326 0.940323 0.659033 0.653167 0.721099 0.714286 0.175777 0.175777 0.862114 0.866992 0.550018 0.550520 0.927038
Table 2 : Exact and estimated gini using DS for different sampling ratios
( 3 runs )
Dataset
Letter
Satimage Segment Shuttle Abalone Waveform 21 Waveform 40
Isolet
Exact gini [ r
0.940323 0.653167 0.714286 0.175777 0.862017 0.546150 0.541134 0.926344
0.940323 0.653167 0.714286 0.175777 0.862477 0.547003 0.541680 0.927403
Number of Intervals
[ 100 I l 25 T 10 [ 5 0.940323 0.940323 0.940323 0.654601 0.654601 0.653167 0.714286 0.714286 0.717774 0.175777 0.175777 0.175777 0.862697 0.862752 0.862907 0.547464 0.547424 0.547353 0.541943 0.542257 0.542894 0.927290 0.927424 0.927403
0.941751 0.654601 0.721154 0.181001 0.238166 0.862724 0.865768 0.550608 0.548090 0.543224 0.543224 0.929721 0.930125
0.941751 0.682317 0.740018
Table 3 : Exact minimum and estimated gini based on SS using different number of intervals
Dataset
Exact gini
200
Letter
Satimage Segment Shuttle Abalone
Wave form 21 Waveform 40
Isolet
0.940323 0.653167 0.714286 0.175777 0.862017 0.546150 0.541134 0.926344
I
100
0.940323 0.940323 0.653167 0.653167 0.714286 0.714286 0.175777 0.175777 0.862017 0.862017 O.546150 0.546150 0.541134 0.541134 0.926807 0.926807
Number of Intervals I } 25 50 0.940323 0.940323 0.653167 0.653167 0.714286 0.714286 0.175777 0.175777 0.862017 0.862017 0.546150 0.546150 0.541134 0.541134 0.927138 0.926344
10
[ 0.941751 0.653167 0.715799 0.175777 0.862017 0.546150 0.541134 0.926344
[
5
0.940323 0.653167 0.721099 0.175777 0.862017 0.546150 0.541134 0.926344
Table 4 : Exact minimum and estimated gini based on SSE using different number of intervals
KDD 98 5
Dataset W~ of
Intervals~
Number II 2o0 I l°°
0.029 0.022
I 5o I 25 I l° 1 5 I 0.174 0.000 0.000 0.000 0.002 0.092 0.000 0.000 0.001 0.006 0.000 0.000 0.003 0.000 0.011 0.011 0.000 0.000 0.000 0.003 0.001 0.032 0.361 0.436 0.005 0.347 0.143 0.007 0.175 0.001 0.062 0.000 0.000 o.00o 0.OOl 0.002 0.019
0.027 0.085 0.022 0.036 0.005 0.014
0.009 0.012 0.002
Letter
Satimage Segment Shuttle Abalone
Waveform 21 Waveform 40
Isolet
Table 5 : Tile fractional survival ratio using different number of intervals using the SSE method of our algorithm . The memory requirements is proportional to q × ( c + f ) , where q is the number of intervals , c is the number of classes , and f is tile number of nuThe conducted experiments showed meric attributes . that 50 200 intervals are sufficient to achieve an acceptable performance . The amount of storage required for keeping the relevant information for these intervals is the whole data set in the much smaller than storing main memory . We expect this to be the case for larger datasets ; though further investigation is required .
The CLOUDS Algorithm
In this section we describe our classifier , which uses a breadth first strategy to build the decision tree and uses the gini index for evaluating the split points . It the SS method or SSE method to derive uses either the splitter at each node of the tree . CLOUDS evaluates the split points for categorical attributes as in SPRINT . However , the evaluation of split points for numeric attributes is different also describe the partitioning internal node .
( as described below ) . step for decomposing an l of the Numeric Attribute For each node at level tree , the range of each numeric attribute is partitioned into q intervals . A random sample S of size s is derived from the dataset . This step requires one read operation of the whole dataset , which is needed only at the root level . The samples for the lower levels are obtained using the sample for the higher levels ; q 1 points are derived from S by regular sampling . This corresponds to q regularly spaced quantiles of the set S . If q is much smaller than S , we expect that the q intervals have close to ~ records with high probability , where n is the total 2 number of records . q
The dataset is read and the class frequencies are computed for each of the q intervals . At this stage , the the SS or SSE methods . splitter For the alive intervals along each numeric attribute ( using the SSE method ) , the class frequencies for intervals are computed . We assume that the size of each is found using either
2Our algorithm does not require intervals to be of equal size though .
6 Alsabti
" with a high probability . of the alive intervals is small enough that it can fit in the main memory.3 Since we are choosing the interthe vals based on regular sampling of a random subset , number of records from the whole dataset in each inThus , terval are close to the memory with high each of these intervals will fit probability when q is O(~ ) , where M is the size of the main memory . For each alive interval , we sort all the points . Since we know the class frequencies at interval boundaries and the points of a alive intervals are sorted , the class frequencies at each point in the alive interval can be computed easily . For each distinct point , we evaluate the 9ini index and keep track of the minimum value , which will be used to determine the best splitting criterion . reading requires
The above operation the entire dataset corresponding to the internal node and storing only the points that belong to alive intervals . If all the alive intervals do not fit in the main memory , they are written to the disk . Another pass is required to read these alive intervals one at a time . If the survival ratio is small , and all the alive intervals can fit memory , this extra pass is not required . in the main criterion the Dataset The splitting
Partitioning is applied on each record to determine its partition ( left or right ) . The number of records read and written is equal to the number of records represented by the internal node . For node i at level 1 of the tree , set S is partitioned , using the splitting criteria , into two sample sets S1 and $2 . The sample sets $1 and $2 are used for left and right subtrees of node i , respectively . While reading the data for splitting a given node , the class frequencies for the interval boundaries are updated . Thus , frequency vectors corresponding to $1 and $2 are updated , which avoids a separate pass over the entire data to evaluate these frequencies .
Assuming that all the alive intervals fit in the main memory , we need to read all the records at most twice and write all the records for each internal node ( except the root node ) . The records read and written correspond only to the internal node(s ) being split . An extra
3Otherwise , the processing of the alive intervals needs to be staged . read is required on the root to create the sample list .
We expect that CLOUDS ( with the SSE method ) and the SPRINT algorithm will build the same tree for most datasets , which is demonstrated by the results obtained in a later subsection . Our theoretical analysis ( not reported here ) shows that CLOUDS is superior SPRINT in terms of I/O and computational ments ( Alsabti , tLunlm , & Singh 1998 ) . require
Experimental Results We conducted several experiments to measure the accuracy , compactness , and efficiency of the CLOUDS classifier and compared it with SPRINT , IND C4 , IND CART , and the DM method . In this section we briefly describe some of these results . For more details the reader is referred to a detailed version of this paper ( Alsabti , Ranka , & Singh 1998 ) .
Table 6 gives the average accuracy and the tree size generated by the DS method . These of the classifiers results and observations by other researchers ( Catlett 1991 ; Chan & Stolfo 1993 ) show that one can potentiaUy lose a significant amount of accuracy if the whole dataset is not used for building the decision tree . However , we will like to note that it may be a manifestation of the size of the dataset . It is not clear whether this will be the case for larger datasets .
We wanted to study the quality of results obtained by CLOUDS based on the size of the main memory . implementation recursively decomposes the Our current number of intervals into two subsets using a weightedassigned assignment strategy . The number of intervals to each of the datasets to the size of the dataset . Since the size of the datasets available is reasonably small , we achieve the effects of having limited memory by using a stopping criterion based on the number of intervals assigned to a given data subset . The DM method is used when the stopping criterion is met . is proportional to evaluate results
We conducted a set of experiments the in size , accuracy , and execu
SS and SSE methods based on the overall CLOUDS for pruned tree tion time ( see ( Alsabti , Ranka , & Singh 1998 ) for more details ) . The pruned tree sizes generated by the SS method ( not shown here ) are comparable to those the SSE method . However , the SSE method achieved slightly better accuracy for most of the cases ( comparison not shown here ) . The SSE method requires at most 30 % more time than the SS algorithm ( see Table 8 ) . These results show that the overhead of using the SSE SSE over SS is not very large . Given that method is more scalable and less sensitive to the number of intervals , it may be the better option for most datasets . The results presented in the rest of this section assume that SSE method was used .
Table 7 gives the quality of decision trees in terms of accuracy and size ( after FULL MDL based pruning ( Agrawal , Mehta , & Rissanen 1996 ; 1995 ) ) using CLOUDS for different number of intervals at the root . the DM method was used when These assume that for results the number of intervals was less than 10 . The corresponding for CART , C4 , SPRINT and the DM method are also presented . 4 The results the CART , C4 , and SPRINT algorithms have been derived ( Agrawal , Mehta , & Pdssanen 1996 ; from the literature Agrawal , Mehta , & Shafer 1996 ) . These results show that the quality of results obtained is similar or comparable to those of CART , C4 , SPRINT and DM method both in terms of accuracy and the size of the decision tree obtained . Assuming that we started with 200 intervals and divided the datasets into approximately equal datasets at each node , a stopping criterion based on 10 intervals will correspond to application of the heuristic method for around 4 levels .
Table 8 gives the time requirements of our algorithm assuming that the DM method is applied when the size of the data is small enough that less than 10 intervals are assigned to it . These timings have been obtained from the IBM RS/6000 Model 590 workstation running AIX 414 These results assume that the survival ratios are small enough that the alive intervals can be processed ence between using SS versus SSE is Thus , the overhead of processing the alive intervals SSE is not very high . in the main memory . The maximum differ than 30 % . in less
SPRINT spent around 1,000 and 2,000 Synthl and Synth2 datasets , seconds respec(Agrawal , Mehta , & Shafer 1996 ) . These timcomparable due to somewhat in classifying tively ings may not be directly different experimental setups .
Dataset
Letter
Satimage Segment Shuttle Abalone
Waveform 21 Waveform 40
Isolet Synthl Synth2
SSE q=200 8.76 5.40 1.06 3.98 2.89 4.86 9.07 377.63 35.74 73.35
] q=100 8.30 4.57 0.9 3.44 2.52 4.48 8.69 331.14 34.86 72.06
SS q = 200 I q = lOO 8.43 5.43 0.97 3.97 2.40 4.34 8.53 289.13 35.43 74.09
8.26 4.60 0.87 3.41 2.25 4.27 8.26 270.72 34.76 71.79
Table 8 : Time requirements using SS and SSE . DM was applied sizes roughly ~ of original dataset
( in seconds ) of CLOUDS for nodes with
Conclusion
There are several options for developing efficient algorithms for performing such data mining operations on large datasets . One option is to develop algorithms that reduce computational and I/0 requirements , but perform essentially the same operations . Another option is 4The DM and SPRINT should generate the same results .
However , they generated different trees because ( probably ) of different implementation choices .
KDD 98 7
Dataset
Letter
Satimage Shuttle Abalone Isolet
,~.ccuracy
53.6 75.9 99.5 24.1 55.6
I
II °o %
I Tree size Accuracy [ Tree size Accuracy [ Tree size 592.3 69.7" 25.7 85.7 169.0
335.0 45.7 21.0 39.0 107,0
115.0 17.0 9.0 15.0 57.7
69.1 81.7 99.9 24.0 73.1
76.7 83.6 99.9 24.2 78.0
100 %
Accuracy I Treesize 881 127 27 137 261
83.4 86.4 99.9 26.3 82.9
Table 6 : Average ( of three trials ) pruned tree size and accuracy using the DS method lIND CART
847/11995
85.3/90 94.9/ 52 99.9/27
IND C4
SPRINT
868/32413846/1141 85.2/563 86.3/ 159 95.9/102 946/186 99.9/29 99.9/57
Dataset
Letter
Satim’age Segment Shuttle Abalone
Waveform 21 D/ave form 40
Isolet
DM [ [ OLOUDSwithSSE
83.4/881 86.4/127 944/410 99.9/27 26.3/137 773/1684 764/1874 82.9/261 q=200 83.3/893 85.9/135 947/552 99.9/41 26.4/147 768/1728 760/1898 82.6/ 255
1 q= 100 83.4/881 84.9/111 950/512 99.9/33 25.9/135 777/1728 772/1846 82.7/255
Table 7 : The accuracy/pruned tree for nodes with dataset sizes equal to ( roughly ) ~ of the original dataset size obtained for different datasets .
The DM method was applied in CLOUDS falls the computational to develop algorithms which reduce an approximation and I/O requirements by performing or no loss of the actual operation without significant in the latter of accuracy . The CLOUDS classifier category , while SPRINT falls in the former category . However , CLOUDS has been shown to be as accurate as SPRINT and shown to have substantially superior computational We believe approach may provide attractive alternatives methods for other data mining applications for cases in which an implicit optimization performed . to direct especially needs to be characteristics . that such an that the estimate to derive heuristics
It will be interesting that will guarantee is always lower than the actual value . However , this value should be close enough to the actual minimum possible to be small . The new algorithms extended to other splitting function and the gain ratio . ratio be the towing the survival can potentially functions , eg , for
Acknowledgments jay Ranka was supported
The work of Khaled Alsabti was done while he was visthe department of CISE at University of Florida . iting The work of San in part by AFMC and ARPA under and WM 82738 K 19 ( subcontract from Syracuse Univerand in part by ARO under DAAG 55 97 1 0368 sity ) and Q000302 ( subcontract from NMSU ) . The content of the information does not necessarily sition or the policy of the Government and no official endorsement should be inferred .
F19628 94 C 0057 reflect the po
8 Alsabti
References
J . 1995 . MDL
Int’l Con ] . on Knowlin Databases and Data Mining 216
J . 1996 . SLIQ : for Data Mining . Proc . of the Int’l Conference on Extending Database Technol
Agrawal , R . ; Mehta , M . ; and Rissanen , Based Decision Tree Pruning . edge Discovery 221 . Agrawal , R . ; Mehta , M . ; and Rissanen , A Fast Scalable Classifier Fifth ogy 18 32 . J . C . 1996 . Agrawal , R . ; Mehta , M . ; and Sharer , SPt~INT : A Scalable Parallel Classifier for Data Mining . Proc . of the 22th Int’l Conference on Very Large Databases 544 555 . Alsabti , CLOUDS : A Decision Datasets . http : //www . else , ufl . edu/, , , ranka/ . Catlett , J . 1991 . Megainduction : Machine Learnin9 on Very Large Databases . PhD thesis , University of Sydney . Chan , P . K . , and Stolfo , for Multistrategy Int’l Workshop on MuItistrategy Murphy , UCI Repository http://wwwicsuci , Irvine , CA : University In]ormation
1994 . of Machine Learning Databases . edu/ mlearn/MLRepository.html , and Singh , V . 1998 . for Large and Parallel Learning . Proc . Second of Cali]ornia , Department of
S . J . 1993 . Meta Learning and Computer Science . and Aha , D . W .
Learning 15[~165 .
Tree Classifier
K . ; Ranka ,
P . M . ,
S . ;
