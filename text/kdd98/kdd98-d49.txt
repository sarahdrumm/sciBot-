Reinforcement Learning for Trading Systems and Portfolios
John Moody and Matthew Saffell*
Oregon Graduate Institute ,
CSE Dept .
PO Box 91000 , Portland , OR 97291 1000
{moody , saffell}@cseogiedu
Abstract
We propose to train trading systems by optimizing financial objective functions via reinforcement learning . The performance functions that we consider as value functions are profit or wealth , the Sharpe ratio and our recently proposed differential Sharpe ratio for online learning . In Moody & Wu ( 1997 ) , we presented empirical results in controlled experiments that demonstrated the advantages of reinforcement learning relative to supervised learning . Here we extend our previous work to compare Q Learning to a reinforcement learning technique based on real time recurrent learning ( RTRL ) that maximizes immediate reward . Our simulation results include a spectacular demonstration of the presence of predictability in the monthly Standard and Poors 500 stock index for the 25 year period 1970 through 1994 . Our reinforcement trader achieves a simulated out of sample profit of over 4000 % for this period , compared to the return for a buy and hold strategy of about 1300 % ( with dividends reinvested ) . This superior result is achieved with substantially lower risk .
Introduction :
Reinforcement
Learning for Trading and we compare two different
The investor ’s or trader ’s ultimate goal is to optimize some relevant measure of trading system performance , such as profit , economic utility or risk adjusted return . In this paper , we propose to use reinforcement learning to directly optimize such trading system performance functions , reinforcement learning methods . The first uses immediate rewards to train the trading systems , while the second ( Q Learning ( Watkins ) ) approximates discounted future rewards . These methodologies can be applied to optimizing systems designed to trade a single security or to trade a portfolio of securities . In addition , we propose a novel value function for risk adjusted return suitable for online learning : the differential Sharpe ratio .
* The authors are also with Nonlinear Prediction Systems , Tel : ( 503)531 2024 . Copyright ( c ) 1998 , American Association for Artificial Intelligence ( wwwaaaiorg ) All rights reserved .
Trading system profits depend upon sequences of interdependent decisions , and are thus path dependent . Optimal trading decisions when the effects of transactions costs , market impact and taxes are included require knowledge of the current system state . Reinforcement learning provides a more elegant means for training trading systems when state dependent transaction costs are included , than do more standard supervised approaches ( Moody , Wu , Liao & Saffell ) . The reinforcement learning algorithms used here include maximizing immediate reward and Q Learning ( Watkins ) .
Though much theoretical progress has been made in recent years in the area of reinforcement learning , there have been relatively few successful , practical applications of the techniques . Notable examples include Neuro gammon ( Tesauro ) , the asset trader of Neuneier ( 1996 ) , an elevator scheduler ( Crites & Barto ) and space shuttle payload scheduler ( Zhang & Dietterich ) . In this paper we present results for reinforcement learning trading systems that outperform the S&P 500 Stock Index over a 25 year test period , thus demonstrating the presence of predictable structure in US stock prices .
Structure of Trading Portfolios
Systems and
Single Asset with Discrete Size
Traders : Position In this section , we consider performance functions for systems that trade a single security with price series zt . The trader is assumed to take only long , neutral or short positions Ft E { 1 , 0 , 1} of constant magnitude . The constant magnitude assumption can be easily relaxed to enable better risk control . The position Ft is established or maintained at the end of each time interval t , and is re assessed at the end of period t + 1 . A trade is thus possible at the end of each time period , although nonzero trading costs will discourage excessive trading . A trading system return Rt is reMized at the end of the time interval ( t 1 , t ] and includes the profit or loss resulting from the position Ft 1 held during that interval and any transaction cost incurred at time t due to a difference in the positions Ft 1 and Ft .
In order to properly incorporate the effects of
KDD 98 279 trading costs , market impact and taxes transactions ill a trader ’s decision making , the trader must have ininformation and must therefore be reternal state syscurrent . An example of a single asset tem that could take into account transactions costs following and market impact would be one with tile decision with It = ¯ ’} where Ot denotes {Zt , Zt l , the ( learned ) system parameters at time t and h denotes the information set at time t , which includes present and past values of the price series zt and an arbitrary number of other external variables denoted Yr . function : Zt 2 ,
Ft = F(Ot;Ft x,It )
; Yt , Yt 1 ,
Yt 2," of
( (za/~,a
Continuous Quantities
Portfolios : Multiple Assets For trading multiple assets in general ( typically including a risk free instrument ) , a multiple output trading system is required . Denotiug a set of m markets with price series {{z~’} : a = 1,,m} , the market return r~ for price series z~ for the period ending at time t is defined as ~t t/ t 1 ) 1 ) . Defining portfolio weights of the ath asset as Fa0 , a trader that takes only long positions must have portfolio weights that satisfy : F~ _> 0 and m Ea=l One approach on the portfolio weights without requiring that a constrained optimization be performed is to use a trading system Fa0 = {exp[D()]}/{~]~_ t exp[ff()]} for a = 1,,m Here , the fa0 could be linear or more complex functions of the inputs , such as a two layer neural network with sigmoidal internal units and linear outputs . Such a trading system call be optimized using unconstrained optimization methods . Denoting the sets of raw and normalized outputs collectively as vectors f0 and F0 respectively , a recursive trader will have structure Ft = softmax { ft ( Ot 1 ;Ft , , It)} .
= imposing the constraints has softmax outputs : fa to that
1 ’
Financial
Performance
Functions and and Wealth for Traders
Profit Portfolios Trading systems can be optimized by maximizing performance functions U0 such as profit , wealth , utility functions of wealth or performance ratios like the Sharpe ratio . The simplest and most natural performance function for a risk insensitive consider two cases : additive and multiplicative profits . Tile transactions cost rate is denoted ~ . trader is profit . We
Additive profits are appropriate to consider if each trade is for a fixed number of shares or contracts of security zt . This is often the case , for example , when trading small futures accounts or when trading standard US$ FX contracts ill dollar denominated foreign currencies . With tile definitions zt 1 and r[ = z[ z[_ for the price returns of a risky ( traded ) asset and a riskfree asset ( like T Bills ) respectively , the additive profit r t = zt l
280 Moody accunmlated over T time periods with trading position size p > 0 is then defined as :
T
PT = P E R , t=l T t=l
( 1 ) with P0 = 0 and typically FT = Fo = 0 . Equation ( 2 ) holds for continuous quantities also . The wealth defined as WT = Wo + PT .
Multiplicative profits are appropriate when a fixed in fraction of accumulated wealth v > 0 is invested each long or short trade . Here , rt = ( zt/zt 1 1 ) and ,’I t = ( zl/zit_ 1 1 ) . If no short sales are allowed and the leverage factor is set fixed at u = 1 , the wealth at time T is :
T wr = Wo {l+R,} t=l T
= WoII(l+(1 Ft ,)rIt+Ft lrt ) t=l
( 1 alF , F, ll ) .
( 2 )
When multiple assets are considered , tile effective portfolio weightings change with each time step due to price movements . Thus , maintaining constant or desired portfolio weights requires that adjustlnents in positions be made at each time step . The wealth after T periods for a portfolio trading system is
WT = Wol [{I+Rt}=WoU
,=l
"~
F~~]z~
,=l o : , ’ ’ 4 , ( 3 ) 1 6 IF :
, a=l
~ defined as Ft a { t l( where Fta is the effective portfolio weight of asset a before readjusting , {~=l Fbt ltlzblzbtl t lJJ~l . In ( 4 ) , the first factor in the curly brackets is the increase in wealth over tile time interval t prior to rebalancing to achieve the newly specified weights Ff . The second factor is the reduction in wealth due to the rebalancing costs . t l)}/
Za Za t/
Fa and
Return :
The Sharpe
Sharpe Ratios
Risk Adjusted Differential than maximizing profits , most modern fund Rather managers attempt to maximize risk adjusted return as advocated by Modern Portfolio Theory . The Sharpe ratio is tile most widely used measure of risk adjusted return ( Sharpe ) . Denoting as before the trading system returns for period t ( including transactions costs ) as Rt , tile Sharpe ratio is defined to be
ST =
Average(Rt )
Standard Deviation(Rt )
( 4 ) where the average and standard deviation are estimated for periods t = {1 , , T} .
Proper on line learning requires that we compute the influence on the Sharpe ratio of the return at time t . To accomplish this , we have derived a new objective function called the differential Sharpe ratio for on line optimization of trading system performance ( Moody , Wu , Liao & Saffell ) . It is obtained by considering exponential moving averages of the returns and standard deviation of returns in ( 4 ) , and expanding to first order in the decay rate . : St .~ St 1 + .aa ~lo=o + 0( . 2 ) ¯ Noting that only the first order term in this expansion depends upon the return Rt at time t , we define the differential Sharpe ratio as : dSt
Bt IAAt
1At IABt
Ot = d~ ( Bt ,
3/2
A2t_ , )
( 5 )
0 of the system after a sequence of T trades is dUT(O ) ~ dUT { dRt dFt dRt dFt l ~ dO t=l
~t dFt dO + dFt 1
~ J ( 7 )
The above expression as written with scalar Fi applies to the traders of a single risky asset , but can be trivially generalized to the vector case for portfolios .
The system can be optimized in batch mode by repeatedly computing the value of UT on forward passes through the data and adjusting the trading system parameters by using gradient ascent ( with learning rate p ) AO = pdUT(O)/dO or some other optimization method . A simple on line stochastic optimization can be obtained by considering only the term in ( 7 ) that depends on the most recently realized return Rt during a forward pass through the data : where the quantities At and Bt are exponential moving estimates of the first and second moments of Rt : dUt(O ) dUt { dR___~t dFt dRt dFt_ , } d7 dRt dFt dO + dFt_~ d~
( 8 )
At Bt
1
= At 1 = Bt 1 +.ABe = Bt 1
.AAt = At 1
t
.(Rt
At l )
b
.(R2t
Bt 1 )
.(6 ) in the update equations controls
Treating At 1 and Be 1 as numerical constants , note that , the magnitude of the influence of the return Rt on the Sharpe ratio St . Hence , the differential Sharpe ratio represents the influence of the return Rt realized at time t on St .
Reinforcement
Systems
Learning and Portfolios for Trading
The goal in using reinforcement learning to adjust the parameters of a system is to maximize the expected payoff or reward that is generated due to the actions of the system . This is accomplished through trial and error exploration of the environment . The system receives a reinforcement signal from its environment ( a reward ) that provides information on whether its actions are good or bad . The performance functions that we consider are functions of profit or wealth U(WT ) after a sequence of T time steps , or more generally of the whole time sequence of trades U(W1 , W2 , , WT ) as is the case for a path dependent performance function like the Sharpe ratio . In either case , the performance function at time T can be expressed as a function of the sequence of trading returns U(R1 , R2 , , RT ) . We denote this by UT in the rest of this section .
Utility
Immediate
Maximizing Given a trading system model Ft(O ) , the goal is to adjust the parameters 0 in order to maximize UT . This maximization for a complete sequence of T trades can be done off line using dynamic programming or batch versions of recurrent reinforcement learning algorithms . Here we do the optimization on line using a standard reinforcement technique . This reinforcement learning algorithm is based on stochastic gradient ascent . The gradient of UT with respect to the parameters learning
The parameters are then updated on line using AOt = pdUt ( Or)/dOt . Such an algorithm performs a stochastic optimization ( since the system parameters Ot are varied during each forward pass through the training data ) , and is an example of immediate reward reinforcement learning . This approach is described Liao & Saffell ) along with extensive simulation results . in ( Moody , Wu ,
Q Learning training a trader to take actions , we Besides explicitly can also implicitly learn correct actions through the technique of value iteration . In value iteration , an estimate of future costs or rewards is made for a given state , and the action is chosen that minimizes future costs or maximizes future rewards . Here we consider the specific technique named Q Learning ( Watkins ) , which estimates future rewards based on the current state and the current action taken . We can write the Q function version of Bellman ’s equation as
Q*(=,a)= y=0
, ( 9 ) that there in the system and p=~(a ) is where there are n states the probability of transitioning from state x to state y given action a . The advantage of using the Q function is is no need to know the system model p=y(a ) in order to choose the best action . One simply calculates the best action as a* = argmaxa(Q*(x , a) ) . The update rule for training a function approximator is Q(~ , a ) = Q(x , a ) + p(U(z , a ) + 3’ maxb q*(y , b) ) , where p is a learning rate .
Empirical Trader
Results of a Single
Long/Short We have tested techniques for optimizing both profit and the Sharpe ratio in a variety of settings . We present
Security
KDD 98 281
T
_i_
| !
Figure 1 : Boxplot for ensembles of 100 experiments comparing the performance of the "RTRL" and "Qtrader" trading systeins on artificial price series . "Qtrader" outperforms the "RTRL" system in terms of both ( a ) profit and ( b ) risk adjusted returns ( Sharpe ratio ) . The differences are statistically significant . results here on artificial data comparing the performance of a trading system , "RTRL" , trained using real time recurrent learning with the performance of a trading system , "Qtrader" , implemented using Q Learning . We generate log price series of length 10,000 as random walks with autoregressive trend processes . These series are trending on short time scales and have a high level of noise . The results of our simulations indicate that the "RTRL" trader to maximize immediate reward . the Q Learning trading system outperforms is trained that
The "RTP~L" system is initialized randomly at the for a while using labelled data to beginning , trained the weights , and then adapted using real time initialize recurrent learning to optimize the differential Sharpe ratio ( 5 ) . The neural net that is used in the "Qtrader" system starts from random initial weights . It is trained repeatedly on the first 1000 data points with the discount parameter 7 set to 0 to allow the network to learn immediate reward . Then 7 is set equal to 0.9 and training continues with the second thousand data points being used as a validation set . The value function used here is profit . the data are partitioned
Figure 1 shows box plots summarizing test performances for ensembles of 100 experiments . In these simulations , into a training set consisting of the first 2,000 samples and a test set containing the last 8,000 samples . Each trial has different realizations of the artificial price process and different random initial parameter values . The transaction costs are set at 0.2 % , and we observe the cumulative profit and Sharpe ratio over the test data set . We find that the "Qtrader" system , which looks at future profits , significantly outperforms the "RTRL" system which looks to maximize immediate rewards because it is effectively into the future when making decisions . looking farther
/ TBill Asset Allocation
S&:P 500 Long/Short Asset Allocation A long/short trading system is trained on monthly SgzP 500 stock index and 3 month TBill data to maximize
System and Data
System
282 Moody the differential Sharpe ratio . The S&P 500 target series is the total return index computed by reinvesting dividends . The 84 input series used in the trading systems include both financial and macroeconomic data . All data are obtained from Citibase , and the maeroeconomic series are lagged by one month to reflect ing delays . report
A total of 45 years of monthly data are used , from 20 January 1950 through December 1994 . The first years of data are used only for the initial training of the system . The test period is the 25 year period from January 1970 through December 1994 . Tile experimental results for the 25 year test period are true ex ante simulated trading results .
For each year dnring 1970 through 1994 , the system is trained on a moving window of the previous 20 years of data . For 1970 , the system is initialized with random parameters . For the 24 subsequent years , the previously learned parameters are used to initialize the training . In this way , the system is able to adapt to changing market and economic conditions . Within the moving training window , the "RTRL" systems use the first 10 years for stochastic optimization of system parameters , and the subsequent 10 years for validating early stopping of training . The networks are linear , and are regularized using quadratic weight decay during training with a regularization parameter of 001 The "Qtrader" systems use a bootstrap sample of the 20 year training window for training , and the final 10 years of the training window are used for validating early stopping of training . The networks are two layer feedforward networks with 30 tanh units in the hidden layer . panel of the initial for the "RTRL" system , and 10 trials
Experimental Results The left in Figure 2 shows box plots summarizing the test performance for the full 25 year test period of the trading systems with system parameters various realizations over 30 trials for the "Qtrader" system1 . The transaction cost is set at 05 % Profits are reinvested during trading , and multiplicative profits are used when calculating the wealth . The notches in the box plots indicate robust estimates of the 95 % confidence intervals on the hypothesis that the median is equal to the performance of the buy and hold strategy . The horizontal mance of the "RTRL" voting , "Qtrader" voting and buy and hold strategies for the same test period . The annualized monthly Sharpe ratios of the buy and hold stratand the "RTRL" egy , the "Qtrader" voting strategy voting strategy are 0.34 , 0.63 and 0.83 respectively . The Sharpe ratios calculated here are for the excess returns of the strategies over the 3 month treasury bill rate . lines show the perfor
Figure 2 shows results for following the strategy of taking positions based on a majority vote of the ensembles of trading systems compared with the buy and hold strategy . We can see that the trading systems go short the S&P 500 during critical periods , such as the oil price
ITen trials were done for the "Qtrader" system due to the amount of computation required in training the systems
,d lo’
!
"[
197"0 i z i
¢¢
.i 1~
.1~ .1
. ¯ tI" Il~t~lP ii I i I i i~l ~ | i i f iii i | i i ’ ii i ’ , _~ : ~,,~ ,,, , , I ~L , i t i i I~
II I_l_ll h Ii ii t , , , ¯ tl
Iii fll i i I L_I ~j i , ii
I I I_ I I
: ,
!
Figure 2 : Test results for ensembles of simulations using the S~:P 500 stock index and 3 month Treasury Bill data over the 1970 1994 time period . The figure shows the equity curves associated with the voting systems and the buy and hold strategy , as well as the voting trading signals produced by the systems . The solid curves correspond to the "RTlq.L" voting system performance , dashed curves to the "Qtrader" voting system and the dashed and dotted curves indicate the buy and hold performance . Initial equity is set to 1 , and transaction costs are set at 05 % In both simulations , the traders avoid the dramatic losses that the buy and hold strategy the "R.TRL" trader makes money during the crash of 1987 , while the "Qtrader" system avoids the large losses associated with the buy and hold strategy during the same period . incurred during 1974 . In addition , shock of 1974 , the tight money ( high interest rate ) periods of the early 1980 ’s , the market correction of 1984 and the 1987 crash . This ability to take advantage of high treasury bill rates or to avoid periods of substantial stock market loss is the major factor in the long term success of these trading models . One exception is that the "I~TRL" trading system remains long during the 1991 stock market correction associated with the Persian Gulf war , though the "Qtrader" system does identify the correction . On the whole though , the "Qtrader" system trades much more frequently than the "RTRL" system , and in the end does not perform as well on this data set .
From these results we find that both trading sysas mea tems outperform the buy and hold strategy , sured by both accumulated wealth and Sharpe ratio . significant and supThese differences are statistically port the proposition that there is predictability in the US stock and treasury bill markets during the 25 year period 1970 through 1994 . A more detailed presentation of the "R.TRL" results Liao & Saffell ) . is presented in ( Moody , Wu ,
Conclusions and Extensions results
( Moody , Wu ,
In this paper , we have trained trading systems via reinforcement learning to optimize financial objective functions including our recently proposed differential Sharpe ratio for online learning . We have also provided simulation results that demonstrate the presence of predictability in the monthly SSzP 500 Stock Index for the 25 year period 1970 through 1994 . We have previously shown with extensive simulation Liao L ; Saffell ) that the "RTRL" trading system significantly outperforms systems trained using supervised methods for traders of both single securities and portfolios . The superiority of reinforcement learning over supervised learning is most striking when state dependent transaction costs are taken into account . Here we show that the Q Learning approach can significantly improve on the "lq.TRL" method when trading ties , as it does for our artificial the "Qtrader" system does not perform as well as the "RTlq.L" system on the S~zP 500 / TBill asset allocation problem , possibly due to its more frequent trading . This effect deserves further exploration . data set . However , single securi
Acknowledgements
We acknowledge support for this work from Nonlinear Prediction Systems and from DARPA under contract DAAH0196 C R026 and AASERT grant DAAH04 95 1 0485 .
References
Crites , R . H . & Barto , A . G . ( 1996 ) , Improving elevator performance using reinforcement learning , in D . S . Touretzky , M . C . Mozer & M . E . Hasselmo , eds , ’Advances in NIPS’ , Vol . 8 , pp . 1017 1023 .
Moody , J . & Wu , L . ( 1997 ) , Optimization of trading systems and portfolios , in Y . Abu Mostafa , A . N . Refenes & A . S . Weigend , eds , ’Neural Networks in the Capital Markets’ , World Scientific , London . Moody , J . , Wu , L . , Liao , Y . & Saffell , M . ( 1998 ) , ’Performance functions and reinforcement learning for trading systems and portfolios’ , Journal of Forecasting 17 . To appear . Neuneier , R . ( 1996 ) , Optimal asset allocation using adaptive dynamic programming , in D . S . Touretzky , M . C . Mozer & M . E . Hasselmo , eds , ’Advances in NIPS’ , Vol . 8 , pp . 952958 .
Sharpe , W . F . ( 1966 ) , ’Mutual fund performance’ , Journal o ] Business pp . 119 138 .
Tesauro , G . ( 1989 ) ,
’Neurogammon wins the computer olympiad’ , Neural Computation 1 , 321 323 .
Watkins , C . J . C . H . ( 1989 ) , Learning with Delayed Rewards , PhD thesis , Cambridge University , Psychology Department . Zhang , W . & Dietterich , T . G . ( 1996 ) , High performance job shop schedttling with a time delay td(A ) network , in D . S . Touretzky , M . C . Mozer & M . E . Hasselmo , eds , ’Advances in NIPS’ , Vol . 8 , pp . 1024 1030 .
KDD 98 283
