Clustering Spatial Data Using Random Walks
[ Extended Abstract ]
∗
David Harel
Yehuda Koren harel@wisdomweizmannacil yehuda@wisdomweizmannacil
Dept . of Computer Science and Applied Mathematics The Weizmann Institute of Science , Rehovot , Israel
ABSTRACT Discovering significant patterns that exist implicitly in huge spatial databases is an important computational task . A common approach to this problem is to use cluster analysis . We propose a novel approach to clustering , based on the deterministic analysis of random walks on a weighted graph generated from the data . Our approach can decompose the data into arbitrarily shaped clusters of different sizes and densities , overcoming noise and outliers that may blur the natural decomposition of the data . The method requires only O(n log n ) time , and one of its variants needs only constant space .
1 .
INTRODUCTION
Spatial data are the data related to objects that occupy space [ 7 ] . Advances in database technologies have resulted in huge amounts of spatial data , and knowledge discovery techniques become essential tools for successful analysis of spatial data bases . This paper deals with clustering , which is one of the central techniques in spatial data mining .
Clustering methods are used to discover natural groups in data sets , and to identify abstract structures that might reside there , without having any background knowledge of the characteristics of the data . Prior literature on the clustering problem is huge , see eg , [ 5 ] . However , to a large extent the problem remains elusive , and there is still a dire need for a clustering method that is natural and robust , yet very efficient in dealing with large data sets .
The characteristics of spatial data pose several difficulties for clustering algorithms . Since we are seeking a natural decomposition , the clusters may have arbitrary shapes and non uniform sizes . Moreover , different clusters may have different densities . Another issue is the existence of noise , which may interfere the clustering process and should be identified . Regarding complexity , the huge sizes of spatial databases imply the need for very efficient clustering algo∗ For a full version of this paper see [ 4 ] rithms . Furthermore , it is impractical to assume that the entire database can reside in the main memory all at once . In this paper , we present a new approach to clustering spatial data , based on deterministic exploration of random walks on a weighted graph associated with the data . The heart of the method is in what we shall be calling separating operators , which are applied to the graph iteratively . Their effect is to ‘sharpen’ the distinction between the weights of inter cluster edges ( those that ought to separate clusters ) and intra cluster edges ( those that ought to remain inside a single cluster ) , by decreasing the former and increasing the latter . The operators can be used on their own or can be embedded in a classical agglomerative clustering framework . The resulting algorithms are simple , fast and general , and seem to cope successfully with the unique difficulties common to spatial data . We exhibit encouraging results of applying these algorithms to several recently published data sets .
2 . BASIC NOTIONS
We will be using standard graph theoretic notions . Specifically , let G(V , E , w ) be a weighted graph . The w is a weighting function w : E −→ R , that measures the similarity between pairs of items ( a higher value means more similar ) . Let S ⊆ V . The set of nodes that are connected to some node of S by a path with at most k edges is denoted by V k(S ) . The degree of G , denoted by deg(G ) , is the maximal number of edges incident to some single node of G . The subgraph of G induced by S is denoted by G(S ) . The edge between i and j is denoted by 'i , jff . Sometimes , when the context is clear , we will write simply 'i , jff instead of 'i , jff ∈ E .
A random walk is a natural stochastic process on graphs . Given a graph and a start node , we select a neighbor of the node at random , and ‘go there’ , after which we continue the random walk from the newly chosen node . The probability of a transition from node i to node j , is pij = di where di =
i,k . w(i , k ) is the weighted degree of node i . w(i,j ) visit(i ) ∈ R
Now , denote by P k n the vector whose j th component is the probability that a random walk originating at i will visit node j in its k th step .
The escape probability from a source node s to a target node t , denoted by Pescape(s , t ) , is defined as the probability that a random walk originating at s will reach t before returning to s . This probability can be computed as follows .
For every i ∈ V , define a variable ρi satisfying : ρs = 0 , pij · ρj and ρi =
ρt = 1 , for i = s , i = t
.
i,j .
The values of ρi are calculated by solving these equations , and then the desired escape probability is given by : Pescape(s , t ) =
s,i . psi · ρi
3 . MODELING THE DATA
Our method , as many other hierarchical clustering algorithms , deals with a graph representation of the data . Several sparse graph structures have been used for modeling data points , see , eg , [ 5 ] . We mention here two commonly used ones : the Delaunay triangulation and the k mutual neighborhood graph .
The Delaunay triangulation ( DT ) of a point set is the dual of the famous Voronoi diagram , which is a partition of the space into cells , one for each data point , so that the cell for data point a consists of that region of the space that is closer to a than to any other data point . An edge in the DT connects points a and b if and only if the Voronoi cells containing a and b share a common boundary . Hence , edges of the DT capture spatial proximity . Regarding computation efficiency , many O(n log n ) time and O(n ) space algorithms exist for computing the DT of a planar point set . The k mutual neighborhood graph contains all edges 'a , bff for which a is one of the k nearest neighbors of b , and b is one of the k nearest neighbors of a . In general , the k nearest neighbors of each point can be found in time O(n log n ) and O(n ) space for any fixed arbitrary dimension [ 1 ] . When working with spatial databases , the k nearest neighbors can be computed alternatively , using region queries and nearest neighbor queries . These queries are implemented using sophisticated spatial index structures , which consume on average O(log n ) time . More importantly , they are very economical with respect to memory [ 7 ] . Using this query based technique , the k mutual neighborhood graph can be computed in average time of O(n log n ) and using a negligible amount of memory . In many cases it may be best to use the intersection of these two graphs , ie , to include only those edges that appear in both the DT and the k mutual neighborhood graph . Further details are given in [ 4 ] . Regarding edge weights , we adopt a commonly used approach : the weight of the edge 'a , bff is exp(− d(a,b)2 ave2 ) , where d(a , b ) is the Euclidean distance between a and b , and ave is the average Euclidean distance between two adjacent points in the graph .
4 . CLUSTERING USING RANDOM WALKS 4.1 Separators and separating operators
Our approach to identifying natural clusters in a graph is to find ways to compute an ‘intimacy relation’ between the nodes incident to each of the graph ’s edges . In other words , we want to be able to decide for each edge if it should cross the boundaries of two clusters , or , rather , if the relationship between its two incident nodes is sufficiently intimate for them to be contained in a common cluster . Definition 41 Let the graph G(V , E ) be clustered by C = ( C1 , . . . , Ck ) . An edge 'u , vff ∈ E is called a separating edge for C , or a separator for short , if u ∈ Ci , v ∈ Cj for i = j .
Any set of edges F ⊂ E gives rise to an induced clustering CF , obtained by simply taking the clusters to be the connected components of the graph G(V , E − F ) . The set F will then contain precisely the separating edges of CF . Another way of putting this is that if we can indeed decide which are the separators of a natural clustering of G , we are done , since we will simply take the clustering to be CF for the discovered set F of separators .
We have decided to concentrate on discovering a set of separating edges , since the decision as to whether an edge should be separating involves only relatively local considerations . Globally speaking , there might not be much difference between two neighboring nodes , and the reasons for placing two neighbors in different clusters will most often be some sharp local transition of some characteristic of the cluster , which global considerations may decay , see eg , Figure 1 .
Figure 1 : Two natural clusters . The only reason for separating the black points from the white ( hollow ) ones lies in local considerations . Most global approaches that seek for an intimate relation between every two points in the cluster , such as k means , will probably fail here and might attach points from the right hand side of the black cluster to the white cluster .
The strategy we propose for identifying separators is to use an iterative process of separation . Separation reweights edges by local considerations in such a way that the weight of an edge connecting ‘intimately related’ nodes is increased , and for others it is decreased . This is a kind of sharpening pass , in which the edges are reweighted to sharpen the distinction between ( eventual ) separating and non separating edges . When the separating operation is iterated several times , a sort of ‘zero one’ phenomenon emerges , whereby the weight of an edge that should be a separator notably diminishes .
We now offer two methods for performing the edge separation , both based on deterministic analysis of random walks .
NS : Separation by neighborhood similarity A helpful property of the vector P k visit(i ) is that it provides the level of nearness or intimacy between the node i and every other node , based on the structure of the graph . Actually , P k visit(i ) generalizes the concept of weighted neighborhoods , since P 1 visit(i ) is exactly the weighted neighborhood ∞ visit(i ) does not depend on i and is equal to of i . Also P the stationary distribution of the graph . Hence , the value of P k visit(i ) is not very interesting for overly large values of k . visit(· ) , which is defined ≤k We will actually be using the term P to be visit(v ) .
k i=1 P i
Now , in order to estimate the closeness of two nodes v and ≤k visit(v ) u , we fix some small k ( eg , k = 3 ) and compare P ≤k visit(u ) . The smaller the difference the greater the and P ≤k intimacy between u and v . The reason we use P visit here and not P k visit is that for a bipartite subgraph the values of P k visit can be very different , since the two random walks originating from u and v cannot visit the same node at the same time . However , if we are willing to sum some steps of the two walks , we may find that they visit roughly the same nodes .
We now define the separating operator itself :
Definition 42 Let G(V , E , w ) be a weighted graph and k be some small constant . The separation of G by neighborhood similarity , denoted by N S(G ) , is defined to be : dfn = Gs(V , E , ws ) ,
N S(G ) where ∀'v , uff ∈ E , ws(u , v ) = sim k
≤k visit(v ) , P
≤k visit(u ) )
( P
Here , simk(.x , .y ) is some similarity measure of the vectors .x and .y , whose value increases as .x and .y become more similar . A suitable choice is : k f
( .x , .y ) dfn
= exp(2k − .x − .yL1 ) − 1
The norm L1 is defined in the standard way : For a,b ∈ R .a − .bL1 = i=1 |ai − bi|
n n ,
Another suitable choice is the cosine , or the correlation , of .x and .y , defined as :
( .x , .x ) · fi where ( ·,· ) denotes the inner product . cos(.x , .y ) =
( .x , .y ) fi
( .y , .y )
≤k visit(v ) and P
The key component in computing N S(G ) is the calcula≤k visit(u ) . Since the graphs that we want tion of P ≤k to cluster are of bounded degree , P visit(u ) can be computed in time and space O(deg(G)k ) , which is independent of the size of G and can be treated as a constant . Hence , N S(G ) can be computed in space O(1 ) and time Θ(|E| ) , which in this case is just Θ(n ) .
CE : Separation by circular escape An alternative method for capturing the extent of intimacy between nodes u and v , is by the probability that a random walk that starts at v visits u exactly once before returning to v for the first time . ( This notion is symmetric , since the event obtained by exchanging the roles of v and u has the same probability . ) If v and u are in different natural clusters , the probability of such an event will be low , since a random walk that visits v will likely return to v before reaching u ( and the same with u and v exchanged ) . The probability of this event is given by : Pescape(v , u ) · Pescape(u , v )
Seeking efficient computation , and on the reasonable assumption that data relevant to the intimacy of v and u lies in a relatively small neighborhood around v and u , we can constrain our attention to a limited neighborhood , by the following :
Definition 43 Let G(V , E , w ) be a graph , and let k be some constant . Denote by P Pescape(v , u ) , but computed using random walks on the subgraph G(V k({v , u} ) ) instead of on the original graph G . The circular escape probability of v and u is defined to be :
( k ) escape(v , u ) the probability k
CE
( v , u ) dfn = P
( k ) escape(v , u ) · P
( k ) escape(u , v ) .
We can now define separation by circular escape :
Definition 44 Let G(V , E , w ) be a weighted graph , and let k be some small constant . The separation of G by circular escape , denoted by CE(G ) , is defined to be : dfn = Gs(V , E , ws )
CE(G ) where ∀'v , uff ∈ E , ws(u , v ) = CE k
( v , u )
For graphs with bounded degree , the size of G(V k(v , u ) ) is independent of the size of G , so that CEk(v , u ) can be computed essentially in constant time and space . Hence , as with N S(G ) , the separating operator CE(G ) can be computed in time Θ(|E| ) = Θ(n ) and space O(1 ) .
Our experiments show that in general the CE operator yields better results than the NS operator . However , the computation of the CE operator is clearly more complicated in terms of numerical precision , as well as running time . 4.2 Clustering by separation
The idea of separating operators is to uncover and bring to the surface a closeness between nodes that exists implicitly in the structure of the graph . Separating operators increase the weights of intra cluster edges and decrease those of intercluster ones . Iterating the separating operators sharpens the distinction further . After a small number of iterations we expect the difference between the weights of the two kinds of edges to differ sufficiently to be readily apparent , because the weights of separators are expected to diminish significantly . Moreover , Iterating the separating operators causes information from distant parts of the graph to ‘flow in’ , reaching the areas where separating decisions are to be made . A detailed demonstration of activating the separating operators on two graphs is given in [ 4 ] . 4.3 Clustering spatial points
We now illustrate the ability of our method to cluster “ correctly ” 2D points , in a number of typical cases , some of which have been shown to be problematic for agglomerative methods [ 6 ] . ( More extensive examples are given in Section 6 . ) We show only examples in 2D , although the method works well in higher dimensions too , because two dimensions are easier to visualize and evaluate .
We have used 10 mutual neighborhood graphs for modeling the points ( intersection with the Delaunay triangulation gives similar results ) . The results are achieved using 3 iterations of either CE or NS , with k = 3 . For NS , we took the function sim(·,· ) to be f ( ·,· ) . In general , other choices work equally well .
The partition of the edges into separators and non separators is based on a threshold value , such that all the edges whose weight is below this value are declared as separators . Without loss of generality , we may restrict ourselves to the O(n ) edge weights as candidates for being thresholds . The actual threshold value ( or several , if a hierarchy of decompositions is called for ) , is found by some statistical test , eg , inspecting the edge weight frequency histogram , where the frequency of the separators’ weights is usually smaller , since most of the edges are inside the clusters , and have higher weights than those of the separators .
Figure 2 shows the clustering decomposition of three data sets using our algorithm .
The data set DS1 shows the inherent capability of our algorithms to cluster at different resolutions at once , ie , to detect several groups with different intra group densities .
This ability is beyond the capabilities of many clustering algorithms that can show the denser clusters only after breaking up the sparser clusters . Data set DS2 demonstrates the ability of our algorithm to separate the two left hand side clusters , despite the fact that the distance between these clusters is smaller than the distance between points inside the right hand side cluster .
The data set DS3 exhibits the capability of our algorithm to take into account the structural properties of the data set , which is the only clue for separating these evenly spaced points .
DS1
DS2
DS3
Figure 2 : Clustering of several data sets . Different clusters are indicated by different colors , and note that each set results in three clusters .
When there is a hierarchy of suitable decompositions , our method can reveal it by using a different threshold for each level of the hierarchy . For example , consider the two data sets in Figure 3 . For each of these we have used two different thresholds , to achieve two decompositions .
It is noteworthy that the general methodology of revealing the graph structure by exploration of random walks is fundamental and robust enough to be applied to the clustering of spatial data , even though our use of spatial properties of the data is minimum .
One should observe that the memory requirements of our clustering method are low , which is very important for huge spatial data bases that cannot fit in main memory . The only “ global ” operation in the process is that of computing connected components after removing separators . Since this operation does not require too many accesses to each single node , it can be performed quite efficiently without copying the entire data base into main memory .
5 .
INTEGRATION WITH AGGLOMERATIVE CLUSTERING
Agglomerative clustering is a well known hierarchical clustering method that starts from the trivial partition of n points into n clusters of size 1 and continues by repeatedly merging pairs of clusters . At each step the two clusters that are most similar are merged , until the clustering is satisfactory . Different similarity measures between clusters result in different agglomerative algorithms .
The separation operators can be used as a preprocess
T hreshold ∼ 0
0.01 T hreshold 27.17 ( 0.06 T hreshold 19.16 )
T hreshold ∼ 0
8.97 T hreshold 9.52
( 18.02 T hreshold 18.39 )
Figure 3 : Clustering at multiple resolutions using different thresholds . When values of CE are different from values of NS , the CE values are given in parentheses . CE values are multiplied by 100 . ing stage before activating agglomerative clustering on the graph . Such a preprocessing sharpens the edge weights , adding structural knowledge to them , and greatly enhances the agglomerative algorithms , as it can effectively prevent bad local merging opposing the graph structure .
Implementation of the agglomerative algorithm can be done using a dynamic graph structure . At each step we take the edge of the highest weight , merge ( “ contract ” ) its two endpoints , and update all the adjacent edges . When contracting nodes u and v having a common neighbor t , the way we determine the weight of the edge between t and the contracted node uniquely distinguishes between different variants of the agglomerative procedure . For example , when using maximal similarity – “ single link ” , we take this weight as max{w(v , t ) , w(u , t)} , while when using total similarity we fix the weight as w(v , t ) + w(u , t ) . For a bounded degree graph , which is our case , each such step can be carried out in time O(log n ) , using a binary heap .
It is interesting that the clustering method we have described in the previous section is in fact completely equivalent to a “ single link ” algorithm preceded by a separation operation . Hence we can view the integration of the separation operation with the agglomerative algorithm as a generalization of the method we have discussed in the previous section , that enables us to use any variant of the agglomerative algorithm .
We have found particularly effective the normalized total similarity variant , in which we measure the similarity between two clusters as the total sum of the weights of the original edges connecting these clusters . We would like to eliminate the tendency of such a procedure to contract pairs of nodes representing large clusters whose connectivity is high due to their sizes . Accordingly , we normalize the weights by dividing them by some power of the sizes of the relevant clusters . More precisely , we measure the similarity of two clusters C1 and C2 by : w(C1 , C2 ) fi|C1| + d d fi|C2| where w(C1 , C2 ) is the sum of original edge weights between C1 and C2 , and d is the dimension of the space in which the points lie . We took d tion of the size of the boundaries of the clusters C1 and C2 , respectively . fi|C2| as an approxima fi|C1| and d
The overall time complexity of our algorithm is O(n log n ) , which includes the time needed for constructing the graph and the time needed for performing n contractions using a binary heap . This equals the time complexity of the method described in the previous section ( because of the graph construction stage ) . However , the space complexity is now worse . We need Θ(n ) memory for efficiently handling the binary heap . Selecting meaningful decompositions An agglomerative clustering algorithm provides us with a dendrogram , which is a pyramid of nested clustering decompositions . The question of which are the meaningful decompositions inside the dendrogram , still remains .
Each level in the dendrogram is constructed from the level below , by merging two clusters . We associate with each level a grade that measures the importance of that level . Inspired by the work of [ 3 ] , a rather effective way of measuring the importance of a level is by evaluating how sharp is the change that this level introduces to the clustering decomposition . Since changes that are involved with small clusters do not have a large influence , we define the prominency rank of a level in the dendrogram , in which the clusters Ci and Cj of the level below were merged , as : |Ci| · |Cj|
We demonstrate the effectiveness of this measure in the next section .
6 . EXAMPLES
In this section we show the results of running our algorithm on several data sets from the literature . We modeled these data sets using the intersection of the Delaunay triangulation and the 15 mutual neighborhood graph . For all the results we have used total similarity agglomerative clustering , preceded by 2 iterations of the NS separation operator with k = 3 and similarity function defined as cos(·,· ) . Using the CE operator , changing the value of k , or increasing the number of iterations , do not have a significant effect on the results . Using the method described in Section 4 may change the results in few cases .
We implemented the algorithm in C++ , running on a Pentium III 800MHz processor . The code for constructing the Delaunay triangulation is of Triangle , which is available from URL : http://wwwcscmuedu/˜quake/trianglehtml The reader is encouraged to see the full electronic version of this paper [ 4 ] , in order to view the figures of this section in larger and clearer format , and in color .
Figure 4 shows the results of the algorithm on data sets taken from [ 6 ] . These data sets contain clusters of different shapes , sizes and densities and also random noise . A nice property of our algorithm is that random noise gets to stay inside small clusters . After clustering the data , the algorithm treats all the relatively small clusters , whose sizes are below half of the average cluster size , as noise , and simply omits them , showing only the larger clusters .
Figure 5 shows the result of the algorithm applied to a data set from [ 2 ] . We show two levels in the hierarchy , rep resenting two possible decompositions . We are particularly happy with the algorithm ’s ability to break the cross shaped cluster into 4 highly connected clusters , as shown in Figure 5(c ) .
In Figure 6 , which was produced by adding points to a data set given in [ 2 ] , we show the noteworthy capability of the algorithm to identify clusters of different densities at the same level of the hierarchy . Notice that the intra distance between the points inside the right hand side cluster , is larger than the inter distance between several other clusters .
Throughout all the examples given in this section we have used the prominency rank introduced in Section 5 to reveal the most meaningful levels in the dendrogram . Figure 7 demonstrates its capability with respect to the data set DS4 ( shown in Figure 4 ) . We have chosen the five levels with the highest prominency ranks , and for each level we show the level that precedes it . It can be seen that these five levels are exactly the five places where the six large natural clusters are merged . In this figure we have chosen not to hide the noise , so the reader can see the results of the algorithm before removing the noise .
Table 6 gives the actual running times of the algorithm on the data sets given here . We should mention that our code is not optimized , and the running time can certainly be improved .
DS4 : 8000 points
DS5 : 8000 points
DS6 : 10000 points
DS7 : 8000 points
DS8 : 8000 points
Figure 4 : Data sets taken from [ 6 ] ( see [ 4 ] for larger , clearer color versions of this figure and of Figs . 5–7 ) .
Data Set
DS4 DS5 DS6 DS7 DS8 DS9 DS10
Size Graph construction 8000 8000 10000 8000 8000 8000 3374
0.4 0.41 0.5 0.4 0.39 0.33 0.14
Separation Agglomeration Overall Ratio P oints Sec
0.88 0.83 1.12 0.89 0.93 0.66 0.26
0.19 0.19 0.26 0.2 0.2 0.21 0.07
1.47 1.43 1.88 1.49 1.52 1.2 0.47
5434 5587 5311 5358 5256 6656 7178
Table 1 : Running time ( in seconds ; non optimized ) of the various components of the clustering algorithm
( a ) DS9 : 8000 points
( b )
( c )
Figure 5 : Two different clusterings of a data set taken from [ 2 ]
DS10 : 3374 points
Figure 6 : A data set with clusters of different densities level 31 level 42 level 61 level 62 level 88
Figure 7 : A hierarchy containing five decompositions of DS4 corresponding to the five levels with the highest prominency rank . ( Level index indicates the number of clusters . )
7 . CONCLUSION
We have introduced a novel algorithm for achieving a hierarchical clustering of spatial data , using random walk based separating operators . This approach seems to have several advantages .
First , it is robust in the presence of noise and outliers , and is flexible in handling data of different densities . It can reveal clusters of any shape without a special tendency towards spherically shaped clusters or ones of similar sizes . At the same time , the decisions the algorithm makes are based on the relevant structure of the associated graph , making it essentially immune to outliers and noise .
The second advantage is the running time and space requirements . The time complexity of our algorithm applied to n data points is O(n log n ) , and its practical running time , in general , is very fast . We have been able to cluster 10,000 points in less than two seconds . Regarding space complexity , one of the variants of the algorithm can be applied with a constant amount of space .
Since the algorithm does not rely on spatial knowledge , we plan to try it on other types of data . We have already used a variant of it for image segmentation with very encouraging results , and will report on it separately .
8 . REFERENCES [ 1 ] M . T . Dickerson and D . Eppstein , “ Algorithms for proximity problems in higher dimensions ” , Comp . Geom . Theory and Applications , 5 ( 1996 ) , 277–291 .
[ 2 ] V . Estivill Castro and I . Lee , “ AUTOCLUST :
Automatic Clustering via Boundary Extraction for Mining Massive Point Data Sets ” , 5th International Conference on Geocomputation , GeoComputation CD ROM : GC049 , ISBN 0 9533477 2 9 .
[ 3 ] Y . Gdalyahu , D . Weinshall and M . Werman ,
“ Stochastic Image Segmentation by Typical Cuts ” , Proceedings IEEE Conference on Computer Vision and Pattern Recognition , 1999 , pp . 588–601 .
[ 4 ] D . Harel and Y . Koren , “ Clustering Spatial Data Using Random Walks ” , Technical Report MCS01 08 , Dept . of Computer Science and Applied Mathematics , The Weizmann Institute of Science , 2001 . Available at : wwwwisdomweizmannacil/reportshtml
[ 5 ] A . K . Jain and R . C . Dubes , Algorithms for Clustering Data , Prentice Hall , Englewood Cliffs , New Jersy , 1988 .
[ 6 ] G . Karypis , E . Han , and V . Kumar , “ CHAMELEON :
A Hierarchical Clustering Algorithm Using Dynamic Modeling ” , IEEE Computer , 32 ( 1999 ) , 68–75 . [ 7 ] X . Xu , M . Ester , HP Kriegel and J . Sander ,
“ Clustering and Knowledge Discovery in Spatial Databases ” , Vistas in Astronomy , 41 ( 1997 ) , 397–403 .
