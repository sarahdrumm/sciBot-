Empirical Bayes Screening for Multi Item Associations
William DuMouchel and Daryl Pregibon
AT&T Labs—Research
180 Park Avenue
Florham Park , NJ 07932 , USA
{dumouchel , daryl}@researchattcom
ABSTRACT This paper considers the framework of the so called “ market basket problem ” , in which a database of transactions is mined for the occurrence of unusually frequent item sets . In our case , “ unusually frequent ” involves estimates of the frequency of each item set divided by a baseline frequency computed as if items occurred independently . The focus is on obtaining reliable estimates of this measure of interestingness for all item sets , even item sets with relatively low frequencies . For example , in a medical database of patient histories , unusual item sets including the item “ patient death ” ( or other serious adverse event ) might hopefully be flagged with as few as 5 or 10 occurrences of the item set , it being unacceptable to require that item sets occur in as many as 0.1 % of millions of patient reports before the data mining algorithm detects a signal . Similar considerations apply in fraud detection applications . Thus we abandon the requirement that interesting item sets must contain a relatively large fixed minimal support , and adopt a criterion based on the results of fitting an empirical Bayes model to the item set counts . The model allows us to define a 95 % Bayesian lower confidence limit the “ interestingness ” measure of every item set , whereupon the item sets can be ranked according to their empirical Bayes confidence limits . For item sets of size J > 2 , we also distinguish between multi item associations that can be explained by the observed J(J–1)/2 pairwise associations , and item sets that are significantly more frequent than their pairwise associations would suggest . Such item sets can uncover complex or synergistic mechanisms generating multi item associations . This methodology has been applied within the US Food and Drug Administration ( FDA ) to databases of adverse drug reaction reports and within AT&T to customer international calling histories . We also present graphical techniques for exploring and understanding the modeling results . for
Categories and Subject Descriptors H28 [ Database Management ] Database Applications data mining , statistical databases .
General Terms Statistical Models , Data Mining , Knowledge Discovery .
Keywords Association rules , empirical Bayes methods , gamma Poisson model , market basket problem , shrinkage estimation .
1 . THE MARKET BASKET PROBLEM A common data mining task is the search for associations of items in a database of transactions . Each transaction , such as a list of items purchased , or a list of diagnoses and medications in a patient medical report , defines a subset of all possible items in the union of all transactions . If there are K total items , we define for each transaction , binary random variables X1 , … , XK that are 1 if the corresponding item is included in the transaction , and 0 otherwise . If the total number of transactions in the database is N , then the data being modeled could be represented as an N x K matrix X = Xik , i = 1 , … , N ; k = 1 , … , K . To add more realism to the statistical model , we do not assume that the distribution of each row of X is identical . Rather , we assume that the transactions are stratified , for example by store location in the case of supermarket scanner data , or by patient age , sex , etc . in the case of medical data . Time period of the transaction could also be a stratification variable . The purpose of stratification is to avoid finding spurious associations between two items merely because they both tend to be more frequent in the same strata . We assume that associations due to stratification variables ( so called between strata associations ) are not of interest , and instead require the methodology to identify items that are associated within strata . For example , certain upscale store locations might have greater sales volume of high priced items , but we may only wish to say such items have interesting associations if they tend to be purchased together more frequently than their per store marginal frequencies indicate , since an overall association would show up even if the items were purchased independently within each store . As the preceding paragraph indicates , we are departing from the majority of published treatments of the market basket problem by going beyond the enumeration of the support ( proportion of transactions ) of frequent item sets . No matter how frequently an item set occurs , we assume that it is of little interest to the analyst if its frequency is about the same as would be predicted if the members of the item set occur independently within each stratum . This comparison frequency , based on within strata independence , will be called the baseline frequency of the particular item set , and our measure of interestingness is the ratio of actual to baseline frequency . Two earlier papers that focus on interestingness measures as deviations from expected frequencies predicted by independence for the market basket problem are Silverstein , Brin and Motwandi [ 14 ] and Aggarwal and Yu [ 1 ] . Considering the former paper first , [ 14 ] rightly criticizes the more common support confidence framework for association rules and suggests using a chi squared test statistic for the hypothesis of complete independence of items in an item set as the basis for comparing the degree of dependence among different item sets . They develop a theory for using this test statistic that relies on the property of upward closure , namely that the computed chi squared statistic testing independence for a set of items ( binary variables in a database ) can only increase if you increase the number of items ( size or dimensionality of the item set ) . Unfortunately , the authors mistakenly state that the degrees of freedom of the chi squared statistic is always just 1 , for all values of J , the item set size . In fact , the degrees of freedom of their test statistic is 2J – J – 1 . Thus , although the test statistic will increase as more variables are included in the computations , the statistical significance or “ p value ” will not be monotone and may increase or decrease , thereby invalidating the supposed theoretical properties of their proposed procedures and computational methods . In addition , as demonstrated in DuMouchel [ 8 ] and discussed in Section 3 below , relying solely on a for measuring dependence has the built in flaw of being overly sensitive to sample size . The latter paper [ 1 ] also criticizes the standard support confidence framework for item sets , pointing out how it is susceptible to identifying spurious associations . These authors propose a measure they call collective strength . If I is an item set , let v(I ) be the violation rate of the item set in the database , which is the fraction of transactions that contain some , but not all , of the members of I . Then , if E[ ] refers to expectation under the assumption of independence of items , collective strength is defined as significance statistical statistic test
C(I ) = ( 1 – v(I ) ) E[v(I ) ] / v(I ) ( 1 – E[v(I) ] ) computational properties , which allow the
For example , for item sets of size 3 , the notion of collective ( 0,0,0 ) and strength pools the proportions of the two triples ( 1,1,1 ) , and contrasts their proportion with v(I ) , the pooled proportions of the other six possible triples . Then the ratio ( 1v(I))/v(I ) is divided by the corresponding ratio of expected values under independence . Aggarwal and Yu [ 1 ] show that collective strength avoids the undesirable spuriousness to which the support confidence framework is susceptible , yet has at least as nice efficient enumeration of what they define as “ strongly collective item sets . ” However , in our opinion , this is achieved at a considerable loss of interpretability and practical value as a measure of association . For example , let us continue the example of an item set of size 3 , and suppose the three items each occur in 10 % of transactions . the probabilities of observing ( 0,0,0 ) and ( 1,1,1 ) are .93 = .729 and .13 = .001 , respectively , leading to E[1 – v(I ) ] = .729 + .001 = .73 and E[v(I ) ] = 27 Note how the probability of observing ( 0,0,0 ) dominates this calculation , compared to that of ( 1,1,1 ) , even though it is the simultaneous presence ( not absence ) of the three items that is almost always of more interest in applications . Suppose that the observed probabilities of ( 0,0,0 ) and ( 1,1,1 ) are .75 and .02 , in which case the triple of items is present 20 times more than expected from independence . But this sense of a 20:1 increase is lost in the computation of collective strength , where C(I ) = ( 77)(27 ) / ( 23)(73 ) = 1.24 is close to 1 , the value of C(I ) under independence . This undesirable property of collective strength will persist whenever the individual items are not very frequent ( eg , when most items occur in less than one third of independence assumption ,
Under the transactions ) and we believe this describes the majority of market basket problems . Both the cited papers , as well as [ 6 ] , refer to the simple ratio of observed to baseline frequencies of occurrence as the interest of an item set , which we denote R . The primary criticisms of R as a measure of dependence are 1 . Since R is bounded above by 1/P(A)P(B)… , if all the items the range of R
A , B , … in an item set are quite probable , can be restricted to seemingly small values .
2 . Since R is symmetric in the items A , B , … , it does not indicate predictive power or suggest causation .
3 . Computing all item sets with greater than a specified value of R will be costly computationally , since a great many item sets with small support can have large values of R .
We believe that the first criticism is mitigated by the fact that the vast majority of market basket problems deal with items whose probabilities are not large . Data mining is , by definition , a search for low probability events of high value . We view the lack of directionality of R as more an advantage than a disadvantage . Once an item set is identified as unusually frequent , domain knowledge is required to identify mechanisms and potential causality among the items in the item set , something far beyond the presumption of a statistical algorithm . The third criticism above identifies both a computational and a statistical issue . We do not focus on computational issues here , which are already well treated in the association rule literature , but rather we focus on the statistical issues that arise for item sets with small support . A novel element of our approach to identifying interesting item sets is the ability to detect interesting associations even if they have relatively small support . To do so , we must explicitly allow for noise or spurious association due to small samples . Consider an item set observed in 200 transactions , even though the baseline ( independence model ) frequency is just 2 . How should this 200:2 ratio of actual to baseline be compared to other item sets having the same ratio , such as 20:0.2 , or even 2:0.02 ? Our answer involves an empirical Bayes model , described in detail in [ 8 ] , that produces a posterior distribution for the true ratio of actual the observed counts of every item set have separate Poisson distributions . Our conservative estimate of the ratio of actual to baseline is the lower 95 % Bayesian confidence limit for this ratio , which we denote λ05 When observed counts are large , λ.05 will be almost identical to the naïve observed ratio , namely R = 100 in the 200:2 example . However , λ.05 will be rather less than 100 for the case of 20:0.2 , and will be much smaller , perhaps near 1 , in the 2:0.02 example . This tendency of empirical Bayes estimates to reduce the observed effect when sample sizes are small gives rise to the label shrinkage estimate . To summarize , we introduce three improvements to the existing market basket methodology . First , we allow stratification adjustments to eliminate spurious association due to noninteresting stratification variables . Second , we focus on the ratio of item set frequencies to baseline frequencies computed via an independence assumption , rather than merely the support of each item set . And third , we replace the observed ratios of actual to baseline frequencies by empirical Bayes shrinkage estimates in to baseline expected frequencies , assuming that order to smooth away the noise due to small samples . A fourth innovation , applicable only to item sets of size greater than 2 , will be described in Section 4 . 2 . BASELINE AND RELATIVE RATES Let Ns be the number of transactions in stratum s , let Nsj be the number of transactions in stratum s that include item j , let Nsjk be the number of transactions in stratum s that include both item j and item k , and so forth , where s = 1 , … , S , the number of strata , and 1<j<k<K . Define Psj = Nsj/Ns as the proportion of transactions in stratum s that include item j . The baseline frequencies for item sets of size 1 , 2 , 3 , and 4 are , respectively , ej , ejk , ejkl , and ejklm , where s Nsj ej = Σ ejkl = Σ s NsPsj = Σ s NsPsjPskPsl ejk = Σ ejklm = Σ s NsPsjPsk s NsPsjPskPslPsm
Analogous definitions are used for baseline frequencies of larger item sets . This method of computing separate baseline frequency estimates for each stratum and then adding across strata became standard in the biostatistics literature after the article by Mantel The method provides protection against and Haenzel [ 12 ] . [ 15 ] whereby associations that hold Simpson ’s paradox separately within each stratum can disappear or even be reversed when data are pooled across strata . DuMouchel [ 8 , Table 5 and related discussion ] provides an example of how failure to stratify by age could lead to an incorrect assessment that polio vaccine is associated with sudden infant death syndrome ( SIDS ) . Since polio vaccine is overwhelmingly given to infants , and of course only infants develop SIDS , a database including suspected adverse drug reactions for patients of all ages will show an excess of SIDS cases from polio vaccine recipients , although many well regarded clinical trials of vaccine recipients show no such relationship . We use lower case ns for total across stratum counts : nj = Σ = ej , njk = Σ ( j<k<l ) , ( j<k<l<m ) , etc . , the raw relative rates are defined as s Nsj sNsjk , etc . Then , for all item sets defined by ( j<k ) ,
Rjk = njk/ejk , Rjkl = njkl/ejkl , Rjklm = njklm/ejklm .
In domains for which only very large counts ( eg , n > 500 ) are of interest , then sorting and otherwise examining item sets according to their values of R may provide a sufficient analysis . It is recommended that separate examinations be conducted for item sets of size 2 , of size 3 , etc . , because in general the independence baseline frequencies tend to fit the raw ns more poorly for larger item sets than for smaller item sets . That is , the distribution of values of R will usually be different for pairs than for triples or quadruples or higher way item sets . 3 . EMPIRICAL BAYES ESTIMATATION The empirical Bayes approach smoothes the values of R to allow more reliable estimates for smaller sample sizes . The empirical Bayes process is repeated separately for each size item set . The process for pairs is the same as for triples , quadruples , etc . , so for simplicity we will drop the subscripts in our notation and assume that we have a large set of values ( n , e ) where each observed count n is a draw from a Poisson distribution with its own unknown mean µ=λe . Our goal is to evaluate the posterior distribution of each value of λ = µ/e = E[R ] . This statistical framework , where there are a great many similar estimation problems that can be formulated within a common model , is a natural application of empirical Bayes methodology , sometimes also called hierarchical modeling . See the books [ 7 ] or [ 13 ] for extensive expositions of the methodology . DuMouchel et al [ 9 ] and [ 8 ] introduced the application of empirical Bayes estimation to data mining of large sparse frequency tables in the domains of natural language processing and screening of adverse drug reaction reports , respectively . The latter paper is accompanied by the discussion of two FDA scientists , Drs . Ana Szarfman and Robert O’Neill , who describe the adoption and successful application of the methodology at the FDA . Those two papers considered the analysis of large sparse two dimensional tables , whereas in this paper we extend the methodology to J dimensions , where J is the item set size being considered . This Section provides an overview of the rationale and properties of the empirical Bayes method applied to the market basket problem , while a more formal development is deferred to the Appendix . Empirical Bayes estimation enhances the simple use of the separate values of R = n/e as the estimates for each separate λ by the many values of λ are adding the prior information that connected in that they can be treated as having arisen from a common super population of λ values . The method assumes that the set of unknown λs are distributed according to the continuous parametric density function of a specific form π(λ| θ ) . The parameter vector θ is estimated from the data , the feature that distinguishes empirical Bayes methods from full Bayes methods . The estimation of θ involves fitting the distribution of all the values of R , and once θ has been estimated , the prior distribution of every λ is assumed to be π(λ| θˆ ) . Since this allows us to improve over the simple use of each R as estimates of the corresponding λs , we say that the values of R “ borrow strength ” from each other to improve every estimate . To summarize , the empirical Bayes algorithm involves the following series of assumptions and steps : 1 . A collection of pairs ( n , e ) , where each n > n* , the minimum support .
2 . Assume that conditional on known λ , each n is a Poisson random variable with mean λe .
3 . Assume that for some fixed θ , the λs are distributed according to a family of prior distributions , π(λ| θ ) . As discussed in the Appendix , we use a mixture of two gamma distributions as a convenient five parameter family .
4 . Compute the unconditional distribution of each n as f(n ) = , where Poi( ) is the Poisson density
λθλπλ e
( )
) d
|
∫ ( | nPoi function .
5 . Use the product of all expressions f(n ) in Step 4 to compute the maximum likelihood estimate , θˆ .
6 . For each ( n , e ) use Bayes rule to compute the posterior distribution of λ as
( nPoi
|
θλπλe )ˆ|
(
)
/ f(n ) .
7 . For each λ , use the results of Step 6 to obtain the posterior mean of the empirical Bayes geometric mean of λ . Αlso obtain λ.05 , the log(λ ) and exponentiate it to obtain Λ ,
5th percentile of the posterior distribution of λ , for use as a Bayesian lower confidence limit .
Although the above procedure may seem complicated , as discussed in the Appendix , the amount of computation involved is small compared to the computation required just to tabulate the item set frequencies n . The resulting estimates Λ and lower confidence limits λ.05 are much more reliable than R when n and e are not large , and they will be very close to R when n and e are large . [ 8 , Tables 4 and 6 ] compared three methods of ranking collections of pairs ( n , e ) ( in the adverse drug reaction domain ) : based on the value of R , based on the value of Λ , and based on the significance level of the hypothesis test that n ~ Poisson(e ) . The highest ranking pairs according to R had n = 1 or 2 but tiny e , such as ( n = 1 , e = .0003 , R = 3300 , Λ = 24 ) The highestranking pairs according to statistical significance had huge n , but not very large values of R , such as ( n = 7500 , e = 500 , R = 15 , Λ = 15 ) . The highest ranking pairs according to the empirical Bayes geometric mean were a compromise between these extremes , such as ( n = 300 , e = .2 , R = 1500 , Λ = 1200 ) . Ranking item sets according to their significance levels of an hypothesis test of independence tends to give too much weight to highest frequency item sets that may have only moderately high interest measures λ . We believe that in most domains , ( n = 300 , e = 0.2 ) is more “ interesting ” than is ( n = 7500 , e = 500 ) . Sorting item sets of each size by the largest values of Λ or λ.05 is one recommended way to prioritize the examination of the modeling results . This assumes that the ratio parameter λ is of primary interest , and in our experience that assumption is reasonable when the main objective is to uncover causal relationships among the items . On the other hand , in many business applications the discovery value of an association may be roughly proportional the corresponding item set occurs in excess of that predicted by independence . In such a situation , we recommend sorting the item sets by the variable EXCESS , where to the number of times that
EXCESS = e × ( λ.05 – 1 ) .
( 1 ) EXCESS is interpreted as a conservative ( 95 % lower confidence limit ) estimate of the number of times that the item set occurred in excess of the level of pure noise . 4 . MULTI ITEM ASSOCIATION VERSUS
PAIRWISE ASSOCIATION
For pairs of items , the interpretation of association , or deviation from independence , is straightforward , since there is only one degree of freedom for dependence and the single correlation between the two items is a familiar concept . For larger item sets the situation is more complicated . Contributing to the mutual dependence of J items , there are 2J – J – 1 degrees of freedom , which can be partitioned into J(J – 1)/2 pairwise correlations and the remaining 2J – J(J + 1)/2 – 1 degrees of freedom for higherorder association . For triples , there are 3 pairwise associations and 1 degree of freedom for 3 factor interaction . For quadruples , there are 6 pairwise associations and 5 degrees of freedom for 3and 4 factor the frequency of a triple or quadruple is much greater than independence predicts , what actually has been found ? Suppose a triple ABC is unusually frequent . Is that just because AB and/or interactions . When an analyst finds that three occur
AC and/or BC are unusually frequent , or is there something the triple that all special about frequently in transactions ? If the former , then the mining of triples hasn’t really found anything that couldn’t have been deduced from the results of examining pairs . If the latter , further investigation may uncover an important new insight into the domain . For example , in the medical domain , suppose in a database of patient adverse drug reactions , A and B are two drugs , and C is the occurrence of kidney failure . In case 1 , A and B may act independently upon the kidney , but since A and B are sometimes prescribed together there may be many occurrences of ABC . In case 2 , A and B may have no effect on the kidney if taken alone , but when taken together a drug interaction occurs that often leads to kidney It would be valuable to be able to recognize case 2 failure . situations automatically . In general , it would be quite valuable to automatically pick out the multi item associations that cannot be explained by the pairwise associations in the item set . After all , there may be tens of thousands of supposedly interesting triples and quadruples to investigate after performing a market basket analysis , and it would be nice to know which of them can be discarded as “ nothing new ” once you have completed the analysis of pairs . We will address this problem using the standard statistical theory of log linear models as treated in textbooks like [ 4 ] or [ 5 ] . Consider frequency table determined by the joint distribution of any J>2 columns of X . The counts in the table are denoted , where each j1 , … , jJ is either 0 or 1 . The J variables are said to obey the “ all two factor ” model if the expected frequencies of the array of counts are of the form the 2J jjm 21
Jj
E[ jjm 21
Jj
] = j ab 1 1 j b 2
2
j cb J
J jj 21 12 jj c 31 13 j 32 j c 23
c j J
J j −− 1 J ,1 J
( 2 ) is the canonical model the all two factor model
The model parameters are a , b1 , … , bJ , c12 , … , cJ 1,J . Thus , for the item set defined by these J items , the all two factor model predicts that E[m00…0 ] = a transactions contain none of the J items , and that E[m11…1 ] = ab1…bJ c12…cJ 1,J transactions contain the complete item set . Log linear model theory states that for determining a joint distribution from all the two way marginal distributions without adding any more complex information . For example , the sufficient statistics for fitting the all two factor model are the J×(J–1)/2 2×2 tables of counts from the two way marginals . The fitted values ( 2 ) are easily computed using the iterative proportional fitting algorithm [ 4 ] , which is guaranteed to converge . For any item set of size J > 2 , whose frequency has been computed as n = m11…1 > n* , the frequencies of smaller included item sets will be already counted and available to supply the statistics necessary to fit the all two factor model and therefore its value of E[m11…1 ] . This leads to the following definitions for each item set of size greater than 2 : eAll2F = E[m11…1 ] = predicted count of all two factor model ( 3 ) based on all two way distributions
EXCESS2 = Λ×e – eAll2F ( 4 ) EXCESS2 is an estimate of transactions containing the item set over and above those that can be explained by the pairwise associations of the items in the item set . Although EXCESS2 could be negative , we hesitate interpreting such item sets , focusing mainly on large values of the number of
EXCESS2 as possibly indicating complex relationships involving more than pairwise association among the items of the item set . Note that in computing ( 3 ) , rather than use the raw ( and possibly unstable ) two way counts to fit the all two factor model , we use the shrinkage estimates of two way counts based on substituting µjk = Λjk×ejk for the raw two way count njk , for every pair of items ( j , k ) contained in the item set . The four elements of the 2×2 table used are thus µjk , nj – µjk , nk – µjk , and N – nj – nk + µjk . 5 . EXAMPLE : INTERNATIONAL
CALLING BEHAVIOR
In this section we apply the ideas to an example drawn from telecommunications . We do not pursue a complete analysis of these data . Rather we highlight those aspects of the analysis that differentiates our work from that reported in the literature . In support of security operations , AT&T maintains a calling signature for all accounts that make international calls . This signature contains features of calling behavior such as estimated calling rate ( eg , calls/week ) , call time ( eg , time of day and day of week ) , call duration , and call terminations ( eg , countries called ) that serve as a baseline for detecting anomalous activity . We focus on the call terminations component of the signature that consists of a list of the top 5 recently and frequently called countries for each account . This list evolves through time using a probabilistic bumping algorithm , so that while the list itself has a fixed length , the labels and associated frequencies of those labels change as calls are made . The complete item set consists of the 228 countries where AT&T terminates international calls . We generate "market baskets" by taking monthly snapshots of the top 5 country list for each account for three consecutive months . Note that on average , our accounts call only two different countries per month so that for most accounts there are fewer than 5 countries ( items ) in their monthly "market basket" . There are about 7 million accounts regularly make international calls . These accounts are stratified along several dimensions : account type : business/ residence/ casual(unknown ) line type : multi line/ single line usage category : low/ medium/ high These account descriptors define 18 strata corresponding to the unique combinations of their levels . The number of accounts in each stratum varies from a low of a few hundred to a high of 2 million . the communication behavior is known to be different across these variables . In searching for interesting country combinations , we would not want to highlight country combinations that were interesting solely due to the individual countries’ popularity in the same strata . Our data set consists of over 20 million transactions ( 7M/month × 3 months ) . We chose the minimum support size of n* = 4 , and by construction , we only examined item sets up to size J = 5 . Each of the three monthly analyses required approximately 52 minutes of CPU time on a Pentium 750 MHz processor . Almost all of this time was spent generating the collection of item sets satisfying the minimum support requirement ( including about 16,000 pairs , 120,000 triples , 150,000 4 tuples and 29,000 5 to stratify the data is desirable that
It since posterior distributions , tuples ) . About 3 of the 52 CPU minutes were devoted to the statistical computations , namely computing the maximum likelihood estimates of θ and the statistics associated with the ( empirical Bayes ) including the interestingness measures Λ , λ.05 , EXCESS , and EXCESS2 . Minimum support size . A common rule of thumb for specifying the minimum support size is to use 0.1 % of the number of transactions . Since each month has approximately 7 million transactions , this would limit the analysis to item sets with frequencies of 7,000 or greater . Applying this rule to item sets of size one ( ie , single countries ) would eliminate over half ( 120 of 228 ) of the countries from the analysis . For applications in fraud detection and targeted marketing , this is clearly far too large .
Figure 1 . Shading of Country pairs based on raw counts n .
Figure 2 . Shading of Country pairs based on empirical Bayes lower limit λλλλ05050505 Figure 1 displays the support for item sets of size 2 ( ie , country pairs ) . The layout of the countries along the rows and columns roughly corresponds to geography . It was determined by a cluster analysis using an inter country distance matrix based on an average Λ over the three months . We employ a gray scale to denote country pairs satisfying minimum support at four levels : white : n < 10 dark gray : 100 < n < 1000 light gray : 10 < n < 100 black : 1000 < n
Only 1,633 of the 25,878 ( =228*227/2 ) possible country pairs have support n >1000 . The situation gets more extreme as the size of the item sets increases . Lowering the minimum support requirement to n*=4 leads to 66 % of the country pairs being eligible for the subsequent analysis . Shrinkage . Our proposed analysis focuses on estimates of the ratios of observed to baseline cell Figure 2 displays the conservative estimate λ.05 averaged over the three successive months . The gray scale corresponds to increasing frequencies .
λ.05 , with lightest gray corresponding to 2≤ λ.05< 4 and the darkest to λ.05 > 16 . White locations have either n < 4 or λ.05 < 2 . Just over 8 % of the country pairs are shaded ; this means that in these cells , our lower confidence limit has at least twice as many accounts calling this pair of countries than we would expect under called independently of each other . An interesting aspect of Figure 2 is that λ.05 is greatest for country pairs in the same part of the world — this is intuitively reasonable . assumption that the countries are
1000 2000
5000 10000
30000
100000
300000
1000000
Estimates of Lamda gabonmrtnasengl(11 ) gabonivcstmali(13 ) ivcstmalitogo(14 ) gnbsuniuestome(17 ) cpvdegnbsuprtgl(18 ) amrmasamoatonga(21 ) fijisamoatonga(23 ) djbtikenyasomla(25 ) kyrgzrussauzbek(28 ) beninghanatogo(33 ) ivcstlibrasileo(38 ) ghanaivcsttogo(39 ) albnamnacoserba(45 ) ghanaguinasileo(55 ) armnaazrbjrussa(63 ) marshmcrsapalau(73 ) ertraethpasudan(89 ) fijinewzdtonga(123 ) gmbiaguinasileo(172 ) amrmanewzdsamoa(250 ) dmncagrndamntrt(746 )
L L L
M M M L L
L
L
M
M M L L M R M L
L
R
R
R
R R
R R
M M
R
R
M
R
L L
M M
R R L
L
M
R
R M L
M
R
L
M
R
L
M R
L
M
M L
R
R
M L
R
1000 2000
5000 10000
30000
100000
300000
1000000
M : PostGeomMean
L : Lower05Limit
R : Observed R with 99.9 % Classical Conf . Int .
Figure 3 . Shrinkage for some month 2 triples having very large λλλλ05050505 and a wide range of n ( in parentheses )
Figure 3 displays a comparison of the raw value of R = n/e with the empirical Bayes estimates λ.05 and Λ for some of the month 2 triples that have very large λ05 The triple of countries and the observed n of the triple are listed on the vertical axis . For each triple , the graph shows a classical 99.9 % confidence interval for λ , based on the assumption that n is distributed Poisson(λe ) , as well as the two shrinkage estimators . The triples are ordered in increasing n from top to bottom of the graph , making it easy to see that for small n the shrinkage is more drastic than for larger n . One exception to this rule is the triple “ marshmcrsapalau ” having ( n = 73 , e = 0.0002 ) , which exhibits drastic shrinkage even though n is fairly large . This happens because the Bayesian estimate tends to shrink large R when e is very small , even if n is moderately large . One way to ascertain the significance of individual measures when looking at a large collection is to use a quantile quantile plot . This technique is standard in the statistics community as it allows one to visually calibrate a collection of measures that are all drawn from a common distribution . The intuition is that any sample drawn from a common distribution will have a smallest and a largest value — the question is "How big is big?" . The quantile quantile plot attempts to answer this question by displaying the quantiles ( eg , ordered values ) of the observed measure against the quantiles of the assumed distribution of the measure . If the assumed distribution is correct , apart from random variation , the configuration of points should follow a straight line . Systematic departures from linearity indicate that the assumed distribution is not correct — common anomalies being systematic curvature or outliers at the extremes . Figure 4 is an example of a "standard normal quantile" plot , socalled because the assumed distribution is a normal ( ie , Gaussian ) distribution with mean zero and unit standard deviation . In it we display the ordered values of the standardized change in the Λ measure ( also referred to as “ EBGM ” ) from month one to month two . ( Because our methods yield a posterior variance for every estimated log λ , we can compute approximately standard normal test statistics for comparing the ratios of corresponding λs from one month to the next . ) Mild the distribution of this systematic curvature suggests that measure is not Gaussian . The more relevant feature for this discussion however is the suggestion of a few outliers ( labeled in the figure ) . We can provide a partial interpretation for the two extreme country pairs ( St Kitts , Vanuatu ) and ( Niue , Vanuatu ) . All three countries are islands that host "adult entertainment" telecommunication services . The providers of these services earn commissions from the state owned phone company for the traffic that they attract . In the case of ( Niue , Vanuatu ) there appears to have been a drop off in the relative rate of accounts calling both of these countries in month two . The opposite is true for ( St Kitts , Vanuatu ) where there appears to be a surge in the relative rate of accounts that are calling both of these geographically dispersed islands . Indeed , callers are often referred to other "hot lines" and callers unaware of international dialing codes have little idea on where the calls terminate — until they receive their bill . This analysis provided us with our first indication that the adult entertainment business model is making a significant entry into the Caribbean area . stkit.vantu
2 M G B E = 1 M G B E r o f c i t s i t t a S t s e T f o s e l i t n a u Q
0 1
5
0
5 cnada.stkit niue.vantu
4
2 Quantiles of a Standard Normal
0
2
4
Figure 4 . QQ plot for test statistics comparing each pair ’s ΛΛΛΛ from month 1 to month 2 . the items , or if it
Multi item versus pairwise association . The interpretation of "interesting" large item sets can be confusing since it is often unclear whether the item set is interesting because it contains all is interesting because it consists of interesting subsets of items . Our models allow these alternatives to be compared by fitting all two factor log linear models and comparing the fits to the shrinkage estimates . Figure 5 displays the different measures of excess frequency of 4 tuples that result depending on whether or not we adjust for pairwise association . The figure displays the excess frequency for item sets of size four depending on whether comparison is to a complete independence model ( abscissa = EXCESS ) or the all two factor model ( ordinate = EXCESS2 ) . Figure 5 includes all 7,718 4 tuples in month 1 where both EXCESS and EXCESS2 are nonnegative . Several of the points in Figure 5 have been identified with their country names and the observed n . We comment on a few of them : taken into are account , interactions
( Austria , Germany , Mexico , UK ) : This 4 tuple is unusual in that the independence model suggests that this item set is not so interesting ( EXCESS almost 0 ) , but when the pairwise the interestingness of the 4 tuple is increased ( EXCESS2 > 600 ) . Since some of the pairwise associations are negative , the all two factor model predicts that this 4 tuple would occur somewhat less than independence predicts . ( Australia , Canada , India , UK ) : This 4 tuple has about 3600 more accounts calling them than independence predicts , but when pairwise interactions are taken into account , this 4 tuple has just about the frequency that we would expect . ( China , Hong Kong , Mexico , Taiwan ) : The 1585 accounts calling these four countries are far in excess of what either the independence model or the all two factor model predicts . are drawn from statistical
6 . DISCUSSION We introduced three variations on the market basket problem that considerations . First we introduced stratification of transactions by features that are known to correlate with item sets . Second we dramatically reduced the recommended minimal support size for item sets by introducing an empirical Bayes model that effectively takes into account variation associated with small frequencies . And finally we built on earlier work that considers interestingness measures that assess departures of observed frequencies from baseline frequencies . For item sets of size two , the independence model provides a natural baseline , but we argue that in larger item sets interpretations are sometimes aided by comparisons with fitted frequencies from other models , especially the log linear model specifying all two factor interactions . In every case , our methods provide for a reliable ranking of item sets by "interestingness." Given an interesting item set , our methods do not explicitly choose a predictive association rule . We prefer to let a domain expert make judgments about potential cause and effect or directional association . For example , our discovery that adult entertainment lines had spread to St . Kitts in the Caribbean was easy to verify once the outlier in Figure 4 was presented to someone familiar with the situation in Vanuatu . We deliberately did not focus on algorithmic issues . In our experience the computing time to enumerate item sets and compute interestingness measures is negligible compared to the hours of analysis time spent perusing the voluminous output . We have developed and continue to develop visualization techniques to aid this phase of the problem . The figures in this paper are examples of some of the static displays that we feel are valuable . Yet the real power of visualization lies in interactive approaches , especially in high dimensions ( ie , large item sets ) . We are currently exploring such techniques and hope to report on this research at KDD2002 .
0 0 0 1
0 0 8
0 0 6
0 0 4
0 0 2
0 l e d o M r o t c a F 2 l l
A r e v o
2 s s e c x E chinahngngmexcotaiwn:1585 cnadagrmnyindiauk:4760 cnadaelsdrgtmlamexco:947 astragrmnymexcouk:1009 chinacnadajapantaiwn:1964 cnadapkstnsarabuk:3550 astlacnadaindiauk:4812
0
1000
2000
3000
Excess Over Independence
Figure 5 . Excess vs . Excess2 for all 4 tuples where both are nonnegative ( observed n listed after “ : ” ) .
7 . REFERENCES [ 1 ] Aggarwal CC , Yu PS ( 1998 ) A new framework for item set generation . Proc . of ACM PODS Symposium on Principles of Database Systems , Seattle , WA , pp . 18 24 .
[ 2 ] Agrawal R , Imilienski T , Swami A ( 1993 ) Mining association rules between sets of items in large databases . Proc . ACM SIGMOD Intl . Conf . On Mgnt . of Data , pp . 207 216 .
[ 3 ] Agrawal R , Srikant S ( 1994 ) Fast algorithms for mining association rules . In Proc . 20th VLDB Conf , Santiago , Chile .
[ 4 ] Agresti A ( 1990 ) Categorical Data Analysis . New York :
John Wiley .
[ 5 ] Bishop YMM , Fienberg SE , Holland PW ( 1975 ) Discrete
Multivariate Analysis Cambridge , MA : MIT Press .
[ 6 ] Brin S , Motwani R , Ullman JD , Tsur S ( 1997 ) Dynamic item set counting and implication rules for market basket data . Proc . ACM SIGMOD 1997 Intl . Conf . on Mgnt . of Data , pp . 255 264 .
[ 7 ] Bryck A , Raudenbush S ( 1992 ) Hierarchical Linear
Models . Newbury Park , CA : Sage Publications .
[ 8 ] DuMouchel W ( 1999 ) Bayesian data mining in large frequency tables , with an application to the FDA Spontaneous Reporting System ( with discussion ) , The American Statistician , 53:177 202 .
[ 9 ] DuMouchel W , Friedman C , Hripcsak G , Johnson S ,
Clayton P ( 1996 ) Two applications of statistical modeling to natural language processing . AI and Statistics V , ch . 39 , edited by D . Fisher and H . Lenz , Springer Verlag .
[ 10 ] DuMouchel W , Volinsky C , Johnson T , Cortes C , Pregibon
D ( 1999 ) Squashing flat files flatter , Proc . KDD 1999 , ACM Press , San Diego , CA , p . 6 15 .
[ 11 ] Johnson N , Kotz S ( 1969 ) Discrete Distributions . Houghton
Mifflin , now distributed by New York : John Wiley .
[ 12 ] Mantel N , Haenszel W ( 1959 ) Statistical aspects of the analysis of data from retrospective studies of disease . J . Natl . Cancer Inst . 22 : 719 748 .
[ 13 ] O’Hagan A ( 1994 ) Kendall ’s Advanced Theory of Statistics , vol . 2 , Bayesian Inference . New York : Halstead Press ( John Wiley ) .
[ 14 ] Silverstein C , Brin S , Motwani R ( 1998 ) Beyond market baskets : generalizing association rules to dependence rules . Data Mining and Knowledge Discovery 2 : 39 68 .
[ 15 ] Simpson EH ( 1951 ) The interpretation of interaction in contingency tables . J . Royal Statistical Soc . , B13:238 241 .
8 . APPENDIX : EMPIRICAL BAYES
SHRINKAGE ESTIMATES
This development follows that of DuMouchel [ 8 ] . Rather than treat the separate values of λ as unrelated constants , assume that each λ is a random variable drawn from a common prior distribution . This distribution is assumed be a mixture of two gamma distributions . The density function of a gamma distribution , having mean = α/β and variance = α/β2
, is g(λ ; α , β ) = βα λα−1 e
−βλ
/ Γ(α )
The prior probability density of λ is assumed to be ) + , β
2 , p ) = p g(λ ; α
1 ( 1 − p ) g(λ ; α
π(λ ; α
1 , α
1 , β
2 , β
, β
1
2
2
)
( A1 )
2
2
1
1
1
2
2
/β
, β
( 1 ) has
The 1 , α density in , β
/β 2 2 + ( 1 – p ) α
1 + ( 1 – p ) α /β )2 + pα /β /β
Therefore the prior mean of λ under this mixture model is pα 2 , and its prior variance is p(1 – p)(α /β 1 1 1 − α 2 . The exact choice of prior distribution for λ is not so important as that it have several free parameters so that the distribution of λ can be fit using observed free parameters : data . θ = ( α triples , quadruples , etc . have separate values of θ . The family of gamma distributions are often used to model populations of Poisson rates because of the conjugate relationship between the Poisson and gamma distributions are simplified in two respects : first , the marginal distribution of each n is a mixture of negative binomial distributions ; and second , the posterior distribution of each λis a mixture of two gamma distributions with modified parameters . Assuming that θ and e are known , then the distribution of n is
The distributions for pairs ,
( [7][11][13] ) . calculations
2 , p ) .
The five
2
Prob(n ) = p f(n ; α
1 f(n ; α , β , e ) = ( 1 + β/e )
, e ) + ( 1 – p ) f(n ; α −n(1 + e/β )
, β
1
−α Γ(α + n ) / Γ(α ) n!
, β
, e ) ,
2
2
( A2 )
Let Qn be the posterior probability that λ came from the first the component of the mixture , given n . formula for Qn is Qn = p f(n ; α
From Bayes rule ,
, e ) / [ p f(n ; α
, e ) +
, β
, β
1
1
1
( A3 ) The posterior distribution of λ , after observing n , can be represented as
2
2
1 ( 1 – p ) f(n ; α
, β
, e ) ]
1 + n , β
λ | n ~ π(λ ; α
1 + e , α
2 + n , β
( A4 ) where π( ) is given by ( A1 ) . Using well known properties of gamma distributions , the posterior expectations of λ and log(λ ) are given by
2 + e , Qn )
E[λ | n ] = Qn ( α
1
+ n)/(β
( 1 – Qn ) ( α 2 E[log(λ ) | n ] = Qn [ ψ(α
+ e ) + 1 + n)/(β + e ) + n ) – log(β
2
1
( A5 )
( 1 – Qn ) [ ψ(α
2
+ n ) – log(β
2 + e ) ]
( A6 )
1 + e ) ] + is the digamma function , where ψ(x ) the derivative of log[Γ(x)].The posterior geometric mean ( denoted EBGM in [ 8 ] and denoted Λ in this paper ) , of λ is then the exponential of ( A6 ) :
Λ = exp(Qn [ ψ(α 1 ( 1 – Qn ) [ ψ(α
2
+ n ) – log(β + n ) – log(β
1 + e ) ] + 2 + e) ] )
( A7 )
This provides a “ best ” point estimate of each ratio λ , in the sense of minimizing squared error loss in log(λ ) . On the other hand , if a conservative estimate of λ is desired , in which the risk of overestimating every λ is constrained to be just 5 % , then we can compute the 5th percentile of the posterior distribution of each λ , denoted λ.05 , as the solution to the equation :
λ 05 . n
1
1
2
λ
;(
, +
, + n
, + e
, + n
) dQe
0.05 = ∫ 0
βαβαλπ 2
( A8 ) where π( ) is given by ( A1 ) . The integral in ( A8 ) is easily computed using standard computer approximations of the incomplete gamma function , while the solution λ.05 requires an iterative technique such as Newton ’s method . If e and n are both large , then Λ and λ.05 will both approach R = n/e , as can be seen from ( A6 ) , noting that for large arguments , ψ(x ) ~ log(x ) , and log(α + n ) – log(β + e ) will approach log(n/e ) . However , when e or n are not large , then the effect of using Λ is to “ shrink ” R toward smaller values , and the shrinkage is even greater for λ05 This is exactly the desired effect when sampling variation makes the true degree of association between the items in the item set uncertain . To evaluate ( A7 ) or ( A8 ) , estimates of θ = ( α 2 , p ) are obtained by considering the marginal distribution of the set of all n for a given item set size , which is given in ( A2 ) . The negative binomial distributions f( ) in ( A2 ) are derived as a mixture of Poisson distributions , where the Poisson means have a gamma distribution [ 11 , p . 125 ] . The likelihood function for θ is the product of mixtures of two negative binomial densities :
1 , α
, β
, β
1
2
L(θ ) = Π{p f(n ; α
, β
1
1
, e ) + ( 1 – p ) f(n ; α
, β
2
2
, e)} ( A9 )
The maximum likelihood estimate of θ is the vector that maximizes ( A9 ) , where the product in ( A9 ) is over all possible item sets of the given size . The maximization involves an iterative search in the five dimensional parameter space , where each iteration involves computing log[L(θ ) ] and its first and second derivatives . Since log[L(θ ) ] would be the sum of an astronomical number of terms if we included all possible item sets ( eg , including item sets that were not observed in the database and have n = 0 ) , we use two modifications of ( A9 ) to allow the procedure to scale up to large values of K , the number of unique items . First , we modify the likelihood to condition on n > n* , where n* is the minimum support that we wish to consider for item sets of the given size . This allows us to merely tabulate and include in the likelihood item sets having n > n* , but , to properly accommodate this , we must replace f(n ; α , β , e ) in ( A9 ) by the conditional density of n , given that n > n* , namely
;( nf *
βα ,
,
, ne
* )
=
;( nf
βα
,
1/[ ) , e
−
− 1* n ∑ = 0 m
; ( mf
βα
,
] ) , e
( A10 )
Even choosing n* = 1 saves an enormous amount of computation compared to enumerating all possible item sets and computing the corresponding values of e and f( ) . As mentioned in the previous sections , we are focusing on applications where moderate values of n are of interest and so we take n* to be at least 1 but as small as computational resources allow . If n* is as large as several hundred , the noise in the counts for n > n* will typically be so little that the gain from computing Λ or λ.05 may be small . In order to further reduce the computational burden of the iterative search to maximize L(θ ) , we further reduce the number of terms in ( A9 ) by the technique of data squashing [ 10 ] . The collection of all item set descriptors ( n , e ) , where n > n* , is replaced by a smaller set of M triples ( ni , ei , wi ) , i = 1 , … , M . For each unique observed value of n , a set of values of ( ni = n , ei , wi ) are chosen so that the distribution of the ei weighted by the corresponding wi mimics the distribution of the original values of e for that value of n . ( A simple version of data squashing is to partition the values of e into bins for each unique value of n and replace the set of ( n , e ) in each bin by the single triple ( ni = n , ei , wi ) , where ei is the bin mean and wi is the bin count of the ith bin . ) This reduces the expression for L(θ ) to just M terms , namely : θ )(
−+ 1(
( ) nfp
* ) ]
* ) pf
L
βα ; ,2
= ∏ =
βα 1 ,1 ne i ne i n iw
(
;
,
,
,
,
*
2
* i i
M [ 1 i
1 = .1 , β
1 = .1 , α
2 = 10 , β
In our experience where M is a few thousand , the maximization typically takes between 5 and 15 iterations from the starting point θ0 = ( α 2 = 10 , p = pmax ) , where pmax is the value from among ( 0.2 , 0.5 , 0.8 ) having the largest value of L(θ0 ) . [ These starting values are used for item pairs , J = 2 . Larger values of J tend to lead to more dispersed distributions of λ , so we divide the above starting values of the αs and βs by 3J−2 . ] This experience also indicates that the computational effort of tabulating all the item sets having n > n* , which uses an algorithm similar to the Apriori algorithm [ 2][3 ] involving one
Thus pass over the transaction database for each item set size , is a much greater computational burden than the empirical Bayes calculations described in this Appendix and the log linear model calculations discussed in Section 4 . there is no computation related reason not to compute Λ , λ.05 , EXCESS and EXCESS2 routinely . Table 1 shows the estimates and standard errors of the hyperparameters θ = ( α1 , β1 , α2 , β2 , p ) for each month and for the item set sizes 2–5 . Standard errors are based on the second derivatives of the log likelihood function . It can be seen that the parameter values are quite consistent across databases , but that the values of α and β tend to decrease as the size of the item set increases . This indicates that the distribution of λ becomes more and more dispersed as the dimension goes up , with more outlying item sets having very large frequencies compared to the baseline frequency based on the independence assumption . This also shows that the optimum amount of shrinkage varies depending on item set size . We have not performed extensive goodness of fit analyses to the prior distribution family used here . One approach is to fit different strata separately and see if they lead to similar sets of hyper parameters . If not , it is easy to combine results after separate fits to different subsets of the database or to different subsets of item sets . Classical hypothesis tests for goodness of fit to the observed distribution of ( n , e ) are possible but may not be useful , since massive data sets are bound to yield small "pvalues" for rejecting almost any null hypothesis , even though the computed shrinkage estimates perform well as interestingness measures .
3
3
3
2
2
2 p
Dec
Nov
Oct
Nov
Dec
Oct
1 p
Beta2
Beta1
0.2067 0.005411 0.2068 0.005448 0.2051 0.005536 0.1806 0.001951 0.1803 0.001978 0.1787
Table 1 . Hyper parameter estimates and standard errors for the three months for dimensions 2 through 5 . Month Dim Parameter Alpha1 0.03935 0.1834 0.001915 0.01574 0.03962 0.1845 0.001933 0.01594 0.04174 0.1917 0.002039 0.01643 0.004768 0.001012 0.000111 0.000193 0.001062 0.005027 0.0002021 0.000118 0.001147 0.005523 0.0001328 0.002002 0.000218 0.001037 0.000688 0.000134 0.000026 0.0008258 0.001493 0.0001558 0.000036 0.0008782 0.001534 0.0001669 0.000040 0.000291 0.000258 0.000026 0.000075 0.001313 0.000971 0.0003172 0.000069 0.00174 0.001474 0.0004084 0.0001061 0.003155
Alpha2 2.843 0.06065 2.857 0.06153 2.792 0.06052 1.061 0.0135 1.059 0.0136 1.082 0.01381 0.02398 0.003049 0.03096 0.003939 0.02563 0.003259 0.02204 0.002775 0.02604 0.003313 0.02851 0.003656
4.275 0.1051 4.219 0.1047 4.109 0.1027 2.075 0.02612 2.044 0.02595 2.076 0.02616 0.8088 0.007799 0.8202 0.008709 0.7966 0.007906 1.227 0.02236 1.309 0.02427 1.322 0.02539
Estimate St.Error Estimate St.Error Estimate St.Error Estimate St.Error Estimate St.Error Estimate St.Error Estimate St.Error Estimate St.Error Estimate St.Error Estimate St.Error Estimate St.Error Estimate St.Error
0.7933 0.005411 0.7932 0.005448 0.7949 0.005536 0.8194 0.001951 0.8197 0.001978 0.8213 0.002002 0.8784 0.001316 0.873 0.001396 0.8782 0.001397 0.8836 0.002687 0.8693 0.002942 0.8644 0.003155
0.1216 0.001316 0.127 0.001396 0.1218 0.001397 0.1164 0.002687 0.1307 0.002942 0.1356
Dec
Oct
Dec
Nov
Oct
Nov
5
4
4
4
5
5
