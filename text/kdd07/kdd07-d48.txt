Exploiting Duality in Summarization with Deterministic
Guarantees∗
Panagiotis Karras University of Hong Kong
Pokfulam Road , Hong Kong , China pkarras@cshkuhk
Dimitris Sacharidis
National Technical University of Athens
Zographou , Athens , Greece dsachar@dblabntuagr
Nikos Mamoulis University of Hong Kong
Pokfulam Road , Hong Kong , China nikos@cshkuhk
ABSTRACT Summarization is an important task in data mining . A major challenge over the past years has been the efficient construction of fixed space synopses that provide a deterministic quality guarantee , often expressed in terms of a maximum error metric . Histograms and several hierarchical techniques have been proposed for this problem . However , their time and/or space complexities remain impractically high and depend not only on the data set size n , but also on the space budget B . These handicaps stem from a requirement to tabulate all allocations of synopsis space to different regions of the data . In this paper we develop an alternative methodology that dispels these deficiencies , thanks to a fruitful application of the solution to the dual problem : given a maximum allowed error , determine the minimum space synopsis that achieves it . Compared to the state of the art , our histogram construction algorithm reduces time complexity by ( at least ) a B log factor and our hierlog archical synopsis algorithm reduces the complexity by ( at least ) a +log n in time and B(1− log B log n ) in space , where ∗ factor of is the optimal error . These complexity advantages offer both a spaceefficiency and a scalability that previous approaches lacked . We verify the benefits of our approach in practice by experimentation . Categories and Subject Descriptors F.2 [ Analysis of Algorithms and Complexity ] : Miscellaneous ; H.3 [ Information Storage and Retrieval ] : Miscellaneous ; H24 [ Database Management ] : Systems—Query processing General Terms Algorithms , Experimentation , Theory , Performance Keywords efficiency , histograms , synopses , wavelets log ∗ log
2 ∗ n
2
B
∗ Work supported by grant 7160/05E from Hong Kong RGC and by project PENED 2003 in the 3rd Community Support Programme .
1 Introduction
The need to reduce a very large data set into a compact representation or synopsis that captures its basic characteristics arises often ; it finds application in OLAP/DSS systems [ 28 ] , approximate query answering [ 26 , 3 ] , cost based query optimization [ 24 ] , time series indexing [ 4 ] , data mining [ 23 ] , data stream approximation [ 2 , 5 ] and the efficient handling of multi measure [ 6 ] and multidimensional data sets [ 18 ] . Diverse synopsis data structures have been proposed [ 9 ] ; the goal with all of them is to minimize an appropriate error metric over the original data in a given space budget . Past research has led the way from conventional synopsis techniques such as histograms [ 15 , 17 , 26 , 14 , 11 , 2 ] and Haar wavelets [ 24 , 28 , 3 , 7 , 8 , 20 , 11 , 12 , 13 , 6 ] to more sophisticated ones such as compact hierarchical histograms [ 27 ] and the Haar+ tree [ 21 ] . A general optimal histogram construction algorithm was presented by Jagadish et al . [ 17 ] and later specialized by Guha et al . [ 14 ] for the case of maximum error metrics . The practical usefulness of this class of error metrics has spawned focused attention to them in recent studies , all based on hierarchical synopsis structures . A dynamic programming algorithm that derives the optimal Haar wavelet synopsis for a maximum error metric ( as opposed to the computationally easier Euclidean error ) was developed by Garofalakis and Kumar [ 8 ] and optimized in terms of space and time by Guha [ 11 ] . Later , Guha and Harb [ 12 , 13 ] improved on the robustness of the restricted Haar wavelet synopsis model of [ 8 , 11 ] . In their approach , wavelet coefficient values in the synopsis are arbitrary , and differ from those in the wavelet transform of the data ; their approximation scheme for unrestricted Haar wavelet synopses achieves both higher accuracy of approximation and better asymptotic behavior in time than the restricted model . Recent research has created less restrictive hierarchical synopsis data structures in two independent routes [ 27 , 21 ] . The Haar+ tree [ 21 ] goes further , in terms of flexibility , than the unrestricted Haar wavelet model , by enhancing the structure itself . The Compact Hierarchical Histogram ( CHH ) was independently introduced in [ 27 ] ; as we observe in this paper , it is a special case of a Haar+ tree . In other words , a Haar+ structure merges the unrestricted Haar wavelet and the CHH models . Both [ 27 ] and [ 21 ] experimentally demonstrate that the structures they propose can , in certain circumstances , achieve higher quality of approximation than the optimal histogram of [ 17 , 14 ] . Despite this progress , the complexities of all summarization algorithms for maximum error metrics are still inefficiently high . The main reason for this defect is their dependence on the given synopsis space budget B , due to a requirement to tabulate possible allocations of space to different data intervals ; the problem is gravest in the models of [ 12 , 13 , 21 ] , due to their two dimensional tabulation over both space and candidate approximation values . An effort to tame these space complexities [ 11 ] did not manage to eradicate their dependence on B ; besides , as we show , it creates an unwieldy tradeoff between time and space efficiency in the case of maximum error metrics . In this paper we eliminate these shortcomings with an alternative approach , based on a lucrative application of the solution to the dual , error bounded problem : detect a spaceoptimal synopsis under an error bound . Our solutions do not tabulate over B and do not present performance tradeoffs . Compared to the state of the art , in histogram construction we reduce the time complexity from ( at least ) O(nB log2 n ) to O(n log ∗ ) ; in hierarchical synopsis construction , we reduce the complexity from ( at least ) O(R2n log2 B ) to O(R2n(log ∗ + log n ) ) in time and from ( at least ) O(RB log n is the optimal error and R the cardinality of an examined value set . We experimentally verify the practical implications of this reduction .
B ) to O(R log n+n ) in space , where ∗
2 Background and Related Work In this section we briefly present previous approaches to offline data reduction with a maximum error deterministic guarantee . We consider the principal synopsis structures employed , namely plain histograms and hierarchical representations . Under both approaches , given an n size data vector D = d0 , d1 , . . . , dn−1 . , the problem is to devise an approximate representation ˆD of D using at most B space , so that a given error metric in the approximation is minimized . Maximum error metrics are most generally expressed in their weighted version :
Lw∞ ˆD , D = max i nwi| ˆdi − di|o ,
1 where ˆdi denotes the reconstructed value for di and wi denotes a weight for the corresponding error value ; in the case of the maximum relative error ( MRE ) , it is wi = max{|di|,S} , where S > 0 is a sanity bound that prevents small values from unnaturally dominating the error result [ 8 ] . In the case that ∀i , wi = 1 , the error metric at hand is the maximum absolute error ( MAE ) . Previous studies [ 17 , 8 , 12 , 13 , 27 , 21 ] have generalized their results into wider classes of distributive and Minkowski distance metrics . Still , the sub class of maximum error metrics remains more practically interesting than the esoteric metrics of those classes [ 7 ] .
2.1 Histogram based Data Reduction
A histogram synopsis ( also called segmentation or partitioning ) divides D into B ' n successive disjoint intervals [ bi , ei ] , 1 ≤ i ≤ B called buckets or segments , and attributes a single value vi to each of them that approximates all consecutive values therein , dj , j ∈ [ bi , ei ] . A single bucket ( segment ) can be expressed by the triad si = {bi , ei , vi} . Given a target error metric , the best value for vi is defined as a function of the data values in [ bi , ei].1 2B − 1 numbers suffice to represent a B bucket histogram ( since ∀i , 1 < i ≤ B , bi = ei−1 + 1 and the edges are fixed ) . Initial work on histograms focused on heuristics [ 16 ] . An O(n2B ) dynamic programming algorithm that assigns optimal bucket boundaries for the Euclidean ( L2 ) error metric ( O(n3B ) for other metrics ) was presented2 in [ 17 ] . The basic idea behind it is that the b optimal histogram for D can be recursively derived from the space of ( b−1)optimal partitionings of prefix vectors of D . For a maximum error
1For the Euclidean error , the optimal vi is the mean of the values in [ bi , ei ] [ 17 ] ; for MAE it is the mean of maximum and minimum values in [ bi , ei ] , while for MRE a case analysis is given in [ 14 ] . 2Since this problem is a special case of the problem of approximating a curve by line segments , the solution of [ 17 ] is a special case of the algorithm introduced in [ 1 ] .
E(i , b ) = min 1≤j<i metric , the minimal error E(i , b ) of a b bucket histogram of the prefix vector d0 , d1 , . . . , di . is recursively expressed as : {max{E(j , b − 1),E(j + 1 , i)}}
( 1 ) where E(j+1 , i ) measures the minimal maximum error for a bucket that contains the items dj+1 , . . . , di The resulting algorithm requires an O(nB ) tabulation of minimized error values E(i , b ) and chosen last bucket boundaries j corresponding to those optimal error values . Guha et al . [ 14 ] proposed a specialization of the general purpose algorithm of [ 17 ] for ( among others ) MRE ( applicable to any maximum error metric ) . The crucial observation is that , in order to determine the j that minimizes the max function in Equation 1 , it suffices to perform a binary search , since E(j , b− 1 ) and E(j + 1 , i ) are monotonic functions of j . [ 14 ] employs an interval tree to determine the minimum error for a bucket in logarithmic time . Hence this algorithm requires O(nB log2 n ) time and O(nB ) space . Table 1 summarizes the complexity results of previous work on the offline one dimensional histogram construction problem for maximum error metrics and introduces the complexity of the solution3 we propose ; B is the space bound expressed as the number of buckets and ∗ is the optimal error . The space efficient variant of the algorithm in [ 14 ] is discussed in Section 3 .
Algorithm Type time efficient space efficient
Reference
[ 17 ] [ 14 ]
[ 14 , 11 ] This work
Time
O(n3B )
Space O(nB ) O(nB log2 n ) O(nB ) O(n ) O(nB log3 n ) O(n log ∗ O(n ) )
Summary of results for optimal offline oneTable 1 : dimensional histogram construction ( maximum error metrics ) 2.2 Hierarchical Data Reduction
Another stream of research has been based on index structures that represent the data in consecutive hierarchical levels of detail . This approach started with the application of the Haar wavelet decomposition , long used in signal processing [ 19 ] . Most recently , two independent , yet interrelated structures employing a hierarchy have been introduced [ 21 , 27 ] . We now review this research . The Haar Wavelet Hierarchy . The Haar wavelet hierarchy can be visualized through a complete binary tree , the Haar tree . The coefficient in the Haar tree root node contains the overall average value and each other coefficient value ci contributes the value +ci to all data values ( leaves ) in its left sub tree and −ci to those in its right sub tree . Hence each original data value is reconstructed by adding/subtracting the coefficients in the path towards its position . Figure 1a depicts the Haar decomposition of an example data vector D of 8 values ( shown at the leaves of the tree ) . Value d3 = −6 can be reconstructed as +c0 + c1 − c2 + c5 . A Haar wavelet synopsis of D is a vector ˆZ of B ' n non zero i , ci . terms , such that its inverse wavelet transform ˆD = W−1(ˆZ ) approximates the data vector D . Figure 1b shows a { 0 , 4, 3,−2 4 , 6 5,−7} synopsis for the data array of Figure 1a , with maximum absolute error 4 . This is the optimal MAE synopsis with B = 4 . For the Euclidean error ( L2 ) , the optimal Haar wavelet synopsis consists of the top B normalized coefficients of the complete Haar wavelet trans|c|√ form [ 19 ] ; the normalized value of a coefficient c is 2 , where . 3After this work was submitted for publication , [ 2 ] proposed an O(n + n log U ) algorithm for offline histogram summarization , where U is the size of the domain for data values ; as U can be arbitrarily large , our solution retains its competitiveness towards that algorithm too . log n B is the level where c resides in the Haar tree . For example , the L2optimal synopsis , with B = 2 , for the data vector in Figure 1a is { 0 , 4, 5,−7} This computational convenience has allowed for the extension of the L2 synopsis methodology to various settings [ 10 , 18 , 5 , 6 ] . On the other hand , the problem is computationally harder for maximum error metrics . Restricted Synopses for Maximum Error Metrics . After its identification in [ 24 ] , the first systematic treatment of the space bounded Haar wavelet synopsis problem for maximum error metrics was based on a randomized rounding scheme [ 7 ] . However , as shown in [ 14 ] and [ 8 ] , this scheme does not produce results of high quality . Garofalakis and Kumar [ 8 ] suggested a dynamic programming ( DP ) scheme that deterministically retains the optimal coefficient subset of a dataset ’s Haar wavelet transform . [ 20 ] proposed a streaming capable and reliable greedy counterpart to this solution . Muthukrishnan [ 25 ] suggested that an algorithm solving the dual , error bounded problem4 can provide a shortcut to the solution of the space bounded problem , gaining a log n ∗ factor time complexlog ity advantage . Still , these solutions are all confined to the restricted variant of problem , in which a coefficient may be only assigned a fixed value in the complete Haar tree ( candidate assigned values are also fixed in advance in the low quality probabilistic model ) . l = 0 l = 1 l = 2 l = 3
+
7 c5 +
6 d3
2 c2 +
6
1 d2 c4 +
11 d1 c0 c1
4 +
1
3 c3 +
4
6 d6 c6 +
2 d5
8 d4
2 c7 +
6 d7
10 d8 c0
4 + l = 0 l = 1 l = 2 l = 3
+
7 c5 +
3 d3
+
6
2 d2 c4 +
10 d1
2 c3 +
+
2 d5
2 d6
+
6 d7
6 d8
11 d4
( a ) Haar tree ( n = 8 )
( b ) Unrestricted Haar wavelet synopsis
Figure 1 : A Haar tree and unrestricted synopsis ( n = 8 )
Unrestricted Synopses for Maximum Error Metrics . Guha and Harb [ 12 , 13 ] discerned that the values assigned to the coefficients retained in a wavelet synopsis can be arbitrary and provided a fully polynomial time approximation scheme ( FPAS ) for the resulting unrestricted space bounded Haar wavelet synopsis problem . The solution of [ 12 , 13 ] is a DP algorithm guided by a two dimensional tabulation per Haar tree node . Each node ci calculates the minimum attainable error E(i , v , b ) over both every possible incoming value5 v and every possible amount of space b allocated to the subtree rooted at ci ; possible incoming values are discretized by a resolution step δ . For each E(i , v , b ) entry , both the δ optimal assigned value z ( also quantized as a multiple of δ ) and the δ optimal distribution of b units of space among the left iL and right iR subtrees of ci are detected . This DP recursion can be summarized as : .≤b max E(iL , v , bff ) , ) , E(iR , v , b − bff .≤b−1 max E(iL , v + z , bff ) , E(iR , v − z , b − 1 − bff
) Computing E(0 , 0 , B ) determines the best B nodes to keep in the synopsis and the best values z to be assigned to each of these nodes for a given value of δ . The ranges of incoming values v and assigned values z to be tested per node can be restricted using the maximum absolute value M in D [ 12 ] , or , more efficiently , by a guessed upper bound E for the target minimized error [ 13 ] . In both 4That is , find a minimal space synopsis achieving error bound . 5The incoming value of a node ci is the value constructed by the path from the root of the sparse Haar tree up to ci . For example , the incoming value of node c7 in the tree of Figure 1b is c0 − c3 = 6 .
E(i , v , b ) = min
8>>< >> : min 0≤b min z,0≤b
E δ ) of the cases , the resulting cardinality R = O( M δ ) or R = O( set of examined values enters the complexity expressions . The Haar+ Tree The Haar+ tree [ 21 ] extends the Haar wavelet hierarchy by allowing extra coefficient values which contribute their ( signed ) value to a single dyadic interval alone . In the example Haar+ tree of Figure 2 , node c0 ( root coefficient ) contributes its value to all approximated data values {d0 , d1 , d2 , d3} . The root is followed by a binary tree of triads ( C1 , C2 and C3 ) , which substitute the single non root coefficients of the classical Haar tree . In each such triad ( eg , C1 ) , the head coefficient ( eg , c1 ) contributes its value positively to its left sub tree and the same value negatively to its right sub tree . The left ( eg , c2 ) and right ( eg , c3 ) supplementary coefficients contribute their values positively only in the single subinterval that they affect ( eg , c2 contributes positively to d0 and d1 only ) . An optimal synopsis of space budget B for a given error metric E places B non zero coefficient values at any positions in the Haar+ tree so that E is minimized . For example , for the four element data set {5 , 3 , 12 , 4} the 2 term Haar+ synopsis that minimizes the MAE consists of the coefficients {c0 = 4 , c8 = 8} . The Haar+ structure outperforms its predecessors in both accuracy of approximation and synopsis construction time [ 21 ] . co c2
+ c4
C2 c5
+
+ d0
C1 c3 + c8
+
+
+ c1 c6 + d1
+ d2 c7
C3 c9 + d3
Figure 2 : An One Dimensional Haar+ Tree
Compact Hierarchical Histograms The Compact Hierarchical Histogram ( CHH ) [ 27 ] is a related data approximation structure , which defines a ( binary by default ) hierarchy of ( dyadic ) intervals and selects an optimal subset of nodes to represent the approximated data In fact , the CHH structure is equivalent to a Haar+ tree , in set . which only supplementary coefficients are allowed . With the benefit of hindsight , a Haar+ tree can be seen as a merging of a CHH and a Haar tree . [ 27 ] proposed heuristic CHH construction techniques , after observing that the calculation of the optimal value to retain on a node is computationally hard , due to the interdependence between nodes in the hierarchy . On the other hand , [ 21 ] eschews this problem by an approximation technique , similar to that in [ 12 , 13 ] , which provably approximates the theoretically optimal solution by a small margin of error . Hence , the Haar+ technique can achieve at least as high accuracy as an heuristically derived CHH over a binary hierarchy due to both its structural and algorithmic advantages .
2.3 A Space Efficiency Technique
Guha [ 11 ] identified space as the most significant resource for an offline summarization problem and furnished a space efficiency paradigm for synopsis construction . His main idea is to avoid storing all tabulated results throughout the DP ; part of them can be dropped and re computed later . In histogram construction , the tabulation ( Equation 1 ) on {i , b} should progress with increasing b , 1 ≤ b ≤ B ( ie , the loop of b is the outer loop ) . Since the values E(∗ , b ) are fully determined by E(∗ , b− 1 ) , after a b column has been used to calculate the ( b +1) column , it is dropped . Hence the space is O(n ) . Besides , the tabulation also detects and stores the single bucket M ( i , b ) in the optimal b partitioning of d0 , d1 , . . . , di . that contains the middle data item ( n ) of the summarized vector . After the optimal error E(n , B ) and middle item bucket M = M ( n , B ) have been established , the two O( n 2 ) independent sub
2
Reference
Time
[ 7 ] [ 8 ] [ 20 ] [ 11 ] [ 25 ] [ 27 ] [ 27 ]
O(nq2B log(qB ) )
O(n2B log B ) O(n log3 n )
O(n2 ) O(n2 log
∗ log n )
O(nB log n log B ) O(nB log2 n log B )
O(R2n log2 B )
[ 12 , 13 , 21 ] [ 12 , 13 , 21 ] This work O(R2n(log ∗
O(R2n log n log2 B )
+ log n ) )
Space
O(n + qB log2 n )
O(n2B ) O(n log n )
O(n ) O(n )
Synopsis Model probabilistic restricted Haar optimal restricted Haar greedy restricted Haar optimal restricted Haar optimal restricted Haar
O(B log2 n + n )
O(nB log n ) O(R min{B2 log n B O(RB log n B + n ) O(R log n + n )
, n log B} )
Compact Hierarchical Histogram ( time efficient ) Compact Hierarchical Histogram ( space efficient ) unrestricted Haar and Haar+ ( time efficient ) unrestricted Haar and Haar+ ( space efficient ) unrestricted Haar and Haar+
Table 2 : Summary of results for offline one dimensional hierarchical synopsis construction ( maximum error metrics ) fi=1 2fi   n problems for the intervals on the left and right of M are re solved recursively . Hence , the total time for the general error histogram 2 2 B = construction algorithm [ 17 ] becomes O Plog n O(n2B ) , ie , the re computation cost is amortized . [ 11 ] applies the same methodology to the restricted Haar wavelet synopsis algorithm of [ 8 ] . In this case , the required tabulation progresses in a bottom up fashion in the Haar tree ; all table entries on a parent node are computed from the tables of its children nodes , which can then be dropped . Accordingly , at most log n + 1 tables need be concurrently stored , covering one path through the Haar tree . After the solution is established at the top level of the Haar tree , the two half size sub problems in the two sub trees of c1 are re solved [ 11 ] . Restricted Haar wavelet synopsis construction requires time quadratic to n , because each of n Haar tree nodes has to consider O(2log n ) = O(n ) possible choices of values in its ancestor set [ 8 ] . Hence , the re computation cost is amortized in this case as well . Table 2 summarizes the complexity results of previous work on the offline one dimensional space bounded hierarchical synopsis construction problem for maximum error metrics and introduces the complexity of the solution we propose ; q is a probability quantization parameter , R is the cardinality of the examined set of incoming or assigned values per coefficient , and ∗ is the optimal error . We explain the space and time efficient variants in the sequel . 3 Motivation The state of the art for all examined methods features a demanding tabulation over space allocations [ 14 , 12 , 13 , 27 , 21 ] . Guha strived to tame these space demands [ 11 ] ; the result was good , but not sufficient : the burden of space tabulation remains . This burden is heaviest for the unrestricted Haar and Haar+ methods : their twodimensional tabulation renders their memory requirement impractical for large data sizes . Besides , the amortization achieved by the paradigm of [ 11 ] does not hold for the algorithms of time linear ( or near linear ) to n reviewed in Section 2 . Applied on them , the paradigm creates a tradeoff between space and time efficiency , as [ 21 ] presented for the Haar+ case . Hence , applied on the MRE algorithm of Section 2.1 , this technique decreases its space requirements to O(n ) , but increases its time complexity to O(nB log3 n ) ( Table 1 ) . The same holds for the unrestricted Haar and the Haar+ cases of Section 2.2 ( Table 2 ) . In the space efficient variant , after the arrays E(iL,∗,∗ ) and E(iR,∗,∗ ) have been used to calculate the entries of E(i,∗,∗ ) , they are dropped . Again , at most log n + 1 arrays need to be concurrently stored . The price for this space efficiency is an extra log n time complexity factor due to re computation . For B√ the time efficient variant two different approaches are possible : If n , then it is advantageous to maintain all E(i,∗,∗ ) arrays in memory . The size of the array at node ci , residing in level .i in the
√
√ tree , is O  R min{B , 2fii} complexity of O ( Rn log B ) . Still , if B'√
, which , after summation , gives a space n , then it is preferable to keep only the at most log n + 1 necessary arrays , with the full solutions corresponding to each of their entries appended on them as lists , as suggested in [ 13 ] . The size of a solution maintained with each entry of an array at level .i is at most min{B , 2fii} , therefore the space required for an array at level .i is O  R(min{B , 2fii})2 . This sums up to a space complexity of O  RB2 log n B . The two B )⇔B = expressions are equal when nlogB = B2log( n n . Values of B both higher and lower than n are likely to occur , thus the preferable method depends on the application at hand . Table 2 shows both . A similar performance tradeoff applies to the winner greedy heuristic of [ 27 ] ( Table 2 ) . Overall , the complexity question on summarization with deterministic guarantees remains unsatisfactorily resolved . In this paper , we provide an alternative methodology that addresses6 these shortcomings . We show how the space bounded summarization problems can be solved more efficiently by exploiting their duality to the corresponding errorbounded problems through binary search ; in those dual problems , the goal is to minimize the space of a synopsis that achieves error no larger than an error bound . In the sequel , we formulate and solve the error bounded histogram and hierarchical synopses problems . Then we define and analyze Indirect synopsis construction algorithms for the corresponding space bounded problems . 4 This section introduces our solution to the space bounded histogram construction problem for weighted maximum error metrics . Our technique utilizes the solution to the complementary error bounded problem . Section 4.1 presents a linear algorithm7 for this auxiliary problem , which achieves the minimal space B∗ under an error bound ; we discuss how the solution can be tested on whether it achieves , secondarily , the minimal error ∗ in the required space B∗ , providing a strong optimization . In Section 4.2 we exploit this solution in order to efficiently solve the space bounded histogram construction problem , which is our main interest and contribution . 4.1 Error bounded Histogram Construction We formulate the Lw∞ bounded histogram construction problem : Problem 1 Given a data vector D and an Lw∞ error bound , construct a histogram H of D with the minimum number of buckets B∗
Indirect Histogram Construction
, such that Lw∞ ( D , H ) ≤ .
6The basic idea behind this methodology was applied for the restricted Haar wavelet synopsis problem in [ 25 ] , but yielded only a marginal benefit ( see Table 2 ) that did not reveal its full potential . 7A similar algorithm was proposed in [ 22 ] for the effective summarization of data streams , albeit it treated the MAE metric only . wi
, di +
LEMMA 1 . Let B∗ be a B∗
Our algorithm for this problem establishes a minimal space histogram H of B∗ buckets that satisfies in one linear pass , drawing from the following Lemma . be the minimum number of buckets required to satisfy the bound for data vector D and H = {{bi , ei , vi}} , 1 ≤ i ≤ B∗ bucket histogram such that the achieved error is Lw∞ ( H , D ) ≤ . Furthermore , let Lw∞,i be the error in bucket ( segment ) si = {bi , ei , vi} ∈ H , i < B∗ . Then , if after we advance the right bucket boundary ei by one position , so that the bucket becomes ˜si = {bi , ei + 1 , ˜vi} , the new bucket error value ≤ , then the new segmentation ˜H as a whole also remains ˜Lw∞,i achieves the error bound Lw∞ ˜H , D ≤ . Based on Lemma 1 , the MinHistSpace algorithm of Figure 3 performs a linear scan of the data . During this scan , it extends the right boundary of the running segment si as long as Lw∞,i ≤ . An encountered data item di with error weight wi defines a tolerance interval [ di − wi ] ; bucket values within this interval satisfy for di . The algorithm only needs to calculate the intersection I of such intervals for arriving data items . When I becomes null , si cannot be magnified any more . Then a new bucket boundary is inserted before the last read data item . The value assigned to the formed bucket is defined as v = wj dj +wkdk , where dj , dk are the data items responsible for the limits8 a , b of the last non null value of I = [ a , b ] . Hence MinHistSpace needs O(n ) time and space . As an example , assume that we want to find a histogram with L∞ error at most = 5 approximating the data vector D = {11,−1,−6 , 8,−2 , 6 , 6 , 10} . MinHistSpace scans D and computes H incrementally . d0 = 11 combined with d1 = −1 violate the bound ( the absolute error of such a bucket is 11−(−1 ) = 6 ) . Thus , the first bucket has b1 = e1 = 0 and value v1 = 11 . The algorithm continues by putting d1 in the next bucket which is terminated when d3 = 8 is found ( d3 violates ) . Continuing this way , MinHistSpace eventually computes the histogram H = {{0 , 0 , 11},{1 , 2,−3.5},{3 , 6 , 3},{7 , 7 , 10}} . Algorithm MinHistSpace( ) Input : Output : 1 . 2 . 3 . 4 . 5 . 7 . 8 . 9 . 10 . 11 . read di ; compute vr , Lw∞,r for the r th bucket from data read so far ; if  Lw∞,r er = i − 1 ; vr = prev ; r := r + 1 ; // fix r th bucket re compute vr from di ; //initialize new bucket error bound , n data vector [ d0 , . . . , dn−1 ] histogram partitioning H that satisfies er = n ; // fix last bucket return created partitioning H ; i = 0 ; r = 1 ; while ( i < n ) prev = vr ; i := i + 1 ;
> ; wj +wk
2 i i i
, e1
, e1
, eff i ] ⊂ [ sff
, eff i ] . Since [ sff would not have less segments than H . Let {sff
} ∈ Hff Hff , vff be the first such segment encountered from left ; then sff i = s1 i , hence [ s1 i ] satisfies , its subdivision i [ s1 i + 1 ] can also satisfy this bound as a bucket . However , if ali gorithm MinHistSpace has fixed the i th segment as [ s1 i ] , then i i + 1 ] could not make a segment satisfying . By the interval [ s1 i reductio ad absurdum , it follows that there is no histogram segmentation Hff as we assumed . Hence the B is the optimal space B∗
, eff
, e1
, e1 i i
.
The following lemma defines an error optimality test for the histogram returned by MinHistSpace . Given the result H of an execution of MinHistSpace , the objective of the test is to determine , with one more call of MinHistSpace ( ie , in linear time ) , whether the actual Lw∞ error of H is the minimum possible for the space B∗ occupied by H . LEMMA 2 . Let H be the B∗ if and only if ˜B > B∗
bucket histogram segmentation of D for error bound returned by MinHistSpace and ¯ ≤ be the actual Lw∞ error of H . Let ˜H be the ˜B bucket histogram segmentation of D returned by MinHistSpace running under the constraint Lw∞,r < ¯ , allowing error values less than but not equal to ¯ . Let be the minimum Lw∞ error of a histogram segmentation of D in ∗ B∗ buckets . Then ¯ = ∗ PROOF . B∗ is the least number of buckets required to satisfy error bound ≥ ¯ , hence ˜B ≥ B∗ . If ˜B = B∗ , then there exists bucket histogram partitioning of D with Lw∞ error less than a B∗ ¯ , hence H has not achieved the optimal error ∗ in B∗ buckets . Therefore ¯ = ∗ ⇒ ˜B > B∗ then any histogram partitioning of D with Lw∞ error less than ¯ requires more than B∗ buckets , hence H has achieved error optimality . Thus , ˜B > B∗ ⇒ ¯ = ∗
. In conclusion , ¯ = ∗ ⇔ ˜B > B∗
. In reverse , if ˜B > B∗
.
. u = Lw∞ error of equi width B histogram ; elow = 0 ; ehigh = u ; while not finished
Algorithm IndirectHist(B ) space bound B , n data vector [ d0 , . . . , dn−1 ] Input : Output : Lw∞ error optimal histogram partitioning H 1 . 2 . 3 . 4 . 5 . 6 . 7 . 8 . 9 . 10 . 11 . 12 . 13 . emid = ( ehigh + elow)/2 ; H=MinHistSpace(emid ) ; ¯B = size of H ; ¯ = actual Lw∞ error of H ; /* ¯ ≤ */ if ( ¯B ≤ B )
˜H=MinHistSpace(< ¯ ) ; ˜B = size of ˜H ; if ( ˜B > B ) else if ( ¯B > B ) elow = emid finished := 1 ; /* optimal result found */ else ehigh = ¯ ; return H ;
Figure 4 : Indirect histogram construction algorithm
Figure 3 : Minimum Space Histogram construction algorithm
4.2 Application to the Space Bounded Problem
We now prove that MinHistSpace achieves space optimality .
, vff
}} , 1 ≤ i ≤ Bff
THEOREM 1 . The histogram H returned by MinHistSpace has subject to the Lw∞ error bound . achieved the minimal space B∗ PROOF . Let B be the number of buckets in H . Assume there exists a histogram segmentation of D in Bff < B segments Hff = {{bff , such that Lw∞ ( Hff , D ) < . Then there , eff will be at least one segment {bff > e1 i , where e1 i is the right boundary of the i th segment in H , otherwise 8The item di most distant from the limit at hand , hence of smallest weight wi , is chosen in case more than one items are responsible for the same limit . such that eff
} ∈ Hff
, vff
, eff i i i i i i i
We now define an efficient algorithm for space bounded histogram construction under a maximum error metric that exploits the solution to the dual error bounded problem . Formally , given a data vector D and a space bound B , we seek a histogram with at most B buckets that has minimal Lw∞ error . The crucial observation is that the Lw∞ error of the optimal B histogram is monotonically non decreasing with B . Therefore we can apply binary search with guesses of in the space of error bounded problems . This idea is materialized by our IndirectHist algorithm shown in Figure 4 . In our implementation , the seed value of is obtained by linearly measuring the Lw∞ error of an equi width B bucket histogram of D , which provides an upper bound for the B optimal
Lw∞ error . Thereafter , the MinHistSpace procedure is repeatedly invoked with binary search on the error bound value ; it performs an optimality test , as defined in Lemma 2 , for each guessed error bound value that does not require more than B space . The search terminates when the guessed error bound reaches a value that requires a histogram of ¯B ≤ B space and actual error ¯ , while the optimality test indicates than any error bound < ¯ requires ˜B > B space ; then an optimal histogram of minimum error ∗ = ¯ in the space budget B has been created . At line 8 of Figure 4 , the call MinHistSpace(< ¯ ) corresponds to a variation of MinHistSpace(¯ ) , in which the condition at line 5 of Figure 3 is replaced by ( Lw∞,r ≥ ¯ ) . This search process brings an O(log ∗ ) runtime factor9 , hence the time complexity of the Indirect algorithm is O(n log ∗ ) . Section 6.2 verifies the time advantage of this algorithm in practice . 5
Indirect Space bounded Hierarchical Synopses
In this section we introduce our solution to the space bounded hierarchical synopsis problem for maximum error metrics . We study both the unrestricted Haar and Haar+ models . Again our technique exploits the solution to the dual error bounded problem . 5.1 Error bounded Hierarchical Synopses We formulate a strong version of the Lw∞ bounded hierarchical synopsis problem as follows : Problem 2 Given a data vector D and an error bound , construct a representation ˆZ of D , producing a reconstruction ˆD , such that Lw∞ D , ˆD ≤ and the number of non zero entries s∗ in ˆZ is minimized . Of all representations with s∗ non zero terms satisfying , select the one with the minimal actual error ∗ ≤ . An incoming value at node ci of the Haar tree ( or triad Ci of the Haar+ structure ) is a value reconstructed in the path of ancestor coefficients from the root node up to ci in the sparse representation ˆZ of D . In a wavelet decomposition W(D ) , this is the average value in the interval I under the scope of ci , henceforward called real incoming value at ci . Similarly , an assigned value at node ci is a coefficient value retained at that node in ˆZ ; in W(D ) , this is the actual semi difference of the average values in the two sub intervals IL , IR under the scope of ci , henceforward called real assigned value . For example , in Figure 1a , the real incoming value of node c6 is 2 , while the incoming value constructed for this node in the synopsis of Figure 1b is also 2 . On the other hand , the incoming value of node c3 in Figure 1b is 4 , whereas the corresponding real incoming value is 5 ( see Figure 1a ) . Similarly , the real assigned value to node c3 is −3 , whereas the value assigned to this node in the synopsis −2 . These concepts are directly extended to the Haar+ tree [ 21 ] . In order to construct our solution , we need to explore the space of possible retained coefficients and values assigned to them . We use a dynamic programming ( DP ) framework , as in previous hierarchical synopsis algorithms [ 6 , 7 , 8 , 11 , 25 , 12 , 13 , 21 ] . In a bottom up process , this algorithm considers all possible incoming values v and , for each v , all possible assigned values zv i at each node ci of the Haar tree and determines the optimal value to assign at ci for v ; in a Haar+ tree , possible head and left/right supplementary coefficients , zh , zl , zr , on a triad Ci are all examined . We quantize the ( real valued ) domains of v and zv i into multiples of a small resolution step δ . The next section outlines some lemmata
9The log function expresses the dependence of running time on the derived error value ; it is to be understood as a growth function , as in [ 25 ] ; the case ∗ ≤ 1 does not imply non positive time .
) + 1 = O(
δ
| ≤ ( 2(¯ −|vi−v| )
| ≤ ¯ − |vi − v| . that establish upper and lower bounds for these domains . 511 Delimiting the Value Domains Haar Wavelet Synopses We study the simple Haar wavelet case first . As we will see , despite its disadvantage in accuracy and , for non maximum error metrics , complexity , in relation to the Haar+ tree , the classical Haar wavelet structure has an advantage in its potential for delimitation of search space for maximum error metrics . LEMMA 3 . Let vi be the real incoming value at node ci . Let v be an incoming value to ci for which the error bound under the Lw∞ metric can be satisfied , and ¯ = minj∈I{|wj|} , where I is the interval under the scope of node ci ; then |vi − v| ≤ ¯ . Lemma 3 implies that the finite set Si ⊂ IR of possible incoming values we have to examine at node ci consists of the multiples of δ in the interval [ vi − ¯ , vi + ¯ ] ; thus , |Si| ≤ ( 2¯ δ ) . 10 We now demarcate the assigned values . LEMMA 4 . Let vi be the real incoming value to node ci , zi the real assigned value at ci , v ∈ Si be a possible incoming value to ci for which the maximum error bound can be satisfied , and zv i be a value that can be assigned at ci for incoming value v , satisfying ; then |zi − zv Lemma 4 implies that the finite set S v ⊂ IR of possible assigned values we have to examine at node ci , for a given incoming value v ∈ Si , consists of the multiples of δ in the interval [ zi − ( ¯ −|vi − v| ) , zi + ( ¯ −|vi − v|) ] ; hence , |S v ) + 1 = O( δ ) . Lemmata 3 and 4 are most simple in the case of the maximum absolute error metric , when ∀i , wi = 1 ; in the case of the maximum relative error metric , ¯ = · max{S , maxj∈I{|dj|}} , where S is the sanity bound . Naturally , the same lemmata hold with any upper bound E for the optimal Lw∞ error of a synopsis , even when that error is not known in advance . This observation will be useful in our implementation of the direct solution to the space bounded problem ( Section 63 ) Haar+ Synopses Delimitation lemmata analogous to Lemmata 3 and 4 also apply to the Haar+ structure . [ 21 ] shows how the flexibility of this structure enables an equally robust delimitation of the search space , based on minimum and maximum data values , for any target error metric ; this comes in contrast to the classical Haar tree , where such target generic delimitation is not possible . However , due to the same flexibility , the delimitation that exploits a given error bound , particular to the case of a maximum error metric , is less tight with the Haar+ structure . In this case , the delimitation lemmata take the following forms . LEMMA 5 . Let mi be the minimum and Mi the maximum individual data value under the scope of triad Ci and v ∈ Si be a possible incoming value at Ci for which the maximum error bound is satisfied , and ¯ = minj∈I{|wj|} , where I is the interval under the scope of Ci ; then v ∈ [ mi − ¯ , Mi + ¯ ] . Lemma 5 implies that the set Si of incoming values we have to examine for triad Ci consists of the multiples of δ in the interval [ mi − ¯ , Mi + ¯ ] ; thus , |Si| ≤ ( Mi−mi+2¯ δ ) , where ∆ is the difference of the minimum from the maximum value in D . We now demarcate the values assigned to the head coefficient . i
δ
) + 1 = O( ∆
δ i i
LEMMA 6 . Let v ∈ Si be a possible incoming value at Ci and zh ∈ S v i,H be a value that can be assigned at the head coefficient of Ci for incoming value v , satisfying the individual data error bound ; then |zh| ≤ min{Mi − v , v − mi} + ¯ . 10The inequality ≤ accommodates for the variation in the number of integers in a fixed interval . i,H
| = O( ∆
Lemma 6 implies that the finite set of possible assigned values we have to examine for the head coefficient at Ci is S v i,H , where |S v δ ) . The possible assigned values at the left and right supplementary coefficients of triad Ci can be delimited in a similar fashion . Based on this delimitation , we devise our dynamic programming solution . Its essence is the same in both the Haar wavelet and the Haar+ case . We use the former model as our illustrative example . The extension to the latter is straightforward by incorporating provisions for the supplementary coefficients .
512 Deriving the Answer
In a nutshell , our recursive MinHaarSpace procedure works in a bottom up left to right scan over the Haar ( or Haar+ ) tree . At each visited node ci it calculates an array A of size |Si| from the precalculated arrays L and R of its children nodes ciL , ciR ( a single array C for the child iC of the root node ) . A holds an entry A[v ] for each possible incoming value v at ci ( a single element A for the root node ) . Such an entry contains : ( i ) the minimum number A[v].s = S(i , v ) of non zero coefficients that need to be retained in the sub tree rooted at ci with incoming value v , so that the resulting synopsis satisfies the error bound ; ( ii ) the δ optimal value A[v].z to assign at ci , for incoming value v ; and ( iii ) the actual minimized maximum error A[v].e thus obtained in the scope of ci . S(i , v ) is recursively expressed as :
S(0 , 0 ) = min z∈S0 0 S(i , v ) = min z∈Sv i
{S(iC , z ) + ( z = 0)} {S(iL , v + z ) + S(iR , v − z ) + ( z = 0)} to assign at ci is the one that minimizes the Lw∞ error yielded at the two affected data values : wiL|diL − ( v + z∗ )| and wiR|diR − ( v − z∗ )| . This maximum error is minimized when the two are equal : wiL|diL − ( v + z∗ = −wiR diR +v(wiR In the case of the maximum absolute error , this is the actual Haar wavelet decomposition value at node ci . Hence , for last level nodes , we do not need to consider multiples of δ ; the value of E(i , v ) for a last level node is :
)| = wiR|diR − ( v − z∗
)| ⇔ z∗
−wiL ) wiL +wiR wiL diL
.
E(i , v ) =( max{wiL|diL − v| , wiR|diR − v|} , 0 ∈ S v 0 /∈ S v
|diL + diR − 2v| , wiL wiR wiL +wiR i i
This error value has to be assigned to A[v].e in this case ; A[v].z is either 0 or z∗ , respectively . A pseudo code for the proposed recursive MinHaarSpace DP procedure is shown in Figure 5 . Following the generic space efficiency paradigm of [ 11 ] , the memory occupied by the arrays C , L and R needs to be reserved only when their entries are first computed and is freed after they have in turn been used for the creation of A . Therefore , for a data set of size n , the maximum number of arrays that need to be concurrently stored is log n + 1 : one array for each level of resolution plus the currently computed ones . This maximum is necessitated when the right bound post order recursion reaches the right most Haar tree node . This basic bottom up process computes the wavelet transform ’s incoming and assigned values on the fly in order to define the sets Si and S v i as it needs . Hence a recursive procedure that derives the δ optimal space result answer without constructing the synopsis itself is defined .
Algorithm MinHaarSpace(i , ) Input : Output : index i , error bound , n data vector D = [ d0 , . . . , dn−1 ] array A with retained value z for ci , minimum space s occupied in sub tree and error e for each v ∈ Si if ( i = 0 ) // root node C = MinHaarSpace(1 , ) ; compute s , z ∈ S0
0 , e of A from C ;
) then // internal node else if ( i < N 2
L = MinHaarSpace(iL , ) ; R = MinHaarSpace(iR , ) ; for each v ∈ Si compute s , z ∈ Sv else if ( i ≥ N for each v ∈ Si
2
) then // leaf node i , e of A[v ] from L , R ; compute s ∈ {0 , 1} , z ∈ {0 , ci} , e of A[v ] from D ;
1 . 2 . 3 . 4 . 5 . 6 . 7 . 8 . 9 . 10 . 11 .
The above equations compute the least of ( i ) the minimum required space if a non zero coefficient value z is assigned at node ci ; and ( ii ) the required space if a zero value is assigned at it . The latter case applies only if 0 ∈ S v i . For economy in presentation , the +1 term that appears in the former case is uniformly expressed by the boolean integer ( z = 0 ) . This convention is used throughout this section . For a last level node ( i ≥ n 2 ) , the value of S(i , v ) is 0 if the coefficient at ci can be omitted with incoming value v , or 0 ∈ S v i , 1 otherwise . The former case occurs if and only if the maximum error yielded by v at the affected data values below ci satisfies . The s entry of array A at node ci for each allowed incoming value v , A[v].s , is computed from those of arrays L and R of children nodes ciL and ciR ( array C for the child ciC of the root node ) . Let ¯S v ⊂ IR denote the set of those assigned values at node ci for incoming value v that require the minimum space in order to achieve the error bound : i
¯S 0 0 = argmin z∈S0 0 ¯S v i = argmin z∈Sv i
{S(iC , z ) + ( z = 0)} {S(iL , v + z ) + S(iR , v − z ) + ( z = 0)}
The δ optimal value to select is the one among these candidates that also minimizes , in a secondary priority , the obtained Lw∞ error in the scope of ci . Let E(i , v ) be the minimum Lw∞ error obtained in the scope of ci with incoming value v and an assigned value z , with S(i , v ) coefficients retained in the sub tree rooted at ci :
E(0 , 0 ) = min z∈ ¯S0 0 E(i , v ) = min z∈ ¯Sv i
{E(iC , z)} {max{E(iL , v + z ) , E(iR , v − z)}}
This error value is assigned to A[v].e ; the value A[v].z is the assigned value that minimizes the error expression above . For a last level node ( i ≥ n i , then the best non zero value z∗
2 ) , if 0 /∈ S v return A ;
Figure 5 : Recursive Minimum Space procedure
| possible assigned values needs O(
Complexity Analysis The result array A on each node ci holds |Si| entries , one for each possible incoming value , hence its size δ ) ; besides , at each node ci and for each v ∈ Si , the loop is O( through all |S v δ ) time . Hence , the runtime of MinHaarSpace(0 , ) is O ( δ )2n . Besides , since complexity is O  δ log n + n , where n stands for the storage of at most log n + 1 arrays need to be concurrently stored , the space the data . i
513 Constructing the Synopsis
The construction of the synopsis after the δ optimal answer has been established by a run of MinHaarSpace(0 , ) presents us with a time space tradeoff . We outline both alternatives . The Space Efficient Solution . After MinHaarSpace returns from the topmost level , so that the values of c0 and c1 have been established , we can call a process that reenters the problem in the two branches of c1 and recomputes the respective solutions thereafter , fi=0 2fi n
δ )2 Plog n recursively . The total running time is the sum of the basic running time for all re entered sub problems . Setting . as the Haar tree 2 = O  ( level , this sum becomes O ( δ )2n log n . δ log n + n , On the other hand , the space complexity remains O   as we need to maintain the stored data set ( or its wavelet transform ) throughout the computation . The Time Efficient Solution . Alternatively , we may choose to maintain all necessary computed information throughout the recursion of MinHaarSpace . This maintenance allows us to construct the final solution as soon as the minimum space has been derived . Consider a DP array entry A[v ] at node ci ; this entry describes the local part of a candidate solution , for incoming value v , which has already been calculated in the sub tree rooted at ci . The rest of this candidate solution is maintained by annexing to entry A[v ] the set of all coefficient values retained in it . Therewith the sub problem re entry is avoided . The total running time remains only O  ( δ )2n . For each DP array entry A[v ] of node ci at level . , a set of at most min{BM , 2fi} coefficients is retained , where BM is the maximum size of a candidate solution stored throughout the computation . Thus , the space complexity becomes BM . In this case , O storage of the decomposition is not required . fi=0 min{BM , 2fi} = O
δ Plog n
BM log n
δ
514 Verifying Space Optimality
2(E− )
MinHaarSpace approximates the optimal solution in IR . In the space bounded problem , if EB is the optimal maximum absolute ( L∞ ) error for a B term synopsis in IR , then rounding its values to the closest multiples of δ can increase that error by at most 2 min{B , log n} [ 12 , 21 ] . For the error bounded problem , we can δ formulate the conditions under which the minimum space under resolution δ is the optimal in IR , as follows : THEOREM 2 . Let B be the minimum space , under resolution δ , that satisfies the L∞ error bound , and E be the minimum L∞ error that can be achieved within space B − 1 , under resolution min{B−1,log n} then B is the minimum space required to δ . If δ < satisfy error bound in IR . PROOF . If E ≤ , then the approximation algorithm for the error bounded problem with bound would find the solution with B − 1 space ; hence E > . Let EB−1 be the error achieved by the optimal ( B − 1) term representation in IR . B is the optimal space for bound if and only if cannot be satisfied with less than B non zero terms ; hence it should be EB−1 > . Since E ≤ EB−1 + δ 2 min{B − 1 , log n} , a sufficient condition for optimality is E − δ 2 min{B − 1 , log n} > , or δ < According to Theorem 2 , in order to ascertain that the answer B , derived for an L∞ error bound under resolution δ , is optimal in IR , we need to derive the error result E for the space bounded problem with space bound B−1 under δ . If δ and E satisfy the condition δ < min{B−1,log n} , then B is optimal ; otherwise , we set a smaller value of δ and repeat the process until we reach space optimality . min{B−1,log n} .
2(E− )
2(E− )
5.2 Application to the Space Bounded Problem
As discussed in Section 2.2 , the state of the art solution [ 12 , 13 , 21 ] for space bounded hierarchical synopsis construction is burdened by a two dimensional tabulation of E(i , v , b ) entries per node . We infer that , as in the histogram case , the space bounded problem can be more efficiently solved through a binary search invocation of the algorithm for the error bounded that shuns the tabulation over b . In our implementation , the upper bound of in the search is the E corresponding to the synopsis of B largest Haar decomposition coefficients by absolute value , easily computed in O(n log B ) time ; the lower bound of is the ( B + 1) th highest absolute coefficient value |zk| . Given that the solution to the strong error bounded problem minimizes the error within the δoptimal space , its application to the space bounded problem yields the δ optimal error when the binary search converges to the space budget B . Still , in order to ensure the termination of the search , MinHaarSpace also performs an optimality test ( as in Section 4.2 ) for guessed error bound values that require less than B space . Hence , the search terminates when it reaches an error bound that either requires a synopsis of exactly B space , or requires a synopsis of ¯B < B space and actual error ¯ , while any error bound < ¯ requires ˜B > B space . When the tested error bound is decreased during the binary search , the minimum error derived for the previous bound is used for determining the new bound . Figure 6 shows a pseudocode for this IndirectHaar algorithm . u = Lw∞ error of B largest term synopsis ; l = ( B + 1) th largest coefficient ; elow = l ; ehigh = u ; while ( not finished )
Algorithm IndirectHaar(B ) space bound B , n data vector [ d0 , . . . , dn−1 ] Input : Output : Lw∞ error optimal B sized unrestricted synopsis 1 . 2 . 3 . 4 . 5 . 6 . 7 . 8 . 9 . 10 . 11 . 12 . 13 . 14 . 15 . emid = ( ehigh + elow)/2 ; ˆZ=MinHaarSpace(emid ) ; ¯B = size of ˆZ ; ¯ = actual Lw∞ error of ˆZ ; /* ¯ ≤ */ if ( ¯B < B )
˜Z=MinHaarSpace(< ¯ ) ; ˜B = size of ˜Z ; if ( ˜B > B ) else if ( ¯B > B ) elow = emid else finished := 1 ; /* ¯B = B */ finished := 1 ; /* optimal result found */ else ehigh = ¯ ; return ˆZ ;
Figure 6 : Indirect hierarchical synopsis construction
δ )2n(log ∗ E
Complexity Analysis . As in the histogram case , the binary search increases the time requirements of the error bounded problem by an O(log ∗ ) worst case factor . We present a space efficient solution without dependence on B . The advantage of the alternative timeefficient solution in time is negligible , since the O(log ∗ ) factor is comparable to the log n factor which is paid only once for synopsis construction . Since the highest value of the changing bound is E , the runtime of this Indirect algorithm is O  ( + log n ) . The former log term expresses the cost of the binary search , while the latter expresses the cost of constructing the B term synopsis in a space efficient manner after the optimal error value ∗ has been established . This complexity absorbs the O(n log B ) term for determining the seeds of the search . The log ∗ factor does not grow with n , hence this runtime is decisively lower than the E O  ( δ )2n log n log2 B runtime of the space efficient Direct algorithm and , unless11 n Blog B , lower than its O  ( E δ )2n log2 B the Indirect algorithm requires basic runtime too . E B log n δ log n + n space , which is lower than the O   B + n O   space of the space efficient Direct algorithm in cases where log n ' B−1 . This inequality holds in reasonable sumB log n marization scenaria , assuming B ≤ n B 2 . In addition , the respective O(n2) time restricted algorithm uses O(B log n B + n ) = O(n ) E space [ 11 ] , which becomes larger than O   δ log n + n when B log n δ and , B 11The constraint is verified for reasonable B n ratios ; eg for B = 16 , Blog B = 65536 .
δ log n . This inequality holds when B E
⇔ n B
Besides ,
E
E δ
B
B )B n
δ ⇔ n B E
B
B− E additionally , ( n δ ; hence , it holds for large enough summarization problems . In conclusion , this Indirect algorithm has better asymptotic behavior than both direct counterparts in time and space . Section 6.3 verifies the runtime benefit of this Indirect algorithm in practice .
5.3 Comparison to the Restricted Haar Strategy log n computation in this case costs O Plog n−1
Our focus is the Indirect solution to the space bounded problem . We have devised an algorithm for the error bounded problem in order to serve this goal . Still , this algorithm can present an independent interest of its own . In this context , it is comparable 2 to the O( n log n ) time restricted Haar algorithm for the maximumerror bounded problem that was proposed in [ 25 ] . This algorithm tries the incoming values yielded by all 2fi ancestor subsets of a node ci at level . in the Haar tree ; it stops recursing and resorts to local search within each of the n sub trees in the bottom ( log log n ) Haar tree levels ; the examined assigned values for ci are zi and 0 . The application of Lemmata 3 and 4 prunes ancestor subsets that add up to prohibited incoming values in this algorithm , yet does not annul its near quadratic time complexity . This complexity is due to the fact that the restricted strategy explicitly enumerates and examines different ancestor subsets whose coefficients may add up to nearby incoming values . In contrast , the unrestricted Haar and Haar+ strategies precalculate a set of equally spaced allowed incoming values that anticipate all possible contributions ancestor coefficients can add up to . The algorithm in [ 25 ] determines the minimum space without constructing the synopsis itself , using O(n ) space for storing the decomposition . A sub problem re2 . Setk = ting k = log n 2 O( n log n ) . Hence , constructing the synopsis does not present a timespace tradeoff . On the other hand , the algorithm of Section 5.1 is linear to n ; hence , for sufficiently large n , it outpaces the nearquadratic restricted Haar algorithm . For an appropriate value of δ , it produces better synopses too ( as in [ 12 , 13 , 21] ) . Hence , our algorithm for the error bounded problem not only provides the basis for a more efficient solution to the dual space bounded problem , but treats the error bounded problem itself more efficiently and accurately than previous approaches too . 6 Experimental Evaluation In this section we present experimental results demonstrating the advantage of our Indirect solutions vs . the respective Direct for all considered summarization methods . Both solutions compute synopses of equal error ( in the hierarchical cases , for equal resolution δ ) ; hence our comparison pertains to runtime ( with MAE as the target metric ) ; besides , the space advantage for hierarchical synopses is clear , due to its connection with B . All algorithms were implemented with the g++ 343 compiler and run on a 4 CPU Opteron 2.2GHz machine with 4GB of main memory running Solaris .
2 )2
2 , the complexity becomes O nPlog n k=1
2fi( n
1 log fi=0 n k
2
6.1 Description of Data
We used two real data sets . The first data set ( TM ) is a sequence of 178,080 sea surface temperature measures extracted from drifting buoys positioned throughout the equatorial Pacific . The average value in TM is 26.75 and the set has a standard deviation of 191 The second data set ( FC ) is extracted from a relation of 581,012 tuples describing the forest cover type for 30 x 30 meter cells , obtained from US Forest Service . FC contains the frequencies of the distinct values of attribute aspect in the relation . The frequencies average at 1613 ( standard deviation : 730 ) and feature spikes of large values ( min value : 499 , max value : 6308 ) . FC and TM were downloaded from the UCI KDD Archive.12
6.2 Histogram based Summarization
In this experiment we measure the runtime for histogram construction . We compare the Direct solution of [ 14 ] , in its space efficient variant , to our Indirect method ( Section 42 ) For the Direct method , we measured the basic runtime required to derive the optimal error result only . The Indirect algorithm computes the optimal histogram segmentation per se , which is the same for both . Figure 7a shows their performance as a function of n with a constant summarization ratio B = n/64 for various sized subsets of the TM data set . As expected from our theoretical analysis , Indirect vastly outperforms Direct . Our second experiment measures the running time with respect to the bucket space B for a constant data size n . Figure 7b shows the results for the FC data set with constant n = 360 . As expected , the Indirect method exhibits its independence of B in both cases ; it always terminated after a few repetitions ( mean 123 ) In contrast , the runtime of Direct grows with B .
) c e s ( e m T i
Indirect Direct
10000
1000
100
10
1
0.1
0.01
256
512
1024 2048 4096 8192 16384 32768 65536 131072
Size n
) c e s ( e m T i
0.2
0.15
0.1
0.05
0
Indirect Direct
10
20
30
40
60
50 Space B
70
80
90
100
( a ) Runtime vs . n , B = n/64
( b ) Runtime vs . B Figure 7 : Runtime comparison ( Histograms )
6.3 Hierarchical Summarization
Haar Wavelet Synopsis Construction In this experiment we measure the runtime of hierarchical summarization algorithms , starting with the classical Haar case . We first compare the Indirect method of Section 5.2 to two versions of the maximum error unrestricted Haar synopsis algorithm of [ 12 , 13 ] . The former , Direct , first calculates , in O(n log B ) time , the target error for the synopsis consisting of the top B Haar wavelet terms by absolute value ; then it employs it for bounding the search space . The latter , OracleDirect , is an infeasible algorithm , which represents a conceptual limit for the best case performance of the guess based solution of [ 13 ] . In OracleDirect , the value of the final optimal error is assumed to be provided in advance by an oracle , hence its search space is optimally delimited . Both direct algorithms compute the same error result as Indirect . After experimentation , we settled for a reasonable , in the given data set , constant value of δ = 0.1 ( ie the resolution step for delimiting the domains of incoming and assigned values ) for all three algorithms . Smaller values burdened the running time without significant quality increase ; larger values were undermining the quality of the synopses . We also ran an enhanced version of the restricted algorithm of [ 8 , 11 ] ( Restricted ) , which also prunes its search space using a precalculated error bound . For all algorithms , we measured the basic time required to derive ( for OracleDirect , to verify ) the optimal error result . Figure 8a shows ( on a log log graph ) their performance as a function of n with a constant summarization ratio B = n/64 for various sized subsets of the TM data set . As expected , Indirect presents the most afford12Available at http://kddicsuciedu/ able runtime growth ; not only it outperforms Direct , but it outpaces the OracleDirect too ; hence it invariably produces identical quality in shorter time and smaller space . Besides , Restricted , which achieves lower synopsis quality , undergoes the fastest growth , eventually becoming the slowest , due to its quadratic time complexity ; this result reconfirms the finding of [ 12 ] in the realm of these pruning intensive enhanced variants and with larger data sizes that reveal the disadvantage more clearly . Figure 8b plots ( on a loglin graph ) the runtime for the FC data set with respect to B ( for constant n = 512 , obtained after zero padding the wavelet decomposition ) , setting δ at 5 and 10 . The results for Direct exhibit the interplay between two factors affecting the running time : One the one hand , the increase of B results into tighter delimitation of the search space based on a smaller pre calculated error upper bound , with a significant impact on running time . However , B affects the time complexity itself as well . Hence the runtime of Direct presents unstable behavior , with a maximum at the intermediary position B = 15 . On the other hand , the runtime of Indirect tends to decrease as B ( hence the tightness of the error bound ) grows ; this algorithm terminated after a few repetitions , even fewer than in the histogram case : it converges more robustly thanks to its exploitation of the strong version of the error bounded problem .
Indirect Direct Oracle Direct Restricted
1000
100
10
1
0.1
) c e s ( e m T i
) c e s ( e m T i
Indirect 5 Direct 5 Indirect 10 Direct 10
100
10
1
0.1
0.01
256
512
1024
2048
4096
8192 16384 32768 65536 131072
10
20
30
40
50
60
70
80
90
100
Size n
Space B
( a ) Runtime vs . n , B = n/64
( b ) Runtime vs . B Figure 8 : Runtime comparison ( Haar wavelets )
Haar+ Synopsis Construction We repeated , in the Haar+ case , with the same data sets and configurations , the comparison between the Indirect method of Section 5.2 and its Direct counterpart [ 21 ] ; this also prunes its search space as much as possible , using a precalculated error bound and Lemmata 5 and 6 . Figure 9 shows the results . The runtime of Direct is larger in this case , due to the less intensive pruning that the Haar+ structure allows . However , the performance of Indirect is equally satisfactory as in the classical Haar case ( compare Figures 8a and 9a ) . The difference is more conspicuous for runtime versus synopsis size B ( Figure 9b ) . In this case , the increasing tightness of the pre calculated error bound cannot overcome the effect of the increasing B itself on the runtime of Direct ; hence , it grows with B . Still , the runtime of Indirect shows a decreasing trend as B grows in this case too . The plots of Figure 9 have identical x axes to those of Figure 8 , but their logarithmic y axes are scaled differently for the sake of readability .
Indirect Direct
10000
1000
100
10
1
) c e s ( e m T i
1000
100
10
1
) c e s ( e m T i
7 Conclusions In this paper we have examined the problem of summarization with deterministic guarantees from a new perspective , applied on state of the art histogram and hierarchical methods . We demonstrated the advantage gained by solving the computationally heavy and memory hungry space bounded problems through their lighter error bounded counterparts ; this advantage consists of complexities which are lower , independent of synopsis space and free of performance tradeoffs ; it stems from the removal of a tabulation that hindered previous solutions and , in the hierarchical case , the tight delimitation of the search space . In conclusion , our solutions provide the most recommendable option for the time and space efficient offline summarization of very large data sets with a maximum error guarantee . In the future , we plan to extend our techniques to the summarization of multi measure and multidimensional data . 8 References
[ 1 ] R . Bellman . On the approximation of curves by line segments using dynamic programming . Communications of the ACM , 4(6):284 , 1961 .
[ 2 ] C . Buragohain , N . Shrivastava , and S . Suri . Space efficient streaming algorithms for the maximum error histogram . In ICDE , 2007 .
[ 3 ] K . Chakrabarti , M . Garofalakis , R . Rastogi , and K . Shim . Approximate query processing using wavelets . VLDB J . , 10(2 3):199–223 , 2001 ( also VLDB 2000 ) .
[ 4 ] K . Chakrabarti , E . Keogh , S . Mehrotra , and M . Pazzani . Locally adaptive dimensionality reduction for indexing large time series databases . TODS , 27(2):188–228 , 2002 ( also SIGMOD 2001 ) .
[ 5 ] G . Cormode , M . Garofalakis , and D . Sacharidis . Fast approximate wavelet tracking on streams . In EDBT , 2006 .
[ 6 ] A . Deligiannakis , M . Garofalakis , and N . Roussopoulos . Extended wavelets for multiple measures . TODS , 32(1 ) , 2007 ( also SIGMOD 2003 ) .
[ 7 ] M . Garofalakis and P . B . Gibbons . Probabilistic wavelet synopses . TODS ,
29(1):43–90 , 2004 ( also SIGMOD 2002 ) .
[ 8 ] M . Garofalakis and A . Kumar . Wavelet synopses for general error metrics .
TODS , 30(4):888–928 , 2005 ( also PODS 2004 ) .
[ 9 ] P . B . Gibbons and Y . Matias . Synopsis data structures for massive data sets . In
SODA , 1999 .
[ 10 ] A . C . Gilbert , Y . Kotidis , S . Muthukrishnan , and M . J . Strauss . One pass wavelet decompositions of data streams . IEEE TKDE , 15(3):541–554 , 2003 .
[ 11 ] S . Guha . Space efficiency in synopsis construction algorithms . In VLDB , 2005 . [ 12 ] S . Guha and B . Harb . Wavelet synopsis for data streams : minimizing non euclidean error . In KDD , 2005 .
[ 13 ] S . Guha and B . Harb . Approximation algorithms for wavelet transform coding of data streams . In SODA , 2006 .
[ 14 ] S . Guha , K . Shim , and J . Woo . REHIST : Relative error histogram construction algorithms . In VLDB , 2004 .
[ 15 ] Y . E . Ioannidis . Universality of serial histograms . In VLDB , 1993 . [ 16 ] Y . E . Ioannidis . The history of histograms ( abridged ) . In VLDB , 2003 . [ 17 ] H . V . Jagadish , N . Koudas , S . Muthukrishnan , V . Poosala , K . C . Sevcik , and
T . Suel . Optimal histograms with quality guarantees . In VLDB , 1998 .
[ 18 ] M . Jahangiri , D . Sacharidis , and C . Shahabi . SHIFT SPLIT : I/O efficient maintenance of wavelet transformed multidimensional data . In SIGMOD , 2005 .
[ 19 ] B . Jawerth and W . Sweldens . An overview of wavelet based multiresolution analyses . SIAM Rev . , 36(3):377–412 , 1994 .
[ 20 ] P . Karras and N . Mamoulis . One pass wavelet synopses for maximum error
[ 21 ] P . Karras and N . Mamoulis . The Haar+ tree : a refined synopsis data structure . In metrics . In VLDB , 2005 .
ICDE , 2007 .
[ 22 ] I . Lazaridis and S . Mehrotra . Capturing sensor generated time series with quality guarantees . In ICDE , 2003 .
Indirect 5 Direct 5 Indirect 10 Direct 10
[ 23 ] T . Li , Q . Li , S . Zhu , and M . Ogihara . A survey on wavelet applications in data mining . SIGKDD Explorations Newsletter , 4(2):49–68 , 2002 .
[ 24 ] Y . Matias , J . S . Vitter , and M . Wang . Wavelet based histograms for selectivity estimation . In SIGMOD , 1998 .
[ 25 ] S . Muthukrishnan . Subquadratic algorithms for workload aware Haar wavelet synopses . In FSTTCS , 2005 .
[ 26 ] V . Poosala , V . Ganti , and Y . E . Ioannidis . Approximate query answering using histograms . IEEE Data Eng . Bull . , 22(4):5–14 , 1999 .
[ 27 ] F . Reiss , M . Garofalakis , and J . M . Hellerstein . Compact histograms for hierarchical identifiers . In VLDB , 2006 .
[ 28 ] J . S . Vitter and M . Wang . Approximate computation of multidimensional aggregates of sparse data using wavelets . In SIGMOD , 1999 .
256
512
1024 2048 4096 8192 16384 32768 65536 131072
10
20
30
40
50
60
70
80
90
100
Size n
( a ) Runtime vs . n , B = n/64
Space B
( b ) Runtime vs . B
Figure 9 : Runtime comparison ( Haar+ )
