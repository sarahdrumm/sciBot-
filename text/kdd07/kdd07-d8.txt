A Spectral Clustering Approach to Optimally Combining
Numerical Vectors with a Modular Network
Motoki Shiga
Bioinformatics Center
Kyoto University
Ichigaku Takigawa Bioinformatics Center
Kyoto University
Gokasho Uji 611 0011 , Japan shiga@kuicr.kyoto
Gokasho Uji 611 0011 , Japan takigawa@kuicr.kyoto uacjp uacjp
Hiroshi Mamitsuka Bioinformatics Center
Kyoto University
Gokasho Uji 611 0011 , Japan mami@kuicr.kyoto uacjp
ABSTRACT We address the issue of clustering numerical vectors with a network . The problem setting is basically equivalent to constrained clustering by Wagstaff and Cardie [ 20 ] and semisupervised clustering by Basu et al . [ 2 ] , but our focus is more on the optimal combination of two heterogeneous data sources . An application of this setting is web pages which can be numerically vectorized by their contents , eg term frequencies , and which are hyperlinked to each other , showing a network . Another typical application is genes whose behavior can be numerically measured and a gene network can be given from another data source . We first define a new graph clustering measure which we call normalized network modularity , by balancing the cluster size of the original modularity . We then propose a new clustering method which integrates the cost of clustering numerical vectors with the cost of maximizing the normalized network modularity into a spectral relaxation problem . Our learning algorithm is based on spectral clustering which makes our issue an eigenvalue problem and uses k means for final cluster assignments . A significant advantage of our method is that we can optimize the weight parameter for balancing the two costs from the given data by choosing the minimum total cost . We evaluated the performance of our proposed method using a variety of datasets including synthetic data as well as real world data from molecular biology . Experimental results showed that our method is effective enough to have good results for clustering by numerical vectors and a network .
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications— Data mining ; I53 [ Pattern Recognition ] : Clustering— Algorithms
General Terms Algorithms and Experimentation
Keywords Network modularity , spectral clustering , heterogeneous data sources , k means , eigenvalue problem
1 .
INTRODUCTION
Clustering , a major research subject in data mining , has been successfully applied to a wide variety of areas in the real world . In this paper , we address the issue of clustering numerical vectors with a network . This is a general setting which can be found in a lot of applications and basically equivalent to constrained clustering by Wagstaff and Cardie [ 20 ] and semi supervised clustering by Basu et al . [ 2 ] , but our focus is more on the optimal combination of two heterogeneous data sources , numerical vectors and a network . A typical application is web pages . This case , web pages will be clustered by their contents , say term frequencies , based on the assumption that if the contents of a page are very similar to those of another , these pages can be in the same cluster . In contrast , web pages are linked together , forming a network in which nodes and edges correspond to web pages and hyperlinks between them , respectively , and can be clustered based on the hyperlink connectivity . Clustering web pages based on hyperlinks is exactly graph partitioning . Standard criteria for graph partitioning are ratio cut [ 8 ] and normalized cut [ 15 ] . Simply speaking , these criteria are to minimize the number of inter cluster edges relevant to the size of a cluster . The number of inter cluster edges which is called graph min cut is to check the partitioning performance while the size of a cluster is to avoid a small cluster which might be generated by outliers . In other words , these criteria are obtained by normalizing the graph min cut by the cluster size . Given a network , minimizing normalized cut ( or ratio cut ) is a trace optimization problem which is NP hard . Thus usually this problem is converted by spectral relaxation into an optimization problem with a constraint which can be solved by Lagrange multipliers , and the solution is given by an eigenvalue problem . This is called spectral graph clustering which is difficult to assign a cluster label to each node definitely . Thus usually k means is finally applied to cluster assignments using the resultant eigenvectors .
In the problem setting of graph partitioning , a numerical weight can be attached to each edge of a given graph , and you may think that the similarity between two web pages can be a weight for the hyperlink between them . However , we assume that a given graph and a given set of numerical vectors are independently observed . This assumption is natural . For example , the contents of web pages and their links are independently generated . More concretely , there is a case that two web pages are very similar to each other , even if there are no links between them . Thus we note that our problem cannot be solved directly by using an existing graph partitioning method only .
Another application is genes . Genes are expressed and function in a cell . Currently the quantitative expression of thousands of genes can be measured simultaneously by using the latest technology in genetic engineering , called cDNA microarray . We can have a numerical vector ( generally called a profile ) for each gene by repeating the experiment of cDNA microarray . However cDNA microarray data is very noisy and unreliable . Thus naturally we often need another data source in clustering genes , since precise gene clustering is important in predicting gene function [ 16 ] . We can have more reliable information on genes as a gene network , although they are confined to relatively well studied genes . For example , literature information provides us with the co occurrence frequencies of genes in medical documents which can be turned into a network of genes by setting a cut off value against the frequencies . Similarly , metabolic or gene regulatory networks which are generated from literature are much more reliable than microarray data .
A potential approach for our problem setting would be to integrate the two data sources , numerical vectors and a network . A typical example of this direction is semisupervised clustering based on a hidden Markov random field ( HMRF ) [ 2 ] . Semi supervised clustering by HMRF is clustering numerical vectors by minimizing the objective function containing squared Euclidean distance as well as weighted network constraints . This method was extended to a more general framework in which the objective function can be expressed as a trace optimization problem for which an efficient weighted kernel k means algorithm was proposed [ 11 ] . This work is based on the idea that minimizing the cost ( or the objective function ) of semi supervised clustering by HMRF can be a trace optimization problem , which is true of minimizing the objective function of the weighted kernel k means and more generally , minimizing a graph cut criterion such as normalized cut or ratio cut [ 3 ] . This work inspired us to combine numerical vectors with a network , but we note that our criterion for graph partitioning is clearly different from that in [ 11 ] and our focus is more on optimally combining the two data sources in terms of clustering .
Recent analysis on networks in the real world data have revealed that they have some common global characteristics such as small world phenomena [ 21 ] , scale free property [ 1 ] , self similarity [ 17 ] and hierarchical modularity [ 14 ] . We can expect that the performance of clustering in our problem setting might be improved by using some global network property than the vicinity information like the Markov property . In light of the above , we propose a new method for clustering numerical vectors with a network . We focus on network modularity , a global feature found in a lot of real world networks such as gene networks [ 14 ] and must be a powerful criterion for clustering by the graph connectivity . The orig inal network modularity [ 13 , 7 , 6 ] is , intuitively , the number of intra cluster edges minus the square of the number of inter cluster edges . That is , this measure is given by using only the edges of a graph and is not balanced by the cluster size . Thus we first define normalized network modularity which is obtained by dividing the original network modularity by the cluster size . We then integrate the normalized network modularity with the cost of clustering numerical vectors into the framework of a trace optimization problem . Our clustering algorithm is based on spectral clustering by which our issue is relaxed into an eigenvalue problem and the final clusters are assigned by k means clustering algorithm from the resultant eigenvectors . We stress that our work is an approach for clustering with not only numerical vectors but the network modularity . A significant merit of our method is that we can optimize the weight parameter for balancing the two data sources by choosing that which minimizes the total cost .
We evaluated the performance of our method using three types of datasets including both synthetic and real world data . Our first dataset was synthetic both in numerical vectors and a network . Numerical vectors were generated randomly according to a mixture of von Mises Fisher distributions , and a network was generated by selecting node pairs randomly . We confirmed the effectiveness of our method by checking normalized mutual information ( NMI ) , a measure to check the performance of clustering methods , and the total cost of clustering , with varying the value of the weight parameter for balancing the two data sources . In particular , we found that NMI was mostly maximized at the weight parameter value which minimized the total cost , meaning that our strategy of choosing the weight parameter value of the minimum total cost worked successfully . The second dataset was synthetic numerical vectors and a real metabolic network having the scale free property and unbalanced cluster sizes . From the experimental results on this dataset , we showed that our method of optimally combining numerical vectors with a given network worked favorably against even a real scale free network with unbalanced cluster sizes . The third dataset was real microarray expression profiles corresponding to numerical vectors and the real gene network used in the second dataset . We note that gene expression measured by microarray is heavily noisy and unreliable while the network we used is from a database which is manually curated and trustworthy . Interestingly the resultant weight parameter value was extremely biased to the network information , being consistent with the above reliability fact of the two input data sources .
2 . METHOD 2.1 Preliminaries and Notations
We describe the notations that will be used throughout this paper . Let N be the number of given numerical vectors ( data points ) . Let E be the N × N matrix whose entries are all one . Let X := ( x1,··· , xN ) be given numerical vectors . Each xn has p entries , and let xn(i ) be the i th entry of xn . Let ¯X := ( ¯x1 , ¯x2 , . . . , ¯xN ) where ¯xn := xn/ i=1 xn(i)2 . Y = ¯X T ¯X . Let G be a given network with N nodes and edges . Let W ∈ fiN×N be a non negative , symmetric matrix whose ( i , j ) entry , wij is a non negative weight pPp
PN between nodes i and j . If there is no edge between nodes i and j , wij is zero . We note that in our problem setting , W is an input having all information on a given graph G PN and is often called a weight matrix or an affinity matrix . Let ¯W be a matrix whose ( i , j ) entry ¯wij satisfies that j=1 wij . Let Dd be a N × N diagonal ma¯wij = wij/ j=1 wij . Let trix whose ( i , i ) entry di satisfies that di = D := dT d where d := ( d1,··· , dN ) . Let M be a matrix which satisfies M := D−1 Let K be the number of clusters which is an input . Let I K be the identity matrix of size K . Let Z := ( z1,·· · , zK ) be k = ( zk,1,··· , zk,N ) an unsigned cluster assignment in which zT where zk,n ( ∈ {0 , 1} ) is 1 if xn is in cluster k , otherwise zero . ZT Z . Let — := ( —1,· ·· , —K ) where —k be the rep˜Z := Z√ resentative ( or the cluster center ) of cluster k . Let Zk be a set of nodes in a given graph ( or numerical vectors ) in cluster k , and let |Zk| be the number of all nodes in clusk=1Zk . Let V be a diagonal matrix whose ter k . Z := ∪K ( k , k ) entry is |Zk| . L(Zk,Zk . ) := j∈Zk . wij , and L := L(Z , Z ) . d W .
P
P i∈Zk
Let J be a cost of clustering numerical vectors X ( or/and nodes in network G ) . Let ω be a numerical parameter which takes a value between zero and one , and balances the two data sources , ie numerical vectors X and network G . 2.2 k means
We first briefly review the k means clustering algorithm which is widely used in a lot of applications . The cost ( or the objective function ) of the k means algorithm is given as follows :
Jnum(X,Z ; — ) =
1 N
Dist(xi , —k ) ,
( 1 )
KX
X k=1 i∈Zk where Dist(xi , —k ) is a distance between numerical vector xi and cluster representative —k of cluster k . We can use any distance such as Euclidean distance , cosine similarity and 1−Pearson correlation coefficient . In our experiments , we used cosine similarity which is used for clustering highdimensional data such as text documents [ 25 ] :
!
. n —kp 1 − xT xT n xn
Dist(xi , —k ) =
1 2
The k means algorithm minimizes the cost of Eq ( 1 ) by repeating the following two steps alternately until convergence : 1 ) updating the cluster representative and 2 ) updating cluster labels . Figure 1 shows the pseudocode of the k means clustering algorithm . 2.3 Maximizing Normalized Network Modu larity
231 Spectral Graph Partitioning We briefly review k way graph partitioning which divides nodes of a given graph ( network ) into k clusters . The standard criteria to be minimized in k way graph partitioning are ratio cut and normalized cut which are given as follows :
X X k
L(Zk , Z \ Zk )
|Zk|
L(Zk , Z \ Zk )
L(Zk , Z )
Ratio cut :
Normalized cut : k
——————————————————————– Input : X , K , Z ( 0 ) , —(0 ) Output : Z , — , J k means ( X , K , Z ( 0 ) , —(0 ) ) 1 : Z ← Z ( 0 ) , — ← —(0 ) √ 2 : ¯X ← X/ P 3 : while J( ¯X,Z ; — ) is not converged do 4 :
X T X k ← 1 —(t+1 ) |Z(t ) Z ( t+1 ) ← arg minZ J( ¯X,Z ; —(t+1 ) ) Z ← Z ( t+1 ) , — ← —(t+1 ) , J ← J( ¯X,Z ; — )
5 : 6 : 7 : end while ——————————————————————–
¯xj ( k = 1,··· , K ) j∈Z(t )
| k k
Figure 1 : Pseudocode of k means .
The numerator which is common to the above two cuts is the so called graph min cut . If we use the graph min cut only , the clustering result is very sensitive to outliers . That is , a small cluster might be formed , if this cluster is relatively isolated from other nodes . So we need to normalize the graph min cut by the number of nodes ( ratio cut ) or the total weight ( normalized cut ) in each cluster .
We can see that the above criteria can be rewritten as follows :
Ratio Cut :
Normalized Cut :
X X k k ( Dd − W )zk zT zT k zk k ( Dd − W )zk zT
.
. zT k Dczk k
Finding a set of clusters which minimizes this criterion is an NP hard problem , and we then solve this problem by relaxing the discrete cluster indicator matrix to a real valued one . We can then have the following optimization problem : minimize subject to
( Dd − W ) ˜Z ) tr( ˜Z T ˜ZT ˜Z = I K . ( Ratio Cut ) ( ˜ZT Dd ˜Z = IK . ( Normalized Cut ) )
By using Lagrange multipliers , we can easily find that the solution of this optimization can be an eigenvalue problem . In a standard manner of spectral clustering , after solving the eigenvalue problem , we first select the resultant K − 1 eigenvectors with the minimum eigenvalues . Then , since we cannot directly assign a cluster label to each numerical vector ( or each node in a given graph ) by the resultant eigenvectors , we apply the k means clustering algorithm to the selected K − 1 eigenvectors after their normalization . 232 Maximizing Normalized Network Modularity with Spectral Graph Partitioning
Network modularity is originally defined as follows [ 6 ] :
„
)
«2
Q(G ) =
2ek(G )
L
− gk(G )
L
,
( 2 )
(
KX k=1
.
. where ek(G ) is the number of edges in cluster k and gk(G ) is the total sum of degrees over all nodes in cluster k . As shown in the above , the weight attached to each edge was not considered in the original definition of network modularity .
——————————————————————– Input : W , K , Z ( 0 ) , —(0 ) Output : Z d W
1 : Compute Dd from W . 2 : M ← D−1 3 : Compute H of M . 4 : Normalize H into ¯H . 5 : [ Z , — , J ] ← k means( ¯H , K,Z ( 0 ) , —(0 ) ) ——————————————————————–
Figure 2 : Pseudocode of spectral clustering for normalized network modularity .
This modification allows M to be a very sparse matrix , meaning that we can reduce the computational cost of solving the eigenvalue problem in Eq ( 5 ) . After solving Eq ( 5 ) , we have the resultant K − 1 eigenvectors with the minimum eigenvalues . That is , we can have N × ( K − 1 ) matrix , H = ( h1 , ··· , hK−1 ) where hi be the i th eigenvector of the selected K − 1 eigenvectors . We then normalize this matrix into N × ( K − 1 ) matrix , ¯H in which ( n , k ) entry ¯hnk satisfies ¯hnk = hnk/ nk where hnk is ( n , k ) entry of H . This eigenmatrix ¯H can be an input of k means . Figure 2 shows the pseudocode of this process . 2.4 Proposed Algorithm for Optimally Combining Numerical Vectors with Normalized Network Modularity qPK−1 k=1 h2
241 Spectral Clustering with Numerical Values and a Network
We describe our proposed algorithm for combining two data sources : numerical vectors and a given weighted network .
We first set the cost ( the objective function ) of clustering numerical vectors as follows :
Jnum( ¯X,Z ; — ) =
1 N
KX
X k=1 i∈Zk
( 1 − ¯xT i —k ) ,
1 2 where the cluster representative —k of cluster k is given as follows :
” i —k
X j∈Zk
¯xj
“ 1 − ¯xT 0 @1 − ¯xT X i
1|Zk| i∈Zk j∈Zk
«
—k =
KX KX k=1 k=1
1 2N
1 2N
1 2
− 1 2N
1|Zk| X X KX i∈Zk i∈Zk k=1
„
1 A
¯xj
X j∈Zk
¯xT i ¯xj
1|Zk| X
Jnum( ¯X,Z ) =
=
= and it can then be a trace optimization problem :
Jnum( ¯X , Z ) =
− tr
1 2
ZT ( 2N )
−1Y Z
Z T Z
( 6 )
The solution can be found via Lagrange multipliers in the following typical eigenvalue problem :
The cost of k means can be rewritten as follows :
Hereafter , we incorporate the edge weight into the network modularity , meaning that the binary definition on each edge is turned into a numerical one . We note that the property of the network modularity is totally kept in this extension . Eq ( 2 ) can then be rewritten by using only L as follows :
Q(W ) =
L(Zk,Zk )
−
L(Zk , Z )
L k=1
L
„
)
«2
.
(
KX
This measure is defined by the ( weighted ) number of edges only , meaning that the original modularity considers the number of edges only . More importantly , the original network modularity is not balanced by the cluster size , meaning that a cluster might become small when affected by outliers . Thus we define the new measure which we call normalized network modularity whose cost ( or the objective function ) is given as follows : Jnet(W ,Z ) =
«2 − L(Zk,Zk )
L(Zk,Z )
( „
KX
)
.
L
L
N|Zk| k=1
This equation indicates that the larger negative value of this cost a clustered network has , the higher normalized network modularity this network has . The problem of finding the set of clusters which minimizes this cost is NP hard . We then apply the spectral clustering approach to minimize the cost of normalized network modularity , J(W ,Z ) . In the following derivation , we partly borrow the idea of the spectral graph clustering by White and Smith which was developed for the original network modularity [ 22 ] . We can first modify the problem of minimizing J(W , Z ) into the problem of minimizing the following trace :
`
´
!
W
Z
Jnet(W ,Z ) = tr
Z T N
.
( 3 )
1
L2 D − 1 Z T Z
L
As shown in the spectral graph clustering using ratio ( or normalized ) cut , matrix Z must satisfy the following constraint since each node falls into one cluster only :
Z T Z = V .
We can then replace Z with ˜Z and rewrite the trace optimization problem in the following by relaxing the discrete cluster indicators into real valued indicators :
„
„
«
«
D − 1 L
W
˜Z minimize subject to tr
˜ZT
1 L2 ˜Z T ˜Z = I K . «
1 L2
D − 1 L
W
˜Z = ˜ZΛ ,
„
„
« where Λ is a diagonal matrix of Lagrange multipliers . This can be further modified using ¯W , the normalized W , into :
¯W − 1 L2
1 L
E
˜Z = ˜ZΛ .
( 4 ) When n → ∞ , the second term of Eq ( 4 ) approaches zero faster than the first term . In addition , we can approximate ¯W by D−1 d W . We then approximate Eq ( 4 ) by the following simple eigenvalue problem :
M ˜Z = ˜ZΛ .
( 5 )
We then combine the cost of clustering numerical vectors which is shown in Eq ( 6 ) with the cost of the normalized network modularity which is given in Eq ( 3 ) , using ω for balancing the two costs : Jtotal( ¯X , W ,Z ) ` „
´ = ωJnet(W , Z ) + ( 1 − ω)Jnum( ¯X,Z ) Z
W − 1−ω
) «
( j ff
= tr
ZT
ωN
2N
Y
L2 D − ωN L Z T Z D − ωN L
ωN L2
W − 1 − ω
2N
Y
˜Z
= tr
˜ZT
Finding a set of clusters minimizing the integrated cost is also an NP hard problem , and so we solve this problem by relaxing discrete cluster indicators into real valued ones . By doing so , we can have the following optimization problem : j
„
ωN L2
D − ωN L
W − 1 − ω
2N minimize tr
˜ZT subject to
˜Z T ˜Z = I K
« ff
Y
˜Z
( 7 )
This optimization problem can be also turned by Lagrange multipliers into the following eigenvalue problem for the total cost :
M ω ˜Z = ˜ZΛ ,
( 8 ) where
M ω =
ωN L2
D − ωN L
W − 1 − ω
2N
Y .
Once we have the above eigenvalue problem , we can use the same manner of clustering as done in spectral graph clustering . That is , given N × N matrix M , we can obtain the resultant K − 1 eigenvectors with the minimum eigenvalues , to generate N × ( K − 1 ) matrix S = ( s1 , ··· , sK−1 ) where si is the i th eigenvector of the selected K − 1 eigenvectors . We then use the k means clustering algorithm , after normalizing S into ¯S which is the N × ( K − 1 ) matrix in which ( n , k ) entry ¯snk satisfies ¯snk = snk/ nk where snk is ( n , k ) entry of S . qPK−1 k=1 s2
As we saw in spectral graph clustering , the constraint given in Eq ( 7 ) can be modified into another form . For example , we can use ˜ZT Dd ˜Z = I K which was used for normalized cut in graph partitioning . This case , M ω is given as follows :
M ω = D
− 1 2 d
(
ωN L2
D − ωN L
W − 1 − ω
2N
Y )D
− 1 2 d
.
We used this constraint in our experiments , since normalized cut is more often used in graph partitioning than ratio cut . In addition , White and Smith [ 22 ] also used this modification when they practically applied their spectral graph clustering with the original network modularity to the real world datasets . 242 Estimating ω The parameter ω depends on the spectral space which is generated by combining the two data sources , meaning that the choice of ω will heavily affect the clustering result . So we propose a method to estimate the optimal ω value from In this method , varying ω from given two data sources . zero to one , we choose the ω which gives the minimum total cost , Jtotal . Figure 3 shows the pseudocode of the whole procedure of our proposed algorithm .
X T X
——————————————————————– Input : X , W , K , Z ( 0 ) , —(0 ) Output : Z √ 1 : ¯X ← X/ 2 : Y ← ¯X T ¯X 3 : Compute Dd and D from W 4 : for ω = 0 to 1 do 5 : M ω ← D L2 D − ωN ( ωN Compute S of M ω . 6 : 7 : Normalize S into ¯S . [ Zω , —ω , Jω ] ← k means( ¯S , K,Z ( 0 ) , —(0 ) ) 8 : 9 : end for 10 : ωmin ← arg minω Jω 11 : Z ← Zωmin ——————————————————————–
W − 1−ω
Y )D
− 1 2 d
− 1 2 d
2N
L
.
Figure 3 : Pseudocode of the proposed algorithm .
3 . EXPERIMENTS 3.1 Dataset 1 : Synthetic Numerical Vectors and
Synthetic Random Network
311 Data 1 . Synthetic Numerical Vectors : We first assume that numerical vectors are randomly generated according to a mixture of von Mises Fisher distributions [ 12 ] in which each component corresponds to a cluster :
KX
Z π
0 p(x ) =
αkcp(κ)e
κ—T k x
, k=1
PK where αk ( k = 1 , ··· , K ) are mixture proportions satisfying k=1 αk = 1 and 0 ≤ αk ≤ 1 , and cp(κ ) is given as that follows :
κp/2−1 cp(κ ) =
( 2π)p/2Ip/2−1(κ )
, where Ip/2−1(κ ) is the type 1 Bessel function of order p which is given as follows :
Ip(κ ) =
1 π
κ cos t e cos(pt)dt .
3
√
2 ,−0.5)T , —4 = ( −0.5 , − √
Interested readers should refer [ 4 ] on the method for randomly generating numerical values according to this distribution . We used the following settings : K = 4 , p = 3 , αk = 0.25 ( k = 1 , . . . , 4 ) , —1 = ( 0 , 0 , 1)T , —2 = ( 0 , 0 , −0.5)T , —3 = ( −0.5 , 2 ,−0.5)T . Another parameter , κ which is called concentration parameter , behaves like the inverse of the variance of numerical vectors in a cluster . Thus numerical vectors which are generated with a larger κ will be more concentrated on cluster representatives and be more easily clustered . Thus we changed κ in our experiments to check the effect of κ on clustering results .
3
2 . Synthetic Random Network : To generate a network , we first assigned a cluster label to each node . We generated a network in which the number of nodes and the number of edges in each cluster are kept the same for all clusters . Thus the generated network was defined by three parameters : Nv ( the number of nodes in a cluster ) , Ne ( the
1
0
3 s
1 1
1
0
3 s
1 1
1
0
3 s
1 1
0 s 2
1
1 ( a )
1
0 s 1
0 s 2
1
1 ( b )
1
0 s 1
0 s 2
1
1 ( c )
1
0 s 1
Figure 4 : Dataset 1 : The distribution of eigenvectors sn ( n = 1 , . . . , 400 ) which are shown by four different symbols ( ◦ , ∗ , +,fi ) corresponding to four different clusters at Nin = 250 , κ = 5 , ( a ) ω = 0 and J = 0.0932 , ( b ) ω = 0.3 and J = 0.0538 and ( c ) ω = 1 and J = 00809
2 L(Zk,Zk ) ( k = 1 , ··· , K ) . We total number of edges ) and Nin ( the number of edges in a cluster ) which is equal to 1 then chose a value of Nv to generate a set of nodes and randomly chose node pairs which are connected by edges to satisfy the values of Ne and Nin . In Dataset 1 , we used Nv = 100 ( meaning that the total number of nodes is 400 ) and Ne = 1 , 600 , while Nin was changed to check how the clustering performance is affected by Nin .
312 Performance Results To evaluate our clustering results using true cluster labels , we used normalized mutual information ( NMI ) [ 18 ] which has been used in a lot of applications to measure the performance of clustering methods [ 24 ] . A larger NMI value indicates a better clustering result . Interested readers should see [ 18 ] for the detail of NMI . We first checked the distribution of sn ( n = 1,··· , 400 ) , ie the resultant eigenvectors of the eigenvalue problem in Eq ( 8 ) , when we fixed κ = 5 and Nin = 250 . Figure 4 shows the three distributions of eigenvectors which are shown in four different symbols ( ◦ , ∗ , +,fi ) corresponding to four different clusters , when ω was at 0 , 0.3 and 1 . This figure shows that the distribution of eigenvectors changes with varying ω . In particular , we can see that when ω = 0.3 , the eigenvectors were separated most clearly among the three cases . In fact , when ω=0 , 0.3 and 1 , J was 0.0932 , 0.0538 and 0.0809 , respectively , indicating that eigenvectors at ω = 0.3 were the most concentrated on the cluster representatives among the three cases of ω .
We then checked the effectiveness of our method of optimally combining two different data sources by using the cost J and NMI , when ω is changed . We used each data set of all combinations of κ = 1 , 5 and 50 and Nin = 250 , 280 and 300 . Figure 5 shows J of all these cases , and Figure 6 shows NMI of all these cases . From these figures , first of all , we can see that with increasing Nin ( (a ) → ( b ) → ( c) ) , the modularity became higher , resulting with decreasing the cost ( J ) and increasing NMI . This might be clearer if we focus on the ω of one . On the other hand , we can see that with increasing κ ( 1 → 5 → 50 ) , numerical vectors were more concentrated on their cluster representatives , resulting with decreasing the cost ( J ) and increasing NMI . This might be also clearer if we focus on the ω of zero .
In all 18 curves in Figures 5 and 6 , the best value is obtained when 0 < ω < 1 , indicating that combining two data sources improved the cost J and NMI of ω = 0 and ω = 1 . In particular , we emphasize that the ω value of the minimum J was mostly consistent with that of the maximum NMI . For example , when Nin = 250 and κ = 1 , J was minimized at ω = 0.5 and the maximum NMI was at ω = 04 Similarly , at κ = 5 , the minimum J was at ω = 0.4 where the maximum NMI was obtained . This was true of Nin = 280 and κ = 1 where ω = 0.5 provided the best in both NMI and J . These results imply that our method of selecting ω worked effectively for optimally combining numerical vectors with a given network . An interesting finding is that in a balanced case in which the cost ( and NMI ) of ω = 0 is almost the same as that of ω = 1 , the curve became concave ( and convex for NMI ) , indicating that the minimum cost ( and the maximum NMI ) can be easily found . On the other hand , in an unbalanced case such as κ = 1 and Nin = 310 in ( c ) , the curve was not necessarily concave ( and convex for NMI ) , meaning that only one data source ( ie network information ) , is much more informative for clustering than the other ( ie numerical vectors ) . This is natural , because numerical vectors at κ = 1 were widely distributed , not concentrating on the cluster representatives , and it would be difficult to do clustering by them , comparing to the case of Nin = 310 where clustering could be relatively easy by using network information only . Thus in such a case , our method might choose ω = 1 ( or ω = 0 ) , but this would be the right selection , since the NMI at ω = 1 ( or ω = 0 ) must be the maximum or very close to the maximum in this case . Using the same parameter setting as that in Figure 6 , we finally checked NMI by repeating randomly generating datasets 400 times and averaging the performance over them . Figure 7 shows the averaged NMI obtained by this experiment . The curves in this figure were almost similar to those in Figure 6 , implying that our results were very stable . Totally we can say that our method optimally combined the two synthetic data sources .
) J ( t s o C
0.12
0.1
0.08
0.06
0.04
0.02
0
0
I
M N
1
0.8
0.6
0.4
0.2
0
0
1
κ = 1 κ = 5 κ = 50
0.12
0.1
0.08
0.06
0.04
0.02
0
0
0.5 ω ( a )
κ = 1 κ = 5 κ = 50
1
0.12
0.1
0.08
0.06
0.04
0.02
0
0
0.5 ω ( b )
Figure 5 : Dataset 1 : J at Nin = ( a ) 250 , ( b ) 280 and ( c ) 310 .
1
1
0.8
0.6
0.4
0.2
0
0
κ = 1 κ = 5 κ = 50
0.5 ω ( a )
1
κ = 1 κ = 5 κ = 50
1
0.8
0.6
0.4
0.2
0
0.5 ω ( b )
Figure 6 : Dataset 1 : NMI at Nin = ( a ) 250 , ( b ) 280 and ( c ) 310 .
1
1
κ = 1 κ = 5 κ = 50
0.5 ω ( c )
κ = 1 κ = 5 κ = 50
ω 0.5 ( c )
3.2 Dataset 2 : Synthetic Numerical Vectors and
Real Scale free Network
321 Data 1 . Synthetic Numerical Vectors : Numerical values of a gene , such as gene expression , are usually measured experimentally , meaning that these values are noisy , comparing to the network which we can derive from a curated database in molecular biology1 . Thus in this experiment , we generated synthetic numerical vectors to check the performance of our method of combining two data sources .
As we used a real metabolic network of 636 Saccharomyces cerevisiae genes ( See below the way to generate this network. ) , we first assigned a cluster label to each node of the network in the following way : 1 ) We fixed the number of clusters and repeated running spectral graph clustering by White and Smith [ 22 ] 1,000 times on the real metabolic network , measuring the original network modularity on the resultant clusters at each time . 2 ) Out of the 1,000 runs , we then chose the clusters with the highest network modularity2 , and these clusters were used to assign a cluster label
1We show this fact more clearly in the experiment using Dataset 3 . 2We note that the clusters with the highest modularity can to each node . That is , these clusters were used as standard data for evaluation .
We then generated numerical vectors ( corresponding to nodes in the metabolic network ) in each cluster , assuming that they can be generated according to a component of the von Mises Fisher distribution mixture as in Section 311 We used the following settings : K = 10 , p = 5 , αk = 0.1 ( k = 1,·· · , 10 ) , —1 = ( 1 , 0 , 0 , 0 , 0)T , —2 = ( −1 , 0 , 0 , 0 , 0)T , —3 = ( 0 , 1 , 0 , 0 , 0)T , —4 = ( 0,−1 , 0 , 0 , 0)T , —5 = ( 0 , 0 , 1 , 0 , 0)T , —6 = ( 0 , 0,−1 , 0 , 0)T , —7 = ( 0 , 0 , 0 , 1 , 0)T , —8 = ( 0 , 0 , 0,−1 , 0)T , —9 = ( 0 , 0 , 0 , 0 , 1)T , —10 = ( 0 , 0 , 0 , 0,−1)T . We changed κ to check how clustering results are affected by κ .
2 . Real Metabolic Network : Metabolism is represented by a directed graph , called a metabolic pathway which shows biochemical processes of synthesizing small molecules in a cell . Each node is labeled by a chemical compound . A directed edge from a node , say node A , to another , say node B , is a chemical reaction , meaning that the compound corresponding to node B is synthesized from that for node A , and each edge is labeled by a ( enzyme ) gene which catalyzes the corresponding chemical reaction . From a metabolic path not be obtained so easily by our method of ω = 1 , since in spectral graph clustering , the final solution is obtained by k means and is always an approximation .
I
M N e g a r e v A
1
0.8
0.6
0.4
0.2
0
0
1
1
0.8
0.6
0.4
0.2
0
0
κ = 1 κ = 5 κ = 50
0.5 ω ( a )
1
κ = 1 κ = 5 κ = 50
1
0.8
0.6
0.4
0.2
0
0
0.5 ω ( b )
1
κ = 1 κ = 5 κ = 50
0.5 ω ( c )
Figure 7 : Dataset 1 : Average NMI over 400 replicates at Nin = ( a ) 250 , ( b ) 280 and ( c ) 310 .
0.15
) J ( t s o C
0.1
0.05
0
0
κ = 10 κ = 10 2 κ = 10 3
1
I
M N
0.9
0.8
0.7
0.6
1
0
0.5 ω ( a )
1
κ = 10 κ = 102 κ = 103
0.5 ω ( b )
Figure 8 : Dataset 2 : Cost ( J ) and NMI . way which is stored in the KEGG ( Kyoto Encyclopedia of Genes and Genomes ) database [ 10 ] , we generated a network by connecting two genes by an undirected edge , if they catalyze two neighboring chemical reactions in the metabolic pathway . The generated network which we call metabolic network had 636 nodes and 3,104 edges . Most importantly , this network has two important network features : the scalefree property [ 1 ] as well as hierarchical network modularity [ 14 ] .
322 Performance Results We used NMI again to validate the clustering results obtained by our method , with the standard data for evaluation . Figure 8 ( a ) shows the total cost J with changing ω , and Figure 8 ( b ) shows NMI with changing ω . The results we can derive from these figures were mostly consistent with those obtained by Dataset 1 . For example , at κ = 102 and 103 where the distribution of numerical vectors was relatively concentrated on their cluster representatives , the curves could be concave for J ( and convex for NMI ) , and the ω value of the minimum cost and the ω value of the maximum NMI were easily found . Furthermore they were In addition , at κ = 1 mostly consistent with each other . where numerical vectors were distributed broadly , the curve was not necessarily a strong concave for the cost J ( and con vex for NMI ) , implying that this case , the network information would be more useful for clustering than the numerical vectors . In fact , NMI at ω = 1 reached around 0.9 , a very high value , whereas that at ω = 0 stays at less than 06
Figure 9 shows the clustering results obtained by our method at three different ω values when κ = 10 . Ten different colors correspond to ten different clusters . Figure 10 shows the true cluster labels we used for both evaluation and generating numerical vectors . From these figures , we can easily see that the clustering result at ω = 0.5 was the most similar to the true cluster labels among the three networks in Figure 9 . For example , the center part of the true clusters were colored orange and dark blue , and this was consistent with the network at ω = 0.5 in Figure 9 ( b ) . On the other hand , this center part has a lot of different colors at ω = 0 in Figure 9 ( a ) and is colored dark blue only at ω = 1 in Figure 9 ( c ) .
From these results , we can say that our method worked effectively for Dataset 2 which contains a real metabolic network with the scale free property as well as unbalanced cluster sizes . 3.3 Dataset 3 : Real Numerical Vectors and Real
Scale free Network
331 Data 1 . Real Numerical Vectors : Microarray Gene Expression We used a microarray gene expression dataset [ 9 ] which has 300 expression profiles ( numerical vectors ) while around 200 missing values only . This dataset has been often used in the literature of microarray expression analysis [ 26 , 23 ] . All missing values were interpolated by using the 10nearest least square method by [ 19 ] .
2 . Real Metabolic Network : We used the real metabolic network in Dataset 2 . 332 Performance Results In order to evaluate the clustering result by our method , we used ten categories in metabolism which were stored in the KEGG database . At least one of the ten categories could be assigned to each metabolic gene . We note that these ten categories are not defined directly from the metabolic pathways in the KEGG database , and so these ten clusters cannot be identified by using the metabolic network in our
( a )
( b )
( c )
Figure 9 : Dataset 2 : Clustered metabolic networks at κ = 10 and ω= ( a ) 0 , ( b ) 0.5 and ( c ) 1 .
) J ( t s o C
0.16
0.14
0.12
0.1
0.08
0.06
0.04
0.02 0
0.4
0.35
0.3
0.25
0.2
0.15
0.1
I
M N
0.2
0.4
ω
0.6
0.8
1
0.05
0
0.2
( a )
Proposed method Normalized cut Ratio cut
0.6
0.8
1
0.4
ω ( b )
Figure 10 : Dataset 2 : True cluster labels . experiment only . We then used NMI again to validate the performance of our clustering result .
Figure 11 ( a ) shows the cost J of our method with changing ω . Figure 11 ( b ) shows NMI of our method with changing ω . The curves in these figures were not strong concave for J ( and convex for NMI ) , meaning that the two data sources are heavily unbalanced as pointed out in the experimental results of Dataset 1 and Dataset 2 . In fact , by looking carefully , we can see that the minimum cost was at around ω = 0.8 to 0.9 where the best NMI was obtained , and the difference between the best NMI and NMI at ω = 1 was insignificant . Also Figure 11 ( b ) shows the averaged result over 200 runs of each of the two spectral graph partitioning methods using normalized cut and ratio cut . We note that
Figure 11 : Dataset 3 : Cost ( J ) and NMI . these methods used graph information only , and the results of them are shown as dotted straight lines in the figure . Our method significantly outperformed these two methods even at ω = 1 , implying that normalized network modularity is more effective for clustering than normalized cut and ratio cut .
We repeated the same experiment replacing the above microarray dataset with datasets derived from a large microarray database [ 5 ] , and found that the results were almost similar to the above case . From this result , we can say that the metabolic network derived from a curated database is more reliable in clustering metabolic genes than microarray expression . This result is consistent with existing understanding on data reliability in molecular biology .
For Dataset 3 , the ω for the minimum cost took a value which was very close to one , around where NMI was maximum or very close to the maximum . Thus we think that our method succeeded for this dataset . As well in Dataset 2 , our method worked favorably for optimally combining the real metabolic network with more reliable numerical vectors . Thus if we have more reliable numerical vectors on genes , the clustering result would be improved much more . Over all we can say that our method itself is very promising .
4 . CONCLUDING REMARKS
We have presented a new spectral approach to clustering numerical vectors with a network . The focus of our method was on network modularity , a key network property in clustering , and we defined a new criterion , normalized network modularity , for combining the two different data sources in the framework of spectral clustering . A significant advantage of our method is that we can optimize the weight parameter for balancing the two data sources , ie numerical vectors and a network . Also our algorithm is time efficient , and practical computation time was less than one minute for any dataset in our experiment . Experimental results obtained by using three different types of datasets showed that our method worked favorably for optimally combining numerical vectors and a network .
In this paper , we have focused on two data sources , ie numerical vectors and a network , both in methodology and experiments . Our method can be easily extended to a more general framework for combining multiple heterogeneous data sources for clustering , and by doing so , the resultant clustering performance might be improved by adding another different data source . Thus interesting future work is to apply the proposed method to a variety of real world datasets to characterize more systematically under which the proposed method works well . And this would be performed by not only two different data sources but also more than two heterogeneous data sources , say numerical vectors , a network and another different type of dataset . It would also be interesting to develop , in the context of clustering , a general criterion which can cover a lot of distances for numerical values as well as graph partitioning criteria such as normalized cut , ratio cut and network modularity .
5 . ACKNOWLEDGMENTS
This work is supported in part by Bioinformatics Education Program ” Education and Research Organization for Genome Information Science ” and Kyoto University 21st Century COE Program ” Knowledge Information Infrastructure for Genome Science ” with support from MEXT , Japan .
6 . REFERENCES
[ 1 ] A L Barab´asi and A . Reka . Emergence of scaling in random networks . Science , 286:509–512 , 1999 .
[ 2 ] S . Basu , M . Bilenko , and R . J . Mooney . A probabilistic framework for semi supervised clustering . In KDD , pages 59–68 , August 2004 .
[ 3 ] I . S . Dhillon , Y . Guan , and B . Kulis . Kernel k means , spectral clustering and normalized cuts . In KDD , pages 551–556 , 2004 .
[ 4 ] I . S . Dhillon and S . Sra . Modeling data using directional distributions . Technical Report TR 06 03 , University of Texas , Dept . of Computer Sciences , 2003 .
[ 5 ] R . Edgar , M . Domrachev , and A . E . Lash . Gene expression omnibus : NCBI gene expression and hybridization array data repository . NAR , 30(1):207–210 , 2002 .
[ 6 ] R . Guimera and L . A . Nunes Amaral . Functional cartography of complex metabolic networks . Nature , 433(7028):895–900 , 2005 .
[ 7 ] R . Guimera , M . Sales Pardo , and L . A . N . Amaral . Modularity from fluctuations in random graphs and complex networks . Phys . Rev . E , 70:025101 , 2004 .
[ 8 ] L . Hagen and A . B . Kahng . New spectral methods for ratio cut partitioning and clustering . IEEE TCAD , 11:1074–1085 , 1992 .
[ 9 ] T . R . Hughes et al . Functional discovery via a compendium of expression profiles . Cell , 102(1):109–126 , 2000 .
[ 10 ] M . Kanehisa et al . From genomics to chemical genomics : new developments in KEGG . NAR , 34:D354–357 , 2006 .
[ 11 ] B . Kulis , S . Basu , I . Dhillon , and R . J . Mooney .
Semi supervised graph clustering : A kernel approach . In ICML , pages 457–464 , 2005 .
[ 12 ] K . V . Mardia and P . E . Jupp . Directional Statistics .
John Wiley & Sons , second edition , 2000 .
[ 13 ] M . E . J . Newman and M . Girvan . Finding and evaluating community structure in networks . Phys . Rev . E , 69:026113 , 2004 .
[ 14 ] E . Ravasz et al . Hierarchical organization of modularity in metabolic networks . Science , 297(5589):1551–1555 , 2002 .
[ 15 ] J . Shi and J . Malik . Normalized cuts and image segmentation . IEEE PAMI , 22(8):888–905 , 2000 .
[ 16 ] M . Shiga , I . Takigawa and H . Mamitsuka . Annotating gene function by combining expression data with a modular gene network . To appear in ISMB , 2007 .
[ 17 ] C . Song , S . Havlin , and H . A . Makse . Self similarity of complex networks . Nature , 433:392–395 , 2005 .
[ 18 ] A . Strehl and J . Ghosh . Relationship based clustering and visualization for high dimensional data mining . INFORMS Journal on Computing , 15(2):208–230 , 2003 .
[ 19 ] O . Troyanskaya et al . Missing value estimation methods for DNA microarrays . Bioinformatics , 17(6):520–525 , 2001 .
[ 20 ] K . Wagstaff and C . Cardie . Clustering with instance level constraints . In ICML , pages 1103–1110 , 2000 .
[ 21 ] D . J . Watts and S . H . Strogatz . Collective dynamics of
’s mall world’ networks . Nature , 393:440–442 , 1998 .
[ 22 ] S . White and P . Smyth . A spectral clustering approach to finding communities in graphs . In SDM , pages 76–84 , 2005 .
[ 23 ] L . F . Wu et al . Large scale prediction of saccharomyces cerevisiae gene function using overlapping transcriptional clusters . Nat . Genet . , 31(3):255–265 , 2002 .
[ 24 ] S . Zhong and J . Ghosh . A unified framework for model based clustering . JMLR , 4:1001–1037 , 2003 .
[ 25 ] S . Zhong and J . Ghosh . Generative model based document clustering : A comparative study . KAIS , 8(3):374–384 , 2005 .
[ 26 ] X . Zhou , M . C . Kao , and W . H . Wong . Transitive functional annotation by shortest path analysis of gene expression data . PNAS , 99(20):12783–12788 , 2002 .
