Mining Correlated Bursty Topic Patterns from
Coordinated Text Streams
Xuanhui Wang , ChengXiang Zhai , Xiao Hu , Richard Sproat
University of Illinois at Urbana Champaign fxwang20 , czhai , xiaohu , rwsg@uiuc.edu
Urbana , IL 61801
ABSTRACT Previous work on text mining has almost exclusively focused on a single stream . However , we often have available multiple text streams indexed by the same set of time points ( called coordinated text streams ) , which offer new opportunities for text mining . For example , when a major event happens , all the news articles published by different agencies in different languages tend to cover the same event for a certain period , exhibiting a correlated bursty topic pattern in all the news article streams . In general , mining correlated bursty topic patterns from coordinated text streams can reveal interesting latent associations or events behind these streams . In this paper , we define and study this novel text mining problem . We propose a general probabilistic algorithm which can effectively discover correlated bursty patterns and their bursty periods across text streams even if the streams have completely different vocabularies ( eg , English vs Chinese ) . Evaluation of the proposed method on a news data set and a literature data set shows that it can effectively discover quite meaningful topic patterns from both data sets : the patterns discovered from the news data set accurately reveal the major common events covered in the two streams of news articles ( in English and Chinese , respectively ) , while the patterns discovered from two database publication streams match well with the major research paradigm shifts in database research . Since the proposed method is general and does not require the streams to share vocabulary , it can be applied to any coordinated text streams to discover correlated topic patterns that burst in multiple streams in the same period .
Categories and Subject Descriptors : H33 [ Information Search and Retrieval ] : Clustering , Text Mining
General Terms : Algorithms
Keywords : Correlated bursty patterns , coordinated streams , clustering , reinforcement .
1 .
INTRODUCTION
Text streams are ubiquitous and are often naturally formed as new information is incrementally created and accumulated . For example , newswires publish news articles everyday on the Web to report new events to users , generating news streams in different languages such as English , Chinese , and Spanish . Search engines accept and answer end users’ queries from all over the world continuously , creating streams of queries . Researchers publish scientific papers year by year , forming literature streams . Blog authors regularly publish blog articles , forming a dynamic stream of blog articles .
One interesting characteristic of a text stream is that there is often an intensive coverage of some topic within a certain period , which we refer to as a bursty topic pattern . For example , when a major event happens in the world , all news articles tend to have intensive coverage of the event ; as a result , there would be a coverage burst of the topic lasting for a certain period . Similarly , when a new research direction is opened up in a research field , many publications in the new direction tend to be generated , again forming a bursty pattern about the research topic . Mining such bursty topic patterns can help reveal the underlying events and has potentially many applications , such as monitoring opinions , analyzing trends , and summarizing the major topics in a text stream .
So far , text mining research has almost exclusively focused on mining one single text stream . For example , the Topic Detection and Tracking ( TDT ) work [ 5 , 24 , 23 , 4 ] has focused on detecting new events and tracking known events in a single news article stream . Other work on extracting bursty patterns has also focused on only one single stream ( see , eg , [ 18 , 19 , 12 , 8 , 15 , 14] ) . However , we often have available multiple related text streams indexed by the same set of time points ( called coordinated text streams ) , which offer new opportunities for text mining . In particular , we may discover correlated bursty topic patterns from multiple coordinated text streams . A correlated bursty topic pattern refers to simultaneous bursting of some related topics in all the text streams ; it is often associated with some underlying event that has influenced the generation of all the text streams involved .
For example , when a major event happens , all the news articles published by different agencies in different languages tend to cover the same event for a certain period , exhibiting a correlated bursty topic pattern in all the news article streams . Exploiting multiple streams to detect the latent events would be more accurate than using only one single stream as the latter may not be able to distinguish a global event from a local event , leading to mixed mining results . Also , when there is a major shift in research paradigm in a field , all the journals or conferences in the field will likely have a coverage burst of the new research paradigm ; again , mining multiple journals or conferences can recover the research paradigm shift more accurately than using only one single publication source .
In general , mining correlated bursty topic patterns from coordinated text streams is quite interesting for several reasons : ( 1 ) It can help discover interesting common ( causal ) events that have influenced all the streams . ( 2 ) It can reveal interesting associations and linkages between the involved streams . ( 3 ) It can help discover \local" ( ie , streamspecific ) patterns more accurately by factoring out the \global noise." For example , identifying correlated bursty topic patterns from news streams in different natural languages such as English and Chinese can not only reveal the same major events covered by both streams , but also create associations of English terms and Chinese terms ; such associations would be very useful for cross lingual information retrieval , integration , and summarization [ 22 , 20 ] . The discovered common events in multiple news streams can also facilitate discovery of local events specific to one stream ( eg , Chinese news ) which would otherwise have be mixed with the common/global events .
In this paper , we define and study the novel problem of mining correlated bursty topic patterns from multiple coordinated text streams . We propose probabilistic mixture models which can identify the bursty patterns and their bursty periods from coordinated streams simultaneously even if the streams have completely different vocabularies ( eg , English and Chinese ) . The basic idea of our approach is to introduce a latent cause variable to model the underlying events to be discovered and model the text data in multiple streams with a mixture model involving multinomial component topic models . Each topic model is a word distribution with the high probability words indicating the topic content . We do not require the multiple streams to share the same vocabulary ; instead , we rely on the correlation between the time distributions of topics to \align" topics from multiple streams . Thus the proposed methods can actually be applied to any discrete data streams , though we have only evaluated it using text streams in this paper . By fitting such a model to the available text streams using the Expectation Maximization ( EM ) algorithm , we can obtain the topic models associated with each value of the latent cause variable ; these topic models together with their peaking time periods are taken as the correlated bursty topic patterns that we want to discover .
We further propose two extensions to this basic mining approach : ( 1 ) We incorporate local dependency ( along the time line ) into the mixture model to further favor a topic model that can explain well all the documents in a consecutive time period . This allows us to discover consecutive bursty periods . ( 2 ) We propose a mutual reinforcement method which allows multiple streams to work together to further improve the quality of the identified correlated bursty patterns by selecting terms that truly have strong global correlations across all the streams .
We test the proposed methods on two data sets { news streams and literature streams . Experiment results show that the proposed methods can effectively discover quite meaningful topic patterns from both data sets : the patterns discovered from the news data set can accurately reveal the major common events covered in the two streams of news articles ( in English and Chinese , respectively ) , while the patterns discovered from two database publication streams match well with the major research paradigm shifts in database research . The proposed two extensions ( ie , local dependency and mutual reinforcement ) are also both effective for further improving the quality of discovered patterns .
The rest of the paper is organized as follows . We first review the related work in Section 2 . Then we define the problem of mining coordinated streams in Section 3 and present the proposed mining methods in Section 4 . We report the experimental results in Section 5 and conclude in Section 6 .
2 . RELATED WORK
Our work is related to several lines of work in text mining , stream data mining , and multilingual natural language processing .
First , the work on Topic Detection and Tracking ( TDT ) [ 4 , 5 , 24 , 23 ] all aims to detect and track events from a stream of news stories , thus is related to our work . However , this body of work all considers a single news stream and does not address the issue of multiple subtopics within a news article . Our work is more related to the retrospective version of TDT [ 24 ] where the whole stream is analyzed . A main difference between our work and the TDT work is that we consider multiple coordinated text streams and mine correlated bursty topic patterns .
Second , bursty patterns or events are recently studied[18 , 19 , 12 , 8 ] . In [ 12 ] , an infinite automaton was proposed to identify bursty features and their bursty structures ; it has been used in [ 13 ] to identify the bursty evolution of blogspace . However , the work is restricted in only identifying bursty features one by one and does not group the features to find interesting topic patterns . In [ 18 , 19 ] and [ 8 ] , bursty features are identified heuristically with multiple steps . For example , in [ 18 , 19 ] , for each named entity and noun phrase in the stream , 2 tests are performed to identify the days in which the test scores are higher than a threshold . In [ 8 ] , binomial distributions are calculated to identify the bursty features . All these methods process the features one by one and only a single stream is analyzed . One problem of such methods is that the results are often quite sensitive to some noisy features which may be incidentally bursty and the bursty pattern is not meaningful . Our method is more robust since it identifies bursty patterns by pooling together many words which share similar patterns . Furthermore , in [ 18 , 19 ] and [ 8 ] it is shown to be difficult for their methods to find long consecutive time periods . In contrast , our methods ( especially the local dependency model ) can help find long consecutive periods of bursty patterns .
Data streams and time series data are extensively studied in the database and data mining communities [ 9 , 1 ] . Much of the emphasis there is on similarity search , which is to find similar time series sequences given a time series query ( eg , [ 3 , 21] ) , and on classification or incremental clustering of data streams ( eg,[11 , 2] ) .
Temporal information has also been used to identify semantically similar search engine queries [ 7 ] , to integrate multilingual information [ 20 ] , and to acquire lexical associations for transliteration and translation [ 17 ] . We propose a mixture model for coordinated text streams with temporal information . It is an extension of Probabilistic Latent Semantic Analysis ( PLSA ) [ 10 ] and is also related to some other recent extensions such as [ 25 , 16 ] .
3 . PROBLEM FORMULATION
In this section , we formally define the problem of mining correlated bursty topic patterns from multiple coordinated text streams . We first define text stream .
Definition 1
( Text Stream ) . A text stream S of length n and with vocabulary V is an ordered sequence of text samples ( S1 ; S2 ; :: : ; Sn ) indexed by time , where Si is a sequence of words from the vocabulary set V at time point i .
For example , in a news article stream S , Si could be a concatenation of all the news articles published on date i , while in a search engine query log stream S0 , S0 i could be all the queries sent to the search engine on date i .
Definition 2
( Coordinated Text Streams ) . A set of text streams is called coordinated text streams if all the streams share the same time index and have the same length . Formally , a set of m coordinated text streams with length n is S = fS1 ; :: : ; Smg , where Si = ( Si1 ; :: : ; Sin ) is the i th stream with vocabulary Vi . Sij is the text sample at time point j in the i th stream , thus it consists of a sequence of words from Vi .
Note that we allow each stream Si to have a potentially distinct vocabulary set Vi ; this allows us to conveniently model text streams in different natural languages such as English , Spanish , and Chinese .
In order to define the concept correlated bursty topic pat tern , we first define topic .
Definition 3
( Topic ) . A topic in stream Si is defined as a probability distribution of words in vocabulary set Vi . We also call such a word distribution a topic model .
Using a word distribution ( ie , unigram language model ) to represent a topic has been quite common in text mining ( see eg [ 10 , 6 , 25 , 16] ) . Intuitively , a topic model would assign high probabilities to those words that can characterize the topic well . For example , the topic model about the 9{11 terrorist attack may have high probabilities for words such as \attack" , \terror" , \terrorist" , \Afghan" , and \Bin Laden" , but very small probabilities for words such as \Olympic" , \game" , \sport" , and \swimming" , whereas the topic model about an Olympic swimming event would likely be the opposite .
While any topics that we can discover from a text stream would be interesting , we are particularly interested in a special kind of topics which we refer to as \bursty topics." These are topics that are covered intensively within a relatively long consecutive time period in a stream . Bursty topics are interesting because they tend to be associated with some major events , and discovering such events is our goal . We now formally define a bursty topic .
Definition 4
( Bursty Topic ) . Let be a topic ( model ) in stream Si . Let t 2 [ 1 ; n ] be a time index variable and p(jt ; Si ) be the relative coverage of the topic at time t in stream Si . is a bursty topic in stream Si if 9t1 ; t2 2 [ 1 ; n ] such that t2 , t1 and 8t 2 [ t1 ; t2 ] , p(jt ; Si ) where is a span threshold and is a coverage threshold . Intuitively , the first condition ensures that the topic is covered in the stream for a relatively long consecutive period ; the second ensures that the coverage of the topic is relatively intensive .
Finally , we define a correlated bursty topic pattern , which is a set of topics ( each from a different stream ) that are bursty during the same time period . Formally ,
Definition 5
( Correlated Bursty Topic Patten ) . A correlated bursty topic pattern in a set of coordinated text streams S = fS1 ; :: : ; Smg is defined as a set of topics f1 ; :: : ; mg such that i is a bursty topic in stream Si and 9t1 ; t2 2 [ 1 ; n ] such that t2 , t1 and 8t 2 [ t1 ; t2 ] , 8i 2 [ 1 ; m ] , p(ijt ; Si ) where is a span threshold and is a coverage threshold .
According to these definitions , the problem of mining correlated bursty topic patterns from a set of coordinated text streams mainly involves three challenges : ( 1 ) We need to discover bursty topics from each stream . As we will show later , a straightforward application of existing approaches to topic discovery cannot effectively detect topics that are bursty . ( 2 ) We need to locate the bursty period of a bursty topic . The coverage of a bursty topic may be uneven and unsmooth even during the bursty period , making it a challenge to accurately detect the bursty period boundaries . ( 3 ) Since the topics forming a correlated bursty topic pattern must be bursty during the same period in multiple streams , we need to coordinate the discovery of bursty topics in all the streams to more effectively focus on the truly correlated topics . In particular , the discovery of topics in one stream should pay attention to which period other streams suggest to be promising for finding a correlated bursty topic pattern .
4 . COORDINATED MIXTURE MODEL
In this section , we describe our coordinated mixture model to discover correlated bursty topic patterns from coordinated text streams . 4.1 Basic Idea y c n e u q e r f e v i t l a e r
0.1
0.08
0.06
0.04
0.02
0 terror English attack English attack Chinese time
Figure 1 : Examples of bursty words related to 9{ 11 event in the news data . x axis denotes the time points and y axis is the relative frequency .
The basic idea of our approach is to align the text samples from different streams based on the shared time stamps and discover topics from multiple streams simultaneously with a single probabilistic mixture model . Recent work such as [ 10 ,
6 , 16 ] has shown that probabilistic mixture models are quite effective for discovering topics from text . Their basic idea is to represent a topic by a word distribution and assume that a text collection is \generated" by repeatedly sampling words from a mixture of multiple topic distributions . By fitting the mixture model to the text data , we can then obtain an estimate of each word distribution , which we can take as a discovered topic . Different methods differ in the way of mixing the word distributions and estimating the parameters .
A straightforward way of applying such a method to our problem would be to use a mixture model to discover topics from each stream and then try to match the topics across streams in hope of detecting some topics that happen to burst during the same period . However , there are two problems with this simple approach : ( 1 ) We will need to match topics across different streams , which is difficult because the vocabularies of different streams do not necessarily overlap . ( 2 ) The topics discovered in each stream may explain the corresponding stream well but not necessarily match the common topics shared by multiple streams . Indeed , a shared topic may not fit a specific stream so well as some variant of the topic .
This analysis suggests that we should somehow make these mixture models designed for different streams \communicate" with each other so that they would all focus more on discovering the common topics shared by all streams . We achieve this goal by aligning the text samples from different streams based on their common time stamps and fit all the streams with a single mixture model . Specifically , we would merge the text samples on the same time point ( keeping their stream identities ) to form a unified text sample on the time point . In order to match topic models across different streams , we also align the topic models from different streams ; again , we keep their stream identities . Since we have kept the stream identities of both the text sample and the topic model , we can fit the right model to the right data when fitting the whole mixture model to all the streams .
We call such a mixture model a coordinated mixture model because the mixture models for all streams \coordinate" with each other so that each would focus more on topics that have strong correlations with topics in other streams . After we fit such a coordinated mixture model to all the streams , we will obtain all the topic models . Since the topic models from different streams are already aligned with each other in advance , we naturally obtain a correlated bursty topic pattern if all the involved topics are bursty in a similar period .
Intuitively , each topic model ( ie , word distribution ) defines a soft cluster in the sense that it specifies the probability of membership of each word in the cluster , and our mixture model would group words based on their co occurrences in samples of the same stream to form word clusters and match clusters across streams based on the correlations between the temporal distributions of the clusters from different streams .
Note that our model does not require different streams to share any vocabulary ; instead it exploits the fact that topics involved in a correlated bursty topic pattern tend to have similar temporal distribution to match the topics from different streams . In Figure 1 , based on the news data set used in our experiment ( see Section 5 ) , we show strong correlations of the relative frequency ( ie , frequency of a word in each time point normalized by its total frequency ) distributions over time between two English words \terror" and \attack" as well as between the English word \attack" in an English news stream and its Chinese translation in a Chinese stream . In general , when the multiple streams share some common causal factors ( eg , affected by the same event ) , we will observe such correlations .
4.2 Formal De.nition
We now give the formal definition of the proposed coor dinated mixture model .
421 The Generative Model Let S = fS1 ; :: : ; Smg be m coordinated text streams with vocabularies V1 ; :: : ; Vm . Without loss of generality , we assume there are k correlated bursty topic patterns in our streams and associate a latent cause variable z 2 [ 1 ; k ] with them ; a different value of z would indicate a different pattern . Given a value j of z , we have a set of \aligned" topic models , each corresponding to a single stream . The topic model for stream Si is given by P ( wjz = j ; i ) where w 2 Vi . That is , the fP ( wjz = j ; i)gi2[1;m ] defines a potential correlated bursty topic pattern .
In order to explain all the words in our streams , including those that are not involved in any correlated bursty topic pattern , we further introduce a background topic model P ( wjB ; i ) for each stream Si .
In general , we assume that a word w appearing at time t in stream Si with probability P ( wjt ; i ) ( ie , w is a word in text sample Sit ) can either be a background word ( thus should be generated using the background model ) or potentially cover any of the k patterns ( thus should be generated from a mixture of the k pattern models ) . In other words , w would be regarded as a sample drawn from the following mixture model :
P ( wjt ; i ) = BP ( wjB ; i)+(1 , B )
P ( zjt)P ( wjz ; i ) ( 1 ) k z=1 where B is the mixture weight of the background model , and P ( zjt ) is the probability of choosing pattern z at time point t .
The log likelihood of generating text sample Sit is thus log P ( Sit ) =   w2Vi c(w ; Sit ) log P ( wjt ; i ) where c(w ; Sit ) is the count of word w in Sit . Therefore , the log likelihood of generating all the m coordinated streams is : n m log P ( S ) = c(w ; Sit ) log P ( wjt ; i )
( 2 ) t=1 i=1 w2Vi
Our coordinated mixture model can be regarded as an ex tension of Probabilistic Latent Semantic Analysis ( PLSA ) [ 10 ] to model coordinated streams . Note that the P ( wjt ; i ) ’s for different streams are coordinated because of the common variable t , and P ( zjt ) , which is independent of any stream , forcing all the streams to have the same preferences for the bursty topic patterns . On the other hand , P ( wjt ; i ) is different for a different stream , allowing us to model different vocabularies .
        422 Parameter Estimation We estimate the parameters of the coordinated mixture model by fitting the model to our coordinated text stream data . To model the background words in our streams ( from some prior knowledge ) and regularize our model , we fix our B to a constant and set
P ( wjB ; i ) = c(w ; Si ) w c(w ; Si ) where c(w ; Si ) is the count of word w in stream Si .
The remaining parameters to estimate are P ( wjz ; i ) and P ( zjt ) . Without assuming any prior knowledge , we may use the maximum likelihood estimator and use the expectationmaximization ( EM ) algorithm to compute an estimate iteratively . The expectation step is to calculate :
P ( zjt ; w ; i ) =
( 1 , B )P ( l)(zjt)P ( l)(wjz ; i )
B P ( wjB ; i ) + ( 1 , B ) z P ( l)(zjt)P ( l)(wjz ; i )
:
The maximization step is to update the probabilities :
P ( l+1)(zjt )
=
P ( l+1)(wjz ; i ) = i w2Vi c(w;Sit )P ( zjt;w;i ) c(w;Sit )P ( zjt;w;i ) w2Vi i t c(w;Sit )P ( zjt;w;i ) z w2Vi t c(w;Sit )P ( zjt;w;i )
The basic idea of this approach is to exploit the fact that those words in different streams about the same common causal factor can be expected to have strong correlations between their frequency distributions over time , whereas a noisy \local word" in a stream is unlikely to have strong correlations with them . Thus we can use the corresponding topic models in other streams ( ie , fP ( wjz ; j)gj6=i ) to help filter out \local noise" in P ( wjz ; i ) . Specifically , we would adjust P ( wjz ; i ) to promote words that are highly correlated with high probability words in all other streams . We iteratively do such adjustment for all the topic models .
To implement this idea , we first compute the global correlations between all the high probability words in one stream and those in another , where the probability of a word is given by the corresponding topic model ( ie , P ( wjz ; i) ) . Following [ 20 ] , we represent each word from stream Si by the normalized empirical frequency distribution vector over the entire time span of the stream :
P ( tjw ) = c(w ; Sit ) t c(w ; Sit )
( 3 )
We can then compute the Pearson correlation coefficient r(x ; y ) between two words x and y as follows :
4.3 Constraining EM with Temporal
Dependency
One deficiency of the basic coordinated mixture model is not capturing the dependency among the consecutive time points in covering topics . Specifically , each text sample makes its own , independent choice among the possible topics to cover . Intuitively , however , the text samples within a consecutive time period tend to be influenced by the same event(s ) . So it would be desirable to somehow force all of them to make similar choices of topics .
To implement this intuition , we propose to modify the EM algorithm to impose a temporal dependency constraint on P ( zjt ) so that during each iteration P ( zjt ) would be smoothed ( constrained ) by both its neighbors P ( zjt,1 ) and P ( zjt + 1 ) :
Q(l+1)(zjt ) = P ( l+1)(zjt ) = Q(l+1 ) ( zjt,1 ) w2Vi w2Vi z i i
2(1+ ) c(w;Sit )P ( zjt;w;i ) + Q(l+1 ) ( zjt )
1+
+ Q(l+1 ) ( zjt+1 )
2(1+ ) c(w;Sit )P ( zjt;w;i )
Here Q(l+1)( jt ) is the original formula in EM iteration . The parameter is to control the amount of smoothing . A larger would impose a stronger dependency constraint among adjacent time points . When = 0 , we impose no constraint and have P ( l+1)( jt ) = Q(l+1)( jt ) . We found in our experiments that introducing a non zero can smooth a bursty pattern and help discover consecutive bursty periods . 4.4 Mutual Reinforcement across Streams
Although the discovery of correlated bursty patterns is coordinated across streams through the shared time points , the discovered topic models in each stream ( ie , P ( wjz ; i ) ) can be biased by some stream specific local themes , which may peak at about the same time as the true correlated bursty topic patterns . As a result , P ( wjz ; i ) may have mixed subtopics . To \clean up" P ( wjz ; i ) and make it more focused on the correlated topics across all other streams , we propose to use a reinforcement method to reward words from different streams that are strongly correlated with each other over the entire time span . r(x ; y ) = n i=1 xiyi , 1 n i , 1 n ( n i=1 xi n i=1 yi i , 1 n ( n
( i=1 x2 n i=1 yi)2 ) ( 4 ) where xi and yi are i th entries in x and y ’s frequency distribution vectors defined above . n i=1 xi)2)( i=1 y2 n
With these correlation values , we then use the following iterative procedure to adjust each P ( wjz ; i ) :
Q(wjz ; i ) = P ( l)(wjz ; i ) P ( l+1)(wjz ; i ) =
Q(wjz;i ) j:j6=i w0 2Vj
Q(wjz;i ) w2Vi r(w ; w0)P ( l)(w0jz ; j )
( 5 )
Intuitively , such a mutual reinforcement procedure rewards the high probability words in stream i ( according to P ( wjz ; i ) ) which have high correlations with the high probability words in its correlated topic model of other streams P ( w0jz ; j ) ’s .
To increase the efficiency , we can perform mutual reinforcement updating on only the words with high probabilities according to P ( wjz ; i ) and let the iteration stop after a few iterations . ( In our experiments , the ranking of words according to the updated probabilities usually becomes stable after 10 iterations . ) 4.5 Discover Correlated Busty Topic Patterns After we get all the parameters P ( wjz ; i ) and P ( zjt ) estimated , we can discover the correlated bursty topic patterns directly . Since P ( zjt ) is stream independent , given a span threshold and a coverage threshold , an identified pattern is a correlated bursty topic pattern if P ( zjt ) satisfies the constraints in Definition 4 of bursty topic pattern .
Intuitively , P ( zjt ) represents the strength of pattern z at time t and the high probability words according to P ( wjz ; i ) characterize the content of bursty pattern z in stream i . In the experiment , we use P ( zjt ) and P ( wjz ; i ) to represent the identified correlated bursty topic patterns . For example , in the correlated news streams in different languages where a correlated bursty topic corresponds to a real world event , P ( zjt ) ’s are the intensiveness of the corresponding event over time , which can be used to identify the bursty
News streams
English Chinese
Literature streams SIGMOD VLDB
Length #Word #Article Data size
148
70,049 34,751 111MB
148 9,720 43,488 63MB
31
1,930 1,787 138KB
31
2,144 2,143 108KB
Table 1 : Statistics of two stream data sets period , and P ( wjz ; i ) ’s indicate what the event is about in different languages . 4.6 Discussion
O( m i=1 jSij . k + m
The proposed method can work with quite large text collections . Each iteration in the EM algorithm has complexity i=1 jVij . k + n . k ) . For the mutual reinforcement , we only need to calculate the top N words of each stream given a pattern . Each iteration of reinforcement has complexity O(N . N . k ) . In practice , N = 100 is enough since the words outside of top 100 have very low probabilities .
Although presented as a model on text streams with words as units , our model is quite general and can be applied to any discrete data streams with any interesting features as units .
5 . EXPERIMENTS
To evaluate our methods of identifying major correlated bursty topic patterns , we conduct experiments on a news data set and a literature data set . The correlated bursty topic patterns in news streams are highly related to major real world events and the patterns in literature streams can indicate research paradigm shifts . In this section , we show that our proposed methods can identify meaningful correlated bursty topic patterns from these two types of coordinated streams to reveal the major real world events and research paradigm shifts with appropriate time line . 5.1 Data Sets
511 News Streams Our news streams consist of six months’ news articles of Xinhua English and Chinese newswires dated from June 8th , 2001 through November 7th , 2001 . There are altogether 43,488 documents in Chinese and 34,751 documents in English distributed in the 148 days . Each day is used as a time point in these two streams thus the length of the coordinated streams is 148 .
In each stream , a text sample with time stamp i is the concatenation of all news articles appearing on date i . For Chinese news articles , there is no \space" between two Chinese words and the meaning of each single Chinese character can be interpreted quite differently depending on its context . The ambiguity of Chinese characters can be alleviated much by grouping them into bi grams . Thus , without using sophisticated Chinese segmentation tools , we use bigrams of Chinese characters as words in the Chinese news stream . Therefore , at each time point ( ie , day ) , each news stream has a sequence of the aforementioned words which appear in the corresponding stream at that time point . As we will show , even with such crude segmentation , the mining results are already quite interesting . Naturally , with a better segmentation tool , our mining results can be further improved .
512 Literature Streams The data set in literature domain we use is similar to the one used in [ 12 ] . Specifically , we use all the paper titles of SIGMOD and VLDB from the years 1975{2005 in our experiment . SIGMOD and VDLB are two major conferences in database research community , which have existed for more than 30 years since 1975 . Publications accumulated in the two conferences over years naturally form coordinated text streams . In this data set , each year is treated as a time point and thus the length of the coordinated streams is 31 .
The statistics of the news and literature data sets are shown in Table 1 . Besides in different genres , the two data sets are clearly different in statistic measures . We use these two data sets to test the generality of our proposed methods . 5.2 Parameter Setting
In the coordinated mixture models , there are several userinput parameters which provide flexibility for bursty topic pattern analysis . These parameters are set empirically , as in principle , it is impossible to optimize these parameters without relying on domain knowledge . We discuss the effect of different parameters as follows .
Parameter B is to control the strength of the background model . The background model captures global common words in each stream . A larger B will force discovered bursty patterns to be more discriminative from each other but an extremely large B will attract too much useful information to the background model thus weaken the interpretability of the discovered patterns . Empirically , a B suitable for text documents is between 0.9 and 0.95 and we use B = 0:95 in our experiments .
Parameter is to control the dependency strength among adjacent time points in the streams . A high forces a high dependency , ie , assumes that adjacent time points have very similar bursty topic patterns . A extreme lower value such as 0 can not fully utilize the consecutive property of data streams . In principle , can be set based on the overall similarities among adjacent time points . Empirically , we set = 0:1 in the following experiments .
Parameter k is the number of bursty patterns shared by all coordinated streams in a data set . When no domain knowledge is available as in our experiments , we use a similar strategy as [ 15 ] to determine the number of bursty patterns by enumerating multiple possible values of k and drop the patterns which do not satisfy our bursty pattern definition . 5.3 Results on News Data
On the news streams , we first apply our coordinated mixture model to identify the major correlated bursty topic patterns and then use our mutual reinforcement method across streams to further refine the identified patterns . Bursty patterns which satisfy = 5 for = 0:01 are kept . We finally obtain 7 major bursty topic patterns , which are shown in Figure 3 . Recall that each pattern has its bursty period and a set of high probability words . We show both results in Figure 3(a ) and Figure 3(b ) respectively . In Figure 3(a ) , we plot the strength of these 7 patterns over time points indicated by probability P ( zjt ) which is estimated from our mixture model . In Figure 3(b ) , we list the top 10 words with highest probabilities in each pattern mined from both En
Relational data model Deductive , Object oriented Parallel , Object oriented Data mining , Web XML , Stream , Web
0.6
0.5
0.4
0.3
0.2
0.1
0 2005
1985
1995 year
Deductive ,
Parallel ,
Data Mining ,
XML ,
Object Oriented
Object Oriented
1986 1991 object 0.0486 deductive 0.0308 orient 0.0303
1992 1995 object 0.0687 parallel 0.0334 orient 0.0308 knowledge 0.0255 persistent 0.0177
Web
1996 2001 mine 0.0440 web 0.0279 warehouse 0.0235 dimension 0.0234 extend 0.0240 multidatabase 0.0153 similar 0.0148 deductive 0.0256 object 0.0236 orient 0.0225 knowledge 0.0204 object 0.0759 orient 0.0453 parallel 0.0441 rule 0.0270 warehouse 0.0366 mine 0.0223 web 0.0201 dimension 0.0175
Stream , Web
2002 2005 xml 0.0772 stream 0.0633 service 0.0308 web 0.0228 sql 0.0178 xml 0.0804 stream 0.0594 web 0.0292 service 0.0211 search 0.0199
0.6
0.5
0.4
0.3
0.2
0.1
0 1975
Relational data model 1975 1985 design 0.0418 relational 0.0410 language 0.0333 model 0.0287 file 0.0274 base 0.0880 design 0.0544 data 0.0375 model 0.0350 relational 0.0341 skew 0.0201 composite 0.0191 use 0.0151
Figure 2 : Research paradigm shifts in the literature streams . The words in upper and lower parts of the table above are from SIGMOD and VLDB streams respectively . Shared words are bold faced . glish and Chinese news streams . For each Chinese bi gram , we include its English translation in the parenthesis after itself . Note that Chinese bi grams are not always complete Chinese words . We use \*" to indicate that a bi gram is only a fragment of our translation .
Figure 3 gives us a good , unified summary of the two streams by the 7 identified bursty topic patterns . The 1st pattern is about Communist Party of China ( CPC ) and has a sharp peak around July 1st , 2001 , which is the 80th anniversary of CPC . The 2nd pattern is on the bid of Olympic 2008 and bursts from July 13th , 2001 when Beijing , the Capital of China , won the bid of Olympic 2008 . The 3rd pattern is the 9th swimming championship held in Fukuoka in 2001 . The 4th pattern is the shrine event in Japan and the 5th pattern is the 21st Universiade held in Beijing during Aug . 22nd to Sep . 1st , 2001 . 9{11 event is the 6th pattern in the figure . From Figure 3(a ) , we can see this pattern accurately bursts at September 11th , 2001 . The set of high probability words such as \terror" and \attack" are very informative in both English and Chinese . The last pattern identified by our algorithm is Afghanistan war which happened consequently after the 9{11 event . Though both 9{11 and Afghanistan war are related to terrorists , they are two different events and our methods are able to distinguish them correctly .
The results above show that our proposed methods can successfully identify correlated bursty topic patterns in coordinated news streams . It could have many applications . For example , most of the identified English and Chinese words , although in completely different vocabularies , match very well . This can potentially help cross lingual information retrieval and integration [ 22 , 20 ] . The bursty period of a pattern can be used to analyze the trend of the cause event . Furthermore , combining the identified bursty words and periods together is informative enough to give a unified summary of the coordinated text streams . h t g n e r t s
0.5 0.4 0.3 0.2 0.1 0 200168
Stock Pollution date
2001117
Figure 4 : Result of document based clustering
5.4 Results on Literature Data Set
We apply our methods to the SIGMOD and VLDB stream data , and the major bursty patterns which satisfy = 2 for = 0:01 are shown in Figure 2 . The identified patterns summarize the research paradigm shifts interestingly : at the beginning , the database community focused on relational data model ( 1st pattern : 1975 1985 ) ; then the research focus changed to deductive and object oriented databases ( 2nd pattern : 1986 1991 ) ; after that , many researchers began to study parallel databases while the object oriented database research stayed as a major topic ( 3rd pattern : 1992 1995 ) ; data mining and web related topics became popular after 1996 ( 4th pattern : 1996 2001 ) ; in recent years , XML and stream data management became dominant together with Web related topics ( 5th pattern : 2002 2005 ) .
In a whole , we can see that all the identified paradigm shifts can reflect well the real progress in the database community . This result can not only provide a big picture of the database field to a novice , but also assist an expert in writing overviews about the whole field . 5.5 Detailed Analysis
In this section , based on the news streams , we compare our proposed methods to directly applying PLSA based doc
0.2
0.15
0.1
0.05 h t g n e r t s
0 200181
English
9 { 1 1 se p te m b e r a tta ck u s te rro rist te rro r u nite y o rk w a shingto n sta te ne w
English:9 11 Chinese : Western Development Chinese : 9 11 & National Day
20011020 date
C hine se
W e ste rn d e v e lo p m e nt
9 1 1 & na tio na l d a y
( cid:220)\(the w e ste rn ) 0(d e v e lo p m e nt )
( cid:149)(cid:219)(C hine se m e rcha nts )
=(cid:253)(inv e stm e nt ) fl(e x p lo itu re ) s‚(e d u c a tio n )
'(cid:154)(c u ltu re )
•)(o u r c o u ntry )
( cid:228)(cid:151)(fu nd ) s(te a che r )
9](te rro r ) @(cid:137)(to u rism )
](cid:204)(* te rro rism ) )?(na tio na l d a y )
( cid:137)0(to u rist ) *Z(sty le ) –(cid:151)(go ld e n ) æ(cid:226)(a tta ck )
( cid:219)<(m o o n c a k e )
( cid:151)–(* go ld e n w e e k )
Figure 5 : Aligning patterns across streams . ument clustering method [ 10 , 25 ] and analyze different variants of our methods . 551 Direct application of PLSA To discover bursty patterns , a straightforward method would be to apply document based clustering methods which have been widely used to discover topics in a document collection . However , as will be shown , these methods are not effective in detecting \bursty" patterns . In this experiment , we use the English stream and treat each English article as a document . We use PLSA based clustering method proposed in [ 10 , 25 ] to group articles into clusters . The strength of a cluster in each day is calculated as in [ 15 ] and is proportional to the number of documents in the cluster which appears on that day . In Figure 4 , we show the strength of two biggest clusters identified by PLSA . The first cluster is about \stock" and it has high probability words such as \stock" , \dollar" , and \millions" . The second cluster is about the pollution in China and it has high probability words such as \pollute" , \water" , and \protect" . From their strength over time plotted in Figure 4 , we can see that neither cluster is a meaningful bursty pattern as they occur rather evenly in the whole time period . This shows that document based clustering methods are biased by these \common" topics and are ineffective for detecting bursty patterns . One reason why this is so is because such a method does not utilize the time information in the stream . In contrast , our method utilizes time information and can detect bursty patterns as shown in Figure 3 . 552 Coordinated streams vs single streams Another possible method for finding correlated bursty topic patterns is to first find busty patterns in each single stream and then align patterns across streams according to their temporal overlaps . However , this ad hoc method does not work well and we show an example in Figure 5 . In this figure , one bursty pattern from English news stream and two
English a fgha nista n ta lib a n us te rro r a n th ra x m ilita ry a p e c a tta c k strik e e c o n o m ic
English a fgha nista n ta lib a n
C hine se
( cid:140)F(* A fgha nista n ) ( cid:131)(cid:140)(* A fgha nista n )
9](te rro r )
( cid:204)(cid:204)(cid:204)FFF(m e e tin g ) /(m ilita ry )
˘˘˘(cid:212)(cid:212)(cid:212)(A sia n P a c ifl c )
***ZZZ(sty le ) ( cid:155)(cid:155)(cid:155))))(U .S .A )
( cid:132)(cid:132)(cid:132)(o rg a n iz a tio n )
K(cid:226)(strik e )
C hine se
( cid:131)(cid:140)(* A fgha nista n ) ( cid:140)F(* A fgha nista n ) isla m a b a d
ØØØ(cid:131)(cid:131)(cid:131)(* to A fg h a n ista n ) k a b ul le d m ilita ry strik e k a n d a h a r c iv ilia n te rro r
/(m ilita ry )
///KKK(* m ilita ry strik e ) ///qqq(* m ilita ry a c tio n )
9](te rro r ) K(cid:226)(strik e )
FFF{{{(* A fg h a n i )
)Y(K a b u l )
Figure 6 : Topic models before ( top half ) and after ( bottom half ) mutual reinforcement . Distinct words are bold faced . bursty patterns from the Chinese news stream are shown . The pattern in English stream is the 9{11 event . The first one in Chinese stream is about \western development" and the second is a mix of 9{11 and Chinese \national day holiday" ( October 1st ) events . From Figure 5 , we can see that the bursty periods of both Chinese patterns overlap with that of the 9 11 pattern from English news . Thus both of the Chinese patterns can be aligned with the English 9{11 pattern but neither of them is a good match . This example shows that patterns identified from single streams can be biased by their local patterns and the alignment of such biased patterns across streams could be imprecise and misleading . Since correlated bursty patterns presumably share similar periods , the coordinated mixture model considers all the streams simultaneously and calculates the same bursty periods for their correlated bursty patterns . As shown in Figure 3(a ) , correlated bursty patterns have the same bursty period thus we do not need to align them . By considering coordinated stream simultaneously , our method is more robust for identifying important correlated patterns .
553 Effect of mutual reinforcement We demonstrate the advantage of mutual reinforcement in Figure 6 . Figure 6 ( a ) and Figure 6 ( b ) show the words of the identified patterns before and after mutual reinforcement , where the differences are bold faced . In Figure 6 ( a ) , the words of Afghanistan war are the majority but still mixed with some noisy words such as \APEC" ( The Asia Pacific Economic Cooperation ) and \economic." With mutual reinforcement across Chinese and English streams , those noisy words are suppressed and the pattern becomes more coherent . This is because those major words in both streams are not only the high probability words , but also have high global temporal correlations with their counterparts in the other stream . Our mutual reinforcement procedure can effectively utilize these properties and thus improve the quality of the bursty patterns . h t g n e r t s
0.2 0.15 0.1 0.05 0 200191 lambda=0 : without dependency lambda=0.1 : with dependency date
20011020
Figure 7 : Effect of local dependency .
554 Effect of local dependency The advantage of the constraining EM by local dependency is that it can utilize the consecutive property of a stream when parameter > 0 . To see the effect of applying local dependency , we compare = 0:1 and = 0 in Figure 7 using the example of 9{11 . It is clear that the periods are inconsecutive and rugged for = 0 , making it hard to interpret . On the contrary , using dependency gives us consecutive bursty periods which are natural in reality .
6 . CONCLUSIONS AND FUTURE WORK In this paper , we defined and studied a novel problem of mining correlated bursty patterns from coordinated text streams . We proposed coordinated mixture models which can identify bursty words and their bursty periods simultaneously . Furthermore , we enhanced our model by incorporating local dependency along the time line and proposed a mutual reinforcement approach across streams to further refine the correlated bursty patterns . We evaluated our methods on two data sets : coordinated news streams and coordinated literature streams . On the news streams , the identified bursty patterns reflect the real world events and on the literature streams , the identified bursty patterns well summarize the research paradigm shifts in the database community .
Besides the correlated patterns , there are local bursty patterns in each stream . In the future , we will further study the problem of identifying both global correlated and local patterns . We also plan to apply our methods to other coordinated non text streams to study their correlated bursty patterns . 7 . ACKNOWLEDGMENTS
We thank the anonymous reviewers for their valuable comments . This work is in part supported by a Microsoft Live Labs Research Grant , a Google Research Grant , and an NSF CAREER grant IIS 0347933 . 8 . REFERENCES [ 1 ] C . Aggarwal . Data Streams : Models and Algorithms .
Springer , 2007 .
[ 2 ] C . C . Aggarwal , J . Han , J . Wang , and P . S . Yu . On demand classification of data streams . In KDD , pages 503{508 , 2004 .
[ 3 ] R . Agrawal , K I Lin , H . S . Sawhney , and K . Shim .
Fast similarity search in the presence of noise , scaling , and translation in time series databases . In VLDB , pages 490{501 , 1995 .
[ 4 ] J . Allan , J . Carbonell , G . Doddington , J . Yamron , and Y . Yang . Topic detection and tracking pilot study : Final report . In Proceedings of the DARPA
Broadcast News Transcription and Understanding Workshop , 1998 .
[ 5 ] J . Allan , R . Papka , and V . Lavrenko . On line new event detection and tracking . In SIGIR , pages 37{45 , 1998 .
[ 6 ] D . Blei , A . Ng , and M . Jordan . Latent dirichlet allocation . Journal of Machine Learning Research 3 , 2003 .
[ 7 ] S . Chien and N . Immorlica . Semantic similarity between search engine queries using temporal correlation . In WWW , pages 2{11 , 2005 .
[ 8 ] G . P . C . Fung , J . X . Yu , P . S . Yu , and H . Lu . Parameter free bursty events detection in text streams . In VLDB , pages 181{192 , 2005 .
[ 9 ] J . Han and M . Kamber . Data Mining : Concepts and
Techniques , 2nd Ed . Morgan Kaufmann , 2006 .
[ 10 ] T . Hofmann . Probabilistic latent semantic indexing .
In SIGIR , pages 50{57 , 1999 .
[ 11 ] G . Hulten , L . Spencer , and P . Domingos . Mining time changing data streams . In KDD , pages 97{106 , 2001 .
[ 12 ] J . Kleinberg . Bursty and hierarchical structure in streams . In KDD , pages 91{101 , 2002 .
[ 13 ] R . Kumar , J . Novak , P . Raghavan , and A . Tomkins .
On the bursty evolution of blogspace . In WWW , pages 568{576 , 2003 .
[ 14 ] Q . Mei , C . Liu , H . Su , and C . Zhai . A probabilistic approach to spatiotemporal theme pattern mining on weblogs . In WWW , pages 533{542 , 2006 .
[ 15 ] Q . Mei and C . Zhai . Discovering evolutionary theme patterns from text : an exploration of temporal text mining . In KDD , pages 198{207 , 2005 .
[ 16 ] Q . Mei and C . Zhai . A mixture model for contextual text mining . In KDD , pages 649{655 , 2006 .
[ 17 ] R . Sproat , T . Tao , and C . Zhai . Named entity transliteration with comparable corpora . In ACL , 2006 .
[ 18 ] R . Swan and J . Allan . Extracting significant time varying features from text . In CIKM , pages 38{45 , 1999 .
[ 19 ] R . Swan and J . Allan . Automatic generation of overview timelines . In SIGIR , pages 49{56 , 2000 .
[ 20 ] T . Tao and C . Zhai . Mining comparable bilingual text corpora for cross language information integration . In KDD , pages 691{696 , 2005 .
[ 21 ] M . Vlachos , C . Meek , Z . Vagena , and D . Gunopulos .
Identifying similarities , periodicities and bursts for online search queries . In SIGMOD , pages 131{142 , 2004 .
[ 22 ] J . Xu , R . Weischedel , and C . Nguyen . Evaluating a probabilistic model for cross lingual information retrieval . In SIGIR , pages 105{110 , 2001 .
[ 23 ] Y . Yang , T . Ault , T . Pierce , and C . W . Lattimer .
Improving text categorization methods for event tracking . In SIGIR , pages 65{72 , 2000 .
[ 24 ] Y . Yang , T . Pierce , and J . Carbonell . A study of retrospective and on line event detection . In SIGIR , pages 28{36 , 1998 .
[ 25 ] C . Zhai , A . Velivelli , and B . Yu . A cross collection mixture model for comparative text mining . In KDD , pages 743{748 , 2004 .
0.2
0.15
0.1
0.05
0.2
0.15
0.1
0.05
Communist Party of China Olympic 2008 Swimming Championship Shrine in Japan 21st Universiade 9.11 Afghanistan War
0 200168
2001712
2001811
2001910
20011010
0
2001117 date
( a ) Bursty Periods
Events
O ly m p ic 2 0 0 8 ( 7 .1 1 7 .2 1 )
S w im m ing C h a m p io nsh ip ( 7 .2 0 7 .3 0 )
C P C ( 6 .1 0 7 .1 1 ) w im b led o n 0 .3 3 2 5
8 0 th 0 .2 6 4 1 c p c 0 .1 98 3 c o m m u nist 0 .1 1 7 6 m ilo sevic 0 .0 3 2 0 a nniversa ry 0 .0 1 4 0 h a g u e 0 .0 0 7 0 lea d ersh ip 0 .0 0 6 5 p a rty 0 .0 0 6 0 a id s 0 .0 0 4 3 o ly m p ic 0 .4 6 7 1
2 0 0 8 0 .2 6 5 5 b id 0 .1 5 2 7 a g ra 0 .0 3 3 0 c o ng ra tu la te 0 .0 2 0 5 su c c ess 0 .0 1 0 6 m u sh a rra f 0 .0 1 0 0 b eije 0 .0 0 99 h o st 0 .0 0 92 to ro nto 0 .0 0 7 0 fu k u o k a 0 .4 4 3 9
0 .2 3 4 9 sw im 9th 0 .1 1 3 4 fl na 0 .0 8 1 2 ch a m p io nsh ip 0 .0 6 8 1 g eno a 0 .0 1 5 2 g 8 0 .0 1 0 6 w a h id 0 .0 0 7 7 m p r 0 .0 0 3 7 a b d u rra h m a n 0 .0 0 3 4
( cid:156)›(* ch a m p io nsh ip ) 0 .3 2 92
(cid:156)(w o rld ch a m p io nsh ip ) 0 .2 1 97
( cid:137)y(sw im m ing ) 0 .1 5 92
Æ(cid:151)(* c o m m u nism ) 0 .2 5 5 4
( cid:151)j(* c o m m u nist p a rty ) 0 .2 5 3 4
£(cid:228)(O ly m p ic s ) 0 .2 2 1 1 ( cid:240)fi(B eijing ) 0 .1 4 0 8
)Æ(* c o u ntry c o m m u nism ) 0 .1 6 4 8
£(cid:148)(O ly m p ic c o m m ittee ) 0 .0 7 7 2 j˚(p a rty m em b er ) 0 .0 5 5 9 j{(p a rty ’s ) 0 .0 3 4 4
Oj(p a rty esta b lish m ent ) 0 .0 2 7 9
–#(a nniversa ry ) 0 .0 2 6 1 j˜(* p a rty a ch ievem ent ) 0 .0 2 3 6
|\(b ra nch ) 0 .0 1 7 5 {j(* ’s p a rty ) 0 .0 1 4 0
£(* interna tio na l O ly m p ic s ) 0 .0 7 7 1 y(cid:156)(* sw im m ing ch a m p io nsh ip ) 0 .0 6 3 0 y (* sw im m ing ch a m p io nsh ip ) 0 .0 4 7 9 ( cid:142)(cid:137)(* sw im m ing ch a m p io nsh ip ) 0 .0 4 3 1
#£(* O ly m p ic y ea r ) 0 .0 6 8 5
ø˝(b id ) 0 .0 5 1 8
( cid:148)(cid:204)(* c o m m ittee ) 0 .0 3 5 6
˜(cid:213)(su c c ess ) 0 .0 3 1 6 fiø(* B eijing ’s b id ) 0 .0 2 99
˝£(* b id ) 0 .0 2 7 3
Events
|R(W a ng Y a n ) 0 .0 2 2 6
Vj(* rep o rter M a ) 0 .0 1 4 2
( cid:132)y(* free sty le ) 0 .0 1 3 6 †(* free sty le ) 0 .0 1 2 2
S h rine in J a p a n(7 .3 1 8 .1 8 )
2 1 st U niversia d e(8 .1 8 9.2 )
9 ¢ 1 1 ( 9:1 1 ¡ 1 0 :6 )
A fg h a nista n W a r(1 0 .5 1 1 .7 ) sh rine 0 .4 8 1 3 y a su k u ni 0 .3 7 4 2 sa d c 0 .0 93 3 k o iz u m i 0 .0 1 3 9 c u sto m 0 .0 1 3 2 jeru sa lem
0 .0 0 5 6 m a la w i 0 .0 0 5 2 ja p a n 0 .0 0 5 2 o rient 0 .0 0 2 3 p a lestinia n 0 .0 0 1 4 k(cid:190)(visit ) 0 .1 95 0
,(cid:246)(* sh rine ) 0 .1 93 4 ‰)(* sh rine ) 0 .1 92 1 ),(* sh rine ) 0 .1 91 1
( cid:190)‰(* visit sh rine ) 0 .1 0 1 2
( cid:134)(cid:253)(J a p a n ) 0 .0 5 6 6 ( cid:140)(cid:218)(I g o ) 0 .0 3 8 8
D#(P rim e M inister ) 0 .0 0 5 8
)Q(inva sio n ) 0 .0 0 5 7
( cid:218)C(I g o p la y er ) 0 .0 0 3 2 u niversia d e 0 .1 90 2 g a m e 0 .1 5 3 4 m en 0 .0 8 2 6 w o m en 0 .0 7 1 9 g o ld 0 .0 5 6 2 u niversity 0 .0 5 4 3 c o m p etitio n 0 .0 5 3 0
2 1 st 0 .0 4 5 4 fl na l 0 .0 4 3 3 m ed a l 0 .0 3 0 6
L(cid:228)(u niversia d e ) 0 .4 1 5 9 ( cid:151)](g o ld m ed a l ) 0 .1 2 5 3
( cid:228)˜(sp o rts ) 0 .0 5 0 5 terro r 0 .3 94 6 terro rist 0 .2 2 8 0 a tta ck 0 .1 7 4 6 u s 0 .0 3 8 4 la d en 0 .0 2 3 7 b in 0 .0 2 1 3 o sa m a 0 .0 1 6 6 w a sh ing to n 0 .0 1 5 9 u nite 0 .0 1 3 2 a g a inst 0 .0 0 98
æ(cid:226)(a tta ck ) 0 .3 6 91 9](terro r ) 0 .2 995 a fg h a nista n 0 .6 1 7 7 ta lib a n 0 .1 5 97 isla m a b a d 0 .0 4 3 2 k a b u l 0 .0 3 5 6 a ip 0 .0 1 95 led 0 .0 1 8 8 m ilita ry 0 .0 1 8 7 strik e 0 .0 1 4 6 k a nd a h a r 0 .0 1 3 2 c ivilia n 0 .0 0 7 1
( cid:131)(cid:140)(* A fg h a nista n ) 0 .2 6 3 3 ( cid:140)F(* A fg h a nista n ) 0 .2 6 1 2
( cid:226)/(* a tta ck event ) 0 .1 1 3 0
Ø(cid:131)(* to A fg h a nista n ) 0 .1 0 2 8
)Ł(* na tio na l tea m ) 0 .0 4 8 9 ]æ(* terro ristic a tta ck ) 0 .0 6 4 3 ( cid:147)L(* 2 1 st u niversia d e ) 0 .0 4 0 3
/G(event ) 0 .0 6 1 3
E(w o m a n ) 0 .0 3 6 4
)(cid:228)(* u niversia d e ) 0 .0 3 0 9
( cid:160)C(a th lete ) 0 .0 2 8 0
˜(cid:204)(* sp o rts g a m e ) 0 .0 2 7 3 ( cid:228)(cid:204)(* u niversia d e ) 0 .0 2 6 0
](cid:204)(* terro rism ) 0 .0 4 2 6 =(cid:213)(N ew Y o rk ) 0 .0 1 1 9 ]I(* terro ist ) 0 .0 1 0 0
K(cid:226)(strik e ) 0 .0 0 6 6
( cid:226)9(* b ea t terro rism ) 0 .0 0 3 3
/(m ilita ry ) 0 .0 6 3 7
/K(* m ilita ry strik e ) 0 .0 5 1 7 /q(* m ilita ry a c tio n ) 0 .0 4 94
9](terro r ) 0 .0 4 7 8 K(cid:226)(strik e ) 0 .0 3 94
F{(* A fg h a ni ) 0 .0 3 7 9 { ( ’s a rm y ) 0 .0 2 5 1
( b ) Bursty Words
Figure 3 : Seven events detected from the coordinated news streams . Bursty periods are in parenthesis after the labels . \*" denotes that the Chinese bi gram is part of its translation . The upper part is English words and the lower part is the Chinese bi grams . The number after each word is the probability P ( wjz ; i ) discussed in Section 45
