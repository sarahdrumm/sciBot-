Characterising the Difference
Jilles Vreeken
Matthijs van Leeuwen
Computer Science Department
Universiteit Utrecht , the Netherlands
+31 30 253 90 49
Arno Siebes
{ jillesv , mleeuwen , arno }@csuunl
ABSTRACT Characterising the differences between two databases is an often occurring problem in Data Mining . Detection of change over time is a prime example , comparing databases from two branches is another one . The key problem is to discover the patterns that describe the difference . Emerging patterns provide only a partial answer to this question . In previous work , we showed that the data distribution can be captured in a pattern based model using compression [ 12 ] . Here , we extend this approach to define a generic dissimilarity measure on databases . Moreover , we show that this approach can identify those patterns that characterise the differences between two distributions . Experimental results show that our method provides a wellfounded way to independently measure database dissimilarity that allows for thorough inspection of the actual differences . This illustrates the use of our approach in real world data mining . Categories and Subject Descriptors H28 Data Mining ; I54 Similarity Measures .
General Terms Algorithms , Experimentation , Theory .
Keywords Database dissimilarity , temporal data mining , compression .
1 . INTRODUCTION Comparing databases to find and explain differences is a frequent task in many organisations . The two databases can , eg , be from different branches of the same organisations , such as sales records from different stores of a chain or the “ same ” database at different points in time . In the first case , the goal of the analysis could be to understand why one store has a much higher turnover than the other . In the second case , the goal of the analysis could be to detect changes or drift over time . The problem of this kind of “ difference detection ” has received ample attention , both in the database and in the data mining com munity . In the database community , OLAP [ 2 ] is the prime example . Using roll up and drill down operations , a user can ( manually ) investigate , eg , the difference in sales between the two stores . Emerging pattern mining [ 5 ] is a good example from the data mining community . It discovers those patterns whose support increase significantly from one database to the other . Emerging patterns , though , are often redundant , giving many similar patterns . Also , the growth rate that determines the minimal increase in support has a large impact on the number of resulting patterns . Lower growth rates give large amounts of patterns , of which only some are useful . To discover only “ interesting ” differences would require the data miner to test with multiple growth rate settings and , manually , trace what setting gives the most useful results and filter those from the complete set of emerging patterns . In this paper we propose a new approach to “ difference detection ” that identifies those patterns that characterise the differences between the two databases . In fact , the approach just as easily identifies the characteristic differences between multiple databases . The approach extends our earlier work employing Minimum Description Length ( MDL ) for frequent pattern mining . As in our previous papers , we restrict ourselves to frequent item set mining , although the methodology easily extends to other kinds of patterns and data types , see [ 1 ] . In our first paper [ 12 ] we attacked the well known frequent item set explosion at low support thresholds using MDL . We introduced the KRIMP algorithm that selects that subset of all frequent item sets that gives the best , lossless , compression of the database . KRIMP exploits a compression algorithm that uses a set of frequent item sets as a code table to compress a database . The set of frequent item sets which induces the best compression algorithm is the set of frequent item sets selected by KRIMP . Section 2 gives a brief overview . The MDL philosophy is that the selected subset gives the best approximation of the underlying data distribution . In our second paper [ 10 ] we independently verified this claim by using the compression schemes for classification . Say , we have two classes , C1 and C2 . Select the MDL best set F1 of frequent item sets for the sub database for class C1 and F2 for class C2 . As explained above , this gives us two compression algorithms , configured by code table CT1 based on F1 and code table CT2 based on F2 . A new , unseen , example t can now be compressed by both CT1 and CT2 . In the paper we argued that the Bayes optimal choice is to assign t to the class whose compressor compresses t best . This simple classification algorithm scores on par with state of the art classification algorithms ; Section 2 gives some more details . The approach towards difference detection introduced in this paper is again based on compression . First , we use compression to
Many many patterns
Database codetable codetable
Empty code table codetable codetable
Code table
KRIMP select pattern accept / reject add to code table
MDL codetable codetable compress database
Figure 1 . Finding the right code table with KRIMP . From a set of candidates , MDL picks those patterns that compress the database best . to characterise the differences between define a dissimilarity measure on databases . Then we introduce three ways two ( dis)similar databases . Let db1 and db2 be the two databases , with transactions concerning the same sets of items , of which we need to analyse the differences . In Section 3 , we first consider the difference in compressed length for the transactions in db1 when compressed by the MDLcompression schemes . The MDL principle as well as our results in classification imply that the compression scheme induced from db2 should in general do worse than the scheme induced from db1 . This is verified by some simple experiments . Next , in Section 3.2 , we aggregate these differences per transaction by summing over all transactions in db1 and normalising this sum by the optimal code length for db1 . This aggregation measures how different a database is from db1 . This is verified by experiments that show the correlation between this similarity measure and the confusion matrix of our classification algorithm briefly introduced above and in Section 2 . Finally , in Section 3.3 , this simple measure is turned into a dissimilarity measure for any pair of databases by taking the maximum of how different db1 is from db2 and vice versa . Again , the MDL principle implies that this is a dissimilarity measure . Experiments verify this claim by showing the correlation between this dissimilarity measure and the accuracy of our classification algorithm . The result of Section 3 is a dissimilarity measure for a pair of databases , based on code tables . If the dissimilarity is small , the two databases are more or less the same and a further analysis of the differences will not show anything interesting . The topic of Section 4 is on how to proceed if the dissimilarity is large . In that section , we introduce three ways to characterise these differences . The first approach focuses on the usage patterns of the code table elements , while the second focuses on how ( sets of ) transactions are compressed by the two different schemes . The third and last approach focuses on differences in the code tables themselves . All three approaches highlight complementary , characteristic , differences between the two databases . In Section 5 we discuss related work and describe the differences with our work . We round up with conclusions and future research in Section 6 .
2 . PRELIMINARIES Foundation of all data discussed in this paper is a set of items I , eg , the items for sale in a shop . A transaction t ∈ P(I ) is a set of items , eg , representing the items a client bought at that store . A database db over I is a bag of transactions , eg , the different sale transactions on a given day . An item set I ∈ I occurs in a transaction t ∈ db iff I Œ t . The support of I in db is the number of transactions in the database in which I occurs . 2.1 Compression with Krimp In previous work [ 12 ] we introduced the KRIMP algorithm ; we will now give a quick summary . As mentioned , we restrict ourselves to item set data , although the method can easily be extended to structured data [ 1 ] . In KRIMP , a code table has item sets on the left hand side and a code for each item set on its right hand side . The item sets in the code table are ordered descending on 1 ) item set length and 2 ) support . The actual codes on the right hand side are of no importance : their lengths are . To explain how these lengths are computed we first have to introduce the coding algorithm . A transaction t is encoded by KRIMP by searching for the first item set c in the code table for which c ⊆ t . The code for c becomes part of the encoding of t . If t \ c ∫ « , the algorithm continues to encode t \ c . Since we insist that each code table contains at least all singleton item sets , this algorithm gives a unique encoding to each ( possible ) transaction . The set of item sets used to encode a transaction is called its cover . Note that the coding algorithm implies that a cover consists of non overlapping item sets . The length of an element ’s code in a code table CT depends on the database we want to compress ; the more often a code is used , the shorter it should be . To compute this code length , we encode each transaction in the database db . The frequency of an item set c ∈ CT is the number of transactions t ∈ db which have c in their cover . The relative frequency of c ∈ CT is the probability that c is used to encode an arbitrary t ∈ db . For optimal compression of db , the higher P(c ) , the shorter its code should be . In fact , from information theory [ 8 ] we have the optimal code length for c as : l
CT c )(
−= log( c |(P db
) )
−= log
⎛ ⎜ ⎜ ⎜ ⎝ c freq )( ∑ d freq )( d
∈CT
⎞ ⎟ ⎟ ⎟ ⎠
( 1 )
The length of the encoding of a transaction is now simply the sum of the code lengths of the item sets in its cover . Therefore the encoded size of a transaction t ∈ db compressed using a specified code table CT is calculated as follows :
L CT t )(
=
∑ l CT CTt ,( ) c cover ∈ c )(
( 2 )
CT10
CT22
CT26 s n o i t c a s n a r t
#
60
40
20
0
60
40
20
0
60
40
20
0
0
10
20
30
Figure 2 . Heart ; encoded transaction lengths for all transactions belonging to one class ( DB10 ) , encoded with the code tables con
Code lengths ( bits )
Code lengths ( bits )
Code lengths ( bits )
100
100
100
20
30
40
50
60
20
30
40
50
60
70
40
50
80
60
70
80
70
80
90
0
10
90
90
0
10 structed for each of the three classes ( left to right : CT10 , CT22 , CT26 ) .
The size of the encoded database is the sum of the sizes of the encoded transactions , but can also be computed from the frequencies of each of the elements in the code table :
L CT
( db
)
=
∑ t db ∈
L CT t )(
−=
∑ c CT ∈ freq c )(
⋅ log
⎛ ⎜ ⎜ ⎜ ⎝ c freq )( ∑ freq d )( d
CT ∈
⎞ ⎟ ⎟ ⎟ ⎠
( 3 )
2.2 Finding the Right Code Table Now that the database compression scheme is defined , we can describe the actual algorithm that finds the optimal code table using MDL . For this , we need to take into account both the compressed database size and the size of the code table – otherwise the code table could grow larger than the original database! For the size of the code table , we only count those item sets that have a non zero frequency . The size of the right hand side column is obvious ; it is simply the sum of all the different code lengths . For the size of the left hand side column , note that the simplest valid code table consists only of the singleton item sets . This is the standard encoding ( st ) which we use to compute the size of the item sets in the left hand side column . Hence , the size of the code table is given by :
CTL (
)
= c CT ∈
∑ freq
:
+ l
CT c )( c l )( st c 0)( ≠
( 4 )
In [ 12 ] we defined the optimal set of ( frequent ) item sets as that one whose associated code table minimizes the total compressed size :
)
)
(
+ db
L CT
CTL (
( 5 ) The algorithm starts with a valid code table ( generally only the collection of singletons ) and a sorted list of candidates . These candidates are assumed to be sorted descending on 1 ) support and 2 ) item set length . Each candidate item set is considered by inserting it at the right position in CT and calculating the new total compressed size . A candidate is only kept in the code table iff the resulting total size is smaller than it was before adding the candidate . Moreover , each existing code table element is considered for pruning when a new candidate has been added : when deleting an existing element does not reduce the compressed size it is put back , otherwise it is permanently pruned . The process of constructing code tables is illustrated in Figure 1 . For more details , please see [ 12 ] . 2.3 Classification In [ 10 ] we tested the quality of our code tables as data distribution approximations using classification . Next to the compression ratios , this provided a second measure of how well the code tables describe the data . The intuition behind the classifier is straightforward and implied by MDL . Suppose two databases db1 and db2 , from which their respective code tables CT1 and CT2 are induced with KRIMP . If CT1 gives a shorter encoding for an unseen transaction t than CT2 , the probability that t origins from the distribution of db1 is larger than that it comes from db2 . This intuition can be written down as : l
CT 1 t )(
< l
CT 2 t )(
→ t |P( db 1
)
> t |P( db 2
)
( 6 )
So , the Bayes optimal choice is to assign an unseen transaction to that code table that leads to the shortest code length . The construction of the KRIMP classifier works as follows :
1 . Split the training database according to class , 2 . Remove the item(s ) indicating class from each transaction , 3 . Apply KRIMP to each of the databases . This yields a code table CTi for each class Ci .
Then , to classify an unseen transaction t : 1 . Compute lCTi ( t ) for all classes Ci , 2 . Assign t to the class that minimizes lCTi ( t ) .
Note that we have to do a Laplace correction on each CTi to make sure each possible transaction can be covered by each code table . We showed that classification accuracies obtained with this classifier are on par with those obtained by the best known classifiers . We therefore concluded that KRIMP picks those item sets that together describe the data . See [ 10 ] for more detail . For the sake of readability , we will use some notational shortcuts in the sections that follow :
CT db ( ) i j tCT )( i
= =
L CT i L CT t db ( t )(
) j
( 7 )
During the classification experiments , we made some interesting observations in the distributions of the code lengths ( not shown s n o i t c a s n a r t
#
24
18
12
6
0
0
10
20
30
40
60 Code length differences ( bits )
50
CT14 CT9 s n o i t c a s n a r t
#
24
18
12
6
0
70
80
0
10
20
30
40
60 Code length differences ( bits )
50
CT22 CT9
70
80
Figure 3 . Wine ; code length difference histograms for transactions in DB9 : encoded length differences between CT14 and CT9 ( left ) and between CT22 and CT9 ( right ) . previously ) . Figure 2 shows the encoded lengths for transactions of a single class , encoded by code tables constructed for each of the three classes . Not only gives the code table constructed for these transactions shorter encodings , the standard deviation is also much smaller ( compare the histogram on the left to the other two ) . This means that a better fit of the code table to the distribution of the compressed data results in a smaller standard deviation . 2.4 Experimental Setup Although a lot of time series data is being gathered for analysis , no good benchmark datasets with this type of data currently exist . We therefore decided to use a selection from the UCI repository [ 3 ] , which has been commonly used for emerging patterns [ 5 ] and related topics before . As these are all datasets containing multiple classes , we look at the differences between classes . Hence , we split each dataset on classlabel C and remove this label from each transaction , resulting in a database DBi per class Ci . A code table induced from DBi using KRIMP is written as CTi . For many steps in Sections 3 and 4 , we show results obtained with the datasets Heart and Wine because of their properties : they are interesting because they consist of more than 2 classes , but don’t have too many classes . Please note this selection is only for purpose of presentation ; results we obtained with other ( larger ) datasets are similar . In fact , KRIMP is better at approximating data distributions of larger databases , providing even more reliable results . Characteristics of all datasets used are summarized in Table 8 , together with the minimum support levels we use for mining the frequent item sets that function as candidates for KRIMP . All experiments in this paper are done with all frequent item sets . 3 . DATABASE DISSIMILARITY In this Section , we introduce a dissimilarity measure for transaction databases . This measure indicates whether or not it is worthwhile to analyse the differences between two such databases . If the dissimilarity is low , the differences between the two databases are small . If the measure is high , it is worthwhile to investigate the differences . Rather than defining the similarity measure upfront followed by a discussion and illustration of its properties , we “ develop ” the measure in a few steps as that allows us to discuss the intuition that underlies the definition far easier .
3.1 Differences in Code Lengths The MDL principle implies that the optimal compressor induced from a database db1 will generally provide shorter encodings for its transactions than the optimal compressor induced from another database db2 . Our earlier experiments on classification verify that this is also true for the code table compressors KRIMP discovers heuristically ; see Section 2 . More in particular , denote by MDLi the optimal compressor induced from database dbi and let t be a transaction in db1 . Then , the MDL principle implies that :
•
•
|
− t )( t |)(
MDL 1
MDL 2
( 8 ) is small if t is equally likely generated by the underlying distributions of db1 and db2 . is large if t is more likely generated by the distribution underlying one database than that it is generated by the distribution underlying the other .
In fact the MDL principle implies that if the code length differences are large ( the second case ) , then on average the smallest code length will be MDL1(t ) . Our classification results suggest that something similar should hold for the code table compressors discovered by KRIMP . In other words , we expect that
1
2
− tCT )( tCT )(
( 9 ) measures how characteristic t is for db1 . That is , we expect that this difference is most often positive and large for those transactions that are characteristic for db1 . In Figures 3 and 4 code length differences are shown for two datasets , respectively for transactions of the Wine9 and Heart10 databases . As we expected , virtually all code length differences are positive . This means that in practice the native code table does indeed provide the shortest encoding . In the case of the Wine9 database depicted in Figure 3 , we see a whopping average difference of 45 bits per transaction . The shapes of the two histograms also show a nice clustering of the differences between the encoded lengths . No negative differences occur , each single transaction is compressed better by its native code table . This confirms that MDL creates code tables that are truly specific for the data .
60
40
20 s n o i t c a s n a r t
#
0
10
0
10
20
40
30 60 Code length differences ( bits )
50
CT22 CT10
60
40
20 s n o i t c a s n a r t
#
70
80
0
10
0
10
20
CT26 CT10
70
80
40
30 60 Code length differences ( bits )
50
Figure 4 . Heart ; code length difference histograms for transactions in DB10 : encoded length differences between CT22 and CT10
( left ) and between CT26 and CT10 ( right ) .
We see the same general effect with Heart10 in Figure 4 , as again the peaks of the distribution lay within safe distance from the origin . From the histograms there is little doubt that code tables CT22 and CT26 are encoding data from a different distribution than they’ve been induced from . More importantly , comparing these diagrams unambiguously shows that it is possible to use the differences in encoded lengths to measure the amount of change between data . For example , as the differences on the left histogram are clearly smaller than in the situation on the right , this seems to imply that Heart classes 10 and 22 are more alike than classes 10 and 26 . How to investigate this hypothesis further will be discussed in the next Section . First we continue the development of our dissimilarity measure . 3.2 Aggregating Code Length Differences In the previous subsection we have seen that the histograms of code length differences give good insight in the differences between two databases . The next logical step towards the definition of a dissimilarity measure is to aggregate these differences over the database . That is , to sum the individual code length differences over the complete database . Straightforward aggregation , however , might give misleading results for two reasons :
•
• code length differences can be negative , so even if db1 and db2 are rather different , the aggregated total might be small . if db1 is a large database , the aggregated total might be large even if db2 is very similar to db1 .
As already mentioned in the previous subsection , the MDL principle implies that for the MDL optimal compressors MDL1 and MDL2 , the expected average value of MDL2(t ) – MDL1(t ) is positive . In other words , negative code length differences will be relatively rare and won’t unduly influence the aggregated sum . Our results in classification and , more importantly , the results of the previous subsection indicate that the same observation holds for the code table compressors CT1 and CT2 induced by KRIMP . Clearly , only experiments can verify this claim . The second problem indicated above is , however , already a problem for the MDL optimal compressors MDL1 and MDL2 . For , the expected value of the sum of the code length differences is simply the number of transactions times the expected average code length difference . Since the latter number is positive according to the
MDL principle , the expected value of the sum depends linearly on the number of transactions on the database . Clearly , the “ native ” encoded size of the database , CT1(db1 ) , also depends on the size of the database . Therefore , we choose to counterbalance this problem by dividing the sum of code length differences by this size . Doing this , we end up with the Aggregated Code Length Difference :
ACLD
( db 1
,
CT 2
)
=
CT 2
( db ) 1 CT 1
CT − 1 db ( ) 1
( db 1
)
( 10 )
Note that ACLD is an asymmetric measure : it measures how different db2 is from db1 , not vice versa! While one would expect both to be in the same ballpark , this is by no means given . The asymmetry is further addressed in the next subsection . To clearly indicate the asymmetry , the parameters are asymmetric : the first parameter is a database , while the second is a code table . Given this definition , we can now verify experimentally whether it works or not . That is , do greater dissimilarities imply larger differences and vice versa ?
Table 1 . Heart : aggregated code length differences for
In Table 1 we read the aggregated code length differences for all possible combinations of code tables and class databases for the Heart dataset . It is immediately clear there are distinct differences between the class distributions , as measurements of 1.00 imply code lengths averaging twice as long as that of the actual class . We also notice that while the data distributions of databases 10 and 36 are quite distinct , the lower measurements between the other three classes indicate that their distributions are more alike .
Table 2 . Wine : aggregated code length differences for each database/code table combination .
CT 9 CT 14 CT 22
DB 9 0.00 1.13 1.14
DB 14 1.27 0.00 1.68
DB 22 1.32 1.73 0.00 each database/code table combination . DB 27 DB 10 0.88 0.00 0.65 0.85 0.60 1.65 1.85 0.00 0.87 2.18
DB 26 0.71 0.60 0.00 0.61 0.72
DB 22 0.36 0.00 0.78 0.65 1.07
CT 10 CT 22 CT 26 CT 27 CT 36
DB 36 1.58 1.03 1.25 1.09 0.00
For the Wine database the class distributions are even more adrift than those in the Heart database , for all cross compressions result in encodings more than twice as long as the native ones . This is completely in line with what we’ve seen before in Figure 3 , in which we showed there is no uncertainty in keeping transactions of the Wine databases apart based on encoded lengths . If this technique truly quantifies the likeliness of the distributions belonging to some data , intuition tells us there has to be a close relation with the classification quality based on encoded transaction lengths . We can easily check this by comparing the aggregated code length differences with the confusion matrices for these databases . We therefore ran 10 fold cross validated classification experiments for these databases , as we did for previous work [ 10 ] .
Table 3 . Heart : classification confusion matrix .
Classified as : 10 22 26 27 36
Class 10 137 12 6 8 1
Class 22 24 11 8 10 2
Class 26 9 11 7 7 2
Class 27 6 7 8 9 5
Class 36 3 5 1 4 0
The confusion matrix for the Heart database , in Table 3 , clearly shows the intuition to be correct , as the number of misclassified instances drops completely according to ACLD . While 24 transactions of class 22 are misclassified as belonging to class 10 , we see in Table 1 that these two classes are measured as rather similar . In fact , if we sort the measurements in Table 1 per class , we find the same order as when we sort Table 3 on the number of misclassifications . The measured difference thus directly relates to the ability to distinguish classes .
Table 4 . Wine : classification confusion matrix .
Classified
Class
In Table 4 we see the same pattern with the Wine database as with the Heart database before : the lowest dissimilarities relate to the most misclassifications . We also observe that while analysis of individual code length differences , like Figure 3 , suggests there should be no confusion in classification , a number of transactions are misclassified . These can be tracked back as being artefacts of the 10 fold cross validation on a small database . 3.3 The Database Dissimilarity Measure The experiments presented above verified that the aggregated differences of database encodings provide a reliable means to measure the similarity of one database to another . To make it into a true dissimilarity measure , we would like it to be symmetric . Since the measure should indicate whether or not we should investigate the differences between two databases , we do this by taking the maximum value of two Aggregated Code Length Differences : as : 9 14 22
9 65 5 1
Class 14 3 55 1
Class 22 6 0 42
) ,
,
} )
{ ACLD max
,
CT b
( db b
( db a
CT a
ACLD
( 11 ) This can easily be rewritten in terms of compressed database sizes , without using the ACLD function . Definition 1 : for all databases x and y , define the code table dissimilarity measure DS between x and y as : yxDS
,(
)
= max y x )(CT CT
CT − x )( x
⎧ ⎪ ⎨ ⎪⎩ x )( x
CT x
, y )( y
CT y )( − CT y )( y
⎫ ⎪ ⎬ ⎪⎭
( 12 )
The databases are deemed very similar ( possibly identical ) iff the score is 0 , higher scores indicate higher levels of dissimilarity . Although at first glance this method comes close to being a distance metric for databases , this is not entirely the case . A distance metric D must be a function with nonnegative real values defined on the Cartesian product X ä X of a set X . Furthermore , it must obey the following requirements for every x,y,z ∈ X : 1 ) D(x,y ) = 0 iff x = y ( identity ) 2 ) D(x,y ) = D(y,z ) ( symmetry ) 3 ) D(x,y ) + D(y,z ) ≥ D(x,z ) ( triangle inequality ) For the MDL optimal compressors , we can prove that DS will be positive . For our code table compressors , we can not . However , the experiments in the previous two subsections as well as those in this one indicate that DS is unlikely to be negative . As we can not even guarantee that DS is always positive , we can certainly not prove the identity axiom . The second axiom , the symmetry axiom holds , of course , by definition . For the triangle inequality axiom we have again no proof . However , in the experiments reported on this subsection the axioms hold . In other words , for all practical purposes our measure acts as a distance measure . However , to clearly indicate that our measure is not a proven distance metric we call it a dissimilarity measure . The dissimilarity measurements for the Heart , Nursery and Wine database are given in respectively Tables 5 , 6 and 7 . One of the most striking observations is that many of the measurements are greater than 1.0 , meaning that the cross compressed databases are more than twice as large as the natively compressed databases . The differences between the Nursery16 and Nursery31 datasets are so huge that a dissimilarity measurement of 10.12 is the result : a staggering difference of a factor 11 of the average encoded length of a transaction .
DB 22 DB 26 DB 27 DB 36
Table 5 . Heart : dissimilarity .
DB 10
0.85 1.65 1.85 2.18
DB 22
0.78 0.65 1.07
DB 26
0.61 1.25
DB 27
1.09
Table 6 . Nursery : dissimilarity .
DB 15 DB 16 DB 30 DB 31
DB 2 2.62 2.83 3.10 7.38
DB 15 DB 16 DB 30
1.54
4.05 10.12
2.04 1.91 1.26
Table 7 . Wine : dissimilarity . DB 14
DB 9
DB 14 DB 22
1.27 1.32
1.73
In Table 8 a summary of datasets , their characteristics and dissimilarity results is given . For each dataset , the lowest and the highest observed dissimilarity is listed . A full results overview would obviously require too much space ; datasets with many classes have squared as many database pairs of which the dissimilarity can be measured . Overall , we see that the dissimilarities between the classes of the UCI datasets vary quite a bit . Some datasets seem to have very little difference between classes ( Connect 4 , Adult , TicTacToe ) , others contain rather large dissimilarity ( Mushroom , Iris , Led7 ) . Another interesting comparison is between the dissimilarities and the classification results also reported in that table , taken from [ 10 ] . There is a clear correlation between the two . The larger the dissimilarity , the better the classification results . This pattern is less clear for datasets containing small classes , which is caused by the fact that MDL doesn’t work well for small data sets . This observation is interesting because classification errors are made on individual transactions , whereas DS is an aggregated measure . In other words , the observation verifies that this aggregated measure reflects what happens at the level of individual transactions . This is exactly the property our dissimilarity measure should hold . 4 . CHARACTERISING DIFFERENCES The first benefit of our dissimilarity measure is that it quantifies the difference between databases , the second advantage is the ability to characterise those differences . There are three methods available for difference analysis , which zoom in to separate levels of difference between the distributions . First , we can compare the code table covers of the databases . This directly informs us which patterns that are important in one database are either over or under expressed in another database . The second approach is to zoom in on how specific transactions are covered by the different code tables . This reveals in detail where differences are identified by the code tables . Thirdly , we can extract knowledge about the specific differences and similarities between the distributions from the code tables . 4.1 Comparing database covers The most straightforward , but rather informative method for difference analysis is the direct comparison of database covers . Such evaluation immediately identifies which patterns are over and under expressed , showing us the characteristics of the differences in structure between the two databases . To run this analysis , we first use KRIMP to obtain a code table for database db2 and use it to cover database db1 . Because the item sets and their frequencies in the code table capture the data distribution of database db2 , the frequencies found by covering database db1 are expected to be different if the two databases are different . Identification of these differences is done by finding those patterns in the code table that have a large shift in frequency between the two database covers . The same process can be applied vice versa for even better insight of the differences . If the distribution is really different , we would expect to see a dramatic increase in use of the singletons caused by a decrease in use of the larger , more specific , sets . Slighter differences will lead to more specific shifts in patterns usage , with less of a shift towards singleton usage . An example visualisation can be seen in Figure 5 . A code table for Wine DB9 has been constructed and used to cover all three databases . A quick glance shows that our hypothesis on the use of singletons is correct : DB9 is covered by quite some sets of 2 or more items , but both DB14 and DB22 are covered largely by singletons . Of special interest is the contrast in peaks between the plots , indicating ( strong ) shifts in pattern usage . A rather strong difference
Table 8 . Database characteristics , candidate min sup and class dissimilarity measurements for a range of UCI datasets .
As candidates , all frequent item sets were used up to the given minimum support level .
Dataset Name Adult Chess ( kr k ) Connect 4 Heart Iris Led7 LetterRecog Mushroom Nursery PenDigits TicTacToe Wine
# rows
# classes
KRIMP Min sup
Accuracy ( % )
Dissimilarity ( DS ) Minimum
Maximum
48842 28056 67557 303 150 3200 20000 8124 12960 10992 958 178
2 18 3 5 3 10 26 2 5 10 2 3
20 10 50 1 1 1 50 50 1 20 1 1
84.6 58.0 69.9 52.5 96.0 75.3 68.1 100 92.4 88.6 87.1 97.7
0.60 0.29 0.18 0.61 2.06 1.27 0.43 8.24 1.26 1.33 0.62 1.27
0.60 2.69 0.28 2.18 13.00 11.29 2.83 8.24 10.12 4.43 0.62 1.73
Sets ( at least 2 items )
Singletons
A1
A2
B
DB 9
DB 14
DB 22 y c n e u q e r F
20
10
0
40
30
20 y c n e u q e r F
10
0
50
40
30
20
10
0 y c n e u q e r F
0
20
80
100
40 60 Code table elements
Figure 5 . Comparing database covers . Each database of Wine has been covered by code table CT9 . Visualised is the absolute frequency for each of the code table elements . in pattern usage is visible for the lower indexes in the code table , corresponding to the longest , most specific , patterns . However , in this figure the high peaks are also indicative ; we marked the peaks of an interesting case A1 and A2 . These peaks are at exactly the same code table element , meaning that this pattern is used quite often in the covers of both DB9 and DB14 . Note that it is not used at all in the cover of DB22 ; hence this pattern could really give us a clue as to what differentiates DB9 and DB14 from DB22 . Another interesting peak is the one indicated with B : although it is also applied in the other covers , this pattern is clearly used much more often to cover DB22 . 4.2 Comparing transaction covers A second approach for difference characterisation zooms in on individual database rows , and is thus especially useful when you are interested in specific transactions : why does a certain transaction belong to one database and not to another ? Again , we use our code tables to inspect this . Suppose we have two databases and their respective code tables . After computing the individual code length differences ( as de scribed in Section 3.1 ) , it is easy to pick out those transactions that fit well in one database and not in another . After selecting a transaction , we can cover it with both code tables separately and visualise which patterns are used for this . In general , it will be covered by longer and more frequent patterns if it belongs to a certain distribution than if it does not . Manual inspection of the individual transact tion covers can reveal valuable knowledge . As an example , have a look at another Wine example in Figure 6 . The encodings by CT9 and CT22 of two sets from DB22 are shown . Left and right show the same transactions , but they are covered by different item sets ( depicted by the rounded boxes ) . The item sets are linked to their codes with the dashed lines . The width of each black or white code represents the length of that particular code ; together the sum of these widths makes up the total length of the encoded transaction . Looking at the upper transaction , we observe that both code tables cover the transaction with item sets of intermediate length . However , CT22 uses less and different patterns in its cover than CT9 . Moreover , the code lengths are obviously shorter , relating to high occurrence in the distribution from which CT22 was induced . For further inspection of how important such patterns are , we zoom in to the pattern level in the third approach . The covers of the second transaction give an even larger contrast than the previous one . The native code table covers the transaction with few and large patterns , while the other one uses only singletons . We may therefore conclude this transaction fits very well in its native distribution and very bad in the other . This also shows in the lengths of the encodings . Both examples show again that more singletons are used in a cover when data doesn’t belong to a distribution . 4.3 Comparing code tables The final third method for difference inspection focuses on the individual patterns in a data distribution . In order to pinpoint the differences in this respect , we have to directly compare the patterns in two code tables . The weight and importance of patterns in the code tables cannot be compared naively , as for many of the patterns in a code table there does not have to be a direct equivalent in the other code table . However , the set of patterns in a code table can also be regarded as a database ; in that fashion we can actually apply code tables to each other to find out what the alternative encoded length for each pattern is . For each pattern in a code table we can compare its own encoded length to that of the alternative provided by the other code table , similarly to what we did for transactions in Section 31 Likewise , if the distributions are similar , we expect the encoded lengths to be comparable ; even if the code tables use rather different patterns to encode it . In contrast , exactly those patterns for which the encoded lengths differ significantly mark the difference between the distributions . We analysed the CT14 and CT22 code tables of the Wine dataset , and found further evidence for what puts these databases apart . The first peak in the topmost plot of Figure 5 corresponds to the pattern ( 0 16 19 20 24 ) from CT22 , which due to its high relative usage is encoded natively using only 14bits From the same figure we already know this pattern is not used when covering the other databases ; suggesting that perhaps neither this pattern , nor anything like it exists in the other code tables . Confirmation
16 19 24 15 29
1 25
0 12
13 26
36
62
16 26 36
12 13
0
1
15 19 24 25 29 62
CT22
CT9
0 16 19 20 24
6 7 36
1 25
3
4
29
0
1
3
4
6
7
16 19 20 24 25 29 36
Figure 6 . Wine ; two transactions from DB22 encoded by CT22 ( left ) and CT9 ( right ) . The rounded boxes visualise the item sets making up the cover of the transaction . Each of the item sets is linked to its code by the dashed line . The widths of the black and white encodings represent the actual computed code lengths . comes from an encoded length of 12.6bits that CT14 assigns to this pattern ; making it one of the patterns for which the encoded lengths differ most . As CT14 cannot use any of the more frequently occurring code table patterns , it has to resort to lowfrequency singleton encoding ; arguably the least efficient method for encoding a pattern . From the definition of the Wine database and analysis above we conclude that the main difference between the two classes lies in the combination of certain levels of malic acid ( element 0 ) and a corresponding colour intensity ( 16 ) . While CT22 has a number of patterns that give these short encodings , CT14 has virtually none : this pattern does not occur in this data distribution . The above example evidently shows that the differences between the data distributions can be directly analysed , and that through comparison of the code table encodings key differences can be extracted . Similarities as well as the differences between distributions are pinpointed . 5 . RELATED WORK Our dissimilarity measure DS is clearly related to the Normalized Information Distance ( NID ) and its compression based instantiation NCD [ 11 ] . With the NCD , general compressors like gzip are used as Kolmogorov complexity approximators and as such compressed sizes are used to measure distance between strings . As a generic distance , the NID has been successfully applied in a plethora of clustering tasks including small snippet based language and evolutionary tree rebuilding [ 4 ] . An adaptation was developed that has some practical data mining applications , among which compression based anomaly detection [ 9 ] . However , the aim of the NID is different from ours : compression is only used as a means to quantify differences , not to qualitatively find what these differences are . In contrast , this is the main goal of our line of research . This is illustrated by the results of both our earlier papers and this paper . By considering transactional databases instead of individual strings and building code tables that can be analysed , KRIMP provides a very natural way to gain insight in the differences between data distributions .
Our dissimilarity measure is also related to Emerging Patterns [ 5 ] , although there are major differences . First of all , here we only consider patterns that are MDL wise important with respect to the data distribution of a single database . The code table built allows to investigate other data sets ( or transactions ) from that particular database ’s perspective . This in contrast to Emerging Patterns , which are by definition identified as differences between pairs of databases , without regarding individual data distributions . Although we here focus on identifying differences , KRIMP also reveals similarities between databases ; arguably equally important when inspecting two databases . Also , when a large number n of databases is to be compared , constructing n code tables is computationally less intensive than mining n2 sets of Emerging Patterns . Secondly , Emerging Patterns are defined as patterns having a large difference in support ( growth rate ) between two databases . However , the frequencies used in our approach depend on the database cover , thus taking into account other patterns ( and their order ) in the code table . Through these dependencies , important changes in the structure of the data are enlarged and therefore easier to spot . Thirdly , KRIMP only selects small numbers of patterns . This allows for manual inspection at all stages , from data distribution approximation to difference detection and characterisation . Emerging Patterns suffer from the same combinatory explosion problem as frequent patterns : in order to capture all differences , a low ( zero ) growth rate has to be used , resulting in obstructively many patterns . Shorter descriptions have been defined for EPs , for example using borders [ 6 ] , but as these only give a shorter description for the same set of patterns , manual inspection remains impossible . The set of Emerging Patterns cannot straightforwardly be reduced by KRIMP . First , because it operates on individual databases , not on pairs . Second , to satisfy the MDL assumption , the candidate pattern set should enable the algorithm to grasp full data distributions , not just differences . This is guaranteed by the frequent pattern set , but not by a set solely consisting of EPs .
6 . CONCLUSIONS & FUTURE WORK In previous work , the MDL principle and its implementation in the KRIMP algorithm have proven themselves to be a reliable way for approximating the data distributions of databases . Here , we used this principle to develop a database dissimilarity measure with which characteristic differences between databases can be discovered . Histograms for encoded transaction lengths , and the differences thereof , show differences between data distributions straightforwardly . From the MDL principle , code tables with a good fit on the distribution of some data provide shorter codes and smaller standard deviations than code tables less suited for the data at hand . The code length difference is shown to be a good indication to how well a transaction fits a distribution . We show the informative quality of the aggregation of the code length differences . The measured likenesses show close relation to the confusion matrices of earlier classification experiments ; the number of misclassified instances drops according to this measure . We define a generic dissimilarity measure on databases as the maximum of two mirrored aggregated code length difference measurements ; it is symmetric and well suited to detect and characterise the differences between two databases . While we cannot prove it to fulfil the distance metric axioms , we argued that these hold for all practical purposes . A large advantage of our method is that it allows for thorough inspection of the actual differences between data distributions . Based on the dissimilarity , three methods for detailed inspection are proposed . The most detailed method zooms in onto and compares the patterns that describe the data distribution in the code tables . Individual transactions that do not fit the current distribution well can be identified . Further , it can be analysed why they do not fit that distribution well . Last but not least is the possibility to take a more global stance and pinpoint under or over expressed patterns in the respective databases . Dissimilarity measures are key to many different data mining algorithms . In the near future we expect to apply our measure in a number of bio informatics applications using these algorithms . For example , in those cases where classification appears to be hard ; deeper insight in the causes of these problems might suggest promising research directions .
7 . ACKNOWLEDGMENTS Jilles Vreeken is supported by the NWO Computational Life Sciences programme . Matthijs van Leeuwen is supported by the NBIC BioRange programme . Arno Siebes has to fend for himself .
8 . REFERENCES [ 1 ] Bathoorn , R . , Koopman , A . , and Siebes , A . Reducing the Frequent Pattern Set . In Proceedings IEEE Conf . on Data Mining – Workshops ( ICDMW ’06 ) 2006 , 55 59 .
[ 2 ] Codd , EF , SB Codd , CT Salley , Providing OLAP ( On
LineAnalytical Processing ) to User Analyst : An IT Mandate . http://wwwarborsoftcom/OLAPhtml 1994 .
[ 3 ] Coenen , F . The LUCS KDD Discretised/normalised ARM and CARM Data Library , http://wwwcsclivacuk/~frans/KDD/Software/ , Department of Computer Science , The University of Liverpool , 2003 .
[ 4 ] Cilibrasi , R . , and Vitanyi , P . Clustering by Compression . In IEEE Transactions on Information Theory . 51 , 4 ( Apr 2005 ) , 1523 1545 .
[ 5 ] Dong , G . , and Li , J . Efficient mining of emerging patterns : Discovering trends and differences . In Proc . SIGKDD conference on Knowledge Discovery in Data . ( KDD ’99 ) , 1999 , 43 52 .
[ 6 ] Dong , G . , and Li , J . Mining border descriptions of emerging patterns from dataset pairs . In Knowledge and Information Systems . 8 , 2 ( Aug 2005 ) , 178 202 .
[ 7 ] Goethals , B . et al . Frequent Itemset Mining Implementations
Repository , http://fimicshelsinkifi/
[ 8 ] Grünwald , PD Minimum description length tutorial . In
Advances in Minimum Description Length ( Grünwald , PD , Myung , IJ & Pitt , MA , editors ) . MIT Press , 2005 .
[ 9 ] Keogh , E . , Lonardi , S . , and Ratanamahatana , CA Towards Parameter Free Data Mining . In Proc . SIGKDD conference on Knowledge Discovery in Data . ( KDD’04 ) , 2004 , 206 215 .
[ 10 ] Van Leeuwen , M . , Vreeken , J . , and Siebes , A . Compression Picks Item Sets That Matter . In Proc . Knowledge Discovery in Databases ( PKDD’06 ) , 2006 , 585 592 .
[ 11 ] Li , M . , Chen , X . , Li , X . , Ma , B . , and Vitányi , PMB The
Similarity Metric . In IEEE Transactions on Information Theory . 50 , 12 ( Aug 2004 ) , 3250 3264 .
[ 12 ] Siebes , A . , Vreeken , J . , and Van Leeuwen , M . Item Sets That Compress . In Proceedings of the 2006 SIAM Conference on Data Mining . 2006 , 393 404 .
[ 13 ] Zhang , X . , Guozhu , D . , and Ramamohanarao , K . Informa tion based Classification by Aggregating Emerging Patterns . In Proc . Intelligent Data Engineering and Automated Learning , Data Mining , Financial Engineering , and Intelligent Agents ( IDEAL ) . 2000 , 48 53 .
