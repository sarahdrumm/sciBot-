Learning the Kernel Matrix in Discriminant Analysis via
Quadratically Constrained Quadratic Programming
Jieping Ye
Arizona State University
Tempe , AZ 85287 jiepingye@asuedu
Shuiwang Ji shuiwangji@asuedu
Arizona State University
Tempe , AZ 85287
Jianhui Chen
Arizona State University
Tempe , AZ 85287 jianhuichen@asuedu
ABSTRACT The kernel function plays a central role in kernel methods . In this paper , we consider the automated learning of the kernel matrix over a convex combination of pre specified kernel matrices in Regularized Kernel Discriminant Analysis ( RKDA ) , which performs linear discriminant analysis in the feature space via the kernel trick . Previous studies have shown that this kernel learning problem can be formulated as a semidefinite program ( SDP ) , which is however computationally expensive , even with the recent advances in interior point methods . Based on the equivalence relationship between RKDA and least square problems in the binary class case , we propose a Quadratically Constrained Quadratic Programming ( QCQP ) formulation for the kernel learning problem , which can be solved more efficiently than SDP . While most existing work on kernel learning deal with binary class problems only , we show that our QCQP formulation can be extended naturally to the multi class case . Experimental results on both binary class and multiclass benchmark data sets show the efficacy of the proposed QCQP formulations .
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications Data Mining
General Terms Algorithms
Keywords Quadratically constrained quadratic programming , kernel learning , model selection , kernel discriminant analysis , convex optimization
1 .
INTRODUCTION
Kernel methods [ 21 , 22 ] have been applied successfully in various machine learning tasks , such as dimensionality reduction , clustering , classification , and regression . They work by embedding the data into some high dimensional feature space through the so called kernel function . The key fact underlying the success of kernel methods is that the embedding into feature space can be uniquely determined by specifying the kernel function that computes the dot products between data points in the feature space . In other words , the kernel function implicitly defines the nonlinear mapping to the feature space and expensive computations in the high dimensional space can be avoided by evaluating the kernel function . Therefore , one of the central issues in kernel methods is the problem of kernel selection .
The problem of automated kernel learning has been an active area of research recently . Lanckriet et al . [ 12 ] pioneered the work of learning a linear combination of pre specified kernels for Support Vector Machines ( SVM ) [ 5 , 26 ] using convex programming and it has been improved in [ 3 ] using the Sequential Minimal Optimization ( SMO ) algorithm [ 20 ] . Reformulation of this problem as semi infinite linear program has been proposed in [ 23 ] along with extensions to regression and one class classification . While most approaches produce stationary combinations , Lewis et al . [ 13 ] proposed to use different combinations for different inputs . Argyriou et al . [ 2 ] proposed to combine potentially an infinite number of kernels , which was formulated as a difference of convex ( DC ) program . In general , approaches based on learning a convex combination of kernels facilitate heterogeneous data integration from multiple data sources and it has been applied for integrating various types of biological data , eg , amino acid sequences , hydropathy profiles , and gene expression data for enhanced biological inference [ 11 ] . Jebara [ 9 ] considered the kernel selection problem in the context of multi task learning . Zien and Ong [ 30 ] recently studied multi class multiple kernel learning .
This paper addresses the issue of kernel learning for regularized kernel discriminant analysis [ 16 , 17 ] . The proposed algorithms learn an optimal kernel matrix as a convex combination of a set of pre specified kernel matrices . In this sense , our proposed methods are similar to those proposed in [ 10 , 12 ] and can thus be used for heterogeneous data integration . The problem of kernel learning for discriminant analysis has been addressed originally in [ 6 ] . Kim et al . [ 10 ] formulated this problem as a semidefinite program ( SDP ) , and hence the globally optimal solution can be obtained . The algorithms in [ 6 , 10 ] deal with binary class problems only . They were extended to the multi class case in [ 28 ] . One limitation of the SDP formulation is its high computational cost , even with the recent advances in interior point
Research Track Paper854 methods . In this case , the overhead of model ( kernel ) selection may dominate the learning process . We address this issue in this paper . Key contributions of this paper include : • We propose a Quadratically Constrained Quadratic Programming ( QCQP ) formulation for discriminant kernel learning in the binary class case . It is known that interior point algorithms for solving QCQP problems are much more efficient than those of their SDP counterparts .
• We extend the above formulation to the multi class setting by decomposing the multi class RKDA kernel learning problem into a set of binary class kernel learning problems that are constrained to share a common kernel . It is worth noting that the optimal kernel function is the same for the original and the decomposed formulations , though the optimal transformation matrices may not coincide . That is , the decomposed form is equivalent to the original one for the purpose of kernel learning .
• We conduct extensive experiments to compare the proposed algorithms with several existing approaches for kernel learning , as well as cross validation based approaches under a common experimental setup . Our experimental results validate the effectiveness of the proposed QCQP formulations .
The rest of this paper is organized as follows : Section 2 provides background information on kernel methods , as well as the SDP formulation for kernel learning . Section 3 derives the QCQP formulation for the binary class case . Section 4 extends the QCQP formulation to the multi class case . Section 5 presents our experimental study and this paper concludes with discussion and conclusion in Section 6 .
2 . BACKGROUND Let X denote the input or instance space , which is a subspace of IRd , and Y = {−1 , +1} denote the output or class ie , an input output pair ( x , y ) , label set . An example , where x ∈ X and y ∈ Y , is called positive ( negative ) if its class label y is +1 ( −1 ) . We assume that the examples are drawn randomly and independently from a fixed , but unknown , underlying probability distribution over X × Y . A symmetric function K : X × X → R is called a kernel function [ 21 ] if it satisfies the finitely positive semidefinite property : for any x1,· ·· , xm ∈ X , the Gram matrix G ∈ IRm×m , defined by Gij = K(xi , xj ) is positive semidefinite . Any kernel function K implicitly maps the input set X to a high dimensional ( possibly infinite ) Hilbert space HK equipped with the inner product ( ·,·)HK through a mapping φK : X → HK :
K(x , z ) = ( φK ( x ) , φK(z))HK .
( 1 )
In kernel based classification , the algorithms learn a classifier f : X → {−1 , +1} whose decision boundary between the two classes is affine in the feature space HK :
T f ( x ) = sgn(w
( 2 ) where w ∈ HK is the vector of feature weights , b ∈ IR is the intercept , and sgn(u ) = +1 , if u > 0 , and −1 otherwise . m+} and {x 1 , ··· , x m−} denote the collec− −
1 ,· ·· , x+
Let {x+
φK(x ) + b ) , tions of data points from the positive and negative classes , m+ , x let X = [ x+
1 , ··· , x+
1 ,·· · , x − − m− ] be the respectively . data matrix . The total number of data points in the training set is m = m+ + m− . For a given kernel function K , the basic idea of RKDA in the binary class case is to find a direction in the feature space HK onto which the projections of the two sets {φ(x+ i=1 are well separated . Define the centroids of the two classes as follows : i=1 and {φ(x i )}m− − i )}m+
+ K =
μ
1 m+
φK ( x
+ i ) ,
− K =
μ
1 m−
− i ) ,
φK(x
( 3 ) m−
,i=1 m+
,i=1 m+ m−
,i=1 ,i=1 and the two sample class covariance matrices as follows :
+ K =
S
− K =
S
1 m+
1 m−
( φK(x
+ i ) − μ
+ K)(φK(x
+ i ) − μ
T
+ K )
, i ) − μ −
( φK(x
− K)(φK(x i ) − μ −
− K )
T
.
( 4 )
( 5 )
Specifically , in RKDA the separation between two classes K − μ − is measured by the ratio of the variance ( wT ( μ+ K))2 − K + m−/mS K w , the between the classes to wT  m+/mS+ K − μ variance within the classes . Thus , the following criterion is commonly maximized in RKDA : ( wT ( μ+ wT  m+/mS+ where λ > 0 is a regularization parameter . For a fixed kernel function K and regularization parameter λ , the optimal ∗ ≡ argmaxw {F1(w , K)} , that maximizes weight vector w the objective in Eq ( 6 ) is given by
K + λI w
− K))2 − K + m−/mS
F1(w , K ) =
,
( 6 )
− K + λI )
−1
+
K − μ
− K ) .
+ K + m−/mS w
= ( m+/mS
( 7 ) 1 ( K ) ≡ maxw {F1(w , K)} of the ob∗ The maximum value F jective function in Eq ( 6 ) achieved by the optimal weight vector w
( μ
∗
∗
∗ 1 ( K ) = ( μ
F is given by K−μ
− K )
+
+ K +
S m− m
S
T ' m+ m
−1
−
K + λIff
+
( μ
K−μ
− K ) . ( 8 ) ∗ 1 ( K ) in Eq ( 8 )
It can be shown [ 10 ] that the optimal value F can be simplified to the following :
∗ 1 ( K ) =
F
1 λ
G(I − J(λI + JGJ )
−1
T a
JG)a ,
( 9 ) where I is the identity matrix , a is defined as a = [ 1/m+,· ·· , 1/m+ , −1/m− , ··· ,−1/m− ] the matrix J is defined as : m+ em+ eT 0 m+ ( I − 1
J = 1√ m− ( I − 1 1√ m+ )
T ∈ IR m
,
( 10 )
0 m− em− eT m− ) , and em+ and em− are vectors of all ones of length m+ and m− , respectively . given kernel matrices G1,· ·· , Gp as
In [ 10 ] , G is restricted to be a linear combination of the p
G ∈ G = G))) ) ) p p
G =
θiGi ,
,i=1
θi = 1 , θi ≥ 0 ∀i .
,i=1
( 11 ) It was shown in [ 10 ] that , for a fixed regularization parameter λ , the optimal Gram matrix G computed from the kernel ∗ function K that maximizes F 1 ( K ) given in Eq ( 9 ) can be
Research Track Paper855 obtained by solving a SDP [ 4 , 25 ] . General purpose optimization packages such as SeDuMi [ 24 ] use the interiorpoint methods [ 18 ] to solve SDP . However , it is known that SDP is expensive to solve practically , and thus for problems of moderate size , this overhead of optimal kernel learning is large . In order to alleviate this computational problem , we propose in the next section a convex QCQP formulation for this kernel learning problem . Interior point methods for solving QCQP is more efficient than its SDP counterpart . Furthermore , the proposed QCQP formulation can be extended naturally to the multi class case .
3 . PROPOSED QCQP FORMULATION FOR
BINARY CLASS PROBLEMS
In the following discussion , we work on the centered version of kernel matrices . This is equivalent to a data centering process . More precisely , given a set of p kernel matrices G1,· ·· , Gp , the proposed algorithms learn an optimal kernel matrix ˜G ∈ ˜G , where
˜G = ˜G))) ) ) p p
˜G =
θi ˜Gi ,
,i=1
,i=1
θi ri = 1 , θi ≥ 0 ,
( 12 )
˜Gi = P GiP , ri = trace( ˜Gi ) , and P ∈ IRm×m is the centering matrix defined as
P = I − 1 m
T eme m ,
( 13 ) and em is the vector of all ones of size m .
Consider the maximization of the following objective func tion [ 28 ] :
F2(w , K ) =
K − μ
− ( wT ( μ+ K))2 wT ( ΣK + λI)w
, where ΣK is defined as follows :
ΣK =
1 m
φK ( X ) P φK(X )
T
,
( 14 )
( 15 )
φK(X ) is the data matrix in the feature space given by 1 ),·· · , φK ( x − −
1 ),·· · , φK ( x
+ m+ ) , φK ( x
+
φK(X ) = φK ( x
P is defined in Eq ( 13 ) , and m− ) ,
μK =
1 m m+ ,i=1
φK ( x
+ i ) + m−
,i=1
φK ( x
− i )
( 16 ) is the global centroid of the data in the feature space . It is easy to verify that for fixed K and λ , Eqs . ( 6 ) and ( 14 ) are equivalent for the computation of the optimal w .
Consider the regularized least squares problem , which min imizes the following objective function : w − a||2
F3(w , K ) = ||(φK(X)P )
T
+ λ||w||2
.
( 17 )
It is known that discriminant analysis and least square problems are equivalent in terms of the computation of the optimal w for fixed K and λ in the binary class case [ 15 ] . We show in the following lemma that discriminant analysis and least square problems are also equivalent in terms of the computation of the optimal kernel function K .
Lemma 31 Let w optimization problems :
∗
∗
∗
, ˜w
, and ˜K
∗ solve the following
, K
∗
∗
( w
( ˜w
, K
, ˜K
∗
∗
) = max
K max w
F2(w , K ) ,
) = min min w
F3(w , K ) .
( 18 )
( 19 )
Then w
∗
∗
= ˜w and K
∗
= ˜K
K ∗
.
Proof . The optimal vector w for a fixed K is given by
∗ w
= ( ΣK + λI )
∗ ≡ argmaxw {F2(w , K)} K − μ
− K ) .
( 20 )
+
( μ
−1 is given by +
The maximum value of the objective function in Eq ( 14 ) ∗ achieved by w 2 ( K ) ≡ F2(w ∗ ∗ F It follows from the Sherman Woodbury Morrison formula [ 7 ] that
K − μ
K − μ
( ΣK + λI )
, K ) = ( μ
− K ) .
− K )
−1
( μ
+
T
∗ w
T
+
( μ
−1
−1
− K )
= ( ΣK + λI )
K − μ = 'φK ( X ) P φK ( X ) = '(φK ( X)P )(φK(X)P )
+ λIff + λIff φK ( X) I − P ( λI + P GP )
1 λ
=
T
−1
φK ( X)a
−1
φK ( X)a
Since the vector a defined in Eq ( 10 ) is of zero mean , ie , P a = a , we have
= λ ˜G(λI + ˜G ) = λ( ˜G + λI − λI)(λI + ˜G ) = λI − λ
( λI + ˜G )
−1
,
2
−1
( 24 ) the optimal value F
∗ 2 ( K ) in Eq ( 23 ) can be simplified as
∗ T 2 ( K ) = a
F a − λa
T
( λI + ˜G )
−1 a .
( 25 )
∗
−1
For a fixed K and λ , the optimal ˜w objective function in Eq ( 17 ) is given by that minimizes the
∗
˜w
= 'λI + φK(X)P φK ( X )
Tff φK ( X) I − P ( λI + P GP )
1 λ
=
φK ( X)P a −1
P G a = w
∗
. ( 26 )
The optimal value of the objective function in Eq ( 17 ) is therefore given by ∗ T 3 ( K ) = λa
( 27 )
−1
F a .
( λI + ˜G ) ∗ 2 ( K ) in Eq ( 25 ) is equivalent ∗ 3 ( K ) in Eq ( 27 ) . This completes
Thus , the maximization of F to the minimization of F the proof .
( 21 )
P G a .
T
∗ w
−1
P G a . ( 22 )
−1
P G P a
( 23 )
F and thus ∗ 2 ( K ) = ( μ 1 λ
=
∗ 2 ( K ) =
F
=
T
+
∗ w
T a
− K )
T = a
φK ( X )
K − μ T  G − GP ( λI + P GP ) a P  G − GP ( λI + P GP ) T ' ˜G − ˜G(λI + ˜G ) −1 ˜Gff a , −1 ˜G = ˜G − ˜G(λI + ˜G ) −1
1 λ 1 λ a where ˜G = P GP is derived from G with both rows and columns centered . Since ˜G − ˜G(λI + ˜G )
( ˜G + λI − λI )
−1
Research Track Paper856 Based on this equivalence result , we can formulate the kernel learning problem for RKDA as a convex QCQP problem , as summarized in the following theorem .
Theorem 31 Given a set of p centered kernel matrices ˜G1,· ·· , ˜Gp , the optimal kernel matrix , in the form of linear combination of the given p kernel matrices , that optimizes the criterion in Eq ( 17 ) can be found by solving the following convex QCQP problem : max β,t subject to
T
− 1 β 4 t ≥ 1 ri
T a − 1 4λ t
β + β T ˜Giβ , i = 1,··· , p ,
β
( 28 ) where ri = trace( ˜Gi ) and r = [ r1 , r2,··· , rp ] .
Proof . We consider the dual formulation of the mini mization of F3(w , K ) in terms of w . Denote
η = ( φK ( X)P )
T w − a .
It follows that
F3(w , K ) = ||η||2
+ λ||w||2
.
This leads to the following optimization problem : + λ||w||2 w − a
F3(w , K ) = ||η||2
η = ( φK ( X)P ) subject to min w,η
T
( 29 )
( 30 )
( 31 )
Define its Lagrangian function as follows : L(η , w , β ) = ||η||2
+λ||w||2−β
( (φK(X)P )
T
T w−a−η ) , ( 32 ) where β is the vector of Lagrangian dual variables . Taking the derivative of L(η , w , β ) with respect to η and w and setting them equal to zero , we get
∂L(η , w , β )
∂η
∂L(η , w , β )
∂w
= 2η + β = 0 , = 2λw − φK ( X)P β = 0 .
( 33 )
( 34 )
It follows that
η = − β 2
, and w =
φK ( X)P β
2λ
.
( 35 )
Thus we obtain the following Lagrangian dual function : g(β ) = min w,η
L(η , w , β ) = − 1 4
β
T flI +
1 λ
P GPffi β + β
T a .
The optimal β
∗ is computed by maximizing g(β ) as
∗
β
= argmaxβffl− 1
4
β
T flI +
1 λ
P GPffi β + β
T a .
( 36 )
Since strong duality holds , the optimal kernel is given by solving the following optimization problem :
˜G∈ ˜G max min
β ffl− 1
4
β
T flI +
1 λ
˜Gffi β + β
T a .
( 37 )
We can rewrite the above optimization problem as
θ : θ≥0 , θT r=1 p p
4
β
4
β
1 λ min min max
β − 1 θ : θ≥0 , θT r=1 − 1 θ : θ≥0 , θT r=1 − 1 β − 1 β ffl− 1
T I + T I + ,i=1 4λ a − 1 4λ a − 1 4λ
θi ˜Gi β + β θi ˜Gi β + β
,i=1 1 ,i=1 λ T ˜Giβ − 1 4 θ : θ≥0 , θT r=1 p ,i=1 i fl 1 T ˜Giβffi .
β + β
β + β
β + β max max min
θiβ
θiβ
θiβ ri
T
β
T
β
T
β
4
4
T
T p
T
T a a a T ˜Giβ
T
= max
β
= max
β
= max
= max
( 38 )
The exchange of minimization and maximization in deriving the second equation from the first holds since the objective function is convex in θ and concave in β , the minimization problem is strictly feasible in θ and the maximization problem is strictly feasible in β . Therefore , Slater ’s condition [ 4 ] follows and strong duality holds [ 12 , 4 ] . By simply changing the last term in the above equation to t and moving it to the constraint , we prove this theorem .
Note that general purpose optimization software packages like SeDuMi [ 24 ] and MOSEK [ 1 ] also solve the dual problem . Thus the coefficients θ1 , ··· , θp can be obtained from the dual variables by dividing the corresponding traces of centered kernel matrices . The formulation in Eq ( 28 ) is a QCQP , which is a special form of Second Order Cone Program ( SOCP ) [ 14 ] and SDP . Theoretical results on interior point method [ 18 ] show that QCQP can be solved more efficiently than SDP and it is therefore more scalable to largescale problems . The worst case complexity of this QCQP formulation is O(pn3 ) . Similar ideas have been used in [ 12 ] to formulate the kernel learning problem of SVM as a nonnegative linear combination of some given kernel matrices .
Recall that all kernel learning formulations discussed in [ 10 , 12 ] are constrained to binary class problems . We show in the next section that our formulation in this section can be extended naturally to deal with multi class problems .
4 . PROPOSED QCQP FORMULATION FOR
MULTI CLASS PROBLEMS
In multi class classifications , we are given a data set that consists of m samples {(xi , yi)}m i=1 , where xi ∈ IRd , and yi ∈ {1 , 2,·· · , k} is the class label of the i th sample . Similar to the binary class case , let X = [ x1,··· , xm ] be the data matrix .
In the multi class RKDA formulation , the maximization of the following objective function is commonly used [ 27 ] :
−1
T
BK Wffi , F4(W , K ) = tracefl'W ( 39 ) where W = [ w1 , w2 , ··· , wff ] ( ( < k ) is the transformation matrix , and BK , the so called between class scatter matrix is defined as
( ΣK + λI ) Wff
W
T
( 40 ) H = [ h1 , h2,··· , hk ] , and hi is a vector whose j th entry is
BK = φK(X)HH
φK(X )
,
T
T
Research Track Paper857 given by
− hi(j ) = n − ni ni n n ni if yj = i otherwise .
( 41 )
The optimal W is given by computing the top eigenvectors of the following matrix :
( ΣK + λI )
−1
BK .
Since the weight vectors are in the span of the images of the data points in the feature space , we can express W as W = φK ( X)A for some matrix A ∈ IRm×ff , where A = [ α1,·· · , αff ] . Then F4(W , K ) = tracefl'A
( GP G + λG ) Aff
GAffi .
GHH
T A
−1
T
T
Define two matrices SK t and SK b as follows :
K t = GP G + λG , K b = GHH
G .
T
S
S
( 42 )
( 43 )
Since the null space of SK t exists a nonsingular matrix Z such that [ 7 ] lies in the null space of SK b , there
T
Z
S
K t Z =fl I
0 0 ffi , Z
0
T
S
K b Z =fl Σb
0
0
0 ffi ,
( 44 ) where Σb is diagonal with the diagonal entries sorted in non∗ decreasing order . The optimal A is given by
∗ A
= Zq = [ z1,·· · , zq ] ,
( 45 ) where Zq consists of the first q columns of Z , and q = rank(SK It follows that the optimal value of F4(W , K ) ∗ achieved by the optimal A is given by b ) .
∗
F
4 ( K ) = trace(Σb ) = tracefl'S
K t ff
−1
S
K b ffi .
( 46 )
Note that we have assumed SK t = GP G + λG is nonsingular in the above discussion . In case it is singular , we could use the pseudo inverse , and all the following derivations still hold .
Thus , in the multi class case , the optimal kernel function ∗ K can be computed by maximizing F 4 ( K ) in Eq ( 46 ) , which is however highly nonlinear and difficult to solve . In [ 28 ] , we present an equivalent formulation as the one in Eq ( 46 ) , which is more tractable computationally . The main result is summarized in the following lemma :
Lemma 41 Let F4 be defined as in Eq ( 39 ) and
F5(W , K ) =
( wT i φK ( X)hi)2 wT i ( ΣK + λI)wi
,
( 47 ) k
,i=1 where W = [ w1,· ·· , wk ] , and hi is defined in Eq ( 41 ) . Let solve the following optimization probW lems :
, and ˜K
, ˜W
, K
∗
∗
∗
∗
∗
∗
∗
∗
, K
, ˜K
( W
( ˜W
) = max
K
) = max
K max
W max
W
F4(W , K ) ,
F5(W , K ) .
( 48 )
( 49 )
Then K
∗
= ˜K
∗
.
∗
It is interesting to note that , in general , the optimal W and ˜W for the optimization problems in Eqs . ( 48 ) and ( 49 ) , respectively , are different . Furthermore , the objective function in Eq ( 47 ) is closely related to its binary counterpart in Eq ( 14 ) . Note that a variant of the Fisher Discriminant Ratio ( FDR ) [ 10 ] can be written as :
∗
F2(w , K ) =
( wT φK ( X ) a)2 wT ( ΣK + λI)w
.
( 50 )
Thus F5(W , K ) in Eq ( 47 ) can be interpreted as the weighted summation of the FDRs between the samples in the i th class and the rest where i = 1,·· · , k . The weights can be computed from the definition of H in Eq ( 41 ) as follows : hi = n ni
− ni n − ni = ( n − ni ) ni n n a
,
( i )
= ( n − ni ) ni n
1 ni
− 1 n−ni where a(i ) is obtained from Eq ( 10 ) by taking the samples from the i th class as positive and the rest as negative . Thus the weight for the i th binary classification problem is : ( n− ni)2ni/n .
Next , we propose a QCQP formulation for the multi class RKDA kernel learning problem . Consider the minimization of the following objective function : k
T wi − hi||2
,i=1'||(φK(X)P )
+ λ||wi||2ff , ( 51 ) F6(W , K ) = where W = [ w1,· ·· , wk ] . It is clear that for a fixed K , the computations of wi and wj for i = j are independent of each other . By extending the results from Lemma 3.1 and Lemma 4.1 , it is easy to show that the optimal kernel function K minimizing the objective function in Eq ( 47 ) coincides with the minimizer of F6(W , K ) in Eq ( 51 ) . Motivated by this equivalence result , we derive an efficient QCQP formulation for the multi class RKDA kernel learning problem , as summarized in the following theorem .
Theorem 41 Given a set of p centered kernel matrices ˜G1,··· , ˜Gp , the optimal kernel matrix , in the form of linear combination of the given p kernel matrices , that optimizes the criterion in Eq ( 51 ) can be found by solving the following convex QCQP problem : max
β1,··· ,βk,t subject to k
β
,j=1 t ≥ 1 ri
T j hj − 1 4 k
T t
β j βj − 1 4λ
,j=1 j ˜Giβj , i = 1,· ·· , p .
T
β k
,j=1
( 52 ) where ri = trace( ˜Gi ) .
Proof . We first consider the dual formulation of the minimization of F6(W , K ) in terms of W for a fixed K . Denote
ηi = ( φK ( X)P )
T wi − hi .
( 53 )
Research Track Paper858 It follows that k
F6(w , K ) =
||ηi||2
+ λ
Adding a variable t , we prove the formulation in Eq ( 52 ) .
||wi||2
.
( 54 )
5 . EXPERIMENTAL EVALUATIONS
,i=1 k
,i=1
Define the Lagrangian function as follows : L({ηi , wi , βi}k
||ηi||2 i=1 ) = k k
,i=1 − k ,i=1
+ λ
,i=1 i '(φK(X)P )
T
||wi||2 wi − hi − ηiff ,
T
β
In this section , we empirically evaluate the proposed QCQP formulations in comparison with several existing kernel learning approaches . The experimental setup is similar to the one in [ 10 ] . We used the MOSEK package [ 1 ] to solve the QCQP formulations . 5.1 Experiments on Binary class Problems
In the binary class case , we compared our formulations with the SDP formulation proposed in [ 10 ] , 1 norm soft margin SVM , 2 norm soft margin SVM with and without the regularization parameter C optimized jointly as proposed in [ 12 ] . We also employed double cross validation to choose kernels and regularization parameters for SVM and RKDA . Four data sets were used in the binary case . The sonar , ionosphere , and breast cancer data sets were retrieved from the UCI Machine Learning Repository [ 19 ] . The heart data set was obtained from the STATLOG project1 . For each data set , we randomly partitioned the entire data set into training and test sets with 80 % of the data in the training set and 20 % in the test set . Following [ 10 ] , we focus on learning a convex combination of ten Gaussian kernels :
10
−||x−z||2/σ2 i .
θi e
K(x , z ) =
,i=1 The values of σi are chosen uniformly on the logarithmic −1 , 102 ] , as in [ 10 ] . Then the ten scale over the interval [ 10 kernels were fed into the optimization packages to obtain the corresponding coefficients for each kernel . Finally , the combined kernel was used for classification .
( 59 )
Tables 1 and 2 show the corresponding experimental results on the sonar , heart , ionosphere , and breast cancer data −8 for the proposed QCQP sets . The λ value is fixed to 10 formulation , as used in [ 10 ] . Following [ 12 ] , we fix C to 1 for SM1 and SM2 . We can observe from the tables that the proposed QCQP formulation is comparable to three SVM based approaches , while they are very competitive with other three approaches , including SDPKim , RKDAK,λ , and SVMK,C . Recall that the first five methods in the tables avoid crossvalidation and they can be used for heterogeneous data integration from multiple sources , while RKDAK,λ and SVMK,C choose the single best kernel .
To understand the relative importance of each kernel when they are used individually , we fix the kernel to each of the ten pre specified kernels and tune the λ in RKDA and C in SVM using cross validation and the accuracy of each kernel is recorded ( RKDAλ and SVMC ) . We recorded the number of times that a particular kernel has been selected by RKDAK,λ and SVMK,C in double cross validation . We expect these quantities to have certain relationship with the coefficients learned by convex optimizations .
For the sonar data , cross validated RKDA achieves the best performance on kernels corresponding to θ6 and θ7 while cross validated SVM achieves the highest accuracy on θ6 , θ7 , and θ8 . On the other hand , methods using linear combination of kernels seem to favor kernels corresponding to θ5 , θ6 , and θ7 . For the heart data , cross validated SVM favors kernels corresponding to θ8 , θ9 , and θ10 ( they were
1 http://wwwliaccuppt/ML/old/statlog/datasetshtml where the βi ’s are the vectors of Lagrangian dual variables . Taking the derivative of L with respect to ηi and wi for all i , and setting them equal to zero , we get
∂L ∂ηi ∂L ∂wi
= 2ηi + βi = 0 , = 2λwi − φK ( X)P βi = 0 .
( 55 )
( 56 )
Thus we have
ηi = − βi 2
, and wi =
φK ( X)P βi
2λ
,
( 57 ) and we obtain the following Lagrangian dual function : g(β1 , ··· , βk ) = i=1 ) min wi,ηi,i=1,··· ,k k
L({ηi , wi , βi}k i flI +
1 λ
T
β
= 1 ,··· , β ∗ The optimal β g(β1,··· , βk ) as 1 ,· ·· , β ∗
P GPffi βi + β
,i=1fl− 1 k ) = argmaxβ1,··· ,βk g(β1,· ·· , βk ) . ∗
∗ k can be computed by maximizing i hiffi .
( β
4
T
Since strong duality holds , the optimal kernel function K is given by solving the following optimization problem :
β1,··· ,βk k
˜G∈ ˜G max min
,i=1fl− 1
4
β
T i flI +
1 λ
˜Gffi βi + β
T i hiffi .
( 58 )
Similar to the binary class case , the above optimization problem can be written as min
θ : θ ≥ 0 , θT r = 1
= max
β1,··· ,βk
= max
β1,··· ,βk , . fi k
'j=1 k
4 max
β1 ,··· ,βk , . fi θ : θ ≥ 0 θT r = 1
'j=1ff− 1 , . 'j=1ff− 1 fi j hj − 1 βT min
4 k k
4
'j=1
βT j ffI +
1 λ p
'i=1θi
βT j ffI +
1 λ
˜Gi( βj + βT ˜Gi( βj + βT
θi j hj( ) ff j hj( ) ff ) ˜Giβjffi ffl ff
, p
'i=1 i fi fl j βj − 1 βT
4λ max
1 ri k
'j=1
βT j where the second equality follows since
, . fi − 1 4 min
θ : θ ≥ 0 θT r = 1 k
'j=1ff− 1
4
βT j ffI +
1 λ
= min
θ : θ ≥ 0 θT r = 1
, . fi j hj − 1 βT
4
= k
'j=1 j βj − 1 βT
4λ k
'j=1 p
'i=1 j βj − 1 βT
4λ k
'j=1 max
θ : θ ≥ 0 θT r = 1 p
'i=1 θifi fl
θi
˜Gi( βj + βT k
βT j
'j=1 , . 'i=1 fi p
˜Giβjffi ffl θifi fl k
'j=1 k
+
βT j hj( ) ff j hj ) 'j=1 ff ) ˜Giβjffi ffl ff
βT j
.
Research Track Paper859 Table 1 : Comparison of seven methods for the sonar data set . The seven tested methods , listed from top to bottom are : the QCQP formulation as proposed in Theorem 3.1 , SDP formulation proposed in [ 10 ] , 1norm soft margin SVM , 2 norm soft margin SVM without and with C optimized as proposed in [ 12 ] , RKDA and SVM with the kernels and regularization parameters selected by double cross validation . In general , subscripts of names in the first column are used to denote quantities that are optimized . The ten pre specified kernels are all RBF kernels and the σ values used are 0.10 , 0.22 , 0.46 , 1.00 , 2.15 , 4.46 , 10.00 , 21.54 , 46.42 , 10000 The table is partitioned into three ( row wise ) sections . In the first section , the columns titled with θi are the coefficients learned from the corresponding methods . The column titled with λ/C provides the values of the regularization parameters , whether fixed or learnt , and the test set accuracies are given in the last column . The second section includes RKDA and SVM with kernel and regularization parameter chosen by double cross validation . We also report the number of times that a particular kernel is selected by cross validation . The third section shows the accuracies of RKDA and SVM when the kernel is fixed and the regularization parameters chosen by cross validation . Dashes are used to denote non applicable items .
θ1 0
θ2 0
θ3 0
1.552
0.421
1.914 sonar QCQP SDPKim SM1 SM2 SM2C RKDAK,λ c SVMK,C RKDAλ e SVMC d b
0 0 0 0 0
53.65 53.65
θ4 0
1.200 0.040
0
θ5
1.673 1.013 3.953 2.875 4.253
0 0
θ6
0.976 3.630 5.514 6.765 6.038
9 4
θ7
0.083 0.271 0.491 0.359 0.570
16 16
θ8
θ9
0.214
0.254
θ10 0.824
0 0 0
0 0 0
0.004
0.001
5 8
0 2
0 0 0 0 0 0
0 0
0 0
0.011
0.014
0.084
0 0
0 0
0 0
λa/C 1.0e 5 1.0e 7
1 1
5.5e+7
– – – –
TSA 90.16 88.46 89.75 89.59 89.84 88.86 89.27
– –
60.24 59.91
73.57 73.41
54.95 54.63 −8 for the QCQP formulation . The values shown here were scaled along with the coefficients
84.95 84.22
89.91 90.65
84.95 86.09
90.56 89.67
86.99 89.59
85.52 86.58 aWe originally assigned λ = 10 ( θ1 to θ10 ) on the same scale for ease of presentation . bThe number of times that a kernel is chosen by doubly cross validated RKDA over 30 randomizations . cThe number of times that a kernel is chosen by doubly cross validated SVM over 30 randomizations . dAccuracies of RKDA when the kernel is fixed to each of the ten pre specified kernels and λ is chosen by cross validation . eAccuracies of SVM when the kernel is fixed to each of the ten pre specified kernels and C is chosen by cross validation . chosen 10 , 9 , 9 times out of 30 , respectively ) while crossvalidated RKDA uses kernels corresponding to θ6 , θ7 , and θ8 most frequently . Our QCQP formulation gives the kernel corresponds to θ10 a large weight while all other methods using linear combination of kernels give this kernel a zero weight . Another interesting observation is that SM1 , SM2 , and SM2C give the first kernel a large weight while it performs poorly when used separately . Similar behavior has been observed for the breast cancer data where large weights have been assigned to the second kernel while it is not the best individual kernel . This implies that the best individual kernel may not lead to a large weight when used in combination with others and poorly performed individual kernel may contain complementary information that is useful when combined with other kernels . Such complementary information can not be incorporated when cross validation is used to choose a single best kernel . For the ionosphere data , the best three individual kernels chosen by cross validation are kernels corresponding to θ5 , θ6 , and θ7 . Interestingly , the kernel corresponding to θ5 has a zero weight for all methods using a linear combination of kernels . Overall , the pattern of the coefficients learned from our QCQP formulation is similar to those of the SVM based methods .
We compared the running time of the proposed QCQP formulation with two other RKDA based kernel learning algorithms including the SDP formulation in [ 10 ] and doubly cross validated RKDA ( RKDAK,λ ) . Figure 1 shows the running time of 30 different runs of these three methods , where the x axis denotes the 30 different splits of the data into the training and test sets and the y axis denotes the running time ( in seconds ) . Table 3 summarizes the average running time over 30 runs . It is clear that the proposed QCQP formulation is much more efficient than the SDP formulation . For example , for the breast cancer data set the average running time over 30 splits for SDPKim is about 2 , 000 seconds while the QCQP formulation takes about 6.4 seconds . Results also show that the QCQP formulation is much more efficient than doubly cross validated RKDA .
5.2 Experiments on Multi class Problems
In the multi class experiment , we compared our formulations with KRDA and SVM with kernels and regularization parameters tuned using double cross validation . The methods proposed in [ 10 , 12 ] are restricted to binary class problems only . We used a total of five data sets for this experiment . The USPS handwritten digits database was described in [ 8 ] . We choose the first 3 , 6 , and 8 classes with 100 samples in each class for the experiments . The wine and waveform data were obtained from UCI Machine Learning Repository and the satimage and segment were obtained from the STATLOG project . We used the first 3 , 5 , and 6 classes for the satimage data and the first 3 and 4 classes for the segment data . For each data set , we randomly partitioned the entire set into two subsets with 60 % in the training set and 40 % in the test set . Ten RBF kernels , with the σ assigned the same values as in the binary case were constructed from the training set .
Figure 2 shows the classification results ( measured in error rate ) on ten data sets . We can observe from the figure that the proposed QCQP formulation is very competitive with
Research Track Paper860 Table 2 : Comparison of seven methods for the heart , ionosphere , and cancer data sets . The result of SDPKim on the cancer data set is not shown due to the occurrence of numerical problems when running the SDP solver ( SeDuMi , http://sedumimcmasterca/ ) See the caption and footnotes of Table 1 for explanation .
θ1
1.821 8.530 7.688 7.317 6.746
0 0
58.64 57.96
θ1
6.477 9.452 3.553 2.883 3.910
0 0
65.71 65.38
θ1
θ4 0 0
θ5 0 0
0.002
0.002
θ2
0.206 0.333 0.479 0.669 0.626
0 0
θ3 0
0.004 0.001
0 0 0 0
0 0 0 0
65.06 64.75
θ2
69.62 71.79
θ3
73.33 76.60
θ4
1.315
0.593
2.007
0
0.672 0.682 0.714
0 0
0
0.482 0.683 0.561
0 0
0
0.240 0.196 0.255
2 0
0 0 0 0
77.28 79.93
θ5 0 0 0 0 0 3 8
76.47 66.57
θ2
90.33 89.38
θ3
92.14 93.00
θ4
93.33 94.57
θ5
θ6
0.001 0.272 0.024 0.029 0.036
8 0
79.13 80.30
θ6
3.068 0.401 4.828 5.305 5.300
11 14
94.28 95.04
θ6
θ7 0
0.860 1.813 1.994 1.991
8 2
78.70 81.66
θ7
6.895 0.148 0.221 0.248 0.256
12 4
93.14 93.80
θ7
θ8 0 0 0 0 0 7 10
θ8 0 0 0 0 0 2 2
91.71 93.42
θ8
0.3692
6.356
1.265
0.517
0.135
0.252
3.085
0.281
—
1.797 1.483 1.690
0 0
—
5.706 5.541 4.855
0 0
—
0.179 0.402 0.546
1 0
—
0.008 0.023 0.047
2 0
— 0
0.006 0.003
4 0
—
2.308 2.527 2.521
4 9
— 0
0.013 0.015
1 6
— 0 0 0 1 6 heart QCQP SDPKim SM1 SM2 SM2C RKDAK,λ SVMK,C RKDAλ SVMC ionosphere QCQP SDPKim SM1 SM2 SM2C RKDAK,λ SVMK,C RKDAλ SVMC cancer QCQP SDPKim SM1 SM2 SM2C RKDAK,λ SVMK,C RKDAλ SVMC
77.65 81.54
76.79 82.22
θ9
0.048
θ10 2.579
0 0 0 0 4 9
θ9 0 0 0 0 0 0 0
90.61 92.61
θ9 0 — 0 0 0 3 7
0 0 0 0 3 9
75.92 82.59 θ10 0.023
0 0 0 0 0 2
89.00 91.95 θ10 0.612
— 0 0 0 14 2
λ/C 1.0e 5 1.0e 7
1 1
4.4e+5
– – – –
λ/C 1.0e 4 1.0e 7
1 1
1.4e+7
– – – –
λ/C 1.0e 4
— 1 1
1.0e+4
– – – –
TSA 82.65 81.98 82.59 82.71 82.53 76.54 82.22
– –
TSA 95.10 89.14 95.28 94.81 95.19 93.62 93.52
– –
TSA 97.05
—
97.08 97.15 97.01 95.30 96.62
– –
94.54 92.21
95.32 94.93
96.05 96.03
96.15 96.30
96.30 96.81
95.74 96.88
95.59 96.86
95.59 96.66
95.49 96.69
95.64 96.64
Table 3 : Comparison of running time ( in seconds ) of three methods averaged over 30 random partitions . data sonar heart ionosphere cancer
QCQP SDPKim RKDAK,λ
0.61 1.13 1.98 6.40
28.45 43.06 147.00 2040.60
6.38 9.22 21.19 75.20 the other two methods based on cross validation . Compared with the other two methods , the proposed method learns a convex linear combination of kernels by avoiding the crossvalidation . It is expected to perform even better for heterogeneous data integration , when kernels from multiple data sources contain complementary information .
6 . DISCUSSION AND CONCLUSION
We addressed the issue of automated kernel learning for discriminant analysis in this paper . We formulate this kernel learning problem as a convex program and thus a globally optimal solution is guaranteed . Practically , some convex optimization problems , such as SDP are computationally expensive and we proposed a QCQP formulation for kernel learning , which is much more efficient to solve than SDP . While most existing work on kernel learning only deal with binary class problems , we show that our binary formulation can be extended naturally to the multi class setting .
We conducted extensive experiments to evaluate the proposed algorithms . The proposed formulations are competitive with the SDP based formulation , SVM based methods , and doubly cross validated SVM and RKDA in classification . In terms of running time , the QCQP formulation is much more efficient than its SDP counterpart . When evaluating the relative importance of each kernel ( either used separately or in linear combination ) , we found that the best individual kernel sometimes coincides with the highlyweighted kernels in linear combination and sometimes disagrees considerably . If complementary information exists among different kernels , the proposed formulations can potentially utilize such information .
There are several directions for future work . Most previous formulations for learning SVM kernels are restricted to the binary class case . The idea from this paper may be useful for kernel learning in multi class SVM . Lanckriet et al . [ 12 ] considered the problem of optimizing the kernel and regularization parameter simultaneously . We plan to investigate the effectiveness of incorporating the learning of the
Research Track Paper861 ) d n o c e s n i ( e m i t
35
30
25
20
15
10
5
0
) d n o c e s n i ( e m i t
180
160
140
120
100
80
60
40
20
0
QCQP SDPKim RKDAK,λ
5
10
15
30 splittings sonar
20
25
30
QCQP SDPKim RKDAK,λ
5
10
15
30 splittings ionosphere
20
25
30
) d n o c e s n i ( e m i t
60
50
40
30
20
10
0
) d n o c e s n i ( e m i t
2500
2000
1500
1000
500
0
QCQP SDPKim RKDAK,λ
5
10
15
30 splittings heart
20
25
30
QCQP SDPKim RKDAK,λ
5
10
15
30 splittings cancer
20
25
30
Figure 1 : Comparison of running time ( in seconds ) of three methods on four data sets . regularization parameter in the framework . The proposed QCQP formulations are still computationally expensive , especially for problems with a large number of samples and a large number of classes . We plan to examine other optimization techniques [ 23 , 30 ] for efficient multi class multiple kernel learning in discriminant analysis . The proposed formulations can be applied for heterogeneous data integration from multiple data sources . We plan to apply these approaches to the analysis of biological images [ 29 ] .
Acknowledgments This research is sponsored by the Center for Evolutionary Functional Genomics of the Biodesign Institute at Arizona State University and by the National Science Foundation Grant IIS 0612069 .
7 . REFERENCES [ 1 ] E . D . Andersen and K . D . Andersen . The MOSEK interior point optimizer for linear programming : an implementation of the homogeneous algorithm . In T . T . H . Frenk , K . Roos and S . Zhang , editors , High Performance Optimization , pages 197–232 . Kluwer Academic Publishers , 2000 .
[ 2 ] A . Argyriou , R . Hauser , C . Micchelli , and M . Pontil . A DC programming algorithm for kernel selection . In
Proceedings of the International Conference on Machine Learning , pages 41–48 , 2006 .
[ 3 ] F . R . Bach , G . R . G . Lanckriet , and M . I . Jordan .
Multiple kernel learning , conic duality , and the SMO algorithm . In Proceedings of the International Conference on Machine Learning , 2004 .
[ 4 ] S . Boyd and L . Vandenberghe . Convex Optimization .
Cambridge University Press , 2004 .
[ 5 ] N . Cristianini and J . Taylor . An Introduction to Support Vector Machines and other Kernel based Learning Methods . Cambridge University Press , 2000 .
[ 6 ] G . Fung , M . Dundar , J . Bi , and B . Rao . A fast iterative algorithm for Fisher discriminant using heterogeneous kernels . In Proceedings of the International Conference on Machine Learning , 2004 .
[ 7 ] G . H . Golub and C . F . Van Loan . Matrix
Computations . The Johns Hopkins University Press , third edition , 1996 .
[ 8 ] J . J . Hull . A database for handwritten text recognition research . IEEE Trans . Pattern Analysis Machine Intelligence , 16(5):550–554 , 1994 .
[ 9 ] T . Jebara . Multi task feature and kernel selection for SVMs . In Proceedings of the International Conference on Machine Learning , 2004 .
Research Track Paper862 4.5
4
3.5
3 t e a r r o r r e
2.5
2
1.5
1
0.5
0
QCQP RKDAK,λ SVMK,C
USPS(3 )
USPS(6 )
USPS(8 ) data set wine(3 ) satimage(3 ) t e a r r o r r e
18
16
14
12
10
8
6
4
2
0
QCQP RKDAK,λ SVMK,C satimage(5 ) satimage(6 ) segment(3 ) segment(4 ) waveform(3 ) data set
Figure 2 : Comparison of the error rate of the proposed multi class QCQP formulation with doubly crossvalidated RKDA and SVM on ten data sets . The numbers in the parenthesis denote the total numbers of classes used in each of the data sets .
[ 10 ] S J Kim , A . Magnani , and S . Boyd . Optimal kernel
[ 20 ] J . C . Platt . Fast training of support vector machines selection in kernel Fisher discriminant analysis . In Proceedings of the International Conference on Machine Learning , pages 465–472 , 2006 .
[ 11 ] G . Lanckriet , T . D . Bie , N . Cristianini , M . Jordan , and W . Noble . A statistical framework for genomic data fusion . Bioinformatics , 20(16):2626–2635 , 2004 . using sequential minimal optimization . In Advances in kernel methods : support vector learning , pages 185–208 . MIT Press , Cambridge , MA , USA , 1999 . [ 21 ] S . Sch¨olkopf and A . Smola . Learning with Kernels :
Support Vector Machines,Regularization , Optimization and Beyond . MIT Press , 2002 .
[ 12 ] G . Lanckriet , N . Cristianini , P . Bartlett , L . E . Ghaoui ,
[ 22 ] J . Shawe Taylor and N . Cristianini . Kernel Methods and M . I . Jordan . Learning the kernel matrix with semidefinite programming . Journal of Machine Learning Research , 5:27–72 , 2004 .
[ 13 ] D . Lewis , T . Jebara , and W . S . Noble . Nonstationary kernel combination . In Proceedings of the International Conference on Machine Learning , pages 553–560 , 2006 .
[ 14 ] M . S . Lobo , L . Vandenberghe , S . Boyd , and H . Lebret .
Applications of second order cone programming . Linear Algebra and its Applications , 284:193–228 , 1998 . for Pattern Analysis . Cambridge University Press , 2004 .
[ 23 ] S . Sonnenburg , G . R¨atsch , C . Sch¨afer , and
B . Sch¨olkopf . Large Scale Multiple Kernel Learning . Journal of Machine Learning Research , 7:1531–1565 , July 2006 .
[ 24 ] J . F . Sturm . Using SeDuMi 1.02 , a MATLAB toolbox for optimization over symmetric cones . Optimization Methods and Software , 11 12:625–653 , 1999 . [ 25 ] L . Vandenberghe and S . Boyd . Semidefinite programming . SIAM Review , 38:49–95 , 1996 .
[ 15 ] S . Mika . Kernel Fisher Discriminants . PhD thesis ,
[ 26 ] V . Vapnik . Statistical learning theory . Wiley , New
University of Technology , Berlin , Oct . 2002 .
[ 16 ] S . Mika , G . R¨atsch , and K R M¨uller . A mathematical programming approach to the kernel fisher algorithm . In T . K . Leen , T . G . Dietterich , and V . Tresp , editors , Proceedings of the Annual Conference on Neural Information Processing Systems , pages 591–597 . MIT Press , 2001 .
[ 17 ] S . Mika , G . R¨atsch , J . Weston , B . Sch¨olkopf , and
K R M¨uller . Fisher discriminant analysis with kernels . In Y H Hu , J . Larsen , E . Wilson , and S . Douglas , editors , Neural Networks for Signal Processing IX , pages 41–48 . IEEE , 1999 .
[ 18 ] Y . Nesterov and A . Nemirovskii . Interior point polynomial algorithms in convex programming . SIAM Studies in Applied Mathematics . SIAM , 1994 .
[ 19 ] D . Newman , S . Hettich , C . Blake , and C . Merz . UCI repository of machine learning databases , 1998 .
York , 1998 .
[ 27 ] J . Ye . Characterization of a family of algorithms for generalized discriminant analysis on undersampled problems . Journal of Machine Learning Research , 6:483–502 , 2005 .
[ 28 ] J . Ye , J . Chen , and S . Ji . Discriminant kernel and regularization parameter learning via semidefinite programming . In Proceedings of the International Conference on Machine Learning , 2007 .
[ 29 ] J . Ye , J . Chen , Q . Li , and S . Kumar . Classification of drosophila embryonic developmental stage range based on gene expression pattern images . In Proceedings of the Computational Systems Bioinformatics Conference , pages 293–298 , 2006 .
[ 30 ] A . Zien and C . Ong . Multiclass multiple kernel learning . In Proceedings of the International Conference on Machine Learning , 2007 .
Research Track Paper863
