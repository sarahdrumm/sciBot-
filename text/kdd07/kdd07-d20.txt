Co clustering based Classification for Out of domain
Documents
Wenyuan Dai†
Gui Rong Xue†
Qiang Yang‡ †Shanghai Jiao Tong University , Shanghai , China {dwyak , grxue , yyu}@apexsjtueducn
Yong Yu†
‡Hong Kong University of Science and Technology , Hong Kong , China qyang@cseusthk
ABSTRACT In many real world applications , labeled data are in short supply . It often happens that obtaining labeled data in a new domain is expensive and time consuming , while there may be plenty of labeled data from a related but different domain . Traditional machine learning is not able to cope well with learning across different domains . In this paper , we address this problem for a text mining task , where the labeled data are under one distribution in one domain known as in domain data , while the unlabeled data are under a related but different domain known as out of domain data . Our general goal is to learn from the in domain and apply the learned knowledge to out of domain . We propose a coclustering based classification ( CoCC ) algorithm to tackle this problem . Co clustering is used as a bridge to propagate the class structure and knowledge from the in domain to the out of domain . We present theoretical and empirical analysis to show that our algorithm is able to produce high quality classification results , even when the distributions between the two data are different . The experimental results show that our algorithm greatly improves the classification performance over the traditional learning algorithms .
Categories and Subject Descriptors I26 [ Learning ] : Induction General Terms Algorithms , Experimentation Keywords Classification , Co clustering , Out of domain , Kullback Leibler divergence 1 .
INTRODUCTION
Document classification plays an important role in many text processing tasks , ranging from search engines to online advertisements . Traditional document classification algorithms rely on the availability of a large amount of labeled data . In practice , labeled data are often scarce , especially for learning tasks in new domains . When a task from a new domain comes , it may be the case that we have no labeled data at all . Labeling data for classification can be expensive and time consuming in general , but there may be plenty of labeled data from a related but different domain . This may be the case when the labeled data are out of date , but the new data are obtained from fast evolving information sources . Unfortunately , traditional machine learning fails to deal with this situation , since it requires that the labeled and unlabeled data be drawn from the same distribution . This raises a critical problem on how to learn from the labeled data from one domain , and then classify the documents from another domain accurately .
In this paper , we focus on the problem of classifying documents across different domains . We have a labeled data set Di from one domain which is called in domain , and another data set Do from a related but different domain which is called out of domain . The latter is unlabeled and to be classified . Di and Do are drawn from different distributions , since they are from different domains . This may be the case when we consider two related Web directories , for example , when one directory contains documents about cars , and another about trucks . We assume that , the class labels in Di and the labels to be predicted in Do are drawn from the same class label set C . Furthermore , we assume that even though the two domains are different in distributions , they are similar in the sense that similar words describe simiIn other words , the true probability of a lar categories . class label given a word is very close in the two domains . This assumption is often true , as we will also demonstrate in our experiments , since Di and Do are related text domains , although some words in one domain may be missing in the other domain , which makes the estimated probability in the two domains to be quite different . Under such circumstances , our objective is to accurately classify the outof domain documents in Do , by making use of the in domain data Di and their labels .
We propose a novel co clustering based classification algorithm to solve this problem , as briefly shown in Figure 1 . First , the in domain data Di provide the class structure , which defines the classification task , by propagating label information . Then , co clustering [ 9 ] is extended for outof domain data Do to obtain out of domain document and word clusters , as the step 2 in Figure 1 . Our key extension
Research Track Paper210 1 . Word clustering
2 . Co clustering
Documents in i
Word
Documents in o
1 . Label Propagation
2 . Label Propagation
Figure 1 : The model of our co clustering based classification algorithm to co clustering is that class labels in Di can constrain the word clusters , which is shared among the two domains , as the step 1 in Figure 1 . This allows each out of domain cluster to be mapped to a corresponding class label based on their correlation with the document categories in Di , completing our classification task . A key intuition of our work is that even though the two domains may be under different distributions , we are able to identify a common part between them . In our work , this common part is the common words . Class information and knowledge passes through common words from the in domain to the out of domain . Moreover , the word clustering part of co clustering can even enrich the common words by drawing together seemingly unrelated words .
In this paper , we define a unified information theoretic formulation for the above learning task . The objective function for building the co clustering based categorization is designed to minimize a loss function in mutual information between out of domain documents and words , and between words and class labels in the in domain data set , simultaneously . As a result , the class category knowledge provided by in domain data Di is used as a constraint to enhance the classification of out of domain documents based on coclustering .
To show that our co clustering based classification algorithm works well , we carry out theoretical analysis to show that our algorithm increasingly optimizes the objective function as our algorithm iterates to completion , by converging quickly to a locally optimal co clustering result . We supplement the theoretical study with extensive experiments , where we demonstrate that our algorithm is effective in making the predictions for out of domain documents .
The rest of the paper is organized as follows . In Section 2 , we discuss the related works . In Section 3 , some preliminary concepts from information theory is introduced . The problem formulation is presented in Section 4 . Section 5 proposes our co clustering based classification algorithm . The empirical analysis is presented in Section 6 . Section 7 concludes the whole paper and give some future works . Some detailed proofs of the theoretical conclusions are given in the Appendix .
2 . RELATED WORK
In this section , we review several prior works mostly related to our work , including traditional classification , multitask and multi domain learning , and semi supervised clustering .
2.1 Classification Learning
The traditional classification formulation assumes that the class labels are given for training data under the same distribution as the test data . Two schemes are generally considered , where one is supervised classification and the other is semi supervised classification . Supervised classification focuses on the case where the labeled data are sufficient , and where the learning objective is to estimate a function that maps examples to class labels using the labeled training instances . Naive Bayes Classifiers [ 20 ] and Support Vector Machines [ 3 ] are known as two of the most effective methods for document classification .
Semi supervised classification [ 28 ] addresses the problem that the labeled data are too few to build a good classifier . It makes use of a large amount of unlabeled data , together with a small amount of the labeled data to enhance the classifiers . Many semi supervised learning techniques have been proposed , eg , co training [ 2 ] , EM based methods [ 23 ] , cluster based methods [ 27 ] , transductive learning [ 15 ] etc .
Both supervised and semi supervised classification assume that the distributions of the labeled and unlabeled data should be identical . However , in our problem , the labeled data are from in domain , while the unlabeled data are from out of domain . The distributions of the labeled and unlabeled data are different from each other . This violates the basic assumption of traditional supervised and semisupervised classification algorithms .
2.2 Multi task and Multi domain Learning
Another related learning research area is multi task learning , where the domain specific information in related tasks ( training and test data sets ) are jointly trained in a way that can benefit each other [ 4 ] . A shared representation is exploited while the extra tasks can be used as an inductive bias during learning . By defining the common knowledge carefully , it is possible to allow the knowledge learned from each task to help the learning of other tasks .
In contrast to multi task learning , our problem should be considered as single task learning , since the class labels for in domain and out of domain are from the same class label set . However , our classification problem crosses different domains . This problem can be referred to as multi domain learning , or cross domain learning . [ 25 ] studied on crossdomain learning in neural network , while we focus on the cross domain text classification . [ 7 ] studied a similar problem where they investigated how to learn a general model from the in domain and out of domain labeled data to train a statistical classifier for a natural language Mention Type Classification and Tagging problem . in our work , we assume that the out of domain data are completely unlabeled .
In contrast ,
2.3 Semi supervised Clustering
Semi supervised clustering [ 11 ] builds clusters under some additional constraints provided by a few labeled data , in the form of must links ( two examples must in the same cluster ) and cannot links ( two examples cannot in the same cluster ) . It finds a balance between satisfying these constraints and optimizing the original clustering objective function . Several semi supervised clustering algorithms have been proposed , including [ 1 , 5 , 12 , 10 ] .
Semi supervised clustering provides a good method to make use of a few labeled data in clustering . However , the mustlink and cannot link constraints must be available for clustering to work . When the labeled data are few , the samedistribution requirement can be relaxed . This fact makes it feasible to extend semi supervised clustering for different distribution data sets .
Research Track Paper211 In this paper , we consider a co clustering based classification algorithm which extends the information theoretic co clustering approach of [ 9 ] , where constraints given by indomain data is added to the word clusters [ 8 ] to provide a class structure and partial categorization knowledge . Our algorithm is essentially a classification algorithm using the co clustering technique . It will be shown theoretically and empirically that our algorithm works well for classifying outof domain documents .
In addition to building clusters , we are interested in using the class label knowledge gained from in domain data to help classify out of domain problems , which is not solvable by traditional semi supervised clustering algorithms alone . As we will present later , our algorithm adds constraints on the word clusters to help assign labels to co clustering results . The class structures on word clusters are passed on from the in domain data to the out of domain data , which makes classification possible .
3 . PRELIMINARIES
In this section , we introduce some preliminary concepts from information theory that will be used frequently in this paper . For more details , please refer to [ 6 ]
Let X and Y be random variable sets with a joint distribution p(X , Y ) and marginal distributions p(X ) and p(Y ) . The mutual information I(X ; Y ) is defined as
X
X
I(X ; Y ) = p(x , y ) log x y p(x , y ) p(x)p(y )
.
( 1 )
The mutual information is a measure of the dependency between random variables . It is always non negative , and it is zero if and only if the variables are statistically independent . Higher mutual information values indicate more certainty that one random variable depends on another .
The use of mutual information can also be motivated using the Kullback Leibler ( KL ) divergence or relative entropy measures , defined for two probability mass functions p(x ) and q(x ) ,
X
D(p||q ) = p(x ) log x p(x ) q(x )
.
( 2 )
KL divergence can be considered as a kind of a distance between the two probability distributions , although it is not a real distance measure because it is not symmetric . Besides , KL divergence is always non negative [ 6 ] .
4 . PROBLEM FORMULATION Let Di be the set of in domain data with labels , Do be the set of out of domain data without labels . Di and Do can also be considered as random variable sets that take the in domain and out of domain instances as random variables , respectively . From the in domain data Di , we are able to get a set of class labels C which is the class structure information . The labels ( which are unknown and to be predicted ) of out of domain data Do are also drawn from the same label set C . From Di and Do , the word set W can be obtained from the word occurrences in Di and Do .
In our approach , we take co clustering as a bridge to propagate the knowledge from the in domain to out of domain . Co clustering on out of domain data aims to simultaneously cluster the out of domain documents Do and words W into
|C| document clusters and k word clusters , respectively . Let ˆDo denote the out of domain document clustering , and ˆW denote the word clustering , where | ˆW| = k . The document cluster partition function CDo and the word clusterpartition function CW can be defined as
CDo ( d ) = ˆd , where d ∈ ˆd ∧ ˆd ∈ ˆDo CW ( w ) = ˆw , where w ∈ ˆw ∧ ˆw ∈ ˆW
( 3 )
( 4 ) where ˆd represents the document cluster that d belongs to and ˆw represents the word cluster that w belongs to . Then , the co clustering can be represented by ( CDo , CW ) or ( ˆDo , ˆW ) .
In order to measure the quality of a co clustering , we de fine the loss for co clustering in mutual information as
I(Do;W ) − I( ˆDo ; ˆW ) .
( 5 )
This form of loss function is the same as that used in [ 9 ] . From Equation ( 5 ) , we know that co clustering aims to minimize the loss in mutual information between documents and words before and after clustering . Since our problem is to classify out of domain documents Do , a key point is to add the knowledge about classes to the co clustering process , which is extracted from in domain data Di . In this paper , we use the relationship between word clusters and class labels to apply class label information to the co clustering . We define the loss in mutual information for a word clustering as
I(C;W ) − I(C ; ˆW ) .
( 6 )
This form of loss function is the same as that used in [ 8 ] . Equation ( 6 ) indicates that a word clustering should minimize the loss in mutual information between class labels C and words W before and after clustering , for in domain data .
Integrating Equations ( 5 ) and ( 6 ) , the loss function for co clustering based classification can be obtained :
I(Do;W ) − I( ˆDo ; ˆW ) + λ · ( I(C;W ) − I(C ; ˆW ) )
( 7 ) where λ is a trade off parameter that balances the effect to word clusters from co clustering ( see Equation ( 5 ) ) and word clustering ( see Equation ( 6) ) . The objective is to find a co clustering that minimizes the function value of Equation ( 7 ) .
With Equation ( 7 ) , we can now describe our process of classifying out of domain documents through co clustering . Since I(Do;W ) and I(C;W ) are fixed , minimizing Equation ( 7 ) equivalents maximizing I( ˆDo ; ˆW ) and I(C ; ˆW ) simultaneously – maximizing I( ˆDo ; ˆW ) + λ · I(C ; ˆW ) . Based on the definition of mutual information in Section 3 , in order to maximize I( ˆDo ; ˆW ) and I(C ; ˆW ) , ˆDo should depend on ˆW , and ˆW should depend on C . Under our problem assumption , ˆDo would depend on C , which indicates that the clusters in ˆDo should be rely on the classes in C . We can let the number of document clusters be the same as the number of class labels to enable a 1 1 mapping between them . That is , we let | ˆDo| = |C| , and build a mapping between ˆDo and C based on the dependence between each ˆd ∈ ˆDo and each c ∈ C . Then , using the co clustering based classification approach that optimizes Equation ( 7 ) , the documents in Do will be assigned to their corresponding classes according to cluster membership , which enables our co clustering based classification approach .
Research Track Paper212 In the rest of this section , we will rewrite the objective function in Equation ( 7 ) into another form that is represented by KL divergence . Before rewriting the objective function , let us first define some probability mass functions . Definition 1 . Let f ( Do,W ) denote the joint probability distribution of Do and W . That is f ( d , w ) = p(d , w ) .
( 8 ) ˆf ( Do,W ) denotes the joint probability distribution of Do and W under co clustering ( ˆDo , ˆW ) that
ˆf ( d , w ) = p( ˆd , ˆw)p(d| ˆd)p(w| ˆw ) = p( ˆd , ˆw )
( 9 ) where d ∈ ˆd and w ∈ ˆw , where ˆd is a document cluster , and ˆw is a word cluster , respectively . Similarly , g(C,W ) denotes the joint probability distribution of C and W that p(w ) p( ˆw ) p(d ) p( ˆd )
, g(c , w ) = p(c , w ) .
( 10 ) ˆg(C,W ) denotes the joint probability distribution of C and W under the word clustering ˆW that
ˆg(c , w ) = p(c , ˆw)p(w| ˆw ) = p(c , ˆw ) where w ∈ ˆw . p(w ) p( ˆw )
,
( 11 )
The marginal and conditional probability distributions for f , ˆf , g and ˆg can be defined naturally . For example ,
ˆf ( d ) =
ˆf ( d , w ) , and ˆf ( d|w ) =
ˆf ( d , w ) ˆf ( w )
.
( 12 )
Lemma 1 . For a fixed co clustering ( ˆDo , ˆW ) , we can write the loss in mutual information as
I(Do;W ) − I( ˆDo ; ˆW ) + λ · ( I(C;W ) − I(C ; ˆW ) ) = D(f ( Do,W)|| ˆf ( Do,W ) ) + λ · D(g(C,W)||ˆg(C,W) ) . where D(·||· ) is defined in Equation ( 2 ) .
Proof .
( 13 )
I(Do;W ) − I( ˆDo ; ˆW ) + λ · ( I(C;W ) − I(C ; ˆW ) )
X w
ˆd∈ ˆDo
− d∈ ˆd d∈ ˆd w∈ ˆw
ˆw∈ ˆW
ˆw∈ ˆW
ˆd∈ ˆDo
X X X X 0 X X @X X X X X X X X X X X X X
− λ X
ˆw∈ ˆW
ˆw∈ ˆW
ˆw∈ ˆW w∈ ˆw w∈ ˆw d∈ ˆd
+ λ c∈C c∈C w∈ ˆw
ˆd∈ ˆDo
=
= p(d , w ) log p(d , w ) p(d)p(w )
1 A log p(d , w )
X w∈ ˆw p( ˆd , ˆw ) p( ˆd)p( ˆw ) p(c , w ) log
! p(c , w ) p(d , w ) log p(c , w ) p(c)p(w ) p(c , ˆw ) p(c)p( ˆw ) p(d , w ) p(d ) p( ˆd ) p( ˆd , ˆw ) p(w ) p( ˆw ) p(d , w ) p(d , ˆw ) p(w ) p( ˆw )
( 14 )
( 15 )
+ λ c∈C
ˆw∈ ˆW w∈ ˆw p(d , w ) log
=
X
ˆd∈ ˆDo
+ λ
X X X X
ˆw∈ ˆW d∈ ˆd
X X w∈ ˆw f ( d , w ) log c∈C
ˆw∈ ˆW g(d , w ) log w∈ ˆw f ( d , w ) ˆf ( d , w ) g(d , w ) ˆg(d , w )
( 16 )
= D(f ( Do,W)|| ˆf ( Do,W ) ) + λ · D(g(C,W)||ˆg(C,W ) )
( 17 )
Equation ( 13 ) shows that the loss in mutual information in the objective function equals to the sum of KL divergence between f and ˆf and KL divergence between g and ˆg . To minimize the objective function in Equation ( 7 ) , we need only to minimize the KL divergence between f and ˆf , and the KL divergence between g and ˆg .
5 . CO CLUSTERING BASED
CLASSIFICATION
We now describe the co clustering based classification algorithm for classifying the out of domain data , which minimizes the objective function in Equation ( 13 ) . The objective function given in Equation ( 13 ) is a multi part function which is hard to be optimized . Therefore , we should find a way to make the optimization easier . Lemmas 2 and 3 show an alternative approach , which allows us to iteratively reduce the divergence values .
Lemma 2 . D(f ( Do,W)|| ˆf ( Do,W ) )
X
X
=
ˆd∈ ˆDo
X d∈ ˆd f ( d)D(f ( W|d)|| ˆf ( W| ˆd) ) , X f ( w)D(f ( Do|w)|| ˆf ( Do| ˆw) ) .
D(f ( Do,W)|| ˆf ( Do,W ) )
=
ˆw∈ ˆW w∈ ˆW
Proof . D(f ( Do,W)|| ˆf ( Do,W ) )
X X d∈ ˆd w∈ ˆw
X X X w∈ ˆw
ˆw∈ ˆW f ( d )
ˆd∈ ˆDo
ˆw∈ ˆW
X X X X
ˆd∈ ˆDo
X X X X d∈ ˆd
ˆd∈ ˆDo d∈ ˆd
=
=
=
=
ˆd∈ ˆDo
ˆw∈ ˆW d∈ ˆd f ( d , w ) log f ( d , w ) ˆf ( d , w ) f ( d)f ( w|d ) log X f ( w|d ) log w∈ ˆw f ( d)f ( w|d ) f ( d ) ˆf ( w| ˆd ) f ( w|d ) ˆf ( w| ˆd ) f ( d)D(f ( W|d)|| ˆf ( W| ˆd ) )
( 18 )
( 19 )
( 20 )
( 21 )
( 22 )
( 23 )
Note that Equation ( 21 ) is based on
ˆf ( d , w ) = p( ˆd , ˆw)p(d| ˆd)p(w| ˆw ) = p(d ) p( ˆd , ˆw ) p( ˆd ) = p(d)p( ˆw| ˆd)p(w| ˆw ) = f ( d ) ˆf ( w| ˆd )
( 25 ) The last equality follows by p(d ) = f ( d ) and p( ˆw| ˆd)p(w| ˆw ) = ˆf ( w| ˆd ) . p(w ) p( ˆw )
( 24 )
Research Track Paper213 Using the same argument , we can prove that
D(f ( Do,W)|| ˆf ( Do,W ) )
X
X
=
ˆw∈ ˆW w∈ ˆw f ( w)D(f ( Do|w)|| ˆf ( Do| ˆw ) )
( 26 )
Lemma 2 tells us that minimizing D(f ( W|d)|| ˆf ( W| ˆd ) ) corresponding to a single document d can reduce the global objective function value given in Equation ( 13 ) . The same conclusion can be derived for minimizing D(f ( Do|w)|| ˆf ( Do| ˆw ) ) corresponding to a single word w .
Lemma 3 . D(g(C,W)||ˆg(C,W ) ) =
X
X
ˆw∈ ˆW w∈ ˆw g(w)D(g(C|w)||ˆg(C| ˆw) ) .
( 27 )
The proof of Lemma 3 is omitted , and it can be derived using the similar argument to Lemma 2 . From Lemma 3 , we can obtain the similar conclusion with that in Lemma 2 . According to Lemmas 2 and 3 , our co clustering based classification algorithm , called CoCC , is derived . This algorithms iteratively searches a co clustering for the out ofdomain data , and then assigns class labels to the document clusters to complete the classification task .
Algorithm 1 The Co clustering based Classification ( CoCC ) Algorithm Input : A labeled in domain data set Di ; an unlabeled outof domain data set Do ; a set C of all the class labels ; a set W of all the word features ; initial co clustering ( C ( 0)W ) ; the number of iterations T . Initialize the joint probability distribution f , ˆf , g and ˆg based on Equations ( 8 ) , ( 9 ) , ( 10 ) and ( 11 ) , respectively . For t ← 1 , 3 , 5 , . . . , 2T + 1
( 0)Do , C
1 : Compute the document cluster : the function D(f ( W|d)|| ˆf ( W| ˆd ) ) ( see Equation ( 28) ) . As we discussed above , this can reduce the global objective function value in Equation ( 13 ) . Then , in each iteration , the algorithm chooses the best word cluster ˆw to minimize the function D(f ( Do|w)|| ˆf ( Do| ˆw ) ) and D(g(C|w)||ˆg(C| ˆw ) ) simultaneously ( see Equation ( 29) ) . This can reduce the global objective function value too . We will prove the monotonically decreasing property of the objective function in the following theorem :
Theorem 4 . The algorithm CoCC in Algorithm 1 mono tonically decreases the objective function in Lemma 1 . D(f ( Do,W)|| ˆf D(f ( Do,W)|| ˆf
( Do,W ) ) + λ · D(g(C,W)||ˆg ( Do,W ) ) + λ · D(g(C,W)||ˆg
( t+1 )
( t+1 )
( t )
( t )
( C,W ) ) ≥ ( C,W) ) . ( 30 )
The detailed proof of Theorem 4 is given in the Appendix . Note that , although the algorithm is able to minimize the objective function value in Equation ( 13 ) , it is only able to find a locally minimal one . Finding the global optimal co clustering is NP hard .
Corollary 5 . Algorithm 1 converges in a finite number of iterations .
Proof . Since the total number of co clusterings is finite , the corollary can be derived straightforward from Theorem 4 .
Regarding the computational complexity , suppose the total number of document word co occurrences in Do is N . For each iteration , updating CDo takes O(|C| · N ) , while updating CW takes O((|C| +| ˆW|)· N ) . The number of iterations is T . Therefore , the time complexity of our co clustering based classification algorithm is O((|C|+| ˆW|)·T ·N ) . In the experiments , it is shown that T = 10 is enough for convergence . Considering space complexity , our algorithm need to store all the document word co occurrences and their corresponding probabilities . Thus , the space complexity is O(N ) .
( t)Do ( d ) = arg min C ˆd
D(f ( W|d)|| ˆf
( t−1 )
( W| ˆd ) )
( 28 )
6 . EXPERIMENTS
2 : Update the probability distribution ˆf ( t ) based on C
( t−1)W , and Equation ( 9 ) . C C ˆg(t−1 ) .
( t)W = C
( t−1)W
( t)Do , and ˆg(t ) =
3 : Compute the word cluster :
C
( t+1)W ( w ) = arg min f ( w)D(f ( Do|w)|| ˆf + λ · g(w)D(g(C|w)||ˆg
ˆw
( t )
( Do| ˆw ) ) ( C| ˆw ) ) ( 29 )
( t )
4 : Update the probability distribution ˆg(t+1 ) based on ( t+1)Do =
( t+1)W , and Equation ( 11 ) . ˆf ( t+1 ) = ˆf ( t ) and C C ( t)Do . C
End For
Output : the partition functions C
( T )Do and C
( T )W .
As shown in Algorithm 1 , in each iteration , the algorithm chooses the best document cluster ˆd for each d to minimize
In order to evaluate the properties of our algorithm , we perform the experiments in this section . In the experiments , we focus on the binary classification . Moreover , the data sets are all balanced between the class labels . Note that the binary classifiers can be easily extended for multiple class . 6.1 Data Sets
We conducted experiments on three data sets , 20 Newsgroups [ 18 ] , SRAA [ 21 ] and Reuters 21578 [ 19 ] . In order to make the data set satisfy our problem setting , we split the original data in a way to make the labeled and unlabeled data drawn from related but different domains , as follows .
611
20 Newsgroups
The 20 Newsgroups [ 18 ] is a text collection of approximately 20,000 newsgroup documents , partitioned across 20 different newsgroups nearly evenly . We generated six different data sets for evaluating cross domain classification algorithms . For each data set , two top categories1 are chosen , 1Three top categories , misc , soc and alt are removed , because they are too small .
Research Track Paper214 Data Set comp vs sci rec vs talk rec vs sci sci vs talk comp vs rec comp vs talk
Di comp.graphics composms windowsmisc sci.crypt sci.electronics rec.autos rec.motorcycles talkpoliticsguns talkpoliticsmisc rec.autos recsportbaseball sci.med sci.space sci.electronics sci.med talkpoliticsmisc talkreligionmisc comp.graphics compsysibmpchardware compsysmachardware rec.motorcycles recsporthockey comp.graphics compsysmachardware compwindowsx talkpoliticsmideast talkreligionmisc
Do compsysibmpchardware compsysmachardware compwindowsx sci.med sci.space recsportbaseball recsporthockey talkpoliticsmideast talkreligionmisc rec.motorcycles recsporthockey sci.crypt sci.electronics sci.crypt sci.space talkpoliticsguns talkpoliticsmideast composms windowsmisc compwindowsx rec.autos recsportbaseball composms windowsmisc compsysibmpchardware talkpoliticsguns talkpoliticsmisc
Table 1 : The description of 20 Newsgroups data sets for cross domain classification .
Data Set auto vs aviation real vs simulated
Di sim auto & sim aviation
Do real auto & real aviation real aviation & sim aviation real auto & sim auto
Table 2 : The description of SRAA data sets for cross domain classification . one as positive and the other as negative . Then , we split the data based on sub categories . Different sub categories can be considered as different domains , while the task is defined as top category classification . The splitting strategy ensures the domains of labeled and unlabeled data related , since they are under the same top categories . Besides , the domains are also ensured to be different , since they are drawn from different sub categories . Table 1 shows how we generated the data sets in our experiments .
612
SRAA
SRAA [ 21 ] is a Simulated/Real/Aviation/Auto UseNet data set for document classification . 73,218 UseNet articles are collected from four discussion groups about simulated autos ( sim auto ) , simulated aviation ( sim aviation ) , real autos ( real auto ) and real aviation ( real aviation ) .
Consider the task that aims to predict labels of instances between real and simulated . We use the documents in real auto and sim auto as in domain data , while real aviation and sim aviation as out of domain data . Then , the data set real vs simulated is generated as shown in Table 2 . As a result , all the data in the in domain data set are about autos , while all the data in the out of domain set are about aviation . The auto vs aviation data set is generated in the similar way as shown in Table 2 .
613 Reuters 21578
Reuters 21578 [ 19 ] is one of the most famous test collections for evaluation of automatic text categorization techniques . It contains 5 top categories . Among these categories , orgs , people and places are three big ones . For the category places , we removed all the documents about the USA to make the three categories nearly even , because more than a half of the documents in the corpus are in the USA sub categories . Reuters 21578 corpus also has hier
Data Set real vs simulated auto vs aviation rec vs talk rec vs sci comp vs talk comp vs sci comp vs rec sci vs talk orgs vs places people vs places orgs vs people
K L
1.161 1.126 1.102 1.021 0.967 0.874 0.866 0.854 0.329 0.307 0.303
|Di| 8,000 8,000 3,669 3,961 4,482 3,930 4,904 3,374 1,079 1,239 1,016
Documents
|Do| 8,000 8,000 3,561 3,965 3,652 4,900 3,949 3,828 1,080 1,210 1,046
|W| 14,433 14,433 19,412 18,152 17,918 18,379 18,903 20,057 4,415 4,562 4,771
SVM
Di–Do 0.266 0.228 0.233 0.212 0.103 0.317 0.165 0.226 0.454 0.266 0.297
Do–CV 0.032 0.033 0.003 0.007 0.005 0.012 0.008 0.009 0.085 0.113 0.106
Table 3 : Description of the data sets for crossdomain text classification , including errors given by SVM . “ Di–Do ” means training on Di and testing on Do ; “ Do–CV ” means 10 fold cross validation on Do . The performances are in test error rate . archical structure . We generated three data sets orgs vs people , orgs vs places and people vs places for crossdomain classification in a similar way as what we have done on the 20 Newsgroups and SRAA corpora . Since there are too many sub categories , we can not list the detailed description here .
614 Properties of the Data Sets
Table 3 shows the description of all the data sets . The first three columns of the table show the statistical properties of the data sets . The first two data sets are from SRAA corpus . The next six are generated using 20 Newsgroups data set . The last three are from Reuters 21578 test collection . KLdivergence values calculated by D(Di||Do ) on all the data set are presented in the second column in the table , sorted in decreasing order from top down . It can be seen that the KL divergence values for all the data sets are much larger than the identical distribution case which has a KL value of nearly zero . The next column titled “ Documents ” shows the size of the data sets and the vocabulary set used . Under the column titled “ SVM ” , we show two groups of classification results in two sub columns . First , “ Di–Do ” denotes the test error rate obtained when a classifier is trained based on the in domain data set Di and applied to the out of domain data set Do . The column titled “ Do–CV ” denotes the best case obtained by the corresponding classifier , where the best case is to conduct a 10 fold cross validation on the out of domain data set Do using that classifier . Note in obtaining the best case for each classifier , the training part is labeled data from Do and the test part is also from Do , according to different folds , which gives the best possibly result for that classifier . It can be found that the test error rates , given by SVM , in the case of “ Di–Do ” is much worse than those in the case of “ Do–CV ” . This indicates that our data sets are not suitable for traditional supervised classification algorithms .
Figure 2 shows the document word co occurrence distriIn this figure , bution on the auto vs aviation data set . documents 1 to 8000 are from Di , while documents 8001 to 16000 are from Do . The documents are ordered first by their domains ( Di or Do ) , and second by their categories ( positive or negative ) . The words are sorted by n+(w)/n−(w ) , where n+(w ) and n−(w ) represent the number of word positions w appears in positive and negative documents , respectively . From Figure 2 , it can be found that the distributions of in domain and out of domain data are somewhat different , however the figure also shows large commonness exists between the two domains . In our algorithm , the class infor
Research Track Paper215 The initialization of CoCC is important , since different initialization will lead to different local optimal co clustering . In the experiments , we assign the initial document clustering by NBC . NBC is trained using Di , and then predicts the labels of Do . Then , the documents in Do are assigned to the clusters based on their prediction labels . The initial word clustering is derived by CLUTO [ 17 ] with default parameters . Another important issue to be mentioned is that , in order to avoid infinity values for D(f ( W|d)|| ˆf ( W| ˆd ) ) in Equation ( 28 ) , and D(f ( Do|w)|| ˆf ( Do| ˆw ) ) and D(g(C|w)||ˆg(C| ˆw ) ) in Equation ( 29 ) , Laplacian smoothing [ 22 ] is applied to estimate the probabilities .
Finally , after co clustering , we assign each document d to the class c by c = arg min c∈C D(ˆg(W|c)|| ˆf ( W| ˆd) ) .
( 31 )
Equation ( 31 ) indicates that we always assign the document d to the class c which is most relevant to ˆd . Note that , our objective function in Equation ( 13 ) ensures that C and ˆDo are highly dependent , and hence the assignment makes sense . 6.4 Evaluation Metrics
The performance of the proposed methods was evaluated by test error rate . Let C be the function which maps from document d to its true class label c = C(d ) , and F be the function which maps from document d to its prediction label c = F ( d ) given by the classifiers . Test error rate is defined as
=
|Do| 6.5 Experimental Results
|{d|d ∈ Do ∧ C(d ) = F ( d)}|
.
( 32 )
651 Performance
Table 4 presents the performance on each data set given by NBC , SVM , TSVM , SGT and our algorithm CoCC in test error rate . The implementation details of the algorithms have already been presented in the last subsection , and the parameter setting for CoCC will be given later .
From the table , we can see that CoCC always give the best performances . Besides , it seems that NBC is better for classifying out of domain documents than SVM , although SVM is known as a stronger classifier than NBC . In our opinion , SVM is a relatively strong classifier for traditional classification problem , compared with NBC , but NBC is more general for out of domain data . TSVM and SGT give better performance than NBC and SVM on most data sets , since they utilize the information given by the unlabeled data in Do , although their basic assumption is violated . However , they still fail sometimes , eg TSVM on the orgs vs places data set , and SGT on the orgs vs people data set .
Figure 3 presents the test error rates on different sizes of the labeled in domain data . The labeled data are randomly chosen from Di in the auto vs aviation data set by different proportion . It can be seen that CoCC gives comparable performance even when there is only 10 % of the in domain data , while NBC gets quickly worse when the proportion of in domain data is less than 20 % .
Figure 2 : Document word co occurrence distribution on the auto vs aviation data set mation and knowledge passes through these common information from the in domain to the out of domain . Moreover , the word clustering part in the co clustering can even enrich the common part to further propagate knowledge between different domains . 6.2 Comparison Methods
Since our co clustering based classification algorithm ( CoCC ) is a classification algorithm essentially , we should compare CoCC with the existing classification methods to show the advantages of our algorithm . We take the supervised classification algorithms to be the baseline methods . Naive Bayes Classifier ( NBC ) [ 20 ] and Support Vector Machines ( SVM ) [ 3 ] are introduced in the experiments . Transductive Support Vector Machines ( TSVM ) [ 15 ] and Spectral Graph Transducer ( SGT ) are also introduced as comparison semisupervised learning methods . 6.3
Implementation Details
Data preprocessing has been applied to the raw data . First , we converted all the letters in the text to lower case , and stemmed the words using the Porter stemmer [ 24 ] . Besides , stop words were removed . We used a simple feature selection method , Document Frequency ( DF ) Thresholding [ 26 ] , to cut down the number of features , and speed up the classification . Based on [ 26 ] , DF thresholding , which has comparable performance with Information Gain ( IG ) or CHI , is suggested since it is simplest with lowest cost in computation . In our experiments , we set the DF threshold to 3 .
TF IDF is used for feature weighting when training Support Vector Machines ( SVM ) [ 3 , 15 ] and Spectral Graph Transducer ( SGT ) [ 16 ] . TF is used for feature weighting when training Naive Bayes Classifier ( NBC ) [ 20 ] and our co clustering based classification ( CoCC ) algorithm .
SVM and TSVM are implemented by SVMlight [ 14 ] with default parameters ( linear kernel ) . For more details about SVM and TSVM , please refer to [ 3 ] and [ 15 ] . SGT is implemented by SGTlight [ 13 ] with default parameters ( k = 50 , d = 80 and c = 100 ) . For more details about SGT , please refer to [ 16 ] .
Research Track Paper216 Data Set real vs simulated auto vs aviation rec vs talk rec vs sci comp vs talk comp vs sci comp vs rec sci vs talk orgs vs places people vs places orgs vs people
NBC SVM TSVM SGT CoCC 0.120 0.259 0.068 0.150 0.035 0.235 0.055 0.165 0.020 0.024 0.130 0.207 0.042 0.072 0.054 0.226 0.320 0.377 0.174 0.216 0.236 0.289
0.266 0.228 0.233 0.212 0.103 0.317 0.165 0.226 0.454 0.266 0.297
0.130 0.087 0.091 0.062 0.028 0.279 0.047 0.083 0.385 0.192 0.306
0.130 0.102 0.040 0.062 0.097 0.183 0.098 0.108 0.436 0.231 0.297
Table 4 : Test error rate for each classifier on each data set
0.30
0.25
0.20
0.15
0.10 e t a R r o r r
E t s e T
CoCC NBC
0.05
1 %
2 %
4 %
8 %
Data Size
16 %
32 %
64 %
Figure 3 : Test error rate curve on different size of auto vs aviation data set
652 Convergence
Since our algorithm CoCC is an iterative algorithm , an important issue for CoCC is the convergence property . Theorem 4 has already proven the convergence of CoCC theoretically . Now , let us empirically show the convergence property of CoCC . Figure 4 shows the test error rate curves as functions for each iteration on three data sets , real vs simulated , rec vs sci and orgs vs places . From the figure , it can be seen that CoCC always achieves almost convergence points within 5 iterations . This indicates that CoCC converges very fast . We believe that 10 iterations is enough for CoCC .
653 Parameters Tuning
There are two parameters in our algorithm . One is the trade off parameter λ in Equation ( 7 ) ; the other is the num e t a R r o r r
E t s e T
0.40
0.35
0.30
0.25
0.20
0.15
0.10
0.05
0 real vs simulated rec vs sci orgs vs places
5
10
15
Number of Iterations
Figure 4 : Test error rate curves after each iteration
16 word clusters 64 word clusters 128 word clusters
0.090
0.085
0.080
0.075
0.070 e t a R r o r r
E t s e T
0.065
0.03125 0.0625
0.125
0.25
0.5
λ
1
2
4
8
Figure 5 : Test error rate curve on different λ e t a R r o r r
E t s e T
0.12
0.11
0.10
0.09
0.08
0.07
0.06
2
λ = 1 λ = 0.25 λ = 0.125
4
8
16
32
64
128
256
512
Number of Word Clusters
Figure 6 : Test error rate curve on different number of word clusters ber of word clusters . We perform the parameter tuning on the auto vs aviation data set . When tuning the parameter λ , we tried three different numbers of word clusters – 16 , 64 and 128 . The error rate for each λ from 0.003125 to 8 is given in Figure 5 . According to the figure , we set λ to 0.125 in our experiments . When tuning the number of word clusters , we tried different λs which are 1 , 0.5 and 025 The error rate for each number of word clusters from 2 to 512 is given in Figure 6 . According to the figure , we set the number of word clusters to 128 in our experiments .
654 KL divergence and Improvement We test how the difference in the distribution between Di and Do influence the performance of CoCC . The KLdivergence and relative improvements by test error rate reduction between CoCC and NBC , and between CoCC and SVM are calculated for each data set in Figure 7 . The data sets have been sorted by KL divergence in decreasing order from left to right . In this figure , when the KL divergence is small , the relative improvement is not much significant . The improvement becomes great when KL divergence values become large , in general . But , there are still some exceptional points .
7 . CONCLUSIONS AND FUTURE WORKS In this paper , we presented a novel co clustering based classification algorithm ( CoCC ) to classify out of domain documents . The class structure passes through word clusters from the in domain data to the out of domain data . Additional class label information given by the in domain data is extracted and used for labeling the word clusters for
Research Track Paper217 n o i t c u d e R e t a R r o r r
E
100 %
80 %
60 %
40 %
20 %
0 %
NBC SVM
1
2
3
4
5
6
7
8
9
10
11
Figure 7 : Test error rate reductions against NBC and SVM on all data sets sorted by KL divergence in descending order from left to right out of domain documents . We formulate the problem under an information theoretic scheme , and designed an objective function to minimize the loss in mutual information before and after co clustering based categorization . Our theory shows that CoCC can monotonically reduce the objective function value . The empirically results also support our theoretical analysis . In our experiment , it is shown that CoCC greatly outperforms traditional supervised and semisupervised classification algorithms when classifying out ofdomain documents .
In CoCC , the number of word clusters are quite large ( 128 clusters in the experiments ) to obtain good performance . Since the time complexity of CoCC depends on the number of word clusters , it can inefficient . In the future , we will try to speed up the algorithm to make it more scalable for large data set . Moreover , the parameters in CoCC are tuned manually . In the future , we will investigate automatic approaches to tune the parameters .
8 . REFERENCES [ 1 ] S . Basu , A . Banerjee , and R . J . Mooney .
Semi supervised clustering by seeding . In Proceedings of Ninteenth International Conference on Machine Learning , 2002 .
[ 2 ] A . Blum and T . Mitchell . Combining labeled and unlabeled data with co training . In Proceedings of the Eleventh Annual Conference on Computational Learning Theory , 1998 .
[ 3 ] B . E . Boser , I . Guyon , and V . Vapnik . A training algorithm for optimal margin classifiers . In Proceedings of the Fifth Annual Workshop on Computational Learning Theory , 1992 .
[ 4 ] R . Caruana . Multitask learning . Machine Learning ,
28(1):41–75 , 1997 .
[ 5 ] D . Cohn , R . Caruana , and A . McCallum .
Semi supervised clustering with user feedback . Technical Report TR2003 1892 , Cornell University , 2003 .
[ 6 ] T . M . Cover and J . A . Thomas . Elements of information theory . Wiley Interscience , 1991 .
[ 7 ] H . Daum´e III and D . Marcu . Domain adaptation for statistical classifiers . Journal of Artificial Intelligence Research , 26:101–126 , 2006 .
[ 8 ] I . S . Dhillon , S . Mallela , and R . Kumar . Enhanced word clustering for hierarchical text classification . In
Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2002 .
[ 9 ] I . S . Dhillon , S . Mallela , and D . S . Modha .
Information theoretic co clustering . In Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2003 .
[ 10 ] J . Gao , P N Tan , and H . Cheng . Semi supervised clustering with partial background information . In Proceedings of the Sixth SIAM International Conference on Data Mining , 2006 .
[ 11 ] N . Grira , M . Crucianu , and N . Boujemaa .
Unsupervised and semi supervised clustering : a brief survey , 2005 . In A Review of Machine Learning Techniques for Processing Multimedia Content , Report of the MUSCLE Eurepean Network of Excellence ( 6th Framework Programme ) .
[ 12 ] X . Ji , W . Xu , and S . Zhu . Document clustering with prior knowledge . In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval , 2006 .
[ 13 ] T . Joachims . SGTlight . http://sgtjoachimsorg/ [ 14 ] T . Joachims . SVMlight . http://svmlightjoachimsorg/
[ 15 ] T . Joachims . Transductive inference for text classification using support vector machines . In Proceedings of Sixteenth International Conference on Machine Learning , 1999 .
[ 16 ] T . Joachims . Transductive learning via spectral graph partitioning . In Proceedings of Twentieth International Conference on Machine Learning , 2003 .
[ 17 ] G . Karypis . Cluto – software for clustering high dimensional datasets . http://glarosdtcumnedu/gkhome/views/cluto [ 18 ] K . Lang . Newsweeder : Learning to filter netnews . In Proceedings of the Twelfth International Conference on Machine Learning , 1995 .
[ 19 ] D . D . Lewis . Reuters 21578 test collection . http://wwwdaviddlewiscom/
[ 20 ] D . D . Lewis . Representation and learning in information retrieval . PhD thesis , Amherst , MA , USA , 1992 .
[ 21 ] A . K . McCallum . Simulated/real/aviation/auto usenet data . http://wwwcsumassedu/~mccallum/code datahtml
[ 22 ] T . M . Mitchell . Machine Learning , chapter 6 , page
179 . McGraw Hill , 1997 .
[ 23 ] K . Nigam , A . K . McCallum , S . Thrun , and
T . Mitchell . Text classification from labeled and unlabeled documents using em . Machine Learning , 39(2 3):103–134 , 2000 .
[ 24 ] M . F . Porter . An algorithm for suffix stripping .
Program , 14(3):130–137 , 1980 .
[ 25 ] S . Swarup and S . R . Ray . Cross domain knowledge transfer using structured representations . In Proceedings of the Twenty First National Conference on Artificial Intelligence , 2006 .
[ 26 ] Y . Yang and J . O . Pedersen . A comparative study on feature selection in text categorization . In Proceedings of Fourteenth International Conference on Machine Learning , 1997 .
Research Track Paper218 [ 27 ] H J Zeng , X H Wang , Z . Chen , H . Lu , and W Y
Ma . Cbc : Clustering based text classification requiring minimal labeled data . In Proceedings of the third IEEE International Conference on Data Mining , 2003 .
[ 28 ] X . Zhu . Semi supervised learning literature survey .
Technical Report 1530 , University of Wisconsin–Madison , 2006 .
APPENDIX A . PROOF OF THEOREM 4
We split Theorem 4 into two lemmas , Lemma 6 and Lemma 7 . Lemma 6 proves Theorem 4 for t = 1 , 3 , . . . , 2T + 1 . Lemma 7 proves Theorem 4 for t = 2 , 4 , . . . , 2T + 2 . Combine the two lemmas , Theorem 4 is proved .
Lemma 6 . Theorem 4 holds when t = 1 , 3 , . . . , 2T + 1 .
Proof . For t = 1 , 3 , . . . , 2T + 1 , since C
( t)W = C
( t+1)W , we need only to prove D(f ( Do,W)|| ˆf
( t )
( Do,W ) ) ≥ D(f ( Do,W)|| ˆf
( t+1 )
( Do,W) ) .
ˆf ( t)(w| ˆw ) = ˆf ( t+1)(w| ˆw ) and
X
X
( d)|d∈Do}
X
X d∈ ˆd w∈ ˆw
ˆw∈{C(t+1)W ( w)|w∈W} f ( d)f ( w|d ) log
1
ˆf ( t)( ˆw| ˆd )
( d)|d∈Do}
0 @X
X
ˆw∈{C(t+1)W ( w)|w∈W}
1 A log f ( d)f ( w|d ) d∈ ˆd w∈ ˆw
( d)|d∈Do}
( t+1 ) ˆf
( ˆd )
( t+1 ) ˆf
( ˆw| ˆd ) log
1
ˆf ( t)( ˆw| ˆd )
( 39 )
1
ˆf ( t)( ˆw| ˆd )
( 40 )
X ˆd∈{C(t+1)Do X ˆd∈{C(t+1)Do
X X ˆd∈{C(t+1)Do X X ˆd∈{C(t+1)Do
=
=
≥
ˆw∈{C(t+1)W ( w)|w∈W} ( t+1 ) ˆf
( ˆd )
( d)|d∈Do}
( t+1 ) ˆf
( ˆw| ˆd ) log
1
ˆf ( t+1)( ˆw| ˆd )
( 41 )
ˆw∈{C(t+1)W ( w)|w∈W}
Equation ( 41 ) follows by the non negativity of the KullbackLeibler divergence . Note that −
X
( ˆw| ˆd ) log
( ˆw| ˆd ) log
( t+1 ) ˆf
( t+1 ) ˆf
1
1
( t+1 ) ˆf
( ˆw| ˆd ) log
= D( ˆf
( t+1 )
ˆf ( t)( ˆw| ˆd )
ˆw
ˆf ( t+1)( ˆw| ˆd ) ˆf ( t)( ˆw| ˆd )
X X
ˆw
=
ˆw
ˆf ( t+1)( ˆw| ˆd ) ( ˆW| ˆd ) )
( t )
( ˆW| ˆd)|| ˆf
( 42 )
Lemma 7 . Theorem 4 holds when t = 2 , 4 , . . . , 2T + 2 . Proof . For t = 2 , 4 , . . . , 2T + 2 , X D(f ( Do,W)|| ˆf =
X ( Do,W ) ) + λ · D(g(C,W)||ˆg
( C,W ) )
( t )
( t )
( 35 )
( 36 )
( 37 )
( 38 )
≥
0 B@f ( w )
ˆw∈{C(t)W ( w)|w∈W} w∈ ˆw
X
X X ˆd∈{C(t)Do g(c|w ) log +λ · g(w ) X X
( d)|d∈Do} c∈C d∈ ˆd
ˆw∈{C(t)W ( w)|w∈W} w∈ ˆw
0 B@f ( w )
X X ˆd∈{C(t)Do +λ · g(w ) c∈C
( d)|d∈Do} f ( d|w ) f ( d|w ) log ˆf ( t)(d| ˆw ) ! g(c|w ) ˆg(t)(c| ˆw )
( 43 )
X d∈ ˆd f ( d|w ) log f ( d|w ) ˆf ( t)(d|C !
( t+1)W ( w ) )
( 44 ) g(c|w ) log g(c|w )
ˆg(t)(c|C
( t+1)W ( w ) )
Equation ( 44 ) is based on Equation ( 29 ) . Using the same argument as Lemma 6 , we can prove this lemma .
( t )
=
≥ d∈ ˆd d∈ ˆd f ( d ) w∈ ˆw w∈ ˆw
( d)|d∈Do}
( d)|d∈Do}
ˆw∈{C(t)W ( w)|w∈W}
ˆw∈{C(t)W ( w)|w∈W}
X ( Do,W ) ) f ( d ) X X X X X
X D(f ( Do,W)|| ˆf = X ˆd∈{C(t)Do X X ˆd∈{C(t)Do X X ˆd∈{C(t+1)Do X X X ( d)|d∈Do} ˆd∈{C(t+1)Do f ( d)f ( w|d ) log X X X ˆd∈{C(t+1)Do
ˆw∈{C(t+1)W ( w)|w∈W}
( d)|d∈Do} f ( d)f ( w|d ) log
( d)|d∈Do} w∈ ˆw w∈ ˆw d∈ ˆd d∈ ˆd
≥
= d∈ ˆd w∈ ˆw f ( w|d ) ˆf ( t)(w| ˆd )
( 33 ) f ( w|d ) ( t+1)Do
ˆf ( t)(w|C
( 34 )
( d ) ) f ( w|d ) log f ( w|d ) log f ( d ) f ( w|d ) ˆf ( t)(w| ˆd ) f ( w|d ) log X
ˆw∈{C(t+1)W ( w)|w∈W} f ( w|d )
X ˆf ( t)( ˆw| ˆd ) ˆf ( t)(w| ˆw )
ˆw∈{C(t+1)W ( w)|w∈W} f ( w|d )
ˆf ( t+1)( ˆw| ˆd ) ˆf ( t+1)(w| ˆw )
= D(f ( Do,W)|| ˆf
( t+1 )
( Do,W ) )
Note : Equation ( 33 ) is based on Lemma 2 ; Equation ( 34 ) is based on Equation ( 28 ) ; Equation ( 35 ) follows by re(t+1)W ; Equation ( 36 ) folarranging the sum and C lows by rearranging the sum and total probability theorem ( f ( w| ˆw ) = 0 only for w ∈ ˆw ) ; Equation ( 37 ) is due to
( t)W = C
Research Track Paper219
