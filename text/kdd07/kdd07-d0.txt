A Concept based Model for Enhancing Text Categorization
Shady Shehata
Fakhri Karray
Pattern Analysis and Machine Intelligence ( PAMI ) Research
Pattern Analysis and Machine Intelligence ( PAMI ) Research
Group
Electrical and Computer Engineering Department University of Waterloo
Group
Electrical and Computer Engineering Department University of Waterloo
Mohamed Kamel
Pattern Analysis and Machine Intelligence ( PAMI ) Research
Group
Electrical and Computer Engineering Department University of Waterloo shady@pamiuwaterlooca karray@pamiuwaterlooca mkamel@pamiuwaterlooca
ABSTRACT Most of text categorization techniques are based on word and/or phrase analysis of the text . Statistical analysis of a term frequency captures the importance of the term within a document only . However , two terms can have the same frequency in their documents , but one term contributes more to the meaning of its sentences than the other term . Thus , the underlying model should indicate terms that capture the semantics of text . In this case , the model can capture terms that present the concepts of the sentence , which leads to discover the topic of the document .
A new concept based model that analyzes terms on the sentence and document levels rather than the traditional analysis of document only is introduced . The concept based model can effectively discriminate between non important terms with respect to sentence semantics and terms which hold the concepts that represent the sentence meaning .
The proposed model consists of concept based statistical analyzer , conceptual ontological graph representation , and concept extractor . The term which contributes to the sentence semantics is assigned two different weights by the concept based statistical analyzer and the conceptual ontological graph representation . These two weights are combined into a new weight . The concepts that have maximum combined weights are selected by the concept extractor .
A set of experiments using the proposed concept based model on different datasets in text categorization is conducted . The experiments demonstrate the comparison between traditional weighting and the concept based weighting obtained by the combined approach of the concept based statistical analyzer and the conceptual ontological graph .
The evaluation of results is relied on two quality measures , the Macro averaged F1 and the Error rate . These quality measures are improved when the newly developed concept based model is used to enhance the quality of the text categorization .
Categories and Subject Descriptors I27 [ Artificial Intelligence ] : Natural Language Processing—Language Parsing and Understanding
General Terms Theory , Design , Algorithms , Experimentation
Keywords Concepts , concept based categorization
1 .
INTRODUCTION
Natural Language Processing ( NLP ) is both a modern computational technology and a method of investigating and evaluating claims about human language itself . NLP is a term that links back into the history of Artificial Intelligence ( AI ) , the general study of cognitive function by computational processes , with an emphasis on the role of knowledge representations . The need for representations of human knowledge of the world is required in order to understand human language with computers .
Text mining attempts to discover new , previously unknown information by applying techniques from natural language processing and data mining . Categorization , one of the traditional text mining techniques , is supervised learning paradigm where categorization methods try to assign a document to one or more categories , based on the document content . Classifiers are trained from examples to conduct the category assignment automatically . To facilitate effective and efficient learning , each category is treated as a binary classification problem . The issue here is whether or not a document should be assigned to a particular category or not .
Most of current document categorization methods are based on the vector space model ( VSM ) [ 1 , 15 , 14 ] , which is a widely used data representation . The VSM represents each document as a feature vector of the terms ( words or phrases ) in the document . Each feature vector contains term weights ( usually term frequencies ) of the terms in the document . The similarity between documents is measured by one of several similarity measures that are based on such a feature vector . Examples include the cosine measure and the Jaccard measure .
Usually , in text categorization techniques , the frequency of a term ( word of phrase ) is computed to explore the importance of the term in the document . However , two terms
629Research Track Paper can have the same frequency in a document , but one term contributes more to the meaning of its sentences than the other term . Thus , some terms provide the key concepts in a sentence , and indicate what a sentence is about . It is important to note that extracting the relations between verbs and their arguments in the same sentence has the potential for analyzing terms within a sentence . The information about who is doing what to whom clarifies the contribution of each term in a sentence to the meaning of the main topic of that sentence .
In this paper , a novel concept based model is proposed . In the proposed model , each sentence is labeled by a semantic role labeler that determines the terms which contribute to the sentence semantics associated with their semantic roles in a sentence . Each term that has a semantic role in the sentence , is called concept . Concepts can be either a word or phrase and it is totally dependent on the semantic structure of the sentence . The concept based model analyzes each term within a sentence and a document using the following three components . The first component is the conceptbased statistical analyzer that analyzes each term on the sentence and the document levels . After each sentence is labeled by a semantic role labeler , each term is statistically weighted based on its contribution to the meaning of the sentence . This weight discriminates between non important and important terms with respect to the sentence semantics . The second component is the Conceptual Ontological Graph ( COG ) representation which is based on the conceptual graph theory and utilizes graph properties . The COG representation captures the semantic structure of each term within a sentence and a document , rather than the term frequency within a document only . After each sentence is labeled by a semantic role labeler , all the labeled terms are placed in the COG representation according to their contribution to the meaning of the sentence . Some terms could provide shallow concepts about the meaning of a sentence , but , other terms could provide key concepts that hold the actual meaning of a sentence . Each concept in the COG representation is weighted based on its position in the representation . Thus , the COG representation is used to provide a definite separation among concepts that contribute to the meaning of a sentence . Therefore , the COG representation presents concepts into a hierarchical manner . The key concepts are captured and weighted based on their positions in the COG representation .
At this point , concepts are assigned two different weights using two different techniques which are the concept based statistical analyzer and the COG representation . It is important to note that both of them achieve the same functionality , in different ways . The output of the two techniques are the important weighted concepts with respect to the sentence semantics that each technique captures . However , the weighted concepts that are computed by each technique could not be exactly the same . The important concepts to the concept based statistical analyzer could be nonimportant to the COG representation and vice versa . Therefore , the third component , which is the concept extractor , combines the two different weights computed by the conceptbased statistical analyzer and the COG representation to denote the important concepts with respect to the two techniques . The extracted top concepts are used to build standard normalized feature vectors using the standard vector space model ( VSM ) for the purpose of text categorization .
Weighting based on the matching of concepts in each document , is showed to have a more significant effect on the quality of the text categorization due to the similarity ’s insensitivity to noisy terms that can lead to an incorrect similarity measure . The concepts are less sensitive to noise when it comes to calculating normalized feature vectors . This is due to the fact that these concepts are originally extracted by the semantic role labeler and analyzed using two different techniques with respect to the sentence and document levels . Thus , the matching among these concepts is less likely to be found in non relevant documents to a category . The results produced by the proposed concept based model in text categorization have higher quality than those produced by traditional techniques .
The explanations of the important terms , which are used in this paper , are listed as follows : Verb argument structure : ( e.g John hits the ball ) . ” hits ” is the verb . ” John ” and ” the ball ” are the arguments of the verb ” hits ” , Label : A label is assigned to an argument . e.g : ” John ” has subject ( or Agent ) label . ” the ball ” has object ( or theme ) label , Term : is either an argument or a verb . Term is also either a word or a phrase ( which is a sequence of words ) , Concept : in the new proposed model , concept is a labeled term .
The rest of this paper is organized as follows . Section 2 presents the thematic roles background . Section 3 introduces the concept based model . The experimental results are presented in section 4 . The last section summarizes and suggests future work .
2 . THEMATIC ROLES
Generally , the semantic structure of a sentence can be characterized by a form of verb argument structure . This underlying structure allows the creation of a composite meaning representation from the meanings of the individual concepts in a sentence . The verb argument structure permits a link between the arguments in the surface structures of the input text and their associated semantic roles .
Consider the following example : My daughter wants a doll . This example has the following syntactic argument frames : ( Noun Phrase ( NP ) wants NP ) . In this case , some facts could be driven for the particular verb ” wants ” ( 1 ) There are two arguments to this verb ( 2 ) Both arguments are NPs ( 3 ) The first argument ” my daughter ” is pre verbal and plays the role of the subject ( 4 ) the second argument ” a doll ” is a post verbal and plays the role of the direct object . The study of the roles associated with verbs is referred to a thematic role or case role analysis [ 8 ] . Thematic roles , first proposed by Gruber and Fillmore [ 4 ] , are sets of categories that provide a shallow semantic language to characterize the verb arguments .
Recently , there have been many attempts to label thematic roles in a sentence automatically . Gildea and Jurafsky [ 6 ] were the first to apply a statistical learning technique to the FrameNet database . They presented a discriminative model for determining the most probable role for a constituent , given the frame , predicator , and other features . These probabilities , trained on the FrameNet database , depend on the verb , the head words of the constituents , the voice of the verb ( active , passive ) , the syntactic category ( S , NP , VP , PP , and so on ) and the grammatical function
630Research Track Paper ( subject and object ) of the constituent to be labeled . The authors tested their model on a pre release version of the FrameNet I corpus with approximately 50,000 sentences and 67 frame types . Gildea and Jurafsky ’s model was trained by first using Collins’ parser [ 2 ] , and then deriving its features from the parsing , the original sentence , and the correct FrameNet annotation of that sentence .
A machine learning algorithm for shallow semantic parsing was proposed in [ 13][12][11 ] . It is an extension of the work in [ 6 ] . Their algorithm is based on using Support Vector Machines ( SVM ) which results in improved performance over that of earlier classifiers by [ 6 ] . Shallow semantic parsing is formulated as a multi class categorization problem . SVMs are used to identify the arguments of a given verb in a sentence and classify them by the semantic roles that they play such as AGENT , THEME , GOAL .
3 . CONCEPT BASED MODEL
A raw text document is the input to the proposed model . Each document has well defined sentence boundaries . Each sentence in the document is labeled automatically based on the PropBank notations [ 9 ] . After running the semantic role labeler[9 ] , each sentence in the document might have one or more labeled verb argument structures . The number of generated labeled verb argument structures is entirely dependent on the amount of information in the sentence . The sentence that has many labeled verb argument structures includes many verbs associated with their arguments . The labeled verb argument structures , the output of the role labeling task , are captured and analyzed by the concept based model on the sentence and document levels .
In this model , both the verb and the argument are considered as terms . One term can be an argument to more than one verb in the same sentence . This means that this term can have more than one semantic role in the same sentence . In such cases , this term plays important semantic roles that contribute to the meaning of the sentence . In the concept based model , a labeled term either word or phrase is considered as concept .
The proposed concept based model consists of conceptbased statistical analyzer , COG representation , and concept extractor . The aim of the concept based statistical analyzer is to weight each term on the sentence and the document levels rather than the traditional analysis of document only . The conceptual ontological graph representation presents the sentence structure while maintaining the sentence semantics in the original document . Each concept in the COG representation is weighted based on its position in the representation . The concept extractor combines the two different weights computed by the concept based statistical analyzer and the COG representation to select the maximum weighted concepts .
The proposed model assigns two new weights to each concept in a sentence . The newly proposed weightstat and weightCOG are computed by the concept based statistical analyzer and the COG representation respectively .
The proposed model combines the work discussed in [ 16 ] and [ 17 ] into one concept based model . This combination is achieved in the proposed model by presenting the work in [ 16 ] as the concept based statistical analyzer component and the work in [ 17 ] as the COG representation component . The work discussed in [ 16 ] enhances text clustering quality using statistical method . The proposed model utilizes the
Figure 1 : Concept based Model work of [ 16 ] to achieve statistical concept based weighting , which is the weightstat , on the sentence and the document levels and applies it to text categorization . This weighting discriminates between non important concepts and the key concepts with respect to the sentence meaning .
The work discussed in [ 17 ] enhances text retrieval quality by extracting concepts from the COG representation based on their positions only . The concept based model proposes new weight to each position in the COG representation to achieve more accurate analysis with respect to the sentence semantics . Thus , each concept in the COG representation is assigned a proposed weight , which is weightCOG , based on its position in the representation .
The proposed model takes advantage of the two techniques discussed in [ 16 , 17 ] by first assigning two proposed weights to each concept in a sentence . Secondly , the proposed model combines the proposed weights into a new combined weight and extracts the top concepts which have the maximum combined weights by using the concept extractor . Lastly , the combined weights are used to build normalized feature vectors for text categorization purposes as depicted in Fig ( 1 ) . 3.1 Concept based Statistical Analyzer
The objective of this task is to achieve a concept based statistical term analysis ( word or phrase ) on the sentence and document levels rather than a single term analysis in the document set only .
To analyze each concept at the sentence level , a conceptbased frequency measure , called the conceptual term frequency ( ctf ) is utilized . The ctf is the number of occurrences of concept c in verb argument structures of sentence s . The concept c , which frequently appears in different verb argument structures of the same sentence s , has the principal role of contributing to the meaning of s .
Language IndependentTextTextLanguageDependentNatural Language Processing Semantic Role LabelerSyntax ParserPOS TaggerLanguageDependentNatural Language Processing Semantic Role LabelerSyntax ParserPOS TaggerConcept based ModelSentence SeparatorConcept based Statistical Analyzer(tf : term frequency)(ctf : conceptual term frequency)Conceptual Ontological Graph ( COG ) RepresentationText Pre processorText Pre processorConcept ExtractorConceptsConceptsTop ConceptsTop ConceptsConceptsConceptsDoc ConceptMatrixSVMNBRocchio631Research Track Paper To analyze each concept at the document level , the term frequency tf , the number of occurrences of a concept ( word or phrase ) c in the original document , is calculated .
The concept based weighting is one of the main factors that captures the importance of a concept in a sentence and a document . Thus , the concepts which have highest weights are captured and extracted .
The following is the concept based weighting weightstat which is used to discriminate between non important terms with respect to sentence semantics and terms which hold the concepts that present the meaning of the sentence . weightstati = tf weighti + ctf weighti
( 1 )
In calculating the value of weightstati in equation ( 1 ) , the tf weighti value presents the weight of concept i in document d at the document level and the ctf weighti value presents the weight of the concept i in the document d at the sentence level based on the contribution of concept i to the semantics of the sentences in d . The sum between the two values of tf weighti and ctf weighti presents an accurate measure of the contribution of each concept to the meaning of the sentences and to the topics mentioned in a document . In equation ( 2 ) , the tfij value is normalized by the length of the document vector of the term frequency tfij in the document d , where j = 1 , 2 , , cn tf weighti =
,
( 2 ) where cn is the total number of the concepts which has a term frequency value in the document d .
In equation ( 3 ) , the ctfij value is normalized by the length of the document vector of the conceptual term frequency ctfij in the document d where j = 1 , 2 , , cn cn tfij j=1 ( tfij)2 cn ctfij j=1 ( ctfij)2 ctf weighti =
,
( 3 ) where cn is the total number of concepts which has a conceptual term frequency value in the document d . 3.2 Conceptual Ontological Graph ( COG )
The COG representation is a conceptual graph G = ( C , R ) where the concepts of the sentence , are represented as vertices ( C ) . The relations among the concepts such as agents , objects , and actions are represented as ( R ) . C is a set of nodes {c1 , c2 , , cn} , where each node c represents a concept in the sentence or a nested conceptual graph G ; and R is a set of edges {r1 , r2 , , rm} , such that each edge r is the relation between an ordered pair of nodes ( ci , cj ) .
The output of the role labeling task , which are verbs and their arguments are presented as concepts with relations in the COG representation . This allows the use of more informative concept matching at the sentence level and the document level rather than individual word matching .
The COG representation provides different nested levels of concepts in a hierarchical manner . These levels are constructed based on the importance of the concepts in a sentence , which makes use of analyzing the key concepts in the sentence . The hierarchal representation of the COG , provides a definite separation among concepts which contribute to the meaning of the sentence . This separation is needed to distinguish between shallow concepts and the key concepts in a sentence . The concepts ( labeled terms ) are placed in the COG representation according to the amount of overlapping between these terms with respect to the words .
To present the levels of the COG hierarchy , five types of verb argument structures are utilized and assigned to their corresponding conceptual graphs :
• One : There is only one generated verb argument struc ture .
• Main : There is more than one generated verb argument structure , and the main structure has the maximum number of terms ( arguments ) that refer to other terms in the rest of the verb argument structures .
• Container : There is more than one generated verb argument structure , and the container structure refers to the other arguments , and , at the same time , the container structure does not have the maximum number of referent terms .
• Referenced : The referenced structure has terms ( verb or arguments ) , referred by terms in either the main or the container structure .
• Unreferenced : The terms in the unreferenced structure are not referred by any other terms .
This scheme creates a conceptual graph for each verb argument structure . Each type of verb argument structure is assigned to its corresponding conceptual graph . The COG presents the conceptual graphs as levels , which are determined according to their types .
A new measure LCOG is proposed to rank concepts with respect to the sentence semantics in the COG representation . The proposed LCOG measure is assigned to One , Unreferenced , Main , Container , and Referenced levels in the COG representation with values 1,2,3,4 , and 5 respectively . Instead of selecting concept from only one level in the COG representation , concepts in the entire levels of the COG representation are considered and weighted .
The proposed weightCOG is assigned to each concept pre sented in the COG representation and is calculated by : weightCOGi = tf weighti ∗ LCOGi
( 4 )
In equation(4 ) , the tf weighti value presents the weight of concept i in document d at the document level as shown in equation(2 ) . The LCOGi value presents the importance of the concept i in the document d at the sentence level based on the contribution of concept i to the semantics of the sentences represented by the levels of the COG representation . The multiplication between the two values of tf weighti and LCOGi ranks the concepts in document d with respect to the contribution of each concept to the meaning of the sentences and to the topics mentioned in a document .
For implementation and performance purposes , it is imperative to note that the COG representation maintains the identification number of each concept and each relation
632Research Track Paper node , rather than , the values of the nodes . There is a hash table that includes the unique terms that appeared in each verb argument structure . Thus , the COG is a hierarchy of the identification numbers of the terms that appear in each verb argument structure in the sentence . This is the source of the efficiency of the representation . It is also important to note that ontology languages can not be able to capture the semantic based nested conceptual graphs of the hierarchical structure of the sentence semantics .
The details of constructing of the COG representation are discussed in [ 17 ] . 3.3 Concept Extractor
The process of selecting the top concepts from the concepts extracted by the concept based statistical analyzer and the COG representation is attained by the proposed Concept Extractor Algorithm . 331 Concept based Extractor Algorithm —————————————————————– create a COG for s as in [ 17 ] ci is a new concept in s for each concept ci in s do end for
1 . ddoci is a new Document 2 . L is an empty List ( L is a top concept list ) 3 . for each labeled sentence s in d do 4 . 5 . 6 . 7 . 8 . 9 . 10 . 11 . 12 . 13 . 14 . end for 15 . sort L descendingly based on weightcomb 16 . output the max(weightcomb ) from list L ————————————————————————– compute tfi of ci in d compute ctfi of ci in s in d compute weightstati of concept ci compute weightCOGi of concept ci based on LCOGi compute weightcombi = weightstati ∗ weightCOGi add concept ci to L
The concept extractor algorithm describes the process of combining the weightstat ( computed by the concept based statistical analyzer ) and the weightCOG ( computed by the COG representation ) into one new combined weight called weightcomb . The concept extractor selects the top concepts that have the maximum weightcomb value .
The proposed weightcomb is calculated by : weightcombi = weightstati ∗ weightCOGi
( 5 )
The procedure begins with processing a new document ( at line 1 ) which has well defined sentence boundaries . Each sentence is semantically labeled according to [ 9 ] .
For each labeled sentence ( in the for loop at line 3 ) , concepts of the verb argument structures which represent the semantic structures of the sentence are extracted to construct the COG representation ( at line 4 ) . The same extracted concepts are weighted by the weightstat according to the values of the tf and the ctf ( at lines 7 , 8 , and 9 ) .
During the concept based statistical analysis , each concept in the COG representation is also weighted with different value by the weightCOG ( line 10 ) . The weightstat and the weightCOG are combine into the weightcomb and added to the concepts list L ( at line 11 and 12 ) .
The concepts list L is sorted descendingly based on the weightcomb values . The maximum weighted concepts are chosen as top concepts from the concepts list L . ( at line 15 and 16 )
The concept extractor algorithm is capable of extracting the top concepts in a document ( d ) in O(m ) time , where m is the number of concepts . 3.4 Example of the Concept based Model
Consider the following sentence :
We have noted how some electronic techniques , developed for the defense effort , have eventually been used in commerce and industry .
In this sentence , the semantic role labeler identifies three target words ( verbs ) , marked by bold , which are the verbs that represent the semantic structure of the meaning of the sentence . These verbs are noted , developed , and used . Each one of these verbs has its own arguments as follows :
• [ ARG0 We ] [ TARGET noted ] [ ARG1 how some electronic techniques developed for the defense effort have eventually been used in commerce and industry ]
• We have noted how [ ARG1 some electronic techniques ] [ TARGET developed ] [ ARGM PNC for the defense effort ] have eventually been used in commerce and industry
• We have noted how [ ARG1 some electronic techniques developed for the defense effort ] have [ ARGM TMP eventually ] been [ TARGET used ] [ ARGM LOC in commerce and industry ]
Arguments labels1 are numbered Arg0 , Arg1 , Arg2 , and so on depending on the valency of the verb in sentence . The meaning of each argument label is defined relative to each verb in a lexicon of Frames Files [ 9 ] .
Despite this generality , Arg0 is very consistently assigned an Agent type meaning , while Arg1 has a Patient or Theme meaning almost as consistently [ 9 ] . Thus , this sentence consists of the following three verb argument structures :
1 . First verb argument structure :
• [ ARG0 We ] • [ TARGET noted ] • [ ARG1 how some electronic techniques developed for the defense effort have eventually been used in commerce and industry ]
2 . Second verb argument structure :
• [ ARG1 some electronic techniques ] • [ TARGET developed ] • [ ARGM PNC for the defense effort ]
3 . Third verb argument structure :
1Each set of argument labels and their definitions is called a frameset and provides a unique identifier for the verb sense . Because the meaning of each argument number is defined on a per verb basis , there is no straightforward mapping of meaning between arguments with the same number . For example , arg2 for verb send is the recipient , while for verb comb it is the thing searched for and for verb fill it is the substance filling some container [ 9 ] .
633Research Track Paper • [ ARG1 some electronic techniques developed for the defense effort ]
• [ ARGM TMP eventually ] • [ TARGET used ] • [ ARGM LOC in commerce and industry ]
After each sentence is labeled by a semantic role labeler , a cleaning step is performed to remove stop words that have no significance , and to stem the words using the popular Porter Stemmer algorithm [ 10 ] . The terms generated after this step are called concepts .
In this example , stop words are removed and concepts are shown without stemming for better readability as follows :
1 . Concepts in the first verb argument structure :
• noted • electronic techniques developed defense effort even tually commerce industry
2 . Concepts in the second verb argument structure :
• electronic techniques • developed • defense effort
3 . Concepts in the third verb argument structure :
• electronic techniques developed defense effort • eventually • commerce industry
As mentioned earlier in section ( 1 ) , at this point , concepts are going to be weighted using two different independent techniques which are the concept based statistical analyzer and the COG representation . The outputs of the two techniques denote the important concepts with respect to the sentence semantics that each technique captures . However , the generated concepts from each technique could not be the same . In other words , the important concepts in one technique could be non important to the other technique or the other way around . Thus , the concept extractor combines the weights of both techniques and captures the top concepts that are important with respect to sentence semantics based on the combined weight . 341 Concept based Statistical Analyzer It is imperative to note that these concepts are extracted from the same sentence . Thus , the concepts mentioned in this example sentence are :
• noted • electronic techniques developed defense effort eventu ally commerce industry
• electronic techniques • developed • defense effort • electronic techniques developed defense effort • eventually
Table 1 : Example of Concept based Statistical Analyzer
Row Sentence Number Concepts
( 1 ) ( 2 )
( 3 ) ( 4 ) ( 5 ) ( 6 )
( 7 ) ( 8 )
( 9 ) ( 10 ) ( 11 ) ( 12 ) ( 13 ) ( 14 ) ( 15 ) noted electronic techniques developed defense effort eventually commerce industry electronic techniques developed defense effort electronic techniques developed defense effort eventually commerce industry
Individual Concepts electronic techniques defense effort eventually commerce industry
Conceptual Term Frequency ( CTF ) 1
1
3 3 3 2
2 2 Conceptual Term Frequency ( CTF ) 3 3 3 3 2 2 2
• commerce industry
The traditional analysis methods assign same weight for the words that appear in the same sentence . However , the concept based statistical analyzer discriminates among terms that represent the concepts of the sentence . This discrimination is entirely based on the semantic analysis of the sentence . In this example , some concepts have higher conceptual term frequency ctf than others as shown in Table 1 . In such cases , these concepts ( with high ctf ) contribute to the meaning of the sentence more than concepts ( with low ctf ) . As shown in Table 1 , the concept based statistical ana lyzer computes the ctf measure for :
1 . The concepts which are extracted from the verb argument structures of the sentence , which are in Table 1 from row ( 1 ) to row ( 8 ) .
2 . The concepts which are overlapped with other concepts in the sentence . These concepts are in Table 1 from row ( 3 ) to row ( 8 ) ,
3 . The individual concepts in the sentence , which are in
Table 1 from row ( 9 ) to row ( 15 ) .
In this example , the topic of the sentence is about the electronic techniques . These concepts have the highest ctf value with 3 . In addition , the concept noted which has the lowest ctf , has no significant effect on the topic of the sentence .
It is important to note that concepts such as commerce and industry have the ctf value with 2 which is not the highest . However , these concepts are important to the sentence semantics . They provide significant information about what
634Research Track Paper 3 . ( developed )
• ARG1 [ some electronic techniques ] • ARGM PNC [ for the defense effort ]
In this example , the conceptual graph of the verb noted is the most general graph that has the type main . The other conceptual graphs are referenced graphs . The COG representation is illustrated in ( Fig 2 ) .
After removing the stop words which have no significance , the selected concepts , which provide important information with respect to the sentence semantics , are the concepts appear in the nested conceptual graphs ( as blue color in Fig 2 ) with ” referenced ” type that have the highest LCOG value with 5 . These concepts , which have the ” referenced ” type , are electronic , techniques , developed , defense , effort , commerce , and industry .
It is important to note that in this example , unlike the concept based statistical analysis , the commerce and industry concepts are extracted using the COG representation . However , other examples can demonstrate the other way around in which some important concepts with respect to the sentence semantics are not captured by the COG representation and captured by the concept based statistical analyzer . 343 Concept Extractor The concept extractor combines the weights computed by the concept based statistical analyzer and the COG representation into one combined weight . The weightCOG values for the concepts that have the ” referenced ” type ( the highest LCOG value with 5 ) are combined with the weightstat values into new weightcomb values .
The concepts that have the maximum weightcomb are selected as the top concepts with respect to sentence semantics . In this example , these concepts are electronic , techniques , developed , defense , effort , commerce , and industry .
4 . EXPERIMENTAL RESULTS
To test the effectiveness of using the concepts extracted by the proposed concept based model as an accurate measure to weight terms in the document , a set of experiments of the proposed model in document categorization is conducted .
The experimental setup consisted of three datasets . The first data set contains 23,115 ACM abstract articles collected from the ACM digital library . The ACM articles are classified according to the ACM computing classification system into five main categories : general literature , hardware , computer systems organization , software , and data . The second data set has 12,902 documents from the Reuters 21578 dataset . There are 9,603 documents in the training set , 3,299 documents in the test set , and 8,676 documents are unused . Out of the 5 category sets , the topic category set contains 135 categories , but only 90 categories have at least one document in the training set . These 90 categories were used in the experiment . The third dataset consisted of 361 samples from the Brown corpus [ 5 ] . Each sample has 2000+ words . The Brown corpus main categories used in the experiment were : press : reportage , press : reviews , religion , skills and hobbies , popular lore , belles letters , learned , fiction : science , fiction : romance , and humor .
In the datasets , the text directly is analyzed , rather than , using metadata associated with the text documents . This
Figure 2 : Conceptual Ontological Graph electronic techniques eventually have been used in . Thus , selecting the maximum ctf weight by the concept based statistical analyzer only , leads to lose important concepts such as commerce and industry . 342 Conceptual Ontological Graph ( COG ) Generally , the verb argument structure , which has the maximum number of overlapping words in each term with other verbs , provides the most general concepts of a sentence .
In this example , three conceptual graphs are generated for each verb argument structure as follows :
1 . ( noted )
• ARG0 [ We ] • ARG1 [ how some electronic techniques , developed for the defense effort , have eventually been used in commerce and industry ]
2 . ( used )
• ARG1 [ some electronic techniques developed for the defense effort ]
• ARGM TMP [ eventually ] • ARGM LOC [ in commerce and industry ]
LegendNode Refers to Conceptual GraphConceptual Ontological Graph(COG)Verb NodeArgument NodeConceptual Ontological Graph ( COG ) Representation Wenotedhow some electronic techniques developedfor the defense effort have eventually beenused in commerce and industryARG1some electronic techniquesfor the defense effortARG0developedARGM PNCARG1some electronic techniques developed forthe defense efforteventuallyARGM LOCARG1usedARGM TMPin commerce and industryKeyConceptsConceptual Graph ( type : Main)Conceptual Graph(type : Referenced)(type : Container)KeyConceptsConceptual Graph(type : Referenced)635Research Track Paper Table 2 : Text Classification Improvement using Combined Approach ( weightcomb )
DataSet
Single Term
Concept based
Macro Avg(F1 )
Avg Macro Error Avg(F1 )
Avg Error
Improvement
Reuters
ACM
Brown
SVM NB
Rocchio
SVM NB
Rocchio
SVM NB
Rocchio
0.7421 0.6127 0.6513
0.4973 0.4135 0.4826
0.6143 0.5071 0.5728
0.0871 0.2754 0.1632
0.1782 0.4215 0.2733
0.1134 0.3257 0.2413
0.8953 0.8462 0.8574
0.8263 0.7964 0.7935
0.8753 0.8372 0.8465
0.0121 +20.64 % , 86.10 % 0.0342 +38.11 % , 87.58 % 0.0231 +31.64 % , 85.84 %
0.0532 +66.15 % , 70.14 % 0.0641 +92.59 % , 84.79 % 0.0635 +64.42 % , 76.76 %
0.0211 +42.48 % , 81.39 % 0.0341 +65.09 % , 89.53 % 0.0243 +47.78 % , 89.92 % clearly demonstrates the effect of using concepts on the text categorization process .
For each dataset , stop words are removed from the concepts that are extracted by the proposed model . The extracted concepts are stemmed using the Porter stemmer algorithm [ 10 ] . Concepts are used to build standard normalized feature vectors using the standard vector space model for document representation .
The concept based weights which are calculated by the concept based model are used to compute a document term matrix between documents and concepts . Three standard document categorization techniques are chosen for testing the effect of the concepts on categorization quality : ( 1 ) Support Vector Machine ( SVM ) , ( 2)Rocchio , and ( 3 ) Naive Bayesian ( NB ) . These techniques are used as binary classifiers in which they recognize documents from one specific topic against all other topics . This setup was repeated for every topic .
It is important to note that the concept based weighting is one of the main factors that captures the importance of a concept in a document . Thus , to study the effect of the concept based weighting on the categorization techniques , the entire set of the experiments is achieved by using the proposed concept based weighting weightcomb ( as in equation(5 ) ) for the different categorization techniques .
For the single term weighting , the popular TF IDF [ 3 ] ( Term Frequency/Inverse Document Frequency ) term weighting is adopted . The TF IDF weighting is chosen due to its wide use in the document categorization literature .
In order to evaluate the quality of the text categorization , two widely used evaluation measures in document categorization and retrieval literatures are computed with 5fold cross validation for each classifier . These measures are the Macro averaged performance F1 measure ( the harmonic mean of precision and recall ) and the error rate .
Recall that in binary classification ( relevant/not relevant ) , the following quantities are considered :
• p+ = the number of relevant documents , classified as relevant .
• p− = the number of relevant documents , classified as not relevant .
• n− = the number of not relevant documents , classified as not relevant .
• n+ = the number of not relevant documents , classified as relevant .
Obviously , the total number of documents N is equal to :
N = p+ + n+ + p
−
−
+ n
For the class of relevant documents : p+
P recision(P ) = p+ + n+
Recall(R ) = p+ p+ + p−
The F − measureα is defined as :
Fα =
( 1 + α ) ∗ P ∗ R ( α ∗ P ) + R
The error rate is expressed by :
Error = n+ + p−
N
( 6 )
( 7 )
( 8 )
( 9 )
( 10 )
Generally , the Macro averaged measure is determined by first computing the performance measures per category and then averaging these to compute the global mean .
Basically , the intention is to maximize Macro averaged F1 and minimize the error rate measures to achieve high quality in text categorization .
The results listed in Table(2 ) show the improvement on the categorization quality obtained by the combined approach of the concept based statistical analyzer and the COG representation .
The popular SVMlight implementation [ 7 ] is used with parameter C = 1000 ( tradeoff between training error and margin ) . The result listed in Table(2 ) show that the conceptbased weighting has higher performance than the singleterm weighting .
The percentage of improvement ranges from +20.64 % to +92.59 % increase ( higher is better ) in the Macro averaged F1 and from 70.14 % to 89.92 % drop ( lower is better ) in the
636Research Track Paper error rate as shown in Table(2 ) . It is obvious that the concepts extracted by the concept based model can accurately classify documents into categories .
5 . CONCLUSIONS
This work bridges the gap between natural language processing and text categorization disciplines . A new conceptbased model composed of three components , is proposed to improve the text categorization quality . By exploiting the semantic structure of the sentences in documents , a better text categorization result is achieved . The first component is the concept based statistical analyzer which analyzes the semantic structure of each sentence to capture the sentence concepts using the conceptual term frequency ctf measure . The second component is the conceptual ontological graph ( COG ) . This representation captures the structure of the sentence semantics represented in the COG hierarchical levels . Such a representation allows choosing concepts based n their weights which represent the contribution of each concept to the meaning of the sentence . This leads to perform concept matching and weighting calculations in each document in a very robust and accurate way . The third component is the concept extractor which combines the weights of concepts extracted by the concept based statistical analyzer and the conceptual ontological graph into one top concept list . The extracted top concepts are used to build standard normalized feature vectors using the standard vector space model ( VSM ) for the purpose of text categorization . The quality of the categorization results achieved by the proposed model surpasses that of traditional weighting approaches significantly .
There are a number of suggestions to extend this work . One direction is to link the presented work to web document categorization . Another future direction is to investigate the usage of such models on other corpora and its effect on document categorization results , compared to that of traditional methods .
6 . REFERENCES [ 1 ] K . Aas and L . Eikvil . Text categorisation : A survey . technical report 941 . Technical report , Norwegian Computing Center , June 1999 .
[ 2 ] M . Collins . Head Driven Statistical Model for Natural
Language Parsing . PhD thesis , University of Pennsylvania , 1999 .
[ 3 ] R . Feldman and I . Dagan . Knowledge discovery in textual databases ( kdt ) . In Proceedings of First International Conference on Knowledge Discovery and Data Mining , pages 112–117 , 1995 .
[ 4 ] C . Fillmore . The case for case . Chapter in : Universals in Linguistic Theory . Holt , Rinehart and Winston , Inc . , New York , 1968 .
[ 5 ] W . Francis and H . Kucera . Manual of information to accompany a standard corpus of present day edited american english , for use with digital computers , 1964 .
[ 6 ] D . Gildea and D . Jurafsky . Automatic labeling of semantic roles . Computational Linguistics , 28(3):245–288 , 2002 .
[ 7 ] T . Joachims . Text categorization with support vector machines : learning with many relevant features . In C . N´edellec and C . Rouveirol , editors , Proceedings of ECML 98 , 10th European Conference on Machine Learning , number 1398 , pages 137–142 , Chemnitz , DE , 1998 . Springer Verlag , Heidelberg , DE .
[ 8 ] D . Jurafsky and J . H . Martin . Speech and Language
Processing . Prentice Hall Inc . , 2000 .
[ 9 ] P . Kingsbury and M . Palmer . Propbank : the next level of treebank . In Proceedings of Treebanks and Lexical Theories , 2003 .
[ 10 ] M . F . Porter . An algorithm for suffix stripping .
Program , 14(3):130–137 , July 1980 .
[ 11 ] S . Pradhan , K . Hacioglu , V . Krugler , W . Ward , J . H . Martin , and D . Jurafsky . Support vector learning for semantic argument classification . Machine Learning , 60(1 3):11–39 , 2005 .
[ 12 ] S . Pradhan , K . Hacioglu , W . Ward , J . H . Martin , and
D . Jurafsky . Semantic role parsing : Adding semantic structure to unstructured text . In Proceedings of the 3th IEEE International Conference on Data Mining ( ICDM ) , pages 629–632 , 2003 .
[ 13 ] S . Pradhan , W . Ward , K . Hacioglu , J . Martin , and
D . Jurafsky . Shallow semantic parsing using support vector machines . In Proceedings of the Human Language Technology/North American Association for Computational Linguistics ( HLT/NAACL ) , 2004 .
[ 14 ] G . Salton and M . J . McGill . Introduction to Modern
Information Retrieval . McGraw Hill , 1983 .
[ 15 ] G . Salton , A . Wong , and C . S . Yang . A vector space model for automatic indexing . Communications of the ACM , 18(11):112–117 , 1975 .
[ 16 ] S . Shehata , F . Karray , and M . Kamel . Enhancing text clustering using concept based mining model . In ICDM , pages 1043–1048 , 2006 .
[ 17 ] S . Shehata , F . Karray , and M . Kamel . Enhancing text retrieval performance using conceptual ontological graph . In ICDM Workshops , pages 39–44 , 2006 .
637Research Track Paper
