BoostCluster : Boosting Clustering by Pairwise Constraints
Yi Liu , Rong Jin , and Anil K . Jain
Department of Computer Science and Engineering
Michigan State University
East Lansing , MI 48824 , USA
{liuyi3 , rongjin , jain}@csemsuedu
ABSTRACT Data clustering is an important task in many disciplines . A large number of studies have attempted to improve clustering by using the side information that is often encoded as pairwise constraints . However , these studies focus on designing special clustering algorithms that can effectively exploit the pairwise constraints . We present a boosting framework for data clustering , termed as BoostCluster , that is able to iteratively improve the accuracy of any given clustering algorithm by exploiting the pairwise constraints . The key challenge in designing a boosting framework for data clustering is how to influence an arbitrary clustering algorithm with the side information since clustering algorithms by definition are unsupervised . The proposed framework addresses this problem by dynamically generating new data representations at each iteration that are , on the one hand , adapted to the clustering results at previous iterations by the given algorithm , and on the other hand consistent with the given side information . Our empirical study shows that the proposed boosting framework is effective in improving the performance of a number of popular clustering algorithms ( Kmeans , partitional SingleLink , spectral clustering ) , and its performance is comparable to the state of the art algorithms for data clustering with side information .
Categories and Subject Descriptors I53 [ Clustering ] : Algorithms ; H33 [ Information Search and Retrieval ] : Clustering
General Terms Algorithms
Keywords Boosting , Data clustering , Semi supervised learning , Pairwise constraints
1 .
INTRODUCTION
Data clustering , also called unsupervised learning , is one of the key techniques in data mining that is used to understand and mine the structure of unlabeled data . The idea of improving clustering by side information , sometimes called semi supervised clustering or constrained data clustering , has received significant amount of attention in recent studies on data clustering . Often , the side information is presented in the form of pairwise constraints : the must link pairs where data points should belong to the same cluster , and the cannot link pairs where data points should belong to different clusters . There are two major approaches to semi supervised clustering : the constraint based approach and the approach based on distance metric learning . The first approach employs the side information to restrict the solution space , and only finds the solution that is consistent with the pairwise constraints . The second approach first learns a distance metric from the given pairwise constraints , and computes the pairwise similarity using the learned distance metric . The computed similarity matrix is then used for data clustering .
Although a large number of studies have been devoted to semi supervised clustering , most of them focus on designing special clustering algorithms that can effectively exploit the pairwise constraints . For instance , algorithms in [ 4 , 5 , 23 ] are designed to improve the probabilistic models for data clustering by incorporating the pairwise constraints into the priors of the probabilistic models ; the constrained K means algorithm [ 27 ] exploits the pairwise constraints by adjusting the cluster memberships to be consistent with the given constraints . However , it is often the case that we have a specific clustering algorithm that is specially designed for the target domain , and we are interested in improving the accuracy of this clustering algorithm by the available side information . To this end , we propose a boosting framework for data clustering , termed as BoostCluster , that is able to improve any given clustering algorithm by the pairwise constraints . It is important to note that the proposed algorithm does not make any assumption about the underlying clustering algorithm , and is therefore applicable to any clustering algorithm .
Although a number of boosting algorithms ( eg , [ 10 ] ) have been successfully applied to supervised learning , extending boosting algorithms to data clustering is significantly more challenging . The key difficulty is how to influence an arbitrary clustering algorithm with the given pairwise constraints . To overcome this challenge , we propose to encode the side information into the data representation that is the
2D Projection of Oringinal Data
0.8
0.6
0.4
0.2
0
−0.2
−0.4
−0.6
−0.8
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
( a ) Input data
Data Projected to a 2D Subspace ( Iteration 2 )
2
1.5
1
0.5
0
−0.5
−1
−1.5
−2 −4
Data Projected to a 2D Subspace ( Iteration 1 )
−3
−2
−1
0
1
2
3
4
( b ) Iteration 1
Data Projected to a 2D Subspace ( Iteration 7 )
1
0.5
0
−0.5
−1
−1.5 −4
1
0.8
0.6
0.4
0.2
0
−0.2
−0.4
−0.6
−0.8
−3
−2
−1
0
1
2
3
4
−1 −2.5
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
( c ) Iteration 2
( d ) Iteration 7
Figure 1 : An example illustrating the idea of iterative data projections . ( a ) shows the original data distribution , projected to the space spanned by its two principal components ; ( b) (d ) show the data distributions based on the new representations in the projected subspaces that are generated in iteration 1 , 2 , and 7 . input to the clustering algorithm . More specifically , we will first find the subspace in which data points of the must link pairs are close to each other while data points of the cannotlink pairs are far apart . Then , a new data representation is generated by projecting all the data points into the subspace , and will be used by the given clustering algorithm to find the appropriate cluster assignments . Furthermore , the procedure for identifying the appropriate subspace based on the remaining unsatisfied constraints , and the procedure for clustering data points using the newly generated data representation will alternate iteratively till most of the constraints are satisfied . Although the idea of incorporating constraints into clustering through the generation of new data representations is not completely new , the existing approaches [ 12 , 17 , 19 , 29 ] do not take into account the performance of the clustering algorithms while generating the data representations , and therefore only achieve sub optimal performance .
Figure 1 illustrates the idea of iterative data projection . The data points used in this illustration are sampled from the “ scale ” dataset that will be described later in Section 41 They belong to three clusters that are labeled in Figure 1 by legends 4 , ◦ , and × , respectively . A partitional clustering algorithm is used in this illustration . Sub figure ( a ) shows the original data distribution projected into a 2D space that is generated by PCA . We clearly see that many data points of the cluster × overlap heavily with the data points of the clusters 4 and ◦ , and they are difficult to be separated . The must link and cannot link constraints are indicated in Figure 1(a ) by solid lines and dotted lines , respectively . Figure 1(b) (d ) illustrate the projected data distributions based on the new representations that are gener ated by the proposed boosting framework in iteration 1 , 2 , and 7 , respectively . The data representations generated in different iterations are helpful in separating the data points in the cluster × from those in the other two clusters . The remaining paper is arranged as follows : Section 2 briefly reviews the previous work on semi supervised clustering . Section 3 describes the problem of boosting a given clustering algorithm by a set of pairwise constraints and introduces the proposed BoostCluster framework . Section 4 presents the results of our empirical study . Section 5 states conclusions of this work .
2 . RELATED WORK
The constraint based approach for semi supervised clustering utilizes the side information to restrict the feasible solutions when deciding the cluster assignment . Early work in this category took the side information as hard constraints , and only considered the cluster assignments that were absolutely consistent with the given pairwise constraints . In [ 6 , 27 ] , the authors proposed the constrained K means algorithm by adjusting the cluster memberships to be consistent with the pairwise constraints . In [ 26 ] , a generalized Expectation Maximization ( EM ) algorithm is proposed to incorporate the pairwise constraints into the EM algorithm . In particular , the cluster assignments that are inconsistent with the constraints are excluded from the partition function when computing the posterior probability for the cluster memberships . One problem with treating the side information as hard constraints is that we may not be able to find feasible solutions that are consistent with all the constraints [ 7 ] . To overcome this problem , a number of studies view the side information as soft constraints . The key idea is to penalize , not to exclude , the cluster assignments that are inconsistent with the given pairwise constraints . In [ 4,5,23 ] , the authors present probabilistic models for semi supervised clustering where the pairwise constraints are incorporated into the clustering algorithms through the Bayesian priors . In [ 22 ] , the authors modified the mixture model for data clustering by redefining the data generation process through the introduction of hidden variables . In [ 3 ] , the authors extend the framework of semi supervised clustering by selecting the most informative pairwise constraints to solicit the labeling information . In [ 8 ] , the authors study semi supervised clustering for the hierarchical clustering algorithm . In [ 21 ] , a mean field approximation method was proposed to learn from constraint data . In [ 17 ] , a spectral learning framework was proposed to incorporate the side information into data clustering .
Another approach to semi supervised clustering is to first learn a distance metric from the given pairwise constraints . The pairwise similarity between any two examples is then computed based on the learned distance metric , and a clustering algorithm is applied to the computed similarity matrix . The key to this approach is to effectively learn a distance metric from the side information . Zhang et al . [ 32 ] proposed to learn a distance metric by a linear regression model . Xing et al . [ 29 ] formulated the distance metric learning problem as a constrained convex programming problem . This algorithm is extended to the nonlinear case in [ 20 ] by the introduction of kernels . Yang et al . [ 30 ] proposed a local distance metric algorithm that is designed to address the problem of distance metric learning for multi modal data distributions . Golderberg et al . [ 11 ] presented the neighborhood compo nent analysis algorithm that learns a local distance metric by extending the nearest neighbor classifier . Weinberger [ 28 ] presented a large margin nearest neighbor classifier for distance metric learning that extended the neighborhood component analysis to a maximum margin framework . Discriminative component analysis [ 15 ] learns a distance metric by extending the relevance component analysis to effectively explore both the must link and the cannot link constraints simultaneously . In [ 13,14 ] , the authors extended the boosting algorithms to learn a distance function from given pairwise constraints . Schultz and Joachims [ 25 ] extended the framework of support vector machine to learn distance metrics from the pairwise comparisons .
Finally , a few studies cluster data points by a similarity matrix that is directly modified according to the pairwise constraints . In [ 19 ] , the authors proposed to modify the similarity matrix by linearly combining the original similarity matrix with the pairwise constraints . Klein et al . [ 18 ] proposed to modify the similarity matrix by propagating the pairwise constraints through the nearest neighbors .
3 . BOOSTING CLUSTERING
Let X = ( x1 , . . . , xn ) denote the collection of examples to be clustered , where n is the total number of examples and each example xi ∈ Rd is a vector of d dimensions . We use a matrix S+ ∈ Rn×n to represent all the must link pairs , where S+ i,j is one when examples xi and xj form a mustlink pair , and zero otherwise . Similarly , we use a matrix S− ∈ Rn×n to represent all the cannot link pairs , where S− i,j is one when examples xi and xj form a cannot link pair , and zero otherwise . Let A denote the given clustering algorithm to be improved . In order to make this framework general , we treat the clustering algorithm A as a black box that only takes the data representation of all examples as its input and outputs the cluster memberships for the given examples . Note in this work , we assume that the number of clusters is given . 3.1 Objective Function
The first step toward the boosting algorithm is to construct an appropriate objective function . As described in the introduction section , the goal of the boosting algorithm is to identify the subspace that keeps the data points in the unsatisfied must link pairs close to each other , and keeps the data points from the unsatisfied cannot link pairs well separated . In order to identify which constraint pairs are not well satisfied , we introduce the kernel similarity matrix K ∈ Rn×n , where Ki,j ≥ 0 indicates the confidence of assigning examples xi and xj to the same cluster . When all the constraints are satisifed , we expect to observe a large value for kernel similarity Ki,j if xi and xj form a must link pair , and a small value for Ki,j if xi and xj form a cannotlink pair . Hence , we propose the following objective function to measure the inconsistency between the kernel matrix K and the given pairwise constraints :
L = n n
Xi,j=1
Xa,b=1 i,jS− S+ a,b exp(Ka,b − Ki,j )
( 1 )
In the above , each term within the summation compares Ka,b , ie , the similarity between two points from a cannotlink pair , to Ki,j , ie , the similarity between two data points from a must link pair . By minimizing the objective function
Input
• X : d × n matrix for the input data • A : the given clustering algorithm • s : the number of principal eigenvectors used for • S+ : matrix for must link pairs • S− : matrix for cannot link pairs projection
Output : cluster memberships Algorithm
• Initialize Ki,j = 0 for any i , j = 1 , 2 , . . . , n . • For t = 1 , 2 , . . . , T
– Compute pi,j and qi,j using ( 5 ) and ( 6 ) . – Compute matrix T using ( 10 ) . – Compute the top s eigenvectors and eigenval i=1 of T . ues {(λi , vi)}s – Construct the projection matrix P using ( 11 ) , and generate the new data representation X 0 by projecting the input data X onto P .
– Run the clustering algorithm A using the new data representation X 0 . Compute the matrix ∆ with ∆i,j = 1 when xi and xj are grouped into the same cluster by A , and zero otherwise .
– Compute α using ( 13 ) . – Update the kernel similarity matrix K as
K + α∆ → K
• Run the clustering algorithm A with the kernel matrix K ( if A does not take a kernel similarity matrix as input , a data representation can be generated by the first s + 1 eigenvectors of the matrix K ) .
Figure 2 : Boosting algorithm in ( 1 ) , we will ensure that all the data points in the must link pairs are more similar to each other than the data points in the cannot link pairs .
The objective function in ( 1 ) can also be written as :
L = n Xi,j=1
S+ i,j exp(−Ki,j)!  a,b exp(Ka,b)
S− n
Xa,b=1
 ( 2 ) a,b=1 S− a,b exp(Ka,b ) , i,j=1 S+
The above objective function is a product of two terms : the i,j exp(−Ki,j ) , measures the inconsistency between the kernel similarity matrix K and the first term , ie , Pn must link constraints ; the second term , ie,Pn measures the inconsistency between the kernel similarity matrix K and the cannot link constraints . Thus , by minimizing the product of the two terms , we enforce the kernel matrix K to be consistent with the given pairwise constraints . 3.2 The BoostCluster Framework
The overall idea is to improve the clustering results iteratively . In each iteration , we will first identify a new data representation by minimizing the discrepancy between the kernel matrix K and the pairwise constraints . The new data representation will then be used by the clustering algorithm A to obtain the new clustering results , and the new results will be used in return to update the kernel matrix K . It is important to note that the data representation is generated based on the clustering results . This is where the proposed approach differs from the previous studies , ie , the proposed algorithm is capable of taking into account the performance of the clustering algorithm to be boosted while the others do not .
To boost the performance of a clustering algorithm A given a set of pairwise constraints , we follow the idea of boosting algorithms by iteratively improving the kernel similarity matrix K . Let K denote the current kernel similarity matrix . Let ∆ ∈ {0 , 1}n×n denote the incremental kernel similarity matrix that is inferred from the clustering results generated by the algorithm A . In particular , ∆i,j = 1 when both xi and xj are assigned to the same cluster and ∆i,j = 0 otherwise . The new kernel matrix K 0 is a linear combination of K and ∆ , ie ,
K 0 = K + α∆
( 3 ) where α ≥ 0 is the combination weight . Then , the objective function L for the new kernel K 0 , denoted by L(K 0 ) , is written as : n n
L(K 0 ) = n i,jS− S+ a,b exp(K 0 a,b − K 0
Xi,j=1 Xa,b=1 Xi,j=1 Xa,b=1 pi,jqa,b exp(−α(∆i,j − ∆a,b ) ) i,j ) n
= where pi,j = S+ qa,b = S− i,j exp(−Ki,j ) a,b exp(Ka,b )
( 4 )
( 5 )
( 6 )
In the above , pi,j measures the inconsistency between the kernel matrix K and the must link pair ( xi , xj ) , and qa,b measures the inconsistency between K and the cannot link pair ( xa , xb ) .
Using Jensen ’s inequality , an upper bound for the term
( 7 )
3
1 3
+ 3α
∆i,j − ∆a,b + 1 exp(−α(∆i,j − ∆a,b ) ) can be constructed as follows exp(−α(∆i,j − ∆a,b ) ) = exp−3α ∆i,j − ∆a,b + 1 ≤ In the first step of the above derivation , we rewrite α(∆i,j − ∆a,b ) as a summation over the probability distribution of ( (∆a,b−∆i,j +1)/3 , ( ∆i,j−∆a,b+1)/3 , 1/3 ) . Using the upper bound in ( 7 ) , we can now bound the objective function in ( 4 ) as follows
∆a,b − ∆i,j + 1
∆a,b − ∆i,j + 1 exp(−3α ) + exp(3α ) +
+ 0 ×
1 3
3
3
3 n n
L(K 0 ) = pi,jqa,b exp(−α(∆i,j − ∆a,b ) )
≤
Xi,j=1 Xa,b=1 exp(3α ) − 1 n
3
∆i,j pi,j exp(3α ) + exp(−3α ) + 1
Xi,j=1
+
3 n
Xa,b=1 Xi,j=1 Xa,b n n qa,b − qi,j pi,jqa,b n
Xa,b=1 pa,b 
( 8 )
We can simplify the above expression by defining a matrix T as follows
Ti,j = pi,j a,b=1 pa,b − Pn
Pn qi,j a,b=1 qa,b
The elements in matrix T measure the inconsistency between kernel matrix K and the pairwise constraints . Only the pairwise constraints that are not well satisfied will resulted in large |Ti,j| : a large positive value for Ti,j indicates that K is inconsistent with the must link pair ( xi , xj ) , while a large negative value for Ta,b indicates that K is inconsistent with the cannot link pair ( xa , xb ) . Here , the matrix T plays a similar role as the example weights w of the AdaBoost algorithm , in which w is used to identify the examples that are difficult to classify correctly . in ( 8 ) becomes
Using the notation of matrix T , the upper bound for L −[1 − exp(−3α)]tr(T ∆>)/3 L(K 0 ) ≤ L(K ) × [ exp(3α ) + exp(−3α ) + 1]/3
( 9 ) where
L(K ) = n n
Xi,j=1
Xa,b=1 pi,jqa,b , tr(T ∆> ) =
Ti,j∆i,j n
Xi,j=1
Note that when α = 0 , the right side of ( 9 ) becomes L(K ) , ie , the objective function of the previous iteration . Thus , by minimizing the upper bound in ( 9 ) with respect to α , we are guaranteed to have L(K 0 ) ≤ L(K ) , thus reducing the objective function at successive iterations . As suggested by the inequality in ( 9 ) , to effectively reduce the objective function L , we need to maximize the term tr(T ∆> ) . To obtain the new data representation , we assume that the incremental kernel matrix ∆ can be approximated by a linear projection of the input data X , ie , ∆ ≈ ( P >X)>(P >X ) = X >P P >X where P = ( p1 , p2 , . . . , ps ) is the projection matrix ( s ≤ d ) with each pi ∈ Rd specifying a different projection direction . Using the above expression , tr(T ∆> ) can be written as tr(T ∆> ) ≈ tr(P >XT X >P )
( 10 )
By further assuming orthogonality between any two projection vectors , ie , p> i pj = δ(i , j ) , we have the optimal solution for pi that maximizes the expression in ( 10 ) as the ith maximum eigenvector of matrix XT X > . Let {(λi , vi)}s i=1 denote the top s principal eigenvalues and eigenvectors of matrix XT X > . Then , the optimal projection matrix P is constructed as
P = ( √λ1v1 , √λ2v2 , . . . , √λsvs )
( 11 )
Intuitively , since the matrix T encodes the discrepancy between the current kernel K and the constraints , the projection matrix P , which is generated from the input patterns X and the discrepancy encoded matrix T , will result in a subspace that best preserves the information from the constraints yet to be satisfied .
Using the projection computed in ( 11 ) , we generate a new data representation as X 0 = P >X . This new representation X 0 will be input to the clustering algorithm A to generate new cluster memberships . The resulting cluster memberships are then used to compute the incremental kernel matrix ∆ .
In addition to the projection matrix P , another important question is how to compute the optimal α . We can estimate the optimal α by minimizing the upper bound in ( 9 ) , which leads to α = log[1 + tr(T ∆>)]/6 . However , we can further improve the estimation of α by minimizing the original ob
Change in BoostCluster Objective Function
12
11.8
) P C B L ( g o l
11.6
11.4
11.2
11
10.8 0
10
20
30
#Iterations
40
50
Figure 3 : An example of objective function vs . the number of iterations . jective function in ( 2 ) , which is
L(K 0 ) = n Xi,j=1 = n Xi,j=1 n Xi,j=1 i,j exp(−K 0 S+ pi,jδ(∆i,j , 0 ) + qi,jδ(∆i,j , 0 ) + n i,j)! i,j exp(K 0 S− i,j)! n Xi,j=1 pi,jδ(∆i,j , 1 ) exp(−α)! × Xi,j=1 qi,jδ(∆i,j , 1 ) exp(α)! ( 12 ) Xi,j=1 n
It is not difficult to show that the optimal α that maximizes the above expression is :
α =
1 2 i,j=1 pi,jδ(∆i,j , 1 ) log Pn i,j=1 pi,jδ(∆i,j , 0 ) × Pn i,j=1 qi,jδ(∆i,j , 1)! ( 13 ) Pn Pn i,j=1 qi,jδ(∆i,j , 0 )
Figure 2 summarizes the proposed boosting algorithm . 3.3 Convergence
Similar to most boosting algorithms , we can show that the objective function ( 1 ) is reduced exponentially with successive iterations of the proposed boosting algorithm . This conclusion can be summarized into the following theorem . Theorem 1 . Let ∆1 , ∆2 , . . . , ∆T be the incremental kernel matrices computed from the clustering results by running the boosting algorithm ( in Figure 2 ) . Then , the objective function after T iterations , ie , LT , is bounded as follows :
LT ≤ n Xi,j=1 where
S+
S− i,j! T Yt=1 i,j! n Xi,j=1 ( √AtDt − √BtCt)2
( At + Bt)(Ct + Dt )
γt = where pt using the kernel matrix K at the t th iteration . i,j are computed according to ( 5 ) and ( 6 ) i,j and qt
The above theorem can be proved directly by using the expression in ( 12 ) and the expression for α in ( 13 ) . Due to space constraints , we cannot provide details . Figure 3 shows how the logarithm of the objective function used by the proposed algorithm changes with respect to the number of iterations ; the objective function converges very fast , and becomes flat after 22 iterations . In our experiments , the boosting algorithm usually converges within 25 iterations . 3.4 Computational Issues
In the proposed boosting algorithm , a key step towards finding a good projection matrix P is eigen decomposition of XT X > , as shown in ( 10 ) and ( 11 ) . To efficiently compute XT X > , we first note that XT X > can also be written as :
XT X > = n
Xi,j=1
Ti,jxix> j
( 15 )
Since Ti,j is nonzero only when the example pair ( xi , xj ) corresponds to a given constraint , the above calculation only involves a very small portion of all possible example pairs . Hence , the computational cost for XT X > is only proportional to the number of pairwise constraints . Therefore , XT X > can be calculated efficiently as long as the number of labeled pairs is relatively small .
In addition to XT X > , another major computational cost arises from calculating the principal eigenvectors and eigenvalues of XT X > , particularly when the dimensionality of the feature space is high . For instance , for text data clustering , each document is typically represented by a vector of over 100 , 000 features , and the size of matrix XT X > is over 100 , 000 × 100 , 000 . A straightforward approach is to reduce the dimensionality before running the proposed algorithm . However , most dimensionality reduction algorithms that are capable of handling high dimensional space are unsupervised , and therefore are unable to exploit the pairwise constraints . Here , we propose an algorithm that is able to efficiently compute the eigenvectors of XT X > when the input dimensionality is high . We first realize that each eigenvector of XT X > has to lie in the space that is spanned by the examples used by the constraints . More specifically , we denote by ˜X = ( ˜x1 , ˜x2 , . . . , ˜xm ) the subset of m examples that are involved in the constraints . Then , the eigenvector vi can be written as a linear combination of {˜xi}m wi,k ˜xk = ˜Xwi i=1 , ie , vi =
( 16 ) m
Xk=1
V = ( v1 , v2 . . . , vs ) = ˜X(w1 , w2 , . . . , ws ) = ˜XW .
The proof of this result can be found in Appendix A . We furthermore denote by ˜T the pairwise constraints , where ˜Ti,j denotes the pairwise constraint between ˜xi and ˜xj . Then , wi , i = 1 , 2 , . . . , s correspond to the first s principal eigenvectors of the following generalized eigenvector problem
˜X > ˜X ˜T ˜X > ˜Xwi = λi ˜X > ˜Xwi
( 17 ) Note that since ˜X > ˜X ˜T ˜X > ˜X and ˜X > ˜X are m×m matrices , the cost of computing the eigenvectors is independent of the dimensionality of the input space . The proof of the above result can be found in Appendix B .
( 1 − γt ) ,
( 14 )
More generally , we have
At , Bt , Ct , and Dt are non negative constants , and are computed as
At =
Ct = n n
Xi,j=1 Xi,j=1 pt i,jδ(∆t i,j , 0 ) , Bt = qt i,jδ(∆t i,j , 0 ) , Dt = pt i,jδ(∆t i,j , 1 ) qt i,jδ(∆t i,j , 1 ) n n
Xi,j=1 Xi,j=1 wdbc scale vowel segmentation handwrittendigit pendigit
Name #Attributes #Clusters #Examples 569 625 990 2310 4000 4396
30 4 10 19 256 16
2 3 11 7 10 4
Table 1 : Datasets used in the experiments .
4 . EXPERIMENTS
We now present an empirical evaluation of our proposed boosting framework . In particular , we aim to address the following three questions in our study :
1 . As a general boosting framework , is the proposed method able to improve the performance for any given clustering algorithm ?
2 . How effective is the proposed boosting framework compared to other semi supervised clustering algorithms ?
3 . How robust is the proposed boosting framework in improving the clustering performance by using the pairwise constraints ?
4.1 Experiment Setup
To validate the claim that the proposed boosting algorithm is capable of improving any clustering algorithm by exploiting the pairwise constraints , three popular clustering algorithms are used in our study . They are :
1 . K means algorithm [ 1 ] . It represents the family of clustering algorithms that try to find compact and wellseparated clusters . We adopted the implementation from the Weka software1 .
2 . Partitional SingleLink algorithm ( “ SLINK ” for short ) [ 16 ] .
It represents the family of the hierarchical clustering algorithms . We adopted the implementation from the CLUTO software2
3 . k way spectral clustering ( “ SPEC ” for short ) . It represents the family of spectral methods for data clustering . In particular , we follow the algorithm in [ 24 ] for the implementation of spectral clustering .
Six datasets drawn from the UCI machine learning repository [ 9 ] are used in our study . Table 1 summarizes the information about these datasets3 . As indicated in Table 1 , these datasets vary significantly in their sizes , number of clusters , and number of attributes .
To evaluate the clustering performance , two measurements are used in our experiments . The first measurement is normalized mutual information ( NMI for short ) [ 4 ] , which is defined as
N M I =
2M I(X , X0 )
H(X ) + H(X0 )
1http://wwwcswaikatoacnz/ml/weka/ 2http://glarosdtcumnedu/gkhome/views/cluto 3Note that for the “ pendigit ” dataset , examples in only four classes of letter “ 3 ” , “ 6 ” , “ 8 ” and “ 9 ” are selected from a total of 10 classes because these four letters are in general most difficult to distinguish .
BoostCluster + K−means BoostCluster + SLINK BoostCluster + SPEC MCPKmeans SSKK SpectralLearn + K−means . SpectralLearn + SLINK SpectralLearn + SPEC
Figure 4 : Legends for all algorithms in our comparative study . These legends will be used in all the figures in this paper . where X0 and X denote the random variables of cluster memberships from the ground truth and the output of clustering algorithm , respectively . M I(x , y ) represents the mutual information between random variables x and y , and H(x ) represents the Shannon entropy of random variable x . The second measurement is Pairwise F measure ( PWF1 for short ) , which is the harmonic mean of pairwise precision and recall that are defined as follows precision = recall =
P W F 1 =
#pairs correctly placed in the same cluster
Total #pairs placed in the same cluster
#pairs correctly placed in the same cluster Total #pairs actually in the same cluster 2 × precision × recall precision + recall
The PWF1 measurement defined above is closely related to the metric defined in [ 29 ] that measures the percentage of data pairs correctly clustered together . The key problem with the metric defined in [ 29 ] is that it counts two types of data pairs , ie , pairs of data points assigned to the same cluster and pairs of data points assigned to different clusters , with equal importance . This is problematic because most of the data pairs in practice will consist of data points from different clusters when the number of clusters is large . A similar issue arises in multi class learning , and that is why F measure is widely used for evaluating multi class learning [ 31 ] .
To verify the efficacy of the proposed boosting framework in exploiting the pairwise constraints for data clustering , three baseline approaches are used :
1 . Metric Pairwise Constraints K means ( MPCKmeans for short ) algorithm [ 2,4 ] , which is a probabilistic framework based on Hidden Markov Random Fields .
2 . Semi supervised Kernel K means ( SSKK for short ) algorithm [ 19 ] , which exploits the pairwise constraints by a kernel approach and finds clusters with nonlinear boundaries in the input data space .
3 . Spectral Learning ( SpectralLearn for short ) algorithm [ 17 ] , which applies spectral methods to learn a data representation from the pairwise constraints . The generated data representation can therefore be used by any clustering algorithm to identify the appropriate data clusters . The key difference between spectral learning and our algorithm is that our algorithm generates algorithm specific data representations by taking into account the performance of clustering algorithms .
NMI ( wdbc dataset )
NMI ( scale dataset )
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
I M N
0 0
100
0.6
0.5
0.4
I M N
0.3
0.2
0.1
700
800
0 0
100
200
300
400
500
#Pairwise Constraints
600
200
300
400
500
600
#Pairwise Constraints
0.45
0.35
I M N
0.25
700
800
0.15 0
100
NMI ( vowel dataset )
200
300
400
500
#Pairwise Constraints
600
NMI ( handwrittendigit dataset )
NMI ( segmentation dataset )
0.7
0.6
0.5
0.4
0.3
I M N
0.7
0.6
0.5
0.4
0.3
0.2
0.1
I M N
NMI ( pendigit dataset )
1
0.9
0.8
0.7
0.6
I M N
0.5
0.4
0.3
0.2
0.1
0.2 0
100
1
0.9
1 F W P
0.8
0.7
0.6 0
100
200
300
400
500
600
#Pairwise Constraints
700
800
0 0
100
200
300
400
500
#Pairwise Constraints
600
700
800
0 0
100
200
300
400
500
#Pairwise Constraints
600
Figure 5 : Comparison of clustering performance evaluated by NMI .
PWF1 ( wdbc dataset )
200
300
400
500
600
#Pairwise Constraints
0.9
0.8
1 F W P
0.7
0.6
0.5
700
800
0.4 0
100
PWF1 ( scale dataset )
PWF1 ( vowel dataset )
0.25
1 F W P
700
800
0.15 0
100
200
300
400
500
#Pairwise Constraints
600
200
300
400
500
#Pairwise Constraints
600
PWF1 ( segmentation dataset )
PWF1 ( handwrittendigit dataset )
200
300
400
500
#Pairwise Constraints
600
700
800
0.3 0
100
200
300
400
500
#Pairwise Constraints
600
Figure 6 : Comparison of clustering performance evaluated by PWF1 .
0.6
0.5
1 F W P
0.4
0.3 0
100
200
300
400
500
600
#Pairwise Constraints
0.6
0.5
1 F W P
0.4
0.3
700
800
0.2 0
100
PWF1 ( pendigit dataset )
1
0.9
0.8
0.7
0.6
0.5
0.4
1 F W P
700
800
700
800
700
800
700
800
Previous studies [ 2 , 4 , 17 , 19 ] showed that the above three algorithms deliver the state of the art performance in comparison to other semi supervised clustering algorithms such as the constrained K means .
In summary , we will compare the following semi supervised clustering algorithms in the experiments : the three clustering algorithms ( K means , SLINK , and SPEC ) being boosted by the proposed BoostCluster framework ; the same three clustering algorithms with input from the Spectral Learning algorithm ; the MPCKmeans algorithm ; and the SSKK algorithm . For easy identification in figures , we listed the legends for the above algorithms to be compared , in Figure 4 . These legends apply to all following performance comparison figures ( we will omit showing legends in those figures due to space constraints ) .
Finally , in all the experiments , we vary the number of pairwise constraints from 0 to 800 . Since a random sampling of data pairs tends to find many more cannot link pairs than the must link pairs , in this study , we enforce an equal number of constraints for both must link pairs and cannotlink pairs . The numbers of eigenvectors ( ie , the parameter s in the boosting algorithm shown in Figure 2 ) are determined empirically as follows : 3 for the “ scale ” dataset , 10 for the “ handwrittendigit ” dataset and 5 for the remaining 4 datasets . All the experiments in this study are repeated five times , and the evaluation results averaged over the five trials are reported .
4.2 Generality of the Boosting Framework
Figures 5 and 6 show the clustering performance , evaluated by NMI and PWF1 respectively , of the BoostCluster framework using the three clustering algorithms ( ie , K means , partitional SingleLink , and spectral clustering ) , the same three clustering algorithms with input as the new data representation from the Spectral Learning algorithm , the MPCKmeans algorithm , and the SSKK algorithm .
1 . We observe that for most datasets , the BoostCluster framework is able to improve the clustering performance for all the three clustering algorithms regardless of which evaluation metric is used . This suggests that the proposed framework is effective in exploiting the pairwise constraints to improve clustering performance . MPCKmeans algorithm and SSKK algorithm are also effective in general , however , their clustering performance improvements are less significant , especially for larger datasets ( such as “ handwrittendigit ” and “ pendigit ” ) .
2 . Although SpectralLearn algorithm can also be combined with any clustering algorithm , in our experiments , it does not always improve the clustering algorithm performance . For example , for “ wdbc ” and “ handwrittendigit ” , increasing the number of pairwise constraints deteriorates clustering performance by combining SpectralLearn with any of the three clustering algorithms . Moreover , the effect of SpectralLearning depends on the clustering algorithm . For example , for the “ pendigit ” dataset , SpectralLearn improves the clustering performance of K means and SLINK , but degrades SPEC in general . In comparison , the clustering performance improvement brought by the proposed BoostCluster is substantially more stable and consistent , across different datasets and different clus tering algorithms . This can be attributed to the fact that BoostCluster is adaptive to both clustering algorithms and datasets : in each iteration , it takes the feedback from the result of applying the given clustering algorithm to the particular dataset , and decides how to adjust the kernel matrix . However , SpectralLearn generates new data representations independent from the clustering algorithm that is used .
3 . The performance of the three clustering algorithms ( Kmeans , SLINK , and SPEC ) varies significantly across the six different datasets . For instance , for the “ vowel ” dataset , “ BoostCluster+K means ” algorithm performs considerably worse than “ BoostCluster+SPEC ” algorithm . However , the performance of “ BoostCluster+Kmeans ” algorithm for the “ pendigit ” dataset , is significantly better than that of “ BoostCluster+SPEC ” algorithm . This result also indicates that every clustering algorithm has its own strength , and therefore it is important to develop a general framework that is able to boost the performance of any clustering algorithm by the given pairwise constraints .
4 . The results based on the two different evaluation metrics , namely NMI and PWF1 , are inconsistent on some occasions . For instance , for the “ handwrittingdigit ” dataset , according to NMI , the clustering performance of “ BoostCluster+K means ” and “ BoostCluster+SLINK ” appears to be similar . However , according to PWF1 , “ BoostCluster+SLINK ” performs noticeably better than “ BoostCluster+K means ” . The implication of this finding is the importance of evaluating clustering performance by more than one evaluation metric , since conclusions based on the results of a single evaluation metric could be biased .
4.3 Robustness of Exploiting Pairwise
Constraints
Although the curves in Figures 5 and 6 all display different degrees of “ bumpiness ” , generally speaking , BoostCluster framework , SSKK and MPCKmeans deliver a more robust performance than SpectralLearn algorithm . On the other hand , although for most datasets , the performance curves of SSKK appear to be the most smooth among all the competitors , the resultant improvement is almost always the least noticeable among all the semi supervised clustering algorithms .
To further evaluate the robustness of all the algorithms , we conduct experiments with noisy pairwise constraints . We randomly select 20 % of the pairwise constraints and flip their labels ( ie , a must link pair becomes a cannot link pair and vice versa ) . This setting reflects the scenario when the side information includes incorrect pairwise constraints . It could happen when for instance , the pairwise constraints are derived from the implicit user feedback ( eg , user ratings or click through data ) . Thus , it is important to develop semi supervised clustering algorithms that are resilient to the noisy side information .
Figure 7 shows the performance of all the algorithms , on three selected datasets ( ie , “ scale ” , “ vowel ” , and “ pendigit ” ) when 20 % of the pairwise constraints are noisy 4 . First , by comparing Figures 7 with Figures 5 and 6 , it is not sur
4Due to space constraints , we are unable to show the results for all the six datasets .
0.8
0.7
1 F W P
0.6
0.5
0.25
1 F W P
700
800
0.15 0
100
PWF1 ( pendigit dataset , 20 % noise )
0.9
0.8
0.7
1 F W P
0.6
0.5
0.4
NMI ( scale dataset , 20 % noise )
0.55
0.45
0.35
0.25
0.15
I M N
NMI ( vowel dataset , 20 % noise )
0.4
I M N
0.3
0.2
0.8
0.7
0.6
0.5
I M N
0.4
0.3
0.2
0.1
NMI ( pendigit dataset , 20 % noise )
0.05 0
100
200
300
400
500
#Pairwise Constraints
600
700
800
0.1 0
100
200
300
400
500
#Pairwise Constraints
600
700
800
0 0
100
200
300
400
500
#Pairwise Constraints
600
700
800
PWF1 ( scale dataset , 20 % noise )
PWF1 ( vowel dataset , 20 % noise )
0.4 0
100
200
300
400
500
600
#Pairwise Constraints
200
300
400
500
600
#Pairwise Constraints
700
800
0.3 0
100
200
300
400
500
#Pairwise Constraints
600
700
800
Figure 7 : Comparison of clustering performance with 20 % noise in the pairwise constraints . Graphs in the top row show the NMI measurements , and graphs in the lower row show the PWF1 measurements . prising to observe a degradation in clustering performance when 20 % of the pairwise constraints are noisy . Second , we observe a general trend that a larger number of noisy constraints tend to result in an inferior clustering performance by MPCKmeans . This is in contrast to the results of MPCKmeans shown in Figures 5 and 6 where increasing the number of pairwise constraints usually improves the performance of clustering . This result implies that the MPCKmeans algorithm is unable to effectively exploit the pairwise constraints for data clustering when they are noisy . Similarly , while “ SpectralLearn+K means ” and “ SpectralLearn+SLINK ” are able to noticeably improve the clustering performance with increasing number of noise free pairwise constraints , with 20 % noise in the constraints , their performance degrades with the increasing number of constraints . In comparison , as shown in Figure 7 , BoostCluster framework is generally able to improve the performance of all the three clustering algorithms with increasing number of noisy pairwise constraints , and SSKK algorithm is also able to improve clustering performance despite the noise in pairwise constraints . This indicates that the proposed BoostCluster framework and the SSKK algorithm are more resilient to the noise in the side information .
5 . CONCLUSIONS
In this paper , we have studied the problem of improving data clustering by using side information in the form of pairwise constraints . A general boosting framework has been proposed to improve the accuracy of any given clustering algorithm with a given set of pairwise constraints . Such performance improvement is achieved by iteratively finding new data representations that are consistent with both the clustering results from previous iterations and the pairwise constraints . Empirical study shows that our proposed boosting framework is able to improve the clustering performance of several popular clustering algorithms by using the pairwise constraints .
6 . ACKNOWLEDGMENTS
This research work is supported by NSF grant IIS 0643494 , NIH grants 1R01GM079688 01 and 1R21NS054148 01A1 , and ONR grant N00D140710225 . We would also like to thank anonymous reviewers for their valuable suggestions .
7 . REFERENCES [ 1 ] M . R . Anderberg . Cluster Analysis for Applications .
Academic Press , Inc . , New York , NY , 1973 .
[ 2 ] S . Basu . Semi supervised Clustering : Probabilistic
Models , Algorithms and Experiments . PhD thesis , The University of Texas at Austin , 2005 .
[ 3 ] S . Basu , A . Banerjee , and R . J . Mooney . Active semi supervision for pairwise constrained clustering . In ICDM ’04 , 2004 .
[ 4 ] S . Basu , M . Bilenko , and R . J . Mooney . A probabilistic framework for semi supervised clustering . In SIGKDD ’04 , 2004 .
[ 5 ] R . Bekkerman and M . Sahami . Semi supervised clustering using combinatorial MRFs . In ICML 06 Workshop on Learning in Structured Output Spaces , 2006 .
[ 6 ] K . Bennett , P . Bradley , and A . Demiriz . Constrained k means clustering . Technical Report 2000 65 , Microsoft Research , May 2000 .
[ 7 ] I . Davidson and S . Ravi . Clustering under constraints :
Feasibility results and the k means algorithm . In SIAM Data Mining Conference , 2005 .
[ 8 ] I . Davidson and S . Ravi . Hierarchical clustering with constraints : Theory and practice . In the 9th European Principles and Practice of KDD ( PKDD ) , 2005 .
[ 28 ] K . Weinberger , J . Blitzer , and L . Saul . Distance metric learning for large margin nearest neighbor classification . In NIPS ’06 , 2006 .
[ 9 ] C . B . DJ Newman , S . Hettich and C . Merz . UCI
[ 29 ] E . P . Xing , A . Y . Ng , M . I . Jordan , and S . Russell . repository of machine learning databases , 1998 .
[ 10 ] Y . Freund and R . E . Schapire . A decision theoretic
Distance metric learning with application to clustering with side information . In NIPS ’03 , 2003 . generalization of on line learning and an application to boosting . Journal of Computer and System Sciences , 55(1):119–139 , 1997 .
[ 30 ] L . Yang , R . Jin , R . Sukthankar , and Y . Liu . An efficient algorithm for local distance metric learning . In AAAI ’06 , 2006 .
[ 11 ] J . Goldberger , S . T . Roweis , G . E . Hinton , and R . Salakhutdinov . Neighbourhood components analysis . In NIPS’04 , 2004 .
[ 12 ] M . Halkidi , D . Gunopulos , N . Kumar ,
M . Vazirgiannis , and C . Domeniconi . A framework for semi supervised learning based on subjective and objective clustering criteria . In ICDM ’05 , 2005 .
[ 13 ] T . Hertz , A . Bar Hillel , and D . Weinshall . Boosting margin based distance functions for clustering . In ICML ’04 , 2004 .
[ 14 ] T . Hertz , A . B . Hillel , and D . Weinshall . Learning a kernel function for classification with small training samples . In ICML ’06 , 2006 .
[ 15 ] S . C . H . Hoi , W . Liu , M . R . Lyu , and W Y Ma .
Learning distance metrics with contextual constraints for image retrieval . In CVPR ’06 , 2006 .
[ 16 ] A . K . Jain , M . N . Murty , and P . J . Flynn . Data clustering : a review . ACM Comput . Surv . , 31(3):264–323 , September 1999 .
[ 17 ] S . D . Kamvar , D . Klein , and C . D . Manning . Spectral
Learning . In IJCAI ’03 , 2003 .
[ 18 ] D . Klein , S . D . Kamvar , and C . D . Manning . From instance level constraints to space level constraints : Making the most of prior knowledge in data clustering . In ICML’02 , 2002 .
[ 19 ] B . Kulis , S . Basu , I . Dhillon , and R . Mooney .
Semi supervised graph clustering : a kernel approach . In ICML ’05 , 2005 .
[ 20 ] J . T . Kwok and I . W . Tsang . Learning with idealized kernels . In ICML ’03 , 2003 .
[ 21 ] T . Lange , M . H . C . Law , A . K . Jain , and J . M .
Buhmann . Learning with constrained and unlabelled data . In CVPR ’05 , 2005 .
[ 22 ] M . H . C . Law , A . P . Topchy , and A . K . Jain .
Model based clustering with probabilistic constraints . In SDM ’05 , 2005 .
[ 23 ] Z . Lu and T . Leen . Semi supervised learning with penalized probabilistic clustering . In NIPS ’05 , 2005 .
[ 24 ] A . Y . Ng , M . I . Jordan , and Y . Weiss . On spectral clustering : Analysis and an algorithm . In NIPS ’01 , 2001 .
[ 25 ] M . Schultz and T . Joachims . Learning a distance metric from relative comparisons . In NIPS ’03 , 2003 . [ 26 ] N . Shental , A . Bar Hillel , T . Hertz , and D . Weinshall .
Computing gaussian mixture models with EM using side information . In Proc . of workshop on the continuum from labeled to unlabeled data in machine learning and data mining , 2003 .
[ 27 ] K . Wagstaff , C . Cardie , S . Rogers , and S . Schroedl .
Constrained k means clustering with background knowledge . In ICML ’01 , 2001 .
[ 31 ] Y . Yang and X . Liu . A re examination of text categorization methods . In SIGIR ’99 , 1999 .
[ 32 ] Z . Zhang , J . Kwok , and D . Yeung . Parametric distance metric learning with label information . In IJCAI’03 , 2003 .
APPENDIX A . PROOF 1
We show that every non zero eigenvector vi can be written as a linear combination of ˜xi , i = 1 , 2 , . . . , m , ie , the examples involved in the pairwise constraints . Let v and λ 6= 0 be an eigenvector and eigenvalue of matrix XT X > . We therefore have XT X >v = λv . We further decompose v into two parts : v = vk + v⊥ , where vk represents the projection of v in the subspace spanned by {˜xi}s i=1 , and x⊥ represents the projection perpendicular to {˜xi}s i=1 . To show that v can be written as a linear combination of {˜xi}s i=1 , we need to show v⊥ = 0 . To this end , we first utilize the expression in ( 15 ) to calculate ( v⊥)>XT X > , ie ,
( v⊥)>XT X > = m
Xi,j=1
˜Ti,j(v⊥)> ˜xi ˜x> j = 0>
We then multiply the eigen equation XT X >v = λv by ( v⊥)> , which leads to the following equation
( v⊥)>XT X >v = 0 = λ(v⊥)>v = λkv⊥k2
2
Since λ 6= 0 , we have v⊥ = 0 and v = vk . B . PROOF 2
We will show that for the ith eigenvector vi = ˜Xwi of ˜X ˜T ˜X > , wi corresponds to the ith eigenvector of the generalized eigenvector problem in ( 17 ) . First , we realize that the orthogonality condition v> i vj = δ(i , j ) becomes ˜X ˜X >wj = δi,j . We can write the above condition for all w> i wi , i = 1 , 2 , . . . , s in the matrix form , ie , W > ˜X ˜X >W = Is . Second , the eigenvectors V = ( v1 , v2 , . . . , vs ) are the optimal solution to the following optimization problem , ie , arg max V ∈Rd×s tr(V >XT X >V ) s . t .
V >V = Is
Replacing V in the above optimization problem with V = ˜XW , we have max
W ∈Rm×s tr(W > ˜X > ˜X ˜T ˜X > ˜XW ) s . t . W > ˜X > ˜XW = Is
It is well known that the optimal solution W to the above problem consists of the first s eigenvectors of the generalized eigenvector problem in ( 17 ) .
