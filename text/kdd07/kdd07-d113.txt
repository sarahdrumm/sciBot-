Statistical Change Detection for Multi Dimensional Data
Xiuyao Song , Mingxi Wu , Christopher Jermaine , Sanjay Ranka
Department of Computer and Information Sciences and Engineering xsong,mwu,cjermain,ranka@ciseufledu ∗
University of Florida
Gainesville , FL , USA , 32611
ABSTRACT This paper deals with detecting change of distribution in multi dimensional data sets . For a given baseline data set and a set of newly observed data points , we define a statistical test called the density test for deciding if the observed data points are sampled from the underlying distribution that produced the baseline data set . We define a test statistic that is strictly distribution free under the null hypothesis . Our experimental results show that the density test has substantially more power than the two existing methods for multi dimensional change detection .
Categories and Subject Descriptors G.3 [ Mathematics of Computing ] : PROBABILITY AND STATISTICS—reliability ; H28 [ DATABASE MANAGEMENT ] : Database Applications—algorithms
General Terms Reliability , Algorithms
Keywords Change detection , density test , kernel density estimation
1 .
INTRODUCTION
This paper considers the problem of building a rigorous statistical test for detecting change of distribution in multidimensional data . Such a test would have numerous applications in a variety of disciplines . For example , one may monitor the information about the set of sales observed in the most recent day ( in a commercial enterprise ) or the prescriptions written recently ( in a hospital ) or the phone calls or emails recently observed ( in the security domain ) and ask :
∗Material in this paper is based upon work supported by the National Science Foundation under grants IIS 0325495 , IIS 0347408 , and IIS 0612170 . is the distribution of recently observed data different from what has been observed before ?
In developing a test for distributional change , the primary goal should be to construct a test with the greatest power possible – that is , with the ability to detect the most subtle changes in the data while still maintaining a low false positive rate . The need for power results from the way that such a test will be applied in a typical data mining scenario . The test will usually be run many times during the search process , comparing the data against multiple baselines , or using multiple definitions of “ recent ” ( day , week , month , etc ) In such a situation , a multiple hypothesis testing correction such as Bonferroni ’s inequality would be required to deal with the fact that repeating the test increases the chance for error [ 14 ] . This lowers the acceptable false positive rate for each test . Unfortunately , lowering the false positive rate always lowers the power . Thus , a test should have naturally high power to be useful in this scenario . 1.1 Problem Definition and Prior Work
The abstract problem we consider is as follows . We assume two sets of iid samples S and S′ , taken from two unknown , multi dimensional , non parametric distributions FS and FS ′ , respectively . In practice , S is a baseline data set , and S′ contains the most recently observed data that we wish to test for change . We then define the null hypothesis H0 , which asserts that FS and FS ′ are in fact identical . The goal is to design a proper statistical test that is able to refute H0 if it is not true . If H0 is true , then the probability of making an error ( where the test says that FS and FS ′ are different when in fact they are not ) should be at most p , where p is a user supplied parameter .
For uni dimensional data , many tests from statistics fit into this framework . However , there has been little attention to the problem of extending such tests to multidimensional data . Methods such as outlier detection apply to such data [ 6 , 11 ] , but they do not detect a mismatch in distribution among data sets ; they check for single points that are not in keeping with the baseline distribution . Some work from data mining is relevant to describing changes in multi dimensional data [ 4 , 3 ] but it does not address the statistical significance of those changes . Statistical tests for significant spatial irregularities such as Kulldorff ’s spatial scan statistic [ 12 , 15 ] and those for detecting disease outbreaks [ 21 ] are closely related but are more specific in that they do not check for a generic change of distribution – they look for “ hot spots ” in the data that may signal some sort of disease outbreak .
In fact , aside from solutions that rely on mapping two or three dimensional data to a single dimension and applying one of the uni dimensional tests [ 13 ] , one of the only tests proposed by the statistics community for this problem was published recently , called the cross match test [ 16 ] . However , as we demonstrate experimentally , the cross match test may be of questionable power , and it is computationally expensive , requiring a maximal matching computation over a graph having O((|S| + |S′|)2 ) edges . To function , all of these edges must be stored in main memory ; even then , the running time is cubic in the size of the data set . One additional multidimensional change detection test related to Kulldorff ’s test was proposed by Dasu et al.[8 ] , but this is a test that relies on a discretization of the data space . Space partitioning schemes tend to suffer from the curse of dimensionality . As such , one might expect that the power of the test will suffer in more than a few dimensions – an issue that we consider experimentally later in this paper . 1.2 Our Contributions
This paper ’s contribution is the definition and experimental evaluation of an alternative test for multidimensional distributional change , which we call the density test . The density test itself makes use of several novel statistical techniques to avoid using a space partitioning . For example , in order to obtain a sensitive distance metric even for highdimensional data , we make use of a unique Expectation Maximization [ 9 ] algorithm in conjunction with a kernel density estimator to infer the baseline distribution .
In addition to this key technical contribution , the density test is experimentally compared with two alternative tests via a total of more than 60 different change detection tasks over 13 real data sets , having from 3 to 26 dimensions each . The strengths and weaknesses of the various tests are carefully considered . The density test is found to have substantially more power ( especially for detecting changes along multiple dimensions at once ) compared to the two existing methods , and has a computational cost that is competitive . 1.3 Paper Organization
The remainder of the paper is organized as follows .
In Section 2 , we give the high level overview of the density test . We discuss kernel density estimation in Section 3 . In Section 4 , we define the test statistic and discuss its null distribution . In Section 5 we explain why we need to run the test on two different directions . Section 6 experimentally tests the power and accuracy of the density test compared with the two existing alternatives . Related work is covered in Section 7 , and Section 8 concludes the paper .
2 . HIGH LEVEL VIEW OF DENSITY TEST A generic hypothesis test framework can be used to test for change from S to S′ as follows . First , a test statistic δ(S , S′ ) is defined . In general , δ may encode any arbitrary computation over S and S′ . Let ∆ be a random variable whose distribution is exactly the distribution of δ under the null hypothesis . Then in order to refute the null hypothesis at a significance level p , we simply check whether δ is found to have an extreme value ( assuming a one tailed test , we may check whether P r[∆ ≤ δ ] is less than p ) . If it is , then we can reject the null hypothesis and declare that there has probably been a distributional change observed in the data . An excellent example of a test that makes use of the generic hypothesis test framework is Kulldorff ’s spatial scan statistic [ 12 ] which identifies abnormally “ dense ” regions in a spatial data set that may signal distributional change in spatial data . First , the data space is discretized into a set of cells , and then the baseline distribution parameters are learned by counting the number of baseline data points falling into each cell . The test statistic δ used by Kulldorff ’s method is the maximum value of a likelihood ratio statistic , tested over all possible spatial regions defined by the discretization . The null distribution of the test statistic is obtained by the Monte Carlo method based on the assumption that the underlying distribution is either a non homogeneous Poisson distribution or a Bernoulli distribution .
This section describes a new test for distributional change that we call the density test . At a high level the density test works as follows :
1 . First , the baseline S is randomly partitioned into two halves S = S1 ∪ S2 . S1 will be used for modeling and S2 for testing.1 By partitioning S into two exclusive halves , we make sure that the model is independent of the data that we use to calculate the test statistic .
2 . Next , to compute the statistic δ , we first compute a kernel density estimator using S1 ; the resulting density function is denoted by KS1 . δ is defined based on the difference of log probability density of KS1 at S′ and at S2 ( hence the name “ density test ” ) . Intuitively , if S′ is very different from S , we expect that δ will take on a small negative value since KS1 has higher log probability density at S2 than at S′ . On the other hand , if S′ does not differ much from S , we expect that the value of δ will be reasonably large ( if S′ is comparable to S2 or S′ is even “ closer ” to S1 than S2 ) .
3 . Due to the Central Limit Theorem , we know that ∆ has a normal limiting distribution . After deriving the mean and variance of ∆ , we perform a standard null hypothesis test . Depending on whether the observed δ is significantly far out in the tail of ∆ , we either reject the null hypothesis and declare change , or we declare noChange since we do not have strong enough evidence to reject the null hypothesis .
While the above describes the basic outline of the density test , a few points regarding some specifics of the test bear further discussion in the remainder of the paper . First , we consider the problem of learning a kernel model for S1 . Specifically , we discuss why classical fixed bandwidth methods are a poor choice , and propose a data dependent learning algorithm for computing a more appropriate kernel model . Second , we consider a few issues associated with computing δ . Third , we show how to obtain the null distribution of δ by applying the Central Limit Theorem . A bootstrap resampling technique is proposed to estimate the variance of the null distribution . Fourth , we discuss why ( for maximum power ) the density test should actually be run in two different directions : checking for a change from S to S′ and also checking for a change from S′ to S . 1There is a tradeoff between |S1| and |S2| . A larger |S1| gives more accurate density model , but the variance of the null distribution will increase due to the smaller size of |S2| ( see Section 4 ) . Currently we use two equal sized halves . Determining an optimal partitioning is an avenue for future research .
3 . DENSITY VIA GAUSSIAN KERNELS
In order to compute δ , the density test makes use of a kernel density estimator ( KDE ) , which is one of the most common non parametric approaches for learning a data distribution . A KDE approximates the data distribution via superposition of many individual kernel functions at points sampled from the distribution that is being modeled . By allowing each sample to spread its density to nearby areas of the data space , kernel estimators smooth out the contribution of each observed data point over a local neighborhood . Formally , let x1 , . . . x|S1| be the data points in data set S1 . If S1 is smoothed using a KDE , then the estimated density at any point s is :
KS1 ( s ) = Xxi∈S1
1
|S1|
G(Σxi , s − xi )
( 1 )
In this equation , G is the kernel function . One kernel is the is centered at each of the data points in S1 . Σxi bandwidth or spread of the kernel function centered at xi , and in the case of a k dimensional Gaussian kernel ( which is used by the density test ) , Σxi is a k × k positive definite co variance matrix .
It is generally acknowledged that in practice , the quality of a kernel estimate depends less on the shape of kernel defined by G than on the bandwidth of each kernel . Unfortunately , in defining the density test we faced a significant technical hurdle with respect to choosing the kernel bandwidth : virtually all applications of the KDE method ( and almost all papers in the literature ) make the implicit assumption that all kernels in the model share the same bandwidth ; that is , Σxi = Σxj for any ( xi , xj ) pair . Since the power of the density test fundamentally relies on the quality of the kernel model and its ability to determine when data are abnormal , such an assumption is very problematic . Different portions of the data space tend to have different characteristics in “ real life ” multi dimensional distributions . Often , there is a clear boundary past which it is clear from the data that no density exists ; kernels close to the boundary should have bandwidths that project no density past this boundary . Kernels in an occupied but sparse portion of the data space should have large bandwidths . Kernels in dense regions of the data space should have tighter bandwidths . Visualizations of high dimensional data may show “ spokes ” radiating out in different directions from a central hub in the data . In such a situation , the bandwidth within each spoke should be elongated along the spoke but not allow the projection of density into the empty areas within the spokes .
Given the dearth of appropriate methods for modeling such real life distributional characteristics , we choose to use a variation on the standard statistical method of maximum likelihood estimation to choose the kernel bandwidths that were most likely to have produced the data . Rather than attempting to maximize the likelihood ( or log likelihood ) directly as is usually done , we instead attempt to maximize the so called “ pseudo log likelihood ” of the model over all possible choices of each Σxi , defined as :
L = Xxj ∈S1 log2 4 Xxi∈S1∧i6=j
1
|S1| − 1
G(Σxi , xj − xi)3 5
Note that the pseudo log likelihood is nothing but the loglikelihood of S1 over the model , with the constraint that it is known that no data point was generated by the kernel
( 2 ) centered at it . Thus , the probability that a data point is generated by itself is zero . We do not maximize the loglikelihood directly because each data point occurs exactly on the center of a kernel . Thus , a model constructed via a direct maximization of the log likelihood would simply associate a zero bandwidth kernel with each data point , so that G(Σxi , xi − xi ) is always infinity .
To perform this maximization , we derive an ExpectationMaximization ( EM ) algorithm . Since the kernel model with Gaussian kernels can be viewed as an instance of a Gaussian Mixture Model ( GMM ) with |S1| components , we can use a simple variation on classical Gaussian EM in order to learn the “ optimal ” bandwidths . Actually , applying EM to learn the bandwidths is somewhat simpler than applying EM on a general GMM because the centroids of the Gaussian components are fixed at the data points of S1 , and all Gaussian 1 |S1| . Thus , these parameters components have equal weight do not need to be learned in classical GMM .
The remainder of this section assumes a basic understanding of the EM framework , and how it is used to learn a GMM . Many excellent tutorials cover this subject ( Bilmes’ tutorial [ 5 ] is one of the best known and most complete ) .
Assume S1 has dimensionality k . Let ω1 , . . . ω|S1| be the Gaussian kernels centered at x1 , . . . x|S1| respectively . The update rule for the bandwidth of the kernel associated with data point i is :
Σxi = Pxj ∈S1
P ( ωi|xj)(xj − xi)(xj − xi)T
( 3 )
P ( ωi|xj )
Pxj ∈S1
In Equation ( 3 ) , P ( ωi|xj ) is the probability that xj is generated by kernel associated with xi . This probability is often referred to as the “ soft membership ” of point xj in Gaussian component ωi . The soft membership is computed with the formula given in Equation ( 4 ) . The second line of the equation is customized to our particular situation , where we take into account the constraint that the jth data point cannot be generated by the kernel centered at that data point . Note that Σ|S1| i=1 P ( ωi|xj ) = 1 still holds for all j ’s , j = 1 , 2 , . . . |S1| :
8< :
P ( ωi|xj ) =
P ( ωi|xi ) = 0 p(xj|ωi ) t=1 p(xj|ωt )
Σ|S1| i , j = 1 , 2 , . . . |S1| , i 6= j i = 1 , 2 , . . . |S1|
( 4 ) In this equation , p(xj|ωi ) is the density of the ith Gaussian kernel at the point xj and it is calculated by Equation ( 5 ) . Again , the second line in the equation is given because of the special pseudo log likelihood objective function :
8>< > : p(xj|ωi ) =
1
( 2π)k/2|Σxi |1/2 exp(− i , j = 1 , 2 , . . . |S1| , i 6= j
1 2
( xj − xi)T Σ−1 xi ( xj − xi ) ) p(xi|ωi ) = 0 , i = 1 , 2 , . . . |S1|
( 5 ) Now we can summarize how we optimize the bandwidth of the Gaussian kernels in Algorithm 1 . According to this algorithm , we stop the computation whenever the iteration number exceeds the maximum iteration number allowed , or when the objective function L converges toward its optimal value . We decide that the objective function has converged if the fractional difference of two consecutive L values is less than φ . In our implementation , φ = 0.01 and MaxIteration = 100 .
Algorithm 1 LearnBandwidth(S1 , MaxIteration , φ ) 1 : t = 0 2 : while t < MaxIteration do 3 : 4 :
Compute density p(xj|ωi ) for all i , j by Equation ( 5 ) Compute soft membership P ( ωi|xj ) for all i , j by Equation ( 4 ) Compute bandwidth Σxi for all i by Equation ( 3 ) Compute the objective function Lt by Equation ( 2 ) if Lt−Lt−1
< φ then
5 : 6 : 7 : 8 : 9 : 10 : 11 : end while end if t + +
Lt−1 break
Is This Method Effective ? Ultimately , the answer to this question is determined by the accuracy of the resulting change detection test , but it is easy to give some anecdotal evidence of how effective the KDE resulting from the EM method described above is .
For example , consider the following simple experiment over the 24 dimensional Streamflow data set ( see Section 6 for a description of this data set ) . We first sample 1000 points from the distribution associated with the data set , and then learn two different kernel models from those 1000 points . The first is learned using Scott ’s rule [ 17 ] , which is a standard , uniform bandwidth selection method . The second is learned using the EM algorithm . true distribution
10
5
0
−5
−10
−10
10
5
0
−5
−10
−10
10
5
0
−5
−10
−10
−8
−6
−4
−2
0
2
4
6
8
10
Scott ’s bandwidth
−8
−6
−4
−2
0
2
4
6
8
10 optimal EM bandwidth
−8
−6
−4
−2
0
2
4
6
8
10
Figure 1 : Samples from the original distribution ( top ) , a KDE using Scott ’s bandwidth ( center ) , and a KDE learned using our EM method ( bottom ) .
We then sample three additional 1000 point samples . One from the original distribution , one from a standard bandwidth estimator , and one from our EM estimator . Projections of these three samples onto two dimensions ( the 23rd and 24th dimensions of the Streamflow data set ) are shown in Figure 1 . It is very clear from the three plots that a great deal of information regarding the original distribution has been lost through the application of Scott ’s estimator . Sampling from the resulting KDE seems to have created a large and shapeless “ blob ” of points . On the other hand , the EM estimator seems to do an excellent job of preserving the characteristics of the original distribution . The experiments in Section 6 will show how sensitive the resulting KDE can be when it is used to detect changes in high dimensional data .
4 . THE TEST STATISTIC
In the density test , δ is defined using the log probability density of KS1 at S′ and at S2 . Specifically , δ is simply the difference between the log likelihood ( LLH ) of S′ and the scaled log likelihood of S2 under the inferred density function KS1 :
δ = LLH(KS1 , S′ ) −
|S′| |S2|
× LLH(KS1 , S2 )
( 6 )
Since both S2 and S1 are from FS , if S′ is quite different from S2 , KS1 will be more likely to generate S2 than to generate S′ , which will result in an extreme small negative value for δ . Thus , a hypothesis test using the above test statistic is a lower one sided test .
This particular test statistic is chosen for three reasons :
1 . δ calculated under the null hypothesis is a sample from
∆ , which has a normal distribution ( Section 411 )
2 . The expected value of mean of ∆ is always zero , which means that ∆ is always centered at the origin ( Section 412 )
3 . The variance of ∆ can be estimated in a robust fashion
( Section 413 )
For these reasons , the density test is easily applicable since the null distribution has a known parametric distribution and only one parameter must be estimated . 4.1 Normality of ∆
In this subsection , we consider the distribution function of ∆ in detail .
411 Applying Central Limit Theorem
We can apply the Central Limit Theorem to argue for the normality of ∆ by expanding the formula for the test statistic δ :
δ = LLH(KS1 , S′ ) −
|S′| |S2|
× LLH(KS1 , S2 )
−
|S′| |S2|
× log( Yy∈S2
KS1 ( y ) )
KS1 ( y)9= ;
1
= log8< Yy∈S ′ : log Xx∈S1 = Xy∈S ′ − Xy∈S2
|S′| |S2|
G(Σx , y − x )
|S1|
× log(Xx∈S1
1
|S1|
G(Σx , y − x ) )
( 7 )
Under the null hypothesis , S′ and S2 are two sets of iid samples from FS . That means S′ and S2 are sample points from a set of independent random variables whose probability distribution is FS . Assume S′ is generated by the set of random variables ˘T1 , T2 , . . . , T|S ′|¯ , and S2 is generated by the set of random variables ˘T|S ′|+1 , T|S ′|+2 , . . . , T|S ′|+|S2|¯ .
Beginning with Equation ( 7 ) , the random variable used to produce δ can then be denoted as :
The expected value of ∆ is given by :
E [ ∆ ] = E [ ∆1 − ∆2 ]
= E [ ∆1 ] − E [ ∆2 ]
|S ′|
−
|S ′|+|S2|
Xi=1 log Xx∈S1 Xi=|S ′|+1 Xi=1 f ( Ti ) −
|S ′|
∆ =
= where ,
1
|S1|
G(Σx , Ti − x )
|S′| |S2|
× log(Xx∈S1 Xi=|S ′|+1 g(Ti )
|S ′|+|S2|
1
|S1|
G(Σx , Ti − x ) )
( 8 )
|S ′|+|S2|
Xi=|S ′|+1 g(Ti)3 5
E [ g(Ti ) ]
|S ′|
|S ′|
= E2 Xi=1 4 Xi=1 f ( Ti)3 5 − E2 4 Xi=|S ′|+1 = |S′| × µ − |S2| × E» |S′|
E [ f ( Ti ) ] −
|S ′|+|S2|
|S2|
=
× f ( Ti)}–
= |S′| × µ − |S2| ×
= 0
|S′| |S2|
× µ f ( Ti ) = log Xx∈S1
1
|S1|
G(Σx , Ti − x )
413 Variance of ∆
Similarly we can derive the variance . Assuming Var [ f ( Ti ) ] =
σ2 , the variance of ∆ is given by : g(Ti ) =
× f ( Ti )
|S′| |S2|
It is well known that if we apply a measurable function to a random variable , the result will still be a random variable . Since both f and g are measurable functions , both f ( Ti ) and g(Ti ) are random variables . Let :
∆ = ∆1 − ∆2
|S ′|
∆1 =
∆2 = f ( Ti )
|S ′|+|S2|
Xi=1 Xi=|S ′|+1 g(Ti )
( 9 )
( 10 )
( 11 )
Given this , we can make the following three observations with respect to ∆1 :
1 . Since all T ′ i s , i = 1 , 2 , . . . follow the same distribution FS , it follows that all f ( Ti)′s have an identical distribution f ( FS ) .
2 . Since all T ′ i s , i = 1 , 2 , . . . are independent random vari ables , it follows that all f ( Ti)′s are independent .
3 . ∆1 is the sum of |S′| independent and identicallydistributed random variables . Formally , ∆1 = f ( T1 ) + f ( T2 ) + · · · + f ( T|S ′| ) .
The Central Limit Theorem can then be applied , and as a result , we know that ∆1 approaches a normal distribution – in practice , this happens quite quickly , with only a few dozen samples . A similar argument holds for ∆2 , which also has a normal limiting distribution . In Equation ( 10 ) and Equation ( 11 ) , all T ′ i s , i = 1 , 2 , . . . , |S′| + |S2| are independent , so ∆1 and ∆2 are independent and ∆ follows a normal limiting distribution as well .
412 Mean of ∆
The normal distribution has two parameters : its mean and variance . To calculate the mean , assume E [ f ( Ti ) ] = µ .
Var [ ∆ ] = Var [ ∆1 − ∆2 ]
= Var [ ∆1 ] + Var [ ∆2 ]
=
|S ′|
|S ′|
|S ′|+|S2|
Xi=1
Xi=1
Var [ f ( Ti ) ] +
= Var2 4 f ( Ti)3 5 + Var2 4 Xi=|S ′|+1 = |S′| × σ2 + |S2| × Var» |S′| |S2|«2 = |S′| × σ2 + |S2| ×„ |S′|
|S2|
|S ′|+|S2|
Xi=|S ′|+1 g(Ti)3 5
Var [ g(Ti ) ]
× f ( Ti)–
× σ2
= ( |S′| +
4.2 Estimating σ2
|S′|2 |S2|
)σ2
As long as we know the value of σ2 , we have the exact null distribution . Unfortunately , this value is unknown and must be estimated .
The first idea that comes to mind is to estimate σ2 with the unbiased estimate |S2| |S2|−1 Var [ f ( S2) ] . Unfortunately , this is a bad idea . If this estimator is less than the true σ2 , the estimated distribution of ∆ will improperly shrink towards the mean . This will introduce an additional type I ( false positive ) error , that must be quantified .
In order to define an estimator V such that Prˆσ2 > V˜ =
β , we use the bootstrap percentile method [ 10 ] . Rather than constructing a two tailed confidence interval , we need only compute the one sided ( 1 − β ) upper confidence limit on σ2 and then use this upper limit as the estimator of σ2 . Algorithm 2 gives the process , which introduces an additional type I error of β .
There are now two sources of type I error . First , δ could be far out in the tail of null distribution , even if it is a sample from ∆ . Assume this error is α . Second , we could underestimate the variance of ∆ by using any particular estimate . This error is denoted by β in Algorithm 2 . A bound on the type I error is given by p = α + β .
For a user supplied p , there is a tradeoff between α and β .
Algorithm 2 EstVar(V , S2 , estSize , β ) 1 : Initialize the array Est 2 : for t = 1 to estSize do 3 : 4 : 5 : end for 6 : V ← [ estSize × ( 1 − β)]th percentile of Est 7 :
ˆVar [ ∆ ] ← ( |S′| + |S ′|2
Bootstrap resample R from S2 , where |R| = |S2| Est[t ] ← S2
|S2|−1 Var [ f ( R ) ]
|S2| )V y t i s n e D
0.5
0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0 10
5
0
5
10
Figure 2 : Two distributions that must be checked for deviation in both directions , because one is covered by the other .
Fortunately , it is easily possible to choose α and β such that α + β = p and the power of the resulting test is maximized . To do this , we consider each possible β value ( each of which corresponds to a different bootstrapped variance ) , and compute the resulting cutoff value for declaring a change result . Over all possible β values , we choose the resulting cutoff that is most aggressive , and use that during our actual test . Algorithm 3 gives the process to find such a critical value . stepSize − 1 , M is the number of ( α , β ) pairs p
Algorithm 3 CriticalValue(p , stepSize ) 1 : M = 2 : αi = i × stepSize , i = 1 , 2 , . . . , M 3 : βi = p − αi , i = 1 , 2 , . . . , M 4 : Let C be an array of critical values 5 : for i = 1 to M do 6 : 7 : 8 : end for 9 : Cmax ← largest value in array C .
Run EstVar(V , S2 , estSize , βi ) to obtain ˆVar [ ∆ ] . Find c such that P ( ∆ ≤ c ) = αi , C[i ] = c .
4.3 Applying The Full Test Procedure
In this subsection , we briefly review the full test procedure . First , S is partitioned into S1 and S2 . Then EM is used to learn the kernel model over S1 . Next we calculate the value of δ for a given pair of S2 and S′ through Equation ( 7 ) . Second , we obtain the critical value Cmax through Algorithm 3 . Third , we compare δ with this critical value . If δ < Cmax , we will declare a change . Otherwise , it means δ is not significantly small and we will declare noChange .
5 . RUNNING IN TWO DIRECTIONS
The density test of Section 4.3 is complete , and can be used exactly as described to check for a change of distribu tion . However , the power of the test may be substantially increased if it is run twice , once in each direction . This is because the test is not symmetric , and change from S′ to S may be more obvious than change from S to S′ . For example , imagine that FS is a uni dimensional Gaussian , while FS ′ is a uni dimensional mixture of low variance Gaussians , where Var(FS ) = Var(FS ′ ) and E(FS ) = E(FS ′ ) . This is illustrated in Figure 2 , where FS ′ is a mixture of three Gaussians . A typical sample from FS ′ will have data points located only in relatively high density regions of FS , which means that the δ statistic may be useless for comparing FS and FS ′ . However , a large enough sample from FS will likely have data points located in the low density troughs between the peaks in FS ′ . This renders a test for distributional change from S′ to S far more sensitive than a test of change from S to S′ .
In order to remove the directionality , we run the test twice . First , the test is run exactly as described in Section 4.3 , except that the false positive rate is held at p/2 . If no change is observed , the test is run a second time in exactly the same way , except that the roles of S and S′ are reversed . Running the test twice with a false positive rate of p/2 for each run ensures that the overall false positive rate is less than p .
6 . EXPERIMENTS
This section details a set of experiments designed to test the utility of the density test , as well as two other available tests for multidimensional data . We describe three sets of experiments , each of which is designed to address one of the following questions :
1 . Is the false positive rate of our test correctly governed by the input parameter p ?
2 . How does the power ( false negative rate ) of our approach compare with that of the other two alternative methods , the cross match test and kdq tree test ?
3 . How scalable is our approach compared with the cross match test and kdq tree test ?
Datasets . Our tests make use of 13 experimental data sets , which are generated from the 13 real data sources described in Table 1 . The dimensionality is indicated after each data source .
We require that the size of each data source be reasonably large so that we can sample with replacement in order to obtain a true multivariate probability distribution without too many duplicate values . Among the 13 data sources , five have relatively small size : CAPeak , Bodyfat , Boston , FCATread and FCATmath . For these small sized data sources , we “ bump up ” the data set size to 20,000 points while maintaining the distributional characteristics of the data as follows . We first randomly draw a sample point A from the small sized data source . We then sample five points , A1 to A5 ( with replacement ) from A ’s five nearest neighbors . A and A1 to A5 are averaged to produce a new data point . We repeat the process 20,000 times .
For our experiments , we divide the data sources into 2 groups , the low D group and the high D group . The low D group consists of data sources from 3 to 10 dimensions . The high D group consists of data sources from 15 dimensions to 26 dimensions . All three change detection methods are tested using |S| = |S′| = 850 for the low D group . 850 is
Table 2 : False positive % on low D group data sets . test density cross match kdq tree average D1 D2 D3 D4 D5 D6 6 10 8
3 11 6
8 7 3
3 7 7
8 8 1
5 8 5
4 7 5
Table 3 : False positive % on high D group data sets . average D7 D8 D9 D10 D11 D12 D13 test density kdq tree
6 1
4 1
7 0
3 0
8 0
10 0
2 4
8 0 chosen because this is the largest size that is computationally feasible using the cross match test . However , for the high D group , more samples are required to accurately detect distributional change – a consequence of the well known “ curse of dimensionality ” . For this group , |S| = |S′| = 3500 is used , and the cross match test is not evaluated . 6.1 Experiment One : False Positive Rate
To test whether the various methods can correctly control the false positive rate , we sample N ( 1700 or 7000 ) data points ( with replacement ) from a data space source and call these N data points an instance . We repeat this sampling 100 times to obtain 100 instances . Because an instance is sampled with replacement from a finite population , each of the data points in an instance is identically distributed and any split of the instance in two ( without looking at the data ) will result in two subsets with the same generative distribution .
To estimate the false positive rate of each of the change detection methods over a data set , we split each instance evenly and run the various tests . The change detection test either says there is a distributional change in this instance , or says there is not any distributional change in this instance . The false positive of a test can be estimated by computing the fraction of the time that the test refutes the null hypothesis over the 100 instances of a data set .
The p value is 0.08 for all the three methods . In the density test , estSize = 4000 in Algorithm 2 . In Algorithm 3 , p = 0.04 ( due to the two directional test ) and stepSize = 0002 In the kdq tree test , we use the same parameter settings as in the original paper [ 8 ] . All experiments were run on an Intel Xeon machine with dual CPU of 2.8GHz , and 5GB of RAM . We keep this setup for all the experiments throughout this section .
The false positive results are reported in Table 2 for lowD group and in Table 3 for high D group . In those tables , Di represents the experiment data set created out of the ith data source given in Table 1 . The average false positive rate over 13 experiment data sets is given in the “ average ” column of the tables . Discussion . These results show that all the three change detection methods have an average false positive rate at or less than the 8 % allowed by the supplied p value , and hence a positive result from any of the tests seems to be a safe indicator of distributional change . Both the density test and the cross match test have a false positive rate very close to the desired p value , and the kdq tree test tends to be rather conservative .
We have also run similar tests at other p values , and find that the density test also correctly controls the false positive rate in this case . For p = 0.04 and p = 0.06 , the average false positive rate over all 13 data sets was 3.5 % and 4.7 % respectively . 6.2 Experiment Two : Power Experiment Setup . In order to test the false negative rate of the various tests ( aka the “ power ” ) , it is necessary to create an instance hS , S′i such that S and S′ are samples from FS and FS ′ respectively , where FS ′ is not equivalent to FS . Because any distributional change in the real world takes place gradually , if we take a snapshot of the distributional change at some moment , FS ′ will be a mixture of distribution FS and a different distribution FC . Thus , we choose to model distributional change by creating FS ′ by sampling from some FC with a probability of L and sampling from FS with a probability of 1 − L for some parameter L . Unless otherwise specified , FS is the data source described in the beginning of Section 6 .
We consider five different types of distributional change , created by five different methods for defining FC . Three of the changes take place in the full dimensional space , and two are single dimensional changes . FC is defined as follows .
Full Dimensional Changes gauss : In this case , FC is obtained by sampling from the original data set and adding Gaussian noise to all the dimensions of the data source . gmm : In this case , FC is a GMM which has three equalweight components . The centroids of the Gaussian mixture model are three outliers chosen from the original data source . The covariance matrix uses the variance of the subset on the diagonal . cluster : We divide the data source into two clusters by a Kmeans clustering algorithm . FS is the cluster of larger size , and FC is the cluster of smaller size .
Single Dimensional Changes add1D : In this case , FC is created by adding a standard 1 D Gaussian variable to a single randomly chosen dimension of the original data set . In each instance of the changed data set a random dimension is selected . scale1D : Here FC is created just as in the previous case , except that the value of the randomly selected dimension is multiplied by two .
For each data source , we create five experimental data sets , with each one corresponding to one type of distributional change . Each experimental data set contains 100 individual instances . The parameter L is chosen based upon the characteristics of the data and the type of change . Although the value of L will not affect the fairness of experiment because all the three detection methods are tested on the same sets of data , we still need to carefully choose L in order to make the experiment informative : it should not be too easy to detect the change , nor should it be too difficult . We calibrate the value of L so that the cross match test ( for low D group ) and the kdq tree test ( for high D group ) can detect around 50 % of the changes .
The false negative rates over the low D group are reported in Table 4 . The false negatives of high D group are reported in Table 5 . In order to make our experiments reproducible , we give the values of parameter L for each type of change over all the 13 data sources in Table 6 .
Table 4 : The false negative ( % ) of low D group . changes test density gauss cross match kdq tree density gmm cross match kdq tree density cluster cross match kdq tree density add1D cross match kdq tree density scale1D cross match kdq tree avg D1 D2 D3 D4 D5 D6 0 44 79 1 44 21 1 41 0 29 51 87 20 48 90
0 66 87 0 56 62 0 40 0 27 60 85 23 68 68
0 47 23 0 46 2 0 36 0 33 56 20 2 44 82
3 45 64 0 34 0 0 67 78 36 62 82 17 49 76
0 29 77 1 53 51 0 51 0 27 52 96 20 55 66
0 47 0 0 24 0 1 49 0 26 77 76 2 49 7
1 46 55 0 43 23 0 47 13 30 60 74 14 52 65
Table 5 : The false negative ( % ) of high D group . changes test avg D7 D8 D9 D10 D11 D12 D13 density density
6 gauss kdq tree 55 0 gmm kdq tree 63 7 cluster kdq tree 69 28 add1D kdq tree 75 22 scale1D kdq tree 70 density density density
0 44 0 69 14 86 0 87 0 62
0 32 0 52 1 57 0 79 2 72
0 36 1 69 1 72 70 75 38 68
1 61 0 67 3 79 82 80 58 54
39 60 0 70 18 60 42 60 52 96
0 79 0 56 6 75 1 71 5 74
0 70 0 55 7 51 0 70 1 65
Discussion . Table 4 and Table 5 show that the density test is the most powerful out of the three methods compared . Though it is true that there was some variation from data set to data set , it seems clear that the density test is the most obvious , general purpose choice . For each of the five types of changes , the density test had the lowest average false negative rate . Depending upon the type of change , the kdqtree test and the cross match test were generally comparable on the low D group , with the former doing better on fulldimensional changes , and the latter doing better on singledimensional changes . 6.3 Experiment Three : Scalability Analysis
Our final set of experiments test ability of the three change detection methods to scale to larger data sets . Experiment Setup . We test the scalability of the methods in terms of their running time on data sets of different size . The experiment is run on the 24 dimensional data source Streamflow . We sample repeatedly ( with replacement ) from the Streamflow source to create a series of data sets of size 100 , 200 , 300 , . . . 1000 , 2000 , . . . 7000 .
We record the length of time required to run the change detection test to completion . The density test was implemented in Matlab . The cross match test was implemented in Matlab as well , with the matching algorithm that is required by the test as an external subroutine being implemented in C ( the C implementation was the one recommended by the designer of the cross match test ) . The kdq tree test is implemented in C . The results are reported in Table 7 . Discussion . The kdq tree has significant performance benefits over both the density test and the cross match test . Given that the kdq tree relies mostly on a computationally efficient recursive partitioning of the data space , this is not too surprising . However , the density test still scales to reasonable data set sizes ( see the discussion in the Conclusion Section of the paper ) . The cross match test is generally unusable past a few thousand data points .
We also point out that the density test has a very high , one time , fixed cost for the construction of the KDE . After this cost has been paid , the model can be saved and the test can be run repeatedly in a very small fraction of the time reported in Table 7 . To illustrate the fraction of the total time associated with the density test that is a onetime cost , consider Table 8 . This Table shows the fraction of the running time for the density test that is devoted to one time computation for each of the data set sizes . The average fraction is around 84 % . These results show that if the same algorithm has to be repeated multiple times , the computational cost of the density test is only about 4 times more than the the tree based method . For a dataset with 7000 points , the overall requirements are on the order of a minute . This makes it practical for a number of applications .
7 . RELATED WORK
In this section , we briefly review the three tests from the literature that are closest to our own . We also briefly discuss alternative methods for kernel bandwidth selection .
The first test , based upon Kulldorff ’s spatial scan statistic [ 12 ] , can be used to find significant spatial clusters in data ; the particular application that it was developed for is detecting disease outbreaks . This statistic is obtained by first partitioning the geographic space into cells which may be of irregular shapes . Then a zone ( denoted by z ) is defined as the combined region of any number of contiguous cells which can be enclosed by a circle . Denote the probability that an individual is a case within a zone by p , and use q to denote the probability associated with the region outside the zone . Kulldorff ’s test statistic is defined as the likelihood ratio of supz∈Z , p>q L(z,p,q ) , where L(z , p , q ) denotes the likelihood of observing the cases inside and outside zone z simultaneously . The null hypothesis for the resulting test is H0 : p = q , and the alternative hypothesis is H1 : p > q . The null distribution of the likelihood ratio test statistic is obtained by the Monte Carlo method based on the assumption that the points are generated by either an non homogeneous Poisson process or a Bernoulli process . Several research groups have looked at speeding this test [ 1 , 2 , 15 ] . supp=q L(z,p,q )
Though Kulldorff ’s test does check for a change of distribution , it looks for a very specific distributional change . As such , Kulldorff ’s test is not really suitable for generalpurpose change detection . Dasu et al . [ 8 ] proposed an information theoretic approach to detecting changes in multidimensional data streams that generalizes Kulldorff ’s test . Their approach depends on a spatial partitioning scheme ( called a kdq tree ) to partition the data space into small cells . Based on the data count in each cell , two empirical distributions are built , representing the reference window and the testing window respectively . The Kullback Leibler ( KL ) distance is used to measure the distance between the two empirical distributions . In order to measure the significance of the obtained KL distance , they use a non parametric bootstrap method .
A third test for multivariate distributional change from the statistics literature is the cross match test [ 16 ] . In the cross match test , every observation in S ∪ S′ is ranked along each dimension . The rank of an observation xi , denoted by ri , is a vector with k elements . Then the inter point distance δij between two observations xi and xj is defined to be the Mahalanobis distance between their ranks . Formally , the distance is given by δij = ( ri − rj)T M −1(ri − rj ) , where M is the sample variance covariance matrix of the ranks ri . After that , all the „ |S| + |S′|
2
« pairwise distances are used to construct an optimal non bipartite matching , ie a matching of the observations into disjoint pairs to minimize the total distance within pairs . The cross match statistic , A1 , is defined to be the number of pairs containing one observation from S and one from S′ . A1 is shown to have a restricted occupancy distribution . Furthermore , the conditional distribution of A1 given |S′| converges to the normal distribution .
Several data driven methods have been proposed for kernel bandwidth selection . The plug in rule [ 18 ] seems to be the most widely used . This method assumes fixed bandwidth for all the kernels and is not suitable when data exhibits local scale variations . The optimal bandwidth selection methods proposed by Silverman [ 19 ] and Wand and Jones [ 20 ] are of limited practical use for multidimensional data as the derivation of asymptotics involves multivariate derivatives and higher order Taylor expansions . We did not find any method in the the literature that associates different bandwidth for each kernel for more than three dimensional datasets . The method proposed by Comaniciu [ 7 ] allows for variable bandwidth but assumes that the range of data scales is known and chooses the bandwidth that is the most stable across scales for each data point .
8 . CONCLUSION
We have described a new test for distributional change called the density test , and evaluated the test via an extensive set of experiments . Across all of our experiments , the density test uniformly shows the most power compared to the available alternatives . In terms of running time , the density test is not the most efficient of the three methods tested , but it does easily scale to data sets that are thousands of points in size and is competitive . Furthermore , for an application where new data are repeatedly tested against the same baseline , the high one time cost associated with constructing the kernel model can be amortized across all runs of the test , lowering the computational cost by nearly 90 % .
9 . REFERENCES [ 1 ] D . Agarwal , A . McGregor , J . Phillips ,
S . Venkatasubramanian , and Z . Zhu . Spatial scan statistics : Approximations and performance study . In SIGKDD , 2006 .
[ 2 ] D . Agarwal , J . M . Phillips , and
S . Venkatasubramanian . The hunting of the bump : On maximizing statistical discrepancy . In SODA , 2006 . [ 3 ] C . Aggarwal . A framework for change diagnosis of data streams . In SIGMOD , pages 575–586 , 2003 .
[ 4 ] S . Bay and M . Pazzani . Detecting group differences :
Mining contrast sets . Data Min . Knowl . Discov . , 5(3):213–246 , 2001 .
[ 5 ] J . Bilmes . A gentle tutorial on the em algorithm and its application to parameter estimation for gaussian mixture and hidden markov models , 1997 .
[ 6 ] M . Breunig , H P Kriegel , R . Ng , and J . Sander . Lof : Identifying density based local outliers . In SIGMOD , pages 93–104 , 2000 .
[ 7 ] D . Comaniciu . An algorithm for data driven bandwidth selection . IEEE Transactions on PAMI , 25(2):281–288 , 2003 .
[ 8 ] T . Dasu , S . Krishnan , S . Venkatasubramanian , and
K . Yi . An information theoretic approach to detecting changes in multi dimensional data streams . In Interface , 2006 .
[ 9 ] A . P . Dempster , N . M . Laird , and D . B . Rubin .
Maximum likelihood from incomplete data via the em algorithm . JRSS Series B ) , 39(1):1–38 , 1977 .
[ 10 ] B . Efron and R . J . Tibshirani . An introduction to the Bootstrap , volume 57 of Monographs on Statistics and Applied Probability . Chapman and Hall , 1993 .
[ 11 ] E . Knorr , R . Ng , and V . Tucakov . Distance based outliers : Algorithms and applications . VLDB J . , 8(3 4 ) , 2000 .
[ 12 ] M . Kulldorff . A spatial scan statistic . Comm . in Statistics : Theory and Methods , 26(6):1481–1496 , 1997 .
[ 13 ] J F Maa , D . Pearl , and R . Bartoszynski . Reducing multidimensional two sample data to one dimensional interpoint comparisons . The Annals of Statistics , 24(3):1069–1074 , 1996 .
[ 14 ] R . Miller . Simultaneous Statistical Inference .
McGraw Hill , New York , 1966 .
[ 15 ] D . Neill and A . Moore . Rapid detection of significant spatial clusters . In SIGKDD , 2004 .
[ 16 ] P . R . Rosenbaum . An exact distribution free test comparing two multivariate distributions based on adjacency . JRSS Series B ) , 67(4):515–530 , 2005 .
[ 17 ] D . Scott . Multivariate Density Estimation:Theory ,
Practice and Visualization . Wiley Interscience , New York , 1992 .
[ 18 ] S . Sheather and M . Jones . A reliable databased bandwidth selection method for kernel density estimation . JRSS Series B , ( 53):683–690 , 1991 .
[ 19 ] B . W . Silverman . Density Estimation for Statistics and Data Analysis . Chapman and Hall , London , 1986 . [ 20 ] M . Wand and M . Jones . Kernel Smoothing . Chapman and Hall , 1995 .
[ 21 ] W K Wong , A . Moore , G . Cooper , and M . Wagner .
Bayesian network anomaly pattern detection for disease outbreaks . In ICML , pages 808–815 , 2003 .
Table 1 : The 13 experiment data sources .
Houses(9 D )
El Nino(5 D )
Data source CAPeak ( 3 D )
Description Contains 1,817 records of mountain peaks in California , and includes each peak ’s longitude , latitude and height . The data is from UCI KDD archive . It contains oceanographic and surface meteorological readings taken from a series of buoys positioned throughout the equatorial Pacific . we remove those data records with missing values and end up to have 93,935 records . We also remove the Spatio Temporal attributes from the dataset since the changes on these attributes are not meaningful . The data set is from CMU Statlib datasets Archive . It contains 20,640 observations on housing prices with 9 economic covariates . This data set and following two data sets , spruce and krummholz , are from UCI KDD Archive . These data sets describe the forest cover type for 30 x 30 meter cells . We extract the quantitative attributes of the datasets . The number of records are 35,754 , 211,840 and 20,510 for Ponderosa Pine , Spruce Fir and Krummholz respectively . Spruce(10 D ) Refer to the pine data set . Krummholz ( 10 D ) Refer to the pine data set . Bodyfat(15 D )
Pine(10 D )
Boston(16 D )
Batting(17 D )
The data set is from CMU Statlib datasets archive . It gives the estimates of the percentage of body fat determined by underwater weighing and various body circumference measurements for 252 men . The data set is from CMU Statlib datasets Archive . It consists of 506 records of Boston house price data , with corrections and augmentation of the data with the latitude and longitude of each observation . We remove 5 attributes that are irrelevant to finding interesting changes . This data set and the following pitching data set are from The Lahman Baseball Database . They contain pitching and batting statistics for Major League Baseball players from 1871 through 2005 . The number of records is 85,979 and 36,245 for batting and pitching respectively . Refer to batting data set .
Pitching(22 D ) Streamflow ( 24 D ) We create this data set by the source data from US Geological Survey ( USGS ) and National Climate Data Center ( NCDC ) . It contains 7305 records of the weather and stream flow status on some observation stations in CA state . We applied PCA reduction on the data set to reduce the dimensionality down to 24 . This data set and the following FCATmath dataset is from National Center for Education Statistics ( NCES ) and Florida Information Resource Network . we create the data sets by matching the regular FL schools’ information and the grade 3 ’s FCAT reading and FCAT math scores of year 2002 . The number of records is 1404 for FCATread and 1405 for FCATmath . refer to FCATread data set .
FCATread ( 25 D )
FCATmath(26 D )
Table 6 : The parameter L of every change on all the 13 data sources .
S2 S3 .2 .15 .15 .15
S5 S6 S4 parameterL S1 .2 .15 .1 .17 .11 .12 .2 .1 .1 .12 .14 .12 .12 .3 .12 .35 .25 .4 .25 .35 .18 .3 gauss gmm cluster add1D scale1D
.4
S7 .1 .04
S9 .08
S12 .05
S11 .19
S10 .068
S8 S13 .11 .05 .047 .067 .0435 .065 .058 .035 .03 .25 .3
.025 .25 .2
.11 .8 .8
.03 .1 .2
.12 .019 .035 .035 .08 .3 .3 .2
.25 .2
.3 .2
Table 7 : The running time on Streamflow data set . Time unit is seconds by default . Otherwise , m stands for minutes , and h stands for hours . 300 0.9 10
5k 7k 2m 3m 4m 114 146 15m 51m 2.4h 4.6h 8h 26h 0.3 13
500 100 2 0.2 1.2 31 0.01 0.02 0.03 0.06 0.08
600 700 800 900 7 cross match
200 0.4 4.6
5 65 0.2
2 46 0.1
5 88 0.2 kdq tree density
4k 91
2k 22
3k 52
1k 6
1 19 size
400
0.3
0.8
6k
10
2
3
7
Table 8 : Fraction of the time ( in % ) reported in table 7 that is devoted to one time computation that could be re used for multiple tests . average 100 200 300 400 500 600 700 800 900 1k 2k 3k 4k 5k 6k 7k 88
78
78
81
90
89
91
87
89
87
88
88
87
83
84
73
71
