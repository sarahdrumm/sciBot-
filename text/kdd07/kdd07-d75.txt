Local Decomposition for Rare Class Analysis
Junjie Wu1 , Hui Xiong2 , Peng Wu1 , Jian Chen1
1 Research Center for Contemporary Management , Key Research Institute of Humanities and Social Sciences at Universities , Tsinghua University , China ,
{wujj,wup2,chenj}@semtsinghuaeducn
2 MSIS Department , Rutgers , the State University of New Jersey , USA , hxiong@andromedarutgersedu
ABSTRACT Given its importance , the problem of predicting rare classes in large scale multi labeled data sets has attracted great attentions in the literature . However , the rare class problem remains a critical challenge , because there is no natural way developed for handling imbalanced class distributions . This paper thus fills this crucial void by developing a method for Classification using lOcal clusterinG ( COG ) . Specifically , for a data set with an imbalanced class distribution , we perform clustering within each large class and produce sub classes with relatively balanced sizes . Then , we apply traditional supervised learning algorithms , such as Support Vector Machines ( SVMs ) , for classification . Indeed , our experimental results on various real world data sets show that our method produces significantly higher prediction accuracies on rare classes than state of the art methods . Furthermore , we show that COG can also improve the performance of traditional supervised learning algorithms on data sets with balanced class distributions .
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications— Data Mining ; I52 [ Pattern Recognition ] : Design Methodology—Classifier Design and Evaluation
General Terms Algorithms , Experimentation
Keywords Rare Class Analysis , K means Clustering , Support Vector Machines , Local Clustering
1 .
INTRODUCTION
Classification provides insight into the data by assigning objects to one of several predefined categories . An emerg ing critical challenge for classification is to address so called “ imbalanced classes ” in the data . Specifically , people are interested in predicting rare classes in the data sets with imbalanced class distributions . For example , in the domain of network intrusion detection , the number of malicious network activities is usually very small compared to the number of normal network connections . It is crucial and challenging to build a learning model which has the prediction power to capture future network attacks with low false positive rates . Indeed , rare class analysis is often of great value and is demanded in many real world applications , such as the detection of oil spills in satellite radar images [ 20 ] , the prediction of financial distress in enterprises [ 33 ] , and the diagnoses of rare medical conditions [ 25 ] .
To meet the above challenge , considerable research efforts have been focused on the algorithm level improvement of the existing classifiers for rare class analysis . Two promising research directions are the use of re sampling techniques and cost sensitive learning [ 29 ] . These two methods indeed show encouraging performances in some cases by directly or indirectly adjusting the class sizes to a relatively balanced level . Nevertheless , in this paper , we reveal that the class imbalance problem is strongly related to the presence of complex concepts ( inherent complex structures ) in the data . For imbalanced data sets with complex concepts , it is often not sufficient to simply manipulate the class sizes . In fact , our experimental results show that adjusting the class sizes alone usually can improve the predictive accuracy of the rare classes slightly , but at the cost of seriously decreasing the accuracy of the large classes . As a result , we need to develop a classification method which follows two criteria .
1 . The ability to divide imbalanced classes into relatively balanced classes for classification .
2 . The ability to decompose complex concepts within a class into simple concepts .
Indeed , this paper fills this crucial void by designing a method for classification using local clustering ( COG ) . Specifically , for a data set with an imbalanced class distribution , we perform clustering within each large class and produce sub classes with relatively balanced sizes . Then , we apply traditional supervised learning algorithms , such as SVMs , for classification . Since the clustering is conducted independently within each class but not across the entire data set , we call it local clustering , which is the essential part of our
Research Track Paper814 COG method . By exploiting local clustering within large classes , we can decompose the complex concepts , eg , nonlinear separable concepts for linear classifiers , into relatively simple ones , eg , linearly separable concepts . Another effect of local clustering is to produce subclasses with relatively uniform sizes . In addition , for data sets with highly skewed class distributions , we further integrate the over sampling technique into the COG scheme and propose the COG with over sampling technique ( COG OS ) .
The merit of COG lies in three aspects . First , COG has the ability to divide imbalanced classes into relatively balanced and small sub classes , and thus provide the opportunities in exploiting traditional classification algorithms for better predicting rare classes . Second , similar to the resampling schemes , COG is not a “ bottom level ” algorithm but provides a general framework which can incorporate various existing classifiers . Finally , COG is especially effective on improving the performance of linear classifiers . This is noteworthy , since linear classifiers have shown their unique advantages , such as simplicity and understandability , higher executive efficiency , less parameters , less generalization errors [ 12 , 28 ] , in many cases .
We have conducted extensive experiments on a number of real world data sets . Our experimental results show that , for data sets with imbalanced classes , COG and COG OS show much better performances in predicting rare classes than two popular re sampling schemes as well as two state of the art rule induction classifiers without compromising the prediction accuracies of large classes . In addition , for data sets with balanced classes , we show that our COG method can also improve the performance of traditional linear classifiers , such as SVMs , by decomposing the non linear separable concepts into linearly separable ones .
2 . ALGORITHM DESCRIPTION
In this section , we first describe our methods : COG ( Classification using lOcal clusterinG ) and COG OS ( COG with Over Sampling ) . Then , we present the details about how to perform COG by a simple example . 2.1 COG and COG OS
In a nutshell , COG provides a general framework which can incorporate various linear classifiers and improve their classification performance on data sets with non linear separable concepts as well as imbalanced class distributions . As to COG OS , it is an extended version of COG , which integrates the over sampling technique into the COG scheme for the purpose of better predicting rare classes in data sets with extremely imbalanced class distributions .
P
Figure 1 shows the pseudo code of COG containing four phases . In Phase I , we employ K means clustering on class i ( i = 1 , 2,··· , C ) according to the user preset cluster number K(i ) , and change the instance labels of class i with the subclass labels provided by the K means clustering , thus form a C multi class data set with i=1 K(i ) subclasses . Since we do clustering inside every class ( if necessary ) but not across the entire data set , we call it local clustering . Phase II is dedicated to COG OS . In this phase , we replicate R(j ) times the instances of class j ( j = 1 , 2,··· , C ) , to form a more balanced data set . Phase III is straightforward ; that is , we build the model on the modified data set using a user specified linear classifier . Phase IV is simply for testing ; however , each instance from the test set will be assigned with a label
COG ( Classification using lOcal clusterinG ) Input :
TR : a training data set . TE : a test data set . LCF : a linear classifier , such as SVMs . K : a vector specifies the number of local clusters for each class .
R : a vector specifies the over sampling times for each class . ( for COG OS only )
Output :
CM : the model built on TR . CR : the prediction results . for class i=1 to C //C represents #classes clusterLabel(i)=Clustering(TR(i ) , K(i) ) ; TR(i)*=changeLabel(TR(i ) , clusterLabel(i) ) ; end for
Procedure : Phase I : local clustering 1 . 2 . 3 . 4 . Phase II : over sampling ( for COG OS only ) 5 . 6 . 7 . 8 . Phase III : training 9 . Phase IV : testing/predicting 10 . 11 . 12 . end for TR**=mergei=1,··· ,C ( TR(i)** )
CM=train(TR**,LCF ) ; for class i=1 to C
TR(i)**=replicate(TR(i)* , R(i) ) ; clusterLabel=predict(TE , CM ) ; predictLabel=convertLabel(clusterLabel ) ; CR=compareLabel(predictLabel , trueLabel(TE) ) ; ( for testing only )
Figure 1 : The COG Algorithm of a subclass which must be converted into the label of the corresponding parent class .
There are some points needed to be further addressed . Typically , we do local clustering and over sampling ( if necessary ) on different classes ; that is , in Phase I and II , the cluster number K(i ) is larger than 1 for the relatively large class i , while the replication ratio R(j ) is larger than 1 for the small or rare class j . In practice , we first assign K(i ) with a small number , eg , four , on the large class i ( i = 1,··· , C ) , then use R(j ) on the small class j ( j = 1,··· , C ) to adjust the data set to a relatively balanced situation .
Also , we must emphasize that COG and COG OS have more impact on the performances of linear classifiers . We know that linear classifiers have various merits such as simplicity and understandability , higher executive efficiency , less parameters , less generalization errors , and so on [ 12 , 28 ] . In contrast , although non linear classifiers such as SVMs with RBF kernel can find sophisticated boundaries , their parameters are typically hard to specify , and the use of non linear kernels can easily lead to overfitting [ 12 ] . Therefore , in this paper , we use SVMs with a linear kernel . Furthermore , nonlinear separable concepts in the data is a long standing challenge for the use of linear classifiers . To this end , COG and COG OS can strengthen the use of linear classifiers by decomposing the non linear separable complex concepts into linear separable ones . By contrast , as shown in our experimental results ( Section 4 ) , COG shows no consistent improvements on non linear classifiers such as decision trees and rule based learning algorithms .
While we use k means as the clustering scheme in COG , the choices of clustering algorithms in COG is not limited to k means . Any other clustering algorithm which can produce clusters with relatively balanced sizes , such as the EM algorithm , can also be used in COG . Finally , COG is efficient in terms of the computational performance . First , if K means
Research Track Paper815 class 2 class 3 class 1
0
0.2
0.4
0.6 Original Data
0.8
1
1
0.8
0.6
0.4
0.2
0
1
0.8
0.6
0.4
0.2
0
1
0.8
0.6
0.4
0.2
0
1
0.8
0.6
0.4
0.2
0
MMH
0
0.2
0.4
0.6 "Pure" SVMs
0.8
1
1
0.8
0.6
0.4
0.2
0
0
0.2
0.4
0.6 COG : Phase I
0.8
1
0
0.2
0.4
0.6 COG : Phase III
0.8
1
0
0.2
0.4
0.6 COG : Phase IV
0.8
1
Figure 2 : Illustration of the COG Procedure . is used for the local clustering , the time required in the clustering phase is modest — basically linear in the number of data points [ 29 ] . Then in the training phase , as the number of classes increases , the time required is where T is the time required for the training without local clustering . Since the number of relatively large classes in a data set is often very small , and in practice each K(i ) is usually assigned with a small number , eg , four , we can expect to keep the computational cost of COG in the same level as the original classifier . i=1 K(i ) × T
Q
C
2.2 An Illustration of COG
Here , we use a synthetic data set with three classes and the SVMs classifier to illustrate the process of COG . The sizes of the three classes are 133 , 60 and 165 respectively , and please note that the small class is non linear separable from the other two large classes , as shown in the “ Original Data ” subplot of Figure 2 . The SVMs tool we used here is LIBSVM [ 5 ] with the linear kernel .
First , we build the classification model by simply applying SVMs on the original data set , and the results are shown in the pure SVM subplot . In this subplot , the solid line represents the maximal margin hyperplane ( MMH ) learned by SVMs algorithm . One interesting observation is that the instances of the small class , ie , class 2 , have totally “ disappeared ” ; that is , they are all assigned to either class 1 or class 3 according to the only one MMH . This is due to the non linear separable concepts in the data .
Instead , we employ COG . First , we apply local clustering on class 1 and 3 respectively , given the cluster number is two . The clustering results can be seen in subplot “ COG : Phase I ” . That is , class 1 and class 3 are divided into two subclasses respectively by K means . Thus we obtain a modified data set with five relatively balanced and linear separable classes . Next , we apply SVMs on this five classes data set and get results shown in subplot “ COG : Phase III ” . As can be seen , more MMHs appear in the model , which enables the model to identify the instances of class 2 . Finally , for each instance , we convert its predictive label of some subclass into the label of the parent class . This is equal to delete the MMHs separating the subclasses derived from a same parent class , as indicated by the “ COG : Phase IV ” subplot . Therefore , by applying COG , we build up a more accurate model which can identify the instances from the small class among non linear separable concepts .
3 . COG FOR RARE CLASS ANALYSIS
In this section , we illustrate why COG is especially effective on predicting rare classes using an example . First , we generate synthetic data sets for three different scenarios : data with simple concepts , data with non linear separable concepts , and data with complex concepts . In this example , we again use LIBSVM [ 5 ] with the linear kernel as the classifier in COG and COG OS . Also , the synthetic data sets are two class data sets with two dimensions , and the sizes of the rare and normal classes are 14 and 136 , respectively . 3.1 Data with Simple Concepts
First of all , let ’s give an informal definition . We call that a concept is complex in data if the distributions of the instances from the two classes are too close to be separable by the linear classifiers . Therefore , in this scenario , we have two well separated classes which represent a rather simple concept in the data , as shown in “ Scenario I ” of Figure 3 . As can be seen , for this simple concept , pure SVM separates the rare and normal classes easily and precisely ( the solid line represents the maximal margin hyperplane learned by the SVMs algorithm ) . This implies that the rare class problem will be inapparent in the case of simple concepts . 3.2 Data with Non Linear Separable Concepts In Scenario II , we consider the case that data sets contain non linear separable concepts , which can seriously hinder the performance of linear classifiers . In the COG scheme , we exploit local clustering to divide non linear separable concepts into smaller linear separable concepts . In this way , traditional linear classifiers can still work well in this scenario . In Figure 3 , two subplots of “ Scenario II ” show the process of COG on handling non linear separable concepts . As can be seen in subplot II I , pure SVM cannot effectively identify the rare class , since the instances of rare class and normal class are very close . After applying COG , however , we can divide the large class into two subclasses and form a data set with three linear separable sub classes , which can be easily learned by SVMs , as shown in subplot II II .
Research Track Paper816 Scenario I
1
0.5
0
0
MMH rare class large class
0.5
1
I−I : "Pure" SVMs
Scenario II
1
0.5
0
0
Scenario III
1
0.5
0
0
0.5
1
II−I : "Pure" SVMs
0.5
III−I : COG
1
1
0.5
0
0
1
0.5
0
0
0.5
II−II : COG
1
1
0.5
0.5
III−II : OS
1
0
0
0.5
1
III−III : COG−OS
Figure 3 : Illustration of Various Scenarios .
3.3 Data with Complex Concepts
In this subsection , we consider a more complicated case that data sets contain complex concepts . In other words , the instances of the rare class are adjacent to the instances of the normal class in the data , as shown by subplots of “ Scenario III ” in Figure 3 .
In this scenario , COG alone seems not very helpful for rare class analysis . As can be seen in subplot III I of Figure 3 , COG cannot separate the rare class from one of the subclasses of the normal class . Also , in subplot III II , we can see that the traditional over sampling technique performs poorly for predicting rare class , since many instances of the normal class have been assigned to the rare class ( the replicative time is set to be 8 on the rare class to obtain a relatively balanced distribution ) . However , COG with the over sampling technique ( COG OS ) can help SVMs successfully isolate the over sampled instances of the rare class from the two sub classes partitioned by K means clustering on the normal class , as shown in the Subplot III III of Figure 3 .
Discussion . In summary , the rare class problem is strongly related to the presence of the complex concepts ( inherent complex structures ) in the data . The more complex the concepts are , the more significant the rare class problem is . By applying local clustering , COG can handle the rare class problem in the presence of the non linear separable concepts ; by further incorporating the over sampling scheme , COG OS can handle the rare class problem in the case of more complex concepts . 4 . EXPERIMENTAL EVALUATION
In this section , we present experimental results to validate the performance of the COG and COG OS methods on balanced and imbalanced classification problems . 4.1 The Experimental Setup Experimental Tools . We used four types of classifiers : support vector machines , Bayesian logistic regression , decision trees , and rule based classifiers . Their corresponding imple
Table 1 : Some Notations .
US : OS : COG : COG OS : COG with the over sampling scheme .
Under sampling scheme . Over sampling scheme . Classification using local clustering . mentations are LIBSVM [ 5 ] , BMR [ 1 ] , C4.5 [ 2 ] , and RIPPER [ 7 ] . In all the experiments , default settings were used except that the kernel type of LIBSVM was set to be linear . Thus we have two linear classifiers , ie , LIBSVM and BMR , and two non linear classifiers , ie , C4.5 and RIPPER .
Also , we applied K means , a widely used clustering scheme which tends to produce clusters with relatively uniform sizes , as the clustering method in our COG method . During the Kmeans clustering , for data sets with relatively small number of dimensionality , squared Euclidean distance was used as the proximity measure ; and for data sets with high dimensionality , however , the cosine similarity was used instead . This is due to the fact that Euclidean notion of proximity is not very meaningful for high dimensional data sets , such as document data sets . Note that for each data set , K means ran ten times and returned the best partitioning result .
Finally , some notations are given in Table 1 . The default classifier used in these schemes is SVMs . If other classifiers are used instead , e.g , BMR in the COG scheme , we will explicitly denote it by “ COG(BMR ) ” . Experimental Data Sets . For our experiments , we used a number of benchmark data sets that were obtained from different application domains . Some characteristics of these data sets are shown in Table 2 . In the table , CV — Coefficient of Variation [ 9]— shows the dispersion of the class distribution for each data set . In general the larger the CV value is , the greater the variability is in the data .
UCI Data Sets .
In our experiments , we used eight well known benchmark data sets from UCI Repository [ 26 ] . Among them two data sets , breast w and pima diabetes , are binary data sets from the medical domain . The breast w data set contains two types of results from real world breast
Research Track Paper817 Table 2 : Some Characteristics of Experimental Data Sets . Source
#objects #features #classes MinClassZize MaxClassSize
UCI Data Sets
Dataset § breast w pima diabetes letter
† optdigits page blocks † pendigits † satimage † vowel
UCI UCI UCI UCI UCI UCI UCI UCI
683 768
20000
3823/1797
5473
7494/3498 4435/2000
528/462
Document Data Sets k1b la12
WebACE
TREC
LIBSVM Data Sets fourclass german.numer splice
SVMguide1
LIBSVM LIBSVM LIBSVM LIBSVM
2340 6279
862 1000 1000 3089
9 8 16 64 10 16 36 10
21839 31472
2 24 60 4
2 2 26 10 5 10 6 11
6 6
2 2 2 2
239 268 734
376/174
28
719/335 415/211
48/42
60 521
307 300 483 1089
444 500 813
389/183
4913
780/364 1072/470
48/42
1389 1848
555 700 517 2000
CV
0.424 0.427 0.030
0014/0015
1.953
0042/0042 0425/0368 0000/0000
1.316 0.503
0.407 0.567 0.048 0.417
Notes : The numbers before and after “ / ” are for training and test sets respectively . § †
: 16 instances with missing data have been deleted . : These data sets have been split into training/test sets by the sources . cancer diagnosis , and the pima diabetes data set is about the information of whether the patient shows signs of diabetes according to the WHO criteria . The rest six data sets are frequently used by the pattern recognition community . letter , optdigits and pendigits are data sets containing the information of handwritings ; that is , letter has the letter information from A to Z , and the other two have the number information from 0 to 9 . The satimage data set contains the multi spectral values of pixels in 3 × 3 neighborhoods in a satellite image . The page blocks data set contains the information of five types of blocks from a document page layout . And the last data set vowel was designed for the task of speaker independent recognition of the eleven steady state vowels of British English .
Document Data Sets . We also used high dimensional document data sets in our experiments . The data set k1b was from the WebACE project [ 15 ] . Each document corresponds to a web page listed in the subject hierarchy of Yahoo! . The la12 data set was obtained from articles of the Los Angeles Times that was used in TREC 5 [ 30 ] . The categories correspond to the desk of the paper that each article appeared and include documents from the entertainment , financial , foreign , metro , national , and sports desks . For these two document data sets , we used a stop list to remove common words , and the words were stemmed using Porter ’s suffix stripping algorithm [ 27 ] .
LIBSVM Data Sets . Finally , we applied four binary data sets : fourclass , german.numer , splice and SVMguide1 from the LIBSVM repository [ 5 ] .
Please note that for any data set without an appointed test set , we did random , stratified sampling on it and had 70 % samples as the training set and the rest as the test set . 4.2 The Effect of COG and COG OS on
Imbalanced Data Sets
In this subsection , we show how COG can improve the performance of linear classifiers on imbalanced data sets . As discussed in Section 3 , the problem of imbalanced classes is related to the complex concepts in the data — such as nonlinear separable concepts for linear classifiers . The COG method is a natural solution to this problem : handling the non linear separable concepts as well as making the class sizes be relatively balanced . For data sets with highly imbalanced class sizes , such as binary data sets with rare classes ,
Table 3 : Sampled Data Sets .
Class
Sampling Ratio #instances
Data Set breast w pima diabetes fourclass german.numer splice
SVMguide1
1 2 1 2 1 2 1 2 1 2 1 2
0.20 1.00 0.20 1.00 0.20 1.00 0.20 1.00 0.10 1.00 0.05 1.00
48 444 54 500 62 555 60 700 49 517 53
2000
CV 1.14
1.14
1.13
1.19
1.17
1.34 we apply COG with the over sampling scheme ( COG OS ) . Specifically , for the large class of any binary data set , we did K means clustering on it , and set the cluster number consistently to be 4 ; and for the rare class , we did over sampling on it , and made the size be approximate to the average size of the partitioned large class . In this way , we can have much more balanced data sets . Also , the non linear separable concepts can be transformed into linearly separable concepts .
Also , we prepared the imbalanced data sets with rare classes via sampling on various binary data sets . Specifically , for each data set with two classes , we did random sampling on the small class to turn it into a rare class , then combined it with the original large class to form a sample data set . Detailed information of the samples can be found in Table 3 . We did sampling ten times for each data set and returned the average classification results for it , as shown in Table 4 . Finally , since SVMs shows best classification performance in many cases [ 8 ] , we used it as the benchmark classifier for all the experiments in this subsection .
Results by COG OS on Two class Data Sets . Table 4 shows the performance of COG OS(SVMs ) and pure SVM on six two class data sets . As can be seen , pure SVM assigned all the instances to the large class of data sets pima diabetes , fourclass and germannumer This indicates that pure SVM has no prediction power on rare classes for these three data sets . In contrast , COG OS can successfully identify more than 25 percent instances of the rare classes . Indeed , the F measure values of the rare classes by COG OS(SVMs ) are consistently higher than the F measure values produced by pure SVM for all six data sets , as indicated in Table 4 . For instance , for data sets SVMguide1 and
Research Track Paper818 Table 4 : Classification Results of Sampled Data Sets by COG OS ( SVMs ) .
Data Set breast w
Method SVMs
COG OS pima diabetes
SVMs
COG OS fourclass
SVMs
COG OS german.numer
SVMs
COG OS splice
SVMs
COG OS
SVMguide1
SVMs
COG OS
Precision
N/A N/A
N/A N/A
N/A N/A
N/A N/A
N/A N/A
N/A N/A
0.904 0.428 0.931
0.878 0.987 0.850 0.990
Class #repetitions #clusters Recall 0.871 0.986 0.907 0.982 0.000 #DIV/0! 1.000 0.344 0.949 0.000 #DIV/0! 1.000 0.606 0.971 0.000 #DIV/0! 1.000 0.317 0.915 0.314 0.929 0.493 0.874 0.069 1.000 0.913 0.956
0.921 0.239 0.940 0.289 0.938 0.270 0.950 0.261 0.976 0.365 0.998
1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2
0.902 0.699 0.958
N/A N/A
N/A N/A
N/A N/A
N/A N/A
N/A N/A
1 4
1 4
1 4
1 4
1 4
1 4
N/A N/A
3 1
2 1
2 1
3 1
3 1
9 1
F measure
0.872 0.985 0.873 0.986 N/A 0.949 0.373 0.940 N/A 0.948 0.646 0.964 N/A 0.959 0.270 0.927 0.298 0.933 0.344 0.910 0.104 0.988 0.518 0.976
Notes:1.For SVMs : t 0 . 2 . “ #repetitions ” means the replicative time of each instance during OS . 3 . “ #clusters ” means the preset cluster numbers for K means during COG . 4 . “ N/A ” : not applicable ; “ #DIV/0! ” : divided by zero . fourclass , COG OS(SVMs ) results in the increases of the F measure values by more than 04
Table 5 : Information of SVMguide1 Samples . 5
Sample ID 0.09 Sampling Ratio 99 Size of Rare Class 4 #clusters for Larger Class #repetitions for Rare Class 5 Note:Class sizes of the original data are 1089 and 2000 .
0.07 77 4 6
0.05 55 4 9
0.03 33 4 15
0.11 120
4 4
1
2
3
4
COG OS(SVMs )
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0 e r u s a e m F
” Pure ” SVMs
0.03
0.09
0.11
0.05
0.07
Sampling Ratio for the Small Class of ” SVMguidel ” Data Set
Figure 4 : The Effect of the Size of the Rare Class .
Another observation is that , for COG OS , the increase of the F measure value of the rare class is NOT at the high cost of the prediction accuracy of the large class . For example , for data sets breast w and fourclass , the F measure values of both large and rare classes by COG OS are higher than the F measure values produced by pure SVM . Also , for the rest four data sets , the F measure values of the large class by COG OS are just slightly smaller . This is acceptable since the rare class is usually the major concern in many real world applications .
In addition , we also investigate how the F measure value changes as the increase of the sampling ratio on the small class . As an example on the SVMguide1 data set , Table 5
Table 7 : COG OS vs . the Resampling Scheme .
Data Set breast w
Class 1(rare )
2 total pima diabetes
1(rare ) fourclass german.num splice
SVMguide1
2 total
1(rare )
2 total
1(rare )
2 total
1(rare )
2 total
1(rare )
2 total
COG OS
0.873 0.986 0.975 0.373 0.940 0.890 0.646 0.964 0.935 0.270 0.927 0.868 0.344 0.910 0.843 0.518 0.976 0.955
US
0.858 0.983 0.970 0.320 0.818 0.714 0.339 0.804 0.699 0.255 0.812 0.701 0.268 0.805 0.694 0.308 0.937 0.884
OS
0.834 0.981 0.967 0.331 0.828 0.727 0.346 0.809 0.704 0.258 0.835 0.731 0.349 0.905 0.835 0.397 0.959 0.923
Note : 1.Performances are measured by F measure . 2.SVMs was used as the only classifier . shows the information of various samples as the increase of the size of the rare class . Figure 4 shows the classification results on these samples . Please note that for each sampling ratio , we did sampling ten times and therefore had ten samples for each ratio . Then we applied pure SVM and COG OS on the ten samples respectively , and finally got ten results for each sampling ratio , as indicated by the box plots in Figure 4 . As can be seen , the F measure values by COG OS are consistently higher than the ones produced by pure SVM , no matter what the sampling ratio is . Moreover , COG OS performs much better than pure SVM when the rare class size is relatively small . However , as the increase of the size of the rare class , the performance difference is decreased .
Results by COG on Multi classes Data Sets . In addition to the experiments on the two class data sets , we also employed some multi classes data sets with imbalanced classes to validate the COG method . For these multi classes data sets , we simply used the COG scheme . Since the Euclidean distance is not very meaningful for the sparse doc
Research Track Paper819 Table 6 : Classification Results of Multi classes Data Sets by COG ( SVMs ) . page blocks ( CV=1.953 ) Class
#instances
COG(SVMs )
Precision
F measure
1 2 3 4 5
1 2 3 4 5 6
4913 329 28 88 115 5473
494 1389 142 114 60 141 2340 total k1b ( CV=1.316 ) Class
#instances total la12 ( CV=0.503 ) Class
#instances
1 2 3 4 5 6 total
1848 1497 1042 729 642 521 6279
Pure SVM
#clusters Recall 0.990 0.780 0.556 0.815 0.514 0.962
N/A N/A N/A N/A N/A N/A
Precision
0.971 0.839 1.000 0.917 0.900 0.962
Pure SVM
#clusters Recall 0.968 0.990 0.979 0.829 0.810 0.955 0.969
N/A N/A N/A N/A N/A N/A N/A
Precision
1.000 0.963 0.939 0.967 1.000 0.955 0.969
Pure SVM
#clusters Recall 0.921 0.757 0.663 0.465 0.672 0.329 0.709
N/A N/A N/A N/A N/A N/A N/A
Precision
0.561 0.912 0.759 0.896 0.794 0.917 0.709
F measure #clusters Recall 0.993 0.880 0.556 0.815 0.543 0.971
0.980 0.808 0.714 0.863 0.655 0.962
10 2 1 1 1
F measure #clusters Recall 0.974 0.993 1.000 0.886 0.905 1.000 0.982
0.984 0.976 0.958 0.892 0.895 0.955 0.969
3 9 1 1 1 1
0.977 0.898 0.833 0.917 0.950 0.971 COG(SVMs )
1.000 0.981 0.940 0.969 1.000 0.978 0.982 COG(SVMs )
0.985 0.889 0.667 0.863 0.691 0.971
0.987 0.987 0.969 0.925 0.950 0.989 0.982
Precision
F measure
F measure #clusters Recall 0.886 0.708 0.721 0.564 0.687 0.353 0.713
0.697 0.827 0.707 0.612 0.728 0.485 0.709
2 2 1 1 1 1
Precision
F measure
0.572 0.922 0.728 0.866 0.793 0.881 0.713
0.695 0.801 0.725 0.683 0.736 0.504 0.713
Dataset
Table 8 : Information of kddcup99data and the Modified Data Sets . #features #classes MinClassZize MaxClassSize 391458/223298 489914/289923 492895/286307 kddcup99data UCI KDD 494021/292300 UCI KDD 494021/292300 probe binary † UCI KDD 494021/292300
4107/2377 1126/5993 r2l binary
#objects
41 41 41
Source
52/39
5 2 2
†
CV0
1708/1634 1391/1391 1408/1356
Note : We deleted in the test set 18729 instances whose class labels are not present in the training set . †
: Modified binary data set of kddcup99data . ument data sets k1b and la12 , for the COG method , we used the CLUTO [ 19 ] implementation of K means on these two data sets with cosine similarity as the proximity measure . Table 6 shows the classification results by pure SVM and COG . As can be seen , for data set k1b , the F measure value for every class using COG is higher than that produced by pure SVM . Meanwhile , the results on the page blocks and la12 data sets show a similar trend as k1b . In summary , COG indeed can improve the prediction performance on rare classes , and this improvement is achieved without a big loss of the prediction performance on large classes .
4.3 COG OS vs . the Resampling Schemes
In previous sections , we mentioned that resampling is a widely used technique to improve the classification performance on imbalanced data sets . Here , we compare the performances of COG OS with two resampling strategies — under sampling and over sampling . Details of these two resampling methods can be found in various books [ 29 , 23 ] .
In this experiment , we also employed the six sampled data sets as shown in Table 3 . We set the sampling ratio for under sampling or over sampling to make the modified size of the rare class be approximate to the one of the large class , and the classifier we used here is SVMs . Table 7 shows the results . One observation is that , for all data sets in Table 7 , COG OS performs the best for the rare class , except for one data set : splice , on which COG OS and over sampling show comparable results . Another observation is even more encouraging ; that is , while obtaining excellent performances on the rare classes , COG OS also provides much higher predictive accuracies on the large classes , which has long been the “ choke point ” of the resampling schemes . This is not sur prising since the non linear separable concept in the data is usually the bottle neck of the class imbalance problem . By integrating the over sampling scheme , COG can further improve its ability to identify the instances from the rare class . In summary , compared to two widely used resampling strategies , COG OS shows appealing performances on handling non linear separable data with rare classes , yet keeps a much better performance on large classes .
4.4 COG OS for Network Intrusion Detection Here , we demonstrate an application of COG OS for network intrusion detection . For this experiment , we used a real world network intrusion data set , which is provided as part of the KDD CUP 99 classifier learning contest [ 3 ] , and now is a benchmark data set in the UCI KDD Archive [ 4 ] . The KDD CUP Data Set . The data set was collected by monitoring a real life military computer network that was intentionally peppered with various attacks that hackers would use to break in . Original training set has close to 5 million records belonging to 22 subclasses and 4 classes of attacks , ie , dos , probe , r2l and u2r , and still one normal class . In this experiment , we applied 10 % sample of this original set which is also supplied as part of the contest . We present results for two rare classes : probe and r2l , whose populations in the 10 % sample training set are 0.83 % and 0.23 % , respectively . The test set provided with the 10 % training set , however , has some new subclasses that are not present in the training data . So for the evaluation concern we deleted these new subclasses , and the resultant populations of probe and r2l in the test set are 0.81 % and 2.05 % , respectively . Table 8 shows the detailed information of these data sets . Note that we obtained the probe binary
Research Track Paper820 data set by making the probe class as the rare class , and the rest four classes as one large class . The other data set , ie , r2l binary , was prepared in a similar fashion .
Table 9 : Results on Modified Data Sets . probe binary rare class huge class total r2l binary rare class huge class total
0.798 0.998 0.996
SVMs RIPPER PNrule COG OS 0.806 0.998 0.996 SVMs RIPPER PNrule COG OS 0.262 0.991 0.983
0.881 0.999 0.998
0.496 0.993 0.986
0.360 0.992 0.984
0.884 N/A N/A
0.230 N/A N/A
Note : “ N/A ” means results were not provided by the source paper [ 17 ] .
Table 10 : Classification Accuracies by COG with Different Classifiers .
Data Set #clusters letter
N/A
2 4 6 8 optdigits
N/A
2 4 6 8 pendigits
N/A
2 4 6 8 satimage
N/A
2 4 6 8 vowel
N/A
2 4 6 8
SVMs 0.851 0.872 0.923 0.946 0.952 0.965 0.973 0.973 0.979 0.981 0.953 0.969 0.979 0.981 0.977 0.852 0.860 0.871 0.883 0.881 0.517 0.602 0.582 0.580 0.597
Classifier C4.5 0.862 0.846 0.858 0.854 0.856 0.858 0.880 0.855 0.866 0.860 0.921 0.920 0.927 0.923 0.920 0.854 0.841 0.857 0.850 0.847 0.517 0.392 0.385 0.370 0.346
BMR 0.750 0.762 0.812 0.844 0.850 0.949 0.958 0.970 0.972 0.973 0.901 0.937 0.964 0.962 0.967 0.834 0.837 0.841 0.862 0.863 0.448 0.517 0.541 0.491 0.513
RIPPER
0.839 0.821 0.826 0.818 0.812 0.874 0.840 0.816 0.805 0.771 0.925 0.918 0.917 0.897 0.867 0.854 0.855 0.850 0.848 0.847 0.468 0.312 0.370 0.314 0.251
Notes : 1.All classifiers used default settings except for SVMs : t 0 . 2.For K means , maxIteration=500 , repeat=10 .
The Benchmark Classifiers .
In this experiment , we apply four classifiers : COG OS(SVMs ) , pure SVM , RIPPER [ 7 ] , and PNrule [ 17 ] . For COG OS , the cluster number for the large class is 4 for each data set , and the replicative times of over sampling on the rare classes for probe binary and r2l binary are 30 and 120 , respectively . For SVMs , we set the parameters as : t 0 . Ripper and PNrule are two rule induction classifiers . RIPPER builds rules first for the smallest class and will not build rules for the largest class . Hence , one might expect that RIPPER can provide a good performance on the rare class . As to PNrule , it consists of positive rules ( P rules ) that predict presence of the class , and negative rules ( N rules ) that predict absence of the class . It is right the existence of N rules that can ease the two problems induced by the rare class : splintered false positives and error prone small disjuncts . These two classifiers have shown the appealing performance on classifying the modified binary data sets in Table 8 , and the PNrule classifier even shows superior performance [ 17 ] . To our best knowledge , we used the same source data as [ 17 ] and the pre process procedure for the modified data sets is also very similar to [ 17 ] . Therefore , we simply adopted the results of PNrule in [ 17 ] for our paper . improved non−improved
SVMs
100
90
80
70
60
50
40
30
20
10
0 let . opt . pen . sat . vow .
Data Set
)
%
( s t n e m e v o r p m
I y c a r u c c A h t i w s e s s a C l f o o i t a R
100
BMR
)
%
( s t n e m e v o r p m
I y c a r u c c A h t i w s e s s a C l f o o i t a R
90
80
70
60
50
40
30
20
10
0 let . opt . pen . sat . vow .
Data Set
Figure 5 : Ratio of the Classes with Accuracy Improvements by COG .
The Results . Table 9 shows the classification results by various methods on the probe binary data set . As can be seen , COG OS performs much better than pure SVM and RIPPER on predicting the rare class as well as the large class , while PNrule shows slightly higher F measure on the rare class . For data set r2l binary , however , COG OS shows overwhelming advantages among all classifiers . As indicated in Table 9 , the F measure value of the rare class by COG OS is 0.496 , far more higher than the ones produced by the rest classifiers . Meanwhile , the predictive accuracy of the large class by COG OS is also higher than that of pure SVM and RIPPER . This real world application nicely illustrates the effectiveness of COG OS — the combination of local clustering and over sampling schemes . We believe that COG OS is a prospective solution to the difficult classification problem induced by the non linear separable concepts and imbalanced class distributions . 4.5 The Effect of COG on Balanced Data Sets In the previous subsections , we have shown that COG and COG OS indeed can improve the prediction accuracies of the rare classes for imbalanced data sets . In this subsection , however , we would like to show COG is also applicable to data sets with balanced class distributions .
In this experiment , we used five balanced data sets with CV < 0.5 , ie , letter , optdigits , pendigits , satimage and vowel . Among them , four data sets have been split into training and test sets by UCI repository except for the letter data set . Four classifiers including SVMs , BMR , C4.5 and RIPPER were used for the purpose of comparison . The clustering method in the COG scheme is K means with Euclidean notion proximity , and the cluster number for each class in a data set is exactly the same , ranging from 2 to 8 . Results by COG with Linear Classifiers . Table 10 shows the experimental results on these balanced data sets . As can be seen , for linear classifiers SVMs and BMR , COG indeed can improve the classification accuracies no matter what the cluster number is . For instance , for the data set letter , the accuracies achieved by pure SVM and BMR are merely 0.851 and 0.750 respectively ( as indicated by the italic numbers ) . In contrast , COG with SVMs and BMR can increase the prediction accuracies of the rare classes steadily as the increase of the cluster number , and finally up to 0.952 and 0.850 respectively when the cluster number is 8 . Indeed , the resultant prediction accuracies are 10 % higher than the ones obtained by pure SVM and BMR .
Next , we take a closer look at the performance of COG in the class wise level . Table 11 shows the classification accuracies on the data set optdigits by pure SVM and COG .
Research Track Paper821 Table 11 : Classification Accuracies of optdigits in the Class wise Level .
1
2
3
4
5
6
COG(BMR ) Note : For COG , the cluster number is set to be 8 for every class .
Class SVMs
COG(SVMs )
BMR
0.994 1.000 0.972 1.000
0.967 0.989 0.940 0.978
0.960 0.994 0.977 0.989
0.934 0.973 0.918 0.967
0.989 1.000 0.972 0.967
0.989 0.995 0.978 0.973
7
0.989 0.994 0.978 0.989
8
0.950 0.950 0.911 0.961
9
0.920 0.954 0.902 0.948
10
0.956 0.956 0.939 0.961
)
% i
( n a G y c a r u c c A l a m x a M i
12
10
8
6
4
2
0
COG(SVMs ) COG(BMR )
0.93
0.92
0.91
0.9
0.89
0.88
0.87
0.86
0.85 y c a r u c c A n o i t a c fi i s s a l C letter vowel pendigits Data Set satimage optdigits
"Pure" SVMs
RP(SVMs )
Classification Method
COG(SVMs )
Figure 6 : Comparison of the Maximal Accuracy Gains by COG with SVMs and BMR .
Figure 7 : Comparison of the Classification Accuracies by COG and Random Partitioning .
In the table , we can observe that COG can simultaneously improve the classification accuracies for nearly all the classes of optdigits . In addition , Figure 5 shows the improvement ratio of classes in the five balanced data sets by COG with SVMs and BMR ( “ #clusters ” =8 ) . A very similar improvement trend can be observed for all five data sets . Indeed , COG can transform the non linear separable concepts in the data into the linear separable concepts so as to simultaneously improve the classification performance of linear classifiers for most of the classes in the data .
Another interesting observation is that , the accuracy improvements gained by COG with SVMs and BMR are quite close . To illustrate this , we compute the maximal accuracy gain for each data set ; that is , we first select the highest accuracy among four values achieved in different “ #clusters ” levels , then subtract it by the accuracy obtained by the pure classifier . Figure 6 shows the results . As can be seen , the maximal accuracy gains of all data sets by COG with SVMs and BMR are quite close except for pendigits . This implies that the non linear separable concept in the data is the bottle neck that hinders the analysis of linear classifiers . Results by COG with Non linear Classifiers . Table 10 also shows the results of COG with non linear classifiers such as RIPPER and C4.5 on five balanced data sets . In the table , we can see that COG(RIPPER ) has worse performance than pure RIPPER on all five data sets . This is due to the fact that the rule learning algorithm aims to build up a rule set in a greedy fashion by employing the standard divide and conquer strategy . Meanwhile , COG partitions instances of the same class into different sub classes . This can increase the number of negative examples for some target rules , and ultimately result in missing such rules .
Finally , for another widely used non linear classifier C4.5 , the performance of COG with C4.5 is not consistent on five balanced data sets as shown in Table 10 . For instance , COG can improve the classification accuracy of optdigits , but lead to worse performances on letter , vowel and satimage . This is due to the fact that COG can increase the number of sub classes so as to make the branch splitting decision even harder to make . In other words , the splitting attributes of the tree can be better or worse selected in such “ uncertain ” scenarios , which results in the inconsistent performances .
COG versus Random Partitioning .
In this experiment , we compare the effect of clustering in the COG scheme with that of simple random partitioning . To this end , we take the data set letter and the classifier SVMs to illustrate this . First , we randomly split the letter data set into two parts , 70 % of which as the training set and the rest as the test set ( the class size distribution holds ) . Then we performed training and testing in a very similar fashion to the procedure of COG except that we use random partitioning instead of clustering on each class . Figure 7 shows all the results . Please note that for random partitioning and COG , “ #clusters ” = 4 . As indicated by Figure 7 , while the performance of random partitioning with SVMs , ie , RP(SVMs ) , are slightly better than the results of pure SVM , they are much worse than the results by COG(SVMs ) . This indicates that the clustering phase in COG is very important . In fact , the local clustering process can transform the nonlinear separable concepts of the data into linear or “ quasi ” linear concepts , and so as to improve the classification accuracy of linear classifiers .
In summary , COG is of great use on improving the classification accuracy of linear classifiers by eliminating or mitigating the negative impact of the non linear separable concepts in the data . But for the non linear classifiers , such as C4.5 and RIPPER , COG shows no competitive results .
5 . RELATED WORK
In the literature , there are a number of methods addressing the class imbalance problem . For instance , the sampling based methods are one of the simplest yet effective ones . The over sampling scheme replicates the small classes to match the sizes of large classes [ 22 , 29 ] ; under sampling , however , cuts down the large class sizes to achieve a similar effect [ 21 , 29 ] . Drummond and Holte [ 11 ] provided detailed comparisons on these two resampling schemes . Another popular method is the cost sensitive learning scheme which takes the cost matrix into consideration during model building and generates a model that has the lowest cost . The properties of a cost matrix had been studied by Elkan [ 13 ] . Margineantu and Dietterich [ 24 ] examined various methods for incorporating cost information into the C4.5 learning
Research Track Paper822 algorithm . Other cost sensitive learning methods that are algorithm independent include AdaCost [ 14 ] , MetaCost [ 10 ] , and Costing [ 32 ] . In addition , Joshi et al . [ 18 ] discussed the limitations of boosting algorithms for rare class modeling and proposed PNrule , a two phase rule induction algorithm , to handle the rare class purposefully [ 17 ] . Other algorithms developed for mining rare classes include SMOTE [ 6 ] , RIPPER [ 7 ] etc . A survey paper is given by Weiss [ 31 ] .
Finally , in her inspiring paper , Japkowicz [ 16 ] shows the idea of “ supervised learning with unsupervised output separation ” . This work shares some common grounds with our COG method in terms of combining supervised and unsupervised learning techniques . However , in this paper , we have a novel perspective on rare class analysis . We develop the foundation of classification using local clustering ( COG ) for enhancing linear classifies on handling both balanced and imbalanced classification problems .
6 . CONCLUSIONS
In this paper , we propose a method for classification using local clustering ( COG ) . The key idea is to perform clustering within each class and produce linearly separable subclasses with relatively balanced sizes . For data sets with imbalanced class distributions , the COG method can improve the performance of traditional supervised learning algorithms , such as Support Vector Machines ( SVMs ) , on rare class analysis . In addition , the COG method has the capability in enhancing linear classifiers on data sets containing non linear separable classes . Finally , as demonstrated by our experimental results on various real world data sets , COG with over sampling can have much better prediction performance on rare classes than state of the art methods .
7 . ACKNOWLEDGEMENTS
This research was partially supported by the National Science Foundation of China ( NSFC ) Research Fund Nos . 70621061 and 70518002 . Also , this research was supported in part by a Faculty Research Grant from Rutgers Business School Newark and New Brunswick .
8 . REFERENCES [ 1 ] Bmr . In http://wwwstatrutgersedu/ madigan/BMR/ . [ 2 ] C45 In http://wwwrulequestcom/Personal/ [ 3 ] Kddcup . In http://wwwacmorg/sigs/sigkdd/kddcup/indexphp
[ 4 ] Kddcup99data . In http://kddicsuciedu//databases/kddcup99/kddcup99html
[ 5 ] Libsvm . In wwwcsientuedutw/ cjlin/libsvm/ . [ 6 ] N . Chawla , K . Bowyer , L . Hall , and W . Kegelmeyer . Smote : Synthetic minority over sampling technique . Journal of AI Research , 16:321–357 , 2002 .
[ 7 ] W . Cohen . Fast effective rule induction . In ICML , pages 115–123 , 1995 .
[ 8 ] N . Cristianini and J . Shawe Taylor . An Introduction to
Support Vector Machines ( and other kernel based learning methods ) . Cambridge University Press , 2000 .
[ 9 ] M . DeGroot and M . Schervish . Probability and
Statistics ( 3 edition ) . Addison Wesley , 2001 .
[ 10 ] P . Domingos . Metacost : a general method for making classifiers cost sensitive . In KDD , pages 155–164 , 1999 .
[ 11 ] C . Drummond and R . Holte . C4.5 , class imbalance , and cost sensitivity : Why under sampling beats over sampling . In ICML Workshop , 2003 .
[ 12 ] R . Duda , P . Hart , and D . Stork . Pattern classification .
Wiley New York , 2001 .
[ 13 ] C . Elkan . The foundations of cost sensitive learning .
In IJCAI , pages 973–978 , 2001 .
[ 14 ] W . Fan , S . Stolfo , J . Zhang , and P . Chan . Adacost : misclassification cost sensitive boosting . In ICML , pages 97–105 , 1999 .
[ 15 ] E H Han and et al . Webace : A web agent for document categorization and exploration . In Int’l Conf . on Autonomous Agents , 1998 .
[ 16 ] N . Japkowicz . Supervised learning with unsupervised output separation . In Int’l Conf on Artificial Intelligence and Soft Computing , pages 321–325 , 2002 .
[ 17 ] M . Joshi , R . Agarwal , and V . Kumar . Mining needle in a haystack : Classifying rare classes via two phase rule induction . In SIGMOD , pages 91–102 , 2001 .
[ 18 ] M . Joshi , R . Agarwal , and V . Kumar . Predicting rare classes : Can boosting make any weak learner strong ? In KDD , 2002 .
[ 19 ] G . Karypis . Cluto – software for clustering high dimensional datasets , version 211 In http://glarosdtcumnedu/gkhome/views/cluto
[ 20 ] M . Kubat , R . Holte , and S . Matwin . Machine learning for the detection of oil spills in satellite radar imaages . Machine Learning , 30:195–215 , 1998 .
[ 21 ] M . Kubat and S . Matwin . Addressing the curse of imbalanced training sets : One sided selection . In ICML , pages 179–186 , 1997 .
[ 22 ] C . Ling and C . Li . Data mining for direct marketing : Problems and solutions . In KDD , pages 73–79 , 1998 . [ 23 ] O . Maimon and L . Rokach , editors . The Data Mining and Knowledge Discovery Handbook . Springer , 2005 . [ 24 ] D . Margineantu and T . Dietterich . Learning decision trees for loss minimization in multi class problems . In TR 99 30 03 . Oregon State University , 1999 .
[ 25 ] P . Murphy and D . Aha . In UCI Repository of Machine
Learning Databases . U . of California at Irvine , 1994 . [ 26 ] D . Newman , S . Hettich , C . Blake , and C . Merz . Uci repository of machine learning databases , 1998 . [ 27 ] M . F . Porter . An algorithm for suffix stripping .
Program , 14(3):130–137 , July 1980 .
[ 28 ] S . Raudys and A . Jain . Small sample size effects in statistical pattern recognition : Recommendations for practitioners . TPAMI , 13(3):252–264 , 1991 .
[ 29 ] P N Tan , M . Steinbach , and V . Kumar . Introduction to Data Mining . Addison Wesley , 2005 .
[ 30 ] TREC . In http://trecnistgov [ 31 ] G . Weiss . Mining with rarity : a unifying framework .
ACM SIGKDD Explorations , 6(1):7–19 , 2004 .
[ 32 ] B . Zadrozny , J . Langford , and N . Abe . Cost sensitive learning by cost proportionate example weighting . In ICDM , pages 435–442 , 2003 .
[ 33 ] J . Zurada , B . Foster , and T . Ward . Investigation of artificial neural networks for classifying levels of financial distress of firms : The case of an unbalanced training sample . In Knowledge Discovery for Business Information Systems , pages 397–423 . Kluwer , 2001 .
Research Track Paper823
