Nonlinear Adaptive Distance Metric Learning for Clustering
Jianhui Chen∗ Zheng Zhao∗
Jieping Ye Huan Liu
Department of Computer Science and Engineering
Arizona State University , Tempe , AZ 85287
{jianhui.chen , zheng.zhao , jieping.ye , huanliu}@asuedu
ABSTRACT A good distance metric is crucial for many data mining tasks . To learn a metric in the unsupervised setting , most metric learning algorithms project observed data to a lowdimensional manifold , where geometric relationships such as pairwise distances are preserved . It can be extended to the nonlinear case by applying the kernel trick , which embeds the data into a feature space by specifying the kernel function that computes the dot products between data points in the feature space . In this paper , we propose a novel unsupervised Nonlinear Adaptive Metric Learning algorithm , called NAML , which performs clustering and distance metric learning simultaneously . NAML first maps the data to a high dimensional space through a kernel function ; then applies a linear projection to find a low dimensional manifold where the separability of the data is maximized ; and finally performs clustering in the low dimensional space . The performance of NAML depends on the selection of the kernel function and the projection . We show that the joint kernel learning , dimensionality reduction , and clustering can be formulated as a trace maximization problem , which can be solved via an iterative procedure in the EM framework . Experimental results demonstrated the efficacy of the proposed algorithm .
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications Data Mining
General Terms Algorithms
Keywords Clustering , distance metric , kernel , convex programming ∗
The first two authors contribute equally to the paper .
1 .
INTRODUCTION
Good distance metrics are crucial to many areas in data mining , such as clustering , classification , regression , and semi supervised learning . In distance metric learning , the goal is to achieve better compactness ( reduced dimensionality ) and separability ( inter cluster distance ) on the data , in comparison with usual distance metrics , such as Euclidean distance . With a good distance metric , the construction of the learning models becomes easier and the accuracy of the learning models usually improves [ 34 ] . Based on the availability of the constraint information ( class label ) , distance metric learning algorithms fall into two categories : supervised distance metric learning [ 26 , 31 , 32 , 35 ] and unsupervised distance metric learning [ 4 , 10 , 17 , 23 , 29 ] .
The performance of unsupervised learning algorithms , such as K means is largely dependent on the pairwise similarity , which is commonly determined via a pre specified distance metric . However , learning a good distance metric in the unsupervised setting is challenging due to the absence of any prior knowledge on the data . In this paper , we focus on the problem of unsupervised distance metric learning for clustering . Without any constraint or class label information , most unsupervised metric learning algorithms apply the projection method such that geometric relationships , such as the pairwise distances are preserved in a low dimensional manifold . Commonly used projection ( dimensionality reduction ) methods include the Principle Component Analysis ( PCA ) [ 17 ] , Locally Linear Embedding ( LLE ) [ 23 ] , Laplacian Eigenmap [ 4 ] , and ISOMAP [ 29 ] . Unsupervised learning algorithms , such as K means can then be applied in the dimensionality reduced space , avoiding the curse of dimensionality .
In unsupervised learning , the goal is to find a collection of clusters in the data , which achieves the maximum intercluster separability . Traditionally , dimensionality reduction and clustering are applied in two separate steps . If distance metric learning ( via dimensionality reduction ) and clustering can be performed together , the cluster separability in the data can be better maximized in the dimensionality reduced space . In this paper , we propose a novel algorithm for nonlinear adaptive distance metric learning , called NAML for simultaneous distance metric learning and clustering . NAML first maps the data to a high dimensional space through a kernel function ; next applies a linear projection to find a low dimensional manifold ; and then perform clustering in the low dimensional space . The performance of NAML depends on the selection of the kernel function and the projection . The key idea of NAML is to integrate kernel learning , dimensionality reduction , and clustering in a joint framework so that the separability of the data is maximized in the low dimensional space .
One aspect of NAML shares the same goal of supervised metric learning approaches , which try to adjust the distance among the instances to improve the separability of the data . For example , in [ 26 , 32 ] , the distance metric adjusts the geometry of data , so that the distance between data points from the same class under the metric is small . The metric improves the separability of the data and enhances the performance of classifiers , such as K Nearest Neighbor ( K NN ) . In [ 9 , 19 , 37 ] , a linear projection is performed to learn the distance metric for clustering , which assumes linear separability of the data as in [ 26 , 32 ] . However , many real world applications may involve data with nonlinear and complex patterns . Kernel methods [ 24 , 25 ] have been commonly used to deal with this problem . They work by embedding the input data into a high dimensional feature space through the so called kernel function . The key to the success of kernel methods is that the embedding into a feature space can be uniquely determined by specifying the kernel function that computes the dot products between data points in the feature space . One of the central issues in kernel methods is the selection ( learning ) of a good kernel function . The problem of kernel learning has been an active area of recent research [ 2 , 3 , 13 , 16 , 18 , 20 , 21 , 28 , 33 , 37 ] . The novel aspect of the proposed approach in comparison with these approaches is that NAML does not use any class label . In [ 30 ] , generalized maximum margin clustering was proposed for simultaneous kernel learning and clustering , which was formulated as a semidefinite program ( SDP ) . Besides its high computational cost of solving a SDP problem , the proposed formulation in [ 30 ] is restricted to the two cluster problems only .
We show in this paper that the simultaneous kernel learning , dimensionality reduction , and clustering in NAML can be formulated as a trace maximization problem , which can be solved by an iterative algorithm based on the EM framework . In particular , we show that both dimensionality reduction and clustering can be solved by spectral anslysis , while the kernel learning can be formulated as a Quadratically Constrained Quadratic Programming ( QCQP ) problem , which can be solved more efficiently than SDP . We evaluate the proposed algorithm using benchmark data sets , and the experimental results show the effectiveness of the proposed algorithm .
The remainder of the paper is organized as follows . We introduce the formulation of distance metric learning for the linear case in Section 2 . The formulation is then extended to the nonlinear case in Section 3 . Experimental results are presented in Section 4 . This paper concludes with discussion and future work in Section 5 .
For convenience , we present in Table 1 the important no tations used in the rest of this paper .
2 . ADAPTIVE DISTANCE METRIC LEARN
ING : THE LINEAR CASE
In this section , we present the linear adaptive distance metric learning algorithm from [ 37 ] , which will then be extended to the nonlinear case in the next section . Assume we are given a data set of zero mean , which consists of n samples {xi}n i=1 , where xi ∈ IRm . Denote X = [ x1 , x2 , ··· , xn ] as the data matrix . Consider the pro
Table 1 : Important notations used in the paper .
Notation Description n m k X l W S μ Ci ni μi λ L K G Q L number of samples number of features ( dimensions ) number of clusters data matrix of size m by n reduced dimensionality transformation in the linear case covariance matrix of data in X mean of the data in X the i th cluster in X size of the i th cluster Ci mean of i th cluster Ci regularization parameter cluster indicator matrix of size n by k kernel function kernel Gram matrix of size n by n transformation in the nonlinear case Laplacian matrix of size n by n jection of the data via a linear transformation W ∈ , m×l . Thus , each xi in the m dimensional space is mapped to a vector ˆxi in the l dimensional space as follows : xi ∈ , m → ˆxi = W T xi ∈ , l
( l < m ) .
( 1 )
It has been shown [ 8 , 15 ] that for most high dimensional data sets , almost all low dimensional projections are nearly normal . That is , for large m the projected data {ˆxi}n i=1 is expected to be nearly normal . In this case , a good distance measure is the well known Mahalanobis distance measure defined as follows : dM ( ˆxi , ˆxj ) = ( ˆxi − ˆxj)T ˆS−1(ˆxi − ˆxj ) , where ˆS is the covariance matrix defined as follows :
( 2 )
( 3 )
( 4 )
( 5 ) and ˆμ = 1 i=1 . It follows that
( ˆxi − ˆμ)(ˆxi − ˆμ )
T ,
ˆS =
1 n n
.i=1
ˆS = n
1 n
W T
( xi − μ)(xi − μ ) i=1 ˆxi is the mean of {ˆxi}n n n .i=1 i=1 xi is the mean of {xi}n n n ( xi − μ)(xi − μ )
S =
T
1 n n
.i=1
T W = W T SW , i=1 , and where μ = 1 is the class covariance matrix of the original data in X . For high dimensional data , the estimation of the covariance matrix in Eq ( 5 ) is often not reliable . Thus , the regularization technique [ 11 ] is applied to improve the estimation as follows :
( xi − μ)(xi − μ )
T
+ λIm ,
( 6 )
S =
1 n n
.i=1 where Im is the identity matrix of size m and λ > 0 is a regularization parameter .
Under this new distance measure , K means clustering can be applied to assign {ˆxi}n j=1 , which minimize the following Sum of Squared Error ( SSE ) : i=1 into k disjoint clusters , {Cj}k
SSE({Cj}k j=1 ) = dM ( ˆxi , μj )
2 ,
( 7 ) k
.j=1 .ˆxi∈Cj where the Manalanobis distance dM ( ·,· ) is defined as in Eq ( 2 ) , and μj is the mean of the j th cluster Cj .
As the summation of all pair wise distances is a constant for a fixed W . The minimization of the SSE is equivalent to the maximization of Sum of Squared Intra cluster Error ( SSIE ) defined as follows :
SSIE'{Cj}k j=1ff =.k j=1 nj dM ( μj , ˆμ )
2 ,
( 8 ) where nj is the sample size of the j th cluster Cj , μj is the mean of the j th cluster Cj , and ˆμ is the global mean as defined above . SSIE can be expressed in a compact matrix form as follows . Let F ∈ , n×k be the cluster indicator matrix defined as follows :
F = {fi,j}n×k , where fi,j = 1
( 9 ) The weighted cluster indicator matrix L = [ L1 , L2,· ·· , LK ] is defined as [ 7 , 9 ] :
0
. if xi ∈ Cj otherwise
L = F ( F T F )
− 1 2 ,
( 10 ) where the i th column of L is given by ni
Li = ( 0 , . . . , 0 ,
1 , . . . , 1 , 0 , . . . , 0 )
T /n
1 2 i .
( 11 )
( )
With the weighted cluster indicator matrix L , the Sum of Squared Intra cluster Error ( SSIE ) can be expressed as :
SSIE'{Cj}k j=1ff = trace'LT X T W ˆS−1W T XLff = trace'LT X T W ( W T SW )
−1W T XLff .
The joint metric learning and clustering problem can be formulated as follows [ 37 ] : max W,L trace'LT X T W ( W T SW )
−1W T XLff .
( 12 )
The optimization problem in Eq ( 12 ) maximizes the intercluster distance under the Mahalanobis distance measure determined by the transformation W . Thus , it computes the distance metric and performs the clustering simultaneously .
3 . ADAPTIVE DISTANCE METRIC LEARN
ING : THE NONLINEAR CASE
In this section , we first review the basics of kernel methods . We then present the nonlinear formulation of the adaptive metric learning algorithm from the last section using the kernel trick . Kernel methods [ 24 , 25 ] work by mapping the data into a high dimensional Hilbert space ( feature space ) F equipped with an inner product through a nonlinear mapping φK as :
φK : IR m → F .
The nonlinear mapping can be implicitly specified by a symmetric kernel function K , which computes the inner product of the images of each data pair in the feature space , that is
K(xi , xj ) = ( φK ( xi ) , φK ( xj ) ) , where xi , xj ∈ IRm are training data points . A kernel function K satisfies the finitely positive semidefinite property : for any x1 , ··· , xn ∈ IRm , the so called kernel Gram matrix G , defined as Gij = K(xi , xj ) , is symmetric and positive semidefinite .
The adaptive metric learning problem in Eq ( 12 ) can be extended to the nonlinear case using the kernel trick . Denote φK(X ) as the data matrix in the feature space . For a given kernel function K , the nonlinear adaptive metric learning problem can be formulated as the following trace maximization problem :
−1W T
K SK WK )
T WK(W T trace'LT φK ( X ) max WK ,L where WK is the transformation in the feature space . Assume the data in the feature space has been centered , ie , i=1 φK(xi ) = 0 . Otherwise the kernel centering technique in [ 24 ] can be used . Thus the covariance matrix SK can be expressed as
K φK ( X)Lff , n
SK = φK ( X)φK(X )
T .
( 13 )
It follows from the Representer Theorem [ 24 ] that the optimal transformation WK is in the span of the images of the data points in the feature space . That is ,
( 15 )
( 14 ) for some matrix Q ∈ IRn×l . Thus , the objective function for NAML can be rewritten as
WK = φK ( X)Q , max Q,L traceffLT GQ'QT
( GG + λG)Qff
−1
QT GLfi , where G = φK ( X)T φK ( X ) is the kernel matrix . Here we assume that the matrix GG + λG is nonsingular , and we can use pseudo inverse [ 14 ] to deal with the singular case .
In essence , NAML maps the data into a high dimensional feature space through a nonlinear mapping , where linear projection and clustering are performed to maximize the cluster separability . The representation of the data in the feature space is determined by the nonlinear mapping , which can be implicitly specified by a kernel matrix . The performance of NAML is dependent on the choice of the kernel matrix . We propose to learn an appropriate kernel matrix for NAML in a joint framework , which leads to the following joint trace optimization problem :
( GG + λG)Qff
−1
QT GLfi ,
( 16 ) where the kernel matrix G is restricted to be a convex combination of a given set of p kernel matrices , defined as max Q,L,G traceffLT GQ'QT θiGiffiffiffiffiffi .i=1 p
G ∈ G = p .i=1
θitrace(Gi ) = 1 , θi ≥ 0 ∀i . ( 17 )
The formulation in Eq ( 16 ) performs kernel learning , dimensionality reduction , and clustering simultaneously . However , the joint optimization problem is highly nonlinear and difficult to solve . One key observation is that if two of the three components L , G , and Q are fixed , the optimization problem is easy to solve . This enables us to solve the problem in the EM framework , in which we update L , G , and Q iteratively to find a local solution .
3.1 The computation of L for given Q and G
For a given matrix Q and a given kernel matrix G , computing the optimal L in Eq ( 16 ) is equivalent to solving the following trace maximization problem : trace(LT ˜GL ) , max
( 18 )
L where ˜G is defined as
−1
QT G .
( 19 )
˜G = G Q'QT
( GG + λG)Qff
√ Recall that the entries of the i th column of the weighted cluster indicator matrix L are either 0 or 1/ ni as defined in Eq ( 11 ) . It follows that LT L = Ik , ie , the columns of L are orthonormal . We apply the spectral relaxation technique [ 38 ] for the computation of the optimal L , which is given by the eigenvectors of ˜G as follows :
Theorem 31 ( Ky Fan ) Let ˜G be a symmetric matrix with eigenvalues λ1 ≥ λ2 ≥ · ·· ≥ λn , and the corresponding eigenvectors U = [ u1,·· · , un ] . Then
λ1 + · ·· + λk = max trace(LT ˜GL ) .
LT L=Ik is given by L∗
= [ u1,· ·· , uk ] P ,
Moreover , the optimal L∗ for an arbitrary orthogonal matrix P ∈ IRk×k . In the implementation , we choose the first k eigenvectors of ˜G corresponding the largest k eigenvalues , where k is the number of clusters . For simplicity , we set P to be the identity matrix . Note that we compute the trace value of the matrix LT ˜GL in each iteration as the measure of convergence . When the relative change of the trace value is smaller than a pre specified threshold , the iterative process stops . 3.2 The computation of Q for given L and G
For a given kernel matrix G and a given relaxed weighted cluster indicator matrix L , the trace maximization problem in Eq ( 16 ) is equivalent to the maximization of the following objective function :
F1(G , Q ) = traceff'QT SK2 Qff
−1
QT SK1 Qfi . where the matrices SK1 and SK2 are defined as
( 20 )
( 21 )
SK1 = GLLT G , SK2 = GG + λG .
The optimal Q∗ which maximizes F1(G , Q ) in Eq ( 20 ) is given by solving an eigenvalue problem associated with SK1 and SK2 , as summarized below :
= V .
Theorem 32 Let SK1 and SK2 be defined in Eq ( 21 ) , and V = [ v1,·· · , vq ] be the matrix consisting of the first q eigenvectors of S+ SK1 corresponding to the largest q eigenvalues , where q = rank(SK1 ) . Let Q∗ ≡ argmaxQF1(G , Q ) . K2 Then Q∗ Proof . Let G = U ΣU T be the Singular Value Decomposition ( SVD ) [ 14 ] of G , where U ∈ IRn×n is orthogonal and Σ = diag ( Σt , 0 ) ∈ IRn×n is diagonal , Σt ∈ IRt×t is diagonal with positive diagonal entries , and t = rank(G ) . Let U1 ∈ IRn×t consist of the first t columns of U . Then = U1ΣtU T 1 .
= U diag ( Σt , 0 ) U T
G = U ΣU T
( 22 )
Denote P = ( Σ2 1 L and let P = M ΣP N T be the SVD of P , where M and N are orthogonal and ΣP t + λΣt )
2 ΣtU T
− 1 is diagonal with rank(ΣP ) = rank(SK1 ) = q . Let Z be a nonsingular matrix defined as t + λΣt )
2 M 0
− 1
In−t fi ,
Z = Uff ( Σ2
0
0 fi , Z T SK2 Z =ff It
( 23 ) where In−t is the identity matrix of size n − t . It follows that Z T SK1 Z =ff ˜Σ 0 ( 24 ) where ˜Σ = ( ΣP )2 ∈ IRt×t is diagonal with the diagonal entries sorted in non increasing order . It is clear that the optimal Q∗ , which maximizes F1(G , Q ) , consists of the first q columns of Z . It can be verified that the first q columns of Z gives V = [ v1,·· · , vq ] which consists of the first q eigenvectors of S+ SK1 corresponding to the largest q eigenvalues . K2 This completes the proof of the theorem .
0 fi ,
0
0
0
It is worth noting that the above trace maximization problem is similar to the well known linear discriminant analysis ( LDA ) [ 12 ] . However , they are fundamentally different , as SK1 is different from the so called between class scatter matrix in LDA , due to the spectral relaxation in L . 3.3 The computation of G for given Q and L
Given Q and L , the optimal G can be computed by maximizing F1(G , Q ) in Eq ( 20 ) , where the kernel matrix G is restricted to be a convex combination of a set of prespecified kernel matrices as in Eq ( 17 ) . One key observation for the computation of the optimal G is that the Q matrix in F1(G , Q ) can be replaced by its optimal value Q∗ given in Theorem 32 This significantly simplifies the derivation . Denote
F ∗ 1 ( G ) = max
F1(G , Q ) = F1(G , Q∗
) .
( 25 )
G
It follows from Theorem 3.2 that F ∗
1 ( G ) = trace( ˜Σ ) = trace (SK2 ) = trace'LT G(GG + λG ) = traceffLT Uff ( It + λΣ
0
+SK1 +GLff
−1 t )
−1
0
0 fi U T Lfi .
( 26 )
−1
1 λ
0
0
−1
λ Σt )
+ ( It + 1 −1
In−t fi U T
−1 = It , and t Since  It + λΣ =ffI + Uff  It + 1 λ Σt 1 ( G ) = trace'LT Lff − traceLT ffI +
1 λ , which maximizes F ∗ Thus , the optimal G∗ minimizing the following objective function : −1 we have F ∗
Gfi
−1
,
Gfi L . ( 27 )
−1
1 ( G ) , is given by
F2(G ) = traceLT ffI +
1 λ
Gfi
L .
( 28 )
The minimization of F2(G ) can be solved by gradient descent methods [ 6 ] . However , the computation of its gradient is expensive for each iteration . Following the recent work in [ 36 ] , we can show that this minimization problem can be formulated as a Quadratically Constrained Quadratic Programming ( QCQP ) problem as follows :
Theorem 33 Given a set of p centered kernel matrices G1 , ··· , Gp as defined in Eq ( 17 ) , the minimization of F2(G ) defined above can be formulated as a QCQP problem as follows : max
β1,··· ,βk,t subject to j Lj − 1 βT 4 k t j βj − 1 βT 4λ
.j=1 j Giβj , i = 1,··· , p , βT k
.j=1 t ≥ 1 ri k
.j=1
( 29 ) where L = [ L1,··· , Lk ] and ri = trace(Gi ) .
The coefficient θi for the i th kernel Gi is given by the dual variable corresponding to the i th constraint in Eq ( 29 ) divided by ri . Note that the general purpose optimization software packages like MOSEK [ 1 ] also report the dual variables by solving the dual problem .
3.4 The Main Algorithm
Based on the discussion described above , we propose to develop an iterative algorithm , called NAML , for Nonlinear Adaptive Metric Learning . The pseudo code of the NAML algorithm is given as below .
Algorithm : NAML
Input : X , k , λ , {Ki}p Output : G , L and Q i=1 , i=1 ;
1 . Randomly choose one kernel matrix G from {Ki}p 2 . Compute the initial cluster indicatior matrix L by applying kernel K means on the initial kernel G ; 3 . While relative change of the trace value ≥ do 4 . 5 . 6 . 7 . 8 . End 9 . return G , L and Q ;
Update Q as in Section 3.2 ; Update G as in Section 3.3 ; Update L as in Section 3.1 ; Compute the trace of LT ˜GL as in Eq ( 19 ) ;
The final clustering result is obtained by applying Kmeans on the relaxed cluster indicator matrix L . The convergence of the NAML algorithm is guaranteed , as summarized in the following theorem :
3.5 Connection to Regularized Spectral Clus tering
In this subsection , we show the close connection between the proposed formulation and regularized spectral clustering [ 27 ] . From Eq ( 26 ) , the eigenvectors of G corresponding to the zero eigenvalues can be removed without affecting the value of the objective function . In the following discussions , we assume that G is nonsingular . It follows from Eq ( 26 ) that
−1 Lff .
1 ( G ) = trace'LT  I + λG−1
( 30 ) Next , we consider a specific choice of G by setting G = L−1 , where L is the Laplacian matrix [ 4 ] defined as follows . Let W ∈ Rn×n be a symmetric similarity matrix , and D ∈ Rn×n j=1 Wij . The Laplacian L is defined as [ 4 ] be a diagonal matrix with Dii = n
F ∗
L = D − W .
( 31 )
The centering of the kernel matrix , which is equivalent to the data centering step in NAML , is not required when G = L−1 . It is based on the fact that the inverse of the Laplacian matrix is already centered , as summarized in the following proposition :
Proposition 31 Let L be the Laplacian matrix defined above . Then the inverse of Laplacian , denoted as L−1 , has zero row and column means . In other words , let en be the vector of all ones of size n , then eT nL−1en = 0 .
Proof . Since L is symmetric and positive semidefinite , n be SVD of L , where Un is orthogonal and let L = UnΣnU T Σn has nonnegative diagonal entries . It follows that nWen = 0 . n UnΣnU T nL−1en = eT nDen − eT nLen = 0 , and eT −1 n U T nLen = eT eT n en = eT Thus , eT that eT n UnΣ proof of the proposition . We have assumed that L is nonsingular in the above derivation . For singular L , we can use its pseudo inverse and the result in the proposition above still holds . With this particular choice of G , the objective function in Eq ( 30 ) becomes : n Un = 0 . It follows n en = 0 . This completes the
( 32 )
F ( L ) = trace'LT
( I + λL )
−1Lff ,
( 33 ) which corresponds to clustering with a regularized Laplacian matrix [ 27 ] .
Theorem 34 Algorithm NAML converges in a finite num
4 . EXPERIMENT ber of steps .
Proof . The NAML algorithm updates G , Q , and L iteratively , by maximizing the same objective function , ie , trace(LT ˜GL ) . As the objective value is non decreasing and is bounded from above by a finite number , the algorithm converges in a finite number of steps .
−5 for checking the conIn the implementation , we set = 10 vergence . We observe from our experiments that the NAML algorithm typically converges within 3 to 4 iterations . The time complexity of the NAML algorithm is dominated by the QCQP problem in Theorem 3.3 , whose worst case complexity is O(pk3n3 ) .
We now empirically evaluate the performance of the NAML algorithm in comparison with representative algorithms , and conduct a sensitivity study to evaluate its various components , such as the effect of the regularization parameter λ , and the input kernels . These studies will help us better understand the proposed algorithm , and delineate new challenges and research issues . 4.1 Experiment Setup
To evaluate the performance of NAML , we use the Kmeans algorithm as the baseline for comparison . We also compare the proposed algorithm with three representative unsupervised distance metric learning algorithms : Principle Component Analysis ( PCA ) , Local Linear Embedding
( LLE ) , and Laplacian Eigenmap ( Leigs ) . The Matlab implementations of these algorithms are obtained from corresponding authors’ websites respectively . NAML is also implemented in the Matlab environment and we solve the QCQP problem using MOSEK [ 1 ] . All experiments were conducted on a PENTIUM IV 2.4G PC with 1.5GB RAM . We test the distance metric learning algorithms and Kmeans on eight benchmark data sets . They are six UCI data sets [ 5 ] : iris , lymph , promoter , satimage , solar , wine , and two image data sets : AR03P1 and ORL10P2 . Since MOSEK gives memory overflow error when the number of instances is large , for the satimage data set , we randomly sample 80 instances from each class . The information on the eight test data sets is summarized in Table 2 .
Table 2 : Summary of the benchmark data sets .
Data set iris lymph promoter satimage solar wine AR03P ORL10P
Dimension
Instance
Class
4 18 57 36 12 13
2400 10000
150 148 106 6435 323 178 39 100
3 4 2 6 6 3 3 10
We compare the performance of the algorithms as follows . For each data set , we first run K means and record its clustering results as a baseline . To make the results of different distance metric learning algorithms comparable , the clustering result of K means is used to construct C , the set of k initial centroids , for later experiments . Here k is the number of clusters of the data . We apply PCA , LLE , and Leigs on each data set to learn distance metrics , which are used by K means to learn clusters with the initial centroid set C . Their clustering results are recorded . We also run NAML with C and record its clustering results . This process is repeated for 20 times with different initial centriods for each data set .
411 Performance Measures
As we have the label information of all eight benchmark data sets , the clustering results are evaluated by comparing the obtained label of each data point with the ground truth . We use two standard measurements : the accuracy ( ACC ) and the normalized mutual information ( MI ) measures defined as below . Given a data point xi , let ci and yi be the obtained cluster indicator and the true class label from the data , respectively . The accuracy measure is defined as :
ACC =
1 n.n i=1 f ( yi , map(ci ) ) ,
( 34 ) where f ( yi , yj ) = 0 , yi ( = yj
1 , yi = yj
.
( 35 )
1http://rvl1ecnpurdueedu/∼aleix/face DBhtml set is subsampled down to the size of 60 × 40 = 2400 . 2http://wwwukresearchattcom/facedatabasehtml Data set is subsampled down to the size of 100×100 = 10000
Data
In the equation above , n is the total number of data points and map(c ) is the permutation mapping function that maps each cluster indicator c to its equivalent class label . The mapping is found by using the Kuhn Munkres algorithm [ 22 ] . Let C and Y be the set of cluster indicators and the set of class labels , respectively . The normalized mutual information is defined as :
2 × yi∈Y,cj∈C p(yi , cj ) · log
H(Y ) + H(C ) p(yi,cj ) p(yi)·p(cj )
,
MI(Y , C ) =
( 36 ) where p(ci ) ( or p(yj ) ) denotes the probability that an instance randomly selected from X belongs to cluster ci ( or class yj ) , p(yi , cj ) denotes the joint probability , and H(Y ) ( or H(C ) ) is the entropy of Y ( or C ) .
Since each algorithm is tested for 20 times on each data set , we obtain 20 performance evaluations from ACC and MI measures , respectively . These performance evaluations are averaged and yield 2 final performance evaluations per algorithm on each data set . In the experiment , the reduced dimensionality ( l ) of PCA is selected to retain at least 95 % information of the original data , and the reduced dimensionality ( l ) of NAML is set to k . For each data set , we construct 10 RBF kernels for NAML . 4.2 Experimental Results
−6 , 10
−4 , and 10
Table 3 presents the accuracy ( ACC ) and normalized mutual information ( MI ) results on each data set . The results −2 ) of NAML using 3 different λ values ( 10 −2 performs the best ( inare shown . NAML with λ = 10 cluding the second best without a significant difference with the best ) on 6 data sets in terms of accuracy . On the eight −2 performs the best with an data sets , NAML with λ = 10 average accuracy of 0.747 , which is followed by NAML with −4 with an average accuracy of 0743 LLE performs λ = 10 the third best and PCA is the fourth . Similar trends can also be observed in the MI results .
Experimental results also show that NAML does improve the performance of K means on all eight data sets . For ex−2 improves its ample , on AR03P data , NAML with λ = 10 accuracy from 0.462 to 0.615 , a 15.3 % improvement . In our experiment , NAML converges in less than eight iterations and usually converges in 3 4 iterations . 4.3 Sensitivity Study
In this subsection , we study the effects of various components of NAML . More specifically , we study the effect of the input kernels and the regularization parameter λ .
431
Input Kernels
In Table 4 , we compare the performance of NAML with that of K means using each of ten input kernels , respectively . Thus , we can obtain 10 clustering results from Kmeans with respect to 10 kernels , and further calculate max Ker , min Ker , and ave Ker corresponding to the best , worst , and average performance . NAML uses the same 10 kernels as its input . We can observe from the table that in most cases , NAML performs much better than min Ker ( Kmeans using the worst kernel ) and is comparable to max Ker ( K means using the best kernel ) . This set of results has its ramifications for unsupervised metric learning . When we do not have the prior knowledge about the kernel quality ,
Table 3 : Comparison of accuracy ( ACC ) and normalized mutual information ( MI ) on eight benchmark data sets . The numbers behind NAML are the λ values used . For each data set , the first row and the second row list ACC ( or MI ) and p val , respectively . The p val of each algorithm is generated by comparing its ACC with the highest one . ACCs in boldface are the highest ones or the second highest without significant difference with the highest one , according to p val>01
Meas . Data Set
NAML10−6
NAML10−4
NAML10−2
K means
0.865 0.020 0.701 0.008 0.672 0.074 0.633 0.026 0.576 0.136 0.954
0
0.462
0
0.722 0.003 0.698 0.726
0
0.179 0.169 0.104 0.013 0.515 0.018 0.389 0.163 0.850
0
0.089
0
0.801 0.015 0.457
0.65
0.6
C C A
0.55
0.5
0.45 1
PCA 0.865 0.020 0.701 0.008 0.673 0.089 0.628 0.018 0.575 0.129 0.954
0
0.462
0
0.722 0.003 0.698 0.726
0
0.178 0.150 0.106 0.018 0.515 0.015 0.388 0.103 0.850
0
0.089
0
0.801 0.015 0.457
LLE 0.848
0
0.686
0
0.677 0.053 0.599
0
0.511
0
0.961
0
0.518 0.012 0.715 0.001 0.689 0.631
0
0.173 0.001 0.103 0.005 0.452
0
0.290 0.001 0.837
0
0.196 0.032 0.793
0
0.434
Leigs 0.829 0.004 0.703 0.025 0.656 0.004 0.640
0
0.562 0.010 0.964 0.061 0.403
0
0.333
0
0.636 0.726 0.001 0.164
0
0.087 0.002 0.520
0
0.350 0.107 0.875 0.090 0.040
0
0.368
0
0.391
0.65
0.6
0.55
0.5
0.45
C C A
2
Iteration
3
0.4 1
2
3
4
Iteration
5
6
7 iris lymph promoter satimage solar
ACC wine
AR03P
ORL10P
Average iris lymph promoter satimage solar
MI wine
AR03P
ORL10P
Average
0.908
0.707 0.002 0.689
0.658 0.014 0.593
0.413
0
0.572
0
0.766 0.168 0.663 0.767 0.428 0.183
0
0.156 0.031 0.520 0.006 0.395
0.011
0
0.212 0.003 0.819 0.264 0.383
0.901 0.801 0.709
0
0.698
0.742 0.587 0.587 0.424 0.972
0.572
0
0.766 0.168 0.743 0.780
0.187
0
0.168
0.586 0.482 0.386 0.427 0.893
0.211 0.002 0.819 0.264 0.504
0.883 0.359 0.716
0.698
0.743
0.584 0.326 0.972
0.615
0.768
0.747 0.752
0
0.197
0.148 0.020 0.587
0.342 0.064 0.893
0.301
0.822
0.505
NAML provides a way to learn from multiple input kernels and generate a metric , with which an unsupervised learning algorithm , like K means , is more likely to perform as well as with the best input kernel . Hence , NAML has interesting applications in solving real world clustering problems . For example , in a learning task , the pairwise instance relationship is calculated by a RBF kernel function , and the kernel parameter σ is estimated by several domain experts . According to different understandings of the problem , different experts can assign different values for σ . In this case , NAML ’s capability of combining different perspectives from multiple experts and learning a good metric can be essential for unsupervised learning of nonlinear patterns . Figure 1 shows two sample cases that NAML converges to a good result , even though the quality of the initial kernel is low .
432 Regularization Parameter λ
As discussed in Section 2 , a regularization parameter λ is introduced to improve the reliability of the estimation of the
Figure 1 : Clustering performance improves during the iterative process of NAML on AR03P data set : λ = 0.01 ( left plot ) and λ = 100 ( right plot ) . covariance matrix . The performance of NAML is dependent on the value of λ . In the following experiment , we study the effect of this parameter on the clustering performance of NAML . We can observe from Table 3 that , in general , the regularization helps to improve the performance of NAML .
Table 4 : Comparison of the performance achieved by NAML and K means using each input kernel on benchmark data sets . In the table “ init Ker ” stands for the average accuracy achieved by K means on the initial kernel . “ max Ker ” , “ min Ker ” and “ ave Ker ” stand for the highest , lowest and average accuracy achieved by K mean on ten input kernels . NAML10−4
NAML10−2
NAML10−6
Meas . Data Set init Ker max Ker min Ker iris lymph
ACC promoter satimage solar wine AR03P ORL10P iris lymph promoter satimage solar wine AR03P ORL10P
MI
0.908 0.707 0.689 0.658 0.593 0.413 0.572 0.766 0.767 0.183 0.156 0.520 0.395 0.011 0.212 0.819
0.901 0.709 0.698 0.742 0.587 0.972 0.572 0.766 0.780 0.187 0.168 0.586 0.368 0.893 0.211 0.819
0.883 0.716 0.698 0.743 0.584 0.972 0.615 0.768 0.752 0.197 0.148 0.587 0.342 0.893 0.301 0.822
0.887 0.681 0.585 0.594 0.593 0.933 0.418 0.738 0.729 0.159 0.128 0.485 0.372 0.773 0.170 0.800
0.927 0.716 0.734 0.746 0.600 0.972 0.626 0.762 0.792 0.197 0.223 0.590 0.375 0.893 0.362 0.819
0.887 0.681 0.585 0.594 0.572 0.933 0.418 0.738 0.729 0.159 0.109 0.485 0.344 0.773 0.170 0.800 ave Ker 0.903 0.708 0.686 0.715 0.584 0.967 0.541 0.760 0.771 0.187 0.161 0.568 0.360 0.876 0.267 0.817
In terms of accuracy , on four of eight data sets , the perfor−2 is significantly better than mance of NAML with λ = 10 −6 ) . that with a very small value of regularization ( λ = 10 Similar improvement can be observed in MI results . forms the best when the value of λ is neither too small nor too large . This is partly due to the complementary information that exists among the given collection of kernels , which may be exploited by NAML .
To obtain a better understanding of the effect of the regularization parameter , we tried a series of different λ values −8 to 105 . The ACC and MI results using ranging from 10 various λ values are plotted in Figure 2 . We can observe from the figure that , in general , NAML is not very sensitive to the value of λ , except for the case when λ is very large −6 ) . The use of a λ value in the ( > 102 ) or very small ( < 10 range of [ 10
−4 , 102 ] is helpful in most cases .
We can observe from Figure 2 that a small λ value is less effective than a large λ value . In most cases , a large λ value does not significantly degrade the performance , which is not the case when λ value is very small . Further studies show that when λ is very small , the kernel weights learnt by NAML become close to each other ; while when λ is very large , the kernel weight vector becomes sparse ( many of them are zero ) and only the best kernels or those close to the best ones have non zero weights . We show in Table 5 the weight of each input kernel , when using different λ values on AR03P data . The result suggests that when λ is set to 0 , the weights of all kernels are not zero ; while when λ is large , the weight vector becomes sparse and only a very small number of kernels has non zero weights .
Recall that the optimal combination of kernels is obtained by maximizing trace LT G(GG + λG)+GL . It is clear that when λ approaches to 0 , G(GG + λG)+G approaches to a matrix , which contains 1 as the only nonzero eigenvalue . In this case , the optimization in NAML becomes degenerate . On the other hand , when λ becomes large , the λG term in ( GG+λG)+ dominates . In this case , the optimization prob is essentially equivalent to the selection of a single kernel lem is reduced to the maximization of trace LT GL , which that maximizes trace LT KL . These explain the behavior of NAML for different λ values in Table 5 . However , we can also observe from Figure 2 and Table 5 that NAML per
5 . CONCLUSIONS
In this paper , we propose a nonlinear adaptive distance metric learning algorithm , called NAML . We show that the joint kernel learning , metric learning , and clustering can be formulated as a trace maximization problem , which can be solved iteratively in an EM framework . More specifically , we show that both dimensionality reduction and clustering can be solved by spectral analysis , while the kernel learning can be formulated as a Quadratically Constrained Quadratic Programming problem .
Experimental results on a collection of benchmark data sets demonstrate that NAML is effective in learning a good distance metric and improving the clustering performance . In general , approaches based on learning a convex combination of kernels can be applied for heterogeneous data integration from different data sources . We plan to apply the proposed algorithm for clustering from multiple biological data , eg , amino acid sequences , hydropathy profiles , and gene expression data . We reveal the close connection between the proposed algorithm and regularized spectral clustering . The selection of a good Laplacian matrix , which is determined by several parameters such as the number of nearest neighbors , is one of the key issues in spectral clustering . Another line of future work is to study how to combine a set of pre specified Laplacian matrices to achieve better performance in spectral clustering .
Acknowledgments This research is sponsored in part by Arizona State University and by the National Science Foundation Grant IIS0612069 . iris lymph promoter satimage
I
M
/
C C A
I
M
/
C C A
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0.7
0.65
0.6
0.55
0.5
0.45
0.4
0.35
ACC MI
10−7 10−6 10−5 10−4 10−3 10−2 10−1 100 101 102 103 104 105
λ solar
ACC MI
10−7 10−6 10−5 10−4 10−3 10−2 10−1 100 101 102 103 104 105
λ
I
M
/
C C A
I
M
/
C C A
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
ACC MI
10−7 10−6 10−5 10−4 10−3 10−2 10−1 100 101 102 103 104 105
λ wine
ACC MI
10−7 10−6 10−5 10−4 10−3 10−2 10−1 100 101 102 103 104 105
λ
I
M
/
C C A
I
M
/
C C A
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0.8
0.7
0.6
0.5
0.4
0.3
0.2
ACC MI
10−7 10−6 10−5 10−4 10−3 10−2 10−1 100 101 102 103 104 105
λ ar03p
ACC MI
10−7 10−6 10−5 10−4 10−3 10−2 10−1 100 101 102 103 104 105
λ
I
M
/
C C A
I
M
/
C C A
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.9
0.85
0.8
0.75
0.7
0.65
0.6
0.55
0.5
ACC MI
10−7 10−6 10−5 10−4 10−3 10−2 10−1 100 101 102 103 104 105
λ orl10p
ACC MI
10−7 10−6 10−5 10−4 10−3 10−2 10−1 100 101 102 103 104 105
λ
Figure 2 : ACC and MI vs . different λ values . The x axis corresponds to different λ values and the y axis corresponds to ACC or MI values .
6 . REFERENCES [ 1 ] E . D . Andersen and K . D . Andersen . The MOSEK interior point optimizer for linear programming : an implementation of the homogeneous algorithm . In T . T . H . Frenk , K . Roos and S . Zhang , editors , High Performance Optimization , pages 197–232 . Kluwer Academic Publishers , 2000 .
[ 2 ] A . Argyriou , R . Hauser , C . Micchelli , and M . Pontil . A DC programming algorithm for kernel selection . In Proceedings of the Twenty third International Conference on Machine Learning , pages 41–48 , 2006 .
[ 3 ] F . R . Bach , G . R . G . Lanckriet , and M . I . Jordan .
Multiple kernel learning , conic duality , and the SMO algorithm . In Proceedings of the Twenty first International Conference on Machine Learning , 2004 .
[ 4 ] M . Belkin and P . Niyogi . Laplacian eigenmaps for dimensionality reduction and data representation . Advances in Neural Information Processing Systems , 15 , 2003 .
[ 5 ] C . Blake and C . Merz . UCI repository of machine learning databases , 1998 .
[ 6 ] S . Boyd and L . Vandenberghe . Convex Optimization .
Cambridge University Press , 2004 .
[ 7 ] I . S . Dhillon , Y . Guan , and B . Kulis . A unified view of kernel k means , spectral clustering and graph partitioning . Technical report , TR 04 25 , UTCS , 2004 .
[ 8 ] P . Diaconis and D . Freedman . Asymptotics of graphical projection pursuit . Annuals of Statistics , 12:793–815 , 1984 .
[ 9 ] C . Ding and T . Li . Adaptive dimension reduction using discriminant analysis and k means clustering . In Proceedings of the Twenty fourth International Conference on Machine Learning , 2007 .
[ 10 ] C . Domeniconi , D . Papadopoulos , D . Gunopulos , and
S . Ma . Subspace clustering of high dimensonal data . In Proceedings of the SIAM International Conference on Data Mining , 2004 .
[ 11 ] J . H . Friedman . Regularized discriminant analysis .
Journal of the American Statistical Association , 84(405):165–175 , 1989 .
[ 12 ] K . Fukunaga . Introduction to Statistical Pattern
Recognition . San Diego : Academic Press , 2 edition , 1990 .
[ 13 ] G . Fung , M . Dundar , J . Bi , and B . Rao . A fast iterative algorithm for Fisher discriminant using heterogeneous kernels . In Proceedings of the Twenty first International Conference on Machine Learning , 2004 .
[ 14 ] G . H . Golub and C . F . Van Loan . Matrix
Computations . The Johns Hopkins University Press , third edition , 1996 .
[ 15 ] P . Hall and K . Li . On almost linearity of low dimensional projections from high dimensional data . Annuals of Statistics , 21:867–889 , 1993 .
[ 16 ] T . Jebara . Multi task feature and kernel selection for
SVMs . In Proceedings of the Twenty first International Conference on Machine Learning , 2004 .
[ 17 ] I . Jolliffe . Principal Component Analysis . Springer ;
2nd edition , 2002 .
[ 18 ] S J Kim , A . Magnani , and S . Boyd . Optimal kernel selection in kernel Fisher discriminant analysis . In Proceedings of the Twenty third International Conference on Machine Learning , pages 465–472 , 2006 .
[ 19 ] F . D . la Torre Frade and T . Kanade . Discriminative cluster analysis . In Proceedings of the Twenty third International Conference on Machine Learning , pages 241–248 , 2006 .
[ 20 ] G . Lanckriet , N . Cristianini , P . Bartlett , L . E . Ghaoui , and M . I . Jordan . Learning the kernel matrix with semidefinite programming . Journal of Machine Learning Research , 5:27–72 , 2004 .
Table 5 : The weight of each input kernel when using different λ values on the AR03P data set . The weights of most kernels shrink to zero when λ becomes large . The accuracies of K means using each of the ten kernels are shown in the first row .
AR03P Acc
λ 0 −8 10 −7 10 −6 10 −5 10 −4 10 −3 10 −2 10 −1 10 100 101 102 103 104 105
KER 1 0.426 0.026 0.001 0.001 0.001 0.001 0.001
KER 2 0.456 0.027 0.001 0.001 0.001 0.001 0.001
KER 3 0.436 0.027 0.001 0.001 0.001 0.001 0.001
0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0
KER 4 0.456 0.028 0.001 0.001 0.001 0.001 0.001
0 0 0 0 0 0
0.013 0.014 0.013
KER 5 0.595 0.029 0.001 0.001 0.001 0.001 0.001
KER 6 0.600 0.030 0.002 0.002 0.002 0.002 0.002
0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0
KER 7 0.610 0.031 0.002 0.002 0.002 0.002 0.002 0.001
KER 8 0.610 0.032 0.002 0.002 0.002 0.002 0.002 0.001
KER 9 0.615 0.034 0.002 0.002 0.002 0.002 0.002 0.001
0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0
KER 10
0.615 0.035 0.002 0.002 0.002 0.002 0.002 0.014 0.017 0.017 0.017 0.017 0.017
0 0 0
[ 21 ] D . Lewis , T . Jebara , and W . S . Noble . Nonstationary kernel combination . In Proceedings of the Twenty third International Conference on Machine Learning , pages 553–560 , 2006 .
[ 22 ] L . Lovasz and M . Plummer . Matching Theory . North
Holland , 1986 .
[ 23 ] L . K . Saul and S . T . Roweis . Think globally , fit locally : Unsupervised learning of low dimensional manifolds . Journal of Machine Learning Research , 4:119–155 , 2003 .
[ 24 ] S . Sch¨olkopf and A . Smola . Learning with Kernels :
Support Vector Machines,Regularization , Optimization and Beyond . MIT Press , 2002 .
[ 25 ] J . Shawe Taylor and N . Cristianini . Kernel Methods for Pattern Analysis . Cambridge University Press , 2004 .
[ 26 ] N . Shental , T . Hertz , D . Weinshall , and M . Pavel .
Adjustment learning and relevant component analysis . In Proceedings of the Seventh European Conference on Computer Vision , pages 776–792 , London , UK , 2002 . Springer Verlag .
[ 27 ] A . Smola and I . Kondor . Kernels and regularization on graphs . In Proceedings of the Annual Conference on Computational Learning Theory , 2003 . [ 28 ] S . Sonnenburg , G . R¨atsch , C . Sch¨afer , and
B . Sch¨olkopf . Large Scale Multiple Kernel Learning . Journal of Machine Learning Research , 7:1531–1565 , July 2006 . classification . In Y . Weiss , B . Sch¨olkopf , and J . Platt , editors , Advances in Neural Information Processing Systems , Cambridge , MA , 2005 . MIT Press .
[ 32 ] E . Xing , A . Ng , M . Jordan , and S . Russell . Distance metric learning with application to clustering with side information . In Advances in Neural Information Processing Systems , 2002 .
[ 33 ] B . Yan and C . Domeniconi . An adaptive kernel method for semi supervised clustering . In Proceedings of the Seventeenth European Conference on Machine Learning , 2006 .
[ 34 ] L . Yang and R . Jin . Distance metric learning : A comprehensive survey . Technical report , Department of Computer Science and Engineering , Michigan State University , 2006 .
[ 35 ] L . Yang , R . Jin , R . Sukthankar , and Y . Liu . An efficient algorithm for local distance metric learning . In Proceedings of the Twenty first National Conference on Artificial Intelligence , 2006 .
[ 36 ] J . Ye , S . Ji , and J . Chen . Learning the kernel matrix in discriminant analysis via quadratically constrained quadratic programming . In Proceedings of the Thirteenth ACM SIGKDD International Conference On Knowledge Discovery and Data Mining , 2007 .
[ 37 ] J . Ye , Z . Zhao , and H . Liu . Adaptive distance metric learning for clustering . In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition , 2007 .
[ 29 ] J . Tenenbaum , V . de Silva , and J . Langford . A global
[ 38 ] H . Zha , C . Ding , M . Gu , X . He , and H . Simon . geometric framework for nonlinear dimensionality reduction . Science , 290(5500):2319–2323 , 2000 .
[ 30 ] H . Valizadegan and R . Jin . Generalized maximum margin clustering and unsupervised kernel learning . In Advances in Neural Information Processing Systems , 2006 .
[ 31 ] K . Weinberger , J . Blitzer , and L . Saul . Distance metric learning for large margin nearest neighbor
Spectral relaxation for k means clustering . In Advances in Neural Information Processing Systems , 2001 .
