Mining Optimal Decision Trees from Itemset Lattices
Siegfried Nijssen
KULeuven
Celestijnenlaan 200 A
Leuven , Belgium
Elisa Fromont
KULeuven
Celestijnenlaan 200 A
Leuven , Belgium fsiegfriednijssen,elisafromontg@cskuleuvenbe
ABSTRACT We present DL8 , an exact algorithm for flnding a decision tree that optimizes a ranking function under size , depth , accuracy and leaf constraints . Because the discovery of optimal trees has high theoretical complexity , until now no efiorts have been made to compute such trees for real world datasets . An exact algorithm is of both scientiflc and practical interest . From a scientiflc point of view , it can be used as a gold standard to evaluate the performance of heuristic decision tree learners and to gain new insight in these traditional learners . From the application point of view , it can be used to discover trees that cannot be found by heuristic decision tree learners . The key idea behind our algorithm is the relation between constraints on decision trees and constraints on itemsets . We propose to exploit lattices of itemsets , from which we can extract optimal decision trees in linear time . We give several strategies to e–ciently build these lattices . Experiments show that under the same constraints , DL8 has better test results than C4.5 which conflrm that exhaustive search does not always imply overfltting . The results also show that DL8 is a useful and interesting tool to learn decision trees under constraints .
Keywords Decision tree learning , Frequent itemset mining , Itemset lattices , Formal Concept Analysis , Constraint based mining
1 .
INTRODUCTION
Decision trees are among the most popular prediction models in machine learning and data mining , because there are e–cient , relatively easily understandable learning algorithms and the models are easy to interpret . From this perspective , it is surprising that mining decision trees under constraints has not been given much attention . For the problems listed below , currently no broadly applicable algorithm exists even though steps in this direction were made in [ 8 ] for the last problem :
† given a dataset D , flnd the most accurate tree on training data in which each leaf covers at least n examples ;
† given a dataset D , flnd the k most accurate trees on training data in which the majority class in each leaf covers at least n examples more than any of the minority classes ;
† given a dataset D , flnd the most accurate tree on training data in which each leaf has a high statistical correlation with the target class according to a ´2 test ;
† given a dataset D , flnd the smallest decision tree in which each leaf contains at least n examples , and the expected accuracy is maximized on unseen examples ;
† given a dataset D , flnd the smallest/shallowest deci sion tree which has an accuracy higher than minacc ;
In the interactive process that knowledge discovery in databases is , the ability to pose queries that answer these questions can be very valuable .
Most known algorithms for building decision trees , for instance C4.5 , use a top down induction paradigm , in which a good split is chosen heuristically . If such algorithms do not flnd a tree that satisfles the specifled constraints , this does not mean that such a tree does not exist|it only means that the chosen heuristic is not good enough to flnd it . An exact algorithm could be desirable to answer queries without uncertainty . Furthermore , to assess the quality of heuristic learners , it is of interest to know for a su–ciently large number of datasets what their true optimum under given constraints is . This would allow us to gain deeper insight in the predictive behavior of decision trees . For instance , [ 19 ] reported that for small , mostly artiflcial datasets , small decision trees are not always preferable in terms of generalization ability , while [ 29 ] showed that when learning rules , exhaustive searching and overfltting are orthogonal . An e–cient algorithm for learning decision trees under constraints allows us to investigate these observations for larger datasets and more complex models .
To the best of our knowledge , few attempts have been made to compute exact optimal trees in a setting which optimizes decision trees under a wide range of constraints ; most people have not seriously considered the problem as it is known to be NP hard [ 12 ] , and therefore , an e–cient algorithm can most likely not exist . The data mining community , however , has an interesting track record of dealing with exponential problems with reasonable computation times . In particular , for many years , the problem of frequent itemset mining , which is exponential in its nature , has attracted a lot of research [ 1 , 34 , 11 ] and although this problem is not always e–ciently solvable in theory , in practice many algorithms have been applied successfully .
In this paper , we propose DL8 , an exact algorithm for building decision trees that does not rely on the traditional approach of heuristic top down induction , and addresses the problem of flnding exact optimal decision trees under constraints . Its key feature is that it exploits a relation between constraints on itemsets and decision trees . Even though our algorithm is not expected to work on all possible datasets , we will provide evidence that for a reasonable number of datasets , our approach is feasible and therefore a useful addition to the data mining toolbox .
This paper is organized as follows . In Section 2 , we introduce the concepts of decision trees and itemsets . In Section 3 , we describe precisely which optimal trees we consider . In Section 4 , we motivate the use of such optimal trees . In section 5 , we present our algorithm and its connection to frequent itemset mining . In Section 6 , we evaluate the e–ciency of our algorithm ; we compare the accuracy and size of the trees computed by our system with the trees learned by C45 Section 7 gives related work . We conclude in Section 8 .
2 .
ITEMSET LATTICES FOR DECISION TREE MINING
Let us flrst introduce some background information about frequent itemsets and decision trees .
Let I = fi1 ; i2 ; : : : ; img be a set of items and let D = fT1 ; T2 ; : : : ; Tng be a bag of transactions , where each transaction Tk is an itemset such that Tk ( cid:181 ) I . A transaction Tk contains a set of items I ( cid:181 ) I ifi I ( cid:181 ) Tk . The transaction identifler set ( TID set ) t(I ) ( cid:181 ) f1 ; 2 ; : : : ng of an itemset I ( cid:181 ) I is the set of identiflers of all transactions that contain itemset I .
The frequency of an itemset I ( cid:181 ) I is deflned to be the number of transactions that contain the itemset , ie freq(I ) = jt(I)j ; the support of an itemset is support(I ) = freq=jDj . An itemset I is said to be frequent if its support is higher than a given threshold minsup ; this is written as support(I ) ‚ minsup ( or , equivalently , freq(I ) ‚ minfreq ) .
In this work , we are interested in flnding frequent itemsets for databases that contain examples labeled with classes c 2 C . If we compute the frequency freqc(I ) of an itemset I for each class c separately , we can associate to each itemset the class label for which its frequency is highest . The resulting rule I ! c(I ) , where c(I ) = argmaxc02C freqc0 ( I ) is called a class association rule .
A decision tree aims at classifying examples by sorting them down a tree . The leaves of a tree provide the classiflcations of examples [ 17 ] . Each node of a tree specifles a test on one attribute of an example , and each branch of a node corresponds to one of the possible values of the attribute . We assume that all tests are boolean ; nominal attributes are transformed into boolean attributes by mapping each possible value to a separate attribute . The input of a decision tree learner is then a binary matrix B , where Bij contains the value of attribute i of example j .
Our results are based on the following observation .
Observation 1 . Let us transform a binary table B into transactional form D such that Tj = fijBij = 1g[f:ijBij =
0g . Then the examples that are sorted down every node of a decision tree for B are characterized by an itemset of items occurring in D .
For example , consider the decision tree in Figure 1 . We can determine the leaf to which an example belongs by checking which of the itemsets fBg , f:B ; Cg and f:B ; :Cg it includes . We denote the set of these itemsets with leaves(T ) . Similarly , the itemsets that correspond to paths in the tree are denoted with paths(T ) . In this case , paths(T ) = f ; ; fBg ; f:Bg ; f:B ; Cg ; f:B ; :Cgg .
B
0 C
1
1
1
1
0
0
Figure 1 : An example tree
The leaves of a decision tree correspond to class association rules , as leaves have associated classes . In decision tree learning , it is common to specify a minimum number of examples that should be covered by each leaf . For association rules , this would correspond to giving a support threshold . The accuracy of a decision tree is derived from the number of misclassifled examples in the leaves : accuracy(T ) = jDj¡e(T )
, where jDj e(T ) = X
I2leaves(T ) e(I ) and e(I ) = freq(I)¡freqc(I)(I ) :
A further illustration of the relation between itemsets and decision trees is given in Figure 2 . In this flgure , every node represents an itemset ; an edge denotes a subset relation . Highlighted is one possible decision tree , which is nothing else than a set of itemsets . The branches of the decision tree correspond to subset relations .
From the theory of frequent itemset mining , it is known that itemsets form a lattice ( these are typically depicted as in Figure 2 ) . In this paper we present DL8 , an algorithm for mining Decision trees from Lattices .
3 . QUERIES FOR DECISION TREES
The problems that we address in this paper , can be seen as queries to a database . These queries consist of three parts . The flrst part specifles the constraints on the nodes of the decision trees .
1 . T1 := fT jT 2 DecisionTrees ; 8I 2 paths(T ) ; p(I)g
The set T1 is called the set of locally constrained decision trees and DecisionTrees is the set of all possible decision trees . The predicate p(I ) expresses a constraint on paths . In our simplest setting , p(I ) := ( freq(I ) ‚ minf req ) . The predicate p(I ) must fulflll these properties :
† the evaluation of p(I ) must be independent of the tree
T of which I is part .
† p must be anti monotonic . A predicate p(I ) on itemsets I ( cid:181 ) I is called anti monotonic ifi p(I ) ^ ( I 0 ( cid:181 ) I ) ) p(I 0 ) .
{} A ?
A
¬A
B
¬B
C
¬C
{A} B ?
{B}
{C}
{¬ A}
C ?
{¬ B}
{¬ C}
B
¬B
C
¬C
A
¬A
C
¬C
A
¬A B
¬B
B
¬B C
¬C
A
¬A C
¬C
A
¬A B
¬B
{AB}
{A¬ B}
C ?
{AC}
{A¬ C}
{¬ AB}
{BC}
{¬ C}
{¬ AC }
{¬ BC}
{¬ A¬ B}
{¬ A¬ C }
{¬ B¬ C}
C
¬C
C
¬C
B
¬B
B
¬B
C
¬C
A
¬A
A
¬A
B
¬B
A
¬A
C
¬C
B
¬B
A
¬A
{ABC}
{AB¬ C}
{A¬ BC }
{A¬ B¬ C }
{¬ ABC}
{¬ AB¬ C}
{¬ A¬ BC}
{¬ A¬ B¬ C}
Figure 2 : An itemset lattice for items fA ; :A ; B ; :B ; C ; :Cg ; binary decision tree A(B(C(l,l),l),C(l,l ) ) is hidden in this lattice
We can distinguish two types of local constraints : coveragebased constraints , such as frequency , of which the fulflllment is entirely dependent on t(I ) and pattern based constraints , such as the size of itemsets , of which the fulflllment depends on the properties of the ( items in the ) itemset itself . In the following , we consider only coverage based constraints ; extensions to pattern based constraints are possible , but beyond the scope of this paper .
The second ( optional ) part expresses constraints that refer to the tree as a whole .
2 . T2 := fT jT 2 T1 ; q(T )g
Set T2 is called the set of globally constrained decision trees . Formula q(T ) is a conjunction of constraints of the form f ( T ) • ( cid:181 ) , where f ( T ) can be
† e(T ) , to constrain the error of a tree on a training dataset ;
Query 1 . Small Accurate Trees with Frequent leaves .
T := fT j
T 2 DecisionTrees ; 8I 2 paths(T ) ; freq(I ) ‚ minfreqg output argminT 2T [ e(T ) ; size(T ) ] .
In other words , we have p(T ) := ( freq(I ) ‚ minfreq ) , q(T ) := true and r(T ) := [ e(T ) ; size(T ) ] . This query investigates all decision trees in which each leaf covers at least minfreq examples of the training data . Among these trees , we flnd the smallest most accurate one . To retrieve Accurate Trees of Bounded Size , Query 1 can be transformed such that q(T ) := size(T ) • maxsize .
One possible scenario in which DL8 can be used , is the following . Assume that we have already applied a heuristic decision tree learner , such as C4.5 , and we have some idea about decision tree error ( maxerror ) and size ( maxsize ) . Then we can run the following query :
† ex(T ) , to constrain the expected error on unseen exam
Query 2 . Accurate Trees of Bounded Size and Accuracy . ples , according to some predeflned estimate ;
† size(T ) , to constrain the number of nodes in a tree ;
† depth(T ) , to constrain the length of the longest root leaf path in a tree .
In the mandatory third step , we express a preference for a tree in the set T2 .
3 . output argminT 2T2 [ r1(T ) ; r2(T ) ; : : : ; rn(T ) ]
The tuples r(T ) = [ r1(T ) ; r2(T ) ; : : : ; rn(T ) ] are compared lexicographically and deflne a ranked set of globally constrained decision trees ; ri 2 fe ; ex ; size ; depthg . Our current algorithm requires that at least e and size or ex and size be used in the ranking ; If depth ( respectively size ) is used in the ranking before e or ex , then q must contain an atom depth(T ) • maxdepth ( respectively size(T ) • maxsize ) .
We do not constrain the order of size(T ) , e(T ) and depth(T ) in r . We are minimizing the ranking function r(T ) , thus , our algorithm is an optimization algorithm . The trees that we search for are optimal in terms of the problem setting that is deflned in the query .
To illustrate our querying mechanism we will now give several examples . j j
T2 := fT
T1 := fT
T 2 DecisionTrees ; 8I 2 paths(T ) ; freq(I ) ‚ minfreqg T 2 T1 ; size(T ) • maxsize ; e(T ) • maxerrorg output argminT 2T2 [ size(T ) ; e(T ) ] .
This query flnds the smallest tree that achieves at least the same accuracy as the tree learned by C45
The previous queries aim at flnding compact models that maximize training set accuracy . Such trees might however overflt training data . Another application of DL8 is to obtain trees with high expected accuracy . Several algorithms for estimating test set accuracy have been presented in the literature . One such estimate is at the basis of the reduced error pruning algorithm of C45 Essentially , C4.5 computes an additional penalty term x(freq1(I ) ; : : : freqn(I ) ) for each leaf I of the decision tree , from which we can derive a new estimated number of errors ex(T ) = X
I2leaves(T ) e(I ) + x(freq1(I ) ; : : : freqn(I) ) :
We can now also be interested in answering the following query .
Query 3 . Small Accurate Pruned Trees .
T := fT j
T 2 DecisionTrees ; 8I 2 paths(T ) ; freq(I ) ‚ minfreqg output argminT 2T [ ex(T ) ; size(T ) ] .
This query would flnd the most accurate tree after pruning such as done by C45 Efiectively , the penalty terms make sure that trees with less leaves are sometimes preferable even if they are less accurate .
4 . MOTIVATING EXAMPLES
To motivate our work , it is useful to brie(cid:176)y consider two examples that illustrate what kind of trees cannot be found if the well known information gain ( ratio ) heuristic of C4.5 is used to answer Query 1 of Section 3 .
A B C Class 1 1 1 1 1 1 0 0 0 1
1 1 0 0 0
0 1 1 0 1
# 40£ 40£ 5£ 10£ 5£
A B C Class 1 1 1 0 0 0 0 0 1 0 0 0
1 1 1 1 0 0
1 0 0 1 0 1
# 30£ 20£ 8£ 12£ 12£ 18£
Figure Database 1
3 :
Figure Database 2
4 :
As a flrst example , consider the database in Figure 3 , in which we have 2 target classes . Assume that we are interested in answering Query 1 with minfreq = 10 . An optimal tree exists ( see Figure 1 ) , but a heuristic learner will not flnd it , as it prefers attribute A in the root : A has information gain 0:33 ( resp . ratio 0:54 ) , while B only has information gain 0:26 ( resp . ratio 0:37 ) .
As a second example , consider the database in Figure 4 , which is a variation of the XOR problem . Then the correct answer to Query 1 with minfreq = 1 is given in Figure 5(a ) , but the use of information gain ( ratio ) would yield the tree in Figure 5(b ) , as the information gain ( resp . ratio ) of A is 0:098 ( resp . 0:098 ) , while the information gain of C is 0:029 ( resp . 0:030 ) .
We learn from these examples that the proportions of examples can ‘fool’ heuristic decision trees into an suboptimal shape as already noticed by [ 24 ] . Optimal learners are less sensitive to such behavior .
5 . THE DL8 ALGORITHM
We will now present the DL8 algorithm for answering decision tree queries . Pseudo code of the algorithm is given in Algorithm 1 .
Parameters of DL8 are the local constraint p , the ranking function r , and the global constraints ; each global constraint is passed in a separate parameter ; global constraints that are not specifled , are assumed to be set to 1 . The most important part of DL8 is its recursive search procedure . Given an input itemset I , DL8 Recursive computes one or more decision trees for the transactions t(I ) that contain the itemset I . More than one decision tree is returned only if a depth or size constraint is specifled . Let r(T ) = [ r1(T ) ; : : : ; rn(T ) ] be the ranking function , and let k be the index of the obligatory error function in this ranking . If r1 ; : : : ; rk¡1 2 fdepth ; sizeg then , for every allowed value of depth d and size s , DL8 Recursive outputs the best tree
A
0 C
1
C
1
1
0
0
1
0
0
B
1
0
0
1
C
1 A
0 B
1
1
0
0
1
0
0
1
( a ) Smallest
( b ) Learned using heuristics
Figure 5 : Two accurate trees for database 2
T that can be constructed for the transactions t(I ) according to the ranking [ rk(T ) ; : : : ; rn(T ) ] , such that size(T ) • s and depth(T ) • d .
In DL8 Recursive , we use several functions : l(c ) , which returns a tree consisting of a single leaf with class label c ; n(i ; T1 ; T2 ) , which returns a tree that contains test i in the root , and has T1 and T2 as left hand and right hand branches ; et(T ) , which computes the error of tree T when only the transactions in TID set t are considered ; and flnally , we use a predicate pure(I ) ; predicate pure blocks the recursion if all examples t(I ) belong to the same class .
The algorithm is most easily understood if maxdepth = 1 , maxsize = 1 , maxerror = 1 and r(T ) = [ e(T ) ] ; in this case , DL8 Recursive combines only two trees for each i 2 I , and returns the single most accurate tree in line 35 .
The correctness of the DL8 algorithm is essentially based on the fact that the left hand branch and the right hand branch of a node in a decision tree can be optimized independently . In more detail , the correctness follows from the following observations .
( line 2 9 ) the valid ranges of sizes and depths are computed here if a size or depth constraint was specifled ;
( line 12 ) for each depth and size satisfying the constraints DL8 Recursive flnds the most accurate tree possible . Some of the accuracies might be too low for the given constraint , and are removed from consideration .
( line 20 ) a candidate decision tree for classifying the exam ples t(I ) consists of a single leaf .
( line 21 ) if all examples in a set of transactions belong to the same class , continuing the recursion is not necessary ; after all , any larger tree will not be more accurate than a leaf , and we require that size is used in the ranking . More sophisticated pruning is possible in some special cases . In the Appendix , an improved predicate pure is given , which allows us to stop even if not all examples belong to the same class .
( line 24 ) in this line the anti monotonic property of the predicate p(I ) is used : an itemset that does not satisfy the predicate p(I ) cannot be part of a tree , nor can any of its supersets ; therefore the search is not continued if p(I [ fig ) = false or p(I [ f:ig ) = false .
( line 23{34 ) these lines make sure that each tree that should be part of the output T , is indeed returned . We can prove this by induction . Assume that for the set of transactions t(I ) , tree T should be part of T as it is the most accurate tree that is smaller than s and shallower than d for some s 2 S and d 2 D ; assume T is not a leaf , and contains test i in the root . Then T must have a left hand branch T1 and a righthand branch T2 . Tree T1 must be the most accurate tree that can be constructed for t(I [ fig ) with size at most size(T1 ) and depth at most depth(T1 ) ; similarly , T2 must be the most accurate tree that can be constructed for t(I [ f:ig ) under depth and size constraints . We can inductively assume that trees with these constraints are found by DL8 Recursive(I [ fig ) and DL8 Recursive(I[f:ig ) as size(T1 ) ; size(T2 ) • maxsize and depth(T1 ) ; depth(T2 ) • maxdepth . Consequently T ( or a tree with equal statistics ) must be among the trees found by combining results from the two recursive procedure calls in line 28 .
A key feature of DL8 Recursive is that in line 35 it stores every result that it computes . Consequently , DL8 avoids that optimal decision trees for any itemset are computed more than once ; furthermore , we do not need to store the resulting decision trees entirely ; it is su–cient to store their root and statistics ( error , possibly size and depth ) , as lefthand and right hand subtrees can be recovered from the stored results for the left hand and right hand itemsets if necessary .
Note that in our algorithm , we output the best tree according to the ranking . The k ¡ best trees could also be straightforwardly output .
To e–ciently index the itemsets I , a trie [ 7 ] data structure can be used . As with most data mining algorithms , the most time consuming operations are those that access the data . In the following , we will provide four related strategies to obtain the frequency counts that are necessary to check the constraints and compute accuracies : the simple single step approach , the frequent itemset mining ( FIM ) approach , the constrained FIM approach , and the closure based single step approach . The Simple Single Step Approach The most straightforward approach , referred to as DL8Simple , computes the itemset frequencies while DL8 is executing . In this case , once DL8 Recursive is called for an itemset I , we obtain the frequencies of I in a scan over the data , and store the result to avoid later recomputations . The FIM Approach An alternative approach is based on the observation that every itemset that occurs in a tree , must satisfy the local constraint p . If p is a minimum frequency constraint , we can use a frequent itemset miner to obtain the frequencies in a preprocessing step . DL8 then operates on the resulting set of itemsets , annotating every itemset with optimal decision trees .
Many frequent itemset miners have been studied in the literature ; all of these can be used with small modiflcations to output the frequent itemsets in a convenient form and determine frequencies in multiple classes [ 1 , 34 , 11 , 31 ] .
We implemented an extension of Apriori that flrst computes and stores in a trie all frequent itemsets , and then runs DL8 on the trie . This approach is referred to as AprioriFreq+DL8 . Compared to other itemset miners , we expect that the additional runtime to store all itemsets in Apriori
S ˆ f1 ; 2 ; : : : ; maxsizeg
D ˆ f1 ; 2 ; : : : ; maxdepthg
T ˆ fT jT 2 T ; e(T ) • maxerrorg return undeflned
S ˆ f1g
D ˆ f1g
Algorithm 1 DL8(p , pb , maxsize,maxdepth,maxerror ; r ) 1 : 2 : if maxsize 6= 1 then 3 : 4 : else 5 : 6 : if maxdepth 6= 1 then 7 : 8 : else 9 : 10 : T ˆDL8 Recursive( ; ) 11 : if maxerror 6= 1 then 12 : 13 : if T = ; then 14 : 15 : return argminT 2T r(T ) 16 : 17 : procedure DL8 Recursive(I ) 18 : 19 : 20 : 21 : 22 : 23 : 24 : 25 : 26 : 27 : 28 : 29 : 30 : 31 : 32 : 33 : 34 : 35 : 36 : end procedure
T1 ˆ DL8 Recursive(I [ fig ) T2 ˆ DL8 Recursive(I [ f:ig ) for all T1 2 T1 ; T2 2 T2 do end for store T as the result for I and return T end if
T ˆ ; for all d 2 D ; s 2 S do
C ˆ fl(c(I))g if pure(I ) then for all i 2 I do if DL8 Recursive(I ) was computed before then return stored result store C as the result for I and return C if p(I [ fig ) = true and p(I [ f:ig ) = true then
C ˆ C [ fn(i ; T1 ; T2)g
L ˆ fT 2 Cjdepth(T ) • d ^ size(T ) • sg T ˆ T [ fargminT 2L[rk = et(I)(T ) ; : : : ; rn(T )]g is the lowest , as Apriori already builds a trie of candidate itemsets itself .
If we assume that the output of the frequent itemset miner consists of a graph structure such as Figure 2 , then DL8 operates in time linear in the number of edges of this graph . The Constrained FIM Approach Unfortunately , the frequent itemset mining approach may compute frequencies of itemsets that can never be part of a decision tree . For instance , assume that fAg is a frequent itemset , but f:Ag is not ; then no tree answering example Query 1 will contain a test for attribute A ; itemset fAg is redundant . In this section , we show that an additional local , anti monotonic constraint can be used in the frequent itemset mining process to make sure that no such redundant itemsets are enumerated . Proofs of the theorems given in this section can be found in [ 22 ] .
If we consider the DL8 Simple algorithm , an itemset I = fi1 ; : : : ; ing is stored only if there is an order [ ik1 ; ik2 ; : : : ; ikn ] of the items in I ( which corresponds to an order of recursive calls to DL8 Recursive ) such that for none of the proper preflxes I 0 = [ ik1 ; ik2 ; : : : ; ikm ] ( m < n ) of this order
† the :pure(I 0 ) predicate is false in line ( 21 ) ;
† the conjunction pb(I 0 [ fikm+1 g ) ^ pb(I 0 [ f:ikm+1 g ) is false in line ( 24 ) .
It is helpful to negate the pure predicate , as one can easily see that :pure is an anti monotonic predicate ( every superset of a pure itemset , must also be pure ) . From now on , we will refer to :pure as a leaf constraint , as it deflnes a property that is only allowed to hold in the leaves of a tree . We can now formalize the principle of itemset relevancy .
Definition 1 . Let p1 be a local anti monotonic tree constraint and p2 be an anti monotonic leaf constraint . Then the relevancy of I , denoted by rel(I ) , is deflned by rel(I ) = p1(I ) ^ p2(I ) true false if I = ; if 9i 2 I st rel(I ¡ i ) ^ p2(I ¡ i)^ p1(I ) ^ p1(I ¡ i [ :i ) otherwise
( Case 1 )
( Case 2 ) ( Case 3 )
8>>>< >>> :
Theorem 1 . Let L1 be the set of itemsets stored by DL8Simple , and let L2 be the set of itemsets fI ( cid:181 ) Ijrel(I ) = trueg . Then L1 = L2 .
Relevancy is a property that can be pushed in a frequent itemset mining process .
Theorem 2 . Itemset relevancy is an anti monotonic prop erty .
It is relatively easy to integrate the computation of relevancy in frequent itemset mining algorithms , as long as the order of itemset generation is such that all subsets of an itemset I are enumerated before I is enumerated itself . Assume that we have already computed all relevant itemsets that are a subset of an itemset I . Then we can determine for each i 2 I if the itemset I ¡ i is part of this set , and if so , we can derive the class frequencies of I ¡ i [ :i using the formula freqk(I ¡ i [ :i ) = freqk(I ¡ i ) ¡ freqk(I ) . If for each i either I ¡ i is not relevant , or the predicate p(I ¡ i [ :i ) fails , we can prune I .
Pruning of this kind can be integrated in both depth flrst and breadth flrst frequent itemset miners . Note that the pureness property can be reflned to allow a more e–cient pruning ( see [ 22 ] for the deflnition of the Loose Pureness property ) . In case depth is the flrst ranking function , levelwise algorithms such as Apriori have an important beneflt : after each level of itemsets is generated , we could run DL8 to obtain the most accurate tree up to that depth . Apriori can stop at the lowest level at which a tree is found that fulflls the constraints . We implemented two versions of DL8 in which the relevancy constraints are pushed in the frequent itemset mining process : DL8 Apriori , which is based on Apriori [ 1 ] , and DL8 Eclat , which is based on Eclat [ 34 ] . The Closure Based Single Step Approach In the simple single step approach , we stored the optimal decision trees for every itemset separately . However , if the local constraint is only coverage based , it is easy to see that for two itemsets I1 and I2 , if t(I1 ) = t(I2 ) , the result of DL8 Recursive(I1 ) and DL8 Recursive(I2 ) must be the same . To reduce the number of results that we have to store , we should avoid storing such duplicate sets of results .
The solution that we propose is to compute for every item set its closure . Let i(t ) be the function which computes i(t ) = \k2tTk
Datasets #Ex #Test Datasets #Ex #Test anneal a credit balance breast chess tumor segment soybean
18 55 45
3466
812 653 625 683 3196 768 1000 296 ionosphere 351 mushroom 8124 pendigits 7494 diabetes g credit heart
36 56 13 28 41 25 77 35 99 116 49
336 2310 630 3190 3247 846 435 990 1484 101
36 55 49 48 23 15 splice thyroid vehicle vote vowel yeast zoo
Figure 6 : Datasets description for a TID set t , then the closure of itemset I is the itemset i(t(I) ) . An itemset I is closed ifi I = i(t(I) ) . If t(I1 ) = t(I2 ) it is easy to see that also i(t(I1 ) ) = i(t(I2) ) . Thus , in the trie data structure that is used in the simple single step approach , we could index the results on i(t(I ) ) instead of I itself .
We incorporate this observation as follows in Algorithm 1 . In line 24 , we do not only compute the frequency of an itemset , but also its closure I 0 in the data . In line 18 we check if DL8 Recursive(I ) was already computed earlier by searching for I 0 in a trie data structure . In line 35 , we associate the result to I 0 instead of I itself .
Our single step approach which relies on closed itemset indexing is called DL8 Closed . Obviously , DL8 Closed will never consider more itemsets than DL8 Simple , DL8Apriori or DL8 Eclat ; itemsets stored by DL8 Closed may however be longer as they contain all items in their closure .
Our implementation of DL8 Closed is based on optimization strategies that are common in depth flrst frequent itemset miners , such as the use of projected databases , with modiflcations that make sure that the space complexity of our algorithm is ( cid:181)(n + m ) , where n is the size of the trie that stores all closed itemsets , and m is the size of the binary matrix that contains the data . For more details see [ 22 ] .
6 . EXPERIMENTS
In this section we compare the difierent versions of DL8 in terms of e–ciency ; furthermore , we compare the quality of the constructed trees with those found by J48 , the Java implementation of C4.5 [ 28 ] in Weka [ 32 ] . All experiments were performed on Intel Pentium 4 machines with in between 1GB and 2GB of main memory , running Linux . DL8 and the frequent itemset miners were implemented in C++ . The experiments were performed on UCI datasets [ 21 ] . Numerical data were discretized before applying the learning algorithms using Weka ’s unsupervised discretization method with a number of bins equal to 4 . We limited the number of bins in order to limit the number of created attributes . Figure 6 gives a brief description of the datasets that we used in terms of the number of examples and the number of attributes after binarization . 6.1 Efficiency
The applicability of DL8 is limited by two factors : the amount of itemsets that need to be stored , and the time that it takes to compute these itemsets . We flrst evalu australian credit diabetes vote zoo
1e+10 1e+09 1e+08 1e+07 1e+06 100000 10000 1000 100 10
50
100
150
200
250
300 minfreq s t e s m e t i #
1e+09
1e+08
1e+07
1e+06
100000
10000 s t e s m e t i #
0 10 20 30 40 50 60 70 80 90 100
1e+09 1e+08 1e+07 1e+06 100000 10000 1000 100
10 20 30 40 50 60 70 80 90 100 s t e s m e t i #
1e+06
100000
10000
1000
DL8 Closed DL8 Simple
Closed Freq
DL8 Closed DL8 Simple minfreq diabetes
Closed Freq
DL8 Closed DL8 Simple minfreq vote
1000
100
10
1
0.1
) s ( e m i t n u r
0
10
20
30
40
50
60
70
80
90
100 minfreq
DL8 Closed DL8 Eclat DL8 Apriori LCM Closed
LCM Freq Eclat Freq Apriori Freq Apriori Freq+DL8
0.01
10 20 30 40 50 60 70 80 90 100 minfreq
LCM Closed LCM Freq
DL8 Closed DL8 Eclat DL8 Apriori
1
2
3
4
5 6 minfreq
7
8
9
10
Closed Freq
DL8 Closed DL8 Simple
Closed Freq zoo
) s ( e m i t n u r
100
10
1
0.1
0.01
1
2
3
4
5 6 minfreq
7
8
9
10
DL8 Closed DL8 Eclat DL8 Apriori LCM Closed
LCM Freq Eclat Freq Apriori Freq Apriori Freq+DL8 s t e s m e t i #
) s ( e m i t n u r
10000 1000 100 10 1 0.1 0.01
50 australian credit
100
150
200
250
300 minfreq
DL8 Closed DL8 Eclat DL8 Apriori
LCM Closed LCM Freq Eclat Freq
) s ( e m i t n u r
1000
100
10
1
0.1
Figure 7 : Comparison of the difierent miners on 4 UCI datasets
Algorithm DL8 Closed DL8 Apriori DL8 Eclat Apriori Freq Apriori Freq+DL8 Eclat Freq LCM Freq LCM Closed
Uses relevancy Closed Builds tree
X X X
X X X
X
X
X
Figure 8 : Properties of the algorithms used in the experiments ate experimentally how these factors are in(cid:176)uenced by constraints and properties of the data . Furthermore , we determine how the difierent approaches for computing the itemset lattices compare . A summary of the algorithms can be found in Figure 8 . Besides DL8 Apriori , DL8 Eclat and DL8 Closed , we also include unmodifled implementations of the frequent itemset miners Apriori [ 1 ] , Eclat [ 34 ] and LCM [ 31 ] in the comparison . These implementations were obtained from the FIMI website [ 3 ] . The inclusion of unmodifled algorithms allows us to determine how well relevancy pruning works , and allows us to determine the trade ofi between relevancy pruning and trie construction .
Results for four datasets are listed in Figure 7 . We aborted runs of algorithms that lasted for longer than 1500s . More results can be found in [ 22 ] . We only show datasets here in which frequent itemset miners manage to run within 1500s . The results clearly show that in all cases the number of closed relevant itemsets is the smallest . The difierence between the number of relevant itemsets and the number of frequent itemsets becomes smaller for lower minimum frequency values . The number of frequent itemsets is so large in most cases , that it is impossible to compute or store them within a reasonable amount of time or space . In those datasets where we can use low minimum frequencies ( 15 or smaller ) , the closed itemset miner LCM is usually the fastest ; for low frequency values the number of closed itemsets is almost the same as the number of relevant closed itemsets . Bear in mind , however , that LCM does not output the itemsets in a form that can be used e–ciently by DL8 .
In all cases , DL8 Closed is faster than DL8 Apriori or DL8 Eclat . In those cases where we can store the entire output of Apriori in memory , we see that the additional runtime for storing these results is signiflcant . On the other hand , if we perform relevancy pruning , the resulting algorithm is usually faster than the original itemset miner .
In the datasets shown here , the number of attributes is relatively small . For the datasets with larger number of attributes , such as ionosphere and splice , we found that only DL8 Closed managed to run for support thresholds lower than 25 % , but still was unable to run for support thresholds lower than 10 % .
6.2 Accuracy
In the experiments shown in Figure 9 , we used a stratifled 10 fold cross validation to compute the training and test accuracies of DL8 Closed and J48 . We used the minimum frequency as the local constraint . For each dataset , we lowered this frequency to the lowest value that still allowed the computation to be performed within the memory of our computers . For J48 , results are provided for pruned trees and unpruned trees ; for DL8 results are provided in which the e ( unpruned ) and ex ( pruned ) error functions are optimized ( cf.Queries 1 and 3 of Section 3 ) . First , both algorithms were applied with the same minimum frequency setting constraint . We used a corrected two tailed t test [ 20 ] with a signiflcance threshold of 5 % to compare the test accuracies of both systems . A test set accuracy result is in bold when it is signiflcantly better than its counterpart result on the other system .
The experiments show that both with and without pruning the optimal trees computed by DL8 have a better training accuracy than the trees computed by J48 with the same frequency values . Furthermore , on the test data , in both cases DL8 is signiflcantly better than J48 on 9 of the 20 datasets and only signiflcantly worse on one dataset . When pruned trees are compared to unpruned ones , the sizes of the trees are on average 1.75 times smaller for J48 and 1.5 time smaller for DL8 . After pruning , DL8 ’s trees are still 1.5 times larger than J48 ’s ones . A closer inspection of these trees reveils a similar phenomenon as in the data of Figure 3 : C4.5 ’s trees are smaller as it creates trees with small numbers of incorrectly classifled examples in the leaves , which
Pruned J48 , Minfreq = 2 Size
S
.
Test acc D o N
. r c s i D
. r c s i D
.
D o N
Minfreq
Train acc
Unpruned Test acc
Size
Train acc
Pruned Test acc
Size
Datasets anneal a credit balance breast w chess diabetes g credit heart c ionosph mushro pendigits p tumor segment soybean splice thyroid vehicle vote vowel yeast
#
10 2 45 15 2 40 30 200 15 2 100 30 5 2 50 40 800 600 15 2 200 60 50 700 80 50 20 15 100 70 100 2
%
1.2 .2 6.8 2.4 .3 5.8 4.3 6.2 1.9 .2 10 1.1 1.6 .6 14.2 11.3 9.8 8.0 4.4 .5 8.6 9.5 7.9 21.9 2.4 5.9 4.5 3.4 1.1 7.0 6.7 .1
8 4 J
8 L D
8 4 J
8 L D
8 4 J
8 L D
8 4 J
8 L D
8 4 J
8 L D
8 4 J
8 L D
.85 .89 .87 .81 .90 .93 .96 .91 .79 .90 .73 .77 .88 .94 .83 .89 .92 .58 .44 .63 .72 .51 .55 .74 .91 .63 .96 .96 .36 .39 .53 .74
.87 .89 .88 .85 .90 .97 .96 .91 .83 .99 .75 .84 .94 1 .86 .89 .97 .72 .49 .71 .83 .55 .59 .74 .92 .71 .97 .97 .39 .46 .55 .82
.82 .82 .86 .76 .82 .93 .95 .91 .75 .68 .70 .74 .77 .76 .79 .88 .92 .58 .38 .40 .73 .50 .52 .74 .91 .59 .96 .95 .34 .35 .50 .49
.81 .82 .87 .79 .81 .95 .95 .90 .72 .66 .70 .77 .75 .74 .84 .88 .97 .72 .37 .36 .83 .55 .59 .73 .91 .67 .94 .94 .33 .40 .53 .48
43 107 6 24 99 3 7 9 26 20 6 4 4 68 4 5 5 14 23 116 13 13 15 5 1 17 3 3 11 17 14 501
57 88 11 41 114 10 7 13 55 288 12 12 70 74 7 7 11 17 27 152 15 15 17 5 13 22 15 18 14 21 15 724
.83 .86 .86 .81 .89 .93 .96 .91 .79 .84 .73 .77 .87 .90 .83 .89 .92 .58 .44 .60 .73 .51 .55 .74 .91 .63 .96 .96 .36 .39 .53 .68
.85 .87 .88 .85 .89 .97 .97 .95 .82 .92 .75 .84 .94 .97 .86 .89 .97 .72 .49 .67 .83 .55 .59 .74 .91 .71 .96 .97 .39 .45 .55 .75
.81 .82 .86 .79 .80 .93 .95 .90 .74 .74 .70 .73 .76 .78 .79 .88 .92 .58 .39 .40 .73 .50 .51 .74 .91 .59 .96 .96 .34 .35 .51 .53
.82 .82 .88 .80 .80 .95 .95 .95 .74 .71 .71 .78 .80 .77 .84 .88 .97 .72 .37 .40 .83 .55 .58 .73 .91 .67 .94 .95 .33 .40 .53 .50
22 44 4 17 72 3 7 9 20 69 6 4 17 32 4 5 5 13 19 81 13 11 14 5 1 15 3 3 11 17 11 186
31 46 11 31 65 8 9 13 32 135 10 12 35 50 7 6 11 15 22 105 15 15 17 5 3 22 8 9 14 20 14 307
.82
.84
.80
.96
.99
.74
.71
.78
.86
1 .95
.40
.95
.82
.94 .91 .70
.96
.78
.53
. r c s i D
44
36
72
16
54
69
.
D o N
54
40
84
20
54
41
163
171
32
37
35
.88
.86
.79
.95
0 + 0 + 0 0 0 0 0 0 0 0 0 1 + + 0 0 0 0 0 0 0 0 0 0 0 0 0 + 0 0 1 + +
.73
.70
.80
.91
17 .96 + + 340
0 0
.40
0 0 .97 + + + + + +
.82
81
80
88
.96
.94 + + 127 0 + .99 34 0 + 139 .72 0 0 0 0 + + + + 0 0 + +
186
290
.82
.56
13
27
17 278
81
113
88
127 21 135
13
191
318
Figure 9 : Comparison of J48 and DL8 , with and without pruning cannot be split ofi without violating the constraints . In cases where DL8 ’s accuracy is signiflcantly better , the pruned trees of DL8 are only 3 to 9 nodes larger those of J48 . These results conflrm earlier flndings which show that smaller trees are not always desirable .
Second , in the last six columns of Figure 9 , we give results for J48 with its default minf req = 2 setting , both when using the discretized data , and when using the original , non discretized data . The test accuracies of J48 with minf req = 2 are compared with the test accuracies of DL8 for the various minf req values , when using pruning . The results of the signiflcance test are given in the \S" column : \+" means that J48 is signiflcantly better , \ " that it is signiflcantly worse and \0" that there is no signiflcantly difierence . This comparison shows that J48 ’s test accuracy is signiflcantly higher on 7 of the 20 datasets when using beforehand discretized data . J48 is better on 3 additional datasets if discretization is not performed beforehand , reveiling that a good discretization is sometimes beneflcial . However , for almost all datasets the sizes of the trees are a lot smaller for DL8 . This means that DL8 can be used to compute trees with better size accuracy trade ofis .
Furthermore , one of the strengths of DL8 is that it allows to explicitly restrict the size or accuracy . We therefore studied the relation between decision tree accuracies and sizes in more detail . In Figure 10 , we show results in which the average size of trees constructed by J48 , is taken as a constraint on the size of trees mined by DL8 . None of the results given by DL8 are signiflcantly better nor signiflcantly worse than those given by J48 .
Datasets
Sup Max size
Test acc
Size balance diabetes g credit heart c vote yeast
2 15 100 10 15 10
DL8 100 27 7 14 4
108
J48 DL8 0.81 0.82 0.74 0.75 0.70 0.72 0.80 0.80 0.96 0.95 0.52 0.52
J48 99.0 26.4 6.7 14.0 3.4
DL8 96.6 27.0 7.0 13.0 3.0
107.2
107.0
Figure 10 : In(cid:176)uence of the size constraint on the test accuracy of DL8 ( unpruned )
DL8 can also compute , for every possible size of a decision tree , the smallest error on training data that can possibly be achieved . For two datasets , the results of such a query are given in Figure 11 . In general , if we increase the size of a decision tree , its accuracy improves quickly at flrst . Only small improvements can be obtained by further increasing the size of the tree . If we lower the frequency threshold , we can obtain more accurate trees , but only if we allow a su–cient amount of nodes in the tree .
Figures such as Figure 11 are of practical interest , as they allow a user to trade ofi the interpretability and the accuracy of a model .
The most surprising conclusion that may however be drawn from all our experiments , is that optimal trees perform remarkably well on most datasets . Our algorithm investigates a vast search space , and its results are still competitive in all cases . The experiments indicate that the constraints that we employed , either on size , or on minimum support , are su–cient to reduce model complexities and achieve good australian credit balance scale r o r r e n m i
300
250
200
150
100
50
0
2
4
6
8 10 12 14 16 18 20 decision tree size minsup=55 minsup=60 r o r r e n m i
1000
100
10
0 20 40 60 80 100 120 140 160 180 200 decision tree size minsup=4 minsup=1 minsup=2
Figure 11 : Sizes of decision trees predictive accuracies .
7 . RELATED WORK
The search for optimal decision trees dates back to the 70s , when several dynamic programming algorithms for building such trees were proposed [ 9 , 16 , 26 , 30 , 15 ] . This early work concentrated on flnding small summarizations of input data , and did not study the prediction of unseen examples . Optimization criteria were based on the cost of attributes , and the size or the depth of a tree . Afterwards , attention mostly shifted to heuristic decision tree learners , which were found to obtain satisfactory results for many datasets in a fraction of the runtime ; theoretical results were obtained that show to what extent heuristic decision trees can be considered optimal [ 14 , 6 , 23 ] . Still , the idea of exhaustively flnding optimal decision trees under certain constraints was also studied [ 2 , 19 ] , but only for much smaller datasets and smaller types of trees than studied in this paper . Recently [ 4 ] presented a dynamic programming algorithm that is quite similar to DL8 and its early ancestors . A new optimization criterion was introduced for flnding optimal dyadic decision trees , which use a flxed mechanism for discretization of data . This algorithm was only applied on smaller datasets than our algorithm , and did not investigate the link with data mining algorithms .
More recently , pruning strategies of decision trees have been studied [ 10 ] . DL8 can be conceived as the generalization of these pruning strategies to a larger type of data structure .
Related is also the work of Moore and Lee on the ADtree data structure [ 18 ] . An ADtree can be seen as an index that allows for a quick frequency computation . A key feature of an ADtree is that it does not store specializations of itemsets that are relatively infrequent ; for these itemsets , subsets of the data itself are stored . In DL8 we need to store all itemsets that fulflll the given constraints , and associated information . This is not straightforwardly achieved using ADtrees .
Algorithmically , the tree relevancy constraint is closely related to the condensed representation of – free itemsets [ 5 ] . Indeed , for – = minsup£jDj and p(I ) := ( freq(I ) ‚ minfreq ) , it can be shown that if an itemset is – free , it is also treerelevant . DL8 Closed employs ideas that have also been exploited in the formal concept analysis ( FCA ) community and in closed itemset miners [ 25 ] .
A popular topic in data mining is currently the selection of itemsets from a large set of itemsets found by a frequent itemset mining algorithm ( see for instance , [ 33] ) . DL8 can be seen as one such algorithm for selecting itemsets . It is however the flrst algorithm that outputs a well known type of model , and provides accuracy guarantees for this model .
8 . CONCLUSIONS
We presented DL8 , an algorithm for flnding decision trees that maximize an optimization criterion under constraints , and successfully applied this algorithm on a large number of datasets .
We showed that there is a clear link between DL8 and frequent itemset miners , which means that it is possible to apply many of the optimizations that have been proposed for itemset miners also when mining decision trees under constraints . The investigation that we presented here is only a starting point in this direction ; it is an open question how fast decision tree miners could become if they were thoroughly integrated with algorithms such as LCM or FPGrowth . Our investigations showed that high runtimes are however not as much a problem as the amount of memory required for storing huge amounts of itemsets . A challenging question for future research is what kind of condensed representations could be developed to represent the information that is used by DL8 more compactly .
In experiments we compared the test set accuracies of trees mined by DL8 and C45 Under the same frequency thresholds , we found that the trees learned by DL8 are often signiflcantly more accurate than trees learned by C45 When we compare the best settings of both algorithms , J48 performs signiflcantly better in 45 % of the datasets . E–ciency considerations prevented us from applying DL8 on the thresholds where C4.5 performs best , but preliminary results indicate that the best accuracies are not always obtained for the lowest possible frequency thresholds .
Still , our conclusion that trees mined under declarative constraints perform well both on training and test data , means that constraint based tree miners deserve further study . Many open questions regarding the instability of decision trees , the in(cid:176)uence of size constraints , heuristics , pruning strategies , and so on , may be answered by further studies of the results of DL8 . Future challenges include extensions of DL8 to other types of data , constraints and optimization criteria . DL8 ’s results could be compared to many other types of decision tree learners [ 24 , 27 ] .
Given that DL8 can be seen as a relatively cheap type of post processing on a set of itemsets , DL8 suits itself perfectly for interactive data mining on stored sets of patterns . This means that DL8 might be a key component of inductive databases [ 13 ] that contain both patterns and data .
Acknowledgements Siegfried Nijssen was supported by the EU FET IST project \Inductive Querying" , contract number FP6 516169 . ¶Elisa Fromont was supported through the GOA project 2003/8 , \Inductive Knowledge bases" , and the FWO project \Foundations for inductive databases" . The authors thank Luc De Raedt and Hendrik Blockeel for many interesting discussions ; Ferenc Bodon and Bart Goethals for putting online their implementations of respectively Apriori and Eclat , which we used to implement DL8 , and Takeaki Uno for providing LCM . We also wish to thank Daan Fierens for preprocessing the data that we used in our experiments .
9 . REFERENCES [ 1 ] R . Agrawal , H . Mannila , R . Srikant , H . Toivonen , and
A . I . Verkamo . Fast discovery of association rules . In
Advances in Knowledge Discovery and Data Mining , pages 307{328 . AAAI/MIT Press , 1996 .
[ 2 ] P . Auer , R . C . Holte , and W . Maass . Theory and applications of agnostic pac learning with small decision trees . In ICML , pages 21{29 , 1995 .
[ 3 ] R . J . Bayardo , B . Goethals , and M . J . Zaki , editors . FIMI ’04 , Proceedings of the IEEE ICDM Workshop on Frequent Itemset Mining Implementations , Brighton , UK , volume 126 of CEUR Workshop Proceedings . CEUR WS.org , 2004 .
[ 4 ] G . Blanchard , C . Sch˜afer , Y . Rozenholc , and K . R .
M˜uller . Optimal dyadic decision trees . Mach . Learn . , 66(2 3):209{241 , 2007 .
[ 5 ] J F Boulicaut , A . Bykowski , and C . Rigotti .
Free sets : A condensed representation of boolean data for the approximation of frequency queries . Data Min . Knowl . Discov . , 7(1):5{22 , 2003 .
[ 6 ] T . G . Dietterich , M . J . Kearns , and Y . Mansour .
Applying the waek learning framework to understand and improve c45 In ICML , pages 96{104 , 1996 .
[ 7 ] E . Fredkin . Trie memory . Commun . ACM ,
3(9):490{499 , 1960 .
[ 8 ] E . Fromont , H . Blockeel , and J . Struyf . Integrating decision tree learning into inductive databases . In Revised selected papers of the workshop KDID’06 , LNCS , page to appear . Springer , 2007 .
[ 9 ] M . R . Garey . Optimal binary identiflcation procedures . SIAM Journal of Applied Mathematics , 23(2):173{186 , September 1972 .
[ 10 ] M . N . Garofalakis , D . Hyun , R . Rastogi , and K . Shim .
Building decision trees with constraints . Data Min . Knowl . Discov . , 7(2):187{214 , 2003 . razor in decision tree induction , volume IV : Making Learning Systems Practical , pages 171{187 . MIT Press , 1997 .
[ 20 ] C . Nadeau and Y . Bengio . Inference for the generalization error . Mach . Learn . , 52(3):239{281 , 2003 .
[ 21 ] D . Newman , S . Hettich , C . Blake , and C . Merz . UCI repository of machine learning databases , 1998 .
[ 22 ] S . Nijssen and E . Fromont . Mining optimal decision trees from itemset lattices , 2007 . Technical report CW476 , Dept . Computer Science , Katholieke Universiteit Leuven , March 2007 .
[ 23 ] R . Nock and F . Nielsen . On domain partitioning induction criteria : worst case bounds for the worst case based . Theor . Comput . Sci . , 321(2 3):371{382 , 2004 .
[ 24 ] D . Page and S . Ray . Skewing : An e–cient alternative to lookahead for decision tree induction . In Proceedings of the Eighteenth International Joint Conference on Artiflcial Intelligence ( IJCAI’03 ) , pages 601{612 , 2003 .
[ 25 ] N . Pasquier , Y . Bastide , R . Taouil , and L . Lakhal .
E–cient mining of association rules using closed itemset lattices . Information Systems , 24(1):25{46 , 1999 .
[ 26 ] H . J . Payne and W . S . Meisel . An algorithm for constructing optimal binary decision trees . IEEE Trans . Computers , 26(9):905{916 , 1977 .
[ 27 ] F . Provost and P . Domingos . Tree induction for probability based ranking . Machine Learning , 52:199{215 , 2003 .
[ 28 ] J . R . Quinlan . C4.5 : Programs for Machine Learning .
[ 11 ] J . Han , J . Pei , and Y . Yin . Mining frequent patterns
Morgan Kaufmann , 1993 . without candidate generation . In W . Chen , J . Naughton , and P . A . Bernstein , editors , 2000 ACM SIGMOD Intl . Conference on Management of Data , pages 1{12 . ACM Press , 05 2000 .
[ 12 ] L . Hyafll and R . L . Rivest . Constructing optimal binary decision trees is np complete . Inf . Process . Lett . , 5(1):15{17 , 1976 .
[ 13 ] T . Imielinski and H . Mannila . A database perspective on knowledge discovery . Comm . Of The Acm , 39:58{64 , 1996 .
[ 14 ] M . J . Kearns and Y . Mansour . On the boosting ability of top down decision tree learning algorithms . J . Comput . Syst . Sci . , 58(1):109{128 , 1999 .
[ 15 ] A . Lew . Optimal conversion of extended entry decision tables with general cost criteria . Commun . ACM , 21(4):269{279 , 1978 .
[ 16 ] W . S . Meisel and D . Michalopoulos . A partitioning algorithm with application in pattern classiflcation and the optimization of decision tree . IEEE Trans . Comput . , C 22:93{103 , 1973 .
[ 17 ] T . Mitchell . Machine Learning . McGraw Hill , New
York , 1997 .
[ 18 ] A . Moore and M . S . Lee . Cached su–cient statistics for e–cient machine learning with large datasets . Journal of Artiflcial Intelligence Research , 8:67{91 , March 1998 .
[ 19 ] P . M . Murphy and M . J . Pazzani . Exploring the decision forest : an empirical investigation of Occam ’s
[ 29 ] J . R . Quinlan and R . M . Cameron Jones .
Oversearching and layered search in empirical learning . In IJCAI , pages 1019{1024 , 1995 .
[ 30 ] H . Schumacher and K . C . Sevcik . The synthetic approach to decision table conversion . Commun . ACM , 19(6):343{351 , 1976 .
[ 31 ] T . Uno , M . Kiyomi , and H . Arimura . Lcm ver . 2 :
E–cient mining algorithms for frequent/closed/maximal itemsets . In FIMI ’04 , Proceedings of the IEEE ICDM Workshop on Frequent Itemset Mining Implementations , Brighton , UK , November 1 , volume 126 of CEUR Workshop Proceedings . CEUR WS.org , 2004 .
[ 32 ] I . H . Witten and E . Frank . Data Mining : Practical machine learning tools and techniques . Morgan Kaufmann , San Francisco , 2nd edition edition , 2005 . [ 33 ] X . Yan , H . Cheng , J . Han , and D . Xin . Summarizing itemset patterns : a proflle based approach . In Proceeding of the 11th ACM SIGKDD international conference on Knowledge discovery in data mining ( KDD’05 ) , pages 314{323 , 2005 .
[ 34 ] M . J . Zaki , S . Parthasarathy , M . Ogihara , and W . Li . New algorithms for fast discovery of association rules . Technical Report TR651 , 1997 .
