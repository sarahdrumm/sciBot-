Multiscale Topic Tomography
Ramesh Nallapati
William Cohen
Machine Learning Department
Carnegie Mellon University
Machine Learning Department
Carnegie Mellon University
5000 Forbes Avenue Pittsbugh , PA 15213 nmramesh@cscmuedu
5000 Forbes Avenue Pittsbugh , PA 15213 wcohen@cscmuedu
Susan Ditmore
Carnegie Mellon University
5000 Forbes Avenue Pittsbugh , PA 15213 sditmore@andrewcmuedu
John Lafferty
Machine Learning Department
Carnegie Mellon University
5000 Forbes Avenue Pittsbugh , PA 15213 lafferty@cscmuedu
Kin Ung
Services
Networking and Computing
Johnson and Johnson group
Raritan , NJ kung@ncsusjnjcom
ABSTRACT Modeling the evolution of topics with time is of great value in automatic summarization and analysis of large document collections . In this work , we propose a new probabilistic graphical model to address this issue . The new model , which we call the Multiscale Topic Tomography Model ( MTTM ) , employs non homogeneous Poisson processes to model generation of word counts . The evolution of topics is modeled through a multi scale analysis using Haar wavelets . One of the new features of the model is its modeling the evolution of topics at various time scales of resolution , allowing the user to zoom in and out of the time scales . Our experiments on Science data using the new model uncovers some interesting patterns in topics . The new model is also comparable to LDA in predicting unseen data as demonstrated by our perplexity experiments .
Categories and Subject Descriptors I26 [ Artificial Intelligence ] : Learning ; H28 [ Database Management ] : Database Applications—data mining
General Terms Algorithms , Experimentation
Keywords Topic modeling , Temporal evolution , time scale , Poisson , Probabilistic graphical models , wavelets
1 .
INTRODUCTION
Explosive growth of electronic document collections in the recent past has rendered their analysis by human experts
. extremely tedious and expensive . As a result , an increasing need is felt for automatic algorithms that analyze and summarize the topics contained in such large document collections .
Several probabilistic graphical models have been proposed recently , to address this problem . One of the first probabilistic and truly generative models among them is Latent Dirichlet Allocation ( LDA ) [ 2 ] . LDA models a topic as a multinomial distribution over the vocabulary . Given a document collection , the LDA learns its underlying topics in an unsupervised fashion . In the recent past , several extensions to this model have been proposed such as the Hierarchical Dirichlet Processes [ 12 ] model that automatically discovers the number of topics , Hidden Markov Model LDA [ 5 ] that integrates topic modeling with syntax , Correlated topic models [ 1 ] that model pairwise correlations between topics , etc .
All the aforementioned models ignore an important factor that reveals a huge amount of information contained in large document collections time . Some of the large corpora such as a collection of scientific journals or patent databases span several decades . Hence modeling the evolution and popularity of topics with time can reveal tremendous amount of hidden information in those collections .
Several models have been proposed in the recent past to address this issue . One of the models , called Topics Over Time ( ToT ) [ 13 ] associates a beta distribution over time to each topic that represents the occurrence probability of that topic at any given time . The model learns the parameters of this distribution for each topic based on the time stamps of documents associated with that topic in the collection . This permits us to analyze the popularity of various topics as a function of time .
Another proposed model called the Dynamic Topic Models ( DTM ) [ 3 ] takes a slightly different approach . The DTM explicitly models the evolution of topics with time by estimating the topic distribution at various epochs . Thus the DTM allows us to predict what words are ‘in vogue’ in a particular topic at different points in time . To model the evolution of a topic with time , the authors assume that the natural parameters corresponding to the topic multinomial at each epoch are conditionally distributed by a normal dis tribution with mean equal to the natural parameters at the previous epoch . However , since the normal distribution is not a conjugate to the multinomial distribution , the model does not yield a simple solution to the problems of inference and estimation .
In this work , we present an alternative to the DTM , that is more natural to sequential modeling of counts data . The new model uses conjugate priors on the topic parameters to model evolution of topics , thereby resulting in simpler solutions . In addition , our new model , which we refer to henceforth as the Multiscale Topic Tomography Model ( MTTM ) , allows us to analyze the evolution of topics at various resolutions of time scale . Its expressiveness provides the user with additional flexibility to zoom in and zoom out on the time scale and study the evolution of topics at a chosen time scale . Thus , we believe that the MTTM brings us a step closer to the ultimate goal of effective and fully automatic analysis of document collections .
The rest of the paper is organized as follows . In section 2 , we discuss past work related to the new model . In section 3 , we describe the MTTM in detail including its generative process , the multi scale analysis and the variational methods used for learning and inference . Section 4 presents some of the experiments we performed using the model . Section 5 concludes the paper with some analysis and directions for future work .
2 . PAST WORK
The Poisson distribution , being a natural model for countsdata , has been considered as a potential candidate to model text in the past . One of the earliest models is the 2 Poisson model for information retrieval [ 6 ] , which generates words from a mixture of two classes called elite and non elite classes . This model did not achieve empirical success , mainly owing to the lack of good estimation techniques , but inspired a heuristic model called BM25 [ 11 ] . The latter is considered a strong IR baseline till date .
In the area of text modeling , the GaP model [ 4 ] proposed by Canny uses a combination of Gamma and Poisson distributions to discover latent topics or themes in document collections . The Gamma distribution is used to generate the topic weights vector x in each document , which the author calls theme lengths . The Poisson distribution is used to generate the vector of observed word counts f from expected counts y . The expected counts y are related to the topic weights x through a matrix Λ , given by y = Λx , where each column of Λ represents the probability distribution of words in a topic . Canny developed an EM algorithm to estimate the topic weights x for each document and the global matrix Λ . Furthermore , it is also shown that the model achieves a lower perplexity on test data compared to LDA while also outperforming baseline models on the task of text retrieval . However , the modeling scheme for GaP proposed by Canny optimizes likelihood of the complete data ( ie , the data with the maximum likelihood values used for the unobserved variables ) , whereas a pure generative model should optimize the likelihood of the observed data only . The model presented in this paper is very similar to the Gap model , except that the theme weights in our case are distributed by a Dirichlet distribution over documents instead of a Gamma . This particular definition of Dirichlet means that the topic weights are normalized over all the documents for each topic and not over all the topics per document as in LDA . Also , in our parameter estimation , we are able to optimize a variational lower bound on the observed data log likelihood by marginalizing the theme weights in the complete data loglikelihood . In addition , we extend this model to sequential data by performing multi scale analysis .
In our work , we use the Poisson distribution to model word counts not only because it is a natural choice for countsdata , but also because it is amenable to sequence modeling through Bayesian multiscale analysis . Bayesian multiscale models for Poisson processes were first introduced by Kolaczyk [ 8 ] and were applied to model physical phenomena such as gamma ray bursts . Nowak extended multiscale analysis to build multiscale hidden Markov models and applied it to the problem of image segmentation [ 9 ] . Nowak and Kolaczyk also presented multiscale analysis for the Poisson inverse problem [ 10 ] , which is the problem of estimating latent Poisson means based on observed Poisson data , whose means are related to the latent Poisson means by a known linear function . In this paper , we cast the problem of topic discovery in document collections as a Poisson inverse problem . Unlike in the work of Nowak and Kolaczyk [ 10 ] , we do not assume that the linear relationship between the latent Poisson parameters and observed Poissons is known , which makes the problem slightly more complex . Hence , we use variational approximations to estimate the parameters of the model . We also extend the analysis to multi scale representation of the Poisson parameters , thereby allowing us to model temporal evolution of topics at various time scales .
3 . MULTISCALE TOPIC TOMOGRAPHY
MODEL
3.1 Assumptions and Notations
Following standard notation , we use bold faced letters to represent vectors and matrices and regular font to indicate scalars .
We assume that our document collection is sorted in the ascending order of the publication dates of the documents . We also assume that the sorted collection is divided into 2S equal sized chunks ( where S is an integer ) of size M each , with the chunks {C0 , · · · , C2S −1} indexed in the ascending order of time . Thus , each chunk Ct represents an epoch of time t ranging from the publication date of its earliest published document d = 1 to the publication date of its latest document d = M . Henceforth , we will use the term epoch to also denote the chunk of documents Ct that it corresponds to . We represent each document d in an epoch t by a vector of term counts ntd = {ntd1 , · · · , ntdV } where ntdw is the count of word w in document d from epoch t , and V is the vocabulary size .
We use non homogeneous Poisson processes to model evolution of topics with time . Accordingly , each epoch t is associated with its unique word generating Poisson parameters given by µt = {µt1 , · · · , µtK } corresponding to K topics . Again each µtk is a a vector of Poisson means over the vocabulary given by {µtk1 , · · · , µtkV } . Thus , the parameter µtkw represents the expected number of counts of word w from topic k during the epoch t . Unlike in LDA where topics are represented as multinomial distributions over the vocabulary , we represent topics as vectors of Poisson means over the vocabulary µtk . The variation in the values of the Poisson means of a particular topic as a function of t will
Emission rate from a topic ‘source’ for a given word
µ
θ d=1
µ
θ
µ d=2
θ
µ d=3
θ
µ d=4
θ
µ d=5
Documents act like ‘sinks’ that collect the topic emissions at a proportion
θ d
Figure 1 : The intuitive idea of Topic Tomography provide us information on the evolution of the topic content with time .
We use the terms Poisson rate , Poisson mean and Poisson parameter interchangeably in the rest of the paper . 3.2 Generative process
321 Data generation
Given the Poisson parameters for each epoch , we generate the data as follows . For each epoch t and topic k , we first generate the topic weights vector θtk from a Dirichlet distribution , where θtk = {θtk1 , · · · , θtkM } is a multinomial distribution over the documents in the corresponding chunk Ct . Each component of the multinomial , θtkd , represents the degree to which the document d ‘captures’ the topic k . Then , for each document d in the chunk and for each word w , we generate the counts ntdw using a Poisson distribution whose mean is given by Pk θtkdµtkw , a weighted combina tion of the Poisson means of all the topics corresponding to that word . The generative process is presented more precisely below .
1 . For each epoch t = 0 , · · · , 2S − 1
2 .
3 .
4 .
5 .
6 .
For each topic k = 1 , · · · , K
Generate θtk ∼ Dir( ·
| α )
For each document d = 1 , · · · , M
For each word w = 1 , · · · , V
Generate ntdw ∼ Poiss( ·
|Pk θtkdµtkw )
Note that the topic weights in the linear combination do not sum to 1 since θtkd represents P ( d|k ) , the probability that the topic k appears in document d and not P ( k|d ) , the probability that the document discusses the topic k , as defined in LDA . An intuitive way to understand the new model would be to think of each topic as an emission from a source , and the documents as sinks that share the topic emissions amongst themselves . Thus the new model captures how the topic is sectioned among the documents in a given epoch , hence the name Topic Tomography . This idea is illustrated in figure 1 .
The generative process of the observed data is graphically represented in figure 2 . Accordingly , the data likelihood given the Poisson parameters is given by :
2S
−1
K
P ( n|θ , µ , α ) =
×
M
{
Dir(θtk|α )
Yt=0 Yk=1 Yd=1 Poiss(ntdw|Xk Yw=1
V
θtkdµtkw)} ( 1 )
α
θ
K n
V
M
µ
V
K
S
2
Figure 2 : Graphical representation of data generation
( 0)µ0
β
( 0 ) 0
( 1 ) µ 0
( 1 ) µ 1
β(1 ) 0
( 2 ) µ 0
β
( 1 ) 1
( 2 ) µ 2
( 2 ) µ 1
( 2 ) µ 3
K
V
Figure 3 : Binary tree representation of multiscale parameters for the case S=2
322 Parameter generation
The process described above is already a complete generative process for a document collection . However , we have not yet defined how the Poisson parameters of different epochs are related to each other . In this section , we define a multiscale generative process for the Poisson parameters that allows us to model temporal evolution of topics . First , we define multiscale wavelet parameters given by a binary tree representation as shown below : t
µ(S ) µ(s ) t
= µt for t = 0 , · · · , 2S − 1 = µ(s+1 )
( 2t ) + µ(s+1 ) for s = 0 , , S − 1 and t = 0 , , 2s − 1
( 2t+1 )
( 2 )
( 3 ) t where the index s is called the scale and corresponds to the depth of the tree . The highest scale of resolution given by S corresponds to the leaves of the binary tree , where each leaf node represents an epoch . The multiscale Poisson parameters µ(S ) at each leaf node t ∈ {0 , · · · , 2S − 1} are set equal to the Poisson parameters corresponding to the respective epoch t . At any lower scale of resolution ( 0 ≤ s ≤ S − 1 ) , the Poisson parameter at node t ∈ {0 , · · · , 2s − 1} , given by µ(s ) , is set equal to sum of the corresponding parameters at its two children . The parameters µ(s ) t defined this way are known as the unnormalized Haar wavelet scaling coefficients of µt [ 9 ] . The multi scale Poisson parameters are pictorially represented in figure 3 . t
While each leaf node in the tree corresponds to an epoch , any non leaf node at scale 0 ≤ s ≤ S − 1 corresponds to a larger epoch of time whose span ranges the epochs of the leaf nodes to which it is an ancestor . At scale s = 0 , we have only the root node whose epoch spans the time period of the entire collection and the Poisson parameters at this scale correspond to the average topic representation for the whole corpus . As we descend down the tree to a higher scale of resolution s , we have 2s nodes at that scale with shorter epochs for each node and a breadth wise traversal from left to right gives us the evolution of topics at that scale .
Now , we also define the canonical multiscale parameters
β(s ) t as follows .
β(s ) t
=
µ(s+1 ) ( 2t ) µ(s ) t for s = 0 , , S − 1 and t = 0 , · · · , 2s − 1 ( 4 ) t
In other words , at each scale s ( except s = S ) and for each node t at that scale , β(s ) represents the ratio of the Poisson parameter at the left child and that at the node under consideration . The canonical parameters are also called splitting factors since they govern how the multiscale parameter µ(s ) is ‘split’ between its children . We can also invert the relation in Eq ( 4 ) to obtain t
µ(s ) ( 2t ) = β(s−1 ) ( 2t+1 ) = µ(s−1 ) µ(s ) t t
× µ(s−1 ) t
− µ(s ) ( 2t )
( 5 )
( 6 ) for s = 1 , · · · , S and t = 0 , · · · , 2s−1 − 1 where we obtained Eq ( 6 ) from Eq ( 3 ) . The canonical parameters are represented at the edges of the binary tree in figure 3 to indicate that they are a function of the Poisson parameters at the two nodes that share the respective edges . We will later show that one can factor the joint likelihood of the observed data under the independent Poissons as a multi scale likelihood using the canonical parameters .
Note that setting β(s ) t = 0.5 is equivalent to the relation µ(s+1 ) ( 2t ) = µ(s+1 ) ( 2t+1 ) meaning the multiscale Poissons that share the same parent are equal . This relation should immediately delight a Bayesian statistician , since we can conveniently encode our prior information that the Poisson parameters of a given topic are expected to be more or less the same over various epochs ( in other words , topics do not change too drastically with time ) , by imposing a symmetric , conjugate prior on β(s )
. t
Given this background , the generative process for the Pois son parameters is as shown below .
1 . For each topic k = 0 , · · · , K
2 .
3 .
4 .
5 .
6 .
For each word w = 1 , · · · , V
Generate µ(0 )
0kw ∼ Gamma(·|λµ , δµ )
For each scale s = 0 , · · · , S − 1
For each epoch t = 0 , · · · , 2s − 1
Generate β(s ) tkw ∼ Beta(·|δβ , δβ )
We used the Gamma distribution to generate the Poisson parameters since it is a conjugate prior to the Poisson . We will later show that the observed data log likelihood can be factored into a multiscale log likelihood in which the canonical parameters act as binomial parameters . Hence we used the Beta distribution , their natural conjugate prior , to generate them . In particular , the symmetric Beta ensures that the topic evolution remains smooth .
The prior probability of the model parameters given the hyperparameters δ = {λµ , δµ , δβ} is then given as follows :
K
V
{Gamma(µ(0 )
0kw|λµ , δµ )
P ( µ|δ ) =
×
S−2
Yk=1 Ys=0
2s
Yw=1 Yt=0
−1
Beta(β(s ) tkw|δβ , δβ)}
( 7 )
θ n
K
V
M
µ s=0 t=0
β
α
θ n
K
V
M
µ s=0 t=1
µ s=1 t=0
β
µ s=2 t=0
θ n
K
V
M
µ s=0 t=2
β
µ s=1 t=1
θ n
K
V
M
µ s=0 t=3
V
K
Figure 4 : Graphical representation of MTTM for S=2 : we purposely omitted the hyper parameters in the figure for clarity .
The generative process of the data as well as the model parameters together is represented graphically in figure 4 . Combining Eq ( 7 ) , one can compute the marginal likelihood of the observed data given the topic parameters and the hyper parameters of the priors as follows .
( 1 ) and Eq
P ( n|µ , α , δ ) = Zθ 3.3 Variational EM
{P ( n|θ , µ , α)dθ} P ( µ|δ )
= P ( n|µ , α)P ( µ|δ )
( 8 )
Since estimating the parameters of the model is intractable , we use variational EM to estimate the parameters of the model [ 7 ] . We only summarize the results below but the interested reader may refer to appendix A for more details .
331 Variational E step
We introduce variational parameters given by γtkd and φtdwk , to approximate the observed data log likelihood given in Eq ( 8 ) . One can think of the γtkd as proportional to the posterior probability that document d of epoch t captures topic k . φtdwk can be interpreted as the posterior probability that word w in document d of epoch t came from topic k . We estimate them by maximizing a variational lower bound of Eq ( 8 ) with respect to the variational parameters . We summarize the results in Eq ( 9 ) and Eq ( 10 ) below , with details in appendix A .
φtdwk ∝ µ(S−1 ) tkw exp(ψ(γtkd ) − ψ(
V
γtkd ) )
( 9 )
M
Xd=1
γtkd = α + ntdwφtdwk
( 10 )
Xw=1
332 Variational M step
Instead of directly estimating the parameters µ(S )
In the M step , we estimate the model parameters , namely µ . tkw by maximizing the variational lower bound of Eq ( 8 ) with respect to these parameters , we express the likelihood in a slightly different form so as to be able to estimate the multiscale parameters µ(s ) tkw for 0 ≤ s ≤ S . Since , we are only interested in estimating µ in the M step , we collect all the terms in the variational lower bound of Eq ( 8 ) , given by
Eq ( 19 ) in the appendix , that contain µ(S ) expression L[µ ] as shown below . tkw and call the
2S
−1
V
K
{−µ(S ) tkw + log µ(S ) tkw
Xk=1 ntdwφtdwk}
M
Xd=1
L[µ ] =
2S
−1
=
µ ≡
2S
−1
Xt=0 Xt=0
V
Xt=0 Xw=1 Xw=1
V
K
Xw=1 Xk=1 Xk=1
K
{−µ(S ) tkw + ztwk log µ(S ) tkw} log Poiss(ztwk|µ(S ) tkw )
( 11 )
( 12 ) where ztwk is the latent count of the word w in topic k in the entire chunk of documents corresponding to epoch t ( corred=1 ntdwφtdwk . Although we showed that ztwk ∼ Poiss(·|µ(S ) tkw ) by simple algebraic manipulation , it is also possible to prove it theoretically . This proof is presented in appendix B . sponding to scale S ) and is given by ztwk =PM
In Eq ( 12 ) , the notation
µ ≡ indicates that its left handside is equal to its right hand side as far as the terms containing µ are concerned . Note that Eq ( 11 ) and Eq ( 12 ) differ by the factorPtPwPk log(ztwk! ) but since it doesn’t contain µ , it does not affect our estimation . Also note that one may round off ztwk to the nearest integer to account for the fact that the Poisson generates only integers , but this plays no major role in terms of estimating µ . We now define a new multi scale variable z(s ) twk on the same lines as the multiscale parameters as follows . We will show shortly that L[µ ] can be expressed in terms of this variable . z(S ) twk = ztwk for t = 0 , · · · , 2S − 1 z(s ) twk = z(s+1 )
( 2t)wk + z(s+1 ) for s = 0 , , S − 1 and t = 0 , , 2s − 1
( 2t+1)wk
( 13 )
( 14 )
The simplified version of L[µ ] in ( 12 ) can be equivalently expressed in terms of the multiscale parameters as shown below .
2S
−1
V
K
L[µ ] =
=
+
S−1
Xt=0 Xs=0 Xw=1
V
2s
−1
Xw=1 Xt=0 Xk=1
K
V
Xk=1 Xw=1 log Poiss(ztwk|µ(S ) tkw ) log Bin(z(s+1 )
( 2t)wk|β(s ) tkw , z(s ) twk )
K
Xk=1 log Poiss(z(0 )
0wk|µ(0 )
0kw )
( 15 )
The proof for the above transformation is sketched in appendix C . We do a MAP estimate of the multiscale parameters using Eq ( 15 ) and the priors defined in Eq ( 7 ) to obtain the following relations .
β(s ) tkw =
µ(0 ) tkw = z(s+1 ) ( 2t)wk + δβ − 1 z(s ) twk + 2(δβ − 1 ) z(0 ) 0wk + λµ − 1
1 + δµ
( 16 )
( 17 )
4 . EXPERIMENTS
4.1 Analysis of Science
1000 atom electron gas heat matter quantum l
) e a c s g o l ( e t a r n o s s o P i
100
10
1 1880
1900
1920
1940 year
1960
1980
2000
Figure 7 : Evolution of content bearing words in the topic “ Particle physics ” : the words “ atom ” , “ electron ” and “ quantun ” gain prominence with time , while words such as “ heat ” and “ gas ” lose ground , indicating a paradigm shift in the field from macro matter to micro matter .
We analyzed a subset of 30,000 articles from Science , 250 from each of the 120 years between 1883 and 2002 . This is essentially the same data used by Blei and Lafferty in their experiments with the DTM [ 3 ] . We divided the data into 16 chunks , each consisting of 1875 documents . Each of these chunks represents a 15 year epoch . We then trained a 50 topic 5 scale topic tomography model on this dataset with the following values for the hyper parameters : ( λµ = 1.0001 ; δµ = 1 ; δβ = 50 ; α = 08 ) The large value of δβ ensures that the Poisson parameters of adjacent epochs are nearly equal , resulting in a smooth evolution of topics .
Figure 5 shows the multi scale representation of topic which we labeled “ particle physics ” . We only displayed the top 10 terms that had the highest Poisson means in that topic . The root node of the binary tree corresponds to s = 0 , and it represents the summary of the topic over the entire 120 year span of the collection . At the highest scale ( s = 3 ) displayed , each node presents a snap shot summary of the topic in a 15 year period ( Note that owing to space constraints , we did not display the highest scale of resolution s = 4 ) . Thus , the user can choose one of the four scales of resolution depending on the desired granularity . Inspecting the topic snap shots at the scale s = 3 , one can easily gain an understanding of the evolution of the topic . The gradual transition in the topic from macro matter to micro matter is more apparent from figure 7 , which plots the Poisson rates of a few representative words as a function of time . Table 1 lists the titles of documents that have the highest value of the posterior Dirichlet parameter γtkd among all documents in each epoch . In other words , these are the documents in which the topic particle physics appears with the highest probability in each epoch . The shift in the topic is also evident from these titles .
In figure 6 , we displayed the multiscale representation of another topic , which we labeled “ genetics ” . An examination of the topic snapshots at scale s = 3 clearly shows a gradual transition from evolutionary biology in the late 19th century to modern genetics in the early 21st century . Figure 8 plots the popularity of a few representative terms with time . Table 2 , that displays the titles of documents in which the
1883−2002 energy electron atom radiation theory raise state absorb physical charge
1913−1942
1943−1972 atom energy theory raise electron radiation molecule electric absorb charge energy radiation atom electron raise absorb spectrum nuclear band state
1883−1942 theory energy atom heat raise radiation molecule electron gas particle
1883−1912 theory heat energy gas atom matter molecule produce particle radiation
1943−2002 energy electron atom radiation state absorb raise nuclear laser ion
1973−2002 energy electron atom state laser radiation physical nuclear quantum charge
1883−1897 heat theory energy gas matter atom molecule particle produce wave
1898−1912 theory energy atom heat gas raise molecule matter produce radiation
1913−1927 atom theory energy raise electron radiation molecule electric charge element
1928−1942 raise energy atom theory radiation electron molecule absorb wave particle
1943−1957 energy atom radiation raise absorb electron radioactive ultraviolet absorbing ionize
1958−1972 energy radiation electron atom raise absorb spectrum band nuclear ion
1973−1987
1988−2002 energy electron radiation atom laser nuclear state ion absorb experiment energy electron state atom laser physical quantum charge beam optic s=0 s=1 s=2 s=3
Figure 5 : A 4 scale representation of the topic “ Particle physics ” : words colored red are those whose relative importance in the topic has gone up compared to previous epoch at the same scale . Words colored blue are those whose relative importance has gone down . Words not colored have retained their position compared to previous epoch .
1883−2002 gene dna sequencel group differ chromosome rna two type line
1913−1942 group type number differ form two line chromosome species point
1943−1972 dna chromosome rna gene genetic two type mutant single number i
1883−1942 group differ line form number type point two specimen peculiar
1883−1912 differ line form group number point peculiar two species type
1943−2002 gene dna sequence rna chromosome two genome region clone genetic
1973−2002 gene dna sequence rna genome chromosome clone region two encode
1883−1897 line differ number point form group peculiar two represent type
1898−1912 group form differ species line number type two point peculiar
1913−1927 group number type line differ form chromosome two point division
1928−1942 group type differ number form two chromosome species line study
1943−1957 chromosome two type genetic gene number single differ mutant dna
1958−1972 dna rna chromosome gene genetic two type mutant single hybrid
1973−1987 gene dna sequence rna chromosome region clone genome two contain
1988−2002 gene dna sequence rna genome clone chromosome encode region site s=0 s=1 s=2 s=3
Figure 6 : A 4 scale representation of the topic “ Genetics ” : the color code is same as above .
1890 1903 1927 1936 1949 1964 1978 1992
“ The Cause of Motion in the Radiometer ” “ Electricity at High Pressures ” “ Ionization by Positive Ions ” “ The Production of Cosmic Ray Showers ” “ Luminescent Solids ( Phosphors ) ” “ Mossbauer Effect in Chemistry and Solid State Physics ” “ Analytical Chemistry : Using Lasers to Detect Less and Less ” “ Vibrational Modes and the Dynamic Solvent Effect in Electron and Proton Transfer ”
Table 1 : Documents in which the topic “ Particle physics ” appears with the highest probability in each of the epochs
1893 1911 1922 1941 1949 1965 1979 2000
“ A Space Relation of Numbers ” “ ‘Genotype ” and “ Pure Line ” ” “ Spermatogenesis of the Garter Snake ” “ The Artificial Synthesis of a 42 Chromosome Wheat ” “ Cytological Evidence Opposing the Theory of Brachymeiosis in the Ascomycetes ” “ Bipolarity of Information Transfer from the Salmonella typhimurium Chromosome ” “ Distribution of RNA Transcripts from Structural and Intervening Sequences of the Ovalbumin Gene ” “ DNA Replication Fork Pause Sites Dependent on Transcription ”
Table 2 : Documents in which the topic “ Genetics ” appears with the highest probability in each of the epochs
10000
1000 clone dna gene line group species
100
10 l
) e a c s g o l ( e t a r n o s s o P i
1 1880
1900
1920
1940 year
1960
1980
2000
Figure 8 : Evolution of content bearing words in the topic “ Genetics ” : it is apparent from the figure that words related to modern genetics such as ‘dna’ , ‘clone’ and ‘gene’ exhibit higher emission rates in the late 1990 ’s while words related to evolutionary biology such as ‘group’ and ‘species’ taper off with time . Interestingly , the word ‘dna’ starts coming into prominence only in in the 1950 ’s , just around the time when it was discovered .
Agricultural science Particle Physics Genetics Neuroscience Climate change
0.18
0.16
0.14
0.12
0.1
0.08
0.06
0.04 i
) c p o T ( P
0.02
1880
1900
1920
1940 year
1960
1980
2000
Figure 9 : Occurrence probability of topics with time : we plotted the statistic γtk for the two topics we analyzed earlier , namely “ particle physics ” and “ genetics ” and for three other topics , which we identified as “ agricultural science ” , “ neuroscience ” and “ climate change ” . The plot reveals some interesting patterns . For example , while agricultural science remains more or less stable with time , we see an explosion of genetics in the 1990 ’s . The topics of climate change , atomic physics and neuroscience also exhibit an increasing prominence in the late 20th century , consistent with the trends in the real world . topic appears with the highest probability in each epoch , demonstrates a very similar pattern of topic evolution .
In figure 9 , we plotted another interesting statistic , namely the sum of the posterior Dirichlet parameters of a topic k over all documents in each epoch t given by γtk =Pd γtkd , as a function of t . This statistic is proportional to the occurrence frequency of a topic in a given epoch . We normalized this statistic , so that one can interpret the plot as the probability of occurrence of a topic as a function of time , similar to the plots in [ 13 ] .
Finally , in figure 10 , we plot the Poisson rates of the word ‘reaction’ in three different topics and compared with the total counts in each epoch . The plot clearly demonstrates the utility of the topic model in disambiguating an ambiguous
1000 particle physics blood tests chemistry total count l
) e a c s g o l ( e t a r n o s s o P i
100
10
1 1880
1900
1920
1940 year
1960
1980
2000
Figure 10 : Occurrence rate of “ reaction ” in three different topics : the word ‘reaction’ could have several meanings depending on the context in which it is used . In this plot , we see that it is resolved into three topics ‘particle physics’ , ‘blood tests’ and ‘chemistry’ . While the topic ‘chemistry’ accounts for the majority of occurrences of the word ’reaction’ , it also occurs at a much lower rate in the context of ‘blood tests’ and ‘particle physics’ , where it assumes different connotations . One would not be able to see this from a simple plot of overall occurrence counts of the word ( which is also displayed above . ) word based on its context . 4.2 Perplexity
Perplexity is a standard objective metric that measures the ability of a model to predict unseen data . Lower perplexity means better predictiveness and a better model . In case of documents , the average perplexity of a word in a test set Dtest comprising M documents is defined as
( Perplexity(Dtest|M ) = 2
− log P ( ntest |M ) PM ndw
PV d=1 w=1
)
( 18 ) where ntest is the entire vector of observed word counts in the test set and M is the model .
In this section , we compare the perplexity of the topic tomography model with that of LDA . Note that these two models generate completely different events : while the former models counts data ( eg : 2 a ’s and 3 b ’s ) , the latter models one particular instance of the counts vector ( eg : ‘aabbb’ ) . In order to be able to make a fair comparison , we added the multinomial normalizing coefficient
( Pw ndw)!/(Qw(ndw! ) ) for each document in the expression for LDA likelihood . This term converts the probability of a string to the probability of the corresponding counts vector allowing us to directly compare the perplexities of both the models . Hence the perplexity numbers we show in the plots for LDA may not directly correspond to the values obtained by previous authors [ 2 , 3 ] .
For our experiments , we split the data timewise into 8 chunks each spanning 15 years and compirsing 3750 documents as done in section 41 We further randomly split each chunk into equal halves to generate training and test sets . The train and test sets each have 8 chunks , each of which spans 15 years but consists of only 1875 documents .
We consider three variants of the topic tomography model in our experiments .
The first variant , which we call basic TT , is the closest counterpart to LDA . In this model , we completely ignore the multiscale analysis and assume that the entire training ( or test ) set represents a single epoch . We estimate one set of Poissons for the entire collection , using no prior distributions on the Poisson parameters . The learned model is used to estimate perplexity on the test set , which is done by running the E step of the variational EM algorithm . The second variant , which we name multiple TT model , relaxes the assumption of the basic TT model and estimates topic Poissons for each of the 8 epochs in the training set separately . However , it still does not perform any multiscale analysis and uses no priors on the Poisson means . For each chunk in the test set , we predict the model ’s perplexity by running the E step of the variational EM with respect to the model parameters corresponding to the same epoch in the training set . The last variant is the complete multiscale topic tomography model with multiscale analysis using beta priors on the multiscale binomials with hyper parameters set at the same values used in section 41
For LDA baseline , we used the standard version that estimates a single set of topic multinomials for the whole collection . For all the aforementioned models , we fixed the Dirichlet parameter at 0.8 to encourage sparsity of topics .
Figure 11 compares the perplexity of LDA with the three variants of the topic tomography model as a function of the number of topics used in the model . The figure shows that both LDA and basic TT are almost identical in performance . Also notice that multiscale TT has a consistently lower perplexity than the multiple TT model . This result justifies the intuition behind our definition of the priors in the multiscaleanalysis . The priors allow information to propogate from one epoch to another and hence improve the ability of the model to predict unseen data in any given epoch . Finally , we notice that although the multiple TT and the multiscale TT models have many more parameters than the basic TT model , they produce a slightly higher perplexity on the test set compared to the latter . On inspection , we noticed that the performance comparison on the training data is the exact reverse . This is a clear case of over fitting , where the extra parameters in the multiple TT and the multiscale TT models result in better fitting of the training data , but hurt its generalization ability compared to the basic TT model . Notwithstanding this fact , the multiscale model is still very useful since it allows us to visualize data better , through the multiscale analysis as we have shown earlier .
5 . DISCUSSION
In this work , we presented a new approach to modeling temporal evolution of topics in a large document collection . The new approach , based on non homogeneous Poisson processes , combined with multi scale Haar wavelet analysis is a more natural way to do sequence modeling of counts data than previous approaches . The new model offers us the best features of both the ToT [ 13 ] and DTM [ 3 ] models . While ToT models the probability of occurrence of a topic with time , DTM models the evolution of topic content . The topic tomography model permits us to accomplish both at the same time . In addition , the multiscale analysis used in the model provides us with an additional ‘zoom’ feature that permits the user to examine the topic evolution at multiple scales of resolution .
LDA basic TT multiple TT multiscale TT
14.5
14
13.5
13
12.5
12
11.5 l y t i x e p r e P
11
10
20
30
40
50
K ( num topics )
60
70
80
Figure 11 : Comparison of Perplexities of the models : lower is better .
One of the limitations of the MTTM lies in its generative process : since the Dirichlet distribution that generates topic proportions is defined over the set of documents in a given epoch , the model permits only generating an entire chunk of documents whose size is equal to the training set chunk size . This is the main reason why we used equal sized training and test sets in our perplexity experiments in section 42 We are no longer able to make inference on a single document at a time . One way to overcome this limitation is use a Gamma distribution to generate topic weights for each document independently as done in the Gap model . However , multiscale analysis using the Gamma distributed weights becomes tricky due to the coupling between the Poisson parameters and the Gamma weights . In our case , we were able to uncouple the Poisson parameters from the topic proportions using the relation Pd θtkd = 1 ( see Eq ( 19 ) in appendix
A ) . Nevertheless , we intend to construct a variational algorithm for multi scale analysis using a Gap like model , as part of our future work .
6 . REFERENCES [ 1 ] D . Blei and J . Lafferty . Correlated topic models . In
Advances in Neural Information Processing Systems , 2006 .
[ 2 ] D . Blei , A . Ng , and M . Jordan . Latent dirichlet allocation . Journal of Machine Learning Research , 3 , 2003 .
[ 3 ] D . M . Blei and J . D . Lafferty . Dynamic topic models .
In International conference on Machine learning , pages 113–120 , 2006 .
[ 4 ] J . Canny . Gap : a factor model for discrete data . In
International ACM SIGIR conference on Research and development in information retrieval , pages 122–129 , 2004 .
[ 5 ] T . L . Griffiths , M . Steyvers , D . M . Blei , and J . B .
Tenenbaum . Integrating topics and syntax . In Advances in Neural Information Processing Systems , pages 537–544 , 2005 .
[ 6 ] S . P . Harter . A probabilistic approach to automatic keyword indexing . Journal of the American Society for Information Science , 35:285–295 , 1975 .
[ 7 ] M . I . Jordan , Z . Ghahramani , T . Jaakkola , and L . K . graphical models . Machine Learning , 37(2):183–233 , 1999 .
[ 8 ] E . D . Kolaczyk . Bayesian multiscale models for poisson processes . In Journal of the American Statistical Association , pages 920–933 , 1999 .
[ 9 ] R . Nowak . Multiscale hidden markov models for bayesian image analysis . Bayesian Inference in Wavelet Based Models ( B . Vidakovic and P . Muller , eds. ) , Lecture Notes in Statistics 141 , Springer Verlag . , 1999 .
[ 10 ] R . Nowak and E . Kolaczyk . A statistical multiscale framework for poisson inverse problems . Special issue of IEEE Transactions on Information theory on information theoretic imaging , 2000 .
[ 11 ] S . E . Robertson and S . Walker . Some simple effective approximations to the 2 poisson model for probabilistic weighted retrieval . In International ACM SIGIR conference on Research and development in information retrieval , 1994 .
[ 12 ] Y . Teh , M . Jordan , M . Beal , and D . Blei . Hierarchical dirichlet processes . Technical Report 653 , Department Of Statistics , UC Berkeley , 2003 .
[ 13 ] X . Wang and A . McCallum . Topics over time : a non markov continuous time model of topical trends . In International conference on Knowledge discovery and data mining , pages 424–433 , 2006 .
APPENDIX A . VARIATIONAL INFERENCE
We define the following variational bounds on the loglikelihood of the observed data using Jensen ’s inequality for the log function as shown below .
2S
−1 log(Zθt
{ K Yk=1
Dir(θtk|α)!
θtkdµ(S ) tkw)}dθt ) + log P ( µ|δ ) q(θtk|γ tk)( log Dir(θtk|α )
K
Xk=1 tkw + ntdw log(
θtkdµ(S ) tkw )
K
Xk=1
H(q(θtk|γ tk))} + log P ( µ|δ ) log P ( n , |α , µ , δ ) =
V
M
2S
Xt=0 Yd=1 Poiss(ntdw|Xk Yw=1 {Zθ t Xt=0 Yk=1 Xk=1 Xd=1 Xw=1
θtkdµ(S )
[ −
−1
{
K
M
V
K
K
×
≥
+
− log(ntdw!)])}dθt +
2S
−1
K
Xk=1
≥
+
M
{
Xt=0 Xk=1 Xd=1 Xw=1
V
+
M
V
Xd=1
Xw=1
K
Xk=1 Xk=1
Eq[log Dir(θtk|α ) ] −
K
V
K
Xw=1
Xk=1
µ(S ) tkw ntdw(
φtdwk(log µ(S ) tkw + Eq[log θtkd] ) )
− log(ntdw! ) +
H(q(θtk|γ tk ) ) ntdwH(φtdw)} + log P ( µ|δ )
( 19 )
( 19 ) , we used the relation PM d=1 θtkd = 1
Saul . An introduction to variational methods for where in Eq
We first start with noting that the counts of a word w in a document d from epoch t is distributed as ntdw ∼ Poiss(·|Xk
θtkdµ(S ) tkw )
Now , let us define the variable ztdwk denoting the latent counts of the word w from topic k in the same document .
Now clearly , Pk ztdwk = ntdw . Since the summation of two independent Poisson random variables is also a Poisson variable with mean equal to the sum of the means of the original random variables , we can infer that ztdwk ∼ Poiss(·|θtkdµ(S ) tkw )
Now ztwk is the latent counts of the word w from topic k in the whole chunk that corresponds to epoch t . Therefore , by definition it follows that ztwk =
M
Xd=1 ztdwk
∼ Poiss(·|
θtkdµ(S ) tkw )
M
Xd=1
= Poiss(·|µ(S ) tkw
θtkd ) = Poiss(·|µ(S ) tkw )
( 28 )
M
Xd=1
C . MULTISCALE FACTORIZATION
We first note the result that the joint probability of two independent Poisson variables x1 and x2 can be equivalently expressed as a product of a binomial and a Poisson as follows .
Poiss(x1|µ1)Poiss(x2|µ2 ) exp(−(µ1 + µ2))µx1
1 µx1
1 x1!x2!
( x1 + x2)!
µ1
(
)x1 (
µ2
)x2 x1!x2!
µ1 + µ2 exp(−(µ1 + µ2))(µ1 + µ2)x1+x2
µ1 + µ2
( x1 + x2)! µ1
Bin(x1|x1 + x2 ,
µ1 + µ2 Poiss(x1 + x2|µ1 + µ2 )
)
=
=
×
=
×
Applying this result to the Poisson likelihood terms in lefthand side of Eq ( 15 ) recursively results in its right handside . while q(θtk|γ tk ) and φtdw are variational posterior dirichlet and variational multinomial distributions respectively , Eq[X ] represents the expectation of the random variable X with respect to the distribution q( ) . H( ) represents the entropy of the distribution in its argument .
The terms in Eq ( 19 ) can be expanded as follows .
( α − 1 )
( 20 )
( 21 )
γtkd ) )
γtkd )
Eq[log P ( θtk|α ) ] = Γ(M α ) − M Γ(α ) +Xd
( ψ(γtkd ) − ψ(Xd Eq[log θtkd ] = ψ(γtkd ) − ψ(Xd H(q(θtk|γ tk ) ) = Xd − Xd H(φtdw ) = −Xk
φtdwk log φtdwk
γtkd ) log Γ(γtkd ) − log Γ(Xd ( γtkd − 1)(ψ(γtkd ) − ψ(Xd
γtkd ) )
( 22 )
( 23 )
Plugging back these expansions in Eq ( 19 ) and calling the expression obtained by collecting terms that contain φtdwk , L[φtdwk ] , we have :
L[φtdwk ] = ntdwφtdwk(log µ(S )
− log φtdwk ) tkw + ψ(γtkd ) − ψ(Xd
γtkd )
( 24 )
Taking the partial derivative of L[φtdwk ] with respect to φtdwk gives :
∂L[φtdwk ] ∂φtdwk
= ntdw(log µ(S ) tkw + ψ(γtkd ) − ψ(Xd
γtkd )
( 25 )
− log φtdwk − 1 )
Setting the partial derivative to zero and solving yields the maximizing value of the variational parameter φtdwk as shown in Eq ( 9 ) .
Similarly , collecting the terms in Eq ( 19 ) that contain
γtkd into L[γtkd ] , we get :
L[γtkd ] = ( ψ(γtkd ) − ψ(Xd
γtkd))(α +Xw − γtkd ) + log Γ(γtkd ) − log Γ(Xd ntdwφtdwk
γtkd )
( 26 )
Taking the partial derivative of L[γtkd ] with respect to γtdk gives :
∂L[γtkd ] ∂γtkd
= ( ψ ′(γtkd ) − ψ ′(Xd
γtkd ) ) ntdwφtdwk − γtkd )
( 27 )
( α +Xw
Equating the partial derivative to zero results in the maximizing expression for γtkd shown in Eq ( 10 ) .
B . PROOF THAT ZT W K IS A POISSON VARI
ABLE WITH MEAN µ(S )
T KW
