Density Based Clustering for Real Time Stream Data
Department of Computer Science and
Yixin Chen
Engineering
Washington University in St . Louis
St . Louis , USA chen@csewustledu
Li Tu
Institute of Information Science and Technology
Nanjing University of Aeronautics and
Astronautics litu@nuaaeducn
ABSTRACT Existing data stream clustering algorithms such as CluStream are based on k means . These clustering algorithms are incompetent to find clusters of arbitrary shapes and cannot handle outliers . Further , they require the knowledge of k and user specified time window . To address these issues , this paper proposes D Stream , a framework for clustering stream data using a density based approach . The algorithm uses an online component which maps each input data record into a grid and an offline component which computes the grid density and clusters the grids based on the density . The algorithm adopts a density decaying technique to capture the dynamic changes of a data stream . Exploiting the intricate relationships between the decay factor , data density and cluster structure , our algorithm can efficiently and effectively generate and adjust the clusters in real time . Further , a theoretically sound technique is developed to detect and remove sporadic grids mapped to by outliers in order to dramatically improve the space and time efficiency of the system . The technique makes high speed data stream clustering feasible without degrading the clustering quality . The experimental results show that our algorithm has superior quality and efficiency , can find clusters of arbitrary shapes , and can accurately recognize the evolving behaviors of real time data streams .
1 .
INTRODUCTION
Clustering high dimensional stream data in real time is a difficult and important problem with ample applications such as network intrusion detection , weather monitoring , emergency response systems , stock trading , electronic business , telecommunication , planetary remote sensing , and web site analysis . In these applications , large volume of multidimensional data streams arrive at the data collection center in real time . Examples such as the transactions in a supermarket and the phone records of a mobile phone company illustrate that , the raw data typically have massive volume and can only be scanned once following the temporal or der [ 7 , 8 ] . Recently , there has been active research on how to store , query and analyze data streams .
Clustering is a key data mining task . In this paper , we consider clustering multi dimensional data in the form of a stream , ie a sequence of data records stamped and ordered by time . Stream data clustering analysis causes unprecedented difficulty for traditional clustering algorithms . There are several key challenges . First , the data can only be examined in one pass . Second , viewing a data stream as a long vector of data is not adequate in many applications . In fact , in many applications of data stream clustering , users are more interested in the evolving behaviors of clusters .
Recently , there have been different views and approaches to stream data clustering . Earlier clustering algorithms for data stream uses a single phase model which treats data stream clustering as a continuous version of static data clustering [ 9 ] . These algorithms uses divide and conquer schemes that partition data streams into segments and discover clusters in data streams based on a k means algorithm in finite space [ 10 , 12 ] . A limitation of such schemes is that they put equal weights to outdated and recent data and cannot capture the evolving characteristics of stream data . Movingwindow techniques are proposed to partially address this problem [ 2 , 4 ] .
Another recent data stream clustering paradigm proposed by Aggarwal et al . uses a two phase scheme [ 1 ] which consists of an online component that processes raw data stream and produces summary statistics and an offline component that uses the summary data to generate clusters . Strategies for dividing the time horizon and manage the statistics are studied . The design leads to the CluStream system [ 1 ] . Many recent data stream clustering algorithms are based on CluStream ’s two phase framework . Wang et al . proposed an improved offline component using an incomplete partitioning strategy [ 16 ] . Extensions of this work including clustering multiple data streams [ 6 ] , parallel data streams [ 5 ] , and distributed data steams [ 3 ] , and applications of data stream mining [ 11 , 15 , 13 ] .
A number of limitations of CluStream and other related work lie in the k means algorithm used in their offline component . First , a fundamental drawback of k means is that it aims at identifying spherical clusters but is incapable of revealing clusters of arbitrary shapes . However , nonconvex and interwoven clusters are seen in many applications . Second , the k means algorithm is unable to detect noise and outliers . Third , the k means algorithm requires multiple scans of the data , making it not directly applicable to largevolume data stream . For this reason , the CluStream ar chitecture uses an online processing which compresses raw data stream in micro clusters , which are used as the basic elements in the offline phase .
Density based clustering has been long proposed as another major clustering algorithm [ 14 ] . We find the densitybased method a natural and attractive basic clustering algorithm for data streams , because it can find arbitrarily shaped clusters , it can handle noises and is an one scan algorithm that needs to examine the raw data only once . Further , it does not demand a prior knowledge of the number of clusters k as the k means algorithm does .
In this paper , we propose D Stream , a density based clustering framework for data streams . It is not a simple switch over to use density based instead of k means algorithms for data streams . There are two main technical challenges .
First , it is not desirable to treat the data stream as a long sequence of static data since we are interested in the evolving temporal feature of the data stream . To capture the dynamic changing of clusters , we propose an innovative scheme that associates a decay factor to the density of each data point . Unlike the CluStream architecture which asks the users to input the target time duration for clustering , the decay factor provides a novel mechanism for the system to dynamically and automatically form the clusters by placing more weights on the most recent data without totally discarding the historical information . In addition , D Stream does not require the user to specify the number of clusters k . Thus , D Stream is particularly suitable for users with little domain knowledge on the application data .
Second , due to the large volume of stream data , it is impossible to retain the density information for every data record . Therefore , we propose to partition the data space into discretized fine grids and map new data records into the corresponding grid . Thus , we do not need to retain the raw data and only need to operate on the grids . However , for high dimensional data , the number of grids can be large . Therefore , how to handle with high dimensionality and improve scalability is a critical issue . Fortunately , in practice , most grids are empty or only contain few records and a memory efficient technique for managing such a sparse grid space is developed in D Stream .
By addressing the above issues , we propose D Stream , a density based stream data clustering framework . We study in depth the relationship between time horizon , decay factor , and data density to ensure the generation of high quality clusters , and develop novel strategies for controlling the decay factor and detecting outliers . D Stream automatically and dynamically adjusts the clusters without requiring user specification of target time horizon and number of clusters . The experimental results show that D Stream can find clusters of arbitrary shapes . Comparing to CluStream , D Stream is better in terms of both clustering quality and efficiency and it exhibits high scalability for large scale and high dimensional stream data .
The rest of the paper is organized as follows . In Section 2 , we overview the overall architecture of D Stream . In Section 3 , we present the concept and theory on the proposed density grid and decay factor . In Section 4 , we give the algorithmic details and theoretical analysis for D Stream . We conduct experimental study of D Stream and compare D Stream to CluStream on real world and synthetic data sets in Section 5 and conclude the paper in Section 6 . tc = 0 ; initialize an empty hash table grid list ; while data stream is active do
1 . procedure D Stream 2 . 3 . 4 . 5 . 6 . 7 . 8 . 9 . 10 . 11 . 12 . 13 . 14 . 15 . 16 . 17 . 18 . end procedure end if tc = tc + 1 ; end while read record x = ( x1 , x2 , · · · , xd ) ; determine the density grid g that contains x ; if(g not in grid list ) insert g to grid list ; update the characteristic vector of g ; if tc == gap then call initial clustering(grid list ) ; end if if tc mod gap == 0 then detect and remove sporadic grids from grid list ; call adjust clustering(grid list ) ;
Figure 1 : The overall process of D Stream .
2 . OVERALL ALGORITHM OF D STREAM We overview the overall architecture of D Stream , which assumes a discrete time step model , where the time stamp is labelled by integers 0 , 1 , 2 , · · · , n , · · · . Like CluStream [ 1 ] , DStream has an online component and an offline component . The overall algorithm is outlined in Figure 1 .
For a data stream , at each time step , the online component of D Stream continuously reads a new data record , place the multi dimensional data into a corresponding discretized density grid in the multi dimensional space , and update the characteristic vector of the density grid ( Lines 5 8 of Figure 1 ) . The density grid and characteristic vector are to be described in detail in Section 3 . The offline component dynamically adjusts the clusters every gap time steps , where gap is an integer parameter . After the first gap , the algorithm generates the initial cluster ( Lines 9 11 ) . Then , the algorithm periodically removes sporadic grids and regulates the clusters ( Lines 12 15 ) .
3 . DENSITY GRIDS
In this section , we introduce the concept of density grid and other associated definitions , which form the basis for the D Stream algorithm .
Since it is impossible to retain the raw data , D Stream partitions the multi dimensional data space into many density grids and forms clusters of these grids . This concept is schematically illustrated in Figure 2 . 3.1 Basic definitions
In this paper , we assume that the input data has d dimensions , and each input data record is defined within the space
S = S1 × S2 × · · · × Sd ,
( 1 ) where Si is the definition space for the ith dimension .
In D Stream , we partition the d−dimensional space S into density grids . Suppose for each dimension , its space Si , i = 1 , · · · , d is divided into pi partitions as
Si = Si,1[ Si,2[ · · ·[ Si,pi ,
( 2 ) then the data space S is partitioned into N = Qd i=1 pi density grids . For a density grid g that is composed of
Therefore , we have :
D(g , tn ) = m
Xi=1
D(xi , tn ) + 1 = m
Xi=1
λtn−tl D(xi , tl ) + 1
= λtn−tl m
Xi=1
D(xi , tl ) + 1 = λtn−tl D(g , tl ) + 1 .
' Proposition 3.1 saves huge amount of computation time . To update all grids at each time step requires Θ(N ) computing time for density update at each time step . In contrast , using Proposition 3.1 allows us to update only one grid , leading to a Θ(1 ) running time . The efficiency improvement is significant since the number of grids N is typically large .
Moreover , Proposition 3.1 saves memory space . We find that we do not need to save the time stamps and densities of all the data records in a grid . Instead , for each grid , it suffices to store a characteristic vector defined as follows . We will explain the use of each element in the vector later . Definition 32 ( Characteristic Vector ) The characteristic vector of a grid g is a tuple ( tg , tm , D , label , status ) , where tg is the last time when g is updated , tm is the last time when g is removed from grid list as a sporadic grid ( if ever ) , D is the grid density at the last update , label is the class label of the grid , and status = {SPORADIC , NORMAL} is a label used for removing sporadic grids .
3.2 Density based grid clusters
We now need to decide how to derive clusters based on the density information . Our method is based on the following observation .
Proposition 32 Let X(t ) be the set of all data records that arrive from time 0 to t , we have :
1−λ , for any t = 1 , 2 , · · · ;
1 ) Px∈X(t ) D(x , t ) ≤ 1 2 ) limt→∞Px∈X(t ) D(x , t ) = 1 Proof . For a given time t , Px∈X(t ) D(x , t ) is the sum of density coefficient of the t+1 data records that arrive at time steps 0 , 1 , · · · , t , respectively . For a data record x arriving at time t′ , 0 ≤ t′ ≤ t ( T ( x ) = t′ ) , its density is D(x , t ) = λt−t′ . Therefore , the sum over all the data records is :
1−λ .
Xx∈X(t )
D(x , t ) = t
Xt′=0
λt−t′
=
1 − λt+1
1 − λ
≤
1
1 − λ
.
Also , it is clear that : lim t→∞ Xx∈X(t )
D(x , t ) = lim t←∞
1 − λt+1
1 − λ
=
1
1 − λ
.
' data records in the system will never exceed
Proposition 3.2 shows that the sum of the density of all 1 1−λ . Since i=1 pi grids , the average density of each N(1−λ ) . This obser there are N = Qd grid is no more than but approaching vation motivates the following definitions .
1
At time t , for a grid g , we call it a dense grid if
D(g , t ) ≥
Cm
N ( 1 − λ )
= Dm ,
( 8 )
Figure 2 : Illustration of the use of density grid .
S1,j1 × S2,j2 · · · × Sd,jd , ji = 1 , . . . , pi , we denote it as g = ( j1 , j2 , · · · , jd ) .
( 3 )
A data record x = ( x1 , x2 , · · · , xd ) can be mapped to a density grid g(x ) as follows : g(x ) = ( j1 , j2 , · · · , jd ) where xi ∈ Si,ji .
For each data record x , we assign it a density coefficient which decreases with as x ages . In fact , if x arrives at time tc , we define its time stamp T ( x ) = tc , and its density coefficient D(x , t ) at time t is
D(x , t ) = λt−T ( x ) = λt−tc ,
( 4 ) where λ ∈ ( 0 , 1 ) is a constant called the decay factor .
Definition 31 ( Grid Density ) For a grid g , at a given time t , let E(g , t ) be the set of data records that are map to g at or before time t , its density D(g , t ) is defined as the sum of the density coefficients of all data records that mapped to g . Namely , the density of g at t is :
D(g , t ) = Xx∈E(g,t )
D(x , t ) .
The density of any grid is constantly changing . However , we have found that it is unnecessary to update the density values of all data records and grids at every time step . Instead , it is possible to update the density of a grid only when a new data record is mapped to that grid . For each grid , the time when it receives the last data record should be recorded so that the density of the grid can be updated according to the following result when a new data record arrives at the grid .
Proposition 31 Suppose a grid g receives a new data record at time tn , and suppose the time when g receives the last data record is tl ( tn > tl ) , then the density of g can be updated as follows :
D(g , tn ) = λtn−tl D(g , tl ) + 1
( 5 ) Proof . Let X = {x1 , · · · , xm} be the set of all data records in g at time tl , we have :
D(g , tl ) = m
Xi=1
D(xi , tl ) .
( 6 )
According to ( 4 ) , we have that :
D(xi , tn ) = λtn−T ( xi ) = λtn−tl λtl−T ( xi )
= λtn−tl D(xi , tl ) , for i = 1 , · · · , m .
( 7 ) where Cm > 1 is a parameter controlling the threshold . For example , we set Cm = 3 . We require N > Cm since D(g , t ) cannot exceed 1
1−λ .
At time t , for a grid g , we call it a sparse grid if
D(g , t ) ≤
Cl
N ( 1 − λ )
= Dl ,
( 9 ) where 0 < Cl < 1 . For example , we set Cl = 08
At time t , for a grid g , we call it a transitional grid if
Cl
N ( 1 − λ )
≤ D(g , t ) ≤
Cm
N ( 1 − λ )
.
( 10 )
In the multi dimensional space , we consider connecting neighboring grids , defined below , in order to form clusters .
Definition 33 ( Neighboring Grids ) Consider two den
1 , j1
2 , · · · , j1 d ) and g2 = ( j2 sity grids g1 = ( j1 there exists k , 1 ≤ k ≤ d , such that : 1 ) j1 2 ) |j1 then g1 and g2 are neighboring grids in the kth dimension , denoted as g1 ∼ g2 . i , i = 1 , · · · , k − 1 , k + 1 , · · · , d ; and k| = 1 , i = j2 k − j2
1 , j2
2 , · · · , j2 d ) , if
Definition 34 ( Grid Group ) A set of density grids G = ( g1 , · · · , gm ) is a grid group if for any two grids gi , gj ∈ G , there exist a sequence of grids gk1 , · · · , gkl such that gk1 = gi , gkl = gj , and gk1 ∼ gk2 , gk2 ∼ gk3 , · · · , and gkl−1 ∼ gkl .
Definition 35 ( Inside and Outside Grids ) Consider a grid group G and a grid g ∈ G , suppose g = ( j1 , · · · , jd ) , if g has neighboring grids in every dimension i = 1 , · · · , d , then g is an inside grid in G . Otherwise g is an outside grid in G .
A key decision is the length of the time interval for grid inspection . It is interesting to note that the value of the time interval gap cannot be too large or too small . If gap is too large , dynamical changes of data streams will not be adequately recognized . If gap is too small , it will result in frequent computation by the offline component and increase the workload . When such computation load is too heavy , the processing speed of the offline component may not match the speed of the input data stream .
We propose the following strategy to determine the suitable value of gap . We consider the minimum time needed for a dense grid to degenerate to a sparse grid as well as the minimum time needed for a sparse grid to become a dense grid . Then we set gap to be minimum of these two minimum times in order to ensure that the inspection is frequent enough to detect the density changes of any grid .
Proposition 41 For any dense grid g , the minimum time needed for g to become a sparse grid from being a dense grid is
δ0 = — logλ„ Cl
Cm« .
( 11 )
Proof . According to ( 8 ) , if at time t , a grid g is a dense grid , then we have :
D(g , t ) ≥ Dm =
Cm
N ( 1 − λ )
.
( 12 )
Suppose after δt time , g becomes a sparse grid , then we have :
Now we are ready to define how to form clusters based on
D(g , t + δt ) ≤ Dl =
Cl
N ( 1 − λ )
.
( 13 ) the density of grids .
Definition 36 ( Grid Cluster ) Let G = ( g1 , · · · , gm ) be a grid group , if every inside grid of G is a dense grid and every outside grid is either a dense grid or a transitional grid , then G is a grid cluster .
Intuitively , a grid cluster is a connected grid group which has higher density than the surrounding grids . Note that we always try to merge clusters whenever possible , so the resulting clusters are surrounded by sparse grids .
4 . COMPONENTS OF D STREAM
We now describe in detail the key components of D Stream outline in Figure 1 . As we have discuss in the last section , for each new data record x , we map it to a grid g and use ( 5 ) to update the density of g ( Lines 5 8 of Figure 1 ) . We then periodically ( every gap time steps ) form clusters and remove sporadic grids . In the following , we describe our strategies for determining gap , managing the list of active grids , and generating clusters . 4.1 Grid inspection and time interval gap
To mine the dynamic characteristics of data streams , our density grid scheme developed in Section 3 gradually reduces the density of each data record and grid . A dense grid may degenerate to a transitional or sparse grid if it does not receive no new data for a long time . On the other hand , a sparse grid can be upgraded to a transitional or dense grid after it receives some new data records . Therefore , after a period of time , the density of each grid should be inspected and the clusters adjusted .
On the other hand , let E(g , t ) be the set of data records in g at time t , we have E(g , t ) ⊆ E(g , t + δt ) and :
D(g , t + δt ) = Xx∈E(g,t+δt )
D(x , t + δt )
≥ Xx∈E(g,t ) = Xx∈E(g,t )
D(x , t + δt )
λδt D(x , t ) = λδt D(g , t ) ( 14 )
Combining ( 13 ) and ( 14 ) we get :
λδt D(g , t ) ≤ D(g , t + δt ) ≤
Cl
N ( 1 − λ )
Combining ( 12 ) and ( 15 ) we get :
λδt
Cm
N ( 1 − λ )
≤ λδt D(g , t ) ≤
Cl
N ( 1 − λ ) which yields :
Cm« δt ≥ logλ„ Cl
( 15 )
( 16 )
( 17 )
'
Proposition 42 For any sparse grid g , the minimum time needed for g to become a dense grid from being a sparse grid is
δ1 = — logλ„ N − Cm
N − Cl « .
( 18 )
Proof . According to ( 9 ) , if at time t , a grid g is a sparse grid , then we have :
D(g , t ) ≤ Dl =
Cl
N ( 1 − λ )
.
( 19 )
Suppose after δt time , g becomes a dense grid , then we have :
D(g , t + δt ) ≥ Dm =
Cm
N ( 1 − λ )
.
( 20 )
We also know that :
D(g , t + δt ) = Xx∈E(g,t+δt )
D(x , t + δt )
( 21 )
E(g , t + δt ) can be divided into those points in E(g , t ) and those come after t . The least time for a sparse grid g to become dense is achieved when all the new data records are mapped to g . In this case , there is a new data record mapped to g for any of the time steps from t + 1 until t + δt . The sum of the density of all these new records at time t + δt is
Pδt−1 i=0 λi . Therefore we have :
D(g , t + δt ) ≤ Xx∈E(g,t ) = Xx∈E(g,t ) = λδt D(g , t ) +
D(x , t + δt ) +
λδt D(x , t ) +
1 − λδt 1 − λ
δt−1
Xi=0
λi
1 − λδt 1 − λ
Now we plug ( 20 ) and ( 19 ) into ( 22 ) to obtain :
Cm
N ( 1 − λ )
≤ D(g , t + δt ) ≤ λδt D(g , t ) +
1 − λδt 1 − λ
≤
λδt Cl
N ( 1 − λ )
+
1 − λδt 1 − λ
Solving ( 23 ) yields : which results in :
λδt ≤
N − Cm N − Cl
,
δt ≥ logλ„ N − Cm
N − Cl « .
( 22 )
( 23 )
( 24 )
( 25 )
A key observation is that most of the grids in the space are empty or receive data very infrequently . In our implementation , we allocate memory to store the characteristic vectors for those grids that are not empty , which form a very small subset in the grid space . Unfortunately , in practice , this is still not efficient enough due to the appearance of outlier data that are made from errors , which lead to continual increase of non empty grids that will be processed during clustering . We call such grids sporadic grids since they contain very few data . Since a data stream flows in by massive volume in high speed and it could run for a very long time , sporadic grids accumulate and their number can become exceedingly large , causing the system to operate more and more slowly . Therefore , it is imperative to detect and remove such sporadic grids periodically . This is done in Line 13 of the D Stream algorithm in Figure 1 .
Sparse grid with D ≤ Dl are candidates for sporadic grids . However , there are two reasons for the density of a grid to be less than Dl . The first cause is that it has received very few data , while the second cause is that the grid has previously received many data but the density is reduced by the effect of decay factor . Only the grids in the former case are true sporadic grids that we aim to remove . The sparse grids in the latter case should not be removed since they contain many data records and are often upgraded to transitional or dense grids . We have found through extensive experimentation that wrongly removing these grids in the latter case can significantly deteriorate the clustering quality .
We define a density threshold function to differentiate these two classes of sparse grids .
Definition 41 ( Density Threshold Function ) Suppose the last update time of a grid g is tg , then at time t ( t > tg ) , the density threshold function is
π(tg , t ) =
Cl N t−tg
Xi=0
λi =
Cl(1 − λt−tg+1 )
N ( 1 − λ )
( 27 )
Proposition 43 There are the following properties of the function π(tg , t ) .
( 1 ) If t1 ≤ t2 ≤ t3 , then
λt3−t2 π(t1 , t2 ) + π(t2 + 1 , t3 ) = π(t1 , t3 ) .
( 2 ) If t1 ≤ t2 , then π(t1 , t ) ≥ π(t2 , t ) for any t > t1 , t2 .
Note N − Cm > 0 since Cm < N according to ( 8 ) .
' Based on the two propositions above , we choose gap to be small enough so that any change of a grid from dense to sparse or from sparse to dense can be recognized . Thus , in D Stream we set : gap = min{δ0 , δ1}
Proof . ( 1 ) We see that :
Cl
Cm,— logλ
N − Cm
N − Cl ff
= min— logλ = — logλ„ max Cl
Cm
,
N − Cm
N − Cl ff«
( 26 )
4.2 Detecting and removing sporadic grids
A serious challenge for the density grid scheme is the large number of grids , especially for high dimensional data . For example , if each dimension is divided into 20 regions , there will be 20d possible grids .
λt3−t2 π(t1 , t2 ) + π(t2 + 1 , t3 ) t3−t2−1 t2−t1
λt3−t2+i +
Cl N
λi
Xi=0
Xi=0 Xi=t3−t2 t3−t1
=
=
=
Cl N
Cl N
Cl N
λi +
Cl N t3−t2−1
λi
Xi=0 t3−t1
Xi=0
λi = π(t1 , t3 )
( 2 ) Let ∆t = t2 − t1 , we have
π(t1 , t ) =
=
Cl N
Cl N t−t1
Xi=0 Xi=0 t−t2
λi =
λi +
Cl N
Cl N t−t2+∆t
Xi=0 Xi=t−t2+1 t−t2+∆t
λi
λi
= π(t2 , t ) +
Cl N t−t2+∆t
Xi=t−t2+1
λi ≥ π(t2 , t )
' We use π(tg , t ) to detect sporadic grids from all sparse grids . In the periodic inspection in Line 13 of Figure 1 , at time t , we judge that a sparse grid is a sporadic grid if :
( S1 ) D(g , t ) < π(tg , t ) ; and
( S2 ) t ≥ ( 1 + β)tm if g has been delete before ( at time tm ) , where β > 0 is a constant .
Note that tm and tg are stored in the characteristic vector . In D Stream , we maintain a grid list which includes the grids that are under consideration for clustering analysis . The grid list is implemented as a hash table using doublylinked lists to resolve collision . The hash table allows for fast lookup , update , and deletion . The key of the hash table are the grid coordinates , while the associated data for each grid entry is the characteristic vector .
We use the following rules to delete sporadic grids from grid list .
( D1 ) During the periodic inspection in Line 13 of Figure 1 , all grids satisfying ( S1 ) and ( S2 ) are marked as SPORADIC but wait until the next periodic inspection to be considered for deletion .
( D2 ) In the next periodic inspection , if a grid g marked as SPORADIC has not received any data since last inspection , we remove g from grid list . Otherwise , check if g satisfies ( S1 ) and ( S2 ) : if yes , we keep g marked as SPORADIC but do not remove it ; otherwise , we reset the label to NORMAL .
It should be noted that once a sporadic grid is deleted , its density is in effect reset to zero since its characteristic vector is deleted . A deleted grid may be added back to grid list if there are new data records mapped to it later , but its previous records are discarded and its density restarts from zero . Such a dynamic mechanism maintains a moderate size of the grids in memory , saves computing time for clustering , and prevents infinite accumulation of sporadic grids in memory . Although deleting sporadic grids is critical for the efficient performance of D Stream , an important issue for the correctness of this scheme is whether the deletions affect the clustering results . In particular , since a sporadic grid may receive data later and become a transitional or dense grid , we need to know if it is possible that the deletion prevents this grid from being correctly labelled as a transitional or dense grid . We have designed the density threshold function π(tg , t ) and the deletion rules in such a way that a transitional or dense grid will not be falsely deleted due to the removal of sporadic grids .
Consider a grid g , whose density at time t is D(g , t ) . Suppose that it has been deleted several times before t ( the density is reset to zero each time ) because its density is less than the density threshold function at various times . Suppose these density values are not cleared and suppose all data are kept , the density of grid g would be Da(g , t ) . We call Da(g , t ) the complete density function of the grid g . Now we present several strong theoretical properties of the π(tg , t ) which ensure the proper functioning of the D Stream system . We will show that , if a grid can later become a transitional or dense grid , deleting it as a sporadic grid will not affect its later upgrades .
The first question we investigate is , if a grid g is detected as a sporadic grid , is it possible that g can be non sporadic if it has not been previously deleted from grid list ? It is answered in the following result .
Proposition 44 Suppose the last time a grid g is deleted as a sporadic grid is tm and the last time g receives a data record is tg . If at current time t , we have D(g , t ) < π(tg , t ) , then we also have Da(g , t ) < π(0 , t ) < Dl . Proof . Suppose the grid g has been previously deleted for the periods of ( 0 , t1 ) , ( t1 + 1 , t2 ) , · · · , ( tm−1 + 1 , tm ) , then the density value D(g , ti ) , i = 1m satisfies ( let t0 = −1 ) :
D(g , ti ) < π(ti−1 + 1 , ti ) .
( 28 )
Thus , if all these previous data are not deleted , the complete density function satisfies :
Da(g , t ) =
< m
Xi=1 Xi=1 m
D(g , ti)λt−ti + D(g , t )
π(ti−1 + 1 , ti)λt−ti + π(tg , t )
( 29 )
Since tg ≥ tm + 1 , by property ( 2 ) in Proposition 4.3 , we know
Da(g , t ) <
=
=
· · · m m−1
Xi=1 Xi=1 Xi=1 m−2
π(ti−1 + 1 , ti)λt−ti + π(tm + 1 , t )
π(ti−1 + 1 , ti)λt−ti + π(tm−1 + 1 , t )
π(ti−1 + 1 , ti)λt−ti + π(tm−2 + 1 , t )
= π(0 , t ) =
Cl(1 − λt+1 )
N ( 1 − λ )
< Dl .
( 30 )
The last equalities are based on successive applications of property ( 1 ) in Proposition 43 ' Proposition 4.4 is important since it shows that deleting a sporadic grid will not cause transitional or dense grid be falsely deleted . It shows that , if g is deleted as a sporadic grid at t since D(g , t ) < π(tg , t ) , then even if all the previous deletions have not occured , it is still sporadic and cannot be a transitional or dense grid since Da(g , t ) < Dl .
Proposition 45 Suppose the density of a grid g at time t is D(g , t ) , and g receives no data from t + 1 to t + gap , then there exist t0 > 0 and t1 > 0 such that :
( a ) If D(g , t ) < Dl , then Da(g , t + gap ) < Dl , for t ≥ t0 .
( b ) If D(g , t ) < Dm , then Da(g , t + gap ) < Dm , for t ≥ t1 .
Proof . We prove ( a ) . ( b ) can be proved similarly . Suppose the grid g has been previously deleted for the periods of ( 0 , t1 ) , ( t1 + 1 , t2 ) , · · · , ( tm−1 + 1 , tm ) , then :
Da(g , t + gap ) = m
Xi=1
D(g , ti)λt−ti+gap + D(g , t + gap ) ( 31 )
Since we assume that g receives no data from t+1 to t+gap ,
Da(g , t + gap ) =
< m
Xi=1 Xi=1 m
D(g , ti)λt−ti+gap + D(g , t)λgap
π(ti−1 + 1 , ti)λt−ti+gap + Dlλgap
= π(0 , tm)λt−tm λgap + Dlλgap
( according to ( S2 ) ) < π(0 , tm)λβt/(1+β)λgap + Dlλgap
In order to ensure Da(g , t + gap ) < Dl , we require :
π(0 , tm)λβt/(1+β)λgap + Dlλgap < Dl
⇒ λβt/(1+β ) <
( 1 − λgap)Dl λgapπ(0 , tm )
=
1 − λgap
λgap(1 − λtm+1 )
Thus , ( a ) is true for t0 satisfying : t0 > „ 1 + β
β « logλ„
1 − λgap
λgap(1 − λtm+1)«
' Proposition 4.5 is a key result showing that ( S1 ) , ( S2 ) , ( D1 ) and ( D2 ) work together correctly . It implies that , as time extends for long enough , we will never delete a potential transitional or dense grid due to the previous removals of data . If a grid is sparse ( resp . not dense ) , then when it is deleted , it must be sparse ( resp . not dense ) even considering those deleted data . Note that Da(g , t + gap ) is the density of the grid upon deletion assuming no previous deletion has ever occurred . The result shows that , after an initial phase , deleting sporadic grids does not affect the clustering results . 4.3 Clustering algorithms
We describe the algorithms for generating the initial cluster and for adjusting the clusters every gap steps . The procedure initial clustering ( used in Line 10 of Figure 1 ) is illustrated in Figure 3 . The procedure adjust clustering ( used in Line 14 of Figure 1 ) is illustrated in Figure 4 . They first update the density of all active grids to the current time . Once the density of grids are determined at the given time , the clustering procedure is similar to the standard method used by density based clustering .
It should be noted that , during the computation , whenever we update grids or find neighboring grids , we only consider those grids that are maintained in grid list . Therefore , although the number of possible grids is huge for highdimensional data , most empty or infrequent grids are discarded , which saves computing time and makes our algorithm very fast without deteriorating clustering quality .
5 . EXPERIMENTAL RESULTS
We evaluate the quality and efficiency of D Stream and compare it with CluStream [ 1 ] . All of our experiments are conducted on a PC with 1.7GHz CPU and 256M memory running Windows XP . We have implemented D Stream in foreach cluster c foreach outside grid g of c update the density of all grids in grid list ; assign each dense grid to a distinct cluster ; label all other grids as NO CLASS ; repeat
1 . procedure initial clustering ( grid list ) 2 . 3 . 4 . 5 . 6 . 7 . 8 . 9 . 10 . 11 . 12 . 13 . 14 . end procedure foreach neighboring grid h of g if ( h belongs to cluster c′ ) if ( |c| > |c′| ) label all grids in c′ as in c ; else label all grids in c as in c′ ; else if ( h is transitional ) label h as in c ; until no change in the cluster labels can be made
Figure 3 : The procedure for initial clustering .
1 . procedure adjust clustering ( grid list ) 2 . 3 . update the density of all grids in grid list ; foreach grid g whose attribute ( dense/sparse/transitional ) is changed since last call to adjust clustering( )
4 . 5 . 6 . 7 . 8 .
9 . 10 . 11 . 12 . 13 . 14 . 15 . 16 .
17 . 18 . 19 . 20 . if ( g is a sparse grid ) delete g from its cluster c , label g as NO CLASS ; if ( c becomes unconnected ) split c into two clusters ; else if ( g is a dense grid ) among all neighboring grids of g , find out the grid h whose cluster ch has the largest size ; if ( h is a dense grid ) if ( g is labelled as NO CLASS ) label g as in ch ; else if ( g is in cluster c and |c| > |ch| ) label all grids in ch as in c ; else if ( g is in cluster c and |c| ≤ |ch| ) label all grids in c as in ch ; else if ( h is a transitional grid ) if ( (g is NO CLASS ) and ( h is an outside grid if g is added to ch ) ) label g as in ch ; else if ( g is in cluster c and |c| ≥ |ch| ) move h from cluster ch to c ; else if ( g is a transitional grid ) among neighboring clusters of g , find the largest one c′ satisfying that g is an outside grid if added to it ; label g as in c′ ;
21 . 22 . 23 . end procedure end for
Figure 4 : The procedure for dynamically adjusting clusters .
VC++ 6.0 , which is hooked up with Matlab 6.5 to visualize the results . In all experiments , we use Cm = 3.0 , Cl = 0.8 , λ = 0.998 , and β = 03
We use two testing sets . The first testing set is a real data set used by the KDD CUP 99 . It contains network intrusion detection stream data collected by the MIT Lincoln laboratory [ 1 ] . This data set contains a total of five clusters and each connection record contains 42 attributes . As in [ 1 ] , all the 34 continuous attributes are used for clustering . In addition , we also use some synthetic data sets to test the scalability of D Stream . The synthetic data sets have a varying base size from 30K to 85K , the number of clusters is set to 4 , and the number of dimensions is in the range of 2 to 40 . In the experiments below , we normalize all the attributes of the data sets to [ 0 , 1 ] . Each dimension is evenly partitioned into multiple segments , each with length len . 5.1 Evolving data streams with many outliers We find that the sequence order of data stream can make great effect on the clustering results . In order to validate the effectiveness of D Stream , we generate the synthetic data is 1K/second , which means that there are 1K input data points coming evenly in one second and the whole stream is processed in 85 seconds . We check the clustering results at three different times , including t1 = 25 , t2 = 55 , and t3 = 85 . The clustering results are shown from Figure 8 to 10 . It clearly illustrates that D Stream can adapt timely to the dynamic evolution of stream data and is immune to the outliers . sets according to two different orders .
First , we randomly generate 30K 2 dimensional data set in 4 clusters , including 5K outlier data that are scattered in the space . The distribution of the original data set is shown in Figure 5 . These clusters have nonconvex shapes and some are interwoven . We generate the data sequentially at each time step . At each time , any data point that has not been generated is equally likely to be picked as the new data record . Therefore , data points from different clusters and those outliers alternately appear in the data stream . The final test result by D Stream is shown in Figure 6 . we set len = 005 From Figure 6 , we can see that the algorithm can discover the four clusters without user supply on the number of clusters . It is much more effective than the k means algorithm used by CluStream since k means will fail on such data sets with many outliers . We can also see that our scheme for detecting sporadic grids can effectively remove most outliers .
Figure 7 : Original distribution of the 85K data .
Figure 5 : Original distribution of the 30K data .
Figure 8 : Clustering results at t1 = 25 .
Figure 6 : Final clustering results on the 30K data .
Figure 9 : Clustering results at t2 = 55 .
In the second test , we aim to show that D Stream can capture the dynamic evolution of data clusters and can remove real outlier data during such an adaptive process . To this end , we order the four classes and generate them sequentially one by one . In this test , we generate 85K data points including 10K random outlier data . The data distribution is shown in Figure 7 . The speed of the data stream
5.2 Clustering quality comparison
We test D Stream on the synthetic data set and KDD CUP 99 data set described above under different grid granularity . The correct rates of clustering results at different times are shown in Figure 11 and 12 . In the figures , len
Figure 10 : Clustering results at t3 = 85 . indicates the size of each partitioned segment in the normalized dimensions . For example , when len = 0.02 , there are 50 segments in each dimension . From Figure 11 , the average correct rates on the synthetic data set by D Stream is above 965 % From Figure 12 , the average correct rate on KDD CUP 99 is above 925 %
We also compare the qualities of the clustering results by D Stream and those by CluStream . Due to the nonconvexity of the synthetic data sets , CluStream can not get a correct result . Thus , its quality can not be compared to that of D Stream . Therefore , we only compare the sum of squared distance ( SSQ ) of the two algorithms on the network intrusion data from KDD CUP 99 . Figure 13 shows the results . We can see that the average SSQ values of D Stream at various times are always less than those of CluStream , which implies that data in each of the cluster obtained by D Stream are much more similar than that obtained by CluStream .
Figure 11 : Correct rates of D Stream on synthetic data .
Figure 12 : Correct rates of D Stream on KDD CUP99 data .
Figure 13 : Comparison of D Stream and CluStream on KDD CUP 99 data .
CUP 99 data with different sizes . The results are shown in Figure 14 . We can see that CluStream requires four to six times more clustering time than D Stream . D Stream is efficient since it only puts each new data record to the corresponding grid by the online component without computing distances as CluStream does . Furthermore , the dynamic detection and deletion of sporadic grids save tremendous time . It can also be seen that D Stream has better scalability since its clustering time grows slower with an increasing data size . Next , both algorithms are tested on the KDD CUP 99 data with different dimensionality . We set the size of data set as 100K and vary the dimensionality in the range of 2 to 40 . We list the time costs under different dimensionality by the two algorithms in Figure 15 . D Stream is 3.5 to 11 times faster than CluStream and scales better with an increasing dimensionality . For example , when the dimensionality is increased from 2 to 40 , the time of D Stream only increases by 15 seconds while the time of CluStream increases by 40 seconds .
5.3 Time performance comparison
6 . CONCLUSIONS
We test and compare the clustering speed of D Stream and CluStream . First , both algorithms are tested on the KDD
In this paper , we propose D Stream , a new framework for clustering real time stream data . The algorithm uses an on
7 . REFERENCES [ 1 ] C . C . Aggarwal , J . Han , J . Wang , and P . S . Yu . A framework for clustering evolving data streams . In Proc . VLDB , pages 81–92 , 2003 .
[ 2 ] B . Babcock , M . Datar , R . Motwani , and
L . O’Callaghan . Maintaining variance and k medians over data stream windows . In Proceedings of the twenty second ACM symposium on Principles of database systems , pages 234–243 , 2003 .
[ 3 ] S . Bandyopadhyay , C . Giannella , U . Maulik ,
H . Kargupta , K . Liu , and S . Datta . Clustering distributed data streams in peer to peer environments . Information Sciences , 176(14):1952–1985 , 2006 .
[ 4 ] D . Barbar´a . Requirements for clustering data streams .
SIGKDD Explorations Newsletter , 3(2):23–27 , 2002 . [ 5 ] J . Beringer and E . H¨ullermeier . Online clustering of parallel data streams . Data and Knowledge Engineering , 58(2):180–204 , 2006 .
[ 6 ] BR Dai , JW Huang , MY Yeh , and MS Chen . Adapative clustering for multiple evolving streams . IEEE Transaction On Knowledge and data engineering , 18(9 ) , 2006 .
[ 7 ] M . Garofalakis , J . Gehrke , and R . Rastogi . Querying and mining data streams : you only get one look . In Proc . ACM SIGMOD , pages 635–635 , 2002 .
[ 8 ] L . Golab and M . T . ¨Ozsu . Issues in Data Stream Management . ACM SIGMOD Record , 32(2):5–14 , 2003 .
[ 9 ] S . Guha , A . Meyerson , N . Mishra , R . Motwani , and
L . O’Callaghan . Clustering data streams : Theory and practice . Trans . Know . Eng . , 15(3):515–528 , 2003 .
[ 10 ] S . Guha , N . Mishra , R . Motwani , and L . O’Callaghan .
Clustering data streams . In Annual IEEE Symp . on Foundations of Comp . Sci . , pages 359–366 , 2000 .
[ 11 ] O . Nasraoui , C . Rojas , and C . Cardona . A framework for mining evolving trends in web data streams using dynamic learning and retrospective validation . Computer Networks , 50(10):1488–1512 , 2006 .
[ 12 ] L . O’Callaghan , N . Mishra , A . Meyerson , S . Guha , and R . Motwani . Streaming data algorithms for high quality clustering . In Proc . of 18th International Conference on Data Engineering , pages 685–694 , 2002 .
[ 13 ] S . Oh , J . Kang , Y . Byun , G . Park , and S . Byun . Intrusion detection based on on clustering a data stream . In Third ACIS International Conference on Software Engineering Research , Management and Applications , pages 220–227 , 2005 .
[ 14 ] J . Sander , M . Ester , H . Kriegel , and X . Xu .
Density based clustering in spatial databases : The algorithm gdbscan and its applications . Data Min . Knowl . Discov . , 2(2):169–194 , 1998 .
[ 15 ] H . Sun , G . Yu , Y . Bao , F . Zhao , and D . Wang . S tree : an effective index for clustering arbitrary shapes in data streams . In Research Issues in Data Engineering : Stream Data Mining and Applications , 15th International Workshop on , pages 81–88 , 2005 .
[ 16 ] Z . Wang , B . Wang , C . Zhou , , and X . Xu . Clustering
Data streams on the Two tier structure . Advanced Web Technologies and Applications , pages 416–425 , 2004 .
Figure 14 : Efficiency comparison with varying sizes of data sets .
Figure 15 : Efficiency comparison with varying dimensionality . line component which maps each input data record into a grid and an offline component which computes the density of each grid and clusters the grids using a density based algorithm . In contrast to previous algorithms based on k means , the proposed algorithm can find clusters of arbitrary shapes , automatically determine the number of clusters , and is immune to outliers . The algorithm also proposes a density decaying scheme that can effectively adjust the clusters in real time and capture the evolving behaviors of the data stream . Further , a sophisticated and theoretically sound technique is developed to detect and remove the sporadic grids in order to dramatically improve the space and time efficiency without affecting the clustering results . The technique makes highspeed data stream clustering feasible without degrading the clustering quality . Experimental results on real world and synthetic data validate the design goals and show that DStream significantly improves over CluStream in terms of both clustering quality and time .
