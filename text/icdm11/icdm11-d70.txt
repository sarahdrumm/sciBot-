A Fixed Parameter Tractable Integer Program for Finding the Maximum Order Preserving Submatrix
Jens Humrich∗ , Thomas Gärtner∗† , and Gemma C . Garriga‡ ∗ Department of Computer Science , University of Bonn , Germany † Knowledge Discovery Department , Fraunhofer IAIS , Germany ‡ Mostrare research team , INRIA Lille Nord Europe , France
Abstract—Order preserving submatrices are an important tool for the analysis of gene expression data . As finding large order preserving submatrices is a computationally hard problem , previous work has investigated both exact but exponential time as well as polynomial time but inexact algorithms for finding large order preserving submatrices . In this paper , we propose a novel exact algorithm to find maximum order preserving submatrices which is fixed parameter tractable with respect to the number of columns of the provided gene expression data . In particular , our algorithm is based on solving a sequence of mixed integer linear programs and it exhibits better guarantees as well as better runtime performance as compared to the state of the art exact algorithms . Our empirical study in benchmark datasets shows large improvement in terms of computational speed .
I . INTRODUCTION
Since the introduction of the order preserving submatrix problem for gene expression data by Ben Dor et al in 2003 [ 1 ] , a large variety of solutions has been suggested in literature . The motivations for the different solutions were the need for ( i ) exact solutions , ( ii ) theoretical guarantees , ( iii ) scalable algorithms , and ( iv ) solutions robust to noise . In this paper , we address all four desiderata by one algorithm .
Order preserving submatrices were proposed as interesting local patterns in gene expression data to achieve a higher robustness with respect to the actual expression levels [ 1 ] . An order preserving submatrix of a matrix A ∈ Rn×m is a submatrix , induced by a set of rows R ⊆ {1 , . . . , n} and columns C ⊆ {1 , . . . , m} , for which an ordering π of the columns exist , such that along this ordering the values of every row of the submatrix are strictly increasing . Each entry A(i , j ) corresponds to the readout associated to row ( gene ) i and column ( experiment ) j of the matrix A and typically it holds that n * m . Two pattern mining problems naturally arise in the context of order preserving submatrices : ( a ) enumerating all order preserving submatrices and ( b ) finding a largest order preserving submatrix . The enumeration problem suffers from the potentially exponential output and from having to select the interesting ones of those . The maximisation problem can be shown to be NP hard by reducing it to its decision version , which is know to be NP complete [ 1 ] . Therefore we will investigate algorithms for the maximisation problem with , in the worst case , exponential run time .
To find the largest order preserving submatrix it is sufficient to enumerate all possible subsets of rows and columns [ 2 ] , as their permutation can be derived from the entries . This results in an algorithm with run time exponential in n + m . To avoid the brute force enumeration in some cases , Trapp and Prokopyev [ 2 ] proposed an integer linear program . While empirically the MILP of Trapp and Prokopyev is in many cases much faster than brute force enumeration , as we will discuss later , their algorithm has worse theoretical guarantees , its run time is exponential not only in n + m but also in nm . In this paper , we propose a novel MILP which is fixed parameter tractable with parameter m , that is , that has run time exponential in m but only polynomial in n . Experimental results confirm the superior performance of our formulation . Building on this improvement , we also propose solutions to a noise tolerant maximum order preserving submatrix problem .
II . PRELIMINARIES
Consider a real valued matrix A ∈ Rn×m . Let [ [n ] ] denote the set {1 , . . . , n} . For subsets of row and column indices R ⊆ [ [n] ] , C ⊆ [ [m] ] , and a permutation π : [ [m ] ] ↔ [ [m ] ] the submatrix A(R , π(C ) ) is the result of arranging the columns in C of the submatrix given by R × C in A according to π . A triple ( R , C , π ) of rows R , columns C , and permutation π induces an order preserving submatrix in A ∈ Rn×m if
A(R , π(C ) ) is ordered , that is , if for all i ∈ R and all j , j0 ∈ C it holds that π(j ) < π(j0 ) implies A(i , j ) < A(i , j0 ) . Such triples ( R , C , π ) will be called OPSM patterns .
The central decision problem of OPSM is : Problem 2.1 ( ∃ OPSM ) : Given a matrix A ∈ Rn×m and parameters ρ ≤ n , γ ≤ m , decide if there is an OPSM pattern ( R , C , π ) with |R| = ρ and |C| = γ . It has been shown that ∃ OPSM is NP complete [ 1 ] .
In the optimization version of this problem , we are interested in finding the maximum OPSM pattern with respect to |R||C| for subsets of rows R ⊆ [ [n ] ] and columns C ⊆ [ [m] ] . As the decision problem is NP complete , the associated optimization problem is NP hard . It is formally defined as :
Problem 2.2 ( MAX OPSM ) : Given a matrix A ∈ Rn×m , find a OPSM pattern ( R , C , π ) such that |R||C| is maximal . The MAX OPSM problem was studied in [ 2 ] , where the authors propose an integer programming based algorithm .
A . Tractability and Brute Force Search
As mentioned in the introduction , to find the largest OPSMpattern it is sufficient to enumerate all possible subsets of rows and columns [ 2 ] , as their permutation can be derived from the entries . Using this observation , we have a run time for the brute force search of O(n exp(n + m ) ) instead of the full O(m! exp(n + m) ) .
As n can be very large , an algorithm with computational complexity larger than exp(n ) is intractable . In contrast , as m is typically very small in the applications we consider , complexities of the form poly(n ) exp(m ) can still be considered tractable . Such algorithms are often called fixed parameter tractable , in this case , with respect to the parameter m . To achieve such an algorithm , the following observation [ 1 ] is central : Given a set of columns C and a permutation π , the set of rows R such that ( R , C , π ) is an OPSM pattern and such that |R||C| is maximal , can be found in time linear in nm . Hence , it is easy to derive an improved brute force algorithm with run time O(n2m exp(m) ) .
Let now πi denote the permutation that puts the i th row in increasing order and by ˆAi = A([i+1 : n ] , πi([[m]] ) ) ∈ Rˆn×m , the submatrix of A starting with the i+1 th row up to the last row , whose columns are permuted according to πi . It is now sufficient to enumerate sets of columns and check for which rows these columns are increasing in each matrix of the set { ˆAi}i . Notice that the i th row itself can be skipped , since it is in increasing order .
To analyse the time complexity of integer as well as mixed integer linear programs ( MILP ) we assume a MILP solver with run time exponential in the number of binary variables and cubic in the number of real variables . Such a MILP solver is easily attained by brute force enumeration and a much better worst case run time can not be hoped for .
B . Related work
Ben Dor et al . [ 1 ] proposed originally a heuristic algorithm for mining OPSM patterns based on a stochastic model which adds rows to an OPSM pattern based on their fitness . The complete enumeration of all OPSM patterns is not ensured , neither having found the solution of the maximum OPSM among those patterns . Rho and Park [ 3 ] , described a genetic algorithm which adds new columns depending on the support of rows for the particular order . Gao et al . [ 4 ] proposed an algorithm for finding OPSM patterns by a row growing strategy with a constrained solution space .
In [ 5 ] , the authors propose to mine for relaxed OPSMpatterns , defined as patterns that are similar in terms of overlapping of the longest common subsequence , to a hidden backbone order in the data . Their algorithm does not ensure complete enumeration . In [ 6 ] the proposal is to mine for OPSM patterns where the attribute order might be slightly violated ; patterns are then grouped in clusters that deviate from the concensus order by up to a certain fraction of attributes . None of these algorithms can make an exhaustive enumeration , nor they approach the optimization problem related to finding maximum tolerant OPSM .
The first paper to work on MAX OPSM is Trapp and Prokopyev [ 2 ] . They motivate the need to have exact optimization approaches making use of mixed integer programming in order to facilitate the theoretical analysis of the problem and practical interpretation of the result . Our approach builds on this work and we therefore describe their approach in more detail in the next section .
C . Exact MAX OPSM by Integer Programming
We next consider the integer linear programming formulation proposed by Trapp and Prokopyev [ 2 ] . The main idea is to find the largest row and column subsets ( R , C ) for every matrix in { ˆAr}r such that ˆAr(R , C ) is ordered . Let ˆnr = n−r and as the index r will usually clear from the context we will from now use ˆA , ˆn instead of ˆAr , ˆnr . The optimisation in each step is then over ˆn binary variables xi , m binary variables yi , and ˆn × m binary variables zij . We will refer to the integer program formulation of Trapp and Prokopyev [ 2 ] by CF OPSM ( for compact formulation of the order preserving submatrix ) . It is defined as :
X
X argmax X,Y,Z zij j i
 zij + zik ≤ xi for all j < k , and ˆA(i , j ) ≥ ˆA(i , k ) for i ∈ [ [ˆn ] ]
( 1a ) st
Constraints ( 1b ) to ( 1d ) enforce that zij = xiyj , zij ≤ xi for all i ∈ [ [ˆn] ] , j ∈ [ [m ] ] ( 1b ) zij ≤ yj for all i ∈ [ [ˆn] ] , j ∈ [ [m ] ] ( 1c ) zij ≥ xi + yj − 1 for all i ∈ [ [ˆn] ] , j ∈ [ [m ] ] ( 1d ) X ∈ {0 , 1}ˆn , Y ∈ {0 , 1}m , Z ∈ {0 , 1}ˆn×m ( 1e ) that is , an entry is considered as ‘selected’ if and only if the corresponding row and column is ‘selected’ . Constraint ( 1a ) ensures that the solution of CF OPSM is such that ˆAr(C∗ , R∗ ) with R∗ = {i | xi = 1} and C∗ = {j | yj = 1} is ordered . For that it prevents entries from being included in the solution that contradict the permutation of row r . This optimisation problem has ˆn + m + ˆnm binary variables and at 2 ˆnm(m − 1 ) constraints . It is hence not fixedmost 3ˆnm + 1 parameter tractable with respect to the parameter m .
To overcome the severe run time demands , Trapp and Prokopyev [ 2 ] added further constraints in their integer program , that we will describe in the next subsection . Most importantly they added constraints on the solution space such that not always the largest OPSM pattern could be found . In Section V we will compare our algorithm with the run time of the best implementation proposed by Trapp and Prokopyev on similar hardware .
III . FIXED PARAMETER TRACTABLE MAX OPSM
In this section we will develop a novel mixed integer linear programming based approach for solving MAX OPSM exactly . The main achievement of this algorithm is that it is fixed parameter tractable with respect to the number of columns of the given matrix as the parameter . Along the way , we will show how the number of variables as well as constraints can be reduced and how upper and lower bounds on the size of the solution can help speed up the overall algorithm .
A . Reducing the Number of Variables
Most of the constraints in Equation ( 1 ) serve only to ensure that zij = xiyj . If we can rewrite the objective function i i xi
P trivial asP
P j zij is equivalent toP and constraint ( 1a ) in terms of xi and yj only , we will thus be able to drop zij . For the target function this rewriting is j yj . To address the constraints , consider first the case xi = 0 : In this case constraint ( 1b ) enforces both zij = 0 and zik = 0 and constraint ( 1a ) is trivially ensured . In the case that xi = 1 , however , constraint ( 1a ) is needed to ensure that the solution is a valid OPSM , ie , that yj + yk ≤ 1 for conflicting columns k , j . It can thus be replaced by the equivalent condition yj + yk ≤ 2 − xi , leading to : argmax
X
X
X,Y i st j xi yj
 yj + yk ≤ 2 − xi for all j < k , and
ˆA(i , j ) ≥ ˆA(i , k ) for i ∈ [ [ˆn ] ]
X ∈ {0 , 1}ˆn , Y ∈ {0 , 1}m .
Observe that this problem has only ˆn + m binary variables 2 ˆnm(m − 1 ) constraints . On the other hand , the and at most 1 objective function is now non linear . Indeed , even maximising the continuous relaxation of this quadratic target function is unlikely to be possible in polynomial time , as the objective function is non convex . To solve this optimisation problem , we replace it by a sequence of problems for γ = 2 , . . . , m , j yj ≥ γ . We now have the each with the constraint P following : argmax
X,Y
X i st xi
 yj + yk ≤ 2 − xi for all j < k , and ˆA(i , j ) ≥ ˆA(i , k ) for i ∈ [ [ˆn ] ]
X yj ≥ γ j
X ∈ {0 , 1}ˆn , Y ∈ {0 , 1}m
( 3a )
This problem has only ˆn + m binary variables , at most 2 ˆnm(m − 1 ) constraints , and needs to be solved nm
1 + 1 times .
B . Reducing the Number of Constraints
Trapp and Prokopyev [ 2 ] proposed to combine several inequality constraints belonging to the same row . As n * m , however , a better reduction is possible if we combine the constraints belonging to the same columns . Consider a set of rows {i1 , . . . , il} in ˆA for which the same columns j and k with j < k satisfy ˆA(r , j ) ≥ A(r , k ) for all r ∈ {i1 , . . . , il} . Since all variables are binary , we can combine the constraints ( 3a ) for such sets of rows into yj + yk ≤ 2− 1 l ( xi1 − xi2 − . . . xil ) .
This way we reduce all constraints for a pair of columns into one single constraint . To compute the constraints we loop over all possible pairs j , k of columns such that j < k and check whether certain row entries are ascending . This can be done in O(n2m ) . The reduced optimisation problem is : argmax
X,Y xi
X i
 st yj + yk ≤ 2 − 1 X |Ijk| yj = γ j
X ∈ {0 , 1}ˆn , Y ∈ {0 , 1}m
X i∈Ijk xi for all j < k
( 4a )
( 4b )
( 4c ) with Ijk = {i : ˆA(i , j ) ≥ ˆA(i , k)} .
This linear program has only m + n binary variables and at most 1 + 1
2 m(m − 1 ) constraints .
C . Relaxing the Variables
To derive an optimisation problem that is fixed parameter tractable with respect to m , we relax the constraint X ∈ {0 , 1}ˆn to X ∈ [ 0 , 1]ˆn and observe the implication of constraint ( 4a ) and the objective function on the solution vector . We distinguish two cases according to the value of the binary variables yj and yk .
1 and yk = 1 . The constraint ( 4a ) is then P equal to zero and constraint ( 4a ) becomesP P xi ∈ [ 0 , 1 ] . The maximisation overP
First consider the case that yj + yk = 2 , ie , that yj = xi ≤ 0 . Since X ∈ [ 0 , 1]ˆn this implies xi = 0 for all i ∈ Ijk . In the case that yj + yk < 2 , one of the variables , yj or yk , is xi ≤ |Ijk| or xi ≤ 2|Ijk| . This constraint is trivially true and does not restrict xi . This shows that if Y is a binary vector , xi will either be constrained to be 0 or only bound by its domain i xi ensures then that xi i∈Ijk i∈Ijk i∈Ijk automatically takes values in {0 , 1} .
Algorithm 1 : Bounded MILP Require : Matrix ˆAr ∈ Rˆnr×m , number of columns γ ,
Ensure : A pair ( R , C ) , such that ˆAr(R , C ) ) is ordered and a lower bound l as well as an upper bound u on the number of selected rows . and |R||C| is maximal within the given constraints , or two empty sets 1 X , Y ← solution of mixed integer program ( 5 ) ; 2 R ← {i + r : Xi > 0} ∪ {r} ; 3 C ← {j : Yj = 1} ; 4 return R , C mentioned above , for each row i we then only have too consider the ordered submatrices of ˆAi = A([[n ] ] \ [ [i] ] , πi([[m]]) ) . Each loop checks first whether a solution better than the currently best solution may exist and only if this is the case , it starts looking for such a solution . If a better solution is found , it replaces the currently best one . The check is in two steps : The first check skips solving any MILP if previously found solutions imply that there is no solution to the current MILP that can improve over the best solution found so far . The second check solves a bounded MILP whose solution will imply whether the solution to the unbounded MILP will certainly improve over the currently best solution or not .
The bounded and unbounded MILP are instances of the following optimisation problem with parameters γ , u , and l : argmax
X,Y xi
X i
 st yj ≥ γ yj + yk ≤ 2 − 1 X |Ijk| l ≤X xi ≤ u j
X ∈ [ 0 , 1]ˆn , Y ∈ {0 , 1}m i
( 5 )
X i∈Ijk xi for all j < k
For a fixed permutation , we now have a mixed integer linear program ( MILP ) which is fixed parameter tractable with respect to the parameter m .
D . Fixed Parameter Tractability with Ijk = {i : ˆA(i , j ) ≥ ˆA(i , k)} .
Here , γ is the number of columns that should be considered , and u , l are upper , lower bounds on the number of rows , respectively .
To solve MAX OPSM we thus have to loop over possible permutations . As for the brute force algorithm as well as the algorithm of Trapp and Prokopyev , it is not necessary to iterate over all n! permutations . Instead , it is sufficient to iterate only over the permutations πi induced by the rows i of matrix A . As is given in Algorithm 1 ,
The complete subroutine solving the MILP and convertreferred to as ing its output BoundedMILP( ˆA , γ , l , u ) . The relaxed constraints for the row indices together with the upper bound u on their sum , might lead to non integer solutions . In this case , however , we will solve another mixed integer program with no upper bound to find the correct solution . The complete algorithm to solve MAX OPSM is given in Algorithm 2 . we compare our run times against the times reported in their paper . As Trapp and Prokopyev , we also report the run time for varying numbers of columns in the solution for i ← 1 to n do if Ui < db/γe then go to Line 4 ; C , R ← BoundedMILP( ˆAi , γ , 2,db/γe ) ; if |R| ≥ db/γe then
Algorithm 2 : Quick ILP for MAX OPSM Require : Matrix A ∈ Rn×m Ensure : An OPSM pattern ( R∗ , C∗ , π∗ ) , such that |R||C| is maximal 1 b ← m ; 2 U ← ( n − 1 , n − 2 , . . . , 0 ) ∈ Rn ; 3 for γ ← 3 to m do 4 5 6 7 8 9 10 11 12 13 14 15 16 end 17 print π∗ , R∗ , C∗ ; end if |R| > 1 then Ui ← |R| ; if |C||R| ≥ b then b ← |C||R| ; R∗ ← R;C∗ ← C;π∗ ← π ;
C , R ← BoundedMILP( ˆAi , γ,db/γe , Ui ) ; end end
IV . NOISE TOLERANT MAX OPSM WITH INTEGER
PROGRAMMING
We now consider noise tolerant OPSM patterns . Definition 4.1 ( Noise tolerant OPSM pattern ( OPSM) ) :
For a given noise tolerance ∈ R , a triple ( R , C , π ) of induces a noise rows R , columns C , permutation π , tolerant order preserving submatrix in A ∈ Rn×m if for all i ∈ R and all j , j0 ∈ C it holds that π(j ) < π(j0 ) implies A(i , j ) < A(i , j0 ) + .
Positive tolerance parameters give the level of violation that is tolerated between the selected columns in the OPSM pattern and negative tolerance parameters give the level stability that is required between the selected columns in the OPSM pattern . To find MAX OPSM patterns , for each column pair j < k of a row i , the constraints are now constructed as Ijk = {i : ˆA(i , j ) − ˆA(i , k ) ≥ } .
V . EXPERIMENTS
A . Experimental setup
We ran all experiments on a Intel Core 2 Duo CPU at 3GHz and a total of 4 GB RAM which is considered comparable to or less powerful than the Xeon processor used in [ 2 ] . Therefore ,
As mentioned above , to cope with the run time demands of their algorithm ( TR K ) , Trapp and Prokopyev introduced constraints on the search space . Depending on these settings , their algorithm does or does not find the exact optimal solution . In their experiments they ran their algorithm with a sequence of varying constraints , which in many cases would lead to the optimal solution but not always . Hence we report two run times for our algorithm : On the one hand we report the time needed to find a solution of the same quality as reported by Trapp and Prokopyev ( henceforth referred to as QuickILPρ ) ; on the other hand we report the run time needed to find the exact optimum ( henceforth referred to as QuickILP ) .
We ran a detailed comparison on two real world datasets , as in [ 2 ] we consider the Human Gene Expression Index ( HuGE ) of size 1125× 23 and the Breast Cancer Data Set ( BRCA ) of size 3226 × 22 .
Table I
RUN TIME COMPARISON ( IN MINUTES ) OF QUICKILP AGAINST THE TR K IN THE HuGE DATASET . THE NUMBER OF COLUMNS γ IS FIXED TO THE
GIVEN VALUES IN ALL ALGORITHMS . ALGORITHM TR K AND
QUICKILPρ INCLUDE ALSO CONSTRAINTS ON THE NUMBER OF ROWS
SOLVING OPSM .
Cols γ
3 4 5 6 7 8 9
TR K ( min )
335 489 1909 437 524 449 311
QuickILPρ ( min )
<1 <1 1 4 25 34 40
QuickILP ( min )
45 91 180 275 294 421 403
Table II
SIZES OF THE OPSM FOUND BY THE ALGORITHMS IN HuGE DATA . PARAMETER γ CONSTRAINTS THE NUMBER OF COLUMNS IN THE
SOLUTION . ALGORITHM TR K INCLUDES CONSTRAINTS IN ROWS AND THEREFORE IT MIGHT NOT FIND A MAX OPSM PATTERN . ALGORITHM
QUICKILP FINDS ALWAYS A MAX OPSM PATTERN .
Cols γ
3 4 5 6 7 8 9
Rows TR K
569 335 180 95 49 22 11
Rows QuickILP
569 335 180 95 51 26 12
The run times for HuGE are reported in Table I and the different solution sizes in Table II . The run times for BRCA are reported in Table III for the constraint search space and in Table IV for the exact solution .
RUN TIME COMPARISON ( IN MINUTES ) IN BRCA DATA BETWEEN TR K AND QUICKILPρ , WITH CONSTRAINTS ON COLUMNS AND ON ROWS .
Table III
Cols γ
4
6
8
Rows ρ
347 520 798 42 63 127 7 10 14
TR K ( min )
220 586 1974 71 166 2121 17 435 2370
QuickILPρ ( min )
1 1 1 4 6 36 4 10 62
RUN TIME TO FIND THE MAX OPSM FOR THE BRCA DATASET .
Table IV
Cols γ
3 4 5 6 7 8 9 10 11 quickILP ( min )
404 653 1860 2643 3408 2895 2082 2359 713
Rows 1623 799 354 134 53 21 11 5 4
B . Noise tolerant OPSM
To study the effect of our noise tolerance parameter , we used synthetic data . We randomly generate matrices from a normal distribution zero mean and unit variance , selected rows and columns to form a OPSM pattern ( that is , we sorted these entries ) , and finally perturbed them again by additive noise with zero mean and variance σ ∈ {0 , 2−8 , 2−6 , 2−4 , 2−2} . We performed experiments with different tolerance parameters ∈ {0 , 2−8 , 2−6 , 2−4 , 2−2} . Each experiment is repeated 25 times and we measure the average quality of retrieving the implanted MAX OPSM by the Jaccard distance : fififi( ˜C × ˜R ) ∩ ( C∗ × R∗ ) fififi fififi( ˜C × ˜R ) ∪ ( C∗ × R∗ ) fififi .
1 − where C∗ , R∗ denote the columns and rows selected by the algorithm and ˜C , ˜R denote the columns and rows of the planted OPSM . The results are illustrated in Figure 1 .
VI . DISCUSSIONS AND CONCLUSIONS
As motivated by the recent literature , finding exact solutions to biological and medical problems is of crucial importance for the associated applications . In this paper we deal with the problem of finding a largest order preserving submatrix exactly . Building on the solution proposed by Trapp and Prokopyev in [ 2 ] , we propose here a fixed parameter tractable
Figure 1 . Average Jaccard distance obtained after the 25 rounds for each configuration of our synthetic experiment . Each configuration corresponds to a given value of standard deviation σ ( in columns ) and used by the algorithm ( in rows ) . Left plot shows experiments in 40 × 20 sized matrices ; right plot shows experiments in 20 × 10 sized matrices . algorithm with respect to the number of columns of the gene expression data . We discuss the complexity analysis and other properties of our algorithm and show how our formulation can be easily extended to mine largest noise tolerant versions of the same problem . An empirical study on two real world datasets confirmed the expected large improvement .
ACKNOWLEDGMENTS
Part of this work was supported by the German Science Foundation ( DFG ) in the Emmy Noether Program under ‘GA 1615/1 1’ . The authors thank Michael Kamp for helpful comments on the paper .
REFERENCES
[ 1 ] A . Ben Dor , B . Chor , R . Karp , and Z . Yakhini . Discovering local structure in gene expression data : the order preserving submatrix problem . In Proc . of the 6th annual international conference on Computational biology , RECOMB ’02 , pages 49–57 , 2002 .
[ 2 ] A . C . Trapp and O . A . Prokopyev . Solving the order preserving submatrix problem via integer programming . INFORMS J . on Computing , 22:387– 400 , 2010 .
[ 3 ] H . Roh and S . Park . A novel evolutionary algorithm for bi clustering of gene expression data based on the order preserving sub matrix ( opsm ) constraint . In 8th IEEE Int Conference on BioInformatics and BioEngineering , 2008 .
[ 4 ] B . J . Gao , O . L . Griffith , M . Ester , and S . J . M . Jones . Discovering significant opsm subspace clusters in massive gene expression data . In 2006 ACM SIGKDD Int . Conf . on Knowledge Discovery and Data Mining , pages 922–928 , 2006 .
[ 5 ] Q . Fang , W . Ng , and J . Feng . Discovering significant relaxed orderpreserving submatrices . In Proc . of the 16th ACM SIGKDD Int . Conf . on Knowledge discovery and data mining , pages 433–442 , 2010 .
[ 6 ] M . Zhang , W . Wang , and J . Liu . Mining approximate order preserving clusters in the presence of noise , 2008 .
02−82−62−42−2 02−82−62−42−2σ40x2002−82−62−42−2 02−82−62−42−2σ20x10000102030405060708
