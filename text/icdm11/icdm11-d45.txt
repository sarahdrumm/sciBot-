2011 11th IEEE International Conference on Data Mining
Ranking Web based Partial Orders by Significance
Using a Markov Reference Model
Michel Speiser , Gianluca Antonini , Abderrahim Labbi
IBM Zurich Research Laboratory
S¨aumerstrasse 4 , CH 8803 R¨uschlikon , Switzerland
Email : {msp,gan,abl}@zurichibmcom
Abstract—Mining web traffic data has been addressed in literature mostly using sequential pattern mining techniques . Recently , a more powerful pattern called partial order was introduced , with the hope of providing a more compact result set . A further approach towards this goal , valid for both sequential patterns and partial orders , consists in building a statistical significance test for frequent patterns . Our method is based on probabilistic generative models and provides a direct way to rank the extracted patterns . It leaves open the number of patterns of interest , which depends on the application , but provides an alternative criterion to frequency of occurrence : statistical significance . In this paper , we focus on the construction of an algorithm which calculates the probability of partial orders under a first order Markov reference model , and we show how to use those probabilities to assess the statistical significance of a set of mined partial orders .
Keywords partial order ; poset ; probability ; Markov ; statis tical ; significance ; test ; web ; pattern ; ranking
I . INTRODUCTION
In the data mining community , a large amount of work has been done on the problem of sequential pattern mining with potential applications to several domains like telecommunications , bioinformatics , software engineering and the Web ( [1 ] , [ 2] ) . Given a database of sequences and a frequency threshold min sup , the goal is the extraction of subsequences occurring in at least min sup sequences . Efficient algorithms to solve this problem were developed , but the resulting set of patterns can pose problems in terms of its size . In order to “ condense ” the results , statistical methods can be used ; see eg [ 3 ] where a significance score is derived , allowing to rank sequential patterns . The test is based on an independence reference model , meaning that each symbol is assumed to occur independently of previous symbols . Another line of research has been to use more general patterns . In particular , the partial order shows a higher expressive power than the sequential pattern and thus provides more summarized , compact units of information . One could argue that the subgraph pattern is even more general , but for web analytics we hold that the partial order is more useful , because chronological precedence , an important feature in web browsing , is expressed unambiguously . The problem of mining partial orders has been addressed by [ 4 ] , [ 5 ] , [ 6 ] and the algorithm developed in [ 5 ] , Frecpo , is the starting point used in this work . While the result set is substantially smaller than for sequential patterns , mining partial orders in real web data still leaves us with a rather large result set , limiting the actionability of the method . In this work we extend the statistical testing framework to partial orders , using a Markov model as reference , rather than an independence model such as in [ 7 ] . For this , we propose an algorithm , Mapropo ( Markov probability of partial order ) , proving its correctness and establishing its complexity . Since the Markov model contains the independence model as a special case , it has a greater modeling power .
The paper is organized as follows . In the next Section we provide some background on partial orders and the Frecpo algorithm . In Section III we introduce our significance test for partial orders and derive the algorithm informally , discussing the underlying rationale . A formal definition , including a proof of correctness and the related complexity analysis are given in Section IV . In Section V we first give an overview of the web traffic data used in the experiments ; for confidentiality reasons , data description is minimal and anonymized in the pages’ URLs . We then report some results , and we conclude in Section VI with final remarks and indications for future research .
II . BACKGROUND ON PARTIAL ORDERS
A relation is a partial order on a set if it is reflexive , antisymmetric and transitive . If the set ( also called poset ) is finite , the partial order can be visualized as a directed acyclic graph . In Figure 1(a ) we show an example of a partial order R0 . Whenever there is an arc from node n1 to node n2 , it means that n1 <R0 n2 . Note that we do not draw the arcs corresponding to the reflexive elements of R0 , eg ( a , a ) . A partial order R can also be represented by its transitive reduction , which is the ( unique ) graph with the fewest arcs such that for every n1 <R n2 the graph contains a directed path from n1 to n2 . The transitive reduction of R0 is shown in Figure 1(b ) . Conversely , the transitive closure of a graph G is the graph which contains an arc ( n1 , n2 ) whenever a directed path from n1 to n2 exists in G . Naturally , the
1550 4786/11 $26.00 © 2011 IEEE DOI 101109/ICDM2011122
665 graph in Figure 1(a ) is the transitive closure of the graph in Figure 1(b ) . In this work , we define partial orders on symbol alphabets ( representing sets of web pages ) in order to capture precedence hierarchies of those symbols . c b b d a e c a d e
( a ) A partial order R0 .
( b ) Transitive reduction representation of R0 .
Figure 1 : Two representations of the same partial order R0 . Because it is more readable , we generally choose to represent partial orders by their transitive reduction graph .
We now consider symbol sequences on a given alphabet , and suppose that a sequence can contain each symbol at most once . Notice that a sequence can be regarded as a partial order on its constituent symbols . A partial order R is said to be supported by a sequence s when all symbols in R occur in s , in an order compatible with R . More formally , s supports R if R ⊂ C(s ) , where C(s ) is the transitive closure of s . As an example , we report in Figure 2 some partial orders supported by the sequence abc . a b a c
( a ) R1
( b ) R2 a a b b c
( c ) R3 c
( d ) R4 principle is that a pattern can only be as frequent as its subpatterns , because every sequence supporting the pattern also supports all sub patterns . The algorithm recursively builds “ projected databases ” , ie subsets of the sequence database , maintaining a list of patterns , and adding extensions of each to the list , until no frequent extension can be found . One of the assumptions of the Frecpo algorithm is that the sequences in the database are repetition free . This is an important assumption , because Frecpo relies on transitive closures when checking for support . While transitive closures are directed acyclic graphs in the case of repetition free sequences , they are not in the general case , which can result in very counter intuitive support . For instance , the transitive closure of the sequence abcdea is fully connected , and hence the sequence would support any partial order whose alphabet is a subset of {a , b , c , d , e} , for instance e → d → c → b . However , our investigation in Section V tells us that web visit data typically do exhibit repetitions . We hence adapt our data by transforming each sequence into its subsequence of first occurrences ( that is , if a page is visited several times in the same visit , we remove all but the first occurrence ) .
III . SIGNIFICANCE OF PARTIAL ORDERS
While Frecpo produces frequent closed partial orders , it provides no assistance in selecting the most interesting among them ( beyond frequency ) . Our goal is to devise a measure of statistical significance for partial orders , which can act as a proxy for interestingness . In order to do this , we create a reference model and propose a method to calculate the probability of a partial order under that model , ie , how likely it is that a sequence generated by the reference model supports the partial order . The expected support of a partial order can then be compared with the observed support , and a binomial test can help us decide whether or not to retain the pattern , and provide a ranking of a list of patterns . In this Section we derive the method informally , and provide a formal algorithm in Section IV .
Figure 2 : Partial orders supported by the sequence abc .
A . Markov reference model
A partial order R is said to be closed if there is no partial order R . ⊃ R which has the same support as R . Consider for instance the sequence database DB = {abc , abcd , abcde , abcdef } . For this database the partial order R1 has support 4 , but it is not closed because R1 ⊂ R3 and R3 also has support 4 . A closed partial order R is called frequent if its support exceeds some ( arbitrary ) threshold min sup .
A . The Frecpo algorithm
An algorithm called Frecpo for extracting frequent closed partial orders from a sequence database is given in [ 5 ] . Like many other mining algorithms [ 8 ] , [ 2 ] , Frecpo proceeds by growing patterns from scratch ( [9] ) . The underlying
We choose to use a first order Markov model as a reference against which significance is tested . As a justification for this reference , one may argue that a Markov model is easily understandable in the context of a website . A graph where nodes are pages , with transition probabilities written on arcs , is fairly intuitive , and a natural object to be looking at when studying behavior in a web . Modeling web browsing in this way has proven quite fruitful . For instance , the basic version of the famous PageRank algorithm [ 10 ] considers the web as a Markov structure and computes importance rankings for each page based on the eigenvalues of the corresponding transition matrix . Thus , building a Markov model provides a first insight into the structure of the data ; for instance it could be used to produce visualizations of the kind proposed in [ 11 ] .
666
On top of this , we can build a framework to decide if a certain pattern is significant compared to this reference , ie which will detect things which are “ surprising ” from a Markov perspective . Together , these insights are a powerful combination for studying web browsing behavior . To build the Markov model , we can use as states the observed pages augmented with two artificial nodes Enter and Exit , and as transition probabilities the observed transition frequencies . This corresponds to the maximum likelihood approach ( [12] ) . Specifically , we set as transition probability from page i to page j pij
.
= Nij Ni• where Ni• stands for j Nij , and Nij is the number of visits in which the transition i → j occurs . We add the = 1 . In this way we transform exception that pExit Exit Exit into the model ’s only sink ( which is a state that , once reached , is never left ) . The fact that potentially many probabilities are set to zero , when in reality they should be small but positive , is not a problem ; we will only use the model as a baseline to gauge the significance of frequent patterns , all of which obviously have positive probability under the model .
B . Constructing an algorithm
Consider the partial order R shown in Figure 3(a ) . We wish to calculate the probability that the reference model generates a sequence which supports R . We write this as ∩ τc < ∞ ∩ τd < ∞ ) P ( τa < τc
∩ τb < τd
∩ τb < τc where τa stands for the time at which the symbol a first = ∞ occurs in the sequence . We use the convention that τa means that the symbol a does not occur in the sequence . A stepwise approach seems to be required . To start , we want to calculate the probabilities of a random ( Markov ) sequence reaching each of the relevant subsets of the symbols of the partial order first : {a} , {b} or {c , d , Exit} . If the latter is reached first , the partial order fails to be supported by the sequence . On the other hand , if for instance b happens to be the first visited among the symbols of the partial order , the sequence has a chance of supporting it . From that point we would like to know the probabilities of first reaching {a} , {d} and {c , Exit} – note that we no longer care about occurrences of b ; the order only concerns first occurrences . And so on . This gives rise to a scenario graph ( shown in Figure 3(b) ) . This graph captures all the ways in which a sequence can support the partial order . In each node , two sets are shown : the target symbols and the forbidden symbols . The goal of the game then becomes to put probabilities on the arcs and nodes of the scenario graph . For each of the nodes of this graph , a modified Markov model has to be constructed , in order to calculate the probabilities to put on the arcs . How
667
{a , b}{c , d , Exit} a b
{b}{c , d , Exit}
{a , d}{c , Exit} b a d
{c , d}{Exit}
{a}{c , Exit} c d a
{d}{Exit}
{c}{Exit} a c b d d c
∅{Exit}
( a ) Partial order R
( b ) Scenario graph for R
Figure 3 : Given a partial order R , we construct a scenario graph which enumerates all scenarios achieving R . The two sets displayed in each node correspond to target symbols and failure symbols , respectively . this is done is shown in the next Subsection . Conceptually however it should already be clear that , in this way , it is possible to calculate the probability of any partial order occurring under the Markov hypothesis ; simply calculate the probability of reaching the bottom node of the scenario graph .
1 ) Markov model construction : The original Markov model from III A needs to be transformed for each node like so : denote by A the set of of the scenario graph , symbols/web pages , and let F be the set of forbidden states , T the set of target states and A − ( T ∪ F ) all the remaining , “ indifferent ” states . Make sinks out of all states of interest ; for each i ∈ T ∪ F and j ∈ A : set pij = I{i = j} , where I is the indicator function . In this way , we obtain a Markov model which has equivalent dynamics to the original one as long as it navigates through the “ indifferent ” states A−(T ∪F ) , but as soon as it reaches one of the “ interesting ” states T ∪ F , it remains there . The key point is that the probability of reaching x as the first among T ∪ F in the original model is then the same as the probability of being absorbed by x in the transformed model .
2 ) Absorption probabilities : We now wish to calculate the probabilities of absorption of each of the states ( sinks ) of interest . An absorption probability corresponds to the probability of an arc in the scenario graph . In [ 13 ] , it is shown how to compute absorption probabilities of a finite discrete time homogeneous Markov chain . The computation is rather straightforward ; we write the transition matrix P in such a way that all sinks come first , ie
Input : A Markov reference model M and a partial order R Output : Prob . that R is supported by a sequence from M fi
'
P =
I O B Q where I and O correspond to the identity matrix and the zero matrix , respectively , Q is the transition matrix of the sub model of the transient1 states in A − ( T ∪ F ) , and B gives the transition probabilities from each transient state to each sink in T ∪ F . The probabilities of absorption are then given by the matrix A = SB , where S = ( I − Q)−1
In other words , Aij gives the probability of being absorbed by sink j when starting in the transient state i .
IV . THE MAPROPO ALGORITHM
In the previous Section we discussed the rationale behind the Mapropo algorithm while here we present it formally .
A . Variables and methods
We assume that the algorithm has access to certain methods and variables which are described as follows . The function call newnode = scenario–node(T , F ) creates a scenario node newnode using sets T and F as target and forbidden symbols , respectively , provided that no such node exists ( if it exists , the function returns a reference to that existing node ) . When the method node.create–model( ) is called on scenario node node , the modified Markov model is created in node . The probability of being absorbed by sink x is then available as nodepout[x ] Also , the probability of reaching node is calculated as node.p = x nodepin[x ] A list of variables of a scenario node reads as follows :
.
R
R = alphabet(R )
A T0 = source nodes of R F0 = A ∪ {Exit} − T0 start = scenario−node(T0 , F0 ) start.pin[Enter ] = 1 Q = empty queue Q.enqueue(start ) while Q is not empty do current = Q.dequeue( ) current.create−model( ) for t ∈ current.T do newtargets = ∅ for child s of t in the partial order R do if current.T ∩ parents(s ) = t then add s to newtargets end if end for T = newtargets ∪ current.T − {t} F = current.F − newtargets f uture = scenario−node(T , F ) f uture.pin[t ] = current.pout[t ] if f uture.queued = false then
Q.enqueue(f uture ) f uture.queued = true end if end for end while return current.p
Figure 4 : Mapropo
• T : set of target symbols . • F : set of failure symbols . • queued : boolean indicating whether the node has been inserted into the queue Q .
• p : probability that the scenario node is reached . • pin[x ] : probability that the scenario node is reached via symbol x .
• pout[x ] : probability that sink x is reached .
B . A simple example
Suppose we have a visit database DB1 for which we build a Markov reference model M1 , and extract frequent closed partial order R1 using Frecpo – see Figures 5(a ) and 5(b ) , respectively . Since R1 is a sequence ( total order ) , the resulting scenario graph in Figure 5(c ) is extremely simple ; the size of the target set never goes above 1 . Also , because the Markov model contains no cycles , the probability calculations are trivial .
1In a finite Markov model such as we are considering , a transient state is a state which we eventually never return to . In this work , each state is either transient or a sink .
C . Proof of correctness
Theorem 1 . Given a Markov model M with unique source ’Enter’ and unique sink ’Exit’ , and a partial order R , the Mapropo algorithm returns the probability that a random sequence generated by M supports R .
In order to show that the algorithm is correct , we will first establish some useful definitions and intermediary results . The term alphabet is used in relation to various objects , and invariably designates the set of symbols occurring in the object . In what follows , we will consider that R is some fixed partial order . The alphabet of R is denoted A R , and ( x ) designates the set {y : ( y , x ) ∈ T r(R)} where parentsR T r(R ) stands for the transitive reduction of R . Likewise , ( x ) designates the set of symbols of R from ancestorsR which there exists a directed path to x of length ≥ 1 . Finally , a symbol x ∈ A ( x ) = ∅ is called an orphan . We first show that the algorithm enumerates all possible ways in which R can be supported , and then proceed to show that the probability calculations are correct . Let us
R with parentsR
668
Enter
0.5
0.5 a b
0.9
0.7 c
0.1
0.5
0.3
0.3
0.2 d
1 e
1
Exit a c d
( a ) Markov model M1
( b ) R1 n0 : {a}{c , d , Exit} : n0.p = 1 n0.pout
[ a ] = n1.pin
[ a ] = 0.5 n1 : {c}{d , Exit} : n1.p = 0.5 n1.pout
[ c ] = n2.pin
[ c ] = 0.45 n2 : {d}{Exit} : n2.p = 0.45 n2.pout
[ d ] = n3.pin
[ d ] = 0.225 n3 : ∅{Exit} : n3.p = 0.225
( c ) Scenario graph of R1 for M1 .
Figure 5 : A very simple example of a Mapropo execution . Based on the Markov model M1 , Mapropo determines the probability of partial order R1 to be 0225 introduce some graph theoretic concepts which will be useful in visualizing the logic of the algorithm . Definition . A lineage L of R is a subset of A parenthood . That is , ∀x ∈ L : parentsR Definition . Given a lineage L , a symbol x /∈ L with parentsR
( x ) ⊂ L is called an heir of L .
R closed under
( x ) ⊂ L .
Note that orphans are heirs of any lineage they do not belong to ; in particular , any orphan is an heir of the empty lineage . heirs is called an extension of L .
Lemma 1 . The extension of a lineage is again a lineage .
Proof : Suppose lineage L is extended by a subset S of its heirs . By definition of lineage we have ∀x ∈ L : ( x ) ⊂ L . In addition , by definition of heir , we parentsR have that ∀x ∈ S : parentsR ( x ) ⊂ L . Hence ∀x ∈ L ∪ S : parentsR
( x ) ⊂ L ⊂ L ∪ S , that is , L ∪ S is a lineage .
Lemma 2 . If a lineage Lsmall of R is a subset of another lineage Lbig of R , then Lbig can be obtained by successive extensions of Lsmall .
Proof : Define the generation of x ∈ Lbig as follows . If x is an orphan , gen(x ) = 0 . Otherwise , gen(x ) = max y∈parentsR(x ) gen(y ) + 1
Now , notice that we can extend Lsmall by adding the symbols x ∈ Lbig −Lsmall of generation 0 , since they are all orphans . Call this lineage G0 . Next , if all symbols x ∈ Lbig up to generation i are in lineage Gi , the symbols y ∈ Lbig −Lsmall of generation i + 1 are all heirs of Gi and can hence extend the latter to Gi+1 . This can be repeated until the first n such that Gn
= Lbig .
Corollary 1 . Any lineage L of R can be obtained by successive extensions of the empty lineage .
Let us now describe in precise terms what Mapropo is designed to achieve .
Definition . A scenario is a permutation π = ( π1 , π2 , , π|AR| ) of A
R such that .i < j : πj <R πi .
Informally , a scenario is a way in which partial order R can be supported by a sequence .
) with ' ≤ |A
Definition . If the scenario π exists , a prefix π( . ) = ( π1 , π2 , , π . scenario of length ' . Lemma 3 . At each scenario node , V ˙∪T ˙∪F = A R 2 , where V designates the set of visited symbols .
R
| is called a partial
∪{Exit}
Proof : The invariant obviously holds for the scenario node start . Next , suppose it holds for all scenario nodes created so far . Notice that from the current scenario node , a new scenario node is created by visiting an element t ∈ T which is then removed from T . Furthermore , all elements which are removed from F are added to T . Since this is the complete list of modifications to V , T and F , the left handside evaluates to the same set for every scenario node , while the right hand side contains no variables .
Corollary 2 . At each scenario node the set of visited symbols is V = A
∪ {Exit} − ( T ∪ F ) .
R
2A ˙∪ B results in the ordinary union of A and B , but indicates that A
Definition . The union of a lineage L with a subset of its and B are disjoint sets .
669
Lemma 4 . At each scenario node the set of visited symbols V is a lineage , and T is the set of heirs of V .
Proof : The result holds for the scenario node start , where the expression in Corollary 2 evaluates to the empty set , and T consists of all orphans . Next , suppose it holds for all scenario nodes created so far . Notice that we transition to a future scenario node by visiting an heir t ∈ current.T of current.V , so by Lemma 1 the resulting set f uture.V is a lineage . We obtain f uture.T by removing t from current.T and adding those children of t whose parents have all been visited , so f uture.T is the set of heirs of f uture.V . Lemma 5 . For each lineage L ⊂ A scenario node at which the set of visited symbols is L .
R there is at most one
Proof : The scenario nodes are all created by the method scenario–node(T , F ) , and if a scenario node with target set T and failure set F already exists , the method merely returns a reference to that scenario node . In other words , each scenario node is identified by its sets T and F .
Recall that by Lemma 3 the sets V , T and F are disjoint . Suppose there are two scenario nodes with the same V but different ( T , F ) . Since they have the same V they also have the same T ∪ F , so they must differ both on T and F . This is impossible since , by Lemma 4 , T is the set of heirs of V , which is uniquely determined by V .
Lemma 6 . If a partial scenario π( . ) exists , it leads to a scenario node with visited lineage V = alphabet(π( ) )
Proof : It is straightforward to see this for π(0 ) ( scenario node start ) . Suppose it is true for π(k ) with k < ' . Since at each scenario node we visit every heir in T , symbol πk+1 is bound to be added to the visited symbols of a future scenario node . Thus the result also holds for π(k+1 ) . Lemma 7 . For each lineage L ⊂ A scenario node at which the set of visited symbols is L .
R there is at least one
Proof : Since L is a lineage , by Corollary 1 it can be extended from the empty lineage , which is the lineage corresponding to scenario node start . Choose any order in which heirs can be successively added . Clearly , the order defines a partial scenario with alphabet L , hence by Lemma 6 a corresponding scenario node exists .
Theorem 2 . There is a bijection between lineages of R and Mapropo scenario nodes .
Proof : Lemmas 4 , 5 and 7 yield the result .
Corollary 3 . There is exactly one scenario node with visited lineage V = A
R . this point , hopefully the reader
At is convinced that Mapropo ’s scenario nodes are the appropriate stepping stones to enumerate all ways in which a sequence can support the partial order R . An augmented illustration of
670 a c b d a
{a,b}{c,d,Exit} b a c b d a c b d
{b}{c,d,Exit}
{a,d}{c,Exit} b c a b c d {c,d}{Exit} a d a b c d {a}{c,Exit} d a a b a b c d {d}{Exit} c d {c}{Exit} d a b c c d {}{Exit}
Figure 6 : Redux of Mapropo scenario graph 3(b ) . Forbidden symbols are shown in red , lineage symbols in blue ( checkerboard ) , heirs in white . the scenario graph of Figure 3 is given in Figure 6 . It now remains to be shown that the probabilities calculated in Mapropo are correct .
Lemma 8 . Within a scenario node , the construction given in Sections III B1 and III B2 allows the correct mapping from pin to pout .
Proof : Given a subset of states S ⊂ A and an initial z∈S τz
= min distribution pin we aim to calculate the probability P ( τs ) of reaching s before all others in S , for each s ∈ S . The approach is to transform all states s ∈ S into sinks , and to calculate the absorption matrix A , the correctness of which is given in [ 13 ] . If the process
= min
= ∞ for reaches a state s ∈ S , then τs is finite while τz all z ∈ S − {s} , since the process then stays in s forever . For all states which are not in S , the dynamics remain the same as in M , since we do not modify their outgoing transition probabilities . Hence the probability of the event τs The entry Axy gives the probability of being absorbed by sink y when starting in state x . If one knows the probability px of starting in state x , the probability of being absorbed · Axy . Incidentally , px is given by by sink y is evidently px pin[x ] , and the value xpx z∈S τz is the same as in the original model M .
· Axy is stored in pout[y ] .
.
Lemma 9 . The scenario nodes are queued in Q in growing order of |V | .
Proof : Scenario nodes are queued in the order in which they are created . The first created node start has |V | = 0 , and all nodes with |V | = 1 are created from there . Now suppose the result holds up to |V | = k . Since scenario nodes with |V | = k + 1 can only be created ( and enqueued ) from nodes with |V | = k , which are already enqueued , the order is maintained here too .
Lemma 10 . At the scenario node with visited lineage L , pin[x ] is the probability that a random sequence generated by Markov model M completes a partial scenario with alphabet L ending with symbol x . Furthermore , pout[y ] is the probability that a random sequence generated by M completes a partial scenario with alphabet L ∪ {y} ending with symbol y .
Proof : First consider the scenario node start . Since every sequence generated by M starts from the state Enter , it is correct to set pin[Enter ] = 1 . By Lemma 8 , we obtain Markov probabilities for reaching each relevant symbol x ∈ T ∪ F first . These probabilities are stored as pout[x ] . Designate by SNi the set of scenario nodes with visited lineage of size i . By Lemma 9 , the method create–model( ) ∈ SNi before it is called in has been called in each sni any sni+1 ∈ SNi+1 . Suppose now that all scenario nodes in SNi have correct probabilities . Notice that all links to scenario nodes in SNi+1 come from the aforementioned . Because the lineage of visited symbols identifies a scenario node , there can be at most one incoming arc per symbol x . Let sni+1 ∈ SNi+1 have visited lineage L . The probability pin[x ] of arriving at scenario node sni+1 via symbol x is thus precisely the probability pout[x ] of reaching x in the ∈ SNi with lineage L − {x} . Within scenario node sni sni+1 , again , Lemma 8 ensures the correct mapping from pin to pout .
.
Corollary 4 . At the final scenario node , the sum x∈AR pin[x ] evaluates to the probability that R is sup ported by a random sequence generated by M .
Theorem 1 immediately follows , which means that Mapropo fulfills its contract . The algorithm is thus correct .
671
D . Complexity analysis
Let us consider the algorithmic complexity of Mapropo . Designate by A the set of symbols representing pages of our R the subset of A which web , by R a partial order and by A occurs in R . Furthermore , let N be the number of visits and λ the average length of visits .
1 ) Markov models : The construction of the original Markov model requires looking at each page transition , of which there are N ·λ , and adding them to the appropriate cell of an ( |A| + 2 ) × ( |A| + 2) sized “ transition count ” matrix . In the end we normalize the rows in order to obtain the transition probability estimates which constitute the original Markov model . Given this model , a modified model can be constructed in time O(|A|2 ) , and the calculation of the matrix inverse using a straightforward algorithm like Gaussian elimination is O(|A|3 ) . Theoretically the timebound given by [ 14 ] brings the exponent down from 3 to ∼ 2376 Note however that typically |A| << N so small powers should be manageable . If |A| is very big , chances are ( at least for web browsing data ) that the resulting Markov model ’s transition matrix is quite sparse . Efficient methods [ 15 ] can then be used to calculate the inverse .
2 ) Scenario graph : The number of nodes in the scenario graph is a tricky part , since a partial order R can generate a scenario graph of up to 2|AR| nodes ; see Figure 7 for an example . Note that this is also the worst case . A little thought reveals that the number of nodes in the scenario graph is the number of lineages in the partial order ( see IV C for a proof ) . Obviously the number of lineages , in turn , can not exceed the number of subsets of symbols occurring in R , ie 2|AR| . While this may seem like bad news , in practice ( at least in web mining ) interesting patterns are usually of rather small size , and mostly sequential rather than parallel ( ie few combinatorial explosions ) . A lower bound of the number of scenario nodes can be obtained using the apogee of R , which we define as follows . Let L be the collection of lineages of R , and for L ∈ L let HL be the set of heirs of L . Then apogee(R ) = max L∈L
|HL
| .
The apogee is the cardinality of the largest set of heirs of any lineage of R . Since the algorithm considers all lineages , at least once during the execution we will be faced with a set of heirs of size apogee(R ) , which can be visited in any order . Hence the number of scenario nodes can be bounded below by 2apogee(R ) , which is a tight bound since it corresponds to the number of scenario nodes in the example . Another issue is the number of arcs in the scenario graph . While the operations are much less expensive per arc than per node , the sheer number of arcs may be problematic . For instance , in the Rbad example in Figure 7 , the number of arcs can be calculated as follows . Each node in layer k is connected to r − k nodes in the subsequent layer k + 1 . This ( r − k ) = 2r−1 · r arcs , results in a total count of
. ff r k=0 r k r parents a1 a2 a3 a4 ar one child b
( a ) A partial order Rbad
A − {b , Exit} a1 a2 ar ff r 0 nodes
A − {a1}{b , Exit}
A − {a2}{b , Exit}
A − {ar}{b , Exit}
{ar}{b , Exit}
{ar−1}{b , Exit}
{a1}{b , Exit} ff r r nodes ar ar−1 a1
{b}{Exit} b
∅{Exit}
( b ) Scenario graph for Rbad
. ff
Figure 7 : If in partial order Rbad the symbol b has r parents a1 , , ar then the number of nodes in the scenario graph is of the order
= 2r . r k=0 r k
R k ff which turns out to be of a similar order as the number of nodes . Again , this corresponds to the worst case . Since all |A | symbols need to occur at some point in the sequence , a natural way of layering the scenario graph is to define layers with respect to how many of the symbols have already occurred ( incidentally , this is the way in which graphs are depicted in the examples ) . Obviously there are then |A |+1 layers . Now , in layer k the number of nodes can not exceed |AR| since the collection of sets of k symbols which have already occurred can not be larger than the collection of all subsets of size k . Additionally , each node in this layer can have at most |A | − k outgoing arcs , corresponding to the number of symbols which have not occurred up to that point . This leaves us with a maximum number of arcs of | . Following an analogous reasoning , we obtain that for partial order R the minimum number of arcs is 2apogee(R)−1·apogee(R ) , which is again tight by virtue of the example .
| − k ) = 2|AR|−1 · |A
.|AR| ff
( |A
R
|AR| k k=0
R
R
R
3 ) Total complexity : We have derived the complexity of constructing and solving a Markov model , which needs to be done for each node in the scenario graph . We also know the maximal number of nodes in the scenario graph , as well as the maximal number of arcs . Let CN be the complexity per scenario node , and CA the complexity per arc in the scenario graph . Then , calculating the probabilities of a set
672 of partial orders R given a database has complexity
)
(
(
R∈R
(
O
N · λ +
2|AR| · CN
+
2|AR|−1 · |A
| · CA
R
R∈R
R∈R and since CA and CN are O(1 ) and O(|A|3 ) , respectively , we can rewrite this as the simpler expression
)
O
N · λ + |A|3 ·
2|AR|
.
This corresponds to the worst case in every respect , ie if no matrix sparseness can be leveraged to improve the efficiency of probability calculations , and if all partial orders are of the “ worst ” kind ( that is , fully parallel ) . In terms of lower bounds , we can give
)
(
Ω
N · λ + CN
·
2apogee(R )
R∈R where we do not insert a specific expression for CN since as mentioned there is room for optimization to make it potentially much smaller than O(|A|3 ) . Further possibilities for optimization could lie in pruning very low probability scenario nodes , and perhaps storing results in a look up table when evaluating a large number of similar partial orders with potentially large overlap . In conclusion , the approach is practical when the partial orders are small or mostly sequential . The size of the database per se does not matter all that much since the complexity is linear in N , however if the resulting Markov model is very big and/or dense then performance could be an issue . As a benchmark , using a laptop with Intel CPU T2600 clocked at 2.16 GHz and 2 GB of RAM , for a sequence database of length N = 20000 with λ ffi 7 and alphabet size |A| ffi 1500 , the Markov reference model is built in 3 seconds , and for every partial order the time per scenario node is about 2 seconds .
V . EXPERIMENTAL RESULTS
Our working sample consists of about 20000 visits to the website of a popular software and technology company . On average visits are quite short ( mean 7.3 pages viewed , median 5 ) , but there are a few visits which are exceedingly long , ranging into the hundreds of views . It is quite likely that these latter visits were not made by humans ( eg webcrawler traffic ) and should hence be removed if the goal is to study human visiting behavior ; visits above length 20 represent only 4.5 % of visits , however , they represent 24 % of the transitions . Many visits contain repetitions – views corresponding to pages which have already been viewed in the same visit . For instance , the sequence abacab contains 3 repetitions . Only roughly one third of the visits in our database are repetition free . The bulk of visits contains between 1 and 5 repetitions , while a small portion contains between 6 and 10 . If we consider the collection of visits as a directed graph , linking all pages between which at least one transition occurs in the database , we obtain a graph with 1502 nodes and 11392 arcs , with an average degree of 15.1 , which contains 4 weakly connected and 117 strongly connected components . The diameter of the graph is 13 and the average clustering coefficient 0281
We now describe how the Mapropo algorithm can be used in this context . First , the visits database is transformed into the subsequences of first occurrences , and then Frecpo is used to harvest all frequent closed partial orders occurring in at least 1 % of the sequences . This yields a set of 729 partial orders . Next , a Markov reference model for the database is built , and Mapropo is used to calculate the probability of each of the partial orders . Finally , we wish to select those partial orders which are most “ surprising ” with respect to the Markov reference model , because these are the patterns which will tend to be the least visible on any first order visualization of web traffic data . In order to do this , we use the following testing setup . We assume that visits are independent from each other . The support SR of a partial order R can be seen as a binomial random variable Bin(N , pR ) , where N is the size of the database , and pR the ( true ) probability that a visit supports R . The null hypothesis is that the probability pR equals ˆpR given by Mapropo . Since we are interested in patterns which are over represented with respect to the Markov reference , we posit the alternative hypothesis that the probability of support of R is greater than that calculated by Mapropo . More succinctly :
H0 : pR HA
= ˆpR : pR > ˆpR
The p value of the one sided binomial test provides a measure of significance of the partial order – the smaller the p value of a pattern , the more over represented the pattern with respect to the Markov reference . For instance , in Figure 8 we have highlighted all partial orders with a p value below 1 % . Figure 9 shows a few of these . Figure 10(a ) illustrates the fact that the ranking using a Markov model as reference strongly differs from that using an independence model ( such as proposed in eg [ 7] ) . The estimated rank correlation ( Spearman ’s ρ ) is 0441 The two rankings’ top n lists overlap only weakly even for large n , as can be seen in Figure 10(b ) . This indicates that it is highly beneficial to use a Markov reference model if one can be obtained ; firstly , as mentioned , it provides in itself a good first order insight into the data , and secondly , as shown , it strongly differs on higher order insights from an independence reference , but is more accurate by virtue of generalization .
673
Figure 8 : The partial orders in the lower part are strongly over represented with respect to the reference .
Home
Careers
Optimization
Graduates
Strategy
Finance
( a ) Ra : observed support 494 , expected support 419 .
( b ) Rb : observed support 165 , expected support 121 .
Optimization
Home
Work
People
Business Issues
Ideas
Midmarket
( c ) Rc : observed support 432 , expected support 209 .
( d ) Rd : observed support 130 , expected support 104 .
Figure 9 : Significant frequent closed partial orders . Pattern Rc indicates that 432 visits included the pages Optimization , People , Work and Ideas , in which Optimization was visited before both People and Work , and Ideas was visited last among those pages .
Ranking scatterplot l e d o m v o k r a M y b k n a R
0 0 6
0 0 4
0 0 2
0
0
200 Rank by independence model
400
600
( a ) Rankings of partial orders using a Markov reference model strongly differ from those using an independence model .
Overlap of rankings s n r e t t a p d e k n a r p o t n f o p a l r e v O
0 1
.
8 0
.
6 0
.
4 0
.
2 0
.
0 0
. for Frecpo , dealing with directed graphs with cycles and better understanding their interpretation in the context of web browsing behavior .
REFERENCES
[ 1 ] W . Wang and J . Yang , Mining Sequential Patterns from Large Data Sets , ser . The Kluwer International Series on Advances in Database Systems , Vol . 28 . Secaucus , NJ , USA : SpringerVerlag New York Inc . , 2005 .
[ 2 ] J . Pei , J . Han , B . Mortazavi Asl , J . Wang , H . Pinto , Q . Chen , U . Dayal , and M C Hsu , “ Mining sequential patterns by pattern growth : The prefixspan approach , ” IEEE Transactions on Knowledge and Data Engineering , vol . 16 , no . 11 , p . 2004 , 2004 .
[ 3 ] R . Gwadera and F . Crestani , “ Ranking sequential patterns with respect to significance , ” in PAKDD ( 1 ) , 2010 , pp . 286– 299 .
[ 4 ] H . Mannila and C . Meek , “ Global partial orders from sequential data , ” in KDD ’00 : Proceedings of the sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining . ACM , 2000 , pp . 161–168 .
[ 5 ] J . Pei , H . Wang , J . Liu , K . Wang , J . Wang , and P . S . Yu , “ Discovering frequent closed partial orders from strings , ” IEEE Transactions on Knowledge and Data Engineering , vol . 18 , pp . 1467–1481 , November 2006 .
[ 6 ] G . Casas Garriga , “ Summarizing sequential data with closed partial orders , ” in Proceedings of the Fifth SIAM International Conference on Data Mining , 2005 , pp . 380–390 .
[ 7 ] R . Gwadera , G . Antonini , and A . Labbi , “ Mining actionable partial orders in collections of sequences , ” in ECML PKDD 2011 , 2011 .
[ 8 ] R . Agrawal and R . Srikant , “ Fast algorithms for mining association rules , ” in VLDB , 1994 , pp . 487–499 .
1
2
5
10
20
50
100
200
500 n
[ 9 ] D . J . Cook and L . B . Holder , Mining Graph Data .
John
Wiley & Sons Inc . , 2007 .
( b ) The top n patterns of both rankings overlap quite weakly even for large n .
Figure 10 : Rankings : Markov vs . independence reference .
VI . CONCLUSION
We presented an algorithm to compute the probability of a partial order occurring under a Markov model , and used it to construct a statistical significance test for frequent partial orders . The objective was to provide a tool that could help in ranking such patterns , to make the result set easier to manage and more actionable for decisions . We also illustrated that using a Markov reference model is very beneficial compared to an independence model , using real web browsing data . We are currently working on visualizations based on combinations of top ranked partial orders , with the goal of providing a compact representation of traffic for a given visit graph of web pages . We are also working more in detail on how to relax the repetition free assumption , necessary
[ 10 ] L . Page , S . Brin , R . Motwani , and T . Winograd , “ The pagerank citation ranking : Bringing order to the web . ” Stanford InfoLab , Technical Report 1999 66 , 1999 .
[ 11 ] J . Chen , T . Zheng , W . Thorne , O . R . Zaiane , and R . Goebel , “ Visual data mining of web navigational data , ” in 11th International Conference Information Visualization ( IV’07 ) , Zurich , Switzerland , 2007 , pp . 649–656 .
[ 12 ] P . Guttorp , Stochastic Modelling of Scientific Data . Chapman and Hall/CRC , 1995 .
[ 13 ] P . Bremaud , Markov Chains : Gibbs Fields , Monte Carlo Simulation , and Queues , corrected ed . Springer Verlag New York Inc . , Feb . 2001 .
[ 14 ] D . Coppersmith and S . Winograd , “ Matrix multiplication via arithmetic progressions , ” Journal of Symbolic Computation , vol . 9 , no . 3 , pp . 251 – 280 , 1990 .
[ 15 ] S . Li , “ Fast algorithms for sparse matrix inverse computa tions , ” PhD dissertation , Stanford University , 2009 .
674
