2011 11th IEEE International Conference on Data Mining
A Robust Clustering Algorithm Based on Aggregated Heat Kernel Mapping
Hao Huang∗ , Shinjae Yoo† Hong Qin∗ , and Dantong Yu† ∗Department of Computer Science , Stony Brook University Email : haohuang@csstonybrookedu , qin@csstonybrookedu †Computational Science Center , Brookhaven National Laboratory
Email : sjyoo@bnl.gov , dtyu@bnl.gov
Abstract—Current spectral clustering algorithms suffer from both sensitivity to scaling parameter selection in similarity matrix construction , and data perturbation . This paper aims to improve robustness in clustering algorithms and combat these two limitations based on heat kernel theory . Heat kernel can statistically depict traces of random walk , so it has an intrinsic connection with diffusion distance , with which we can ensure robustness during any clustering process . By integrating heat distributed along time scale , we propose a novel method called Aggregated Heat Kernel ( AHK ) to measure the distance between each point pair in their eigenspace . Using AHK and Laplace Beltrami Normalization ( LBN ) we are able to apply an advanced noise resisting robust spectral mapping to original dataset . Moreover it offers stability on scaling parameter tuning . Experimental results show that , compared to other popular spectral clustering methods , our algorithm can achieve robust clustering results on both synthetic and UCI real datasets .
Keywords Spectral analysis ; Diffusion processes ; Green ’s function methods
I . INTRODUCTION
Clustering analysis is one of the most important unsupervised knowledge exploration tools in knowledge discovery and data mining . It is especially of value when we have no or limited prior knowledge about the data being acquired or the clustered results are needed to be fed into succeeding phases of the data analysis pipeline .
However , clustering analysis is of little use if the clustered results are radically different when the scaling parameters of clustering algorithms are slightly modified or even with very little data perturbation ( noise or outliers ) . We call such susceptibility the sensitivity of clustering algorithms , and one of the most desirable properties of clustering algorithms is robustness . In particular , the robustness of clustering algorithms should be measured in the following aspects : ( 1 ) not sensitive to any small change of parameters ; ( 2 ) not sensitive to data perturbation ; ( 3 ) non degraded performance even with significant noise level or less correct parameter settings ; and ( 4 ) competitive and comparable results when comparing with those less robust clustering algorithms without any data perturbation and with correct parameter settings . Robust clustering algorithms are highly desirable to combat both scaling parameter tuning sensitivity and noise sensitivity . With these robustness properties , we can reliably analyze data and conduct other data driven tasks in succeeding analysis steps . The robustness property is equally significant for domain experts who do not have strong machine learning background as they become much more comfortable in utilizing robust clustering algorithms . It is imperative to develop robust clustering algorithms [ 5 ] , and this paper serves this pressing need .
Towards robustness , researchers have explored various techniques , including robust statistics [ 14 ] , noise in sensitive regression [ 3 ] , and noise robust clustering [ 17 ] . However , robust clustering approaches considering both parameter tuning sensitivity and noise sensitivity are rather rare . In fact , as shown in Figure 1 , scaling parameter tuning of spectral clustering may affect the quality of clusters significantly , moreover , in Figure 1(c ) we can see that both tuning sensitivity and data perturbation are correlated to each other . This paper proposes a unified probabilistic method based on diffusion theory , in this way we try to avoid the influence of both scaling parameter tuning and data perturbation . Since we concentrate on global distribution when we conduct clustering , the embedded structure must be invariant to local perturbation ( noise or outliers ) , and they should be determined only by visible neighborhood while avoiding negative effects from changing scaling parameters . Heat kernel , as the fundamental solution of heat diffusion on manifolds , offers a statistical description on random walk , so it can be employed to build a diffusion map based on global information . In this paper , we unite spectral clustering and heat diffusion theory together and show that it facilitates robustness to both scaling parameter tuning and data perturbation .
A . Motivation
For similarity measure , we typically employ Gaussian kernel as it is one of the most widely used metric . As shown in Figure 1 , it is a well known problem that the scaling parameter , σ , of Gaussian kernel for the affinity matrix has significant impact on discovering embedded structure because σ determines whether two points are considered similar ( neighbor ) or not [ 25 ] . Although several methods have been proposed to address this problem ( eg , [ 30 ] , [ 17] ) , it remains challenging to find a certain range which is
1550 4786/11 $26.00 © 2011 IEEE DOI 101109/ICDM201115
270
( a ) σ = 0.22
( b ) σ = 0.23
( c ) σ = 0.22 with noises
Figure 1 . The sensitivity example of Spectral Clustering Algorithm ( NJW ) with respect to scaling parameter σ and noises . The small perturbation of scaling parameter or data points gives rise to radically different results in spectral clustering . large enough to maintain optimal , yet data dependent performance . The second challenge in using spectral clustering is the clustering quality with respect to data noise . As noted in [ 18 ] , spectral clustering is less sensitive to data perturbation than popular K means algorithms . Yet , depending on the application domain or inappropriate preprocessing of data , spectral clustering can still be susceptible to data noise [ 31 ] , which tends to make clustering parameter selection even more difficult , especially when making use of scaling parameter σ of Gaussian kernel . Since parameter selection can be significantly affected by the noise level of data , we must address robust spectral clustering in terms of parameter selection and noise simultaneously .
To overcome such difficulties of spectral clustering , we consider heat equation in diffusion theory , which has the built in robustness of data perturbation and an intrinsic relationship with spectral clustering . Diffusion distance is based on Markov matrix which is a stochastic matrix representing a random walk on graph [ 19 ] , it can consider up to t steps out of all the possible paths bridging any two points , which makes it much more robust than geodesic distance [ 6 ] . Diffusion distance has a potential to be more robust to data perturbation via a family of diffusion maps [ 6 ] . In this paper , we focus on heat kernel [ 13 ] which offers a natural mechanism to express diffusion distance through heat dissipation process . Heat kernel makes use of not only eigenvectors but also eigenvalues , which give us insight regarding the relative importance of eigenvectors . Inspired by the concept of heat kernel diffusion distance , a more stable clustering algorithm could be designed in terms of data perturbation because it considers multiple paths like diffusion maps . Typically , any diffusion method often starts with some local observation ( eg , Euclidean distance ) which is then refined into a global metric ( eg , geodesic or heat kernel distance ) through propagation . Nonetheless , existing methods still need to make non intuitive decisions at various stages for selecting neighbors , global similarity , and embedded reconstruction . As a result , burdensome parameter selection is unavoidable in the current state of the art .
B . Contribution
This paper articulates a novel unsupervised robust spectral clustering method to combat the problem of scaling parameter tuning and data perturbation . It is built on top of spectral clustering and heat kernel theory for robust diffusion with the following contributions : ( 1 ) We derive a robust heat kernel by integrating all time scales of heat kernel into one single term , namely Aggregated Heat Kernel ( AHK ) ( Section III ) . As a result , we removed the time scaling parameter of heat kernel and design a complete robust clustering algorithm . We discuss the connection of this kernel with other popular robust clustering approaches .
( 2 ) We investigate the best matching normalization approaches for our proposed AHK , which is critical in parameter sensitivity and noise robustness . LaplaceBeltrami Normalization ( LBN ) [ 6 ] is another key ingredient in our clustering framework , which has a very close relationship with diffusion theory and spectral clustering as well . We integrate LBN into our clustering framework rather than the standard graph Laplacian normalization , so that we can recover Riemannian manifold structure regardless the density distribution of dataset . ( 3 ) Our novel clustering algorithm ( Section IV ) , combining Aggregated Heat Kernel with the best matching normalization approaches , delivers robust clustering results in terms of both parameter selection and noise level .
( 4 ) We systematically evaluate the proposed algorithm with several closely related baseline clustering algorithms on a number of synthetic and benchmark datasets ( Section V ) . We focus on the sensitivity of parameter selections ( eg , both global and local scaling parameters of Gaussian kernel ) and the sensitivity of noise level . Our experimental results confirm that the proposed algorithm produces not only competitive results of carefully tuned baselines on non noisy datasets but also outperforms existing results with noisy or off the sweet spot parameters .
II . BACKGROUND AND DIFFUSION THEORY
Since our new method is founded upon both spectral clustering and heat diffusion , we shall briefly review the
271 basic idea of spectral clustering , diffusion maps , and heat equation , and address the weakness of existing approaches .
A . Spectral Clustering
Algorithm 1 : SpectralClustering(X,k ) Input : Input data X ∈ Rn×m , and k is the number of clusters
Output : Cluster assignments of n instances 1 Compute the affinity matrix W ∈ Rn×n where W ( i , j ) = exp(||x(i ) − x(j)||/2σ2 ) ; .n 2 Compute the diagonal matrix D ∈ Rn×n where j=1 W ( i , j ) and D(i , j ) = 0 if i fi= j ; D(i , i ) = 3 Compute the graph Laplacian L where Lnn = D − W , Lrw = I − D −1W or Lsym = I − D −1/2 ; 4 Compute the first k eigenvectors ψ of L , ψ = {ψ(1 ) , ψ(2 ) , . . . , ψ(k)} ; 5 Re normalize the rows of ψ ∈ Rn×k into q ψ(i , q)2)1/2 . ; Yij = ψ(i , j)/( 6 Run k means with Y ∈ Rn×k ;
−1/2W D
.
Among several kinds of clustering algorithms , we focus on spectral clustering , which has gained popularity in the last decade in data mining community because of its ability to discover embedded data structure . Spectral clustering ( Algorithm 1 ) has been known as one of the most popular clustering algorithms nowadays . It has strong connection with graph cutting , in the way that spectral clustering uses eigenspace to solve relaxed forms of the balanced graph partitioning problem [ 22 ] . Another aspect of spectral clustering is that , it can capture the manifold structure of data as shown in Figure 1 , which is difficult or impossible to achieve for other popular k means or similar algorithms .
However , there are two challenges in spectral clustering . First , the selection of scaling parameter σ of affinity matrix computation could affect the clustering results radically ( Figure 1 ) because this parameter determines the neighborhood . Second , it is still sensitive to noise . For instance in Figure 1(c ) , with only a few noisy instances , the clustering result is quite different and the optimal range of scaling parameter σ is also changed .
B . Diffusion Maps
In 2006 , Coifman et al . [ 6 ] designed a framework based on diffusion process to consider both eigenvalues and eigenvectors . The non negativity property of affinity matrix W allows us to normalize it into a Markov transition matrix −1W where the states of the corresponding Markov P = D process are data points , which enables us to analyze it as random walk . It is straightforward to calculate the transition probability , pt(i , j ) ( the probability of transition from i to j after t steps or time ) using entries from P . The diffusion distance between two points at time scale t is
2 t ( i , j ) =
D fi
[ k
( pt(i , k ) − pt(j , k))2
φ1(k )
] ,
( 1 ) where φ1(z ) is the stationary distribution of the random walk ( trivial left eigenvector ) . So the diffusion maps at time scale t project the data point to m dimensional eigenspace as
Ψt : x → [ λ t t 1ψ1(x ) , λ 2ψ2(x ) , , λ t mψm(x) ] ,
( 2 ) where λi are eigenvalues and ψi are the corresponding right eigenvectors of P [ 20 ] . In this way the diffusion distance between two points becomes mfi
2 t ( x , y ) =
D
2t i ( ψi(x ) − ψi(y))2 ] .
[ λ
( 3 ) i=1
By projecting the data to diffusion space , the effect of scaling parameter in Gaussian similarity is reduced . However , the scaling parameter t in diffusion space is still very essential in terms of the transitive connectivity : small scaling t makes the loosely connected graph into slightly stronger connection within t connections , while large scaling t makes the graph tend to be more strongly connected . In 2009 , Richards et al . [ 27 ] proposed multiscale diffusion distance , which considers all possible paths between each point pair in diffusion space across all time scales t , so that multiscale diffusion distance is more robust to the structure at different time scales . To do this , λt i of Equation ( 2 ) is replaced by i = λi/(1 − λi ) . t
( 4 )
∞fi
λ t=1
So they eliminated the effect of different time scales .
C . Heat Equation
Our proposed work is strongly inspired by heat kernel theory [ 13 ] and its attractive properties . For instance , it is symmetric , positive semi definite , multiscale , and stable . Moreover , it can be interpreted as the transition density function of Brownian motion [ 29 ] , which is the most fundamental continuous time Markov process .
Specifically , the heat equation is associated with normal ized graph Laplacian , Lrw , which can be defined by
= −LrwHt ,
∂Ht ∂t
( 5 ) −tLrw is the heat kernel on Riemannian where Ht = e manifold M and t is the time scaling parameter [ 10 ] . For fiΛψ , the heat kernel can be re written as follows : Lrw = ψ nfi
Ht(x , y ) =
−λit
[ e
ψi(x)ψi(y) ] ,
( 6 ) i=1 where Ht(x , y ) represents the amount of heat being transferred from x to y in time t given a unit heat source at x .
272
( a ) σ = 0.15
( b ) σ = 0.16
( c ) σ = 0.28
Figure 2 . examples .
The sensitivity of time scaling parameter t on Iris dataset is shown in NMI ( Section V ) . We use random walk normalization for all three
III . AGGREGATED HEAT KERNEL
A . Aggregated Heat Kernel
As discussed earlier , heat kernel is multiscale . For small t , the function Ht(x,∗ ) is mainly determined by the nearby neighborhood of x , and this area grows bigger as t increases . In other words , for small t , Ht(x,∗ ) only reflects local properties of the area around x but large t captures the properties from larger area or even global structure . But this additional one more degree of freedom makes it difficult to determine t in any algorithm ( Figure 2 ) because we have little clue about how to find the best t , which is similar to the scaling parameter σ of Gaussian similarity . In other words , the clustering result could become sensitive due to this time scaling selection .
We propose a new approach by integrating the entire time scale from zero to infinity on heat kernel , which is called Aggregated Heat Kernel ( AHK ) . nfi
' ∞
Ht(x , y)dt =
[ (1/λi)ψi(x)ψi(y) ] . ( 7 )
H(x , y ) = t=0 i=1
AHK inherits many powerful properties from heat kernel . Among them , the most relevant ones to our current work include ff • Symmetric : H(x , y ) = H(y , x ) . • Semigroup identity : H(x , y ) = • Positive semi definite :
. c1 , c2 , , cn are real numbers .
H(x , z)H(y , z)dz . i,j H(x , y)cicj ≥ 0 , where
M
From Figure 2 we observe that in conventional heat kernel the time scaling parameter t is also correlated with the scaling parameter σ and it needs to be carefully tuned . But AHK is better than traditional HK on most of the time parameters . AHK is originally defined by the anisotropic transition kernel such as Lrw but we could generalize AHK to Hsym of symmetric Lsym or Hnn of unnormalized Lnn .
B . Connections to AHK
In this subsection we built theoretical connections from
AHK to the other existing popular techniques .
273
Inverse Laplacian : AHK , H , is pseudo inverse or Moor Penrose inverse [ 11 ] . By doing so , we achieve multiscale heat diffusion . Instead of doing pseudo inverse , we could directly inverse graph Laplacian matrix [ 17 ] .
( I + αLsym)−1
,
( 8 ) where α is the positive regularization parameter and I allows us to invert Laplacian matrix always . Note that , [ 17 ] used this direct inversion to get noise robust clustering results .
Commute Distance : Commute distance C(x , y ) between x and y is the expected random walk round trip travel time . AHK is also known as Green ’s function [ 26 ] , which is closely related to commute distance ( CD ) or resistance distance . The Green ’s function is left inverse operator of Laplace operator , Hrw·Lrw = I . For Hnn constructed from unnormalized Lnn , commute distance can be defined as C(x , y ) = vol(Hnn(x , x ) + Hnn(y , y ) − 2Hnn(x , y) ) , ( 9 ) .n where vol = i=1 D(i , i ) . Just like AHK , commute distance also considers all possible length , paths and their weights , which is more robust than the shortest path . Note that , commute distance can also be expressed by the random walk or symmetric graph Laplacian normalization [ 26 ] .
Multiscale Diffusion Map : Commute distance is also related to diffusion distance . By replacing Equation ( 7 ) into the above equation , we get
C(x , y ) = vol
[ (1/λi)(ψi(x ) − ψi(y))2 ] ,
( 10 ) nfi i=2 mfi
∞fi and also multiscale diffusion distance can be defined by :
2 t ( x , y ) =
D
[ 1/(1 − λ
2 i )(ψi(x ) − ψi(y))2 ] .
( 11 ) t=0 i=1
.∞ t=0 λt
Both commute distance and diffusion distance look similar but they have different eigenvalue weighting and different Laplacian normalization .
Multiscale diffusion distance [ 27 ] can also be represented i = 1/λi , which shares the same weighting with by H but it is for distance weighting . If the time summation starts from t = 1 , then it is exactly the same as the multiscale diffusion map ( MDM ) of Equation ( 4 ) . Both of eigenvalue weighting ( starting t = 0 and t = 1 ) will show quite similar weighting distribution anyway for 0.5 ≤ λ ≤ 2 , which is common for most of normalized graph Laplacian .
C . Normalization
Even though we made proper connections among similar approaches , most of them used different normalization without thorough evaluation . Therefore it is not clear what is the best way to normalize graph Laplacian matrix for our proposed H . It is shown in [ 16 ] that if we assume uniform sampling of data points from a sub manifold M , the eigenvectors of Lrw with σ → 0 and n → ∞ , tend to approximate Laplace Beltrami operator on M , which guarantees manifold structure reconstruction . However , in reality , the sampled data points tend to be nonuniform and show skewed density distributions , resulting in poor manifold structure reconstruction in AHK . To improve the distributional sensitivity of Random Walk ( RW ) normalization , we consider the following two additional normalizations :
Laplacian appears to have the ability of separation but the distance among documents are very close to each other compared to other normalizations . Symmetric normalization also shows very good separation and ball shape reconstruction but symmetric normalization is not anisotropic transition . For our future experiments , we mainly focus on LBN but we provide further detailed analysis across different datasets regarding different normalization effects and approaches in Section V .
IV . NEW ALGORITHM
After investigating some nice properties of heat kernel , it now sets a stage for us to introduce a novel robust spectral clustering algorithm using both AHK and LBN ( Algorithm 2 ) , which is less sensitive to the scaling parameter selection and noise perturbation . Let X be a matrix of size n × m , where n is the number of data points and m is the number of dimensions , our algorithm is detailed in Algorithm 2 .
−α ( α ) = D W W D ( α)−1 ( α ) = I − D
−α
,
( 12 )
( α )
,
L
W
( 13 ) where α is a normalization parameter and D(α ) is a diagonal matrix with the sum of row weight of W ( α ) .
• If α = 0 , L(0 ) = Lrw ( Random Walk normalization ) . • If α = 1/2 , then it is Fokker Planck ( FP ) diffusion . • If α = 1 , it is Laplace Beltrami Normalization ( LBN ) . The relations among those three normalizations are well described in [ 6 ] . Depending on α , LBN can also be reduced to Random Walk or FP diffusion . In particular , we focus on LBN because it removes the influence of the dataset density and recovers manifold structures on M with the condition of both σ → 0 and n → ∞ [ 6 ] . In other words , the additional re normalization of affinity matrix W enables us to reconstruct manifold structures better under non uniform density distribution , so that our clustering results can be less sensitive to noise and scaling parameter sensitivity .
D . Comparison
Figure 3 shows the effects of different approaches and normalizations on 20 newsgroup text data ( 20ngC ) ( Section V ) . True inversion and commute distance show the worst results in separating three topics . Although they share the same Laplacian matrix inversion approaches , the results are quite different . Interestingly multiscale diffusion map shows the best separation among non AHK approaches . In case of AHK , most of normalization approaches except unnormalized Laplacian reconstruct ball shape of topic distribution . The original Random Walk ( RW ) normalization shows the most mixture of three topics but as we add the additional normalization of Equation ( 12 ) , we reconstruct better manifold structures . LBN shows the best coherent and condensed reconstruction quality . AHK with unnormalized
274
Algorithm 2 : AHKClustering(X,k,γ , xxx ) Input : Input data X ∈ Rn×m , k is the number of clusters , γ is an eigenvalue smoothing parameter , and xxx is a normalization method
Output : Cluster assignments of n instances
.n i=2[
1 Construct Laplacian Lxxx ; 2 Compute generalized eigenvectors ψ(i ) and corresponding eigenvalues λi , i = 1 , 2 , , n . ; 3 Construct Hxxx matrix with ψ(i ) and λi , where Hxxx(x , y ) = 4 Compute the first k eigenvectors ψs of Hxxx , ψs = {ψs(1 ) , ψs(2 ) , . . . , ψs(k)} ; 5 Re normalize the rows of ψs ∈ Rn×k into Yij = ψs(i , j)/( 6 Run k means with Y ∈ Rn×k ;
γ+λi ψ(i , x)ψ(i , y) ] ; q ψs(i , q)2)1/2 . ;
.
1
We suggest to use LBN as normalization choice of our proposed algorithm . This algorithm undergoes a kind of data warping by using LBN ( Step 1 ) and AHK ( Step 2 and 3 ) . Then we perform the second eigenvalue decomposition ( Step 4 ) and then normalize its row ( Step 5 ) . k means algorithm is used for final clustering . We assume that the entire graph is well connected , so that the eigenvectors except the first one are included in Step 3 . If unconnected , a threshold can be set to filter out the smaller eigenvalues and the corresponding eigenvectors . The eigenvector smoothing parameter γ of Step 3 is added to stabilize the affinity matrix computation . Regarding computational complexity , eigenvalue decomposition is the most time consuming step , which will dominate our computation . There are many iterative methods to conduct eigenvalue decomposition ( eg , power iteration [ 2] ) , but in general finding the eigenvalues reduces to matrix multiplication by computing a symbolic determinant , which gives a running time of O(n3 + n2log2n ) [ 24 ] .
0.2
0.1
0
−0.1
−0.2 0.2
1
0.5
0
−0.5
−1 −1
0.04
0.02
0
−0.02
−0.04
0.1
0
−0.1
0
−0.1
−0.2
−0.2
0.2
0.1
0.04
0.02
0 −0.02
−0.04
−0.06
−0.1
0
−0.05
1
0.5
0
−0.5
0.05
−1 1
0.5
0
−0.5
−1
−1.5
−2
−2.5 −1 x 10−3
1
0.5
0
−0.5 x 10−3
−1.5
−2
−1.5
−1 x 10−3
−2
( a ) True Inversion + SY M
( b ) Multiscale Diffusion Map + RW
( c ) Commute Distance ( Hnn )
−1
−1
( d ) AHK + No normalization(Hnn )
−2.5
−2.5
1
0.5
0
−0.5
−1
−1 1
−0.5
0.5
0
0.5
0
−0.5
1
0.5
0
−0.5
1
−1 1
0.5
0.5
0
−0.5
0
−0.5
1
0.5
0
−0.5
1
−1 1
0.5
0.5
0
−0.5
0
−0.5
−0.5
0
0.5
1
1
( e ) AHK + SY M ( Hsym )
−1
−1
( f ) AHK + RW ( Hrw )
−1
−1
( g ) AHK + F P ( Hf p )
−1
−1
( h ) AHK + LBN ( Hlbn )
1
0.5
0
−0.5
Figure 3 . Effects on embedded construction on 20ngC dataset is shown .
STATISTICS OF OUR EVALUATION DATASETS
Table I
Data Set Iris Glass PenDigits01 PenDigits17 PolBooks UBMCBlog AGBlog 20ngA 20ngB 20ngC 20ngD FaceContour
# instances 150 214 200 200 105 404 1222 200 400 600 400 266
# attributes 4 9 16 16 105 404 1222 61188 61188 61188 61188 2
# clusters 3 6 2 2 3 2 2 2 2 3 4 3
1 2 3 4 5 6 7 8 9 10 11 12
V . EXPERIMENTAL RESULTS
A . Experimental Setup
Dataset . To demonstrate the robustness of our proposed method , we evaluate our algorithm on one synthetic FaceContour dataset and seven UCI benchmark datasets including four text datasets , and three network datasets , summarized in Table I . Such diverse combination of data is intended for our comprehensive study . Iris dataset is a collection of three species of irises where one is linearly separable but the other two are not [ 9 ] . Glass includes types of glasses for criminological investigation [ 8 ] . We also test two handwritten pendigit data PenDigits01 and PenDigits17 with digit “ 0 ” vs . “ 1 ” for easy task and “ 1 ” vs . “ 7 ” for challenging task . PolBooks is a network data for co purchasing pattern of 105 political books of three classes [ 21 ] . UBMCBlog and AGBlog are political blog connection network data [ 15][1 ] . 20ng is 20 newsgroup text data [ 23 ] . 20ngA includes 100 messages from misc.forsale and 100 messages from socreligionchristian 20ngB and 20ngD add 100 messages to each category and 20ngC adds 200 from talkpoliticsguns to 20ngB . To show noise robustness , we also add noise from 10 % to 100 % by 10 % increment on both FaceContour and 20ngD . Noise in FaceContour is uniformly distributed . Noise in 20ngD comes from two other different news group talkpoliticsguns and recsportbaseball
Similarity Measure . We apply mainly Gaussian similarity as our similarity measure . For network data ( PolBooks , UBMCBlog , and AGBlog ) , the affinity matrix is a binary link matrix where A(i , j ) = 1 if there is an edge from i to j , and A(i , j ) = 0 otherwise . By the nature of text data , cosine similarity metric is the only metric we apply for text and we use word counts as features except stop words and singleton words .
Baselines . We compare our results to five competitive clustering algorithms . For our basis of spectral clustering , we choose symmetric graph Laplacian spectral clustering ( NJW ) [ 22 ] . To show parameter tuning sensitivity , we include Self Tuning ( ST ) spectral clustering [ 32 ] . As our diffusion map baseline , Multiscale Diffusion Maps ( MDM ) [ 27 ] and Commute Distance ( CD ) [ 26 ] are considered . Finally , to compare noise robustness , we add Noise Robust Spectral Clustering ( NR ) [ 17 ] .
Evaluation . Since we have the ground truth of labels for each data , we compare our clustered results with the labels . We use several popular evaluations in our experiment ( eg , purity , normalized mutual information ( NMI) ) . Due to space limitation , NMI is used as our only evaluation metric among all being described because most of clustering algorithm papers make use of NMI as their primary evaluation metric . Detailed definition of NMI can be referred to [ 28 ] .
Parameters . Other than scaling parameter σ of Gaussian similarity , our proposed algorithm has one eigenvalue smoothing parameter γ . In our experiments , if we have big enough σ ≥ 0.2 , we do not need to set γ but if we set
275
γ = 0.01 , it makes our proposed algorithm stable even with very small σ and we apply the same γ to commute distance and it shows better stability as well . In our k means implementation , we evaluate Within Cluster Sum of Square ( WCSS ) scores of each random trials and we choose the best one out of 100 random trials [ 12 ] .
As for both global and local scaling , we run experiments with all the sigma inside the range to test NMI average for each noise level using our algorithm and other five algorithms . We find the average performance along global scaling parameter σ ( [0.1 , 8 ] , with 0.1 as step size between 0.1 to 1 and 0.5 as step size between 1 to 8 ) . For local scaling parameters , [ 5 , 50 ] , 1 as step size is used . The only difference in local scaling lies at the number of parameters and selection of different algorithms . NR seeks σ and β based on the largest eigen gap [ 17 ] . However , eigen gap works poorly on our benchmark dataset and we use the same parameter for both σ and β . ST selects its only scaling parameter as σi ( σj ) where σi ( σj ) is the distance from point i ( j ) to its kth nearest neighbor [ 32 ] . Our algorithm , as well as NJW and MDM , all follow the same way as ST for local scaling experiments ( all the source code and datasets we have used is available at http://wwwcssunysbedu/∼huang3/ ) B . AHK Normalization and Cosine Similarity Analysis
We evaluate different normalization methods for our proposed aggregated heat kernel ( AHK ) . To avoid tuning scaling parameter σ , we adopt cosine similarity , which makes our proposed algorithm parameter tuning free . Table II documents the clustering results ( NMI ) of five different normalizations : no normalization ( NN ) , symmetric normalization ( SYM ) , random walk ( RW ) normalization , Fokker Planck ( FP ) diffusion , and Laplace Beltrami normalization ( LBN ) . These normalization methods are simply applied in Step one of Algorithm 2 .
In Table II , LBN shows the best overall performance across different types of data . Specifically , LBN shows the best performance on text data and competitive performance on network datasets . On remaining dataset , it shows the best results along with SYM . From now on , AHK uses only LBN normalization . Compared with LBN , NN , RW and FP show relatively low performance but FP shows slightly better performance than RW , which supports the argument that the first normalization is helpful . Interestingly NN , RW and FP show quite worse performance on text data . These observations suggest that the density distribution plays a role in reconstructing manifold structures of real world datasets and LBN is a better choice . No normalization shows the worst performance among five approaches , which indicates the importance of normalization .
Table III summarizes six different approaches using cosine similarity . Our new AHK shows the best or very close to the best performances . MDM shows the second best on all but weak performance on text data . ST and NJW show
AHK NORMALIZATION COMPARISON USING COSINE SIMILARITY
Table II
Data Set Iris PenDigits01 PenDigits17 PolBooks UBMCBlog AGBlog 20ngA 20ngB 20ngC 20ngD 0 % noise 20ngD 50 % noise 20ngD 100 % noise Average
NN 8.8 100 2.3 56.9 5.7 1.1 6.2 1.9 15.7 3.0 4.7 5.1 17.6
RW 40.6 95.9 0.9 56.9 73.7 41.1 8.0 0.0 2.6 0.0 0.7 0.1 26.7
FP 40.6 100 13.8 54.0 40.7 0.0 78.2 37.2 12.4 10.2 4.0 5.6 33.1
SYM LBN 70.4 60.8 100 100 16.1 16.1 58.3 56.7 73.7 72.8 74.9 70.2 80.8 73.6 71.8 67.8 67.2 38.5 61.7 56.4 49.0 31.1 43.7 33.7 64.0 57.0 quite similar performance on cosine similarity because of no σ tuning . Although commute distance shares similar motivation with MDM , CD appears to be worse than MDM especially on text data . NR shows the worst performance except PenDigits01 and PolBooks .
COMPARISON AMONG SIX APPROACHES USING COSINE SIMILARITY
Table III
Data Set Iris PenDigits01 PenDigits17 PolBooks UBMCBlog AGBlog 20ngA 20ngB 20ngC 20ngD 0 % noise 20ngD 50 % noise 20ngD 100 % noise Average
NR 8.8 100 2.4 57.5 2.4 0.5 2.4 1.6 2.2 2.4 2.8 2.6 15.5
CD 48.4 95.9 12.9 52.0 0.1 0.4 0.7 0.3 1.7 0.0 2.0 0.2 17.9
NJW 63.5 100 20.4 54.2 73.8 0.2 75.9 10.0 34.9 56.8 38.5 39.5 47.3
ST MDM AHK 70.4 72.3 100 100 20.4 16.1 58.3 56.3 72.8 73.8 70.2 0.2 80.8 75.9 71.8 5.0 67.2 34.4 61.7 55.4 49.0 41.5 43.7 39.5 64.0 47.9
93.1 100 20.7 58.7 74.9 71.7 78.2 2.4 38.2 53.5 42.1 38.8 56.0
C . Robustness to Scaling Parameter
To systematically manifest the sensitivity of different algorithms on different scaling parameters , we test them respectively on a series of global and local scaling parameters . Datasets used here are Iris and Glass from UCI including 40 % and 20 % noise levels , and synthetic noisy dataset FaceContour with 40 % noise level . For noisy dataset , we repeat randomization 20 times to get stable results . The quantitative results are shown in Figure 4 . We can see that our new AHK algorithm is either less sensitive or at least comparable to other five algorithms using both global and local scaling parameters . Moreover , our algorithm is either the best or close to the best with noisy datasets and stays at the top .
D . Robustness to Noise
We conduct experiments on controlled noisy datasets to examine the performance of our algorithm and make
276
( a ) Glass , global scaling
( b ) Glass , local scaling
( c ) Glass with 20 % noise , global scaling
( d ) Glass with 20 % noise , local scaling
( e ) Iris , global scaling
( f ) Iris , local scaling
( g ) Iris with 40 % noise , global scaling
( h ) Iris with 40 % noise , local scaling
( i ) FaceContour 40 % noise , global scaling
( j ) FaceContour 40 % noise , local scaling
Figure 4 . Comparison of six algorithms using different scaling parameters .
277 comparison with the other five algorithms . The data sets are FaceContour with uniformly distributed noise of different noise levels ( 0 % , 10 % , 20 % , ··· , 100% ) . To show the noise robustness and avoid parameter tuning of scaling parameter , we average all the scaling parameters . The experimental results are documented in Figure 5 . AHK indicates the best ( global scaling ) and the second best ( local scaling ) results as it always stays in the best candidate list . Although NR shows the best performance with local scaling , it is one of the worst performer on global scaling . Such fluctuating performance of NR is consistent throughout scaling experiments . Similar to NR , MDM works poorly on local scaling but MDM had shown stable performance in previous scaling experiments . Overall , AHK shows robust performance across different noise conditions including cosine similarity of Table III .
( a ) Global parameter tuning
( b ) Local parameter tuning
Figure 5 . Algorithmic performance on different noise levels .
E . Discussion
Although our proposed clustering algorithm requires no further parameter tuning except σ , we can make it faster by dropping less informative eigenvectors or we can fine tune special cases . In Step 3 of Algorithm 2 , we may use smaller number of eigenvectors than n within the range of 300 to the number of data points . Normally , due to the dramatic value drop down of eigenvalues , it is safe to choose from 300 to 500 for the data sets no larger than 104 .
Choosing the number of eigenvectors k of Step 4 may also affect clustering results . We typically set this value as the number of clusters , as most spectral clustering algorithms
278 take the same strategy . However , if the original data has strong manifold structures in k dimensions where k is the number of clusters , then other spectral cluster algorithms may fail to reconstruct original manifold structure but our proposed algorithm may be able to reconstruct this original manifold structure in the first k eigenvectors , which may produce worse results . Should such situation occurs , we could simply add one or two additional eigenvectors , which are expected to greatly improve the results . In reality , it did not happen on our benchmark dataset or it will not happen in high dimensional or noisy data because it is much more difficult to reconstruct original manifold structure .
VI . RELATED WORK
Mean shift clustering [ 7 ] and spectral clustering [ 22 ] [ 31 ] [ 19 ] have shown good performance in some clustering tasks . However , both of them are sensitive to scaling parameters . To improve , Zelnik Manor and Perona proposed to use local scale [ 32 ] which fully considers the local structure of dataset using neighbor adaptive scale . They introduced a local scaling value σ for each data point . However , the local scaling in [ 32 ] depends on distance between certain point pi and its Qth neighbor . So users still need to specify Q , which is also sensitive to the clustering result for tuning . Compared with the above methods , our method can maintain the similar performance which is insensitive to the scaling parameters .
In [ 17 ] , the authors proposed a noise robust spectral clustering algorithm . But our experimental results have clearly demonstrated that our method has better performance . Recently in [ 4 ] , M estimation robust statistics is used in a robust path based similarity measure which requires no local parameters to be set manually , nonetheless , prior knowledge of data domain is required . In contrast , users need no prior knowledge when using our algorithm .
VII . CONCLUSION
We have developed a new spectral clustering algorithm with robustness to both scaling parameter tuning and data perturbation . The mathematically rigorous theory of our work , together with the new AHK algorithm , are originated from heat kernel and diffusion maps . In technical essence , our AHK permits reorganizing the spectral embedded structure regardless sub optimal scaling parameter selection , noise perturbation , and non uniform density distribution . Extensive experiments and evaluations have demonstrated robust performance with our AHK algorithm in comparison with other popular spectral clustering algorithms . Immediate future work will be concentrated on constructing local and global coordinates with the goal of learning the intrinsic structure of data .
VIII . ACKNOWLEDGEMENTS
We gratefully thank all the anonymous reviewers for constructive suggestions toward paper improvement . This research is supported in part by NSF grants IIS 0710819 , IIS 0949467 , IIS 1047715 , and IIS 1049448 . It is also supported by a BNL LDRD project 10 001 .
[ 17 ] Z . Li , J . Liu , S . Chen , and X . Tang . Noise robust spectral clustering . In Proceedings of IEEE 11th International Conference on Computer Vision , pages 1–8 , 2007 .
REFERENCES
[ 1 ] L . A . Adamic and N . Glance . The political blogosphere and the 2004 us election . In Proceedings of the 3rd international workshop on Link discovery , pages 36–43 , 2005 .
[ 2 ] R . Badeau , B . David , and G . Richard . Fast approximated power iteration subspace tracking . In Signal Processing , IEEE Transactions on 2005 , pages 2931–2941 , 2005 .
[ 3 ] G . Camps Valls , L . Bruzzone , J . L . Rojo Alvarez , and F . Melgani . Robust support vector regression for biophysical variable estimation from remotely sensed images . Geoscience and Remote Sensing Letters , IEEE , 3(3):339–343 , 2006 .
[ 4 ] H . Chang and D . Y . Yeung . Robust path based spectral clustering . Pattern Recognition , 41:191–203 , 2008 .
[ 5 ] Y . Chi , X . Song , D . Zhou , K . Hino , and B . L . Tseng . On evolutionary spectral clustering . ACM TKDD , 3(4):1–30 , 2009 .
[ 6 ] R . R . Coifman and S . Lafon . Diffusion maps . Applied and
Computational Harmonic Analysis , 21(1):5–30 , 2006 .
[ 7 ] D . Comaniciu and P . Meer . Mean shift : a robust approach IEEE Trans . Pattern Analysis toward feature space analysis . and Machine Intelligence , 24(5):603–619 , 2002 .
[ 8 ] I . W . Evett and E . J . Spiehler . Rule induction in forensic science . 1987 .
[ 9 ] R . A . Fisher . The use of multiple measurement in taxonomic problems . Annual Eugenics , 7(2):179–188 , 1936 .
[ 10 ] A . Grigor’yan . Estimates of heat kernels on riemannian manifolds . In Proceedings on Spectral Theory and Geometry . ICMS Instructional Conference , pages 140–225 , 1999 .
[ 11 ] I . Gutman and W . Xiao . The generalized inverse of the laplacian matrix and some applications . Bulletin TCXXIX de lAcadmie serbe des sciences et des arts2004 Classe des sciences mathematiques et naturelles No 29 , pages 1–9 , 2004 .
[ 12 ] J . A . Hartigan and M . A . Wong . Algorithm as 136 : a k means clustering algorithm . Appl . Stat . , 28:100–108 , 1978 .
[ 13 ] E . Hsu . Stochastic analysis on manifolds . Graduate Studies in Mathematics , 38 , 2002 .
[ 14 ] P . J . Huber . Robust Statistics . New York:Wiley .
[ 15 ] A . Kale , A . Karandikar , P . Kolari , A . Java , T . Finin , and A . Joshi . Modeling trust and influence in the blogosphere using link polarity . In Proceedings of the International Conference on Weblogs and Social Media ( ICWSM ) , 2007 .
[ 16 ] S . Lafon , Y . Keller , and R . R . Coifman . Data fusion and multicue data matching by diffusion maps . IEEE Trans . on Pattern Analysis and Machine Intelligence , 28:1784–1797 , 2006 .
[ 18 ] U . V . Luxburg . A tutorial on spectral clustering . Statistics and Computing , 17(4):395–416 , 2007 .
[ 19 ] M . Meila and J . Shi . A random walks view of spectral segmentation . 8th International Workshop on Artificial Intelligence and Statistics , 2001 .
[ 20 ] B . Nadler , S . Lafon , R . Coifman , and I . Kevrekidis . Diffusion maps , spectral clustering and eigenfunctions of fokkerplanck operators . NIPS , 2005 .
[ 21 ] M . E . J . Newman and M . Girvan . Finding and evaluating community structure in networks . Phys . Rev . E69 , 2004 .
[ 22 ] A . Ng , M . Jordan , and Y . Weiss . On spectral clustering : analysis and an algorithm . In Advances in Neural Information Processing Systems 14 , pages 849–856 , 2002 .
[ 23 ] K . Nigam , A . K . McCallum , S . Thrun , and T . M . Mitchelle . Text classification from labeled and unlabeled documents using em . Machine Learning , 39:103–134 , 2000 .
[ 24 ] V . Y . Pan and Z . Q . Chen . The complexity of the matrix eigenproblem . the thirty first annual ACM symposium on Theory of computing , pages 507– 516 , 1999 .
In STOC’99 Proceedings of
[ 25 ] P . Perona and W . T . Freeman . A factorization approach to grouping . In Proceedings of the 5th European Conference on Computer Vision , pages 655–670 , 1998 .
[ 26 ] H . Qiu and E . R . Hancock . Clustering and embedding using commute times . IEEE Transactions on Pattern Analysis and Machine Intelligence , 29(11):1873–1890 , 2007 .
[ 27 ] J . W . Richards , P . E . Freeman , A . B . Lee , and C . M . Schafer . Accurate parameter estimation for star formation history in galaxies using sdss spectra . In MNRAS,399 , pages 1044–1057 , 2009 .
[ 28 ] A . Strehl and J . Ghosh . Cluster ensembles ł a knowledge reuse framework for combining multiple partitions . In J . Mach . Learn . Res . , pages 583–617 , 2003 .
[ 29 ] J . Sun , M . Ovsjanikov , and L . Guibas . A concise and provably informative multi scale signature based on heat diffusion . SGP , 2009 .
[ 30 ] H . Valizadegan and R . Jin . Generalized maximum margin clustering and unsupervised kernel learning . NIPS , 19:1417– 1424 , 2007 .
[ 31 ] D . Verma and M . Meila . Comparison of spectral clustering methods . UW CSE Technical report , 2001 .
[ 32 ] L . Zelnik manor and P . Perona . Self tuning spectral clusterIn Advances in Neural Information Processing Systems ing . 17 , pages 1601–1608 , 2004 .
279
