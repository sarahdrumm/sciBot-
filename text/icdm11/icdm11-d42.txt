Characterizing Inverse Time Dependency in Multi class Learning
Institute for Interdisciplinary Information Sciences
Microsoft Research Asia
Danqi Chen
Weizhu Chen
Tsinghua University
Beijing , China cdq10131@gmail.com
Beijing , China wzchen@microsoft.com
Qiang Yang
Hong Kong University of Science & Technology
Hong Kong qyang@cseusthk
Abstract—The training time of most learning algorithms increases as the size of training data increases . Yet , recent advances in linear binary SVM and LR challenge this commonsense by proposing an inverse dependency property , where the training time decreases as the size of training data increases . In this paper , we study the inverse dependency property of multi class classification problem . We describe a general framework for multi class classification problem with a single objective to achieve inverse dependency and extend it to three popular multi class algorithms . We present theoretical results demonstrating its convergence and inverse dependency guarantee . We conduct experiments to empirically verify the inverse dependency of all the three algorithms on large scale datasets as well as to ensure the accuracy .
Keywords multi class learning ; large scale classification ; su pervised learning ; inverse dependency ;
I . INTRODUCTION
In conventional regularized learning , as the size of the training data expands , a learning algorithm is expected to take more runtime to learn from the data . Yet , recent works propose a counter intuitive observation , such that given a fixed degree of accuracy , the running time of some linear algorithms may decrease as the size of the training data increases . [ 1 ] first put forward a linear SVM solver called Pegasos and demonstrated the inverse timedependency property of linear binary SVM . [ 2 ] generalized the inverse time dependency property by a general Primal Gradient Solver to Lp norms , and extended the property to binary logistic regression algorithms and the least square loss functions .
The inverse dependency property is indeed a very desirable one due to its sub linear complexity in terms of data size . This makes learning algorithms very efficient and scalable . For example , Pegasos and related algorithms can train an effective binary classifier for the RCV1 dataset with about 600,000 samples in seconds . Previous works have demonstrated this effectiveness for binary classification problems . However , in many real world scenarios , machine learning applications belong to the multi class problems , where the outputs have K categories such that K > 2 . Yet , how to achieve the inverse dependency property for the multi class problem is an under explored problem .
Contributions In this paper , we make several advances in inverse time dependency . First , we put forward a general framework for multi class classification algorithms with a single objective to achieve inverse dependency . We then extend it to three popular multi class algorithms : multiclass logistic regression , multi class SVM and multi class perceptron . Second , we present theoretical results to prove the algorithms’ convergence . Simultaneously , we provide a proof of the inverse dependency property thus that all three algorithms can achieve a sub linear complexity for largescale datasets . It is worth noting that the convergence and proofs of inverse time are the keys to ensure the inverse dependency property . Third , we conduct experiments on three large scale datasets . We show that all three algorithms achieve inverse dependency with high efficiency . Finally , to ensure the effectiveness of the algorithms , we compare them with popular alternatives in the literature . Empirical results substantiate our theoretical results and confirm their effectiveness in terms of accuracy .
II . PRELIMINARIES
Given a training set X = {(xi , yi)}N
Before delving into details of inverse dependency , we describe the generalization objective for multi class classification in a regularized learning setting . We consider the loss functions of the three selected popular learning algorithms to demonstrate the inverse dependency property . A . Regularized Learning ⊂ Rd × {1 , 2 , . . . , K} with N samples , where d is the dimension of features and K is the number of classes , we assume that input samples are iid from an unknown probability distribution P . A general multi class classification algorithm focuses on learning a linear predictor wk for each class k where k ∈ {1 , 2 , . . . , K} ; we may use w to denote the collection of all wk . Following the regularized learning theory in [ 3 ] , [ 4 ] , we may express the generalization objective as : i=1
Fλ
= λ · re(w1 , . . . , wK ) + l(w1 , . . . , wK ) = λ · re(w ) + E(x,y)∼P ( l(w ; ( x , y) ) )
( 1 ) ( 2 ) where re(w1 , . . . , wK ) is the regularized term with a positive regularization parameter λ , and l(w1 , . . . , wK ) is the loss function measuring the discrepancy between the predicted output and the real output . Although the goal is to minimize the generalization objective , a practical approach
III . THE MAIN RESULTS
In this section , we propose a general stochastic subgradient descent framework for solving multi class classification problems , which is inspired by the Pegasos algorithm [ 1 ] for solving 2 class SVM . Then we explore the required properties of loss functions and apply the framework to the three multi class algorithms .
The basic idea to solve a linear model is based on the sub gradient descent :
( w(t )
1 , . . . , w(t )
K ) = ( w(t−1 ) −η( ∂f
, . . . , w(t−1 ) ) K ∂w1 , . . . , ∂f ∂wK )
1
( 12 )
K ) as (
1 , . . . , w'
1 , . . . , w . where η is the learning rate . Note that if f is not differentiable at ( w . K ) , it picks any sub gradient belonging to ∂f ( w' ) . The computation cost is expensive since the empirical objective has as many terms as the amount of training data . An alternative solution is to randomly pick a fixed size subset At ⊆ X where |At| = r in each iteration , and then use the temporal objective function f ( t ) to approximate f :
∂f ∂wK
∂f ∂w1
, . . . , l(w1 , . . . , wK ; ( x , y ) )
( 13 )
K . k=1
( t ) f
=
λ 2 w . kwk +
1 r
.
( x,y)∈At
.
( 5 )
For k = 1 , 2 , . . . , K , we compute often tries to minimize the empirical objective , where the averaged loss over N samples is used to approximate the generalized loss . As a special case of treating the L2 norm as the regularizer :
K . k=1 f =
λ 2 w . kwk +
1 N
N . i=1 l(w1 , . . . , wK ; ( xi , yi ) )
( 3 ) where the loss function l(w1 , . . . , wK ; ( xi , yi ) ) may vary in different algorithms .
B . Multi class Logistic Regression ( LR )
A multi class logistic regression model estimates the conditional class probabilities as
P r(Ck|w1 , . . . , wK ; x ) = ew . k x .K j=1 ew . j x
( 4 ) for k = 1 , 2 , . . . , K and Ck denotes class k . The parameters w1 , . . . , wK are learned by the maximum likelihood approach , which is equivalent to minimizing the following negative log likelihood loss : δyi,k log P r(Ck|w ; xi ) '
E(w1 , . . . , wK|X ) = − N . K . N .
K . fi i=1
−w . yi xi + log (
= i=1 k=1 . j xi ) w e j=1 where δi,j is the Kronecker delta function with δi,j = 1 when i = j and δi,j = 0 otherwise . Thus , the loss function of multi class LR in Eq 3 is lLR = −w . yx + log (
K .
. j x
( 6 )
) . e w
C . Multi class SVM j=1
In [ 5 ] , a solution for multi class SVM by solving a single optimization problem was proposed . For this solution , the optimization problem for multi class SVM is formulated as :
K . min w,ξ
λ 2 xi − w . k=1
N . w . kwk +
1 N
ξi i=1
( 7 ) k i − ξi , i = 1 , 2 , . . . , N ( 8 ) where ek yi kxi ≥ e subject to w . i = 1 − δyi,k . The loss function is kx − w . lSV M = max{0 , max kfi=y
{1 + w . yx}} .
( 9 )
D . Muli class Perceptron
Multi class perceptron model aims to find the K para maters w1 , . . . , wK to maximize xi − max w . yi w . kxi k
. ff i
,
( 10 ) which can be understood intuitively as penalizing the largest incorrect labelling . We may take its negative value as the loss function : lP er = max k
{w . kx} − w . yx
( 11 )
K
1 r
∂wk
( x,y)∈At
= λwk +
∂l(w1 , . . . , wK ; ( x , y ) )
, . . . , w(t−1 )
) and use it as an approximation of 1 , and then apply the sub gradient descent Eq 12 to
∂f ( t ) ∂wk at ( w(t−1 ) ∂f ∂wk compute ( w(t ) is the number of iterations . Hence , Eq 12 becomes , . . . , w(t−1 ) λtr ( v1 , . . . , vK ) ( w(t−1 )
K ) . Here we set ηt = 1 t ( w(t−1 ) − 1
1 , . . . , w(t )
, . . . , w(t−1 )
K ) = t−1
λt , where t
1 , . . . , w(t )
∂l(w1,,wK ;(x,y ) ) with vk =
( w(t )
( 14 )
) .
K
)
1
1
( x,y)∈At
K
∂wk
The framework of our multi class inverse dependency solver MCID Solver is given in Alg1 It is worth noting
Algorithm 1 MCID Solver INPUT : X , λ , T , r K ← 0 1 , . . . , w(0 ) 1 : w(0 ) 2 : for t = 1 , 2 , . . . , T do Randomly choose At ⊂ X where |At| = r 3 : for k = 1 , 2 , . . . , K do 4 : t w(t−1 ) 5 : for ( x , y ) ∈ At do 6 : 7 : k ← w(t ) w(t ) 8 : OUTPUT : w(T ) for k = 1 , 2 , . . . , K do ∂l ∂wk k − 1 , . . . , w(T ) k ← t−1 w(t )
( w(t−1 )
λtr
K k
1
K
1
, . . . , w(t−1 )
) that we sample a fixed size subset in each iteration , thus the runtime of a single iteration does not increase as the data size increases , which provides a guarantee for the inverse time dependency property . A . Applications
In this sub section , we will utilize the above framework into the three linear classification algorithms respectively . To adapt the three different loss functions , we compute the sub gradients first .
• Multi class logistic regression : Given Eq 6 , we can compute ∂lLR wk directly :
= −δy,kx +
∂lLR ∂wk
. k
K x · x . w j ew j=1 e x
( 15 )
∗
• Multi class
SVM : Given Eq 9 . Let kx1,lSV M = max{0 , 1 + w . arg maxk'=y w . ( δk∗,kx − δy,kx 1 + w . otherwise 0
∂lSV M ∂wk
= k∗x − w . k∗x− w . yx > 0 k
= y x}
( 16 ) • Multi class perceptron : Given Eq 11 . Similarly , we de fine k
∗
= arg maxk w . kx , lP er becomes lP er = w . ∂lP er ∂wk k∗ x − w . yx = δk∗,kx − δy,kx
The pseudo codes of multi class logistic regression , multiclass SVM and multi class perceptron are given in Alg.2 , Alg.3 and Alg4 Note that we only write the update procedure here ( line 6 ∼ 8 in Alg1 ) Algorithm 2 Multi class Logistic Regression 1 : for ( x , y ) ∈ At do for k = 1 , . . . , K do ok ← ew(t−1 ) z ← .K k=1 ok for k = 1 , . . . , K do k ← w(t ) k − 1 w(t ) λtr y ← w(t ) λtr x y + 1
2 : 3 : 4 : 5 : 6 : 7 : w(t ) ok z x
. x k
2 :
Algorithm 3 Multi class SVM 1 : for ( x , y ) ∈ At do . ∗ ← arg maxk'=y w(t−1 ) x k . . x − w(t−1 ) if 1 + w(t−1 ) x > 0 then k∗ − 1 k∗ ← w(t ) w(t ) y ← w(t ) w(t ) y + 1 y λtr x λtr x k∗
3 :
5 :
4 : k
IV . ANALYSIS
In this section , we prove two main theorems in the paper . One theorem shows that our algorithms can converge with a rate 1 T , and the other shows that our algorithms have an inverse time dependency on the size of training data when their loss functions satisfy the convexity and boundedness
1Note that if there are more than one k achieving the maximum w . kxi , we can pick any one for k∗
.
( 17 )
• Boundedness : the sub gradient can be bounded by a
Algorithm 4 Multi class Perceptron 1 : for ( x , y ) ∈ At do k∗ − 1 y + 1
∗ ← arg maxk w(t−1 ) k∗ ← w(t ) y ← w(t )
2 : 3 : w(t ) 4 : w(t ) k λtr x λtr x x
. k properties . Before discussing these results , we first prove that the three loss functions lLR , lSV M , lP er all satisfy convexity and boundedness , so that the latter proofs can be easily adapted to the three algorithms . A . Convexity & Boundedness
To achieve the inverse dependency and the consistency property of the MCID Solver , there are two specific requirements : the convexity of the loss functions and the boundedness of the sub gradient of the objective function : • Convexity : l(w1 , . . . , wK ; ( x , y ) ) satisfies the convex ity wrt w1 , . . . , wK . ie : For any 0 < α < 1 , ( w(1 )
1 , . . . , w(1 )
1 , . . . , w(2 )
K ) , ( w(2 )
K ) , then l(α(w(1 ) ≤ αl(w(1 )
1 , . . . , w(1 ) 1 , . . . , w(1 )
K ) + ( 1 − α)(w(2 ) K ) + ( 1 − α)l(w(2 )
1 , . . . , w(2 ) 1 , . . . , w(2 ) K )
K ) ) positive constant :
∃C > 0 , ∀1 ≤ k ≤ K,| ∂f ∂wk
| ≤ C
Theorem 1 . The three loss functions lLR , lSV M , lP er that are given in Eq 6 , Eq 9 , Eq 11 all satisfy the properties of convexity and boundedness .
Proof : For convexity , we prove lSV M here , the conlLR and lP er can be proved similarly . Let then lSV M = vexity of hk,y(w1 , . . . , wK ) = 1 + w . max{0 , maxk'=y hk,y(w)} . Since kx − w . y x ,
{hk,y(αw(1 ) + ( 1 − α)w(2))} {αhk,y(w(1 ) ) + ( 1 − α)hk,y(w(2))} max kfi=y = max kfi=y ≤ α max kfi=y ≤ α max{0 , max kfi=y = αlSV M ( w(1 ) ) + ( 1 − α)lSV M ( w(2 ) ) hk,y(w(1 ) ) + ( 1 − α ) max kfi=y hk,y(w(2 ) ) hk,y(w(1))} + ( 1 − α ) max{0 , max kfi=y hk,y(w(2))} and 0 ≤ αlSV M ( w(1 ) ) +(1 − α)lSV M ( w(2 ) ) , we reach our conclusion . For boundedness , we assume that ( x , y ) ∈ X , x ≤ R . We can easily verify that the subgradients in Eq 15 , Eq 16 , Eq 17 all satisfy | ≤ x,| ∂lSV M ∂wk
| ≤ x,| ∂lP er ∂wk
| ∂lLR ∂wk
| ≤ x
Thus the sub gradients are bounded by R .
B . Convergence
Next , we prove that the framework has the desired converge rate in this subsection . For this proof , we start by introducing a lemma :
Lemma 1 . Let f ( 1 ) , f ( 2 ) , . . . , f ( T ) be a sequence of functions such that for all t = 1 , . . . , T , f ( t ) = λh + g(t ) with h being strongly convex wrt ·2 and g(t ) is a convex function . Let w(0 ) , w(1 ) , . . . , w(T ) be a sequence of vectors such that w(0 ) = 0 , for t ≥ 1 , w(t ) = w(t−1 ) − 1 λt ( λw(t−1 ) + v(t ) ) where v(t ) ∈ ∂g(t)(w(t−1) ) . Then T .
T .
T .
( t )
( w(t )
) − min w
( t )
( w ) ≤ 1 2 f t=1 f t=1 v(t)2
2 t=1
λt
This proof can be learnt as a special case of Theorem 2 in [ 6 ] . Theorem 2 . Assume that f , f ( t ) are respectively given in the form of Eq 3 and Eq 13 where l satisfies convexity and boundedness . Let δ ∈ ( 0 , 1 ) , then with a probability of at least 1 − δ over the choices of A1 , . . . , AT , we have f ( w(T )
1
, . . . , w(T )
K ) ≤ min f ( w1 , . . . , wK ) + c log T
δλT
( 18 ) it
T
T t=1 f ( t)(w(t ) ) − min is convex , is straightforward to ( x,y)∈At l(w1 , . . . , wK ; ( x , y ) ) is con(w1 , . . . , wK)2 2 is strongly contheret=1 f ( t)(w ) can be bounded T T . Furthermore , since l satisfies boundt=1 f ( t)(w(t ) ) − min can be λ where c is a positive constant . Denote
Proof : Since l show that g(t ) = 1 r vex . Plus that h = 1 vex , {f ( t)} satisfies the condition in Lemma 1 , 2 fore by 1 t=1 2 edness , thus bounded by c log T 1 , . . . , w∗ ( w∗ T .
K ) = arg minw f ( w1 , . . . , wK ) , there is t=1 f ( t)(w ) ffv(t)ff2
.T
λt
2
( w∗
1 , . . . , w∗
K ) ≤ c log T
( t ) f
( t )
( w(t )
1 , . . . , w(t ) f t=1
K ) − T . t=1
Taking the expectation of the above inequality and picking an index t0 uniformly at random from {1 , 2 , . . . , T} , we have
E(f ( w(t0 )
1
, . . . , w(t0 )
K ) ) − f ( w∗
1 , . . . , w∗
K ) ≤ c log T λT
.
Applying Markov inequality , we can arrive at the conclusion that with probability of at least 1 − δ , f ( w(t0 )
1
, . . . , w(t0 )
K ) − f ( w∗
1 , . . . , w∗
K ) ≤ c log T δλT
.
Note that we can view T as a random index drawn from {1 , 2 , . . . , ˆT} where ˆT > T , thus we can return ( w(T ) K ) after T iterations instead of a random index .
, . . . , w(T )
1
Theorem 2 states that the framework in Alg.1 can converge with rate of 1 T whenever the loss functions satisfy convexity and boundedness . We have shown that the three loss functions lLR , lSV M , lP er all satisfy the two properties , so our three algorithms can achieve an accuracy of with a confidence 1 − δ with ˜O( 1 C . Inverse Time Dependency
δλ ) iterations .
In this subsection , we will show that to attain a fixed generalization error , more training data will yield less training time . Below is the main theorem .
λ
. therefore ˆT = ˜O(
(
Kd∗r/δ
'(wfi
α
1 ,,wfi
K
)' )2− ˜O( 1 N )
) , we conclude our proof with ˆT showing an inverse dependency on N .
Theorem 3 . For any ( w . to attain a generalization error E(l( ˆw1 , . . . , ˆwK ) ) ≤ K ) ) + α with a confidence 1 − δ where l E(l(w . satisfies convexity and “ boundedness ” , ( ˆw1 , . . . , ˆwK ) is a solution of f in Eq 3 with an accuracy . ie ,
1 , . . . , w .
1 , . . . , w . if we want
K ) ,
( ˆw1 , . . . , ˆwK ) ≤ min f ( w1 , . . . , wK ) + , the runtime ˆT of our algorithm has an inverse dependency on the size of training set N :
∗
ˆT = ˜O(
( d 1,,wfi
α rK/δ
K ) )2 − ˜O( 1
N )
( wfi
)
( 19 )
Proof : Following the “ oracle ” analysis in [ 7 ] , we can conclude that
E(l( ˆw1 , . . . , ˆwK ) ) ≤ E(l(w' ( w'
1 , . . . , w' 1 , . . . , w'
+
K ) ) + ˜O( K )2
+ ˜O(
λ 2
)
∗
Kd r δλ ˆT 1 λN
)(20 )
The above conclusion is obtained from the main result in [ 3 ] and the result of Theorem 2 : O( ) = O( Kd∗r δλ ˆT ) . If we ' choose fi
)
1
˜O(
K )
λ = ˜Θ
1 , . . . , w'
( w' the bound becomes fi E(l( ˆw1 , . . . , ˆwK ) ) ) ≤ E(l(w' ( w'
1 , . . . , w'
+ ˜O
K )
)
˜O(
∗
Kd ˆT δ r
+
1 N
)
,
'
K ) )
1 , . . . , w' 1 N
Kd∗r ˆT δ
+
)
V . EXPERIMENTS
In this section , we first present experimental results to demonstrate the inverse dependency property of our three algorithms and strengthen our theoretical results . We next compare the performance of our algorithms with the alternatives in the literature , including the SVMmulticlass[8 ] for multi class SVM , and “ one versus the rest ” ( denoted as OVR ) methods based on binary classification algorithms to indirectly solve the multi class SVM and LR problem .
A . Datasets
Table I lists the datasets used in our experiments .
Table I
DATASETS
K # training Dataset 665,265 RCV1 4 4 518,571 RCV1 53 53 ODP 11 569,068 d
# test d 19,806 47,236 73 15,564 47,236 65 142,267 61,016 190
∗
RCV1 We construct our RCV1 4 and RCV1 53 datasets from the Reuters RCV1 collection [ 9 ] . To generate the RCV1 4 dataset with four classes , we map the dataset to the first level {CCAT , ECAT , GCAT , MCAT} and remove s d n o c e S y c a r u c c A
0 0 1
0 9
0 8
0 7
0 6
0 5
0 4
0 3
0 2
0 1
0
0
.
0 7
.
0 9 6
.
0 8 6
0
.
7 6
0
.
6 6
.
0 5 6
Multi−class Logistic Regression on RCV1−4
Multi−class SVM on RCV1−4
Multi−class Perceptron on RCV1−4
>95.8 % >96.3 % >96.8 % s d n o c e S
0 3
5 2
0 2
5 1
0 1
5
0
>95.5 % >96.0 % >96.5 % s d n o c e S
0 3
5 2
0 2
5 1
0 1
5
0
>95.6 % >95.8 % >96.0 %
5000 10000
100000
300000
500000
5000 10000
100000
300000
500000
5000 10000
100000
300000
500000
Number of Training Data
Number of Training Data
Number of Training Data
Figure 1 .
Inverse Dependency of 3 Algorithms on RCV1 4 Dataset .
Multi−class Logistic Regression on ODP
Multi−class SVM on ODP
Multi−class Perceptron on ODP
500000 300000 100000
500000 300000 100000 y c a r u c c A
.
0 0 7
.
5 8 6
.
0 7 6
.
5 5 6
.
0 4 6
.
5 2 6
.
0 1 6
500000 300000 100000 y c a r u c c A
.
0 0 7
.
5 8 6
.
0 7 6
.
5 5 6
.
0 4 6
.
5 2 6
.
0 1 6
5000
10000
50000 100000 300000 Number of Training Data Figure 2 .
500000
5000
10000
50000 100000 300000 Number of Training Data
500000
5000
10000
50000 100000 300000 Number of Training Data
500000
Inverse Dependency of 3 Algorithms on ODP Dataset . those documents which have more than one label . Similarly , we map the data set to the second level of the RCV1 topic hierarchy and remove the documents that only have labels of the first level or with multiple labels . This generates a 53 class dataset : RCV1 53 . ODP The original dataset has 714,875 samples with 3,637,434 features . Since the number of features is too large , we performed feature selection on the original dataset using document frequency ( DF ) . We computed the document frequency for each feature and removed those features of frequency < 100 and removed documents with no features . We choose 80 % of the samples for training and the others for testing . Consequently , we get a new dataset that includes 569 , 068 training samples and 142 , 267 testing samples with 61 , 016 features .
B . Inverse Dependency Experiment
In this experiment , we verify the inverse dependency property of our algorithms on different datasets . It is worth noting that due to the randomness of the stochastic algorithm , we run all following experiments 10 times and take the average results . We verify the inverse dependency from two different perspectives . The first one is to measure the running time in seconds against various training data size , until the algorithms reach a certain accuracy on the testing set . We show the experimental results in Fig 1 , which demonstrates that when the accuracy is fixed , the training time decreases as the size of training data increases , and this trend is consistent for different accuracy . The other experiment is to measure the the accuracy against various training data sizes until the algorithms finish various fixed number iterations . We present the results in Fig 2 , which illustrates that given the same number of iterations , the accuracy of our algorithms increases as the size of training data increases . Both of the above results fully substantiate the theoretical result of inverse time dependency .
C . Comparison with OVR Method
In this experiment , we compare the accuracy of multiclass LR , multi class SVM and the general OVR methods.To make the experiment more solid , we chose the 2 class SVM classifier Pegasos [ 1 ] and 2 class logistic regression classifier in [ 2 ] with the 2 norm regularizer , which have both been proved effective and satisfy inverse dependency . For simplification , we call the above 2 algorithms “ K Pegasos ” and “ K binaryLR ” respectively . The time complexity to train K binary classifiers of K Pegasos and K binaryLR is the same as our algorithms . Thus , in this experiment we fix the parameters λ , r , and then compare the accuracy of the algorithms against a set of distinct number of iterations . Fig 3 gives the results of the four algorithms on the three datasets . These results clearly indicate that our multi class LR can outperform K binaryLR and multi class SVM can outperform K Pegasos in the above datasets .
D . Comparison with SVMmulticlass
The state of the art multi class
SVM classifier SVMmulticlass [ 8 ] uses the same formulation as our multi class SVM classifier and its runtime scales linearly with the number of training examples . We test our program on three datasets against the solution generated by SVMmulticlass and compare their performance . After running the SVMmulticlass on the datasets , we find that SVMmulticlass cannot be adapted to large scale datasets very well . It causes the “ out of memory ” error on all three datasets in a server with 32G RAM . To ensure a comparison between these two algorithms , we take half of the training samples of RCV1 4 and RCV1 53 for training . The experimental results are presented in Fig 4 , which illustrates that our SVM classifier has an advantage in time efficiency over SVMmulticlass in achieving the same
RCV1−4 Dataset
RCV1−53 Dataset
ODP Dataset y c a r u c c A
7 9 0
.
5 9 0
.
3 9 0
.
1 9 0
.
9 8 0
. y c a r u c c A
9 8 0
.
5 8 0
.
1 8 0
.
7 7 0
.
3 7 0
.
9 6 0
.
K−Pegasos Multi−class SVM K−binaryLR Multi−class LR y c a r u c c A
2 7 0
.
7 6 0
.
2 6 0
.
7 5 0
.
2 5 0
.
K−Pegasos Multi−class SVM K−binaryLR Multi−class LR
K−Pegasos Multi−class SVM K−binaryLR Multi−class LR
10000
50000 100000
400000
800000
10000
Number of Iterations
30000
50000
70000 Number of Iterations
90000
5000
10000
300000 50000 100000 Number of Iterations
500000
Figure 3 . The Accuracy of 4 algorithms on 3 Datasets accuracy . For example , SVMmulticlass takes 77.88 seconds to achieve 97.2 % accuracy on RCV1 4 while our SVM classifier only takes 7.28 seconds . This may be attributed to the time complexity difference that our algorithms have a sub linear complexity while SVMmulticlass incurs a linear complexity . We conclude that our algorithm performs faster on large scale datasets than SVMmulticlass .
RCV1−4 Dataset
RCV1−53 Dataset s d n o c e S n i e m i t n u R
Multiclass SVM Multiclass Perceptron Multiclass LR SVM^multiclass
0 8
0 6
0 4
0 2
0 s d n o c e S n i e m i t n u R
Multiclass SVM Multiclass Perceptron Multiclass LR SVM^multiclass
0 0 5
0 0 4
0 0 3
0 0 2
0 0 1
0
85.0 % 90.0 % 95.0 % 96.0 % 96.4 % 96.8 % 97.2 %
77.0 % 79.0 % 81.0 % 83.0 % 85.0 % 87.0 % 89.0 %
Accuracy
Accuracy
Figure 4 . Our SVM classifier vs . SVMmulticlass .
VI . CONCLUSION
In this paper , we have studied the inverse dependency property for multi class classification problems and applied it to three popular learning algorithms . We successfully demonstrated the algorithms’ inverse dependency property theoretically and empirically . As a surprising result , our algorithms can train a multi class classifier very efficiently with an order of magnitude speed up as compared with the state of the art multi class classifier while ensuring a very high accuracy .
VII . DISCUSSION & FUTURE WORK
Based on the oracle analysis in Eq 20 , as we fix λ and ˆT , the upper bound of the expected error of the solution ˆw should monotonously decrease as we increase the training data size . However , if we examine the experimental results in Fig 1 and Fig 2 in more detail , we can observe an inconsistent phenomenon : when the size of the training data is small , the decrease of the training time is significant as we increase the training data ; on the other hand , as the size of the training data reaches some value , the training time no longer declines with the increase of the training data ie , the lines in above figures keep flat or fluctuating as we increase the training data after this value . This may contradict with the inverse time depenency property . Therefore , to explore the applicability of inverse dependency with respect to the theoretical assumptions and the statistical properties of real datasets is an important but still under explored problem . And this is one of our future work to further explore inverse dependency .
ACKNOWLEDGMENT
This work was supported in part by the National Basic Research Program of China Grant 2007CB807900 , 2007CB807901 , the National Natural Science Foundation of China Grant 61033001 , 61061130540 , 61073174 , and the Hong Kong RGC grant 621010 .
REFERENCES
[ 1 ] S . Shalev Shwartz , Y . Singer , and N . Srebro , “ Pegasos : Primal estimated sub gradient solver for svm , ” in ICML , 2007 .
[ 2 ] Z . Zhu , W . Chen , C . Zhu , G . Wang , H . Wang , and Z . Cheng , “ Inverse time dependency in convex regularized learning , ” in ICDM , 2009 .
[ 3 ] K . Sridharan , N . Srebro , and S . Shalev Shwartz , “ Fast rates for regularized objectives , ” in NIPS , 2008 .
[ 4 ] L . Bottou and O.Brousquet , “ The tradeoff of large scale learning , ” in NIPS , 2007 .
[ 5 ] K . Crammer and Y . Singer , “ On the algorithmic implementation of multiclass kernel based vector machines , ” Technical report , School of Computer Science and Engineering , Hebrew University , 2001 .
[ 6 ] S . Kakade and S . Shalev Shwartz , “ Mind the duality gap : Logarithmic regret algorithms for online optimization , ” in NIPS , 2009 .
[ 7 ] S . Shalev Shwartz and N . Srebro , “ Svm optimization : Inverse dependency on training set size , ” in ICML , 2008 .
[ 8 ] T .
Joachims , vector machine , ” http://svmlightjoachimsorg/svm multiclasshtml
“ Multi class support
[ 9 ] D . Lewis , Y . Yang , T . Rose , and F . Li , “ Rcv1 : A new benchmark collection for text categorization research , ” Journal of Machine Learning Research , 2004 .
