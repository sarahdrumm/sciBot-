2011 11th IEEE International Conference on Data Mining
Minimizing Seed Set for Viral Marketing
Cheng Long , Raymond Chi Wing Wong
Department of Computer Science and Engineering
The Hong Kong University of Science and Technology
{clong , raywong}@cseusthk
Abstract—Viral marketing has attracted considerable concerns in recent years due to its novel idea of leveraging the social network to propagate the awareness of products . Specifically , viral marketing is to first target a limited number of users ( seeds ) in the social network by providing incentives , and these targeted users would then initiate the process of awareness spread by propagating the information to their friends via their social relationships . Extensive studies have been conducted for maximizing the awareness spread given the number of seeds . However , all of them fail to consider the common scenario of viral marketing where companies hope to use as few seeds as possible yet influencing at least a certain number of users . In this paper , we propose a new problem , called 𝐽 MIN Seed , whose objective is to minimize the number of seeds while at least 𝐽 users are influenced . 𝐽 MIN Seed , unfortunately , is proved to be NP hard in this work . In such case , we develop a greedy algorithm that can provide error guarantees for 𝐽 MIN Seed . Furthermore , for the problem setting where 𝐽 is equal to the number of all users in the social network , denoted by Full Coverage , we design other efficient algorithms . Extensive experiments were conducted on real datasets to verify our algorithm .
I . INTRODUCTION
Viral marketing is an advertising strategy that takes the advantage of the effect of “ word of mouth ” among the relationships of individuals to promote a product . Instead of covering massive users directly as traditional advertising methods [ 1 ] do , viral marketing targets a limited number of initial users ( by providing incentives ) and utilizes their social relationships , such as friends , families and co workers , to further spread the awareness of the product among individuals . Each individual who gets the awareness of the product is said to be influenced . The number of all influenced individuals corresponds to the influence incurred by the initial users . According to some recent research studies [ 2 ] , people tend to trust the information from their friends , relatives or families more than that from general advertising media like TVs . Hence , is believed that viral marketing is one of the most effective marketing strategies [ 3 ] . In fact , extensive commercial instances of viral marketing succeed in real life . For example , Nike Inc . used social networking websites such as orkut.com and facebook.com to market products successfully [ 4 ] . it
The propagation process of viral marketing within a social network can be described in the following way . At the beginning , the advertiser selects a set of initial users and provides these users incentives so that they are willing to initiate the awareness spread of the product in the social network . We call these initial users seeds . Once the propagation is initiated , the information of the product diffuses or spreads
1550 4786/11 $26.00 © 2011 IEEE DOI 101109/ICDM201199
427
Ada
.8
.6
Bob
.7
.7
Connie
.9
.6
David n1 n2
1 n3
1
1 n4
Fig 1 . Social network ( IC model )
Fig 2 . Counter example ( 𝛼(⋅ ) ) via the relationships among users in the social network . A lot of models about how the above diffusion process works have been proposed [ 5 10 ] . Among them , the Independent Cascade Model ( IC model ) [ 5 , 6 ] and the Linear Threshold Model ( LT model ) [ 7 , 8 ] are the two that are widely used in the literature . In the social network , the IC model simulates the situation where for each influenced user 𝑢 , each of its neighbors has a probability to be influenced by 𝑢 , while the LT model captures the phenomenon where each user ’s tendency to become influenced increases when more of its neighbors become influenced .
Consider the following scenario of viral marketing . A company wants to advertise a new product via viral marketing within a social network . Specifically , it hopes that at least a certain number of users , says 𝐽 , in the social network must be influenced yet the number of seeds for viral marketing should be as small as possible . Clearly , the above problem can be formalized as follows . Given a social network 𝐺(𝑉 , 𝐸 ) , we want to find a set of seeds such that the size of the seed set is minimized and at least 𝐽 users are influenced at the end of viral marketing . We call this problem 𝐽 MIN Seed .
We use Figure 1 to illustrate the main idea of the 𝐽 MINSeed problem . The four nodes shown in Figure 1 represent four members in a family , namely Ada , Bob , Connie and David , respectively . In the following , we use the terms “ nodes ” and “ users ” interchangeably since they correspond to the same concept . The directed edge ( 𝑢 , 𝑣 ) with the weight of 𝑤𝑢,𝑣 indicates that node 𝑢 has the probability of 𝑤𝑢,𝑣 to influence node 𝑣 for the awareness of the product . Now , we want to find the smallest seed set such that at least 3 nodes can be influenced by this seed set . It is easy to verify that the expected influence incurred by seed set {𝐴𝑑𝑎} is about 3.571 under the IC model and no smaller seed set can incur at least 3 influenced nodes . Hence , seed set {𝐴𝑑𝑎} is our solution .
𝐽 MIN Seed can be applied to most ( if not all ) applica
1The computation of the expected influence incurred by a seed is calculated by considering all cascades from this seed . Eg , the expected influence on Bob incurred by Ada is 1 − ( 1 − 0.8 ) ⋅ ( 1 − 0.6 ⋅ 0.7 ) = 0884 tions of viral marketing . Intuitively , 𝐽 MIN Seed asks for the minimum cost ( seeds ) while satisfying an explicit requirement of revenue ( influenced nodes ) . Clearly , in the mechanism of viral marketing , a seed and an influenced node correspond to cost and potential revenue of a company , respectively , Because the company has to pay the seeds for incentives , while an influenced node might bring revenue to the company . In many cases , companies face the situation where the goal of revenue has been set up explicitly and the cost should be minimized . Thus , 𝐽 MIN Seed meets these companies’ demands .
Another area where 𝐽 MIN Seed can be widely used is the “ majority decision rule ” ( eg , the three fifths majority rule in the US Senate ) . By majority decision rule , we mean the principle under which the decision is determined by the majority ( or a certain portion ) of participants . That is , in order to affect a group of people to make a decision , eg , purchasing our products , we only need to convince a certain number of members in this group , says 𝐽 , which is the threshold of the number of people to agree on the decision . Clearly , for these kinds of applications , 𝐽 MIN Seed could be used to affect the decision of the whole group and yield the minimum cost . In fact , 𝐽 MIN Seed is particularly useful in the election campaigns where the “ majority decision rule ” is adopted .
No existing studies have been conducted for 𝐽 MIN Seed even though it plays an essential role in the viral marketing field . In fact , most existing studies related to viral marketing focus on maximizing the influence incurred by a certain number of seeds , says 𝑘 [ 11 16 ] . Specifically , they aim at maximizing the number of influenced nodes when only 𝑘 seeds are available . We denote this problem by 𝑘 MAX Influence . Clearly , 𝐽 MIN Seed and 𝑘 MAX Influence have different goals with different given resources .
Na¨ıvely , we can solve the 𝐽 MIN Seed problem by adapting an existing algorithm for 𝑘 MAX Influence . Let 𝑘 be the number of seeds . We set 𝑘 = 1 at the beginning and increment 𝑘 by 1 at the end of each iteration . For each iteration , we use an existing algorithm for 𝑘 MAX Influence to calculate the maximum number of nodes , denoted by 𝐼 , that can to 𝑘 . If be influenced by a seed set with the size equal 𝐼 ≥ 𝐽 , we stop our process and return the current number 𝑘 . Otherwise , we increment 𝑘 by 1 and perform the next iteration . However , this na¨ıve method is very time consuming since it issues the existing algorithm for 𝑘 MAX Influence many times for solving 𝐽 MIN Seed . Note that 𝑘 MAX Influence is NP hard [ 12 ] . Any existing algorithm for 𝑘 MAX Influence is computation expensive , which results in this na¨ıve method with a high computation cost . Hence , we should resort to other more efficient solutions .
In this paper , 𝐽 MIN Seed is , unfortunately , proved to be NP hard . Motivated by this , we design an approximate ( greedy ) algorithm for 𝐽 MIN Seed . Specifically , our algorithm iteratively adds into a seed set one node that generates the greatest influence gain until the influence incurred by the seed set is at least 𝐽 . Besides , we work out an additive error bound and a multiplicative error bound for this greedy algorithm .
In some cases , the companies would set the parameter 𝐽 for 𝐽 MIN Seed to be the total number of users in the underlying social network since they want to influence as many users as possible . Motivated by this , we further discuss our 𝐽 MINSeed problem under the special setting where 𝐽 = ∣𝑉 ∣ ( the total number of users ) . We call this special instance of 𝐽 MINSeed as Full Coverage for which we design other efficient algorithms .
We summarize our contributions as follows . Firstly , to the best of our knowledge , we are the first to propose the 𝐽 MINSeed problem , which is a fundamental problem in viral marketing . Secondly , we prove that 𝐽 MIN Seed is NP hard in this paper . Under such situation , we develop a greedy algorithm framework for 𝐽 MIN Seed , which , fortunately , can provide error guarantees for the approximation error . Thirdly , for the Full Coverage problem ( ie , 𝐽 MIN Seed where 𝐽 = ∣𝑉 ∣ ) , we observe some interesting properties and thus design some other efficient algorithms . Finally , we conducted extensive experiments which verified our algorithms .
The rest of the paper is organized as follows . Section II covers the related work of our problem , while Section III provides the formal definition of the 𝐽 MIN Seed problem and some relevant properties . We show how to calculate the influence incurred by a seed set in Section IV , which is followed by Section V discussing our greedy algorithm framework . In Section VI , we discuss the Full Coverage problem . We conduct our empirical studies in Section VII and conclude our paper in Section VIII .
II . RELATED WORK
In Section II A , we discuss two widely used diffusion models in a social network , and in Section II B , we give the related work about the influence maximization problem .
A . Diffusion Models
Given a social network represented in a directed graph 𝐺 , we denote 𝑉 to be the set containing all the nodes in 𝐺 each of which corresponds to a user and 𝐸 to be the set containing all the directed edges in 𝐺 . Each edge 𝑒 ∈ 𝐸 in form of ( 𝑢 , 𝑣 ) is associated with a weight 𝑤𝑢,𝑣 ∈ [ 0 , 1 ] . Different diffusion models have different meanings on weights . In the following , we discuss the meanings for two popular diffusion models , namely the Independent Cascade ( IC ) model and the Linear Threshold ( LT ) model .
1 ) Independent Cascade ( IC ) Model [ 7 , 8 ] : The first model is the Independent Cascade ( IC ) model . In this model , the influence is based on how a single node influences each of its single neighbor . The weight 𝑤𝑢,𝑣 of an edge ( 𝑢 , 𝑣 ) corresponds to the probability that node 𝑢 influences node 𝑣 . Let 𝑆0 be the initial set of influenced nodes ( seeds in our problem ) . The diffusion process involves a number of steps where each step corresponds to the influence spread from some influenced nodes to other non influenced nodes . At step 𝑡 , all influenced nodes at step 𝑡 − 1 remain influenced , and each node that becomes influenced at step 𝑡 − 1 for the first time has one chance to influence its non influenced neighbors .
428
Specifically , when an influenced node 𝑢 attempts to influence its non influenced neighbor 𝑣 , the probability that 𝑣 becomes influenced is equal to 𝑤𝑢,𝑣 . The propagation process halts at step 𝑡 if no nodes become influenced at step 𝑡−1 . The running example in Figure 1 is based on the IC model .
For a graph under the IC model , we say that the graph is deterministic if all its edges have the probabilities equal to 1 . Otherwise , we say that it is probabilistic .
∑
2 ) Linear Threshold ( LT ) Model [ 5 , 6 ] : The second model is the Linear Threshold ( LT ) model . In this model , the influence is based on how a single node is influenced by its multiple neighbors together . The weight 𝑤𝑢,𝑣 of an edge ( 𝑢 , 𝑣 ) corresponds to the relative strength that node 𝑣 is influenced by its neighbor 𝑢 ( among all of 𝑣 ’s neighbors ) . Besides , for ( 𝑢,𝑣)∈𝐸 𝑤𝑢,𝑣 ≤ 1 . The dynamics each 𝑣 ∈ 𝑉 , it holds that of the process proceeds as follows . Each node 𝑣 selects a threshold value 𝜃𝑣 from range [ 0 , 1 ] randomly . Same as the IC model , let 𝑆0 be the set of initial influenced nodes . At step 𝑡 , the non influenced node 𝑣 , for which the total weight of ∑ the edges from its influenced neighbors exceeds its threshold ( 𝑢,𝑣)∈𝐸 and 𝑢 is influenced 𝑤𝑢,𝑣 ≥ 𝜃𝑣 ) , becomes influenced . ( The spread process terminates when no more influence spread is possible .
For a graph under the LT model , we say that the graph is deterministic if the thresholds of all its nodes have been set before the process of influence spread . Otherwise , we say that it is probabilistic .
B . Influence Maximization
Motivated by the fact that social network plays a fundamental role in spreading ideas , innovations and information , Domingoes and Richardson proposed to use social networks for marketing purpose , which is called viral marketing [ 11 , 17 ] . By viral marketing , they aimed at selecting a limited number of seeds such that the influence incurred by these seeds is maximized . We call this fundamental problem as the influence maximization problem .
In [ 12 ] , Kempe et al . formalized the above influence maximization problem as a discrete optimization problem called 𝑘 MAX Influence for the first time . Given a social network 𝐺(𝑉 , 𝐸 ) and an integer 𝑘 , find 𝑘 seeds such that the incurred influence is maximized . Kempe et al . proved that 𝑘 MAXInfluence is NP hard for both the IC model and the LT model . To achieve better efficiency , they provided a ( 1−1/𝑒)approximation algorithm for 𝑘 MAX Influence .
Recently , several studies have been conducted to solve 𝑘MAX Influence in a more efficient and/or scalable way than the aforementioned approximate algorithm in [ 12 ] . Specifically , in [ 13 ] , Leskovec et al . employed a “ lazy forward ” strategy to select seeds , which has been shown to be effective for reducing the cost of the influence propagation of nodes . In [ 14 ] , Kimura et al . proposed a new shortestpath cascade model , based on which , they developed efficient algorithms for 𝑘 MAX Influence . Motivated by the drawback of non scalability of all aforementioned solutions for 𝑘 MAXInfluence , Chen et al . proposed an arborescence based heuris tic algorithm , which was verified to be quite scalable to largescale social networks [ 15 ] .
The influence maximization problem has been extended into the setting with multiple products instead of a single product . Bharathi et al . solved the influence maximization problem for multiple competitive products using game theoretical methods [ 18 , 19 ] , while Datta et al . proposed the influence maximization problem for multiple non competitive products [ 16 ] . Apart from these studies aiming at maximizing the influence , considerable efforts have been devoted to the diffusion models in social networks [ 9 , 10 ] .
Clearly , most of the existing studies related to viral marketing aim at maximizing the influence incurred by a limited number of seeds ( ie , 𝑘 MAX Influence ) . While our problem , 𝐽 MIN Seed , is targeted to minimize the number of seeds while satisfying the requirement of influencing at least a certain number of users in the social network . As discussed in Section I , a na¨ıve adaption of any existing algorithm for 𝑘 MAX Influence is time consuming .
III . PROBLEM
We first formalize 𝐽 MIN Seed in Section III A . In Section III B , we provide several properties related to 𝐽 MINSeed .
A . Problem Definition
Given a set 𝑆 of seeds , we define the influence incurred by the seed set 𝑆 ( or simply the influence of 𝑆 ) , denoted by 𝜎(𝑆 ) , to be the expected number of nodes influenced during the diffusion process initiated by 𝑆 . How to calculate 𝜎(𝑆 ) under different diffusion models given 𝑆 will be discussed in Section IV .
Problem 1 ( 𝐽 MIN Seed ) : Given a social network 𝐺(𝑉 , 𝐸 ) and an integer 𝐽 , find a set 𝑆 of seeds such that the size of the seed set is minimized and 𝜎(𝑆 ) ≥ 𝐽 .
We say that node 𝑢 is covered by seed set 𝑆 if 𝑢 is influenced during the influence diffusion process initiated by 𝑆 . It is easy to see that 𝐽 MIN Seed aims at minimizing the number of seeds while satisfying the requirement of covering at least 𝐽 nodes . Given a node 𝑥 in 𝑉 and a subset 𝑆 of 𝑉 , the marginal gain of inserting 𝑥 into 𝑆 , denoted by 𝐺𝑥(𝑆 ) , is defined to be 𝜎(𝑆 ∪ {𝑥} ) − 𝜎(𝑆 ) .
We show the hardness of 𝐽 MIN Seed with the following theorem .
Theorem 1 : The 𝐽 MIN Seed problem is NP hard for both the IC model and the LT model . Proof . The proof can be found in [ 20 ] .
B . Properties
Since the analysis of the error bounds of our approximate algorithms to be discussed is based on the property that function 𝜎(⋅ ) is submodular , we first briefly introduce the concept of submodular function , denoted by 𝑓(⋅ ) . After that , we provide several properties related to the influence diffusion process in a social network . Definition 1 ( Submodularity ) : Let 𝑈 be a universe set of elements and 𝑆 be a subset of 𝑈 . Function 𝑓(⋅ ) which maps
429
𝑆 to a non negative value is said to be submodular if given any 𝑆 ⊆ 𝑈 and any 𝑇 ⊆ 𝑈 where 𝑆 ⊆ 𝑇 , it holds for any element 𝑥 ∈ 𝑈 − 𝑇 that 𝑓(𝑆 ∪ {𝑥} ) − 𝑓(𝑆 ) ≥ 𝑓(𝑇 ∪ {𝑥} ) − 𝑓(𝑇 ) . In other words , we say 𝑓(⋅ ) is submodular if it satisfies the “ diminishing marginal gain ” property : the marginal gain of inserting a new element into a set 𝑇 is at most the marginal gain of inserting the same element into a subset of 𝑇 . According to [ 12 ] , function 𝜎(⋅ ) is submodular for both the IC model and the LT model . The main idea is as follows . When we add a new node 𝑥 into a seed set 𝑆 , the influence incurred by the node 𝑥 ( without considering the nodes in 𝑆 ) might overlap with that incurred by 𝑆 . The larger 𝑆 is , the more overlap might happen . Hence , the marginal gain is smaller on a ( larger ) set compared to that on any of its subsets . We formalize this statement with the following Property 1 . The proof can be found in [ 12 ] . Property 1 : Function 𝜎(⋅ ) is submodular for both the IC model and the LT model . To illustrate the concept of submodular functions , consider Figure 1 . Assume that a seed set 𝑇 is {𝐴𝑑𝑎} . Let a subset 𝑆 of 𝑇 be ∅ . We insert into seed sets 𝑇 and 𝑆 the same node 𝐵𝑜𝑏 . In fact , it is easy to calculate 𝜎(∅ ) = 0 , 𝜎({𝐴𝑑𝑎} ) = 3.57 , 𝜎({𝐵𝑜𝑏} ) = 2.64 and 𝜎({𝐴𝑑𝑎 , 𝐵𝑜𝑏} ) = 383 Consequently , we know the marginal gain of adding a new node 𝐵𝑜𝑏 into set 𝑇 , ie , 𝜎({𝐴𝑑𝑎 , 𝐵𝑜𝑏} ) − 𝜎({𝐴𝑑𝑎} ) = 0.26 , is smaller than that of adding 𝐵𝑜𝑏 into one of its subsets 𝑆 , ie , 𝜎({𝐵𝑜𝑏})− 𝜎(∅ ) = 264 In the 𝑘 MAX Influence problem , we have a submodular function 𝜎(⋅ ) which takes a set of seeds as an input and returns the expected number of influenced nodes incurred by the seed set as an output . Similarly , in the 𝐽 MIN Seed problem , we define a function 𝛼(⋅ ) which takes a set of influenced nodes as an input and returns the smallest number of seeds needed to influence these nodes as an output . One may ask : Is function 𝛼(⋅ ) also submodular ? Unfortunately , the answer is “ no ” which is formalized with the following Property 2 . Property 2 : Function 𝛼(⋅ ) is not submodular for both the
IC model and the LT model . Proof . We prove Property 2 by constructing a problem instance where 𝛼(⋅ ) does not satisfy the aforementioned conditions of a submodular function . We first discuss the case for the IC model . Consider the example as shown in Figure 2 . In this figure , there are four nodes , namely 𝑛1 , 𝑛2 , 𝑛3 and 𝑛4 . We assume that each edge is associated with its weight equal to 1 , which indicates that an influenced node 𝑢 will influence a non influenced node 𝑣 definitely when there is an edge from 𝑢 to 𝑣 . Let set 𝑇 be {𝑛1 , 𝑛3 , 𝑛4} and a subset of 𝑇 , says 𝑆 , be {𝑛3 , 𝑛4} . Obviously , when node 𝑛1 is influenced , it will further influence node 𝑛3 and node 𝑛4 , ie , all the nodes in 𝑇 will be influenced when 𝑛1 is selected as a seed . Thus , 𝛼(𝑇 ) = 1 . Similarly , we know that 𝛼(𝑆 ) = 1 . Now , we add node 𝑛2 into both 𝑇 and 𝑆 and then obtain 𝛼(𝑇 ∪ {𝑛2} ) = 2 ( by the seed set {𝑛1 , 𝑛2} ) and 𝛼(𝑆 ∪ {𝑛2} ) = 1 ( by the seed set {𝑛2} ) . As a result , we know that 𝛼(𝑇 ∪ {𝑛2} ) − 𝛼(𝑇 ) = 1 > 𝛼(𝑆 ∪ {𝑛2} ) − 𝛼(𝑆 ) = 0 , which , however , violates the conditions of a submodular function .
Next , we discuss the case for the LT model . Consider the special case where each node ’s threshold is equal to a value slightly greater than 0 . Consequently , a node will be influenced whenever one of its neighbors becomes influenced . The resulting diffusion process is actually identical to the special case for the IC model where the weights of all edges are 1s . That is , the example in Figure 2 can also be applied for the LT model . Hence , Property 2 also holds for the LT model .
Property 2 suggests that we cannot directly adapt existing techniques for the 𝑘 MAX Influence problem ( which involves a submodular function as an objective function ) to our 𝐽 MINSeed problem ( which involves a non submodular function as an objective function ) .
IV . INFLUENCE CALCULATION
We describe how we compute the influence of a given seed set ( ie , 𝜎(⋅ ) ) under the IC model ( Section IV A ) and the LT model ( Section IV B ) .
A . IC model
It has been proved in [ 15 ] that the process of calculating the influence given a seed set for the IC model is #P hard . That is , computing the exact influence is hard . Thus , we have to resort to approximate algorithms for efficiency .
Intuitively , the hardness of calculating the influence is due to the fact that the edges in the social network under the IC model are probabilistic in the sense that the propagation of influence via an edge happens with probability . In contrast , when the social network is deterministic , ie , the probability associated with each edge is exactly 1 , we only need to traverse the graph from each seed in a breadth first manner and return all visited nodes as the influenced nodes incurred by the seed set , thus resulting in a linear time algorithm for influence calculation . In view of the above discussion , we use sampling to calculate the ( approximate ) influence as follows . Let 𝐺(𝑉 , 𝐸 ) be the original probabilistic social network and 𝑆 be the seed set . Instead of calculating the influence on 𝐺 directly , we calculate the influence on each sampled graph from 𝐺 using the same seed set 𝑆 and finally average the incurred influences on all sampled graphs to obtain the approximate one for the original probabilistic graph . To obtain the sampled graph of 𝐺(𝑉 , 𝐸 ) each time , we keep the node set 𝑉 unchanged , remove the edge ( 𝑢 , 𝑣 ) with the probability of 1 − 𝑤𝑢,𝑣 for each edge ( 𝑢 , 𝑣 ) ∈ 𝐸 and assign each remaining edge with the weight equal to 1 . In this way , we can obtain that the probability that an edge ( 𝑢 , 𝑣 ) remains in the resulting graph is 𝑤𝑢,𝑣 . Note that the resulting sampled graph is deterministic . We call such a process as social network sampling .
Conceptually , given a probabilistic graph 𝐺(𝑉 , 𝐸 ) , each 𝐺 ’s sampled graph is generated with probability equal to a certain value . As a result , the influence calculated based on each 𝐺 ’s sampled graph has one specific probability to be equal to the exact influence on the original probabilistic graph 𝐺 given the same seed set . That is , the exact influence for 𝐺 is the expected influence for a sampled graph of 𝐺 . Based on this ,
430 we can use Hoeffding ’s Inequality to analyze the error incurred by our sampling method . We state our result with the following Lemma 1 .
Lemma 1 : Let 𝑐 be a real number between 0 and 1 . Given a seed set 𝑆 and a social network 𝐺(𝑉 , 𝐸 ) under the IC model , the sampling method stated above achieves a ( 1 ± 𝜖)approximation of the influence incurred by 𝑆 on 𝐺 with the confidence at least 𝑐 by performing the sampling process at least ( ∣𝑉 ∣−1)2𝑙𝑛(2/(1−𝑐 ) ) Proof : The proof can be found in our technical report [ 20 ] .
2𝜖2∣𝑆∣2 times .
Algorithm 1 Greedy Algorithm Framework Input : 𝐺(𝑉 , 𝐸 ) : a social network .
𝐽 : the required number of nodes to be influenced
Output : 𝑆 : a seed set . 1 : 𝑆 ← ∅ 2 : while 𝜎(𝑆 ) < 𝐽 do 3 : 4 : 5 : return 𝑆
𝑢 ← arg max𝑥∈𝑉 −𝑆(𝜎(𝑆 ∪ {𝑥} ) − 𝜎(𝑆 ) ) 𝑆 ← 𝑆 ∪ {𝑢}
B . LT model
Similar to the case under the IC model , the influence calculation for the LT model is much easier when the graph is deterministic ( ie , the threshold of each node has been specified before the process of influence spread ) . We illustrate the main idea as follows . For an influenced node 𝑢 , all we need to do is to add the corresponding influence to each of its non influenced neighbors and check whether each of its noninfluenced neighbors , says 𝑣 , has received enough influence ( 𝜃𝑣 ) to be influenced . If so , we change node 𝑣 to be influenced . Otherwise , we leave node 𝑣 non influenced . At the beginning , we initialize the set of influenced nodes to be the seed set 𝑆 . Then , we perform the above process for each influenced node until no new influenced nodes are generated .
With the view of the above discussion , we perform the influence calculation on a probabilistic graph for the LT model in the same way as the IC model except for the sampling method . Specifically , to sample a probabilistic graph 𝐺 under the LT model , we pick a real number from range [ 0 , 1 ] uniformly as the threshold of each node in 𝐺 to form a deterministic graph . We perform the sampling process multiple times , and for each resulting deterministic graph , we run the algorithm for a deterministic graph ( just described above ) to obtain the incurred influence . Finally , we average the influences on all sampled graphs to obtain the approximate influence .
Clearly , we can derive a similar lemma as Lemma 1 for the
LT model .
V . GREEDY ALGORITHM
We present in Section V A the framework of our greedy algorithm which finds a seed set by adding a seed into the seed set iteratively . Section V B provides the analysis of this algorithm framework , while Section V C discusses two different implementations of this algorithm framework .
A . Algorithm Framework
As proved in Section III , 𝐽 MIN Seed is NP hard . It is expected that there is no efficient exact algorithm for 𝐽 MINSeed . As discussed in Section I , if we want to solve 𝐽 MINSeed , a na¨ıve adaption of any existing algorithm originally designed for 𝑘 MAX Influence is time consuming . The major reason is that it executes an existing algorithm many times and the execution of this existing algorithm for an iteration is independent of the execution of the same algorithm for the next iteration . Motivated by this observation , we propose
431 a greedy algorithm which solves 𝐽 MIN Seed efficiently by executing an iteration based on the results from its previous iteration .
Specifically , we first initialize a seed set 𝑆 to be an empty set . Then , we select a non seed node 𝑢 such that the marginal gain of inserting 𝑢 into 𝑆 is the greatest and then we insert 𝑢 into 𝑆 . We repeat the above steps until at least 𝐽 nodes are influenced . Algorithm 1 presents this greedy algorithm framework .
This greedy algorithm is similar to the algorithm from [ 12 ] for 𝑘 MAX Influence except the stopping criterion , but they have different theoretical results . The stopping criterion in this greedy algorithm is 𝜎(𝑆 ) ≥ 𝐽 and the stopping criterion in the algorithm from [ 12 ] is ∣𝑆∣ ≥ 𝑘 where 𝑘 is a user parameter of 𝑘 MAX Influence . Note that our greedy algorithm for 𝐽MIN Seed has theoretical results which guarantee the number of seeds used while the algorithm for 𝑘 MAX Influence has theoretical results which guarantee the number of influenced nodes .
B . Theoretical Analysis
In this part , we show that the greedy algorithm framework in Algorithm 1 can return the seed set with both an additive error guarantee and a multiplicative error guarantee .
The greedy algorithm gives the following additive error bound .
Lemma 2 ( Additive Error Guarantee ) : Let ℎ be the size of the seed set returned by the greedy algorithm framework in Algorithm 1 and 𝑡 be the size of the optimal seed set for 𝐽MIN Seed . The greedy algorithm framework in Algorithm 1 gives an additive error bound equal to 1/𝑒 ⋅ 𝐽 + 1 . That is , ℎ− 𝑡 ≤ 1/𝑒⋅ 𝐽 + 1 . Here , 𝑒 is the natural logarithmic base .
Before we give the multiplicative error bound of the greedy algorithm , we first give some notations . Suppose that the greedy algorithm terminates after ℎ iterations . We denote 𝑆𝑖 to be the seed set maintained by the greedy algorithm at the end of iteration 𝑖 where 𝑖 = 1 , 2 , , ℎ . Let 𝑆0 denote the seed set maintained before the greedy algorithm starts ( ie , an empty set ) . Note that 𝜎(𝑆𝑖 ) < 𝐽 for 𝑖 = 1 , 2 , , ℎ − 1 and 𝜎(𝑆ℎ ) ≥ 𝐽 .
In the following , we give the multiplicative error bound of the greedy algorithm framework in Algorithm 1 . Lemma 3 ( Multiplicative Error Guarantee ) : Let 𝜎′(𝑆 ) = min{𝜎(𝑆 ) , 𝐽} . The greedy algorithm framework in Algorithm 1 is a ( 1 + min{𝑘1 , 𝑘2 , 𝑘3}) approximation of 𝐽 MINSeed , where 𝑘1 = ln 𝜎′(𝑆ℎ)−𝜎′(𝑆ℎ−1 ) , and
𝐽−𝜎′(𝑆ℎ−1 ) , 𝑘2 = ln
𝜎′(𝑆1 )
𝐽
𝜎′({𝑥} )
𝜎′(𝑆𝑖∪{𝑥})−𝜎′(𝑆𝑖)∣𝑥 ∈ 𝑉 , 0 ≤ 𝑖 ≤ ℎ , 𝜎′(𝑆𝑖 ∪
𝑘3 = ln(max{ {𝑥} ) − 𝜎′(𝑆𝑖 ) > 0} ) . C . Implementations As can be seen , the efficiency of Algorithm 1 relies on the calculation of the influence of a given seed set ( operator 𝜎(⋅) ) . However , the influence calculation process for the IC model is #P hard [ 15 ] . Under such a circumstance , we adopt the sampling method discussed in Section IV when using operator 𝜎(⋅ ) . We denote this implementation by Greedy1 .
In fact , we have an alternative implementation of Algorithm 1 as follows . Instead of sampling the social network to be deterministic when calculating the influence incurred by a given seed set , we can sample the social network to generate a certain number of deterministic graphs only at the beginning . Then , we solve the 𝐽 MIN Seed problem on each such deterministic graph using Algorithm 1 , where the cost of operator 𝜎(⋅ ) simply becomes the time to traverse the graph . At the end , we return the average of the sizes of the seed sets returned by the algorithm based on all samples ( deterministic graphs ) . We call this alternative implementation as Greedy2 .
VI . FULL COVERAGE
In some applications , we are interested in influencing all nodes in the social network . For example , a government wants to promote some campaigns like an election and an awareness of some infectious diseases . In these applications , 𝐽 is set to ∣𝑉 ∣ . We call this special instance of 𝐽 MIN Seed as FullCoverage . In Section VI A , we give some interesting observations and present an efficient algorithm on deterministic graphs for the IC model , while in Section VI B , we develop our probabilistic algorithm which can provide an arbitrarily small error for the IC model .
A . Full Coverage on Deterministic Graph ( IC Model )
According to Theorem 1 , in general , it is NP hard to solve the 𝐽 MIN Seed problem on a graph ( either probabilistic or deterministic ) for the IC model . However , on a deterministic graph for the IC model , Full Coverage is not NP hard yet easy to solve . In the following , we design an efficient algorithm to handle Full Coverage on a deterministic graph 𝐺(𝑉 , 𝐸 ) .
Before illustrating our efficient method for Full Coverage , we first introduce the following two observations .
Observation 1 : On a deterministic graph , if a node within a strongly connected component ( SCC ) is influenced , then it will influence all nodes in this SCC .
Observation 2 : Any node with in degree equal to 0 must be selected as a seed in order to be influenced . This is because it cannot be influenced by other nodes .
Based on the above two observations , we design our method called Decompose and Pick as follows .
At the first step , we decompose the deterministic graph into a number of strongly connected components ( SCCs ) , namely 𝑠𝑐𝑐1 , 𝑠𝑐𝑐2 , , 𝑠𝑐𝑐𝑚 . This step can be achieved by adopting some existing methods in the rich literature for finding all SCCs in a graph . In our implementation for this step , we adopt
432 as follows . Initially , 𝐸′
. If not , we insert ( 𝑣𝑖 , 𝑣𝑗 ) into 𝐸′ the SCC computation algorithm developed by Kosaraju et al [ 21 ] , which runs in 𝑂(∣𝑉 ∣ + ∣𝐸∣ ) time . For the second step , we construct a new graph 𝐺(𝑉 ′ , 𝐸′ ) based on 𝐺(𝑉 , 𝐸 ) . Specifically , for constructing 𝑉 ′ , we create a new node 𝑣𝑖 for each SCC 𝑠𝑐𝑐𝑖 obtained in Step 1 . We construct 𝐸′ is set to an empty set . For each ( 𝑢 , 𝑣 ) ∈ 𝐸 , we find the SCC containing 𝑢 ( 𝑣 ) in 𝐺(𝑉 , 𝐸 ) , says 𝑠𝑐𝑐𝑖 ( 𝑠𝑐𝑐𝑗 ) . Then , we find the node 𝑣𝑖 ( 𝑣𝑗 ) representing 𝑠𝑐𝑐𝑖 ( 𝑠𝑐𝑐𝑗 ) in 𝐺(𝑉 ′ , 𝐸′ ) . We check whether ( 𝑣𝑖 , 𝑣𝑗 ) ∈ 𝐸′ . Clearly , the cost is 𝑂(∣𝑉 ∣ ) , while the cost for generating for constructing 𝑉 ′ is 𝑂(∣𝐸∣ ⋅ 𝐶𝑐ℎ𝑒𝑐𝑘 ) , where 𝐶𝑐ℎ𝑒𝑐𝑘 indicates the cost for 𝐸′ checking whether a specific edge ( 𝑣𝑖 , 𝑣𝑗 ) has been constructed before in 𝐸′ . 𝐶𝑐ℎ𝑒𝑐𝑘 depends on the structure for storing 𝐺(𝑉 ′ , 𝐸′ ) . Specifically , 𝐶𝑐ℎ𝑒𝑐𝑘 is 𝑂(1 ) when 𝐺(𝑉 ′ , 𝐸′ ) is stored in an adjacency matrix . With this data structure , the overall cost for Step 2 is 𝑂(∣𝑉 ∣+∣𝐸∣ ) . In case that 𝐺(𝑉 ′ , 𝐸′ ) is maintained in an adjacency list , 𝐶𝑐ℎ𝑒𝑐𝑘 becomes 𝑂(∣𝐸′∣ ) ( bounded by 𝑂(∣𝐸∣) ) , resulting in Step 2 ’s complexity equal to 𝑂(∣𝑉 ∣ + ∣𝐸∣2 ) in the worst case . To further reduce the complexity of Step 2 in this case , we do not check the existence of each newly formed edge in the new graph every time we create a new edge . Instead , we create all newly formed edges and perform sorting on all the newly formed edges to filter out any redundant edges in 𝐸′ , thus yielding the cost of Step 2 equal to 𝑂(∣𝑉 ∣ + ∣𝐸∣ ⋅ log ∣𝐸∣ ) . Note that there exist no SCCs in the constructed graph 𝐺′(𝑉 ′ , 𝐸′ ) . For the last step , we simply pick the nodes with in degree equal to 0 in 𝐺(𝑉 ′ , 𝐸′ ) and for each such node 𝑣𝑖 , we insert into the seed set 𝑆 a node randomly from its corresponding 𝑠𝑐𝑐𝑖 in the original 𝐺(𝑉 , 𝐸 ) . Since there exist no SCCs in 𝐺′(𝑉 ′ , 𝐸′ ) , it is possible to perform a topological sort on 𝐺′(𝑉 ′ , 𝐸′ ) . Hence , the seed set consisting of all the nodes with in degree equal to 0 in 𝐺′(𝑉 ′ , 𝐸′ ) would influence all nodes in 𝐺′(𝑉 ′ , 𝐸′ ) . Since each node in 𝐺′(𝑉 ′ , 𝐸′ ) corresponds to a SCC structure in 𝐺(𝑉 , 𝐸 ) , according to Observation 2 , we conclude that the seed set 𝑆 constructed at the last step would influence ∣𝑉 ∣ nodes in 𝐺(𝑉 , 𝐸 ) ( deterministic ) . Clearly , the cost of Step 3 is 𝑂(∣𝑉 ′∣+∣𝐸′∣ ) ( DFS/BFS ) , which is bounded by 𝑂(∣𝑉 ∣ + ∣𝐸∣ ) . In summary , the worst case time complexity of Decomposeand Pick is 𝑂(∣𝑉 ∣ + ∣𝐸∣ ) and 𝑂(∣𝑉 ∣ + ∣𝐸∣ ⋅ log ∣𝐸∣ ) when the new graph is maintained in an adjacency matrix and an adjacency list , respectively .
B . Full Coverage on Probabilistic Graph ( IC Model )
At it this moment , is quite straightforward to perform our probabilistic algorithm for Full Coverage based on social network sampling and Decompose and Pick as follows . We first use social network sampling to generate a certain number of deterministic graphs . Then , on each such deterministic graph , we run Decompose and Pick to obtain its corresponding seed set which covers all the nodes in the social network and the corresponding size of the seed set . At the end , we average the sizes of the seed sets obtained for all samples ( deterministic graphs ) to approximate the solution of Full Coverage on a general ( probabilistic ) social network . Again , using Hoeffding ’s Inequality , for a real number 𝑐 between 0 and 1 , we can provide users with a ( 1 ± 𝜖) approximation solution for any positive real number 𝜖 with confidence at least 𝑐 by performing the sampling process at least ( ∣𝑉 ∣−1)2𝑙𝑛(2/(1−𝑐 ) ) times . The proof is similar to that of Lemma 1 .
2𝜖2
VII . EMPIRICAL STUDY
We set up our experiments in Section VII A and give the corresponding experimental results in Section VII B .
A . Experimental Setup
We conducted our experiments on a 2.26GHz machine with 4GB memory under a Linux platform . All algorithms were implemented in C/C++ .
1 ) Datasets : We used four real datasets for our empirical study , namely HEP T , Epinions , Amazon and DBLP . HEP T is a collaboration network generated from “ High Energy PhysicsTheory ” section of the e print arXiv ( http://wwwarXivorg ) In this collaboration network , each node represents one specific author and each edge indicates a co author relationship between the two authors corresponding to the nodes incident to the edge . The second one , Epinions , is a who trust whom network at Epinions.com , where each node represents a member of the site and the link from member 𝑢 to member 𝑣 means that 𝑢 trusts 𝑣 ( ie , 𝑣 has a certain influence on 𝑢 ) . The third real dataset , Amazon , is a product co purchasing network extracted from Amazon.com with nodes and edges representing products and co purchasing relationships , respectively . We believe that product 𝑢 has an influence on product 𝑣 if 𝑣 is purchased often with 𝑢 . Both of Epinions and Amazon are maintained by Jure Leskovec . Our last real dataset , DBLP , is another collaboration network of computer science bibliography database maintained by Michael Ley . We summarize the features of the above real datasets in Table I . For efficiency , we ran our algorithms on the samples of the aforementioned real datasets with the sampling ratio equal to one percent . The sampling process is done as follows . We randomly choose a node as the root and then perform a breadth first traversal ( BFT ) from this root . If the BFT from one root cannot cover our targeted number of nodes , we continue to pick more new roots randomly and perform BFTs from them until we obtain our expected number of nodes . Next , we construct the edges by keeping the original edges between the nodes traversed .
2 ) Configurations : ( 1 ) Weight generation for the IC model : We use the QUADRIVALENCY model to generate the weights . Specifically , for each edge , we uniformly choose a value from set {0.1 , 0.25 , 0.5 , 0.75} , each of which represents minor , low , medium and high influence , respectively . ( 2 ) Weight generation for the LT model : For each node 𝑢 , let 𝑑𝑢 denote its in degree , we assign the weight of each edge to 𝑢 as 1/𝑑𝑢 . In this case , each node obtains the equivalent influence from each of its neighbors . ( 3 ) No . of Times for Sampling : For each influence calculation under both the IC model and the LT model , we perform the graph sampling process 10000 times by default . ( 4 ) Parameter 𝐽 : In the following , we denote
TABLE I
STATISTICS OF REAL DATASETS
Dataset No . of Nodes No . of Edges
HEP T 15233 58891
Epinions 75888 508837
Amazon 262111 1234877
DBLP 654628 1990259 parameter 𝐽 as a relative real number between 0 and 1 denoting the fraction of the influenced nodes among all nodes in the social network ( instead of an absolute positive integer denoting the total number of influenced nodes ) because a relative measure is more meaningful than an absolute measure in the experiments . We set 𝐽 to be 0.5 by default . Alternative configurations considered are {0.1 , 0.25 , 0.5 , 0.75 , 1} .
3 ) Algorithms : We compare our greedy algorithm with several other common heuristic algorithms . We list all the algorithms studied in our experiments as follows . ( 1 ) Greedy1 : We denote our first implementation of Algorithm 1 by Greedy1 . As stated before , we only conduct the graph sampling process when performing the influence calculation . ( 2 ) Greedy2 : Greedy2 corresponds to the alternative implementation of Algorithm 1 . ( 3 ) Degree heuristic : We implemented this baseline algorithm using the heuristic of nodes’ out degree . Specifically , we repeatedly pick the node with the largest out degree yet un covered and add it into the seed set until the incurred influence exceeds the threshold . We denote this heuristic algorithm as Degree heuristic . ( 4 ) Centrality heuristic : Centralityheuristic indicates another heuristic algorithm based on the nodes’ distance centrality . In sociology , distance centrality is a common measurement of nodes’ importance in a social network based on the assumption that a node with short distances to other nodes would probably have a higher chance to influence them . In Centrality heuristic , we select the seeds in a decreasing order of nodes’ distance centralities until the requirement of influencing at least 𝐽 nodes is met . ( 5 ) Random : Finally , we consider the method of selecting seeds from the un covered nodes at random as a baseline . Correspondingly , we denote it by Random .
In the experiment , we do not compare our algorithms with the na¨ıve adaption of an existing algorithm for 𝑘 MAXInfluence described in Section I because this na¨ıve adaption is time consuming as discussed in Section V .
B . Experiment Results
For the sake of space , we show the results for the IC model only . The results for the LT model can be found in [ 20 ] .
1 ) No . of Seeds : We measure the quality of the algorithm for 𝐽 MIN Seed by using the number of seeds returned by the algorithm . Clearly , the fewer the seeds an algorithm returns , the better it is .
We study the qualities of the five aforementioned algorithms by comparing the number of seeds returned by them . Specifically , we vary parameter 𝐽 from 0.1 to 1 . The experimental results are shown in Figure 3 . Consider the results on HEP T ( Figure 3(a ) ) as an example . We find algorithms Greedy1 and Greedy 2 are comparable in terms of quality . Both of them outperform other heuristic algorithms significantly . Similar results can be found in other real datasets .
433
) s d e e S f o r e b m u N
( r o r r
E
Greedy1 Greedy2 Additive error bound
60
40
20
0
0.1
0.25
0.75
1
0.5 J
) s d e e S f o
. o N f o o i t a R
( r o r r
E
Greedy Multiplicative error bound
6
4
2
0
0.1
0.25
0.75
1
0.5
J
( a ) Additive Error
( b ) Multiplicative Error
Fig 5 . Error Analysis ( IC Model )
2 ) Running Time : We explore the efficiency of different algorithms by comparing their running times . Again , we vary 𝐽 , and for each setting of 𝐽 , we record the corresponding running time of each algorithm .
According to the results shown in Figure 4 , we find that Greedy1 is the slowest algorithm . The reason is that Greedy1 selects the seeds by calculating the marginal gain of each nonseed at each iteration and then picking the one with the largest marginal gain while other heuristic algorithms simply choose the non seed with the best heuristic value ( eg , out degree and centrality ) . However , the alternative implementation of our greedy algorithm , ie , Greedy2 , shows its advantage in terms of efficiency . Greedy2 is faster than Greedy1 because the total cost of sampling in Greedy2 is much smaller than that in Greedy1 . Besides , Random is slower than Greedy2 , though the cost of choosing a seed in Random is 𝑂(1 ) . This is because Random usually has to select more seeds than Greedy2 in order to incur the same amount of influence and for each iteration , Random also needs to calculate the influence incurred by the current seed set .
3 ) Error Analysis : To verify the error bounds derived in this paper , we also conducted the experiments which compare the number of seeds returned by our algorithms with the optimal one on small datasets ( 0.5 % of the HEP T dataset ) . We performed Brute Force searching to obtain the optimal solution . According to the results in Figure 5(a ) , the additive errors incurred by our algorithms are generally much smaller than the theoretical error bounds on the real dataset . In Figure 5(b ) , we find that the multiplicative error of our greedy algorithm grows slowly when 𝐽 increases . Besides , we discover that 𝑘2 is the smallest among 𝑘1 , 𝑘2 and 𝑘3 in most cases of our experiments . That is , the multiplicative bound becomes ( 1 + 𝑘2 ) ( ie , ( 1 + ln 𝜎′(𝑆ℎ)−𝜎′(𝑆ℎ−1) ) ) in these cases . Based on this , we can explain the phenomenon in Figure 5(b ) that the theoretical multiplicative error bound does not change too much when we increase 𝐽 from 0.75 to 1 .
𝜎′(𝑆1 )
4 ) Full Coverage Experiments : We conducted experiments for Full Coverage and the corresponding results can be found in [ 20 ] .
Conclusion : Greedy1 and Greedy2 both give the smallest seed set compared with other algorithms Degree Heuristic , Centrality Heuristic and Random . In addition , the difference between the size of a seed set returned by Greedy1 or Greedy2 and the minimum ( optimal ) seed size is significantly smaller than the theoretical bound . Besides , Greedy2 performs faster than Greedy1 .
VIII . CONCLUSION
In this paper , we propose a new viral marketing problem called 𝐽 MIN Seed , which has extensive applications in real world . We then prove that 𝐽 MIN Seed is NP hard under two popular diffusion models ( ie , the IC model and the LT model ) . To solve 𝐽 MIN Seed effectively , we develop a greedy algorithm , which can provide approximation guarantees . Besides , for the special setting where 𝐽 is equal to the number of all users in the social network ( ie , Full Coverage ) , we design other efficient algorithms . Finally , we conducted extensive experiments on real datasets , which verified the effectiveness and efficiency of our greedy algorithm . For future work , we plan to study the properties of our new problem under diffusion models other than the IC model and the LT model . Finding other solutions of Full Coverage for the LT model is another interesting direction . Acknowledgements : The research is supported by HKRGC GRF 621309 and Direct Allocation Grant DAG11EG05G .
REFERENCES
[ 1 ] J . Bryant and D . Miron , “ Theory and research in mass communication , ”
Journal of communication , vol . 54 , no . 4 , pp . 662–704 , 2004 .
[ 2 ] J . Nail , “ The consumer advertising backlash , ” Forrester Research , 2004 . [ 3 ] I . R . Misner , The World ’s best known marketing secret : Building your business with word of mouth marketing . Bard Press , 2nd edition , 1999 . [ 4 ] A . Johnson , “ nike tops list of most viral brands on facebook twitter , ”
2010 . [ Online ] . Available : http://wwwkikabinkcom/news/
[ 5 ] M . Granovetter , “ Threshold models of collective behavior , ” The Ameri can Journal of Sociology , vol . 83 , no . 6 , pp . 1420–1443 , 1978 .
[ 6 ] T . C . Schelling , Micromotives and macrobehavior . WW Norton and
Company , 2006 .
[ 7 ] J . Goldenberg , B . Libai , and E . Muller , “ Talk of the network : A complex systems look at the underlying process of word of mouth , ” Marketing Letters , vol . 12 , no . 3 , pp . 211–223 , 2001 .
[ 8 ] —— , “ Using complex systems analysis to advance marketing theory development : Modeling heterogeneity effects on new product growth through stochastic cellular automata , ” Academy of Marketing Science Review , vol . 9 , no . 3 , pp . 1–18 , 2001 .
[ 9 ] D . Gruhl , R . Guha , D . Liben Nowell , and A . Tomkins , “ Information diffusion through blogspace , ” in WWW , 2004 .
[ 10 ] H . Ma , H . Yang , M . R . Lyu , and I . King , “ Mining social networks using heat diffusion processes for marketing candidates selection , ” in CIKM , 2008 .
[ 11 ] P . Domingos and M . Richardson , “ Mining the network value of cus tomers , ” in KDD , 2001 .
[ 12 ] D . Kempe , J . Kleinberg , and E . Tardos , “ Maximizing the spread of influence through a social network , ” in SIGKDD , 2003 .
[ 13 ] J . Leskovec , A . Krause , C . Guestrin , C . Faloutsos , J . VanBriesen , and N . Glance , “ Cost effective outbreak detection in networks , ” in SIGKDD , 2007 .
[ 14 ] M . Kimura and K . Saito , “ Tractable models for information diffusion in social networks , ” PKDD , 2006 .
[ 15 ] W . Chen , C . Wang , and Y . Wang , “ Scalable influence maximization for prevalent viral marketing in large scale social networks , ” in SIGKDD , 2010 .
[ 16 ] S . Datta , A . Majumder , and N . Shrivastava , “ Viral marketing for multiple products , ” in ICDM , 2010 .
[ 17 ] M . Richardson and P . Domingos , “ Mining knowledge sharing sites for viral marketing , ” in SIGKDD , 2002 .
[ 18 ] S . Bharathi , D . Kempe , and M . Salek , “ Competitive influence maximization in social networks , ” Internet and Network Economics , pp . 306–311 , 2007 .
[ 19 ] T . Carnes , C . Nagarajan , S . M . Wild , and A . van Zuylen , “ Maximizing influence in a competitive social network : a follower ’s perspective , ” in Proceedings of the ninth international conference on Electronic commerce . ACM , 2007 , pp . 351–360 .
434
Greedy1 Greedy2 Random Degree heuristic Centrality heuristic
104
103
102
101
100 s d e e s f o r e b m u N
0.1
0.25
0.5
J
0.75
1
( a ) HEP T
Greedy1 Greedy2 Random Degree heuristic Centrality heuristic
104
) s ( e m i i t g n n n u R
103
102
101
100
0.1
0.25
0.5
J
Greedy1 Greedy2 Random Degree heuristic Centrality heuristic
105
104
103
102
101 s d e e S f o r e b m u N
100
0.1
0.25
0.75
1
0.5
J s d e e S f o r e b m u N
106 105 104 103 102 101 100
0.1
Greedy1 Greedy2 Random Degree heuristic Centrality heuristic
Greedy1 Greedy2 Random Degree heuristic Centrality heuristic s d e e S f o r e b m u N
106 105 104 103 102 101
0.25
0.5
J
0.75
1
0.1
0.25
0.75
1
0.5
J
( b ) Epinions
( c ) Amazon
( d ) DBLP
Fig 3 . Number of Seeds ( IC Model )
Greedy1 Greedy2 Random Degree heuristic Centrality heuristic
) s ( e m i i t g n n u R
106
105
104
103
Greedy1 Greedy2 Random Degree heuristic Centrality heuristic
108 107 106 105 104 103 102
) s ( e m i i t g n n n u R
Greedy1 Greedy2 Random Degree heuristic Centrality heuristic
109 108 107 106 105 104 103
) s ( e m i i t g n n n u R
0.75
1
0.1
0.25
0.75
1
0.1
0.25
0.5
J
0.75
1
0.1
0.25
0.5
J
0.75
1
0.5
J
( a ) HEP T
( b ) Epinions
( c ) Amazon
( d ) DBLP
Fig 4 . Running Time ( IC Model )
[ 20 ] C . Long and R . C W Wong , “ Minimizing seed set for viral marketing , ” [ Online ] . Available : http://wwwcseusthk/∼raywong/paper/J
2011 . MIN Seed technical.pdf
[ 21 ] T . H . Cormen , C . E . Leiserson , R . L . Rivest , and C . Stein , Introduction to algorithms . The MIT press , 2009 .
[ 22 ] G . L . Nemhauser , L . A . Wolsey , and M . L . Fisher , “ An analysis of approximations for maximizing submodular set functions I , ” Mathematical Programming , vol . 14 , no . 1 , pp . 265–294 , 1978 .
[ 23 ] L . A . Wolsey , “ An analysis of the greedy algorithm for the submodular set covering problem , ” COMBINATORIA , vol . 2 , no . 4 , pp . 385–393 , 1981 .
APPENDIX : PROOF OF LEMMAS/THEOREMS
Proof of Lemma 2 . Firstly , we give the theoretical bound on the influence for 𝑘 MAX Influence . The problem of determining the 𝑘 element set 𝑆 ⊂ 𝑉 that maximizes the value of 𝜎(⋅ ) is NP hard . Fortunately , according to [ 22 ] , a simple greedy algorithm can solve this maximization problem with the approximation factor of ( 1− 1/𝑒 ) by initializing an empty set 𝑆 and iteratively adding the node such that the marginal gain of inserting this node into the current set 𝑆 is the greatest one until 𝑘 nodes have been added . We present this interesting tractability property of maximizing a submodular function in Lemma 4 as follows .
Lemma 4 ( [22] ) : For a non negative , monotone submodular function 𝑓 , we obtain a set 𝑆 of size 𝑘 by initializing set 𝑆 to be an empty set and then iteratively adding the node 𝑢 one at a time such that the marginal gain of inserting 𝑢 into the current set 𝑆 is the greatest . Assume that 𝑆∗ is the set with 𝑘 elements that maximizes function 𝑓 , ie , the optimal 𝑘 element set . Then , 𝑓(𝑆 ) ≥ ( 1− 1/𝑒)⋅ 𝑓(𝑆∗ ) , where 𝑒 is the natural logarithmic base .
Secondly , we derive the additive error bound on the seed set size for 𝐽 MIN Seed based on the aforementioned bound . As discussed in Section III , 𝜎(⋅ ) is submodular . Clearly , 𝜎(⋅ ) is also non negative and monotone . The framework in Algorithm 1 involves a number of iterations ( lines 2 4 ) where the size of the seed set 𝑆 is incremented by one for each iteration . We say that the framework in Algorithm 1 is at stage
435
𝑗 if the seed set 𝑆 contains 𝑗 seeds at the end of an iteration . The seed set 𝑆 at stage 𝑗 is denoted by 𝑆𝑗 . Consequently , according to Lemma 4 , at each stage 𝑗 , we conclude that
𝜎(𝑆𝑗 ) ≥ ( 1 − 1/𝑒 ) ⋅ 𝜎(𝑆∗ 𝑗 )
( 1 ) 𝑗 is the set that provides the maximum value of 𝜎(⋅ ) where 𝑆∗ over all possible seed sets of size 𝑗 .
Note that the total number of stages for the greedy process is equal to ℎ ( ie , the size of the seed set returned by the algorithm ) . That is , the greedy process stops at stage ℎ . Thus , we know that 𝜎(𝑆ℎ ) ≥ 𝐽 and the greedy solution for 𝐽 MINSeed is 𝑆ℎ . Consider the last two stages , namely stage ℎ − 1 and stage ℎ . We know that 𝜎(𝑆ℎ−1 ) < 𝐽 and 𝜎(𝑆ℎ ) ≥ 𝐽 . Since 𝜎(𝑆∗
ℎ ) ≥ 𝜎(𝑆ℎ ) , we have 𝜎(𝑆∗
ℎ ) ≥ 𝐽 .
Now , we want to explore the relationship between ℎ and 𝑡 .
Note that the following inequality holds .
𝑡 ≤ ℎ
( 2 ) Consider two stages , stage 𝑖 and stage 𝑖 + 1 , such that 𝜎(𝑆𝑖 ) < ( 1 − 1/𝑒 ) ⋅ 𝐽 while 𝜎(𝑆𝑖+1 ) ≥ ( 1 − 1/𝑒 ) ⋅ 𝐽 . According to Inequality ( 1 ) , we know 𝜎(𝑆∗ 𝑖 ) < 𝐽 . ( This is 𝑖 ) ≥ 𝐽 , then we have 𝜎(𝑆𝑖 ) ≥ ( 1 − 1/𝑒 ) ⋅ 𝐽 because if 𝜎(𝑆∗ with Inequality ( 1 ) , which contradicts 𝜎(𝑆𝑖 ) < ( 1 − 1/𝑒 ) ⋅ 𝐽 ) . As a result , we have the following inequality
𝑡 > 𝑖
( 3 ) due to the monotonicity property of 𝜎(⋅ ) . According to Inequality ( 2 ) and Inequality ( 3 ) , we obtain 𝑡 ∈ [ 𝑖+1 , ℎ ] . That is , the additive error of our greedy algorithm ( ie , ℎ− 𝑡 ) is bounded by the number of stages between stage 𝑖+1 and stage ℎ . Since 𝜎(𝑆𝑖+1 ) ≥ ( 1−1/𝑒)⋅𝐽 and 𝜎(𝑆ℎ−1 ) < 𝐽 , the difference of the influence incurred between stage 𝑖 + 1 and stage ℎ−1 is bounded by 𝐽−(1−1/𝑒)⋅𝐽 = 1/𝑒⋅𝐽 . Since each stage increases at least 1 influenced node ( seed itself ) , it is easy to see that the number of stages between stage 𝑖 + 1 and stage ℎ − 1 is at most 1/𝑒 ⋅ 𝐽 . Consequently , the number of stages between stage 𝑖+1 and stage ℎ is at most 1/𝑒⋅𝐽 +1 . As a result , ℎ − 𝑡 ≤ 1/𝑒 ⋅ 𝐽 + 1 . Proof of Lemma 3 . This proof involves four parts . In the first part , we construct a new problem 𝑃 ′ based on the submodular function 𝜎′(⋅ ) ( instead of 𝜎(⋅) ) . In the second part , we show the multiplicative error bound of the greedy algorithm in Algorithm 1 ( using 𝜎′(⋅ ) instead of 𝜎(⋅ ) ) for this new problem 𝑃 ′ . We denote this adapted greedy algorithm by 𝐴′ . For simplicity , we denote the original greedy algorithm in Algorithm 1 using 𝜎(⋅ ) by 𝐴 . In the third part , we show that this new problem is equivalent to the 𝐽 MIN Seed problem . In the fourth part , we show that the multiplicative error bound deduced in the second part can be used as the multiplicative error bound of algorithm 𝐴 for 𝐽 MIN Seed . 𝜎′(𝑆 ) = min{𝜎(𝑆 ) , 𝐽} . Problem 𝑃 ′
Firstly , we construct a new problem 𝑃 ′ as follows . Note that is formalized as follows . arg min{∣𝑆∣ : 𝜎′(𝑆 ) = 𝜎′(𝑉 ) , 𝑆 ⊆ 𝑉 } .
( 4 )
𝑥∈𝑆 𝑔(𝑥 ) for problem 𝑃 ′
Secondly , we show the multiplicative error bound of algorithm 𝐴′ Lemma 5 ( [23] ) : Given problem arg min{∑ by using the following Lemma 5 [ 23 ] . : 𝑓(𝑆 ) = 𝑓(𝑈 ) , 𝑆 ⊆ 𝑈} where 𝑓 is a nondecreasing and submodular function defined on subsets of a finite set 𝑈 , and 𝑔 is a function defined on 𝑈 . Consider the greedy algorithm that selects 𝑥 in 𝑈 − 𝑆 such that ( 𝑓(𝑆 ∪{𝑥})− 𝑓(𝑆))/𝑔(𝑥 ) is the greatest and adds it into 𝑆 at each iteration . The process stops when 𝑓(𝑆 ) = 𝑓(𝑈 ) . Assume that the greedy algorithm terminates after ℎ iterations and let 𝑆𝑖 denote the seed set at iteration 𝑖 ( 𝑆0 = ∅ ) . The greedy algorithm provides a ( 1 + min{𝑘1 , 𝑘2 , 𝑘3}) approximation of the above problem , where 𝑘1 = ln 𝑓 ( 𝑈 )−𝑓 ( ∅ ) 𝑓 ( 𝑈 )−𝑓 ( 𝑆ℎ−1 ) , 𝑘2 = ln 𝑓 ( 𝑆1)−𝑓 ( ∅ ) 𝑓 ( 𝑆ℎ)−𝑓 ( 𝑆ℎ−1 ) , and 𝑓 ( 𝑆𝑖∪{𝑥})−𝑓 ( 𝑆𝑖)∣𝑥 ∈ 𝑈 , 0 ≤ 𝑖 ≤ ℎ , 𝑓(𝑆𝑖 ∪ 𝑘3 = ln(max{ 𝑓 ( {𝑥})−𝑓 ( ∅ ) {𝑥} ) − 𝑓(𝑆𝑖 ) > 0} ) . as follows . It is easy to verify that 𝜎′(⋅ ) is a non decreasing and submodular function defined on subsets of a finite set 𝑉 . We set 𝑈 to be 𝑉 and set 𝑓(⋅ ) to be 𝜎′(⋅ ) . We also define 𝑔(𝑥 ) to be 1 for 𝑥∈𝑆 𝑔(𝑥 ) = ∣𝑆∣ . We re write each 𝑥 ∈ 𝑉 ( or 𝑈 ) . Note that Problem 𝑃 ′
We apply the above lemma for problem 𝑃 ′
( 4 ) as follows .
∑ arg min{∑ 𝑥∈𝑆
𝑔(𝑥 ) : 𝜎′(𝑆 ) = 𝜎′(𝑉 ) , 𝑆 ⊆ 𝑉 } .
( 5 )
The above form of problem 𝑃 ′ is exactly the form of the problem described in Lemma 5 . Suppose that we adopt the greedy algorithm in Algorithm 1 for problem 𝑃 ′ by using 𝜎′(⋅ ) instead of 𝜎(⋅ ) , ie , algorithm 𝐴′ . It is easy to verify that algorithm 𝐴′ follows the steps of the greedy algorithm described in Lemma 5 ( ie , selecting the node 𝑥 such that ( 𝜎′(𝑆 ∪ {𝑥} ) − 𝜎′(𝑆))/𝑔(𝑥 ) is the greatest where 𝑔(𝑥 ) is exactly equal to 1 ) . By Lemma 5 , the greedy algorithm 𝐴′ gives ( 1 + min{𝑘1 , 𝑘2 , 𝑘3}) approximation of for problem 𝑃 ′ problem 𝑃 ′ 𝐽−𝜎′(𝑆ℎ−1 ) , 𝑘2 = ln 𝜎′(𝑆1)−𝜎′(∅ ) 𝜎′(𝑆ℎ)−𝜎′(𝑆ℎ−1 ) , and 𝑘3 =
, where 𝑘1 = ln 𝜎′(𝑉 )−𝜎′(∅ ) 𝜎′(𝑆1 ) 𝜎′(𝑆ℎ)−𝜎′(𝑆ℎ−1 ) = ln
𝜎′(𝑉 )−𝜎′(𝑆ℎ−1 ) = ln
𝐽
𝜎′({𝑥} ) ln(max{ 𝜎′(𝑆𝑖 ) > 0} ) .
𝜎′(𝑆𝑖∪{𝑥})−𝜎′(𝑆𝑖)∣𝑥 ∈ 𝑉 , 0 ≤ 𝑖 ≤ ℎ , 𝜎′(𝑆𝑖 ∪ {𝑥} ) − is equivalent to the 𝐽∑ MIN Seed problem which can be formalized as follows ( since
Thirdly , we show that problem 𝑃 ′ 𝑥∈𝑆 𝑔(𝑥 ) = ∣𝑆∣ ) .
𝑔(𝑥 ) : 𝜎(𝑆 ) ≥ 𝐽 , 𝑆 ⊆ 𝑉 } .
( 6 ) arg min{∑ 𝑥∈𝑆 for the new problem 𝑃 ′
In the following , we show that the set of all possible solutions for the problem in form of ( 6 ) ( ie , the 𝐽 MIN Seed problem ) is equivalent to the set of all possible solutions for the problem in form of ( 5 ) ( ie , problem 𝑃 ′ ) . Note that the objective functions in both problems are equal . The remaining issue is to show that the constraints for one problem are the same as those for the other problem . Suppose that 𝑆 is a solution for the problem in form of ( 6 ) . We know that 𝜎(𝑆 ) ≥ 𝐽 and 𝑆 ⊆ 𝑉 . We derive that 𝜎′(𝑆 ) = 𝐽 . Since 𝜎′(𝑉 ) = 𝐽 , we have 𝜎′(𝑆 ) = 𝜎′(𝑉 ) and 𝑆 ⊆ 𝑉 ( which are the constraints for the problem in form of ( 5) ) . Suppose that 𝑆 is a solution for the problem in form of ( 5 ) . We know that 𝜎′(𝑆 ) = 𝜎′(𝑉 ) and 𝑆 ⊆ 𝑉 . Since 𝜎′(𝑉 ) = 𝐽 , we have 𝜎′(𝑆 ) = 𝐽 . Considering 𝜎′(𝑆 ) = min{𝜎(𝑆 ) , 𝐽} , we derive that 𝜎(𝑆 ) ≥ 𝐽 . So , we have 𝜎(𝑆 ) ≥ 𝐽 and 𝑆 ⊆ 𝑉 ( which are the constraints for the problem in form of ( 6) ) . Fourthly , we show that the size of the solution ( ie , ∣𝑆∣ ) returned by algorithm 𝐴′ is equal to that returned by algorithm 𝐴 for 𝐽 MIN Seed . Since 𝜎(𝑆𝑖 ) < 𝐽 for 1 ≤ 𝑖 ≤ ℎ − 1 , we know that 𝜎′(𝑆𝑖 ) = 𝜎(𝑆𝑖 ) for 1 ≤ 𝑖 ≤ ℎ − 1 . We also know that the element 𝑥 in 𝑉 − 𝑆𝑖−1 that maximizes 𝜎(𝑆𝑖−1 ∪ {𝑥} ) − 𝜎(𝑆𝑖−1 ) ( which is chosen at iteration 𝑖 by algorithm 𝐴 ) would also be the element that maximizes 𝜎′(𝑆𝑖−1 ∪ {𝑥} ) − 𝜎′(𝑆𝑖−1 ) ( which is chosen at ) for 𝑖 = 1 , 2 , , ℎ − 1 . That is , iteration 𝑖 by algorithm 𝐴′ algorithm 𝐴′ would proceed in the same way as algorithm 𝐴 at iteration 𝑖 = 1 , 2 , , ℎ−1 . Consider iteration ℎ of algorithm 𝐴 . We denote the element selected by algorithm 𝐴 by 𝑥ℎ . Then , we know 𝜎(𝑆ℎ−1 ∪ {𝑥ℎ} ) ≥ 𝐽 since algorithm 𝐴 stops at iteration ℎ . Consider iteration ℎ of algorithm 𝐴′ . This iteration is also the last iteration of 𝐴′ . This is because there exists an element 𝑥 in 𝑉 − 𝑆ℎ−1 such that 𝜎′(𝑆ℎ−1 ∪{𝑥} ) = 𝜎′(𝑉 )(= 𝐽 ) ( since 𝑥 can be equal to 𝑥ℎ where 𝜎′(𝑆ℎ−1 ∪{𝑥ℎ} ) = 𝐽 ) . Note that this element 𝑥 maximizes 𝜎′(𝑆ℎ−1∪{𝑥})−𝜎′(𝑆ℎ−1 ) and thus is selected by 𝐴′ . We conclude that both algorithms 𝐴 and 𝐴′ terminates at iteration ℎ . Since the number of iterations for an algorithm ( 𝐴 or 𝐴′ ) corresponds to the size of the solution returned by the algorithm , we deduce that the size of the solution returned by algorithm 𝐴′ is equal to that returned by algorithm 𝐴 . In view of the above discussion , we know that problem 𝑃 ′ is equivalent to 𝐽 MIN Seed and algorithm 𝐴′ for problem 𝑃 ′ would proceed in the same way as algorithm 𝐴 for 𝐽 MINSeed . As a result , the multiplicative bound of algorithm 𝐴′ for problem 𝑃 ′ in the second part also applies to algorithm 𝐴 ( ie , the greedy algorithm in Algorithm 1 ) for 𝐽 MIN Seed .
436
