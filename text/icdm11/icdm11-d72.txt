2011 11th IEEE International Conference on Data Mining
Signature Pattern Covering via
Local Greedy Algorithm and Pattern Shrink
Hyungsul Kim , Sungjin Im , Tarek Abdelzaher , Jiawei Han
David Sheridan , Shobha Vasudevan
Department of Computer Science
Department of Electrical and Computer Engineering
University of Illinois at Urbana Champaign
{hkim21 , im3 , zaher , hanj}@illinois.edu
University of Illinois at Urbana Champaign
{dsherid3 , shobhav}@illinois.edu
Abstract—Pattern mining is a fundamental problem that has a wide range of applications . In this paper , we study the problem of finding a minimum set of signature patterns that explain all data . In the problem , we are given objects where each object has an itemset and a label . A pattern is called a signature pattern if all objects with the pattern have the same label . This problem has many interesting applications such as assertion mining in hardware design and identifying failure causes from various log data . We show that the previous pattern mining methods are not suitable for mining signature patterns and identify the problems . Then we propose a novel pattern enumeration method which we call Pattern Shrink . Our method is strongly coupled with another novel method that is very similar to finding a local optimum with a negligible loss in performance . Our proposed methods show a speedup of more than ten times over the previous methods . Our methods are flexible enough to be extended to mining high confidence patterns , instead of signature patterns .
Keywords Pattern Covering ; Set Covering ; Signature Min ing ; Local Greedy Algorithm
I . INTRODUCTION
Discovering invariant properties from raw data is one of the most fundamental problems that have a wide range of applications and a huge impact . For example , failing to detect a hardware bug before mass producing the hardware could cost hardware manufacturers millions of dollars and a lot of precious time in such a fast moving industry . Naturally , there have been a large amount of effort on testing and verification of hardware designs . However , such a crucial task has become substantially challenging with the increasing complexity of modern hardware .
One method that has become a powerful tool for design and verification is assertion based verification [ 12 ] , [ 13 ] , [ 19 ] . An assertion is a statement of design intent which should hold true for any input stimulus . Assertions are used in many stages of the hardware design cycle including design verification [ 5 ] and post silicon validation [ 5 ] , [ 6 ] . However , this verification methodology has a disadvantage that it can be very difficult for a human to come up with a minimal set of assertions that properly verifies the design . Also , it can be difficult for a human to thoroughly reason with a design , especially when the design involves several layers of hierarchy and substantial complexity in a timely manner . Thus pattern mining tools such as GoldMine [ 25 ] that automatically generate assertions were proposed .
Another popular problem is to identify causes from labeled data . For example , DustMiner [ 16 ] uses data mining technique to find patterns from the sensor network logs of both success and failure cases to discover the causes of its failures . In software development , the causes of the program failures can be identified by mining patterns only shown in the failure traces [ 9 ] , [ 31 ] . In chemical compound analysis , pattern mining is also used to identify unique chemical substructures that give some properties of the compounds . For example , the DTP AIDS Antiviral Screen program [ 1 ] has tested a set of 43576 chemical compounds for evidence of anti HIV activity . The database has three labels on the compounds – confirmed active ( CA ) , confirmed moderately active(CM ) , and confirmed inactive ( CI ) . Using pattern mining on the database , chemical substructures that only appear in the CA compounds can be identified and suggested to be further investigated by additional experiments [ 11 ] , [ 24 ] .
In this paper , we study the problem of mining signature patterns in objects in the given datasets . We say that a pattern is a signature pattern if such a pattern appears with a single label or in a single class . Signature patterns are the extreme case of emerging patterns [ 10 ] , [ 18 ] whose supports increase significantly from one class to another . Intuitively , if one finds a signature pattern in an object , one can immediately determine its label . Signature pattern mining is general enough to capture all applications and examples above . For example , in the case of assertion mining , the class label of an object is the value of the output variable .
Our goal is to discover a set of signature patterns that explains all of the objects in datasets . In other words , we want to discover a set of signature patterns such that each object has at least one signature pattern . In addition , we want to minimize the set of signature patterns . This will summarize the vast amount of information and make important properties easier to find .
An important challenge in mining a minimum set of signature patterns is to design efficient mining algorithms . Since there are exponential number of signature patterns ,
1550 4786/11 $26.00 © 2011 IEEE DOI 101109/ICDM2011131
330 if we mine all signature patterns first , those patterns will exceed memory and also slow down the process of pattern selection when we find a minimum set of signature patterns .
Similar problems have been observed in discriminative pattern mining [ 8 ] , [ 17 ] , [ 23 ] , [ 26 ] . In discriminative pattern mining , they directly mine a set of discriminative patterns , instead of mining all frequent patterns and then selecting a set of frequent patterns as discriminative patterns . Such a direct mining is an iterative algorithm , in which one or a few patterns are mined and added to its result pattern set in each iteration . The direct mining , however , is turned out to be an order of magnitude slower than the two step approach in the signature pattern covering problem , contrasting to the results in discriminative pattern mining . The main reason is due to the redundant visits on the pattern search space , an inevitable result of any iterative pattern mining method .
In this study , we analyze the unexpected performance of the direct mining algorithm and develop a new direct mining algorithm for finding a minimum set of signature patterns . Since finding a minimum set of signature patterns is very similar to the set covering problem , one of the NP hard problems , we cannot obtain an optimal solution in polynomial time . For the set covering problem , there is a wellknown greedy algorithm that has the best approximation ratio . Instead of using the traditional greedy algorithm , our algorithm uses a new greedy algorithm which has an almost same approximation ratio as the traditional greedy algorithm . As a result , our algorithm reduces the number of redundant visits on the pattern search space , thereby improving the efficiency of the direct mining by more than an order of magnitude factor . We also propose pattern shrink method which enumerate patterns by shrinking patterns rather than growing . Most of the signature patterns are infrequent due to its strong requirement that a signature pattern must appear in only a single class . Thus traditional pattern mining techniques that have focused on finding frequent patterns inherently suffer from inefficient pattern enumeration and search space exploration . This motivates us to propose a new pattern enumeration method , called pattern shrink to reduce the pattern search space size .
Our contributions in this paper can be summarized as follows :
1 ) We propose a new greedy algorithm for the set covering problem with a negligible loss in approximation factor , and use it as a submodule for our direct pattern mining algorithm . Our method reduces the number of redundant visits on the pattern search space more than two orders of magnitude .
2 ) We develop a new pattern enumeration method , called pattern shrink method that prunes the pattern search space better than the traditional pattern growth method .
II . PROBLEM DEFINITION
} . Each object oi
⊆ I of items and a label yi
In order to formally define the problem of finding a minimum set of signature patterns , we need to define some notations . In the problem , we are given a set I of items and a set L of labels . We are also given a set of objects , O = {o1 , o2 , . . . , on is a pair ∈ L , that is of a set Xi oi = ( Xi , yi ) . Notation wise , we may use X(o ) and y(o ) to denote the itemset and label of the object o , respectively ; hence X(oi ) = Xi and y(oi ) = yi . A pattern P is a subset of I . The support of P , denoted by sup(P ) , is defined to be the set of objects whose itemsets are supersets of P , ie sup(P ) := {o ∈ O | X(o ) ⊇ P} . We say a pattern P is a signature pattern if all objects in sup(P ) have the same label . In other words , if we find a signature pattern P in an object o ( more precisely X(o) ) , we can immediately determine the object ’s label y(o ) . We say that a pattern P covers an object o if P ⊆ X(o ) . A set of signature patterns Q is called a signature pattern cover if all objects in O are covered by at least one pattern in Q . The size of a signature pattern cover is naturally defined to be |Q| . Our goal is to find a minimum sized signature pattern cover . We call the problem the signature pattern covering problem ( SPCP ) because it is very similar to the set covering problem , one of the well known NP hard problems . The relation between the two problems will be discussed in the following section .
For the problem to be well defined , we require that the objects are consistent in the following sense : For any two objects oi and oj such that Xi to see that any collection of signature patterns cannot cover both oi and oj if yi
⊆ Xj , yi = yj . It is easy ff= yj .
To have a feel of the problem , consider the following toy example .
Object o1 o2 o3 o4 o5 o6 o7 o8
Itemset {A , B , C , D} {A , C , D , F } {A , C , E} {A , C , E , F } {B , C , D} {B , C , E , F } {B , D , E , F } {C , D , E , F }
Label
+
+ − − +
+ − −
Consider the pattern P = {B , C} . The support of P , sup(P ) , is {o1 , o5 , o6} . Since all objects in sup(P ) have the same label “ + ” , P is a signature pattern . In contrast , a pattern {B , D} is not a signature pattern because its support is {o1 , o5 , o7} , and o1 and o7 have different labels . Consider a set of the patterns Q = {P1 , P2 , P3 , P4} where P1 = {A , C , E} , P2 = {D , E} , P3 = {A , C , D} , and P4 = {B , C} . It is easy to check that Q is a signature pattern cover because all the patterns in Q are signature patterns , and all objects are covered by at least one pattern in Q . For
331
Algorithm 1 Global Greedy Algorithm
.
S∈C S ff= U do
Snew = argmaxS .∈S | fi . C ← C ∪ {Snew
}
' ∪ Sfi|
S∈C S
1 : C = ∅
2 : while 3 :
4 : 5 : end while
6 : return C example , o4 and o6 are covered by P1 and P4 respectively . In the example , Q is a minimum signature pattern cover .
III . BACKGROUND AND RELATED WORK
In this section we give a quick overview of the set covering problem and the connection to the SPCP . We will also go over several approaches that were previously proposed for pattern mining . Finally , we will discuss why previous approaches are not suitable for signature pattern mining .
A . Set Covering and Greedy Algorithm
The set covering problem is a well known NP hard problem . In the problem , we are given a universe of elements ,
U = {e1 , e2 , . . . , en } , and a collection of subsets of elements S = {S1 , S2 , . . . , Sm ⊆ U . A set S is said to cover an element e if e ∈ S . We say that a collection C ⊆ S of subsets of items is a set cover if S∈C S = U . The set covering problem is to find a minimum sized set cover . In other words , the goal is to cover all elements in U using the smallest number of sets in S . Since the set covering
} where Si
. problem is NP hard , we cannot obtain an optimal solution in polynomial time . One of the best approximation algorithms is the greedy algorithm described in Algorithm 1 . In each iteration , the algorithm increases the cover C by adding one set to S that covers the largest number of elements that are not currently covered by any set in C . Throughout this paper , we will refer the algorithm to the global greedy algorithm since at each time it adds a “ best ” set from the given collection S . log n approximation , ie |C C greedy and C and the optimal set cover respectively . greedy
It is well known that the global greedy algorithm is a
| , where opt are the set returned by the greedy algorithm
| ≤ log n · |C opt
B . Signature Pattern Covering
The signature pattern cover problem is in structure very similar to the set cover problem and indeed can be viewed as a special case of the set covering problem as follows . Let
U := {o1 , o2 , , on } be the collection of all objects in O . Let S = {sup(P ) | P is a signature pattern} . Then finding a minimum signature pattern cover is equivalent to finding a minimum set cover .
However there is an important difference between the two problems when it comes to designing efficient algorithms . In
332 the set covering problem , the collection S of subsets of items is given as input . To the contrary , signature patterns are not pre given , thus have to be mined . One direction in obtaining the minimum signature pattern cover is to first generate all signature patterns and then find the minimum cover using Algorithm 1 . Since this approach completely separates two tasks of building all patterns and of finding the minimum cover , it is called “ two step approach . ”
Although the two step approach is simple and intuitive , the first step of enumerating patterns could be very timeconsuming and require a lot of memory when there are a large number of signature patterns . A large set of signature patterns also severely slows down the set covering algorithm in the second step .
To address this issue , one may consider an alternative approach called “ one step approach ” , or direct mining that was proposed in the problem of discriminative pattern mining [ 7 ] , [ 8 ] , [ 17 ] . The one step approach directly mines discriminative patterns , instead of finding all frequent patterns and selecting a small number of discriminative pattern among them . This type of algorithms were explored in [ 8 ] , [ 17 ] , [ 20 ] , [ 23 ] , [ 22 ] . Their algorithms iteratively find a best pattern in terms of their defined gain function , which is used to measure its discriminative power . During the pattern search that finds a best pattern in each iteration , a pattern in consideration is further extended only when its extension is promising . Algorithm 2 gives a high level description of the one step approach .
The coverage gain of the pattern P for a given set of patterns Q , denoted by gain(P ;Q ) , is defined as the number of uncovered objects that are covered by P , but not by any pattern in Q . The function gainBound(P ;Q ) gives an upper bound on the coverage gain of the pattern P . That is , no extended patterns of P have a coverage gain of more than gainBound(P ;Q ) . The efficiency gain comes from the pruning of pattern search space by the function gainBound . We implemented the two step and one step approaches for solving the assertion mining problem , one of the problems that were used in our evaluation . The assertion mining is a special case of SPCP in the domain of hardware design .
Figure 1 shows the comparison of the performance of the two approaches on synthetic datasets ; the experiments will be discussed in detail in Section V . Surprisingly , the onestep approach performs two orders of magnitude slower than the two step approach as shown in the first graph . Knowing that the one step approach prunes more search space by using a gain function and its bound function , this result is very unexpected , and opposed to the results observed in discriminative pattern mining [ 8 ] , [ 26 ] , [ 23 ] , [ 17 ] .
The poor performance of the one step approach can be explained by its repeated search space traversals . Due to its iterative nature , the one step approach might visit some search space multiple times . In Figure 1 , the second graph shows the number of nodes in the search space in each itera
103
102
101
100
10−1
10−2
Algorithm
● One−step Two−step
●
●
●
●
●
●
●
●
●
●
●
●
Algorithm
One−step Two−step
8
6
4
2
0
) i
K ( e z S e c a p S h c r a e S
) c e s ( i e m T g n n n u R i
Algorithm
One−step Two−step
8
6
4
2
0
) i
K ( e z S e c a p S h c r a e S d e t l a u m u C
6
14 The Number of Input Variables
10
12
8
16
5
10
15
Iteration
20
25
30
5
10
15
Iteration
20
25
30
Figure 1 . The Unexpectedly Poor Performance of the One step Approach
Algorithm 2 One step Approach Global variables : maxGain , maxP at procedure one step approach 1 : Q = ∅
7 : end while branch and bound({},Q )
2 : while there exists an uncovered object do 3 : maxGain = 0 4 : maxP at = null 5 : 6 : Q = Q ∪ {maxP at} 8 : return Q function branch and bound(P,Q ) 1 : for P fi ∈ {extended patterns of P} do if gain(P ;Q ) > maxGain then is a signature pattern then if P fi
2 :
3 : maxGain = gain(P fi;Q ) maxP at = P fi
4 :
5 :
6 :
7 :
8 :
9 : end if else if gainBound(P fi;Q ) > maxGain then branch and bound(P fi,Q )
10 : end if end if 11 : 12 : end for tion for the one step approach . Since the two step approach is not an iterative algorithm , the total search space size is plotted for the sake of comparison . As shown in the graph , the search space of the one step approach in each iteration is an order of magnitude smaller than the search space of the two step approach . This means that the search space pruning works effectively . However , as shown in the third graph , the visited search space size grows over iterations finally to three times the total search space size of the two step approach . The problem of the redundant exploration of search space arises also in discriminative pattern mining . To address the problem of the redundant exploration of search space in [ 8 ] , [ 26 ] , they keep eliminating some objects in each
333 iteration assuming that they are covered enough by the chosen patterns . This approach is , however , not applicable to the signature pattern covering , because we need to keep all objects to verify signature patterns . In [ 23 ] , [ 17 ] , the entire search space in previous iterations is stored in memory and reused to avoid the redundant visits to the search space . This method , however , requires a huge memory to store the whole search space .
So far , we discussed the problems of the two step approach : 1 ) the search space in the first step is too large to enumerate , and 2 ) the large number of signature patterns generated in the first step may exceed memory and substantially slow down the set covering algorithm in the second step . Also we observed the one step approach suffers from the redundant visits to the search space due to its iterative nature . These shortcomings of previous approaches for the signature pattern covering problem motivated us to develop a new one step approach which is presented in the next section .
IV . METHODOLOGY
A . Local Greedy Algorithm
In this section , we propose a new greedy algorithm , and new pattern enumeration method to address the issues we discussed in the previous section .
Recall that the problem of the one step approach does too many redundant pattern enumerations over iterations . We need to first understand why so many redundant visits are unavoidable in the algorithm .
In each iteration , the algorithm looks for the best pattern in terms of its coverage gain . Thus in order to find the best signature pattern , it needs to consider all possible patterns . Even though gainBound function helps prune out unpromising patterns , we still need to make sure that the best pattern has a better coverage gain than all other patterns . Because the best pattern should be superior to all other patterns , it inevitably results in a number of redundant visits in the search space . This is the first key observation we made . As an analogy in Algebra , finding the best signature pattern is similar to finding a global maximum for a given function .
Global Greedy Algorithm
P1
P4
P8
Local Greedy Algorithm uncovered object o
P1
P4
P8
P9
P7
P2
P3
P2
P3
P9
P7
P5
P6
P5
P6
Figure 2 . Global Greedy Algorithm and Local Greedy Algorithm
Algorithm 3 Local Greedy Algorithm for Set Covering
.
S∈C S ff= U do
Randomly pick an uncovered element e S∈C S
Snew = argmaxS .:S .∈S,e∈S . | fi . C ← C ∪ {Snew
}
' ∪ Sfi|
1 : C = ∅
2 : while 3 :
4 :
5 : 6 : end while
7 : return C
Finding a global maximum is much more challenging than finding a local maximum , which can be done by comparing the function values near the local maximum . Motivated by this observation , we propose a new greedy algorithm for the set covering problem in Algorithm 3 .
In each iteration , Algorithm 3 first finds an uncovered element e . Then it looks for the best among the sets with e such that the best set covers the maximum number of uncovered elements . By the restriction that the best set should include the uncovered element e , only a small number of sets are considered in each iteration . Since the algorithm considers a different element e in each iteration , intuitively it will explore a fairly different group of sets in every iteration . Hence we name Algorithm 3 the local greedy algorithm because the exploration is restricted to a collection of sets with e .
In addition to its reduced search space in each iteration , Algorithm 3 has essentially the same approximation guarantee as Algorithm 1 .
Theorem 1 . The local greedy algorithm gives an O(log n)approximation with probability at least 1 − 1/n .
} . Let ei
Proof : Consider a fixed optimal solution , C∗ = ∈ U be the element that is con1 , S∗ 2 , , S∗ ∩ U| ≥ |U|/2k , otherwise small . Further , we say that j having ei ,
{S∗ sidered in the ith iteration . We say that S∗ |S∗ ei is good if there exists a large optimal set S∗ is large if k j j otherwise bad . If ei is good , a new set Ai being added , newly covers at least |U|/2k elements in U . Notice that there are at most |U|/2 bad elements in U , since a small optimal set has at most |U|/2k uncovered elements . Since ei is randomly chosen from U , we know that ei is good with probability at least 1/2 . Note that the local greedy algorithm successfully covers all elements before it considers 2k log n good elements ei , since each good uncovered element decreases |U| by a factor of at most 1 − 1/2k : ( 1 − 1 2k )2k log nn < 1 . We know that the probability that the local greedy algorithm does not finish until the ( 16k log n)th iteration is bounded by the probability that there are at most ( 2k log n ) good elements among e1 , e2 , , e16k log n , which is at most 1 nk by Chernoff inequality . Since the optimal set cover C∗ has size k , the theorem follows .
We note that it is necessary that the local greedy algorithm considers a randomly chosen uncovered element . Otherwise there exists an instance for which the algorithm has approximation ratio Ω( n ) . Although the analysis is asymptotically tight , we note that by considering multiple uncovered elements and picking the best in each iteration , the algorithm can slightly improve its approximation factor .
√
By replacing the global greedy set cover algorithm in Algorithm 2 with the local greedy set cover algorithm , we obtain a new one step approach .
The benefit of the new one step approach with the local greedy algorithm is illustrated in Figure 2 . In Figure 2 , there are objects in two classes , and the classes are represented by a red circle and a blue cross . Also , there are patterns
{P1 , P2 , . . . , P9} where their supports are indicated by their is respective circle in Figure 2 . Suppose that no object covered yet . If we use the global greedy set cover algorithm , we need to find a signature pattern that covers maximum number of uncovered objects . Thus we need to check all the patterns from P1 to P9 to find a signature pattern with the maximum coverage gain , which is P6 . On the other hand , in the local greedy algorithm , we first pick an uncovered object
334
Algorithm 4 Pattern Shrink Global variables : maxGain , maxP at procedure one step approach 1 : Q = ∅ 2 : while there exists an uncovered object o do 3 : maxGain = 0 4 : maxP at = null 5 : 6 : Q = Q ∪ {maxP at} 8 : return Q function branch and bound(P , o,Q ) 1 : for P fi ∈ {shrunk patterns of P} do branch and bound(P , o,Q )
7 : end while if P fi is a signature pattern then if gain(P fi;Q ) > maxGain then maxGain = gain(P fi;Q ) maxP at = P fi end if if gainBound(P fi;Q ) > maxGain then branch and bound(P fi , o,Q ) end if
2 :
3 :
4 :
5 :
6 :
7 :
8 :
9 : o and find a signature pattern with the maximum coverage gain that covers o . The second diagram in Figure 2 shows that we only need to consider P1 , P2 , and P4 , since they are the only patterns that cover o . Hence we can significantly reduce the pattern search space by using the local greedy algorithm .
B . Pattern Shrink
Another novel method we propose together with the local greedy set cover algorithm is a new pattern enumeration method pattern shrink . In the traditional pattern mining , patterns are enumerated by starting with an empty pattern and by being extended in a canonical order [ 14 ] , [ 21 ] , [ 27 ] , [ 30 ] , [ 28 ] , [ 29 ] . This pattern enumeration is called pattern growth [ 14 ] .
In the signature pattern covering problem , we are interested only in signature patterns . If we start with an empty pattern , we have to extend the pattern until it becomes a signature pattern because it will cover less objects with further extension . Instead of starting with an empty pattern , we can start with the largest pattern and shrink it . In the signature pattern covering , the question is which pattern is the largest one . In the local greedy algorithm , the uncovered object o is the largest pattern that covers o . Thus , we can start with the uncovered object o and shrink it until it becomes not a signature pattern .
For better understanding , consider the toy example used in Section II . In an iteration of the local greedy algorithm , let o7 be the uncovered object picked . Figure 3 shows the search space of patterns which covers o , and indicates the search directions of pattern growth and pattern shrink . The red patterns are the signature patterns . Figure 3 has dotted lines between two patterns Pi and Pj if Pi can be extended to Pj by adding one more item . As shown in the figure , the signature patterns colored in red are much fewer than other patterns , and are located closer from the uncovered object o than from the empty pattern .
Figure 3 also shows the search trees of the search spaces visited by pattern growth and pattern shrink , respectively . The first tree that represents the search space of pattern growth starts with the empty pattern which is {} . Then it extends the pattern by adding B . Adding A is not considered here because o should be covered . Next it extends the pattern further by adding D , and adding E . At the point that the pattern becomes {B , D , E} , the pattern becomes a signature comes back to the pattern {B} , and extends it again by adding E . It prunes only two patterns , {B , D , E , F} and {D , E , F} . pattern , where it stops extending the pattern . The algorithm
The second tree in Figure 3 represents the search space of pattern shrink . It starts with an object o whose itemset is a signature pattern . It first shrinks the pattern by removing F to {B , D , E} . Then it further shrinks by removing E . Since the pattern {B , D} is not a signature pattern , we
335 end if 10 : 11 : end for stop shrinking further . Then it goes back to the pattern
{B , D , E} , and shrinks it by removing D . In total , six patterns are pruned .
Pattern growth visits all the non signature patterns , while pattern shrink visits all the signature patterns . Since in many applications , the number of signature patterns is much smaller than the number of non signature patterns , pattern shrink can perform substantially faster than pattern growth . In the opposite case , patterns shrink still performs as fast as pattern growth . Signature patterns are generally very small in such a case , and thus the pattern covering has only small number of signature patterns . Therefore , the computational cost is not expensive for both methods as we will see in the experiments with high confidence patterns in Section V .
The pattern shrink algorithm is described in Algorithm 4 .
V . EXPERIMENTS
We evaluated the performance of our algorithm on hardware chip design datasets and UCI Machine Learning classification datasets . The algorithms we implemented and compared include : the pattern shrink method with the local greedy algorithm ( PatShrink ) , the two step algorithm ( TwoStep ) , the pattern growth method with the global greedy algorithm ( PatGrowthG ) , and the pattern growth method with the local greedy algorithm ( PatGrowthL ) . By implementing PatGrowthG and PatGrowthL , we were able to see
Empty Condition
{ }
Pattern Shrink
{ B }
{ D }
{ E }
{ F }
Pattern Growth
{ }
Pattern Shrink
{ B , D , E , F}
{ B , D }
{ B , E }
{ B , F }
{ D , E }
{ D , F }
{ E , F }
{ B , D }
{ B , E }
{ B , F }
{ D , E }
{ D , F }
{ E , F }
{ B , D }
{ B , E }
{ D , E }
{ B , F }
{ D , F }
{ E , F }
{ B }
{ D }
{ E }
{ F }
{ B , D , E }
{ B , D , F }
{ B , E , F }
{ D , E , F } pruned
{ B , D , E }
{ B , D , F}
{ B , E , F}
{ D , E , F }
{ B , D , E } { B , D , F }
{ B , E , F }
{ D , E , F } pruned pruned pruned
{ B }
{ D }
{ E } pruned
{ F }
Pattern Growth
{ B , D , E , F}
Uncovered Object o pruned
{ B , D , E , F }
{ }
Figure 3 . Pattern Growth vs . Pattern Shrink the respective efficiency gain that the local greedy algorithm and pattern shrink method give .
A . Hardware Chip Design Datasets
As discussed in the introduction , it is one of interesting applications of the signature pattern covering to discover invariant properties in hardware designs , which is called assertion mining . A hardware design has a set of input variables {x1 , x2 , . . . , xn
} and an output variable y . For
) c e s ( i e m T g n n n u R i
103
102
101
100
10−1
10−2
Algorithm
● PatGrowthG PatGrowthL TwoStep PatShrink
●
●
●
●
●
●
●
●
●
8
●
●
●
6
10
Input Size
12
14
16 i e z S e c a p S h c r a e S
108
106
104
102
●
●
●
●
●
●
●
Algorithm
● PatGrowthG PatGrowthL TwoStep PatShrink
●
●
●
●
●
6
8
10
Input Size
12
14
16 the design , we are given as input a set of traces . We can naturally convert each trace into an object as follows . For each variable xi , there is exactly one corresponding item Xi , and there are two possible labels 0 and 1 . We interpret xi as the indicator variable that shows if Xi is in the itemset or not , and the y value as the object ’s label . For example , the trace {x1 = 1 , x2 = 0 , x3 = 0 , x4 = 1 , y = 0} is translated into {X 1 } with the label of 0 . Since there could
1 , X 0
2 , X 0
3 , X 1 4 be exponentially many traces , the traces are given in the format of a binary decision diagram ( BDD ) , a compact tree representation of a boolean function . three
We used different modules : the writeback stage module ( wb stage ) from the Rigel 1000+ core processor[15 ] , the cache controller ( b100 ) from the OpenRisc CPU [ 2 ] , and memory management unit ( mmu ) from the Sun OpenSparc T2 [ 3 ] . Each module has several output variables for which assertions are generated . To find a set of assertions for each output variable as small as possible , we ran the implemented algorithms for signature pattern covering . Note that a minimum signature cover implies a minimum set of system invariants that satisfy all possible traces in the given hardware design . The names of the datasets used in the experiments are in the format <module name>.<output variable name> .
The wb stage module is a small module having 4 8 input variables for each output variable , b100 is a medium size of module having 11 27 input variables for each output variable , and mmu is a relatively large module , consisting of 25 26 input variables . Efficiency Evaluation : Table I shows the running time of the four algorithms for the chip design datasets . PatShrink is
336
Figure 4 . The Performance on Synthetic Datasets an order of magnitude faster than TwoStep and PatGrowthL and two orders of magnitude faster than PatGrowthG . The difference between PatGrowthG and PatGrowthL and between PatGrowthL and PatShrink confirms that the efficiency gains come from both local greedy algorithm and patternshrink . The visited search space size of each algorithm in Table I also confirms that both local greedy algorithm and pat shrink reduce the redundant visits in the search space . In order to see the performance with the growing number of input variables , we generated synthetic datasets . For each n , the number of input variables , we randomly generated a decision tree with 2n−3 leaf nodes . The boolean function corresponding to the randomly generated decision tree was encoded in BDD and given as input . The result in Figure 4 confirms that PatShrink is more than an order of magnitude faster than other methods . Effectiveness of Signature Pattern Covers : To evaluate the quality of the signature pattern covers , we used a decisiontree based rule mining method as the baseline . A decision tree is a very popular method that is used in classification , or to generate rules from data . We used GoldMine [ 25 ] that mines a set of assertions by building a decision tree [ 25 ] . Table II shows the number of assertions generated by GoldMine ( DT ) , the global greedy algorithm ( Global ) , and the local greedy algorithm ( Local ) .
In general , the decision tree method generates more assertions than other greedy methods .
THE RUNNING TIMES(SEC ) AND SEARCH SPACE SIZE IN THE CHIP DESIGN DATASETS
Table I
Chip Design
TwoStep
PatGrowthG
PatGrowthL
PatShrink
TwoStep
PatGrowthG
PatGrowthL
PatShrink wb stage.wb valid0 wb stage.wb valid1 wb stage.wb data0 wb stage.wb data1 b100.r cache inhibit b100.r hitmiss eval b100.r load b100.r saved addr mmu.mmu asi mra not sca mmu.mmu dae tid mmu.mmu asi tid
0.0015 0.0016 0.0017 0.0024 0.0025 0.0028 0.0202 N/A 1.3009 N/A N/A
0.0015 0.0020 0.0022 0.0044 0.0076 0.007 0.0849 T/O 1.114 291.8 817.4
0.0016 0.0018 0.0019 0.0027 0.0039 0.0027 0.0244 31460.0 0.6021 29.69 115.0
0.0015 0.0018 0.0018 0.0025 0.0024 0.0019 0.0048 4222.0 0.1102 1.377 1.828
29 79 82 167 1160 1804 19,207 N/A 611,506 N/A N/A
43 120 183 344 996 1215 14,830 T/O 119,333 38,481,324 114,110,199
28 74 103 177 450 297 5,691 1,781,021,358 100,712 6,002,302 23,914,556
23 71 94 242 173 84 303 23,253,215 6,147 54,074 78,744
THE HARDWARE CHIP DESIGN DATASETS AND THE SIZE OF
SIGNATURE PATTERN COVERS
Table II
Decision Tree
0/100
Local Greedy
0/100
Chip Design
DT
Global wb stage.wb valid0 wb stage.wb valid1 wb stage.wb data0 wb stage.wb data1 b100.r cache inhibit b100.r hitmiss eval b100.r load b100.r saved addr mmu.mmu asi mra not sca mmu.mmu dae tid mmu.mmu asi tid
7 11 15 21 25 13 60 2752 45 121 128
6 10 11 15 14 10 18 2090 26 79 82
Local
6 10 13 17 14 9 18 2089 31 93 92
There was no significant difference found between the global and local greedy algorithm in their cover size . However , for mmu.mmu dae tid and mmu.mmu asi tid , the local greedy algorithm performed worse than the global greedy algorithm . As mentioned in Section IV , the local greedy algorithm can be improved by randomly choosing two ( or more ) uncovered elements instead of one element in each iteration , and selecting the pattern that has better coverage gain . In our experiments , choosing two uncovered elements in the local greedy algorithm improved the results of the last three datasets from 31 , 93 and 92 to 27 , 85 and 88 , respectively .
Subjective Designer Rankings : To evaluate the quality of each assertion in the signature pattern covers , we asked the designer of wb stage modules to rank a set of assertions generated by the decision tree method and the local greedy algorithm . The designer was not informed of the difference between the two sets of assertions . The rankings were assigned from 1 to 3 ; Rank 1 Uninteresting assertions , the designer would not use in testing , Rank 2 Somewhat interesting , the designer might use in testing , and Rank 3 Very interesting , the designer would likely use in testing .
The results in Figure 5 show that the local greedy algorithm have much higher percentage of Rank 3 assertions than the decision tree method .
337
80
20
Ranking
80
20
Ranking
Rank 1 Rank 2 Rank 3
Rank 1 Rank 2 Rank 3
60
40
60
40
Figure 5 . Subjective Rankings on Assertions
B . UCI Classification Datasets
In order to support the effectiveness of our methods , we performed further experiments on the datasets from the UCI Machine Learning Repository . As we did with the hardware chip design datasets , we converted the objects features into itemsets . Continuous attributes were discretized . The results of these additional experiments show that our methods are not restricted to assertion mining , but can be used for the general signature pattern mining . Performance Evaluation : Figure 6 shows the running times and search space sizes of the four algorithms on the 17 datasets . The datasets are sorted in increasing order of the running time of TwoStep algorithm . For all 17 datasets , PatShrink ourperformed all other methods . The third graph in Figure 6 compares the size of signature pattern covers generated by the global greedy algorithm and the local greedy algorithm . For each dataset , both greedy algorithms generated a very similar number of signature patterns . Reduced Redundant Visits on Search Space : Interestingly , there is an outstanding peak in the running time curve of PatGrowthG algorithm in Figure 6 . This peak is matched with the peaks in the search space size and cover size of the algorithm in Figure 6 . However , two other iterative algorithms PatGrowthL and PatShrink show a steady performance without such a peak . A possible explanation for this is as follows . When the minimum size of the signature pattern
Algorithm
● PatGrowthG PatGrowthL TwoStep PatShrink
500
400
300
200
100
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
Algorithm
● PatGrowthG PatGrowthL TwoStep PatShrink
80
60
40
20
●
●
●
●
●
●
●
●
●
●
●
●
Algorithm Global Local
●
●
●
●
●
150
100
50
0 e z S i r e v o C
)
M
( i e z S e c a p S h c r a e S
) c e s ( i e m T g n n n u R i labor iris breast wine cleve zoo anneal hepati tic−tac horse UCI Datasets austra crx germ an vehicle sonar auto hypo labor iris breast wine cleve zoo anneal hepati tic−tac horse UCI Datasets austra crx germ an vehicle sonar auto hypo labor iris breast wine cleve zoo tic−tac anneal horse hepati UCI Datasets austra crx vehicle germ an sonar
Figure 6 . The Performance on the UCI Datasets auto hypo
●
Auto
Algorithm
● PatGrowthG PatGrowthL TwoStep PatShrink
●
●
●
●
●
●
●
●
●
250
200
150
100
50 i
) c e s ( e m T g n n n u R i
●
●
) c e s ( i e m T g n n n u R i
Sonar
Algorithm
● PatGrowthG PatGrowthL TwoStep PatShrink
140
120
100
80
60
40
20 cover is large , or equivalently when the signature patterns in the cover are large , PatGrowthG is forced to examine a lot of patterns since it repeatedly makes redundant searches over a large number of iterations . This experiment supports our claim that the local greedy algorithm successfully reduces the number of redundant visits in the pattern search space .
C . High Confidence Patterns i(P )|
|sup(P )| maxi |sup i(P )| is the size of the support of P with the label
In practice , real world data may have some noise , thus it may not be desirable to mine only signature patterns , which appear in a single class . A natural relaxation is to mine a minimum set of high confidence patterns . The confidence of a pattern P , which is originally used in association rule mining studies [ 4 ] , is defined as , where |sup of i . A high confidence pattern is a pattern that has a confidence of at least minconf , a constant threshold . Such high confidence patterns can be used for effective description of data in a class and for generation of classification rules . Note that signature patterns are high confidence patterns with minconf = 10 We tested the four algorithms for mining such relaxed patterns on the auto and sonar datasets from the UCI datasets . Figure 7 shows the performance results . Overall , the running time of the algorithms grows with the minimum confidence threshold . An interesting observation is that PatGrowthG performs faster than TwoStep when minconf ≤ 0.8 on the auto dataset , but severely slows down as the threshold exceeds 08 This is also due to the algorithm ’s redundant visits made over iterations . When minconf ≤ 0.8 , the size of the signature pattern cover is no greater than 20 . However , the size grows with minconf , and it becomes 48 when minconf = 10 Similar results were found on the sonar dataset .
When minconf is small , high confidence patterns are reached faster by pattern growth than by pattern shrink . However , due to the small size of the minimum pattern cover , the performances of the methods are not distinguishable as shown in Figure 7 . In this experiment , for the relaxed and more general problem of mining a minimum set of patterns of high confidence , PatShrink still performed the
338
●
●
●
●
●
●
●
●
●
●
0.5
0.6 Minimum Confidence
0.8
0.7
0.9
1.0
0.5
0.6 Minimum Confidence
0.8
0.7
0.9
1.0
Figure 7 . The Performance on Various Minimum Confidence Threshold best , and showed an order of magnitude speed up over other methods .
VI . CONCLUSIONS
In this work , we formally defined the signature pattern covering problem , and proposed its efficient iterative patter mining algorithm with the local greedy algorithm and pattern shrink method .
We analyzed the inherited problem of the iterative pattern mining methods ; there are many redundant search space visits . For resolving the problem in the signature pattern covering problem , we first proposed the local greedy algorithm for the set covering problem and proved that its approximation ratio is essentially the best achievable approximation ratio . Then , we used the local greedy algorithm in the direct mining of signature pattern covers . Also , we observed that for mining signature patterns , it is better to start with a largest pattern and shrink it , instead of starting with an empty pattern and growing it as the traditional pattern mining methods . Experimental results showed that the direct mining of signature patterns with the local greedy algorithm and pattern shrink is more than an order of magnitude faster that other approaches . We also observed that pattern shrink method works in high confidence pattern mining as well as in signature pattern mining . Thus , the proposed algorithm is applicable to mining a minimum set of high confidence patterns as well . For the future work , we plan to apply our methodologies in other iterative pattern mining algorithms like direct discriminative pattern mining algorithms .
REFERENCES
[ 1 ] Dtp aids antiviral screen program . http://dtpncinihgov
[ 2 ] Openrisc . http://wwwopencoresorg
[ 3 ] Sun opensparc t2 . http://wwwopensparcnet
[ 4 ] R . Agrawal , H . Mannila , R . Srikant , H . Toivonen , and A . I . Verkamo . Advances in knowledge discovery and data mining . chapter Fast discovery of association rules , pages 307–328 . American Association for Artificial Intelligence , Menlo Park , CA , USA , 1996 .
[ 5 ] M . Boule , J S Chenard , and Z . Zilic . Assertion checkers in verification , silicon debug and in field diagnosis . In ISQED ’07 : Proc . of the 8th Intl . Symposium on Quality Electronic Design , pages 613–620 , 2007 .
[ 6 ] M . Boul´e and Z . Zilic . Automata based assertion checker synthesis of psl properties . ACM Trans . Des . Autom . Electron . Syst . , 13(1):1–21 , 2008 .
[ 7 ] B . Bringmann , S . Nijssen , and A . Zimmermann . Patternbased classification : A unifying perspective . In LeGo From Local Patterns to Global Models Workshop in Proc . of PKDD , 2009 .
[ 16 ] M . M . H . Khan , H . K . Le , H . Ahmadi , T . F . Abdelzaher , and J . Han . Dustminer : troubleshooting interactive complexity bugs in sensor networks . In Proceedings of the 6th ACM conference on Embedded network sensor systems , SenSys ’08 , pages 99–112 , New York , NY , USA , 2008 . ACM .
[ 17 ] H . Kim , S . Kim , T . Weninger , J . Han , and T . Abdelzaher . Ndpmine : Efficiently mining discriminative numerical features for pattern based classification . In Machine Learning and Knowledge Discovery in Databases , pages 35–50–50 , 2010 .
[ 18 ] J . Li , K . Ramamohanarao , and G . Dong . The space of jumping emerging patterns and its incremental maintenance algorithms . In Proceedings of the Seventeenth International Conference on Machine Learning , ICML ’00 , pages 551– 558 , San Francisco , CA , USA , 2000 . Morgan Kaufmann Publishers Inc .
[ 19 ] Mentor
Graphics . in verification http://wwwmentorcom/products/fv/methodologies/abv/ mentor
Assertion based graphics .
[ 20 ] S . Nijssen , T . Guns , and L . D . Raedt . Correlated itemset mining in roc space : a constraint programming approach . In KDD , 2009 .
[ 21 ] J . Pei , J . Han , B . Mortazavi Asl , H . Pinto , Q . Chen , U . Dayal , and M C Hsu . Prefixspan : Mining sequential patterns efficiently by prefix projected pattern growth . volume 0 , page 0215 , 2001 .
[ 8 ] H . Cheng , X . Yan , J . Han , and P . S . Yu . Direct discriminative pattern mining for effective classification . In ICDE , 2008 .
[ 22 ] H . Saigo , N . Kr¨amer , and K . Tsuda . Partial least squares regression for graph mining . In KDD , 2008 .
[ 9 ] H . Cleve and A . Zeller . Locating causes of program failures . In Proceedings of the 27th international conference on Software engineering , ICSE ’05 , pages 342–351 , New York , NY , USA , 2005 . ACM .
[ 10 ] G . Dong and J . Li . Efficient mining of emerging patterns : discovering trends and differences . In Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining , KDD ’99 , pages 43–52 , New York , NY , USA , 1999 . ACM .
[ 11 ] W . Fan , K . Zhang , H . Cheng , J . Gao , X . Yan , J . Han , P . S . Yu , and O . Verscheure . Direct mining of discriminative and essential frequent patterns via model based search tree . In KDD , 2008 .
[ 12 ] H . Foster , D . Lacey , and A . Krolnik . Assertion Based Design .
Kluwer Academic Publishers , 2003 .
[ 23 ] H . Saigo , S . Nowozin , T . Kadowaki , T . Kudo , and K . Tsuda . gboost : a mathematical programming approach to graph classification and regression . Mach . Learn . , 75(1):69–89 , 2009 .
[ 24 ] R . M . H . Ting and J . Bailey . Mining minimal contrast subgraph patterns . In SDM , 2006 .
[ 25 ] S . Vasudevan , D . Sheridan , D . Tcheng , S . Patel , W . Tuohy , and D . Johnson . Goldmine : Automatic assertion generation using data mining and static analysis . In Proc . of the Conf . on Design , automation and test in Europe , 2010 .
[ 26 ] X . Yan , H . Cheng , J . Han , and P . S . Yu . Mining significant graph patterns by leap search . In Proceedings of the 2008 ACM SIGMOD international conference on Management of data , SIGMOD ’08 , pages 433–444 , 2008 .
[ 27 ] X . Yan and J . Han . gspan : Graph based substructure pattern mining . page 721 , 2002 .
[ 13 ] A . Gupta . Assertion based verification turns the corner . IEEE
Des . Test , 19(4):131–132 , 2002 .
[ 28 ] M . J . Zaki . Spade : An efficient algorithm for mining frequent sequences . volume 42 , pages 31–60 , 2001 .
[ 14 ] J . Han , J . Pei , and Y . Yin . Mining frequent patterns without candidate generation . In SIGMOD Conference , pages 1–12 , 2000 .
[ 29 ] M . J . Zaki . Efficiently mining frequent trees in a forest : Algorithms and applications . volume 17 , pages 1021–1035 , 2005 .
[ 15 ] J . H . Kelm , D . R . Johnson , M . R . Johnson , N . C . Crago , W . Tuohy , A . Mahesri , S . S . Lumetta , M . I . Frank , and S . J . Patel . Rigel : an architecture and scalable programming interface for a 1000 core accelerator . In ISCA ’09 : Proceedings of the 36th annual international symposium on Computer architecture , pages 140–151 , 2009 .
339
[ 30 ] M . J . Zaki and C . jui Hsiao . Charm : An efficient algorithm for closed itemset mining . pages 457–473 , 2002 .
[ 31 ] A . Zeller and R . Hildebrandt . failure inducing input . gineering , 28:183–200 , 2002 .
Simplifying and isolating IEEE Transactions on Software En
