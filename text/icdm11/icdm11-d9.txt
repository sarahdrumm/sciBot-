One Class Classification for Anomaly Detection :
Support Vector Data Description Revisited
Eric J . Pauwels and Onkar Ambekar
Centrum Wiskunde & Informatica CWI ,
Science Park 123 , 1098 XG Amsterdam , The Netherlands ericpauwels@cwinl http://wwwcwinl
ABSTRACT The Support Vector Data Description ( SVDD ) has been introduced to address the problem of anomaly ( or outlier ) detection . It essentially fits the smallest possible sphere around the given data points , allowing some points to be excluded as outliers . Whether or not a point is excluded , is governed by a slack variable . Mathematically , the values for the slack variables are obtained by minimizing a cost function that balances the size of the sphere against the penalty associated with outliers . In this paper we argue that the SVDD slack variables lack a clear geometric meaning , and we therefore re analyze the cost function to get a better insight into the characteristics of the solution . We also introduce and analyze two new definitions of slack variables and show that one of the proposed methods behaves more robustly with respect to outliers , thus providing tighter bounds compared to SVDD .
Key words : One class classification , outlier detection , anomaly detection , support vector data description , minimal sphere fitting
1
Introduction
In a conventional classification problem , the aim is to find a classifier that optimally separates two ( or more ) classes . The input to the problem is a labelled training set comprising a roughly comparable number of exemplars from each class . Howerever , there are types of problems in which this assumption of ( approximate ) equi distribution of exemplars no longer holds . The prototypical example that springs to mind is anomaly detection . By its very definition , an anomaly is a rare event and training data will more often than not contain very few or even no anomalous exemplars . Furthermore , anomalies can often only be exposed when looked at in context , ie when compared to the majority of regular points . Anomaly detection therefore provides an example of so called one class classification , the gist of which amounts to the following : Given data points that all originated from a single class but are possibly contaminated with a small number of outliers , find the class boundary .
In this paper we will focus on an optimization approach championed by Tax [ 8 ] and Sch¨olkopf etal [ 4 ] . The starting point is a classical problem in quadratic
2
Support Vector Data Description Revisited programming : given a set of n points x1 , . . . , xn in a p dimensional space , find the most tightly fitting ( hyper)sphere that encompasses all . Denoting the centre of this sphere by a and its radius by R , this problem boils down to a constrained minimization problem : min a,R
R2 subject to xi − a2 ≤ R2 ,
∀i = 1 , . . . , n .
( 1 )
However , if the possibility exist that the dataset has been contaminated with a small number of anomalies , it might prove beneficial to exclude suspicious points from the sphere and label them as outliers . This then allows one to shrink the sphere and obtain a better optimum for the criterion in eq(1 ) Obviously , in order to keep the problem non trivial , one needs to introduce some sort of penalty for the excluded points . In [ 8 ] and [ 4 ] the authors take their cue from standard support vector machines ( SVM ) and propose the use of non negative slack variables meant to relax the inclusion criterion in eq(1 ) More precisely , for each point they introduce a variable ξi ≥ 0 such that xi − a2 ≤ R2 + ξi .
( 2 )
This relaxation of the constraints is then offset by adding a penalty term to the cost function :
ζ(R , a , ξ ) := R2 + C
ξi . n i=1
The constant C is a ( pre defined ) unit cost that governs the trade off between the size of the sphere and the number of outliers . After these modifications the authors in [ 8 , 4 ] arrive at the following constrained optimization problem : given n data points x1 , . . . , xn and a pre defined unit cost C , find n i=1
{R2 + C min a,R,ξ
ξi} st ∀i = 1 , . . . , n : xi − a2 ≤ R2 + ξi ,
ξi ≥ 0 . ( 3 )
The resulting data summarization segregates “ regular ” points on the inside from “ outliers ” on the outside of the sphere and is called support vector data description ( SVDD ) .
Aim of this paper The starting point for this paper is the observation that the slack variables in eq.(3 ) lack a straightforward geometrical interpretation . Indeed , denoting di = xi − a , it transpires that the slack variables can be represented explicitly as :
ξi = ( d2 i − R2)+ = i − R2 if di > R , if di ≤ R . 0
( 4 ) fl d2
However , except in the case where the dimension of the ambient space ( p ) equals two or three , these slack variables don’t have an obvious geometric interpretation .
Support Vector Data Description Revisited
3
It would therefore be more natural to set the slack variable equal to ϕi = ( di − R)+ upon which the relaxed constraints can be expressed as :
∀i : xi − a ≤ R + ϕi , ϕi ≥ 0 .
The corresponding penalized function would then take the form :
( 5 )
( 6 ) i
ζ2(a , R ) := R2 + C
ϕ2 i .
( Notice that we can drop ϕ from the list of arguments as it can be computed as soon as a and R are specified ) . For lack of space we will not be able to study this alternative in detail . Suffice it to say that the solution includes non acceptable , trivial configurations . However , there is no obvious reason why the variables in the cost function should appear as squares . This suggests that we also should look at a second — completely linear — alternative :
ζ1(a , R ) := R + C
ϕi .
( 7 ) i
The goal of this paper is therefore twofold . Firstly , we want to re analyze the original optimization problem ( 3 ) as introduced in [ 8 ] and [ 4 ] . However , in contradistinction to these authors , we will refrain from casting it in its dual form , but focus on the primal problem instead . This will furnish us with additional insights into the geometry and behaviour of the solutions . Secondly , we will then extend this analysis to the alternative ζ1 ( see eq . 7 ) mentioned above and conclude that , in some respects , it is preferable to the original . In fact the difference between these two solutions is not unlike the difference in behaviour between the mean and median ( for a quick preview of this result , we suggest to have a peek at Fig 2 ) .
Related work Although lack of space precludes a comprehensive revision of all related work , it is fair to say that after the seminal papers [ 5 , 8 ] most activity focussed on applications , in particular clustering , see eg [ 1 ] . In particular , a lot of research has gone into the appropriate choice of the Gaussian kernel size when using the kernelized version of this technique [ 3 , 2 ] , as well as efficient methods for cluster labeling . In [ 6 ] a different direction of generalization is pursued : rather than mapping the data into a high dimensional feature space , the spherical constraints are relaxed into ellipsoidal ones in the original data space , thereby side stepping the vexing question of kernel choice .
2 Support Vector Data Description Revisited
In this section we will re analyze the cost function ( 3 ) which lies at the heart of the SVDD classifier . However , rather than recasting the problem in its dual form ( as is done in [ 8 ] and [ 4] ) , we will focus directly on the primal problem . This allows us to gain additional insight in the qualitative behaviour of the solutions ( cf . section 2.2 ) as well as sharpen the bounds on the unit cost C ( see item 3 of Prop . 1 ) .
4
Support Vector Data Description Revisited
2.1 Outlier Detection as an Optimization Problem
Recall from ( 3 ) that the anomaly ( aka outlier ) detection problem has been recast into the following constrained optimization problem . As input we accept n points x1 , . . . , xn in p dimensional space , and some fixed pre defined unit cost C . In addition , we introduce a vector ξ = ( ξ1 , . . . , ξn ) of n slack variables in terms of which we can define the cost function
ζ(a , R , ξ ) := R2 + C
ξi .
( 8 ) i
The SVDD outlier detection ( as introduced in [ 8 ] and [ 4 ] ) now amounts to finding the solution to the following constrained minimization problem : st ∀i = 1 , . . . , n : xi − a2 ≤ R2 + ξi ,
ξi ≥ 0 .
ζ(a , R , ξ )
( 9 ) min a,R,ξ
If we denote the distance of each point xi to the centre a as di = xi − a then it ’s straightforward to see that the slack variables can be explicified as ξi := ( d2 i − R2)+ , where the ramp function x+ is defined by : fl x if x ≥ 0 ,
0 if x < 0 , x+ :=
ζ(a , R ) = R2 + C i − R2)+ . ( d2
( 10 )
( 11 )
This allows us to rewrite the cost function in a more concise form : i
Notice that the cost function is now a function of a and R only , with all other constraints absorbed in the ramp function x+ . From this representation it immediately transpires that ζ is continuous in its arguments , albeit not everywhere differentiable .
2.2 Properties of the solution
Proposition 1 The solution of the ( unconstrained ) optimization problem
( a∗ , R∗ ) := arg min a,R
ζ(a , R ) where
ζ(a , R ) = R2 + C has the following qualitative properties : i − R2)+ ( d2 i=1
( 12 ) n
1 . Behaviour of the marginal functions :
( a ) Keeping R fixed , ζ is a convex function of the centre a . ( b ) Keeping a fixed , ζ is piecewise quadratic in R .
Support Vector Data Description Revisited
5
2 . Location of the optimal centre a∗ : The centre of the optimal sphere can be specified as a weighted mean of the data points
( 13 )
( 14 )
( 15 ) where such that hi = i hixi i hi a∗ =
 1 i if di > R∗ 0 ≤ θi ≤ 1 if di = R∗ if di < R∗ . 0 hi = 1/C .
3 . Dependency on penalty cost C :
The value of the unit cost C determines the qualitative behaviour of the solution . More precisely : – If C < 1/n then the optimal radius R∗ will be zero , ie all points will – If C ≥ 1/2 all points will be enclosed , and the sphere will be the minimum – For values 1/n ≤ C ≤ 1/2 , the qualitative shape of the solution changes reside outside of the sphere . volume enclosing sphere . whenever C = 1/k for k = 2 , 3 , . . . n .
PROOF
1 . Behaviour of the marginal functions
– 1.a : Keeping R fixed , ζ is a convex function of the centre a . Assuming that in eq . ( 12 ) the radius R and cost C are fixed , the dependency of the cost functional is completely captured by second term : i − R2)+ ≡
( d2 i i max{d2 i − R2 , 0} .
Convexity of ζ as a function of a is now immediate as each d2 i ( a ) = xi − a2 is convex and both the operations of maximization and summing are convexity preserving .
– 1.b : Keeping a fixed , ζ is piecewise quadratic in R .
Introducing i ≡ d2 the auxiliary binary variables : bi(R ) = for a fixed ,
( 16 ) allows us to rewrite ( d2 fl 1 if di > R , 0 if di ≤ R , i − R2)+ ≡ bi(R)(d2 n
1 − C i − R2 ) , from which n
ζ(R ) = bi(R )
R2 + C bi(R)d2 i ,
( 17 ) i=1 i=1
6
Support Vector Data Description Revisited or again , where
ζ(R ) = β(R ) R2 + Cγ(R ) . and γ(R ) :=
( 18 )
( 19 ) bi(R)d2 i .
β(R ) := 1 − C bi(R ) i i
As it is clear that the coefficients β and γ are piecewise constant , producing a jump whenever R grows beyond one of the distances di , it follows that ζ(R ) is ( continuous ) piecewise quadratic . More precisely , if we assume that the points xi have been ( re )labeled such that d1 ≡ x1 − a ≤ d2 ≡ x2 − a ≤ . . . ≤ dn ≡ xn − a , then for 0 ≤ R < d1 , all bi(R ) = 1 and hence β(R ) = 1 − nC . On the interval d1 ≤ R < d2 we find that b1 = 0 while b2 = b3 = . . . bn = 1 implying that β(R ) = 1 − ( n − 1)C , and so on . So we conclude that β is a piecewise constant function , making an upward jump of size C whenever R passes a di . This is illustrated in Fig 1 where the bottom figure plots the piecewise constant coefficient β for two different values of C , while the corresponding ζ functions are plotted in the top graph . Clearly , every β plateau gives rise to a different quadratic part of ζ . More importantly , as long as β(R ) < 0 the resulting quadratic part in ζ is strictly decreasing . Hence we conclude that the minimum of ζ occurs at the point R∗ = arg min ζ(R ) where β jumps above zero . Indeed , at that point , the corresponding quadratic becomes strictly increasing , forcing the minimum to be located at the jump between the two segments . From the above we can also conclude that the optimal radius R∗ = arg min ζ(R ) is unique except when C = 1/k for some integer 1 ≤ k ≤ n . In those instances there will be an R segment on which bi = k , forcing the corresponding
β coefficient to vanish . This then gives rise to a flat , horizontal plateau of minimal values for the ζ function . In such cases we will pick ( arbitrarily ) the maximal possible value for R , ie : R∗ := sup{R : ζ(R ) is minimal} . Finally , we want to draw attention to the fact that the optimal sphere always passes through at least one data point , as the optimal radius R∗ coincides with at least one di .
2 . Location of the optimal centre Earlier we pointed out that the ζ(a , R ) is continuous but not everywhere differentiable . This means that we cannot simply insist on vanishing gradients to determine the optimum , as the gradient might not exist . However , we can take advantage of a more general concept that is similar in spirit : subgradients . Recall that for a differentiable convex function f , the graph of the function lies above every tangent . Mathematically this can be reformulated by saying that at any x : f(y ) ≥ ∇f(x ) · ( y − x ) ,
∀y .
( 20 )
If f is not necessarily differentiable at x then we will say that any vector gx is a subgradient at x if : f(y ) ≥ gx · ( y − x ) ,
∀y .
( 21 )
Support Vector Data Description Revisited
7
The collection of all subgradients at a point x is called the subdifferential of f at x and denoted by ∂f(x ) . Notice that the subdifferential is a set valued function! It is now easy to prove that the classical condition for x∗ to be the minimum of a convex function f ( ie ∇f(x∗ ) = 0 ) can be generalized to non differentiable functions as :
( 22 ) To apply the above the problem at hand , we first note that the subdifferential
0 ∈ ∂f(x∗ ) . of the ramp function x+ is given by :
∂x+ = if x < 0 if x = 0 if x > 0
( ie set valued )
( 23 )
[ 0 , 1 ] 1 as at x = 0 any straight line with slope between 0 and 1 will be located under the graph of the ramp function . To streamline notation , we introduce ( a version of ) the Heaviside stepfunction
H(x ) =
( ie set valued )
( 24 ) if x > 0 0 ≤ h ≤ 1 if x = 0 0 if x < 0
To forestall confusion we point out that , unlike when used as a distribution , this definition of the Heaviside function insists its value at the origin is between zero and one . Using this convention , we have the convenient shorthand notation :
∂x+ = H(x ) .
Computing the subgradients ( for convenience we will drop the notational distinction between standard and sub gradients ) we obtain :
 0  1 i − R2 ) ( xi − a ) i − R2 )
H(d2 i H(d2
= 2R − 2RC
∂ζ ∂R ∇aζ = −2C
( 25 )
( 26 ) i where we used the well known fact :
∇a(d2 i ) = ∇a||xi − a||2 = ∇a(xi · xi − 2xi · a + a · a ) = −2(xi − a ) .
( 27 )
Insisting that zero is indeed a subgradient means that we need to pick values hi := H(d2 i − R2 ) such that :
0 ∈ ∂ζ/∂R ⇒ n 0 ∈ ∇aζ ⇒ n i=1 i=1 hi = 1/C hi(xi − a ) = 0
( 28 )
( 29 )
8
Support Vector Data Description Revisited
The above characterization allows us to draw a number of straightforward conclusions ( for notational convenience we will drop the asterisk to indicate optimality , and simply write a∗ = a and R∗ = R ) : 1 . Combining eqs.(28 ) and ( 29 ) it immediately transpires that or again , and more suggestively ,
( 30 )
( 31 ) hi xi ,
. a = a = C hi xi hi i − R2 ) + θi + i:di=R 1 a = i:di=R
θixi + θi + i:di>R 1 i:di=R i:di>R
Furthermore , the sums in the RHS can be split into three parts depending on whether a point lies inside ( di < R ) , on ( di = R ) or outside ( di > R ) the sphere , eg : i − R2 ) +
H(d2 i hi = = 0 + i:di<R
H(d2 i:di=R i:di>R where 0 ≤ θi ≡ H(di − R = 0 ) ≤ 1 . Hence : i:di>R xi
H(d2 i − R2 )
( 32 )
( 33 )
This representation highlights the fact that the centre a is a weighted mean of the points on or outside the sphere ( the so called support vectors ( SV ) , [ 8] ) , while the points inside the sphere exert no influence on its position . Notice that the points outside of the sphere are assigned maximal weight .
2 . If we denote the number of points inside , on and outside the sphere by nin , non and nout respectively , then by definition #SV = non+nout . Invoking eq . ( 28 ) and combining this with the fact that 0 ≤ θi ≤ 1 it follows that
1/C = hi =
θi +
1 i i:di=R i:di>R
Hence , since 0 ≤ θi ≤ 1 it can be concluded that nout =
1 ≤ 1/C ≤ non + nout = #SV
( 34 )
( 35 ) di>R
Put differently : ( a ) 1/C is a lower bound on the number of support vectors ( #SV ) . ( b ) 1/C is an upper bound on the number of outliers ( nout ) . The same result was obtained by Sch¨olkopf [ 4 ] , who introduced the parameter ν = 1/nC as a bound on the fraction of support vectors ( #SV /n ) and outliers ( nout/n ) .
Support Vector Data Description Revisited
9
In this section we try to gain further 3 . Dependency on unit cost C insight into how the cost function determines the behaviour of the optimum . Let us assume that we have already minimized the cost function ( 11 ) and identified the optimal centre a∗ and corresponding radius R∗ . For convenience ’s sake , we again assume that we have relabeled the data points in such a way that the distances di = xi − a∗ are ordered in ascending order : 0 ≤ d1 ≤ d2 ≤ . . . ≤ dn . We now investigate how the total cost ζ depends on the unit cost C in the neighbourhood of this optimum . Figure 1 nicely illustrate the influence of the unit cost C on the qualitative behaviour of the optimal radius R∗ . Indeed , increasing C slightly has the following effects on the β function :
– The values of the coefficients hi will change ( cf . eq . 28 ) which in turn will result in a shift of the optimal centre ( through eq . 31 ) . As a consequence the distances di to the data points xi will slightly change , resulting in slight shifts of the step locations of the β function . Since the position of the optimal radius R∗ coincides with one of these step locations ( viz . the jump from a negative to a positive β segment ) , increasing C slightly will typically induces small changes in R∗ . However , from time to time , one will witness a jump like change in R∗ as explained below .
– Since the size of a β step equals the unit cost , slightly increasing C will push the each β segment slightly downwards as the maximum of β remains fixed at one ( ie limR→∞ β(R ) = 1 ) . As a consequence , β segments that are originally positive , will at some point dip below the X axis . As this happens , the corresponding quadratic segment will make the transition from convex and increasing to concave and decreasing forcing the minimum R∗ to make a jump . bi = 1 for all i = 1 , . . . , n and hence β(R ) = 1−C
This now allows us to draw a number of straightforward conclusions about the constraints on the unit cost C . – The first segment of the β function occurs for 0 ≤ R < d1 . On this segment i bi = 1−nC . If C < 1/n , then β > 0 on this first segment and hence on all the subsequent ones . In that case , ζ(R ) is strictly increasing and has a single trivial minimum at R∗ = 0 . Put differently , in order to have a non trivial optimization problem , we need to insist on C ≥ 1/n ( cf . item 3 in proposition 1 ) . icicic – If , on the other hand , we want to make sure that there are no outliers , then the optimum R∗ has to coincide with the last jump , ie R∗ = dn . This implies that the quadratic segment on the interval [ dn−1 , dn ] has to be decreasing ( or i bi ≤ 0 . Since on this last segment we have that all bi vanish except for bn , it follows that β(R ) = 1 − C ≤ 0 ( and vice versa ) . We therefore conclude that for values C ≥ 1 there will be no outliers . This result was also obtained in [ 8 , 5 ] but we can now further tighten the above bound by observing that when the optimal sphere encloses all points , flat ) , and consequently β(R ) = 1 − C
10
Support Vector Data Description Revisited it has to pass through at least two points ( irrespective of the ambient dimension ) . This implies that dn−1 = dn and the first non trivial interval preceding dn is in fact [ dn−2 , dn−1 ] . Rerunning the above analysis , we can conclude that C ≥ 1/2 implies that all data points are enclosed . increasing . On that interval we know that
– Using the same logic , if we insist that at most k out of n are outside the circle , we need to make sure that the quadratic on [ dn−k , dn−k+1 ] is convex and i bi = k . Hence we conclude that on this interval β(R ) = 1 − kC > 0 or again : C < 1/k . Hence , ν = 1/nC > k/n is an upper bound on the fraction of points outside the descriptor ( cf . [ 4] ) .
– In fact , by incorporating some straightforward geometric constraints into the set up we can further narrow down the different possible configuration . As a simple example , consider the case of a generic 2 dimensional data set . The sphere then reduces to a circle and we can conclude that – since we assume the data set to be generic – the number of points on the optimal circle ( ie non ) either equals 1 ( as the optimal circle passes through at least one point ) , 2 or 3 . Indeed , there is a vanishing probability that a generic data set will have 4 ( or more ) co circular points ( points on the same circle ) . In this case we can rewrite the Sch¨olkopf inequality ( 35 ) as : nout ≤ 1/C ≤ nout + 3
For values C < 1/3 it then follows that
3 < 1/C ≤ nout + 3 ⇒ nout > 0 .
So we arrive at the somewhat surprising conclusion that if the unit cost is less than 1/3 , we are guaranteed to have at least one outlier , no matter what the data set looks like ( as long as it is generic ) . This is somewhat counter intuitive as far as the usual concept of an outlier is concerned!
This concludes the proof .
QED
3 Linear Slacks and Linear Loss
3.1 Basic analysis i
As announced earlier , this section busies itself with minimizing the linear function
ϕi
ζ1(a , R ) := R + C subject to ∀i : di ≡ xi − a ≤ R + ϕi , ϕi ≥ 0 . ( 36 ) Again , we absorb the constraints into the function by introducing the ramp function :
ζ1(a , R ) = R + C
( di − R)+
( 37 ) i
Support Vector Data Description Revisited
11
Taking subgradients with respect to a and R yields : H(di − R )
= 1 − C
( xi − a2 ) = − ( xi − a )
( xi − a ) xi − a
H(di − R ) xi − a .
∂ζ1 ∂R ∇aζ1 = −C
∇a(di ) = ∇a since it is straightforward to check that :
Equating the gradient to zero and re introducing the notation hi = H(di − R ) we find that the optimum is characterized by :
= 0 ⇒ n ∇aζ1 = 0 ⇒ n
∂ζ1 ∂R i=1 i=1 hi = 1/C ( xi − a ) xi − a = 0 hi
( 38 )
( 39 )
Notice how eq . ( 38 ) is identical to eq . ( 28 ) whereas eq . ( 39 ) is similar but subtly different from eq(29 ) In more detail :
1 . Once again we can make the distinction between the nin points that reside inside the sphere , the non points that lie on the sphere and the nout points that are outside the sphere . The latter two categories constitute the support vectors : #SV = non + nout . Hence ,
1/C = i hi = = di<R hi + hi + di=R θi + nout . di>R hi
So also in this case we get ( cf . eq . ( 35) ) : di=R nout ≤ 1 C
≤ #SV .
( 40 )
2 . Comparing eqs . ( 39 ) and ( 29 ) we conclude that we can expect the solution corresponding to linear loss function ( 36 ) to be more robust with respect to outliers . Indeed , in Section 2 we’ve already argued that eq . ( 29 ) implies that the sphere ’s centre is the ( weighted ) mean of the support vectors . Noticing that in eq . ( 39 ) the vectors have been substituted by the corresponding unit vectors reveals that in the case of a linear loss function the centre can be thought of as the weighted median of the support vectors . Indeed , for a set of 1 dimensional points x1 , . . . , xn the median m is defined by the fact that it separates the data set into two equal parts . Noticing that ( xi−m)/|xi−m| =
12
Support Vector Data Description Revisited sgn(xi − m ) equals −1 , 0 or 1 depending on whether xi < m , xi = m or xi > m respectively , we see that the median can indeed be defined implicitly by : i
( xi − m ) xi − m = 0 .
This characterization of the median has the obvious advantage that the generalization to higher dimensions is straightforward [ 7 ] . The improved robustness of the solution of the linear cost function ( 36 ) with respect to the original one ( 7 ) is nicely illustrated in Fig 2 .
3.2 Further properties To gain further insight in the behaviour of solutions we once again assume that the centre of the sphere has already been located , so that the cost function depends solely on R . We also assume that the points have been labeled to produce an increasing sequence of distances di = xi − a . Hence : ζ1(R ) = R+C
( di−R)H(di−R ) = R+C
( di−R ) = R+C bi(di−R ) , i i i where we have once again re introduced the binary auxiliary variables bi defined in eq.(16 ) Rearranging the terms we arrive at :
ζ1(R ) =
1 − C bi(R )
R + C bi(R)di ,
( 41 ) slope equal to 1 − C bi . For notational convenience , we define which elucidates that the function is piecewise linear , with a piecewise constant i
β(R ) = 1 − C bi(R ) i and δ(R ) = bi(R)di , resulting in ζ1(R ) = β(R ) R + Cδ(R ) . Furthermore , β(0 ) = 1− nC and increases by jumps of size ( multiples of ) C to reach 1 when R = dn . Hence the minimum R∗ is located at the distance di for which β jumps above zero .
These considerations allow us to mirror the conclusions we obtained for the original cost function : 1 . The optimal value of R∗ coincides with one of the distances di which means 2 . The optimal value R∗ changes discontinuously whenever the unit cost takes that the optimal circle passes through at least one of the data points . on a value C = 1/k ( for k = 2 , . . . , n ) .
3 . Non trivial solutions exist only within the range :
1 n
≤ C ≤ 1 2 .
For other values of C either all or no points are outliers .
4 . The Sch¨olkopf bounds ( 35 ) ( and the ensuing conclusions ) prevail .
Support Vector Data Description Revisited
13
4 Conclusions
In this paper we re examined the support vector data descriptor ( SVDD ) ( introduced by [ 8 ] and [ 5 ] ) for one class classification . Our investigation was prompted by the observation that the definition of slack variables as specified in the SVDD approach , lacks a clear geometric interpretation . We therefore re analyzed the SVDD constrained optimization problem , focussing on the primal formulation , as this allowed us to gain further insight into the behaviour of the solutions . We applied the same analysis to two natural alternatives for the SVDD function . The first one turned out to suffer from unacceptable limitations , but the second one produces results that are very similar to the original formulation , but enjoys enhanced robustness with respect to outliers . We therefore think it could serve as an alternative for the original .
This research is partially supported by the Specific TarAcknowledgement geted Research Project ( STReP ) FireSense Fire Detection and Management through a Multi Sensor Network for the Protection of Cultural Heritage Areas from the Risk of Fire and Extreme Weather Conditions ( FP7 ENV 20091244088 FIRESENSE ) of the European Union ’s 7th Framework Programme Environment ( including Climate Change ) .
References
1 . Ben Hur A . , Horn D . , Siegelmann H . T . , and Vapnik V . : Support vector clustering .
Journal of Machine Learning Research , 2:125137 ( 2001 )
2 . Lee J . and Lee D . : An improved cluster labeling method for support vector clustering . IEEE Transactions on Pattern Analysis and Machine Intelligence , 27:461464 , ( 2005 )
3 . Lee S . and Daniels K . : Cone cluster labeling for support vector clustering . In
Proceedings , 2006 SIAM Conference on Data Mining , pages 484488 , ( 2006 )
4 . Sch¨olkopf , B . , Williamson , RC , Shrinking the tube : A new support vector regres sion algorithm . Advances in Neural Information Processing Systems , ( 1999 )
5 . Sch¨olkopf , B . , Williamson , R . , Smola , A . , Shawe Taylor , J . , and Platt , J . , Support vector method for novelty detection . In Advances in Neural Information Processing Systems , 12:582 588 , MIT Press , ( 2000 )
6 . Shioda R . and Tuncel L . : Clustering via Minimum Volume Ellipsoids . Journal of
Comp . Optimization and App . vol.37(3 ) ( 2007 )
7 . Small , CG , A survey of multidimensional medians . International Statistical Re view , 58(3):263 277 , ( 1990 )
8 . Tax , DMJ , Duin RPW , Support vector domain description . Pattern Recogni tion Letters , 20(11 13):1191 1199 , December ( 1999 )
9 . Tax , DMJL : One class classification : concept learning in the absence of counter example . PhD Thesis , TU Delft , 2001 .
10 . Ypma , A . , Duin , R . , Support objects for domain approximation . In ICANN ,
Skovde , Sweden ( 1998 )
14
Support Vector Data Description Revisited
1− C
Fig 1 . Top : Total cost ζ ( for two slightly different values of the unit cost C ) as a function of the radius R for a simple data set comprising four points . This continuous function is composed of quadratic segments β(R)R2 + Cγ(R ) . The piecewise constant behaviour of the β coefficient ( which determines whether the segment is increasing or decreasing ) is plotted in the bottom figure . Bottom : The quadratic coefficient β(R ) = i bi(R ) is a piecewise constant function for which the jumps occur whenever R equals one of the distances di = xi − a . For C = 0.31 this jump occurs around 0.7 resulting in a ζ minimum at that same value . Increasing C slightly to C = 0.33 pushes the 2nd β segment below zero , resulting in a ζ minimum equal to d2 ≈ 092
Support Vector Data Description Revisited
15
Fig 2 . Comparison of the optimal sphere for the original SVDD function ( in blue , cf . eq . ( 9 ) , and the linear alternative ( in black , cf . eq . ( 36) ) . The data sets in the top and bottom figures are identical except for the starred point on the right which , in the bottom figure ( different scale! ) , has been moved far away from the rest of the cluster . Clearly , the optimal circle based on the linear function is essentially unaffected whereas the SVDD solution is dramatically inflated by this outlier .
