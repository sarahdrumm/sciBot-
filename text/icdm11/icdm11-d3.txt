BibClus : A Clustering Algorithm of Bibliographic Networks by Message Passing on
Center Linkage Structure
Xiaoran Xuy , Zhi Hong Dengyzfi yKey Laboratory of Machine Perception ( Ministry of Education ) , zThe State Key Lab of Computer Science , Institute of Software ,
School of Electronic Engineering and Computer Science , Peking University
Chinese Academy of Sciences , Beijing 100190 , China xuxiaoran@pkueducn , zhdeng@cispkueducn
Abstract—Multi type objects with multi type relations are ubiquitous in real world networks , eg bibliographic networks . Such networks are also called heterogeneous information networks . However , the research on clustering for heterogeneous information networks is little . A new algorithm , called NetClus , has been proposed in recent two years . Although NetClus is applied on a heterogeneous information network with a star network schema , considering the relations between center objects and all attribute objects linking to them , it ignores the relations between center objects such as citation relations , which also contain rich information . Hence , we think the star network schema cannot be used to characterize all possible relations without integrating the linkage structure among center objects , which we call the Center Linkage Structure , and there has been no practical way good enough to solve it . In this paper , we present a novel algorithm , BibClus , for clustering heterogeneous objects with center linkage structure by taking a bibliographic information network as an example . In BibClus , we build a probabilistic model of pairwise hidden Markov random field ( P HMRF ) to characterize the center linkage structure , and convert to a factor graph . We further combine EM algorithm with factor graph theory , and design an efficient way based on message passing algorithm to inference marginal probabilities and estimate parameters at each iteration of EM . We also study how factor functions affect clustering performance with different function forms and constraints . For evaluating our proposed method , we have conducted thorough experiments on a real dataset that we had crawled from ACM Digital Library . The experimental results show that BibClus is effective and has a much higher quantity than the recently proposed algorithm , NetClus , in both recall and precision . it
Keywords heterogeneous information network , clustering , factor graph , message passing
I . INTRODUCTION
Networks in the real world usually show two main characteristics at least : heterogeneity and transitivity . The heterogeneity means the multiplicity of not only object types but also relation types among objects ; the transitivity means that any object ’s characteristic can be spread along relation chains , as a message , to its neighbors and even far objects . These networks are also called heterogeneous information fiCorresponding author networks , such as Internet composed of gigantic networks of web pages , semantic networks with millions of concepts obtained from Wikipedia or WordNet , and friendship networks drawn from social web sites . Many interesting problems have been proposed on such networks , and the common issue is how to take advantage of the heterogeneity and transitivity effectively to yield a better result . Among these problems , clustering on a heterogeneous information network can discover the hidden cluster structure , and give us a grand view of the huge network . Traditional methods of clustering based on the feature vector data type , eg . K means and modelbased clustering , are unsuitable for linked or networked data obviously . Although there are already some clustering approaches based on graphs or networks [ 1][2 ] , most studies are on homogeneous networks . A recent algorithm NetClus [ 3 ] deals with heterogeneous networks with star network schema . Unfortunately , the schema overlooks the potential linkage structure among center objects . In fact , each center object is affected by not only the attribute objects linking to it , but also other center objects nearby . This motivates us to design a more powerful model to include the center linkage structure .
This paper focus on clustering heterogeneous objects with center linkage structure by taking a bibliographic network as an example . Consider , for instance , a bibliographic information network example in Fig 1 , which is composed of abundant information including papers , authors , conferences and terms . Each paper is linked to a set of authors who wrote it , a conference that published it and a bag of terms that describe it . Since a paper is surrounded by the other types of bibliographic objects , we call it a center object and the others attribute objects . Besides the relations above , two papers may also have a citation relation , implying an underlying linkage structure among center objects .
Suppose that , in Fig 1 , each object carries a known class label or a probability distribution of belonging to classes . A relation between two objects is more likely to be established , if they come from the same class or their distributions are close to each other . For example , a paper that discusses a problem about data mining ( DM ) is more likely , with
1 ) We develop a probabilistic graphical model for clustering which introduces the center linkage structure . Unlike prior work that only models a homogeneous network or involves part of relations , we add certain relations among center objects to those original relations between center objects and the corresponding attribute objects . That will reveal more information about the hidden cluster structure .
2 ) While the EM algorithm has been widely used in clustering , how to apply it on a factor graph , has been rarely studied and applied . In this paper , we propose an approach to fit parameters about a factor graph indirectly under the EM framework , by using the message passing algorithm . We expand the application scope of the EM algorithm , and the approach is not limited to the clustering problem .
3 ) To improve the clustering performance , we work out the interesting idea of controlling message passing by enforcing specific function forms and constraints on local factor functions . Each local function is designed to measure influence degree of each edge between two nodes or of the node itself .
With the great
II . RELATED WORK improvement of networked and linked data on the web , traditional clustering approaches are no longer suitable for today ’s needs . Many methods have been developed to find clusters in networks or graphs . To begin with , some clustering algorithms focus on graph partition , without consulting objects’ information . With treating all nodes and edges on an equal basis , they aim at partitioning a graph into several subgraphs to optimize objective functions based on different criteria , such as minimum cut , min max cut [ 4 ] and normalized cut [ 5 ] , etc . The Girvan Newman algorithm [ 6 ] is one of these methods , which is based on the iterative removal of edges with high “ betweenness ” scores . Later , both linked and observed data information is taken into account , leading to a more accurate and meaningful clustering result [ 7 ] . Probabilistic graphical models , such as Markov random field [ 8][9 ] , are used to characterize the hidden structures in networks . Some relational clustering methods use link structure to infer and learn model parameters [ 10 ] . However , lots of such networks are homogeneous . Now , the clustering problem for heterogeneous information networks with relations of different types and from different sources has been proposed [ 11 ] . NetClus [ 3 ] is an algorithm which is able to deal with heterogeneous networks with multi typed objects . But the star network schema defined on NetClus limits some relations , so that the underlying network structure cannot be disclosed enough .
Factor graph has close connections with Markov random field and Bayesian network [ 12][13 ] . The message passing way has been successful as a decoding algorithm for errorcorrecting codes defined on Tanner graphs [ 14 ] , and also
Figure 1 . A bibliographic network a high probability , to be written by a DM expert , to be published by a conference such as ICDM , to be described with specific terminologies of data mining , and also to be cited by or cite other papers on the same issue in the DM field . The various relations stand for certain statistical dependencies to share the same characteristics , that is , one is liable to be influenced by its neighbors . However , influence degrees caused by different objects may vary from one another , which means relation strengths need to be regulated properly . Since influence can be propagated along a chain , we could extend the range from one ’s neighbors to a clique or a little community , so that one ’s characteristic can affect the nearby objects beyond its neighbors . In a word , two main considerations , which we take into account for clustering over a bibliographic network , are presented as follows :
1 ) How to regulate the influence . We construct a pairwise hidden Markov random field ( P HMRF ) to model the structure of bibliographic network , and use a local compatibility function to represent a relation between two adjacent nodes vi and vj . The local compatibility function indicates statistical dependency , but not a real probability , which means that vi should be ” compatible ” with the adjacent node vj . Under the framework of the EM algorithm , we can estimate these compatibility functions , with certain pre prepared function forms and constraints .
2 ) How to propagate the influence . We regard the procedure of propagating influences as a way of passing conditional probabilities to compute marginal probabilities as in a Bayesian network . However , our model is based on an undirected graph . We build a factor graph by converting a P HMRF and implement the message passing algorithm to obtain marginal probabilities . In other words , one ’s neighboring node not only plays a role of directly impacting on it , but also passes on influences from other nodes to it .
On account of the two consideration above , in this paper , we study the problem of clustering over a heterogeneous information network based on message passing in a factor graph , and develop a novel clustering algorithm , BibClus . The chief contributions of this paper are :
( cid:72)(cid:85)(cid:72)(cid:85)(cid:72)(cid:85)(cid:73)(cid:72)(cid:85)(cid:72)(cid:72)(cid:73)(cid:72)(cid:85)(cid:72)(cid:72)(cid:73)(cid:72)(cid:85)(cid:72)(cid:72)(cid:36)(cid:87)(cid:75)(cid:85)(cid:36)(cid:87)(cid:75)(cid:85)(cid:36)(cid:87)(cid:75)(cid:85)(cid:72)(cid:85)(cid:72)(cid:85)(cid:72)(cid:85 ) in some computer vision problems on a underlying MRF [ 15][16 ] . Being inspired , we extend this message passing way to solve the clustering problem on heterogeneous networks . As far as we know , there is little study of using message passing to do that . Recently , a method called “ Affinity Propagation ” [ 17 ] has been proved more efficient and effective by building a factor graph to pass messages like “ who you think is the center ” . However , this method is not applied for heterogeneous networks . In this paper , we propose a different way to pass messages describing the influence between two neighboring nodes .
III . PROBLEM FORMALIZATION
Rs .
∪
∪
Ot and E =
In this section , we present the problem formulation and define some concepts and notations used throughout the paper . Information Network . Definition 1 : Heterogeneous Given a set of objects from T types {Ot}T t=1 and their internal relationships from S types {Rs}S s=1 , where Ot denotes a set of objects belonging to tth type , and Rs denotes a set of relationships pertaining to sth type , a network G = ⟨V , E⟩ is called an heterogeneous information network , if V = Based on the definition above , we can construct a bibliographic network . In a bibliographic document set D = {di} , each paper di consists of a collection of bibliographic objects from author set A , conference set B , term set W and paper set D , denoted as O(di ) = A(di)∪ B(di)∪ W ( di)∪ D(di ) , which is an n tuple relation set . And in the corresponding network , V = A ∪ B ∪ W ∪ D , and ∀e = ⟨vi , vj⟩ , vi ∈ D(or vi ∈ A ∪ B ∪ W ) ∧ vj ∈ A ∪ B ∪ W ( or vj ∈ D ) or vi , vj ∈ D . Definition 2 : Center Linkage Structure . We call D center type and call the others attribute types . A network structure on D is a center linkage structure , defined as Gcen = ⟨Vcen , Ecen⟩ , if Vcen = D , ∀vi ∈ Vcen , xi = {o|o ∈ A(di ) ∪ B(di ) ∪ W ( di)} is an associated variable representing the relations between vi and its attribute objects , and ∀eij = ⟨vi , vj⟩ ∈ Ecen , vi ∈ D(vj ) or vj ∈ D(vi ) . With the definition of a center linkage structure , eij could be characterized by citation relations in a bibliographic network but ignoring direction . As we see , the center linkage structure Gcen is a subgraph induced from the bibliographic network , where each node carries a set of attribute nodes linking to it . In this paper , our aim is to find potential clusters {c1 , . . . , cK} with the fixed cluster number K for bibliographic networks . A key issue is how to characterize and model a cluster by certain parameters . Traditionally , under the setting that each point is a multi dimensional vector in a vector space , a cluster is usually defined by a center and a radius(or a variance ) . However , a bibliographic network with center linkage structure , does not necessarily form several clusters around certain points significantly under input data involving information about objects’ types and their binary relations . Therefore , instead of centers and radiuses , we use probability parameters to model a specific cluster by constructing a generative model , where Θ = ( θ1 , θ2 , . . . , θK ) are global parameters and each θk = {pk(vi)|vi ∈ D} ∪ {pk(uj|Ot)|uj ∈ Ot , Ot = A , B , W} ∪ {pk(Ot)|Ot = A , B , W} ∪ {pk} represents the conditional probability of an individual center or attribute object in a given cluster and the prior probability of the kth cluster . Here , pk(vi ) is a shorthand for p(vi|D , ck ) and pk(uj|Ot ) for p(uj|Ot , ck ) and pk(Ot ) for p(Ot|ck ) and pk for pk(ck ) . There are some ∑ normalization conditions that Θ should satisfy :
∑
∑ pk(vi ) = 1 , vi∈D pk(uj|Ot ) = 1 , uj∈Ot
Ot=A,B,W IV . MODEL AND ALGORITHM pk(Ot ) = 1
This section presents two graph models based on center linkage structure and describes a way of message passing , which constitutes the basic part of BibClus .
A . P HMFR on Center Linkage Structure
Given a center linkage structure , we suppose that each center node vi has an attached hidden variable zi , ranging over a set of cluster labels , and generates xi as an observed variable . We further suppose that there is some structure of statistical dependency between zi and its neighbors zj , described by an undirected graph . The HMRF here is said to be “ pairwise ” because each pair of nodes zi and zj shares a relation of statistical dependency , represented as ψij(zi , zj ) . Also , at each position of zi , a statistical dependency exists between zi and its observed node xi called the “ evidence ” , written as ϕi(zi , xi ) . Then , the overall joint probability is :
∏
∏ p(Z , X ) =
1 Λ
ψij(zi , zj )
ϕi(zi , xi )
( ij ) i where Λ is a normalization constant .
Note that , given cluster labels of all Z , the set of observed variables X are conditionally independent : p(xi|zi ) p(X|Z ) =
∏ i
( 1 )
( 2 )
Each observed variable xi is a set of attribute objects linking to zi . We make an assumption that given zi , the probabilities to generate different attribute objects are conditionally independent . Hence , with parameters θk characterizing the cluster ck , we have :
∏ p(um|θk)wim
∏ p(xi|zi = ck ) = p(xi|θk ) = um∈xi ( pk(um|Oum)p(Oum ))wim
= um∈xi where wim is the weight of the relation between vi and um , and Oum is the type of attribute object um . mψik→zi mzi→ψij
ψij zj mψij→zi mzj→ψij mϕi→zi zi
ϕi
Figure 2 . Converting a P HMRF to a factor graph
Figure 3 . A factor graph fragment showing message update rule
The priori probability of hidden variable zi represented as p(zi = ck ) can be viewed as the posterior probability for object vi in the kth cluster , which we can write as p(ck|vi ) . So by Bayes’ Rule , we can simply calculate the ∑ priori probability of zi : pk · pk(vi ) p(zi = ck ) = p(ck|vi ) = k pk · pk(vi ) Likewise , the posterior probability for observed variable ∑ p(zi = ck)p(xi|zi = ck ) k p(zi = ck)p(xi|zi = ck )
∑ p(ck)p(vi|ck ) k p(ck)p(vi|ck ) xi can be calculated as follows : p(zi = ck|xi ) =
( 3 )
=
In order to avoid zero probability during calculation , we adopt a naive smoothing method using uniform distribution with a smoothing parameter τ , before applying the parameters θk to calculate the probabilities listed above :
( vi ) = τ · pk(vi ) + ( 1 − τ ) · 1|D| ( uj ) = τ · pk(uj|Ouj )pk(Ouj ) + ( 1 − τ ) · pnew k pnew k
B . Message Passing on Factor Graph
1
|A ∪ B ∪ W|
In order to obtain marginal probabilities from joint probability , we build a factor graph according to the factorization form of p(Z , X ) , and implement the message passing algorithm .
1 ) Converting P HMRF to Factor Graph : We rewrite the joint probability function of P HMRF as :
∏
∏ p(Z ) =
1 Λ
ψij(zi , zj )
ϕi(zi )
( ij ) i
( 4 ) where ϕi(zi ) is a shorthand for ϕi(zi , xi ) with considering xi to be fixed . We focus on the joint probability of unknown variables Z , and we are interested in a marginal probability of zi , which is also the posterior probability of object vi belonging to a cluster . To compute p(zi ) , we have to sum p(Z ) over all possible values of the variables and their combinations that just have zi fixed as p(z1 , . . . , zn ) . Unfortunately , such p(zi ) = a computation works in a time that grows exponentially with N . So , we will introduce an efficient method based on
{z1,,zn}\zi
∑ message passing on a factor graph , whose time complexity increases by N .
A factor graph is a bipartite graph that expresses the structure of function factorization . It has a variable node for each variable in function arguments , and a factor node for each local function . Now we describe how to convert a P HMRF into an equivalent factor graph . Fig 2 shows the transformation from a P HMRF into a factor graph . Note that we only include hidden variables as variable nodes , shown as a circle . Instead of ϕi(zi , xi ) , we use ϕi(zi ) without observed node xi as a factor node , which a square with one link stands for . The function ψij(zi , zj ) is denoted as a square with two links as the other type of factor node . the message from factor node ϕi
2 ) Implementing Message Passing : Before describing the message passing algorithm , we introduce three kinds of messages : to variable node zi denoted as mϕi→zi(zi ) , from factor node ψij to zi as mψij→zi(zi ) and from zi to ψij as mzi→ψij ( zi ) . Each message , such as mψij→zi(zi ) , is a vector of the same dimensionality as zi , with each component being proportional to how likely node ψij thinks it is that variable zi might be in the corresponding state . These messages are updated according to the rule illustrated in Fig 3 , usually called the sum product update rule : mϕi→zi(zi ) = ϕi(zi ) mψij→zi ( zi ) = zj mzi→ψij ( zi ) = mϕi→zi ( zi )
∏
ψij(zi , zj)mzj→ψij ( zj ) mψik→zi ( zi )
ψik∈N ( zi)\ψij
( 5 ) ( 6 )
∑
( 7 ) where N ( zi ) is the set of the neighbors of zi . In another way , we can only take mψij→zi(zi ) into account by substituting Eq 5,6 into Eq 7 : mψij→zi(zi ) =
ψij(zi , zj)ϕj(zj )
∑
∏ mψkj→zj ( zj ) zj
ψkj∈N ( zj )\ψij
At the end , the marginal function p(zi ) will be the product of all incoming messages :
∏ p(zi ) = ϕi(zi )
ψij∈N ( zi ) mψij→zi ( zi )
( cid:43)(cid:76)(cid:71)(cid:71)(cid:72)fi(cid:71)(cid:72)(cid:50)(cid:86)(cid:72)(cid:85)(cid:72)(cid:71)fi(cid:71)(cid:72)(cid:85)(cid:76)(cid:79)(cid:72)fi(cid:71)(cid:72)(cid:87)(cid:85)fi(cid:71)(cid:72 ) According to the factor graph theory , the message passing algorithm gives the exact marginal probabilities for all variable nodes in a cycle free graph . However , it seems that the argument for the exactness will break down when cycles are present in graph . In fact , some equivalent algorithms have achieved excellent experimental results in error correcting codes defined on Tanner graphs with cycles [ 14 ] and etc . Yedidia et al . showed that the fixed points of the algorithm correspond to Bethe free energy minima [ 13 ] . And McEliece et al . conjectured that the algorithm on graphs with cycles converges with a high probability to an approximate optimal solution [ 18 ] .
Now we present the message passing algorithm for a graph with cycles . The message passing schedule must synchronize messages with a global discrete time clock , with at most one message passed on any edge in any given direction at one time . As shown in Algorithm 1 , we suppose that all the messages can be initialized with an unit function . Then , each message sent from v at time t will just depend on the local factor function and the most recent messages received at v at time t− 1 , and replace the previous message in the same direction of the same edge . We terminate the schedule after a fixed number of iterations or the change of messages between at t and t − 1 falls below a threshold .
Algorithm 1 The message passing schedule Input : factor graph G , factor functions ϕi and ψij ; Output : marginal probabilities p(zi ) ; 1 : Initialize the messages m(0 ) for all e ∈ E . 2 : for t = 1 , . . . , T or until convergence do Update m(t ) using m(t−1 ) for all e ∈ E . 3 : 4 : end for 5 : Compute p(zi ) using ϕi(zi ) and m(t)(zi ) arriving at zi .
V . FITTING PARAMETERS
In the model discussed in Section 4 , both the parameters Θ and local factor functions are unknown . This section presents the method to estimate factor functions and fit model parameters , which is the focal part of our algorithm .
A . EM Framework
The EM algorithm can be applied to learn the model parameters Θ . In a P HMRF model , since both Z and Θ are unknown , the clustering problem on a bibliographic network can be viewed as an “ incomplete data ” problem solved by EM .
First , we define a likelihood function as L(Θ ; X , Z ) = p(X , Z ; Θ ) , called the complete data likelihood . Note that it is intractable to directly maximize L(Θ ; X , Z ) , because Z is unknown , hidden , and presumably governed by an underlying distribution . We take the expected value of the complete data log likelihood log L(Θ ; X , Z ) with respect to
= p(Z|X ; Θ(t ) ) log p(Z ; Θ ) + the conditional distribution of Z given X under the current ∑ ∑ parameters Θ(t ) , which is called E Step : Q(Θ|Θ(t ) ) = EZ|X;Θ(t)[log L(Θ ; X , Z ) ] = 1⃝ + 2⃝ where 1⃝ indicates the first part and 2⃝ indicates the second part.Then , we compute a better estimator of Θ that maximizes Q to be the new parameters , called M Step :
Z
Z p(Z|X ; Θ(t ) ) log p(X|Z ; Θ )
Θ(t+1 ) = arg max
Θ
Q(Θ|Θ(t ) ) the process of E Step and M Step will
Finally , iterate until convergence . Although there is no guarantee that Θ(t ) converges to a global maximum likelihood estimator , it may converge to a local maximum depending on initial values . In the P HMRF model based on center linkage structure , the value of p(Z|X ; Θ(t ) ) can be approximately obtained under the current parameters Θ(t ) , leaving unknown p(Z ; Θ ) and p(X|Z ; Θ ) . The parameters of each cluster θk , defined in Section 3 , can be split into two parts the generative probabilities of center objects and attribute objects . Since the two parts’ normalization conditions are independent to each other , the estimating of them might be handled respectively . Also , p(X|Z ; Θ ) involves only the attribute part of Θ , because its factorization form merely contains the generative probabilities of attribute objects according to Eq 1,2 . And p(Z ; Θ ) has solely to do with the hidden variables relating to the center part . Therefore , the procedure to find the maximum of Q should be equivalent to the procedure that we obtain the maximum of 1⃝ and 2⃝ respectively . B . Estimating Attribute Part
Let ’s deal with 2⃝ first , for the way to locate its maximum , which is shown below , is relatively easier . The unknown conditional distribution p(X|Z ; Θ ) is not related to the hidden graph structure , and can be factorized as the product of local conditional probabilities of xi given zi under Θ : p(zi|X ; Θ(t ) ) log p(xi|zi ; Θ ) p(zi|xi ; Θ(t ) ) log p(xi|zi ; Θ )
2⃝ = ≈
∑ ∑ i
∑ ∑ zi i zi an approximation
Here , we make that p(zi|X ; Θ(t ) ) ≈ p(zi|xi ; Θ(t) ) , so that we can make use of the known values of p(zi|xi ; Θ(t) ) , calculated under Θ(t ) by Eq 3 . For each zi , we have : assumption
( p(zi = ck|xi ; θ(t ) k ) log p(xi|zi = ck ; θk ) p(zi = ck|xi ; θ(t ) k ) log pk(um)wim
)
( 8 )
∏ um∈xi
∑ ∑ ck
= ck which is sort of like a mixture model . To estimate the parameters , ie pk(um ) here , which maximize the likelihood , we substitute Eq 8 into part 2⃝ and rewrite it as follows : 2⃝ =
log pk(um ) wimp(zi = ck|xi ; θ(t ) k )
∑
∑
∑
 m ck i∈ym i∈ym and obtain : ˆpk(um ) ∝∑ where ym = {i|um ∈ xi} is a set of indices of center objects that is equivalent to an inverted list of xi which contains um , and pk(um ) is to be estimated . We use Lagrange multiplier to solve the optimization problem with constraints , wimp(zi = ck|xi ; θ(t ) k ) . After obtaining ˆpk(um ) , we can further compute pk(Ot ) and pk(um|Ot ) , which is the attribute part of Θ(t+1 ) . C . Estimating Center Objects’ Part As for 1⃝ , we cannot derive a formula containing generative probabilities as 2⃝ for p(Z|Θ ) . The estimating task cannot be finished directly . We have to factorize p(Z|Θ ) into local factor functions , so that we are able to infer marginal probability for each variable zi by using the message passing algorithm , and then obtain the center part of parameters , ie pk(vi ) and pk . Before passing messages , we need to estimate the factor functions that maximize the likelihood .
∑
∑
ϕi(zi ) = 1 ,
Note that “ amplitude ” differences of factor functions have no impact on the joint probability , which organizes factor functions in the form of a product with a constant Λ to normalize . We offer a normalization condition for each ψij(zi , zj ) = 1 . factor function : Although “ amplitude ” for factor functions is ineffective , we still think different factor nodes have different importance and we should design a form or a parameter to regulate them . It ’s obvious that a factor function has no effect when being an unit function , ie ϕi(zi ) ≡ 1 K . Hence , by combining an original factor function with an unit function , we consider a new factor function of ϕnew
( zi ) defined as follows : zi,zj zi i
· ( 1 − γi )
1 K
( zi ) = ϕi(zi ) · γi + i i i
ϕnew
( 9 ) where γi ( 0 ≤ γi ≤ 1 ) is a parameter to regulate the importance of ϕi(zi ) . It seems that unit functions act as “ diluent ” for factor nodes here . By “ dilution “ , each com(zi ) has a nonnegative lower bound , that is , ponent of ϕnew ( zi ) ≥ 1−γi K for all values of zi . ϕnew
1 ) Univariate Factor Function : Each univariate function ϕi(zi ) characterizes the local feature of a single variable node . We regulate it using a parameter γi like Eq 9 . Parameter setting of γi should depend on the specific problem itself . Since our task is to cluster objects on a bibliographic network , the probability of variable node zi indicates how likely the node belongs to each cluster , which both the node itself and its neighbors give the contribution to . Under the previous parameters Θ(t ) and message passing procedure , for zi , we have the probability p(t)(zi ) and all the messages sent from ψij as m(t ) ψij→zi local influence loi at zi as :
( zi ) . Based on them , we define
{
{
−
−
( zi
∑ ( ∑ ∏ zi
)} )}
1 p(t)(zi )
1 m(t)(zi ) loi = exp p(t)(zi ) log and define external influence exi to zi as : exi = exp m(t)(zi ) log m(t)(zi ) =
1 Ji
ψij∈N ( zi ) m(t )
ψij→zi
( zi ) where Ji is a normalization constant . We suppose that γi has a positive correlation with loi and has a negative correlation with exi , so that we set γi = loi . In a later section , we will explain why we choose this setting in detail . loi+exi
2 ) Bivariate Factor Function : Each bivariate factor function ψij(zi , zj ) describes the interaction between two variable nodes linking to ψij . When zi takes on a cluster label ck with high probability , zj is more likely to be assigned to the same cluster . We define the form of ψij(zi , zj ) as :
ψij(zi , zj ) = eλi;j If0g(zi−zj )
1 Λi,j taking on the same cluster label , Λi,j where λi,j is used to weigh the interaction intensity between zi and zj is a normalization constant and I{0}(· ) is an indicator function defined as I{0}(x ) = 1 if x = 0 and I{0}(x ) = 0 otherwise . Note that λi,j must be non negative , for it makes no sense that λi,j falls below zero , meaning that the mutual influence between zi and zj on distinct cluster labels could overtake that on the same labels . To insure the effectiveness and rationality of the interaction between zi and zj , we should set a positive lower bound λlow for λi,j . In a word , we want to control the message passing procedure by regulating factor functions to achieve a better clustering result . into 1⃝ and rewrite it as follows : 1⃝ = log ∑
3 ) Solving Optimization Problem : Let ’s substitute Eq 4 p(zi|X ; Θ(t ) ) log ϕi(zi)+ p(zi , zj|X ; Θ(t ) ) log ψij(zi , zj ) ∑ ∑
∑
∑
∑
1 Λ
+ zi i p(zi|xi ; Θ(t ) ) log ϕi(zi)+
( i,j )
∑ ≈ log zi,zj 1 Λ
∑
+ i zi p(zi|xi , Θ(t))p(zj|xj ; Θ(t ) ) log ψij(zi , zj )
( i,j ) zi,zj where we make another assumption that p(zi , zj|X ; Θ(t ) ) ≈ p(zi|xi ; Θ(t))p(zj|xj ; Θ(t ) ) and ignore Λ ’s variational amount when differentiating the formula for simplicity . To
∑ p(zi|xi ; Θ(t ) ) log ϕi(zi ) zi estimate ϕi and ψij , consider with constraints as:∑
∑ zi and constraints as : ∑ zi,zj
ϕi(zi ) = 1 , ϕi(zi ) ≥ 1 − γi
K p(zi|xi ; Θ(t))p(zj|xj ; Θ(t ) ) log ψij(zi , zj ) with
ψij(zi , zj ) = 1
ψij(zi , zj ) = zi,zj
1 Λi,j eλi;j If0g(zi−zj ) , λi,j ≥ λlow
{
It is a nonlinear programming problem with equality and inequality constraints . According to the Kuhn Tucker conditions , we obtain the eventual solution . For ϕi(zi ) : p(zi = ck|xi ; θ(t ) k ) ≤ ρ
1−γi ξ · p(zi = ck|xi ; θ(t )
K
ϕi(zi = ck ) = k ) otherwise 1 ) , . . . , p(zi = cK|xi ; θ(0 ) where ρ and ξ are to be calculated in the following . We arrange p(zi = c1|xi ; θ(0 ) K ) in ascending order to make a sequence ρ1 , . . . , ρK ( ρk ≤ ρk+1 ) . We define a function ¯h(m ) on m ∈ {1 , . . . , K − 1} as : )
K k=m+1 ρk ρm+1
1 − γi (
∑
¯h(m ) =
)
−
( ∑
K 1 − m fi
( 1−γi )
K
/
∗ ≤ ¯h(m ) and ρ = ρmfi , ξ = ∗ must satisfy : where m
¯h(m − 1 ) ≤ m For ψij(zi , zj ) , we define δ as :
( K − 1 ) p(zi , zj|X ; Θ(t ) ) −∑
∑ k p(zi = ck , zj = ck|X ; θ(t ) k ) { k p(zi = ck , zj = ck|X ; θ(t ) k ) δ ≥ eλlow log δ λlow otherwise
λij =
∑
δ = zi,zj and have :
K k=mfi+1 ρk
,
D . Running Time Analysis
There are two main loops in the entire process . The running time of the inner loop , calculating beliefs to estimate the center part of parameters indirectly , is about O(M · Ec ) , where Ec represents the edge number among center objects and M the average iteration number . Note that the message passing algorithm can be implemented by parallel computing . The outer loop is to estimate the attribute part of parameters and factor functions , of which the running time is about O(Ea + Ec ) in each iteration , where Ea represents the edge number between center objects and attribute ones . Suppose the iteration number of the outer loop is N , then the entire running time is O(N ( Ea+Ec+M·Ec) ) . Experimental reports show that the loops converge rapidly .
In summary , the whole algorithm BibClus works as shown in Algorithm 2 . We start with initial parameters Θ(0 ) . In E step , we compute probabilities p(zi|xi , Θ(t ) ) under the current parameters . In M step , we first estimate the attribute part of Θ , and then obtain factor functions that maximize Q and run the message passing algorithm to estimate the center part . The two steps are repeated until convergence . Note that the procedure of passing messages runs in a time that grows linearly with the edge number . So , BibClus can be very efficient .
Algorithm 2 The BibClus Algorithm Input : bibliographic network G , number of clusters K ; Output : posterior probabilities of each objects p(ck|oi ) ; 1 : Initialize the model parameters Θ(0 ) randomly . 2 : for t = 1 , . . . , T or until convergence do 3 :
E step : Give Θ(t ) , compute p(zi|xi ) according to Eq 3 base on a P HMRF .
4 : M step : By maximizing the likelihood , first estimate the attribute part of Θ . Second , estimate factor functions ϕi , ψij , and use message passing way as Algorithm 1 to compute marginal probabilities p(zi ) . Then , compute the center part of Θ . Update the model parameters Θ(t+1 )
5 : 6 : end for 7 : Compute p(ck|oi ) using the model parameters Θ .
VI . EXPERIMENTS
We now prove the effectiveness of our proposed algorithm BibClus on a bibliographic network by comparison with state of the art algorithms . In addition , we study on parameters which are related to the forms and constraints of local factor functions , and then study the convergence .
A . Data Set
We download more than 160000 web pages from ACM Digital Library1 by a web crawler . Each web page has a wealth of information , such as title , authors , conference , abstract , keywords ,references and index terms , etc , which is far more abundant than DBLP . We pick 10 classes according to primary classification from index terms , shown in Tab1 As we see , these 10 classes are similar , and we think it makes it more convincible to test the clustering effect . In bibliographic data , authors and conferences are ready made , while terms are acquired by extracting from titles , abstracts and keywords . We have totally got 16745 authors , 208 conferences , 26087 terms and 10021 papers to build a huge sparse bibliographic network , where each paper has 2.84 authors , 1.0 conference , 30.38 terms and 2.75 cited papers .
1ACM Digital Library : http://portalacmorg/
Table I
TEN SELECTED CLASSES
ACM Classification E.Data E1Data Structures H.Information Systems H2Database Management H24Systems H.Information Systems H2Database Management H28Database applications Subjects:Data mining H.Information Systems H2Database Management H25Heterogeneous Databases H.Information Systems H3Information Storage and Retrieval H33Information Search and Retrieval I.Computing Methodologies I2Artificial Intelligence I21Applications and Expert Systems I.Computing Methodologies I2Artificial Intelligence I24Knowledge Representation Formalisms and Methods I.Computing Methodologies I2Artificial Intelligence I26Learning I.Computing Methodologies I2Artificial Intelligence I27Natural Language Processing I.Computing Methodologies I5Pattern Recognition
#Paper
312 1539 1021 86 3638 547 403 1177 361 972
COMPARISON RESULTS ON TEN CLASSES
Table II
COMPARISON RESULTS ON FIVE CLASSES
Table III
Purity 0.4393 0.4603 0.5076 0.5241
NMI 0.1490 0.1781 0.2256 0.2393
RI 0.7252 0.7466 0.7723 0.7839
Pr . 0.2257 0.2673 0.3544 0.4382
Re . 0.1659 0.1688 0.1979 0.3680
F . 0.1912 0.2069 0.2540 0.4001
Algorithm K means
Model based
NetClus BibClus
Purity 0.4588 0.5116 0.5741 0.6221
NMI 0.1163 0.1599 0.2064 0.2123
RI 0.6428 0.6788 0.7105 0.7199
Pr . 0.3234 0.3853 0.4614 0.4856
Re . 0.2804 0.2934 0.3501 0.4107
F . 0.3003 0.3332 0.3981 0.4450
Algorithm K means
Model based
NetClus BibClus
B . Comparison with Baseline Methods
1 ) Baseline Methods : We compare BibClus with the following clustering methods :
• K means : We represent each paper as an M dimension vector , where M is the total number of attribute objects and each component is 1 if the attribute object contained and 0 otherwise .
• Model based Clustering : This method also adopts the input data as K means , but uses EM algorithm to infer the parameters by building a mixture model . It has the same shortage as K means that network structures have never been taken into account .
• NetClus : This method is a ranking based clustering algorithm for heterogeneous information network with star network schema . But star network schema ignores potential linkage structure among center objects .
2 ) Evaluating Criterion : Since we have already acquired papers’ class information from index terms , we can use the set of classes as an evaluation benchmark or gold standard . Then we can compute an external criterion that evaluates how well the clustering matches the gold standard classes : ∑ • Purity : By assigning each cluster to the class which is most frequent in the cluster , the purity is measured k maxj |ωk ∩ cj| , where as : purity(Ω , C ) = 1 Ω = {ω1 , ω2 , . . . , ωK} is the set of clusters and N C = {c1 , c2 , . . . , cJ} is the set of classes . • NMI : Normalized mutual information . The criterion is ( H(Ω)+H(C))/2 , where I is
I(Ω,C ) defined as : N M I(Ω , C ) = mutual information and H is entropy .
• Rand index : Considering N ( N−1 ) pairs of objects , they are from the same class or not , and they may also be assigned to the same cluster or not . There are four kinds of results : true positive(TP ) , true negative(TN ) , false
2 positive(FP ) and false negative(FN ) . The rand index(RI ) measures the percentage of correct results as : RI = T P +F P +F N +T N .
T P +T N
T P +F N .
• Precision and Recall : They are two metrics to evaluate correctness . Precision is defined as P = T P/(T P + F P ) , while recall is defined as : R = T P and the recall as : F = ( β2+1)P·R
• F Measure : The metric considers both the precision β2P +R . Here , we set β = 1 . 3 ) Experimental Initialization : Except K means , the other three methods , including BibClus , utilize the EM algorithm in different ways . However , EM is prone to get stuck in local optima if the seeds are not chosen well . Since finding good seeds is critical for any application based on EM , we use an initial assignment which the K means clustering provides . To make it fair , the initial assignment should be applied to all the others , ie model based clustering , NetClus and BibClus .
4 ) Experimental Result : The first two clustering methods are very typical in traditional approaches , while the last two are based on heterogeneous information networks , considering linked data . The experimental results in Tab.2 show that the last two greatly exceed the first two on any criterion . That fully proves heterogeneous network structures play a great role in discovering potential clusters . We then focus on NetClus and BibClus . We know BibClus ’s difference from NetClus is that it integrates citation relations as a description of the underlying structures among center objects , and adopts a technology to regulate and pass the influence between two nearby nodes . The results tell us that BibClus is superior to NetClus , especially in precision and recall . The results of BibClus in Tab.2 are acquired under the parameter setting that γi = 0.5·loi and λlow = 30 , after 10 E step and 0.5·loi+exi M step iterations .
Figure 4 . Parameter study of fli and low
Figure 5 . LOCAL and LOEX
Relation between compactness / entropy and F measure in
Figure 6 . Convergence Study
Another experiment is taken on a higher level of the classification hierarchy , which we merge class labels into five classes : ( 1 ) data structures , ( 2 ) database management , ( 3 ) information storage and retrieval , ( 4 ) artificial intelligence , ( 5 ) pattern recognition . From Tab.3 , we can see that BibClus also achieves a better performance than NetClus on any criterion .
C . Study on Parameters
In our algorithm , there are two kinds of parameters γi and λlow to regulate the messages or influences . In order to measure the goodness of clustering under different settings of parameters . We define an internal criterion compactness , which is used to measure the difference between withincluster similarity and between cluster similarity :
Comp =
1 K pk(vi ) − 1
K − 1 pl(vi )
∑
 ∑ k vi∈Ck

∑ l̸=k
Also , we calculate the average entropy of posterior probabilities for all objects to evaluate the determinacy of a result . If an object has an approximately equal probability of belonging to each cluster , which corresponds to a high entropy , it ’s apparently a bad result .
1 ) Parameter γi : In a factor graph , each variable node has a parameter γi , which is used to regulate local univariate factor function . First , we design three ways to express γi . CONS is a simple way to set all γi to be a constant , γi = 1 , meaning no “ dilution ” . LOCAL considers local influence for each variable node defined in Section 5.3 and has γi = loi , for example , when a node zi got a better result with a lower probability entropy in recent previous step , its γi may increase to enhance the influence from itself . LOEX considers both local and external influences with γi as a proportion between them . We use compactness and f measure to test the impact of these three ways . From Fig 4(a ) , we can see that CONS has a high compactness but a low F measure , while LOCAL has a high F measure but a poor compactness . We think the reason may be that the network is so sparse that lots of nodes are trapped in small components or fragments of graph , especially isolated . In this situation , messages cannot be passed to them and external influence becomes very little . Then LOEX will improve the proportion of local influence but LOCAL cannot . Thus , LOEX is able to obtain a good f measure with a better compactness .
−α·H(zi ) in LOCAL and γi = α·loi α·loi+exi
Then , to fully explain that LOEX is better , we expand the forms of LOEX and LOCAL with a parameter respectively : in LOEX . γi = e When we vary α , both methods change their compactness , average entropy and F measure . Fig 5 shows the relations between compactness/average entropy and F measure , which proves that LOEX overmatches LOCAL .
2 ) Parameter λlow : The interaction intensity between zi and zj on the same value is measured by λij , which impacts the propagation of influence through factor node ψij . The value of λij is estimated by maximizing likelihood . A lower bound λlow is set to give a minimum constraint of λij . Fig 4(b ) shows the necessity of having a λlow . When λlow is below 5 , the F measure drops sharply , reaching to the level of baseline methods . The reason can be explained using the function form of ψij from a view of message passing . We emphasize that mutual interaction occurs intensely between two adjacent nodes zi and zj when they take on the same
CONSLOCALLOEX0020406081(a ) 0204060801000250303504045F−measure(b)CompactnessF−measure040506070809033034035036037038(a ) CompactnessF−measure LOEXLOCAL04060811214033034035036037038(b ) EntropyF−measure LOEXLOCAL0510152000204Iteration NumberNMI0510152000.5Iteration NumberF−measure0510152000.51Iteration NumberPrecision0510152000204Iteration NumberRecall0510152006081Iteration NumberEntropy05101520070809Iteration NumberCompactness cluster label . If lower bound λlow is two small , the estimated value might lose its meaning . When λlow grows to about 25 , the F measure enters a relatively stable high level . At this level , influence can spread quickly along chains consisting of a collection of connected nodes .
D . Study on Convergence
As an iteration algorithm with two level loops , its convergence should be considered to evaluate the method ’s efficiency . Fig 6 shows the changing of six criteria along with the outer loop iteration number . Note that we impose on the inner loop with a maximum iteration of 20 , because message changes will decrease nearly to zero after 20 iterations according to experiment reports . We can see all criteria converge rapidly and reach a stable level after only a few iterations . Though the EM framework can trap in local minimal solution , our results on F measure , precision and recall show the final levels are almost twice higher than the initial assignment given by K means , proving that our algorithm makes a marked improvement . As the initial assignment amounts to kind of hard clustering , the average entropy will rise up and the compactness will go down slightly at last , which indicates that a distribution over cluster labels instead of a hard assignment is used to describe object cluster membership .
VII . CONCLUSIONS
In this paper , we develop a new algorithm called BibClus for clustering on a bibliographic network , which is a typical heterogeneous information network involving rich information about objects and their relations . We consider the relations not only between center objects and attribute objects , but also among center objects , characterized by center linkage structure . Based on center linkage structure , we build a probabilistic graphical model of P HMRF to model the clustering problem . Then , a creative idea is shown to estimate model parameters by passing messages on factor graph under the EM framework . Our experiments on ACM data show that BibClus produces clustering results with a higher precision and recall than any baseline method . In future , we will study how to find the optimal regulating parameters automatically on a factor graph and extend this method to more general heterogeneous networks .
VIII . ACKNOWLEDGEMENT
This work is partially supported by Project 61170091 , supported by National Natural Science Foundation of China and Project 2009AA01Z136 and supported by the National High Technology Research and Development Program of China ( 863 Program ) .
REFERENCES
[ 1 ] S . White and P . Smyth , “ A spectral clustering approach to finding communities in graph , ” in SDM , 2005 .
[ 2 ] X . Xu , N . Yuruk , Z . Feng , and T . A . J . Schweiger , “ Scan : a structural clustering algorithm for networks , ” in KDD , 2007 , pp . 824–833 .
[ 3 ] Y . Sun , Y . Yu , and J . Han , “ Ranking based clustering of heterogeneous information networks with star network schema , ” in KDD , 2009 , pp . 797–806 .
[ 4 ] C . H . Q . Ding , X . He , H . Zha , M . Gu , and H . D . Simon , “ A min max cut algorithm for graph partitioning and data clustering , ” in ICDM , 2001 , pp . 107–114 .
[ 5 ] J . Shi and J . Malik , “ Normalized cuts and image segmenta tion , ” in CVPR , 1997 , pp . 731–737 .
[ 6 ] M . E . J . Girvan , M . Newman , “ Community structure in social and biological networks , ” PROCEEDINGS NATIONAL ACADEMY OF SCIENCES USA , vol . 99 , no . 12 , pp . 7821– 7826 , 2002 .
[ 7 ] X . Wang , N . Mohanty , and A . McCallum , “ Group and topic discovery from relations and their attributes , ” in NIPS , 2005 .
[ 8 ] S . Z . Li , Markov Random Field Modeling in Computer Vision .
Springer Verlag , 1995 .
[ 9 ] Y . Zhang , M . Brady , and S . M . Smith , “ Segmentation of brain mr images through a hidden markov random field model and the expectation maximization algorithm , ” IEEE Trans . Med . Imaging , vol . 20 , no . 1 , pp . 45–57 , 2001 .
[ 10 ] L . Getoor and B . Taskar , Introduction to Statistical Relational Learning ( Adaptive Computation and Machine Learning ) . The MIT Press , 2007 .
[ 11 ] W . Tang , Z . Lu , and I . S . Dhillon , “ Clustering with multiple graphs , ” in ICDM , 2009 , pp . 1016–1021 .
[ 12 ] F . R . Kschischang , B . J . Frey , and H A Loeliger , “ Factor graphs and the sum product algorithm , ” IEEE Transactions on Information Theory , vol . 47 , no . 2 , pp . 498–519 , 2001 .
[ 13 ] J . S . Yedidia , W . T . Freeman , and Y . Weiss , “ Understanding belief propagation and its generalizations , ” Mitsubishi Electric Research Laboratories , Tech . Rep . , Nov . 2001 . [ Online ] . Available : http://wwwmerlcom/reports/docs/TR2001 22pdf
[ 14 ] B . J . Frey and D . J . C . MacKay , “ A revolution : Belief propagation in graphs with cycles , ” in NIPS , 1997 .
[ 15 ] W . T . Freeman and E . C . Pasztor , “ Learning low level vision , ” in ICCV , 1999 , pp . 1182–1189 .
[ 16 ] H . Schwarz and M . Wien , “ DSP applications signal and image processing with belief propagation , ” vol . 25 , no . 2 , pp . 114–141 , 2008 .
[ 17 ] Frey and Dueck , “ Clustering by passing messages between data points , ” SCIENCE : Science , vol . 315 , 2007 .
[ 18 ] R . J . McEliece , D . J . C . MacKay , and J F Cheng , “ Turbo decoding as an instance of pearl ’s ” belief propagation ” algorithm , ” IEEE Journal on Selected Areas in Communications , vol . 16 , no . 2 , pp . 140–152 , 1998 .
