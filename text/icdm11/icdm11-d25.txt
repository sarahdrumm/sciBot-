Helix : Unsupervised Grammar Induction for Structured Activity Recognition
Huan Kai Peng , Pang Wu , Jiang Zhu , and Joy Ying Zhang {huankai.peng , pang.wu , jiang.zhu , joyzhang}@svcmuedu
Carnegie Mellon University , Moffett Field , CA , USA
Abstract—The omnipresence of mobile sensors has brought tremendous opportunities to ubiquitous computing systems . In many natural settings , however , their broader applications are hindered by three main challenges : rarity of labels , uncertainty of activity granularities , and the difficulty of multi dimensional sensor fusion . In this paper , we propose building a grammar to address all these challenges using a language based approach . The proposed algorithm , called Helix , first generates an initial vocabulary using unlabeled sensor readings , followed by iteratively combining statistically collocated sub activities across sensor dimensions and grouping similar activities together to discover higher level activities . The experiments using a 20 minute ping pong game demonstrate favorable results compared to a Hierarchical Hidden Markov Model ( HHMM ) baseline . Closer investigations to the learned grammar also shows that the learned grammar captures the natural structure of the underlying activities .
Keywords Ubiquitous Knowledge Discovery ; Heterogeneous
Sensor Fusion ; Unsupervised Grammar Induction .
I . INTRODUCTION
The penetration of smartphones with various embedded sensors makes available logging human activities with sensory data in large scale . Such a data quantity brings opportunities for Ubiquitous Computing Systems ( UCS ) to mine human activities . Three types of UCS applications depend upon accurate and flexible activity recognition :
∙ Context aware mobile computing utilizes users’ current activity , e,g . , whether the user is “ driving ” , “ meeting ” or “ jogging ” can help determine if an incoming call should be answered .
∙ Behavior ware mobile computing utilizes users’ behavior pattern , eg , turning on the home heater before the user returns home based on his/her commuting pattern , or allocating the wireless bandwidth based on the user ’s mobility pattern in an office building .
∙ Activity monitoring uses sensors embedded in the building/furniture or worn on body that allows caregivers to monitor elderlies who choose to age in place . In particular , detection of abnormal activities ( eg , accidental falls ) can be used for alerting the caregiver for rapid response .
Empirically , we observe two additional challenges in sensor based activity recognition compared to standard sequence classification . They are heterogeneous sensor fusion and unsupervised structural activity recognition .
In standard sequence classification , input data are usually one dimension sequences , such as words or DNA nucleotides . For UCS applications , activities are captured by various sensors such as accelerometers , gyroscopes , GPS receiver , WiFi receiver , microphone , and so on . These sensors are heterogeneous in data format ( eg , real number value or location coordinates ) , sampling rate , and semantic meaning . Past research has shown that utilizing multiple sensors can improve activity recognition .
Another practical challenge is the need for unsupervised structural activity recognition . Supervised methods use samples with predefined labels such as “ walking ” or “ running ” . Practically , however , not only it is unrealistic to assume large amounts of labeled data , but also it ’s limited to recognize only the predefined activities . In many applications , it is more useful to recognize activities at high level , or more generally , in a structured manner . Such an approach will allow a UCS to identify high level activities like “ playing a tennis game ” , lower level ones like “ a play ” , or even breaking down to “ step ” or “ swing ” , etc .
In this work , we propose a novel approach called Helix to induce the underlying grammar of human activities . Inspired by grammar induction in natural language processing , Helix is a greedy method that iteratively does two things . First , it identifies high level activities through bracketing two statistically collocated sub activities . For example , if “ swing backward ” and “ swing forward ” occur together much more than random , we merge them into “ swing backward then forward ” . Second , it generalizes formally or semantically similar activities into yet higher level activities . For example , if “ swing backward then forward ” and “ swing backward then tip ” looks similar and both occur mostly between “ moving back ” and “ hit the ball ” , then we generalize them into a higher level activity “ returning the ball ” . We evaluate Helix in our Lifelogger system , compare its performance to Hierarchical Hidden Markov Model ( HHMM ) , and find favorable results in both recognition accuracy and computational cost .
II . UNSUPERVISED ACTIVITY STRUCTURE LEARNING
USING A LANGUAGE BASED APPROACH
With illustrations in Figure 1 , we first define the problem as follows : given 𝑑 dimensional sensor readings 𝑆 = {𝑆1 , . . . , 𝑆∣𝑑∣} with some underlying activities , we define the multi level Activity Text as 𝐴 = {𝐴1 , . . . , 𝐴𝐿} of 𝐿 levels , where 𝐴𝑙 = {𝑎𝑙 𝑖 represents
1 , . . . , 𝑎𝑙∣𝐴𝑙∣} in which each 𝑎𝑙
Sa
Sb
Sc
A1
A2 A3
A1
B1
B2
C1
A1B2
B1C1
A1B2B1C1
A1
A1
A1
A1 B2
A1B2
B1
C1
B1C1
A1
B2
A1B2
B1
C1
B1C1
A1B2B1C1
A1B2B1C1
G :
A1
B2
B1
C1
V1 = {A1 , B2 , B1 , C1}
A1B2
B1C1
V2 = {A1B2 , B1C1}
A1B1B1C1
V3 = {A1B2B1C1}
A1 = {B1 , A1 , C1 , B2 , A1 , C1 , B1,B2 , A1 , B1 , C1 , B2 , A1} A2 = {B1C1 , A1B1 , A1 , B1C1 , A1B2 , B1C1 , A1B2} A3 = {A1B2B1C1 , A1 , A1B2B1C1 , A1B2B1C1}
Figure 1 . Grammar induction using Helix . 𝐴 , 𝑉 , and 𝐺 denote the multi level activities , vocabulary , and induced grammar , respectively . the 𝑖 th activity in level 𝑙 . Here 𝐴1 denotes lowest level activities ( eg , “ twisting the waist ” ) while 𝐴𝐿 denotes the highest ( eg , “ playing a game ” ) . Subject to 𝐴 , we define an Activity Grammar 𝐺 = {𝑉 , 𝑅} , where 𝑉 denotes a multilevel Activity Vocabulary of 𝐿 levels and 𝑅 denotes the Relations specifying the structure of the grammar . Specifically , 𝑉 = {𝑉 1 , . . . , 𝑉 𝐿} , where 𝑉 𝑙 represents the activity vocabulary of the 𝑙 th semantic level , such that each 𝑙 th 𝑖 ∈ {𝑉 1∪⋅⋅⋅∪𝑉 𝑙} . Accordingly , any directed level activity 𝑎𝑙 𝑖 → 𝑣𝑙2 relationship ( 𝑣𝑙1 is a superactivity or a generalized activity of 𝑣𝑙1 𝑖 .
𝑗 ) ∈ 𝑅 iff 𝑙2 > 𝑙1 and 𝑣𝑙2
𝑗
Definition . Unsupervised Activity Structure Learning . Subject to a set of unlabeled multi dimensional sensor readings 𝑆 generated by some activities , the Unsupervised Activity Structure Learning Problem is defined as finding a grammar 𝐺 = {𝑉 , 𝑅} that best describes the hierarchical activity structure embedded in 𝑆 .
We then describe how to find such a grammar 𝐺 using an unsupervised algorithm consisting of three main steps : ( 1 ) vocabulary initialization , ( 2 ) collocation discovery , and ( 3 ) vocabulary generalization .
A . Vocabulary Initialization using Time series Motifs
An established approach to build a vocabulary from timeseries is by finding time series motifs [ 8 ] , the recurring patterns of similar subsequences . As in the top left of Figure 1 , we extend [ 8 ] to extract similar subsequences . They are then assigned labels to form the initial vocabulary . In the example , 𝑉 1 = {A1 , B1 , B2 , C1} and 𝐴1 = {B1 , A1 , C1 , B2 , C1 , B1 , B2 , A1 , B1 , C1 , B2 , A1} .
B . Super Activity Discovery by Statistical Collocation
In Statistical Natural Language Processing ( SNLP ) , Collocation [ 7 ] is the “ expression of two or more words that correspond to some conventional way of saying things ” . In the context of super activity discovery , collocation is the combination of two activities where their joint occurrence frequency is significantly larger than what could be resulted randomly from their marginal frequencies .
To discover super activities using statistical collocation , we first assume that the activity pairs occurring jointly within a window may be components of one super activity , like an intertwined double helix . For example , the grey dotted box in Figure 1 includes 3 activity pairs ( [B1,B2 ] , [ B1,C1 ] , and [ B2,C1] ) . Note that the order within the pair matters : two sub activities from the same dimension are ordered by time ( [B1,B2] ) , whereas two sub activities from different dimensions are ordered by their dimension rank ( [B2,C1] ) . The rationale is that for a super activity , the ordering between its sub activities in the same dimension usually matters ( eg , leaning forward then stand up ) , but not so much for those from different dimensions ( doing an exam while feeling pressure ) .
By sliding the grey window along the current activity text ( 𝐴1 ) , we accumulate the joint frequencies of all activity pairs and record them in a Joint Frequency Table ( upper left of Figure 2 ) . The Marginal Frequency Table ( upper right of Figure 2 ) is also accumulated similarly . For example , the pair [ B1,C1 ] will account for one [ B1, ] and one [ ,C1 ] . Although the content of the two tables depends on the length and the step size of the sliding window , we found that the collocation discovery results are not sensitive to them , because as the size of a super activity grows , the region within which other activities can co occur with it also stretches . Empirically , setting the sliding window ’s step size to an atomic activity length ( eg , 0.5s ) and the window size to a multiple of it ( eg , 5s ) gives good results .
With the joint and marginal frequency tables , we test each activity pair for its statistical collocation significance . We do so by constructing the contingency table for each activity pair and calculating the 𝜒2 ( chi square ) statistics [ 7 ] :
𝜒2 =
∑
( 𝑂𝑖𝑗 − 𝐸𝑖𝑗)2
𝑖,𝑗
𝐸𝑖𝑗
( 1 )
Joinnt Freq . Taable w1 A1 A1 A1 B1 B1 B2 w2 B1 B2 C1 B2 C1 B2
Freq .
5 12 6 3 10 6
Freq .
Margiinal Freq . Table w1 A1 B1 B2 w2 B1 B2 C1
23 13 6 5 15 22
Continggency Taable of [ [A1,B2 ]
B2 ¬ B2
A1 12 3 15
¬ A1 11 16 27
χ2 =
23 19 42
42 × ( 12 × 16 − 11 × 3 )
23 × 19 × 15 × 27
= 5.99 ≥ α0.95 = 0.3841
Figure 2 . contingency table of the pair ( A1,B2 ) .
Joint and marginal frequencies of activity pairs , with the
A0A0A0A0
A0A0A0A0A0
A0A0A0A0A1
A0A0A0A0A0'
( a )
A1
A1
A5
A2
A1
A1
A9
A2
( b )
A5
A9
A5'
Figure 3 .
( a ) Content based and ( b ) context based generalizations . where 𝑂𝑖𝑗 denotes the ( 𝑖 , 𝑗) th entry of the contingency table and 𝐸𝑖𝑗 denotes the expectation of the entry derived from their marginals . For [ A1,B2 ] in the Figure 2 , 𝜒2 = 5.99 ≥ 𝜒2 0.05(1 ) = 3841 Therefore , we can merge [ A1,B2 ] to form a super activity with 95 % confidence level , here we use 𝛼𝑇 𝐻 = 𝜒2 0.05(1 ) as the threshold parameter controlling the merging strength . Repeating this step for three iterations , we obtain the grammar in the right bottom of Figure 1 .
C . Vocabulary Generalization
The collocation discovery works when there are repeating sequences of sub activities . For real human activities , however , repetitions are unlikely to be exact . For example , when one is playing ping pong , he may perform a cut , a lift , or a regular play , which can have various sequence combinations . In order to detect higher level activities in such a condition , we need to generalize similar activities before constructing the joint and marginal frequency tables . Such a clustering is done according to two similarity measures : ( 1 ) Content Similarity ( 𝜙𝑒 ) and ( 2 ) Context Similarity ( 𝜙𝑥 ) .
1 ) Content similarity ( 𝜙𝑒 ) : As in Figure 3(a ) , the first generalization accounts for activities similar in content , including two main cases . The first case is when multiple activities are both repetitions of the same sub activity , eg , A0A0A0A0 , and A0A0A0A0A0 ( left two nodes at the top ) . The second case is when two activities differ in only minor portion in their composition , eg , A0A0A0A0A0 , and A0A0A0A0A1 ( right two nodes ) .
To consider both of these cases of two similar phrases ( activities ) 𝑣1 and 𝑣2 , we first calculate their 𝑛 gram preci sion and recall . Wlog , we assume ∣𝑣1∣ ≤ ∣𝑣2∣ and 𝑛 = 2 . For the example in Figure 3(a ) , 𝑣1 = A0A0A0A0 and 𝑣2 = A0A0A0A0A1 , such that 𝑛𝐺𝑟𝑎𝑚(𝐴 ) consists of 3 A0A0 ’s and whereas 𝑛𝐺𝑟𝑎𝑚(𝐵 ) consists of 3 A0A0 ’s and 1 A0A1 . Accordingly , we have :
𝑃 𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 =
𝑅𝑒𝑐𝑎𝑙𝑙 =
∣𝑛𝐺𝑟𝑎𝑚(𝑣1 ) ∩ 𝑛𝐺𝑟𝑎𝑚(𝑣2)∣ ∣𝑛𝐺𝑟𝑎𝑚(𝑣1 ) ∩ 𝑛𝐺𝑟𝑎𝑚(𝑣2)∣
∣𝑛𝐺𝑟𝑎𝑚(𝑣1)∣ ∣𝑛𝐺𝑟𝑎𝑚(𝑣2)∣ 𝜙𝑒 = 2 × 𝑃 𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 × √ √ 𝑅𝑒𝑐𝑎𝑙𝑙 𝑅𝑒𝑐𝑎𝑙𝑙
𝑃 𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 +
= 1
= 0.75
= 0.93 ( 2 )
The square root of the recall term is to award that case when two phrases are very similar in content but differs in length . 2 ) Context similarity ( 𝜙𝑥 ) : The second generalization accounts for activities occurring within similar context . For example , A5 , and A9 in Figure 3(b ) are both preceded by 2 A1 ’s and followed by A2 , suggesting that these two activities are likely to be semantically similar . We represent the context of an activity as ⃗𝑐 = ( 𝑛1 , . . . , 𝑛∣𝑉 ∣ ) , representing the number of times every activity co occurs with the activity . Then the context similarity 𝜙𝑥 between two activities is the cosine distance between their context vectors :
𝜙𝑥 =
⃗𝑐1 ⋅ ⃗𝑐2 ∣ ⃗𝑐1∣∣ ⃗𝑐2∣
( 3 )
Before clustering , we need to aggregate the content and context similarities into one single similarity measure 𝜙 . Intuitively , the arithmetic , geometric , and harmonic means are all reasonable candidates . However , in order to make each of 𝜙𝑒 and 𝜙𝑥 sufficient for generalization ( instead of requiring both ) , we use the arithmetic mean . This aggregate 𝜙 is then used in a complete link clustering algorithm where a link exists between two activities only if their similarity 𝜙 is larger than a threshold 𝜙𝑇 𝐻 . Consequently , all activities grouped together in a cluster are all highly similar to each other . Algorithm 1 summarizes the overall Helix algorithm .
III . EXPERIMENTS
We implemented Helix as the activity structure discovery engine of our Lifelogger system [ 11 ] as in Figure 4 . The grammar learning for a 20 minute pingpong session takes less than 1 minute where the peak memory consumption is less than 10 MB . For the dataset , we collect all availible sensor readings from a smartphone plus a Electroencephalography ( EEG ) to record the electrical activity of the participant ’s brain . During the experiment , the participant first does three kinds of serves ( the cut , the lift , and the regular ) for 5 times each . Then he does 3 practice rounds with an opponent , followed by two matches for 11 and 6 points , respectively . These activities are annotated with three semantic levels of ground truth as depicted in Table I . The first level ground truth 𝑇1 is based on a play , eg , Cut , Lift , and Regular . The second level ground truth 𝑇2 is based on
THE pingpong DATASET’S GROUND TRUTH LABELS
Table I
Semantic Lv .
Level 1 ( 𝑇1 )
Level 2 ( 𝑇2 )
Level 3 ( 𝑇3 )
Label Cut Lift Regular CutReply LiftReply RegularReply CutServeTest LiftServeTest RegularServeTest CutReplyTest LiftReplyTest RegularReplyTest GameTest ServeTest ReplyTest MatchTest
Qty . 14 14 23 7 18 30 2 2 2 2 2 2 2 1 1 1 complicated and consumes >16GB memory for a height of 6 . We therefore stay with the setting based on ground truth .
A . Evaluation of the Learned Activity Grammar
We evaluate the learned grammar by the mutual information of its vocabulary with the ground truth . We first split the whole pingpong dataset into units of 1ms . In each unit 𝑖 , there is a ground truth label 𝑔𝑖 using one level of ground truth labels 𝑇 𝑘 , with a corresponding learned label 𝑝𝑖 using a level of learned vocabulary 𝑉 𝑙 . We then measure the similarity of the two labelings by :
MI(𝑇 𝑘 , 𝑉 𝑙 ) = H𝑝(𝑇 𝑘 ) + H𝑝(𝑉 𝑙 ) − H𝑝(𝑇 𝑘 , 𝑉 𝑙 )
( 4 ) where MI(𝑇 𝑘 , 𝑉 𝑙 ) denotes the mutual information between the two labelings , H𝑝(𝑇 𝑘 ) and H𝑝(𝑉 𝑙 ) denote the entropy of the ground truth and the learned labeling , respectively , and H𝑝(𝑇 𝑘 , 𝑉 𝑙 ) denotes the joint entropy of the two labelings . Figure 5 compares the results between Helix and HHMM with three levels of ground truth using different sensor dimension combinations . The 𝑦 axis denotes mutual information ; the 𝑥 axis denotes vocabulary level . Several peaks are marked with their y axis value , indicating the vocabulary levels that best describe the underlying activity .
Accordingly , three observations are made . ( 1 ) Using all sensor combinations , Helix ’s results generally have two peak regions ( the first at vocabulary level 2∼5 and the second at level 11∼17 ) . Since such peak regions occurs in similar vocabulary levels across all ground truth levels , this suggests that the activity structure underlying the pingpong dataset reflects these two levels by nature , which are naturally correlated to human annotated ground truth at all semantic levels . In contrast , HHMM ’s results have only 1 peaking region . This reveals the first advantage of Helix over HHMM : by offering more levels of vocabulary ( with < 10𝑀 𝐵 memory ) , it is more flexible in capturing the underlying activities in multiple granularities . This advantage is confirmed by the constantly better global peak mutual information of Helix ( 0.69 , 0.78 , 0.91 , and 0.91 ) over that of the classic
Figure 4 . Helix implementation in CMU ’s Lifelogger system . individual tests ( 5 to 10 plays ) , eg , CutServeTest and CutReplyTest . The third level 𝑇3 is based on groups of tests , eg , ServeTest , ReplyTest , and GameTest . We take 9 dimensional raw sensor reading data including 3 axis accelerometer ( X , Y , and Z axises represented as Sensor A ∼ C ) and EEG ( high pass and low pass signals for Alpha , Beta , and Gamma waves , represented as Sensor D ∼ I ) .
As a baseline , we implemented an Hierarchical HMM ( HHMM ) with the Matlab Bayes Net Toolbox [ 9 ] . To have a reasonably good baseline , we set the HHMM structure according to ground truth characteristics : the HHMM height is set to 3 and the number of states at level 1 , 2 and 3 are set to 3 , 5 , and 10 , respectively . We also tried to set the HHMM to have a large height , but that makes the model too
Algorithm 1 Hierarchical Activity Structure Discovery Input : 𝑆 = {𝑆1 , . . . , 𝑆𝑑} : 𝑑 dimensional sensor readings Input : 𝛼𝑇 𝐻 : merging threshold parameter Input : 𝛿𝑇 𝐻 : generalization threshold parameter Output : 𝐺 = {𝑉 , 𝑅} : discovered hierarchical grammar Output : 𝐴 : hierarchical activities labeled using 𝑉 1 : ( 𝐴1 , 𝑉 1 ) = initialize the vocabulary by motif discovery 2 : ( 𝐴′ , 𝑉 ′ ) = ( 𝐴1 , 𝑉 1 ) 3 : 𝑙 = 1 4 : while TRUE do 5 : 6 : 7 : 8 : 9 : 10 : 11 : 12 : end while 13 : 𝑉 = {𝑉 1 , . . . , 𝑉 𝑙−1} 14 : return 𝐺 = {𝑉 , 𝑅} , 𝐴
𝑙 = 𝑙 + 1 ( 𝐴𝑙 , 𝑉 𝑙 ) = discover collocations from ( 𝐴′ , 𝑉 ′ ) break if ∣𝑉 𝑙∣ == 0 for all 𝑣𝑖 ∈ 𝑉 𝑙 do end for ( 𝐴′ , 𝑉 ′ ) = generalize the vocabulary from ( 𝐴𝑙 , 𝑉 𝑙 ) add edges ( 𝑣𝑖 , 𝑣𝑗 ) into 𝑅 for all collocations
Helix
HHMM
0.69
1 0.8 0.6 0.4 0.2 0
0.38
1
3
5
7
9
11
13
15
1
2
3
4
Vocabulary Level
( a ) 1 sensor dimensions : A
0.75
0.57
1 0.8 0.6 0.4 0.2 0
1
3
5
7
9
11 13 15 17
1
2
3
4
Vocabulary Level
( b ) 3 sensors dimensions : ABC
0.91
1
3
5
7
9
0.90
1
3
5
7
9
1 0.8 0.6 0.4 0.2 0
0.35
1 0.8 0.6 0.4 0.2 0
13
11 Vocabulary Level
1
2
3
4
( c ) 6 sensors dimensions : DEFGHI
0.26
1
2
3
4
13
11 Vocabulary Level
( d ) 9 sensors dimensions : ABCDEFGHI
Figure 5 . Mutual information result comparison between Helix and HHMM with three levels of ground truth using different sensor dimension combinations : ( a ) accelerometer x axis ( A ) , ( b ) accelerometer ’s 3 axises ( ABC ) , ( c ) 6 EEG dimensions ( DEFGHI ) , and ( d ) all accelerometer and EEG sensors ( ABCDEFGHI ) . HHMM ( 0.38 , 0.58 , 0.34 , 0.25 ) using various sensor combinations , respectively . ( 2 ) A closer look at the two local peak regions shows that the peaks of lower level groundtruth often occur at slightly lower vocabulary levels . For different sensor combinations from Figure 5(a ) to Figure 5(d ) , the peaks for 𝑇1 occur at vocabulary levels 9 , 2 , 2 , and 2 ; the peaks for 𝑇2 occur at vocabulary levels 11 , 3 , 3 , and 3 ; the peaks for 𝑇3 occur at vocabulary levels 11 , 3 , 4 , and 3 . This further confirms that the learned grammar properly reflects the human annotated ground truth . ( 3 ) For all sensor combinations , the results of 𝑇2 and 𝑇3 are very similar in higher vocabulary levels . This can be explained by the fact that 𝑇2 and 𝑇3 reflect very similar semantic levels themselves . Plus , looking at Table I , each label of 𝑇3 has only 1 instance . Such a non repeating pattern is difficult to discover , and thus its best corresponding grammar n o i t a m r o f n
I l a u t u M
1
0.8
0.6
0.4
0.2
0
1 0.8 0.6 0.4 0.2 0 n o i t a m r o f n
I l a u t u M
1 0.8 0.6 0.4 0.2 0 n o i t a m r o f n
I l a u t u M
1 0.8 0.6 0.4 0.2 0 n o i t a m r o f n
I l a u t u M
T1 T2 T3
T1 T2 T3
T1 T2 T3
T1 T2 T3
1
0.8
0.6
0.4
0.2
0 n o i t a m r o f n
I l a u t u M
Helix
HHMM
1
0.8
0.6
0.4
0.2
0
T1 T2 T3
A
ABC
DEFGHI ABCDEFGHI
A
ABC
DEFGHI ABCDEFGHI
Sensor Dimension Combination
Figure 6 . Peak mutual information result for Helix and HHMM with three levels of ground truth using different sensor dimension combinations . source
LiftReply[1 ]
CutReply[1 ] node139
Regular[1 ]
RegularReply[1 ]
Cut[1 ] Lift[1 ]
LiftTest[2 ]
LiftReplyTest[2 ] RegularTest[2 ] GameTest[2 ] CutTest[2 ] ServeTest[3 ] ReplyTest[3 ] MatchTest[3 ]
CutReplyTest[2 ]
RegularReplyTest[2 ] sink
Figure 7 . The learned grammar using Helix . level could actually match the one of 𝑇2 , given 𝑇2 and 𝑇3 represent the same underlying activities .
To see how the effects associated with multiple sensors , we further plot the global peak mutual information values separately in Figure 6 . As more ( 1 , 3 , 6 , and 9 ) sensors are incorporated , the performance of HHMM degrades sharply , due to the known difficulty of learning patterns from heterogeneous sensors . In contrast , the performance of Helix shows little degradation . This shows the second advantage of Helix : it combines only statistically relevant sensors to form higher level activities , which can adapt to each activity dynamically . Therefore , as additional sensors are provided , Helix can harvest the additional information while being less confused by the associated noise .
B . Microscope Analysis of Grammar Structure
We visualize the learned activity grammar in Figure 7 . To keep a reasonable number of nodes for visual clarity , we only use sensor A with the parameters of peak mutual information . The source and the sink are virtual nodes ; the node level immediately below the source represents the initial vocabulary ; the one immediately above the sink node represents the highest level activity . We annotate all groundtruth labels to the respective nodes having the highest mutual information , where the labels’ semantic levels are appended in the brackets . Four observations can be made : ( 1 ) from top to bottom , the vocabulary size , ie , the number of nodes at each level , grows and shrinks iteratively . This corresponds to the fact that merging ( gray edges ) usually increases the vocabulary size whereas generalization decreases it . This maintains a reasonable vocabulary size in the learning process , and explains why Helix can yield more granularity levels with less memory consumption ( <10MB for 15 levels ) compared to HHMM ( 16GB for 6 levels ) . ( 2 ) There are cases that several labels share the same node of the highest mutual information . Since the learning is completely datadriven , this suggests that from the data ’s perspective , theses labels may reflect to very similar activities . ( 3 ) All 𝑇3 labels matches to the same node that is also shared with some 𝑇2 labels . This confirms our previous conjecture that the non repeating patterns in 𝑇3 correspond to the same higherlevel vocabularies as that of 𝑇2 in the learned grammar . ( 4 ) Even though there is no one to one mapping between the learned grammar and human annotated labels , the labels with higher semantic levels still correspond to higher level nodes closer to the figure ’s bottom , and vice versa . This shows the learned grammar and human annotated labels capture the same underlying activity structure .
IV . RELATED WORK
There are only few unsupervised activity recognition works compared to supervised ones . In [ 10 ] , the authors combine Hidden Markov Model ( HMM ) and Gaussian Mixture Model ( GMM ) to cluster accelerometer data into groups.Also , [ 3 ] , [ 2 ] use K means to discover low level activity clusters while applying topic modeling to discover high level activities.For sensor selection and fusion , [ 6 ] , [ 5 ] , [ 4 ] compare the predictive performances of various sensors.Also , [ 1 ] , [ 12 ] applies static fusion to multiple sensors .
V . CONCLUSION
In this paper , we propose an unsupervised algorithm , Helix , for building an activity grammar to address the three key challenges for ubiquitous computing systems in natural settings , namely , rare labels , unknown activity granularities , and varying sets of activity defining sensors . The experiments using a 20 minute ping pong dataset demonstrate favorable results compared to a Hierarchical Hidden Markov Model ( HHMM ) baseline . Helix ’s advantage is three fold : ( 1 ) It is an unsupervised algorithm , which still reserves the flexibility to utilize partial or later available labels . ( 2 ) It is built from bottom up , considering all activity granularities . ( 3 ) It fuses heterogenous sensors dynamically , and is more resilient to the noise introduced by multi dimensional sensors . Accordingly , this work contributes to the ubiquitous computing community by being the first work to our knowledge that considers all the three key challenges to understand human activities in natural settings . In the future , we plan to : ( 1 ) improve the grammar quality , ( 2 ) maintain an online grammar , and ( 3 ) do multi level recognition .
VI . ACKNOWLEDGEMENT
This work was supported by CyLab at Carnegie Mellon under grants DAAD19 02 1 0389 , W911NF 09 1 0273 from the Army Research Office , Google research award on “ Social Behavior Sensing and Reality Mining ” and the Cisco research award on “ Behavior Modeling for Human Network ” . We would also like to thank Ross Grieshaber for proofreading the manuscript .
REFERENCES
[ 1 ] U . Blanke , B . Schiele , M . Kreil , P . Lukowicz , B . Sick , and T . Gruber . All for one or one for all ? Combining Heterogeneous Features for Activity Spotting . In International Conference on Pervasive Computing and Communications Workshops ( PERCOM Workshops ) . Ieee , Mar . 2010 .
[ 2 ] K . Farrahi and D . Gatica perez . Discovering Routines from Large Scale Human Locations using Probabilistic Topic Models . ACM Transactions on Intelligent Systems , 2010 .
[ 3 ] T . Huynh , M . Fritz , and B . Schiele . Discovery of activity International conference on patterns using topic models . Ubiquitous computing UbiComp ’08 , page 10 , 2008 .
[ 4 ] T . Huynh and B . Schiele . Analyzing features for activity recognition . Joint conference on Smart objects and ambient intelligence innovative context aware services : usages and technologies ( sOc EUSAI ) , 2005 .
[ 5 ] J . Lester , T . Choudhury , N . Kern , G . Borriello , and B . Hannaford . A Hybrid Discriminative / Generative Approach for Modeling Human Activities . In International Joint Conference on Artificial Intelligence ( IJCAI ) , 2005 .
[ 6 ] B . Logan , J . Healey , M . Philipose , E . Tapia , and S . Intille . A long term evaluation of sensing modalities for activity recognition . In International conference on Ubiquitous computing , pages 483–500 , 2007 .
[ 7 ] C . D . Manning and H . Schuetze . Foundations of Statistical
Natural Language Processing . The MIT Press , June 1999 .
[ 8 ] A . Mueen , E . Keogh , Q . Zhu , and S . Cash . Exact discovery of time series motifs . Proceedings of the SIAM , 2009 .
[ 9 ] K . Murphy and M . Paskin . Linear Time Inference in Hierarchical HMMs . In Advances in neural information processing systems , 2002 .
[ 10 ] A . Nguyen , D . Moore , and I . McCowan . Unsupervised clustering of free living human activities using ambulatory accelerometry . the IEEE Engineering in Medicine and Biology Society ( EMBS ) , Lyon , France , 2007 .
In International Conference of
[ 11 ] P . Wu , H K Peng , J . Zhu , and Y . Zhang .
SensCare : Semi Automatic Activity Summarization System for Elderly Care . In International Conference on Mobile Computing , Applications , and Services ( MobiCASE ) , 2011 .
[ 12 ] A . Zinnen , U . Blanke , and B . Schiele . An Analysis of SensorInterna
Oriented vs . Model Based Activity Recognition . tional Symposium on Wearable Computers , Sept . 2009 .
