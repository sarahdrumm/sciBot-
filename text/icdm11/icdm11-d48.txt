Learning from Negative Examples in Set Expansion
Prateek Jindal
Dept . of Computer Science
UIUC
Urbana , IL , USA jindal2@illinois.edu
Dan Roth
Dept . of Computer Science
UIUC
Urbana , IL , USA danr@illinois.edu
Abstractâ€”This paper addresses the task of set expansion on free text . Set expansion has been viewed as a problem of generating an extensive list of instances of a concept of interest , given a few examples of the concept as input . Our key contribution is that we show that the concept definition can be significantly improved by specifying some negative examples in the input , along with the positive examples . The state of the art centroid based approach to set expansion doesnâ€™t readily admit the negative examples . We develop an inference based approach to set expansion which naturally allows for negative examples and show that it performs significantly better than a strong baseline .
I . INTRODUCTION
This paper addresses the task of set expansion on free text . Set expansion has been viewed as a problem of generating an extensive list of instances of a concept of interest , given a few examples of the concept as input . For example , if the seed set is {Steffi Graf , Martina Hingis , Serena Williams} , the system should output an extensive list of female tennis players .
We focus on set expansion from free text , as opposed to web based approaches that build on existing lists . The key method used for set expansion from free text is distributional similarity . For example , the state of the art systems [ 1 ] , [ 2 ] use a centroid based approach wherein they first find the centroid of the entities in the seed set and then find the entities that are similar to the centroid . Most of the work on set expansion has focussed on taking only positive examples . For example , as discussed above , to produce a list of female tennis players , a few names of female tennis players are given as input to the system . However , just specifying a few female tennis players doesnâ€™t define the concept precisely enough . The set expansion systems tend to output some male tennis players along with female tennis players . Specifying a few names of male tennis players as negative examples defines the concept more precisely .
Table I compares the state of the art approach for setexpansion on free text with the approach presented in this paper . The table shows only a small portion of the lists generated by the system . We used 7 positive examples for both approaches , and only 1 negative example was used for the proposed approach . The errors have been underlined
FEMALE TENNIS PLAYERS
State of the art Monica Seles
Steffi Graf
Martina Hingis
Mary Pierce
Lindsay Davenport Jennifer Capriati
Kim Clijsters
Mary Joe Fernandez
Nathalie Tauziat
Kimiko Date
Conchita Martinez
Anke Huber Judith Wiesner Andre Agassi Pete Sampras Jana Novotna
Karina Habsudova
Jim Courier Justine Henin Julie Halard
Meredith McGrath Goran Ivanisevic
Jelena Dokic
Michael Chang
This Paper Mary Pierce Monica Seles Martina Hingis
Lindsay Davenport
Steffi Graf
Jennifer Capriati
Kim Clijsters
Karina Habsudova Sandrine Testud
Kimiko Date Chanda Rubin Anke Huber
Nathalie Tauziat
Jana Novotna
Conchita Martinez
Nathalie Dechy Amanda Coetzer Barbara Paulus
Arantxa Sanchez Vicario
Amy Frazier Iva Majoli
Magdalena Maleeva
Jelena Dokic Julie Halard
Table I : This table compares the state of the art approach for setexpansion on free text with the approach presented in this paper . The bold and italicized entries correspond to male tennis players and are erroneous . The addition of only 1 negative example to the seed set improves the listquality significantly . The second column contains no errors . and italicized . We see that the output in the 1ğ‘ ğ‘¡ column is corrupted by male tennis players . Adding only 1 negative example to the seed set improves the list quality significantly . The second column contains no errors . In this paper , we propose ways to learn from negative examples in setexpansion and show significant improvement .
We present an inference based approach to set expansion which doesnâ€™t rely on computing the centroid . The new approach naturally allows for both positive and negative examples in the seed set . We also extended the centroidbased approach so that it can accept negative examples . We used this improved version of centroid based approach as a baseline system and show in the experiments that the inference based approach we developed significantly outperforms this baseline .
II . RELATED WORK
The task of set expansion has been addressed in several works . We briefly discuss some of the most significant efforts towards this task . Google Sets and Boowa [ 3 ] are webbased set expansion methods . For set expansion on freetext ( [4 ] , [ 5 ] , [ 1 ] , [ 2] ) , pattern recognition and distributional similarity have primarily been used . Some works on setexpansion ( [6 ] , [ 7 ] , [ 8 ] ) have focussed on integrating information across several types of sources such as structured , semi structured , unstructured text , query logs etc .
There has also been some work on the use of negative examples in set expansion . Thelen and Riloff [ 9 ] and Lin et al . [ 10 ] present a framework to simultaneously learn several semantic classes . In this framework , instances which have been accepted by one semantic class serve as negative examples for all other semantic classes . This approach is limited because it necessitates the simultaneous learning of several semantic classes . Moreover , negative examples are not useful if the semantic classes addressed are not related to one another . Lin et al . note that it is not easy to acquire good negative examples . The approach presented here , on the other hand , allows the use of negative examples even when there is only one semantic class .
The focus of this work is on set expansion from free text . Thus , we do not compare our system with systems which use textual sources other than free text ( eg semi structured web pages or query logs ) . The works of Sarmento et al . [ 1 ] and Pantel et al . [ 2 ] are the state of the art works that are most related to our approach and , therefore , in our experiments , we compare their centroid based approach with the approach developed here .
III . CENTROID BASED APPROACH TO SET EXPANSION A . Feature Vector Generation
The input to our set expansion system consists of free text . To extract the relevant entities from the text , we preprocess the corpus with a state of the art Named Entity Recognition tool developed by Ratinov and Roth1 [ 11 ] . Our experiments were done only for entities of type PER , and we denote the set of all distinct entities recognized in the corpus by ğ¸ . ğ‘’ğ‘– represents the ğ‘–ğ‘¡â„ entity . The features of an entity ğ‘’ğ‘– are composed of the words appearing in a window of size ğ‘Š centered on each mention of the entity ğ‘’ğ‘– . We use discounted PMI [ 12 ] to measure the association of a feature with the entity . Table II gives some of the features for two different entities as generated from the corpus . The numbers along with the features indicate the absolute frequency of the feature .
B . List Generation
We compute the similarity between any two entities using the cosine coefficient . Given an entity ğ‘’ğ‘– , we compute the
1http://cogcompcsillinoisedu/page/software similarity between ğ‘’ğ‘– and all other entities in the entity set ğ¸ . We then sort all the entities in ğ¸ based on this similarity score in decreasing order . The resulting ranked list has the property that entities with lower rank are more similar to ğ‘’ğ‘– than entities with higher rank . We call this list the set of neighbors of ğ‘’ğ‘– , denoted as NBRLIST(ğ‘’ğ‘– ) . In the centroid based approach , first of all , centroid ( ğ’ ) is computed by averaging the frequency vectors of entities in the seed set ( ğ’® ) and then computing the discounted PMI of the resulting frequency vector . Next , NBRLIST of the centroid is computed as described above and the system outputs the first ğ‘€ members of NBRLIST .
IV . LEARNING FROM NEGATIVE EXAMPLES IN
CENTROID BASED APPROACH
The centroid based approach to set expansion doesnâ€™t easily allow learning from negative examples . In this section , we present a novel framework which allows the incorporation of negative examples in a centroid based approach .
The active features of any entity are those features which have non zero frequency . The active features of the centroid are the union of the active features of the entities in the seed set . The active features of the centroid are not equally important . To incorporate this knowledge into set expansion , we associate a weight term with each entry in the vocabulary . Higher weight would mean that a particular word is more relevant to the underlying concept . By incorporating these weights into the cosine similarity metric , the new formula to compute the similarity between entities ğ‘’ğ‘– and ğ‘’ğ‘— becomes :
âˆ‘ âˆšâˆ‘
ğ‘¤ğ‘ ğ‘–ğ‘šğ‘–ğ‘— =
âˆšâˆ‘
ğ‘˜ ğ‘¤ğ‘˜ğ‘‘ğ‘ğ‘šğ‘–ğ‘–ğ‘˜ğ‘‘ğ‘ğ‘šğ‘–ğ‘—ğ‘˜ ğ‘˜ ğ‘‘ğ‘ğ‘šğ‘–2 ğ‘–ğ‘˜
ğ‘˜ ğ‘‘ğ‘ğ‘šğ‘–2 ğ‘—ğ‘˜
( 1 ) where ğ‘¤ğ‘˜ is the weight associated with the ğ‘˜ğ‘¡â„ word and dpmi refers to the discounted PMI values . We wish to learn a weight vector ğ‘¤ such that the similarity between the positive examples and the centroid becomes more than a prespecified threshold ğœ . Moreover , we want that the similarity between negative examples and the centroid should become less than a prespecified threshold ğœ— . We accomplish this objective using the following linear program :
âˆ‘
âˆ‘ st
ğ‘šğ‘ğ‘¥
ğ‘¤ğ‘ ğ‘–ğ‘šğ’ğ‘–
ğ‘’ğ‘–âˆˆğ‘ƒ ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’ğ¸ğ‘¥ğ‘ğ‘šğ‘ğ‘™ğ‘’ğ‘  âˆ’
âˆ‘
ğ‘’ğ‘—âˆˆğ‘ ğ‘’ğ‘”ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ğ¸ğ‘¥ğ‘ğ‘šğ‘ğ‘™ğ‘’ğ‘ 
ğ‘¤ğ‘ ğ‘–ğ‘šğ’ğ‘—
ğ‘˜
ğ‘¤ğ‘˜ â‰¤ num of non zero entries in centroid ğ‘¤ğ‘ ğ‘–ğ‘šğ’ğ‘– â‰¥ ğœ âˆ€ğ‘’ğ‘– âˆˆ ğ‘ƒ ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’ğ¸ğ‘¥ğ‘ğ‘šğ‘ğ‘™ğ‘’ğ‘  ğ‘¤ğ‘ ğ‘–ğ‘šğ’ğ‘– â‰¤ ğœ— âˆ€ğ‘’ğ‘– âˆˆ ğ‘ ğ‘’ğ‘”ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ğ¸ğ‘¥ğ‘ğ‘šğ‘ğ‘™ğ‘’ğ‘ 
( 2 )
( 3 )
( 4 ) ( 5 ) ( 6 ) ( 7 )
ğ‘¤ğ‘˜ â‰¥ 0 ğ‘¤ğ‘˜ â‰¤ ğœ†
âˆ€ğ‘˜ âˆ€ğ‘˜
Entity
Bill Clinton
Pete Sampras
Examples of Feature Vectors [ President , 24912 ] , [ administration , 790 ] , [ House , 766 ] , [ visit , 761 ] , [ talks , 742 ] , [ announced , 737 ] , [ summit , 703 ] , [ White , 684 ] , [ Republican , 541 ] , [ WASHINGTON , 508 ] , [ Congress , 490 ] , [ Democratic , 318 ] , [ budget , 243 ] , [ veto , 230 ] , [ government , 219 ] , [ election , 192 ] , [ political , 182 ] , [ Hillary , 149 ] [ USA , 323 ] , [ World , 254 ] , [ champion , 226 ] , [ number one , 191 ] , [ defending , 124 ] , [ final , 115 ] , [ American , 112 ] , [ pts , 99 ] , [ beat , 86 ] , [ round , 81 ] , [ tennis , 73 ] , [ singles , 65 ] , [ Wimbledon , 62 ] , [ seeded , 40 ] , [ lost , 39 ] , [ semi final , 38 ] , [ Grand , 36 ] , [ Slam , 36 ] , [ tournament , 34 ] , [ top seed , 32 ] , [ Tennis , 5 ]
Table II : This table shows some of the features for two different entities . The numbers along with the features indicate the absolute frequency of the feature appearing with the entity under consideration .
In the above linear program , Equation ( 2 ) is the objective of the linear program which aims at
1 ) maximizing the similarity between positive examples and the centroid and and the centroid .
2 ) minimizing the similarity between negative examples Note that ğ’ refers to centroid in Equations ( 2 ) , ( 4 ) and ( 5 ) . V . INFERENCE BASED APPROACH TO SET EXPANSION The centroid based approach to set expansion has some limitations . In the centroid based approach , the centroid is supposed to fully represent the underlying concept , and all the similarity scores are computed with respect to the centroid . There is no way to confirm the decisions made with respect to the centroid . There is a lot of information in the individual positive and negative examples which is not exploited in the centroid based approach . Moreover , as more and more positive examples are added to the seed set , the number of active features of the centroid keeps increasing and this may lead to over generalization .
In this section , we present an inference based method for set expansion . Unlike Section III , we do not compute the centroid of the positive examples . The new approach is based on the intuition that the positive and negative examples can complement each othersâ€™ decision to better represent the underlying concept . Each example can be thought of as an expert which provides positive or negative evidence regarding the membership of any entity in the underlying concept . We develop a mechanism to combine the suggestions of such experts .
Algorithm 1 gives the procedure for inference based set expansion . In steps 1 and 2 , we compute the NBRLIST of positive and negative examples respectively ( see Section III B ) . Entities which have high similarity to the positive examples are more likely to belong to the underlying concept , while entities which have high similarity to the negative examples are likely to not belong to the underlying concept . Steps 1 and 2 of Algorithm 1 generate one list corresponding to every positive and negative example . We associate a reward ( or penalty ) with each entity in these lists based on the rank of the entity . Our reward ( or penalty ) function is based on the effective length , â„’ , of a list . The entities which have higher rank than the effective length of the list are given a reward ( or penalty ) of zero .
Algorithm 1 : InferenceBasedSetExpansion Input
: ğ¸ ( Entity Set ) , ğ‘Š ( List of positive examples ) , ğµ ( List of negative examples )
Output : ğ¿ ( Ranked List ) begin
1
2
3
4
5
6
Compute NBRLISTs of Positive Examples for ğ‘— â† 1 to âˆ£ğ‘Šâˆ£ do for ğ‘– â† 1 to âˆ£ğ¸âˆ£ do
ğ‘Š ğ‘†ğ‘‰ğ‘—[ğ‘– ] â†âˆ’ ğ‘ ğ‘–ğ‘šğ‘–ğ‘¤ğ‘—
Sort the entities in ğ¸ based on ğ‘Š ğ‘†ğ‘‰ğ‘— and store the result in ğ‘Š ğ¿ğ‘—
Compute NBRLISTs of Negative Examples for ğ‘— â† 1 to âˆ£ğµâˆ£ do for ğ‘– â† 1 to âˆ£ğ¸âˆ£ do
ğµğ‘†ğ‘‰ğ‘—[ğ‘– ] â†âˆ’ ğ‘ ğ‘–ğ‘šğ‘–ğ‘ğ‘—
Sort the entities in ğ¸ based on ğµğ‘†ğ‘‰ğ‘— and store the result in ğµğ¿ğ‘— Initialize the score for each entity to 0 for ğ‘– â† 1 to âˆ£ğ¸âˆ£ do
ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ğ‘– â†âˆ’ 0
Compute the contribution from positive examples for ğ‘— â† 1 to âˆ£ğ‘Š ğ¿âˆ£ do for ğ‘– â† 1 to âˆ£ğ¸âˆ£ do ğ‘’ â†âˆ’ ğ‘Š ğ¿ğ‘—[ğ‘– ] ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ğ‘’ â†âˆ’ ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ğ‘’ + ğ‘Ÿğ‘’ğ‘¤ğ‘ğ‘Ÿğ‘‘(ğ‘– , 0 ) + ğ‘Š ğ‘†ğ‘‰ğ‘—[ğ‘’ ]
Compute the contribution from negative examples for ğ‘— â† 1 to âˆ£ğµğ¿âˆ£ do for ğ‘– â† 1 to âˆ£ğ¸âˆ£ do ğ‘’ â†âˆ’ ğµğ¿ğ‘—[ğ‘– ] ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ğ‘’ â†âˆ’ ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ğ‘’ + ğ‘Ÿğ‘’ğ‘¤ğ‘ğ‘Ÿğ‘‘(ğ‘– , 1 ) ğ¿ â†âˆ’ List of entities in ğ¸ sorted by ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’
Effective length , â„’ , of a list is computed by multiplying the required list length ( or cut off ) by a list factor , â„± . If ğ‘€ is the specified cut off , then â„’ = ğ‘€ Ã— â„± . The reward is calculated according to the following equation :
P A M
80
75
70
65
60
55
50
45
40
0
LF = 1 LF = 2 LF = 3 LF = 4 LF = 5 Centroid
3
6
9
12
15
Number of Seeds
Figure 1 : This figure shows the effect of list factor ( â„± ) on the performance of set expansion . When averaged across different number of seeds , â„± = 2 gave the best results .
ğ‘Ÿğ‘’ğ‘¤ğ‘ğ‘Ÿğ‘‘(ğ‘Ÿ , ğ‘› ) =
â§â¨ â©
( âˆ’1)ğ‘› Ã— â„’ Ã— ğ‘ ( âˆ’1)ğ‘› Ã— ( â„’ âˆ’ ğ‘Ÿ ) Ã— ğ‘ 0 if ğ‘Ÿ = 1 if 1 < ğ‘Ÿ â‰¤ â„’ otherwise
( 8 ) In the above equation , ğ‘Ÿ refers to the rank of the entity in the list . ğ‘› is set to 0 for lists corresponding to positive examples and to 1 for lists corresponding to negative examples . Thus , for lists corresponding to negative examples , the reward is negative and hence , acts like a penalty . The values of all the parameters were determined empirically . Equation ( 8 ) gives higher reward or penalty to the entities with lower rank . Figure 1 shows the effect of â„± on the Mean Average Precision ( MAP ) ( please see Section VI for a discussion on MAP ) for the concept of female tennis players as the number of seeds is varied . Only positive examples were used for generating Figure 1 . To find the best value of â„± , we take the average of MAP across different number of seeds . We find that â„± = 2 has the highest average of 677 Although â„± = 1 gives good performance for higher number of seeds , its performance is quite low when the number of seeds is small . As we increase the value of â„± beyond 2 , the performance goes on decreasing . We used â„± = 2 for all our experiments . Steps 3 , 4 and 5 in Algorithm 1 compute the score for each entity in the entity set ğ¸ . Step 3 initializes the score for each entity to 0 . Steps 4 and 5 compute the contributions from the lists corresponding to the positive and negative examples , towards the score of entities . If ğ‘Ÿğ‘ğ‘›ğ‘˜(ğ‘– , ğ‘— ) respectively , denotes the rank of entity ğ‘’ğ‘– in the list corresponding to example ğ‘’ğ‘— , then the final score of entity ğ‘’ğ‘– can be written as :
Attribute
Number of Files
Total Size
Number of Docs
Total Tokens
Vocabulary Size
Distinct Entities ( PER )
Value
44
1,216 MB 656,269
170,969,000
548,862 386,209
Table III : Characteristics of AFE section of GCOR
ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ğ‘– =
[ ğ‘Ÿğ‘’ğ‘¤ğ‘ğ‘Ÿğ‘‘(ğ‘Ÿğ‘ğ‘›ğ‘˜(ğ‘– , ğ‘¤ ) , 0 ) + ğ‘ ğ‘–ğ‘šğ‘–ğ‘¤ ]
[ ğ‘Ÿğ‘’ğ‘¤ğ‘ğ‘Ÿğ‘‘(ğ‘Ÿğ‘ğ‘›ğ‘˜(ğ‘– , ğ‘ ) , 1 ) ]
( 9 )
âˆ‘ âˆ‘ ğ‘¤âˆˆğ‘Š +
ğ‘âˆˆğµ
In the above equation , ğ‘Š and ğµ refer to the list of positive and negative examples , respectively . Finally , step 6 of Algorithm 1 sorts the entities in descending order based on the final score as computed in Equation ( 9 ) . The first ğ‘€ members of the resulting list are output by the system . Here , ğ‘€ is the required list length .
VI . EXPERIMENTS
We used the AFE section of English Gigaword Corpus ( henceforth referred to as GCOR ) for our experiments . This is a comprehensive archive of newswire text data in English that is available from LDC . The characteristics of the corpus are shown in Table III .
The parameters of linear program ( Eqns ( 2) (7 ) ) and reward function ( Eqn ( 8 ) ) were determined empirically . ğœ , ğœ— , ğœ† , ğ‘ and ğ‘ were set to 0.2 , 0.0001 , 10 , 100 and 10 respectively .
A . Inference vs Centroid Based Approaches
We use the following notation to refer to the three set expansion systems that we present and compare :
1 ) SEC Set Expansion system using Centroid . This is the current state of the art [ 1 ] , [ 2 ] and was presented in Section III . This system canâ€™t learn from the negative examples .
2 ) SECW Set Expansion system using Centroid where Weights are associated with the vocabulary terms . This system was developed and explained in Section IV . This system can learn from negative examples .
3 ) SEI Set Expansion system using Inference . This is the new approach to set expansion and it was developed and explained in Section V .
SEC and SECW serve as baseline systems . Table IV compares the performance of SEI with the two baselines on 5 different concepts as mentioned below :
1 ) Female Tennis Players ( FTP ) 2 ) Indian Politicians ( IP ) 3 ) Athletes ( ATH )
P A M
95
90
85
80
75
70
65
60
55
50
45
40
0
Female Tennis Players
SEI SECW OnlyPos Pos+Neg
3
6
9
12
15
18
21
Number of Seeds
P A M
65
60
55
50
45
40
35
30
25
20
0
Indian Politicians
SEI SECW OnlyPos Pos+Neg
3
6
9
12
15
18
21
Number of Seeds
P A M
75
70
65
60
55
50
45
40
35
30
0
Athletes
SEI SECW OnlyPos Pos+Neg
3
6
9
12
15
18
21
Number of Seeds
( a ) Set Expansion Results for FTP
( b ) Set Expansion Results for IP
( c ) Set Expansion Results for ATH
P A M
70
65
60
55
50
45
40
35
0
Film Actors
SEI SECW OnlyPos Pos+Neg
3
6
9
12
15
18
21
Number of Seeds
P A M
75
70
65
60
55
50
45
40
0
Australian Cricketers
SEI SECW OnlyPos Pos+Neg
3
6
9
12
15
18
Number of Seeds
( d ) Set Expansion Results for FA
( e ) Set Expansion Results for AC
Figure 2 : This figure shows the MAP values for 5 different concepts for both SEI and SECW ( Baseline ) . Two things can immediately be noted from the graphs : ( 1 ) Negative examples significantly improve the MAP values for both SEI and SECW . ( 2 ) SEI performs much better than SECW for all the five concepts .
Concept
FTP IP ATH FA AC
SEC 57.9 36.4 49.6 55.1 59.7
SECW SEI 89.6 64.2 73.3 69.5 72.6
70.3 48.9 54.9 59.2 51.6
Table IV : A comparison of the MAP values of SEI with the 2 baselines on 5 different concepts . Our algorithm , SEI , performs significantly better than both baselines on all the concepts . SECW is our improvement to the centroid method and is the second best . It performs better than SEC ( current state of the art ) on all concepts except AC . For details , please refer to Section VI A .
4 ) Film Actors ( FA ) 5 ) Australian Cricketers ( AC )
The evaluation metric used in Table IV and in later experiments is Mean Average Precision ( MAP ) . MAP is the mean value of average precision ( AP ) computed for each ranked list separately . For a list of length â„³ , the AP is given by the following equation :
âˆ‘â„³
ğ‘Ÿ=1[ğ‘ƒ ( ğ‘Ÿ ) Ã— ğ‘Ÿğ‘’ğ‘™(ğ‘Ÿ ) ]
#TrueEntities
ğ´ğ‘ƒ =
( 10 )
In the above equation , r is the rank , rel is a binary function on the relevance of a given rank and P(r ) is the precision at given cutoff rank .
For the results presented in Table IV , we used 12 positive and 6 negative examples for each concept other than AC . For AC , we used 9 positive and 6 negative examples .
Table IV clearly shows that SEI does much better than both the baselines on all concepts . We observed that the lists produced by SEI hardly contained any errors in the first half of the lists as we also saw in Table I . This is because the entities which come in the beginning of the list in SEI get high scores from several positive examples and are not penalized by any of the negative examples . On the other hand , SEC and SECW are unable to make such inferences . We also see from Table IV that with the exception of AC , SECW performs better than SEC on all concepts . The better performance of SECW is due to the use of negative examples . Thus , the strategy developed in Section IV for incorporating negative examples is quite effective .
For further analysis , in Figure 2 , we compare the performance of SEI with SECW on all the concepts as the seed set size is increased . First we supplied only positive examples as indicated by the circle markers in Figure 2 .
Desired Concept
Female Tennis Players
Film Actors
Athletes
Indian Politicians
Australian Cricketers
Negative Examples Male Tennis Players
Musicians , Film Directors
Football Platers , Skiers
Other Politicians
Cricketers from other countries
Table V : This table shows the negative examples that were used for different concepts . We see that the negative examples are closely related to the instances of the desired concept .
After supplying sufficient number of positive examples , we provided negative feedback on 6 examples as indicated by square markers in Figure 2 . For clarity , we do not show the performance of SEC in Figure 2 . SEC performs similarly to SECW on positive examples but is unable to learn from negative examples .
Two conclusions can readily be drawn from Figure 2 :
1 ) Negative examples significantly improve the perfor mance of set expansion for both SEI and SECW . 2 ) SEI performs much better than SECW on all the concepts irrespective of the seed set size .
Table V categorizes the negative examples that were used for different concepts . We see that the good negative examples are closely related to the true instances of the desired concept . For example , the negative examples for the concept Australian Cricketers consist of the cricket players from other countries . We can see from Figure 2 that for SEI , the negative examples improve the MAP for FTP , IP , ATH , FA and AC by 17.6 % , 9.4 % , 7.9 % , 2.8 % and 10.0 % respectively .
VII . CONCLUSIONS
This paper showed that negative examples can significantly improve the performance of set expansion by helping to better define the underlying concept . We incorporated weights in the commonly used centroid based approach so that it can benefit from negative examples . We developed a new , inference based approach to set expansion which naturally allows for negative examples and showed that it performs significantly better than the centroid based approach .
ACKNOWLEDGMENTS
This research was supported by Grant HHS 90TR0003/01 and by the Defense Advanced Research Projects Agency ( DARPA ) Machine Reading Program under Air Force Research Laboratory ( AFRL ) prime contract no . FA8750 09 C0181 . Its contents are solely the responsibility of the authors and do not necessarily represent the official views of the HHS , DARPA , AFRL , or the US government .
REFERENCES
[ 1 ] L . Sarmento , V . Jijkuon , M . de Rijke , and E . Oliveira , â€œ More like these : growing entity classes from seeds , â€ in Proceedings of the sixteenth ACM conference on CIKM . ACM , 2007 , pp . 959â€“962 .
[ 2 ] P . Pantel , E . Crestan , A . Borkovsky , A . Popescu , and V . Vyas , â€œ Web scale distributional similarity and entity set expansion , â€ in Proceedings of the 2009 Conference on EMNLP . ACL , 2009 , pp . 938â€“947 .
[ 3 ] R . Wang and W . Cohen , â€œ Language independent set expanIEEE sion of named entities using the web , â€ in ICDM . Computer Society , 2007 , pp . 342â€“350 .
[ 4 ] E . Riloff and R . Jones , â€œ Learning dictionaries for information extraction by multi level bootstrapping , â€ in Proceedings of the National Conference on Artificial Intelligence . JOHN WILEY & SONS LTD , 1999 , pp . 474â€“479 .
[ 5 ] P . Talukdar , T . Brants , M . Liberman , and F . Pereira , â€œ A context pattern induction method for named entity extraction , â€ in Proceedings of the Tenth Conference on CoNLL . ACL , 2006 , pp . 141â€“148 .
[ 6 ] P . Talukdar , J . Reisinger , M . PasÂ¸ca , D . Ravichandran , R . Bhagat , and F . Pereira , â€œ Weakly supervised acquisition of labeled class instances using graph random walks , â€ in Proceedings of the Conference on EMNLP . ACL , 2008 , pp . 582â€“590 .
[ 7 ] M . Pennacchiotti and P . Pantel , â€œ Entity extraction via ensemble semantics , â€ in Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing : Volume 1 Volume 1 . Association for Computational Linguistics , 2009 , pp . 238â€“247 .
[ 8 ] M . Pasca and B . Van Durme , â€œ Weakly supervised acquisition of open domain classes and class attributes from web documents and query logs , â€ in Proceedings of the 46th Annual Meeting of the ACL ( ACL 08 ) . Citeseer , 2008 , pp . 19â€“27 .
[ 9 ] M . Thelen and E . Riloff , â€œ A bootstrapping method for learning semantic lexicons using extraction pattern contexts , â€ in Proceedings of the ACL 02 conference on Empirical methods in natural language processing Volume 10 . Association for Computational Linguistics , 2002 , pp . 214â€“221 .
[ 10 ] W . Lin , R . Yangarber , and R . Grishman , â€œ Bootstrapped learning of semantic classes from positive and negative examples , â€ in Proceedings of ICML 2003 Workshop on The Continuum from Labeled to Unlabeled Data , vol . 1 , no . 4 , 2003 , p . 21 .
[ 11 ] L . Ratinov and D . Roth , â€œ Design challenges and misconceptions in named entity recognition , â€ in Proceedings of the Thirteenth Conference on Computational Natural Language Learning . Association for Computational Linguistics , 2009 , pp . 147â€“155 .
[ 12 ] P . Pantel and D . Lin , â€œ Discovering word senses from text , â€ in Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining . ACM , 2002 , pp . 613â€“619 .
