2011 11th IEEE International Conference on Data Mining
Detection of Arbitrarily Oriented Synchronized Clusters in High dimensional Data
Junming Shao∗ , Claudia Plant† , Qinli Yang‡ and Christian B¨ohm∗
∗Institute for Computer Science , University of Munich , Munich , Germany
∗Department of Scientific Computing , Florida State University , Tallahassee , FL , USA .
‡School of Engineering , University of Edinburgh , Edinburgh , UK
Abstract—How to address the challenges of the “ curse of dimensionality ” in clustering ? Clustering is a powerful data mining technique for structuring and organizing vast amounts of data . However , the high dimensional data space is usually very sparse and meaningful clusters can only be found in lower dimensional subspaces . In many applications the subspaces hosting the clusters provide valuable information for interpreting the major patterns in the data . Detection of subspace clusters is challenging since usually many of the attributes are noisy , some attributes may exhibit correlations among each other and only few of the attributes truly contribute to the cluster structure . In this paper , we propose ORSC ( Arbitrarily ORiented Synchronized Clusters ) , a novel effective and efficient method to subspace clustering inspired by synchronization . Synchronization is a basic phenomenon prevalent in nature , capable of controlling even highly complex processes such as opinion formation in a group . Control of complex processes is achieved by simple operations based on interactions between objects . Relying on the interaction model for synchronization , our approach ORSC ( 1 ) naturally detects correlation clusters in arbitrarily oriented subspaces , including ( 2 ) arbitrarilyshaped non linear correlation clusters . Our approach is ( 3 ) robust against noise points and outliers . In contrast to previous methods , ORSC is ( 4 ) easy to parameterize , since there is no need to specify the subspace dimensionality and all interesting subspace clusters can be detected . Finally,(5 ) ORSC outperforms most comparison methods in terms of runtime efficiency and is highly scalable to large and high dimensional data sets .
Keywords subspace clustering ; synchronization ; interaction model ; high dimensional data ;
I . INTRODUCTION
Nowadays , tremendous amounts of data in a large variety of application domains are produced . These real world data sets are often represented as sparse , high dimensional feature vectors , contaminated by outliers and noise objects . Clustering such high dimensional data becomes difficult for traditional clustering methods due to the famous problem called “ curse of dimensionality ” . To cure the problem in the concept of subspace clustering has been clustering , introduced to detect clusters in subspaces of the original space .
Currently , a number of subspace clustering algorithms have already been proposed , which can be mainly distinguished as axis parallel subspaces clustering , eg CLIQUE [ 5 ] , PROCLUS [ 3 ] , SUBCLU[11 ] and arbitrarily oriented subspace clustering ( also called generalized subspace clus tering , or correlation clustering ) , eg ORCLUS [ 4 ] , 4C [ 8 ] , Curler [ 19 ] . However , most of these established algorithms fail to discover clusters of complex shapes and various densities . Another challenge for subspace clustering is the search strategy for the suitable subspaces . The number of possible axis parallel subspaces is exponential ( 2d ) in the number of dimensions d , and the number of arbitrarily oriented subspaces is even infinite . Therefore , a complete enumeration of all possible subspaces to be checked for clusters is not feasible . Consequently , all previous solutions rely on specific assumptions and heuristics , and try to find promising subspaces during the clustering process , for instance in an iterative optimization .
To deal with these problems , in this paper , we consider subspace clustering from a new perspective : synchronization . Here , let us first illustrate some fundamental concepts for synchronization based subspace clustering .
A . Synchronization and Synchronized Cluster Synchronization is a prevalent phenomenon in nature that a group of events spontaneously come into co occurrence with a common rhythm , despite of the differences between individual rhythms of the events . It is known that synchronization is rooted in human life from the metabolic processes in our cells to the highest cognitive tasks we perform as a group of individuals [ 6 ] . A paradigmatic example of synchronization phenomena in nature is the synchronous flashing of fireflies observed in some South Asian forests [ 1 ] . At night , in the beginning , they flash independently , but after a short period of time the whole swarm is flashing in unison , creating one of the most striking visual effects ever seen . During last several decades , synchronization has attracted a large volume of interest in physics , biology , ecology , sociology and other fields of science and technology . For example , Rhouma et al . [ 17 ] introduce an efficient synchronization model that organizes a population of integrate and fire oscillators into stable and structured groups . Arenas et al . [ 7 ] investigate the synchronization phenomena for network analysis , and study the relationship between topological scales and dynamic time scales in complex networks . Aeyels et al . [ 2 ] introduce a mathematical model for investigating the dynamics of chaos system and apply it to a system of interconnected water basins . Recently , B¨ohm et al . [ 9],[18 ] propose an extensive
1550 4786/11 $26.00 © 2011 IEEE DOI 101109/ICDM201150
607
B . Contributions Inherited by the concept of synchronization , in this paper , we propose a new subspace clustering approach , ORSC ( arbitrarily ORiented Synchronized Clusters ) . The major benefits of our algorithm ORSC can be summarized as follows :
1 ) Natural data structure exploring . The synchronized cluster formation is driven by the mutual interactions among objects relying on intrinsic data structure .
2 ) Detection of arbitrarily shaped correlation clusters . Without any data distribution assumption , ORSC allows detecting clusters with arbitrary numbers , shapes and sizes in subspaces .
3 ) Outlier robustness . Since the outliers cannot easily synchronize with other objects , they can be effectively distinguished from cluster objects .
4 ) Efficient subspace searching . Inherited by the properties of synchronization , the subspace search strategy of ORSC does not need scan possible subspaces but simply , finds all subspace clusters by searching all synchronized phases .
The remainder of this paper is organized as follows : we briefly survey related work in Section II . Section III presents our new concept and algorithm in detail . Section IV contains an extensive experimental evaluation and Section V concludes the paper .
II . RELATED WORK
As most traditional clustering algorithms fail to detect clusters embedded in high dimensional data , various subspace clustering approaches have been studied . Subspace clustering algorithms like CLIQUE [ 5 ] ENCLUS [ 10 ] , projected clustering algorithms such as PROCLUS [ 3 ] , DOC [ 16 ] , and SUBCLU [ 11 ] only find axis parallel clusters . Pattern based subspace clustering methods ( eg [ 20],[14 ] ) aim to group objects that exhibit a similar trend in a subset of attributes into clusters rather than objects with low distance . However , themselves to finding only clusters that represent pairwise positive correlations in the data set . Here , we focus on the more recent correlation clustering algorithms which can find clusters in arbitrarily oriented subspaces . Currently , many arbitrarily oriented subspace clustering algorithms such as ORCLUS [ 4 ] , 4C [ 8 ] , Curler [ 19 ] have been proposed . they limit
ORCLUS [ 4 ] is one of the iterative top down search methods for arbitrarily projected subspaces . It integrates PCA into k means and includes three steps : assign clusters , determinate subspaces and merge them . However , it requires users to specify the number of clusters and the subspace dimensionality in advance . ORCLUS tends to fail if the estimation does not match with the actual number of clusters . Moreover , due to using random sampling to improve computation speed and scalability , it may suffer from missing
608
Figure 1 . Illustration of Synchronized Clusters . Arrows indicate the main directions of movements of objects during the process towards synchronization and the red cross points illustrate the final states of cluster objects , which are formed as synchronized clusters in subspaces .
Kuromato model to simulate the dynamics of each object during the process of synchronization and explore clusters and outliers in combination with Minimum Description Length .
Inspired by these synchronization phenomena and models , we propose a novel method to subspace clustering . The key idea is to consider subspace clustering as a dynamic process towards synchronization . We view each object as an oscillator which interacts with other objects in arbitrarily oriented subspaces . Through interactions , all cluster objects in arbitrarily oriented subspaces will synchronize together , which called synchronized clusters in this paper . Let us demonstrate the concept with the help of a simple 3dimensional data set . Fig 1(a ) and Fig 1(c ) represent the X Y and Y Z projection of the data , respectively . In Cluster C1 , the X and Y dimensions are strongly correlated and the Z dimension is not cluster specific white noise . Cluster C2 only exists in the Y Z space and the X dimension is noise . To find such clusters , in this context , like oscillators , each object interacts with similar objects in axis parallel or arbitrarily oriented subspaces . Objects will gradually change their states and move to other objects driving by its intrinsic structure , which we will elaborate in detail in Section III . The arrows in Fig 1(a ) and Fig 1(c ) illustrate the main directions of movements for objects . Fig 1(b ) and Fig 1(d ) further indicate the final states of objects after synchronization . Finally , all objects of a common subspace cluster move together and form as synchronized clusters ( cf . Fig 1(b ) and Fig 1(d) ) . smaller clusters . 4C [ 8 ] combines PCA and density based clustering to identify local subgroups of the data objects sharing a uniform but arbitrarily complex correlation . It formalizes the correlation connected cluster as a dense region of points in the d dimensional feature space having at least one principal axis with low variation along this axis . Like ORCLUS , 4C limits itself to identify linear correlation clusters without considering nonlinear correlation clusters . Actually , in real life datasets , correlation between attributes could however be nonlinear . To address the nonlinear issue , Curler [ 19 ] is proposed which allows detecting both global and local orientations of the clusters . This method merges the microclusters generated by the EM variant algorithm according to their co sharing level . Therefore , the resulting clusters may represent a more complex , not necessarily liner correlation . However , the performance of Curler is very sensitive to noise and strongly depends on the suitable parametrization .
III . THE ALGORITHM ORSC
In this section , we introduce ORSC to discover all possible clusters in arbitrarily oriented subspaces . We first give an overview of our novel perspective of subspace clustering and then construct the weighted interaction model to simulate the dynamics of objects towards synchronization . Finally an efficient searching strategy is used to find all synchronized clusters .
A . A New Perspective of Subspace Clustering As we introduced in Section I , we consider the subspace analysis from a novel perspective : synchronization . The key point is to view each object as an oscillator which moves according to an interaction model ( cf . Section III B ) . For each object , we define suitable interaction partners in the local neighborhood . The strength of corresponding interactions are provided by the local structure of the neighborhood . To detect arbitrarily oriented subspace clusters we integrate PCA into the interaction model . The interactions are weighted by the main directions of the PCA result . Thus , all objects in an arbitrarily oriented subspace cluster can easily synchronize together . In the following , we elaborate this weighted interaction model .
B . Weighted Interaction Model Currently , the most successful way to explore the synchronization phenomena is the Kuramoto Model [ 12 ] , [ 13 ] , which is motivated by the behavior of systems of chemical and biological oscillators . It is a mathematical model used to describe the dynamics of a large set of phase oscillators by coupling the sine of their phase differences . All frequencies of oscillators should be identical or nearly identical . Formally , the Kuramoto model ( KM ) consists of a population of N coupled phase oscillators , where the phase of the i th unit ,
609
Figure 2 .
Range search with different distance functions . denoted by θi , evolves in time according to the following dynamics :
N j=1 dθi dt
= ωi +
K N sin(θj − θi ) , ( i = 1 , . . . , N )
( 1 ) where ωi stands for natural frequency of the i th unit and the constant K describes the coupling strength between units . sin(· ) is the coupling function . This model well describes the collective behavior of all coupled phase oscillators towards synchronization , which implies that all the oscillators interact with each other and will synchronize together finally . However , this situation rarely occurs in real life systems . Local synchronization is observed more frequently , which means a local ensemble of oscillators synchronize together , where the whole set of oscillators is split into several clusters of mutually synchronized oscillators .
Therefore , in order to introduce the Kuramoto model into subspace clustering , we reconsider it in a different way .
1 ) Local Interaction . To exploit the hidden clusters or patterns in arbitrarily oriented subspaces , the local structure of data should be investigated . Therefore , we focus on the dynamics of objects in a local way .
2 ) Weighted Interaction . In high dimensional space , the correlations in the dimensions are often specific to data locality , which means that some objects are correlated with respect to one given set of dimensions and others are correlated with respect to a different set of dimensions . The coupling strength of the interaction of objects should be considered with different weights , depending how relevant the dimensions locally are .
In the following , we will reformulate our interaction model based on the above two criteria .
1 ) Local Interaction : To formalize the local interaction the intuitive way is to consider its ε for each object , neighborhood as follows . ( ε NEIGHBORHOOD ) Given a ε ∈ R and DEFINITION 1 x ∈ D , the ε neighborhood of an object x , denoted by Nε(x ) , is defined as :
Nε(x ) = {y ∈ D|dist(y , x ) ≤ ε}
( 2 ) where dist(y , x ) is a metric distance function and Euclidean distance is used here .
However , this definition of the ε neighborhood search cannot meet well our goals since it does not consider the
PCovariance ΣxεPεMahalanobisdistanceEuclidian distanceε local data distribution . We intend to search for objects which are near together according to the local subspace cluster structure . Therefore , we use the Mahalanobis distance instead of Euclidean distance to define similarity . DEFINITION 2 ( ε NEIGHBORHOOD WITH MAHALANOBIS DISTANCE ) Given a ε ∈ R and x ∈ D , the ε neighborhood of an object x with Mahalanobis distance , N m ( x ) , is defined as :
ε ( x ) = {y ∈ D|
N m
( y − x ) · Σ−1 x · ( y − x)T ≤ }
( 3 ) where Σx is the covariance matrix of ε neighborhood of x . Since the Mahalanobis distance considers the local data distribution , it can better search similar objects considering the local cluster structure and is also less sensitive to noise . Fig 2 illustrates the intuition of the similarity according to Euclidean and Mahalonobis distance , respectively .
According to Definition 2 , we extend the Kuramoto model in a local fashion , where each object interacts with its εNeighborhood with mahalanobis distance during time revolution . Moreover , since we have no external knowledge of each object , all objects are assumed to be the same frequency ω , which well fits the condition of Kuramoto model . Here , we view each dimension of an object as a phase oscillator and its original value represents the initial phase . Formally , let x ∈ Rd be an object in the data set D and ε ( x ) is xi be the i th dimension of the data object x . N m the ε neighborhood of object x . According to Eq ( 1 ) , the dynamics of each dimension xi of the object x with a local interaction is further written as : K ε ( x)| |N m sin(yi − xi ) dxi dt
= ω +
( 4 ) y∈N m
ε ( x )
Let dt = ∆t , then : xi(t + 1 ) = xi(t ) + ∆t· ω+
ε ( x(t))| ·
∆t · K y(t)∈N m
+
|N m
ε ( x(t ) )
( 5 ) sin(yi(t ) − xi(t ) )
Since the term ∆t · ω is the same for each dimension of all objects and thus can be ignored . Let C = ∆t · K , the dynamics of each dimension xi of an object x over time is written as :
ε ( x(t))| ·
C
|N m xi(t+1 ) = xi(t)+ y(t)∈N m where t = ( 0,··· , T ) is the time step .
ε ( x(t ) ) sin(yi(t)−xi(t ) )
2 ) Weighted Interaction Determination : For most existing interaction models , eg [ 2 ] , [ 7 ] , [ 9 ] , [ 12 ] , the coupling strength of the object interactions is constant . However , this is not appropriate for subspace clustering since clusters exist in different subspaces . Thus , to ensure all cluster objects can synchronize in corresponding subspaces , the strength of interactions is guided by the local data structure of the
Figure 3 . Weighted interaction between two objects . objects . For an object x , we expect that the interactions along the main directions of the local cluster structure ( relevant and potentially correlated dimensions ) are imposed much higher weights while those in irrelevant dimensions have lower weights . Therefore , to determine the main directions of the local cluster structure , PCA is used to decompose the covariance matrix Σ of objects N m ε ( x ) , which is denoted by Σ = V EV T . The orthogonal Matrix V is called eigenvector matrix and the diagonal matrix E is called eigenvalue matrix . The eigenvectors represent the principal directions of these similar objects and the eigenvalues represent the variance along these directions . We normalize the eigenvalues into the interval ( 0 , 1 ) by dividing each eigenvalue λi with the sum of all eigenvalues . Then , the normalized eigenvalues are viewed as the interaction weights along with the corresponding principal directions . Finally , we project the difference vector between two objects onto these orthogonal eigenvectors and couple the difference with corresponding normalized eigenvalues . Formally , the weighted interaction between two objects is defined as follows : DEFINITION 3 ( WEIGHTED INTERACTION ) Let x ∈ Rd be an object in the data set D . N m ε ( x(t ) ) is the ε Neighborhood of the object x and y ∈ N m ε ( x(t) ) . v1 , , vd and λ1 , , λd are the eigenvectors and eigenvalues by PCA decomposition ε ( x(t) ) . The weighted interof the covariance matrix of N m action between the object y ∈ N m ε ( x(t ) and the object x , denoted by W I(y−x ) , is defined as :
W I(y−x ) =
λi · sin(proj(∆(y , x ) , vk ) )
( 7 ) d k=1 where ∆(y , x ) = y− x means the difference vector between y and x , proj(∆(y , x ) , vi ) means the projection of vector ∆(x , y ) onto vi . Since the eigenvectors vi are unit vectors , therefore , proj(∆(y , x ) , vi ) = ( ∆(y , x ) fi vi ) · vi
( 8 )
( 6 ) where fi means the inner product .
To illustrate the weighted interaction , Fig 3 gives an example with a 2 dimensional data set . Given an object P , first , the ε neighborhood of object P with Mahalanobis distance are obtained . Then we decompose the covariance matrix of these objects by PCA and obtain the eigenvectors ( v1 , v2 ) and eigenvalues ( λ1 , λ2 ) respectively . For each interaction with object P , eg Q − P interaction , the
610
Pεεv1(λ1)Q1v1 ( λ1)v2(λ2)PQQ2Q ( a ) t = 0
( b ) t = 2
( c ) t = 6
( d ) t = 8
( e ) t = 10
( f ) Main Directions
( g ) Order Parameter
( h ) Synchronized Clusters
Figure 4 .
Illustration of dynamics of objects according to the interaction model , where t is time step .
−−→ Q1P and the second direction v2 , denoted by
−−→ difference vector QP is projected to the first direction v1 −−→ with Q2P . The interaction between objects Q and P is finally determined with λ1 · sin(
−−→ Q1P ) + λ2 · sin(
−−→ Q2P ) .
Finally , the dynamics of each dimension xi of the object x is governed by : xi(t+1 ) = xi(t)+
· y(t)∈N m
ε ( x(t ) ) k=1
1
ε ( x(t))|· d |N m λk·sin(proj(i)(∆(x(t ) , y(t) ) , vk ) )
( 9 ) where proj(i ) means the i th dimension of the projected vector . The object x at time step t = 0 : x(0)(x1(0);··· ; xd(0 ) ) represents the initial state of the object . The term xi(t + 1 ) describes the renewed state value of i th dimension of object x at time point ( t + 1 ) .
To determine the termination of the dynamic process , a synchronization order parameter r is defined as measuring the degree of synchronization of objects . DEFINITION 4 ( SYNCHRONIZATION ORDER PARAMETER ) The synchronization order parameter r characterizing the degree of synchronization is defined as the average movements of objects over time :
1
N i=1 r =
1 N
|Nε(x)| y∈Nε(x )
W I(y−x )
( 10 )
The value of r decreases when more and more objects synchronize together as time evolves . The process of synchronization terminates when r converges , which indicates there is no further change of objects . Due to space limitation , the proof of the convergence of r is not provided here .
611
C . Simulation of the Object Dynamics After formulating our interaction model , we can simulate the dynamics of objects to investigate all clusters in arbitrarily oriented subspaces . Generally , the dynamics of objects involve the following steps :
1 ) After initialization ( t = 0 ) , all objects in the data set have their own states ( feature vectors ) .
2 ) As time evolves ( t > 0 ) , for each object x(t ) , the similar objects according to Mahalanobis distance ε ( x(t ) ) are searched . Then PCA is applied to deN m ε ( x(t ) ) to obtain compose the covariance matrix of N m the corresponding eigenvectors and eigenvalues . The renewed state of object x(t ) is then determined by Eq 9 .
3 ) During the process towards synchronization , the order parameter r(t ) ( Eq 10 ) is calculated to test for convergence .
To illustrate the dynamics of objects according to our interaction model , for the ease of illustration , a 3 dimensional data set is provided as an example . Fig 4(a ) ( t=0 ) shows the initial states of objects , where three clusters exist in the data set : a 3 dimensional Gaussian cluster in full dimensions , a 2dimensional linear correlation cluster in arbitrarily oriented subspace and 1 dimensional linear cluster in Z axis . When t > 0 , each object starts to interact with similar objects according to the local cluster structure . For relevant dimensions , the object interaction is imposed much higher strength while lower impact for irrelevant dimensions . Eg , in Fig 4(b) (e ) , the objects in the Gaussian cluster gradually move together from all directions in the three dimensions since these objects belong to a common 3 d subspace cluster . We can see that the eigenvalues along with main directions of these objects ( eigenvectors ) decomposed by PCA are very similar ( λ1 ≈ λ2 ≈ λ3 , see Fig 4(f) ) . Therefore , the object interactions are imposed similar strengths and gradually group together . This situation is different from objects in the other two clusters . For the objects in the 2dimensional correlation cluster , the main directions of the cluster structure are not along with the coordinate axis but arbitrarily oriented . The eigenvalues on the corresponding eigenvectors decomposed by PCA are thus very different , where ( λ1 > λ2 and λ3 ≈ 0 ) . These cluster objects therefore mainly move towards two directions ( v1 ) and ( v2 ) with weights λ1 and λ2 respectively ( Fig 4(f) ) . Similarly , the 1dimensional linear cluster objects tend to move towards one direction ( v1 ) . Through such weighted interactions , finally , all cluster objects synchronize together in the corresponding subspaces ( Fig 4(e ) and Fig 4(h) ) . During the process towards synchronization , the synchronization order parameter will decrease and finally converge . ( Fig 4(g) ) .
D . Synchronized Clusters Search After the simulation of dynamics of objects by our interaction model , cluster objects in arbitrarily oriented subspaces synchronize together . To find these synchronized clusters , the intuitive way is to find all synchronized phases and corresponding objects . The principle of our strategy is to consider the subspace search from objects instead of dimensionality . Specifically , after the simulation of dynamics of objects , for each object we investigate whether other objects synchronize with it in any dimension that has the same phase . If it does not synchronize with any dimension of other objects ( with same phase ) , this object is viewed as noise . If it synchronizes with some dimensions of other objects , these synchronized dimensions are regarded as a synchronized subspace and these synchronized objects are formed as a synchronized cluster . We repeat this process for each object and finally we get all synchronized subspaces and corresponding synchronized clusters .
To illustrate the search strategy , let us look at Table I . Supposing there are 7 objects with 4 dimensions , after weighted interaction among objects , the final states of objects are outlined in the left part of Table I . For each object , we find its synchronized dimensions and corresponding objects . Eg , object #1 synchronizes with objects #2 to #4 in dimensions 1 and 2 . Therefore , a synchronized subspace ( 1,2 ) is created and the corresponding objects #1 to #4 are added to a cluster C1 ( 1,2,3,4 ) in this subspace . At the same time , object #1 also synchronizes with object #5 in dimension 4 . A new synchronized subspace ( 4 ) is created and corresponding new cluster C2 ( 1,5 ) . Similarly object #2 synchronizes with objects #3 and #4 in dimensions ( 1,2,4 ) and a new subspace is thus created and obtains corresponding cluster C3 . This process is repeated until all objects are investigated . In addition , since object #7 does not synchronize with any other object at any dimension , it is thus viewed as noise in the full dimensional space .
Table I
ILLUSTRATION OF THE SUBSPACE SEARCH STRATEGY . Obj . d1 d2 d3 d4 Syn . Dim . New Subs . Cluster 1
0.1 0.2 0.1 0.3 1,2
4
0.1 0.2 0.2 0.2 1,2,4 0.1 0.2 0.7 0.2 1,2,3,4 0.1 0.2 0.7 0.2 1,2,3,4 0.3 0.4 0.3 0.3 3 0.9 0.5 0.3 0.1 3 0.7 0.6 0.4 0.5 Null
2 3 4 5 6 7
( 1,2 ) ( 4 ) ( 1,2,4 ) ( 1,2,3,4 ) ( 3 )
C1 ( 1 2 3 4 ) C2 ( 1 5 ) C3 ( 2 3 4 ) C4 ( 3 4 ) C5 ( 5 6 ) Noise
Once we have detected all synchronized clusters , we can also determine their dimensionality : DEFINITION 5 ( DIMENSIONALITY OF A SYNCHRONIZED CLUSTER ) Let S ⊆ D be a synchronized cluster and Σ = V EV T , E = diag(λ1,··· λd ) be the eigenvalues of S in descending order . Given a δ ≈ 0 , the dimensionality of S wrt δ is η if d − η eigenvalues of S are close to zero ( Φ(λi ) ≤ δ and Φ(λi ) = λi/λ1 ) , which is denoted by DIMSYNCLUη δ . Finally , the Pseudocode of the ORSC Algorithm is illustrated in Fig 5 .
E . Parameter Setting To simulate the dynamics of objects , an interaction range ( ε ) needs to be specified in our Interaction Model . The question is : how to determine the ε value and how does the clustering results change when the ε value is adjusted ? In order to generate a stable interaction among objects , a heuristic way is to use the average value of the k nearest neighbor distance determined by a sample from the data set for a small k . Fig 6(a ) shows a simple data set which consists of 2 clusters plus outliers . We start with a very small value and then gradually increase this value to see the change of clustering results . With different parameter ε , we compute the number of clusters which are detected . We can see that the same clustering results ( 2 clusters ) are obtained with a fairly long stable range ( see Fig 6(b) ) . In comparison to other subspace clustering algorithms , the parametrization of our algorithm is more flexible and robust . The reason is that our clustering is a dynamic process , which moves each object during the process towards synchronization . With a small interaction range , in the beginning , a few objects interact with each other . But after several time steps , the objects in a cluster will gradually move closer and thus more and more objects can interact with each other and finally synchronize together . If a larger interaction range is selected , the only difference is that more objects interact with each other at the beginning and thus tend to synchronize earlier .
F . Complexity Analysis For the simulation of the dynamics of objects at each time step , we need to search ε Neighborhood of each object with
612 algorithm [ S , C ] = ORSC(D , ε )
= Simulation(D , ε ) ;
D [ S , C ] = SearchCluster(D
)
Return S , C ;
//Objects’ dynamics Simulation . Function D
=Simulation(D , ε ) ; while(loopFlag=true ) for(each object x ∈ D )
Search Nε(x ) of object x ; Compute N m Determine the interaction directions and weights by PCA ; Obtain new state of object x with interaction model ( Eq 9 ) ;
ε ( x ) by Definition 2 in Nε(x ) ; end for
Set D Compute synchronization order parameter r ; if( r converges ) to be the new states of all objects ; loopFlag=false ; end if end while return D
;
Else
End If
Else
End For
End For return S , C ;
//Search Synchronized Clusters .
Function [ S , C]=SearchCluster(D
) ;
S = null ; C = null ; //S saves subspaces and C saves clusters for(each object x ∈ D
) for(each object y ∈ D
)
If y synchronized with x in dimensions L ( eg dist(yi , xi ) < 1e − 4 ) ;
If ( L exists in S ) and synchronized phases are same
Add y to the exist corresponding cluster ;
Create a new synchronized subspace L ; Create a new cluster CL ; Add x and y to the new cluster CL ; S.add(L ) , C.add(CL ) ; y is viewed as noise object ;
Figure 5 . Pseudocode of the ORSC Algorithm .
( a ) Figure 6 .
Impact of parameter setting for clustering .
( b )
Mahalanobis distance . The covariance matrix with Euclidean distance query computation can be evaluated in O(N · d ) time . The covariance matrix is then used to calculate the Mahalanobis distance and the time complexity approximately requires O(d3 ) time . We further use PCA to decompose the covariance matrix with Mahalanobis distance and it thus requires O(d3 ) time . For each object ’s interaction , O(d3 ) time is required . Therefore , for all objects together , the simulation of dynamics at one time step is O(N 2·d+N·d3 ) .
613
If there exists an efficient index , the complexity reduces to O(T · N log N · d + log N · d ) . For all time steps towards synchronization , the time complexity of objects’ simulation is O(T · ( N 2 · d + N · d3 ) ) in worst case . T is the number of time steps . In most cases , T is small with 5 ≤ T ≤ 20 . For the time complexity of subspaces and the corresponding clusters search , the most bottom up establishing algorithms of subspace clustering need O(2d ) time . For our search , we do not need to enumerate all subspaces . Instead , we find all synchronized subspaces by investigating the synchronized dimensions of each object . Therefore , the time complexity becomes O(N 2· d ) . Finally , the time complexity of our algorithm in the worst case is O(T · ( N 2 · d + N · d3 ) + N 2 · d ) .
IV . EXPERIMENTAL EVALUATION
To extensively study the performance of ORSC , we perform experiments on several synthetic and real world data sets . We compare the performance of ORSC to ORCLUS [ 4 ] , 4C [ 8 ] and Curler [ 19 ] . We select these particular algorithms because they are representatives of different algorithmic paradigms : ORCLUS is a K means style iterative partitioning algorithm ; 4C is a local density based method ; Curler tries to find non linear correlation clusters based on EM clustering . All the data are normalized and all experiments have been performed on a workstation with 2.4 GHz CPU and 2.0 GB RAM .
Moreover , we report two established measures for cluster quality [ 15 ] : Normalized Mutual Information ( NMI ) , Adjusted Mutual Information ( AMI ) to evaluate different clustering results . For both measures , higher values represent better clustering performance . We also provide precision ( P ) and recall ( R ) as validity measures to analyze each individual cluster .
A . Effectiveness
1 ) Synthetic Data : We start the evaluation of ORSC with several synthetic data sets to facilitate presentation and demonstrate its benefits . Fig 7 displays the clustering results on a 3 dimensional synthetic data with all comparing subspace clustering algorithms . The data consists of 11 clusters of different dimensionality , object density and correlation strength plus noise ( Fig 7(a) ) . ORSC with parameter ε = 0.06 successfully detects all these correlation clusters , including five 1 dimensional linear correlation clusters , two 2 dimensional linear plane clusters , two 2 dimensional nonlinear clusters and two 3 dimensional clusters ( Fig 7(a) ) . ORCLUS requires the number of cluster K and the average subspace dimensionality l as input parameter . We specify K = 11 and obtain the best results with l = 3 , which are indicated in Fig 7(b ) . Fig 7(c ) displays the best clustering results of 4C with parameters = 0.03 , M inP ts = 6 and λ = 3 . For Curler , we use all default parameter values which are suggested by authors . It obtains as many as
00204060810020406081000200400600801012014051015202530Parameter εNumber of Clustersε range ( a ) ORSC
( b ) ORCLUS
( c ) 4C
( d ) Curler
Figure 7 . Comparison with different algorithms on a 3 d synthetic data .
COMPARATIVE EVALUATION OF DIFFERENT SUBSPACE APPROACHES ON HIGH DIMENSIONAL SYNTHETIC DATA SETS .
Table II
Data
DS1 d
5
DS2
10
DS3
15
DS4
20
DS5
30
#Clu . Dim . of Clu .
1
1
2
2
3
3
5
10,5
10,10
20,15,10
ORCLUS 1 ( Dim . : 3 )
True clusters found by 4C
Curler
1 ( Dim . : 3 )
1 ( Dim . : 3 )
ORSC
1 ( Dim . : 3 )
( P=327%;R=912 % )
( P=100%;R=100 % )
( P=990%;R=204 % )
( P=100%;R=97.0 % )
1 ( Dim . : 5 )
( P=278%;R=622 % ) 2 ( Dim . : 10,5 ) ( P=169%,744 % ) ( R=140%,616 % ) 2 ( Dim . : 10,10 ) ( P=17.9%,100 % ) ( R=132%,740 % )
3 ( Dim . : 20,15,10 ) ( P=217%,123%,132 % ) ( R=100%,675%,725 % )
1 ( Dim . : 5 )
( P=100%;R=94.2 % )
1 ( Dim . : 10 )
( P=100%,R=99.6 % )
2 ( Dim . : 10,10 ) ( P=100%,100 % ) ( R=100%,100 % ) 1 ( Dim . : 20 )
( P=100 % ; R=98.5 % )
1 ( Dim . : 5 )
( P=484%;R=118 % ) 2 ( Dim . : 10,5 ) ( P=145%,120 % ) ( R=193%,160 % ) 2 ( Dim . : 10,10 ) ( P=12.5%,100 % ) ( R=12.5%,100 % )
Dim . : 5 )
( P=100%;R=97.4 % ) 2 ( Dim . : 10,5 ) ( P=100%,99.6 % ) ( R=99.6%,100 % ) 2 ( Dim . : 10,10 ) ( P=100%,99.6 % ) ( R=100%,99.6 % )
3 ( Dim . : 20,15,20 ) ( P=100%,995%,769 % )
( R=99.0%,100%,5 % )
3 ( Dim . : 20,15,10 ) ( P=100%,985%,996 % ) ( R=100%,99.5%,100 % )
150 clusters ( Fig 7(d) ) . For better investigating different clustering results , we focus on the detailed view of the 2d linear plane cluster and four 1 d linear clusters in the center of the data set ( cf . Fig 8 ) . It is obvious that ORSC outperforms the competitors , where all correlation clusters are successful detected with high precision and recall . The evaluation of the clustering results is further illustrated in Table III .
To further evaluate our algorithm ORSC , we generate five high dimensional data sets . In each data set , several clusters are hidden in subspaces of varying dimensionality plus noise . All dimensions for noise objects were drawn independently at random from the uniform distribution . The values of dimensions for cluster objects were generated from a d dimensional ( the number of dimensionality for clusters ) multivariate Gaussian distribution with different means and covariance matrices . We successively increased the dimensionality and also added noise dimensions . For comparison , we check whether each algorithm can detect these clusters with suitable parameters . We report its precision and recall for each cluster . The results with different subspace clustering algorithms are depicted in Table II . In all these data sets , ORSC finds the synthetic clusters in corresponding subspaces with both high recall and precision . In contrast to the comparing algorithms , there is no need
Figure 8 . Detailed view of clustering results with different algorithms on part of 3 d synthetic data . to specify the subspace dimensionality and all interesting subspace clusters are detected . For 4C and ORCLUS , we manually specify the subspace dimensionality although it is difficult to know in real world . 4C performs very well on the first two low dimensional data sets . But it fails to find 5 dimensional clusters in the 15 dimensional data set . This
614
PlaneLine1Line2Line3Line4Plane Line 1 Line 2 Line 3 Line 4 P=100%P=997%P=971%P=993%P=998%R100%(a) ORSCMd1lllR=100% R=973% R=100% R=100% R=100% LiMerged: 1 cluster Split: 7 clusters  Split: 15 clusters  (b) 4CPlane Lines (c) CurlerPlane1Plane2Plane3Lines(d) ORCLUS Figure 9 . Clusters found by ORSC on the Ecoli data set ( ε = 0.25 )
Table III
Ecoli data
EVALUATION ON DIFFERENT DATA SETS . Synthetic data AMI NMI 0.980 0.981 0.829 0.830 0.596 0.598 0.583 0.561
AMI 0.670 0.328 0.430 0.049
NMI 0.682 0.338 0.452 0.060 Table IV
NMI 0.701 0.474 0.191
0
ORSC CLUSTERING RESULTS ON WINE DATA .
Wine data
AMI 0.695 0.469 0.182
0
Methods
ORSC
4C
ORCLUS
Curler
Cluster ID Type1
Type2
Type3
1 2 3 4
58 0 0 0
3 53 5 4
0 0 48 0
Prec . Rec . 95.2 % 98.3 % 100 % 73.6 % 90.6 % 100 % 100 % 5.6 % is also the same for the fifth data set , where it cannot find the 10 dimensional cluster and 15 dimensional cluster because these two clusters are not dense in the full 30 dimensional feature space any more . The algorithm ORCLUS is sensitive to noise and usually the noise objects are included in some clusters . In addition , ORCLUS often merges these subspace clusters together , which thus results in low precision . The algorithm Curler is also sensitive to noise and cannot yield good results for all data sets . However , to ORCLUS , it tends to split true clusters into several distinct clusters . Therefore , the obtained clusters are usually of low recall . in contrast
2 ) Real World Data : In the following , we evaluate the performance of ORSC on two real world data sets which are publicly available at the UCI machine learning repository . Ecoli Data : This Ecoli data deriving from a study on protein location consists of 336 instances . It includes eight highly unbalanced classes having from 2 to 142 objects per class . Each instance is described by 7 attributes . ORSC detects 6 meaningful clusters for this data set . Each cluster is mainly represented as one class , see Fig 9 in detail . Cluster 1 is composed of 147 instances and 140 out of them belong to cytoplasm , which results in P = 95.24 % and R = 9790 % Cluster 2 includes 56 instances and mainly represents perisplasm with P = 85.71 % and R = 9231 % The type of inner membrane without signal sequence is
615 identified by cluster 3 and cluster 4 together . Similarly , the cluster 5 and cluster 6 represent the type outer membrane and outer membrane lipoprotein with high precision 92.86 % and 100 % respectively . The 4C algorithm obtains best results with parameters M inpts = 6 , = 0.15 and λ = 7 . It detects 6 clusters but with low class purity . ORCLUS obtains its best results with parameters k = 8 and l = 7 . However , like 4C , many instances are wrongly assigned . The algorithm Curler has similar results like 4C and ORCLUS , which cannot predict the protein location effectively . The evaluation of the clustering results is further illustrated in Table III .
Wine Data : The well known wine data set is the result of a chemical analysis of wine grown in the same region in Italy but deriving from three different cultivars . The analysis determined the quantities of 13 constituents found in each of the three types of wine . The class distribution is as follows : type1 : 59 ; type2 : 71 , type3 : 48 . Table IV gives the clustering results of ORSC with parameter ε = 06 It detects 4 clusters and 8 instances are viewed as noise . Three main detected clusters match with corresponding wine types with high precision and recall . In detail , cluster 1 includes nearly all instances ( 58 out of 59 instances ) of type 1 plus 3 instances of type 2 . The 53 instances in cluster 2 completely belong to type2 . Cluster 3 consists of all instances of type 3 . In addition , the cluster 4 includes 4 instances of type 2 . It is clear that ORSC can discover interesting patterns of the data set effectively . For 4C with parameter M inpts = 6 , = 0.5 and λ = 13 , it discovers 2 clusters and 34 instances are regarded as noise . As many instances of the two clusters are wrongly clustered and it is difficult to distinguish the three cultivar types . The algorithm of ORCLUS cannot obtain good clustering results although we manually specify the number of clusters with parameters k = 3 and l = 13 . Curler even cannot find any correlation cluster for the data set with different parameters . All instances are assigned to the same cluster . The evaluation of these clustering results is further indicated in Table III . B . Efficiency Fig 10(a ) shows the results of runtime experiments for a 3 d synthetic data set with varying number of objects in the range of 500 to 30,000 . ORCLUS and ORSC scale nearly linear against the size of the data set while 4C scales
Cluster Purity : 95.24 % Cluster Purity : 85.71%Cluster Purity : 92.86 % Cluster Purity : 100 % [ 6 ] A . Arenas , A . D. Guilera , J . Kurths , Y . Moreno , and C . S . Zhou . Synchronization in complex networks . Phys . Rep . 469:93–153 , 2008 .
[ 7 ] A . Arenas , A . Diaz Guilera , and C . J . Perez Vicente . Synchronization reveals topological scales in complex networks . Phys . Rev . Lett . , 96(11):1–4 , 2006 .
[ 8 ] C . B¨ohm , K . Kailing , P . Kr¨oger , and A . Zimek . ComputIn SIGMOD ing clusters of correlation connected objects . Conference , pages 455–466 , 2004 .
[ 9 ] C . B¨ohm , C . Plant , J . Shao , and Q . Yang . Clustering by synchronization . In KDD Conference , pages 583–592 , 2010 .
[ 10 ] C . H . Cheng , A . W C Fu , and Y . Zhang . Entropy based In KDD subspace clustering for mining numerical data . Conference , pages 84–93 , 1999 .
[ 11 ] P . Kr¨oger , H P Kriegel , and K . Kailing . Density connected In SDM subspace clustering for high dimensional data . Conference , 2004 .
[ 12 ] Y . Kuramoto .
Self entrainment of a population of couIn Proceedings of the Internapled nonlinear oscillators . tional Symposium on Mathematical Problems in Theoretical Physics,Lecture Notes in Physics , pages 420–422 . edited by H . Araki ( Springer , New York , USA ) , 1975 .
[ 13 ] Y . Kuramoto . Chemical oscillations , waves , and turbulence .
Springer Verlag , Berlin , 1984 .
[ 14 ] J . Liu and W . Wang . Op cluster : Clustering by tendency in In ICDM Conference , pages 187– high dimensional space . 194 , 2003 .
[ 15 ] X . V . Nguyen , J . Epps , and J . Bailey . Information theoretic measures for clusterings comparison : is a correction for chance necessary ? In ICML Conference , pages 1073–1080 , 2009 .
[ 16 ] C . M . Procopiuc , M . Jones , P . K . Agarwal , and T . M . Murali . In
A monte carlo algorithm for fast projective clustering . SIGMOD Conference , pages 418–427 , 2002 .
[ 17 ] M . B . H . Rhouma , H . Frigui . Self organization of pulsecoupled oscillators with application to clustering . IEEE Transactions on Pattern Analysis and Machine Intelligence , 23(2 ) : 180 195 , 2001 .
[ 18 ] J . Shao , C . B¨ohm , Q . Yang , and C . Plant . Synchronization based outlier detection . In ECML/PKDD Conference , pages 245–260 , 2010 .
[ 19 ] A . K . H . Tung , X . Xu , and B . C . Ooi . Curler : Finding In SIGMOD and visualizing nonlinear correlated clusters . Conference , pages 467–478 , 2005 .
[ 20 ] H . Wang , W . Wang , J . Yang , and P . S . Yu . Clustering by pattern similarity in large data sets . In SIGMOD Conference , pages 394–405 , 2002 .
Figure 10 . Scalability of ORSC against the size and dimensionality of the data set . quadratically . The runtime of Curler is rather high for large number of objects and thus only the runtime for maximal 10,000 objects is displayed . For comparison of the scalability in the dimensionality d , data sets consisting of 2000 objects of dimensionality ranging from 5 to 50 are generated . Fig 10 ( b ) reports the runtime against dimensionality . Since all algorithms are involved with PCA , they scale similar with the dimensionality . ORCLUS is the fastest approach but its effectiveness is not satisfying ( cf . Section IV A ) . ORSC outperforms 4C and Curler . In summary , ORSC scales very well with the number of objects and dimensionality .
V . CONCLUSIONS
In this paper , we propose ORSC , a novel subspace clustering algorithm based on synchronization . We consider each dimension of object as a phase oscillator and simulate the dynamics of each object according to our proposed interaction model . Through the weighted non linear interaction among objects , all axis parallel and arbitrarily oriented subspace clusters naturally synchronize together . To the best of our knowledge , ORSC is the first approach relating the problem of finding subspace clusters in high dimensional data to synchronization . Extensive experiments demonstrated the superior efficiency and effectiveness of our ORSC algorithm .
REFERENCES
[ 1 ] J . A . Acebron , L . L . Bonilla , C . J . P . Vicente , F . Ritort and R . Spigler . The Kuramoto model : A simple paradigm for synchronization phenomena . Rev . of Modern Physics,77(2):137185 , 2005 .
[ 2 ] D . Aeyels and F . D . Smet . A mathematical model for the dynamics of clustering . Physica D : Nonlinear Phenomena , 273(19):2517–2530 , 2008 .
[ 3 ] C . C . Aggarwal , C . M . Procopiuc , J . L . Wolf , P . S . Yu , and J . S . Park . Fast algorithms for projected clustering . In SIGMOD Conference , pages 61–72 , 1999 .
[ 4 ] C . C . Aggarwal and P . S . Yu . Finding generalized projected clusters in high dimensional spaces . In SIGMOD Conference , pages 70–81 , 2000 .
[ 5 ] R . Agrawal , J . Gehrke , D . Gunopulos , and P . Raghavan . Automatic subspace clustering of high dimensional data for data mining applications . In SIGMOD Conference , pages 94– 105 , 1998 .
616
800090006000700080009000ORSC50006000700080009000me (s)ORSCCurler4C3000400050006000700080009000Runtime (s)ORSCCurler4CORCLUS20003000400050006000700080009000Runtime (s)ORSCCurler4CORCLUS0100020003000400050006000700080009000Runtime (s)ORSCCurler4CORCLUS0100020003000400050006000700080009000050001000015000200002500030000Runtime (s)Number of ObjectsORSCCurler4CORCLUS0100020003000400050006000700080009000050001000015000200002500030000Runtime (s)Number of ObjectsORSCCurler4CORCLUS150200250300350400450500Runtime (s)ORSCCurler 4C ORCLUS05010015020025030035040045050001020304050Runtime (s)DimensionalityORSCCurler 4C ORCLUS
