2011 11th IEEE International Conference on Data Mining
Heuristic Updatable Weighted Random Subspaces for Non stationary Environments
T . Ryan Hoens
Nitesh V . Chawla
Robi Polikar
Department of Computer Science
Department of Computer Science
Electrical and Computer and Engineering
University of Notre Dame Notre Dame , Indiana 46556 thoens@csendedu and Engineering
University of Notre Dame Notre Dame , Indiana 46556 nchawla@csendedu
Engineering
Rowan University
Glassboro , NJ 08028 polikar@rowan.edu
Abstract—Learning in non stationary environments is an increasingly important problem in a wide variety of real world applications . In non stationary environments data arrives incrementally , however the underlying generating function may change over time . While there is a variety of research into such environments , the research mainly consists of detecting concept drift ( and then relearning the model ) , or developing classifiers which adapt to drift incrementally .
We introduce Heuristic Updatable Weighted Random Subspaces ( HUWRS ) , a new technique based on the Random Subspace Method that detects drift in individual features via the use of Hellinger distance , a distributional divergence metric . Through the use of subspaces , HUWRS allows for a more finegrained approach to dealing with concept drift which is robust to feature drift even without class labels . We then compare our approach to two state of the art algorithms , concluding that for a wide range of datasets and window sizes HUWRS outperforms the other methods .
Keywords Concept Drift ; Hellinger Distance ; Random Sub spaces ; Non stationary learning ;
I . INTRODUCTION
One of the current topics in data mining research is learning from non stationary data streams . In such learning scenarios , data arrive incrementally ( or in batches ) , and , over the course of time , the underlying data generation function may change . Learning in such environments requires the classifier be able to respond to such changes , while ensuring that it still retains all ( relevant ) past knowledge . These two competing forces bring rise to what is known as the stabilityplasticity dilemma [ 11 ] . This issue is further complicated if data cannot be stored due to space constraints .
Since in non stationary environments the underlying generating function changes — or drifts — over time , such environments are said to exhibit concept drift . Typically when referencing concept drift , the drift is further specified as being real ( ie , the class conditional changes ) or virtual ( ie , the prior probabilities change ) [ 33 ] . While concept drift can be “ real ” or “ virtual ” , it can also occur gradually , instantaneously ( sometimes called concept change ) , or in a cyclic manner ( sometimes called a recurring context ) . learning environments , instances arrive sequentially , ie , an unlabeled instance first
In traditional incremental arrives for testing , and then , once a prediction is given , a label is made available to update the classifier . Alternatively , data may arrive in batches , ie , a non empty set of instances first arrive unlabeled for testing . Once predictions have been given for all instances in the set , the labels for each of the instances are made available in order to update the classifier . For the sake of completeness , we consider both types of drift ( ie , “ real ” and “ virtual ” ) occurring gradually , as sudden concept change , or as a recurring concept . Due to the nature of our algorithm , however , we consider the case where instances arrive in batches instead of on line . We discuss our future plans on mitigating this shortcoming in Section IX .
Contributions : The current state of the art techniques for learning in non stationary environments consider concept drift to have occurred if drift is detected in any of the features . We claim that this is an oversimplification , as a different features may drift at different rates and times , and the information available in non drifting features can be leveraged into making strong predictions . Therefore , we propose a method of learning in subspaces of the feature space called Heuristic Updatable Weighted Random Subspaces ( HUWRS ) ( Section III ) , where each base classifier is weighted based on the amount of drift experienced in its subset of features . We then compare HUWRS to a variety of state of the art models ( Section VI ) . We also specifically demonstrate how HUWRS exploits the nature of learning in subspaces to provide an accurate classifier even without the aid of labels ( Section VII ) . We conclude with a discussion and future work ( Section IX ) .
II . MOTIVATION
The prevailing unstated assumption in the current state of the art in non stationary learning is that a drift in a single feature results in having to relearn the entire model . In spite of this , when generating synthetic datasets for concept drift it is common practice to provide an option that defines what percentage of the features experience concept drift , while the remainder continue to be drawn from their original distribution . Such differing assumptions indicate that there is a disconnect between the state of the art learning algorithms ,
1550 4786/11 $26.00 © 2011 IEEE DOI 101109/ICDM201175
241 and the stream generation algorithms . This disconnect can be costly , as the amount of information ( in terms of the models learned and data ) discarded can be vast . This is especially true if only one out of tens or hundreds of features are drifting , in which case building a model on the remaining features would be a more practical solution than relearning the model once the drift in the drifting feature is too great . In order to avoid this specious assumption , we recommend a different approach to learning in non stationary data streams . Instead of building models on all features , and thus requiring retraining when any type of concept drift is detected , we recommend building multiple models on subsets of the features . This technique , which has been demonstrated to be effective in traditional data mining tasks [ 16 ] , has received little attention in concept drift research . With this goal in mind , we require a method for detecting individual drifting features . Feature drift is commonly detected by using the distribution of each feature with respect to each of the classes . Storing so much data per classifier is often unsatisfactory , however , as it is preferable to build models that rely only on the dataset for the current time step to update , rather than previous ones as well . Learning under these assumptions is called incremental learning .
In order to overcome this limitation , we first note that we can , instead of storing the feature value for each instance , bin each of the features by class . Binning is a common machine learning technique whereby continuous features are discretized into “ bins ” — the count of each bin represents the number of instances which have a feature value in the range of the bin1 . Through the use of binning we can reduce the space required from being proportional to the number of instances , down to a fixed constant due to the number of bins . As the datasets continue to grow in size , this will result in substantial memory savings .
The primary task is now to use the binned feature values to determine if — and to what extent — concept drift has occurred . One attractive option , explored previously as a method of detecting dataset drift [ 6 ] , [ 27 ] , is to use Hellinger distance [ 15 ] , as it is insensitive to the number of examples in a sample . Specifically , in order to detect concept drift we can , considering each class individually , measure the distance between the feature in the training dataset as well as the feature ’s current distribution . This is powerful , as it allows us to compare the amount of feature drift encountered even if the two samples are not ( near ) equisized ( unlike Euclidean distance ) . Furthermore , the use of Hellinger distance also allows us to assign a weight to a feature based on how close it is to the reference distribution ( ie , how similar the newly seen instances are to the instances the classifier was trained on ) .
One common challenge when dealing with concept drift is updating the classifier when labels are unavailable . That
1Specifically , we consider equal width binning is , most methods are unable to cope with feature drift if labels are unavailable , since the class label is required in order to detect drift . However , this overlooks the fact that while “ real ” drift cannot be detected without labels ( as it is , by definition , a change in the class conditionals ) , “ virtual ” drift can still be detected . As Hellinger distance is a measure of distributional divergence , in addition to comparing each feature with respect to the class , we can also ignore the class when performing the computation . In this way we can obtain a weight between distributions of only unlabeled instances .
III . METHOD
In this section we begin by describing the Random Subspace Method ( RSM ) [ 16 ] in the traditional data mining context . We then extend this idea to the context of batched learning in the presence of concept drift with the use of Hellinger distance .
A . The Random Subspace Method
In the data mining community , an ensemble is a collection of base learners ( also known as base classifiers ) each built on ( in some sense ) separate training datasets , which then classify new instances by voting the individual base learners predictions [ 7 ] . Two of the most widely used ensemble methods are bagging [ 4 ] and AdaBoost [ 9 ] .
Another widely used ensemble method is the Random Subspace Method ( RSM ) [ 16 ] . In the RSM ( described in Algorithm 1 ) while multiple base learners are trained on the same dataset , each base learner actively uses only a ( random ) subset of the available features . Because each base classifier learns on incomplete data , each individual base classifier is ( typically ) less effective than a single classifier trained on all of the data . Since the RSM combines multiple classifiers of this type , each with a random bias based on the features it sees , RSM often prove more effective than learning the base classifier on all of the features .
In the context of drifting features , learning on only a subset of the features is an advantage , as it means that if not all features drift simultaneously then not all base learners must be retrained . Similarly , this also enables each of the base learners in the ensemble to be weighted by the subset of features it was built on . That is , if a base learner was built on a feature which exhibits concept drift , we can reduce the weight of that classifier relative to the other classifiers .
B . Heuristic Updatable Weighted Random Subspaces for Non stationary Learning
In the previous section we saw how the RSM is applied in the typical batch learning context , and motivated its adaptation to non stationary environments . In this section we further explore the requirements for adaptation into nonstationary environments . As a result , we must first be able to detect feature drift . Once we can detect this drift , we can
242
Algorithm 1 𝑅𝑎𝑛𝑑𝑜𝑚 𝑆𝑢𝑏𝑠𝑝𝑎𝑐𝑒 𝑀 𝑒𝑡ℎ𝑜𝑑 Require : Training set 𝑋 , number of features to consider 𝑝 , and number of classifiers to train 𝑛 > 0 .
Ensure : CLASSIFIER is the model trained on training set
𝑋 , consisting of 𝑛 classifiers . for 𝑖 = 1 to 𝑛 do
Select 𝐹 , a random subset of the features such that ∣𝐹∣ = 𝑝 . Let 𝑌 ← 𝑋 . for all 𝑎 such that 𝑎 is a feature of 𝑌 do if 𝑎 /∈ 𝐹 then
Remove feature 𝑎 from 𝑌 end if end for Train CLASSIFIER𝑖 on dataset 𝑌 . end for weight each classifier appropriately in order to improve the overall classification accuracy of the ensemble .
In order to detect the drift of a feature , we introduce Hellinger distance [ 15 ] . We then specify how to incorporate it into the RSM in order to combat concept drift .
1 ) Hellinger Distance : Fundamentally , Hellinger distance is a measure of distributional divergence [ 20 ] , [ 32 ] . In order let ( 𝑃 ,𝐵,𝜈 ) be a to define Hellinger distance formally , measure space [ 13 ] , where 𝑃 is the set of all probability measures on 𝐵 that are absolutely continuous with respect to 𝜈 . Consider two probability measures 𝑃1 , 𝑃2 ∈ 𝑃 . The Bhattacharyya coefficient between 𝑃1 and 𝑃2 is defined as :
√
∫
𝑝(𝑃1 , 𝑃2 ) =
Ω
𝑑𝑃1 𝑑𝜈
⋅ 𝑑𝑃2 𝑑𝜈
𝑑𝜈 .
( 1 )
The Hellinger distance is derived using the Bhattacharyya coefficient as :
ℎ𝐻 ( 𝑃1 , 𝑃2 ) = 2
√
∫ 1 − ( √
Ω
[ ⎷∫
=
Ω
𝑑𝑃1 𝑑𝜈
⋅ 𝑑𝑃2 √ 𝑑𝜈
−
𝑑𝑃1 𝑑𝜈
𝑑𝑃2 𝑑𝜈
] )2
𝑑𝜈
𝑑𝜈 .
( 2 )
While this equation gives us the ability to compare two continuous distributions , when comparing two features we only have access to a sample of discrete instances . With this in mind , the discrete version of Hellinger distance is defined as :
√ 𝑃 ( 𝐷2∣𝑋 = 𝑖 )
)2
,
𝑑𝐻 ( 𝐷1 , 𝐷2 ) =
𝑃 ( 𝐷1∣𝑋 = 𝑖 ) −
√∑
( √
𝑖∈𝑉
( 3 ) where 𝐷1 and 𝐷2 are the distributions of a feature at two different time s2.eps , and 𝑋 = 𝑖 is the set of instance with value 𝑖 for the current feature . If the feature is continuous , we use equal width binning on the feature to turn it into a discrete feature . Note that 𝐷1 and 𝐷2 can either be all of the instances seen so far , or merely the instances available for a particular class . We discus the implications of this in future sections .
While Hellinger distance is applicable to detecting drift in a feature , it is ill suited for use as a weight . Since a low Hellinger distance means a high agreement in the two distributions , a low Hellinger distance should correspond to a high weight . Thus we obtain a weight from Hellinger distance2 as :
√ 2 − 𝑑𝐻 ( 𝐷1 , 𝐷2 )
√ 2 resulting in a normalized [ 0,1 ] weight .
ℎ𝑤(𝐷1 , 𝐷2 ) =
,
( 4 )
2 ) Combining Hellinger Distance and the RSM : By combining the RSM with Hellinger weights , we now define Heuristic Updatable Weighted Random Subspaces ( HUWRS ) ( Algorithms 2 and 3 ) . There are two main facets to the algorithm . First , Algorithm 2 updates the classifier when labeled instances become available . Second , Algorithm 3 updates the classifier as new testing ( ie , unlabeled ) instances become available .
Algorithm 2 𝑇 𝑟𝑎𝑖𝑛 𝐻𝑈 𝑊 𝑅𝑆 𝑀 𝑒𝑡ℎ𝑜𝑑 Require : Training sets 𝑋 , number of features 𝑝 , retraining threshold 𝑡 , number of bins 𝑏 , and ensemble size 𝑛 > 0 . Ensure : CLASSIFIER is the model trained on training set 𝑋 consisting of 𝑛 classifiers , FEATURES𝑖 is a vector consisting of the features used to train CLASSIFIER𝑖 , CLASS WEIGHT is a length 𝑛 vector containing the weights given to each base classifier . ← CLASSIFIER , FEATURES 𝑅𝑎𝑛𝑑𝑜𝑚 𝑆𝑢𝑏𝑠𝑝𝑎𝑐𝑒 𝑀 𝑒𝑡ℎ𝑜𝑑(𝑋0 ) for 𝑐 = 1 to 𝑛 do
1 end for while New time step 𝑖 is available do
BINS𝑐 ← 𝐵𝐼𝑁 ( 𝑋0 , FEATURES𝑐 , 𝑏 ) CLASS WEIGHT ← −→ CLASSLESS WEIGHT ← −→ 𝑡𝑒𝑚𝑝 ← 𝐵𝐼𝑁 ( 𝑋𝑖 , FEATURES𝑐 , 𝑏 ) CLASS WEIGHT𝑐 ← 𝐶𝑙𝑎𝑠𝑠 𝐻𝑊 ( 𝑡𝑒𝑚𝑝 , BINS𝑐 ) CLASSLESS WEIGHT𝑐 ← 0 if CLASS WEIGHT𝑐 < 𝑡 then for 𝑐 = 1 to 𝑛 do
0
Train CLASSIFIER𝑖 on dataset 𝑋𝑖 using features in FEATURES𝑖 . CLASS WEIGHT𝑐 ← 1 BINS𝑐 ← 𝐵𝐼𝑁 ( 𝑋𝑖 , FEATURES𝑐 , 𝑏 ) end if end for end while
2 is the maximum Hellinger distance between two distri
√
2Note that butions .
243
Algorithm 3 𝑇 𝑒𝑠𝑡 𝐻𝑈 𝑊 𝑅𝑆 𝑀 𝑒𝑡ℎ𝑜𝑑 Require : CLASSIFIER as learned using
−→ 0
𝑇 𝑟𝑎𝑖𝑛 𝑈 𝑝𝑑𝑎𝑡𝑎𝑏𝑙𝑒 𝑅𝑎𝑛𝑑𝑜𝑚 𝑆𝑢𝑏𝑠𝑝𝑎𝑐𝑒 𝑀 𝑒𝑡ℎ𝑜𝑑 , testing instance 𝑥 , set of previously seen testing instances 𝑋 , intra batch update frequency 𝑢 , and 𝑇 𝑒𝑠𝑡 returns a probability vector associated with testing 𝑥 on a classifier . Ensure : 𝑝𝑟𝑜𝑏𝑖 contains the probability that 𝑥 is class 𝑖 , and CLASSLESS WEIGHTupdated if ∣𝑋∣ ≥ 𝑢 . ∪{𝑥} 𝑝𝑟𝑜𝑏 = 𝑛𝑢𝑚 𝑐𝑙𝑎𝑠𝑠𝑒𝑠 ← the number of classes in the dataset . 𝑋 ← 𝑋 for 𝑐 = 1 to 𝑛 do ∣𝑋∣ ≥ 𝑢 then 𝑡𝑚𝑝 𝑏𝑖𝑛𝑠 ← 𝐵𝐼𝑁 ( 𝑋 , FEATURES𝑐 , 𝑏 ) CLASSLESS WEIGHT𝑐 𝐶𝑙𝑎𝑠𝑠𝑙𝑒𝑠𝑠 𝐻𝑊 ( 𝑡𝑚𝑝 𝑏𝑖𝑛𝑠 , BINS𝑐 )
← if end if 𝑤 ← CLASS WEIGHT𝑐 + CLASSLESS WEIGHT𝑐 𝑝𝑟𝑜𝑏 ← 𝑝𝑟𝑜𝑏 + ( 𝑤 ⋅ 𝑇 𝑒𝑠𝑡(CLASSIFIER𝑐 , 𝑥)/𝑛 ) end for
When labeled instances are not available , we are unable to use the class labels to determine whether or not concept drift has occurred . That is , we cannot detect if “ real ” concept drift has occurred , but can only detect “ virtual ” drift . In order to detect virtual drift , we compute the Hellinger distance between the binned feature values for each base learner3 and the current dataset , ignoring the class values .
Specifically , let 𝐷1 and 𝐷2 be two separate sets of probability distributions over a set of features . That is , let 𝐷1,𝑓 denote the probability distribution for dataset 𝐷1 and feature 𝑓 . We can convert Hellinger distance into a Hellinger weight as :
𝑛∑
𝑎=1
√ 2 − 𝑑𝐻 ( 𝐷1,𝑓 , 𝐷2,𝑓 )
√
2
𝐶𝑙𝑎𝑠𝑠𝑙𝑒𝑠𝑠 𝐻𝑊 ( 𝐷1 , 𝐷2 ) =
1 𝑛
( 5 ) where 𝑛 is the number of features to consider . When classes are not available , the Hellinger weight is computed as the average of the Hellinger weights of each of the features .
The value of this approach is that we can attempt to characterize the drift between batches of instances even when labels are not available . Hence , we can update the weight periodically , and dynamically react to virtual drift as more instances become available . We perform this check at periodic intervals ( eg , every 𝑋 % of a batch , we consider only the last 𝑋 % instances to update the intra batch weight ) . The ability to update each classifier ’s weight is important because in special cases ( eg , loan data ) , the length of time between seeing a batch of instances to test on and then
3Note that even if a pair of classifiers share a feature , the bins may be different since one may have been retrained on a different time period due to a drift in a second , unshared , feature .
244 their subsequent labels may be very long ( eg , years ) . By updating the classifier using only unlabeled data , we can exploit a frequently ignored portion of the data in order to improve classification accuracy over the course of time .
Similar to other techniques , we can also update weights when labels are available ( enabling us to detect “ real ” concept drift in the process ) . To do so , we consider not just the distribution of the feature over the entire dataset , but the distribution of the feature for each class over the dataset . That is , for each class we compute the Hellinger distance between the two distributions and average them . Given these weights , we then choose the minimum value as the weight of the classifier . If this minimum weight is below a threshold , we relearn the classifier on the new data .
Specifically , let 𝐷1 and 𝐷2 be two separate sets of probability distributions over a set of features and classes . That is , let 𝐷1,𝑓,𝑖 denote the probability distribution for dataset 𝐷1 , feature 𝑓 , and class 𝑖 . We convert Hellinger distance into a Hellinger weight as :
𝑐∑
√
𝑖=1
2 − ℎ𝑑(𝐷1,𝑓,𝑖 , 𝐷2,𝑓,𝑖 )
√ 2
𝐶𝑙𝑎𝑠𝑠 𝐻𝑊 ( 𝐷1 , 𝐷2 ) = min 𝑓∈𝑓 𝑡𝑟𝑠
1 𝑐
( 6 ) where 𝑓 𝑡𝑟𝑠 is the set of features to consider , and 𝑐 is the number of classes .
With these two methods
( 𝐶𝑙𝑎𝑠𝑠𝑙𝑒𝑠𝑠 𝐻𝑊 and 𝐶𝑙𝑎𝑠𝑠 𝐻𝑊 ) of detecting concept drift ( and consequently weighting each classifier ) , we must determine how to combine them into a single , unified , weight . To do so , if 𝑐𝑙𝑎𝑠𝑠 𝑤𝑒𝑖𝑔ℎ𝑡 we simply add the two values , the classifier on the previous denotes the weight of batch , and 𝑐𝑙𝑎𝑠𝑠𝑙𝑒𝑠𝑠 𝑤𝑒𝑖𝑔ℎ𝑡 denote the weight of the then , when classifying classifier on the current batch , an the classifier ’s by 𝑤𝑒𝑖𝑔ℎ𝑡 = 𝑐𝑙𝑎𝑠𝑠 𝑤𝑒𝑖𝑔ℎ𝑡 + 𝑐𝑙𝑎𝑠𝑠𝑙𝑒𝑠𝑠 𝑤𝑒𝑖𝑔ℎ𝑡 . instance , we weight output ie ,
IV . MEMORY USAGE
One of the important factors when developing an algorithm that handles concept drift in data streams is to ensure that the algorithm does not use a ( potentially ) infinite amount of memory . We now demonstrate that the memory usage of HUWRS is finite and bounded .
Consider how HUWRS is defined : the ensemble requires a number 𝑏 which represents how many bins are to be used to detect concept drift , an ensemble size 𝑛 , and a classifier type . Assume that the base classifier is guaranteed to use no more than 𝑥 bytes of memory . For each base classifier , we must maintain 𝑏 integers for each class ( ie , the number of instances in each of the bins for each class ) . Letting the number of classes be 𝑐 , that is 4⋅𝑏⋅𝑐 ( or 8 for 64 bit integers , however the factor of 2 does not affect the argument ) bytes of storage required per classifier .
In addition to the space used by each classifier , we also must account for the amount of space consumed by the saved test instances . Since we can control the number of instances stored by the ensemble , we assume that we allow , at most , 𝑦 bytes of space to be consumed by these instances . Given this , the overall memory utilization of the ensemble is , at most , 𝑦+𝑛⋅(4⋅𝑏⋅𝑐+𝑥 ) bytes . Since this number is finite and bounded ( by definition of each of the terms ) , we see that HUWRS can be easily modified to perform inside most memory footprints via appropriate choices of parameters .
V . EXPERIMENTAL DESIGN
We begin with a description of the implementation details of the methods used , defining relevant parameters for each of the algorithms . We then describe each of the datasets used . testing each instance in a chunk without labels first . Once all instances in a chunk were tested , we computed the accuracy of the classifier over the chunk , introduced labels for all of the instances , finally presenting them to the classifier for updating . The accuracy presented for each algorithm is computed as the average chunk performance of the classifier on the dataset .
We chose to perform the experiments over multiple different sizes of chunks in order to test HUWRS ’s robustness to the different chunk sizes . Each chunk size was chosen such that the dataset would yield at least 10 chunks at the chosen size . See Section IX for a more detailed explanation of our assumptions .
A . Implementation Details
B . Datasets
We implemented HUWRS in MOA [ 2 ] , a stream mining suite based off of Weka [ 12 ] . For our comparative analysis , we chose two state of the art algorithms : Adaptive Hoeffding Option Trees ( AHOT ) [ 3 ] , and Dynamic Weighted Majority ( DWM ) [ 26 ] ( further information about these methods can be found in Section VIII ) . As HUWRS is an ensemble applicable to any traditional base classifier , we chose C4.4 [ 30 ] decision trees , ie , unpruned , uncollapsed C4.5 decision trees [ 31 ] with Laplace smoothing applied at the leaves . As outlined by Kolter and Maloof [ 26 ] , the base learner used used for DWM was standard Na¨ıve Bayes .
In addition to a base learner , HUWRS requires several other parameters . As recommended for many ensembles , we set the ensemble size for HUWRS to 𝑛 = 100 . When considering the threshold and the number of bins , due to experiments that tested the effectiveness of Hellinger distance in the context of drift detection , we set 𝑡 = 0.7 , and 𝑏 = 30 . The application of weights across all datasets in this manner sets a general guideline , and allow us to test the method without overfitting to any particular dataset . In addition to these parameters , we also define a frequency for updating the weights of the classifier based on test instances . Due to the varying sizes of the training datasets , we chose to update the classless weights every 10 % of the window size , or 30 instances , whichever is larger . We can hence ensure a minimum number of instances to consider , while still allowing for multiple weight updates .
Finally , while HUWRS relies on using a subset of features to train each base learner , a comprehensive search through different subspace size may lead to dramatic overfitting and overstated performance . Therefore , instead of choosing a fixed subspace size for each classifier , every time a classifier is initially trained we choose a random subset size between 10 % and 90 % of the number of features . Thus we can test not only the effectiveness of the algorithm , but also its robustness to a variety of subspace sizes .
As HUWRS is a batch learner , we applied an interleaved batch chunk test then train on binary balanced datasets . That is , we partitioned the dataset into multiple equisized chunks ,
245
To ensure a fair comparison , we tested the algorithms on multiple real world and synthetic datasets , the details of which are available in Table I . The choice to use a combination of real and synthetic datasets is motivated by the fact that while concept drift is assumed in real datasets , it is difficult to identify . For this reason , synthetic datasets are often considered when learning in the presence of concept drift , as the drift type and properties can be precisely controlled . We now describe the datasets in detail , referring to the original publication for more information .
One limitation of testing with real datasets is that only one proper ordering exists , ie , since the dataset comes from a real source , the instances must appear to a classifier in a single order . Therefore the results for all real datasets are presented as the average of the accuracies on each chunk . For the synthetic dataset , however , we were able to run five iterations , and obtain the average overall performance for each set of parameters . elec2 : One widely used dataset in the concept drift community is the electricity dataset originally due to Harries [ 14 ] . The dataset consists of records collected from the Australian New South Wales Electricity Market , where prices vary based on the demand present in the market . Data points are collected in 30 minute intervals , where the class label indicates a change in price over the last 24 hours , thereby providing a level of smoothing . email data : The email created dataset , by Katakis , Tsoumakas , and Vlahavas [ 23 ] , consists of a stream of emails sequentially presented to a user who , in turn , determines whether the emails are spam ( not desirable ) or ham ( desirable ) according to personal preference . The dataset is composed of a stream of emails covering several topics , namely : science/medicine , science/space , and recreation/sports/baseball . The stream is then partition into five , 300 instance chunks , where , in the odd number chunks science/medicine is considered as the positive class while the other two topics are considered the negative class . Conversely in the even number chunks , science/medicine is considered as the negative class , while the other two topics are the positive class . Thus this dataset is an example of concept change , rather than concept drift . concept drift situation . We then discuss our results on the real datasets ( Figure 2 ) , showing how we can effectively function in such spaces as well . spam data : The spam dataset , also created by Katakis , Tsoumakas , and Vlahavas [ 21 ] , consists of spam/ham emails sent to a user collected from Spam Assassin ( http://spamassassinapacheorg/ ) According to the creators , the spam messages present in the dataset exhibit gradual , rather than instantaneous , concept drift . usenet1 : The usenet1 created [ 22 ] , was dataset , and Vlahavas by Katakis , Tsoumakas , generated from three newsgroups , namely : science/medicine , science/space , recreation/sports/baseball . The dataset is broken into chunks of 300 instances , where the chunks are defined as in email data . and usenet2 : In the usenet2 dataset , created by Katakis , Tsoumakas , and Vlahavas [ 22 ] , the same newsgroups were used as in usenet1 . The positive class , however , was chosen as only one topic at a time . Therefore , the positive class was , in order : science/medicine , science/space , recreation/sports/baseball , science/space , science/medicine .
RBF : The Random RBF ( Radial Basis Function ) generator is a synthetic dataset generator available in the MOA data mining suite . In this generator , a fixed number of random centroids are generated , where each centroid has a random location , a standard deviation , a class label , and a weight . Each new instance is generated by choosing a centroid at random ( based on weights ) . Drift is introduced by selecting a subset of the features ( in our experiments we consider drift in 4 of the 20 features ) , and introducing random Gaussian noise , where the center of the Gaussian drifts at the rate controlled by a parameter . In order to capture both slow and fast moving drift , we chose values of : 0.5 , 1 , 2 , 5 , and 10 .
Dataset elec2 email data spam data usenet1 usenet2 RBF {0.5,1,2,5,10}
# Ftrs
6 914 500 100 100 20
WS
100 , 300 , 500 , 700 , 1000
30 , 50 , 70 , 100 , 150
30 , 50 , 70 , 100 , 200 , 500
30 , 50 , 70 , 100 , 150 30 , 50 , 70 , 100 , 150 5000 , 10000 , 50000
# Insts 45,312 1,500 9,324 1,500 1,500
1,000,000
Table I
STATISTICS FOR THE DATASETS USED . # FTRS IS THE NUMBER OF
FEATURES , # INSTS IS THE NUMBER OF INSTANCES , WS DENOTES THE VARIOUS WINDOW SIZES USED FOR THE DATASET . WHILE RBF IS ONLY LISTED ONCE , IT WAS BUILT WITH MULTIPLE DIFFERENT PARAMETERS
FOR DRIFT SPEED AS ENUMERATED .
VI . RESULTS
We begin by discussion the synthetic dataset ( Figure 1 , as it illustrates the effectiveness of our algorithm in a known
246
A . Results on the Synthetic Datasets
For the synthetic dataset ( Figure 1 ) , HUWRS consistently outperforms the other two by up to approximately 20 % in accuracy . This is a strong result , as it validates empirically what HUWRS aimed to overcome . That is , this experiment shows that HUWRS can quickly and easily overcome drift in a subset of the features in order to generate a highly accurate ensemble of classifiers .
One interesting observation in these results is that HUWRS’ method of drift detection seems much more suited to faster moving concept drift ( Figure 1e ) rather than drift that occurs more gradually ( Figure 1a ) . This results from the fact that while in fast drift HUWRS is able to quickly ( and efficiently ) update the weights of each base learner before seeing labels , slow drift is missed . The weights of the base learners are therefore not properly updated , thereby negatively impacting performance .
Another observation which can be made is that HUWRS seems to preform universally better as the window size increases . This is due to the fact that HUWRS , upon detecting drift , retrains any “ bad ” base classifiers ( ie , those using the drifting attributes ) and resets its weight to be unit . This , effectively , over weights these “ bad ” classifiers as they still contain drifting attributes . When larger window sizes are used , however , the intra batch weighting mechanism is able to act more effectively over the incoming instances , and thus able to keep the weight of the “ bad ” classifiers down , resulting in a more effective ensemble .
B . Results on the Real World Datasets
When considering the results on the real datasets ( Figure 2 ) , we see two major trends . First , for all datasets except usenet2 , HUWRS outperforms the others in the vast majority of cases . This performance increase , however , is not limited to a single window size . Instead there exist a large number of ( contiguous ) cases for which HUWRS outperforms the others . Secondly , for the majority of datasets , the accuracy of the classifiers tends to decrease as the window size is increased . We begin by addressing the performance of the classifiers on usenet2 .
While for the vast majority of cases we see that HUWRS outperforms the others , usenet2 shows a more convoluted picture . In order to understand the performance of the algorithms on usenet2 , we consider how it was created . As described in Section V B , usenet2 was created by collecting data for three concepts , and every 300 instances selecting a different concept to represent the positive class . Since the concept to represent the positive class reoccurs , DWM is well equipped for the task ( assuming the window size selected is not too large ) . That is , since DWM ( potentially )
5000 10000 15000 20000 25000 30000 35000 40000 45000 50000
5000 10000 15000 20000 25000 30000 35000 40000 45000 50000
Window Size ( instances ) ( c ) RBF 2
HUWRS AHOT DWM
Window Size ( instances ) ( a ) RBF 0.5 98 96 94 92 90 88 86 84 82 80 78 76
( y c a r u c c A
)
%
Window Size ( instances ) ( b ) RBF 1
HUWRS AHOT DWM
)
%
( y c a r u c c A
96 94 92 90 88 86 84 82 80 78 76
76
74
72
70
68
66
64
62
)
%
( y c a r u c c A
HUWRS AHOT DWM
)
%
( y c a r u c c A
60
5000 10000 15000 20000 25000 30000 35000 40000 45000 50000
84 82 80 78 76 74 72 70 68 66 64
HUWRS AHOT DWM
)
%
( y c a r u c c A
94 92 90 88 86 84 82 80 78 76 74
HUWRS AHOT DWM
5000 10000 15000 20000 25000 30000 35000 40000 45000 50000
5000 10000 15000 20000 25000 30000 35000 40000 45000 50000
Figure 1 . Accuracy results for the three classifiers used ( Heuristic Updatable Weighted Random Subspaces , Adaptive Hoeffding Option Tree , and Dynamic Weighted Majority ) on the synthetic datasets
Window Size ( instances ) ( d ) RBF 5
Window Size ( instances ) ( e ) RBF 10 learns a classifier at each time step , and weights each according to its performance on the current concept , it can ( potentially ) reuse previously learned base classifiers when the concept reoccurs . It is therefore not surprising to see DWM perform well on this task with reoccurring concepts . It is worth noting , however , that HUWRS does provide the highest overall accuracy . This shows that even for datasets which DWM is well suited , HUWRS can still provide exceedingly high performance .
The second main trend observed is a decrease in accuracy as the window size increases . This affect is not surprising , as in the email ( Figure 2b ) , usenet1 ( Figure 2d ) , and usenet2 ( Figure 2e ) datasets in particular , there is a sharp concept change after 300 instances . Thus when the window size increases ( especially to 150 instances ) , the methods perform very poorly on the first 150 instances after the concept change ( as there are no class labels available to notice the change ) , explaining the observed drop in performance on each of the datasets . For the spam dataset , such a drop in performance may be due to the fact that , over such the methods are not able to ( relatively ) large windows , capture the drift fast enough , and thereby observe vast drops in performance .
VII . ANALYSIS OF THE INTRA BATCH WEIGHTING
TECHNIQUE
Given the results presented in Section VI A , one obvious question is how effective the intra batch weighting scheme is at detecting and mitigating concept drift . In order to test this , we performed a modification of the experiments of the previous section on the RBF dataset . Instead of each classifier receiving instances as a series of chunks ( ie , chunk test then train ) , each classifier instead only received the labels for the first chunk , with the remaining instance ’s classes withheld for the entirety of the experiment . In this way we tested how effective the ensemble is at dealing with drift without the presence of labels , particularly relative to the other methods .
The results of this experiment are shown in Figure 3 . Given the results , there are two main observations to be made . First , AHOT and DWM perform significantly worse than when they are given labels , with decreases of approximately 10 20 % in accuracy . This result is not surprising , as neither of these methods is able to detect concept drift without labels , and thus are ill suited to task of learning without labels . The second observation stems from the fact that not only does HUWRS maintain a higher performance ( in this case significantly higher than the others ) , its performance actually increases over that of its performance when given labels .
The increase in performance observed in this context in HUWRS is due mainly to the retrain feature ( ie , when a classifier ’s weight drops below a certain performance threshold it is retrained ) . Since in the RBF case drift is continuously occurring in a portion of the features , every time a classifier using one of the drifting features is retrained , it is mistakenly given a high weight under the assumption that the next time step will look similar to the current one . This increased weight results in the predictions made by these classifiers being given a weight which is too high , as the features are still drifting . Since the base classifiers are quickly no longer relevant due to the concept drift , this
247
)
%
( y c a r u c c A
75
70
65
60
55
50
20
Window Size ( instances )
( b ) email data 76
HUWRS AHOT DWM
)
%
( y c a r u c c A
74
72
70
68
66
64
20
HUWRS AHOT DWM
66.5
66
65.5
65
64.5
64
)
%
( y c a r u c c A
63.5
100 200 300 400 500 600 700 800 900 1000
Window Size ( instances ) ( a ) elec2
)
%
( y c a r u c c A
75
70
65
60
55
50
45
40
20
HUWRS AHOT DWM
40
60
80
100
120
140
160
)
%
( y c a r u c c A
90
88
86
84
82
80
78
76
74
72
HUWRS AHOT DWM
0
50 100 150 200 250 300 350 400 450 500
Window Size ( instances ) ( c ) spam data
HUWRS AHOT DWM
40
60
80
100
120
140
160
Window Size ( instances ) ( d ) usenet1
40
60
80
100
120
140
160
Window Size ( instances ) ( e ) usenet2
Figure 2 . Accuracy results for the three classifiers used ( Updtable Weighted Random Subspaces , Adaptive Hoeffding Option Tree , and Dynamic Weighted Majority ) on the real datasets results in decreased performance .
Finally , given that the performance of HUWRS actually increases ( unlike the other two methods ) when labels are omitted , we claim that our intra batch weighting scheme offers a benefit to classification performance .
VIII . RELATED WORK
In order to learn in the presence of concept drift , algorithm designers must deal with two main problems . The first problem is detecting concept drift present in the stream . Once concept drift has been detected , one must then determine how to best adapt to make the most appropriate predictions on the new data .
In the next section we present a variety of state of the art learning algorithms for concept drift ( eg , AHOT and DWM ) . Subsequently , we introduce a variety of popular concept drift detection techniques .
A . State of the Art Techniques
One base learner which has been heavily studied in the is the decision tree ; of which context of concept drift the most common traditional variant is C4.5 [ 31 ] . The original extension of the decision tree learning algorithm , called VFDT , was proposed by Domingos and Hulten [ 8 ] . In VFDT , Hoeffding bounds [ 17 ] , [ 28 ] are used to grow decision trees in streaming data . The authors show that in the case of streaming data , applying Hoeffding bounds to a subset of the data can , with high confidence , choose the same split feature as a method using all of the data . This observation allows for trees to be grown online that are nearly equivalent to those built offline . Since its inception ,
VFDT has been the basis for numerous extensions and improvements [ 19 ] , [ 18 ] , [ 1 ] .
As an extension to standard decision trees , Buntine [ 5 ] introduced option trees , which were further explored by Kohavi and Kunz [ 25 ] . In standard decision trees , there is only one possible path from the root to a leaf node , where predictions are made . In option trees , however , a new type of node — known as an option node — is added to the tree , which splits the path along multiple split nodes . Pfahringer , Holmes , and Kirby combined the concept of Hoeffding trees and option trees to create Hoeffding Option Trees [ 29 ] . They combine these two methods by beginning with a standard Hoeffding tree and , as data arrives , if a new split is found to be better than the current split at a point in the tree , an option node is added and both splits are kept . Further extending Hoeffding Option Trees are Bifet et . al . [ 3 ] . In their extension , called Adaptive Hoeffding Option Trees , each leaf is provided with an exponential weighted moving average estimator , where the decay is fixed at 02 The weight of each leaf is then proportional to the square of the inverse of the error .
While Hoeffding Trees build a single decision tree ( or in the case of Hoeffding Option Trees , a single option tree ) , Dynamic Weighted Majority ( DWM ) [ 26 ] instead creates an ensemble of classifiers . In order to test an instance using DWM , every new instance is classified by the ensemble by using a weighted vote of each of its base classifiers .
In order to train DWM , given a new training instance , the ensemble begins by attempting to classify it . Each base classifier which misclassifies the instance has its weight reduced by a multiplicative constant 𝛽 . If the entire ensemble
248
74
72
70
68
66
64
62
60
58
)
%
( y c a r u c c A
HUWRS AHOT DWM
85
80
75
70
65
60
)
%
( y c a r u c c A
HUWRS AHOT DWM
95
90
85
80
75
70
65
60
)
%
( y c a r u c c A
HUWRS AHOT DWM
56
5000 10000 15000 20000 25000 30000 35000 40000 45000 50000
55
5000 10000 15000 20000 25000 30000 35000 40000 45000 50000
55
5000 10000 15000 20000 25000 30000 35000 40000 45000 50000
Window Size ( instances ) ( a ) RBF 0.5
Window Size ( instances ) ( b ) RBF 1
100
95
90
85
80
75
70
65
60
)
%
( y c a r u c c A
HUWRS AHOT DWM
100
95
90
85
80
75
70
65
60
)
%
( y c a r u c c A
Window Size ( instances ) ( c ) RBF 2
HUWRS AHOT DWM
55
5000 10000 15000 20000 25000 30000 35000 40000 45000 50000
55
5000 10000 15000 20000 25000 30000 35000 40000 45000 50000
Window Size ( instances ) ( d ) RBF 5
Window Size ( instances ) ( e ) RBF 10
Figure 3 . Accuracy results for the three classifiers used ( Heuristic Updatable Weighted Random Subspaces , Adaptive Hoeffding Option Tree , and Dynamic Weighted Majority ) on the synthetic datasets when labels are only available for the first chunk . misclassifies the instance , the ensemble adds a new classifier with a weight of 1 after rescaling the weights of each existing classifier to 1 ( in order to prevent new classifiers from dominating the voting process ) . The ensemble then removes each classifier with a weight lower than some threshold 𝜃 , and then provides the instance to each of the base classifiers for updating .
B . Drift Detection Techniques
One popular technique for drift detection is due to Klinkenberg and Joachims [ 24 ] who proposed a method based on Support Vector Machines ( SVMs ) . Specifically , they proposed the use of 𝜉𝛼 estimator to compute a bound on the error rate of the SVM . Specifically , assuming 𝑡 they use the 𝜉𝛼 estimator to compute 𝑡 error batches , bounds . The first error bound corresponds to learning the classifier on just the newest batch , the second bound corresponds to learning on the newest 2 batches , etc . They then choose the window size which has the minimum estimated error .
Gama et . al . [ 10 ] proposed a method based on the error rate over time of the learning algorithm . To accomplish this , they assume that each new instance represents a random Bernoulli trial . Based on this , they then compute the probability of observing a “ false ” for instance 𝑖 ( 𝑝𝑖 ) , and the 𝑝𝑖(1 − 𝑝𝑖)/𝑖 ) . They then argue standard deviation ( 𝑠𝑖 = that a significant increase in the error rate denotes a concept drift . With this in mind , they state that their algorithm issues a warning if 𝑝𝑖 + 𝑠𝑖 ≥ 𝑝𝑚𝑖𝑛 + 2𝑠𝑚𝑖𝑛 , and drift is detected if 𝑝𝑖 + 𝑠𝑖 ≥ 𝑝𝑚𝑖𝑛 + 3𝑠𝑚𝑖𝑛 .
√
IX . DISCUSSION AND FUTURE WORK
We introduced Heuristic Updatable Weighted Random Subspaces ( HUWRS ) as a method for dealing with concept drift in data streams . In this method an ensemble of classifiers is built , such that each classifier is learned on a different subspace of the features . When ( sufficient ) drift is detected in a feature , only those classifiers learned on that feature are retrained , thereby enabling the classifier to perform less retraining . Additionally , we demonstrate how the algorithm is able to cope even in scenarios where labels are rare , updating the weights of each classifier to reflect the current , perceived , drift in each of the features .
In order to prove the effectiveness of HUWRS , we compared it to two other state of the art methods , namely Adaptive Hoeffding Option Trees , and Dynamic Weighted Majority . For a wide variety of datasets and window sizes , we found that HUWRS outperformed these other methods . Additionally , we tested the case where each of the methods was only given one chunk with labels . In this non standard scenario , HUWRS was able to demonstrate exceedingly high performance in the face of concept drift .
One of the obvious shortcomings of HUWRS when compared to the other methods is the requirement that instances arrive in batches . This shortcoming is due first to our choice of a non incremental base learner ( C4.4 ) , and secondly to our choice of drift detection technique ( ie , a simple , nonsliding windowed technique ) . In future work we seek to remedy this failing , making HUWRS more applicable to a wider variety of problem . Due to the wide range of window sizes for which HUWRS outperformed the other state of the art methods , we believe this is an attainable goal .
249
Given all of these considerations , in scenarios where data arrives in batches and is subject to either gradual concept drift or sudden concept change , we recommend using Heuristic Updatable Weighted Random Subspaces .
ACKNOWLEDGEMENTS
[ 18 ] S . Hoeglinger and R . Pears . Use of hoeffding trees in concept In ICIAFS , pages 57 –62 , Dec based data stream mining . 2007 .
[ 19 ] G . Hulten , L . Spencer , and P . Domingos . Mining timechanging data streams . In KDD , pages 97–106 . ACM , 2001 .
Work is supported in part by the NSF Grant ECCS0926170 , NSF Grant ECCS 092159 , and the Notebaert Premier Fellowship .
[ 20 ] T . Kailath . The divergence and bhattacharyya distance measures in signal selection . IEEE Transactions on Communication Technology , 15(1):52–60 , 1967 .
REFERENCES
[ 1 ] A . Bifet and R . Gavalda . Adaptive learning from evolving data streams . Advances in Intelligent Data Analysis VIII , pages 249–260 , 2009 .
[ 2 ] A . Bifet , G . Holmes , R . Kirkby , and B . Pfahringer . Moa :
Massive online analysis . JMLR , 11:1601–1604 , 2010 .
[ 3 ] A . Bifet , G . Holmes , B . Pfahringer , R . Kirkby , and R . Gavald`a . New ensemble methods for evolving data streams . In KDD , pages 139–148 . ACM , 2009 .
[ 4 ] L . Breiman .
Bagging predictors . Machine Learning ,
24(2):123–140 , 1996 .
[ 5 ] W . Buntine . Learning classification trees .
Computing , 2(2):63–73 , 1992 .
Statistics and
[ 6 ] D . Cieslak and N . Chawla . A framework for monitoring classifiers’ performance : when and why failure occurs ? KAIS , 18(1):83–108 , 2009 .
[ 7 ] T . Dietterich . Ensemble methods in machine learning . MCS , pages 1–15 , 2000 .
[ 8 ] P . Domingos and G . Hulten . Mining high speed data streams .
In KDD , pages 71–80 . ACM , 2000 .
[ 9 ] Y . Freund and R . Schapire . A decision theoretic generalization of on line learning and an application to boosting . COLT , pages 23–37 , 1995 .
[ 10 ] J . Gama , P . Medas , G . Castillo , and P . Rodrigues . Learning with drift detection . SBIA , pages 66–112 , 2004 .
[ 21 ] I . Katakis , G . Tsoumakas , and I . Vlahavas . Dynamic feature space and incremental feature selection for the classification of textual data streams . Knowledge Discovery from Data Streams , pages 107–116 , 2006 .
[ 22 ] I . Katakis , G . Tsoumakas , and I . Vlahavas . An Ensemble of Classifiers for coping with Recurring Contexts in Data Streams . In ECAI , pages 377–382 , Patras , Greece , 2008 .
[ 23 ] I . Katakis , G . Tsoumakas , and I . Vlahavas . Tracking recurring contexts using ensemble classifiers : an application to email filtering . KAIS , 22(3):371–391 , 2010 .
[ 24 ] R . Klinkenberg and T . Joachims . Detecting concept drift with support vector machines . In ICML . Citeseer , 2000 .
[ 25 ] R . Kohavi and C . Kunz . Option decision trees with majority votes . In ICML , pages 161–169 , 1997 .
[ 26 ] J . Kolter and M . Maloof . Dynamic weighted majority : A new ensemble method for tracking concept drift . In ICDM , pages 123–130 . IEEE , 2003 .
[ 27 ] R . Lichtenwalter and N . V . Chawla . Adaptive methods for classification in arbitrarily imbalanced and drifting data streams . In New Frontiers in Applied Data Mining , volume 5669 of Lecture Notes in Computer Science , pages 53–75 . Springer Berlin / Heidelberg , 2010 .
[ 28 ] A . M . Oded Maron . Hoeffding races : Accelerating model selection search for classification and function approximation . In NIPS , volume 6 , pages 59–66 , April 1994 .
[ 11 ] S . Grossberg . Nonlinear neural networks : Principles , mechanisms , and architectures . Neural Networks , 1(1):17–61 , 1988 .
[ 29 ] B . Pfahringer , G . Holmes , and R . Kirkby . New options for hoeffding trees . AAI , pages 90–99 , 2007 .
[ 12 ] M . Hall , E . Frank , G . Holmes , B . Pfahringer , P . Reutemann , and I . H . Witten . The weka data mining software : an update . SIGKDD Explorations Newsletter , 11(1):10–18 , 2009 .
[ 13 ] P . Halmos . Measure theory . Van Nostrand and Co . , 1950 .
[ 14 ] M . Harries . pricing . 1999 .
Splice 2 comparative evaluation : Electricity
[ 15 ] E . Hellinger . Neue begr¨undung der theorie quadratischer formen von unendlichvielen ver¨anderlichen ( german ) . Journal fur die Reine und Angewandte Mathematik , 136:210–271 , 1909 .
[ 16 ] T . Ho . The random subspace method for constructing decision forests . PAMI , 20(8):832–844 , 1998 .
[ 17 ] W . Hoeffding . Probability inequalities for sums of bounded random variables . JASA , 58(301):13–30 , 1963 .
[ 30 ] F . Provost and P . Domingos . Tree induction for probabilitybased ranking . Machine Learning , 52(3):199–215 , September 2003 .
[ 31 ] J . Quinlan . C4.5 : programs for machine learning . 1993 .
[ 32 ] C . Rao . A review of canonical coordinates and an alternative to correspondence analysis using hellinger distance . Questiio ( Quaderns d’Estadistica i Investigacio Operativa ) , 19:23–63 , 1995 .
[ 33 ] A . Tsymbal . The problem of concept drift : definitions and related work . Informe t´ecnico : TCD CS 2004 15 , Departament of Computer Science Trinity College , Dublin , https://www . cs . tcd . ie/publications/techreports/reports , 4:2004–15 , 2004 .
250
