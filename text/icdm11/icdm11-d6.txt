Patent Maintenance Recommendation with Patent Information Network Model
Xin Jin1 , Scott Spangler2 , Ying Chen2 , Keke Cai3 , Rui Ma3 , Li Zhang3 , Xian Wu3 and Jiawei Han1
1Department of Computer Science , University of Illinois at Urbana Champaign , IL , USA
2IBM Almaden Research Center , San Jose , CA , USA
3IBM China Research Lab , Beijing , PRC xinjin3@illinois.edu spangles@almadenibmcom yingchen@usibmcom
{caikeke,maruicrl,lizhang,wuxian}@cnibmcom hanj@illinois.edu
Abstract—Patents are of crucial importance for businesses , because they provide legal protection for the invented techniques , processes or products . A patent can be held for up to 20 years . However , large maintenance fees need to be paid to keep it enforceable . If the patent is deemed not valuable , the owner may decide to abandon it by stopping paying the maintenance fees to reduce the cost . For large companies or organizations , making such decisions is difficult because too many patents need to be investigated . In this paper , we introduce the new patent mining problem of automatic patent maintenance prediction , and propose a systematic solution to analyze patents for recommending patent maintenance decision . We model the patents as a heterogeneous time evolving information network and propose new patent features to build model for a ranked prediction on whether to maintain or abandon a patent . In addition , a network based refinement approach is proposed to further improve the performance . We have conducted experiments on the large scale United States Patent and Trademark Office ( USPTO ) database which contains over four million granted patents . The results show that our technique can achieve high performance .
Keywords patent mining ; ranking ; prediction ; patent main tenance ; patent information network
I . INTRODUCTION
Intelligent management of patents [ 1 ] is very important , because patents form one crucial component of the engine that powers the economy . They are of utmost importance for businesses , because they provide legal protection for the invented techniques , processes or products .
A patent gives a set of exclusive rights to the inventor or the assignee ( company or organization ) for up to twenty years . However , during the twenty years , maintenance fees or renewal fees need to be paid to keep the granted patent valid ; otherwise , the patent will expire early . In the United States , a utility patent filed on or after 12/12/1980 must be renewed 3.5 ( E1 ) , 7.5 ( E2 ) and 11.5 ( E3 ) years after grant of the patent . In the United Kingdom , maintenance fees are due on the fourth anniversary of the filing date and every year after that . In China , maintenance fees have to be paid every year . For a patent filed in Canada , the maintenance fees have to be paid every year subsequent to the second anniversary of the application date .
For a big company or organization which owns a large portfolio of patents , the cost is too high to maintain all patents . If the assignee/owner considers a patent to be not valuable or profitable anymore , the maintenance fee will not be paid in order to save money . Because of this , many patents are abandoned before the full twenty year term . Figure 1 shows that of the US granted patents and earlyexpired patents over 33 % were abandoned .
Figure 1 . Granted and not maintained for all US patents . X axis presents the year , and Y axis represents the number of patents in that year . The blue curve denotes the total granted patents at each year . The red curve denotes the patents filed in that year but abandoned later in the 3.5th ( E1 ) year , 7.5th ( E2 ) year or 11.5th ( E3 ) year . The green curve denotes the patents that were abandoned in that year .
Most patents are owned by companies ( such as IBM , HP , General Moters and Kodak ) and organizations ( such as US Department of Energy , NASA , Navy and Army ) . Different owners have different strategies for patent maintenance . Maintenance decisions can be based on the following factors :
• Quality of the writing of the patent . If a patent is not well written , it has higher chance to be rejected by the patent office or ruled as invalid in court .
• Novelty of the patent . A patent should invent novel idea or technique . If a granted patent is found to not novel , the owner should abandon it to avoid possible lawsuit . One way to estimate the novelty of a patent is to analyze the patent text and calculate the age of the n gram terms [ 2 ] . • Technique value . the invented technique of a
If patent becomes out of date and another technique has emerged as a better solution , then there is no need to maintain the patent .
• Technique relevancy . A company/organization tends to keep patents that are related to its current core interests . If a company shifts its interest or core technique to a new one , the patents related to the old one will probably be not maintained .
• Economic value . If a patent is making profit for the company , then the company will likely maintain it . In many cases , only the company itself knows such confidential information on how much profit a patent is making , so we leave this factor to the company to consider .
Patent maintenance decision is traditionally made manually . However , for large companies or organizations , making a decision on which part of the patent portfolio to maintain becomes a difficult task because there are so many patents that need to be investigated . A good decision should achieve a balance between lowering the cost of paying maintenance fees for low valued patents and reducing the risk of abandoning high valued patents which should be maintained .
In this paper , we propose a system and an automatic method to analyze patents for patent maintenance decision prediction . The prediction model can be used to make recommendation for patent portfolio managers to help decide on patent maintenance .
II . RELATED WORK
Existing web services ( such as Google Patent Search , MicroPatent and Delphion ) and research work on patent analysis and management have been focusing on automatic patent classification [ 3 ] [ 4][5 ] , prior art search [ 6 ] [ 7 ] , patent value analysis [ 8 ] [ 9 ] , patent quality prediction [ 10 ] [ 11 ] [ 12 ] [ 13 ] , novelty detection [ 2 ] [ 14 ] or traditional link based searching using [ 15 ] or HITS [ 16 ] .
Due to the growing number of patent applications every year , patent classification [ 3 ] [ 4][5 ] based on patent content analysis has been proposed to help assign patent applications to the right examiners and classify patents to patent classification schemes , such as the United States Patent Classification ( USPC ) system and the International Patent Classification ( IPC ) system . However , for patent maintenance decision , the patents are already published and the class information is already known .
Research on patent quality prediction [ 10 ] [ 11 ] [ 12 ] [ 13 ] estimates patent quality from the perspective of court validity rulings or the number of forward citations . Various patent content and citation features are designed or adopted to build the prediction model for patent quality . A high quality patent tends to be maintained in the future . However , such patent could still be abandoned for various reasons , eg , the invention becomes an out dated technique , the company has shifted to new and better techniques .
To the best of our knowledge , our work is the first to study the problem of automatic patent maintenance decision recommendation . We propose a systematic solution which achieves high performance .
III . SYSTEM FRAMEWORK
We present a framework system to analyze patents for maintenance decision recommendation . The goal is to automatically extract patent properties from various aspects to help the company make a decision on maintenance , based on a model built from patent features and historical maintenance decisions . Note that this is a service that current patent search , analysis or management systems , such as Google Patent Search1 , Delphion2 , Derwent Innovations Index by Thomson Reuters3 , ESPACENET by EPO4 , USPTO5 , PatentCafe6 , PatentStorm7 and PeerToPatent8 , do not support .
Figure 2 . System framework of analyzing patents for maintenance decision recommendation .
As shown in Figure 2 , the general system framework works as follows : ( 1 ) beginning with a collection of patent databases , such as US Granted , US Applications , EP Granted & Applications and WO Applications ; ( 2 ) ( optional ) perform a query to select a subset of patents that are particularly interesting to the user , such as company name , patent class or any topic keyword ; ( 3 ) extract patent features ; ( 4 ) obtain a training data with historical maintenance decisions available and build prediction model with network based refinement ; and ( 5 ) for testing or to be decided patents , compute a
1Google Patent Search . http://wwwgooglecom/patents 2Delphion . http://wwwdelphioncom 3Derwent . http://thomsonreuters.com 4ESPACENET . http://wwwespacenetcom 5USPTO . http://wwwusptogov 6PatentCafe . http://wwwpatentcafecom 7PatentStorm . http://wwwpatentstormus 8PeerToPatent . http://wwwpeertopatentorg ranking score based on the model and make recommendation for maintenance decision .
Figure 3 shows a screenshot of the web interface for the patent maintenance decision aid system .
Table I
META FEATURES
Feature Pubyear Waityear
NInv NAss
NClaim NUSclass
NIPC Kind NFC NBC NOC NTC
Description
Publication year
Pubyear minus filed year
Number of Inventors Number of Assignees
Number of Claims
Number of US classes Number of IPC codes
US patent kind
Number of Forward Citations Number of Backward Citations
Number of Other Citations Number of Total Citations
B . Novelty Features
Figure 3 . Patent maintenance decision aid system screenshot .
IV . PATENT INFORMATION NETWORK AND FEATURES
As shown in Figure 4 , we model the patents as a heterogeneous time evolving information network G = hV , E , W , T i , with different types of nodes V , such as patents , inventors , assignees and USPC/IPC classes . E indicates the links between nodes with weighting W , and T is the time information . Patents are directly linked together by citations ( solid lines ) and content similarity ( dashed lines ) . The network evolves with new patents published and some old patents abandoned . We propose the following patent features .
A novel patent tends to use more new or recent words/phrases [ 17 ] . Such patents also have higher chance to be ruled as valid in court [ 10 ] . We extract ( top k ) key n grams and the age of each n gram , the finial novelty features are computed as statistic summarization of the ages . The related studies include new topic detection algorithms [ 18][19 ] .
C . Writing Quality Features
We extract content writing features based on the text information of the patent to estimate two kinds of patent writing quality : ( 1 ) Complexity , a complex patent tends to use more words in order to describe the various aspects of the invention . So we extract the number of words and the number of unique words in the claim , title , abstract and description , respectively . ( 2 ) Completion , based on US patent regulation , the patent claims need to be validated by complete description [ 10 ] . A patent should not claim something that is not adequately described in the detailed description section . So we compute the pair wise text similarities between abstract , claims and description as features to indicate the writing completion property . To calculate the similarity , we first preprocess the text by stopword removal , synonymy merging , stemming [ 20 ] and noise removal . Then we convert the text to vector space model ( VSM ) [ 21 ] with tf ∗ idf style term weighting ( such as PN [ 22 ] and BM25 [ 23 ] ) to compute the relevance . Note that statistic language model ( SLM ) [ 24 ] with Kullback Leibler ( KL ) divergence or Jensen Shannon divergence is also applicable .
Figure 4 . Patent Information Network .
D . Trend Features
A . Meta Features
Each patent contains meta information , such as publication year , filed year , kind , inventor , assignee , US class , IPC code and cited patents . In order to capture the basic information of the patent , we extract meta features from the meta information . Table I lists 12 meta features .
Based on the time evolving property of the patent information network , we extract the trend of a technique , indicated as a keyword or USPC/IPC class . If a technique is popular , it would be used or mentioned in many related patents . Denote a trend as T = hf1 , f2 , , fni(fi ∈ Z ) , where f requency fi is the number of related patents published in year i . T is a time ordered non negative integer series .
Table II
WRITING QUALITY FEATURES
Feature TitleL
TitleUW ClaimL
Description Title Length Title Unique Words number Claim Length
ClaimUW Claim Unique Words number FClaimL
First Claim Length
FClaimUW First Claim Unique Words number
AbsL
AbsUW
DesL
DesUW CohAC CohAD CohCD
Abstract Length Abstract Unique Words Description Length Description Unique Words Cohesion between Abstract and Claims Cohesion between Abstract and Description Cohesion between Claims and Description
As an example , Figure 5 shows the trend for US class 711169 , which is 711 ( Electrical Computers and Digital Processing Systems : Memory ) with subclass 169 ( Memory Access Pipelining ) .
Figure 5 . Trend features of a patent published in 2007 for USPC class 711169 . X axis denotes the publish year , and Y axis denotes the number of patents published in each corresponding year .
To mark the beginning of a trend , we define start(T ) as the first year with a non zero frequency , start(T ) = min{i|fi > 0}
( 1 )
1 ) Basic Trend Features : Consider a time point q and a trend T , we define the following 7 basic trend features . We consider the time point in the year level , which could the year a patent was published/filed or the year a maintenance decision was made .
Given a patent , we obtain a time point q as its publication year ( or the maintenance decision year , based on the feature ) , and we define the following basic trend features based on both q and the trend T .
( 1 ) age . The age of a time point q in a trend T is defined as the time gap from q to the start of the trend . age(q , T ) = q − start(T )
( 5 )
( 2 ) age2max . The age2max of a time point q in a trend T is defined as the time gap from q to the maxyear of the trend . age2max(q , T ) = q − maxyear(T )
( 6 )
We use age2max to indicate the relation between q and the trend ’s peak . If age2max > 0 , p happens after the peak ; otherwise , p locates in the increasing stage of the trend .
( 3 ) thisSup . The thisSup feature indicates the number of patents published in time q . It captures the popularity of the trend in that particular time point . thisSup(q , T ) = fq
( 7 )
( 4 ) maxSup . The maxSup feature captures the maximum popularity of the trend T , which is the same as max(T ) . maxSup(q , T ) = max(T )
( 8 )
( 5 ) sumSup . The sumSup feature captures the overall popularity of the trend T , which is the same as sum(T ) . sumSup(q , T ) = sum(T )
( 9 )
We use thisSup , maxSup and sumSup to indicate the popularity of the technique from different perspectives .
( 6 ) thisMaxRatio . The thisM axRatio feature is defined as the ratio of thisSup to maxSup . If the value is close to 1 , then the popularity is close to the peak , which means it is still in the popular stage of the trend .
To indicate the peak of a trend , we define max(T ) as the thisM axRatio(q , T ) = fq/max(T )
( 10 ) maximum frequency . max(T ) = max{fi}
( 2 )
We define maxyear(T ) as the year which achieves max(T ) . If there are multiple choices , we choose the earliest one . maxyear(T ) = min{i|fi = max(T )}
( 3 )
In order to capture the overall volume , we define sum(T ) as the sum of all frequencies over the whole time range . sum(T ) = n
X i=1 fi
( 4 )
( 7 ) thisAllRatio . The thisAllRatio feature is defined as the ratio of thisSup to sumSup . It indicates the degree of importance for the popularity at time point p compared with the overall popularity . thisAllRatio(q , T ) = fq/sum(T )
( 11 )
Example . Figure 5 shows several basic trend features . Suppose we consider a time point q = 2007 , then the values for those basic trend features are : age = a = 2007−1976 = 31 , age2max = b = 2007 − 2003 = 4 , thisSup = d = 45 , maxSup = c = 74 , sumSup = 774 , thisM axRatio = d/c = 45/74 and thisAllRatio = d/sumSup = 45/774 .
2 ) Collective Trend Features : Given a patent p and a technique ci , we can obtain a set of basic trend features based on T ( ci ) , the trend of ci . However , a patent usually belongs to several techniques . We define collective features to consider the collective effect of all those techniques . Given a set C of m techniques : C = {c1 , c2 , , cm} , the collective features for age are defined as ,
M Aage(p ) = max{age(year(p ) , T ( ci))|ci ∈ C}
( 12 )
M Iage(p ) = min{age(year(p ) , T ( ci))|ci ∈ C}
( 13 )
SU age(p ) = m
X i=1 age(year(p ) , T ( ci ) )
( 14 ) lower ECount does not necessarily mean higher quality . So we use EP ratio(p , vi ) = ECount(p , vi)/P Count(p , vi ) to show the percentage of patents that were not maintained . A high quality inventor tends to have low EP ratio score .
First Inventor Features . In some cases , a patent ’s first inventor makes more contribution to the patent than the coinventors . So the quality of the first inventor could be a good indicator of the quality of the patent . We use iv1PCount , iv1ECount and iv1EPratio to represent the features of the first author .
Collective Inventor Features . When a patent has multiple inventors , we extract collective inventor features , by computing the max , min , sum , mean values of P Count , ECount and EP ratio among the inventors .
M Eage(p ) = m
X i=1 age(year(p ) , T ( ci))/m
( 15 )
F . Assignee Profile Features
Considering the time evolving property , we choose the value of year(p ) for different cases : ( 1 ) the year the patent p was published and ( 2 ) the decision making year . The collective features for other basic trend features can be generated similarly .
Based on the patent sources , we extract two sets of collective trend features :
• General set . In the general set , the trend series T is generated from all related patents in the database . It can reflect the overall trend of the technique .
• Assignee oriented set . In the assignee oriented set , T is formed by only considering the patents of a target company/organization . It is possible that a company ’s trend on a topic is decreasing even when the general trend does not , which may indicates that the company has reduced its efforts on this technique and does not want to maintain related patents .
E . Inventor Features
The quality of a patent is closely related to the quality of its inventors . If an inventor ’s previous patents are all maintained , then it is more likely that his/her recent patent will also be maintained . Consider a patent p and its inventors V = {v1 , , vm} , where m is the number of inventors of p . We extract three basic inventor features to capture the quality of an inventor , estimated by the maintenance history of the inventor .
( 1 ) PCount , Denote P Count(p , vi ) as the number of patents of inventor vi which were published before patent p . An inventor with many patents has a higher possibility to be a valuable inventor .
( 2 ) ECount . Denote ECount(p , vi ) as the number of early expired patents of inventor vi which were published before patent p . A good inventor tends to have a lower number of early expired patents .
( 3 ) EPratio . A senior inventor may publish more patents than a young inventor . In such case , a higher P Count or
We compute a patent ’s content similarity to the assignee ’s collection of patents to see how closely the patent is related to the assignee profile .
Denote Pc as the set of patents with c as its assignee ( or one of its assignees ) , and P y c ⊆ Pc as the patents which were published in year y . Given a window size w ∈ Z , we define an assignee profile AP ( c , y , w ) ( AP for short ) as the collection of patents by assignee c which were published around year y within time window w .
AP ( c , y , w ) = {p|p ∈ Pc , y − w ≤ year(p ) ≤ y}
( 16 )
We define feature P AP ( p , c , w ) sum ( PAPsum for short ) to capture the overall similarity between patent p and the assignee ’s profile ,
P AP ( p , c , w ) sum = X
Sim(p , pi )
( 17 ) pi∈AP p∧pi6=p where AP p = AP ( c , year(p ) , w ) and Sim(p , pi ) is the text similarity between patent p and pi .
To reduce the effect of profile size , we also use the mean feature P AP ( p , c , w ) mean ( PAPmean for short ) ,
P AP ( p , c , w ) mean =
1
|AP p| X pi∈AP p∧pi6=p
Sim(p , pi )
( 18 ) In order to measure maximum relevance to patents in the profile , we define the max feature P AP ( p , c , w ) max ( PAPmax for short ) as the similarity between p and the most similar patent in the profile ,
P AP ( p , c , w ) max = max{Sim(p , pi)|pi ∈ AP p} ( 19 )
P AP max may be impacted by outliers . It is possible that the assignee profile has a patent that is very similar to the query patent but all others are totally unrelated . So we define AP K p ⊆ AP p as the top k most similar patents to p , and extract feature P AP K(p , c , w ) sum ( PAPKsum for short ) to capture the overall similarity between patent p and AP K p ,
P AP K(p , c , w ) sum = X
Sim(p , pi )
( 20 ) pi∈AP K p∧pi6=p
To reduce the effect of profile size on PAPKsum , we extract the mean feature P AP K(p , c , w ) mean ( PAPKmean for short ) by making PAPKsum divided by the size ,
P AP K(p , c , w ) mean =
1
|AP K p| X pi∈AP K p∧pi6=p
Sim(p , pi )
( 21 ) The profile features for each patent p are dynamic features . We generate different sets based on the values of y as the patent publish year and the decision making years .
V . PREDICTION MODEL
A . Initial Prediction
For each granted US patent , there will be at most three maintenance decisions . For each such decision year , we extract the time sensitive features , such as trend and profile features . So in general , a patent could generate at most three training instances , each with a binary label maintain or abandon . Then based on the historical patent maintenance decision instances , we build a decision tree classifier ( other classifiers , such as RIPPER and SVM , can also be used ) to learn a classification model . For any future decision year , we can predict maintenance decisions for the patents based on the classifier .
B . Network based Refinement
We propose an optimization framework based on the patent information network to refine the prediction scores initially obtained from learning from the patent features . The basic goal is to utilize the network structure to reveal the relationship between patents , which can help improve the prediction performance by analyzing the patents collectively instead of independently .
In a patent information network , V is the set of inventors V A and vertices/nodes ( including the patents , USPC/IPC classification codes V C ) , and E is the set of different types of edges between the nodes . EL is the set of edges formed by patent citations , ES is the set of edges modeling the similarity between patents , EA is the set of edges between patent and inventor , EC is the set of edges between patent and USPC/IPC class . wij is the patent text similarity score .
We denote V T as the set of training patent decision points , and V P is the set of testing patent decision points . The value of such point ri is 1 if the decision is to maintain , and 0 if the patent is abandoned . An edge ( i , j ) denotes the edge between the corresponding two patents .
Figure 6 shows an example . The solid blue points are patents published in that particular year . At different time snapshots , the same location means the same patent . The light blue point means the patent was maintained , while the light red point indicates that the patent was abandoned . The blue links are the citation links between patents . The dashed edge means the two patents are content similar . Three patents P 1 , P 2 and P 3 were published in 1997 . In their first maintenance decision year 2001 , they form virtual decision points . P 1 and P 3 were maintained , but P 2 was abandoned . In the second maintenance decision year 2005 , P 3 was also abandoned . P 1 and P 3 share the same inventor and are content related .
Figure 6 . Time evolving heterogeneous patent information network .
C . Optimization Framework
We refine the prediction scores R = {ri}(i = 1 , , |V | ) based on the heterogeneous patent network by minimizing the following objective function ,
O(R ) = α X i∈V T +γL X
||ri − r† i ||2 + β X i∈V P
||ri − r‡ i ||2
||ri − rj||2 + γS X wij ||ri − rj ||2 hi,ji∈EL hi,ji∈ES
+δA X
ΨA ai||ri − ra||2 + δC X
ΨC ci||ri − rc||2
( i,a)∈EA
( i,c)∈EC
( 22 )
ΨA ai is the importance of inventor a to patent i . It is defined as a function of whether this inventor is the patent ’s first inventor , and the maintenance percentage of the inventor ’s history .
ΨC ci is the importance of USPC/IPC class c to patent i . We define this as the maintenance purity of the patents in the class . The idea is that if most patents assigned with the class is maintained , then the possibility is high for other related patents to be maintained . r† i ∈ {0 , 1} is the decision for each training patent point i ∈ V T , and r‡ i ∈ [ 0 , 1 ] is the initial prediction score for each testing patent . The value of ri serves as a confidence score .
Letting ∂O(R )
∂p = 0 , we obtain the updating formula for rp as rp =
βr‡ p + γL P
( j,p)∈EL rj + γS P
( j,p)∈ES wpj rj
Zp
δA P
( a,p)∈EA
+
ΨA apra + δC P
( c,p)∈EA
Zp
ΨC cprc
( 27 ) where Zp is the normalization factor ,
Zp = β + γLN I p + γS X wpj
+δA X
( j,p)∈ES ap + δC X
ΨA
( p,a)∈EA
( p,c)∈EC
ΨC cp
( 28 )
For an inventor a ∈ VA ,
∂O(R )
∂a
= −2δA X
ΨA at(rt − ra )
( t,a)∈EA
−2δA X
ΨA ap(rp − ra )
( 29 )
( p,a)∈EA
Letting ∂O(R )
∂a = 0 , we obtain the updating formula for ra as
If ri = 0.5 , it means we are not confident with whether to maintain or abandon the patent . If the value is close to 1 or 0 , we are more confident to make the judgment . We use the network structure to smooth the confidence scores over the nodes .
In the objective function , the first and second components mean that the refined scores should not deviate too much from the training and initial prediction scores .
The 3rd and 4th components mean if two patents are linked together or content similarity to each other , they may tend to have similar maintenance decision .
The 5th and 6th components mean patents may have similar decision as the general decision trend of its belonging inventor or class that has high maintenance purity ( either mostly maintained or mostly abandoned ) .
Our goal is to find R = R∗ to minimize the cost function , R∗ = argmin{O(R)} . To minimize O(R ) , we compute its first order partial derivatives ,
For a training instance t ∈ V T ,
∂O(R )
∂t
= 2α(rt − r† t ) ( rt − rj ) + 2γS X
+2γL X wtj(rt − rj )
( t,j)∈EL
( t,j)∈ES
+2δA X
ΨA at(rt − ra ) + 2δC X
ΨC ct(rt − rc )
( t,a)∈EA
( t,c)∈EC
Letting ∂O(R )
∂t = 0 , we obtain the updating formula for rt as rt =
αr† t + γL P
( j,t)∈EL rj + γS P
( j,t)∈ES wtjrj
Zt
δA P
( a,t)∈EA
+
ΨA atra + δC P
( c,t)∈EA
Zt
ΨC ctrc
( 24 ) where Zt is the normalization factor ,
Zt = α + γLN I t + γS X wtj
+δA X
( j,t)∈ES at + δC X
ΨA
( t,a)∈EA
( t,c)∈EC
ΨC ct
( 25 ) is the number of inlinks for patent t . where N I t
For a testing instance p ∈ V P ,
∂O(R )
∂p
= 2β(rp − r‡ p )
( 23 ) ra =
P
ΨA atrt + P
( p,a)∈EA
( t,a)∈EA P
( t,a)∈EA
ΨA at + P
( p,a)∈EA
ΨA aprp
ΨA ap
( 30 )
Similarly , we can obtain the updating formula of rc for each class c ∈ VC .
The final score R∗ is calculated by iteratively updating all t ∈ V T , p ∈ V P , a ∈ VA , c ∈ VC via the corresponding equations . In our experiments , we set the parameters as α = 10 , β = 5 , and γL = γS = δA = δC = 1 .
VI . COMPLEXITY ANALYSIS
The proposed framework consists of three major computation components : feature extraction , initial prediction and network based refinement . Most features can be computed efficiently , the computational complexity of the writing quality features is O(n ) , where n is the number of patents , the complexity of profile features is O(nm ) , where m is the profile size . The complexity of the initial prediction depends on the chosen classification algorithm . The complexity of the proposed refinement approach is O(I|E| ) , where I is the number of iterations , and |E| is the number of edges in the network .
+2γL X
( rp − rj ) + 2γS X wpj ( rp − rj )
VII . EXPERIMENTS
( p,j)∈EL
( p,j)∈ES
A . Dataset
+2δA X
ΨA at(rp − ra ) + 2δC X
ΨC cp(rp − rc )
( p,a)∈EA
( p,c)∈EC
( 26 )
We conduct experiments on the large scale United States Patent and Trademark Office ( USPTO ) database which contains over 4 millions granted patents . Those patents are used to compute the global trend features for the techniques . We investigate major companies ( such as BASF , Boeing , Kodak , General Motor and General Electric ) and organizations ( such as US Department of Energy , NASA , Navy and Army ) with large patent portfolios from a variety of fields . Table III lists the patent maintenance information of each company or organization . PN is the number of US granted patents , MN is the number of maintained patents , and MPratio = MN/PN . We use id instead of the original name to represent a company/organization .
The historical maintenance decisions are used as the ground truth . For each test year , we use the patents that require maintenance decision as testing , and all previous patent decisions as training . We use the recent past six years for testing and report the averaged result . Experiments are performed for each company/organization .
PATENT MAINTENANCE INFORMATION OF COMPANIES/ORGANIZATIONS .
Table III
NameId
N1 N2 N3 N4 N5 N6 N7 N8 N9 N10 N11 N12 N13 N14 N15 N16 N17 N18 N19 N20 N21 N22 N23 N24
PN 2935 4524 10569 4410 2180 1864 1412 3863 6264 2866 8011 520 2679 16369 2920 2908 1507 6179 2689 7605 2763 4991 2850 2768
MN MPratio 774 1661 5415 2951 1123 835 590 1123 2687 1400 3974 280 1233 8989 1622 1629 695 3440 810 2614 1386 3194 1662 988
0.264 0.367 0.512 0.669 0.515 0.448 0.418 0.291 0.429 0.488 0.496 0.538 0.460 0.549 0.555 0.560 0.461 0.557 0.301 0.344 0.502 0.640 0.583 0.357
B . Importance of Features
We use Probabilistic Significance score , proposed by Amir and Lipika [ 25 ] , to measure the importance of the introduced features and report the top features in table IV . Note that other measures can also be used , such as information gain , gainratio , p value of statistic test and chi square .
C . Accuracy Comparison
We compare our approach with two baseline algorithms : Probability Guess ( PG ) and TextC . PG uses the maintenance probability of the training data to guess the label of the testing data . This is the bottom line , any learning approach should achieve better accuracy than PG to be effective . TextC
PROBABILISTIC SIGNIFICANCE OF TOP FEATURES .
Table IV
Type
Inventor
Trend
Profile
Writing
Meta
Top Features ivEPratio mean iv1EPratio max iv1ECount max thisSup min age min thisMaxRatio min age2max min
PAPsum
PAPKmean
CohAD AbsL
ClaimUW
CohCD
Kind
NClaim
Probabilistic Significance
0.490 0.426 0.349 0.319 0.177 0.136 0.125 0.166 0.138 0.175 0.123 0.122 0.121 0.151 0.078 treats the patents as text documents represented in the term vector space , and builds a text classifier ( we use the kNearest Neighbor in the experiment ) .
Table V reports the accuracy of the algorithms for each company or organization . Gain1 is the accuracy gain of our approach over baseline 1 , and Gain2 is the accuracy gain over baseline 2 . Our approach achieves significantly higher accuracy than the baselines , because the proposed features can capture the characteristics that are important for patent maintenance decision making .
ACCURACY FOR EACH COMPANY/ORGANIZATION .
Table V
NameId
N1 N2 N3 N4 N5 N6 N7 N8 N9 N10 N11 N12 N13 N14 N15 N16 N17 N18 N19 N20 N21 N22 N23 N24
PG 51.3 52.0 60.9 76.5 62.1 61.1 57.8 52.3 62.3 61.0 62.5 69.5 62.9 60.0 64.4 60.7 63.8 64.7 53.5 52.6 51.3 72.0 68.4 53.2
TextC Ours 81.3 53.7 78.0 56.0 77.7 67.2 90.0 81.2 82.9 77.1 59.0 80.6 78.9 60.5 75.9 48.3 80.2 68.8 73.8 66.4 63.3 80.3 87.6 82.2 79.6 66.6 79.3 70.2 79.6 69.0 65.8 70.8 78.1 62.5 83.3 68.1 78.8 54.9 73.6 54.0 76.8 60.4 79.7 85.6 86.7 75.9 64.4 73.1
Gain1 Gain2 27.6 30.0 22.0 26.0 10.6 16.8 8.8 13.5 5.8 20.8 19.5 21.6 18.4 21.1 27.6 23.6 11.4 17.8 7.3 12.8 17.8 17.0 5.4 18.1 13.0 16.7 9.1 19.3 10.6 15.1 10.1 5.0 15.6 14.3 15.2 18.6 24.0 25.3 19.7 21.0 16.4 25.5 13.6 5.9 10.8 18.4 19.9 8.7
D . Accuracy wrt Confidence
Our prediction model not only provides a maintain or expire decision recommendation , but also provides a con
F . Convergence of the Refinement Algorithm
Figure 9 shows the value of ∆R at each iteration during the network based refinement procedure for one company . We can see from the curve that it can converge very soon . Other companies/organizations have similar result .
Figure 9 . Convergence of our optimization procedure . X axis denotes the number of iteration . Y axis denotes ∆R(ie , Delta R )
VIII . CONCLUSION
Patent maintenance decisions are traditionally made manually . However , for large companies or organizations , making a decision on which portion of the patent portfolio to maintain is difficult because there are so many patents ( and their related patents ) need to be investigated and many aspects need to be considered when making such both financially and strategically important decisions .
We conclude by summarizing our main contributions as follows :
( 1 ) We introduce the new patent mining problem – automatic patent maintenance decision recommendation , and propose a systematic approach to solve this problem .
( 2 ) We propose a set of new patent features to capture important textual and time evolving properties for patent maintenance . Experiment results show that they are informative and useful for building high accurate classification model .
( 3 ) We propose a network based refinement approach utilizing the patent information network for prediction smoothing and optimization . It can help obtain improved confidence scores for better performance .
( 4 ) We conduct experiments on the large scale United States Patent and Trademark Office ( USPTO ) database which contains over 4 millions of granted patents . The results on major companies/organizations show that our technique can achieve high prediction accuracy . Our method is particularly successful for high confidence predictions .
( 5 ) We have implemented our framework as a web service system to provide intelligent decision making for business clients .
Figure 7 . Accuracy wrt confidence . X axis denotes the confidence score , and Y axis denotes the accuracy . fidence score . For patents with high confidence score , our model can get high accuracy . Figure 7 shows the accuracy wrt confidence score for several example assignees . As the confidence score threshold gets higher , the accuracy also gets higher . Other datasets show similar results .
E . Impact of the Refinement Algorithm
The proposed network based refinement algorithm can improve the performance by refining the initial prediction scores , especially for improving the accuracy for confidence levels . Figure 8 shows the average accuracy wrt confidence score of all the companies for with and without the refinement . We can see that the refinement algorithm can improve the accuracy of all the four confidence levels .
We have conducted experiments using different initial classifiers , such as RandomForest , RIPPER and RBFNetwork . The results also show that the refinement can improve the accuracy for different confidence levels . The major reason is that by utilizing the network structure to reveal the patent relation , we can smooth the scores to achieve better confidence for the prediction .
Figure 8 . Accuracy wrt confidence score with and without refinement . X axis denotes the confidence score , and Y axis denotes the accuracy .
ACKNOWLEDGMENT
We thank Yan Liu for her special contribution on patent quality features . We also thank Alfredo Alba , Ana Lelescu and Thomas D . Griffin for their great help . The work was supported by IBM and in part by US National Science Foundation grants IIS 0905215 , CCF 0905014 , Cooperative Agreement No . W911NF 09 2 0053 ( NS CTA ) , and MURI award FA9550 08 1 0265 .
REFERENCES
[ 1 ] Y . Chen , S . Spangler , J . Kreulen , S . Boyer , T . D . Griffin , A . Alba , A . Behal , B . He , L . Kato , A . Lelescu , C . Kieliszewski , X . Wu , and L . Zhang , “ SIMPLE : A strategic information mining platform for licensing and execution , ” in Proceedings of the 2009 IEEE International Conference on Data Mining Workshops , ser . ICDMW ’09 . Washington , DC , USA : IEEE Computer Society , 2009 , pp . 270–275 .
[ 2 ] M . A . Hasan , W . S . Spangler , T . Griffin , and A . Alba , “ COA : finding novel patents through text analysis , ” in KDD ’09 : Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining . New York , NY , USA : ACM , 2009 , pp . 1175–1184 .
[ 3 ] C . J . Fall , A . T¨orcsv´ari , K . Benzineb , and G . Karetka , “ Automated categorization in the international patent classification , ” SIGIR Forum , vol . 37 , no . 1 , pp . 10–25 , 2003 .
[ 4 ] A . Pesenhofer , S . Edler , H . Berger , and M . Dittenbach , “ Towards a patent taxonomy integration and interaction framework , ” in PaIR ’08 : Proceeding of the 1st ACM workshop on Patent information retrieval . New York , NY , USA : ACM , 2008 , pp . 19–24 .
[ 5 ] X . Li , H . Chen , Z . Zhang , J . Li , and J . Nunamaker , Jr . , “ Managing knowledge in light of its evolution process : An empirical study on citation network based patent classification , ” Journal of Management Information Systems , vol . 26 , no . 1 , pp . 129–154 , 2009 .
[ 6 ] S . Tiwana and E . Horowitz , “ Findcite : automatically finding prior art patents , ” in PaIR ’09 : Proceeding of the 2nd international workshop on Patent information retrieval . New York , NY , USA : ACM , 2009 , pp . 37–40 .
[ 7 ] S . Bashir and A . Rauber , “ Improving retrievability of patents in prior art search , ” in Proceedings of Advances in Information Retrieval , 32nd European Conference on IR Research ( ECIR’10 ) , 2010 , pp . 457–470 .
[ 8 ] M . Hirschey and V . J . Richardson , “ Valuation effects of patent quality : A comparison for japanese and us firms , ” PacificBasin Finance Journal , vol . 9 , no . 1 , pp . 65–82 , 2001 .
[ 9 ] R . P . Wagner , “ Understanding patent quality mechanisms , ” University of Pennsylvania Law Review , vol . 157 , p . 2135 , 2009 .
[ 10 ] Y . Liu , P . yun Hseuh , R . Lawrence , S . Meliksetian , C . Perlich , and A . Veen , “ Latent graphical models for quantifying and predicting patent quality . ”
IBM Research Report , 2010 .
[ 11 ] H . Kashima , S . Hido , Y . Tsuboi , A . Tajima , T . Ueno , N . Shibata , I . Sakata , and T . Watanabe , “ Predictive modeling of patent quality by using text mining , ” in International Association for Management of Technology ( IAMOT ) 2010 Proceedings , 2010 .
[ 12 ] R . Mann , “ A new look at patent quality , ” in American Law and Economics Association Annual Meetings . The Berkeley Electronic Press , 2008 .
[ 13 ] R . Mann and M . Underweiser , “ A new look at patent quality : The Berkeley
Relating patent prosecution to validity . ” Electronic Press , 2010 .
[ 14 ] M . A . Hasan and W . S . Spangler , “ Assessing patent value through advanced text analysis , ” in ICAIL ’07 : Proceedings of the 11th international conference on Artificial intelligence and law . New York , NY , USA : ACM , 2007 , pp . 191–192 .
[ 15 ] L . Page , S . Brin , R . Motwani , and T . Winograd , “ The pagerank citation ranking : Bringing order to the web , ” Stanford University , Tech . Rep . SIDL WP 1999 0120 , 1999 .
[ 16 ] S . Chakrabarti , B . Dom , P . Raghavan , and S . Rajagopalan , “ Automatic resource compilation by analyzing hyperlink structure and associated text , ” in 7th World Wide Web Conference ( WWW’97 ) , 1997 , pp . 65–74 .
[ 17 ] M . A . Hasan , W . S . Spangler , T . Griffin , and A . Alba , “ COA : finding novel patents through text analysis , ” in KDD’09 , 2009 , pp . 1175–1184 .
[ 18 ] R . Kosala and H . Blockeel , “ Web mining research : A survey , ”
ACM SIGKDD EXPLORATIONS , 2000 .
[ 19 ] I . De and A . Kontostathis , “ Experiments in first story detection , ” in Proceedings of the 2005 National Conference on Undergraduate Research ( NCUR ) , 2005 .
[ 20 ] Porter , “ An algorithm for suffix stripping , ” Program , vol . 14 , no . 3 , pp . 130–137 , 1980 .
[ 21 ] G . Salton , A . Wong , and C . S . Yang , “ A vector space model for automatic indexing , ” Communication of ACM , vol . 18 , no . 11 , pp . 613–620 , 1975 .
[ 22 ] A . Singhal , C . Buckley , and M . Mitra , “ Pivoted document length normalization , ” in Proceedings of the ACM SIGIR96 Conference , 1996 , pp . 21–29 .
[ 23 ] S . Robertson and S . Walker , “ Some simple effective approximations to the 2 poisson model for probabilistic weighted retrieval , ” in Proceedings of ACM SIGIR’94 Conference , 1994 , pp . 232–241 .
[ 24 ] J . M . Ponte and W . B . Croft , “ A language modeling approach to information retrieval , ” in SIGIR ’98 : . New York , NY , USA : ACM , 1998 , pp . 275–281 .
[ 25 ] A . Ahmad and L . Dey , “ A feature selection technique for classificatory analysis , ” Pattern Recognition Letters , vol . 26 , no . 1 , pp . 43–56 , 2005 .
