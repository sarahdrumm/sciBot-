2011 11th IEEE International Conference on Data Mining
Using Bayesian Network Learning Algorithm to Discover Causal Relations in
Multivariate Time Series
Zhenxing Wang
∗ and Laiwan Chan
†
,
The Chinese University of Hong Kong , Shatin , NT Hong Kong zxwang@csecuhkeduhk lwchan@csecuhkeduhk
∗
Email :
†
Abstract—Many applications naturally involve time series data , and the vector autoregression ( VAR ) and the structural VAR ( SVAR ) are dominant tools to investigate relations between variables in time series . In the first part of this work , we show that the SVAR method is incapable of identifying contemporaneous causal relations when data follow Gaussian distributions . In addition , least squares estimators become unreliable when the scales of the problems are large and observations are limited . In the remaining part , we propose an approach to apply Bayesian network learning algorithms to identify SVARs from time series data in order to capture both temporal and contemporaneous causal relations and avoid high order statistical tests . The difficulty of applying Bayesian network learning algorithms to time series is that the sizes of the networks corresponding to time series tend to be large and high order statistical tests are required by Bayesian network learning algorithms in this case . To overcome the difficulty , we show that the search space of conditioning sets dseparating two vertices should be subsets of Markov blankets . Based on this fact , we propose an algorithm learning Bayesian networks locally and making the largest order of statistical tests independent of the scales of the problems . Empirical results show that our algorithm outperforms existing methods in terms of both efficiency and accuracy .
Keywords VAR , SVAR , Bayesian networks , Causality ;
I . Introduction
The VAR model is widely used for analyzing relations between variables in time series data since the pioneering works [ 1 ] . This method infers the relations between variables by regressing the current observation on past observations and interprets the autoregression coefficients as causal effects . There are two main drawbacks of this method . The first one is that the VAR method provides no information on contemporaneous causal order . The second one is that least square estimators become unreliable when the dimensions of time series are large and the number of samples are limited . To capture both temporal and contemporaneous causal relations , the SVAR model is proposed that estimates contemporaneous coefficient matrices by orthogonalizing the VAR residuals [ 2 ] , [ 3 ] , [ 4 ] . However , this paper shows that the SVAR model is incapable of determining contemporaneous causal orders when data follow Gaussian distributions . Recently , graphical models have been recognized useful on time series analysis and Bayesian network learning algorithms have been used to identify the contemporaneous causal orders in SVARs [ 5 ] , [ 6 ] . The main idea of these works is using the least squares approach to estimate a VAR from time series data , and then applying Bayesian network learning algorithms to the VAR residuals to identify contemporaneous causal orders . These works can solve the contemporaneous causal order problem but rely on the the difficulty quality of the estimated VARs . Therefore , of unreliable least squares estimators involved by highdimensional time series still exists in these works .
In this work , we provide a deep analysis on the reason that the SVAR model is incapable of identifying the contemporaneous causal orders . We show that the incapability is because the SVAR model fails to discover the structure faithful to the distributions of data . Since Bayesian network learning algorithms can be employed to identify faithful structures , in the remaining part of the work , we propose a solution of applying Bayesian network learning algorithms to mine causal relations in time series . In our approach , the Bayesian network learning algorithm is applied to time series data instead of the VAR residuals , and temporal and contemporaneous causal relations are considered in a unified way . The key step of doing this is finding a robust Bayesian network learning algorithm to deal with large size Bayesian networks . The reason is as follows . To represent a VAR model by a Bayesian network , the number of the vertices in the Bayesian network is several times larger than that of iid data because the past observations need to be considered . To decide whether there exists an edge between two vertices , the Bayesian network learning algorithms need to search all subsets of the entire vertex set to find a set of vertices d separating these two vertices , and the results of conditional independence ( CI ) tests serve as indicators of d separation [ 7 ] , [ 8 ] . Large networks pose a difficulty on Bayesian network learning algorithms because of unreliable high order CI tests [ 9 ] . The key step of using Bayesian network learning algorithm for causal mining in time series is determining how to avoid high order CI tests even when the sizes of the underlying Bayesian networks are large . This work shows that to infer the structures of Bayesian networks , it is sufficient to test independence conditioned on all subsets of Markov blankets , so that the largest order of CI tests does not dependent on the sizes of networks but on the sizes of Markov blankets . With this results , determining how to
1550 4786/11 $26.00 © 2011 IEEE DOI 101109/ICDM2011153
814 avoid high order CI tests when learning Bayesian networks is equivalent to determining how to identify Markov blankets without involving high order CI tests . In this work , we apply the IAMB algorithm [ 10 ] to learn Markov blankets from data . The contributions of this work are summarized as follows : ( i ) this work formally shows that the reason of the SVAR model being incapable of identifying contemporaneous causal order is that the faithfulness condition is violated under the SVAR model ; ( ii ) this work provides a graphtheoretic method estimating SVARs and VARs that avoids high order regressions ; ( iii ) this work proposes a robust Bayesian network learning algorithm that restricts the search space of conditioning sets within a subset of Markov blanket . The rest of the paper is organized as follows . Section 2 reviews previous works related to ours . Section 3 discusses the reason that the SVAR model is incapable of identifying contemporaneous causal order and the necessity of Bayesian network learning algorithms . Section 4 explains how to use our heuristic Bayesian network learning algorithm to estimate SVARs and VARs . Section 5 presents some empirical comparison of our algorithm and existing algorithms . Section 6 concludes our paper with a summary of our finds .
II . Related Works
Related works in the VAR community . The VAR model has been the focus of interest in economic time series analysis since [ 1 ] , followed by a series of varieties . One of the varieties related to causal relation mining is the SVAR model [ 2 ] , [ 3 ] , [ 4 ] . In SVAR model , contemporaneous correlations are assumed to be causally interpreted . One typical solution of this type of methods is estimating a standard VAR model and then orthogonalizing the covariance matrix of the residuals using the Cholesky decomposition . However , this work will show that under the SVAR framework , all the contemporaneous causal orders are equivalent for Gaussian processes , and the faithfulness condition is violated . The method presented in [ 11 ] requires prior knowledge of the contemporaneous causal orders to identify the true SVAR models .
Related works in the Bayesian network learning community . Bayesian network learning algorithms have been widely used for causal discovery since the pioneering works [ 7 ] , [ 8 ] . To reduce computational complexity , the PC algorithm [ 12 ] is a representative one that has polynomial time complexity when Bayesian networks are sparse but not efficient when networks are relatively dense . Markov blanket information is used by Bayesian network learning algorithms to reduce computational complexity and avoid high order CI tests . The algorithms using Markov blanket information are GS [ 13 ] , MMHC [ 14 ] and TC [ 15 ] . The TwoPhase algorithm [ 16 ] , which improves theTPDA algorithm [ 17 ] , learns Bayesian networks from Markov random fields . In this work , we improve the Two Phase algorithm by using Markov blanket information .
Related works combining VAR models and causal mining models . Bayesian network learning algorithms are used for identifying the contemporaneous causal order of a SVAR[5 ] , [ 6 ] . In [ 5 ] , the authors apply the PC algorithm to search the contemporaneous causal order in the residual covariance matrix . In TSCM [ 6 ] , a score based Bayesian network learning algorithm is used to discover the causal structures in SVARs . Besides Bayesian network learning algorithms , the LiNGAM algorithm [ 18 ] , a causation mining algorithm based on independence component analysis , has been applied for determining the causal orders in SVARs [ 19 ] , and in that work , the authors show that ignoring the contemporaneous causal effects can lead to wrong estimation of temporal causal relations . All these works need to estimate VAR models first , and then apply causation mining algorithms to the residual covariance matrices . Therefore , these methods are incapable of overcoming the difficulty that least squares estimators are unreliable when the scales of the problems are large .
III . SVAR being Incapability of Identifying contemporaneous causal order k .
We denote an observed time series by Xi(t ) = xi(t ) for 1 ≤ i ≤ n and 1 ≤ t ≤ m , where Xi(t ) denotes random variable , xi(t ) is the corresponding observation , i is the index of random variables , and t is the index of time . The collection of all the variables at time t is denoted by n × 1 vector X(t ) and the observations x(t ) . The dynamic of the time series under the VAR model is
AτX(t − τ ) + E(t )
τ=1
X(t ) =
( 1 ) where k is the order of the VAR , Aτ , 0 ≤ τ ≤ k , are n × n coefficient matrices , E(t ) are disturbances satisfying E(t ) . E(s ) for 1 ≤ t , s ≤ m , t fi s , and the covariance matrix of E is Cov[E ] = ΣE . A VAR with order k is denoted by VAR(k ) . The coefficient matrices of the VAR in equation ( 1 ) can be estimated from data by the least squares method as follows [ 20 ] . Denote the observations by concise matrix notations :
Y = [ x(k + 1)··· x(m) ] , Z =
⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ x(k ) x(k − 1 )
. . . x(k + 1 ) x(k )
. . . x(1 ) x(2 )
. . . . . . . . . . . . x(m − 1 ) x(m − 2 )
. . . x(m − k )
⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ ( 2 )
( 3 )
The least squares estimators of the coefficient matrices are given by the following formula :
ˆA = [ ˆA1 . . . ˆAk ] = YZ
( ZZ
)
'
'
−1 and the regression residuals are calculated by the formula :
ˆe(t ) = x(t ) − k .
ˆAτx(t − τ )
( 4 )
The drawback of using the least squares approach to estimate the structures of VARs is that when the dimensions of
τ=1
815
VARs are large and sample sizes are limited , the estimate coefficient matrices become unreliable . Even worse , when the number of samples is smaller than the production of the dimension of the VAR and the order of the VAR , namely m < n×k , the matrix ZZ becomes a singular matrix , and the inverse of ZZ does not exist so the least squares approach fails . This is an important motivation for us to apply the Bayesian network learning algorithm directly to the time series data instead of the regression residuals .
'
'
The SVAR model assumes that the covariance matrix of disturbances can be interpreted as the contemporaneous causal structure , and SVARs can be identified from VARs by applying Cholesky decomposition to the covariance matrix of the disturbances as follows :
' Cov[E ] = ΣE = LL where L is a lower triangular matrix with strictly positive ' denotes the transpose of L . For each diagonal entries , and L E(t ) , find W(t ) satisfying
E(t ) = LW(t )
( 6 )
Plugging equation ( 6 ) into equation ( 1 ) and rearranging the formula , we get where B0 = I − L equation ( 6 ) , we know that
BτX(t − τ ) + W(t )
X(t ) = −1 and Bτ = L
τ=0
( 7 ) −1Aτ . From equation ( 5 ) and
Cov[W ] = L
−1ΣE(L
−1 )
' = I
( 8 ) k . that in equation ( 7 ) is known as a SVAR . The The model information of the contemporaneous causal order of a time series is implied in the contemporaneous coefficient matrix of the true SVAR . The true SVAR is distinguished from the other SVARs by the fact the disturbances are independent rather than uncorrelated , and the true SVAR is assumed corresponding to the data generating process [ 5 ] . We show that the procedure of identifying the true SVAR is equivalent to identifying the structure equation model ( SEM ) generating the disturbances of the original VAR . Suppose that there exists a permutation matrix P denoting the true contemporaneous causal order of the SVAR . The disturbances under the true contemporaneous causal order are given by
EP(t ) = PE(t )
( 9 )
Denote the diagonal matrix
'
D = PDiag(σ2 11
( 10 ) where σ2 ii denotes the variance of the ith entry in E , and the covariance matrix of EP can be decomposed in accordance with Cholesky decomposition as follows
, , σ2
, σ2 22 nn)P
Cov[EP ] = PΣEP
' ' = LPL P
= LPD
− 1 2 DD
− 1 ' 2 L P
'
= MDM
( 11 )
816 where − 1 2 = PDiag(
D
1 σ11
, 1 σ22
, , 1 σnn
' , M = LPD
− 1
2
)P
( 12 )
LP is a lower triangular matrix got by applying Cholesky decomposition to ΣEP , and the main diagonal entries of LP are the corresponding standard deviations of EP . We restrict − 1 the main diagonal entries of M by D 2 , so that M is a lower triangular matrix with all the diagonal elements 1 . And there exists a vector WP(t ) satisfying
EP(t ) = LPWP(t ) = LPD
2 WP(t ) = MUP(t )
− 1 2 D 1 −1)EP(t ) + UP(t )
= ( I − M = C0EP(t ) + UP(t )
( 13 )
( 5 ) where Cov[WP(t ) ] = I , C0 = I − M
−1 , UP(t ) = D 1 2 Cov[WP]D 1
Cov[UP ] = D 1
2 WP(t ) ,
2 = D ( 14 )
The coefficient matrix C0 is a strictly lower triangular matrix −1 is a lower triangular matrix with all diagonal because M entries 1 . For the permutation matrix P corresponding to the true contemporaneous causal order , disturbances UP satisfy UPi . UP j for 1 ≤ i , j ≤ n and i fi j , and equation ( 13 ) is the SEM generating the disturbances of the VAR in equation ( 1 ) . Plugging equation ( 13 ) into equation ( 1 ) , we have
XP(t ) = k . k .
τ=1
=
τ=0
'
XP(t − τ ) + EP(t )
PAτP
CτXP(t − τ ) + UP(t )
( 15 ) where
XP(t ) = PX(t ) ,
Cτ = M
−1PAτP
' , 1 ≤ τ ≤ k
( 16 )
Because the elements in UP are mutually independent , the SVAR in equation ( 15 ) is assumed corresponding to the data generating procedure . The strictly lower triangular coefficient matrix C0 captures the contemporaneous causal order in this case . The non zero elements c0i j in C0 denotes that x j is a contemporaneous cause of xi . The difficulty of analyzing contemporaneous causal order by identifying the data generating process is that for a time series with dimension n , there are n! possible orders , and under each of them , we need to test independence of regression residuals . Even worse , when the given time series is a Gaussian process , the elements in residual UP are mutually independent for arbitrary permutation P , because uncorrelation is equivalent to independence in this case . However , different P results in different contemporaneous coefficient matrix C0 and makes contemporaneous causal order analysis fail . In this case , prior knowledge is required to resolve the confusion .
The concepts related to Bayesian network learning provide a deep insight of the reason that the SVAR model is incapable of identifying contemporaneous causal order . The SVAR model cannot distinguish these orders under which the faithfulness condition is satisfied from those under which the faithfulness condition is violated . A distribution q is faithful to a Bayesian network if X and Y independent conditioned on Z implies Z d separating X and Y [ 12 ] . The previous work [ 21 ] shows that a distribution satisfying the Markov property of a Bayesian network is faithful to the network almost surely , which means that with probability one , the distribution of data is faithful to the Bayesian network corresponding to the true data generating process . We illustrate the concepts by Figure 1 . In Figure 1 , the disturbance E = ( E1 , E2 , E3 ) is generated by the SEM in the left , and all Ui , i = 1 , 2 , 3 , are drawn from standard Gaussian distribution and mutually independent . There is no causal effect between disturbances E1 and E2 , and both E1 and E2 are the causes of E3 . The corresponding structure is illustrated by the Bayesian network in the right of Figure 1 . The distribution of E is faithful to the Bayesian network in Figure 1 because E1 . E2 implies that E1 and E2 are d separated by empty set , which means that E1 and E2 are not directly connected in the network . The true permutation matrix P and the corresponding parameters are
'
⎡⎢⎢⎢⎢⎢⎢⎢⎣ 0
0 1
⎤⎥⎥⎥⎥⎥⎥⎥⎦ , MP =
⎡⎢⎢⎢⎢⎢⎢⎢⎣ 1
0 1
0 0 0
⎡⎢⎢⎢⎢⎢⎢⎢⎣ 1
0 0
⎤⎥⎥⎥⎥⎥⎥⎥⎦
0 1 0
0 0 1
Another permutation matrix Q and the corresponding parameters are given as follows :
⎤⎥⎥⎥⎥⎥⎥⎥⎦ , Cov[UP ] = ⎤⎥⎥⎥⎥⎥⎥⎥⎥⎦
0 0 1 2
0 0 0
⎤⎥⎥⎥⎥⎥⎥⎥⎦
0 1 0
0 0 1
0 1 1
0 0 1
0 − 1
3 1 2
⎡⎢⎢⎢⎢⎢⎢⎢⎢⎣ ⎡⎢⎢⎢⎢⎢⎢⎢⎣ 3
0 0
P = I , CP 0
=
⎡⎢⎢⎢⎢⎢⎢⎢⎣ 0 ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎣
0 1 1 − 1 − 1
3
3
0 1 0
1 0 0
0 1 − 1
3
Q =
MQ =
0 0 1
⎤⎥⎥⎥⎥⎥⎥⎥⎦
0 0 1
⎤⎥⎥⎥⎥⎥⎥⎥⎥⎦
CQ 0
=
Cov[UQ ] =
For both permutation P and Q , the elements in residual U are mutually independent , and the SVAR model cannot tell which one corresponds the true data generating procedure . It is obvious that the faithfulness condition is violated under permutation Q . The reason is as follows . The regression coefficient between E1 and E2 under permutation Q , in this case cQ 032 , the entry in the third row and second column of CQ 0 , is not zero , which means that E1 and E2 are connected by an edge in the Bayesian network corresponding to permutation Q , and in this case , E1 and E2 cannot be dseparated . The independence of E1 and E2 under Q is the result of special regression coefficients . To be more specific , for each pair of mutually independent disturbances Ei and E j , i < j , the SVAR model always finds the coefficient matrix C0 making i .
Cov[Ei , E j ] = mikm jkVar[Uk ] = 0
( 17 ) k=1 where Cov[Ei , E j ] is the covariance between Ei and E j , mik is the entry in the i th row and k th column of M and from equation ( 16 ) we know that mik is a function of C0 . For this reason the SVAR model cannot tell the coefficient matrices corresponding to the faithful structures from the coefficient
817 matrices satisfying dependency relations by choosing special regression coefficients . The Bayesian network learning algorithms always find the faithful structures by CI tests . Given the fact that E1 and E2 independent the corresponding entry in the coefficient matrix is restricted to be zero so that coefficient matrix CQ is ruled out . In addition , Bayesian 0 network learning algorithms identify the contemporaneous causal order by finding all the V structures [ 22 ] . If the two parents of a collider are not connected by an edge , the collider and its two parents form a V structure in a Bayesian network . In the Bayesian network in Figure 1 , the vertices E1 , E2 , and E3 form a V structure , and the Bayesian network learning algorithms identify it by the results that E1 and E2 are independent of each other conditioned on the empty set and dependent of each other conditioned on E3 .
E1 = U1
E2 = U2
E3 = E1 + E2 + U3
Figure 1 . An illustration of violation of the faithfulness condition in SVAR
IV . Estimating SVARs by Bayesian Network Learning
Algorithm
In this section , a novel algorithm for discovering VARs and SVARs from data is presented . The algorithm is sound under the following assumptions : ( i ) the coefficient matrices of SVARs are time invariant ; ( ii ) the coefficient matrix C0 corresponds to a directed acyclic graph . Namely there exists a permutation matrix P , so that PC0P is a strictly lower triangular matrix , which is a common assumption in causal analysis ; ( iii ) there exist reliable statistical tests of conditional independence .
'
A . Representing SVARs by Bayesian networks
One of the distinction between this work and previous works using Bayesian network learning algorithms to learn SVARs is that in this work , the Bayesian network learning algorithm is applied to the time series data instead of regression residuals . To represent a SVAR(k ) by a Bayesian network , the vertex set contains the current observation and previous k observations . Each vertex in the network corresponds a random variable Xi(t ) for 1 ≤ i ≤ n and 0 ≤ t ≤ k , where n denotes the dimension of the time series , and Xi(t ) denotes the ith element of X(t ) . To represent a n dimensional SVAR(k ) , there are n × ( k + 1 ) vertices in the corresponding Bayesian network . There is an edge from X j(t− s−τ ) to Xi(t− s ) if and only if cτi j fi 0 for 0 ≤ s ≤ k−τ . The reason is that the SVARs considered in this work have time invariant coefficient matrices , which implies that the causal relations between variables are also time invariant . The non zero entry cτi j indicates that X j(t − τ ) is a cause of Xi(t ) , which also implies that X j(t − s − τ ) is a cause of Xi(t − s ) for all s > 0 . Consider a SVAR(2 ) with following ⎡⎢⎢⎢⎢⎢⎢⎢⎣ ⎤⎥⎥⎥⎥⎥⎥⎥⎦ coefficient matrices : C0 =
⎤⎥⎥⎥⎥⎥⎥⎥⎦ C1 =
⎤⎥⎥⎥⎥⎥⎥⎥⎦ C2 =
⎡⎢⎢⎢⎢⎢⎢⎢⎣
0 0.5 0
0 0 0.3
0 0 0
0 0.5 0
0.4 0 0
0 0.3 0
0 0 0.2
0.7 0 0
⎡⎢⎢⎢⎢⎢⎢⎢⎣ 0.3
0 0.6
The Bayesian network corresponding to this time series is given in Figure 2 . The dimension of the time series is 3 , and the order of the SVAR is 2 . Therefore , there are 3×(2+1 ) = 9 vertices in the network . There is an edge from X1(t ) to X2(t ) since c021 = 0.5 , which also implies an edge from X1(t−1 ) to X2(t− 1 ) and an edge from X1(t− 2 ) to X2(t− 2 ) . Time series data pose a challenge on Bayesian network learning because the sizes of the networks grow with both the dimensions and the orders of time series . The difficulty motives us to address a Bayesian network learning algorithm that has good performance on large networks .
Figure 2 . An illustration of representing time series by Bayesian network
B . Markov Two Phase Algorithm
We improve the Two Phase algorithm [ 16 ] ( denoted by Basic2P in this work ) and apply the enhanced algorithm to analyze time series data . General idea of Basic2P comes from the observation that the structures of Bayesian networks are similar with those of Markov random fields . Phase One constructs a Markov random field from data , and Phase Two removes redundant edges from the Markov random field and orients the edges to get the true Bayesian network . The Basic2P algorithm has two drawbacks when dealing with large size Bayesian networks : ( i ) phase one of Basic2P learns Markov random fields using high order CI tests . To be more specific , to decide whether there is an edge in the Markov random field between two vertices U and V , Basic2P needs to test the independence of U and V conditioned on the entire vertex set excluding the given two vertices . ( ii ) Basic2P does not use the Markov blanket information obtained in phase one to reduce the search space of conditioning sets in phase two . To avoid the high order CI tests when learning Markov random fields , we use the IAMB algorithm [ 10 ] to identify the Markov blanket of each vertex from data . Markov random fields can be constructed in accordance with the fact that in a Markov random field , a vertex and every vertices in its Markov blanket are directly connected as Figure 3 shows , where MB( ) denotes Markov blanket . We use the absolute values of partial correlation coefficients as the heuristic in this work instead of mutual information used in the original IAMB . After IAMB , phase
4 5 6 7 8 9 10 11 12
13 14 15 16 17 18 19 20 21 22 23 24
Algorithm 1 : Two Phase algorithm with Markov Blanket information Procedure : Markov2P ( D , k ) Data : Time series data set D Result : The complete PDAG GP(V , E ) of underlying Bayesian network GB , where V and E denote the vertex set and the edge set of the network respectively .
1 begin 2 3
// Phase I starts from here initialize a Markov random field GM(V , ˜E ) by mapping each X ∈ V to a random variable in D or one of its k lags and setting ˜E = ∅ ; IAMB ( D ) ; foreach X ∈ V and Y ∈ MB(X ) do add edge {X , Y} into ˜E ; end foreach // Phase II starts from here initialize E = ˜E ; foreach edge {X , Y} ∈ E do
)
S = the smaller set of MB(X ) and MB(Y ) ; S = ADJ(X , Y ) S ; // ADJ(X , Y ) is the set containing all the vertices on the simple paths connecting X and Y in GM(V , ˜E ) ; // S is the search space of conditioning sets if ∃C ⊂ S , ρXY·C = 0 then remove {X , Y} from E ; foreach V ∈ S \ C do ) if V ∈ MB(X )
MB(Y ) then orient V as a child of both X and Y ; end if end foreach end if end foreach // Edge orientation starts from here orient edges form lags to the variables at the current time ; perform constraint propagation rules ;
25 26 end two of Markov2P removes redundant edges from Markov random fields to recover the skeletons of Bayesian networks . In Basic2P , the search space of conditioning sets is restricted to the set of vertices on the simple paths connecting two endpoints of an edge . In Markov2P , we show that the search space of conditioning sets can be further reduced . To be more precise , for any two vertices X and Y , if there exists a set of vertices conditioned on which X and Y are independent , we are always capable to make the two vertices independent by choosing vertices only from either MB(X ) or MB(Y ) instead of the entire vertex set . We illustrate this idea by Figure 3 . Consider the redundant
818
) edge {A , M0} of the Markov random field in Figure 3 ( a ) , which should be removed in phase two . In phase two of Basic2P , it searches conditioning sets from all the vertices on the simple paths connecting A and M0 . We denote this set by ADJ(A , M0 ) , and ADJ(A , M0 ) is composed of the three vertices under the colored circle : M4 , M6 and a grey vertex . Phase two of Markov2P reduces the search space to ADJ(A , M0 ) MB(A ) that contains only M4 and M6 . In this case , we exclude one vertex from the search space and cut the size of the searching space by half , because the algorithm has to go through all possible subsets in it . On large networks , this improvement can be significant . Another advantage is that by reducing the size of the search space , Markov2P has less chance to test independence conditioned on large sets , which makes CI tests more stable . By CI tests with Markov blanket information , phase two of Markov2P checks every edge in the Markov random fields and removes redundant ones . Finally , Markov2P orients edges in the skeleton by identifying all the V structures in the network . For two vertices X and Y , if there exists an edge between X and Y in the Markov random field , and the edge is excluded from Bayesian network , X and Y must be spouses in the underlying Bayesian network , and the children of both X and Y must be included in S = ADJ(X , Y ) MB(Y ) . In this case , if there exists a set C ⊂ S making X and Y independent conditioned on C , which is indicated by ρXY·C = 0 , where ρXY·C denotes the partial correlation coefficient between X and Y given C , the children of both X and Y cannot be in C , and therefore , in S\C . After orienting edges in the skeleton by identifying all the V structures in the network , Markov2P poses the directions of the undirected edges from lags to the current variables . For the remaining undirected edges , Markov2P applies the constraint propagation rules used in the PC algorithm [ 12 ] to identify the orientations . Without other information , Markov2P cannot distinguish Bayesian networks in a Markov equivalent class that contains Bayesian networks with the same skeleton and V structures . Therefore , Markov2P outputs the complete partial directed acyclic graph ( PDAG ) that represents the Markov equivalent class of the true Bayesian network .
MB(X )
)
)
( a ) Markov random field
( b ) Bayesian network
Figure 3 . An illustration of Markov blanket : ( a ) and ( b ) denote a Markov random field and a Bayesian network defined on the same vertex set respectively . In Bayesian network ( b ) , MB(A ) contains vertices M0 to M6 , and in Markov random filed ( a ) , vertices in MB(A ) are all neighbors of A .
819
C . Getting Back SVARs and VARs from Bayesian Networks
Algorithm 2 : Bayesian VAR Procedure : BayesVAR ( D , k ) Data : Time series data set D Result : The Estimate Coefficient Matrices of the
SVAR(k ) and the VAR(k )
1 begin 2 3 4
Markov2P ( D , k ) ; // Learn a Bayesian network foreach Xi(t ) ∈ X(t ) do regress Xi(t ) on Pa(Xi(t ) ) with ordinary least squares method ; // Pa(Xi(t ) ) denotes the parents of Xi(t ) in the Bayesian network ;
5 6 7
8 9
10 11 12 13 14
15
16
17 end foreach foreach cτi j do
// cτi j is the element at the i th row j th column of the coefficient matrix Cτ of the SVAR(k ) if X j(t − τ ) ∈ Pa(Xi(t ) ) then assign the corresponding regression coefficient to cτi j ; else cτi j = 0 ; end if end foreach find the permutation matrix P makes PC0P lower triangular matrix ; for 0 ≤ τ ≤ k ; −1 ; for 1 ≤ τ ≤ k ; // Aτ are the '
Cτ = PCτP M = ( I − C0 ) Aτ = PMCτP
'
' a coefficient matrices of the VAR(k )
18 end
Some tasks may require to estimate the coefficient matrices rather than causal relations . In this section , we show that the coefficient matrices can be estimated based on the Bayesian networks obtained , and high order statistic tests are avoided by this approach , which makes the estimators more reliable than directly applying the least squares approach to the time series . The idea of our approach is as follows . The structures of Bayesian networks indicate which random variables should be taken as the independent variables when we use least squares approach to estimate the coefficient matrices of SVARs , and the coefficient matrices of the VARs can be calculated based on those of the SVARs in accordance with equation ( 16 ) . To be more specific , to regress a variable at the current time in a SVAR(k ) , it is not necessary to take all the variables in past k time as independent variables , but only the parents of the dependent variable in the Bayesian network . The justification is an immediate result of the Markov property of Bayesian network : a variable in a Bayesian network is independent of its nondescendants given its parents , and the variables in the past time are all nondescendants of the variables in the current time .
We present this idea formally in the procedure BayesVAR . In the BayesVAR algorithm , we first apply Markov2P to the time series data to learn a Bayesian network that corresponds to the true SVAR . With the Bayesian network obtained , we regress every variable at the current time on its parents , and the regression coefficient is the estimator of the corresponding element in the coefficient matrix of the SVAR . The next step is finding the permutation matrix P that implies the contemporaneous causal order , and calculating the coefficient matrices of the VAR according to equation ( 16 ) . From the procedure , we can see that BayesVAR has essential distinction from previous works [ 5 ] , [ 6 ] , [ 19 ] . In these works , the VAR is the necessary first step , and the Bayesian network algorithms are applied to the residuals of the VAR to identifying the contemporaneous causal order , so that the true SVAR can be distinguished from others . In our work , the VARs and SVARs are identified in a reversed order . The SVARs are identified first by applying Markov2P to the time series data , and the VARs are figured out based on the SVAR . The advantage of solving the problem in this way is that high order statistic tests required by the VAR approach can be avoided by choosing efficient Bayesian network learning algorithms . With the Bayesian network information , the number of independent variables is |Pa(Xi(t))| for each Xi(t ) , while in the VAR approach , the number is k× n for a n dimensional VAR(k ) as equation ( 2 ) shows .
V . Experimental Results
A . Experiment on Artificial Data
To investigate the performance of the BayesVAR algorithm , we conducted a series of simulations . We design the experiment to check if empirical results support our theoretical analysis . To be specific , we are going to test ( i ) if the BayesVAR algorithm is capable to identify contemporaneous causal relations ; ( ii ) if the Markov2P algorithm avoids highorder CI tests when dealing with large Bayesian networks . We also compare BayesVAR with the following algorithms : • The VAR+PC algorithm [ 5 ] . This approach applies VAR to time series first , and then uses PC to discover the contemporaneous causal order on regression residuals . • The GS algorithm [ 13 ] . GS is a Bayesian network learning algorithm that infers the structures of Bayesian networks locally by identifying the Markov blanket of each vertex . In this work , we apply GS to the Bayesian networks corresponding to time series .
• The TC algorithm [ 15 ] . TC is a Bayesian network learning algorithm that identifies Markov blankets by linear regression . In this work , we apply TC to the Bayesian networks corresponding to time series .
• The BayesVAR algorithm . BayesVAR uses Markov2P to learn the Bayesian networks corresponding to time series and estimates VARs based on the Bayesian networks obtained .
Table I
Bayesian Networks Used in Evaluation
Networks Insurance [ 23 ] Mildew [ 24 ] Alarm [ 25 ] Hailfinder [ 24 ]
Num . Variables 27 35 37 56
Num . Max In/Out Edges 52 46 46 66
Degree 3/7 3/3 4/5 4/16
We empirically evaluate the listed algorithms on Gaussian time series , and the data are generated according to some real world Bayesian networks . The networks used in the evaluation are listed in Table I , which are obtained from real decision support systems and publicly available from Bayesian Network Repository [ 26 ] . Data are generated by SVARs corresponding to these networks by the following way . Every coefficient matrix Cτ is constructed according the structures of the Bayesian networks . Namely , the ( i , j)th entry of Cτ is nonzero if and only if there is an edge from variable j to variable i . And then for a SVAR(k ) , we generate k +1 coefficient matrices , and in this case , there are ( k + 1 ) × n vertices in the Bayesian network corresponding to the SVAR if the original real world network contains n vertices . Weights are sampled from uniform distribution between 0.1 and 0.9 for nonzero entries in Cτ , and then we normalize Cτ , so that SVARs are stable . Each variable in SVARs is evaluated according to the dynamic of the SVAR for t steps , where t is the sample size in the experiment . Disturbance terms are drawn from standard Gaussian distribution , so that all SVARs are Gaussian processes , and zero partial correlation can be tested by Fisher ’s Z transformation of partial correlation coefficients [ 27 ] . The significant level is set to be 005 The performance of the algorithms are qualified in terms of efficiency and accuracy . To evaluate efficiency , we use the number of CI tests required by each algorithm as the metric , because all the algorithms in comparison are based on dependency analysis approach . For accuracy , we use precision , recall , and F1 measure as evaluation metrics , all of which are defined based on the similarity between the output Bayesian network and the target one . We use adjacent matrix to represent a Bayesian network . Let ¯A denote the target adjacent matrix , and ˆA the output one , precision P and recall R are defined as follows : |{(i , j ) : ¯A(i , j ) = ˆA(i , j ) = 1}|
|{(i , j ) : ¯A(i , j ) = ˆA(i , j ) = 1}|
R =
|{(i , j ) : ¯A(i , j ) = 1}|
P =
|{(i , j ) : ˆA(i , j ) = 1}|
Given precision and recall , F1 is defined as :
2 ∗ P ∗ R P + R
F1 =
We first check the accuracy of the BayesVAR identifying both temporal and contemporaneous causal relations , and how the performance varies with the scales of the problems and the number of available observations . In this experiment , we generate SVAR(1 ) and SVAR(2 ) based on each
820
1
0.8
1 F
0.6
0.4
1
0.8
1 F
0.6
0.4
VAR+PC(1 ) GS(1 ) TC(1 ) BayesVAR(1 )
103
Sample size
104
0.2
102
0 102
VAR+PC(1 ) GS(1 ) TC(1 ) BayesVAR(1 )
103
Sample size
104
0.2
102
0.2
102
0.8
0.6
1 F
0.4
0.2
( a ) Insurance
1
0.8
1 F
0.6
0.4
1
0.8
1 F
0.6
0.4
0.8
0.6
1 F
0.4
0.2
VAR+PC(2 ) GS(2 ) TC(2 ) BayesVAR(2 )
103
Sample size
104
VAR+PC(2 ) GS(2 ) TC(2 ) BayesVAR(2 )
103
Sample size
104
VAR+PC(1 ) GS(1 ) TC(1 ) BayesVAR(1 )
103
Sample size
104
0.2
102
0 102
1
0.8
1 F
0.6
0.4
0.2
102
( b ) Mildew
VAR+PC(1 ) GS(1 ) TC(1 ) BayesVAR(1 )
103
Sample size
104
1
0.8
0.6
0.4
0.2
1 F
0 102
VAR+PC(2 ) GS(2 ) TC(2 ) BayesVAR(2 )
103
Sample size
104
VAR+PC(2 ) GS(2 ) TC(2 ) BayesVAR(2 )
103
Sample size
104
( c ) Alarm
( d ) Hailfinder
Figure 4 . Evaluate BayesVAR against VAR+PC , GS , and TC on first and second order SVARs : BayesVAR(# ) VAR+PC(# ) , GS(# ) , and TC(# ) denote the results on SVAR(# )
Average sizes , maximal sizes of conditioning sets , and the number of CI tests required by each algorithm when 8192 samples available
Table II
( a ) The SVAR based on Insurance Network
# Statistics Avg . |C| Max |C| # CI tests
# Statistics Avg . |C| Max |C| # CI tests
# Statistics Avg . |C| Max |C| # CI tests
# Statistics Avg . |C| Max |C| # CI tests
VAR+PC(1 ) 13.5 27 749048
VAR+PC(1 ) 16.4 35 683278
VAR+PC(1 ) 17.5 37 749582
VAR+PC(1 ) 29.4 56 5403893
GS(1 ) 11.8 33 4108
GS(1 ) 12.5 38 3285
GS(1 ) 12.1 43 5182
GS(1 ) 18.5 58 7692
TC(1 ) 13.1 52 3186
BayesVAR(1 ) 10.8 19 2663
VAR+PC(2 ) 21.2 54 792303
( b ) The SVAR based on Mildew Network VAR+PC(2 ) 30.2 70 684064
BayesVAR(1 ) 9.7 31 1987
TC(1 ) 14.7 68 2349
( c ) The SVAR based on Alarm Network VAR+PC(2 ) 33.1 74 789301
BayesVAR(1 ) 11.3 33 3192
TC(1 ) 13.8 72 4036
( d ) The SVAR based on Hailfinder Network
TC(1 ) 14.4 110 6275
BayesVAR(1 ) 12.7 45 4524
VAR+PC(2 ) 49.2 112 5790843
GS(2 ) 16.5 48 4732
GS(2 ) 22.0 52 4873
GS(2 ) 24.5 61 6835
GS(2 ) 32.6 74 10480
TC(2 ) 17.7 69 3903
TC(2 ) 27.3 103 3593
TC(2 ) 26.8 109 5832
TC(2 ) 39.7 166 7831
BayesVAR(2 ) 14.7 26 2840
BayesVAR(2 ) 17.3 37 2317
BayesVAR(2 ) 17.8 36 4786
BayesVAR(2 ) 23.3 43 6146 network , and for each SVAR , the sample size varies from 128 , 256 , 512 , 1024 , 2048 , 4096 to 8192 . The results are summarized in Figure 4 . From the results , we can see that BayesVAR outperforms the other algorithms in comparison on all SVARs , and the GS algorithm performs better than the remaining two . The BayesVAR outperforms the GS algorithm for two reasons . The first one is that the BayesVAR uses partial correlation as heuristic when identifying Markov blankets while the GS uses a static procedure . The second reason is that BayesVAR employs ADJ information to restrict the search space of conditioning sets to subsets of Markov blankets . More importantly , we can see that the performance of BayesVAR does not decrease as the scale of the problem increases . For example , BayesVAR has higher F1 value on SVAR(1 ) corresponding to the Hailfinder network than that corresponding to the Insurance network . The reason is that BayesVAR searches the conditioning sets within Markov blankets and uses a heuristic Markov blanket learning algorithm to avoid high order CI tests when identifying Markov blankets from data . The VAR+PC and the TC perform worse for the reason that both of these two algorithms rely on high order CI tests when the scales of
821 the problems are large . The VAR+PC algorithm applies the VAR approach first , which regresses each variable in the current time on all the variables in the past k time for a SVAR(k ) . Therefore , the maximum size of conditioning sets is n × k for a n dimensional SVAR(k ) . The TC algorithm uses linear regression to identify the Markov blanket of each variable , and the size of the conditioning set in this case is n × ( k + 1 ) − 2 . Therefore , the F1 value of VAR+PC and TC drop dramatically on large networks when samples are limited . For example , on SVAR(1 ) based on the Hailfinder network and 128 samples available , there is a sudden drop of the F1 value of the TC algorithm . Table II provides a deep insight on the sizes of conditioning sets required by the algorithms in comparison when 8192 samples are available . From Table II , we can see that the maximum sizes of conditioning sets of VAR+PC and TC are consist with the theoretical analysis , and the average sizes of conditioning sets of these two algorithms tend to be larger than the other two . The BayesVAR has the smallest maximum size and average size of conditioning sets on every setting because it employs the Markov blanket information and the ADJ information to restrict the search space , which means that BayesVAR is more robust .
B . Application in Finance
We use the BayesVAR algorithm to find the causal relations among several world stock indices . The chosen indices are DJI in USA , FT S E in UK , N225 in Japan , HS I in Hong Kong , and the S S EC in China , and all the data are publicly available from Yahoo finance database . We use the daily dividend/split adjusted closing prices from 3rd January 2007 to 31st December 2010 and we choose the DJI trading date as the date basis . There are 1008 observations during this period . For the prices of the other indies not available in some DJI trading dates , we simply estimate the prices by linear interpolation . The analysis is based on daily return calculated by the following formula p(t − 1 ) p(t ) − p(t − 1 ) r(t ) = where p(t ) and p(t − 1 ) denote the adjusted daily closing price of the current trading date and previous trading date respectively , and r(t ) denotes the daily return of the current trading date .
The SVAR(1 ) and VAR(1 ) on the stock index data set are estimated by the BayesVAR algorithm . The structure of the SVAR(1 ) is illustrated in Figure 5 . The numbers on the edges are regression coefficients those denote the strength of the causal effects . The SVAR(1 ) in Figure 5 indicates that DJI has significant temporal causal effect on all the other indices , which is consistent with our knowledge . The FS T E has a temporal causal effect on DJI , which indicates the close connection between the markets of USA and UK . An interesting fact is that all the autoregression coefficients
822 for N225 , FT S E , and DJI are negative . Although there are other paths implies positive causal effects from the previous observation to the current one , the overall effects are still negative . For example , there are three pathes from N225t−1 to N225t , and the causal effect of each path equals to the production of the weights of the edges on that path . The causal effect of the path ( N225t−1 , HS It−1 , FT S Et−1 , DJIt−1 , N225t ) is 0.27× 0.3× 0.72× 0.25 ≈ 0.015 , and the summation of the weights of the three paths is −023 The contemporaneous causal order identified by BayesVAR is consist with the time difference among the markets in comparison , although the time difference information is not provided to BayesVAR . All the contemporaneous causal effect are positive , which indicates that the markets tend to move to the same direction in one day . The estimated VAR(1 ) is illustrated in Figure 5 . We can see that no information about the contemporaneous causal effect are explicitly illustrated by the VAR(1 ) . In addition , the temporal causal effects in the VAR(1 ) are quite different from those in the SVAR(1 ) . For example , there is no causal effect from FT S Et−1 to DJIt in the VAR(1 ) . Detailed discussion on the structures of VARs deviate from the true causal relations can be found in [ 19 ] .
VI . Concluding Remarks
In this work , we show that the VAR and the SVAR models are incapable of identifying contemporaneous causal order from Gaussian time series data and provide a solution to the problem based on Bayesian network learning . The BayesVAR algorithm proposed in this work is distinct from the existing works for the reason that the BayesVAR algorithm learns Bayesian networks directly from time series data and does not depend on the results of VAR models . By doing this , the BayesVAR algorithm avoids the highorder CI tests involved by estimating VARs from data . To overcome the difficulty that the sizes of the Bayesian networks corresponding to time series tend to be large , we propose the Markov2P algorithm to learn Bayesian networks locally . By restricting the search space of conditioning sets within subsets of Markov blankets , the orders of the CI tests do not grow fast with the scales of the problems . The Markov2P algorithm can be viewed as an enhanced version of both TPDA and GS . It improves TPDA in the sense that Markov2P provides a concise step to learn the approximate skeletons of Bayesian networks from data . It improves GS by employing both local and global features to reduce the search space of conditioning sets . To be more specific , the GS algorithm restricts the search space to Markov blankets , and the Markov2P algorithm shows that it is sufficient to consider the vertices in the intersection of ADJ sets and Markov blankets . The the Markov blanket information is local because it is related with a vertex and its neighbors , and the ADJ information is global because it is obtained only when the whole network is taken into account .
( a ) SVAR(1 )
( b ) VAR(1 )
Figure 5 . Estimated SVAR(1 ) and VAR(1 ) on the stock index data
Acknowledgment
The work described in this paper was partially supported by a grant from the Research Grants Council of the Hong Kong Special Administration Region , China .
References
[ 13 ] D . Margaritis and S . Thrun , “ Bayesian network induction via local neighborhoods , ” in Advances in Neural Information Processing Systems 12 . MIT Press , 1999 , pp . 505–511 .
[ 14 ] I . Tsamardinos , Brown , and A . Constantin , “ The max min hill climbing bayesian network structure learning algorithm , ” Machine Learning , vol . 65 , no . 1 , pp . 31–78 , October 2006 .
[ 1 ] S . Christopher A , “ Macroeconomics and reality , ” Economet rica , vol . 48 , no . 1 , pp . 1–48 , January 1980 .
[ 15 ] J P Pellet and A . Elisseeff , “ Using markov blankets for causal structure learning , ” J . Mach . Learn . Res . , vol . 9 , 2008 .
[ 2 ] C . A . Sims , “ Are forecasting models usable for policy anal ysis ? ” Quarterly Review , no . Win , pp . 2–16 , 1986 .
[ 16 ] Z . Wang and L . Chan , “ An efficient causal discovery algorithm for linear models , ” in SIGKDD , 2010 , pp . 1109–1118 .
[ 3 ] B . S . Bernanke , “ Alternative explanations of the moneyincome correlation , ” National Bureau of Economic Research , Inc , NBER Working Papers 1842 , Feb . 1986 .
[ 4 ] O . J . Blanchard and M . W . Watson , “ Are business cycles all alike ? ” National Bureau of Economic Research , NBER Working Paper 1392 , December 1987 .
[ 5 ] S . Demiralp and K . D . Hoover , “ Searching for the causal structure of a vector autoregression , ” Oxford Bulletin of Economics and Statistics , vol . 65 , no . s1 , pp . 745–767 , December 2003 .
[ 6 ] P . Chen and H . Chihying , “ Learning causal relations in multivariate time series data , ” Economics The Open Access , Open Assessment E Journal , vol . 1 , no . 11 , pp . 1–43 , 2007 .
[ 7 ] P . Spirtes , C . Glymour , and R . Scheines , “ From probability to causality , ” in Proc . of Advanced Computing for the Social Sciences , 1990 .
[ 8 ] J . Pearl and T . Verma , “ A theory of inferred causation , ” in Proc . of the Second Int . Conf . on Principles of Knowledge Representation and Reasoning , 1991 .
[ 9 ] J . Ramsey , J . Zhang , and P . Spirtes , “ Adjacency faithfulness and conservative causal inference , ” in UAI , 2006 .
[ 10 ] I . Tsamardinos , C . F . Aliferis , and E . Statnikov , “ Algorithms for large scale markov blanket discovery , ” in In The 16th International FLAIRS Conference , St , 2003 , pp . 376–380 .
[ 11 ] N . Swanson and C . Granger , “ Impulse response functions based on causal approach to residual orthogonalization in vector autoregressions , ” Tech . Rep . , 1994 .
[ 17 ] J . Cheng , R . Greiner , J . Kelly , D . A . Bell , and W . Liu , “ Learning bayesian networks from data : An informationtheory based approach , ” Artif . Intell . , vol . 137 , no . 1 2 , 2002 .
[ 18 ] S . Shimizu , P . O . Hoyer , A . Hyv¨arinen , and A . Kerminen , “ A linear non gaussian acyclic model for causal discovery , ” J . Mach . Learn . Res . , vol . 7 , 2006 .
[ 19 ] A . Hyvarinen , S . Shimizu , and P . Hoyer , “ Causal modelling combining instantaneous and lagged effects : an identifiable model based on non gaussianity , ” in ICML , 2008 , pp . 424– 431 .
[ 20 ] J . Richard and D . Wichern , Applied Multivariate Statistical
Analysis . New Jersey : Prentice Hall Inc , 2002 .
[ 21 ] C . Meek , “ Strong completeness and faithfulness in bayesian networks , ” in UAI , 1995 , pp . 411–418 .
[ 22 ] P . Spirtes , C . Glymour , and R . Scheines , Causation , Predic tion , and Search . Berlin : Springer Verlag , 1993 .
[ 23 ] J . Binder , D . Koller , S . Russell , and K . Kanazawa , “ Adaptive probabilistic networks with hidden variables , ” Machine Learning , vol . 29 , no . 2 , pp . 213 – 244 , 1997 .
[ 24 ] A . L . Jensen and F . V . Jensen , “ Midas : An influence diagram for management of mildew in winter wheat , ” in UAI , San Francisco , CA , USA , 1996 , pp . 349–356 .
[ 25 ] I . A . Beinlich , H . J . Suermondt , R . M . Chavez , and G . F . Cooper , “ The ALARM Monitoring System : A Case Study with Two Probabilistic Inference Techniques for Belief Networks , ” in Second European Conf . on Artif . Intell . in Medicine , vol . 38 , London , Great Britain , 1989 , pp . 247–256 .
[ 26 ] [ Online ] .
Available : compbio/Repository/ http://wwwcshujiacil/site//labs/
[ 12 ] P . Spirtes and C . Glymour , “ An algorithm for fast recovery of sparse causal graphs , ” Social Science Computer Review , vol . 9 , no . 1 , October 1991 .
[ 27 ] R . A . Fisher , “ Frequency distribution of the values of the correlation coefficient in samples from an indefinitely large population , ” Biometrika , vol . 10 , no . 4 , pp . 507–521 , 1915 .
823
