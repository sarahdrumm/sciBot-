2011 11th IEEE International Conference on Data Mining
Clusterability Analysis and Incremental Sampling for Nystr¨om Extension Based
Spectral Clustering
Xianchao Zhang School of Software
Dalian University of Technology
Dalian , China
Email : xczhang@dluteducn
Quanzeng You
School of Software
Dalian University of Technology
Dalian , China
Email : ultimateyou@gmail.com
Abstract—To alleviate the memory and computational burdens of spectral clustering for large scale problems , some kind of low rank matrix approximation is usually employed . Nystr¨om method is an efficient technique to generate lowrank matrix approximation and its most important aspect is sampling . The matrix approximation errors of several sampling schemes have been theoretically analyzed for a number of learning tasks . However , the impact of matrix approximation error on the clustering performance of spectral clustering has not been studied . In this paper , we firstly analyze the performance of Nystr¨om method in terms of clusterability , thus answer the impact of matrix approximation error on the clustering performance of spectral clustering . Our analysis immediately suggests an incremental sampling scheme for the Nystr¨om method based spectral clustering . Experimental results show that the proposed incremental sampling scheme outperforms existing sampling schemes on various clustering tasks and image segmentation applications , and its efficiency is comparable with existing sampling schemes .
Keywords spectral clustering ; Nystr¨om extension ; incremen tal sampling
I . INTRODUCTION
In recent years , spectral clustering has become one of the most popular modern clustering algorithms since it often outperforms traditional clustering algorithms such as the kmeans algorithm in many clustering tasks . Nevertheless , it has limited applicability to large scale problems due to its computational complexity of O(n3 ) and space complexity of O(n2 ) , with n the number of data points . To alleviate the memory and computational burdens of spectral clustering , some kind of low rank matrix approximation is usually employed .
Nystr¨om method is an efficient technique to generate low rank matrix approximations which has been used in a number of supervised/unsupervised learning tasks . In using Nystr¨om method , firstly m landmark points are picked from n instances , then , the eigen decomposition of an n×n matrix can be approximated by using a much smaller matrix of m×m . Since different samplings produce different approximation results , the most important aspect of Nystr¨om method is sampling . Random sampling is the most commonly used
1550 4786/11 $26.00 © 2011 IEEE DOI 101109/ICDM201135
942 sampling scheme for Nystr¨om method . Recently , some research on how to lower the matrix approximation error has been conducted . Drineas & Mahoney [ 5 ] proposed a columnsampling method , and revealed a probabilistic error bound of the sampling scheme . Zhang et al . [ 20 ] provided a more concrete analysis of the Nystr¨om low rank approximation and proposed a k means based sampling scheme . A concrete comparison of uniform versus non uniform sampling was presented by Kumar et al . [ 9 ] , they also gave a performance bound for Nystr¨om method with uniform sampling without replacement . Belabbas & Wolfe [ 1 ] proposed two alternative sampling strategies for Nystr¨om method , one employed a randomized approach and the other one provided a much deterministic way to select landmark points .
Existing sampling schemes are designed and analyzed for lowering the error of the matrix approximation and applied to general kernel based learning tasks . As to spectral clustering , though better matrix approximation implies better clustering result , the impact of matrix approximation error on the clustering performance is not clear . Note that in spectral clustering , the clusters are calculated with the top k eigenvectors Vk of the similarity matrix . While using the Nystr¨om method , the Vk was approximated by the top k eigenvectors ˆVk calculated with the sampled matrix . This kind of eigenvector permutation causes loss of clustering accuracy and it is expected that the clusterability [ 7 ] of Vk and ˆVk should be similar to achieve accurate clustering results .
In this paper , we first analyze the difference of clusterabilities between Vk and ˆVk , which is characterized by the canonical angle between Vk and ˆVk . We give a formula to calculate the canonical angle between Vk and ˆVk , which shows that the difference of clusterabilities between Vk and ˆVk is bounded by the Schur complement of the approximated similarity matrix . In this way , the impact of the matrix approximation error on the clustering performance is answered . Next , we exploit the properties of Schur complement to analyze the matrix approximation error . The analysis result shows that when adding a new landmark point to p fixed landmark points , the matrix approximation error of the p + 1 landmark points could be calculated with the matrix approximation error of the p landmark points . This immediately suggests an incremental sampling scheme for Nystr¨om extension based spectral clustering . The incremental sampling starts with a fixed number of initial landmark points and selects new landmark points one by one such that the estimated matrix approximation error is minimized , until a desired number of landmark points is reached . The proposed incremental sampling ( IS ) scheme preponderates over existing sampling schemes such as random sampling ( RS ) , k means based sampling ( KS ) and weighted sampling(WS ) in the following aspects . ( 1 ) The number of landmark points could be controlled by the expected approximation error since IS picks the landmark points incrementally . ( 2 ) IS exploits only the similarities between sampled points and the unsampled points , without considering representations of data attributes , thus it could easily be applied to applications such as image segmentation where other schemes like k means based sampling can not by applied . ( 3 ) Most importantly , IS results in higher clustering qualities than existing sampling schemes ; this is verified with various experiments on tasks of clustering synthetic data sets , clustering UCI data sets and segmentation of images from the Berkeley Segmentation Dataset and Benchmark . Our experiments also show that the efficiency of IS is higher than WS and is comparable with RS and KS . The rest of the paper is organized as follows : some related work is discussed in Section II . In Section III , we propose a brief introduction of Nystr¨om extension and its application to spectral clustering . An error analysis of the approximated eigenvectors will be proposed in Section IV , and an incremental sampling scheme will also be proposed . We evaluate the proposed method in Section V . Finally , we conclude our work in Section VI .
II . RELATED WORK
Spectral clustering , which is derived from spectral graph theory , has shown a wide range of applicabilities such as image segmentation , speech separation and so on . Different approaches of normalizing the similarity matrix W corresponding to different objectives of spectral clustering . In this paper , we are mainly concerned with the NCut [ 13 ] , where the . −1/2 affinity matrix W is normalized with ¯W = D where di = j W ( i , j ) and D = diag(d1 , . . . , dn ) . For a complete discussion of different normalized steps and cut criteria , see[17 ] therein .
−1/2W D
Applying spectral clustering to large scale problems has been a hot topic recently . Employing sparse strategy together with advanced eigen decomposition techniques such as Lanczos , the clustering procedure can be significantly accelerated . But [ 2 ] has shown the inferiority of the approach . A t nearest neighbor based sparse strategy is proposed in [ 15 ] , but substantial time will be spent in constructing the similarity matrix .
943
Another class of strategies for tackling the complexity problem stems from Nystr¨om extension , which is originally designed for numerical solution of integral equations . The Nystr¨om method was employed to construct a low rank approximation to a Gram matrix in [ 18 ] . The success of their work made Nystr¨om method a popular tool for matrix approximation . In [ 2 ] , the authors addressed the normalization and orthogonality problems and successfully applied Nystr¨om extension to spectral clustering .
The landmark points sampling is crucial for the performances of Nystr¨om based machine learning algorithms . Drineas et al . proposed the diagonal sampling and the column norm sampling methods in [ 5 ] and [ 4 ] respectively . The probability of the i th row being sampled is propositional to Wii or the L2 norm of the column . In spectral clustering Wii equals to 1 for every i , thus making the sampling method infeasible in the spectral clustering settings . The column norm sampling requires O(n2 ) time and space , therefore is unsuitable for large scale problems . Furthermore , Kumar et al . [ 10 ] showed that the two methods are even inferior to the random sampling . They also provided the performance bounds for different sampling methods , their work mainly involved the problem of low rank approximation .
A k means based sampling method was introduced by Zhang et al . [ 20 ] . Their analysis revealed that the Nystr¨om approximation error was mostly affected by the quantization error(error of quantizing each data point with the closest landmark point ) . As k means is known to be capable of giving a local minimum of the quantization error , thus the centers obtained from k means can be designated as the landmark points . However , they assumed the kernel K should satisfied Kij−Kst ≤ C(fi xi−xj fi2 + fi xs−xt fi2 ) which could limit the applicability of the algorithm , eg , it is not applicable to image segmentation . In addition , k means will add new data points into the data set , which may cause unsteady results .
Belabbas & Wolfe[1 ] proposed two other approaches by applying the determinant of the kernel matrix to select landmark points . The first approach was a randomized one . Let I denote the set of indices of the landmark points , then the probability of choosing I was in proportion to the determinant of the similarity matrix between landmark points . They used the Schur complement to analyze the Nystr¨om reconstruction error , and concluded that the bigger the determinant , the smaller the error was . Let n be the number of total data samples and m be the number of landmark points , there will be a total of candidates for I . To circumvent this heavy computational burden , they employed a simulation algorithm to choose the desired I . The second method chose I such that I contains the indices of the k largest diagonal elements of the approximated matrix . This method cannot be used in spectral clustering , where all the diagonal elements are equal to 1 . Actually our n m fi
' analysis is also based on the Schur complement , but we use Schur complement in a different way , the results will be shown in the following sections .
III . PROBLEM DEFINITION
In this section , a brief introduction of Nystr¨om extension is presented , and its application to spectral clustering is reviewed .
A . The Nystr¨om Extension
The Nystr¨om method is originally used to solve the eigenfuntion of integral equations of the form ff
K(x , y)φ(y)p(y)dy = λφ(x )
( 1 ) matrix between all the data points , without loss of generality , rearrange the points such that the landmark points come first . In this way , W can be written in a block matrix form
(
W =
( 5 ) where A ∈ R(n−m)×(n−m ) is the similarity matrix between the unsampled points .
S E ET A
Using the −1 [ US ; ET USΛ S ] , estimated as approximated it is shown in [ 2 ]
ˆU
= that W can be eigenvectors (
ˆW = ˆU ΛS ˆU
T
=
E S −1E ET ET S
.
( 6 ) where p(y ) indicates the underlying probability density function , φ(x ) represents the eigenfunction and K(x , y ) denotes the similarity between x and y . It is applied to speed up kernel machines in [ 18 ] . To start with , one need to choose m landmark points Z = {z1 , z2 , . . . , zm} from the given data sets X = {x1 , x2 , . . . , xn} with xi ∈ Rd ( m ff n ) . For any given point x in X , using Nystr¨om method we have ,
K(x , zi ) ˆφ(zi ) = λ ˆφ(x )
( 2 ) m i=1
1 m where ˆφ(x ) is an approximation to the true φ(x ) . Eq ( 2 ) cannot be solved directly , as ˆφ(x ) and λ are both unknown . Denote the similarity matrix between the landmark points by S with Sij = K(zi , zj ) . If we substitute x with zi in Eq ( 2 ) , and write it in matrix form S ˆΦ = m ˆΦΛ
( 3 ) where Φ = [ ˆφ1 ˆφ2 ··· ˆφm ] are the eigenvectors of S and Λ = diag{ˆλ1 , ˆλ2,··· , ˆλm} is a diagonal matrix . For an unsampled point x , the j th eigenfunction at x can be approximated as
ˆφj(x ) =
1 mˆλi m i=1
K(x , zi ) ˆφj(zi ) .
( 4 )
With the above equation , the eigenvector for any given point x can be approximated by the eigenvectors of the landmark points Z .
B . Applied to Spectral Clustering
In spectral clustering , the eigenvectors are required to be orthogonal to each other . Therefore , how to orthogonalize the approximated eigenvectors is key to applying Nystr¨om method to spectral clustering .
Let S = USΛSU T
S be the eigen decomposition of S , E denote the similarity matrix between sample points and the remaining points , with E ∈ Rm×(n−m ) . Then Eq ( 4 ) can −1 be written in matrix form as ET USΛ S , which signifies the affinities between the unsampled points are no longer needed to be computed . Denote W ∈ Rn×n be the similarity
Recall that , to calculate the NCut [ 13 ] , the similarity matrix is required to be normalized . We need to know the row sums of W to acquire D . Without knowing A , D can be estimated through the row sums of ˆW . Depending on the definiteness of S , two different approaches are proposed [ 2 ] .
1 ) When S is positive definite : When matrix S is positive definite , all the eigenvalues of matrix S are positive and −1/2 is defined . The normalized approximated eigenvectors S are obtained by
(
−1/2
S ET
ˆV = ( 7 ) S −1/2EET S −1/2 with eigen decomposition
UBΛ
−1/2 B where B = S +S UBΛBU T B .
2 ) When S is a non positive matrix : Let Z = ˆU Λ1/2 , so that ˆW = ZZ T . Let F ΣF T denote the diagonalization of −1/2 , it is easy to verify that Z T Z . Then define V = ZF Σ ˆW = V ΣV T and V T V = I .
IV . ERROR ANALYSIS OF APPROXIMATED
EIGENVECTORS
In this section , we give an error analysis of the approximated eigenvectors , then an incremental sampling scheme based on the variance of E is presented . Before we introduce the sampling scheme , a toy example is employed to explain the main idea of the proposed algorithm .
A . A Motivating Example
A very simple example is used to explain our sampling scheme . Assume that we are given a data set X = {x1 , x2 , . . . , x10} , and these data points belong to four different clusters . Specifically , we assume {x1 , x2 , x3} ∈ C1 , {x4 , x5 , x6} ∈ C2 , {x7 , x8} ∈ C3 and {x9 , x10} ∈ C4 . Let I i be an i× i matrix with all entries equal to 1 . Then in the ideal case , where the intra cluster similarities are 1 and the inter cluster similarities are 0 , the similarity matrix for X is
⎡ ⎢⎢⎣
I 3 0 0 0
0 I 3 0 0
0 0 I 2 0
0 0 0 I 2
⎤ ⎥⎥⎦ .
( 8 )
W =
944
Now , suppose four landmark points {z1 , z2 , z3 , z4} are chosen from the four clusters respectively . The similarity matrix formed by the landmark points would be an 4 × 4 identity matrix . The i th eigenvector of an identity matrix would be a vector ei with eii = 1 and eij = 0 for j = i . The Nystr¨om method discussed in Section III can successfully embed the points of the same cluster in X into a same point , and points of different clusters will be projected to different points . Thus it is trivial for k means ( used as the final step in spectral clustering ) to give the correct clustering results . However , if the chosen four landmark points comes from only two different clusters , for example z1 , z2 ∈ C1 and z3 , z4 ∈ C2 , spectral clustering using Nystr¨om extension will fail to cluster C3 and C4 as the points in C3 ∪ C4 will be embedded into the same point . In other words the points in C3 ∪ C4 are not well represented by the landmark points . The above phenomenon suggests that the landmark points should be chosen in such a way that all the points should be represented by the landmark points appropriately . Continuing with the above toy example , if we have chosen Z3 = {z1 , z2 , z3} from C1 , C2 , C3 respectively . In order to achieve the correct clustering result , we want to choose the next sample z4 such that z4 ∈ C4 . Denote ax as a similarity vector whose elements are the similarities between a point x and all the sampled landmark points . Note that for any point x ∈ Ci ( i = 1 , 2 , 3 ) , the similarity matrix between x and Z3 would be ax = ei , while for x ∈ C4 , the similarity vector is ax = [ 0 0 0]T . If we view ax as the observed values of some variable y , for x ∈ C4 , the variance of y is var(y ) = 0 , and for x /∈ C4 var(y ) = 2 9 . Hence if we want to choose z4 such that z4 ∈ C4 , we can calculate the variance as above , the point with the minimum variance should be our best choice .
B . Theoretical Analysis
As discussed in previous sections , to spectral clustering , though better matrix approximation implies better clustering result , the impact of matrix approximation error on clustering performance is not clear . It was noted in [ 8][7 ] , the perturbation in the similarity matrix differ greatly with the perturbation of eigenvectors . In [ 7 ] , the term clusterability is used to measure the clustering performance of a set of given eigenvectors . If two sets of eigenvectors have a small clusterability difference , their clustering results are similar . Let Vk and ˆVk represents the top k eigenvectors of W and ˆW respectively , the clusterability difference of Vk and ˆVk cannot be simply characterized by the norm fi Vk− ˆVk fiF . Actually as shown in [ 7 ] , the clusterability difference is closely related to the canonical angles , which is defined as below . Definition 1 . Let λ1 ≤ λ2 ··· ≤ λn be the singular values of Vk ˆVk . Then θi = arccos λi are the canonical angles between Vk and ˆVk . Let Θ = diag(θ1 , θ2 , . . . , θk ) be the diagonal matrix of canonical angles between Vk and ˆVk .
Given the above definition , the clusterability difference between Vk and ˆVk is determined by the canonical angle , ie , a small value of canonical angle represents a small clusterability difference between Vk and ˆVk . sin Θ is used to represents the clusterability difference between Vk and ˆVk . The following lemma ( which is also known as sin Θ theorem ) gives the relationship between the perturbed eigenvectors and the canonical angles . [ 3 ] Denote Vk = [ v1 , v2,··· , vk ] and ˆVk = Lemma 1 . [ ˆv1 , ˆv2 , . . . , ˆvk ] , with {λi , vi|λ1 ≥ λ2 ≥ ··· ≥ λn , i = 1 , . . . , n} and {ˆλi , ˆvi|ˆλ1 ≥ ˆλ2 ≥ ··· ≥ ˆλn , i = 1 , . . . , n} be the eigensystem of W and ˆW respectively . If |ˆλk−λk+1| ≥ α and ˆλk ≥ α for some α > 0 , then fi sin Θ fiF≤ 1 α fi W ˆVk − ˆVk ˆΣk fiF
( 9 )
Applying Lemma 1 to Nystr¨om method based spectral clustering , we can give an upper bound of fi sin Θ fi in the following theorem . Theorem 1 . Given the definition above , if |ˆλk − λk+1| ≥ α and ˆλk ≥ α for some α > 0 , then fi sin Θ fiF≤ fi A − E
T
−1
E fiF
S
( 10 )
√ k α
Proof : As the column of ˆVk and the diagonal elements of ˆΣk are the top k eigenvectors corresponding eigenvalues of ˆW . We have ˆW ˆVk = ˆVkΣk , therefore fi W ˆVk − ˆVk ˆΣk fiF = fi W ˆVk − ˆW ˆVk fiF
≤ fi W − ˆW fiFfi ˆVk fiF = fi A − ˆA fiFfi ˆVk fiF = fi A − E ffl
−1
S
T
√
E fiFfi ˆVk fiF
According to the orthogonality of ˆVk , it is straightforward to have fi ˆVk fiF =
( 11 ) where T r(· ) denote the trace of a matrix . Substitute the above results into Eq ( 9 ) will conclude the proof .
ˆV T k ˆVk
T r
= k
In Theorem 1 , the conclusion holds on the condition that columns of ˆVk are orthogonal to each other , this is satisfied for spectral clustering . 1 . The term A− ET S −1E is referred to as Schur complement . In this way , the performance of the Nystr¨om method based spectral clustering is related to Schur complement . Let WI×J ( use WI as the abbreviation of WI×I ) be a s × t matrix whose ( i , j) th entry is given by ( WI×J )ij = WIiJj , where I = {I1 , I2 , . . . , Is} , J = {J1 , J2 , . . . , Jt} are the indices of W . Then S = WI , the ( i , j) th entry of Schur complement can be given by the following lemma .
1In other low rank approximations , this condition doesn’t necessarily hold .
945
Lemma 2 . [ 1 ] Given Schur complement SC(WI ) = A − −1E , then the entry wise of Schur complement can be ET S is given by
SCij(WI ) = det(WI∪{i}×I∪{j} ) det(WI )
( 12 )
According to Lemma 2 , a sampling method based on the determinant of S is proposed in [ 1 ] . In the following , we will analyze the Schur complement in different view , which leads to another sampling strategy . Our sampling method behaves in an incremental way . Assume that we have picked p landmark points from the whole data set , we want to choose the ( p + 1) th landmark point such that the approximation error will be substantially reduced . Then we can repeat the strategy , until we reach a total of m landmark points . Denote the set of indices of the sampled landmark points as I , and suppose that the index of the next chosen point is k , then we have
( SCij(WI∪{k} ) ) = det(WI∪{k,i}×I∪{k,j} ) det(WI∪{k} )
.
( 13 ) to p fixed landmark points ,
The following theorem shows that when adding a new landmark point the matrix approximation error of the p + 1 landmark points could be calculated with the matrix approximation error of the p landmark points . Theorem 2 . Given the Schur complement and the matrix W , if det(WI∪{i}×I{j} ) = 0 and det(WI ) = 0 , then ∃α , β ∈ R(p+1 ) and γ ∈ Rp , such that
SCij(WI∪{k} ) = SCij(WI )
αT W T
I∪{i}×I∪{j}β + 1 γT WI γ + 1
( 14 )
Proof : Let Δn represent the numerator of Eq ( 13 ) , and Δd be the denominator of Eq ( 13 ) . Using some algebra techniques , Δn can be written as ( in the following section we denote p = |I| ) , let k be the index of the ( p + 1) th landmark point , ( −1 )
WI∪{k,i}\s×I∪{j}
Wsk det
Δn =
( s+p+1 ) p+2 p+1 s=1 p+1 t=1
Wsk
( s+p+1 )
( −1 )
( −1 )
WI∪{i}\s×I∪{j}\t p+1
WI∪{i}×I∪{j} ij WI∪{i}×I∪{j} wj + det
WskWtk det
( −1 ) t=1 s+t
=
= s=1 det p+1 s=1 + det T i ¯W
= w
+ Wkk det
( t+p+1 )
Wtk
WI∪{i}×I∪{j}
WI∪{i}\s×I∪{j}\t
, where ¯W ij is the cofactor matrix of WI∪{i}×I∪{j} , wi , wj ∈ Rp+1 with wm = [ WI1k , WI2k , . . . , WIpk , Wmk ] , m = i , j . If det(WI∪{i}×I∪{j} ) = 0 , then columns of WI∪{i}×I∪{j} are linearly independent vectors(the same is true for the rows ) . Therefore , wj is in the range of WI∪{i}×I∪{j} , which is equal to
∃ β ∈ R p+1 st wj = WI∪{i}×I∪{j}β .
Then , one can easily verify that
T i ¯W w ij wj = det
= det
WI∪{i}×I∪{j} WI∪{i}×I∪{j}
T i β w T WI∪{i}×I∪{j}β ,
α
I∪{i}×I∪{j} and wi = W T where similarly , in the above deduction , we assume wi is in the range of W T I∪{i}×I∪{j}α . In the above equation , we use the fact that ¯W ijWI∪{i}×I∪{j} = det(WI∪{i}×I∪{j})I(p ) ( here I(p ) is an Rp×p identity matrix ) . Similarly , the denominator can be reformulated as
Δd = det(WI∪{k}×I∪{k} ) p+1 p s=1
( −1 ) p
=
= s+k
Wsk det
WI∪{k}\s×I
( −1 ) s+t
WskWtk det
WI\s×I\t t=1 s=1 + det(WI )
= w
I
T I ¯W wI + det ( WI ) where w = [ WI1k , WI2k . . . WIpk ] and ¯W I ∈ Rp×p is the cofactor matrix of WI . Similarly , according to the assumption det(WI ) = 0 , we have wI is in the range of WI with wI = WI γ then we reach the following equation
Δd = w
I
T I ¯W wI + det ( WI )
= det ( WI ) γ
T
WI γ + det ( WI ) ,
From the above results , we conclude that Eq ( 14 ) hold .
C . Incremental Sampling Algorithm Based on Variance Let C = γT WI γ ( note that WI is semi definite since W is semi definite in spectral clustering , thus C ≥ 0 ) , if we choose the next landmark point zp+1 such that C is large , then the overall error of Schur complement will be reduced . Denote {vi , λi|λ1 ≤ λ2 ≤ ··· ≤ λp , i = 1 , . . . , p} to be the eigen decomposition of WI , according to Theorem 2 , if det(WI ) = 0 , then Vp = {v1 , . . . , vp} will be the orthogonal basis of vector space Rp . Hence , we can rewrite γ as the linear combination of Vp ,
γ = a1v1 + a2v2 + ··· + apvp
( 15 ) where ai ∈ R is the correlation coefficient . Under this condition , we have
T
C = γ
WI γ = a
If we fix fiγfi2 =
.
2λ2 + ··· + a
2
2 1λ1 + a i , then larger a2 i a2 p leads to larger C . In other words , γ should be aligned to the biggest eigenvector of WI . Note that , as shown in Section III , in spectral clustering the similarity matrix WI should be normalized .
2 pλp .
( 16 )
946
The largest eigenvector of the normalized similarity matrix . ¯W is D1/21 [ 17 ] . Let DI ∈ Rp×p be a diagonal matrix with j∈I Wij(row sums of the landmark points ) , and ( DI )ii = E ∈ Rp×p be the diagonal matrix with Eii = j Wij(row sums of the whole data points ) . The largest eigenvector of −1/2 1 . However , in Nystr¨om extenI WI D D −1/2 , and sion , WI is normalized with ¯WI = F we have −1/2
−1/2WI F
−1/2 I
−1/2 I
. is D
−1/2
−1/2
F
WI F
F
1/21 = F = F
= F
−1 I WI F −1 I WI 1
−1/2 −1/2 −1/2
DI D
DI D DI 1
F
1/21 ( 17 )
−1/2WI F where F = DI is a subset of D only containing the selected indices . And if we further assume that DI ii almost equals for every i and the same is true for Fii . Under −1/2 this assumption , the largest eigenvector of F ( according to Eqn.(17 ) ) almost equal everywhere . In other words , the largest eigenvector is well aligned to 1 . Therefore , to attain a smaller SC error , we require that γ ≈ c1 ( c is constant scalar ) , in which case the coefficient vector γ is well aligned to 1 . Moreover , according to the assumptions above , wI = ¯WI γ is also required to be almost constant everywhere . This can be characterized by the variance of variable Wij . Thus , we can choose zp+1 in such a way that the variance of {WI1,Ip+1 , WI2,Ip+1 , . . . , WIp,Ip+1} is the smallest . The above idea is summarized in Algorithm 1 .
Algorithm 1 Incremental Sampling Based on Variance Input : X = {x1 , x2 , . . . , xn} : data sets m : number of landmark points Output : I = {i1 , i2 , . . . , im} the indices of sampled points S ∈ Rm×m : similarity matrix between landmark E ∈ Rm×(n−m ) : similarity matrix between land points mark points and remaining points
1 : Randomly chooses 2 points from the X , and add indices into I
2 : Calculate E the similarity matrix between the chosen points and remaining points
3 : Calculate S the similarity matrix of the chosen points 4 : Calculate s ∈ R(n−p ) , the column variance of E 5 : while |I| < m do
6 : 7 :
8 :
Find smin = min s , and add its index p into I Calculate similarity between xp and the remaining points , update E such that E ∈ R(|I|)×(n−|I| ) Update S to add the similarity between xp and other samples Calculate s ∈ R(n−p ) , the column variance of E
9 : 10 : end while
Firstly the sampling algorithm randomly choose two
947 points 2 from data set X . Then it computes the similarity matrix E between the sampled points and the remaining points . The point with the smallest variance will be picked . Repeat the procedure , until a total of m landmark points are sampled . In terms of computing similarity matrix , during each iteration only the similarities between the newly sampled point zp+1 and the remaining points need to be computed . And the results can be stored for future use(in the next iteration and in the Nystr¨om extension ) .
The total time complexity of Nystr¨om extension is O(m3)+O(nm2 ) , with O(m3 ) for the eigen decomposition of S and O(nm2 ) for Nystr¨om extension and orthogonalization of the approximated eigenvectors . The memory usage for Nystr¨om based spectral clustering is O(nm ) , mainly used for the storage of the similarity matrix . In Algorithm 1 , the additional expense mainly lies in the calculation of variance of E . The calculation of variance will give an additional O(nm2 ) operation , which is in the same order of magnitude with the random sampling based Nystr¨om method . To reduce the computational burden , when the number of landmark points m or the total number of data points n is too large , we only need to randomly choose t ( t ff m and is constant ) columns from E during each iteration , and calculate the variance of these t columns , instead of the whole E . Then , we choose the next point zp+1 from the t candidate points . With this strategy , the total cost for the proposed algorithm will be reduced to O(tmn ) . to apply spectral clustering . For instance ,
The proposed sampling algorithm chooses landmark points according to their similarities . This greatly improves the applicability of the algorithm . As similarity is essential in image segmentation , the data points only consist of the pixel values , however the similarity may take texture , position and other factors into consideration . Therefore , k means based sampling strategy may not be an appropriate approach to select landmark points .
V . EXPERIMENTAL RESULTS
We compare our incremental sampling(IS ) with random sampling(RS ) , the k means sampling(KS ) [ 20 ] , and the weighted sampling(WS ) [ 1 ] . To make KS more accurate when applying to spectral clustering algorithm , we also add the orthogonal operation .
The normalized mutual information(NMI)[16 ] is used to evaluate the performance of the four schemes for data clustering . Given two clustering results , Δ = {C1 , C2 , . . . , Cc} k} of X(|X| = n ) , let ni and n ' ' ' and Δ i ' i separately . Let be the number of objects in cluster Ci and C nst denote the number of objects that are in cluster Cs as
' 2 , . . . , C
= {C
' 1 , C
2Here , the initialization of the first two points may influence the performance of the algorithm . However , the results in our experiments are superior compared with other algorithms without using any heuristic method for initialization . well as in cluster C of Δ and Δ
' t , then the normalized mutual information
' is ffl.c
.c
.k s=1 t=1 log
.k
NMI = s=1 ns log ns n nnst nsn . t t=1 n
.
( 18 )
' t log n . t n
Given the true labels Γ of X and a clustering result Δ , we can calculate the NMI between Γ and Δ . The larger the NMI , the better the clustering result Δ when considering the true labels .
To give a more accurate evaluation of the algorithms , for a given number of landmark points , we repeat the experiment 50 times for each algorithm , and the averaged NMI is reported . The parameter configuration for all the four algorithms are the same . Moreover , in KS , as pointed in [ 20 ] , the k means algorithm doesn’t need to converge . For simplicity , we set the maximum iterations of k means to be 10 , which is the default setting in [ 20 ] . Since the number of sampling candidates of WS is , Metropolis algorithm is employed to reduce the complexity . To make a balance between the performance and the time complexity , we use the same setting as in [ 1 ] . The number of iterations is set to the order of 50m(m is the number of landmark points ) . n m fi
'
A . Clustering Results on Synthetic Data Sets
We compare the clustering results on three synthetic data sets as shown in Figure 1 , which are widely used to evaluate the performance of different clustering algorithms . And as shown in [ 19 ] , spectral clustering algorithm is capable of giving the correct clustering results with a proper similarity matrix . The similarity between point xi and xj is defined using the Gaussian kernel function fi −fi xi − xj fi2
'
2σ2
Wi,j = exp
.
( 19 )
The choice of σ has a significant impact on the performance of spectral clustering[13][19 ] . How to choose an optimal σ for each data set is out of the scope of this paper . In our implementation , we firstly choose a feasible σ such that spectral clustering will give the optimal clustering results . Then , the Nystr¨om method based spectral clustering will use the same σ in the subsequent experiments .
Figure 1 . Three Synthetic Data Sets . From left to right : Three Circles , Half Circle and Two Points , One Circle and Two Points . Different clusters are marked with different colors and symbols .
948
SUMMARY OF FOUR UCI BENCHMARK DATA SETS
Table I
Data Set Iris Wine Glass Ecoli
# of Instances
150 178 214 336
# of Attributes
4 13 9 7
# of Classes
3 3 7 8
The clustering quality comparison of different sampling schemes is shown in Figure 2 with the number of samples increased . Different data sets require different number of landmark points to give acceptable results . For instance , the One Circle and Two Points data set is more challenging and requires more landmark points to give satisfying result . On the other hand , Half Circle and Two Points seems to be easy to cluster . Only 15 points are needed to give correct clustering for the IS and KS . Overall , it is shown that Nystr¨om extension based algorithms exhibit attractive clustering results , even when the number of landmark points is far more less than the total number of data points . In the Three Circles data set , our algorithm(IS ) consistently outperforms the other three algorithms . The results of IS and the KS are comparable on Half Circle and Two Points , and both of them show better performance than the other two sampling schemes . IS also gives significantly better performance on the One Circle and Two Points data set than the other three algorithms .
The time consumption of the four sampling schemes corresponding to the results in Figure 2 is shown in Figure 3 . The WS algorithm consumes much more time compared with other three algorithms . During each iteration , the algorithm needs to calculate the determinant of a m × m matrix , which is the main reason of a such expensive time consumption . The other three algorithms show comparable results . IS and KS consume a slightly higher time than RS .
B . Clustering results on UCI data sets
We also compare the performance of the four sampling schemes on some UCI benchmark data sets . The similarity matrix is constructed according to Eq ( 19 ) . We choose four different data sets namely Iris , Wine , Glass and Ecoli from the UCI Machine Learning Repository . A brief summary of the four data sets is listed in Table I .
The clustering quality results on the UCI data sets are shown in Figure 4 . The results reveal that IS gives much better clustering results than other three algorithms on all the four data sets . In Figure 4(c ) , the NMI value stays in the range of [ 0.33 , 0.36 ] , even though we increase the samples from 10 to 50 . This indicates that the Glass data set is hard for Nystr¨om extension based spectral clustering , but IS also gives a more stable and a better performance than the other three schemes .
The time consumption of the four schemes is shown in Figure 5 . The behavior of the algorithms is similar to their
IS KS WS RS
18
24
30
36
42
48
Number of Landmark Points ( a ) Three Circles
1
0.9
0.8
0.7
0.6
0.5
0.4 l e u a v
I
M N
54
60
6
9
0.65
0.6
0.55
0.5
0.45 l e u a v
I
M N
IS KS WS RS
21
24
0.4
10
15
20
IS KS WS RS
50
55
60
0.9
0.8
0.7
0.6
0.5
0.4
0.3 l e u a v
I
M N
10
8
6
4
2
) s d n o c e s ( e m i t
U P C
6
12
6
12
12
15
18
Number of Landmark Points
( b ) Half Circle and Two Points
25
30
35
40
45
Number of Landmark Points
( c ) One Circle and Two Points
Figure 2 . Clustering results of three different sampling strategy on different synthetic data sets .
IS KS WS RS
IS KS WS RS
1.4
1.2
1
0.8
0.6
0.4
0.2
) s d n o c e s ( e m i t
U P C
IS KS WS RS
) s d n o c e s ( e m i t
U P C
14
12
10
8
6
4
2
18
24
30
36
42
48
54
60
6
9
12
15
18
21
24
10
15
20
25
30
35
40
45
50
55
60
Number of Landmark Points ( a ) Three Circles
Number of Landmark Points
( b ) Half Circle and Two Points
Number of Landmark Points
( c ) One Circle and Two Points
Figure 3 . CPU time comparision of different sampling algorithms on different synthetic data sets .
0.78
0.77
0.76
0.75
0.74
0.73 l e u a v
I
M N
0.72
6
7
8
0.36
0.355
0.35
0.345
0.34
0.335
0.33
0.325 l e u a v
I
M N
0.32
10
15
20 l e u a v
I
M N
0.42
0.4
0.38
0.36
0.34
0.32
0.3
0.28
0.26
0.24
IS KS WS RS
13
14
15
5
7
9
11
9 12 Number of Landmark Points
10
11
15
13 21 Number of Landmark Points
17
19
( a ) Iris
( b ) Wine
0.68
0.66
0.64
0.62
0.6
0.58
0.56
0.54
0.52
0.5 l e u a v
I
M N
IS KS WS RS
IS KS WS RS
23
25
27
29
IS KS WS RS
25
30
35
40
45
50
Number of Landmark Points
0.48
10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100
Number of Landmark Points
( c ) Glass
( d ) Ecoli
Figure 4 . Clustering results on four UCI data sets . performance in synthetic data sets . The WS consumes much more time compared with other three algorithms . All other three algorithms gives similar time consumption .
C . Image Segmentation
In this section , we compare the performance of the sampling schemes when applying to image segmentation . We
949
6
7
8
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
) s d n o c e s ( e m i t
U P C
8
7
6
5
4
3
2
1
) s d n o c e s ( e m i t
U P C
0 10
15
20
IS KS WS RS
2
1.8
1.6
1.4
1.2
1
0.8
0.6
0.4
0.2
) s d n o c e s ( e m i t
U P C
13
14
15
5
7
9
11
9 12 Number of Landmark Points
10
11
IS KS WS RS
15
13 21 Number of Landmark Points
17
19
( a ) Iris
( b ) Wine
IS KS WS RS
60
50
40
30
20
10
) s d n o c e s ( e m i t
U P C
23
25
27
29
IS KS WS RS
25
30
Number of Landmark Points
35
40
45
50
0 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100
Number of Landmark Points
( c ) Glass
( d ) Ecoli
Figure 5 . CPU time comparison of different algorithms on UCI data sets . use the images form the Berkeley Segmentation Dataset[11 ] . Each image contains 154401 pixels(either 481 × 321 or 321×481 ) . Thus , spectral clustering using the complete similarity matrix is infeasible . For the construction of similarity matrix between pixels , we apply the χ2 distance described in[2 ] , which is defined as Q
2
( 20 )
2 ij =
χ
1 2
( hi(q ) − hj(q ) ) hi(q ) + hj(q ) q=1 where hi is histograms of point i and Q is the number of colors considered . In our experiment , the histogram of pixel i is computed within a 7× 7 pixel window by firstly applying the color quantization scheme described in[14 ] . Given the χ2 distance between pixels i and j , the similarity can be defined as Wij = exp(−χ2 ) , which is proven to be positive definite[2 ] .
We use the F measure to evaluate the image segmentation results [ 12][6 ] . There exists several human segmentations for each image in the Berkeley Segmentation Dataset . We use the human segmentations as ground truth and we compute the F measure with regard to the human segmentations . The averaged F measure for each algorithm is reported . Note that , KS is not appropriate in this application . For each pixels , it contains only the color values . But in constructing the similarity matrix , not only the pixel values are take into considered . As a result , using the k means does not give any informative clue for the selection of landmark pixels .
Table II shows the F measure values of IS , WS and RS on
950 the images . In our experiment the number of samples is set to 50 . The results show that IS gives the best F measure value on most images . IS does not perform the best on several images , but the differences between its performances and the best ones are very small . WS exhibits good performance on several images , but it performs very bad on several other images , this indicates that the performance of WS for image segmentation is very unstable . RS performs the worst on many images . Though RS performs the best on several images , its superiorities over IS on these images are very limited . Overall we conclude that IS performs the best for image segmentation .
The right half of Table II shows the time consumption of the three algorithms . RS consumes the least time . IS takes a little more time than RS , but is much faster than WS .
VI . CONCLUSION
The Nystr¨om method is an efficient technique to generate low rank matrix approximation and has been used in many learning tasks on large scale data sets . However , no work has been conducted to study the impact of matrix approximation error on the clustering performance of spectral clustering . In this paper , we have answered this open problem and proposed an incremental sampling scheme for Nystr¨om method spectral clustering . Experimental results have shown the superiority of the proposed sampling scheme over existing sampling schemes . In our current algorithm , the initial landmark points are randomly sampled , a better initialization can help to improve the performance of the proposed scheme . In
[ 6 ] F . Estrada and A . Jepson , “ Benchmarking image segmentation algorithms , ” International Journal of Computer Vision , vol . 85 , no . 2 , pp . 167–181 , 2009 .
[ 7 ] B . Hunter and T . Strohmer , “ Performance Analysis of Spectral Clustering on Compressed , Incomplete and Inaccurate Measurements , ” Arxiv preprint arXiv:1011.0997 , 2010 .
[ 8 ] L . Huang , D . Yan , M . Jordan , and N . Taft , “ Spectral clustering with perturbed data , ” in NIPS’09 , D . Koller , D . Schuurmans , Y . Bengio , and L . Bottou , Eds . , pp . 705–712 .
[ 9 ] S . Kumar , M . Mohri , and A . Talwalkar , “ Sampling techniques for the nystr¨om method , ” Journal of Machine Learning Research , vol . 5 , pp . 304–311 , 2009 .
[ 10 ] M . M . Kumar S . and T . A . , “ On sampling based approximate spectral decomposition , ” in ICML’09 , pp . 553–560 .
[ 11 ] D . Martin , C . Fowlkes , D . Tal , and J . Malik , “ A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics , ” in ICCV’01 , vol . 2 , July , pp . 416–423 .
[ 12 ] D . Martin , C . Fowlkes , and J . Malik , “ Learning to detect natural image boundaries using local brightness , color , and texture cues , ” IEEE Transactions on Pattern Analysis and Machine Intelligence , vol . 26 , no . 5 , pp . 530–549 , 2004 .
[ 13 ] A . Ng , M . Jordan , and Y . Weiss , “ On spectral clustering :
Analysis and an algorithm , ” in NIPS’01 , pp . 849–856 .
[ 14 ] J . Puzicha and S . Belongie , “ Model based halftoning for color IEEE , pp . 629– image segmentation , ” in ICPR’00 , vol . 3 . 632 .
[ 15 ] Y . Song , W . Chen , H . Bai , C . Lin , and E . Chang , “ Parallel spectral clustering , ” Machine Learning and Knowledge Discovery in Databases , pp . 374–389 , 2008 .
[ 16 ] A . Strehl and J . Ghosh , “ Cluster ensembles — a knowledge reuse framework for combining multiple partitions , ” Journal of Machine Learning Research , vol . 3 , pp . 583–617 , March 2003 .
[ 17 ] U . Von Luxburg , “ A tutorial on spectral clustering , ” Statistics and Computing , vol . 17 , no . 4 , pp . 395–416 , 2007 .
[ 18 ] C . Williams and M . Seeger , “ Using the Nystr¨om method to speed up kernel machines , ” in NIPS’01 .
[ 19 ] L . Zelnik Manor and P . Perona , “ Self tuning spectral clustering , ” in NIPS’05 , L . K . Saul , Y . Weiss , and L . Bottou , Eds . Cambridge , MA : MIT Press , pp . 1601–1608 .
[ 20 ] K . Zhang , I . Tsang , and J . Kwok , “ Improved Nystr¨om lowrank approximation and error analysis , ” in ICML’08 , pp . 1232–1239 .
F MEASURE AND CPU TIME EVALUATED ON THE BERKELEY IMAGE
Table II
DATA SET .
Image ID
8023 12084 14037 19021 24077 33039 37073 38082 41033 58060 62096 65033 66053 69015 69020 69040 76053 78004 85048 86000 87046 89072 101085 109053 119082 143090 157055 159008 219090 296007 302008
IS
0.2827 0.4510 0.5049 0.5937 0.6448 0.4327 0.6085 0.4814 0.5737 0.4030 0.5842 0.5084 0.5724 0.7234 0.5193 0.4418 0.5466 0.7194 0.5807 0.5461 0.4432 0.5307 0.6813 0.3640 0.6712 0.5731 0.5250 0.5211 0.5101 0.5326 0.6573
F measure
WS
0.2181 0.4656 0.3722 0.5913 0.3585 0.4278 0.4160 0.0072 0.5743 0.4261 0.1182 0.4977 0.5414 0.6579 0.2169 0.4407 0.3323 0.5691 0.5724 0.6580 0.4162 0.5366 0.6749 0.3998 0.6676 0.5478 0.5369 0.1663 0.4762 0.0188 0.6706
RS
0.2569 0.4437 0.4789 0.6090 0.6318 0.4210 0.5584 0.4833 0.5896 0.4034 0.5589 0.4928 0.5212 0.7187 0.4856 0.4389 0.5362 0.7049 0.5668 0.5190 0.4446 0.5205 0.6722 0.3697 0.6662 0.5574 0.5298 0.5026 0.4759 0.5063 0.6318
CPU time(seconds ) RS IS 2.99 6.41 7.37 3.98 4.12 6.77 3.08 6.24 2.97 6.67 2.37 6.49 5.95 2.28 2.66 5.75 2.37 5.85 2.50 5.59 3.24 9.85 2.97 5.90 6.40 3.50 3.10 6.18 2.47 5.97 2.64 6.38 3.23 6.46 5.65 3.06 2.49 6.03 3.84 6.52 2.70 5.99 2.26 5.86 3.93 9.35 5.68 2.57 2.88 6.39 3.00 6.40 2.47 5.71 2.83 6.21 6.51 3.53 2.82 6.37 6.12 2.73
WS 24.90 23.36 50.75 23.47 51.13 32.60 22.60 22.71 23.31 22.20 23.24 22.31 23.67 23.00 22.67 23.87 23.57 23.29 22.15 22.75 22.71 24.99 24.68 31.62 24.09 23.35 22.49 26.64 23.62 24.67 21.64 future work , we will also exploit our analysis method and sampling scheme to other Nystr¨om method based learning tasks .
REFERENCES
[ 1 ] M A Belabbas and P . J . Wolfe , “ Spectral methods in machine learning and new strategies for very large datasets , ” PNAS , vol . 51 , no . 6 , pp . 369–374 , 2009 .
[ 2 ] F . C . Charless Fowlkes , Serge Belongie and J . Malik , “ Spectral grouping using nystr¨om extension , ” IEEE Transactions on Pattern Analysis and Machine Intelligence , vol . 26 , no . 2 , pp . 214–225 , 2004 .
[ 3 ] C . Davis and W . Kahan , “ The rotation of eigenvectors by a perturbation . iii , ” SIAM Journal on Numerical Analysis , vol . 7 , no . 1 , pp . 1–46 , 1970 .
[ 4 ] P . Drineas , R . Kannan , and M . W . Mahoney , “ Fast monte carlo algorithms for matrices ii : Computing a low rank approximation to a matrix , ” SIAM J . Comput . , vol . 36 , no . 1 , pp . 158–183 , 2006 .
[ 5 ] P . Drineas and M . Mahoney , “ On the Nystr¨om method for approximating a Gram matrix for improved kernel based learning , ” Journal of Machine Learning Research , vol . 6 , pp . 2153–2175 , 2005 .
951
