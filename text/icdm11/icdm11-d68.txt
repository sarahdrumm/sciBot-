2011 11th IEEE International Conference on Data Mining
TWITOBI : A Recommendation System for Twitter Using Probabilistic Modeling
Younghoon Kim
Seoul National University
Seoul , Korea yhkim@kddsnuackr
Kyuseok Shim
Seoul National University
Seoul , Korea shim@eesnuackr
Abstract—Twitter provides search services to help people find new users to follow by recommending popular users or their friends’ friends . However , these services do not offer the most relevant users to follow for a user . Furthermore , Twitter does not provide yet the search services to find the most interesting tweet messages for a user either .
In this paper , we propose TWITOBI , a recommendation system for Twitter using probabilistic modeling for collaborative filtering which can recommend top K users to follow and top K tweets to read for a user . Our novel probabilistic model utilizes not only tweet messages but also the relationships between users . We develop an estimation algorithm for learning our model parameters and present its parallelized algorithm using MapReduce to handle large data . Our performance study with real life data sets confirms the effectiveness and scalability of our algorithms .
Keywords Twitter ; recommendation system ; probabilistic model ; collaborative filtering ; MapReduce
I . INTRODUCTION
Twitter has emerged recently as a new medium in spotlight for communication . Twitter offers a unique mechanism of information diffusion by allowing each user to receive all messages ( called tweets ) from those who he follows . We refer to those who follow a user as followers and refer to those whom a user follows as followees .
Generally , users would like to follow other users who post interesting messages . However , assisting users to find new people to follow is not a simple task . Twitter itself provides search services to help people find new users to follow by recommending popular users or the friends of their friends . However , these services do not offer the most relevant users to follow for a user . Furthermore , recommending the most interesting tweet messages for a user will be very useful , but Twitter does not provide this functionality .
In our paper , we propose TWITOBI , a recommendation system for Twitter using probabilistic modeling for collaborative filtering . Due to the distinctive feature of Twitter which allows users to post their tweet messages and to follow others , we have more opportunities for better recommendations than the other traditional applications . For example , even for the users with few tweet messages , we can do better recommendations using the tweet messages of their followees .
Our probabilistic model is a generalization of the probabilistic latent semantic indexing ( PLSI ) in [ 11 ] . If we blindly apply the model of PLSI to Twitter , we can utilize only the relationships between users and their postings by introducing hidden variables for topics . To take advantage of the Twitter activities , we first propose a new probabilistic model by assuming that topics are selected following not only the user ’s preference to the topics but also the preference of user ’s followees to the topics . We next develop an ExpectationMaximization(EM ) algorithm to learn the parameters of our model by maximizing the log likelihood of expectation .
The service handles a billion tweets every week . In the year of 2010 alone , the average number of tweets per day has nearly tripled from 50 million to 140 million . The number of Twitter users also increase rapidly . In February this year , 460,000 new accounts have been created every day [ 20 ] . Since the probabilistic model based algorithms are computationally very expensive for large data , is it imperative to develop parallel algorithms for scalability .
MapReduce framework [ 6 ] provided a scalable , parallel , shared nothing data processing platform to handle dataintensive applications using large clusters of commodity machines . MapReduce is a programming model for easy development of scalable parallel applications for large amounts of data . Google file system or Hadoop [ 1 ] is a system in which the MapReduce algorithms to build such applications can be easily implemented and executed .
To take advantage of MapReduce framework for Twitter data , we develop the parallel algorithm using MapReduce to build our probabilistic generative model and we adapt the idea of “ sharding ” of users and words proposed in [ 5 ] which splits users and words into a number of partitions to reduce the requirement of main memory in each machine .
The contributions of this paper are as follows : • We propose a novel probabilistic generative model which is suitable for representing the activities in Twitter services .
• We develop an EM algorithm with serial computation to learn the parameters of our probabilistic model and present a parallel EM algorithm using MapReduce to handle large data .
• We also provide the ranking algorithms for recommending the top K followees or the top K tweets to a user . • By performance study with Twitter data , we show the effectiveness and scalability of our algorithms for Twitter .
1550 4786/11 $26.00 © 2011 IEEE DOI 101109/ICDM2011150
340
II . RELATED WORK
We first discuss the model based collaborative filtering algorithms [ 10 ] , [ 12 ] , [ 14 ] , [ 18 ] , [ 19 ] , [ 23 ] and next describe the recent works on recommendations for Twitter [ 9 ] , [ 25 ] . Model based algorithms build models based on the behaviors of users and utilize the models to predict the users’ ratings on unseen items . In [ 17 ] , movie recommendations using a naive Bayesian classifier were introduced and it was shown that recommendations using probabilistic modeling outperform other approaches in terms of precision .
More complex probabilistic models were later proposed in various recommendation applications . In [ 18 ] , Markov process was used to model the purchasing process of market basket data . In [ 12 ] , [ 14 ] , [ 19 ] , recommendations utilizing the PLSI in [ 11 ] were investigated . To reduce the computation of building probabilistic models , a parallelized PLSI algorithm using MapReduce [ 6 ] was presented in [ 5 ] . However , these algorithms build models based on the relationship between users and items , and thus the relationships between users by considering followees and followers in Twitter cannot be used by these algorithms . In [ 28 ] and [ 29 ] , LDA in [ 4 ] is generalized to model both latent topics and hidden communities between users . However , these techniques do not consider the relationship between users and we cannot use those works directly for Twitter data .
To utilize the relationships between users in social networks , recommendation algorithms applying random walk modeling has been proposed in [ 8 ] and [ 23 ] . In [ 23 ] , after building a graph of items to represent the similarities between items , they compute the personalized PageRank [ 10 ] to recommend the top K ranked items . In [ 8 ] , the personalized PageRank [ 10 ] was generalized for bipartite graphs . However , these algorithms do not use the contents such as tweet messages . Our work differs from those algorithms in that we also consider the tweet messages as well as links .
TF IDF weighting is used for recommending users to follow [ 9 ] and tweet messages to read [ 25 ] based on the sum of weights of the common words in their tweets . Even though the above algorithms consider the tweet messages as well as followers and followees , a simple use of TFIDF weighting makes the algorithms to suffer the quality of recommendations . Furthermore , these algorithm can recommend either users or tweets only while our TWITOBI can recommend both users and tweets .
III . PRELIMINARIES
A . Problem Formulation
Assume that we are given a tweet collection T and a set of users U with the relationships of followees and followers . We formulate the following two recommendation problems . • Top K tweet recommendation : For a user u , find topK tweets not written by u in T which the user u would like the most .
User a1 a2 a3 b1 b2
Followees a2 , b1 a1 , a3 a1 , a2 , b1 b2 b1 , a1
Tweets
Financial , IMF , Economic , Crisis
Financial , IMF , Crisis
Economic , Harry
Financial , Harry , Potter , Film
Crisis , Harry , Potter
Figure 1 . An example of users and their tweets
• Top K followee recommendation : For a user u , choose top K users who are not u ’s followees and whom u would like to follow the most .
Example 3.1 : : Consider a set of users U = {a1 , a2 , a3 , b1 , b2} with their followees and tweets shown in Figure 1 . The tweets written by every user are presented after eliminating stop words . In our tweet message data , there exist 2 topics which are ‘Financial crisis’ and a famous film ‘Harry Potter’ . Top 1 tweet recommendation : If we want to recommend a tweet to a1 , the tweet written by a2 is better than that by b1 since the tweet by a2 is more similar to that by a1 . Furthermore , if we examine the words in the tweet by a3 , the words for both topics are evenly distributed and it is difficult to select the best topic to recommend to a3 . However , since the major topic covered by the tweets written by a3 ’s followees is ‘Financial crisis’ , it is probably better to recommend the tweet by a1 or a2 than that by b1 or b2 . Top 1 followee recommendation : Consider the problem of recommending a new followee for a1 . If we choose followee candidates by considering the ‘friends of friends’ , the candidates are {a3 , b2} in which a3 is a followee of the followee a2 and b2 is a followee of the followee b1 . Among the candidates , recommending a3 to a1 will be probably better since a3 is a followee of a2 whose tweet mainly covers ‘Financial crisis’ , while b2 is a followee of b1 who has more interests in ‘Harry Potter’ .
B . Definitions for Probabilistic Models
Observed data : A tweet is a short message in Twitter service . Let U be a set of Twitter users and let T be a set of tweets written by all users in U where each tweet t ∈ T is a bag of words with deleting the stop words from its original tweet message . Let W be a set of words occurring in any tweet t ∈ T and let n(t , w ) denote the number of occurrences of w in a tweet t ∈ T . Every tweet t ∈ T is written by a single user u ∈ U . For each user u ∈ U , let Tu and Ou denote the set of tweets in T written by the user u and the subset of users in U whom the user u follows ( ie , the followees of u ) respectively . We refer to each user in Ou as a followee of u . Similarly , for each user u , we let Iu be the set of users in U who follow u and we refer to every user in Iu as a follower of u .
Unobserved topics : We assume that there exist m major topics denoted by Z = {z1 , z2 , , zm} in the tweet messages of T . The zis in Z are also used as hidden variables to represent m topics in our probabilistic model . Note that we do not know in advance the actual topic words representing each topic in Z .
341
Figure 2 . An graphical model
IV . OUR GENERATIVE MODEL FOR TWITTER
Many previous works have shown the effectiveness of mixture models in clustering text collections by hidden topics using conditional probability distributions [ 4 ] , [ 11 ] , [ 13 ] , [ 15 ] . We next propose our new probabilistic model to be used for recommending followees or tweets for a user in Twitter .
A . Our Probabilistic Model
In our generative model , we assume that each word in a tweet is chosen for a topic in Z as the model of PLSI in [ 11 ] does . However , we assume that the topics are not only selected by a user but also chosen under the influence of the followees of the user . Correspondingly , we introduce the following three conditional probability distributions : ( 1 ) p(w|z ) is the probability with which the word w ∈ W is selected for the topic z ∈ Z .
( 2 ) p(z|u ) is the probability with which the user u ∈ U selects the topic z ∈ Z .
( 3 ) p(f |u ) is the probability with which the user u ∈ U is influenced by the followee f ∈ Ou for selecting topics . Word selection model : Given a topic z ∈ Z , a word w ∈ W is chosen with the probability p(w|z ) depending on w∈W p(w|z ) = 1 for every topic z only . Clearly , we have z ∈ Z . Semantically , the probability distribution represents the relevancy of words to describe the given topic z .
.
Topic selection model : A topic z is selected with the dependency between the author u and u ’s followees . We introduce the conditional probability distribution p(z|u ) which implies the preference of a user u ∈ U to a given topic z ∈ Z . We also introduce the conditional probability distribution p(f |u ) which represents the preference of u to z∈Z p(z|u ) = 1 a followee f ∈ Ou . Obviously , we have and p(f |u ) = 1 for every user u ∈ U .
.
.
Example 4.1 : : Let us assume that a Twitter user ‘John’ shows some interest in the topic of ‘Financial crisis’ but also follows ‘Emma Watson’ who is the heroine of ‘Harry Potter’ . The p(z|John ) is probably high with z=‘Financial crisis’ and is low with z=‘Harry Potter’ . However , John sometimes may choose the topic of ‘Harry Potter’ because he is influenced by Emma Watson ’s recent tweets . Certainly , as John agrees more with the opinions of his followees , he would write more tweets on his followee ’s favorite topics . f ∈Ou
The graphical representation of our mixture model is presented in Figure 2 . According to this generative model ,
342 a user would write his tweets by repeatedly performing the following steps stochastically while sampling words :
1 . Each user u first decides whether to choose the topic of his own interest with the probability α or one of their followees’ interest with the probability ( 1 α ) . Since α is a probability value , we have α ∈ [ 0 , 1 ] .
1 (a ) . If the user u decides to choose a topic with u ’s own interest , a topic z is chosen according to the conditional probability distribution of p(z|u ) .
1 (b ) . If the user u decides to choose a topic based on the interest of one of u ’s followees , u first chooses a followee f among the users in Ou according to the conditional probability distribution of p(f |u ) and next selects a topic z according to the conditional probability distribution of p(z|f ) .
2 . With the topic z chosen in the above , a word w is sampled according to the conditional probability distribution of p(w|z ) .
Note that if α=1 ( ie , if topics are always chosen by the user u ’s own interest ) , our generative model becomes PLSI model in [ 11 ] . We now formally present our probabilistic model and its maximum likelihood estimate to compute the conditional probability distributions p(w|z ) , p(z|u ) and p(f |u ) for a given Twitter data .
B . The Likelihood of a Tweet Collection
Let T be a collection of tweets . For each user u ∈ U , we have a tuple including a user u , Tu which is the set of tweets written by u and Ou which is the set of followees of u in Twitter . Each observed data is represented as fiud , Tu , Ou' . With the probability p(u , Tu , Ou ) , the likelihood of the whole collection T according to our generative model is u∈U p(u , Tu , Ou ) . Since each word in a tweet is L = sampled identically and independently , the likelihood is fi
.
L =
= u∈U
. u∈U p(u , Ou ) · p(Tu|u , Ou )
.
. t∈Tu w∈W p(u , Ou ) · p(w|u , Ou)n(t,w ) ,
( 1 ) where n(t , w ) denotes the number of appearances of the word w in the tweet t .
.
By marginalization with the random variable z for topics , z∈Z p(w|z , u , Ou)· p(z|u , Ou ) . For p(w|u , Ou ) becomes a given z , since w is conditionally independent to u and Ou in our generative model , the probability p(w|z , u , Ou ) becomes p(w|z ) and the likelihood becomes
L =
. u∈U p(u , Ou )
.
. t∈Tu w∈W z∈Z
⎡ ⎣ff
⎤ ⎦ P ( w|z)p(z|u , Ou ) n(t,w )
.(2 )
If we assume that a user u follows the followee f in Ou identically and independently with the probability distribution p(f |u ) , p(u , Ou ) becomes p(u ) p(f |u ) . Furthermore , for given u and Ou , a topic z is chosen by the probability αp(z|u ) if a user u selects the topic with u ’s f ∈Ou fi own interest . If user u selects the topic with the interest of a user f in Ou , the topic z is selected by the probability ( 1 − α)p(f |u)p(z|f ) . Thus , the likelihood is ⎡ ⎣ff p(w|z)· ⎤ ⎤ ⎦ ⎦ p(f |u)p(z|f )
⎡ ⎣αp(z|u ) + ( 1 − α )
⎡ ⎣p(u )
⎤ ⎦ .
P ( f |u ) ff
.
.
. n(t,w ) f ∈Ou
L =
( 3 ) u∈U t∈Tu w∈W z∈Z
. f ∈Ou
C . The Maximum Likelihood Estimate
Assume that the observed data is generated from our generative model . With a random variable u representing a user , z representing a topic and w representing a word , let Θ denote the three conditional probability distributions p(f |u ) , p(z|u ) and p(w|z ) which are initially unknown model parameters . We wish to find Θ such that the likelihood L in Equation ( 3 ) is maximized . This is known as the Maximum Likelihood ( ML ) estimation [ 16 ] for Θ . In order to estimate Θ , it is typical to introduce the log likelihood function defined as , ff ff ff log L = log p(u ) + u∈U ff ff ff u∈U f ∈Ou n(t , w ) log u∈U w∈W t∈Tu
⎡ ⎣αp(z|u ) + ( 1 − α ) ff f ∈Ou log p(f |u ) + ⎡ ⎣ff z∈Z p(w|z)· ⎤ ⎤ ⎦ ⎦ . p(f |u)p(z|f )
( 4 )
The likelihood function is considered to be a function of the parameter Θ given a tweet collection T . Since log L is a strictly increasing function , the values of Θ which maximize log likelihood of log L also maximizes the likelihood L [ 22 ] . Thus , we have to compute the model parameters Θ which not only maximize the log likelihood log L in Equation ( 4 ) p(f |u ) = 1 and but also satisfy . z∈Z p(z|u ) = 1 , f ∈Ou
.
. w∈W p(w|z ) = 1 .
V . ESTIMATION OF MODEL PARAMETERS WITH OUR EM
ALGORITHM
Without any prior knowledge , we may use the maximum likelihood estimator to compute all the parameters . Specifically , we can use the Expectation Maximization ( EM ) algorithm [ 7 ] to compute the maximum likelihood estimate iteratively . An EM algorithm performs iterations of two steps which consist of an expectation step ( E step ) and a maximization step ( M step ) . In E step , we compute the probability expectation of the hidden variables using the current estimate of parameters , and in M step , we derive the parameters maximizing the log likelihood using the expectations computed in E step . Those parameters estimated in M step are then used in E step of the next iteration .
E Step : This step computes the expectations of the hidden variables . Here , the hidden variable is ( 1 ) the topic φ which is chosen by a user u , ( 2 ) the user θ which indicates the user
343
! "# $ $ $
$#

Figure 3 . The conditional probability distributions who determined the topic z when a user u writes a tweet t . The user θ is u if u chooses the topic z with u ’s own interest and the user θ is f if u is influenced by f who is a followee in Ou when u writes the tweet t . Let p(φ=z|w , u , Ou ) be the expected probability that a word w is generated from the topic z in the tweet of a user u whose followees are Ou . Similarly , let p(θ=u|z , u , Ou ) be the expected probability that for given a user u , a topic z is chosen according to the favorite topics of u himself . Moreover , let p(θ=f |z , u , Ou ) be the expected probability that a user u selects a topic z according to the interest of u ’s followee f in Ou . The formulas for computing the expectations of the probabilities p(φ=z|w , u , Ou ) , p(θ=u|z , u , Ou ) and p(θ=f |z , u , Ou ) are presented in Figure 4 . Since Ou is fixed for each u , we will denote p(φ=z|w , u , Ou ) as p(φ=z|w , u ) in short . Similarly , let p(θ=u|z , u ) and p(θ=f |z , u ) be p(θ=u|z , u , Ou ) and p(θ=f |z , u , Ou ) respectively in short .
M Step : In order to find the parameters Θ which maximize Equation ( 4 ) , we use the method of Lagrange multipliers [ 2 ] . We present the obtained formulas for the M Step of our EM algorithm in Figure 4 .
We iterate E Step and M Step until the log likelihood in Equation ( 4 ) converges . The algorithm is only guaranteed to find a local maximum of the likelihood . Thus , we perform multiple trials to obtain a local maximum that is close to a global optimum .
Example 5.1 : : Consider the example of users and their tweets in Figure 1 . Since the tweets contain two topics which are ‘Financial crisis’ and the famous film ‘Harry Potter’ , we set the number of topics m to 2 in this example . Let α In the E step of the first iteration , p(φ=z|w , u ) , be 02 p(θ=u|w , z , u ) and p(θ=f |z , u ) are randomly initialized .
In the M step of the first iteration , we compute p(w|z ) , p(z|u ) and p(f |u ) with the previously initialized values of p(φ=z|w , u ) , p(θ=u|z , u ) and p(θ=f |z , u ) . For example , consider the Equation ( 8 ) . For w=‘Financial’ and z=0 , the nominator of p(w|z ) is computed as 0.6 ∗ 3 and p(w|z ) becomes 016 Similarly , p(w|z ) , p(z|u ) and p(f |u ) are computed and the log likelihood L after the first iteration becomes 4335 After repeating E step and M step until the log likelihood converges to 39.44 , we obtain the conditional probability distributions as shown in Figure 3 .
As we can expect from Twitter message data in Figure 1 , the users a1 and a2 have high probability for the topic z2 . In contrast , the users b1 and b2 have high probability for the p(φ=z|w , u ) =
( p(w|z){αp(z|u ) + ( 1 − α )
( f ∈Ou p(f |u)p(z|f )}
, z.∈Z p(w|zfi){αp(zfi|u ) + ( 1 − α ) f .∈Ou p(f fi|u)p(zfi|f fi)}
( p(θ=u|z , u ) = p(θ=f |z , u ) = p(w|z ) =
αp(w|z)p(z|u )
(
αp(z|u ) + ( 1 − α ) f .∈Ou p(f fi|u)p(z|f fi )
(
( 1 − α)p(f |u)p(z|f ) ( p(f fi|u)p(z|f fi ) f .∈Ou n(t , w)p(φ=z|w , u )
(
αp(z|u ) + ( 1 − α ) ( (
( u∈U t∈Tu w . ∈W
( u∈U
(
,
,
, n(t , wfi)p(φ=z|wfi , u ) t∈Tu
( p(z|u ) =
( p(f |u ) = z.∈Z { ( 1 +
|Ou| + t∈Tu
( ( t∈Tu t∈Tu w∈W n(t , w)p(φ=z|w , u)p(θ=u|w , z ) + ( w∈W n(t , w)p(φ=zfi|w , u)p(θ=u|w , zfi ) + ( ( z∈Z n(t , w)p(φ=z|w , u)p(θ=f |z , u )
(
(
(
( i∈Iu w∈W f ∈Ou t∈Tu w∈W z∈Z n(t , w)p(φ=z|w , u)p(θ=f |z , u )
(
( t∈Ti
( i∈Iu t∈Ti
. w∈W n(t , w)p(φ=z|w , i)p(θ=u|z , i ) ( w∈W n(t , w)p(φ=zfi|w , i)p(θ=u|zfi , i)}
( 5 )
( 6 )
( 7 )
( 8 )
,
( 9 )
( 10 )
Figure 4 . The formulas for E step and M step topic z1 . From the values of p(w|z ) in Figure 3(c ) , we can see that that words ‘Financial’ , ‘IMF’ and ‘Crisis’ occur more frequently for z2 while ‘Harry’ , ‘Potter’ and ‘Film’ tend to appear for z1 .
Time and space complexities : Due to lack of space , we do not provide the detailed explanation for time and space complexities . Let NO be the maximum number of followees w∈W n(t , w ) . The overall for all users and let TW = time complexity of a single iteration in our EM algorithm becomes O(NO · |Z| · |U | · |W | + NO · TW · |Z| ) .
.
. t∈T
To maintain all values of p(φ=z|w , u)s , p(θ=u|z , u)s and p(θ=f |z , u)s , we need O(|Z|·|U |·|W |+NO ·|Z|·|U | ) space of main memory . Since maintaining the model parameters of all values of p(w|z)s , p(z|u)s and p(f |u)s takes O(|W | · |Z|+|Z|·|U |+NO ·|U | ) space , the total space complexity of our EM algorithm becomes O(|Z|·|U |·|W |+ NO ·|Z|·|U | ) .
VI . UTILIZING OUR MODEL PARAMETERS FOR
RECOMMENDATIONS
Once the parameters in our model are estimated , recommendations can be made by utilizing the model parameters . Recommending top K followees : We utilize the random surfer model of personalized PageRank for recommendation in [ 23 ] . Let a user u is a random surfer who visits other Twitter users by following the links to followees . The random surfer always begins surfing from visiting his own followees and in the currently visiting user v , he decides to follow the link to v ’s followees with probability d and visits a user f ∈ Ov with probability p(f |v ) . Sometimes , with probability ( 1 − d ) , the random surfer stops following the followees’ links and start a new surfing from one in Ou . In this case , the random surfer u chooses a starting user in Ou with probability p(f |u ) . The recursive equation for the rank Ru(v ) that the random surfer u visits the user v is formulated as the following : ff
Ru(v ) = ( 1 − d ) · p(v|u ) + d ·
( 11 ) The rank Ru(v ) can be computed with the power method ( see [ 21 ] ) used for computing PageRank in [ 3 ] , which repeats the computation in Equation ( 11 ) until the rank p(v|i ) · Ru(i ) . i∈Iv
Ru(v ) of every user does not change any more . Then , we can recommend the top K ranked users to the user u .
Recommending top K tweets : It is important to recommend interesting tweets even if they are not written by the followees . Thus , we compute the score function Su(t ) as
Su(t ) = ω · p(f |u ) + ( 1 − ω ) p(z|u ) · max w∈t p(w|z ) ,
( 12 ) ff z∈Z where ω is a constant for weighting in the range of [ 0 , 1 ] . The first term of Su(t ) in Equation ( 12 ) computes u ’s preference to the author of the tweet t . The second term in Equation ( 12 ) calculates u ’s preference to the topics appearing in the tweet t . Since the length of each tweet t is generally very short , for a specific topic z , only very small number of words describing the topic z will show up in t . w∈t p(w|z ) as the similarity of t Thus , if we simply use to the topic z , long tweet messages tend to have high scores . w∈t p(w|z ) becomes generally larger for every topic , since with increasing the number of words in t . Thus , we use maxw∈t p(w|z ) as the similarity of t to the topic z appearing in the tweet t instead . We also found this tendency by experimental study with real life data .
.
VII . TWITOBI MR : THE PARALLEL EM ALGORITHM
USING MAPREDUCE
A . The Outline of Parallelization
We now present the EM algorithm using MapReduce to compute the model parameters p(z|u ) , p(w|z ) and p(f |u ) in our model .
Parameter partitioning : We assume that we have NR × NC machines where each machine is represented by M(i,j ) with 1 ≤ i ≤ NR and 1 ≤ j ≤ NC . Then , we conceptually split the users U and words W into NR and NC disjoint partitions respectively such that U = U1 ∪ U2 . . . ∪ UNR and W = W1∪W2∪ . . .∪WNC . Let us define OUi = ∪u∈Ui Ou . In each machine M(i,j ) , we keep the entries of p(z|u ) , p(w|z ) and p(f |u ) in main memory for every u , w , z and f such that u ∈ Ui , w ∈ Wj , z ∈ Z and f ∈ OUi .
The tweet messages and link information of Twitter users are partitioned into each machine M(i,j ) to be given as input
344 uk , . . . , tNC uk where the sub tweet tj of map functions as follows : For each user u ∈ Ui , the k th tweet message tuk written by the user u is first split into sub tweets t1 uk represents the set of every word appearing in the tweet tuk and Wj together , and then each tj uk is stored in the disk of M(i,j ) . Furthermore , for each user u ∈ Ui , the tuple of ( u,Ou,Iu ) for the user u is stored with replication in the disks of all machines M(i,1 ) , M(i,2 ) , . . . , M(i,NC ) .
Overview of the Parallel EM algorithm : In each machine M(i,j ) , the iteration of two steps using map and reduce functions twice is repeatedly performed by our EM algorithm as follows . Note that every map function executed in the same machine shares the model parameter values stored in the main memory .
• Step 1 Computation of a common expression : Each map function in the first step is called with a tuple ( u,Ou,Iu ) as input . We compute the values of X(u , z ) which are commonly used in the Equations ( 5) (7 ) .
• Step 2 Computation of all model parameter values : Each map function in the second step is executed with either a sub tweet tj uk or a tuple ( u,Ou,Iu ) as input . We computes all model parameter values in Equations ( 8) (10 ) using X(u , z ) computed previously .
After the above two steps are finished , the computed values of p(w|z ) , p(w|z ) and p(z|u ) are broadcasted to M(i,j ) according to u∈Ui and w∈Wj . Then , each machine updates the model parameters in its main memory with these broadcasted values . When the log likelihood of log L does not improve any more , we stop our iterations . B . Computation of a Common Expression X(u , z )
We first define X(u , z ) below which appears in all Equa tions ( 5) (7 )
X(u , z ) = αp(z|u ) + ( 1 − α ) ff f ∈Ou p(f |u)p(z|f ) .
( 13 )
We want to make each reduce function to compute X(u , z)s with all z ∈ Z for a single user u . Thus , the values of p(z|u ) , p(f |u ) and p(z|f ) with all z ∈ Z and all followees f ∈ Ou should be sent to the reduce function to be called with the key u . In this step , the input given to each map function has the form of ( u,Ou,Iu ) only . In this section , we let Ou and Iu be {f1 , f2 , . . . , f|Ou|} and {i1 , i2 , . . . , i|Iu|} respectively .
Let L1(u ) = [ (z1 , p(z1|u) ) , . . . , ( z|Z| , p(z|Z||u) ) ] and L2(u ) = [ (f1 , p(f1|u) ) , . . . , ( f|Ou| , p(f|Ou||u)) ] . Basically , L1(u ) represents the list of p(z|u)s with all z ∈ Z and L2(u ) is the list of p(f |u)s with all f ∈ Ou . Inside of the map function , we first emit a key value pair fiu , [ T1 , L1(u ) , L2(u ) ] ' . Furthermore , for every follower i ∈ Iu , a keyvalue pair fii , [ T2 , u , L1(u ) ] ' is emitted . Here T1 and T2 are simply used to represent the types of values in the list so that the reduce functions can discriminate the values in the value list . Then , a reduce function is called with fiu , [ T1 , L1(u ) , L2(u) ] , [ T2 , f1 , L1(f1) ] , . . . , [ T2 , f|Ou| , L1(f|Ou| ) ]
' where L1(u ) is the list of p(z|u)s , L2(u ) is the list of p(f |u)s and L1(fk ) represents the list of p(z|fk)s . Thus , we can compute X(u , z)s in Equation ( 13 ) with every z∈Z for the a user u in this reduce function . We refer to the map and reduce functions for computing X(u , z)s as TOPICPREF.map and TOPIC PREFreduce
C . Computation of All Model Parameter Values
The map and reduce functions in Step 2 compute p(w|z ) , p(z|u ) and p(f |u ) together using X(u , z ) produced in Step 1 . The input for each map function in this step is either a tuple ( u,Ou,Iu ) or a sub tweet tj uk , which is the subset of words w∈Wj in the k th tweet message tj uk of the user u . Even though p(w|z ) , p(z|u ) and p(f |u ) are computed together , we illustrate the steps of computation for each of them separately .
Computing p(w|z ) : We can rewrite p(w|z ) in Equation
( 8 ) as N1(w , z)/D1(z ) below : p(w|z ) = N1(w , z)/D1(z ) ,
U1(w , z , u ) =
N1(w , z ) =
D1(z ) =
( ff p(w|z)X(u , z ) z . ∈Z p(w|zfi)X(u , zfi ) ff
, ff ff u∈U t∈Tu n(t , w)U1(w , z , u ) , ff n(t , w)U1(w , z , u ) .
( 14 )
( 15 )
( 16 ) w∈W u∈U t∈Tu
Since both of N1(w , z ) and D1(z ) require the common expression U1(w , z , u ) shown in Equation ( 14 ) , we first compute U1(w , z , u)s in map functions , and N1(w , z ) and D1(z ) are next computed in reduce functions . We compute N1(w , z ) by summing U1(w , z , u ) for every occurrence of w in the tweet collection . Note that summing U1(w , z , u ) for every occurrence of w has the effect of multiplying n(t , w ) to U1(w , z , u ) . To calculate D1(z)s , each map function emits key value pairs with zs as keys and U1(w , z , u)s as values . To compute p(w|z)s , only the input of the sub tweets tj uks are used in map functions . Since each machine M(i,j ) has X(u , z)s and p(w|z)s in main memory for all u ∈ Ui , w ∈ Wj and z ∈ Z , each map function called with a tj uk can compute U1(w , z , u ) easily , and emits fi(N1 , w ) , ( z , U1(w , z , u))' and fi(D1 , z ) , U1(w , z , u)' where N1 and D1 are used to represent the type of each key . For each ( N1 , w ) , a reduce function is invoked with its list of ( z , U1(w , z , u))s to compute N1(w , z)s for all z ∈ Z and emits fi(N1 , w , z ) , N1(w , z)' . Similarly , for each ( D1 , z ) , another reduce function is invoked with its list of U1(w , z , u)s to calculate D1(z ) and emits fi(D1 , z ) , D1(z)' .
We can compute all p(w|z)s using N1(w , z)/D1(z)s in main function and broadcast to the machines M(i,j)s with 1 ≤ i ≤ NR and w ∈ Wj . However , we can also distribute the computation of N1(w , z)/D1(z)s to all machines . To achieve this goal , N1(w , z)s are broadcasted to the machines M(i,j ) with 1 ≤ i ≤ NR and w ∈ Wj . D1(z)s are also broadcasted to all machines ( ie M(i,j)s with 1 ≤ i ≤ NR
345 and with 1 ≤ j ≤ NC ) . Then , in the machine M(i,j ) , we compute p(w|z)s with w ∈ Wj and z ∈ Z .
Computing p(z|u ) : We can rewrite the formula for p(z|u ) in Equation ( 9 ) as the following : N2(u , z ) , p(z|u ) = N2(u , z)/ ff z∈Z
αp(w|z )
( z.∈Z p(w|zfi)X(u , zfi ) ( ( 1 − α)p(w|z)p(f |u ) z.∈Z p(w|zfi)X(u , zfi ) ff
U21(u , z , w ) =
U22(f , z , u , w ) = ff
N2(u , z ) = t∈Tu ff w∈W ff n(t , w)p(z|u)U21(u , z , w ) , ff n(t , w)p(z|u)U22(u , z , f , w ) .
+ f ∈Iu t∈Tf w∈W
( 17 )
( 18 )
( 19 )
( 20 )
We compute N2(u , z ) by summing U21(u , z , w ) · p(z|u ) and U22(u , z , f , w ) · p(z|u ) with all f ∈ Iu for every occurrence of w in the tweet collection . a a tuple sub tweet tj uk or fi(N2 , u ) ,
Each map function in the second step is executed ( u,Ou,Iu ) with either input . Each map function invoked with a tuple as ( u,Ou,Iu ) emits is [ (z1 , p(z1|u) ) , . . . , ( z|Z| , p(z|Z||u)) ] . Every map function invoked with a sub tweet tj uk first emits the key value pair fi(N2 , u ) , [ T1 , z , U21(u , z , w)]' for every z ∈ Z and emits a key value pair fi(N2 , f ) , [ T1 , z , U22(f , z , u , w)]' for every pair of ( f , z ) with f ∈ Ou and z ∈ Z . Here T0 and T1 are the types of the value lists .
( T0 , L(u))' where L(u )
For each key ( N2 , u ) with its list consisting of ( z , U21(u , z , w))s , ( z , U22(u , z , f , w))s and L(u ) , a reduce function is executed to calculate N2(u , z)s by aggregating ( U21(u , z , w ) · p(z|u))s and ( U22(u , z , f , w ) · p(z|u))s for every z ∈ Z . Then , using the computed N2(u , z)s , we calculate p(z|u ) and emits the key value pair fi(P , u , z ) , p(z|u)' where the key ( P , u , z ) represents the value in the output is the value of p(z|u ) . Finally , p(z|u)s are broadcasted to the machines M(i,j ) with 1≤j≤NC and u ∈ Ui .
Computing p(f |u ) : We can rewrite p(f |u ) in Equation
( 10 ) as the following : p(f |u ) = N3(u , f )/ ff
N3(u , f ) . f ∈Ou
U3(u , f , z , w ) = ff
N3(u , f ) =
( ( 1 − α)p(w|z)p(f |u ) z.∈Z p(w|zfi)X(u , zfi ) ff ff n(t , w)p(z|f )U3(u , f , z , w )
( 21 )
( 22 )
( 23 ) t∈Tu w∈W z∈Z
We compute N3(u , f ) by summing U3(u , f , z , w ) · p(z|f ) for every occurrence of w in the tweet collection . Each map function in the second step is executed with either a sub tweet tj uk or a tuple ( u,Ou,Iu ) as input . When the input of a map function is a form of ( u,Ou,Iu ) , for every f ∈ Iu , we emit fi(N3 , f ) , ( T0 , u , L(u))' where L(u ) is [ (z1 , p(z1|u) , , ( z|Z| , p(z|Z||u) ] . When the input is a subtweet tj uk , we enumerate z∈Z and emit the key value pair fi(N3 , u ) , ( T1 , f , z , U3(u , f , z , w))' for each z∈Z . A reduce function is called with each key of u and a uk , for every w∈tj
346 list of values whose type is either a list consisting of L(f ) for f ∈Ou and ( f , z , U3(u , f , z , w))s . Thus , we can compute N3(u , f ) by simply summing U3(u , f , z , w ) · p(z|f ) . For each value of f , we calculate p(f |u ) by using the computed values of N3(u , f )s .
Our map and reduce functions used to compute p(w|z)s , p(z|u)s and p(f |u)s are referred to as EST PARAM.map and EST PARAM.reduce respectively . Due to lack of space , we omit the pseudocodes of both functions .
We need to check log L to decide whether we will repeat the iteration of the two steps more . We omit here how to calculate log L in each iteration of our MapReduce algorithm due to lack of space .
D . TWITOBI MR : The EM Algorithm with MapReduce
We will refer our proposed parallel EM algorithm to TWITOBI MR . Given two types of inputs which are a tweet collection T and tuples ( u , Ou , Iu ) for all u ∈ U , we repeat the two steps of MapReduce until the log likelihood log L converges . In each iteration step , we first invoke the MapReduce with TOPIC PREF.map and TOPIC PREF.reduce to compute X(u , z)s . Then we broadcast X(u , z)s to the machines M(i,j ) with u ∈ Ui and 1≤j≤NC . Next , the MapReduce with EST PARAM.map and EST PARAM.reduce are executed to compute the model parameters . With the computed parameters , we broadcast not only p(z|u)s and p(f |u)s to the machines M(i,j ) with u ∈ Ui and 1≤j≤NC but also p(w|z)s to the machines M(i,j ) with w ∈ Wj and 1≤i≤NR . Finally , if the log likelihood log L of the tweet collection does not increase any more , we stop the iteration . Example 7.1 : : Consider the tweet collection in Figure 1 . Suppose that α is 0.2 and the number of topics is 2 . We also assume that model parameters p(z|u)s , p(w|z)s and p(f |u)s are initialized as Figure 5(a ) . We show the two steps of the first iteration by TWITOBI MR in Figure 5(b ) and Figure 5(c ) . The map functions of the first step are invoked with a tuple ( u,Ou,Iu ) and compute X(u , z)s . For example , with the user a1 and his followers Iu , a map function emits the key value pair of x and a list of ( z,p(z|u))s for every x ∈ ( {u} ∪ Iu ) as illustrated in Figure 5(b ) . A reduce function called with the key a1 receives the list of p(z|a1)s ( with z ∈ Z ) and p(z|f )s ( with z ∈ Z and f ∈ Oa1 ) . Then , we can compute X(a1 , z ) . All values of X(u , z)s computed for every u and z are shown in Figure 5(b ) .
The map functions of the second step are called with two types of input data : a sub tweet tj uk and a tuple ( u,Ou,Iu ) . Let us consider what map and reduce functions do for computing p(z1|a1 ) . When the input is the sub tweet written by a1 , for example , we emit fi(N2 , a1),(T1 , z1 , 0.12)' where U21(a1 , z1 , ‘IMF’)=0.12 with the word ‘IMF’ in the subtweet . Similarly , we emit fi(N2 , a1),(T1 , z1 , U21(a1 , z1 , w))' and fi(N2 , a1),(T1 , a2 , U22(a1 , z , f , w))' with w∈W and ( a1,Oa1,Ia1 ) , f ∈Oa1 . When the input tuple the is
Figure 5 . The first iteration of our EM algorithm using MapReduce we emit fi(N2 , a1),(T0 , 0.6 , 0.4)' where p(z1|a1)=0.6 and p(z2|a1)=04 from the values of T0
The reduce function with the key ( N2,a1 ) first reads p(z1|a1 ) type and computes N2(a1 , z1)=1.92 by adding ( p · p(z1|a1))s with every ( T1 , z1 , p ) in the value list . After computing N2(a1 , z2 ) = 1.28 similarly , we compute p(z1|a1)=0.6 and p(z2|a1)=0.4 using N2(a1 , z1 ) and N2(a1 , z2 ) . We show the updated model parameters after the first iteration of our EM algorithm in Figure 5(d ) .
VIII . EXPERIMENTS
We empirically evaluated the performance of our proposed algorithms . All experiments reported in this section were performed on the machines with Intel(R ) Core(TM)2 Duo CPU 2.66GHz and 2GB of main memory running Linux operating systems . All algorithms were implemented using Javac Compiler of version 16 Furthermore , we used Hadoop 0202 for the MapReduce implementation [ 1 ] . For our experiments , we implemented the following algorithms . • TWITOBI EM : It is the implementation of our EM algorithm for serial computing presented in Section V . • TWITOBI MR : It is the MapReduce implementation of our EM algorithm introduced in Section VII .
• TWITOBI F : This is the implementation of our topK followee recommendation algorithm in Section VI which uses p(f |u)s produced by TWITOBI EM .
• TWITOBI T : It is the implementation of our top K tweet recommendation algorithm in Section VI which uses the model parameters obtained by TWITOBI EM . • TFIDF F : It is the implementation of the state of theart algorithm for followee recommendations proposed in [ 9 ] . This algorithm uses TF IDF weighting with the followees/followers of a user and the words in tweets written by the user as illustrated in Section II .
• TFIDF T : It is the implementation of the state of theart algorithm for tweet recommendations presented in [ 25 ] . It uses TF IDF weighting with the words in tweets written by users as illustrated in Section II .
Since a single trial of EM algorithms generally finds a local maximum of the likelihood , EM algorithms typically perform multiple trials to obtain several local maxima and choose the best one as the model parameter values . In our experiments , all EM algorithms perform 10 trials for local maxima . We also assume the convergence of each trial is obtained when the difference of the values of log likelihood log L of previous and current iterations is less than 100 .
We initialize the model parameters p(w|z ) , p(z|u ) and p(f |u ) for our EM algorithm as follows : we first assign an integer in the range of [ 1 , 10 ] uniformly to each of them w∈W p(w|z ) . and for each z∈Z , we divide p(w|z)s by Similarly , for each u∈U , we divide p(z|u)s and p(f |u)s by .
.
. z∈Z p(z|u ) and f ∈Ou p(f |u ) respectively .
A . Data Sets
For experimental study , we evaluate the algorithms on real life data sets . We downloaded 12,098,339 tweet messages of 8,405 Twitter users using the Twitter API [ 26 ] . We call this data as ORG DATA . The users in ORG DATA have 221 followees on the average .
To generate test data set , we first select 400 users randomly from ORG DATA where every user has at least 10 followees as well as 10 tweet messages written by either the user or its followees . Then , we select 10 followees and 10 tweets randomly for every one of the 400 users .
To evaluate the quality of the top K followee recommendations , we produce the followee test data called TEST FDATA which consists of the 400 users with their selected ten followees . For the top K tweet recommendations , we
347
TWITOBI F(m=10 ) TWITOBI F(m=15 ) TWITOBI F(m=20 ) TWITOBI F(m=5 ) TFIDF F
K i t a n o s c e r P i k t a l l a c e r
0.6
0.5
0.4
0.3
0.2
0.1
5
10
15
20
25
30
K
( a ) Recall at k
0.4
0.35
0.3
0.25
0.2
0.15
0.1
5
TWITOBI F(m=10 ) TWITOBI F(m=15 ) TWITOBI F(m=20 ) TWITOBI F(m=5 ) TFIDF F
10
15
20
25
30
K k n a r t i h e g a r e v A
0.11
0.1
0.09
0.08
0.07
0.06
0.05
0.04
TWITOBI F(alpha=0.8 ) TWITOBI F(alpha=0.2 ) TWITOBI F(alpha=0.5 ) TWITOBI F(alpha=1.0 ) TFIDF F
TWITOBI F(m=10 ) TWITOBI F(m=20 ) TWITOBI F(m=15 ) TWITOBI F(m=5 ) TFIDF F k t a l l a c e r
0.6
0.5
0.4
0.3
0.2
0.1
5
10
15
20
25
30
5
10
15
20
25
30
K
( b ) Precision at k
( c ) Average hit rank
Figure 6 . Top K followee recommendation with varying K , m and α
K
( d ) Recall at k
TWITOBI T(alpha=0.5 ) TWITOBI T(alpha=0.2 ) TWITOBI T(alpha=0.8 ) TWITOBI T(alpha=1.0 ) TFIDF T k t a l l a c e r
) c e s ( e m i t n o i t u c e x E generate the tweet test data called TEST T DATA which is all 400 users with their selected 10 tweets . Since we usually want to recommend recent tweet messages only for Twitter , TEST T DATA contains the selected 10 tweets of all 400 users only .
For our EM algorithm , we produce the train data called TRAIN DATA by removing both of the ten selected followees and ten tweets for all 400 users from ORG DATA .
B . Qualities of our Recommendation Algorithms
We conducted our experiments with varying the number of recommendations K , the number of hidden topics m , the probability α that users choose the topics with their own interests and the weight ω in the score function for recommending top K tweets . The default values of these parameters are : K=20 , m=10 , α=0.8 and ω=05
Quality measures : We computed three quality measures called recall at k , precision at k [ 27 ] and average hit rank [ 24 ] . ( The recall at k is also known as hit rate [ 24] . ) In topK followee recommendations , for a test user u , let h be the number of true followees in the recommended top K followees and nT ( u ) be the number of u ’s true followees in the test data set . The recall at k and precision at k for u are h/nT ( u ) and h/K respectively . The average hit rank is for measuring the effectiveness of ranking for each test user . When p1 , p2 , , ph are the position of the true followees in the recommended top K followees , the average hit rank for a test user is ( 1/nT ( u ) ) · i=1(1/pi ) . As the true followees appear with high ranks in the recommended followees , this measure becomes larger .
.h
In top K tweet recommendations , for each test user u , we select top K tweets among the tweets in the test data set and compute the recall at k and average hit rank similarly . Top K followee recommendation : We varied K from 5 to 30 and m from 5 to 20 . Figure 6(a)–(c ) show the averages of recall at ks , precision at ks and average hitranks with 400 test users respectively . As we recommend more followees , since we have more chance to answer the true followees correctly , both recall at k and average hit rank grow gradually with increasing K . However , the precisionat k measure decreases when K is increased because more false followees can be recommended with larger K . The graphs confirm that our recommendation algorithm with
348
0.5
0.4
0.3
0.2
0.1
TWITOBI T(m=10 ) TWITOBI T(m=5 ) TWITOBI T(m=15 ) TWITOBI T(m=20 ) TFIDF T k t a l l a c e r
0.5
0.4
0.3
0.2
0.1
5
10
15
20
25
30
5
10
15
20
25
30
K
( a ) Varying m
K
( b ) Varying α
Figure 7 . The recall at ks for top K tweet recommendation
TWITOBI MR(m=20 ) TWITOBI MR(m=10 ) Ideal
TWITOBI MR(m=20 ) TWITOBI MR(m=10 )
100000
10000
30 d e e p s e v i t a e R l
25
20
15
10
5
1 2
6
15
20
1 2
6
15
20
Number of machines
Number of machines
( a ) Execution time
( b ) Relative speedup
Figure 8 . The scalability of TWITOBI MR every range of m outperforms TFIDF F in terms of recallat k , precision at k and average hit rank . Furthermore , both graphs show that when we give 10 hidden topics for our model , TWITOBI EM obtains the best quality of recommendation in our experiments .
We also varied α from 0.2 to 1 with increasing K from 5 to 30 and plotted the average recall at k in Figure 6(d ) . The graphs show that our recommendation algorithm with every range of α shows better quality than TFIDF F in terms of recall at k . With precision at k and average hit rank , our algorithm also outperforms TFIDF F but we do not provide the graphs for precision at k and average hit rank due to the lack of space . Our algorithm shows the best performance with the model parameters from TWITOBI EM using α=08 Top K tweet recommendation : We first ran TWITOBI T with varying ω from 0 to 1 . The the hit ratio was similar for the range of ω from 0.25 to 1 . However , when ω=0 , the score function does not utilize the parameter values p(f |u)s ( ie the information of followees ) at all for recommending tweets and thus the hit ratio becomes very low . Thus , we set the default value of ω to 0.5 in the our experiments .
We varied m from 5 to 20 while increasing K from 5 to 30 and plotted the recall at ks in Figure 7(a ) . We also varied α from 0.2 to 1 while increasing K from 5 to 30 and plotted the recall at ks in Figure 7(b ) . We do not provide the graphs for precision at k and average hit rank since they show almost the same trends with the top K followee recommendation in Figure 6(b ) and Figure 6(c ) .
Both graphs confirm that our recommendation algorithm with every range of m and α outperforms TFIDF T in terms of recall at k . Furthermore , the graphs show that our top K recommendation algorithm with α=1 performs worse than our algorithm with other values of α . Since our probabilistic model and EM algorithm with α=1 become the same with those of PSLI [ 11 ] , it shows that our model fits better than PLSI for the recommendation of tweet messages in Twitter .
C . Scalability of our EM Algorithm using MapReduce
We conducted experiments for TWITOBI MR varying the number of machines s to confirm that our algorithm is scalable . The default values of α and m are 0.8 and 10 respectively . We increased the number of machines s from 2 to 20 varying the number of partitions for users NR and the number of partitions for words NC . Since NR × NC should be the same as s , when s=2 , we set NR=2 and NC=1 . When s=6 , we set NR=3 and NC=2 . When s=15 , we set NR=5 and NC=3 . For s=20 , NR=5 and NC=4 .
Scalability : We varied the number of machines s from 1 to 20 with m=10 and m=20 . The log scale was used in the y axis of graph in Figure 8(a ) . As we use larger number of machines , the execution times for TWITOBI EM decrease since the algorithm is processed in parallel with MapReduce . In Figure 8(b ) , we show the same experimental result with “ relative scale ” which is the ratio between the execution time with one machine and the execution time with the current number of machines . We also plot the ideal speedup in Figure 8(b ) . For the relative scale , our algorithm shows linear speedup with increasing the number of machines .
TWITOBI EM and TWITOBI MR : We also experimented TWITOBI EM with a single machine . We found that TWITOBI EM is slightly faster than TWITOBI MR with two machines . However , as the number of machines increases , TWITOBI MR gets faster gradually . With 20 machines , TWITOBI MR was 10.6 times faster than TWITOBI EM using a single machine . We omit the detailed figures due to lack of space .
IX . CONCLUSION
In this paper , we proposed TWITOBI , a recommendation system for Twitter using probabilistic modeling to recommend top K users to follow and top K tweets to read for a user . We developed an estimation algorithm for learning our model parameters and its parallelized algorithm using MapReduce to handle large data . We also presented the ranking algorithms for top K followees or top K tweets to a user . Our performance study with real life data sets showed effectiveness and scalability of our algorithms .
ACKNOWLEDGMENT
This work was supported by the National Research Foundation of Korea(NRF ) grant funded by the Korea government(MEST ) ( No . 2011 0000349 ) .
REFERENCES
[ 1 ] Apache . Apache hadoop . http://hadoopapacheorg , 2010 . [ 2 ] D . P . Bertsekas . Nonlinear Programming ( Second ed ) Cambridge ,
1999 .
[ 3 ] M . Bianchini , M . Gori , and F . Scarselli . Inside pagerank . ACM Trans .
Internet Techn . , 5(1):92–128 , 2005 .
[ 4 ] D . M . Blei , A . Y . Ng , and M . I . Jordan . Latent dirichlet allocation .
Journal of Machine Learning Research , 2003 .
[ 5 ] A . Das , M . Datar , A . Garg , and S . Rajaram . Google news personal ization : scalable online collaborative filtering . In WWW , 2007 .
[ 6 ] J . Dean and S . Ghemawat . Mapreduce : Simplified data processing on large clusters . In OSDI , 2004 .
[ 7 ] A . P . Dempster , N . M . Laird , and D . B . Rubin . Maximum likelihood from incomplete data via the em algorithm . Journal of Royal Statist . Soc . , 39:1–38 , 1977 .
[ 8 ] H . Deng , M . R . Lyu , and I . King . A generalized co hits algorithm and its application to bipartite graphs . In KDD , 2009 .
[ 9 ] J . Hannon , M . Bennett , and B . Smyth . Recommending twitter users to follow using content and collaborative filtering approaches . In RecSys , 2010 .
[ 10 ] T . Haveliwala , S . Kamvar , and G . Jeh . An analytical comparison of approaches to personalizing pagerank . In Preprint , 2003 .
[ 11 ] T . Hofmann . Probabilistic latent semantic indexing . In SIGIR , 1999 . [ 12 ] T . Hofmann . Latent semantic models for collaborative filtering . ACM
Trans . Inf . Syst . , 22(1 ) , 2004 .
[ 13 ] W . Li and A . McCallum . Pachinko allocation : Dag structured mixture models of topic correlations . In ICML , 2006 .
[ 14 ] B . Mehta , T . Hofmann , and W . Nejdl . Robust collaborative filtering .
In RecSys , 2007 .
[ 15 ] Q . Mei , X . Ling , M . Wondra , H . Su , and C . Zhai . Topic sentiment mixture : modeling facets and opinions in weblogs . In WWW , 2007 .
[ 16 ] T . M . Mitchell . Machine Learning . WCB/McGraw Hill , 1997 . [ 17 ] M . J . Pazzani and D . Billsus . Learning and revising user profiles : The identification of interesting web sites . Machine Learning , 27(3):313– 331 , 1997 .
[ 18 ] G . Shani , D . Heckerman , and R . I . Brafman . An mdp based recom mender system . Journal of Machine Learning Research , 2005 .
[ 19 ] L . Si and R . Jin . Flexible mixture model for collaborative filtering .
In ICML , 2003 .
[ 20 ] C . Smith . show stunning growth .
Twitter user statistics http://wwwhuffingtonpostcom/2011/03/14/twitter userstatistics n 835581.html , 2011 .
[ 21 ] G . Strang . Linear Algebra and Its Applications . Brooks Cole , 1988 . [ 22 ] C . F . J . Wu . On the convergence properties of the em algorithm . The
Annals of Statistics , 11(1):95–103 , 1983 .
[ 23 ] H . Yildirim and M . S . Krishnamoorthy . A random walk method for alleviating the sparsity problem in collaborative filtering . In RecSys , 2008 .
[ 24 ] M . Deshpande and G . Karypis .
Item based top N recommendation algorithms . ACM Trans . Inf . Syst . , 2004 .
[ 25 ] J . Chen , R . Nairn , L . Nelson , M . S . Bernstein and E . H . Chi . Short and tweet : experiments on recommending content from information streams . In CHI , 2010 .
[ 26 ] Twitter API Document . http://devtwittercom/doc , 2011 . [ 27 ] B . Vuong , E . Lim , A . Sun , M . Le and H . W . Lauw . On ranking controversies in wikipedia : models and evaluation . In WSDM , 2008 . [ 28 ] J . Chang and D . M . Blei . Relational Topic Models for Document
Networks . Journal of Machine Learning Research , 2009 .
[ 29 ] Y . Liu , A . Niculescu Mizil and W . Gryc . Topic link LDA : joint models of topic and author community . ICML , 2009 .
349
