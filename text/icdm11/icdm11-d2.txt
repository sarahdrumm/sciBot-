LPTA : A Probabilistic Model for Latent Periodic Topic Analysis
Zhijun Yin1 , Liangliang Cao2 , Jiawei Han1 , Chengxiang Zhai1 , Thomas Huang3
1Department of Computer Science , University of Illinois at Urbana Champaign , Urbana , IL , USA
2IBM T . J . Watson Research Center , Hawthorne , NY , USA
3Department of ECE and Beckman Institute , University of Illinois at Urbana Champaign , Urbana , IL , USA zyin3@illinois.edu , liangliangcao@usibmcom , hanj@csuiucedu , czhai@csuiucedu , t huang1@illinois.edu
Abstract—This paper studies the problem of latent periodic topic analysis from timestamped documents . The examples of timestamped documents include news articles , sales records , financial reports , TV programs , and more recently , posts from social media websites such as Flickr , Twitter , and Facebook . Different from detecting periodic patterns in traditional time series database , we discover the topics of coherent semantics and periodic characteristics where a topic is represented by a distribution of words . We propose a model called LPTA ( Latent Periodic Topic Analysis ) that exploits the periodicity of the terms as well as term co occurrences . To show the effectiveness of our model , we collect several representative datasets including Seminar , DBLP and Flickr . The results show that our model can discover the latent periodic topics effectively and leverage the information from both text and time well .
Keywords periodic topics ; topic modeling ;
I . INTRODUCTION
Periodic phenomena exist ubiquitously in our lives , and lots of natural and social topics have periodic recurring patterns . Hurricanes strike over the similar seasons every year . Many music and film festivals are held during similar periods annually . Sales offered by different brands culminate during Thanksgiving and Christmas every year . TV programs usually follow weekly schedules . Publicly traded companies are required to disclose information on an ongoing basis by submitting both annual reports and quarterly reports . Due to the prevalent existence of periodic topics , periodicity analysis is important in real world . Based on the discovered periodic patterns , people can not only analyze natural phenomena and human behavior , but also predict the future trends and help decision making .
Nowadays with the development of the Web , many text data exist with time information , eg , news articles associated with their publishing dates , tagged photos annotated with their taken dates in Flickr1 and published tweets along with their upload times in Twitter2 . A lot of useful information is embedded in these text data , and it is interesting to discover topics that are periodic and characterize their temporal patterns .
Due to the importance of periodicity analysis , many research works have been proposed in periodicity detection for
1http://wwwflickrcom 2http://twitter.com time series database [ 20 ] , [ 9 ] , [ 30 ] , [ 7 ] , [ 26 ] . Some studies follow the similar strategies to analyze the time distribution of a single tag or query to detect periodic patterns [ 25 ] , [ 6 ] , [ 22 ] . However , most of the existing studies are limited to time series database and cannot be applied on text data directly . First , a single word is not enough to describe a topic , and more words are needed to summarize a topic comprehensively . Second , analyzing the periodicity of single terms only is not sufficient to discover periodic topics . For example , the words like “ music ” , “ festival ” and “ chicago ” may not have periodic patterns if considered separately , but there may be periodic topics if these words are considered together . Third , there are synonyms and polysemy words due to the language diversity , which makes the problem even more challenging .
In this paper , we propose a model called LPTA ( Latent Periodic Topic Analysis ) to handle the above difficulties . Instead of analyzing periodicity based on the occurrence of single terms or patterns , our model exploits the periodicity of the terms as well as term co occurrences , and in the end discovers the periodic topics where a topic is represented by a distribution of words . Our method can be viewed as a variant of latent topic models , where a document is generated by several latent topics which correspond to the semantic concepts of interests . Popular latent topic models include Probabilistic Latent Semantic Analysis ( PLSA ) [ 11 ] , Latent Dirichlet Allocation ( LDA ) [ 4 ] , and many variants of them ( see Section VI for a detailed review of these models ) . Unlike these traditional models , LPTA focuses on the periodic property in the time domain . The goal of learning LPTA is not only to find a latent topic space to fit the data corpus , but also detect whether a topic is periodic or not .
The contributions of this paper are summarized as follows .
1 ) We introduce the problem of latent periodic topic analysis that has not been studied before .
2 ) We propose the LPTA model to discover periodic topics by exploring both the periodic properties and the co occurrence structures of the terms .
3 ) We perform extensive experiments on several representative datasets to demonstrate the effectiveness of our method .
Table I
NOTATIONS USED IN THE PAPER .
Description Vocabulary ( word set ) , w is a word in V Document collection A document d that consists of words and timestamp The text of document d The timestamp of document d The topic set , z is a topic in Z The word distribution set for Z
V D d wd td Z θ
The rest of the paper is organized as follows . We formulate the problem of latent periodic topic analysis in Section II . We propose our LPTA model in Section III . We analyze the complexity of the algorithm and illustrate its connection to other models in Section IV . We compare the performance of different methods in Section V . We summarize the related work in Section VI and conclude the paper in Section VII .
II . PROBLEM FORMULATION
In this section , we define the problem of latent periodic topic analysis . The notations used in this paper are listed in Table II .
DEFINITION 1 . A topic is a semantically coherent theme , which is represented by a multinomial distribution of words . Formally , each topic z is represented by a word distribution
θz = {p(w|z)}w∈V st w∈V p(w|z ) = 1 .
DEFINITION 2 . A periodic topic is a topic repeating in regular intervals . Formally , the conditional probability of time t given topic z , ie , p(t|z ) , follows periodic patterns in terms of periodic interval T . In order words , the timestamp distribution for each topic has bursts every interval T . Periodic interval T can be defined by users according to their needs , such as 1 week ( weekly ) , 1 month ( monthly ) , 1 year ( annually ) , etc .
DEFINITION 3 . A timestamped document is a text document associated with a timestamp . A timestamped document can be a news article along with its release date . It can also be a tweet associated with its publishing time in Twitter . Another example is a tagged photo uploaded to Flickr where the tags are considered as text and the time when the photo was taken is considered as its timestamp .
Given the definitions of timestamped document and periodic topic , we define the problem of latent periodic topic analysis as follows .
DEFINITION 4 . Given a collection of timestamped documents D , periodic interval T and the number of topics K , we would like to discover K periodic topics repeating every interval T , ie , θ = {θz}z∈Z where Z is the topic set , along with their time distributions {p(t|z)}z∈Z .
Here we give an example of latent periodic topic analysis . Example 1 . Given a collection of photos related to music festival along with tags and timestamps in Flickr , the desired periodic topics are annual music festivals such as South By Southwest every March , Coachella every April , Lollapalooza every August , etc . As shown in Figure II , the topic related to Coachella festival occurs in April every year . The top words in the topic are coachella(0.1106 ) , music(0.0915 ) , indio(0.0719 ) , california(0.0594 ) and concert(0.0357 ) where the numbers in the parentheses are the weights of the corresponding words in θz provided that topic z is the topic of Coachella festival .
Figure 1 . Coachella festival .
The distribution of the timestamps for the topic related to
In the following sections , we present our model for latent periodic topic analysis .
III . LATENT PERIODIC TOPIC ANALYSIS
In this section , we propose our LPTA ( Latent Periodic Topic Analysis ) model . First , we introduce the general idea of our model . Second , we present the detail of our periodic topic generative process . Third , we explain how to estimate the parameters . A . General Idea
In general , the temporal patterns of topics can be classified into three types : periodic topics , background topics , and bursty topics . A periodic topic is one repeating in regular intervals ; a background topic is one covered uniformly over the entire period ; a bursty topic is a transient topic that is intensively covered only in a certain time period . We assume that a word is generated by a mixture of these topics and infer the most likely time domain behaviors . We will discuss how to model three kinds of topics and then study how to infer the mixture model . To encode the periodic topics , we take both the temporal structure and term cooccurrence into consideration . The words occurring around the same time in each period are likely to be clustered . If two words co occur often in the same documents , they are more likely to belong to the same topic . In order to capture this property , we assume the timestamps of each periodic topic follow similar patterns in each period . Specifically , we model the distribution of timestamps for each periodic topic as a
200620072008200920102011000501 mixture of Gaussian distributions where the interval between the consecutive components is T . In addition to periodic topics , the document collection may contain background words . In order to alleviate the problem of background noises , we model the background topics as well in our model . In particular , the timestamps of the background topics are generated by a uniform distribution . Other than periodic topics and background topics , we employ bursty topics to model patterns with bursting behavior in a short period but not regularly . The timestamps of the bursty topics are generated from a Gaussian distribution . Therefore , the document collection is modeled as a mixture of background topics , bursty topics and periodic topics . By fitting such a mixture model to timestamped text data , we can discover periodic topics along with their time distributions .
B . LPTA Framework st φd = {p(z|d)}z∈Z st
Let us denote the topic set as Z and the word distribution set as θ , ie , {θz}z∈Z where θz = {p(w|z)}w∈V w∈V p(w|z ) = 1 . φ is the multinomial distributions for topics conditioned on documents , ie , {φd}d∈D where z∈Z p(z|d ) = 1 . µ and σ are the collections of the means and standard deviations of timestamps for bursty topics and periodic topics . µz and σz are the mean and standard deviation of timestamps for topic z respectively . The generative procedure of latent periodic topic analysis model is described as follows .
To generate each word in document d from collection D : 1 ) Sample a topic z from multinomial φd .
( a ) If z is a background topic , sample time t from a uniform distribution [ tstart , tend ] , where tstart and tend are the start time and end time of the document collection . ( b ) If z is a bursty topic , sample t from N ( µz , σ2 ( c ) If z is a periodic topic , sample period k of document d from a uniform distribution . Sample time z ) , where T is periodic interval . t from N ( µz + kT , σ2 z ) .
2 ) Sample a word w from multinomial θz . Given the data collection {(wd , td)}d∈D where wd is the word set in document d and td is the timestamp of document d , the log likelihood of the collection given Ψ = {θ , φ , µ , σ} is as follows .
L(Ψ ; D ) = log p(D|Ψ )
= log p(wd , td|Ψ )
( 1 ) d∈D z
If topic z is a background topic , p(t|z ) is modeled as a uniform distribution : p(t|z ) =
1
( 3 ) If topic z is a bursty topic , p(t|z ) is modeled as a Gaussian tend − tstart distribution : p(t|z ) =
1√ 2πσz
( 4 ) If topic z is a periodic topic , p(t|z ) is modeled as a
σ2 z e
− ( t−µz )2 mixture of Gaussian distributions : p(t|z ) = p(t|z , k)p(k )
( 5 ) k where k is the period id , p(t|z , k ) = and p(k ) is uniform in terms of k .
− ( t−µz−kT )2
σ2 z
1√
2πσz e
C . Parameter Estimation
In order to estimate parameters Ψ in Equation 1 , we use maximum likelihood estimation . Specifically , we use Expectation Maximization ( EM ) algorithm to solve the problem , which iteratively computes a local maximum of likelihood . We introduce the probability of the hidden variable p(z|d , w ) , which is the probability that word w in document d belongs to topic z . In the E step , it computes the expectation of the complete likelihood Q(Ψ|Ψ(t) ) , where Ψ(t ) is the value of Ψ estimated in iteration t . In the M step , it finds the estimation Ψ(t+1 ) that maximizes the expectation of the complete likelihood .
In the E step , p(z|d , w ) is updated according to Bayes formula as in Equation 6 . p(z|d , w ) =
In the M step , p(w|z ) and p(z|d ) are updated as follows . p(w|z ) = p(z|d ) = d p(td|z)p(w|z)p(z|d ) z p(td|z)p(w|z)p(z|d ) d n(d , w)p(z|d , w ) w n(d , w)p(z|d , w ) w n(d , w)p(z|d , w ) z n(d , w)p(z|d , w ) w n(d , w)p(z|d , w)td w n(d , w)p(z|d , w ) w n(d , w)p(z|d , w)(td − µz)2 w n(d , w)p(z|d , w ) w d d d
µz = d
If topic z is bursty topic , µz and σz are updated accord ingly as follows .
( 6 )
( 7 )
( 8 )
( 9 )
)1/2
( 10 ) log p(wd , td|Ψ ) = d w n(d , w ) log p(td|z)p(w|z)p(z|d )
( 2 )
σz = ( where n(d , w ) is the count of word w in document d .
If topic z is a periodic topic , we partition the time line into intervals of length T and assume that each document is only related to its corresponding interval . In other words , p(td|z , k ) in Equation 5 is set as 0 if document d is not in the k th interval . µz and σz for periodic topic z can be updated according to the following steps . d w n(d , w)p(z|d , w)(td − IdT ) w n(d , w)p(z|d , w ) w n(d , w)p(z|d , w)(td − µz − IdT )2 w n(d , w)p(z|d , w ) d d
µz = d
σz = (
( 11 )
)1/2
( 12 ) where Id is the corresponding interval of document d .
IV . DISCUSSION
A . Complexity Analysis
We analyze the complexity of parameter estimation process in Section III C . In the E step , it needs O(K|W| ) to calculate p(z|d , w ) in Equation 6 for all ( z , d , w ) triples , where K is the number of topics and |W| is the total counts of the words in all the documents . In the M step , it needs O(K|W| ) to update p(w|z ) according to Equation 7 for all ( w , z ) pairs and O(K|W| ) to update p(z|d ) according to Equation 8 for all ( z , d ) pairs . It needs O(|W| ) to update µz in Equation 9 and O(|W| ) to update σz in Equation 10 for each bursty topic z . Similarly , it needs O(|W| ) to update µz in Equation 11 and O(|W| ) to update σz in Equation 12 for each periodic topic z . Therefore , the complexity of the LPTA model is O(iterK|W| ) , where iter is the number of the iterations in the EM algorithm . B . Parameter Setting ie ,
In LPTA , we have two types of parameters , the number of topics K and the length of periodic interval T . Users can specify the value of K according to their needs . For example , if topics of finer granularity are to be discovered , K can be set to a relatively large number , whereas if topics of coarser granularity are desired , K can be set to a relatively small value . When the parameters are unknown , Schwarz ’s Bayesian information criterion ( BIC ) provides an efficient way to select the parameters . The BIC measure includes two parts : the log likelihood and the model complexity . The first part characterizes the fitness over the observations , while the second is determined by the number of parameters . In practice we can train models with different parameters , and compare their BIC values . The model with the lowest value will be selected as the final model . For periodic interval T , users can specify as 1 week ( for weekly topics ) , 1 year ( for annual topics ) , etc . Besides , instead of fixing the periodic interval as one value , we can also make a mixture of topics with different periodic intervals . In this way , we can discover the topics of different periodic intervals simultaneously . Specifically , a bursty topic can be considered as a periodic topic with only one period during the entire time span . We will study how to extract the periodic interval automatically in future work .
C . Connections to Other Models
Probabilistic Latent Semantic Analysis PLSA is a latent variable model for co occurrence data which associates an unobserved topic variable with the occurrence of a word in a particular document [ 11 ] . PLSA does not consider the time information , and it can be considered as a special case of our LPTA model when all the topics are background topics . Retrospective News Event Detection RED is a probabilistic model to incorporate both content and time information to detect retrospective news event [ 19 ] . Although RED models the time information into the framework , it can only detect bursty topics with unigram models . RED can be considered as a simplified version of our LPTA framework , which contains bursty topics only and uses a mixture of unigram models .
Topic Over Time TOT is an LDA style generative model to extract the evolutionary topic patterns in timestamped documents [ 27 ] . In our model LPTA , we model background topics , bursty topics as well as periodic topics . Compared with TOT , LPTA focuses on recurring periodic topic patterns instead of the evolution of the topics .
V . EXPERIMENT
In this section , we demonstrate the evaluation results of our method . First , we introduce the datasets used in the experiment . Second , we compare our method with other methods on these datasets qualitatively . Third , we use multiple measures including accuracy and normalized mutual information to evaluate our method quantitatively .
A . Datasets
In this paper , we evaluate our ideas on several represen tative datasets from real life to social media .
• Seminar We collected the weekly seminar announcements for one semester from six research groups in computer science department at University of Illinois at Urbana Champaign3 . The research groups include AIIS ( Artificial Intelligence and Information Systems ) , DAIS ( Database and Information Systems ) , Graphics , HCI , Theory and UPCRC ( Universal Parallel Computing Research Center ) . The seminar time is considered as the document timestamp . We would like to discover weekly topics , so we set periodic interval T as 1 week . The dataset has 61 documents and 901 unique words . • DBLP Digital Bibliography Project ( DBLP)4 is a computer science bibliography . We collected the paper titles of several different conferences from 2003 to 2007 . The conferences include WWW , SIGMOD , SIGIR , KDD , VLDB and NIPS . The timestamps of the documents are determined according to the conference programs .
3http://csillinoisedu/ 4http://wwwinformatikuni trierde/∼ley/db/
We would like to discover annual topics , so we set periodic interval T as 1 year . The resulting dataset has 4070 documents and 2132 unique words .
• Flickr Flickr is an online photo sharing website . We crawled images through Flickr API5 . The tags of a photo are considered as document text , while the time when the photo was taken is considered as document timestamp . Specifically , we crawled the photos for several music festivals from 2006 to 2010 including SXSW ( South by Southwest ) , Coachella , Bonnaroo , Lollapalooza and ACL ( Austin City Limits ) . We would like to discover annual topics , so we set periodic interval T as 1 year . The resulting dataset has 84244 documents and 7524 unique words .
B . Qualitative Evaluation
1 ) Topics Discovered by LPTA : We set the number of periodic topics as 6 in both Seminar and DBLP datasets and 5 in Flickr dataset according to our construction of these datasets . We evaluate the change of the number of topics in quantitative evaluation in Section V C . We list selected topics discovered by LPTA in different datasets in Table II . In Seminar dataset , LPTA can effectively discover the topics for different research groups and their corresponding seminar time . For example , Topic 1 is DAIS at 16:00 every Tuesday , where data , text and mining are the popular words . Topic 2 is AIIS at 14:00 every Friday , which focuses on machine learning and algorithms . In DBLP dataset , LPTA can identify six periodic topics , ie , six annual conferences . For example , Topic 1 is KDD in August , which focuses on data mining . Topic 2 is SIGIR . The terms like retrieval , web , search , relevance and evaluation are the core topics in SIGIR . In Flickr dataset , LPTA can clearly detect the music festivals as well as their durations . For example , Topic 1 is about ACL , which is held around late September in zilker park , austin , texas . Since the dates that ACL took place were not fixed every year , ie , Sep 15 17 in 2006 , Sep 14 16 in 2007 , Sep 26 28 in 2008 , Oct 2 4 in 2009 and Oct 8 10 in 2010 , the standard deviation of the timestamps is 10d13h20m . Topic 2 is about Bonnaroo in manchester , tennessee . Since the dates of Bonnaroo did not vary too much every year , the standard deviation of the timestamps for Bonnaroo is only 2d14h21m .
In order
2 ) LPTA vs . Periodicity Detection : to see whether pooling together related words is better than analyzing periodicity at single word level , we make a comparison between LPTA and periodicity detection method . We attempt to detect periodic words by periodicity detection algorithm . Fourier decomposition represents a sequence as a linear combination of the complex sinusoids . To identify the power content of each frequency , the power spectral density PSD
5http://wwwflickrcom/services/api/
( or power spectrum ) of a sequence is used to indicate the signal power at each frequency in the spectrum [ 25 ] . A well known estimator of the PSD is the periodogram , which is a vector comprised of the squared magnitude of the Fourier coefficients . We use AUTOPERIOD [ 26 ] , a two tier approach by considering the information in both the autocorrelation and the periodogram , to detect periods for each word . Unfortunately , the method fails to detect meaningful periodic words because the time series are sparse and few words have apparent periodic patterns . Most of the words do not occur periodically without considering topics .
Compared with single word representation , LPTA uses multiple words to describe a topic . For example , in DBLP dataset , LPTA discovers topic VLDB with the word distribution data 0.0530 , xml 0.0208 , query 0.0196 , queries 0.0176 , efficient 0.0151 , mining 0.0142 , database 0.0136 , streams 0.0112 , databases 00111 We can see that a single word may not be enough to represent such a topic and multiple words can represent a topic better . LPTA can not only provide a more comprehensive description of the topic , but also discover the periodic topic when its consisting words do not have periodic patterns separately . In LPTA , we can plot the time distributions of the discovered topics based on p(d|z ) and document timestamps , where p(d|z ) can be obtained from p(z|d ) according to Bayes’ theorem . In Figure 2 , we plot the time distribution of topic VLDB in DBLP dataset as well as the time distributions of word data , xml and query which are the top popular words in the topic . We can see that topic VLDB has the clear periodic patterns while data , xml and query do not occur periodically . It shows that LPTA can discover the periodic topics effectively even if its consisting words do not have periodic patterns by themselves .
3 ) LPTA vs . Topic Models :
In order to see whether traditional topic models can detect meaningful topics , we compare the results of topic modeling methods including PLSA and LDA with the one of LPTA . We set the number of topics as 6 in both Seminar and DBLP datasets and 5 in Flickr dataset for both PLSA and LDA . We list several selected topics by using PLSA and LDA in Table III . Since the words in computer science areas are closely related , PLSA and LDA cannot identify the topics of different research areas in Seminar dataset . In DBLP dataset , all the topics are similar , so both PLSA and LDA cannot discover the meaningful topic clusters . In Flickr dataset , PLSA mixes several music festivals together . For example , both southbysouthwest and coachella appear in Topic 1 , and in Topic 2 lollapalooza and austincitylimits are merged together . We find that LDA performs better than PLSA in this dataset . LDA can discover several festivals although it mixes coachella and bonnaroo in Topic 1 . Compared with the result of LPTA in Table II , we can see that LPTA can discover the meaningful topics of better quality .
Table II
DBLP
SELECTED PERIODIC TOPICS DISCOVERED BY USING LPTA . THE DATE AND THE DURATION IN THE PARENTHESES ARE THE MEAN AND STANDARD
DEVIATION OF THE TIMESTAMPS FOR THE CORRESPONDING PERIODIC TOPIC .
Seminar
Topic 1 ( DAIS )
Tue 16:00 ( 0h0m0s ) model 0.0166 based 0.0158 mining 0.0151 text 0.0143 network 0.0135 web 0.0119 problem 0.0111 data 0.0111 query 0.0111 latent 0.0095
Topic 2 ( AIIS )
Fri 14:00 ( 0h0m0s ) computer 0.0168 learning 0.0158 machine 0.0138 science 0.0128 algorithms 0.0128 language 0.0118 work 0.0108 problems 0.0108 models 0.0108 prediction 0.0108
Topic 1 ( KDD ) mining 0.0353 data 0.0289 search 0.0233
Topic 2(SIGIR ) Aug 23 ( 10d3h11m ) Aug 3 ( 9d6h56m ) retrieval 0.0495 based 0.0197 web 0.0189 text 0.0171 query 0.0164 search 0.0162 based 0.0195 web 0.0168 clustering 0.0208 learning 0.0159 networks 0.0114 analysis 0.0105 large 0.0104 document 0.0149 language 0.0118 relevance 0.0111 evaluation 0.0111
Flickr
Topic 1 ( ACL )
Sep 29 ( 10d13h20m ) austincitylim . 0.0442 acl 0.0945 austin 0.0827 music 0.0763 limits 0.0441 city 0.0441 texas 0.0426 concert 0.0283 live 0.0212 zilker 0.0173
Topic 2 ( Bonnaroo ) Jun 16 ( 2d14h21m ) bonnaroo 0.1066 music 0.0870 manchester 0.0587 tennessee 0.0518 live 0.0327 concert 0.0275 arts 0.0175 performance 0.0174 backstagegall . 0.0113 rock 0.0111
SELECTED TOPICS DISCOVERED FOR DIFFERENT DATASETS BY USING PLSA AND LDA .
Table III
Seminar
DBLP
PLSA
LDA
PLSA
LDA intel talk
Topic 1 data latent visualizati .
Topic 1 Topic 2 Topic 2 memory problem systems computer algorithm computer science
Topic 1 web data xml data mining parallel science pattern network graph time time agent algorithms queries mining semantic search streams engineering managem . networks influence visualizati . online work data analysis computer systems programm . machine hardware algorithms question visual
Topic 2 search text
Topic 1 web mining
Topic 2 system database databases semantic distributed relational detection automatic services adaptive analysis content ranking applicatic . relevance structure support extraction feedback image graph user user function adaptive evaluation patterns
Flickr
PLSA
LDA
Topic 2
Topic 1 lollapaloo . music
Topic 2 lollapaloo .
Topic 1 sxsw austin music texas southbyso . live atx music chicago concert acl live grantpark indio coachella bonnaroo california manchester music chicago live concert grantpark tennessee august photos summer palmsprin . performan . arts art performan . coachella austincity . downtown livemusic august austin
4 ) Integration of Text and Time Information : To demonstrate the effectiveness of LPTA model for combining the information of both text and time , we study the following two specific cases in DBLP dataset .
SIGMOD vs . VLDB SIGMOD and VLDB are two reputed conferences in database area , and the concentrated topics in these two conferences are similar . Therefore , it is difficult to differentiate these two conferences based on text only . However , SIGMOD is usually held in June , while VLDB is usually held in September . In LPTA , we discover the periodic topics by considering the information from both text and time , so we can easily identify these two topics . We set the number of periodic topics as 2 and show the topics in Table IV . As we can see from Table IV , Topic 1 is SIGMOD on Jun 17 with the standard deviation 7d11h6m and Topic 2 is VLDB on Sep 11 with the standard deviation 9d5h29m . Although the popular words in both of the topics are data , query and xml , these two topics can be clustered because the timestamps form two clusters .
SIGMOD vs . CVPR SIGMOD and CVPR are held in June , so it is difficult to differentiate these two if we rely on time information only . However , SIGMOD is a database conference while CVPR is a computer vision conference .
Therefore , in this case , text information will help identify these two topics even though the timestamps of these two topics overlap with each other . We set the number of periodic topics as 2 and show the topics in Table IV . As we can see from Table IV , Topic 1 is SIGMOD with its focus on data , query , xml , database and system and Topic 2 is CVPR focusing on image , recognition , tracking , detection and segmentation .
5 ) Periodic vs . Bursty Topics : To demonstrate the effectiveness of LPTA model for balancing periodic and bursty topics , we study the following case in Flickr dataset . Instead of pooling the photos related to music festivals all together , we keep the photos related to SXSW and ACL festivals from 2006 to 2010 and those related to Coachella and Lollapalooza in 2009 only . In this way , we simulate the dataset with 2 periodic topics and 2 bursty topics . We set the number of periodic topics as 2 and the number of bursty topics as 2 in LPTA and show the discovered topics in Table V . From Table V , we can see that the words recurring during similar periods every year like sxsw and acl fit into two corresponding periodic topics ( ie , Topic 1 and Topic 2 ) , while the words that occur only in one period like lollapalooza , chicago , grantpark , illinois , coachella , indio
Topic VLDB discovered by LPTA
Word “ data ”
Word “ xml ”
Table IV
PERIODIC TOPICS FOR SIGMOD VS . VLDB AND SIGMOD VS . CVPR DATASETS BY USING LPTA . THE DATE AND THE DURATION ARE THE
MEAN AND STANDARD DEVIATION OF THE TIMESTAMPS . SIGMOD vs . CVPR
SIGMOD vs . VLDB Topic 1
( SIGMOD )
Jun 17
( 7d11h6m ) data query xml database processing efficient databases queries web system
Topic 2 ( VLDB ) Sep 11
( 9d5h29m ) data xml query queries efficient database based databases system processing
Topic 1
( SIGMOD )
Jun 20
( 7d15h42m ) data query xml database processing efficient based system databases queries
( 3d4h37m )
Topic 2 ( CVPR ) Jun 21 image based tracking learning object shape recognition segmentation detection motion
Table V
TOPICS DISCOVERED FOR PERIODIC VS . BURSTY DATASET BY USING LPTA . THE DATE AND THE DURATION ARE THE MEAN AND STANDARD
DEVIATION OF THE TIMESTAMPS .
Bursty topics
Periodic topics
Topic 1
( Lollapalooza ) Aug 8 2009 ( 1d0h12m ) lollapalooza
Topic 2
( Coachella ) Apr 17 2009 ( 10d20h23m ) coachella chicago concert music grantpark august live illinois indio music california concert live desert art performance lolla musicfestival livemusic
( 6d8h33m )
( 14d7h22m )
Topic 3 ( SXSW ) Mar 18 sxsw austin texas music southbysouth . live concert downtown atx gig
Topic 4 ( ACL ) Sep 28 acl austin music city limits texas concert live zilker austincityli .
Word “ query ”
Figure 2 . Time distribution of topic VLDB discovered by LPTA and time distributions of the words in the topic . and california fit into two corresponding bursty topics ( ie , Topic 3 and Topic 4 ) . LPTA can differentiate between the bursty topics and periodic topics in this dataset . The mean dates for periodic topics SXSW and ACL are Mar 18 and Sep 28 every year , and the mean dates for bursty topics Lollapalooza and Coachella are Aug 8 2009 and Apr 17 2009 , respectively .
6 ) Summary : From the above qualitative evaluation , we can see that compared with periodicity detection for every single word , LPTA can not only provide a more comprehensive description of a topic , but also discover the periodic topic even when its consisting words do not have periodic patterns separately . Compared with topic modeling methods including PLSA and LDA , LPTA can discover the periodic topics with more meaningful semantics . Besides , LPTA can identify the mean date and its standard deviation for each periodic topic effectively . From the SIGMOD vs . VLDB and SIGMOD vs . CVPR datasets in DBLP , we can see that it is difficult to discover meaningful topics without the combination of text and time information and LPTA achieves good balance between these two . With regards to the tradeoff between periodic topics and bursty topics , from periodic vs . bursty dataset in Flickr , we can see that the words will fit into the corresponding periodic or bursty topics if they have periodic or bursty patterns .
C . Quantitative Evaluation
1 ) Evaluation Metric : To evaluate the results quantitatively , we provide some evaluation metrics to compare the results . The latent topics discovered by the topic modeling approaches can be regarded as clusters . Based on the estimated conditional probability of topic z given document ie , p(z|d ) , we can infer the cluster label for docud ,
20032004200520062007200800010020030042003200420052006200720080001002003200320042005200620072008000050010015200320042005200620072008000050010015 ment d . Therefore , accuracy ( AC ) and normalized mutual information ( NMI ) can be used to measure the clustering performance [ 5 ] . Given document d , its label ld in the dataset and the topic zd for document d obtained from the topic modeling approach , accuracy is defined as follows . d δ(ld , map(zd ) )
|D|
AC =
( 13 ) where |D| is the number of all the documents and δ(x , y ) is the delta function that is one if x = y and is zero otherwise , and map(zd ) is the permutation mapping function that maps the topic zd of document d to the corresponding label in the dataset . The best mapping between the topics and document labels can be found by Kuhn Munkres algorithm [ 14 ] .
We denote L as the set of document labels obtained from the dataset and Z as the topics obtained from the topic modeling approaches . The mutual information metric M I(L , Z ) between L and Z is defined as follows . l∈L,z∈Z
M I(L , Z ) = p(l , z ) log p(l , z ) p(l)p(z )
( 14 ) where p(l ) and p(z ) are the probabilities that a document arbitrarily selected from the dataset has label l or belongs to topic z , and p(l , z ) is the joint probability that a arbitrarily selected document has label l and belongs to topic z . The normalized mutual information NMI is defined as follows .
N M I(L , Z ) =
M I(L , Z ) max(H(L ) , H(Z ) )
( 15 ) where H(L ) and H(Z ) are the entropies of L and Z . Specifically , N M I = 1 if L and Z are identical , and N M I = 0 if L and Z are independent .
2 ) Performance Evaluations and Comparisons :
In Table VI , we list the comparison of accuracy and normalized mutual information by using different methods in different datasets . We vary the number of topics from 2 to 10 . From Table VI , we can see that LPTA performs significantly better than PLSA and LDA . In average , LDA performs better than PLSA , but is not as good as LPTA . It demonstrates that LPTA makes good use of the text and time information . Accuracy and NMI of PLSA and LDA in Flickr dataset are higher compared with other datasets . The reason is that the topical clusters are relatively apparent in Flickr while the clusters are not clear in both Seminar and DBLP datasets . In Seminar and DBLP datasets , the words are related to computer science , it is difficult to differentiate subjects in various research areas . Especially in DBLP dataset , the conferences from database , data mining , information retrieval and machine learning are closely related to each other , it is difficult to cluster them without considering the periodic patterns , which explains why accuracy and NMI in DBLP dataset are extremely low . However , LPTA is stable and has relatively high values in both accuracy and NMI in all the datasets , because LPTA leverages both the topical clusters and periodic patterns .
VI . RELATED WORK
In this section we discuss related work to our study , including temporal topic mining , event detection and tracking and periodic pattern mining .
Temporal topic mining Topic modeling is a classic problem in text mining . The representative algorithms include PLSA [ 11 ] and LDA [ 4 ] . Besides modeling the text itself , many other methods have been proposed to mine topics from documents associated with timestamps . Wang et al . [ 27 ] used an LDA style topic model to capture both the topic structure and the changes over time . Mei et al . [ 21 ] partitioned the timeline into buckets and proposed a probabilistic approach to model the subtopic themes and spatiotemporal theme patterns simultaneously in weblogs . Wang et al . mined correlated bursty topic patterns from coordinated text streams in [ 28 ] . Blei and Lafferty [ 3 ] employed state space models on the natural parameters of multinomial distributions of topics and design a dynamic topic model to model the time evolution of stream . Iwata et al . [ 12 ] proposed an online topic model for sequentially analyzing the time evolution of topics in document collections , in which current topic specific distributions over words are assumed to be generated based on the multiscale word distributions of the previous epoch . Stochastic EM algorithm was used in the online inference process . In [ 32 ] , Zhang et al . discovered different evolving patterns of clusters , including emergence , disappearance , evolution within a corpus and across different corpora . The problem was formulated as a series of hierarchical Dirichlet processes by adding time dependencies to the adjacent epochs , and a cascaded Gibbs sampling scheme is used to infer the model . All the existing studies on temporal topic mining focus on the evolutionary pattern of the topics , but they do not study the periodic topics . Instead of studying the evolution of the topics , we focus on periodic topic patterns in this paper .
Event detection and tracking In [ 1 ] , Allan et al . introduced the problems of event detection and tracking within a stream of broadcast news stories . To extract meaningful structure from document streams that arrive continuously over time is a fundamental problem in text mining [ 13 ] . Kleinberg developed a formal approach for modeling the stream using an infinite state automaton to identify the bursts efficiently . Fung et al . [ 8 ] proposed Time Driven Documents partition framework to construct a feature based event hierarchy for a text corpus based on a given query . In [ 19 ] , Li et al . proposed a probabilistic model to incorporate both content and time information in a unified framework to detect the retrospective news events . In [ 10 ] , He et al . used concepts from physics to model bursts as intervals of increasing
ACCURACY AND NORMALIZED MUTUAL INFORMATION IN DIFFERENT DATASETS BY USING DIFFERENT METHODS .
Table VI
Seminar
DBLP
Flickr
Accuracy( % )
NMI( % )
Accuracy( % )
NMI( % )
Accuracy( % )
NMI( % )
PLSA LDA LPTA PLSA LDA LPTA PLSA LDA LPTA PLSA LDA LPTA PLSA LDA LPTA PLSA LDA LPTA 37.2 31.1 54.9 37.0 67.4 39.4 79.2 40.1 82.1 43.0 80.2 40.8 77.6 39.0 74.7 35.3 73.1 34.9 69.6 37.9
38.3 51.1 61.5 66.1 67.8 65.9 66.7 65.1 63.6 60.7
49.7 63.1 74.8 85.7 90.2 89.6 86.5 83.7 81.4 78.3
23.9 45.7 56.7 63.0 65.9 63.8 63.1 60.8 58.2 55.7
37.7 51.0 65.4 78.5 90.4 94.5 91.9 90.0 88.1 76.4
34.7 53.0 70.7 82.4 92.3 94.2 91.7 88.8 86.8 77.2
25.4 26.8 27.7 28.7 27.8 26.2 23.9 22.3 20.6 25.5
45.7 57.7 63.7 69.2 67.6 67.2 66.0 64.2 63.1 62.7
11.7 19.0 23.6 25.7 30.6 30.5 30.4 30.5 31.7 26.0
12.3 19.9 24.0 26.6 28.9 29.7 31.0 30.8 30.2 26.0
22.4 35.9 42.2 48.6 47.9 46.5 45.7 44.3 43.5 41.9
28.3 42.1 53.8 59.9 60.2 54.3 53.1 50.6 51.4 50.4
48.9 59.9 70.6 74.8 78.5 71.5 69.8 64.5 67.7 67.3
24.2 26.8 26.5 27.1 26.6 24.0 22.3 20.8 19.6 24.2
31.8 38.0 41.3 42.1 41.9 39.5 40.0 36.9 33.9 38.4
1.9 3.6 3.8 4.5 4.7 4.3 4.4 4.4 4.5 4.0
2.8 3.8 4.5 5.6 5.7 5.8 5.6 5.6 5.5 5.0
K
2 3 4 5 6 7 8 9 10 Avg momentum , which provided a new view of bursty patterns . Besides traditional text documents like news articles and research publications , event detection is also studied in those new social media like Twitter and Flickr [ 2 ] , [ 23 ] . Becker et al . [ 2 ] explored a variety of techniques for learning multifeature similarity metrics for social media documents to detect events . In [ 23 ] , Sakaki et al . proposed an algorithm to monitor tweets to detect real time events such as earthquakes and typhoons . In [ 17 ] , Leskovec et al . proposed a memetracking approach to provide a coherent representation of the news cycle , ie , daily rhythms in the news media . Yang et al . [ 29 ] studied temporal patterns with online content and how the popularity of the content grows and fades over time . These studies of event detection and tracking mainly focus on mining temporal bursts instead of analyzing the topics of periodic patterns that we study in this paper .
Periodic pattern mining Some studies focus on searching periodic patterns in time series databases [ 9 ] , [ 30 ] , [ 31 ] , [ 7 ] . Besides traditional time series databases , some other studies detect periodic events on other datasets such as video [ 24 ] , moving objects [ 18 ] . Lahiri et al . proposed a new mining problem of finding periodic or near periodic subgraphs in dynamic social networks [ 15 ] , [ 16 ] . Compared with the above studies , our paper focuses on the latent periodic topic analysis on the text dataset . Some work studies periodic analysis in text domain [ 25 ] , [ 6 ] , [ 22 ] . In [ 6 ] , Chen et al . analyzed spatial temporal distributions of tag usage to detect events from photos on Flickr and extracted the periodic tags by checking the standard deviation of the distances between every two adjacent entries in the timeline for each tag and clustered the tags into events . In [ 22 ] , Murata et al . classified queries based on the number of search intentions and their temporal features , and performed the Discrete Fourier Transform ( DFT ) on the ratios of each search intention to detect the periodic changes . However , these studies analyze the distributions of single terms only . In this paper , we model the latent periodic topic analysis in a more systematic way where each topic is represented by a word distribution . We analyze the periodic patterns from the perspective of topics instead of single words , and we discover the periodic bursts and the corresponding topics together instead of making it into two separate stages .
VII . CONCLUSION AND FUTURE WORK
In this paper , we introduce the problem of latent periodic topic analysis on timestamped documents . We propose a model called LPTA ( Latent Periodic Topic Analysis ) that exploits both the periodicity of the terms and term cooccurrences . To test our approach , we collect several representative datasets including seminar , DBLP and Flickr . Evaluation results show that our LPTA model works well for discovering the latent periodic topics by combining the information from topical clusters and periodic patterns .
Periodicity analysis is an important task for web mining and social media mining . In the future we will focus on how to extend our current work to handle the increasing amount of web documents and complex structure of social media . We are especially interested in three scenarios :
• Effectively analyzing large scale data . Although we have tested our model in quite a few datasets , these datasets are relatively small compared with web scale information resources . We are interested in designing scalable algorithms that can also handle the potentially noisy data in real life .
• Automatically determining the optimal number of topics in real life . In our current model , the number of topics is given as a parameter . In the future , we plan to use Bayesian information criterion to select the optimal number of topics or employ Dirichlet process for model selection .
• Incorporating the social networks into periodicity detection . In our current scheme , document are treated isolatedly and we do not consider whether these documents come from the same user or users who are close friends . In social media websites such as Flickr and Twitter , the social network plays an important role and incorporates rich information . In the future we would like to combine such network structure for analysis .
ACKNOWLEDGMENT
The work was supported in part by US National Science Foundation grants CCF 0905014 , CNS 0931975 , and IIS09 05215 , the US Army Research Laboratory under Cooperative Agreement No . W911NF 09 2 0053 ( NS CTA ) , and US Air Force Office of Scientific Research MURI award FA9550 08 1 0265 . The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies , either expressed or implied , of the US Government or the Army Research Laboratory . The US Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on .
REFERENCES
[ 1 ] J . Allan , R . Papka , and V . Lavrenko . On line new event detection and tracking . In SIGIR , pages 37–45 , 1998 .
[ 2 ] H . Becker , M . Naaman , and L . Gravano . Learning similarity In WSDM , metrics for event identification in social media . pages 291–300 , 2010 .
[ 3 ] D . M . Blei and J . D . Lafferty . Dynamic topic models .
ICML , pages 113–120 , 2006 .
In
[ 4 ] D . M . Blei , A . Y . Ng , and M . I . Jordan . Latent dirichlet allocation . Journal of Machine Learning Research , 3:993– 1022 , 2003 .
[ 5 ] D . Cai , Q . Mei , J . Han , and C . Zhai . Modeling hidden topics on document manifold . In CIKM , pages 911–920 , 2008 .
[ 6 ] L . Chen and A . Roy . Event detection from flickr data through In CIKM , pages 523–532 , wavelet based spatial analysis . 2009 .
[ 7 ] M . G . Elfeky , W . G . Aref , and A . K . Elmagarmid . Periodicity detection in time series databases . IEEE Trans . Knowl . Data Eng . , 17(7):875–887 , 2005 .
[ 8 ] G . P . C . Fung , J . X . Yu , H . Liu , and P . S . Yu . Time dependent event hierarchy construction . In KDD , pages 300–309 , 2007 .
[ 9 ] J . Han , G . Dong , and Y . Yin . Efficient mining of partial In ICDE , pages periodic patterns in time series database . 106–115 , 1999 .
[ 10 ] D . He and D . S . Parker . Topic dynamics : an alternative model of bursts in streams of topics . In KDD , pages 443–452 , 2010 .
[ 11 ] T . Hofmann . Probabilistic latent semantic indexing . In SIGIR , pages 50–57 , 1999 .
[ 12 ] T . Iwata , T . Yamada , Y . Sakurai , and N . Ueda . Online multiscale dynamic topic models . In KDD , pages 663–672 , 2010 .
[ 13 ] J . M . Kleinberg . Bursty and hierarchical structure in streams .
In KDD , pages 91–101 , 2002 .
[ 14 ] H . W . Kuhn . The Hungarian method for the assignment problem . Naval Research Logistic Quarterly , 2:83–97 , 1955 .
[ 15 ] M . Lahiri and T . Y . Berger Wolf . Mining periodic behavior in dynamic social networks . In ICDM , pages 373–382 , 2008 .
[ 16 ] M . Lahiri and T . Y . Berger Wolf . Periodic subgraph mining in dynamic networks . Knowl . Inf . Syst . , 24(3):467–497 , 2010 .
[ 17 ] J . Leskovec , L . Backstrom , and J . M . Kleinberg . Memetracking and the dynamics of the news cycle . In KDD , pages 497–506 , 2009 .
[ 18 ] Z . Li , B . Ding , J . Han , R . Kays , and P . Nye . Mining periodic In KDD , pages 1099–1108 , behaviors for moving objects . 2010 .
[ 19 ] Z . Li , B . Wang , M . Li , and W Y Ma . A probabilistic model for retrospective news event detection . In SIGIR , pages 106– 113 , 2005 .
[ 20 ] H . Mannila , H . Toivonen , and A . I . Verkamo . Discovering In KDD , pages 210–215 , frequent episodes in sequences . 1995 .
[ 21 ] Q . Mei , C . Liu , H . Su , and C . Zhai . A probabilistic approach to spatiotemporal theme pattern mining on weblogs . In WWW , pages 533–542 , 2006 .
[ 22 ] M . Murata , H . Toda , Y . Matsuura , R . Kataoka , and T . Mochizuki . Detecting periodic changes in search intentions in a search engine . In CIKM , pages 1525–1528 , 2010 .
[ 23 ] T . Sakaki , M . Okazaki , and Y . Matsuo . Earthquake shakes twitter users : real time event detection by social sensors . In WWW , pages 851–860 , 2010 .
[ 24 ] E . P . Vivek , E . Pogalin , and A . W . M . Smeulders . Periodic event detection and recognition in video . In ICASSP , pages 3537–3540 , 2009 .
[ 25 ] M . Vlachos , C . Meek , Z . Vagena , and D . Gunopulos . Identifying similarities , periodicities and bursts for online search queries . In SIGMOD Conference , pages 131–142 , 2004 .
[ 26 ] M . Vlachos , P . S . Yu , and V . Castelli . On periodicity detection and structural periodic similarity . In SDM , 2005 .
[ 27 ] X . Wang and A . McCallum . Topics over time : a non markov continuous time model of topical trends . In KDD , pages 424– 433 , 2006 .
[ 28 ] X . Wang , C . Zhai , X . Hu , and R . Sproat . Mining correlated bursty topic patterns from coordinated text streams . In KDD , pages 784–793 , 2007 .
[ 29 ] J . Yang and J . Leskovec . Patterns of temporal variation in online media . In WSDM , pages 177–186 , 2011 .
[ 30 ] J . Yang , W . Wang , and P . S . Yu . Mining asynchronous IEEE Trans . Knowl . periodic patterns in time series data . Data Eng . , 15(3):613–628 , 2003 .
[ 31 ] J . Yang , W . Wang , and P . S . Yu . Mining surprising periodic patterns . Data Min . Knowl . Discov . , 9(2):189–216 , 2004 .
[ 32 ] J . Zhang , Y . Song , C . Zhang , and S . Liu . Evolutionary hierarchical dirichlet processes for multiple correlated timevarying corpora . In KDD , pages 1079–1088 , 2010 .
