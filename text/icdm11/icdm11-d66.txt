Application of a Unified Medical Data Miner ( UMDM ) for Prediction , Classification , Interpretation and Visualization on Medical Datasets : The Diabetes
Dataset Case
Nawaz Mohamudally1 and Dost Muhammad Khan2
1 Associate Professor & Head School of Innovative Technologies and Engineering ,
University of Technology , Mauritius alimohamudally@utmintnetmu
2 PhD Student , School of Innovative Technologies and Engineering , University of Technology ,
{Mauritius,dostmuhammad_khan}@yahoo.com
ABSTRACT Medical datasets hold huge number of records about the patients , the doctors and the diseases . The extraction of useful information which will provide knowledge in decision making process for the diagnosis and treatment of the diseases are becoming increasingly determinant . Knowledge Discovery and data mining make use of Artificial Intelligence ( AI ) algorithms which are applied to discover hidden patterns and relations in complex datasets using intelligent agents . The existing data mining algorithms and techniques are designed to solve the individual problems , such as classification or clustering . Up till now , no unifying theory is developed . Among the different algorithms in data mining for prediction , classification , interpretation and visualization , ‘k means clustering’ , ‘Decision Trees ( C4.5)’ , ‘Neural Network ( NNs)’ and ‘Data Visualization ( 2D or 3D scattered graphs)’ algorithms are frequently utilized in data mining tools . The choice of the algorithm depends on the intended use of extracted knowledge . In this paper , the mentioned algorithms are unified into a tool , called Unified Medical Data Miner ( UMDM ) that will enable prediction , classification , interpretation and visualization on a diabetes dataset .
Keywords : Medicine , Clustering , Classification & Prediction , Visualization , Agent Data mining , Unified Medical Data Miner .
1 Introduction
The vast amount of data in medical datasets is generated through the health care processes , whereby , clinical datasets are more significant ones . The data mining techniques help to find the relationships between multiple parental variables and the outcomes they influence . The methods and applications of medical data mining are based on computational intelligence such as artificial neural network , k means clustering , decision trees and data visualization [ 1][2][3][4][5][11][15][16][17 ] . The purpose of data mining is to verify the hypothesis prepared by the user and to discover or uncover new patterns from the large datasets . Many classifiers have been introduced for
P . Perner ( Ed. ) : ICDM 2011 , LNAI 6870 , pp . 78–95 , 2011 . © Springer Verlag Berlin Heidelberg 2011
Application of a Unified Medical Data Miner ( UMDM )
79 prediction , including Logistic Regression , Naïve Bayes , Decision Tree , K local hyper plane distance nearest neighbour classifiers , Random Decision Forest , Support Vector Machine ( SVM ) etc [ 21][23 ] . There are different data mining algorithms which can be applied for prediction , classification , interpretation and visualization but ‘kmeans clustering’ , ‘decision trees’ , ‘neural networks’ and ‘data visualization ( 2D or 3D scattered graphs)’ algorithms are commonly adopted in data mining tools . In medical sciences , the classification of medicines , patient records according to their doses etc . can be performed by applying the clustering algorithms . The issue is how to interpret these clusters . To do so visualization tools are indispensable . Taking this aspect into account we are proposing a Unified Medical Data Miner which will unify these data mining algorithms into a single black box so that the user needs to provide the dataset and recommendations from specialist doctor as the input . Figure 1 depicts the inputs and outputs of Unified Medical Data Miner .
A Medical Dataset
Doctor ’s Proposals
Medical Data Miner
Prediction
Classification
Interpretation
Visualization
Fig 1 . A Unified Medical Data Miner
The following are sample questions that may be asked to a specialist medical doctor :
1 What type of prediction , classification , interpretation and visualization is re quired in the medical databases particularly diabetes ?
2 Which attribute or the combinations of the attributes of diabetes dataset have the impact to predict diabetes in the patient ?
3 What are the future requirements for prediction of disease like diabetes ? 4 Relationship between the attributes which will provide some hidden pattern in the dataset .
A multiagent system ( MAS ) is used in this proposed Unified Medical Data Miner , which is capable of performing classification and interpretation . This is a cascaded multiagent system ie the output of an agent is an input for the other agents where ‘kmeans clustering’ algorithm is used for classification and ‘decision tree C4.5’ algorithm is applied for interpretation . For prediction and visualization , separately , neural networks and 2D scattered graphs are used [ 19 ] . The rest of the paper is organized as follows : In section 2 we present an overview of data mining algorithms used in UMDM , section 3 deals with the methodology whereas , the obtained results are discussed in section 4 and finally section 5 presents the conclusion .
80
N . Mohamudally and DM Khan
2 Overview of Data Mining Algorithms Used in the Medical Data
Miner
Data mining algorithms are accepted nowadays due to their robustness , scalability and efficiency in different fields of study like bioinformatics , genetics , medicine and education and many more areas . The classification , clustering , interpretation and data visualization are the main areas of data mining algorithms [ 9][18 ] . Table 1 shows the capabilities and tasks that the different data mining algorithms can perform .
Table 1 . Functions of Different Data Mining Algorithms
Estimation
Interpretation Prediction Classification Visualization
Y
N
Y Y
N
Y Y
Y Y
N
Y
N N
Y
N N
N N
Y
Y
Y Y
Y
Y Y
Y Y
N
Y
Y Y
Y
Y N
N Y
N
N
N N
Y
N N
N N
All
Only 2
All
Only 6
Only 1
DM Algos . Neural Network Decision
Tree
K Means Kohonen
Map Data
Visualization
K NN Link Analysis Regression Bayesian
Classification
Overall Decision
Most of the existing data mining tools emphasize on a specific problem and the tool is limited to a particular set of data for a specific application . These tools depend on the choice of algorithms to apply and how to analyze the output , because most of them are generic and there is no context specific logic that is attached to the application . In this paper we present a unified model of data mining algorithms that performs clustering , classification , prediction and visualization using multiagent system on ‘Diabetes’ dataset . The data can be used either to predict future behavior or to describe patterns in an understandable form within discover process .
2.1 Neural Networks
The neural networks are used for discovering complex or unknown relationships in dataset . They detect patterns from the large datasets for prediction or classification , also used in system performing image and signal processing , pattern recognition , robotics , automatic navigation , prediction and forecasting and simulations . The NNs are more effective and efficient on small to medium sized datasets . The data must be
Application of a Unified Medical Data Miner ( UMDM )
81 trained first by NNs and the process it goes through is considered to be hidden and therefore left unexplained . The neural network starts with an input layer , where each node corresponds to a predictor variable . These input nodes are connected to a number of nodes in a hidden layer . Each input node is connected to every node in the hidden layer . The nodes in the hidden layer may be connected to nodes in another hidden layer , or to an output layer . The output layer consists of one or more response variables . Figure 2 illustrates the different layers of NNs [ 8][20 ] .
Fig 2 . A Neural Network with one hidden layer
Limitations of NNs : The process it goes through is considered to be hidden and therefore left unexplained . This lack of explicitness may lead to less confidence in the results and a lack of willingness to apply those results from data mining , since there is no understanding of how the results came about . It is obvious , as the number of variables of a dataset increases , it will become more difficult to understand how the NNs come to it conclusion . The algorithm is better suited to learning on small to medium sized datasets as it becomes too time inefficient on large sized datasets .
2.2 Decision Tree ( C4.5 ) Algorithm
The decision tree algorithm is used as an efficient method for producing classifiers from data . The goal of supervised learning is to create a classification model , known as a classifier , which will predict , with the values of its available , input attributes and the class for some entity . In other words , classification is the process of dividing the samples into pre defined groups . It is used for decision rules as an output . In order to do mining with the decision trees , the attributes have continuous discrete values , the target attribute values must be provided in advance and the data must be sufficient
82
N . Mohamudally and DM Khan so that the prediction of the results will be possible . Decision trees are faster to use , easier to generate understanding rules and simpler to explain since any decision that is made can be understood by viewing path of decision . They also help to form an accurate , balanced picture of the risks and rewards that can result from a particular choice . The decision rules are obtained in the form of if then else , which can be used for the decision support systems , classification and prediction . Figure 3 illustrates how decision rules are obtained from decision tree algorithm .
Data
C4.5 Algorithm
Decision Rules
Fig 3 . Decision Rules from a C4.5 Algorithm
The different steps of decision tree ( C4.5 ) algorithm are given below :
Step 1 : Let ‘S’ is a training set . If all instances in ‘S’ are positive , then create ‘YES’ node and halt . If all instances in ‘S’ are negative , create a ‘NO’ node and halt . Otherwise select a feature ‘F’ with values v1,,vn and create a decision node .
Step 2 : Partition the training instances in ‘S’ into subsets S1 , S2 , ,Sn according to the values of V .
Step 3 : Apply the algorithm recursively to each of the sets Si .
The decision tree algorithm generates understandable rules , performs classification without requiring much computation , suitable to handle both continuous and categorical variables and provides an for prediction or classification [ 24][25][26][8][6 ] . indication
Limitation of C4.5 : It is good for small problems but quickly becomes cumbersome and hard to read for intermediate sized problems . Special software is required to draw that tree . If there is a noise in the learning set , it will fail to find a tree . The data must be interval or categorical . Any other format of data should be converted into the required format . This process could hide relationships . Over fitting , large set of possible hypotheses , pruning of the tree is required . C4.5 generally represents a finite number of classes or possibilities . It is difficult for decision makers to quantify a finite amount of variables . This sometimes affects the accuracy of the output , hence misleading answer . If the list of variables increases the if then statements created can become more complex . This method is not useful for all types of data mining , such as time series .
2.3 k Means Clustering Algorithm
The ‘k’ , in the k means algorithm stands for number of clusters as an input and the ‘means’ stands for an average , location of all the members of a particular cluster . The algorithm is used for finding the similar patterns , due to its simplicity and fast execution . This algorithm uses a square error criterion in equation 1 for re assignment of any sample from one cluster to another , which will cause a decrease in the total squared error .
Application of a Unified Medical Data Miner ( UMDM )
83
E
N
M
= L ∑∑∑
= 1 i
= 1 j
= 1 k
(
−
CF
) 2
( 1 )
Where ( F C)2 is the distance between the datapoints . It is easy to implement , and its time and space complexity are relatively small . Figure 4 illustrates the working of clustering algorithms .
Dataset
K means Algorithm
Clusters of Dataset
Fig 4 . The Function of the Clustering Algorithms
The different steps of k means clustering algorithm are given below :
Step 1 : Select the value of ‘k’ , the number of clusters . Step 2 : Calculate the initial centroids from the actual sample of dataset . Divide datapoints into ‘k’ clusters .
Step 3 : Move datapoints into clusters using Euclidean ’s distance formula in equation 2 . Recalculate new centroids . These centroids are calculated on the basis of average or means .
( xxd
, i
)
= j
N
∑
= 1 k
( x ik
) 2
− x jk
( 2 )
Step 4 : Repeat step 3 until no datapoint is to be moved .
Where d(xi , xj ) is the distance between xi and xj . xi and xj are the attributes of a given object , where i , j and k vary from 1 to N where N is total number of attributes of that given object , indexes i , j , k and N are all integers [ 27][28][29][30][31][7 ] . The kmeans clustering algorithm is applied in number of areas like , Marketing , Libraries , Insurance , City planning , Earthquake studies , www and Medical Sciences [ 18 ] .
Limitations of k means clustering algorithm : The algorithm is only applicable to datasets where the notion of the ‘mean’ is defined . Thus , it is difficult to apply to categorical datasets . There is , however , a variation of the k means algorithm called kmodes , which clusters categorical data . The algorithm uses the mode instead of the mean as the centroid . The user needs to specify the number of clusters k in advance . In practice , several k values are tried and the one that gives the most desirable result is selected . The algorithm is sensitive to outliers . Outliers are data points that are very far away from other data points . Outliers could be errors in the data recording or some special data points with very different values . The algorithm is sensitive to initial seeds , which are the initially selected centroids . Different initial seeds may result in different clusters . Thus , if the sum of squared error is used as the stopping criterion , the algorithm only achieves local optimal . The global optimal is computationally
84
N . Mohamudally and DM Khan infeasible for large datasets . The algorithm is not suitable for discovering clusters that are not hyper ellipsoids or hyper spheres .
2.4 Data Visualization
This method provides the better understanding of data to the users . Graphics and visualization tools better illustrate the relationship among data and their importance in data analysis cannot be overemphasized . The distributions of values can be displayed by using histograms or box plots . 2D or 3D scattered graphs can also be used . Visualization works because it provides the broader information as opposed to text or numbers . The missing and exceptional values from data , the relationships and patterns within the data are easier to identify when graphically displayed . It allows the user to easily focus and see the patterns and trends amongst data [ 8][20 ] .
Limitations of Data Visualization : One major issue in data visualization is the fact that as the volume of the data increases it becomes difficult to distinguish patterns from datasets , another major issue is that the display format from visualization is restricted to two dimensions by the display device be it a computer screen or a paper .
3 Methodology
We will first apply the k means clustering algorithm on a medical dataset ‘Diabetes’ . This is a dataset/testbed of 790 records . Before applying k means clustering algorithms on this dataset , the data is pre processed , called data standardization . The interval scaled data is properly cleansed by applying the range method . The attributes of the dataset/testbed ‘Diabetes’ are : Number of Times Pregnant ( NTP)(min . age = 21 , max . age = 81 ) , Plasma Glucose Concentration a 2 hours in an oral glucose tolerance test ( PGC ) , Diastolic Blood Pressure ( mm Hg ) ( DBP ) , Triceps Skin Fold Thickness ( mm ) ( TSFT ) , 2 Hour Serum Insulin ( m U/ml ) ( 2HSHI ) , Body Mass Index ( weight in kg/(height in m)^2 ) ( BMI ) , Diabetes Pedigree Function ( DPF ) , Age , Class ( whether diabetes is cat 1 or cat 2 ) [ 10 ] .
There are two main sources of data distribution , first is the centralized data source and second is the distributed data source . The distributed data source has further two approaches of data partitioning , first , the horizontally partitioned data , where same sets of attributes are on each node , this case is also called the homogeneous case . The second is the vertically partitioned data , which requires that different attributes are observed at different nodes , this case is also called the heterogeneous case . It is required that each node must contain a unique identifier to facilitate matching in vertical partition [ 1][9 ] .
The vertical partition is chosen for the dataset ‘Diabetes’ and four partitions are created . The attribute ‘class’ is a unique identifier in all partitions . The selection of attributes in each partitioned table is arbitrary , depends on the user , different combination of attributes will give different results . Tables from 2 to 5 show the vertical partitions of dataset .
Application of a Unified Medical Data Miner ( UMDM )
85
Table 2 . 1st Vertically partitioned Diabetes dataset
NTP
4
2
2
DPF
0.627
0.351
2.288
Class
ive
+ive
ive
Table 3 . 2nd Vertically partitioned Diabetes dataset
DBP
72
66
64
AGE
50
31
33
Class
ive
+ive
ive
Table 4 . 3rd Vertically partitioned Diabetes dataset
TSFT
35
29
0
BMI
33.6
28.1
43.1
Class
ive
+ive
ive
Table 5 . 4th Vertically partitioned Diabetes dataset
PGC
148
85
185
2HIS
0
94
168
Class
ive
+ive
ive
Each partitioned table is a dataset of 790 records ; only 3 records are exemplary shown in each table .
The parameters for the agent of k means clustering algorithm are set for the above created vertical partitioned datasets . The value of ‘k’ , number of clusters and the value of ‘n’ , number of iterations is set to 4 and 100 respectively for each partition ie value of k =4 and value of n=100 , where ‘k’ and ‘n’ are positive nonzero integers . ‘k’ and ‘n’ are the required inputs for the execution of k means clustering algorithm .
86
N . Mohamudally and DM Khan
Therefore , total 16 clusters are generated as an output . The agent of C4.5 ( decision tree ) algorithm gains these clusters as an input and produces the decision rules in the form of ‘if then else’ . For further interpretation and visualization of the results of these clusters , 2D scattered graphs are drawn using data visualization .
4 Results and Discussion
The pattern discovery from large dataset is a three steps process . In first step , one seeks to enumerate all of the associations that occur at least ‘a’ times in the dataset . In the second step , the clusters of the dataset are created and the third and last step is to construct the ‘decision rules’ with ( if then statements ) the valid pattern pairs . Association Analysis : Association mining is concerned with whether the co joint event ( A,B,C,… . ) occurs more or less than would be expected on a chance basis . If it occurs as much ( within a pre specified margin ) , then it is not considered an interesting rule . Predictive Analysis : It is to generate ‘decision rules’ from the diabetes medical dataset using logical operations . The result of these rules after applying on the ‘patient record’ will be either ‘true’ or ‘false’ [ 14][12][13][22 ] .
These four partitioned datasets of medical dataset ‘Diabetes’ are inputted to our proposed UMDM one by one respectively , total sixteen clusters are obtained , four for each partitioned dataset . The 2D scattered graphs of four of these clusters are shown in figures 5 , 6 , 7 and 8 . The purpose of scattered graph is to identify the type of relationship if any between the attributes . The graph is used when a variable exists which is being tested and in this case the attribute or variable ‘class’ is a test attribute .
2D Scattered Graph
I
S H 2
900 800 700 600 500 400 300 200 100 0
0
5
10
15
20 PGC
25
30
35
40
Fig 5 . 2D Scattered Graph of attributes ‘PGC’ and ‘2HIS’
PGC HIS
The graph in figure 5 shows that there is no relationship between the attributes ‘PGC’ and ‘2HIS’ and both have the distinct values , which shows that the value of attribute ‘class’ does not depend on these two attributes , ie if one attribute gives diabetes of category 1 the other will show diabetes of category 2 in the patient .
Application of a Unified Medical Data Miner ( UMDM )
87
2D Scattered Graph
F P D
3
2.5
2
1.5
1
0.5
0
0
5
10
15
20
25
30
35
NTP
Fig 6 . 2D Scattered Graph of attributes ‘NTP’ and ‘DPF’
NTP DPF
The graph in figure 6 shows that the graph can be divided into two regions first is from 0 to 13 and the second is from 14 to 30 . In the first region , there exists a relationship between the attributes ‘PGC’ and ‘2HIS’ , consequently the value of attribute ‘class’ depends on both attributes . In the second region , there is no relationship between these attributes , therefore , the value of attribute ‘class’ is independent of these attributes .
2D Scattered Graph
I
S H 2
450 400 350 300 250 200 150 100 50 0
0
20
40
60
PGC
80
100
120
Fig 7 . 2D Scattered Graph of attributes ‘PGC’ and ‘2HIS’
PGC HIS
The structure of this graph in figure 7 is similar to graph in figure 5 . There is no relationship between the attributes ‘PGC’ and ‘2HIS’ and both have the distinct values , which shows that the value of attribute ‘class’ does not depend on these two attributes , ie if one attribute gives diabetes of category 1 the other will show diabetes of category 2 in the patient .
88
N . Mohamudally and DM Khan
2D Scattered Graph
I
M B
40 35 30 25 20 15 10 5 0
0
50
100
150
200
250
TSFT
Fig 8 . 2D Scattered Graph of attributes ‘TSFT’ and ‘BMI’
TSFK BMI
The graph in figure 8 shows that the graph has two regions ; one region is that where no relationship between the attributes ‘TSFT’ and ‘BMI’ exist , so the value attribute ‘class’ does not depend on the attributes and the second region shows the relationship between these attributes , as a result , the value of attribute ‘class’ depends upon both attributes ‘TSFT’ and ‘BMI’ . Similarly the other graphs can also be drawn . The proposed UMDM also produces the decision rules for each cluster of ‘Diabetes’ dataset . We are taking only two , 1st and 3rd clusters’ rules for the interpretation , as shown below :
The Rules of cluster 1 are :
1 . Rule : 1 if PGC = 165 then 2 . Class = Cat2 else 3 . Rule : 2 if PGC = 153 then 4 . Class = Cat2 else 5 . Rule : 3 if PGC = 157 then 6 . Class = Cat2 else 7 . Rule : 4 if PGC = 139 then 8 . Class = Cat2 else 9 . Rule : 5 if HIS = 545 then 10 . 11 . 12 . 13 .
Class = Cat2 else Rule : 6 if HIS = 744 then Class = Cat2 else Class = Cat1
Fig 9 . Rules of the first cluster
There are six rules in the first cluster . The result for this cluster of ‘Diabetes’ dataset is if the value of attribute ‘PGC’ is more than 130 and the value of attribute ‘HIS’ is more than 500 , as the rules show , then the patient has diabetes of category 2 otherwise category 1 . The decision rules make it easy and simple for the user to interpret and predict this partitioned dataset of diabetes .
Application of a Unified Medical Data Miner ( UMDM )
89
The Rules of cluster 3 are :
1 . Rule : 1 if DPF = 1.32 then 2 . Class = Cat1 else 3 . Rule : 2 if DPF = 2.29 then 4 . Class = Cat1 else 5 . Rule : 3 if NTP = 2 then 6 . Class = Cat2 else 7 . Rule : 4 if DPF = 2.42 then 8 . Class = Cat1 else 9 . Rule : 5 if DPF = 2.14 then 10 . Class = Cat1 else 11 . Rule : 6 if DPF = 1.39 then 12 . Class = Cat1 else 13 . Rule : 7 if DPF = 1.29 then 14 . Class = Cat1 else 15 . Rule : 8 if DPF = 1.26 then 16 . Class = Cat1
Fig 10 . Rules of the third cluster
There are eight rules in the third cluster 3 . The result of this cluster of ‘Diabetes’ dataset is if the value of the attribute ‘DPF’ is equal or more than 1.2 then the patient has diabetes of category 1 and if the value of attribute ‘NTP’ is equal or more than 2 then the patient has diabetes of category 2 , as the rules show . The interpretation and prediction is easy and simple for a user to take decision from these rules of partitioned dataset of diabetes .
The importance of the attributes of dataset ‘Diabetes’ using the selected data min ing algorithms in UMDM is demonstrated in figures 11 , 12 and 13 .
Importance of Variables using K means Algorithm
E U L A V
%
120.00 100.00 80.00 60.00 40.00 20.00 0.00
PGC
AGE
BMI
NTP
TSFT
2HSI
DPF
DBP
VARIABLES
Fig 11 . Graph between Variables and Percentage value using k means Algorithm
The graph in figure 11 shows that the percentage value of attribute ‘PGC’ is 100 percent , the percentage value of attributes ‘AGE’ , ‘BMI’ and ‘NTP’ is 50 percent , the percentage value of attributes ‘TSFT’ , ‘2HSI’ and ‘DPF’ is 30 percent and the
90
N . Mohamudally and DM Khan attribute ‘DBP’ has only 12 percent value . This gives an idea that the attribute ‘PGC’ is one of the most important attribute to predict the category of diabetes from ‘Diabetes’ dataset , the attributes ‘AGE’ , ‘BMI’ and ‘NTP’ can also be taken into account for prediction of diabetes category but the remaining attributes have no significant role in the prediction , using the k means clustering algorithm .
Importance of Variables using Neural Networks Algorithm
E U L A V
%
120.00 100.00 80.00 60.00 40.00 20.00 0.00
DPF
BMI
PGC
AGE
DBP
TSFT
2HSI
NTP
VARIABLES
Fig 12 . Graph between Variables and Percentage value using Neural Networks Algorithm
The graph in figure 12 shows that percentage value of attributes ‘DPF’ , ‘BMI’ , ‘PGC’ , ‘AGE’ , ‘DBP’ and ‘TSTF’ is more than 90 percent and the percentage value of other two attributes ‘NTP’ and ‘2HSI’ is 70 percent , which confirms that all attributes are important in the prediction of the category of diabetes from ‘Diabetes’ dataset , using Neural Networks .
Im portance of Variables using Decision Tree Algorithm
E U L A V %
120.00 100.00 80.00 60.00 40.00 20.00 0.00
PGC
BMI
AGE
DPF
DBP
TSFT
2HSI
NTP
VARIABLES
Fig 13 . Graph between Variables and Percentage value using C4.5 Algorithm
The graph in figure 13 shows that the percentage value of attribute ‘PGC’ is 100 percent , the percentage value of attribute ‘BMI’ is more than 50 percent and the value of attributes ‘AGE’ , ‘DPF’ is more than 30 percent . Conversely , the percentage value of attributes ‘NTP’ , ‘TSFT’ , ‘2HSI’ and ‘DBP’ is very low . This gives an idea that
Application of a Unified Medical Data Miner ( UMDM )
91 two attributes ‘PGC’ and ‘BMI’ play important role in prediction of the category of diabetes from ‘Diabetes’ dataset and the other attributes due to their very low percentage have no impact in prediction , using C4.5 algorithm .
Table 6 . Percentage Importance of Diabetes Dataset Attributes using all three Algorithms
Sr . #
Variables
1 2 3 4 5 6 7 8
PGC AGE BMI NTP TSFT 2HSI DPF DBP k Means 100.00 51.57 50.24 49.15 33.82 28.45 27.86 12.34
C4.5 100.00 36.47 52.71 4.05 9.92 5.88 30.86 27.10
Neural Networks
99.13 96.59 99.53 69.90 90.01 74.53 100.00 95.66
The table 6 summaries the percentage values of all attributes of dataset ‘Diabetes’ using the k means clustering , the Neural Networks and the C4.5 algorithms .
All three Data Mining Algorithms e u l a V e c n a t r o p m
I
%
120.00 100.00 80.00 60.00 40.00 20.00 0.00
PGC
AGE
BMI
NTP
TSFT
2HSI
DPF
DBP
Variables of Dataset 'Diabetes'
K Means Decision Tree Neural Netw orks
Fig 14 . Graph between Variables and percentage Importance Values for all three Algorithms
The graph shows that the percentage values of all the attributes of the given dataset ‘Diabetes’ are high from the Neural Networks as compared to C4.5 and the K means clustering algorithms . The percentage values of all the attributes of the given dataset ‘Diabetes’ are low from C4.5 as compared to the other two algorithms . The graph also illustrates that the percentage values of all the attributes obtained from k means clustering algorithm have intermediate values . The results of Neural Networks show that all the attributes of the given dataset are very important in prediction but when we draw a comparison between all these three algorithms then the attributes ‘PGC’ , ‘BMI’ , ‘AGE’ and ‘DPF’ are very important in the prediction of diabetes of category 1 or 2 in patients .
92
N . Mohamudally and DM Khan
The results obtained from Neural Networks for prediction are illustrated in table 7 .
Table 7 . Performance Metrics
CLASS R Net R Avg . Abs . Max . Abs . RMS Accuracy ( 20 % ) Conf . Interval ( 95 % )
All Train Test
0.66 0.65 0.68
0.66 0.65 0.68
0.26 0.26 0.25
0.95 0.95 0.89
0.35 0.36 0.35
0.52 0.52 0.52
0.69 0.70 0.68
The prediction depends on the R ( Pearson R ) value , RMS ( Root Mean Square ) error , and Avg . Abs . ( Average Absolute ) error , on the other hand Max . Abs . ( Maximum Absolute ) error may sometimes be important . The R value and RMS error indicate how “ close ” one data series is to another , in our case , the data series are the Target ( actual ) output values and the corresponding predicted output values generated by the model . R values range from 1.0 to +10 A larger ( absolute value ) R value indicates a higher correlation . The sign of the R value indicates whether the correlation is positive ( when a value in one series changes , its corresponding value in the other series changes in the same direction ) , or negative ( when a value in one series changes , its corresponding value in the other series changes in the opposite direction ) . An R value of 0.0 means there is no correlation between the two series . In general larger positive R values indicate “ better ” models . RMS error is a measure of the error between corresponding pairs of values in two series of values . Smaller RMS error values are better . Finally , another key to using performance metrics is to compare the same metric computed for different datasets . Note the R values highlighted for the Train and Test sets in the above table . The relatively small difference between values ( 0.65 and 0.68 ) suggests that the model generalizes well and that it is likely to make accurate predictions when it processes new data ( data not obtained from the Train or Test dataset ) .
A graph is drawn between targeted and predicted outputs using Neural Networks .
Figure 15 depicts this graph .
Predicted Model using Neural Networks t u p t u O d e t c i d e r P
2.5
2
1.5
1
0.5
0
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49
Target Output
Target Output Predicted Output
Fig 15 . A Graph between Targeted Output and Predicted Output using Neural Networks
Application of a Unified Medical Data Miner ( UMDM )
93
The graph in figure 15 shows that the predicted outputs and the targeted outputs are close with each other which verify that the given dataset ‘Diabetes’ is properly cleansed and the predicted results using Neural Networks may be more accurate .
5 Conclusion
This research paper presents the prediction , classification , interpretation and visualization of ‘Diabetes’ , a medical dataset , using k means clustering , Decision tree ( C4.5 ) , Neural networks and Data visualization ( 2D graphs ) data mining algorithms . In order to extract useful information and knowledge from medical datasets , these above mentioned tasks are crucial and play pivotal roles . It is clear that no single data mining algorithm is capable to perform all these tasks ; therefore , a unified model of these algorithms is presented in this paper , called Unified Medical Data Miner ( UMDM ) , which accomplishes all the described tasks . The vertical partitions of the given dataset , based on the similar values of the attributes are created as a first step . For the discovery of hidden patterns from the given dataset , data mining algorithms are cascaded ie the output of one algorithm is used as an input for another algorithm . The decision rules obtained from the decision tree algorithm can further be used as simple queries for any medical databases . One interesting finding from this case is that the pattern identified from the given dataset is “ Diabetes of category 1 or 2 depends upon ‘Plasma Glucose Concentration’ , ‘Body Mass Index’ , ‘Diabetes Pedigree Function’ and ‘Age’ attributes ” . We draw the conclusion that the attributes ‘PGC’ , ‘BMI’ , ‘DPF’ and ‘AGE’ of the given dataset ‘Diabetes’ play important role in the prediction whether a patient is diabetic of category 1 or category 2 . However , the results and model proposed in this paper require further validations and opinions from medical experts .
References
[ 1 ] Mullins , IM , et al . : Data mining and clinical data repositories : Insights from a 667,000 patient data set . Computers in Biology and Medicines 36 , 1351–1377 ( 2006 )
[ 2 ] Gupta , A . , Kumar , N . , Bhatnagar , V . : Analysis of Medical Data using Data Mining and Formal Concept Analysis . World Academy of Science , Engineering and Technology 11 ( 2005 )
[ 3 ] Zhu , L . , Wu , B . , Cao , C . : Introduction to Medical Data Mining . Chongqing University ,
Chonging ( 2003 )
[ 4 ] Padmaja , P . , et al . : Characteristic evaluation of diabetes data using clustering techniques . IJCSNS International Journal of Computer Science and Network Security ( IJCSNS ) 8(11 ) ( 2008 )
[ 5 ] Huang , Y . , McCullagh , P . , Black , N . , Harper , R . : Evaluation of Outcome prediction for a Clinical Diabetes Database . In : López , JA , Benfenati , E . , Dubitzky , W . ( eds . ) KELSI 2004 . LNCS ( LNAI ) , vol . 3303 , pp . 181–190 . Springer , Heidelberg ( 2004 )
[ 6 ] MacQueen , JB : Some Methods for classification and Analysis of Multivariate Observations . In : Proceedings of 5th Berkeley Symposium on Mathematical Statistics and Probability , pp . 281–297 . University of California Press , Berkeley ( 1967 )
94
N . Mohamudally and DM Khan
[ 7 ] Davidson , I . : Understanding K Means Non hierarchical Clustering , SUNY Albany –
Technical Report ( 2002 )
[ 8 ] Liu , B . : Web Data Mining Exploring Hyperlinks , Contents , and Usage Data , pp . 124–
139 . Springer , Heidelberg ( 2007 ) ISBN 13 978 3 540 37881 5
[ 9 ] Skrypnik , I . , Terziyan , V . , Puuronen , S . , Tsymbal , A . : Learning Feature Selection for
Medical Databases . In : CBMS 1999 ( 1999 )
[ 10 ] National Institute of Diabetes and Digestive and Kidney Diseases , Pima Indians Diabetes
Dataset ( 2010 ) , http://wwwarchiveicsuciedu/ml/datasets/Diabetes
[ 11 ] Jin , H . : Practical Issues on Privacy Preserving Health Data Mining . In : Washio , T . , Zhou , Z H , Huang , JZ , Hu , X . , Li , J . , Xie , C . , He , J . , Zou , D . , Li , K C , Freire , MM ( eds . ) PAKDD 2007 . LNCS ( LNAI ) , vol . 4819 , pp . 64–75 . Springer , Heidelberg ( 2007 )
[ 12 ] Zhang , S . , Liu , S . , Ou , J . : Data Mining for Intelligent Structure Form Selection Based on Association Rules from a High Rise Case Base . In : Washio , T . , Zhou , Z H , Huang , JZ , Hu , X . , Li , J . , Xie , C . , He , J . , Zou , D . , Li , K C , Freire , MM ( eds . ) PAKDD 2007 . LNCS ( LNAI ) , vol . 4819 , pp . 76–86 . Springer , Heidelberg ( 2007 )
[ 13 ] Zhang , K . , et al . : Discovering Prediction Model for Environmental Distribution Maps . In : Washio , T . , Zhou , Z H , Huang , JZ , Hu , X . , Li , J . , Xie , C . , He , J . , Zou , D . , Li , K C , Freire , MM ( eds . ) PAKDD 2007 . LNCS ( LNAI ) , vol . 4819 , pp . 99–109 . Springer , Heidelberg ( 2007 )
[ 14 ] Li , G . , Sheng , H . : Extracting Features from Gene Ontology for the Identification of Protein Sub cellular Location by Semantic Similarity Measurement . In : 2nd Workshop on Data Mining for Biomedical Applications ( BioDM 2007 ) , China ( 2007 ) .
[ 15 ] Wang , R S , et al . : Detecting Community Structure in Complex Net works by Optimal Rearrangement Clustering . In : 2nd Workshop on Data Mining for Bio medical Applications ( BioDM 2007 ) , China ( 2007 )
[ 16 ] Liu , H . , Xiao , Q . , Zhu , Z . : The HIV Data Mining Tool for Government Decision Making Support . In : 2nd Workshop on Data Mining for Biomedical Applications ( BioDM 2007 ) , China ( 2007 )
[ 17 ] Liu , Y . : Cancer Identification Based on DNA Microarray Data . In : 2nd Workshop on
Data Mining for Biomedical Applications ( BioDM 2007 ) , China ( 2007 )
[ 18 ] Peng , Y . , Kou , G . , Shi , Y . , Chen , Z . : Descriptive Framework for the Field of Data Mining and Knowledge Discovery . International Journal of Information Technology and Decision Making 7(4 ) , 639–682 ( 2008 )
[ 19 ] Max , V . , Rob , H . , Nick , P . : Data mining and Privacy in Public Sector using Intelligent
Agents , Discussion paper , New Zealand ( 2003 )
[ 20 ] Two crows : Introduction to Data Mining and Knowledge Discovery , Third Edition by
Two Crows Corporation ( 1999 ) ISBN : 1 892095 02 5
[ 21 ] Dong , QW , Zhou , SG , Liu , X . : Prediction of protein protein interactions from primary sequences . Int . J . Data Mining and Bioinformatics 4(2 ) , 211–227 ( 2010 )
[ 22 ] Zheng , F . , Shen , X . , Fu , Z . , Zheng , S . , Li , G . : Feature selection for ge nomic data sets through feature clustering , Int . J . Data Mining and Bioinformatics 4(2 ) , 228–240 ( 2010 )
[ 23 ] IIango , BS , Ramaraj , N . : A Hybrid Prediction Model with F score Feature Selection for
Type II Diabetes Databases A2CWiC , India , September 16 17 ( 2010 )
[ 24 ] Hunt , EB , Marin , J . , Stone , PJ : Experiments in induction . Academic Press , New York
( 1966 )
[ 25 ] Quinlan , JR : Discovering rules by induction from large collections of examples . In : Michie , D . ( ed . ) Expert Systems in the Micro Electronic Age . Edinburgh University Press , Edinburgh ( 1979 )
Application of a Unified Medical Data Miner ( UMDM )
95
[ 26 ] Quinlan , JR : C4.5 : Programs for machine learning . Morgan Kaufmann Publishers , San
Mateo ( 1993 )
[ 27 ] Steinbach , M . , Karypis , G . , Kumar , V . : A comparison of document clustering techniques .
In : Proceedings of the KDD Workshop on Text Mining ( 2000 )
[ 28 ] Jain , AK , Dubes , RC : Algorithms for clustering data . Prentice Hall , Englewood Cliffs
( 1988 )
[ 29 ] Lloyd , SP : Least squares quantization in PCM . Unpublished Bell Lab . Tech . Note , portions presented at the Institute of Mathematical Statistics Meeting Atlantic City , NJ ( September 1957 ) ; also IEEE Trans Inform Theory ( Special Issue on Quanti zation ) , IT 28 , 129–137 ( March 1982 )
[ 30 ] Gray , RM , Neuhoff , DL : Quantization . IEEE Trans Inform Theory 44(6 ) , 2325–2384
( 1998 )
[ 31 ] Dhillon , IS , Guan , Y . , Kulis , B . : Kernel k means : spectral clustering and normal ized cuts . In : KDD 2004 , pp . 551–556 ( 2004 )
