Diverse Dimension Decomposition of an Itemset Space
Mikalai Tsytsarau University of Trento tsytsarau@disiunitneu
Francesco Bonchi Yahoo! Research bonchi@yahoo inc.com
Aristides Gionis Yahoo! Research gionis@yahoo inc.com
Themis Palpanas University of Trento themis@disiunitneu
Abstract—We introduce the problem of diverse dimension decomposition in transactional databases . A dimension is a set of mutually exclusive itemsets , and our problem is to find a decomposition of the itemset space into dimensions , which are orthogonal to each other , and that provide high coverage of the input database . The mining framework we propose effectively represents a dimensionality reducing transformation from the space of all items to the space of orthogonal dimensions . Our approach relies on information theoretic concepts , and we are able to formulate the dimension finding problem with a single objective function that simultaneously captures constraints on coverage , exclusivity and orthogonality . We describe an efficient greedy method for finding diverse dimensions from transactional databases . The experimental evaluation of the proposed approach using two real datasets , flickr and delicious , demonstrates the effectiveness of our solution . Although we are motivated by the applications in the collaborative tagging domain , we believe that the mining task we introduce in this paper is general enough to be useful in other application domains . I . INTRODUCTION
In this paper , we are interested in discovering dimensions that represent diverse concepts , such as “ type of photo ” or “ camera brand ” , and whose different values almost partition the dataset . For instance , each dimension in Figure 1 can be seen as a different way of partitioning the transactions in D , and the three dimensions together can be considered as a diverse decomposition of the space of photos . In order to achieve our goal , we adopt an informationtheoretic perspective . While there exist several studies applying joint entropy to the problem of identifying interesting or informative itemsets [ 1]–[6 ] , this body of work can not be applied to the problem of diverse dimension decomposition , as explained next .
Example 2 : Consider the transposed view of the database in Figure 1 , given in Table I . Following the approaches that use joint entropy , we will get sets ( templates ) such as {color , nikon } , having the highest entropy ( dark grey lines ) , or {landscape,sony } as low entropy sets ( light grey lines ) . We notice that high entropy sets are characterized by more uniform appearance of their instantiations in the database ( eg , instances 01 , 10 and 11 appear with roughly the same frequency ) , while low entropy sets accumulate support around the few most frequent instances ( in our example : 00 ) , not necessarily representing mutually exclusive items forming the dimension ( with instances 001 , 010 , 100 ) . Thus , using the existing interestingness measures does not solve our problem . In this paper , we propose entropy measure expressing both the orthogonality among dimensions and the interestingness it also captures of dimensions . Moreover , we show that constraints both on exclusivity and coverage . Based on this measure , we formulate diverse dimension decomposition as the problem of finding an optimal set of k dimensions , minimizing an objective function that closely resembles the mutual information measure , except for a parameter α , which allows the analyst to trade off between information loss and orthogonality of the dimensions .
Our contributions are summarized as follows . • We introduce the novel problem of diverse dimension decomposition in transactional databases , as an optimization problem . We characterize our objective function and show that the selected dimensions explain well the underlying database .
• We prove a property that allows assessing the level of informativeness for newly added dimensions , thus allowing to define criteria for terminating the decomposition .
Collaborative content creation and annotation is one of the main activities and distinguishing features of the Web 20 The common efforts of many users create huge repositories of all sort of media , usually annotated by the users themselves ; for instance photos ( flickr ) , urls ( delicious ) , blogs ( technorati ) , videos ( youtube ) , songs ( last.fm ) , scientific papers ( bibsonomy and citeulike ) , and others . All these platforms provide their users with a repository of resources , and the capability of assigning tags , ie , freely chosen keywords , to these resources . tagged resources can be seen as a transactional database , typical of the frequent itemset mining paradigm : transactions correspond to resources , and items correspond to tags . In this setting we are interested in studying the problem of discovering an item space decomposition , which we define to be a set of orthogonal dimensions with high coverage . A dimension in turn is defined to be a set of itemsets that rarely co occur in the database .
A repository of
Example 1 : Consider for instance a query on flickr for photos about art ( ie , annotated with the tag art ) : the dataset D of such photos can look like the one in Figure 1 . In this setting dimensions might be , for example , such sets of itemsets as { {portrait },{landscape }} or {{canon },{nikon } , {sony }} . Indeed , almost all photos in the dataset contain at most one of the itemsets1 from each of the two dimensions . 1While in this example each dimension is formed by singleton items , in general a dimension is formed by itemsets of any size . t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13
{fish , art , film , portrait , tattoo , xpro , crossprocessed , nikon , skin , n80} {sanfrancisco , black&white , building , art , stairway , fireescape , nikon} {portrait , color , art , me , illustration , blood , adobephotoshop , canon} {travel , brazil , plant , art , nature , color , strong , nikon , nikond70} {sunset , art , museum , landscape , minneapolis , canon , powershotg3} {sculpture , art , 2004 , festival , japan , culture , clay , a70 , canon} {portrait , art , painting , color , europe , sony , sonyericssonk750} {black&white , art , film , photograph , street photo , contax645} {art , black&white , skin , hand , bodypainting , nikon , d70} {red , woman , art , face , color , tear , canon , eos300d} {art , 3d , unfound , photositook , sony , cybershot} {beautiful , woman , black&white , portrait , art} {landscape , nature , sunrise , wallpaper , art}
0 0 1 0 0 0 0
0 0 0 0 1 0 1
0 0 0 0 0 1 0 t5 1 0 0 0 0 1 0 t1 0 1 0 0 0 0 1 t2 0 1 0 0 1 0 0 t8 0 0 0 0 1 0 0 t9 0 1 0 0 1 0 0 t3 1 0 0 1 0 0 1 t4 0 1 0 1 0 0 0 t7 0 0 1 1 0 0 1 t6 1 0 0 0 0 0 0 t10 t11 t12 t13 1 0 0 1 0 0 0
Fig 1 : An example of transactional dataset , having three diverse dimensions ( shown on the right ) . In this specific example from Flickr , each transaction corresponds to a picture , and its associated tags . All pictures have in common the tag art . item canon nikon sony color black&white landscape portrait TABLE I : A transposed view of the dataset in Figure 1 , showing most frequent items taken from several dimensions . • We show that our problem is trivially NP hard , and thus turn our interest to approximation algorithms . We propose a greedy algorithm exploiting the well known FP tree data structure [ 7 ] , and clever pruning of the search space , based on properties we prove in the characterization of the problem .
Web search is another domain in which finding an answer set with diversity is important . Several studies have focused on the problem of search engines query result diversification [ 13]–[16 ] , where the goal is to produce an answer set that includes results relevant to different aspects ( facets ) of the query . In this area , the work mostly related to ours is the paper by Bonchi et al . [ 13 ] where the problem of topical query decomposition is introduced . Given a query and a document retrieval system , the goal is to select a small set of queries representing coherent , conceptually well separated topics , and whose union of resulting documents corresponds approximately to that of the original query . The authors propose two methods , one based on a special instance of the weighted set covering problem , and one based on constrained clustering . B . Space like Representation of Itemset Databases
• We experiment the proposed approach using two realworld large datasets in the collaborative tagging domain , flickr and delicious , demonstrating the effectiveness and scalability of our solution .
The rest of the paper is structured as follows . In the next section we discuss related work and in Section III we formally define the problem of mining diverse dimensions from a transactional dataset . In Section IV we present our methods , while in Section V we report experimental assessment . Finally , we discuss future work and conclude in Section VI .
II . RELATED WORK
We next survey the literature related to our work , dividing it into three independent groups : ( a ) methods that aim at extracting diverse content from web data , ( b ) space like representations of itemset databases , and ( c ) entropy based measures for itemset interestingness . A . Diversity in Information Retrieval
Extracting a set of diverse dimensions , that covers the various aspects of the underlying dataset , can be seen as a problem of automatic facet discovery . Such a facet discovery process has many applications in improving user experience , for instance , tag recommendation [ 8 ] , search and exploration [ 9 ] , tag clustering [ 10]–[12 ] , and more . Although in this paper we deal with the fundamental problem of diverse dimension decomposition in general transactional database , we believe that our proposals can be applied in these problems , and we are indeed motivated by the collaborative tagging domain , as witnessed by our experiments in Section V .
Traditionally , in association rule mining , itemsets are represented as binary vectors in the space of items : each axis corresponds to an item , and binary coordinate values indicate whether each particular item is contained in the itemset . This representation works well , if we are interested in finding association rules of the form {bread,milk } ⇒ {butter } , which capture itemset level correlations in data . However , binary coordinates do not facilitate geometric decompositions of the item space ( which can be interpreted by a human ) . As a possible solution , Korn et al . [ 17 ] used real valued coordinates , where coordinates could be interpreted as quantities of each item employed in the construction of rules . This framework allowed to perform spectral decomposition of the item space ( similar to SVD [ 18] ) , and discovery of Ratio Rules , ie , quantitative correlations between itemsets in data . An example of such rule is {1:bread,2:milk,5:butter } , which says that a typical ratio of bread , milk and butter within the itemsets is {1:2:5 } , so we can predict missing values of different items given these rules . Alternatively , one can represent a database in the transposed space of transactions rather than items ( like the one shown in Table I ) . This is the main idea behind the “ geometrically inspired itemset mining ” framework proposed by Verhein and Chawla [ 19 ] . Their proposal is a framework for frequent itemset mining , which can accept space transformations , such as SVD , subject to the constraint that a measurement function should be able to be computed in the new space . For instance , in the case of SVD , each new axis represents a linear combi
Our work is different nation of transactions , featuring the largest variance in data . However , such a transformation is not very easily interpretable . in that we propose a principled method for decomposing the space of items in a set of orthogonal dimensions that are readily interpretable . Moreover , our problem formulation is based on information theory , and is capable of identifying dimensions in transactional databases in general , regardless whether transactions have real values associated with items or not .
C . Entropy Based Measures of Itemset Interestingness
Knobbe and Ho [ 1 ] , employing Information Theory , define a measure for itemset interestingness , joint entropy , which is optimizing for the uniform co occurrence among items . In their terminology , a set is a template ( or a collection of attributes taking binary values ) , whose instances are itemsets . Entropy is calculated as a negative sum of logarithm multiplied occurrence probabilities for observed instances . This measure indicates how likely a randomly chosen set instance is to appear in data . The same authors also introduced a notion of “ pattern teams ” [ 2 ] , that can be seen as feature sets . They theoretically evaluate the effectiveness of different filtering criteria for feature sets used in machine learning classifiers , noticing that joint entropy does not satisfy the intuitions we use for dimensions ( ie mutual exclusivity , high coverage ) . Instead , the authors find that exclusive coverage ( ie the sum of coverages minus co occurrences ) is much more suitable as a measure optimizing for these intuitions . Continuing the above line of research , Heikinheimo et al . define two related problems , namely , mining high and lowentropy sets [ 5 ] . Zhang and Masseglia [ 6 ] extended their method to work on streaming data and proposed to reduce its output by removing similar sets according to criteria based on mutual information [ 20 ] . Finally , Tatti [ 3 ] and Mampaey et al . [ 4 ] proposed to use joint entropy in an MDL optimization framework , aiming at compressing the database . Maximizing the entropy ensures that all the pattern subsets are uniformly distributed , while the limit on pattern frequency ( according to the exponential frequency decrease assumption ) facilitates the selection of frequent patterns . Although these papers deal with itemset mining using joint entropy , their goal is different from ours : they aim at extracting sets of items , which co occur in the database uniformly ( when optimized for high entropy : same frequency for all subset combinations ) or sparsely ( when optimized for low entropy : only certain subsets are frequent ) . We discussed the difference between these approaches and our proposal earlier , in Example 2 . In contrast to the above methods , we formulate the entropy of a dimension as the uncertainty of the dimension ’s itemsets for each document , and use it as an indicator of quality for dimensions . Moreover , our goal is to find sets of itemsets ( not items ) , which are not only mutually exclusive ( within each dimension ) , but also independent ( across dimensions ) .
III . PROBLEM STATEMENT
We are given a transactional dataset D , ie , a multiset of transactions t ⊆I , where I is a ground set of items . An example of a transactional dataset is given in Figure 1 . As usual we call itemset any set of items X ⊆I , and we denote by D(X ) its supporting set of transactions , ie , D(X ) = {t ∈ D | X ⊆ t} . Moreover we denote by I the space of all possible itemsets on I . In this paper we are studying the following problem . We are given an integer k and the goal is to discover a collection of k dimensions , that decompose the itemset space I . Moreover , we want each dimension to almost partition the dataset D ; that is to say , we want ( almost ) all transactions t ∈D to contain one and only one of itemsets from the dimension . k ∩ X i l = ∅ .
0 , , X i l ∈ δi with l %= k it holds X i
Definition 1 ( Dimension ) : Given an itemset space I , a dimension δi ⊂ I is a collection of pairwise disjoint itemsets , ie , δi = {X i m} , such that for all pairs of itemsets k , X i X i As in decomposition methods in linear algebra , we want to decompose the itemset space in dimensions that can be though as “ orthogonal . ” While orthogonality in linear algebra is a well understood concept , when talking about the itemset space the concept of orthogonality is much less clear . Motivated by our example , we would like to argue that the dimension camera brand = {{canon },{nikon },{sony } , } is orthogonal to the dimension type of photograph = {{portrait } , {landscape },{street photo } , } . The concept of orthogonality can thus be formulated as independence among the dimensions : the fact that a photograph is tagged by nikon should not reveal any information about the type of the photograph . That is , the likelihood of that photograph being portrait or landscape should remain the same as it is non conditional on camera brand . To formalize the above intuition , we use the concept of mutual information . Given two random variables , X and Y , mutual information measures the information shared between them . For example , if X and Y are independent , then knowing X does not give any information about Y and vice versa , so their mutual information is zero . In order to employ the definition of mutual information , we need to define precisely how our dimensions define a probability space , and what is the entropy of this probability space . We provide those definitions in the next section . In addition to finding orthogonal dimensions we also want to find “ useful ” dimensions , in the sense of being able to explain the dataset succinctly . We express this intuition by the concept of coverage . In the previous example , the dimensioncamerabrand has high coverage because most of the photos have one tag from its collection of itemsets {{canon },{nikon } , } . We are able to show that the concept of coverage can also be formulated in an information theoretic manner . More importantly , we are able to combine both desiderata , high coverage and orthogonality , in one single objective function , achieving to simplify our problem formulation as well as the mining algorithm .
A . Entropy of Dimensions
Our goal is to define the entropy H(δi | D ) of the dimension m} of the itemset space I , on the dataset D . δi = {X i We first define the entropy of the dimension δi conditioned on a single transaction t of the dataset .
0 , , X i
H = 0.0
H = 1.0
H = 2.3
H(δi | t ) = − XX i∈δi
P ( X i | t ) log P ( X i | t )
Fig 2 : Entropy for different probability distributions .
The probabilities P ( X i | t ) express the uncertainty that the itemset X i is present in the transaction t , and are defined later in the section . Averaging over all transactions of the dataset D , we now define the entropy of the dimension δi as follows :
H(δi | D ) = Xt∈D
P ( t)H(δi | t ) , where P ( t ) is the frequency of each transaction in the dataset . For instance , if all transactions are distinct , then P ( t ) = 1/|D| . The conditional entropy of one dimension given another , is calculated similar to an ordinary entropy , but counting only documents assigned to itemsets in a dimension being conditioned . Entropies for each itemset are then aggregated with respect to their probabilities , that is :
H(δi | δj , D ) =
1 |δj|
H(δi | X j , D )
· XX j ∈δj Pt∈D P ( X j | t ) P ( X j | t ) · H(δi | X j , t ) ,
, where and
P ( X i | X j , t ) · log P ( X i | X j , t ) .
H(δi | X j , D ) = Xt∈D H(δi | X j , t ) = −XX i∈δi It remains to define the probabilities P ( X i | t ) , which can be interpreted as the probability of an itemset being relevant for a transaction . When computing relevancy probabilities , we may use different set similarity measures , such as cosine similarity ( 1 ) , Jaccard coefficient ( 2 ) or binary inclusion/exclusion ( 3 ) : ( 3 ) ;
P ( X i | t ) :
( 1 ) ;
( 2 ) ; 1 , X i ⊆ t ;
0 , X i %⊆ t .
|X i ∩ t| |X i| · |t|
|X i ∩ t| |X i ∪ t| several
Also note that after computing the set similarity measures we need to normalize them in order to arrive to a valid probability distribution whose values sum up to 1 . The following example describes the meaning of different probability distributions . Example 3 : Let us consider a dimension δi containing five itemsets:{{canon },{nikon },{olympus },{pentax },{sony }} . Each transaction t in the dataset may be relevant to one or itemsets of the dimension , or not relevant at all . Figure 2 shows three different transactions with the following probability distributions : t1 = {pentax,camera,test} is relevant only to {pentax } , with a probability 1.0 ; t2 = {pentax,nikon,test } is relevant to only two cameras , with probabilities 0.5 ; t3 = {dslr,cameras,test} may be relevant to any camera , thus resulting in equal probabilities and maximal entropy . In this example , entropy reflects the uncertainty of the dimension being relevant to a transaction . When only one itemset is relevant we have low entropy , as in the first case . When none of the itemsets is more relevant , resulting in the unclear choice , the entropy becomes high .
B . Problem Formulation
As we mentioned before , the problem we consider is to the input discover k diverse dimensions that explain well dataset . Let us denote by ∆= {δ1 , . . . ,δ k} such a set of k dimensions . Our objective function evaluates the goodness of the dimension set ∆ in terms of entropy and diversity . We define those next .
Definition 2 ( Entropy of dimension set ) :
Given a set of dimensions ∆= {δ1 , . . . ,δ k} , its entropy is defined as the sum of entropies of its dimensions . 2
H(∆ ) = Xδi∈∆
H(δi )
Definition 3 ( Diversity of dimension set ) :
Given a set of dimensions ∆= {δ1 , . . . ,δ k} , its diversity is calculated as the sum of conditional entropies over all pairs of its dimensions .
DIV ( ∆ ) = Xδi,δj ∈∆
H(δi | δj ) which we define here for a pair of dimensions δi and δj .
Central to our problem is the concept of mutual information , Definition 4 ( Mutual Information ) :
I(δi ; δj ) = H(δi ) − H(δi | δj ) = H(δj ) − H(δj | δi ) .
Mutual information of two dimensions is symmetric and is computed by taking the difference between an entropy of the first dimension , H(δi ) , and its conditional entropy given another one , H(δi | δj ) . The latter entropy expresses the amount of information which one dimension contains about another , and we want this amount to be low ( this happens when the conditional entropy of dimension δi remains large after we have identified dimension δj ) . In order to evaluate the goodness of the set of dimensions ∆ we are summing the mutual information among all pairs of dimensions of the set ∆ . We are now ready to formally define our problem . Problem 1 ( Diverse Dimension Decomposition ) : Given a dataset D , find a set of k dimensions ∆ that minimize f ( ∆ ) : ( 1 ) In the above problem definition , we propose using an optimization function f ( ∆ ) derived from mutual information . Additionally , we introduce a parameter α to control the effect of entropy and conditional entropy over the optimization criterion . One can notice that the value of α = 1 corresponds 2Throughout our paper we assume that all entropies are calculated with f(∆ ) = »H(∆ ) −
· DIV ( ∆)– respect to the dataset D , omitting it in order to simplify the notation . k − 1
2α to the case when the criterion is based precisely on the pairwise sum of mutual informations , but we may pick any other positive real value . This gives us the possibility to optimize either for information loss ( when α is small , eg , α = 0 ) , orthogonality ( when α is large , eg , α = 1 ) , or for both ( when α takes an intermediate value ) . Furthermore , we are able to show that by minimizing the objective function ( 1 ) we are also ensuring that the resulting dimensions explain well the underlying dataset . We first define the notion of coverage of a dimension .
Definition 5 ( Coverage of a dimension ) : Coverage C(δ ) of the dimension δ on the dataset D is the fraction of transactions t in D , for which t ∩ X %= ∅ , for some itemset X ∈ δ .
Definition 6 ( Maximal co occurrence of a dimension ) : We define the maximal co occurrence R(δ ) between any number of itemsets in the dimension δ on the dataset D as the fraction of transactions t in D which contain more than one X ∈ δ . The following two lemmas are needed in our exposition that minimizing f ( ∆ ) ensures high coverage .
Lemma 1 : If the value of the objective function is less than a threshold , f ( ∆ ) ≤ ψ , then
H(∆ ) ≤
ψ
.
1 − α
Proof : For all pairs of dimensions δi and δj , we have that H(δi | δj ) ≤ H(δi ) , what implies that I(δi ; δj ) ≥ 0 . In case of a pairwise sum , DIV ( ∆ ) ≤ H(∆ ) · ( k − 1)/2 . Consequently , if [ H(∆ ) − DIV ( ∆ ) · 2α/(k − 1 ) ] ≤ ψ we have that [ H(∆ ) − α · H(∆ ) ] ≤ ψ , or equivalently , H(∆ ) · ( 1 − α ) ≤ ψ , which proves the lemma .
This lemma predicts that for values of α ≥ 1 the entropy becomes unbounded . In other words , when optimizing solely for orthogonality the quality ( entropy ) of dimensions may become uncontrollable as some of them can be added to a collection solely because of their high independence to others . This can happen for dimensions that have negative contributions to f ( ∆ ) because of a high α .
Lemma 2 : Let δ be a dimension with m itemsets , and consider the case that the probabilities P ( X i | t ) take binary values . Then for the coverage C(δ ) of the dimension δ it should be
C(δ ) ≥ 1 −
H(δ )
|D| log m
.
Proof : Entropy takes its maximum value in the case that a transaction is not covered by a dimension δ . Thus , we have , H(δ | t ) = log m . Therefore , the maximal number of not covered transactions would be less than H(δ ) divided by the maximum entropy . Thus , ( 1−C(δ))|D|≤ H(δ)/ log m , which proves the lemma .
Lemma 3 : If probabilities P ( X i | t ) are computed using binary similarities , then maximal co occurrence R between any two itemsets in a dimension δi should be less than its entropy per single transaction : R(δ ) ≤ H(δi )
.
|D| s log 1
Proof : For a dimension δi , let s be the number of cooccurring itemsets in a transaction t , where 2 ≤ s ≤| δi| . Then P ( X i | t ) = 1 s = log s . Therefore , the minimal entropy of single co occurrence would be equal to log 2 . The maximal number of how many times the two itemsets may co occur would be H(δi ) divided by min entropy . Therefore R(δ)|D| = H(δi)/ log 2 = H(δi ) . s ; and H(δi|t ) = −s 1
We are now stating the theorem that small values of f ( ∆ ) imply high coverage . The theorem is a direct consequence of Lemmas 1 and 2 .
Theorem 1 : Let ∆= {δ1 , . . . ,δ k} be a set of k dimensions and C(∆ ) be their total coverage , defined as C(∆ ) = let m0 be the size of the smallest !δ∈∆ C(δ ) . Finally , dimension of ∆ . If f ( ∆ ) ≤ ψ then for the total coverage we have :
C(∆ ) ≥ k −
|D| log m0(1 − α )
.
ψ
Proof : According to Lemma 2 , the sum of dimensions coverages is greater than :
1
Xδ∈∆
C(δ ) ≥ k −
H(δ ) log m0 Applying our notation and using Lemma 1 , we have :
|D| Xδ∈∆
|D| Xδ∈∆
H(δ ) log m
≥ k −
1
C(∆ ) ≥ k −
H(∆ )
|D| log m0
≥ k −
ψ
|D| log m0(1 − α )
We can use the above theorem to evaluate the quality of the dimensions , or to limit the number of dimensions in the result , eg , by conforming to the specified constraint on the minimum coverage . We next evaluate the dependency of f ( ∆ ) over the number of dimensions k . Suppose that we have a set of k dimensions ∆ , and want to add another dimension δ .
Theorem 2 : Adding a candidate dimension δ will improve f ( ∆ ) as long as its average mutual information ( across dimensions ∆ ) is less than a fraction ( 1− 1 2α ) of its total information . Proof : The difference in the optimality value can then be written as follows : dif f = H(δ ) −
2α k
DIV ( ∆ ∪ δ ) +
2α k − 1
DIV ( ∆ ) dif f ≤ H(δ ) −
2α k
[ DIV ( ∆ ∪ δ ) − DIV ( ∆ ) ] dif f ≤ H(δ ) −
2α k Xδk ∈∆
H(δ|δk )
We are interested in cases when this difference will be negative , what corresponds to improving optimality :
H(δ ) −
2α k Xδk ∈∆
H(δ|δk ) ≤ 0
( 1 − 2α)H(δ ) +
2α k Xδk ∈∆ I(δ ; δk ) ≤ ( 1 −
I(δ ; δk ) ≤ 0
1 2α
)H(δ )
1 k Xδk ∈∆ art black&white color canon sony nikon portrait landscape film portrait color landscape black&white color portrait black&white nature sony portrait skin nature film woman woman skin
Fig 3 : FP Tree constructed from the dataset shown in Figure 1 . For the sake of simplicity , we omitted frequency counts from the nodes , cross references among nodes and the header table , showing only the prefix tree . To avoid having a too large figure , the tree is shown after a pruning of itemsets of frequency less than 2 . Nodes highlighted in gray represent items from first order dimensions , which are blocked and become transparent when considering itemsets for a new dimension ( highlighted in blue ) .
In other words , f ( ∆ ) will decrease when dimensions in ∆ contain on average less than β = 1 − 1/2α percent information about δ . This property allows assessing the level of informativeness for newly added dimensions , and defining criteria for terminating the decomposition . For example , if we stop adding new dimensions when f ( ∆ ) starts to increase , we ensure that dimensions will not contain more than β percent of ambiguous information .
IV . ALGORITHM
We observe that Diverse Dimension Decomposition ( Problem 1 ) is NP hard , by reduction from the Set Partitioning problem , where we want to partition a set into non overlapping and non empty parts that cover the entire set . The above operation corresponds to the partition of items in a dimension . Though , our problem is more complex than that , since we are additionally seeking for a partition of the dimensions . This inherent complexity of the problem makes any brute force approach unfeasible , even for relatively small instances of the problem . In the rest of this section , we describe our solution based on a greedy strategy . Algorithm Outline . Since it is hard to come up with a good initial set of k dimensions for optimization , we propose identifying dimensions one by one , as follows . We start by constructing the first , more prominent dimension , according to our objective function f ( ∆ ) . This process begins with an empty single dimension , and on each iteration we decide whether to add new , or grow existing itemsets , according to the strategies discussed below . The construction of each dimension stops either if it is not possible to improve its optimality or if all items have been partitioned . Then , we do the same for the remaining dimensions iteratively , with the only difference that f ( ∆ ) now takes into account all the previously identified dimensions , optimizing with respect to their orthogonality . The Data Structure . To store the data for our problem , we adopt a compressed database representation in the form of the well known FP Tree [ 7 ] data structure . In Figure 3 , we show an example of such a tree , for the transactional dataset of Figure 1 . This structure allows us to perform efficient pruning based on the coverage , co occurrence and non overlap ( partitioning ) requirements , as explained next .
Search Strategies . We now discuss the search strategies that can be used over the FP tree data structure , as well as the pruning techniques that can be applied on top of those . – Breadth first strategy ( expansion ) : a ) Locate , and remove from further consideration , individual nodes for items that are already in the dimension ( according to the non overlap criterion ; for example , nodes , highlighted in gray in Figure 3 ) ; b ) add one of the remaining available singleton items as a new itemset ; we add these items one at a time . – Depth first strategy ( refinement ) : a ) For an itemset in the dimension , locate the correspondent paths in the FP tree ; b ) Expand this itemset by adding one item at a time from the available children nodes of its paths . However , the problem with the above strategies is that neither of them can lead to a good solution , when used independently : the breadth first strategy may include many singleton items so that refinement ( or expansion ) of individual itemsets in a dimension is no longer possible ; the depth first strategy may restrict adding new itemsets to the dimension by expanding existing itemsets with their children items . – Mixed strategy ( expansion + refinement ) : Apply the expansion and refinement steps at every iteration . This is the strategy we use in this paper , and we discuss it in more detail in the following paragraphs ( refer to Algorithm 1 ) . Pruning Strategies : We have already described the basic pruning strategy ( non overlap ) based on our definition for dimensions . Our more advanced pruning strategy is based on the relationship between entropy and such characteristics as coverage and co occurrence , as described in Lemmas 2 3 . For each candidate dimension with entropy H , we are interested in obtaining refined dimensions , which do not exceed this value . Thus , we compute the corresponding thresholds for the minimal coverage C and maximal co occurrence R ( according to the above lemmata ) , and use them for pruning the itemsets which are added or refined . Algorithm Description : We formulate our optimization problem in a greedy fashion , relying on a mixed candidate generation strategy and an iterative refinement of the candidate set . The complexity of this approach ( almost ) linearly depends on the size of the candidate set ( as seen in Figure 5 ) , which we use as a parameter . Another input of our algorithm is the FP Tree , optionally containing only the most frequent items .
Algorithm 1 : Mining Orthogonal Dimensions Name : findNewDimension Input : First order dimensions ∆= {δk} , k < i , Candidate dimensions candidates = {} , FP Tree , memoryBudget Output : Optimal dimension δi repeat out of order i forall the dimension δi ∈candidates.unprocesseddo forall the itemset Ii ∈ δi do forall the items Ij ∈ children(Ii ) , Ij /∈ δi , ∆ do if validItemset(Ii ∪{ Ij} | δi ) then //add one item to the current itemset temp = {δi | Ii = Ii ∪{ Ij}} ; δi temp | ∆ ) ; checkOptimality(δi temp ) ; candidatestempadd(δi end end t n e m e n fi e r n o i s n a p x e end forall the items Ij ∈ I , Ij /∈ δi , ∆ do if validItemset({Ij} | δi ) then //add one more item as an itemset temp = δi ∪{ Ij} ; δi temp | ∆ ) ; checkOptimality(δi temp ) ; candidatestempadd(δi end end end //mark unprocessed as processed candidates += candidates.unprocessed ; //newly generated become unprocessed candidates.unprocessed = candidates.temp ; candidates.temp = {} ; //sort so that most optimal values are first candidates.sort( ) ; //remove candidates exceeding the allocated memory repeat until candidates.size > memoryBudget ; candidatesremove(candidateslastElement ) ; until candidatesunprocessedsize > 0 ; return δi out = candidates.firstElement ;
This initial pruning does not affect the output ( as long as the items forming the dimensions are preserved ) , but significantly reduces the complexity of the problem .
Our general approach starts with an empty set of dimensions , and uses Algorithm 1 to find each new dimension , resulting in the best optimality value when added to the set of previously selected dimensions ; up to the specified number k . The most essential part of this algorithm is the greedy dimension optimization procedure findNewDimension , which takes as a parameter a set of the first order dimensions ∆ , and an empty set of candidates , and after a finite number of iterations ( the first loop ) it converges to the single most optimal dimension , which is added to the ∆ as the next one .
Algorithm 2 : ItemSet pruning method validItemset . Entropy for a given dimension will improve only if the new itemset meets coverage and co occurrence requirements computed using Lemmas 2 3 for the dimension ’s entropy . Name : validItemset Input Output : true if itemset is valid , false otherwise //use Lemma 2 to calculate min coverage covmin = Lemma2(H(δi) ) ; //use Lemma 3 to calculate max co occurrence coocmax = Lemma3(H(δi) ) ; return ( C(Ii ∪ δi ) > covmin & R(Ii,δ i ) ≤ coocmax ) ;
: Dimension δi , itemset Ii , FP Tree
More specifically , Algorithm 1 iteratively refines dimensions in the candidate set ( the empty initial set is refined only by expansion ) and at each iteration performs sorting of candidates according to their optimality . The list of sorted dimensions is then being pruned according to the specified memory budget . By doing this operation , the algorithm ensures that at each step it would refine and check the optimality of only a short list of candidates , which is equal to the memory budget or lower . After all candidates in the list were refined , they are marked as processed ( transferred to the main list ) , and the newly generated list of candidates becomes the next list of unprocessed candidates . The algorithm converges when there are no candidates left in the list , which were not refined . Then it outputs the topmost optimal candidate . More insights on the algorithm can be obtained by examining Figure 4 . In that figure , we depict detailed results of the refinement procedure for two specific domains , namely , “ pyramid ” and “ art ” , from the flickr dataset . In both cases , we focus on the identification of the first dimension , and we depict for each iteration of the Algorithm 1 the size ( number of itemsets ) of the currently best dimension ( bottom graphs ) , as well as the corresponding entropy value ( top graphs ) . We observe that for the “ pyramid ” domain the algorithm quickly increases the size of the dimension by adding more itemsets ( seen as diagonal steps ) and refining them ( seen as horizontal steps ) , resulting in a significant initial improvement of the entropy of the dimension . Starting at iteration 6 , the number of itemsets remains stable , though , the algorithm adds new items to them , leading to further improvements in entropy ( which can be observed by the decrease of value on the topmost graph ) . In contrast , for the “ art ” domain the algorithm starts with a dimension of good quality ( low entropy ) , which after a single refinement ( from iteration 1 to 2 ) stays on the top of the list of candidates till iteration 9 ( the value of entropy does not increase during this interval ) , while other candidates are being refined . Then , starting from the iteration 10 , another candidate refined to a better quality takes its place and shows even better improvement in entropy . Finally , iterations converge and the best dimension is being identified . We note that the final dimensions identified for the “ pyramid ” and “ art ” domains ( after 17 and 15 iterations , respectively ) are also the optimal single dimension decompositions for these domains . y p o r t n e y p o r t n e
0.6 0.5 0.4 0.3 0.2 0.1 0
0.6 0.5 0.4 0.3 0.2 0.1 0 domain ’pyramid’ entropy size
0
2
4
6
8 10 iteration
12
14
16 domain ’art’ entropy size
0
2
4
6
10 8 iteration
12
14
16
5 4 3 2 1 0 18 e z s i
14 12 10 8 6 4 2 0 18 e z s i
Fig 4 : Optimization stats of the 1st dimension for “ pyramid ” and “ art ” ( flickr ) .
V . EXPERIMENTAL EVALUATION
We evaluate our algorithm on two datasets3 containing tagannotated resources . The first dataset , extracted from flickra popular photos sharing website , contains 28 million tagsets ( or transactions ) , obtained by taking annotations for all pictures that contained a specific domain tag , for 34 different domains . To remove noise , we allowed only unique tagsets for each user id . The second dataset contains tagsets from delicious , a social bookmarking website . For this dataset , we selected annotations for URLs starting with specific domain names picked from Yahoo!Directory . Overall , the delicious dataset contains 1.7 million tagsets over 150 domains . The number of unique tags in each of the datasets was about half a million . For both datasets we performed a limited amount of additional cleaning by removing the domain term , numeric and navigational tags , as well as removing some language variability , based on a custom built dictionary . No sophisticated preprocessing was applied , so some of the discovered dimensions in our experimental results still contain repetitions due to synonyms and misuse of tags . A . Performance
In the first set of experiments , we report the execution time ( Figure 5 ) and entropy of the best solution found ( Figure 6 ) , as a function of the maximum number of candidates considered by our algorithm . We vary the number of items between 8 − 20 , over the 150 domains of the delicious dataset . In the graphs , we report the normalized values , averaged over all the 150 domains , as well as the standard deviation for these values ( for most of the points standard deviation is too small and not visible ) . In order to make the results directly comparable to each other , we first normalize each series using the minimum ( maximum ) value of its regression line for the time ( entropy ) 3The url of the web page containing the code and the datasets is not available due to the double blind review process . the algorithm scales linearly with respect graph . Then , we compute the average normalized series , and its deviation . In Figure 5 , we report the averaged normalized execution times versus memory budget . We observe , that an increase in number of items results to an increase in complexity . Overall , to the memory budget . When the number of items becomes large , the complexity is still determined by the memory budget ( remember , that at each iteration the number of refinements is proportional to the size of the candidate set ) . In Figure 6 , we observe that for a small number of items , an increase in memory brings a considerably larger improvement in entropy , than for larger numbers of items . In the case of 8 items , the entropy reaches its minimum for a maximum number of candidates of 32 , which corresponds to the optimal solution . For larger number of items , the same effect is observed for a higher setting of the maximum number of candidates . B . Parameters , Monotonicity , Synthetic Experiments the series drops until
In order to evaluate various properties of our approach in a controlled environment , we constructed a synthetic dataset by generating itemsets for a number of dimensions closely resembling dimensions found in real datasets . These dimensions contained two to five itemsets of sizes up to three items , and we required exactly two dimensions to be present in each dataset . Following the construction of dimensions , we calculated the frequencies of singleton items by applying Zipf ’s law with a specified parameter z , f ( ik ) ∼ 1/kz . This distribution was chosen because it is known to resemble word frequencies in real world datasets . Moreover , it allows to produce frequencies that are close to the uniform ( when z is small ) , or the exponential ( when z is large ) distributions . Evidently , the first case is more challenging for our problem . In the subsequent step , we added a uniform noise of level ν to these frequencies , and harmonized them for the items belonging to each of the dimension ’s itemsets ( to account for the observed rule that itemsets in dimensions usually have equally frequent items ; for example , both tags in {eiffel tower } usually appear together ) . Finally , the itemsets were generated by iteratively sampling the distribution of items with respect to the specified dimensions . In this process , we used Gibbs sampling first to select a dimension ( independently from other dimensions ) and then to select the itemset representing it ( allowing for co occurrence with a level of 025ν ) The rest of items , not covered by any dimension , were distributed with respect to their frequencies . In these experiments , we restricted the number of items to n = 16 , which is equivalent to our minimum support filtering on real datasets . Overall , we were generating 10 thousand itemsets for each dataset to ensure a smooth distribution according to our model . We assessed the quality of the identified dimensions by comparing them to the dimensions used while generating the dataset . Our similarity measure was based on the Hamming distance d between two dimensions ( represented as binary
1⋅102 e m i t d e z i l
20 items 16 items 12 items 8 items
1⋅101 a m r o n e g a r e v a
1⋅100
1
8
2
32 4 max candidates number Fig 5 : Time vs memory budget .
16
20 items 16 items 12 items 8 items y p o r t n e d e z i l a m r o n e g a r e v a
1.06 1.05 1.04 1.03 1.02 1.01 1.00
1
2
8
4 32 max candidates number
16
Fig 6 : Entropy vs memory budget . y t i r a l i m s i e g a r e v a
1.0
0.9
0.8
0.7
0.6 zipf 1.2 0.05
0 zipf 0.8 zipf 0.4 0.2
0.15
0.1 noise level
Fig 7 : Similarity to optimal dimensions versus noise . y t i l a m i t p o d e z i l a m r o n e g a r e v a
6 4 2 0 2 4 6 8 10 alpha 0.25 alpha 0.50 alpha 0.75 2
1
3
4
5
6 number of dimensions
Fig 8 : f ( ∆ ) dependency on the number of dimensions .
64
128
64
128 vectors ) divided by the total number of items : sim(∆;∆ 0 ) = 1 − d(∆;∆ 0)/n . This measure takes values in the range [ 0 ; 1 ] , with higher values indicating stronger similarity : a value of 1 means that the algorithm correctly identified the planted dimensions . We note that this measure does not account for the varying significance of items , which is not favoring our approach , since including low support items in the dimensions represents a challenge , even without the additional noise . The evaluation of quality against noise for different parameters z is shown in Figure 7 . In gray lines we plot the 0.95 confidence intervals for average values . We can see that regardless of the noise added , our method is able to reconstruct almost perfectly the optimal dimensions for a wide range of distributions . As expected , the similarity between the identified and the optimal dimensions decreases on average with growing noise , and is significantly lower for smaller parameters z ( more uniform items distribution ) . In Figure 8 we evaluate the monotonicity of f ( ∆ ) over the number of dimensions k , for different values of α parameter . It is clearly visible that for small values of α optimality gets higher ( worse ) , while for large values every new dimension improves optimality ( albeit , not the quality of extracted dimensions ) . For our experiments we chose α = 0.5 , since it provides a good balance between orthogonality and interestingness , and allows to rely on Theorem 2 ( controlling the decomposition ) for a wide range of data distributions .
C . Qualitative Results
We now report results on a qualitative evaluation of the proposed approach . We ran our algorithms on a set of different domains from flickr and delicious datasets : “ eiffel tower ” , “ art ” , “ hollywood ” , “ pyramid ” , and “ spain ” for flickr ; “ nytimes.com ” , “ lifehacker.com ” , “ dpreview.com ” , “ apple.com ” , “ microsoft.com ” , and “ ixbt.com ” for delicious We use a 3 % minimum support threshold on items for all domains . The results of this experiment are summarized in Table II , where for each domain we report the top dimensions identified by our algorithm . We should note , that because of the fixed minimum support threshold , for some of the domains all available items are allocated to the first few dimensions , thus resulting in the varying number of dimensions being identified . In every case , we limit this number to the 3 top dimensions . The dimensions reported by our algorithm are successfully describing the different concepts under each domain . For example , under the “ eiffel tower ” domain , we have as first dimension the Eiffel Tower in Paris and Las Vegas4 , as second dimension holidays in Paris , and as third dimension architecture , all of which are different concepts related to “ eiffel tower ” . Similarly , the “ dpreview.com ” domain in the delicious dataset is described by the concepts of photographic camera reviews , digital [ photography ] , and shopping .
4The city of Las Vegas ( NV , USA ) hosts a replica of the Eiffel Tower . domain “ jaguar ” domain “ eiffel tower ”
δi collection of itemsets for δi ( flickr ) 1 {automobile} , {zoo} 2 {etype} , {auto} 1 {paris france europe tower} , {lasvegas} 2 {night seine} , {holiday travel} 3 {architecture} 1 {egypt giza cairo sphinx} , {louvre paris museum glass} , 2 {france sky} , {travel teotihuacan} 3 {architecture night} , {chichenitza} domain “ hollywood ” 1 {losangeles california sign} , {star film actor} 2 {us universalstudios} , {hollywoodboulevard night} 3 {theatre party sunset} , {canon street} 1 {painting drawing} , {graffiti streetart} ,
{mexico maya ruins} , {sanfrancisco transamerica} domain “ pyramid ” domain “ art ”
{sculpture museum} , {newyork} , {color} , {photo} , {street} domain “ spain ”
1 {barcelona catalonia} , {madrid europe} ,
{andalusia granada} , {seville} , {valencia} , {holiday travel}
2 {architecture}
δi collection of itemsets for δi ( delicious ) 1 {news politics} , {food health} , {science} , domain “ nytimes.com ”
{article} , {business} , {technology} domain “ dpreview.com ”
1 {photo camera review} , {dslr} 2 {digital} 3 {shopping} domain “ lifehacker.com ” 1 {howto lifehacks tips},{software windows tools freeware} 2 {firefox internet} , {linux utilities} , {email extensions} , {mp3 download} , {organization toread} , {photography} domain “ apple.com ” domain “ microsoft.com ”
{movies trailers},{iphone},{podcast podcasting},{technology}
1 {mac osx software},{ipod itunes music},{video quicktime} , 2 {macosx howto} 1 {windows software tools} , {.net programming} 2 {security xp} 3 {utilities} 1 {hardware software news computers russian} , 2 {article} 3 {reviews}
{photo photography} domain “ ixbt.com ”
TABLE II : Top dimensions for different domains in flickr and delicious
The results of this experiment demonstrate that our approach can effectively identify the diverse concepts related to some domain , in an automatic fashion . Finally , we observe that our algorithm provides meaningful results , even when operating on noisy datasets , such as flickr and delicious , which contain a large number of non useful tags .
VI . CONCLUSIONS AND FUTURE WORK
Motivated by applications on repositories of annotated resources in the collaborative tagging domain , we introduce the problem of diverse dimension decomposition in transactional databases . In particular , we adopt an information theoretic perspective on the problem , relying on entropy for defining a single objective function that simultaneously captures constraints on coverage , exclusivity and orthogonality . We present an approximate greedy method for extracting diverse dimensions , that exploits the FP tree representation of the input transactional dataset and clever pruning techniques . Our experiments on datasets of tagged resources from flickr and delicious confirm effectiveness and efficiency of our proposal . The assessment on synthetic and artificially noisy data confirms that our method is able to reconstruct the “ true ” dimensions , and it withstands noise . In our future investigations , we plan to study and compare both empirically and theoretically , different probability measures , which not only control “ the look ” of identified dimensions , but can even lead to their different semantics . This becomes especially relevant in the context of our optimization problem , where a probability space may well determine the overall shape of the optimization objective and its convergence . We also plan to have a user study for evaluating the discovered dimensions in different domains . A possibility is also that of developing a vertical application exploiting our method for mining diverse dimensions in order to detect , in unsupervised and automatic fashion , collection of web sites with diverse content from delicious
REFERENCES
[ 1 ] A . J . Knobbe and E . K . Y . Ho , “ Maximally informative k itemsets and their efficient discovery , ” in KDD , T . Eliassi Rad , L . H . Ungar , M . Craven , and D . Gunopulos , Eds . ACM , 2006 , pp . 237–244 . [ 2 ] —— , “ Pattern teams , ” in PKDD , ser . Lecture Notes in Computer Science , J . F¨urnkranz , T . Scheffer , and M . Spiliopoulou , Eds . , vol . 4213 . Springer , 2006 , pp . 577–584 . [ 3 ] N . Tatti , “ Probably the best itemsets , ” in KDD , B . Rao , B . Krishnapuram , A . Tomkins , and Q . Yang , Eds . ACM , 2010 , pp . 293–302 . [ 4 ] J . V . Michael Mampaey , Nikolaj Tatti , “ Tell me what i need to know : succinctly summarizing data with itemsets , ” in KDD , 2011 . [ 5 ] H . Heikinheimo , E . Hinkkanen , H . Mannila , T . Mielik¨ainen , and J . K . Sepp¨anen , “ Finding low entropy sets and trees from binary data , ” in KDD , 2007 . [ 6 ] C . Zhang and F . Masseglia , “ Discovering highly informative feature sets from data streams , ” in DEXA , 2010 . [ 7 ] J . Han , J . Pei , and Y . Yin , “ Mining frequent patterns without candidate generation , ” in ACM SIGMOD Conference , 2000 , pp . 1–12 . [ 8 ] B . Sigurbj¨ornsson and R . van Zwol , “ Flickr tag recommendation based on collective knowledge , ” in WWW , 2008 . [ 9 ] R . van Zwol , B . Sigurbj¨ornsson , R . Adapala , L . G . Pueyo , A . Katiyar , K . Kurapati , M . Muralidharan , S . Muthu , V . Murdock , P . Ng , A . Ramani , A . Sahai , S . T . Sathish , H . Vasudev , and U . Vuyyuru , “ Faceted exploration of image search results , ” in WWW , 2010 . [ 10 ] M . Grahl , A . Hotho , and G . Stumme , “ Conceptual clustering of social bookmarking sites , ” in LWA 2007 : Lernen Wissen Adaption , 2007 . [ 11 ] D . Ramage , P . Heymann , C . D . Manning , and H . Garcia Molina , “ Clustering the tagged web , ” in WSDM 2009 : Proc . of the 2nd ACM Int . Conference on Web Search and Data Mining , 2009 . [ 12 ] M . van Leeuwen , F . Bonchi , B . Sigurbj¨ornsson , and A . Siebes , “ Compressing tags to find interesting media groups , ” in CIKM , 2009 . [ 13 ] F . Bonchi , C . Castillo , D . Donato , and A . Gionis , “ Topical query decomposition , ” in KDD , 2008 . [ 14 ] B . Carterette and P . Chandar , “ Probabilistic models of ranking novel documents for faceted topic retrieval , ” in CIKM , 2009 . [ 15 ] R . L . Santos , C . Macdonald , and I . Ounis , “ Exploiting query reformulations for web search result diversification , ” in WWW , 2010 . [ 16 ] G . Capannini , F . M . Nardini , R . Perego , and F . Silvestri , “ Efficient diversification of web search results , ” PVLDB , vol . 4 , no . 7 , pp . 451– 459 , 2011 . [ 17 ] F . Korn , A . Labrinidis , Y . Kotidis , and C . Faloutsos , “ Quantifiable data mining using ratio rules , ” VLDB J . , vol . 8 , no . 3 4 , pp . 254–266 , 2000 . [ 18 ] G . H . Golub and C . F . Van Loan , Matrix Computations , 3rd ed . The Johns Hopkins University Press , October 1996 . [ 19 ] F . Verhein and S . Chawla , “ Geometrically inspired itemset mining , ” in Proc . ICDM 2006 , 2006 , pp . 655–666 . [ 20 ] T . M . Cover and J . A . Thomas , Elements of Information Theory . New york , NY , USA : Wiley & Sons , 1991 .
