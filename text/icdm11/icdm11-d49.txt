Fast and Robust Graph based Transductive Learning via Minimum Tree Cut
Yan Ming Zhang , Kaizhu Huang , and Cheng Lin Liu
Institute of Automation , Chinese Academy of Sciences , Beijing 100090 , China .
National Laboratory of Pattern Recognition ,
{ymzhang , kzhuang , liucl}@nlpriaaccn
Abstract—In this paper , we propose an efficient and robust algorithm for graph based transductive classification . After approximating a graph with a spanning tree , we develop a linear time algorithm to label the tree such that the cut size of the tree is minimized . This significantly improves typical graphbased methods , which either have a cubic time complexity ( for a dense graph ) or O(kn2 ) ( for a sparse graph with k denoting the node degree ) . Furthermore , our method shows great robustness to the graph construction both theoretically and empirically ; this overcomes another big problem of traditional graph based methods . In addition to its good scalability and robustness , the proposed algorithm demonstrates high accuracy . In particular , on a graph with 400 , 000 nodes ( in which 10 , 000 nodes are labeled ) and 10 , 455 , 545 edges , our algorithm achieves the highest accuracy of 99.6 % but takes less than 10 seconds to label all the unlabeled data .
Keywords graph based semi supervised learning ; transduc tive learning ; graph mining ;
I . INTRODUCTION
In many machine learning applications , labeled data are scarce because labeling data is both time consuming and expensive . However , unlabeled data are very easy to collect in many applications such as text categorization and image classification . This has motivated machine learning researchers to develop learning methods that can exploit both labeled and unlabeled data during model learning . Such a learning paradigm developed over the past decade is referred to as semi supervised learning [ 7 ] .
In this paper , we consider a particular setting of semi supervised learning called transductive learning in which the unlabeled test data are also available before model training . Specifically , we are given l labeled data points ( x1 , y1 ) , . . . , ( xl , yl ) and u unlabeled data points xl+1 , . . . , xl+u , where xi , 1 ≤ i ≤ l + u , is the input of a data point and yi ∈ {1 , . . . , K} , 1 ≤ i ≤ l , indicates the class of the labeled data point xi . The goal is to predict the labels of unlabeled data by utilizing the information from both the labeled and unlabeled data .
Among the most popular transductive learning methods are graph based methods [ 25 , 24 , 1 ] . Graph based methods are appealing because : ( 1 ) graph provides a powerful tool to describe the similarity between data ; ( 2 ) for many important applications , like social network , genomic data , web pages etc . , problems directly present themselves as graphs .
Despite of these advantages , graph based methods suffer from two main drawbacks[18 ] . ( 1 ) They are not scalable . Typical graph based methods usually have a cubic time complexity because of the calculation of the inverse of graph Laplacian . Although the complexity can be reduced to O(kn2 ) ( k is the node degree ) when the graph is sparse , it is still not applicable to large scale problems . ( 2 ) Their performance is very sensitive to the graph construction . For the most popular kNN graph or ǫ graph , a small change in k or ǫ would make a big difference in accuracy [ 21 , 14 ] . Thus , one has to carefully tune these parameters in practice , which is however very difficult because of the scarcity of labeled data .
To overcome these problems , in this paper , motivated by the fact that most real world graphs are sparse , we propose to firstly approximate the graph with a spanning tree , and then label the tree in a way such that the overall cut size is minimized . For a given spanning tree , our labeling algorithm has a linear time and space complexity with respect to the data size which makes our method a perfect choice for large scale problems .
We use a minimum spanning tree ( MST ) to approximate a graph . In addition to its simplicity , MST is very robust to the graph . For a connected ǫ graph with the RBF weighting function , we can formally prove that the structure of MST is invariant to the increase of ǫ . We also observe the same property empirically for the kNN graph . This property in turn makes the performance of our method very robust to the graph hyperparameter . Although there are some works that use a tree to approximate a graph , their motivation is to design fast semi supervised learning algorithm . To our best knowledge , this is the first work to extensively explore the robustness property of tree based semi supervised learning methods .
We have performed detailed experiments to confirm the advantage of the proposed method in both scalability and robustness . In term of accuracy , empirical results shows that our method gives good approximations to traditional graph based methods , while is consistently better than other scalable graph based methods .
The rest of this paper is organized as follows . In the next section , we introduce the related work . In Section III , we then present our major work including the notations , the model definition , the detailed algorithm , and the relationship with a previous model . After that , in Section IV , we introduce how to perform transductive classification using our proposed method . In Section V , we provide experimental results to validate the advantages of our method . Finally , we set out the conclusion in Section VI .
II . RELATED WORK
Recently , Herbster et al . [ 12 ] proposed to approximate a graph using a spanning tree , and designed an algorithm to calculate the inverse of the tree ’s Laplacian in O(n2 ) time . Then the perceptron algorithm was performed on this Laplacian kernel matrix to predict all unlabeled data . By calculating one column of the Laplacian kernel for each trial , their algorithm takes O(ln ) time and O(n ) space , where n is the number of the data points .
Herbster and Lever [ 11 ] considered the online graph prediction problem . They proposed to approximate a general graph with a path graph , and use the nearest neighbor classifier to predict on the path graph . They gave cumulative mistake bounds of their algorithms which show improvements over Laplacian based methods when the graph has a large diameter . Cesa Bianchi et al . [ 6 ] extended the work of [ 11 ] from the un weighted graph to the weighted one , and shown the cumulative errors of their algorithm is bounded in terms of the cut size of a random spanning tree . The cost of their algorithm is O(n ) in both time and space requirement . However , these two methods approximated the original graph by exploiting a path graph , which is basically a naive tree structure , ie , a line , and hence too simple and inflexible to generate good performance for practical problems . Experimental results later presented in Section V also validate this point .
Cesa Bianchi et al . [ 5 ] characterized the number of mistakes necessary and sufficient for sequentially predicting a given tree , and built an algorithm to achieve this number by minimizing the cut size of the tree . Their method uses the space linear in n , and the running time is min{K , nf }K + n log DT for online learning where K is the cut size of the labeled tree T , DT is the diameter of T , nf is the number of nodes in T with the degree bigger than 2 . However , for transductive learning , the running time is O(n2 ) . We will discuss the difference between this work and our method in more details in the next section .
Another family of scalable graph based semi supervised learning methods are based on subsampling a small subset from the whole data set . These prototypes are used to build both the prediction function and the adjacency matrix . Methods differ from each other in the approaches to construct the adjacency matrix . Delalleau et al . [ 10 ] built the matrix by directly setting wij = 0 if neither i nor j is a prototype . Zhang et al . [ 23 ] approximated the adjacency matrix by the Nystr¨om method , and Liu et al . [ 16 ] approximated it by a two step transition probability matrix . The running time of all these prototype based methods are O(m2n ) , where m is the number of prototypes . However , these methods need to use feature vectors of data and are hence incapable of handling problems where only graphs are available .
For sparse graph , Spielman and Teng [ 20 ] presented a randomized algorithm to solve diagonally dominant linear systems . It can be used to solve Gaussian random fields [ 25 ] ’s optimization problem in n logO(1 ) n log 1
ǫ time .
III . PREDICTING A TREE : MINIMUM TREE CUT
ALGORITHM
In this section , we present the main work in this paper . We first present notations used throughout the paper and then introduce our work in details .
A . Notations
We use G to denote a graph , T to denote a tree , while E(G ) and E(T ) denote the edge set of graph G and tree T respectively . For a tree , ↑ ( i ) denotes the parent of node i , ↓ ( i ) denotes the set of i ’s children , and ↓∗ ( i ) denotes the set of nodes in the subtree rooted at i . |S| is the size of set S . A cut is an edge connecting two nodes that have different labels . Given a labeling f ∈ {1 , . . . , K}n of a graph G , where K is the number of classes , the cut size of f on G is the sum of weights of all cuts , which can be denoted as
P(i,j)∈E(G ) wij I{fi 6= fj} .
B . Objective Function
Blum et al . [ 3 , 4 ] first proposed to predict the graph with the labeling which minimizes the cut size of the graph , and meanwhile is consistent with all the labeled data . However , for multi class discrete variables , this optimization problem is NP hard . Thus , Zhu et al . [ 25 ] binarized the problem via a standard one vs all scheme , and solved it in the continuous space . Their method is still too expensive to solve large scale problems . Inspired by [ 12 ] and [ 5 ] , we first approximate the graph by a spanning tree , then solve the cut size minimization problem directly on the discrete label set . Formally , the objective function of our method can be formulated as follows : min f X(i,j)∈E(T ) wij I{fi 6= fj}
( 1 ) st i = 1 , . . . , l fi = yi f ∈ {1 , . . . , K}n , where E(T ) is the edge set of the tree T , K is the number of classes , and wij describes the weight for edge ( i , j ) . As we will show in the next subsection , by exploiting the special structure of tree , the above problem can be exactly solved in linear time by a dynamic programming algorithm .
As pointed out by [ 3 , 15 ] , mincut problems may have multiple solutions in theory . But in practice , pairwise distances of high dimensional data points are usually different from each other , which results in different edge weights given by
Function : split internal(i ) begin children ← ∅ ; for each j ∈↓ ( i ) do if j is an external node then output {i , ↓∗ ( j)} as a sc tree ; else if j is a labeled node then push(j ) ; else children ← childrenS{j} ; children ← childrenSsplit internal(j ) ; return : children S {i} ;
Function : split labeled(i ) begin for each j ∈↓ ( i ) do if j is an external node then output {i , ↓∗ ( j)} as a sc tree ; else if j is a labeled node then push(j ) ; output {i , j} as an lb tree ; else children ← split internal(j ) ; output {i , children} as an lb tree ;
Function : split(r ) begin
//r must be a labeled node stack← ∅ ; push(r ) ; while stack6= ∅ do j ← pop( ) ; split labeled(j ) ;
Algorithm 1 : Tree splitting algorithm
( MTC ) algorithm is to compute the function cutsize(i , k ) which is defined as the minimum cut size of the subtree rooted at node i when i is labeled as k . Because of the special structure of tree , this value can be calculated directly by the cutsize values of node i ’s children . Specifically , for an unlabeled node i in an lb tree , we have cutsize(i , k )
( 2 ) min{cutsize(j , k ) , min{cutsize(j , ˜k ) + wij : ˜k 6= k}} .
= X j∈↓(i )
For leaf nodes , recall that the leaves of an lb tree are all labeled . We define the cutsize value of a leaf i as cutsize(i , k ) = fl 0 , k = yi ∞ , k 6= yi
.
( 3 )
Thus , the cutsize values of nodes in the lb tree can be computed from bottom to top by ( 2 ) and ( 3 ) . To store all
Figure 1 . An illustration of lb tree and sc tree . In this partially labeled tree , black nodes denote labeled data , and blank nodes are unlabeled . The subtrees in solid lines are lb trees , and the subtrees in dash lines are sc trees . wij = exp{−|xi − xj|2/σ2} . When weights of edges are different , it is very unlikely to have multiple solutions . This phenomenon has actually been observed in our experiments .
C . Minimum Tree Cut Algorithm ( MTC )
Before detailing the algorithm , we first define some concepts which will be used extensively in our method . The first one , called label bordered tree ( lb tree ) , is firstly introduced in [ 5 ] . Formally , given a partially labeled tree T , an lb tree is any maximal subtree of T whose leaves are all labeled and no internal node is labeled . The second concept is called single color tree ( sc tree ) . Given all the lb trees , an sc tree is defined as any maximum subtree sharing one and only one common node with one single lb tree . Moreover , we call the common node as the connecting node . Obviously , different sc trees have no common nodes . Furthermore , we say an unlabeled node is internal if it is in a certain lb tree , otherwise it is external . See Figure 1 for an example .
Our method proceeds as follows . First , we split a partially labeled tree T into lb subtrees and sc subtrees . Then , we use the proposed MTC algorithm to label each lb tree . Finally , we label all the nodes in one sc tree with its connecting node ’s label , which motivates the name of the sc tree .
To split a partially labeled tree , we first classify unlabeled nodes into internal and external categories . This can be done by a post order visit of the tree from any labeled node . An unlabeled node is internal , if and only if it has at least one child which is internal or labeled . Otherwise , it is external . After that , the tree splitting can be done by calling the split(r ) function in Algorithm 1 . For succinctness , we briefly list the algorithm without specific explanation . Since the algorithm visits each edge at most once , the computational cost of Algorithm 1 is O(n ) .
In the following , we design a novel method to label an lbtree . This is one major contribution of our work . Given an lbtree T , we predict its unlabeled nodes in such a way that the cut size of T is minimized . The core of our minimum tree cut cutsize values for an lb tree , a table of size | ↓∗ ( r)| × K is needed . Since computing cutsize(i , k ) needs | ↓ ( i)|×K op erations , filling the whole table needs PiPk | ↓ ( i)|× K =
| ↓∗ ( r)| × K 2 operations . Therefore , the algorithm needs O(Kn ) space and O(K 2n ) operations to compute all lbtrees ’s cutsize values . When K is big , we could use the one vs all scheme to binarize the problem . As solving each binary classification problem needs O(n ) operations , the total computational cost is reduced to O(Kn ) .
Once the cutsize values of the root r are determined , by definition min{cutsize(r , k ) : k = 1 , . . . , K} is the minimum cut size of the lb tree . We can then predict each unlabeled node to achieve this minimum cut size . This is done by a traversal from top to bottom . For the root node r , we predict it by k∗(r ) = arg min
{cutsize(r , k)} . k
For any other unlabeled node i , suppose we have predicted its parent ’s label as k∗(↑ ( i) ) . We predict i as k∗(i ) which is calculated by k∗(i ) =   k∗(↑ ( i ) )
˜k(i ) cutsize(i , k∗(↑ ( i) ) ) if ≤ cutsize(i , ˜k(i ) ) + wi↑(i ) otherwise , where ˜k(i ) = arg mink{cutsize(i , k ) : k 6= k∗(↑ ( i))} . Since the prediction for each node requires K comparisons , the computational cost for predicting all lb trees is O(Kn ) . Now , we formally prove that the proposed method exactly solves the optimization problem ( 1 ) .
Theorem 1 . The MTC method introduced in the subsection III C optimizes the problem ( 1 ) exactly .
Proof : We denote the optimal value of the problem ( 1 ) as mincut(T , y ) . It is the minimum cut size on a tree T with respect to the given labels y on that tree . Because of the special structure of tree , we have , mincut(T , y ) = XTi∈sc−tree(T ) mincut(Ti , yi )
+ XTi∈lb−tree(T ) mincut(Ti , yi )
≥ XTi∈lb−tree(T ) mincut(Ti , yi ) , where sc − tree(T ) and lb − tree(T ) are the sets of the sc trees and the set of the lb trees of T . On the other hand , suppose the solution given by the MTC method is f ∈ {1 , . . . , K}n . By construction , f induces no cut on any sc tree , and achieves the minimum cut size on each lb tree . Thus , the cut size of f on the whole T is exactly
PTi∈lb−tree(T ) mincut(Ti , yi ) . This completes our proof .
To conclude this part , we emphasis that , since each step of the MTC method ( tree splitting , cutsize computing and labeling ) has a time and space cost of O(n ) , the overall complexity of our method is O(n ) .
D . Insensitiveness to Graph Construction
In this section , we prove the property that the structure of MST is insensitive to the ǫ graph theoretically . Hence , the proposed algorithm is very robust to the parameter in graph construction , making it very desirable for practical problems .
Proposition 1 . Suppose S = {x1 , . . . , xn : xi ∈ Rd} is a set of data points , and G and G′ are two connected graphs built on S by the ǫ ball method with ǫ and ǫ′ , and the RBF weighting function wij = exp{− kxi−xj k2 } . For each edge eij , let its cost πij = 1 . If T and T ′ are minimum spanning wij trees of G and G′ respectively , then E(T ) = E(T ′ ) .
σ2
Proof : To prove this proposition , we first recall Kruskal ’s minimum spanning tree algorithm which proceeds as follow . It first sorts all the edges into an increasing order by their costs ; then the next lightest edge producing no cycle is repeatedly added .
Without loss of generality , we assume ǫ < ǫ′ . Then , we have : ( 1 ) E(G ) ⊆ E(G′ ) ; ( 2 ) for ∀e ∈ E(G ) and ∀e′ ∈ E(G′ ) \ E(G ) , πe < πe′ . Thus , edges which are in E(G′ ) but not in E(G ) are behind the edges in E(G ) in the sorted edge sequence of G′ . Therefore , the procedures of Kruskal ’s algorithm for G and G′ are the same which implies E(T ) = E(T ′ ) .
When the graph is constructed by kNN , we observe the same property in experiments , although we cannot formally prove it . This property makes MST very robust to the graph hyperparameter , and in turn makes the performance of our method very robust to the graph hyperparameter . Experimental results in Section V will further validate this desirable advantage .
E . Relationship with Cesa Bianchi et al . [ 5 ] ’s Method
The idea of minimizing the cut size of a partially labeled tree was first used by Cesa Bianchi et al . [ 5 ] in the online graph prediction setting . Roughly speaking , to predict an unlabeled node i in some lb tree , their method proceeds by performing a post order traversal of the lb tree from node i . When backtracking to node j after all the children of j have been visited , j is assigned a temporary label given by the majority vote among the temporary or labels of its children . This method can merely guarantee the node i ’s predicted label minimizes the cut size of T , while the other temporary labels of unlabeled data do not necessarily minimize the cut size . Therefore , to correctly predict each unlabeled node , a traversal of the lb tree from each unlabeled node should be made ; this leads to the running time as O(n2 ) .
Instead of propagating the temporary labels from bottom to top , our method propagates the minimum cut size which enables us to label the whole lb tree effectively in just two traversals . advantage of RST is that for many graphs , the RST tree can be generated in O(n ) time .
In addition , our method has the important advantage of being able to handle weighted graphs with more than two classes , while Cesa Bianchi et al . [ 5 ] ’s method can only handle un weighted graphs with two classes .
IV . TRANSDUCTIVE CLASSIFICATION WITH MTC
To apply the MTC algorithm to the general transductive classification problem , one has to construct a graph G from the data set , and then generates a spanning tree T from G followed by running MTC on T . In case that the graph is not connected , we can run the MTC algorithm on each connected component respectively . As suggested by [ 12 ] , to further boost the performance , one can use ensembles of trees . Specifically , one can generate many spanning trees , run MTC on each tree , then use the majority vote method to predict unlabeled nodes . In the following , we will discuss different methods for graph construction and spanning tree generation .
A . Tree Construction Methods
Minimum Spanning Tree ( MST ) . MST is a spanning tree that minimizes the total cost . In our setting , the cost of an edge ( i , j ) is defined as πij = 1 , thus , MST of G is wij the spanning tree that solves the problem min{ X(i,j)∈E(T )
πij : T ∈ T ( G)} ,
( 4 ) where T ( G ) is the set of spanning trees of graph G . According to [ 12 ] , this problem is equivalent to max{ X(i,j)∈E(T ) wij : T ∈ T ( G)} .
( 5 )
The above objective function is very intuitive , and formally it says MST best approximates the original graph in term of the trace norm of graph Laplacian . MST can be generated by Kruskal ’s algorithm in O(|E| log n ) time . For sparse graphs , like kNN graphs , it can be sped up to O(n log n ) .
Shortest Path Tree ( SPT ) . SPT is a spanning tree that the distance between a selected node and all other nodes is minimal . Herbster et al . [ 12 ] used this tree to approximately solve the problem of , min{tr(T + − G+ ) : T ∈ T ( G)} ,
( 6 ) where tr(A ) denotes the trace of matrix A , and A+ denotes the Moore Penrose pseudoinverse of A . SPT can be generated by Dijkstra ’s algorithm . With a binary heap , its running time is O((|E| + n ) log n ) . For sparse graphs , it requires O(n log n ) time .
Random Spanning Tree ( RST ) . RST is a spanning tree taken with the probability proportional to the product of its edge weights . Cesa Bianchi et al . [ 6 ] proposed to use this tree to ensure a worst case bound of their algorithm . Another
B . Graph Construction Methods
Many researchers have shown practically and theoretically that the graph plays a crucial role for the success of graphbased transductive methods [ 21 , 17 ] . The most popular graph construction methods include kNN graph and ǫ graph . However , a direct implementation of both types of graphs need O(dn2 ) time where d is the data dimension . There are lots of work for efficient graph construction . For example , Chen et al . [ 8 ] computed an approximating kNN graph via the divide and conquer method . Plaku and Kavraki [ 19 ] developed parallel algorithms to construct graph . Other techniques to speed up the graph construction include the KD tree [ 2 ] and locality sensitive hashing [ 13 ] . However , compared to the complexity of spanning tree generation and the MTC algorithm , graph construction still dominates the time of graph based transductive classification . Note that many methods have been recently proposed to construct graph to better describe the data relationship [ 14 , 9 , 22 ] . However , the high computational cost of these methods prevents them from large scale applications .
V . EXPERIMENTS
In this section , we evaluate our method by performing extensive experiments on real world data sets . We compare our method with two tree based transductive methods : the graph perceptron algorithm ( GPA ) [ 12 ] and the weighted tree algorithm ( WTA ) [ 6 ] , and one classical full graph based method , Gaussian Random Fields ( GRF ) [ 25 ] . We also use 1NN and SVM as baseline methods .
For all graph based methods , following many works in the literature , we exploit the kNN method to construct graph and the RBF function wij = exp{−kxi − xj k2/σ2 0} to weight the edges , where σ2 For tree based methods , we fix k to 100 , because they are insensitive to this parameter ( as validated theoretically in Section III D and empirically in the next subsection ) . For GRF , we manually choose k such that the test error is minimized .
0 is set to P(i,j)∈E kxi − xj k2/|E| .
As reported by [ 12 ] and [ 6 ] , the performance of MST is always the best among different tree construction methods . We also observed similar results in our experiments . Hence , we only report the classification results of different methods on MST .
A . Small Data Sets
In this subsection , to show the effectiveness of the proposed MTC algorithm , we run experiments on two small size data sets1 : ( 1 ) COIL20 data set . This is an image data set which contains 20 objects . The images of each object were taken 5 degrees apart and each object has 72 images . The
1http://wwwzjucadcgcn/dengcai/Data/MLDatahtml size of each image is 32×32 pixels , with 256 grey levels per pixel ; ( 2 ) USPS data set . It contains 7 , 291 + 2 , 007 = 9 , 298 handwritten digit images of size 16 × 16 . Note that , for small data sets , all the graph based methods can perform the learning very efficiently . We hence focus on examining the classification accuracy of different methods here and leave the efficiency evaluations on two large scale data sets in Section V B .
1 ) Classification Accuracy : For each data set , we randomly select l data points as labeled data such that they contain at least one sample from each class , and leave all the other data as unlabeled . Then we perform various learning methods on this partially labeled data set , and record the classification accuracy on the unlabeled data . To control the variance of results , we repeat the procedure 10 times for different training/testing splits , and report the average classification accuracy in Figure 2 .
As observed from the results , the following conclusions can be made . ( 1 ) Among all methods of approximating graph by tree , our proposed MTC method achieves the best classification accuracy in all cases . ( 2 ) MTC gives a very good approximation to the full graph based method GRF , and is even better than GRF on the COIL20 data set . This can be explained by the fact that real world graphs are usually sparse . ( 3 ) Transductive methods overwhelm the supervised methods ( ie , the RBF SVM and 1NN ) when the number of labeled data is limited , while this effect becomes less obvious as the number increases .
2 ) Robustness to Graph Construction : One major problem of graph based transductive learning is that their performance is very sensitive to the graph construction . For the kNN graph or ǫ graph , a small perturbation in k or ǫ and σ2 will result in big changes in accuracy . In this section , we validate the theory that using the minimum spanning tree to approximate a graph can solve this problem effectively .
In this experiment , we fix l = 200 for COIL20 and l = 400 for USPS and construct kNN graphs or ǫ graphs for different k or ǫ and σ2 . We then perform GRF and MTC on the resulting graphs . Specifically , for kNN graphs , we vary k in the set {21 , 22 , 23 , 24 , 25 , 26} with σ2 = σ2 0 , and vary σ2 in the set σ2 0 ∗ {2−3 , 2−2 , 2−1 , 20 , 21 , 22 , 23} with k = 4 . Similarly , for ǫ graphs , we draw the accuracy curves respectively by varying ǫ with a fixed σ . The curves of varying σ with a fixed ǫ demonstrated very similar trend with those of kNN graphs and we hence do not draw them for succinctness .
Figure 3 shows the average classification accuracy over 10 random splits . As we can see , the performance of MTC is almost invariant to the variance of k , σ2 , ǫ . In fact , this effect is due to the robustness of MST with respect to variance of k , ǫ , and σ2 , which we have analyzed in Section III D . This property helps MST to filter out noisy edges , which however may severely hurt the performance of GRF .
B . Large Data Sets
We conduct a series of experiments on two large scale data sets in this section . We will first report results on the MNIST data2 and then a Web spam Data Set3 . All methods are performed on an HP server with a 2.27 GHz Xeon 4 Core CPU , and 16 GB RAM .
1 ) MNIST Data Set : The MNIST data set contains 60 , 000 + 10 , 000 = 70 , 000 handwritten digit images . The size of each image is 28 × 28 pixels , with 256 grey levels per pixel . Our experiment proceeds as follows . First , we randomly select n data points from 70 , 000 images , and construct graph on these data . Then 400 nodes are randomly selected as labeled data , and different algorithms are performed to predict unlabeled nodes . To observe the effect of unlabeled data , we vary n from 10 , 000 to 70 , 000 , and report the average classification accuracy over 10 times training/testing splits and running time in Figure 4 . The reported time includes time for generating MST and labeling the spanning tree . The time for graph construction is not included .
Several observations are highlighted as follows . ( 1 ) Similar to the results in COIL and USPS , MTC gives a good approximation to GRF , and achieves the best accuracy among all the tree based methods . ( 2 ) Unlabeled data indeed help the classification . In general , as the number of unlabeled data increases , the accuracy increases correspondingly . ( 3 ) While the performance of MTC and WTA improves consistently , the accuracy of GPA decreases when the size of unlabeled data is very huge . This is due to the effect of overfitting . For GPA , we are training a linear classifier in a feature space of 70 , 000 dimensions with 400 training samples . This also leads to the same problem . In comparison , the tree based methods MTC and WTA exploit a much simpler structure and hence demonstrate very robust performance . ( 4 ) From Figure 4(b ) , our proposed linear time MTC shows very good scalability as the size of training samples increases . The WTA algorithm is also very fast due to its linear complexity . However , as WTA basically adopts a too simple tree , ie , a path graph , to approximate the graph . This makes its performance usually worse than our proposed MTC method . 2 ) Web spam Data Set : We also apply our method to the 2007 web spam challenge developed by the University of Paris VI . It contains 400 , 000 web pages . A web page is connected to another web page if there is at least one hyperlink from the former to the latter . Thus , the links are directed . Following [ 12 ] , we discard directional information and assign a weight of 1 to unidirectional links and of 2 to the bidirectional links . This results in a graph with 400 , 000 nodes and 10 , 455 , 545 edges . Additional tf idf feature vectors of the web pages’ content are provided for each web page , but we have discarded this information .
2http://yannlecuncom/exdb/mnist/ 3http://webspamlip6fr/wiki/pmwikiphp
1
0.95
0.9
0.85
0.8
0.75
0.7
0.65 y c a r u c c a n o i t a c i f i s s a l c
GPA WTA MTC GRF RBF−SVM 1NN
20
40
80
160
320
640
1280
# of labeled data
( a )
1
0.95
0.9
0.85
0.8
0.75 y c a r u c c a n o i t a c i f i s s a l c
GPA WTA MTC GRF RBF−SVM 1NN
0.7
50
100
200
400
800
1600
3200
# of labeled data
( b )
Figure 2 . Average classification accuracy for different labeled data size : ( a ) COIL20 data set ; ( b ) USPS data set .
There are about 80 % of non spam web pages and 20 % of spam ones . Following the published data set , we use 10 % of labeled web page for training and 90 % for testing .
Experimental results are shown in Table I . Due to the same experimental setup , the results except MTC and WTA are directly copied from [ 12 ] . For ensemble GPA , 21 minimum spanning trees are used , and predictions are made by majority voting . Witshel et al . , Filoce et al . , Benczur et al . are three methods that participated the 2007 web spam challenge . We did not report the result of GRF as it is intractable to run GRF on this large data set . GPA takes about 30 minutes to train a single kernel perceptron . In comparison , our proposed MTC takes less than 10 seconds to complete the classification . Similarly , WTA is also very fast and finishes the learning also within 10 seconds . However , the classification accuracy of WTA is 99.0 , which is significantly lower than that of MTC . This once again demonstrates the advantages of our proposed method . The reported times include the time of generating MST and labeling the tree , but do not include the time of loading graph from disk .
GRF MTC
4
8
16
32
64
# of nearest neighbors
( a )
8
GRF MTC
2^−2
2^−1
2
4
1 2
σ
( c ) y c a r u c c a n o i t a c i f i s s a l c y c a r u c c a n o i t a c i f i s s a l c y c a r u c c a n o i t a c i f i s s a l c y c a r u c c a n o i t a c i f i s s a l c
1
0.95
0.9
0.85
0.8
0.75
0.7
0.65
2
1
0.99
0.98
0.97
0.96 y c a r u c c a n o i t a c i f i s s a l c
0.95
2^−3
1
0.975
0.95
0.925
0.9
0.875 y c a r u c c a n o i t a c i f i s s a l c
GRF MTC
2
0.85
1
5
6
7
3
4 ε
( e )
0.97
0.96
0.95
0.94
0.93
0.92
0.91
0.9
0.89
0.88
2
0.98
0.97
0.96
0.95
0.94
0.93
0.92
0.91
0.9
2^−3
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6
0.55
0.75
GRF MTC
4
8
16
32
64
# of nearest neighbors
( b )
8
GRF MTC
2^−2
2^−1
2
4
1 2
σ
( d )
GRF MTC
1
1.25
1.5
1.75
2
2.25
2.5
ε
( f )
Figure 3 . Robustness of GRF and MTC to kNN graphs and ǫ graph : ( a),(b ) average classification accuracy for different k on COIL and USPS ; ( c),(d ) average classification accuracy for different σ2 on COIL and USPS ; ( e),(f ) average classification accuracy for different ǫ on COIL and USPS .
0.94
0.93
0.92
0.91
0.9
0.89
0.88 y c a r u c c a n o i t a c i f i s s a l c
0.87
10000
100
90
80
70
60
50
40
30
20
10
) s d n o c e s ( e m i t g n i n n u r
0
10000
GPA WTA MTC GRF RBF−SVM
20000
30000
40000
50000
60000
70000
# of data points
( a )
GPA WTA MTC GRF
20000
30000
40000
50000
60000
70000
# of data points
( b )
Figure 4 . Experimental results on MNIST for different data size : ( a ) average classification accuracy ; ( b ) running time .
VI . CONCLUSION
In this paper , we proposed a fast graph based transductive classification method . Inspired by the sparsity of the real world graphs , we first exploited a spanning tree to approximate the graph . Based on minimization of the cut size , we then developed a highly robust and efficient learning algorithm to label the tree . Our algorithm has a linear complexity in terms of both time and space . This is significantly distinctive with typical graph based methods , which either have a cubic time complexity ( for a dense graph ) or a quadratic complexity ( for a sparse graph ) . We evaluated our proposed method on four real world data sets ( including two large scale sets ) . Experimental results showed that our proposed method can usually be superior to other competitive approaches in terms of both classification accuracy and learning efficiency . Future work includes the parallel application of multiple MTC ’s for different lb trees .
ACKNOWLEDGMENT
The work was supported by the National Natural Science Foundation of China ( NSFC ) under grants No . 60825301 and No . 61075052 . The authors want to thank Sergio Ro
CLASSIFICATION ACCURACY ON WEB SPAM DATA SET .
Table I
MTC WTA single GPA ensemble GPA Witshel et al . Filoce et al . Benczur et al . 99.6
99.1
99.5
99.0
97.6
99.4
94.2 jas Galeano , Fabio Vitale and Giovanni Zappella for their helpful discussion .
REFERENCES
[ 1 ] M . Belkin , P . Niyogi , and V . Sindhwani . Manifold regularization : a geometric framework for learning from labeled and unlabeled examples . Journal of Machine Learning Research , 7:2399–2434 , 2006 .
[ 2 ] JL Bentley . Multidimensional divide and conquer .
Communications of the ACM , 23(4):214–229 , 1980 .
[ 3 ] A . Blum and S . Chawla . Learning from labeled and unlabeled data using graph mincuts . In Proceedings of the International Conference on Machine Learning , 2001 .
[ 4 ] A . Blum , J . Lafferty , MR Rwebangira , and R . Reddy . Semi supervised learning using randomized mincuts . In Proceedings of the International Conference on Machine Learning , 2004 .
[ 5 ] N . Cesa Bianchi , C . Gentile , and F . Vitale . Fast and optimal prediction of a labeled tree . In The 22th Annual Conference on Learning Theory , 2009 .
[ 6 ] N . Cesa Bianchi , C . Gentile , IF Vitale , and G . Zappella . Random spanning trees and the prediction of weighted graphs . In Proceedings of the International Conference on Machine Learning , 2010 .
[ 7 ] O . Chapelle , B . Sch¨olkopf , and A . Zien .
Semi
Supervised Learning . MIT Press , 2006 .
[ 8 ] J . Chen , H . Fang , and Y . Saad . Fast approximate kNN graph construction for high dimensional data via recursive Lanczos bisection . The Journal of Machine Learning Research , 10:1989–2012 , 2009 .
[ 9 ] SI Daitch , JA Kelner , and DA Spielman . Fitting a In Proceedings of the Interna graph to vector data . tional Conference on Machine Learning , 2009 .
[ 10 ] O . Delalleau , Y . Bengio , and N . Le Roux . Efficient non parametric function induction in semi supervised learning . In Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics , 2005 . [ 11 ] M . Herbster and M . Lever , G . andPontil . Online In Advances in prediction on large diameter graph . Neural Information Processing Systems , 2008 . [ 12 ] M . Herbster , M . Pontil , and S . R . Galeano .
Fast predciton on a tree . In Advances in Neural Information Processing Systems , 2008 .
[ 13 ] P . Indyk and R . Motwani . Approximate nearest neighbors : towards removing the curse of dimensionality . In
Proceedings of the thirtieth annual ACM symposium on Theory of computing , pages 604–613 . ACM , 1998 . [ 14 ] T . Jebara , J . Wang , and SF Chang . Graph construction and b matching for semi supervised learning . In Proceedings of the International Conference on Machine Learning , 2009 .
[ 15 ] J . Kleinberg and E . Tardos . Algorithm Design . Pearson
Education , 2006 .
[ 16 ] W . Liu , JF He , and SF Chang . Large graph construction for scalable semi supervised learning . In Proceedings of the International Conference on Machine Learning , 2010 .
[ 17 ] M . Maier , U . Von Luxburg , and M . Hein . Influence of graph construction on graph based clustering measures . In Advances in Neural Information Processing Systems , 2009 .
[ 18 ] GS Mann and A . McCallum . Simple , robust , scalable semi supervised learning via expectation regularization . In Proceedings of the International Conference on Machine Learning , 2007 .
[ 19 ] E . Plaku and LE Kavraki . Distributed computation of the knn graph for large high dimensional point sets . Journal of Parallel and Distributed Computing , 67(3):346–359 , 2007 .
[ 20 ] DA Spielman and SH Teng . Nearly linear time algorithms for preconditioning and solving symmetric , diagonally dominant linear systems . Arxiv preprint cs/0607105 , 2006 .
[ 21 ] F . Wang and C . Zhang . Label propagation through linear neighborhoods . IEEE Transactions on Knowledge and Data Engineering , 20(1):55–67 , 2008 .
[ 22 ] S . Yan and H . Wang . Semi supervised learning by sparse representation . In SIAM International Conference on Data Mining , SDM , pages 792–801 , 2009 .
[ 23 ] K . Zhang , JT Kwok , and B . Parvin . Prototype vector machine for large scale semi supervised learning . In Proceedings of the International Conference on Machine Learning , 2009 .
[ 24 ] D . Zhou , O . Bousquet , TN Lal , J . Weston , and B . Scholkopf . Learning with local and global consistency . In Advances in Neural Information Processing Systems 16 , 2004 .
[ 25 ] X . Zhu , Z . Ghahramani , and J . Lafferty .
Semisupervised learning using Gaussian fields and harmonic functions . In Proceedings of the International Conference on Machine Learning , 2003 .
