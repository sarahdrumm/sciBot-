Analysis of Textual Variation by Latent Tree Structures
Teemu Roos and Yuan Zou
Helsinki Institute for Information Technology , HIIT
PO Box 68 , FI 00014 University of Helsinki , Finland
Email : firstnamelastname@cshelsinkifi
Abstract—We introduce Semstem , a new method for the reconstruction of so called stemmatic trees , ie , trees encoding the copying relationships among a set of textual variants . Our method is based on a structural expectation maximization ( structural EM ) algorithm . It is the first computer based method able to estimate general latent tree structures , unlike earlier methods that are usually restricted to bifurcating trees where all the extant texts are placed in the leaf nodes . We present experiments on two well known benchmark data sets , showing that the new method outperforms current stateof the art both in terms of a numerical score as well as interpretability .
Keywords graphical models , stemmatology , textual criticism . latent trees , EM algorithm ,
I . INTRODUCTION
In the popular game known as the broken telephone ( or Chinese whispers , Le t´el´ephone arabe , etc. ) , a message is successively whispered by one player to another until it reaches all the players . The message typically gets distorted along the way , which makes the game amusing . The accumulation of small changes characteristic to the game is also one of the defining features of evolution . A phenomenon that is perhaps lesser known , but even more fitting as an analogue of the broken telephone process , is encountered in textual criticism where texts distorted by transcriptional errors and other changes are reconstructed by identifying such changes and removing them , see [ 5 ] .
The traditional goal of textual criticism is the reconstruction of the original , or at least the most recent common ancestor of the extant manuscripts.Often the reconstruction is preceded by stemmatological analysis , i.e , building a diagram known as a stemma , encoding the usually tree like copying relationships of manuscripts . In biological terms , this corresponds to phylogenetic analysis wherein different species are organized in a so called Tree of Life.1
The adoption of computational methods in textual criticism , and in humanities at large , is still in its infancy . The current applications mainly involve digitized sources , databases , multimedia , and other relatively “ mundane ” tools.2 In contrast , the methodology of the biological sciences has been utterly transformed by mathematical and
1See , for instance , the TREE OF LIFE web project at tolweborg 2We do not intend to play down the importance of such tools but to emphasize that their role is somewhat peripheral in the actual scholarly work , compared to their role in the natural sciences . indeed so much that computational methods ; is now regarded as a new discipline , bioinformatics . It remains to be seen to which extent a similar transformation will take place in the emerging field of digital humanities . it
Since the processes of textual variation resemble those of biological evolution , it is natural to attempt stemmatological analysis by phylogenetic methods . Indeed this has turned out to be a very successful approach , see eg [ 16 ] , [ 18 ] , [ 21 ] . A plethora of phylogenetic methods are available ; for general overviews , see eg [ 3 ] , [ 8 ] , [ 20 ] . The methods can be roughly categorized as distance matrix based methods ( eg [ 19] ) , parsimony methods ( eg [ 7] ) , and model based methods . The latter group , which is of our primary interest , includes methods based on maximum likelihood [ 25 ] and Bayesian inference [ 17 ] , [ 27 ] .
However , certain assumptions that are often valid in the biological domain , are problematic when phylogenetic methods are applied to manuscripts . Two such assumptions are : ( i ) all extant ( observed ) nodes are always placed in the leaf nodes of the tree , and ( ii ) , all trees are bifurcating , ie , all interior nodes have degree three ( one parent , two children ) . Neither assumption is valid in stemmatology : it is not the case that none of the extant manuscripts are ancestors of some other extant manuscripts , and furthermore , it is not true that the number of copies made of each manuscript is either two or nil .
Among phylogenetic methods , a method proposed by Friedman et al . [ 11 ] , called SEMPHY , is particularly relevant . It is based on the structural EM algorithm , proposed earlier by Friedman [ 10 ] . The method constructs phylogenetic trees that are essentially tree structured Bayesian networks with N observed nodes and N −2 latent ( unobserved ) nodes—precisely the kind of bifurcating tree structures that are ubiquitous in phylogenetics . The algorithm is based on alternating between a message passing phase where the distribution of the latent nodes is inferred based on the current structure ( the E step ) , and building a new tree structure based on the observed nodes and the inferred distribution of the latent nodes [ 4 ] ( the M step ) .
In order to guarantee that the resulting tree is bifurcating and that all the observed nodes are leafs , SEMPHY includes an additional step where the tree obtained in the M step , which may violate these restrictions , is converted into an equivalent tree with the desired properties . However , from the stemmatological point of view , there is nothing wrong with multifurcating trees with observed interior nodes . On the contrary , stemmatology proves to be an ideal application for a structural EM approach that , unless specific manipulations are carried out , produces general latent tree structures that are free from the two restrictions mentioned above .
We adapt the structural EM algorithm for stemmatology by applying a model for textual variation , and omitting the aforementioned transformation step . The resulting method is , to our knowledge , the first automatic method for discovering unrestricted tree shaped structures from textual variants . We demonstrate that our adapted algorithm is able to reconstruct the copying relationships of several manuscripts created by copying texts by hand . The resulting stemmata are more accurate and easier to interpret than traditional trees based on phylogenetic methods . Further applications may include the analysis of computer viruses [ 26 ] , plagiarism detection [ 12 ] , and content based social network analysis [ 23 ] .
The rest of the paper is organized as follows : In Sec II we review the model based approach to phylogenetic and stemmatic analysis . In Sec III , we describe the structural EM algorithm in detail . In Secs . IV–V , we describe the data and the experimental set up , and the results . Conclusions and pointers for future work are outlined in Sec VI .
II . MODELING TEXTUAL EVOLUTION
Model based phylogenetic analysis is preceded by a model specification wherein we construct a probabilistic model describing the evolution of the biological units ( individuals or species ) under study . The data on which the analysis is based typically consists of genomic sequences . It is usually assumed that the sites ( positions ) in the sequences evolve independently , although this assumption is not strictly speaking biologically valid.3 Many of the popular evolutionary models can be represented as continuous time Markov chains ( CTMCs ) [ 13 ] . Such models are characterized by a parametric transition probability matrix , which in the case of DNA sequences can be expressed in the form
P ( t ) =
 , pG→A(t ) pC→A(t ) pA→A(t ) pT →A(t ) pA→G(t ) pG→G(t ) pC→G(t ) pT →G(t ) pA→C(t ) pG→C(t ) pC→C(t ) pT →C(t ) pA→T ( t ) pT →T ( t ) pG→T ( t ) pC→T ( t ) with the interpretation that px→y(t ) is the probability that a site in state x evolves into state y in time t . There are various ways to define the transition probabilities,4 and similar models exist for protein sequences .
3Models based on more realistic evolutionary assumptions have also been proposed , see eg [ 2 ] , but despite recent advances , their application is still prohibitively inefficient .
4Here we use the term transition probability to refer to all transitions . The established convention in bioinformatics is to call the probabilities in the top left and bottom right 2 × 2 submatrices transition probabilities , and the remaining ones translation probabilities .
The process is in an equilibrium when the state composition of each site is given by the stationary distribution ( pA , pG , pC , pT ) for which we have px =Xy∈Σ pypy→x(t ) , for all x ∈ Σ = {A , G , C , T } and all t ≥ 0 . Furthermore , the process is said to be time reversible if pxpx→y(t ) = pypy→x(t ) ,
( 1 ) for all ( x , y ) ∈ Σ2 , t ≥ 0 .
The models for textual evolution are much less established as those for genomic evolution . The evolution of words can be modeled similarly , although there the above assumptions are even less realistic . Nonetheless , the approach has been shown to be fruitful ( proving once again the fact about some models being “ wrong ” but “ useful ” ) . In stemmatology , the time variable , t , does not have a similar role as in biological evolution . Namely , the existing manuscripts may remain in the “ stemmatic pool ” ( akin to the so called genetic pool ) and can be used as sources for copying after an arbitrarily long time , and there is in principle no reason to assume that a copy made of an older source manuscript should contain more errors than a copy made of a more recent manuscript . Another major difference in modeling text compared to genomic sequences is that the alphabet is not fixed , although in practice , it seems safe to restrict the readings in each site r to the set of readings observed in at least one of the extant manuscripts , Σ(r ) .
For simplicity , we let the diagonal elements of transition matrix for site r to be the same , which is 1 − α . Thus any other element is α/(kr − 1 ) with 0 < α < 1 , and kr denotes the number of observed unique readings in site r . Hence , each word has the same probability , 1 − α , of staying unchanged when it is copied , and the probability of the word being changed to another is uniform . We have also experimented with models where the transition probabilities the uniform model appears reflect word similarities but to be more robust its simplicity . The probability of change can also be estimated together with the tree structure . However , for simplicity , we assume in this work that 1 − α = 095 In our experiments , the results obtained by estimating 1−α or using other constants within the range [ 0.8 , 1.0 ) results in qualitatively similar results . in all
The corresponding stationary distribution is easily seen x = 1/kr for all x ∈ Σ(r ) . This also to be uniform , ie , p(r ) implies that the model is time reversible , ie , p(r ) x p(r ) x→y = 1/kr · α/(kr − 1 ) = p(r ) y p(r ) y→x ,
( 2 ) for all ( x , y ) ∈ Σ(r)2 symmetric .
, x 6= y ; the case x = y is trivially
III . STRUCTURAL EM
We re write Eq ( 3 ) as
The EM algorithm [ 6 ] is an extremely popular technique for dealing with missing data . Its main use is parameter estimation . However , it can also be used for learning the structure of a Bayesian network , as demonstrated by the structural EM algorithm [ 10 ] . Unlike most structure learning methods , it is applicable when some of the data are missing or when some of the variables are completely unobserved . The expectation ( E ) step in the algorithm performs inference on the missing data to obtain suitable statistics , that can be used in the maximization ( M ) step to construct a model structure . The new structure is then used for obtaining another ( better ) set of statistics in the next iteration . A phylogenetic method based on the structural EM algorithm , called SEMPHY , has also been presented [ 11 ] , where the unobserved ancestral sequences are represented as latent variables , and the learned structure is constrained to be a tree .
In this section , we adapt the phylogenetic structural EM method for stemmatology . We start by discussing the relatively straightforward complete data case . We then resort to the structural EM approach for dealing with latent variables and missing data . For the most part , we follow [ 11 ] .
A . Probability of Stemmatic Trees
Let a stemma , T , be defined as a set of edges , ( i , j ) ∈ {1 , . . . , N +M }2 , where N +M is the number of nodes . We denote the nodes by X1 , . . . , XN +M . Nodes X1 , . . . , XN are assumed to be observed . The remaining ones are latent nodes that correspond to undiscovered manuscripts . For the sake of clarity , in the following we do not consider partially observed manuscripts , although they can be handled in a straightforward way using exactly the same structural EM approach . The algorithm we have implemented handles them , and the experimental results in Sec V address both kinds of missing data .
In the ideal case , when M = 0 , ie , we have the complete set of the manuscripts , the probability of the data given a stemma T is easily computed as
PT ( X1 , . . . , XN ) = nYr=1"P ( X ( r )
1 )
NYi=2
P ( X ( r ) i
Πi )# ,
|X ( r )
( 3 ) where the number of sites ( words ) is n,5 the parent of node i > 1 is denoted by Πi , and we assume without loss of generality that the root node is X1 .
5In order to get the data into the format where each manuscript has the same number of sites , and the same site in different manuscripts corresponds to the readings of the same word ( if it exists in the given manuscript ) requires that the texts be aligned . There are various methods that are commonly used in bioinformatics . We apply similar methods but do not discuss the details due to space restrictions . j
)
)  .
( 4 )
Yi
PT ( X1 , . . . , XN ) = nYr=1
P ( X ( r ) i
) Y(i,j)∈T
P ( X ( r ) i
|X ( r )
P ( X ( r ) i
Due to the fact that the model is time reversible , we have
P ( X ( r ) i
|X ( r ) j
)
P ( X ( r ) i
)
=
P ( X ( r ) j
|X ( r ) i
)
P ( X ( r ) j
)
,
( 5 ) which implies that the formula in Eq ( 4 ) is invariant under changing the root variable and reordering all edges to point away from it . Consequently , unless we have prior information about the ordering of the nodes ( in the form of , eg , timings of the manuscripts ) , different stemmata with the same undirected structure , or skeleton , have the same posterior probability.6
We consider the logarithm of the likelihood , LT , and decompose it into a more liable form as follows nYr=1 ) + X(i,j)∈T
LT ( X1 , . . . , XN ) = log
PT ( X1 , . . . , XN +M )
= nXr=1
 NXi=1 log P ( X ( r ) i log
P ( X ( r ) i
|X ( r )
P ( X ( r ) i j
)
)  .
( 6 ) The first sum inside the brackets is a constant independent of the stemma , and can therefore be ignored . The latter sum can be written as
X(i,j)∈T X(x,y)∈Σ(r)2
1{X ( r ) i = x , X ( r ) j = y} log px→y py
, i = x , X ( r )
( 7 ) where 1{X ( r ) j = y} is the indicator function that takes value one if the argument is true , and zero otherwise . Since the log likelihood decomposes as a sum of terms for different edges in the stemma , we can actually maximize the likelihood by casting the problem as a maximum spanning tree problem . The weights , wi,j , of each pair of nodes are wi,j = nXr=1 X(x,y)∈Σ(r)2
1{X ( r ) i = x , X ( r ) j = y} log px→y py
,
( 8 ) which are symmetric , wi,j = wj,i , by the time reversibility property Eq ( 2 ) . For instance , Kruskal ’s algorithm finds the maximum spanning tree in time O(N log N ) . The above procedure amounts to the popular Chow Liu algorithm [ 4 ] .
B . Expected Log Likelihood
The above complete data case needs to be extended to handle missing data when some of manuscripts are expected
6In phylogenetics , this problem is often solved by adding a so called outgroup species in the data that is known to be outside the group of species under study . In the case of texts , no such outgroup really exists . to be lost , namely M > 0 . Obviously , the actual number of missing manuscripts is very hard , or impossible , to know in advance . Hence , the used number will have to be an educated guess , at best . We follow the convention , originating from phylogenetics , of using M = N − 2 latent nodes . As it turns out , superfluous latent nodes tend to end up as extra leaf nodes or as sequences of degree two nodes , both of which can be pruned out without changing the tree topology in any meaningful way .
Consider the conditional distribution of the latent nodes , XN +1 , . . . , XN +M , given the observed nodes , X1 , . . . , XN , and a fixed tree structure , Tt . We let Q(T : Tt ) denote the expected log likelihood of an arbitrary tree structure :
Q(T : Tt ) = E[LT ( X1 , . . . , XN +M ) | X1 , . . . , XN , Tt ] .
( 9 ) As noted above , the first term inside the brackets in the loglikelihood , Eq ( 6 ) , can be omitted as a constant independent of the tree topology . Obviously , the expectation of a constant is a constant as well , and we are left with nXr=1 X(i,j)∈T
( x,y)∈Σ(r)2
E1{X ( r ) i = x , X ( r ) j = y} log px→y py
( 10 ) where Zt = ( X1 , . . . , Xn , Tt ) , which is easily seen to be equal to fifififi Zt ,
To warm start the structural EM , we initialize the tree by the neighbor joining method [ 19 ] . The algorithm is run until the expected log likelihood converges or a maximum number of iterations is reached . In the end , the tree with the highest expected log likelihood is returned . Pseudo code for the procedure , which we call Semstem , is given in Algorithm 1 .
Algorithm 1 : Semstem begin initialize T0 using NJ method ; let Tmax = T0 ; let Qmax = Q(T0 : T0 ) ; let t = 0 ; repeat
E step : compute the weights wi,j for all pairs of nodes , i , j under tree Tt ; M step : find a new tree Tt+1 by the MST algorithm ; if Q(Tt+1 : Tt ) > Qmax then let Tmax = Tt+1 ; let Qmax = Q(Tt+1 : Tt ) ; let t = t + 1 ; until Tt+1 = Tt or t > tmax ; return Tmax ; end nXr=1 X(i,j)∈T
( x,y)∈Σ(r)2
η(x , y ) log px→y py
, where
η(x , y ) = P ( X ( r ) i = x , X ( r ) j = y | X1 , . . . , XN , Tt ) ( 12 ) denotes the conditional expectation of the indicator function in Eq ( 10 ) .
Analogous to the complete data case of the previous subsection , we now define the weight of a potential edge between nodes i and j as wi,j = nXr=1 X(x,y)∈Σ(r)2
η(x , y ) log px→y py
,
( 13 ) is important to note that η(x , y ) in Eq ( 12 ) where it depends on the structure Tt as well as the observed nodes X1 , . . . , XN . Since it is actually a pairwise conditional probability of two nodes taking the values x and y , respectively , it can be evaluated using standard inference algorithms . Furthermore , since the network topology is assumed to be a tree , the classical message passing ( belief propagation ) algorithm is exact [ 15 ] . The weights of all pairs of nodes can be computed in time O(nN 2|Σmax|2 ) , where |Σmax| denotes the greatest number of variant readings in any given site . We omit further details ; see [ 11 ] .
( 11 )
C . Local Optima
As the usual ( parametric ) EM algorithm , structural EM is a greedy method where the expected log likelihood is never decreased . However , EM tends to get stuck to local optima . To alleviate this problem , Friedman et al . [ 11 ] propose to apply a technique similar to simulated annealing . The idea is to add stochastic perturbations to the weights . The magnitude of the perturbations is gradually decreased by adjusting a ‘temperature’ parameter σt → 0 as t grows .
We add Gaussian noise with variance σ2 t to the elements of the weight matrix :
( 14 ) ewi,j(ti,j ) = wi,j(ti,j ) + ǫi,j ,
To maintain the symmetry of the weight matrix , we let ǫi,j = ǫj,i . The temperature is decreased according to a geometric cooling schedule where σt+1 = ρσt with 0 < ρ < 1 . In the final stage , σ is set to zero to allow the algorithm to converge to a local optimum . In practice , this happens within a couple of dozen iterations at most .
IV . DATA AND EXPERIMENTAL SET UP
To illustrate the method , and to compare its performance against a set of state of the art algorithms applied in stemmatology , we use two artificially generated textual traditions . One could also generate data from a model that produces random copying errors but is generally believed that it manually created data sets are much closer to real world textual traditions . The first data set , Parzival ( see [ 21] ) , contains 21 manuscripts of length 1055 words including gaps created by multiple alignment . The second data set , Notre Besoin de Consolation est Impossible `a Rassarier ( ‘Notre Besoin’ for short ; see [ 1 ] ) contains 14 manuscripts of length 1035 words . The Notre Besoin tradition includes an instance of contamination , ie , a node that has more than one parent . Such cases arise when two or ( rarely ) more manuscripts are consulted when creating a new copy .
We evaluate the methods based on their success of finding a stemma that is close to the truth . Note that we are comparing two arbitrary latent tree structures , and hence , the usual accuracy measures such as counting the number of shared edges , etc . , do not apply . The main problem is that we cannot establish a one to one correspondence between the latent nodes in the true stemma and the estimated one—there is no guarantee that even their number will be the same . Instead , we employ the so called average sign similarity score that was introduced and used in an earlier benchmarking experiment [ 18 ] .
To formally define the average sign similarity , let di,j denote the length of the shortest path ( number of edges ) connecting nodes i and j in the true stemma , and let d′ i,j denote the same for the estimated stemma . For any three distinct nodes i , j , k , we define the local score u(i , j , k ) as if sgn(di,j − di,k ) = sgn(d′ if sgn(di,j − di,k ) = −sgn(d′
1 , 0 , 1/2 , otherwise ; i,j − d′ i,k ) , i,j − d′ i,k ) ,
( 15 )
 where sgn takes values −1 , 0 , +1 , respectively , when the argument is negative , zero , and positive . The average sign similarity score is the average of u(i , j , k ) over all distinct observed ( ie , not latent ) nodes .
Briefly , the greater the score , the more similar the true and the estimated stemmata are , and vice versa . The fact that only triplets involving observed nodes are considered makes it possible to apply the average sign similarity to stemmata with different numbers of latent nodes . Furthermore , since the distances are defined in terms of the shortest path connecting two nodes , the stemmata need not be tree shaped— hence , the case of contamination in the Notre Besoin data set poses no problems .
In order to investigate how the amount of available data affects the performance of the considered methods , we create subsets by randomly removing complete manuscripts as well as parts thereof from the remaining ones . We first remove 10 % , 20 % , 30 % or 40 % of the nodes , and then , for each of the remaining manuscript independently , delete 0 % , 10 % , . . . , 90 % of the text in one or more contiguous randomly selected segments . Each combination of the above percentages is repeated 100 times with a new random seed , and a statistical test ( Wilcoxon signed rank test ) is performed to assess significance .
[ 19 ] ,
Other method included in the comparison are neighborjoining ( NJ ) least squares ( LS ) , maximum parsimony [ 7 ] , all three from the PAUP* package [ 24 ] , maximum likelihood ( ML ) [ 25 ] from the Phylip package [ 9 ] , and the RHM method that has been specifically designed for stemmatology [ 18 ] . The default settings are used for each algorithm . RHM requires that the number of iterations be specified : we use 25000 in each run which is computationally feasible but usually guarantees convergence to the same solution in multiple repeated runs in the used data sets .
In an earlier comparison on a set of benchmarks , including the two data sets we are using , maximum parsimony and RHM were found to perform consistently well [ 18 ] . The earlier comparison was based on particular subsets of the data with a certain number of missing manuscripts and certain deletions in some of the manuscripts , without randomization and repetitions . Consequently , the conclusions in the earlier comparison were not validated by statistical tests .
V . RESULTS
To get an idea of the learning task , consider Figs . 1 and 2 . They illustrate the original structure and the learned trees by Semstem and RHM for different amount of remaining data . For the plots , we chose RHM since it was found to be consistenly good in earlier experiments [ 18 ] as well as ours ( see below ) . The results obtained by other methods such as maximum parsimony were visually similar to those of RHM . In the figures , the positions of observed nodes are fixed to be the same in each graph in order to facilitate comparison . The hidden nodes are placed so as to appropriately show the structure . As mentioned above , RHM as well as all the other methods are only capable of creating trees where the observed nodes are positioned as leafs of the tree . This causes problems for interpreting the resulting trees : especially in the case of Parzival ( Fig 2 ) , the stemmata obtained by Semstem are more easily interpreted than the bifurcating trees obtained by RHM and the other methods . To assess the scores of the methods , Fig 3 gives the the average sign similarity scores of different methods when the number of missing nodes and the amount of missing text is varied . Tables I and II show numerical results . The highest scoring in each case is highlighted . Statistically significant differences are indicated ( see the table caption for details ) . Semstem outperforms other methods in most cases , achieving in some cases scores as high as 80 % , while other methods typically yield significantly lower scores .
VI . CONCLUSIONS AND FUTURE WORK
We presented a new method for discovering latent tree structures for the analysis of textual variation . Unlike earlier methods , which typically produce bifurcating trees , our method is able to produce unrestricted tree structures where the observed texts can be located either as internal nodes or
)
%
( s e i t i r a l i i i m s n g s e g a r e v A
80 % missing texts
60 % missing texts
40 % missing texts
20 % missing texts
90
80
70
60
50
40
G
G
G
G
G G G
G G G G
G
G G G G
G
G G G
G G G
G G G G G G G G G G G G G
G G
G G
G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G
G G G G G
G G G G G G G
G
G G
G
G
G G
G G
G
G
G
G G G
G G
G G G G G G G G G G G G G G G G G G G G G G G G G G G G
G G G G G G G G
G G G G G G G G G G G G G G G
G G G G G G G G G
G G G G G G G G G G G G G G
G G G G G G
G G G G G G G
G G G
G
G
G
G G G G G
G G
G G G G G G G
G G G G G G G
G G G G G
G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G
G G G G G G G G G G G G G G G G G G G GG G G G G G G
G
G G G G G
G G G
G
G G
G
G G G G G
G G G G G
G G G G
G
G G G G G G G G G G G G
G G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G
G
G
G
G
G G
G
G G G
G
G G G
G
G G G G
G G G
G G G G G G G G
G G
G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G
G G
G G G G G
G G
G
G G
G G
G
G G G G
G G G G G G
G G G G G G
G G G G G
G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G
G G G G G G G G G G G
G G G
G
G
G G
G
G G G
G
G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G
G G G G G G G
G G G G G
G G G G G G G G G G G G
G
G G
G G
G G
G
G
G
G G G
G
G
G G G G G G G
G G G GG G G G
G G G G G
G G G G G G G G G G G G G G G G G G G
G G GG
G G G G G G G G G G
G G G G G G G G G G G
G G G G G G G G G G G G G G G G G G G G G G
G G G
G
G G
G
G
G
G G G G
G G G G G G G G G G G G G G G G G G G G GG G G GG G G G
G G G G G G G G G G
G G G G G G G G G G G G G G G GG G G G G G G G G
G G G G G G G G
G G G G G G G G G G G G G G G G G G
G G G
G
G
G
G G G G
G
G
G G G G GG G G G
G
G G G
G G G G G G
G G G G G G G G G G GG G G G G G G G G G G G G G G G G
G G G G G G G G G G G G G G G G G G G G G
G G G G G G G G G G G G
G G G G G G G G
G G G
G G
G
G
G G G G G G G G
G G G G G G G G G G G G G G G G G G G G G G G G
G G GG
G G G G G G G G G G G G G G G G
G G G G G G G G G G G G G G GG G G G G G G G G G G
G G G G G
G
G G G G GG G G G G
G G G G
G
G G G G
G G G G G
G G G G G G G G G G G G G G G G G G G G G G G G G G G G
G G
G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G
G G G G G G G
G G G
G G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G G G
G G
G
G G G G G G G G G G G G
G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G
G G G G G G G G
G G G G
G G G G G G G
G G
G G G G
G G G G
G
G
G
G
G G
G G
G G G G G G G G G GG G G
G
G G G G G G
G G G G G G G G G G G G G G G G
G G G G G G G G G G G G G G G G G G G G G G G GG G G G GG G G G G G
G G G G G G G G G G G G G G
G G G
G
G G G
G
G
G
G
G
G G G G G G G G G G G G G G G G G G G G G G G G G
G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G
G G G G G G G G G G G G G G G G
G
G
G G G G
G G
G G G
G G
G G G G G G G G
G G G G
G G G G G G G G G G G G G G G G G G G
G G G G G G G G G G G G G G G G G G G G G G G G G G G
G G G G G G G G G G G G G G G G G G G G GG G G G G G G G
G G G
G
G G G
G
G G G
G
G G G
G G G
G G G G G GG G G G G G G GGG G GG G G G G G G G G G GG G
GG G G G G G G G G G G G G GG G
G G G
G G G G G G
G G GG G G G G G G G G G G G G G G
G G G G G G G G G G
G
G G G G G
G G
G
G
GG G
G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G
G G G G G G G G G G G G G G G
G
G G G G G G G
G
G
G G G
G
G G
G G
G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G GG G G G G G G G
G G G GGG G G G G
G G G G G G G G G G G G
G G G G G G G G G G G G
G G
G G G
G
G
G
G
G G G
G
G
G G G G G G GG G G G G G G G G G GG G G G G G G G G G G G GG G G G G G G
G
G
G G G G GG G
G G G G G G G G GG G G GG G GGG G G G G G G G
G G
G G G G G G
G G
G G
G G G G G G G
G
G G G G
G GG G G
GG G G GG G G G G GG G G G G GG GGGG GGG GG G G G G G G
G G GG G GG G G G G G G GG G G G G G G G G G GG G G G G GG G G G G G G G
G G G G G
G
G G G G
G G G G G G G
G
G G
G G GG G G G G G GG GG G GG GG G GG G G GG G G G
G G G G G G G G G G
G G G G G G G G
G GG GG G G G GGG G G G G G G G GGGGG G G G G
G G G G G
G G G G G G G G G G
G
G G G G G G G G G
G
G
G G G
G G G G G G GG G G G G G G G G
G G G G
G G
G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G G
G G G G G G G G G G G G G G G G G
G G G G G G G G GG G
G
G
G G G G
G
G
G
G G G
G G G G G G G G G G G G G G G G G G G G G G G
G G G
G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G GG
G
G G G G
G G G
G
G
SEM ML RHM MP NJ LS
SEM ML RHM MP NJ LS
SEM ML RHM MP NJ LS
SEM ML RHM MP NJ LS
Method
( a ) Notre Besoin : 30 % missing nodes , 20–80 % missing text
90
80
70
60
50
40
)
%
( s e i t i r a l i m s i n g s i e g a r e v A
90
80
70
60
50
40
)
%
( s e i t i r a l i m s i n g s i e g a r e v A
90
80
70
60
50
40
)
%
( s e i t i r a l i m s i n g s i e g a r e v A
40 % missing nodes
30 % missing nodes
20 % missing nodes
10 % missing nodes
G GG G G G G G G
G G G G
G G G G G G G
G G GG
G GG G G G GG G G G G G G G
G G
G G GG G G G G GG GGG GG GGG G GG G G G G G G G GG
G G G G G G G G G G G GG G
G
G GG G G G G G G
G
G
G
G G
G
G
G
G G G G
G
G G G G G G GG G G GGG G G G G GGGG G G G G G GG GG GG
G GG G GG G G G G GG G GG
G G G G
G G
G G G G
GG G G GG G G G GG G G
G G
G
GG G G G G G G G G G G G
G G G
G G
G
G
G G G
G
G G
G
G GG G G G G G G GGG G G G G G
G GGG G G G G G G G G G
G G
G G G
G G G G G
G G
G
G G G G
G G G G GG G G G G G G GG
G G G
G G G GGG G GG G G G G
G G G
G G G
G
G
G G G
G GG
G
G G
G
G G G
G
G G G G G G G
G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G
G G G G G G G G G G G G G G G G G G G G G G
G G G G G G G G G G G G G G
G G
G G G G G G
G G G G G G G
G
G
G
GG G
G G
G G G
G G G G
G
G G G G
G G
G G G
G
G G G G G G G G G G G G G
G G G GG G GGG G G G G G GGG G G G G G
G G G G G
G G G G G G G G GG G
GG G
G G G G G
G G G G G G G
G G
G
G
G G G G G
G
G
G G
G GG G G G G G G
GG G G
G
G G G
G G
G G G G
G
G GG G GGG G GGG G G GGG G GG G G GG G G GG
G G G G G G G G G G G GG G G G
G G
G G G G
G G
G G
G G G G G G
G G G G G G
G G
G G G
G
G
G G
G G
G
G
G
G
G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G
G G
G
G G
G G
G G
G
G
G
G
G G G G G G
G G G G G G GG
G G
G
G
G G G G G G G G G G G G G G G G G G G
G G G G
G G G
G G G G GG G G
G G G G G G G GG G G G G G GG G G G GG G
G G G G G G G G G G G G
G G G
G G G
G G G G G
G
G G G G G
G G G G G G G G GG G GG G G GGG GGG G G GG G G G GG G G GG G GG GG G G GG G
G G G G G G G G G G G G G GG G
G G G G G G G G G G GG G G G G G
G G G G G G G G G G G G G G
G
G
G
G
G
G G G G G G GGGG G G G G GG GGGG GGG G GG G GG G G G G
G G G G GG G G G G G
G G
G G GG G GG G G G G G GG G G G G
G
G G G G G G G G G G G G G G G G G G G G
G G G G
G G
G
G G G G G G
G G
G G
G G G G
G G G G G G G G G
G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G
G G G GGG G G G G G G G G G G G G G G G G G G G G G G G G
G
G G G G G G G G
G
G
G G G G
G
G G G G G G G G G G
G G G G G G G G G G G G
G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G
G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G
G G
G G
G
G
G
G
G G
G G G
G G G
G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G
G G G
G G
G G G G G G G G G G G
G G G G G G G GG G G G G G G G G G G
G
G G G GG G G G G G G G G G GG G
G GG G GGG G G G G G G G GGG G G GG G GG G G
G G G G G G G G G G G
G G G G
G G
G G
G G G GG
G G
G G
G G G G
G G G G G G G G G G G G G
G G G
G G G G G G G G G G G G G G G G G
GG G G G G G G G G G GG GG GGG G G G G G G
GG G GG G G G G GG G GGG G G GGG G GGG G GGG GG G G
G
G
G
G G
G G G
G G G G G G G G G G G
G
G G G G G G G G G G G G G G G G G GGGG GG G G G G GG G G GG G G GGG G GG G G G G G G G G G G G G G G
G G G G G G G G GG G G G G G G G G G G G G G
G G G G
G
G G
G
G G
G
G GG G
G G G G G G G G G G G G G G G G G G G G G G G G G G G GG G GG G G G G G G G
G GGG G GG G G GGG G G G G GG G G G G G G G G G G G G G G G
G G G G G G G G G G G G G G G
G G
G
G G G G
G
G G
G
G G G G G
G G G G G G G G G G G G G G G GG G G
G G G G G GG G G G G G G G G
G GGG G GG G G GG G G G G G G G G G GG G G GG GG G G G G G G G G G
G G
G G G G
G
G G G G G
G G G G
G
G
G
G G
G
G G
G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G
G G
G G G G G G
G G G G
G G G G G
G
G G G
G
G G G G G G G G G G G G G G G
G G G G G G GG G G G G G
G G G G G G G G G G G
G G G
G G G G G
G G
G G G G G G G G G G G
G G G G G G GG G G G G G G G
G G G G G G G GG G G G
G
G G G G G
G
G
G
G G G G G G G
G
G G G G G G G G G G G G G G GG G G G G G G G G G G GG G G G G G G G G G G G G G
G G G G G GG G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G
G G G G
G G
G G
G
G
G
G
G
G G G G G G GG G G G G G
G G G G G
G G G G G G G G G GG G G GG G G G G G G G G G G G G G G G
G G G G
G G G G G G G G G G G G G
GG G G G G G G G G G G
G G G G
G G GG G G G G G G G
G G G G G
G
G G
G
G G G G G G G G G G G G G G
G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G
G G G G G
G G G G G G G G G G G G G G
G G G G G G G
G
G
G G G
G G
G G
G
G G
G G
G
G G G G G G G G G G G G G G G G G
G G G G G G G G G G G G G G G G G G GG G G G G G G
G G G G G G GG G G G G G G G G G G G G
G G G G G G G G G G G G G G G G G G G G
G G G G G G G G
G G G
G
SEM ML RHM MP NJ LS
SEM ML RHM MP NJ LS
SEM ML RHM MP NJ LS
SEM ML RHM MP NJ LS
Method
( b ) Notre Besoin : 10–40 % missing nodes , 30 % missing text
80 % missing texts
60 % missing texts
40 % missing texts
20 % missing texts
G
G
G
G G G G G
G G G G
G G G G G G
G G G G G G
G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G GG G G GG G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G
G
G
G G
G G G G
G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G GG G G G G G G G G G G G G G G G GG G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G G G
G
G
G G
G G
G
G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G
G
G
G
G
G
G G
G G
G
G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G GG G G G G G G G G G G G GG G GG G G G G G G G G G G G G G G G G GG G G G G G G G G G G G
G G G
G
G
G
G G
G
G
G G G
G G G G G
G
G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G
G G G
G
G
G G
G
G G G
G G G G G G G G
G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G
G G G G G G
G G
G
G
G G
G G G
G G G G G
G G G G G G G G G G
G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G
G G G G G G G G G G G G G G G G G G G
G
G G
G
G
G
G G G
G
G G G G G G G G G G G G G G G
G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G
G G G G
G
G
G G
G
G
G G
G G
G G G G G
G G
G G G
G G G G G G G G G G
G G G G GG G G G
G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G GG G G G G G G G G G G G
G G
G G
G G
G
G
G
G G
G G G G G G G
G G G G G G G G G G G GG G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G
G G G GG G G G G G G G G G G G G G G G GGG G G G G G G G G G G G G G G
G
G
G
G G
G
G
G
G
G
G G
G G GG G G G G G G
G G GG G G G G G G
G G G G G G G G G G G
G G G G G G G G G G G G GG G G G G G G G G G G G G
G G G G G G G G G G G G G G G G G G G G
G
G G G G G G G G G G
G G
G
G G G
G
G
G G
G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G
G G G GG G G G G G G GG G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G
G
G
G G
G
G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G G G
G G G G G G
G G G G
G G
G
G
G
G
G
G
G G
G G G G G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G
G G G G G G GG G GG GG G G G GG G G G G G G G G G G G G
G G G G G G G
G
G
G
G
G G G
G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G G G
G
G G G
G G
G
G
G
G
G
G G G
G G G G
G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G GG G G G G G G G G G G G G G G G G GG
G G G G G G G G G G G
G
G G
G G G G G
G G G G
G G G G G
G G G
G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G
G
G
G
G G
G
G G G G G G G G G
G G G G
G G GG G G G G G GG G G GG G G G G G G G GG G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G
G G G G G
G
G
G
G G G G G
G G G G G G G G G G G G GG G GG G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G
G G G
G
G
G
G
G G
G G G G G G G G G G
G G GG G G GG G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G
G G G G G G GG G G G G
G G G G G G G G G G G G G G G G G G G G G G G G G G G G
G G G
G G G G
G G
G
G
G G
G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G
G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G
G G G
G
G G G
G
G G
G G
G G
G G GG G G G G G G
G G G G G G G G G G GG G GG GG G G G G G G G G GG G G G G G G G G G G G GG GG G G G G G G G G G G G G G G G G G G G G G G G G G G G G GG G G G
G G G G G G
G G
G G
G
G G G
G G G G G G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G GG G G G G G G G G G G G G G G G G G GG G G G G G G G
G
G
G
G
G
G G G
G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G GG G G G G G G GG G GG G
G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G
G G G
G
G
G G
G G
G G
SEM ML RHM MP NJ LS
SEM ML RHM MP NJ LS
SEM ML RHM MP NJ LS
SEM ML RHM MP NJ LS
Method
( c ) Parzival : 30 % missing nodes , 20–80 % missing text
40 % missing nodes
30 % missing nodes
20 % missing nodes
10 % missing nodes
G
G
G G G G G G
G G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G GG G G G G G G G G G G G G G G G G G G G
G G G G G
G G G G
G G G G
G G
G G
G G
G G G
G G G G G G G G G G
G G G G G G G G G G
G G G G G G G G G G G G G G G G GG G G G G G G G G
G G G G G G G GG G G G G G GG G G G
G G G G G GG G G G G G G G G G G G G G G G
G
G G
G
G
G
G
G G G G
G
G G G G
G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G
G G G G G
G G G G G GG G
G
G G G G
G
G G
G G G G G G G G G G G G G G GG G G G G G G GG G G G G G G G G G G G G G G G G G G GG G G G G G G GG G G GG G G G G G G G G GG G G G G G G G G GG G G G G G G G G G G G G G G G
G G
G G
G
G
G
G G
G
G G G G G G
G G G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G GG G G
G G G G G G G GG G G
G
G
G
G
G
G
G G
G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G GG G G
G G G G
G
G
G
G G G G G GG G G G G G G G G G G G G G G G G G G G G GG G G GG G G G G G G G G G G G G GG G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G
G G G G
G
G
G
G G G G G G G G G G G G G G GGG G G G G G G G
G G G G G GG G G G G G G G G G G G G G G G G G G G G G
G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G
G G G G G G G
G G G G
G G
G
G G G G
G G G
G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G
G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G
G G G G G G G G G G
G G G G G
G
G
G
G G
G G G G
G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G GG G G GG G G G G G G G G GG G
G G
GG
G G
G
G
G G G G
G G
G G G GG G G G G G G G G G G G
GG G G G G G G G G G G GG G G G G G G GG G G G G G GG G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G
G
G G G
G
G G
G G
G
G
G G G G G G G G G G
G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G GG G G G G G G G G G
G
G
G
G G G G G GG G G G G G G G G G G G G G G G G G G G G GG G G GG G G G G G G G G G G G G GG G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G
G G G G
G
G
G
G G G G G G G G G G G G G G GGG G G G G G G G
G G G G G GG G G G G G G G G G G G G G G G G G G G G G
G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G
G G G G G G G
G G G G
G G
G
G G G G
G G G
G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G
G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G
G G G G G G G G G G
G G G G G
G
G
G
G G
G G G G
G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G GG G G GG G G G G G G G G GG G
G G
GG
G G
G
G
G G G G
G G
G G G GG G G G G G G G G G G G
GG G G G G G G G G G G GG G G G G G G GG G G G G G GG G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G
G
G G G
G
G G
G G
G
G
G G G G G G G G G G
G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G GG G G G G G G G G G
G
G
G G
G
G G
G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G
G
G G G
G G GG G
G G G G G G
G G G G G G G G G G G G G G G G GG G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G
G G G G G G G G G G G G G
G G G G G
G
G
G G
G G G G G G
G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G G G
G G G
G G
G G
G G G G G G G G
G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G
G
G
G G
G
G
G
G G
G G G
G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G
G G G G G G
G G G G G G G
G
G G
G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G
G G G G
G
SEM ML RHM MP NJ LS
SEM ML RHM MP NJ LS
SEM ML RHM MP NJ LS
SEM ML RHM MP NJ LS
Method
( d ) Parzival : 10–40 % missing nodes , 30 % missing text
Figure 3 : Scores of different methods with different amounts of missing nodes and text . The boxplot shows the interquartile range in 100 repetitions with different randomly removed subsets of the data . ( SEM = Semstem ; MP = maximum parsimony ; for the other acronyms , see Sec IV . ) leafs . Another advantage is that the degree of the internal nodes is not limited . These two aspects make the results much easier to interpret . Empirical experiments involving two artificially created manuscript collections demonstrate that the new method achieves higher scores than the compared methods representing the current state of the art .
Future work includes studying the scaling properties of the new method when the size of the data sets is increased . The new artificial data sets created recently in the stemmatology community should provide an ideal basis for this . Studies using simulated data with varying parameters for the transition model describing the copying errors will complement such an investigation . Furthermore , it will be interesting to develop more refined models to be used as a basis of the method . Of particular interest are asymmetric models that could be used for identifying the orientation of the edges , and hence , the root of the stemma .
ACKNOWLEDGMENT
The authors thank the anonymous referees for thoughtful comments that have improved the paper . This work was supported in part by the University of Helsinki Research Funds ( project STAM ) , the Finnish Cultural Foundation ( Studia Stemmatologica Workshop ) , The Academy of Finland ( project MODEST ) , and the European Union Network of Excellence PASCAL .
REFERENCES
[ 1 ] P . V . Baret , C . Mac´e . and P Robinson , “ Testing Methods on an Artificially Created Textual Tradition , ” Linguistica computazionale , pp . 255–281 , 2006 .
[ 2 ] A . Bounchard Cˆot´e , M . Jordan , and D . Klein , “ Efficient inference in phylogenetic InDel trees , ” Advances in Neural Inf Proc Syst ( NIPS 2008 ) , pp . 177–184 , 2008
[ 3 ] L . L . Cavalli Sforza and A . W . F . Edwards , “ Phylogenetic analysis—Models and estimation procedures , ” Am . J . Hum . Genet . , vol . 19 , pp . 233–257 , 1967
[ 4 ] C . K . Chow and C . N . Liu , “ Approximating discrete probability distributions with dependence trees , ” IEEE Trans . Inf . Theory , vol . 14 , pp . 462–467 , 1968
[ 5 ] J . Delz , Textkritik und Editionstechnik , in F . Graf ( Ed. ) , Einleitung in die lateinische Philologie , B . G . Teubner , 1997
[ 10 ] N . Friedman , “ Learning belief networks in the presence of missing values and hidden variables , ” Proc . ICML 1997 , pp . 125–133 , 1997
[ 11 ] N . Friedman , M . Ninio , I . Pe’er , and T . Pupko , “ A structural EM algorithm for phylogenetic inference , ” J . Comput . Biol . , vol . 9 , pp . 331–354 , 2002
[ 12 ] C . Liu , C . Chen , J . Han , and P . S . Yu , “ GPLAG : Detection of software plagiarism by program dependence graph analysis , ” Proc . KDD 2006 , pp . 872–881 , 2006
[ 13 ] B . Mau , M . A . Newton , and B . Larget , “ Bayesian Phylogenetic Inference via Markov chain Monte Carlo methods , ” Biometrics , vol . 55 , pp . 1–12 , 1999
[ 14 ] A . Papoulis , Probability , Random Variables , and Stochastic
Processes , 2nd edition , McGraw Hill , 1984
[ 15 ] J . Pearl , “ Reverend Bayes on inference engines : A distributed hierarchical approach , ” Proc . AAAI 1982 , pp . 133–136 , 1982
[ 16 ] P . Robinson and R . J . O’Hara , “ Report on the textual criticism challenge 1991 , ” Bryn Mawr Class . Rev . , vol . 3 , pp . 331–337 , 1992
[ 17 ] F . Ronquist and J . P . Huelsenbeck , “ MRBAYES 3 : Bayesian phylogenetic inference under mixed models , ” Bioinformatics , vol . 19 , pp . 1572–1574 , 2003
[ 18 ] T . Roos and T . Heikkil¨a , “ Evaluating methods for computerassisted stemmatology using artificial benchmark data sets , ” Lit Linguist Comput . , vol . 24 , pp . 417–433 , 2009
[ 19 ] N . Saitou and M . Nei , “ The neighbor joining method : A new method for reconstructing phylogenetic trees , ” Mol Biol Evol , vol . 4 , pp . 406–425 , 1987
[ 20 ] C . Semple and M . A . Steel , Phylogenetics , Oxford University
Press , 2003
[ 21 ] M . Spencer , E . A . Davidson , A . C . Barbrook , and C . J . Howe , “ Phylogenetics of artificial manuscripts , ” J . Theor . Biol . , vol . 227 , pp . 503–511 , 2004
[ 22 ] M . Spencer and C . J . Howe , “ How accurate were scribes ? A mathematical model , ” Lit Linguist Comput . , vol . 17 , pp . 311–322 , 2002
[ 23 ] J . Sun , S . Papadimitriou , C Y Lin , N . Cao , S . Liu , W . Qian , “ MultiVis : Content based social network exploration through multi way visual analysis , ” Proc . SDM 2009 , pp . 1063–1074 , 2009
[ 6 ] A . P . Dempster , N . M . Laird , and D . B . Rubin , “ Maximum likelihood from incomplete data via the EM algorithm , ” J Royal Stat . Soc . Ser . B , vol . 39 , pp . 1–38 , 1977
[ 24 ] D . L . Swofford and D . P . Begle , PAUP : Phylogenetic analysis using parsimony . Version 31 User ’s manual , Smithsonian Institution , Laboratory of Molecular Systematics , 1993
[ 7 ] A . W . F . Edwards and LL Cavalli Sforza , “ The reconstruction of evolution , ” Ann . Hum . Genet . , vol . 27 , pp . 105–106 , 1963
[ 8 ] J . Felsenstein , “ Numerical methods for inferring evolutionary trees , ” Quart . Rev . Biol . , vol . 57 , pp . 379–404 , 1982
[ 9 ] J . Felsenstein , “ PHYLIP Phylogeny Inference Package ( ver sion 3.2 ) , ” Cladistics , vol . 5 , pp . 164–166 , 1989
[ 25 ] E . A . Thompson , Human Evolutionary Trees , Cambridge
University Press , 1975
[ 26 ] S . Wehner , “ Analyzing worms and network traffic using compression , ” J . Comp . Secur . , vol . 15 , pp . 303–320 , 2007
[ 27 ] Z . Yang and B . Rannala , “ Bayesian phylogenetic inference using DNA sequences : A Markov chain Monte Carlo method , ” Mol . Biol . Evol . vol . 14 , pp . 717–724 , 1997
Table I : Means of average sign similarities ( % ) of different methods for the Notre Besoin data set . Results that are better than the others are shown in bold face , with statistical significance indicated by : * p < 0.05 ; ** p < 0.01 ; *** p < 0001
Data set
Nodes missing
Text
( % ) missing ( % )
Semstem
40
30
20
10
90 80 70 60 50 40 30 20 10
90 80 70 60 50 40 30 20 10
90 80 70 60 50 40 30 20 10
90 80 70 60 50 40 30 20 10
51.2 54.3 56.6 59.2 62.8 62.9 63.6 65.0 65.1
51.4 54.6 59.0** 62.8 65.2*** 67.3*** 68.7*** 69.5*** 70.5***
60.7 59.7 63.8 70.0* 75.6*** 77.8*** 78.9*** 79.6*** 81.9***
59.6 59.0 63.7 69.1 71.8* 73.7* 74.9* 76.5** 77.0
Method
RHM
MP
59.0*** 63.1*** 64.6*** 65.7*** 67.4*** 67.9*** 67.9*** 68.0** 67.7
51.3 53.4 56.2 59.0 61.0 60.8 61.6 62.2 61.7
62.1* 61.3* 65.3* 68.6 69.0 70.0 69.8 70.0 70.4
60.4 61.6** 67.3** 68.7 70.9 71.7 72.7 72.7 72.4
51.9 53.7 56.2 60.5 63.5 65.2 65.0 66.4 68.1***
43.9 46.0 50.0 54.9 58.8 59.5 59.4 59.4 61.3
55.5 55.4 60.5 66.5 68.8 70.7 71.3 71.6 71.6
54.6 55.7 62.7 66.6 70.1 70.5 72.6 73.1 73.2
NJ
47.2 52.3 55.2 58.7 61.9 62.8 63.5 63.9 64.5
49.7 52.1 56.5 61.8 62.7 62.6 62.2 62.8 62.2
58.3 58.8 63.6 67.2 71.4 72.4 72.4 72.2 74.4
58.1 59.3 64.8 67.5 70.9 72.5 73.8 74.1 76.4
ML
47.2 52.7 54.6 58.1 61.3 62.5 62.1 62.2 62.3
49.2 51.0 55.3 60.2 63.0 61.0 60.7 60.8 61.2
58.0 58.3 63.7 68.9 71.0 71.9 72.5 72.5 73.2
58.8 58.2 65.6 68.8 70.0 71.7 72.1 72.9 73.3
LS
53.9 55.6 57.6 59.8 62.0 63.1 63.8 63.7 64.5
52.4 53.9 57.2 62.0 62.7 63.1 62.8 62.8 63.6
55.3 55.8 60.6 66.5 69.8 71.9 72.5 72.1 73.6
54.5 56.0 62.4 66.6 69.6 72.6 73.6 74.7 76.6
Table II : Means of average sign similarities ( % ) of different methods for the Parzival data set . Results that are better than the others are shown in bold face , with statistical significance indicated by : * p < 0.05 ; ** p < 0.01 ; *** p < 0001
Data set
Nodes missing
Text
( % ) missing ( % )
Semstem
40
30
20
10
90 80 70 60 50 40 30 20 10
90 80 70 60 50 40 30 20 10
90.0 80 70 60 50 40 30 20 10
90 80 70 60 50 40 30 20 10
58.1 59.0 62.7 68.7* 73.3*** 75.3*** 78.4*** 78.0*** 78.8***
57.0** 58.4* 61.9** 68.0*** 73.7*** 76.9*** 79.7*** 81.0*** 81.7***
56.9** 58.8 62.2 67.3 70.2 74.6 75.5 76.8 77.8
56.2* 58.0* 62.9* 67.8** 72.6*** 75.6*** 76.8*** 78.5*** 79.3***
Method
RHM
53.6 54.0 57.4 61.4 65.6 67.4 70.0 72.3 73.8
52.7 54.4 57.9 61.7 66.6 69.1 69.8 72.1 73.5
53.1 55.9 58.7 63.4 67.3 68.9 71.1 72.3 74.5
52.7 55.9 59.2 63.9 67.0 68.4 71.0 72.7 73.6
MP
58.4 57.1 60.4 64.5 68.2 69.1 69.7 70.0 70.7
55.4 55.7 57.8 61.1 65.1 67.2 69.3 71.2 73.3
52.5 55.6 61.0 66.5 69.8 72.9 74.4 76.2 79.3
54.8 55.4 60.3 66.1 69.0 69.8 71.8 73.8 75.3
ML
55.4 56.6 61.5 66.9 69.5 71.0 71.5 71.6 73.3
53.8 55.3 58.9 63.8 67.8 68.8 71.5 72.8 75.9
54.8 58.6 62.2 67.3 70.9 73.9 75.6 76.9 79.6
54.6 57.1 61.5 66.0 70.1 71.2 73.0 73.9 76.0
NJ
57.0 57.8 60.6 65.5 66.8 68.8 70.1 69.9 70.9
55.8 57.0 60.0 63.3 67.6 69.7 70.6 71.4 71.8
55.0 57.1 60.1 61.9 65.9 68.6 71.2 72.5 74.7
55.2 57.2 59.7 63.7 66.4 68.0 70.3 70.9 72.4
LS
53.8 54.5 58.4 65.3 67.2 68.5 69.6 69.9 70.6
53.8 55.9 59.5 63.9 68.0 70.0 70.9 72.1 73.1
54.4 56.5 60.0 63.5 67.7 69.5 71.5 72.9 74.9
53.4 56.0 61.3 64.6 67.8 69.5 70.8 71.3 72.7
