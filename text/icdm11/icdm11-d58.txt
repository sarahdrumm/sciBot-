Optimizing Performance Measures for Feature Selection
School of Computer Engineering , Nanyang Technological University , Singapore , 639798
Qi Mao and Ivor Wai Hung Tsang Email : {qmao1 , ivortsang}@ntuedusg
Abstract—Feature selection with specific multivariate performance measures is the key to the success of many applications , such as information retrieval and bioinformatics . The existing feature selection methods are usually designed for classification error . In this paper , we present a unified feature selection framework for general loss functions . In particular , we study the novel feature selection paradigm by optimizing multivariate performance measures . The resultant formulation is a challenging problem for high dimensional data . Hence , a two layer cutting plane algorithm is proposed to solve this problem , and the convergence is presented . Extensive experiments on largescale and high dimensional real world datasets show that the proposed method outperforms l1 SVM and SVM RFE when choosing a small subset of features , and achieves significantly improved performances over SVMperf in terms of F1 score .
Keywords feature selection ; multivariate performance mea sure ; multiple kernel learning ; structural SVMs
I . INTRODUCTION
Depending on applications , specific performance measures are required to evaluate the success of a learning algorithm . In text classification , for example , F1 score and Precision/Recall Breakeven Point ( PRBEP ) are used to evaluate the performance of one classifier ; while the error rate is not suitable due to a large imbalance between positive and negative examples . Thereafter , SVMperf [ 1 ] was proposed to directly optimize multivariate losses . Given a training sample of input output pairs ( xi , yi ) ∈ X ×Y for i = 1 , . . . , n drawn from some fixed but unknown probability distribution with X ∈ Rm and Y ∈ {−1 , +1} . The learning problem is treated as a multivariate prediction problem by defining the hypotheses h : X → Y that map a tuple x ∈ X of n feature vectors x = ( x1 , . . . , xn ) to a tuple y ∈ Y of n labels y = ( y1 , . . . , yn ) where X = X × . . . ,X and Y ⊆ {−1 , +1}n . The linear discriminative function of SVMperf is defined as
, iwT xi y n i=1 hw(x ) = arg max y∈Y
( 1 ) where w = [ w1 , . . . , wm]T is the weight vector .
The multivariate loss functions can be easily incorporated into structural SVMs [ 2 ] by 1 slack variable formula as , min w,ξ≥0
1 2 st ∀y w2
2 + Cξ
∈ Y\y : wT n
( yi − y i)xi ≥ ∆(y , y
( 2 )
) − ξ , where ∆(y , y ) is some type of multivariate loss functions and C is a regularization parameter that trades off the i=1 empirical risk and the model complexity . This optimization problem is convex , but there is the exponential size of constraints . Fortunately , this problem can be solved in polynomial time by adopting the sparse approximation algorithm of structural SVMs . As shown in [ 1 ] , optimizing the learning model subject to one specific multivariate measure can really boost the performance of this measure . There are other work for multivariate performance measures such as F score [ 3 ] , NDCG [ 4 ] , ordinal regression [ 5 ] , and ranking loss [ 6 ] .
The learned weight vector w of all these methods is usually non sparse ; however , for some real world applications , such as image and document retrievals , a set of sparse yet discriminative features is a necessity for rapid prediction on massive databases . In addition , there are many noisy or non informative features in documents and images , resulting in degraded retrieval performances . However , existing wrapper based feature selections are usually designed for classification error [ 7 ] , [ 8 ] , [ 9 ] , [ 10 ] , [ 11 ] , [ 12 ] , where l1 norm [ 9 ] , [ 13 ] and l0 norm [ 10 ] , [ 11 ] are widely used . To fulfill the needs of applications , it is imperative to have a feature selection method designed for specific multivariate performance measures .
In this paper , we present a unified feature selection framework for general loss functions based on the proposed regularizer . Particularly , optimizing multivariate performance measures are studied in this framework . To our knowledge , this is the first work to optimize multivariate performance measures for feature selection . The resultant formulation is challenging for high dimensional problems . Hence , we propose a two layer cutting plane algorithm to solve this problem effectively and efficiently , and the convergence of this algorithm is presented . Specifically , training Multiple Kernel Learning ( MKL ) in the primal by cutting plane algorithm is proposed to deal with exponential size of constraints induced by multivariate losses .
Extensive experiments on several challenging and very high dimensional real world datasets show that the proposed method yields better performance than the state of the art feature selection methods , and outperforms SVMperf using all features in terms of multivariate performance measures . In the rest of this paper , we denote the transpose of a vector/matrix by the superscript T and lp norm of a vector v by ||v||p . Binary operator fi represents the elementwise product between two vectors/matrices .
II . FEATURE SELECTION FOR MULTIVARIATE
PERFORMANCE MEASURES d in the real domain D = {d|m
To optimize the multivariate loss functions and learn a sparse feature representation simultaneously , we propose to solve the following jointly convex problem over ( w , ξ ) and j=1 dj = B , 0 ≤ dj ≤ 1,∀j = 1 , . . . , m} with a budget parameter B > 0 , m j=1 d∈D min min w,ξ≥0
1 2
|wj|2 dj
+ Cξ st ∀y
∈ Y\y : wT 1 n n
( yi − y i)xi ≥ ∆(y , y
( 3 )
) − ξ . i=1 y∈Y\y αy 1 n and to ( w , ξ ) is obtained by The partial dual with respect Lagrangian function L(w , ξ , α , τ ) with dual variables α ≥ 0 n and τ ≥ 0 . As the gradients of Lagrangian function with respect to ( w , ξ ) vanish at the optimal points , we obtain the i=1(yi − y KKT conditions wj = dj i)xj,i , y∈Y\y αy ≤ C . By substituting KKT conditions back to L(w , ξ , α , τ ) , we obtain the dual problem as n n ∆(y , y ) , and A = {α| i=1(yi − y where ∆(y , y ) = 0 , ∆(y , y ) > 0 if y = y , Qd ay , ay , ay = 1 y,y = d ) , by = y αy ≤ C , α ≥ 0} . Problem 1 ( 4 ) is a challenging problem because of the exponential size of α and high dimensional vector d for high dimensional problems . i)(xi fi √
α∈A − 1
αy αy Qd min d∈D max y,y +
αy by ,
( 4 ) y y y
2 n
III . THE TWO LAYER CUTTING PLANE ALGORITHM In this section , we propose a two layer cutting plane algorithm to solve Problem ( 4 ) efficiently and effectively . The two layers , namely group feature generation and group feature selection , will be described in Section III A and III B , respectively . The two layer cutting plane algorithm will be presented in Section III C and III D .
A . Group Feature Generation
By denoting S(α , d ) = − 1 y αyby , Problem ( 4 ) turns out to be
2 y y αyαyQd y,y + min d∈D max
α∈A S(α , d ) .
Since domains D and A are nonempty , the function S(α∗ , d ) is closed and convex for all d ∈ D given any α∗ ∈ A , and the function S(α , d∗ ) is closed and concave for all α ∈ A given any d∗ ∈ D , the saddle point property min d∈D max
α∈A S(α , d ) = max
α∈A min d∈D S(α , d ) holds [ 15 ] .
We further denote Fd(α ) = −S(α , d ) , and then the equivalent optimization problems are obtained as d∈D Fd(α )
α∈A max min or min α∈A,γ
γ : γ ≥ Fd(α ) , ∀d ∈ D .
( 5 ) the lower bound approximation of ( 5 ) can be obtained by maxd∈D Fd(α ) ≥ maxt=1,,T Fdt(α ) . Then we minimize the problem ( 5 ) over the set {dt}T t=1 by , min α∈A max t=1,,T
Fdt ( α ) or min α∈A,γ
γ : γ≥Fdt ( α),∀t = 1 , . . ,T
( 6 )
As from [ 17 ] , such cutting plane algorithm can converge to a robust optimal solution within tens of iterations with the exact worst case analysis . Specifically , for a fixed αt , the worst case analysis can be done by solving , dt = arg max d∈D Fd(αt ) ,
( 7 ) which is referred to as the group generation procedure . Even though Problem ( 6 ) and ( 7 ) cannot be solved directly due to the exponential size of α , we will show that they are readily solved in Section III B and Section III D , respectively .
B . Group Feature Selection
By introducing dual variables µ = [ µ1 , µ2 , . . . , µT ]T ≥ 0 , we can transform ( 6 ) to an MKL problem as follows ,
T
αy αy
µtQdt y,y
+
αy by , t=1 y t=1 µt = 1 , µt ≥ 0,∀t = 1 , . . . , T} . y
( 8 ) where MT = {T
α∈A min max µ∈MT
− 1 2 y
However , due to the exponential size of α , the complexity of Problem ( 8 ) remains . In this case , state of the art multiple kernel learning algorithms [ 18 ] , [ 14 ] , [ 19 ] do not work any more . The following proposition shows that we can indirectly solve Problem ( 8 ) in the primal form . Proposition 1 . The primal form of Problem ( 8 ) is min w1,,wT ,ξ≥0
+ Cξ
( 9 ) wt , at y,∀y
∈ Y\y .
According to KKT conditions , the solution of ( 9 ) is
T
2
1 2 wt2 t=1 st ξ ≥ by − T t=1 wt = µt
αyat y y
( 10 )
( 11 ) where µt is a dual value of the tth constraint of ( 6 ) .
Here , we define the regularization term as Ω(w ) =
2 with w = [ w1 , . . . , wT ]T and the empir
,T t=1 wt2
1 2 ical risk function as
Remp(w ) =
1 n max
0 , max y∈Y\y by − T t=1 y
, wt , at which is a convex but non smooth function wrt w . Then we can apply the bundle method [ 20 ] to solve this primal problem . Problem ( 9 ) is transformed as
J ( w ) = Ω(w ) + CRemp(w ) . min w
Cutting plane algorithm [ 16 ] could be used here to solve this problem . Since maxd∈D Fd(α ) ≥ Fdt(α),∀dt ∈ D ,
Since Remp(w ) is a convex function , its subgradient exists everywhere in its domain [ 21 ] . Suppose wk is a point where Remp(w ) is finite , we can formulate the lower bound according to the definition of subgradient ,
Remp(w ) ≥ Remp(wk ) + w − wk , pk , where subgradient pk ∈ ∂wRemp(wk ) is at wk . Given subgradient sequence p1 , p2 , . . . , pK , the tighter lower bound for Remp(w ) can be stated as follows , Remp(w ) ≥ RK where qk = Remp(wk ) − wk , pk . Following the bundle method [ 20 ] , the criterion for selecting the next point wK+1 is to solve the following problem , w , pk + qk emp(w ) = max
0 , max 1≤k≤K
, min w1,,wT ,ξ≥0
1 2 wt2
+ Cξ
( 12 ) st ξ ≥ w , pk + qk,∀k = 1 , . . . , K .
The following Corollary shows that Problem ( 12 ) can be easily solved by QCQP solvers , and the number of variables is independent of the number of examples . Corollary 1 . In terms of Proposition 1 , the dual form of Problem ( 12 ) is
2
T t=1
( 13 )
≤ θ,∀t = 1 , . . . , T ,
−θ +
K flflfl K k=1 k=1
αkqk flflfl2
2
αkpk t max α∈AK max
θ st 1 2 where AK = {K k=1 αk ≤ C , αk ≥ 0,∀k = 1 , . . . , K} , and which is a QCQP problem with T + 1 constraints and K + 1 variables . group set W , , C
Algorithm 1 group feature selection 1 : Input : x = ( x1 , . . . , xn ) , y = ( y1 , . . . , yn ) , an initial 2 : Y = ∅ , k = 0 3 : repeat 4 : 5 : 6 : 7 : 8 : 9 : until optimal k = k + 1 Finding the most violated y Compute pk and qk Y = Y ∪ {y} Solving Problem ( 13 ) over W and Y emp(w ) ,
By setting JK(w ) = Ω(w ) + CRK the optimal condition in Algorithm 1 is min0≤k≤K J ( wK ) − JK(wK ) ≤ . The convergence proof in [ 20 ] does not apply in this case as the Fenchel dual of Ω(w ) fails to satisfy the strong convexity assumption if K > 1 . As K = 1 , Algorithm 1 is exactly the bundle method [ 20 ] . When K ≥ 2 , we can adapt the proof of Theorem 5 in [ 22 ] for the following convergence results .
+ log2
4R2C
Theorem 1 . For any 0 < C , 0 < ≤ 4R2C and any training example ( x1 , y1 ) , . . . , ( xn , yn ) , Algorithm 1 converges to the desired precision after at most ,
∆ iterations . R2 = maxdt,y 1 maxy ∆(y , y ) and ( cid:100 ) . is the integer ceiling function . C . The Proposed Algorithm
16R2C n i)(xifidt)2 , ∆ = i=1(yi−y
2||K
2 − K k=1 αkpk
Algorithm 1 can obtain the optimal solution for the original dual problem ( 6 ) . By denoting Gd(α ) = the group feature gen1 eration layer can directly use the optimal solution of the objective Gd(α ) to approximate the original objective Fd(α ) . The two layer cutting plane algorithm is presented in Algorithm 2 . From the description of Algorithm 2 , it is clear k=1 αkqk , t ||2 n
Algorithm 2 The Two Layer Cutting Plane Algorithm 1 : Input : x = ( x1 , . . . , xn ) , y = ( y1 , . . . , yn ) , , C 2 : W = ∅ , t = 0 3 : repeat 4 : 5 : 6 : W = W ∪ {dt} 7 : 8 : until optimal t = t + 1 Finding the most violated dt group feature selection(x , y , W , , C ) to see that groups are dynamically generated and augmented into active set W for group selection .
In terms of the convergence proof of FGM in [ 7 ] and Theorem 1 , we can obtain the following theorem to illustrate the approximation with an optimal solution to the original problem . By setting Θd(α ) = − 1 y,y + y y αyαyQd
2 y αyby , we obtain the following result :
Theorem 2 . After Algorithm 2 stops in a finite number of steps , the difference between optimal solution ( d∗ , α∗ ) of Problem ( 8 ) and the solution ( d , α ) of Algorithm 2 is S(α∗ , d∗ D . Finding the Most Violated y and d
) − Θd(α ) ≤ .
Algorithm 1 and 2 need to find the most violated y and d , respectively . In this subsection , we discuss how to obtain these quantities efficiently . Algorithm 1 needs to calculate emp(w ) . the subgradient of the empirical risk function RK Since RK the subgradient should be in the convex hull of the gradient of the decomposed functions with the largest objective . Here , we just take one of these subgradients by solving emp(w ) is a pointwise supremum function ,
∆(y yk = arg max y∈Y\y where vi =T n t=1 wT easy to compute pk qk = 1 n i=1 ∆(yk , y ) . t ( xi fi t = − 1 n
( yi − y
, y ) − n n dt ) . After obtaining yk , it is i=1(yi − yk dt ) and i )(xi fi i)vi ,
( 14 )
√
√ i=1
DATASETS USED IN OUR EXPERIMENTS
Table I
Dataset
#classes
#features
News20.binary URL1 Sector News20
2 2 105 20
1,355,191 3,231,961 55,197 62,061
#train points 11,997 20,000 6,412 15,935
#test points 7,999 20,000 3,207 3,993
For finding the most violated y , and computed independently , ∆(y , y ) = n it depends on how to define the loss ∆(y , y ) in Problem ( 14 ) . One of the instances is the Hamming loss which can be decomposed i=1 δ(yi , y i ) , where δ is an indicator function with δ(yi , y i ) = 0 if yi = y i , otherwise 1 . However , there are some multivariate performance measures which could not be solved independently . Fortunately , there are a series of structured loss functions , such as Area Under ROC ( AUC ) , Average Precision ( AP ) , ranking and contingency table scores and other measures listed in [ 1 ] , [ 23 ] , [ 20 ] , which can be implemented efficiently in our algorithms . In this paper , we study several multivariate performance measures based on contingency table including F1 score , Precision/Recall@k and Precision/Recall BreakEven Point ( PRBEP ) as the showcases and their finding yk could be solved in the time complexity O(n2 ) [ 1 ] .
After t iterations in Algorithm 2 , we transform α in Problem ( 7 ) from the exponential size to a small size αt . Now , finding the most violated d becomes m
1 2n2 c2 j dj
( 15 ) where cj = K constraint m dt = arg max n d∈D i=1(yi − yk j=1 i )xi,j . With the budget k=1 αt i=1 di = B in D , ( 15 ) can be solved by first k j ’s in the descent order and then setting the first j to 1 and the rest to 0 . This sorting c2 B numbers corresponding to dt takes only O(m log m ) operations .
IV . EXPERIMENTS
In this Section , we conduct extensive experiments to evaluate the performance of our proposed method and stateof the art feature selection methods : 1 ) SVM RFE [ 12 ] ; 2 ) l1 SVM ; 3 ) FGM [ 7 ] . SVM RFE and FGM use Liblinear software 1 as the QP solver for their SVM subproblems . For l1 SVM , we also use Liblinear software , which implements the state of the art l1 SVM algorithm [ 9 ] . In addition to the comparison for 0 1 loss , several specific measures on the contingency table are investigated on Text datasets by comparing with SVMperf [ 1 ] . All the datasets shown in Table I are of high dimensions .
For convenience , we name our proposed two layer cutting multi , where ∆ represents different type plane algorithm FS∆ of multivariate performance measures . We implemented Algorithm 2 in MATLAB for all the multivariate performance measures listed above , using Mosek as the QCQP solver for Problem ( 13 ) which yields a worse case complexity of O(KT 2 ) . Since the values of both K and T are much smaller than the number of examples n and its dimensionality m , the QCQP is very efficient as well as more accurate for large scale and high dimensional datasets . Furthermore , the codes simultaneously solve the primal and its dual form . So the optimal µ and α can be obtained after solving Problem ( 13 ) . obtained by f ( x ) = w fid , x where w = n
For a test pattern x , the discriminant function can be √ i=1 βixi , dt . This βi = 1 n leads to the faster prediction since only a few of the selected features are involved . After computing pk , the matrices of Problem ( 13 ) can be incrementally updated , so it can be done totally in O(T K 2 ) .
K k=1 αk(yi − yk i ) , and d = T t=1 µt
A . Feature Selection for Accuracy multi
Since [ 1 ] has proven that SVM∆ multi with Hamming loss , namely ∆Err(y , y ) = 2(b + c ) , is the same as SVM . In this subsection , we evaluate the accuracy performances of multi for Hamming loss function , namely FShamming FS∆ as well as other state of the art feature selection methods . We compare these methods on two binary datasets , News20.binary 2 and URL1 in Table I . Both datasets are used in [ 7 ] , and they are already split into training and testing sets . We test FGM and SVM RFE in the grid CF GM = [ 0.001 , 0.01 , 0.1 , 1 , 5 , 10 ] and choose CF GM = 5 which gives good performance for both FGM and SVM RFE . This is the same as [ 7 ] . For FShamming , we do the experiments by fixing CF GMmulti as 0.1 × n for URL1 and 1.0× n for New20binary The setting for budget parameter B = [ 2 , 5 , 10 , 50 , 100 , 150 , 200 , 250 ] for News20.binary , and B = [ 2 , 5 , 10 , 20 , 30 , 40 , 50 , 60 ] for URL1 . The elimination scheme of features for SVM RFE method can be referred to [ 7 ] . For l1 SVM , we report the results of different C values so as to obtain different number of selected features . multi multi
Figure 1 reports the testing accuracy on different datasets . The testing accuracy is comparable among different methods , but both FShamming and FGM can obtain better prediction performances than SVM RFE in a small number ( less than 20 ) of selected features on both News20.binary and URL1 . These results show that the proposed method with Hamming loss can work well on feature selection tasks especially when choosing only a few features . FShamming also performs better than l1 SVM on News20.binary in most range of selected features . This is possibly because l1 models are more sensitive to noisy or redundant features on News20.binary dataset . multi
1http://wwwcsientuedutw/˜cjlin/liblinear/
2http://wwwcsientuedutw/˜cjlin/libsvmtools/datasets
( a ) News20.binary
( b ) URL1 Figure 1 . Testing accuracy on different datasets
( b ) Sector
( c ) News20
( a ) News20.binary
( b ) URL1
Figure 3 . The average performance improvement of FS∆ B on different datasets . multi with varying
The sparsity of features of FShamming
Figure 2 . with varying B on different datasets . Each row bar with different color represents the different subset of features selected under current B , where the white region means the features are not selected . multi multi
Figure 2 shows that our method with the small B will select smaller number of features than the large B . We also observed that most of features selected by the small B also appeared in the subset of features using the large B . This phenomenon can be obviously observed on News20binary This leads to the conclusion that FShamming can select the important features in the given datasets due to the insensitivity of parameter B . However , we notice that not all the features in the selected subset of features with smaller B fall into that of subset of features with the large B , so our method is non monotonic feature selection . This argument is consistent with the test accuracy in Figure 1 . News20.binary seems to be monotonic datasets from Figure 2 , since FShamming , FGM and SVM RFE demonstrate similar performance . However , URL1 is more likely to be non monotonic , as our method and FGM can do better than SVM RFE . All the facts imply that the proposed method is comparable with FGM and SVM RFE . And it also demonstrates the non monotonic property for feature selection . multi
B . Multivariate Performance Measures for Document Retrieval
In this subsection , we focus on feature selection for different multivariate performance measures on imbalanced text data shown in Table I . For multiclass classification problems , one vs . rest strategy is used . The comparing model is SVMperf 3 . Following [ 1 ] , we use the same notation SVM∆ multi for different multivariate performance measures . The command used for training SVMperf can work for different measures by l option 4 . In our experiments , we search the Cperf in the same range [ 2−6 , . . . , 26 ] as in [ 1 ] . We choose the one which demonstrates the best performance of SVM∆ to each multivariate performance measure for comparison . FS∆ fix CF GMmulti = 0.1 × n for News20 except 5.0 × n for Sector . For Rec@k , we use k as twice the number of positive examples , namely Rec@2p . The evaluation for this measure uses the same strategy to label twice the number of positive examples as positive in the test datasets , and then calculate Rec@2p . multi and FShamming multi multi multi multi multi and SVM∆ multi over FShamming multi is consistently better than FShamming
Table II shows the macro average of the performance over all classes in a collection in which both FS∆ multi and FShamming at B = 250 are listed . The improvement of FS∆ multi with respect to different B values are reported in Figure 3 . From Table II , FS∆ on all multivariate performance measures and two multiclass datasets . Similar results can be obtained comparing with SVM∆ multi , while the only exception is the measure Rec@2p on News20 where SVM∆ multi . The largest gains are observed for F1 score on all two text classification tasks . This implies that a small number of features selected by FS∆ multi is enough to obtain comparable or even better performances for different measures than SVM∆ multi using all features . multi is a little better than FS∆
From Figure 3 , FS∆ multi consistently performs better than
3wwwcscornelledu/People/tj/svm light/svm perf.html 4svm perf learn c Cperf w 3 –b 0 train file train model
01002003004005005060708090100# selected featuresTesting accuracy ( in % ) SVM−RFEl1−SVMFGMFSmultihamming02040608010030405060708090100# selected featuresTesting accuracy ( in % ) SVM−RFEl1−SVMFGMFSmultihammingthe sorted index of selected featuresB100200300400500600700251050100150200250the sorted index of selected featuresB10020030040050025102030405060050100150200250020406080BFSmulti∆ − FSmultihamming F1Rec@2pPRBEP050100150200250−60−40−200204060BFSmulti∆ − SVMmulti∆ F1Rec@2pPRBEP0501001502002502030405060BFSmulti∆ − FSmultihamming F1Rec@2pPRBEP050100150200250−40−30−20−100102030BFSmulti∆ − SVMmulti∆ F1Rec@2pPRBEP THE MACRO AVERAGE TESTING PERFORMANCE COMPARISONS AMONG DIFFERENT METHODS . THE QUANTITIES IN THE PARENTHESES REPRESENT
WON/LOST OF THE CURRENT METHOD COMPARING WITH FS∆ multi . THE LAST COLUMN INDICATES THE AVERAGE NUMBER OF FEATURES IS ACTUALLY USED IN THE CURRENT METHOD FOR A SPECIFIC MEASURE . THE SYMBOL ’*’ INDICATES THE LEVEL OF SIGNIFICANCE AT 0.95
ACCORDING TO T TEST APPLIED TO PAIRS OF RESULTS OVER CLASSES
Table II
Dataset
Sector
News20 multi method FS∆ FShamming multi SVM∆ FS∆ FShamming multi SVM∆ multi multi multi
F1 92.07
84.99 ( 12/91)∗ 33.35 ( 1/104)∗
77.56
49.61 ( 0/20)∗ 55.53 ( 0/20)∗
Rec@2p
P RBEP
95.77
90.01 ( 0/71)∗ 95.52 ( 11/19)∗
91.21
66.32 ( 0/20)∗ 93.08 ( 16/2 )
93.25
85.54 ( 0/86)∗ 91.24 ( 11/47)∗
81.46
52.14 ( 0/20)∗ 80.83 ( 6/11 )
#selected features 7876/6589/5083
689.2 55,197
1,301 / 1,186 / 931
485.1 62,061 multi
FShamming for all of the multivariate performance measures from the figures in the left hand side . Moreover , the figures in the right hand side show that the small number of features are good for F1 measures , but poor for other measures . As the number of features increases , Rec@2p and PRBEP can approach to the results of SVM∆ multi and all curves become flat . The performance of P RBEP and Rec@2p is relatively stable when sufficient features are selected , but our method can choose very few features for fast prediction . For F1 measure , our method is consistently better than SVM∆ multi , and the results show significant improvement over all range of B . This improvement may be due to the reduction of noisy or non informative features . Furthermore , FS∆ multi can achieve better performance measures than FShamming
. multi
V . CONCLUSION
In this paper , we propose a unified feature selection framework for general loss functions . We particularly study in details for multivariate losses . To solve the resultant optimization problem , a two layer cutting plane algorithm was proposed , and the convergence analysis is presented . Experimental results show that the proposed method is comparable with FGM and SVM RFE and better than l1 models on feature selection task , and outperforms SVM for multivariate performance measures on full set of features .
REFERENCES
[ 1 ] T . Joachims , “ A support vector method for multivariate per formance measures , ” in ICML , 2005 .
[ 2 ] I . Tsochantaridis , T . Joachims , T . Hofmann , and Y . Altum , “ Large margin methods for structured and interdependent output variables , ” JMLR , vol . 6 , pp . 1453–1484 , 2005 .
[ 3 ] D . R . Musicant , V . Kumar , and A . Ozgur , “ Optimizing fmeasure with support vector machines , ” in Proceedings of the 16th International Florida Artificial Intelligence Research Society Conference , 2003 .
[ 4 ] H . Valizadengan , R . Jin , R . Zhang , and J . Mao , “ Learning to rank by optimizing ndcg measure , ” in NIPS , 2009 .
[ 5 ] T . Joachims , “ Training linear SVMs in linear time , ” in
SIGKDD , 2006 .
[ 6 ] Q . V . Le and A . Smola , “ Direct optimization of ranking measures , ” JMLR , vol . 1 , pp . 1–48 , 2007 .
[ 7 ] M . Tan , L . Wang , and I . W . Tsang , “ Learning sparse SVM for feature selection on very high dimensional datasets , ” in ICML , 2010 .
[ 8 ] Z . Xu , R . Jin , J . Ye , M . R . Lyu , and I . King , “ Non monotonic feature selection , ” in ICML , 2009 .
[ 9 ] G X Yuan , K W Chang , C J Hsieh , and C J Lin , “ A comparison of optimization methods and software for largescale l1 regularized linear classification , ” JMLR , vol . 11 , pp . 3183–3234 , 2010 .
[ 10 ] J . Weston , A . Elisseeff , and B . Scholk¨opf , “ Use of the zeronorm with linear models and kernel methods , ” JMLR , vol . 3 , pp . 1439–1461 , 2003 .
[ 11 ] A . Smalter , J . Huan , and G . Lushington , “ Feature selection in the tensor product feature space , ” in ICDM , 2009 .
[ 12 ] I . Guyou , J . Weston , S . Barnhill , and V . Vapnik , “ Gene selection for cancer classification using support vector machines , ” Machine Learning , vol . 46 , pp . 389–422 , 2002 .
[ 13 ] J . Shi , W . Yin , S . Osher , and P . Sajda , “ A fast hybrid algorithm for large scale l1 regularized logistic regression , ” JMLR , vol . 11 , pp . 713–741 , 2010 .
[ 14 ] A . Rakotomamonjy , F . R . Bach , Y . Grandvalet , and S . Canu ,
“ SimpleMKL , ” JMLR , vol . 3 , pp . 1439–1461 , 2008 .
[ 15 ] J . M . Borwein and A . S . Lewis , Convex Analysis and Non linear Optimization . Springer , 2000 .
[ 16 ] J . E . Kelley , “ The cutting plane algorithm for solving convex programs , ” Journal of the Society for Industrial and Applied Mathematics , vol . 8(4 ) , pp . 703–712 , 1960 .
[ 17 ] A . Mutapcic and S . Boyd , “ Cutting set methods for robust convex optimization with pessimizing oracles , ” Optimization Methods & Software , vol . 24 , no . 3 , p . 381?06 , 2009 .
[ 18 ] S . Sonnenburg , G . R¨atsch , C . Sch¨afer , and B . Scholk¨opf , “ Large scale multiple kernel learning , ” JMLR , vol . 7 , 2006 . [ 19 ] Z . Xu , R . Jin , I . King , and M . R . Lyu , “ An extended level method for efficient multiple kernel learning , ” in NIPS , 2008 . [ 20 ] C . H . Teo , S . Vishwanathan , A . Smola , and Q . V . Le , “ Bundle methods for regularized risk minimization , ” JMLR , pp . 311– 365 , 2010 .
[ 21 ] J . B . Hiriart Urruty and C . Lemarechal , Convex Analysis and
Minimization Algorithms . Springer Verlag , 1993 .
[ 22 ] T . Joachims , T . Finley , and C . J . Yu , “ Cutting plane training of structural SVMs , ” Machine Learning , vol . 77 , pp . 27–59 , 2009 .
[ 23 ] Y . Yue , T . Finley , F . Radlinski , and T . Joachims , “ A support vector method for optimizing average precision , ” in SIGIR , 2007 .
