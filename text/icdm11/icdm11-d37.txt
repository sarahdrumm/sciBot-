Manifold Learning and Missing Data Recovery through Unsupervised Regression
Miguel ´A . Carreira Perpi˜n´an
Zhengdong Lu
EECS , University of California , Merced
Microsoft Research Asia , Beijing http://eecsucmercededu http://researchmicrosoftcom/en us/people/zhengdol
Abstract—We propose an algorithm that , given a high dimensional dataset with missing values , achieves the distinct goals of learning a nonlinear low dimensional representation of the data ( the dimensionality reduction problem ) and reconstructing the missing high dimensional data ( the matrix completion , or imputation , problem ) . The algorithm follows the Dimensionality Reduction by Unsupervised Regression approach , where one alternately optimizes over the latent coordinates given the reconstruction and projection mappings , and vice versa ; but here we also optimize over the missing data , using an efficient , globally convergent Gauss Newton scheme . We also show how to project or reconstruct test data with missing values . We achieve impressive reconstructions while learning good latent representations in image restoration with 50 % missing pixels .
Keywords dimensionality reduction ; manifold learning ; miss ing data ; matrix completion .
In manifold learning , we observe a high dimensional dataset YD×N = ( y1 , . . . , yN ) and want to learn a lowdimensional , latent representation of it based on a small number of degrees of freedom L ≪ D ( the training problem ) . Often we also want to project a new y to a low dimensional x = F(y ) , or to map a new x to a highdimensional y = f ( x ) ( the testing problem ) . A large number of manifold learning algorithms exist , but they typically assume that the training dataset Y and the test vectors are fully observed . Yet in many applications the data available may contain missing values caused by excessive noise , sensor malfunction , nonresponse to specific questions in a survey , or other reasons . For training , sometimes it is possible to extract a sizable , complete subset ( where each yn has no missing entries ) and use a standard algorithm on it , but this still loses useful information that the partially present vectors contain . Given the effort sometimes invested in data collection , discarding partial vectors is wasteful . In other cases , keeping only the fully observed vectors may not leave sufficient data to train at all . It then becomes necessary to use all the existing data and deal with the missing values . Even if we manage to estimate mappings F , f , we cannot apply them directly to a test vector with missing entries . This situation occurs in applications such as collaborative filtering ( eg embedding a new user given some of their movie ratings in a Netflix dataset , or given some of their opinions in Opinion Space [ 1 ] ) or image retrieval ( restoring a new image with missing pixels and projecting it to latent space ) . Our goal in this paper is to propose a principled , effective nonlinear manifold learning method that deals with missing data in both training and test , and also reconstructs ( imputes ) it . We first briefly review existing work in dimensionality reduction and matrix completion , and introduce the Dimensionality Reduction by Unsupervised Regression approach without missing data ( sec . II ) . We then explain our method mDRUR ( sec . III ) , its linear version ( sec . IV ) , and its extension to test data ( sec . V ) , followed by experimental results ( sec . VI ) .
I . RELATED WORK
Many algorithms have been proposed for dimensionality reduction , but far less work exists on dimensionality reduction with missing data . Probabilistic methods provide one principled way to deal with it . Latent variable models ( LVMs ) such as factor analysis [ 2 ] or the generative topographic mapping [ 3 ] define a joint probability model of high and low dimensional variables , and the latter ones are considered as missing data during maximum likelihood estimation with the EM algorithm . In principle , this allows considering the unobserved high dimensional values as missing variables too and having EM deal with them . However , integrating ( even approximately ) over the corresponding posterior distribution for each data point is cumbersome and computationally costly . In practice , this often limits nonlinear LVMs to using Gaussian mixtures and a very small latent dimensionality . One exception is the Laplacian eigenmaps latent variable model [ 4 ] , where training amounts to a spectral problem that yields a kernel density estimate , but it assumes no missing data . In our approach , instead of expensive marginalization and conditioning operations we have far more efficient optimizations that besides decouple significantly , and using arbitrarily many latent dimensions or nonlinear mappings poses no special problem ; we demonstrate this with RBF mappings in 9D latent spaces .
In unsupervised regression , which goes back to early work in factor analysis , one takes the latent coordinates xn of each data point yn as parameters to be estimated together with the reconstruction mapping f that maps latent points to data space . These methods extend trivially to missing data , since their least squares error function takes the form n,d=1 ( ydn − fd(xn))2 so one can simply ignore the terms for which ydn is missing . Linear functions f have been used in the homogeneity analysis literature [ 5 ] , where this approach is n=1 kyn − f ( xn)k2 = PN,D minf ,XPN called “ missing data deleted ” . Nonlinear functions f have been used recently ( neural nets [ 6 ] ; Gaussian processes for collaborative filtering [ 7] ) . Although simple and faster if there are many missing values , this returns only f ( no F ) and XL×N = ( x1 , . . . , xN ) , and reconstructs each y ( or , rather , its missing values ) implicitly as f ( x ) . This has several disadvantages ( see fig . 3 ) : ( 1 ) since x has only L degrees of freedom , for complex data ( eg images ) the reconstruction y = f ( x ) can have low quality ; ( 2 ) it treats the vector output of f ( x ) inconsistently at training and testing , as some components are used ( for the missing yds ) and the rest discarded ; ( 3 ) lacking entirely the missing terms in the error function ignores useful information , as having estimates of the missing ydn provides feedback about the goodness of f — specially if domain knowledge provides a good initial Y . A final drawback even if all yn are observed [ 8 ] is that lacking F in the objective function often distorts the latent space : two nearby y points can have distant xs . These problems are caused by having f both represent the manifold and reconstruct the data . Our proposed approach addresses this in a new , principled way , by separating these two different roles : we consider the missing ys as parameters with the same status as X , f or F , and optimize over them ( note that , without F , optimizing over y makes no difference ) ; and this carries over naturally to missing values in a test y . Until recently , traditional dimensionality reduction with missing data has been unrelated with the problem of matrix completion , which arises commonly in data mining , machine learning and computer vision . Much recent work on matrix completion stems from compressed sensing , whose success assumes the signal is sparse on a certain basis , ie , a lowrank assumption of the matrix , so that its rows ( or columns ) lie on a low dimensional subspace . Our work generalizes this linear assumption and instead assumes the data live on a smooth , nonlinear manifold with low intrinsic dimensionality . Research on compressed sensing on manifolds is limited . Baraniuk and Wakin [ 9 ] point out that when the curvature of the manifold is small enough , a random projection from the data space is likely to keep the metric of the manifold with small distortion , but do not give an algorithm to recover the manifold structure ; Chen et al . [ 10 ] do give a practical algorithm that assumes the manifold to be predetermined , and cannot be applied to partially observed data . Singular value projection ( SVP ) [ 11 ] is a scalable optimization scheme recently proposed for rank minimization with affine constraints , of which low rank matrix completion is a special case . It iterates between a SVD step ( to ensure the lowrank structure ) and a data feeding step ( to incorporate the information of observed entries ) . SVP has theoretical bounds for the exact completion case , and empirically is effective on real world problems , such as MovieLens data . Wang et al . [ 12 ] propose a denoising approach that generalizes matrix completion to local low rank manifolds , but does not recover the nonlinear manifold explicitly .
II . DIMENSIONALITY REDUCTION BY UNSUPERVISED
REGRESSION ( DRUR )
Given fully observed data points YD×N , Dimensionality Reduction by Unsupervised Regression [ 8 ] , [ 13 ] minimizes the following objective function alternately over mappings f , F and latent coordinates XL×N ( where Rf ( f ) , RF(F ) are quadratic regularizers ) : min X,f ,F
Ef ( X , f ) = PN EF(X , F ) = PN
E(X , f , F ) = Ef ( X , f ) + EF(X , F ) n=1 kyn − f ( xn)k2 + λf Rf ( f ) n=1 kxn − F(yn)k2 + λFRF(F ) .
( 1 )
( 2 )
( 3 ) autoencoder PN
E can be seen as unfolding the reconstruction error of an n=1 kyn − f ( F(yn))k2 by introducing auxiliary variables X . This has several advantages : the optimization for fixed X results in two separate regressions over f and F , which are shallower architectures than the autoencoder f ◦F and thus easier to optimize ; and the search space enlarged with X allows to use a good initialization from a spectral method and facilitates escaping local optima . DRUR learns very good embeddings in practice , thanks to the bidirectional constraints imposed by f ( that does not like to map close inputs x to faraway outputs y ) and F ( that does not like to map close inputs y to faraway outputs x ) ; the value of using both mappings F , f as opposed to just one was extensively demonstrated in [ 8 ] . The mappings may be taken nonparametric or parametric . In the latter case , the optimization over X decouples over each point , so we have N optimizations over L parameters each ( instead of one over N L ) , which besides are amenable to a form of the GaussNewton method that avoids ill conditioning ; this gives an orders of magnitude speedup [ 13 ] . Scalability is paramount when reconstructing missing data in a high dimensional yspace , so we focus on the parametric version ( pDRUR ) , but our approach applies to the nonparametric one too .
III . MISSING DATA WITH DRUR ( MDRUR )
Consider now the dimensionality reduction problem where the data matrix YD×N has missing values : each column vector yn can have from 0 to D −1 missing components ( for now , ignore the case where all D components are missing , which means we do not have a vector yn at all ) . We represent the pattern of missing values in Y with a binary matrix MD×N , where mdn = 1 if ydn is observed and 0 otherwise . Since our algorithm will need initial estimates of the missing Y values , mdn = 0 will indicate that ydn is the initial value . In the equations below , we write Y0 to mean the missing entries of Y ( so Y contains Y0 and the present values ) , and likewise yn,0 for column n of Y .
Our approach to extend pDRUR to missing data consists of optimizing its objective function over the missing Y0 values too ( we use Frobenius norms for matrices throughout , input MD×N : binary indicator matrix ( 0 = missing ) input YD×N : observed values & initial missing values input XL×N : initialized by a spectral method Fit parametric mappings f : ( X , Y ) , F : ( Y , X ) repeat
Project reconstruct : for n = 1 , . . . , N
( yn,0 , xn ) = approx . Gauss Newton min . of ( 7 )
Adapt : approximately fit f : ( X , Y ) , F : ( Y , X ) until convergence return Y0 , X , F , f
Figure 1 . Missing data DRUR algorithm . and f ( X ) means f applied vectorwise to each column x ) :
E(Y0 , X , f , F ) = Ef ( Y0 , X , f ) + EF(Y0 , X , F )
Ef ( Y0 , X , f ) = kY − f ( X)k2 + λf kf k2
EF(Y0 , X , F ) = kX − F(Y)k2 + λF kFk2 .
( 4 )
( 5 )
( 6 )
The role of F is fundamental : without it ( using Ef alone ) , optimizing over Y0 yields a trivial solution Y0 = ( f ( X))0 corresponding to the “ missing data deleted ” method discussed earlier . We apply alternating optimization to this mDRUR objective function cyclically over ( X , Y0 ) → ( f , F ) ( see fig . 1 ) . Empirically we find that other orders , such as X → Y0 → ( f , F ) or Y0 → X → ( f , F ) , are consistently slower ( all converge though ) . We deal with each of the two optimization problems separately . At each step we globally rescale X ( and the RBF width of f and weights of F ) so the errors Ef and EF are always comparable .
A . Projection Reconstruction Step : wrt ( X , Y0 )
If optimizing over ( X , Y0 ) ( also over X or Y0 separately ) , eq . ( 4 ) decouples so there are N independent optimizations each over at most D + L variables ( xn , yn,0 ) . This is crucial with some of our applications , eg where yn is a high dimensional image with missing entries . Consider one such term ( omitting the subindex n for simplicity ) :
E(x , y0 ) = ky − f ( x)k2 + kx − F(y)k2
( 7 ) where x ∈ RL , y ∈ RD , L < D , and we partition y as y = ( y0 y1 ) ( possibly reordering indices ) so that y1 is constant ( the present components ) , and y0 is a variable to be optimized over ( the missing components ) . We can then obtain the gradient and Hessian of E wrt ( x , y0 ) as follows :
( 8 )
( 9 )
( 10 )
( 11 )
( 12 )
∂E
∂E ∂y0 ∂ 2E
∂x = 2,−JT = 2,−JT ∂x2 = 2,I + JT = 2,I + JT = −2,JT
∂ 2E ∂y2 ∂ 2E
0 f ( y − f ( x ) ) + x − F(y)F,0(x − F(y ) ) + y0 − ( f ( x))0f ( y − f ( x))f Jf − HT F,0(x − F(y))F,0JF,0 − HT f ,0 + JF,0f ,0 JT
∂x∂y0 where we partition the matrices ( Jacobians of f wrt x , and of F wrt y ) as Jf = ( JT f ,1)T and JF = ( JF,0 JF,1 ) , and other D × 1 vectors are partitioned like y . The leastsquares form of the problem suggests using a Gauss Newton method ( far more effective than gradient descent ) if the dimensionality of y0 is not too large . We capitalize on the positive definite Hessian approximation eH = 2I +
JT f Jf
−Jf ,0−JT
−JT F,0 JT f ,0
−JF,0
F,0JF,0
( 13 ) so the search direction p for optimizing over ( x , y0 ) is the solution of the non sparse linear system eHp = −g ( where g is the gradient ) . In a nonlinear least squares problem of the typical form minx ky − f ( x)k2 ( ie , using Ef alone , no F ) , the Gauss Newton method has the disadvantage that the approximate Hessian of f has the form 2JT f Jf , and thus can become singular or ill conditioned , leading to unstable directions and steps . One then needs to correct for this , eg by adding an adaptive bias to its diagonal , as in the Levenberg Marquardt method [ 14 ] ; searching for a good bias involves solving multiple linear systems at each iteration . In our case , the F term kx − F(y)k2 , quadratic on x , introduces an additive Jacobian 2I that acts as an in built regularizer . The same applies to the Jacobian wrt y0 . This means our Gauss Newton direction is robust without the need of any modifications ; with usual line search conditions and a bounded Jacobian , global convergence is assured [ 14 , p . 40 ] . See section III C for the computational cost .
B . Adaptation Step : wrt ( f , F )
This is as with DRUR , ie , solving two separate regressions for f and F , resp . Take the one for f . We have used two parametric forms : linear mappings f ( x ) = Ax + a , and radial basis function ( RBF ) networks f ( x ) = WΦ(x ) with M ≪ N Gaussian RBFs φm(x ) = exp(− 1 2 k(x − µm)/σk2 ) . For both , our regularizer is a quadratic penalty on the weights : kf k2 = kAk2 ( linear ) , kWk2 ( RBF ) . In RBFs we update the centers µm by k means , and σ by cross validation ( training the RBF on a subset of the data and testing it on the rest ) . The weights and biases have a unique solution given by a linear system , with a computational cost O(N M ( M + D ) ) in training time and O(M D ) in memory for the RBF . The gradient of f wrt x ( Jacobian J ) , needed in the optimization over ( Y0 , X ) , is J(x ) = 1 m=1 wmφm(x)(µm − x)T .
σ2 PM
C . Practical Considerations
Initialization : As with any nonlinear method , mDRUR ’s user parameters are those parameters for the mappings ( eg number of basis functions M and regularization λ ≥ 0 ) that would be needed in a standard regression . local optima exist , but they are a fair price to pay for the higher quality achievable . When no domain knowledge is available , we have obtained best results by initializing Y0 with SVP [ 11 ] with a large rank ( or with regularized linear mDRUR ; both achieve very similar results ) ; this beats using zero or mean imputation . Given Y0 , we initialize X using a spectral method applied to Y . Experimentally , we note mDRUR is very good at improving an initial solution , and that even if ( f , F ) get stuck in a poor local optimum , the reconstructed Y0 can still be quite good .
Very high dimension of the missing values : Call Dn = dim yn,0 the number of missing variables in yn for each n = 1 , . . . , N . In some applications , such as image reconstruction , matrix completion or recommender systems , Dn can be large . While mDRUR ’s formulation is very efficient—each yn decouples from all others—solving ( or even storing the relevant matrices of ) the linear system in each linear mDRUR or Gauss Newton step becomes expensive , with a cost O((L + Dn)3 ) . We can then use the following strategies . A simple but slow option is ( stochastic ) gradient descent . More complicated options are GaussNewton CG ( which solves the linear systems approximately with the conjugate gradient method ) and limited memory BFGS ( which approximates the Hessian using a relatively small number of column vectors ) . A much simpler strategy that we have found useful is to use an approximate , fast step where we ignore the F term in the error and thus set yn,0 = ( f ( xn))0 . If this decreases the DRUR error , we accept the step and optimize the error over xn ; otherwise , we reject it and do the usual optimization over ( yn0 , xn ) . This guarantees we decrease the error at each iteration , and is much faster with large Dn . Finally , in matrix completion problems where there may be no a priori preference over Y or YT , we should choose the alternative with fewer rows , so the cost of each step is the smaller of the two .
D . Relation with Low Rank Factorization
If the mappings f , F can be written as a linear combination of basis functions , as is the case for linear mappings and RBF networks , then ( absorbing the respective biases into Y and X ) the terms kY − f ( X)k2 = kY − Wf Φf ( X)k2 and kX − F(Y)k2 = kX − WFΦF(Y)k2 are SVD like . At convergence we obtain two approximate low rank factorizations Y ≈ Wf Φf ( X ) = f ( X ) and X ≈ WFΦF(Y ) = F(Y ) with Wf of D × Mf and Φf ( X ) of Mf × N , WF of L × MF and ΦF(Y ) of MF × N , and the ranks are given by ( strictly , smaller or equal than ) the numbers of basis functions Mf and MF , respectively ( Mf , MF ≪ N ) .
IV . LINEAR MDRUR
With linear mappings f ( x ) = Ax + a , F(y ) = By + b , the mDRUR objective function is E(Y0 , X , A , a , B , b ) = kY − AX − a1T k2 + λf kAk2 ( 14 )
+ kX − BY − b1T k2 + λF kBk2 where 1 is a column vector of N ones . If we partition matrices as A = , A0
A1 and B = ( B0 B1 ) , and vector a like y , the gradient wrt x and y0 ( for a given n ) is :
∂E
∂x = 2,−AT ( y − Ax − a ) + x − By − b
0 ( x − By − b ) + y0 − A0x − a0
= 2,−BT
∂E ∂y0
( 15 )
( 16 ) so equating it to zero yields a linear system for the optimum ( or other stationary points ) :
I+ AT A −AT
−A0−BT
0 BT
0 B0 ( x y0 ) = − AT a−b−(AT
0 B1y1+BT
1 +B1)y1
0 b−a0
−B0
BT
0 which is analogous to the Gauss Newton linear system , and non sparse as well . This yields the projection reconstruction step . As for the adaptation step , the linear regressions for f and F yield
A = ( Y − a1T )XT ( XXT + λf I)−1 , a = y − Ax B = ( X − b1T )YT ( YYT + λFI)−1 , b = x − By where x = X1/N and y = Y1/N . Both steps define a globally convergent fixed point iteration . With no missing data and no regularization , the global minimum of E is PCA : A = BT = UL ( leading eigenvectors of the covariance LY − x1T . matrix of Y ) , a = y , b = x , and X = UT With missing data , E has local optima ; adding regularization seems to reduce their effect .
V . PROJECTING AND RECONSTRUCTING TEST DATA
WITH MISSING VALUES
Given a test point y ∈ RD with missing values , we propose to reconstruct y and project it as F(y ) as follows . We consider an mDRUR training problem with augmented matrices ( Y y ) and ( X x ) , minimizing over x and y0 and keeping everything else fixed ( Y , y1 , X , f , F ) . This way , the trained model does not change , but we do constrain x and y0 to conform to it . This results in minimizing the error function ( 7 ) ( and we do not need Y and X , just the mappings f and F ) , and is thus formally identical to the projectreconstruct step over the nth point ( xn , yn,0 ) . Hence , we can use the same algorithm : Gauss Newton for RBFs , or a single linear system for linear mappings ( with the computational cost described earlier ) . We initialize ( x , y0 ) to the pair ( xn , yn ) for which yn is closest to y in the present variables ( in some applications the user may provide better initial values based on prior information ) . After minimizing ( 7 ) , we discard x and return y as the reconstructed input , and F(y ) as its low dimensional projection . If there are many missing values , then ( 7 ) may have multiple minima corresponding to genuinely correct reconstructions of y ; which one is found depends on the initial pair . The approach carries over to the case where we are given a latent point x with missing values , but this is practically less useful . This approach is consistent with the existing mDRUR model in that , if the test point coincides with one of the training points ( and has the same missing values ) then its reconstruction , projection and x value coincide with those found during training ( as follows from the fact that the error function was the same in both cases ) . Our approach seamlessly accomodates arbitrary patterns of missing values .
3
2
1
0
−1
−2
−3
4
3
2
1
0
−1
−2
−3
4
2
4
2
0
−2
−4
−4
2
0
−2
0
−2
0
−2
−4
−4
4
2
Figure 2 . black + , SVP : red ◦ , mDRUR : blue ◦ ) .
100D trefoil data : recovered Y mapped back to 3D ( original :
VI . EXPERIMENTS
We apply mDRUR on missing data tasks with different scale , dimensionality , and characteristics . We assume the observed entries are uniformly distributed across Y . While this may not represent well certain matrix completion tasks with other distributions ( eg power law [ 15] ) , it serves as a fair test bed for the various manifold learning algorithms we try . These are : the low rank matrix completion model SVP , as linear baseline ; the nonlinear PCA ( nlPCA ) of [ 6 ] ( using their code ) , which is an unsupervised regression method using only f ( a neural net ) ; and mDRUR ( with Y initialized from SVP ) . The results for linear mDRUR ( not shown ) were very similar to those of SVP , and consistently worse than mDRUR ’s over different proportions of missing data . We always report the result with best parameter choices for competing models ( SVP : rank ; nlPCA : number of latent variables and hidden nodes ) . We report the reconstruction error wrt the ground truth kYrec − Yk , where Yrec is the Y reconstructed from partial observations .
A . Synthetic Example : 100D Trefoil Data
The original trefoil dataset has 377 points in 3D . We first embed them in a 100D space through a random linear mapping and obtain Y100×377 , then we randomly select 7 % entries from Y as observed ( 93 % missing ) . For mDRUR , we use the first L = 3 principal components of the SVP reconstruction as initial X , Mf = 50 , MF = 10 , λf = λF = 002 Fig 2 shows each method ’s resulting Y mapped back to the original 3D space . Besides learning the nonlinear manifold ( X not shown ) , mDRUR ’s reconstruction drastically denoises the wandering points produced by SVP back to the 1D trefoil manifold . This is consistent with the reconstruction error : SVP 15.25 , mDRUR 660
B . MNIST Digit ‘7’
We consider all the 6 265 ‘7 ’s in the MNIST dataset , and declare as missing 50 % pixels selected uniformly . We rearrange each 28×28 image into a 784D vector ( destroying their 2D structure ) , yielding high dimensional data Y784×5000 for training and 1 265 testing images ; each ydn value is a grayscale in [ 0 , 256 ] . Methods : SVP : rank 18 ; nlPCA : neural net , 12 × 10 × 784 units ( ie , L = 12 ) ; mDRUR : L = 9 , Mf = 1 000 , λf = 0.01 , MF = 50 , λF = 0.1 , initial Y from SVP , initial X from Laplacian eigenmaps on Y . Unlike the trefoil data , where we know beforehand that the manifold is 1D and closed , the MNIST data might have several disconnected manifolds with different intrinsic dimensionality . Still , as shown in fig . 3 , we achieve a fairly good image restoration by exploiting only the manifold structure of the data ( since we removed the 2D image structure ) . Visually SVP and nlPCA do a decent job , but mDRUR is the clear winner , consistent with the error : SVP 51 000 , nlPCA 97 000 , mDRUR 45 000 . Restoring a test image with mDRUR takes 0.48 s in a modern PC ( of which 25 % is spent in finding the initial ( xn , yn) ) , with an average pixel error of 21 ± 56 grayscales ( over 256 ) . Again , the fact that the reconstructed images look like well formed 7s ( note in the bottom panel of fig . 3 , second row , that some unusual strokes that were missing have been removed in the reconstruction ) suggests that mDRUR has captured fundamental aspects of the manifold . This experiment also demonstrates the power of reconstructing y0 by making it a free parameter separate from f and F in mDRUR , rather than being the output of f ( without F ) as in nlPCA : the latter reconstructions are visibly blurred and display artifacts . After all , it is impossible to reconstruct very accurately an arbitrary 784D ‘7’ image from only 9 or 12 degrees of freedom . The role of f is to smooth , not interpolate , the ( noisy ) data .
In a similar experiment with a more obvious low dimensional manifold structure , we created rotated versions of an MNIST digit ’3’ every 4 degrees ( a closed 1D nonlinear manifold in 784D ) and made 40 % pixels randomly missing . The errors for SVP ( rank 6 ) , nlPCA ( L = 2 ) and mDRUR ( L = 2 ) were 29.1 , 34.9 and 20.8 , respectively .
VII . CONCLUSION
We have shown how to learn a Dimensionality Reduction by Unsupervised Regression model for high dimensional data with missing values , where knowledge about the highdimensional data constrains the low dimensional representation , and vice versa . By introducing parameters ( f , F ) and Y0 , we separate two distinct roles that previous algorithms conflated into one : learning a nonlinear , low dimensional representation of the data , and recovering the missing data . This idea applies in a unified way to training and testing , and to arbitrary patterns of missing data . Our work also provides a new approach to the problem of matrix completion , by reconstructing the data subject to implicit , local low rank and smoothness constraints . mDRUR improves over other dimensionality reduction and matrix completion methods , and performs well with large proportions of missing data , even if few ( or any ) vectors are complete . Nonlinear manifold learning with missing data is a difficult and overlooked problem . Our improved results make a case for going beyond linear and spectral methods .
ACKNOWLEDGMENTS
Work supported by NSF CAREER award IIS–0754089 . original masked
SVP nlPCA y = f ( x ) nlPCA y0 = ( f ( x))0 mDRUR y = f ( x ) mDRUR y0 original masked mDRUR original masked mDRUR original masked mDRUR original masked mDRUR
Figure 3 . Restoring MNIST digits ‘7’ . Upper panel : 3 training images for all methods . Lower panel : 8 test images with mDRUR . “ Masked ” shows the 50 % missing pixels in red ( you may need to zoom in ) . Grayscales clipped to [ 0 , 256 ] . “ y = f ( x ) ” means reconstructing all pixels ( missing or not ) with the output of f . “ y0 = ( f ( x))0 ” means reconstructing with the output of f only the missing pixels and leaving the rest to their present values . For mDRUR , “ y0 ” means reconstructing only the missing pixels with the free y0 parameters .
REFERENCES
[ 1 ] S . Faridani , E . Bitton , K . Ryokai , and K . Goldberg , “ Opinion space : A scalable tool for browsing online comments , ” in CHI , 2010 , pp . 1175–1184 .
[ 2 ] B . S . Everitt , An Introduction to Latent Variable Models ,
Chapman & Hall , 1984 .
[ 3 ] C . M . Bishop , M . Svens´en , and C . K . I . Williams , “ GTM : The generative topographic mapping , ” Neural Computation , vol . 10 , no . 1 , pp . 215–234 , Jan . 1998 .
[ 4 ] M . ´A . Carreira Perpi˜n´an and Z . Lu , “ The Laplacian Eigenmaps Latent Variable Model , ” in AISTATS , 2007 , pp . 59–66 .
[ 5 ] A . Gifi , Nonlinear Multivariate Analysis . Wiley , 1990 .
[ 6 ] M . Scholz , F . Kaplan , C . L . Guy , J . Kopka , and J . Selbig , “ Non linear PCA : A missing data approach , ” Bioinformatics , vol . 21 , no . 20 , pp . 3887–3895 , Oct . 15 2005 .
[ 8 ] M . ´A . Carreira Perpi˜n´an and Z . Lu , “ Dimensionality reduc tion by unsupervised regression , ” in CVPR , 2008 .
[ 9 ] R . G . Baraniuk and M . B . Wakin , “ Random projections of smooth manifolds , ” Foundations of Computational Mathematics , vol . 9 , no . 1 , pp . 51–77 , Feb . 2009 .
[ 10 ] M . Chen , J . Silva , J . Paisley , C . Wang , D . Dunson , and L . Carin , “ Compressive sensing on manifolds using a nonparametric mixture of factor analyzers : Algorithm and performance bounds , ” IEEE Trans . Sig . Proc . , vol . 58 , 2010 .
[ 11 ] P . Jain , R . Meka , and I . S . Dhillon , “ Guaranteed rank mini mization via singular value projection , ” in NIPS , 2010 .
[ 12 ] W . Wang , M . ´A . Carreira Perpi˜n´an , and Z . Lu , “ A denoising view of matrix completion , ” in NIPS , 2011 .
[ 13 ] M . ´A . Carreira Perpi˜n´an and Z . Lu , “ Parametric dimensionality reduction by unsupervised regression , ” in CVPR , 2010 .
[ 14 ] J . Nocedal and S . J . Wright , Numerical Optimization , 2nd ed . ,
Springer Verlag , 2006 .
[ 7 ] N . D . Lawrence and R . Urtasun , “ Non linear matrix factor
[ 15 ] R . Meka , P . Jain , and I . Dhillon , “ Matrix completion from ization with Gaussian processes , ” in ICML , 2009 . power law distributed samples , ” in NIPS , 2009 .
