2011 11th IEEE International Conference on Data Mining
A New Multi Task Learning Method for Personalized Activity Recognition
Xu Sun∗† , Hisashi Kashima∗ , Ryota Tomioka∗ , Naonori Ueda‡ and Ping Li† ∗Department of Mathematical Informatics , The University of Tokyo , Tokyo , JP
†Department of Statistical Science , Cornell University , Ithaca , NY , USA
‡NTT Communication Science Laboratories , Kyoto , JP xusun@cornell.edu kashima@mistiu tokyoacjp tomioka@mistiu tokyoacjp ueda@cslabkeclnttcojp pingli@cornell.edu
Abstract—Personalized activity recognition usually faces the problem of data sparseness . We aim at improving accuracy of personalized activity recognition by incorporating the information from other persons . We propose a new online multi task learning method for personalized activity recognition . The proposed online multi task learning method automatically learns the “ transfer factors ” ( similarities ) among different tasks ( ie , among different persons in our case ) . Experiments demonstrate that the proposed method significantly outperforms existing methods . The novelty of this paper is twofold : ( 1 ) A new multitask learning framework , which can naturally learn similarities among tasks ; ( 2 ) To our knowledge , this is the first study of large scale personalized activity recognition .
PRIOR ACCELEROMETER BASED ACTIVITY RECOGNITION STUDIES .
Table I
Bao [ 1 ] Ravi [ 3 ] P¨arkk¨a [ 2 ] Huynh [ 5 ] Sun [ 6 ] This Work
#Persons
20 2 16 1 ≤ 20 20
Models
DTs
DTs , SVMs
DTs
Bayesian LTM CRFs , LCRFs
Multi Task Learner
Continuous
× × × √ √ √
Personalize
Limited
× × × × √
II . RELATED WORK AND MOTIVATIONS
I . INTRODUCTION
A . Activity Recognition
Although there was a considerable literature on sensor based activity recognition , most of the prior work discussed activity recognition in predefined limited environments [ 1 ] , [ 2 ] , [ 3 ] . For example , most of the prior work assumed the beginning and ending time(s ) of each activity are known beforehand , and the constructed recognition system only need to perform simple classifications of activities [ 1 ] , [ 2 ] , [ 3 ] . However , the case for real life activity sequences , in which the boundaries of the activities are unknown beforehand [ 4 ] . this is not
More importantly , to the best of our knowledge , there is no previous work that systematically studied personalized activity recognition . Because of the difficulty of collecting training data for activity recognition , most of the prior work simply merge all personal data for training . We will show in our experiments that simply merging the personal data for training an activity recognizer will result in weak performance . Due to the fact that different persons usually have very different activity patterns , it is natural to construct personalized activity recognizers ( for different persons ) . However , the new problem is the data sparseness of personalized activity recognition , because usually each person only has very limited amount of labeled training data . To realize personalized learning in activity recognition , we exploit multi task learning where each task corresponds to a specific person in activity recognition . We will propose an online multi task learning method for personalized and continuous activity recognition .
Most of the prior work on activity recognition treated the task as a single label classification problem [ 1 ] , [ 2 ] , [ 3 ] . Given a sequence of sensor signals , the activity recognition system predicts a single label ( representing a type of activity ) for the whole sequence . Ravi et al . [ 3 ] used decision trees ( DTs ) , support vector machines ( SVMs ) and K nearest neighbors ( KNNs ) models for classification . Bao and Intille [ 1 ] and P¨arkk¨a et al . [ 2 ] used decision trees for classification . A few other works treated the task as a structured classification problem . Huynh et al . [ 5 ] tried to discover latent activity patterns by using a Bayesian latent topic model ( Bayesian LTM ) . Most recently , Sun et al . [ 6 ] , [ 4 ] used conditional random fields ( CRFs ) and latent conditional random fields ( LCRFs ) for activity recognition . To our knowledge , there is only very limited work on the study of personalized activity recognition . A major reason is that most of the previous studies contain only a few participants . The limited number of participants is inadequate for a reliable study of personalized activity recognition . For example , in Ravi et al . [ 3 ] , the data was collected from two persons . In Huynh et al . [ 5 ] , the data was collected from only one person . In P¨arkk¨a et al . [ 2 ] , the data was collected from 16 persons . Because of the difficulty of collecting training data , most of the prior work simply merge all personalized data for training . Personalized activity recognition and how to solve data sparseness in personalized activity recognition were not adequately studied . Table I summarizes prior work on activity recognition .
1550 4786/11 $26.00 © 2011 IEEE DOI 101109/ICDM201114
1218
B . Conditional Random Fields
Conditional random fields ( CRFs ) are very popular models for structured classification [ 7 ] . Assuming a feature function that maps a pair of observation sequence x and label sequence y to a global feature vector f , the probability of a label sequence y conditioned on the observation sequence x is modeled as follows [ 7 ] :
.
.
.
'
P ( y|x , w ) = exp w ∀y . exp f ( y , x ) . w f ( y fi ' , x ) fi ,
( 1 ) where w is a parameter vector .
Given a training set consisting of n labeled sequences , ( xi , yi ) , for i = 1 . . . n , parameter estimation is performed by maximizing an objective function . For simplicity , we denote log P ( yi|xi , w ) as .(i , w ) . The final objective function is as follows : nff
L(w ) =
.(i , w ) − ||w||2
2σ2
.
( 2 )
C . Stochastic Gradient Descent i=1
To speed up the training , people turn to online training methods . A representative online training method is the stochastic gradient descent ( SGD ) [ 8 ] . Suppose ˆS is a randomly drawn subset of the full training set S , the stochastic objective function is then given by
Lstoch(w , ˆS ) =
.(i , w ) − | ˆS| |S|
||w||2 2σ2
. ff i∈S
The extreme case is a batch size of 1 , and it gives the maximum frequency of updates , which we adopt in this work . In this case , | ˆS| = 1 and |S| = n ( suppose the full training set contains n samples ) . In this case , we have
Lstoch(w , ˆS ) = .(i , w ) − 1 n
( 3 ) where ˆS = {i} . The model parameters are updated in such a way :
, wk+1 = wk + γk∇wk
Lstoch(w , ˆS ) ,
( 4 )
||w||2 2σ2 where k is the update counter , γk is the learning rate [ 6 ] , [ 4 ] .
D . Multi Task Learning
There was quite limited study on systematically combining online learning with multi task learning . The existing multi tasking learning methods are mainly focused on matrix regularization ( eg , [ 9 ] , [ 10] ) , and online learning is not well studied in such settings . Two recent studies considered online learning in multi task setting [ 11 ] , [ 12 ] . Our multitask learning proposal will be substantially different from them . While Yang et al . [ 12 ] focused on multi task feature selection and Agarwal et al . [ 11 ] focused on online matrix regularization , our proposal relates to neither feature selection nor matrix regularization . We will propose a tighter combination of online learning and multi task learning , with a new objective function and a novel training method .
III . A NEW MULTI TASK LEARNING FRAMEWORK In this section , we introduce the multi task learning framework . For every positive integer q , we define Nq = {1 , . . . , q} . Let T be the number of tasks ( number of persons in activity recognition ) which we want to simultaneously learn . For each task t ∈ NT , there are n data examples {(xt,i , yt,i ) : i ∈ Nn} available . In practice , the number of examples per task may vary but we have kept it constant for simplicity of notation . We use D to denote the n × T matrix whose t th column is given by the vector dt of data examples .
A . Model
Our goal is to learn the vectors w1 , . . . , wT from the data D . For denotational simplicity , we assume that each of the weight vectors is of the same size f ( feature dimension ) , and corresponds to the same ordering of features . We use W to denote the f × T matrix whose t th column is given by the vector wt . We learn W by maximizing1 the objective function ,
Obj(W , D ) . Likelihood(W , D ) − R(W ) ,
( 5 ) where Likelihood(W , D ) is the averaged likelihood on the tasks , namely , ff
Likelihood(W , D ) = t∈NT and L(wt , D ) is defined as follows :
L(wt , D ) .
αt,t.L(wt , dt . )
.
L(wt , D ) , fi
( 6 )
( 7 ) ff
. t.∈NT
αt,t . is a real valued transfer factor between two tasks , with αt,t . = αt.,t ( symmetric ) . Intuitively , a transfer factor αt,t . measures the similarity between the t th task and the t' th task . For example , in activity recognition , αt,t . estimates the similarity of the activity patterns between the person t and the person t' . L(wt , dt . ) is defined as follows :
L(wt , dt . ) .
= log P ( yt,i|xt,i , wt )
t ( i , wt ) ,
( 8 ) ff ff i∈Nn i∈Nn where P ( · ) is a prescribed probability function . In this paper , we use the CRF probability function , Eq ( 1 ) . The second step is just a simplified denotation by defining t ( i , wt ) . log P ( yt,i|xt,i , wt ) .
1Maximization is only for simplicity of presentation . Actually , we mini mize the − log of the objective function .
1219
∗
Algorithm Learning with fixed transfer factors ( OMTF ) Input : W ← 0 , D , A for t ← 1 to T for 1 to convergence for 1 to n . . gt ← − 1 n∇wt . for t' ← 1 to T . . . Draw i ∈ Nn uniformly at random . . . gt ← gt + A . . . wt ← wt + γgt . ∗ .
Output : ∀t , wt converges to w W
∗ t ; ie , W converges to t,t.∇wt t ( i , wt ) ∗
||wt||2 2σ2 t
Algorithm Learning with unknown transfer factors ( OMT ) Input : W ← 0 , D , A ← 0 for 1 to convergence . W ← OMT F(W , D , A ) for t ← 1 to T . . . . . for t' ← 1 to T . Update At,t . using Eq ( 15 ) or Eq ( 14 ) Output : A empirically converges to ˆA . W converges to ˆW .
Figure 1 . Online multi task learning algorithms ( using batch size of 1 ) . The derivation of 1 n before the regularization term was explained in Eq ( 3 ) .
Finally , R(W ) is a regularization term for dealing with overfitting . In this paper , we simply use L2 regularization : ff t∈NT
R(W ) =
||wt||2 2σ2 t
.
( 9 )
To summarize , our multi task learning objective function is as follows :
Obj(W , D ) =
. fi t,t.∈NT
αt,t .
. i∈Nn fit . ( i , wt )
'
−
. t∈NT
||wt||2 2σ2 t
.
To simplify denotation , we introduce a T × T matrix A , such that At,t . . αt,t We also introduce a T ×T functional matrix Φ , such that Φt,t . . L(wt , dt . ) . Then , the objective function can be compactly expressed as follows : ||wt||2 2σ2 t
Obj(W , D ) = tr(AΦ ff
) −
( 10 )
.
, t∈NT where tr means trace . In the following content , we will first discussion a simple case that the transfer factor matrix A is fixed . After that , we will focus on the case that A is unknown .
1220
B . Learning with Fixed Transfer Factors
With fixed transfer factors , the optimization problem is as follows : ff t∈NT
||wt||2 2σ2 t
.
( 11 )
∗
W
= argmax
W
∗
.
) −
Φ tr(A
It is clear to see that we can independently optimize wt and wt . when t = t' . In other words , we can independently optimize each column of W , and therefore derive the ∗ . For wt ( ie , the t’th column optimal weight matrix W of W ) , its optimal form is :
∗ t = argmax w wt
ψ(wt , D ) ,
( 12 ) where ψ(wt , D ) has the form as follows :
ψ(wt , D ) = t,t.L(wt , dt . ) α∗
)
( ff t.∈NT
− ||wt||2 2σ2 t
.
( 13 )
This optimization problem is a cost sensitive optimization problem . We present a cost sensitive online training algorithm , called online multi task learning with fixed transferfactors ( OMT F ) , for this optimization . The OMT F algorithm is shown in Figure 1 .
Given certain conditions , we can theoretically show that the parameters W produced by the OMT F online learning ∗ of Eq algorithm are convergent towards the maximum W ( 10 ) . For saving space , we omit the details of convergence analysis . We can also see the convergence of the proposed method in the section of experiments .
C . Learning with Unknown Transfer factors
For many practical applications , the transfer factors are hidden variables that are unknown . To solve this problem , we present a heuristic learning algorithm , called OMT , to learn transfer factors and model weights in alternating optimization ( see the bottom of Figure 1 ) . Here , the OMTF algorithm is employed as a subroutine . In the beginning , model weights W and transfer factors A are initialized by 0 matrix . W is then optimized to ˆW by using the OMT F algorithm , based on the fixed A . Then , in an alternative way , A is updated based on the optimized weights ˆW . After that , W are optimized again based on updated ( and fixed ) transfer factors . This iterative process continues until empirical convergence of A and W .
In updating transfer factors A based on W , a natural idea is to estimate a transfer factor αt,t . based on the similarity between weight vectors , wt and wt . The similarity between weight vectors can be calculated by using kernels , including the popular Gaussian RBF and polynomial kernels . We can define Gaussian RBF kernel to estimate similarity between two tasks :
αt,t . . 1
C exp(−||wt − wt.||2
2σ2
) ,
( 14 )
Table II
FEATURES USED IN THE ACTIVITY RECOGNITION TASK . A × B MEANS
A CARTESIAN PRODUCT BETWEEN TWO SETS ; i REPRESENTS THE WINDOW INDEX ; yi AND yi−1yi REPRESENTS CRF LABEL AND
LABEL TRANSITION . SINCE THE SINGLE AXIS BASED FEATURES ON
THE THREE AXES ARE EXTRACTED IN THE SAME WAY , FOR SIMPLICITY ,
WE ONLY DESCRIBE THE FEATURES ON ONE AXIS . FOR MULTI AXIS BASED FEATURES , WE USE 1 , 2 , AND 3 TO INDEX/REPRESENT THE
THREE AXES .
Single axis based features : ( 1 ) Signal strength features : {si−2 , si−1 , si , si+1 , si+2 , si−1si , sisi+1} ×{yi , yi−1yi} ( 2 ) Mean feature : mi ×{yi , yi−1yi} ( 3 ) Standard deviation feature : di ×{yi , yi−1yi} ( 4 ) Energy feature : ei ×{yi , yi−1yi} Multi axis based features : ( 1 ) Correlation features : {c1,2,i , c2,3,i , c1,3,i} ×{yi , yi−1yi} where C is a real valued constant for tuning the magnitude of transfer factors . Intuitively , a big C will result in “ weak multi tasking ” and a small C will make “ strong multitasking ” . σ is used to control the variance of a Gaussian RBF function . Alternatively , we can use polynomial kernel ( normalized ) to estimate similarities between tasks : wt , wt.(d
αt,t . . 1 C
,
||wt||d · ||wt.||d
( 15 ) where wt , wt.( means inner product between the two . t wt. ) ; d is the degree of the polynomial vectors ( ie , w kernel ; ||wt||d · ||wt.||d is the normalizer ; C is a real value constant for controlling the magnitude of transfer factors . Actually the normalized polynomial kernel is natural and easy to understand . For simplicity , we typically set d = 1 . In preliminary experiments , we find the polynomial kernel works better ( more robust ) than the RBF kernel . Hence , we will focus on the polynomial kernel in the experiments .
D . Accelerated OMT Learning
The OMT learning algorithm can be further accelerated using more frequent update of the transfer factors , A . The naive OMT learning algorithm waits for the convergence of the model weights W ( in the OMT F step ) before updating the transfer factors A . In practice , we can update transferfactors A before the convergence of the model weights W . For example , we can update transfer factors A after running only one iteration of the OMT F algorithm . This can bring a much faster empirical convergence of the OMT learning . We will adopt this accelerated version of the OMT learning for experiments . In the experiment section , we will compare the ( accelerated ) OMT method with a variety of strong baseline methods .
IV . EXPERIMENTS ON ALKAN DATA
We use the ALKAN dataset [ 13 ] for experiments . This dataset contains 2,061 sessions , with totally 3,899,155 samples ( in a temporal sequence ) . The data was collected
Table III
RESULTS ON THE DATA OF 5 PERSONS , 10 PERSONS , AND 20 PERSONS . OMT IS THE PROPOSED METHOD . SGD Single IS THE PERSONALIZED
SGD TRAINING ; SGD Merged IS THE MERGED SGD TRAINING .
#Person = 5 SGD Merged SGD Single OMT , C=80 ( prop . ) #Person = 10 SGD Merged SGD Single OMT , C=40 ( prop . ) #Person = 20 SGD Merged SGD Single OMT ( prop . )
Ov . Accuracy ( St . Deviation ) 57.65 ( ±1.06 ) 68.19 ( ±0.19 ) 69.84 ( ±0.86 ) Ov . Accuracy ( St . Deviation ) 63.25 ( ±0.16 ) 68.34 ( ±0.25 ) 72.80 ( ±0.61 ) Ov . Accuracy ( St . Deviation ) 62.53 ( ±1.12 ) 62.46 ( ±0.56 ) 63.90 ( ±0.40 ) by iPod accelerometers with the sampling frequency of 20HZ . A sample contains 4 values : time stamp and triaxial singals . For example , {539.266(s ) , 0.091(g ) , 0.145(g ) , 1.051(g)} 2 . There are five kinds of activity labels : act 0 means “ walking/running ” , act 1 means “ on elevator/escalator ” , act 2 means “ taking car/bus/train ” , act 3 means “ standing/sitting/discussing/at dinner ” , and act 4 means “ other ( more trivial ) activities ( eg , dressing ) ” .
We randomly selected 85 % of samples for training , 5 % samples for tuning hyper parameters ( development data ) , and the rest 10 % samples for testing . Following [ 6 ] , the evaluation metric are sample accuracy ( % ) ( the number of correctly predicted samples divided by the total number of samples ) . We also considered other evaluation metrics , like precision and recall , in preliminary experiments . However , we found precision and recall tended to be misleading in this task , because an activity segment is very long ( typically contains thousands of time windows ) , and small difference on the boundaries of segments can cause very different precision and recall . On the other hand , the accuracy metric is much more reliable in this scenario .
A . Feature Engineering
Following prior work in activity recognition [ 1 ] , [ 2 ] , [ 3 ] , [ 5 ] , we use acceleration features , mean features , standard deviation , energy , and correlation features ( see Table II ) . We denote the window index as i . The mean feature is simply .|w| k=1 sk the averaged signal strength in a window : mi = , |w| where s1 , s2 , . . . are the signal magnitudes in a window . The . The deviation energy feature is defined as ei = .|w|
.|w| k=1 s2 |w| feature is defined as di = , where the mi is the mean value defined before . The correlation feature is defined as c1,2,i = , where the d1,i and d2,i are the deviation values on the i’th window of the axis 1 and the covariance1,2,i d1,id2,i k=1(sk−mi)2
|w| k
2In the example , ‘g’ is the acceleration of gravity .
1221
Overall Performance ( 5 Persons )
Multi−Task ( 5 Persons )
SGD−Single ( 5 Persons )
SGD−Merged ( 5 Persons )
70
65
60
)
%
( y c a r u c c A
55
0
Multi−Task
SGD−Single
SGD−Merged
100
80
60
40
)
%
( y c a r u c c A
100
80
60
40
)
%
( y c a r u c c A
100
80
60
40
)
%
( y c a r u c c A
50
100
0
50
100
0
50
100
0
50
100
Overall Performance ( 10 Persons ) 75
100
Multi−Task ( 10 Persons )
SGD−Single ( 10 Persons )
SGD−Merged ( 10 Persons )
100
80
60
40
)
%
( y c a r u c c A
100
80
60
40
)
%
( y c a r u c c A
Multi−Task
SGD−Single
SGD−Merged
)
%
( y c a r u c c A
80
60
40
50
100
0
50
100
0
50
100
0
50
100
)
%
( y c a r u c c A
70
65
60
0
)
%
( y c a r u c c A
64
63
62
61
60
0
Overall Performance ( 20 Persons ) 65
100
Multi−Task
SGD−Single
SGD−Merged
)
%
( y c a r u c c A
80
60
40
Multi−Task ( 20 Persons )
SGD−Single ( 20 Persons )
SGD−Merged ( 20 Persons )
100
80
60
40
)
%
( y c a r u c c A
100
80
60
40
)
%
( y c a r u c c A
50
100
0
50
100
0
50
100
0
50
100
Figure 2 . Overall and personal accuracy curves of the different methods ( 5 person , 10 person , and 20 person , respectively ) . The overall accuracy curves are used to compare the OMT method with baselines . The personal accuracy curves are used for showing the diversity and distribution of the performance among persons . axis 2 , respectively . The covariance1,2,i is the covariance value between the i’th windows of the axis 1 and the axis2 . We defined correlation feature between other axis pairs in the same manner .
B . Experimental Setting
Two baselines are adopted , including the SGD Single training for each single person ( using only this person ’s data for training ) , and the SGD Merged training ( merging all the training data of different persons to train a unified model ) . We employed an L2 prior for all methods , by setting the variance σ = 2 . For the OMT method , its hyper parameters ( ie , C and d ) are tuned by using development data . In preliminary experiments , we find using d = 1 worked well . For C , we test C = 5 , 10 , 20 , 40 , 80 , 160 on development data , and choose the optimal one . We will show detailed values of C in experimental results . r o t c a F − r e f s n a r T
7
6
5
4
3
2
1
0 x 10−3
Task−Similarities ( 5 Persons )
Task−Similarities ( 10 Persons )
0.015
0.01
0.005 r o t c a F − r e f s n a r T
0
20
40
60
Passes
80
100
0
0
20
40
60
80
100
Passes
Figure 3 . Convergence of the learned OMT transfer factors between personal data pairs . A curve ( i , j ) corresponds to αi,j between the persons i and j ( that is , the task similarity between person i and j ) . For simplicity , we omit the ( i , j ) information ( because we only focus on the convergence here ) . Similar tendencies were also observed on the 20 person data .
C . Results and Discussion
To study multi task learning with different scales , we perform experiments on 5 person , 10 person , and 20 person
1222 data in an incremental way ( see Table III ) . As we can see , the OMT method significantly outperformed baselines .
Note that the overall accuracies of 5 person , 10 person , and 20 person datasets are not directly comparable to each other , simply because the datasets are different . For example , the 20 person dataset contains the newly added 15 persons ( compared with the 5 person dataset ) , and the newly added 15 persons may have more noisy data . Nevertheless , the personal accuracies for specific persons are comparable among different scales .
1 ) Overall and Personal Curves : In Figure 2 , we show the accuracy curves by varying the number of training passes . From the overall curves , we can clearly see the superiority of the OMT method over other methods in different scales .
We can see the personal curves are very diversified , and simply merging their data for unified SGD training is frustrating . The OMT method is an ideal solution for this diversified situation .
2 ) Convergence of Transfer Factors :
In Figure 3 , we show curves of the transfer factors . As we can see , the transfer factors were convergent as the OMT learning went on .
In principle , the proposed method should be able to learn even negative transfer factors among tasks ( eg , if task a and task b have opposite patterns ) . However , in this dataset , we did not observe negative similarities . We observed that all transfer factors were non negative . This is also good in another aspect ( convex analysis ) : it indicates that the objective function of multi task learning will be convex and its optimum will be unique .
V . CONCLUSIONS AND FUTURE WORK
We studied personalized activity recognition , and proposed a new multi task learning method , which can naturally learn similarities among different tasks ( persons ) . Experiments demonstrated that personalized activity recognition with multi task learning performed much better than singleperson based learning and merged learning . Note that the proposed multi task learning method is a general technique , and it can be easily applied to other tasks . As future work , we plan to apply this method to other large scale data mining tasks .
ACKNOWLEDGMENTS
XS , HK , and NU are supported by the FIRST Program of JSPS . PL is partially supported by the National Science Foundation ( DMS 0808864 ) and the Office of Naval Research ( YIP N000140910911 ) .
REFERENCES
[ 1 ] L . Bao and S . S . Intille , “ Activity recognition from userannotated acceleration data . ” in Pervasive Computing , ser . Lecture Notes in Computer Science 3001 . Springer , 2004 , pp . 1–17 .
[ 2 ] J . Pr¨akk¨a , M . Ermes , P . Korpip¨a¨a , J . M¨antyj¨arvi , J . Peltola , and I . Korhonen , “ Activity classification using realistic data from wearable sensors . ” IEEE Transactions on Information Technology in Biomedicine , vol . 10 , no . 1 , pp . 119–128 , 2006 .
[ 3 ] N . Ravi , N . Dandekar , P . Mysore , and M . L . Littman , “ Activity recognition from accelerometer data . ” in Proceedings of AAAI’05 , 2005 , pp . 1541–1546 .
[ 4 ] X . Sun , H . Kashima , R . Tomioka , and N . Ueda , “ Large scale real life action recognition using conditional random fields with stochastic training . ” in PAKDD ( 2011 ) , ser . Lecture Notes in Computer Science , J . Z . Huang , L . Cao , and J . Srivastava , Eds . , vol . 6635 . Springer , 2011 , pp . 222–233 .
[ 5 ] T . Huynh , M . Fritz , and B . Schiele , “ Discovery of activity patterns using topic models , ” in Proceedings of the 10th international conference on Ubiquitous computing . ACM , 2008 , pp . 10–19 .
[ 6 ] X . Sun , H . Kashima , T . Matsuzaki , and N . Ueda , “ Averaged stochastic gradient descent with feedback : An accurate , robust , and fast training method . ” in Proceedings of the 10th International Conference on Data Mining ( ICDM’10 ) , 2010 , pp . 1067–1072 .
[ 7 ] J . Lafferty , A . McCallum , and F . Pereira , “ Conditional random fields : Probabilistic models for segmenting and labeling sequence data , ” in Proceedings of the 18th International Conference on Machine Learning ( ICML’01 ) , 2001 , pp . 282– 289 .
[ 8 ] L . Bottou , “ Online algorithms and stochastic approximations , ” Online Learning and Neural Networks . Saad , David . Cambridge University Press , 1998 .
[ 9 ] A . Argyriou , T . Evgeniou , and M . Pontil , “ Convex multi task feature learning . ” Machine Learning , vol . 73 , no . 3 , pp . 243– 272 , 2008 .
[ 10 ] Y . Xue , D . Dunson , and L . Carin , “ The matrix stick breaking process for flexible multi task learning , ” in Proceedings of the 24th international conference on Machine learning ( ICML’07 ) . Corvalis , Oregon : ACM , 2007 , pp . 1063–1070 .
[ 11 ] A . Agarwal , A . Rakhlin , and P . Bartlett , “ Matrix regularization techniques for online multitask learning , ” EECS Department , University of California , Berkeley , Tech . Rep . UCB/EECS 2008 138 , Oct 2008 .
[ 12 ] H . Yang , I . King , and M . R . Lyu , “ Online learning for multitask feature selection . ” in Proceedings of CIKM’10 . ACM , 2010 , pp . 1693–1696 .
[ 13 ] Y . Hattori , M . Takemori , S . Inoue , G . Hirakawa , and O . Sudo , “ Operation and baseline assessment of large scale activity gathering system by mobile device , ” in Proceedings of DICOMO’10 , 2010 .
1223
