2011 11th IEEE International Conference on Data Mining
Partitionable kernels for mapping kernels
Kilho Shin
Graduate School of Applied Informatics
University of Hyogo
Kobe , Japan yshin@aiu hyogoacjp
Abstract—Many of tree kernels in the literature are designed tanking advantage of the mapping kernel framework . The most important advantage of using this framework is that we have a strong theorem to examine positive definiteness of the resulting tree kernels . In the mapping kernel framework , each data object is viewed as a collection of components , and a mapping kernel for a pair of data objects is determined as a sum of kernel values of component pairs over a certain range determined according to the purpose of use of the resulting mapping kernel . For those tree kernels known to belong to the mapping kernel category , the string kernel of the product type is commonly used to compute the kernel values of component pairs . This is because it is known that use of the producttype string kernel together with the mapping kernel framework allows us to have recursive formulas to calculate the resulting tree kernels efficiently . We significantly generalizes this result . In fact , we show that we can use partitionable kernels , a new class of string kernels instead of the product type string kernel to enjoy the same advantage , that is , efficient computation based on recursive formulas . The class of partitionable kernels is abundant , and contains the product type string kernels just as an instance . Also , this result , not limited to tree kernels , can be applied to general mapping kernels after we formalize the decomposition properties of trees as the new notion of pretty decomposability .
I . INTRODUCTION
The remarkable success of support vector machine in the recent ten years emphasizes the importance of designing good kernels . In fact , the efficiency and the predictive performance of SVM are greatly affected by the quality of kernels used , and we have two fundamental requirements for good kernels : ( 1 ) positive definiteness and ( 2 ) efficiency of computation ( [1] ) . These two requirements are not always easy to support , and hence , an ad hoc design of kernels could result in miserable failure . In this regard , a theoretical approach to design good kernels is important . For example , Haussler ’s convolution kernel ( [2 ] ) is an effective tool to design positive definite kernels . Recently , Shin et al . ( [3 ] ) generalized Haussler ’s convolution kernel , and introduced the notion of mapping kernel . Moreover , they applied the framework of mapping kernel to tree structures , and revealed that 13 of 19 tree kernels known in the literature are defined
1550 4786/11 $26.00 © 2011 IEEE DOI 101109/ICDM2011115
645 according to the single template formula of
.
|X.|fi
K ( X , Y ) =
( X.,Y .)∈MX,Y i=1
ϕ(xi , yi ) . fi fi
, Y fi and Y
In the formula , X and Y denote trees ; MX,Y represents a set of isomorphic pairs ( X ) of substructures of X and Y ; {x1 , . . . , x|X.|} and {y1 , . . . , y|Y .|} are the vertex sets fi determines of X a bijective correspondence between the vertex sets ) ; and ϕ is a primitive kernel defined over an alphabet of ( labels of ) vertices . Shin et al . showed a general condition of MX,Y for K ( X , Y ) to become positive definite . fi ( the isomorphism of X fi and Y
'|X.|
Furthermore , by taking advantage of certain good decomposability of the structure of trees and the form of the inner i=1 ϕ(xi , yi ) , we can derive a recursive formula to kernel compute K ( X , Y ) efficiently .
The main contribution of this paper is to generalize this result in two ways . On one hand , we formalize the structural features of trees useful for efficient computation of K ( X , Y ) , and introduce the notion of pretty decomposability of MX,Y . By this notion , we can extend the range of application of the method from trees to more general structures . On the other hand , we define the class of partitionable kernels , which includes the kernel i=1 ϕ(xi , yi ) as an instance , and extend the range of inner kernels to which the aforementioned efficient method to compute K ( X , Y ) is applicable . Hence , we will show that a mapping kernel
'|X.|
.
K ( x , y ) =
( x,y)∈Mx,y fi fi
)
, y
κ(x has a set of recursive formulas to compute itself , if Mx,y is pretty decomposable and κ is partitionable . Moreover , we define an invariant hidden degree for partitionable kernels , and show that the hidden degree one exactly characterizes the set of kernels of the form of i=1 ϕ(xi , yi ) . For an arbitrary positive hidden degree , we have instances of the partitionable kernel with the hidden degree .
'|X.|
In the remainder of this introductory section , we take the theory of tree edit distance and tree kernel as an example , and clarify the problem that we are going to struggle with and Kuboyama ( [6 ] ) introduced a kernel that is defined as the exponentiated soft minimum of costs of edit scripts . fi
K
( X , Y ) = e
σ:X→Y
−λγ(σ )
.
( 2 )
This is a clear contrast with Ta¨ı ’s edit distance , which is defined as the hard minimum . The exponentiated soft minimum gives an analytic approximation of the hard minimum , and the following asymptotic property holds .
. fi lim λ→∞
1−λ log
−λai e
= min{a1 , . . . , an} ff n . i
Although the kernel is not always positive definiteness , it is positive definite for the practical setting of γ.a → b' = 1 − δa,b , where δa,b is Kronecker ’s delta , that is , δa,b is 1 if a = b , and is 0 otherwise .
Moreover , the normalized form of this kernel , that is
K ( X , Y ) = g(X)g(Y )K
( 3 ) x∈X eλγx→•( , turns out to have an effiwhere g(X ) = cient method of calculation . In fact , the following recursive formulas yield an efficient dynamic programing algorithm .
( X , Y ) ,
' fi fi
K ( X , Y ) = ff fl e e
·
◦ .
X , ffi
◦ X ,
◦ Y )
1 + K (
−λγ•X→•(
• K ( X , Y ) =
−λγ•→•Y ( e ◦ .
• K ( X , Y ) + K ( X ,
◦ Y ) + K ( −λγ•X→•Y (
X , Y ) − K ( ◦ ffi ·fl fi ( 4 ) X , 1 + K ( Y ) The reader may notice a similarity between the formulas ( 1 ) and ( 4 ) . Both include γ.• Y ' , γ.• X → • X → •' , γ.• → • ◦ . ◦ . ◦ fi fi Y ) , X ) and ( X , Y ) , ( X , X , and the resulting dynamic programming algorithms for edit distance and kernel have the same time complexity O(|X|3/2 |Y |3/2 ) by taking advantage of Demaine ’s strategy ( [7] ) . This similarity is not just a coincidence , but is because these recursive expressions are designed based on the same structural decomposition rules of trees .
Y ' , ( X ,
◦ Y ) , ( fi Y )
In addition , this method to calculate Shin and Kuboyama ’s kernel is extended to arbitrary tree kernels of the form of
.
|X.|fi
K ( X , Y ) =
( X.,Y .)∈MX,Y i=1
ϕ(xi , yi ) .
( 5 ) fi fi
, Y
) , that is , for X fi . X and Y
In the above , MX,Y is the entire set of isomorphic pairs of fi . Y , substructures ( X fi there exists a unique bijection between the vertex set of X fi that preserves the parent and sibling orders and that of Y at the same time . Moreover , ( xi , yi ) are the corresponding fi under this bijection , and ϕ is pairs of vertices of X a kernel determined over the set of vertices . To obtain the tree kernel ( 3 ) from ( 5 ) , we have only to let fi and Y
−λγx→y( e
−λγx→•(
−λγy→•( . e
( 6 )
ϕ(x , y ) = e
646
Symbol
Definition
•X The root of .X ◦X The remaining sub forest when eliminating •X from X .X The leftmost subtree of X fiX The remaining sub forest when eliminating .X from X
Figure 1 . Definition of •X , ◦X , .X and fiX in this paper . Throughout in this paper , by trees , we always mean rooted , ordered and labeled trees .
Introduced by Ta¨ı et al . ( [4] ) , Ta¨ı edit distance is widely used for comparison of trees , and is defined as follows . An edit script σ that converts a tree X into another Y is a finite sequence of edit operations , where each edit operation can either be : 1 ) deletion of a vertex x from X , written as .x → •' , 2 ) insertion of a vertex y of Y into X , .• → y' , 3 ) substitution of a vertex y of Y for x of X , .x → y' . To each operation .a → b' , is associated a cost γ.a → b' which is symmetric in the sense that γ.a → b' = γ.b → a' holds . The cost γ(σ ) of a script σ is the sum of the costs of all edit operations in σ . Then , Ta¨ı edit distance d(X , Y ) between X and Y is defined as the minimum over the costs of all scripts that convert X into Y , which can be formulated as follows . d(X , Y ) = min{γ(σ ) | σ : X → Y }
Ta¨ı et al . also showed that their tree edit distance can be computed by an efficient dynamic programming algorithm determined by the following recursive expression .
⎧⎨ ⎩ d(X , Y ) = min γ.• X → •' + d( γ.• → • Y ' + d( X → • ◦ .
◦ X , Y ) , Y ' + d(X , ◦ Y ) , ◦ . Y ) + d(
γ.•
X , fi X , fi Y )
⎫⎬ ⎭ ( 1 )
The range of X and Y is extended to forests , ordered sequences of one or more trees , and • . X and fi X are operations on forests defined as depicted by Figure 1 .
◦ X ,
X ,
On the other hand , since Collins and Duffy ( [5 ] ) introduced the first instance of tree kernels , many tree kernels have been proposed . In relation to tree edit distance , Shin
On the other hand , late the kernel e−λγfix→•ffe−λγfiy→•ff in ( 4 ) with ϕ( e−λγfix→yff the recursive formulas to calcu(5 ) are obtained by simply replacing
• Y ) .
• X ,
In order to investigate positive definiteness of the kernel ( 5 ) , we can take advantage of a general result for mapping kernels . A mapping kernel is abstractly defined by
.
K ( X , Y ) =
( X.,Y .)∈MX,Y fi
, Y fi
) ,
κ(X fi fi
) ∈ MY,X , and ( X and the set {MX,Y | X , Y } is called a mapping system . Theorem 1 of [ 6 ] asserts that K is positive definite for any positive definite κ , if , and only if , the mapping system is ) ∈ MX,Y implies transitive in the following sense : ( X ) ∈ MY,Z ( Y ) ∈ MX,Z . The mapping kernel has a wide implies ( X range of application . Shin and Kuboyama ( [3 ] ) reported that 18 of 19 tree kernels known in the literature can be defined as mapping kernels , and in particular , 13 of 18 are of the form ( 5 ) for different MX,Y .
) ∈ MX,Y ∧(Y
, X
, Z
, Z
, Y
, Y fi fi fi fi fi fi fi fi
The aim of this paper is to extend the aforementioned technique for efficient computation of tree kernels to general mapping kernels . In fact , we investigate conditions of Mx,y and κ such that a mapping kernel determined by
.
K ( X , Y ) =
κ(x1 . . . x|X.| , y1 . . . y|Y .| )
( X.,Y .)∈MX,Y i=0 ffi ffl∞ fl Σi × Σi has recursive formulas to calculate itself efficiently . Here and throughout in this paper , we assume Mx,y is a subset of , where Σ denotes an alphabet , and hence , an element of Mx,y is a pair of strings of the same length . This setting is not too restrictive in practice . For example , a substructure of a tree can be viewed as a sequence of vertices by aligning the vertices in the order of any of inorder , post order and pre order traversals . With this setting , we introduce two conditions , pretty decomposability for mapping systems MX,Y and partionability for κ , and show that , if a mapping system Mx,y is pretty decomposable and κ is partitionable kernel , such recursive formulas exist .
The organization of this paper is as follows . After introducing the definition of partitionable kernels in Section II A , we show several examples of partitionable kernels in Section II B . In Section II C , we see several important properties of partitionable kernels . The properties include closedness of partitionable kernels for addition , multiplication and scholar multiplication . In Section II D , we first see the reason why partitionable kernels can provide recursive formulas to compute the resulting mapping kernels by taking tree kernels as an instance , and then , introduce the notion of pretty decomposability . In Section III , we see time efficiency and predictive performance of partitionable kernelbased mapping tree kernels through experiments with four tree kernels and three tree datasets .
647
II . PARTITIONABLE STRING KERNELS
A . Definitions
In order to introduce the definition of partitionable kernels in Definition 3 , we first see a couple of preliminary definitions . Definition 1 . An integral kernel κ[∗ ] over Σ is a family of string kernels {κ[i ] : Σi × Σi −→ R | i ∈ {0} ∪ N} . In particular , Σ0 is a singleton set of the null string {∅} , and a real value is assigned to κ[0](∅,∅ ) . fi Definition 2 . Let x = x1 . . . xk and y = y1 . . . yk be strings in Σk and ( be a positive integer . When a sequence of indices 0 = j0 ≤ j1 ≤ j2 ≤ ··· ≤ jff−1 ≤ jff = k is given , the corresponding ( partition of the pair ( x , y ) is defined as a sequence of ( pairs of strings ( xP1 , yP1 ) , . . . , ( xP' , xP' ) such that xPi = xji−1+1 . . . xji and yPi = yji−1+1 . . . yji . For ji = ji+1 , we define that xPi and yPi represent the null string ∅ . ( x , y ) by ( (xL , yL ) , ( xR , yR) ) . fi
For simplicity of description , we denote a 2 partition of
Now , we are ready to introduce partitionable kernels .
[ ∗ ] Definition 3 . A family of integral kernels κ = ( κ n ) is said to be partitionable , when there exist n dimensional square matrices Q1 , . . . , Qn such that
[ ∗ ] 1 , . . . κ
[ k ] i ( x , y ) = κ(xL
κ
, yL
)Qi tκ(xR
, yR
) holds for any non negative integer k , any strings x and y of length k , and any 2 partition ( (xL , yL ) , ( xR , yR ) ) of ( x , y ) . κ(xL , yL ) and tκ(xR , yR ) are the row and column and vectors determined by t , where the index κ sequence 0 = j0 ≤ j1 ≤ j2 = k determines the relevant 2 partition . fi
[ j1 ] κ 1 [ k−j1 ] ( xR , yR ) , . . . , κ n
( xL , yL ) , . . . , κ
[ j1 ] n ( xL , yL )
( xR , yR )
[ k−j1 ] 1
The constraint of 2 partition in Definition 3 is not essential . When we define ( degree form p as a homogeneous polynomial of the form
. fffi p(X 1 , . . . , X ff ) =
( j1,,j')∈{1,,n}' cj1,,j'
X(i,ji ) , i=1 where X i = ( X(i,1 ) , . . . , X(i,n ) ) is a vector of independent variables , Theorem 1 indicates that the same concept can be defined using ( degree form with ( )= 2 . Theorem 1 . For a family of kernels κ = ( κ the following are equivalent to each other .
[ ∗ ] 1 , . . . , κ
[ ∗ ] n ) ,
1 ) κ is partitionable . 2 ) For some
'
( there that κ(xP1 , xP1 ) , . . . , κ(xP' , yP' )
2 , such p1 , . . . , pn exist [ ∗ ] i ( x , y ) κ holds for
( degree = any ffi fl forms pi ( partition .
3 ) For
' fl
( any pff 1 , . . . , pff n there that κ(xP1 , xP1 ) , . . . , κ(xP' , yP' )
2 , such ffi forms pi ( partition . exist [ ∗ ] i ( x , y ) κ holds for
( degree = any
Example [ ∗ ] 1 , e ( e
[ ∗ ] 0 , e
2 . For [ ∗ ] 2 , . . . ) defined below are partitionable . arbitrary kernel κ over Σ , an
[ k ] d ( x , y ) = e
⎧⎪⎪⎪⎪⎨ ⎪⎪⎪⎪⎩
1 , 0 ,
.
1≤i1<···<id≤k dfi . j=1 if d = 0 , if d > k , if 0 < d ≤ k .
κ(xij , yij )
In fact , e
[ ∗ ] d ( x , y ) = holds . In addition , e [ ∗]∞ ( x , y ) = e e C . Important properties
[ ∗]∞ ( xL , yL ) · e i+j=d e
[ ∗ ] i ( xL
'k [ ∗]∞ ( xR , yR ) .
[ k]∞ ( x , y ) =
) · e
[ ∗ ] j ( xR
, yL
, yR
) i=1 κ(xi , yi ) satisfies
Partitionable string kernels have several good properties . Proposition 1 . Let integral kernels κ[∗ ] and λ[∗ ] be partitionable . Then , κ[∗ ] + λ[∗ ] , cκ[∗ ] and κ[∗ ] · λ[∗ ] are also partitionable . In addition , hd(κ[∗ ] + λ[∗ ] ) ≤ hd(κ[∗ ] ) + hd(λ[∗] ) , hd(cκ[∗ ] ) = hd(κ[∗ ] ) and hd(κ[∗ ] · λ[∗ ] ) ≤ hd(κ[∗])· hd(λ[∗ ] ) hold .
Due to these properties , we can derive an indefinitely abundant pool of partitionable kernels from a small number of seeds . Regarding positive definiteness , we have the following property . [ ∗ ] [ ∗ ] Proposition 2 . Let κ = ( κ n ) be a partitionable 1 , . . . , κ [ 1 ] [ 1 ] n are all positive definite and kernel family . If κ 1 , . . . , κ [ ∗ ] [ ∗ ] Qi are all non negative , κ n are positive definite . 1 , . . . , κ
Of course , the converse does not necessarily hold .
Example 3 . Alghough cos is positive definite for κ(x , y ) = x − y , Q 1 0 0 −1 holds . cos[∗ ]
[ ∗ ] κ
=
κ
We say that κ[∗ ] is character wise symmetric , iff ,
[ k ]
[ k ]
( x1 . . . xk , y1 . . . yk ) = κ
( xσ(1 ) . . . xσ(k ) , yσ(1 ) . . . yσ(k ) ) κ holds for arbitrary k and permutation σ ∈ Sk . Although whether all partitionable kernels are character wise symmetric is an open problem , we have the following properties . Proposition 3 . Let ( κ family . If all of Qi are symmetric , all of κ wise symmetric . For n = 2 , if κ independent , all of Qi are symmetric .
[ ∗ ] n ) be a partitionable kernel [ ∗ ] i are character[1 ] 2 are linearly
[ ∗ ] 1 , . . . , κ
[ 1 ] 1 and κ
Proof : 1 . For a permutation σ ∈ Sk , we let σx1 . . . xk = [ k ] i ( σx , σy ) . We xσ(1 ) . . . xσ(k ) , and show κ prove the claim by mathematical induction on k . When k = 2 , the claim follows from
[ k ] i ( x , y ) = κ
Proof : We prove that 1 implies 3 by the mathematical induction on ( . Given an ( partition ( xP1 , . . . , xP' ) , we let xL = xP1 and xR = xP2 . . .xP' , where indicates [ ∗ ] i ( x , y ) = the concatenation of strings ( vectors ) . Due to κ tκ(xR , yR ) and the hypothesis of induction , we κ(xL , yL)Qi have [ ∗ ] i ( x ) = κ(xP1 , yPi )Qi
κ
κ(xP2 , yP2 ) , . . . , κ(xP' , yP' )
. j=1,,n t fl ff−1 p j
We can define the ( degree form pff i by ffi ff−1 p j ff i ( X 1 , . . . , X ff ) = X 1Qi p t
( X 2 , . . . , X ff )
. j=1,,n is evident
It that 3 implies 2 . To prove that 2 implies 1 , we let ( xP1 , . . . , xP' ) = ( xL , xR,∅ , . . . ,∅ ) and ( yP1 , . . . , yP' ) = ( yL , yR,∅ , . . . ,∅ ) . Then , we have ffi ) , κ(∅,∅ ) , . . . , κ(∅,∅ ) ) fi(x , y ) = pi
) , κ(xR
κ(xL
, yR
, yL fl
.
Qi is determined by
X 1Qi tX 2 = pi(X 1 , X 2 , ( ∅,∅ ) , . . . , f ( ∅,∅) ) . there exists a partitionable kernel is said to be parfamily if [ ∗ ] n ) that contains κ[∗ ] . fi
Definition 4 . An integral kernel κ[∗ ] titionable , [ ∗ ] ( κ 1 , . . . , κ Definition 5 . The hidden degree of a partitionable kernel κ[∗ ] that is not identically 0 is defined as the minimum [ ∗ ] [ ∗ ] n of the partitionable kernel families ( κ n ) 1 , κ that includes κ[∗ ] . We denote the hidden degree of κ[∗ ] by hd(κ[∗] ) , and define hd(κ[∗ ] ) = 0 for κ[∗ ] ≡ 0 . fi
[ ∗ ] 2 . . . , κ
B . Examples
We show a couple of examples of partitionable kernels .
Example 1 . For an arbitrary kernel κ over Σ , sin cos
[ ∗ ] κ defined below are partitionable .
[ 0 ]
κ ( ∅,∅ ) = 0 , sin sin
[ k ] κ ( x , y ) = sin
κ(xi , yi )
[ ∗ ] κ and fi fi ff k . ff k . i=1
[ 0 ]
κ ( ∅,∅ ) = 1 , [ ∗ ] κ ( x , y ) = sin cos sin
[ ∗ ] κ ( x , y ) = cos cos cos
[ k ] κ ( x , y ) = cos
κ(xi , yi )
[ ∗ ] κ ( xL
)
, yL ) cos [ ∗ ] κ ( xL , yL ) cos [ ∗ ] κ ( xL
[ ∗ ] κ ( xR , yL ) sin [ ∗ ] κ ( xR , yL ) sin i=1 , yR [ ∗ ] κ ( xR , yR [ ∗ ] κ ( xR
)
+ cos [ ∗ ] κ ( xL − sin
, yR
)
, yR
)
[ 2 ] i ( x1x2 , y1y2 ) = κ(x1 , y1)Qi
κ tκ(x2 , y2 )
= κ(x2 , y2)Qi tκ(x1 , y1 ) = κ
[ 2 ] i ( x2x1 , y2y1 ) .
648
When k > 2 , it suffices to prove the claim for the case of σ = ( i , i + 1 ) . First , we assume that i < k − 1 , and take the 2 partition corresponding to 0 = j0 ≤ j1 = i + 1 < j2 = k . By the hypothesis of induction , we have σxR tκ( tκ(xR
)Qi )Qi
, , yR
σyL σyL
[ k ] i (
σx ,
σyR
κ
)
)
,
σxL σy ) = κ( σxL = κ( = κ(xL
, , yL
)Qi tκ(xR
, yR
) = κ
[ k ] i ( x , y ) .
In the case of i = k − 1 , we have onlyt to let j1 = k − 2 , and exchange the roles of ( xL , yL ) and ( xR , yR ) . [ ∗ ] [ ∗ ] 2 . We will see that Q1 is symmetric . Since κ 1 and κ 2 are linearly independent , we have
[ 0 ] 1 [ 0 ] 2
κ κ
Q1
=
1 0 and [ κ
[ 0 ] 1 , κ
[ 0 ] 2 ]Q1 = [ 1 , 0 ] .
The claim follows . [ ∗ ] i
Even if all κ necessarily symmetric . For example , let κ i = 1 , 2 . When we define Qi = are character wise symmetric , Qi are not i ( x , y ) ≡ i for [ ∗ ] to satisfy ai ci bi di i = κ
[ k ] i = [ 1 , 2 ] ai ci bi di
1 2
= ai + 2bi + 2ci + 4di ,
Qi is not necessarily symmetric .
For partitionable kernels of hd(κ[∗ ] ) = 1 , we have the following beautiful theorem . Theorem 2 . If κ[∗ ] 1−k
[ k ]
[ 0 ]
( x , y ) = ( κ
)
κ
D . Kernel computation i=1 kfi is partitionable with hd(κ[∗ ] ) = 1 ,
[ 1 ]
κ
( xi , yi ) holds . ffi
Lemma 1 shows the key property of partitionable kernels flffl∞ with respect to computational feasibility of mapping kernels ffl∞ derived from them . In the remainder of this paper , I denotes i=0(Σi × Σi ) P0 , that is , the entire set of finite subsets i=0(Σi × Σi ) . of Lemma 1 . Let ( κ family . Hence , there exists Qi such that [ ∗ ] 1 ( xL , yR
[ ∗ ] n ) be a partitionable kernel
[ ∗ ] i ( x , y ) =(κ
) , . . . , κ holds for all i . Then , if M1 and M2 are in I ,
) , . . . , κ [ ∗ ] 1 ( xR
))Qi [ ∗ ] 1 ( xR
[ ∗ ] 1 , . . . , κ
, yL t ( κ
[ ∗ ] 1 ( xL
, yR
, yL
) )
κ
.
[ ∗ ] i ( x , y ) =
κ
( x,y)∈M1ffM2
( K1,1 , . . . , K1,n)Qi t ( K2,1 , . . . , K2,n )
[ ∗ ] j ( x , y ) .
We let ab denote the concatenation of strings a and b , holds for Ki,j = and define M1M2 ∈ I by
( x,y)∈Mi κ
{(x1x2 , y1y2 ) | ( x1 , y1 ) ∈ M1 , ( x2 , y2 ) ∈ M2} .
Since proving Lemma 1 is easy , we see how this property is useful to calculate partitionable kernel based mapping kernels using an example . In the example , we let x and y be ordered forests , and consider the entire set of isomorphic pairs of substructures of X and Y . Converting a substructure z of x or y into a string of vertices by aligning the vertices of z in the order of in order traversal , Mx,y . I is derived from the set of isomorphic substructure pairs , where Σ denotes the alphabet composed of vertices . Letting Mx,y = {(xfi y ∈ yfi} , it is easy to • • see that the following decomposition formulas hold .
) ∈ Mx,y | •
, yfi
,
•
Mx,y fifl Mx,y = M•x,•y fifl
Mx,◦y ∪ M◦x,y Mx,y = Mx,◦y ∩ M◦x,y = M◦x,◦y fifl M•x,•yM◦x,◦y • . ffi fifl In the above , fi denotes the disjoint union . [ ∗ ] i ( xfi κ ( x,y)∈Mx,y
When we let Ki(x , y ) =
M•x,•yM◦x,◦yMfix,fiy
M•x,•yMfix,fiy x ∈ xfi ffi
, yfi
) , the for
( 7 ) ( 8 ) ffi
( 9 ) ffi
.
Ki(x , y ) = mulas of ( 7 ) and ( 8 ) imply [ ∗ ] i ( xfi κ ( x,y)∈•Mx,y ◦ + Ki(x , y ) + Ki(
, yfi
) x , y ) − Ki( ◦
◦ x ,
◦ y ) .
.
The first term of the right hand side can be further decomposed by the formula ( 9 ) as • [ 1 ] i ( x ,
) = κ
• y )
[ ∗ ] i ( xfi ( x,y)∈•Mx,y
κ
, yfi . ( x,y)∈M•x,•y
+
[ ∗ ] i ( xfi , yfi . κ ffM◦x,◦y ffM◦x,◦y ( x,y)∈M•x,•y
+
. ) + ( x,y)∈M•x,•y , yfi
[ ∗ ] i ( xfi κ ffMfix,fiy
, yfi
)
[ ∗ ] i ( xfi κ ffMfix,fiy
) , and we can apply Lemma 1 to the last three terms of the right hand side . For example , by Lemma 1 , the second term
, yfi turns out to be . [ ∗ ] i ( xfi fl κ ( x,y)∈M•x,•y ffM◦x,◦y fl • • K1( = x , y ) , . . . , Kn( fl t fl • x , t
K1( • y ) , . . . , κ ◦ .
[ 1 ] 1 (
◦ .
◦ .
◦ . x ,
=
κ
)
• x ,
• y ) y ) , . . . , Kn( [ 1 ] n (
• y )
• x , ffi ffi
Qi ◦ . x ,
Qi ◦ . ffi ffi
.
◦ . y )
◦ . y )
K1( x , y ) , . . . , Kn( x ,
( 7 ) ,
( 8 ) and ( 9 ) the formulas of together with Thus , Lemma 1 provide us with a method to reduce calculation of K1(x , y ) , . . . , Kn(x , y ) to that of K1 , . . . , Kn for smaller forests . By applying this method iteratively , we can finally reach the concrete values of K1(x , y ) , . . . , Kn(x , y ) .
In [ 8 ] , in addition to the formulas of ( 7 ) , ( 8 ) and ( 9 ) , seven sets of decomposition formulas are presented in order
649
'|x.| to calculate seven different classes of tree kernels . These tree fi kernels are of the form of i ) , and hence , are derived from partitionable kernels with hidden degree 1 .
( x,y)∈Mx,y i=1 ϕ(x fi i , y
In the same way as mentioned above , Lemma 1 enables us to take advantage of these decomposition formulas in order to compute novel mapping tree kernels derived by replacing fi i ) with partitionable kernels of higher hidden
'|x.| i=1 ϕ(x fi i , y degree .
In the following , we will generalize what we saw in the above on decomposition formulas of tree mapping systems , and will introduce the notion of pretty decomposability as a formalized condition for mapping systems that allows application of Lemma 1 in order to calculate the resulting mapping kernels . We let ( X , < ) be an ( infinite ) ordered set of objects such that , for any x ∈ X , there exists a positive integer p and , if x ≥ x1 ≥ ··· ≥ xq for p < q , then xp = xp+1 = ··· = xq holds . In addition , we assume that a mapping system M : X × X → I and a finite set of reduction operations π1 , . . . , πff are given . A reduction operation is a mapping π : X → X such that π(x ) < x holds for ∀x ∈ X unless x is minimal . We define E as the minimum subset of ( X × X )I that satisfies the following conditions . We let X and Y be variables that move over X .
1 ) The constant mapping ( X , Y ) → ∅ is in E . 2 ) M ( X , πi(Y ) ) and M ( πi(X ) , Y ) are in E . 3 ) If e(X , Y ) is in E , e(πi(X ) , Y ) and e(X , πi(Y ) ) are in E . 4 ) If e(X , Y ) and e and e(X , Y )e fi
( X , Y ) are in E , e(X , Y )∪ e fi ( X , Y ) are in E .
( X , Y ) We can view e(X , Y ) ∈ E as an expression with respect to X and Y composed of symbols ∅ , M ( ·,· ) , πi,∪ and .
Pretty decomposable defined below is a condition of a mapping system M such that the resulting mapping kernels have recursive decomposition formulas when the sub kernels are partitionable . Definition 6 . A mapping system M is said to be pretty decomposable , if , and only if , there exists a finite set of expressions {e1(X , Y ) , . . . , em(X , Y )} ⊂ E that satisfies the following . 1 ) M is identical to e1(X , Y ) as a mapping from X ×X fi to I . fi
( X , Y ) ∪ e fifi fi fifi
( X , Y ) ∩ e
2 ) If a sub expression e some ei(X , Y ) , e same mapping as some ej(X , Y ) .
( X , Y ) appears in ( X , Y ) determines the fi When X is the set of ordered forests , we can define an order > so that x > y , if , and only if , y is a proper substructure of x . Then , the operations • x are all reduction operators . Moreover , under these settings , the mapping system M ( x , y ) = Mx,y is pretty decomposable . x and . x , fi x , ◦ the decomposition formulas of ( 7 ) , ( 8 ) and ( 9 ) In fact , determines a set of expressions that satisfies the conditions described in Definition 6 . Theorem 3 . Let Mx,y be pretty decomposable and [ ∗ ] n ) be a partitionable kernel family . There exists ( κ a set of recursive decomposition formulas that reduces calculation of
[ ∗ ] 1 , . . . , κ
.
Ki(x , y ) =
( x,y)∈M ( x,y )
κ
[ ∗ ] i ( xfi
, yfi
) fi fi e fi fi fifi fififi
, yfi
[ ∗ ] i ( xfi
( X , Y ) = e for i = 1 , . . . , n to calculation of a finite set of {Kij ( xj , yj ) | j = 1 , . . . , N} such that at least one of xj < x and yj < y holds for any j = 1 , . . . , N .
( X , Y ) and e
Proof : ( Sketch ) First , we introduce an order ≤ into E . We denote e ≤ e fi if , and only if , for any ( x , y ) ∈ X × X , there exists an injective mapping f : e(x , y ) → e ( x , y ) such that each component of f ( ξ , η ) contains the corresponding component of ( ξ , η ) . Note that , if ( ξ , η ) ∈ e(x , y ) , the components ξ and η are strings over Σ of the same length . fi and e is not Also e < e identical to e fi indicates the case that e ≤ e fi as a mapping from X × X to I .
We prove that the assertion of the theorem holds for ) when e is a sub expression of ( x,y)∈e(x,y ) κ some of ei by the mathematical induction on the order < of E . fifi fi We investigate two cases one by one : e ( X , Y ) = ( X , Y )∪e ( X , Y )e fififi ( X , Y ) . ( X , Y ) ∪ e fifi ( X , Y ) . If First , we assume e(X , Y ) = e fi fifi ( x , y ) . e ( x , y ) holds for all ( x , y ) , we can eliminate e fi without harming the result at all . Thus , we can such e ( x , y ) )= ∅ holds for some ( x , y ) , and ( x , y)\e fifi assume that e fi . In the same way , we have hence , we can conclude e > e ( X , Y )∩e e > e ( X , Y ) = ei(X , Y ) holds for some i ( Definition 6 ) , and hence e > ei holds . The . claim follows from the hypothesis of induction and fifi . On the other hand , e .
[ ∗ ] i ( xfi . ( x,y)∈e(x,y )
, yfi [ ∗ ] i ( xfi ( x,y)∈e(x,y )
[ ∗ ] i ( xfi , yfi . ( x,y)∈e(x,y ) ) − [ ∗ ] , yfi i ( xfi κ ( x,y)∈ei(x,y ) ( X , Y )e fifi Secondly , we assume e(X , Y ) = e ( X , Y ) . If ( x , y ) = ∅ for all ( x , y ) , we can eliminate such e fifi without fifi e fi harming the result at all . Thus , we can assume that e > e [ ∗ ] , yfi i ( xfi and e > e ( x,y)∈e(x,y ) κ )
[ ∗ ] j ( xfi , yfi function is and ) from claim follows ( x,y)∈e(x,y ) κ Given x ∈ X , since the length of a path from x to a minimal object is bounded above by p , a program that iteratively evaluate the recursive decomposition formulas obtained by Theorem 3 stops within finite lengths of time . Furthermore , the time complexity of this program when fifi . By Lemma 1 , , yfi the hypothesis of induction .
( x,y)∈e(x,y ) κ ) . The of [ ∗ ] j ( xfi
, yfi
) =
)+
) .
κ
κ
κ fifi a fi fi
650 applied to κ[∗ ] program when calculating is hd(κ[∗ ] ) times as large as that of the
.
K ( X , Y ) =
( X.,Y .)∈M ( X,Y )
1 .
This is because the program has to calculate hd(κ[∗ ] ) mapping kernels in parallel .
III . EMPIRICAL RESULTS
In this section , we will show some experimental results . In the experiments , we experimented with four different mapping tree kernels , which are all derived from partitionable kernels . Two of the kernels ( K0 and K3 ) are selected from the literature , while the others ( K1 and K2 ) are newly examined in this paper .
To begin with , we would like to describe the purpose of our experiments briefly . The main contribution of this paper is to propose an abundant class of mapping kernels that have practical computational complexity , and hence , we think that it is not so meaningful to focus on a small number of instances of partitionable kernels out of a large number of candidates , and to run experiments with them for the purpose of comparing them with kernels known in the literature . In fact , Proposition 1 , for example , provides us with a method to explore novel kernels in a large space of partitionable kernels . Hence , evaluation of predictive performance of a few instances will say almost nothing about the property of the entire class . In addition , a kernel which is effective to a particular type of datasets is not always effective to another type of datasets . Also , the fact that a kernel is not useful for some type of datasets does not necessarily mean that it is always useless . When we understand that the purpose of our contribution is never to propose kernels that will be effective to particular types of datasets , experimental results with a small number of specific datasets can only show a potential of the kernel class . Thus , the purpose of the experiments to describe here is to show ( 1 ) the fundamental computational feasibility of partitionable kernel based mapping kernels and ( 2 ) their potential in terms of predictive performance when applied to practical problems .
All tree kernels Ki used in the experiments are mapping . The mapping kernels derived from partitionable kernels κ system MX,Y is determined to be the entire set of isomorphic substructures of trees X and Y . Hence , fi
.
[ ∗ ] i fi
[ ∗ ] i ( X
κ
, Y
)
Ki(X , Y ) =
( X.,Y .)∈MX,Y [ ∗ ] 0 and κ
[ ∗ ] 3 are of hidden [ ∗ ] 2 are of hidden degree 2 and 3 , forms a partitionable kernel family [ ∗ ] 1 . Table I and and κ
[ ∗ ] 0
[ ∗ ] 1 and κ holds . The partitionable kernels κ degree 1 , while κ respectively . In fact , κ with κ Table II describe Ki and κi .
[ ∗ ] 1 does with κ
[ ∗ ] 0 , and κ
[ ∗ ] 2
For the experiments , we use three datasets , named colon cancer , cystic and leukemia , from the KEGG/GLYCAN database [ 9 ] . The total number of and averages of sizes and heights of the examples in those datasets are given as follows .
Dataset
# Examples Ave . Size Ave . Height colon cancer cystic leukemia
134 160 442
8.4 8.3 13.5
5.6 5.0 7.4
The following is the steps of the experiment with each dataset .
1 ) For each tree kernel , compute the corresponding Gram matrix . The matrix includes N 2 elements , when the relevant dataset includes N sample trees . An element of the matrix includes the decay factor λ , and is a function of λ .
2 ) Generate 10 pairs of a training dataset and a test dataset by splitting the relevant dataset independently at random . The training dataset is so generated that it is approximately four times larger than the test dataset . 3 ) For each combination of a dataset pair and a tree kernel , perform the following steps . a ) Run a grid search to find optimal values for the decay factor λ and the regulation parameter C of SVM . Hence , repeat run of five fold cross validation on the training data changing λ and C and select the parameter assignment that exhibits the greatest AUC of ROC Curve value . b ) Make SVM learn the entire training data with the optimal parameters obtained in 3a . c ) Evaluate three measures , namely , AUC of ROC Curve , F Score and Accuracy by applying the hypothesis ( model ) obtained in 3b to the test dataset .
In these steps , SVM refers to the Gram matrix generated in Step 1 in one of the following ways . In one way , SVM uses values Ki(X , Y ) found in the matrix as they are . In the other way , it uses normalized values √
Ki(X,Y )
Ki(X,X)Ki(Y,Y ) .
In computing the Gram matrices in Step 1 , we employed the accelerating technique reported in [ 8 ] , which first extracts all of the substructures up to congruence that appear in a dataset , and then calculate kernel values for all possible substructure pairs in the incremental order of their sizes . The followings are the run time in seconds of having calculated these Gram matrices with a laptop PC . Since K0 , K1 and K2 are calculated simultaneously , only the total run time is displayed .
Kernel K0 , K1 and K2 K3 colon cancer
17.9 sec 3.8 sec cystic 31.6 sec 5.1 sec leukemia 661.8 sec 261.8 sec
651
DEFINITION OF THE MAPPING TREE KERNELS : K0 , K1 , K2 AND K3
Table I
Kernel
K0
K1
K2
K3
Explanation A weighted count of congruent substructure pairs . The weight is determined by λn with the decay factor λ and the size n of the substructure . A weighted sum of the sizes of congruent substructure pairs . Hence , λnn is summed up over the entire congruent substructure pairs . A weighted sum of the squared sizes of congruent substructure pairs . Hence , λnn2 is summed up over the entire congruent substructure pairs . A tree kernel derived from Ta¨ı tree edit distance with the cost function γ.x → y' = 1 − δx,y .
DEFINITION OF THE PARTITIONABLE KERNELS : κ[∗ ]
0 , κ[∗ ]
1 , κ[∗ ]
2 AND κ[∗ ]
3
Table II
Sub kernel ( |X'| = |Y '| = n ) ' 0 ( X' , Y ' ) = κ[n ] '
1 ( X' , Y ' ) = κ[n ]
λδxi,yi
λδxi,yi i=1 i=1 n . fi n . fi n . n . i=1
2 ( X' , Y ' ) = κ[n ]
3 ( X' , Y ' ) = κ[n ]
λδxi,yi eλ(δxi,yi
+1 ) fi fi nff nff i=1 i=1
·
·
' '2
δxi,yi
δxi,yi
κ[0 ] i
1
0
0
1
Quadratic Form κ[∗ ] 0 ( x , y ) = κ[∗ ]
0 ( xL , yL)κ[∗ ]
1 ( xR , yR )
κ[∗ ] 1 ( x , y ) = κ[∗ ]
0 ( xL , yL)κ[∗ ]
1 ( xR , yR ) + κ[∗ ]
1 ( xL , yL)κ[∗ ]
0 ( xR , yR )
κ[∗ ] 2 ( x , y ) = κ[∗ ]
0 ( xL , yL)κ[∗ ]
2 ( xR , yR ) + κ[∗ ]
2 ( xL , yL)κ[∗ ]
0 ( xR , yR )
κ[∗ ] 3 ( x , y ) = κ[∗ ]
3 ( xL , yL)κ[∗ ]
+2κ[∗ ]
1 ( xL , yL)κ[∗ ] 3 ( xR , yR )
1 ( xR , yR ) i=1
As described in the above , the new mapping kernels K0 and K1 , which are derived from partitionable kernels of higher hidden degree , can be efficiently computed .
With respect to the predictive performance of the kernels , Table III shows the averages of AUC of ROC Curve , FScore and Accuracy values obtained from the test over 10 training test dataset pairs . “ Ki ( norm ) ” indicates that we ran the test with normalized kernel values . The figures in bold font face indicate the greatest values in their columns .
At a glance of the table , K0 , K1 and K2 appear more appropriate than the others for colon cancer , leukemia and cystic , respectively . Also , K3 is effective to leukemia , although it is not the best .
Table IV , V and VI are for further investigation of this observation , and show p values of the paired T test of AUCof ROC Curve , F Score and Accuracy values over the ten runs of test performed for each combination of a tree kernel and a dataset . In the tables , the p values smaller than 0.01 are highlighted .
Table IV and VI exhibit clear patterns . colon cancer : Superiority of the method of using normalized kernel values ( that is , Ki ( normal ) for i = 0 , 1 , 2 , 3 ) over the method of using kernel values as they are is evident . We can deny the corresponding null hypotheses with the sufficiently small significance level of 1 % . By contrast , we cannot conclude that any of Ki ( normal ) is better than the others without committing risk of error . leukemia : K2 , K2 ( norm ) and K3 show evident inferiority to the other kernels . Also , the p values in Table VI indicate that the evidences we have obtained from the experiments are insufficient to deny the null hypotheses that the other five are mutually comparable .
On the other hand , the pattern that we can observe in
Table V is less clear . cystic : Without too much risk of error , for i = 0 , 1 , 2 , we could conclude that K3 is inferior to K0 , K1 and K2 , no matter whether it is used with normalization . On the other hand , although we can perceive that the use of normalized Ki values always yields better results than the use of Ki values as they are , we have to use a large significance level to draw this conclusion .
Although the datasets and the mapping kernel examples used in the experiments were limited , at least , we can conclude that partitionable kernels of higher hidden degrees have potential to yield practically efficient mapping kernels with good predictive performance .
IV . CONCLUSION
We have introduced the notions of pretty decomposability and partitionable string kernels , and have proved that , if the mapping system is pretty decomposable and the subkernel is partitionable , there exists a set of recursive formulas to compute the resulting mapping kernel efficiently . Also , we have showed we could introduce an invariant called
652
AVERAGES OF MEASUREMENTS OVER 10 PAIRS OF TRAINING AND TEST DATASETS
Table III
Kernels K0 K0 ( norm ) K1 K1 ( norm ) K2 K2 ( norm ) K3 K3 ( norm ) colon cancer
AUC 0.88977 0.93375 0.88003 0.93504 0.88586 0.93193 0.79819 0.91241
F Score 0.87245 0.91573 0.83706 0.91482 0.82541 0.90653 0.77757 0.87466
Accuracy 0.82090 0.88955 0.75373 0.88806 0.72985 0.87612 0.63731 0.82836 cystic F Score 0.78008 0.73308 0.78226 0.74043 0.78700 0.74155 0.63478 0.70893
AUC 0.78194 0.76622 0.79329 0.77438 0.79322 0.77846 0.71489 0.73285
Accuracy
0.735 0.7025 0.74125 0.7075 0.74375 0.70875 0.530 0.66625
AUC 0.95999 0.97168 0.97090 0.97216 0.93242 0.83973 0.90198 0.96323 leukemia F Score 0.87345 0.88397 0.87083 0.88215 0.71954 0.74488 0.19990 0.88284
Accuracy 0.93258 0.93370 0.93034 0.93483 0.87087 0.85843 0.74719 0.93483
P VALUES OF PAIRED T TEST FOR COLON CANCER DATASET
Table IV
Upper triangle : AUC , Lower triangle : F Score
K0 K0 ( norm ) K1 K1 ( norm ) K2 K2 ( norm ) K3 K3 ( norm )
K0 K0 ( norm ) K1 K1 ( norm ) K2 K2 ( norm ) K3 K3 ( norm )
K0
–
0.04518 0.04480 0.04449 0.06331 0.04897 0.00008 0.81095
K0 ( norm ) 0.00035
–
0.00957 0.80264 0.00771 0.17572 0.00001 0.02136
K1
0.25434 0.00100
–
0.00845 0.63102 0.00647 0.00651 0.03258
K1 ( norm ) 0.00017 0.47732 0.00067
–
0.01003 0.10286 0.00001 0.02062
K2
0.39651 0.00047 0.56539 0.00025
–
0.00903 0.01892 0.04436
K2 ( norm ) 0.00111 0.61081 0.00166 0.26707 0.00169
–
0.00000 0.01589
K3
K3 ( norm )
0.02296 0.00276 0.03143 0.00291 0.03872 0.00304
–
0.00001
0.38339 0.29781 0.24360 0.29424 0.32506 0.36786 0.00420
–
K0
K0 ( norm )
K1
– – – – – – – –
0.05912
– – – – – – –
0.04648 0.00997
– – – – – –
Accuracy K1 ( norm )
0.06063 0.72632 0.00918
– – – – –
K2
K2 ( norm )
K3
K3 ( norm )
0.05177 0.00627 0.60768 0.00801
– – – –
0.07338 0.12123 0.00821 0.08684 0.00737
– – –
0.00005 0.00000 0.00602 0.00000 0.01742 0.00000
– –
0.64072 0.02070 0.03157 0.02132 0.03049 0.01993 0.00000
–
P VALUES OF PAIRED T TEST FOR CYSTIC DATASET
Table V
Upper Triangle : AUC ; Lower Triangle : F Score
K0
K0 ( norm )
K1
K1 ( norm )
K2
K2 ( norm )
K3
K0 K0 ( norm ) K1 K1 ( norm ) K2 K2 ( norm ) K3 K3 ( norm )
–
0.00393 0.73600 0.02739 0.47997 0.02797 0.05528 0.00155
0.08847
–
0.00937 0.30529 0.01294 0.30766 0.20194 0.12043
0.05479 0.02422
–
0.3861 0.48571 0.04290 0.05661 0.00299
0.37188 0.01846 0.08078
–
0.04057 0.75020 0.20022 0.01360
0.09695 0.04169 0.97035 0.12245
–
0.04035 0.04582 0.00333
0.67457 0.01002 0.14178 0.11927 0.19847
–
0.19149 0.00382
0.11480 0.22090 0.06449 0.14794 0.06975 0.11749
–
0.36126
K0
K0 ( norm )
K1
K0 K0 ( norm ) K1 K1 ( norm ) K2 K2 ( norm ) K3 K3 ( norm )
– – – – – – – –
0.03606
– – – – – – –
0.13818 0.03312
– – – – – –
K2
K2 ( norm )
K3
0.38156 0.05908 0.71634 0.10107
– – – –
0.11320 0.44016 0.08492 0.72631 0.11076
– – –
0.00000 0.00005 0.00000 0.00009 0.00000 0.00006
– –
Accuracy K1 ( norm )
0.09095 0.39936 0.07060
– – – – –
653
K3 ( norm ) 0.00343 0.01934 0.00123 0.00712 0.00197 0.00266 0.59184
–
K3 ( norm ) 0.00164 0.01619 0.00155 0.00294 0.00372 0.00056 0.00039
–
P VALUES OF PAIRED T TEST FOR LEUKEMIA DATASET
Table VI
Upper Triangle : AUC ; Lower Triangle : F Score
K0
K0 ( norm )
K1
K1 ( norm )
K2
K0 K0 ( norm ) K1 K1 ( norm ) K2 K2 ( norm ) K3 K3 ( norm )
–
0.23557 0.64166 0.29866 0.00010 0.00015 0.00004 0.47065
0.03938
–
0.04423 0.84837 0.00002 0.00002 0.00004 0.92085
0.02851 0.89573
–
0.20154 0.00004 0.00010 0.00005 0.27984
0.09467 0.86997 0.84300
–
0.00009 0.00011 0.00005 0.95203
0.00033 0.00000 0.00010 0.00002
–
0.31038 0.00039 0.00005
K2 ( norm ) 0.00007 0.00001 0.00009 0.00002 0.00023
–
0.00027 0.00029
K3
K3 ( norm )
0.00002 0.00000 0.00001 0.00000 0.00024 0.00481
–
0.00005
0.48853 0.01608 0.20138 0.12343 0.00001 0.00003 0.00000
–
K0
K0 ( norm )
K1
Accuracy K1 ( norm )
K2
K0 K0 ( norm ) K1 K1 ( norm ) K2 K2 ( norm ) K3 K3 ( norm )
– – – – – – – –
0.81137
– – – – – – –
0.44333 0.34344
– – – – – –
0.59105 0.82265 0.30923
– – – – –
0.00004 0.00000 0.00001 0.00006
– – – –
K2 ( norm ) 0.00008 0.00001 0.00003 0.00003 0.27021
– – –
K3
K3 ( norm )
0.00001 0.00001 0.00001 0.00002 0.00045 0.00199
– –
0.72631 0.85892 0.39936 1.00000 0.00001 0.00010 0.00002
–
[ 6 ] K . Shin and T . Kuboyama , “ A generalization of Haussler ’s convolution kernel mapping kernel , ” in ICML 2008 , 2008 .
[ 7 ] E . D . Demaine , S . Mozes , B . Rossman , and O . Weimann , “ An optimal decomposition algorithm for tree edit distance , ” in The 34th International Colloquium on Automata , languages and Programming ( ICALP ) , 2007 .
[ 8 ] K . Shin , M . Cuturi , and T . Kuboyama , “ Mapping kernels for trees , ” in ICML 2011 , 2011 .
[ 9 ] K . Hashimoto , S . Goto , S . Kawano , K . F . Aoki Kinoshita , and N . Ueda , “ Kegg as a glycome informatics resource , ” Glycobiology , vol . 16 , pp . 63R – 70R , 2006 . the string kernels of the product hidden degree to partitionable kernels . The hidden degree of a partitionable kernel linearly affects the time complexity of the mapping kernel derived from the partitionable kernel . In addition , type turn out to be characterized as partitionable kernels of hidden degree one . Although we know that any partitionable kernel of hidden degree two can be reduced one of the three normalized forms , we know only little about partitionable kernels of hidden degree higher than two . Also , whether any partitionable kernel is character wise symmetric is an open problem . If this were proved affirmatively , we would not have to pay much attention to converting mappings from structures to strings any more .
REFERENCES
[ 1 ] T . G¨artner , “ A survey of kernels for structured data . ” SIGKDD
Explorations , vol . 5 , no . 1 , pp . 49–58 , 2003 .
[ 2 ] D . Haussler , “ Convolution kernels on discrete structures , ” Dept . of Computer Science , University of California at Santa Cruz , UCSC CRL 99 10 , 1999 .
[ 3 ] K . Shin and T . Kuboyama , “ Generalization of haussler ’s convolution kernel mapping kernel and its application to tree kernels , ” J . Comput . Sci . Technol , vol . 25(5) : , pp . 1040–1054 , 2010 .
[ 4 ] K . C . Ta¨ı , “ The tree to tree correction problem , ” JACM , vol . 26 , no . 3 , pp . 422–433 , Jul . 1979 .
[ 5 ] M . Collins and N . Duffy , “ Convolution kernels for natural language , ” in Advances in Neural Information Processing Systems 14 [ Neural Information Processing Systems : Natural and Synthetic , NIPS 2001 ] . MIT Press , 2001 , pp . 625–632 .
654
