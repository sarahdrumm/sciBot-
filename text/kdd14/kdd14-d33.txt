Detecting Anomalies in Dynamic Rating Data :
A Robust Probabilistic Model for Rating Evolution
Stephan Günnemann Nikou Günnemann Christos Faloutsos
Carnegie Mellon University , USA
{sguennem , nguennem , christos}@cscmuedu
ABSTRACT Rating data is ubiquitous on websites such as Amazon , TripAdvisor , or Yelp . Since ratings are not static but given at various points in time , a temporal analysis of rating data provides deeper insights into the evolution of a product ’s quality . In this work , we tackle the following question : Given the time stamped rating data for a product or service , how can we detect the general rating behavior of users as well as time intervals where the ratings behave anomalous ?
We propose a Bayesian model that represents the rating data as sequence of categorical mixture models . In contrast to existing methods , our method does not require any aggregation of the input but it operates on the original time stamped data . To capture the dynamic effects of the ratings , the categorical mixtures are temporally constrained : Anomalies can occur in specific time intervals only and the general rating behavior should evolve smoothly over time . Our method automatically determines the intervals where anomalies occur , and it captures the temporal effects of the general behavior by using a state space model on the natural parameters of the categorical distributions . For learning our model , we propose an efficient algorithm combining principles from variational inference and dynamic programming . In our experimental study we show the effectiveness of our method and we present interesting discoveries on multiple real world datasets . Categories and Subject Descriptors H28 [ Database Management ] : Database Applications —Data mining ; I26 [ Artificial Intelligence ] : Learning Keywords robust mining ; anomaly detection ; categorical mixtures
1 .
INTRODUCTION
Online rating data provides customers valuable information about products and services and supports their decision making process . Exploiting and presenting this data is a key feature of websites such as Amazon , Yelp , or Tripadvisor . Besides the usefulness of rating data for customers , also companies and manufactures can benefit from it by , eg , using the data to detect functional weaknesses of their products or changes in the customers’ satisfaction .
In this work , we propose a method for analyzing rating data that incorporates the data ’s temporal characteristics . Given the time stamped rating data for a product or service , we aim at detecting the general rating behavior of the users ( called the base behavior ) as well as time intervals in which these ratings deviate from the general population ( anomalous behavior ) . The base behavior describes the general quality of a product or service accounting for temporal evolutions , eg , resulting from decreasing quality due to technical progress of competing products . The anomalies , in contrast , deviate from the base behavior and might , eg , occur due to spammers trying to push the success of a product or due to changes in the service quality .
Fig 1 : Left : Time stamped rating data analyzed by our method ( here : a hotel from TripAdvisor ) . Right : Extracted base behavior . Upper corner : Anomalous behavior detected in summer 2005 .
In Figure 1 , we show a real world example for such an effect . The data we analyzed here is a hotel from the TripAdvisor database . On the left , we show a subset of the original time stamped data . The colors indicate the different star ratings , and the height of each bar the number of these ratings at the current time . Obviously , it is hard to analyze such data by hand . In particular , keep in mind that the ratings are not uniformly distributed over time .
On the right , we illustrate the detected base behavior of our method . As one can see , the base behavior nicely shows the general rating behavior of the users and evolves smoothly over time with primarily medium to high ratings . Additionally , our method has found anomalous behavior in the months of July and August 2005 . As shown on the upper right , in these intervals , the fraction of low ratings ( red and yellow ) is highly increased compared to the base behavior ( 65 % low ratings compared to around 30 % in the base behavior ) . As we will show in Section 5 , these anomalies occurred due to problems in the service of the hotel .
0123452002‐09‐012004‐09‐012006‐09‐012008‐09‐015 stars4 stars3 stars2 stars1 star0%25%50%75%100%2002‐09‐012004‐09‐012006‐09‐012008‐09‐01anomalousbehaviorinsummer 200537%28%base behavior841 In general , our method detects time intervals in dynamic rating data which show anomalous behavior and – at the same time – it detects the base behavior if the data would not be polluted by anomalies . Besides using this principle to detect weaknesses in products and services , it can generally be used to filter out rating information which behave anomalous . Thereby , prospective customers might be provided with a cleaned history about the product ; or , one might specifically highlight these time intervals to the users to provide the whole picture on a product ( since otherwise these anomalous ratings are hidden in the larger set of normal behavior ) . As an additional benefit , we can exploit our method to predict the base behavior of future ratings . Accordingly , when new ratings arrive , we can estimate whether they match or deviate from the predicted behavior – thus , giving indication of new anomalies .
So far , there exists only a single method which analyzes temporal rating data under the presence of anomalies [ 7 ] . A potential drawback of [ 7 ] , however , is the necessary aggregation/binning of the data . When using a coarse aggregation , the temporal effects of the data are not well captured ( in the extreme , all data is a single bin ) . When using a fine aggregation , the analyzed distributions might degenerate ( in the extreme , many bins are empty ) . In our model we avoid this problem by directly operating on the time stamped data which is modeled via a sequence of categorical mixture models . We explicitly keep into account that ratings might not uniformly arrive over time . Furthermore , the work [ 7 ] assumes that anomalies occur at individual points in time . Our work captures the effects of real data much better by accounting for multiple different types of anomalies appearing across several time intervals . Our contributions are : • Mining task : We present a technique for the analysis of time stamped rating data . Our method detects the base behavior of users as well as time intervals where potential anomalies occurred . Additionally , our technique can be used to predict the rating behavior at future time points . • Theoretical soundness : Our method is based on a sound Bayesian model that represents the rating data as a sequence of temporally constrained categorical mixture models . To capture the temporal effects of the base behavior we use a state space model on the natural parameters of the categorical distributions .
• Algorithm design : We propose an efficient algorithm for learning our model which combines principles from variational inference and dynamic programming .
• Effectiveness : We evaluate our method on different real world datasets and we show its effectiveness by presenting interesting findings .
2 . BAYESIAN FRAMEWORK
In this section , we introduce our model for detecting the base rating behavior of users as well as time intervals in which anomalies have been occurred . Following convention , we do not distinguish between a random variable X and its realization X = x if it is clear from the context . As an abbreviation , we denote sets of random variables with the index ∗ , eg z(t)∗ i } with i in the corresponding domain , and z is an abbreviation for the set z(∗)∗ . Vectors of ( random ) variables are written in bold font , eg b , while the entries of the vectors are given in standard font , eg bi . We write t ∈ T , as a shortcut for t ∈ {1 , . . . , T} . is the set {z(t )
Fig 2 : Graphical model of our approach
Preliminaries . The data we consider is a time stamped collection of ratings . Let x(t ) i denote the i th rating occurred at time index t , and s(t ) the time stamp at time index t . At each time index we might observe multiple ratings ( eg if time stamps are only measured/provided on a daily basis ) . We denote with n(t ) the number of ratings at time index t . We assume the data to be ordered according to time , ie s(t ) occurs after s(t−1 ) . We denote with ∆(t1,t2 ) the elapsed time between time stamp s(t1 ) and s(t2 ) and we set ∆(t ) := ∆(t−1,t ) . Note that for each t a different ∆(t ) might be observed since we do not require a fixed binning or aggregation of the rating data . We denote with T the number of time indices ( ie the number of distinct time stamps ) and we assume that users can choose ratings based on a rating scale with S different ratings ( eg stars from 1 to S ) . As an abbreviation for later use , we define i = s}| to be the number of n(t ) s ratings at time t which possess the evaluation s ∈ S .
:= |{i ∈ {1 , . . . , n(t)} | x(t )
Generative Process .
We model the rating data including potential anomalies via a probabilistic generative process . An overview of our generative process showing the used variables and their dependencies is illustrated by the graphical model in Figure 2 . In the following we discuss the details of this process .
Given the observed rating data X , our aim is to extract the base behavior of the users and intervals in time where anomalies occur . Since the observed data might already be polluted by anomalies , we cannot directly use it to estimate the base behavior . Instead , we assume that the observed data is obtained by mixing the ( unkown ) base user behavior with the ( unknown ) anomaly behavior . Thus , both types of behavior are represented as latent variables which are not directly observed but inferred by our technique . Technically , at each point in time we have a categorical mixture model as illustrated in Figure 3 . To incorporate the temporal properties of the data , we perform additional modeling :
Part 1 : Mixing anomalies and base behavior . In contrast to the base behavior , which is present over the whole timespan , we assume that anomalies are rare events ( otherwise , they would correspond to the majority of the data , making the term “ anomaly ” rather meaningless ) and occur only in a specific time interval like , eg , during a short attack of spam . Technically , instead of using an individual anomaly at each point in time , we restrict the number of anomalies to be small , ie smaller than a number K ( we discuss later how to choose this parameter ) , and we temporally constrain the “ influence ” of each anomaly to a small time interval . For each anomaly , we define this interval by the random variables Lk and Uk , denoting the lower and upper bound of time indices at which the kth anomaly occurs .
K = number ofanomalyintervalsT = number oftime stampszᵢ⁽¹⁾ zᵢ⁽²⁾ zᵢ⁽ᵀ⁾ xᵢ⁽¹⁾ xᵢ⁽²⁾ xᵢ⁽ᵀ⁾ b⁽¹⁾b⁽ᵀ⁾b(cid:3771)⁽¹⁾b(cid:3771)⁽ᵀ⁾b(cid:3771)⁽²⁾(L*,U*)kϵKrkkϵKiϵn⁽ᵀ⁾ okb⁽²⁾zᵢ⁽²⁾ iϵn(2)iϵn(1)evolving base behavioranomalies842
In the following , we will use the function k(t ) = k if ∃x : x = k ∧ Lx ≤ t ≤ Ux 0 else
( 1 ) which maps the time index t to the corresponding anomaly ( or to 0 if no anomaly exists at this point in time ) . Here , we require disjointness of the different intervals .
Note that the use of anomaly intervals is a huge advantage in contrast to [ 7 ] , which models the anomalies at each time point individually . The potential of this extension is best shown for the case of very fine grained time stamps : In this case , we usually expect only a single rating per time stamp , ie n(t ) = 1 . To capture anomalies at multiple consecutive points in time , the work of [ 7 ] has to use multiple anomalies , while in our work a single interval suffices .
At this point we want to mention the difference between outliers and anomalies [ 7 ] : While outliers are irregular behavior attributed to mostly random corruptions of the data ( like , eg , measurement errors ) , anomalies are irregular behavior that follow a specific pattern ( like , eg , time points with consistently low ratings due to a change in the product ’s quality ) . In our work , we consider anomalous behavior . Giving the above information , the observed data at time t is modeled as a categorical mixture model defined by i ∼ x(t )
Categorical(π(t ) ) Categorical(ok(t ) ) if z(t ) if z(t ) i = 0 i = 1 i ∼ z(t )
0 Bernoulli(rk(t ) ) if k(t ) = 0 else
( 2 )
( 3 ) i
Here , z(t ) is the indicator variable showing which ratings are anomalies . π(t ) ∈ [ 01]S is the vector describing the base behavior at time t , ok ∈ [ 01]S is the kth anomaly behavior , and rk is the mixing proportion . The higher the value of rk , the higher the proportion of anomalies within the corresponding interval . If no anomaly is present at time t , all variables z(t)∗ are set to zero , corresponding of using only the base behavior at this point in time . Thus , the mixture model ’s components referring to the anomalies are constrained to specific intervals ( cf . Figure 3 ) .
For a Bayesian treatment , we equip the variables with corresponding prior distributions . We use ok ∼ Dir( ˆα ) rk ∼ Beta( ˆα , ˆβ ) due to the properties of conjugacy . The vector ˆα , for example , can be used to specify prior knowledge about potential anomalies ( eg anomalies should represent primarily low ratings ) . In all of our experiments we use ˆα = 1 and ˆα = ˆβ = 1 corresponding to non informative priors .
For the lower and upper bounds we exploit the idea that anomalies should appear primarily in short time intervals . That is , we assume that the probability to observe an anomaly over a very long time frame is lower than the probability to observe anomalies over only a few time points . We capture this effect by drawing the lower and upper bounds according to an exponential distribution controlled by the duration ∆(Lk,Uk ) of the anomaly interval . Formally p(L∗ , U∗ ) ∝ k=1 e−λ·∆(Lk ,Uk )
K
( 4 ) if disjoint intervals else
0
Please note that this is the joint distribution over all intervals since we require their disjointness . The larger λ , the
Fig 3 : Illustration of the generative process using temporally constrained categorical mixture models larger the bias to small anomaly intervals . Per default , if no prior knowledge is given , one should select λ = 0 . In this case , any combination of L∗ , U∗ is equally likely , corresponding to a non informative prior . Note also that λ = 0 is a valid prior since the domain of L∗ , U∗ is finite .
Part 2 : Smoothness of the base behavior . So far , we have not specified any distribution on the base behavior π(t ) . The core idea is that the base behavior should evolve smoothly over time according to the general behavior of the users . That is , we want to incorporate the temporal properties of the data .
As pointed out in [ 6 ] , the ( mean ) parameters of the categorical distribution and their corresponding Dirichlet hyperparameters are not amenable to sequential modeling . Therefore , we exploit a similar idea as proposed in [ 6 , 17 ] : instead of operating on the ( mean ) parameters π(t ) , we operate on the natural parameters ( cf . exponential family [ 5] ) . For the categorical distribution , the natural parameters are b(t ) simply given by the logs of the mean values , ie s = log(π(t ) S ) . While the mean parameters are restricted to lie on the simplex , the natural parameters are unconstrained , leading to an elegant way of sequential modeling . s /π(t )
In the following , we only operate on the natural parameters b(t ) . If the mean parameters are required ( eg as in Equation 2 ) , we can simply apply the transformation
π(t ) s = exp(b(t ) s ) j∈S exp(b(t ) j )
= : π(b(t))s
Note that the term b(t ) S is always 0 . Therefore , we can ignore it for the remainder of the discussion , thus , operating effectively on an S − 1 dimensional space . Given the natural parameters b(t ) at each time index t ∈ T , we model their smoothness by exploiting the idea of linear state space systems [ 5 ] in combination with Brownian motion [ 11 , 17 ] . First , we assume an underlying state space b(t ) which temporally evolves over time via b(t ) ∼ N ( b(t−1 ) , ∆(t ) · Q ) b(1 ) ∼ N ( b0 , Q0 )
( 5 )
We call this space the “ smoothed ” base behavior .
Intuitively , the state of the smoothed base behavior at time t corresponds to the old state plus a small deviation government by the noise covariance matrix ∆(t ) · Q . The larger the time difference between two observed ratings , the larger the corresponding covariance . That is , we effectively allow a higher change in the base behavior if the elapsed time between two ratings is high . If time points are very close to each other , we allow only small changes in the base behavior . In the limit , this discrete time Gaussian random walk corresponds to Brownian motion [ 11 , 17 ] .
This process captures naturally the effects of rating data . In the case of movies , for example , one might see many rat t=0 T1 Star2 Stars3 StarsL₁ U₁ L₂ U₂ time1 star2 stars3 starsL₁ U₁ L₂ U₂ anomaliesevolving base behaviorobserveddata843 ings appearing in short time frames during the time the movie has been released to the theaters and again many ratings a few month later when the DVD has been released . Both time frames potentially describe different base behavior due to different audiences .
Given the smoothed base behavior , the actual base be havior is now obtained by the simple random process b(t ) ∼ N ( b(t ) , R )
( 6 ) which again allows a small deviation between the base behavior and its smoothed counterpart . Note that we do not directly impose the temporal evolution between the variables b(∗ ) , but via the state spaceb(∗ ) . This additional layer is in between the time points . If we would not use b(∗ ) , a single particular beneficial if the number of ratings varies strongly time point with a huge amount of ratings could dominate most of the temporal behavior .
Finally , we add corresponding priors to the newly introduced parameters . By exploiting the fact of conjugacy it follows that Q is drawn from an Inverse Wishart distribution , ie Q ∼ W−1(Ψ0 q can be used to control the smoothness of the base behavior by providing prior knowledge about the noise covariance . Simi larly , R follows an Inverse Wishart distribution and ( b0 , Q0 ) q ) . The parameters Ψ0 q and ν0 q , ν0 a Normal Inverse Wishart distribution .
Summary and Discussion .
Overall , our generative process captures the temporal properties of the data by modeling a smooth base behavior as well as accounting for anomalies which are constrained to occur at certain time intervals . We will show in Section 3 how we perform efficient ( approximate ) inference for this model .
Model Selection . So far , we assumed the number K of anomalies is given . If not apriori known , we can estimate it via model selection . We choose the Bayesian information criterion [ 5 ] . Any other criterion can be used as well . As we will see in Sec 3 , we will integrate out all latent variables except of Θ = {L∗ , U∗ , Q , R} . Thus , increasing the value of K by one , increases the number of free parameters in our model by about 2 ( the lower and upper bound of the new anomaly interval ) . This is a slight overestimate since the intervals need to be disjoint and , thus , they are not completely independently free variables . Given this result , we can choose the K minimizing the BIC equation
BIC(K ) = −2 · lnLK + ( 2 · K + c ) · ln( n(t ) ) t
Here , the constant c denotes the parameters of the model which do not increase when increasing K . Since the value of c does not affect the optimal choice for K , we can simply set it to 0 . The term LK denotes the likelihood of the data when using K anomaly intervals . We can approximate it with the technique shown in Section 3 .
Prediction . Since the base behavior evolves via a linear state space system , we are able to predict the behavior at future points in time . Combining Eq 5 and 6 , it follows b(T +1 ) ∼ N ( b(T ) , R + ∆(T +1 ) · Q )
Thus , given estimates for b(T ) , R , and Q ( cf . Section 3 ) ,
( 7 )
3 . ALGORITHM
While the previous section focused on the model ’s generative process , we now present our learning technique . That is , given a set of observations X we aim at inferring the values of the hidden variables which best describe the observed data . There are multiple ways to formulate this objective . In this work , we treat the variables Θ = {L∗ , U∗ , Q , R} as parameters and we are interested in finding their maximum a posteriori estimate ΘM AP as well as the posterior distribution p(V |X , ΘM AP ) of the latent variables V =
{o∗ , r∗ , z(∗)∗ , b(∗),b(∗)} ( which can then , eg , be used to pick specific realizations of the latent variables ) . 3.1 Variational EM
Since exact inference in our model is intractable , we compute an approximation using variational expectation maximization [ 5 ] . The idea is to approximate p(V |X , Θ ) by a tractable family of parametrized distributions q(V |Ω ) . The parameters Ω are the free variational parameters . These parameters are optimized such that the best approximation between q and p is obtained . This corresponds to the expectation step of the variational EM method . Technically , we minimize the Kullback Leibler divergence between q and p by optimizing over Ω . Using Jensen ’s inequality , minimizing the KL divergence is equivalent to maximizing the following lower bound on the log marginal likelihood [ 5 ] : L(X ; Θ , Ω ) = Eq[ln p(X , V , Θ ) ] + H(q )
( 8 ) where Eq[ . ] denotes the expectation wrt the q distribution and H the entropy . Given an approximation of p(V |X , Θ ) via q(V |Ω ) , we then determine updated parameter values for Θ by again maximizing Equation 8 . This corresponds to the maximization step of the EM method.1 In short , the general processing scheme of our method is to alternatingly maximize L(X ; Θ , Ω ) wrt Ω and Θ . As we will later see , we actually interweave both steps by simultaneously optimizing parts of Θ and Ω . t k k t i q3(z(t ) i )
311 Variational distribution In contrast to the frequently used mean field approxima q1(ok)· q4(b(t ) ) · q5(b(1 ) ) · tion , which assumes a fully factorized distribution , we use p(V | X , Θ ) ≈ q(V |Ω ) := q2(rk)· q5(b(t ) |b(t−1 ) ) via a Kalman filter where it follows that q5(b(t)|b(t−1 ) ) is a Normal distribution given by N ( b(t ) | µt|T , Pt|T ) . For the
We retain the sequential structure of the smoothed base behavior in q5 . Indeed , as described later , we determine q5
· remaining variational distributions we use |φt,i ) q1(ok ) = Dir(ok|αk ) q3(z(t ) q2(rk ) = Beta(rk|αk , βk ) q4(b(t ) ) = N ( b(t)|µ(t ) , v(t ) · I ) where Ω = {α∗ , α∗ , β∗ , φ∗,∗ , µ(∗ ) , v(∗)} are the variational parameters to be optimized . i ) = Bernoulli(z(t ) t>1 i comparing the observed ratings at time T + 1 against the predicted base behavior can be used as an indicator whether new anomalies have been occurred .
1The only actual difference between these steps is that Θ represents a point estimate of the random variables , while Ω represents the hyperparameters of a full distribution .
844 i = x(t ) i ) and q3(z(t )
Note that the distributions q3(z(t ) i ) are identical when x(t ) i , ie when both ratings have the same value . Thus , in practice we do not need to keep track of n(t ) many different distributions at time t but it is sufficient to record S many distributions ; one for each possible evaluation . We denote with φs t the variational parameter of the distribtion q3 for all ratings showing evaluation s at time t . 312 Optimization Procedures As described above , our goal is to update the values of Ω and Θ by maximizing ( or more generally increasing ) the value of Equation 8 . One crucial requirement of our technique was to ensure the efficiency of our method . In the following , we want to highlight the most important results . 3.2 Optimizing the Lower/Upper Bounds
A first naive solution to update the lower/upper bounds of the anomaly intervals would be to test any possible combination . Obviously , this solution is not efficient and requires time O(T 2 ) already for a single anomaly . We provide a principle which is linear in the number of time stamps .
Simultaneous Optimization
We start with the case of a single anomaly and uniform gaps between all time stamps , ie it holds K=1 and ∆(t)=1 for all t . 321 Equation 3 shows the dependency between L1/U1 and z . Intuitively , the bounds act as a switch on the distribution of z : if z is outside of the interval , it is the trivial 0 distribution ; if it is inside , it is a Bernoulli . Accordingly , assuming the posterior distribution for z ( or its approximation q3 ) is given , an optimization of L1/U1 is rather meaningless since one trivially has to capture all time points where the distribution is not the constant 0 . Therefore , we propose to simultaneously optimize L1/U1 and q3 to maximize Equation 8 . Observation : If we know that a time point t fulfills t ∈ [ Lk , Uk ] , the optimal distribution of q3(z(t ) i ) can be computed independent from all other points in time . The optimal distribution is obtained by setting its variational parameter φt,i/φs t to the value as derived in Sec 33 In particular , this value is independent of the actual values of Lk and Uk ( knowing that t ∈ [ Lk , Uk] ) . Based on this result we can also compute the entropy ht,s := H(q3(z(t ) t ln φs i ) ) = −φs t − ( 1 − φs t ) ln(1 − φs t ) i =s . If t ∈ [ Lk , Uk ] , we have q3(z(t ) for all z(t ) 0 ) = 1 and we define the entropy H(q3 ) to be zero . fulfilling x(t ) i i =
Using these results and the derivations of the appendix , as well as removing all terms which are independent of L1 , U1 and q3 , we can reformulate Equation 8 to : ln p(L∗ , U∗)+
Eq[ln p(z(t )
|)+ln p(x(t )
|)]+H(q3(z(t ) i t∈T i∈n(t )
= −λ·∆(L1,U1)+ s ·Eq n(t ) ln π(b(t))s
+ i
S t∈[L1,U1 ] s=1 i
) )
, t∈[L1,U1 ]
[ n(t ) s
· ht,s + n(t ) s
· φs t ] · ( Eq[ln(1 − rk(t) ) ] + Eq[ln π(b(t))s] ) ] t · ( Eq[ln rk(t ) ] + Eq[ln ok(t),s ] ) ln π(b(t))s
+ n(t ) s
· Eq f1(t )
( 9 ) t∈[L1,U1 ]
+ n(t ) s
· [ 1 − φs
S t∈T s=1
= λ +
S s=1 where we used the fact ∆(Lk,Uk ) = Uk − Lk and we defined fk(t ) := −λ+ t·[Eq[ln rk]+Eq[ln ok,s]−Eq ln π(b(t))s s ·φs n(t )
S
] s=1
+ n(t ) s
· [ 1 − φs t ] · Eq[ln(1 − rk ) ] + n(t ) s
· ht,s
Intuitively , the function f1(t ) measures the “ gain ” in the log likelihood when adding t to the anomaly interval .
The first two terms of Eq 9 can be removed since they are constant wrt the bounds and q3 and thus do not affect the optimal solution . Accordingly , maximizing Eq 8/9 wrt L1 , U1 , and q3 is equivalent to solving
∗ 1 , U
( L
∗ 1 ) = arg max ( L1,U1 ) f1(t ) with 1 ≤ L1 ≤ U1 ≤ T
U1 t=L1
Since the function f1 is independent of L1/U1 , ie the terms f1(t ) are constant within the current optimization step , we can record all values f1(t ) in a finite array of length T . Thus , the above problem corresponds to the Maximum Subarray Problem . Using Kadane ’s algorithm [ 4 ] , this problem can be solved in time O(T ) . 322 Non uniform gaps between timestamps So far , we assumed ∆(t ) = 1 for all t ∈ T . We now generalize the above result to handle varying values for ∆(t ) . Wlog , due to finite precision in the measurement of time ( eg UTC timestamps are usually measured in seconds ) , we can assume ∆(t ) ∈ N+ . Thus , a naive approach to handle the scenario of non uniform gaps is to “ blow up ” the actual data by “ artificial ” time points where no ratings occur . After including the artificial time points , the ∆ values are again equal to 1 , and the previous technique can be applied . Figure 4 top and middle show this principle . Obviously , this principle is not suitable for huge time gaps and the new size of the array can be arbitrarily large . Considering f1(t ) , it becomes apparent that its value evaluates to −λ for each artificial time point . When searching for the subarray with maximal sum , these negative entries will never occur at the beginning/end of the anomaly interval [ 3 ] . If they would be at the beginning/end , one could easily shorten the interval to obtain a new one with higher sum . Thus , artificial time points are either completely contained in the interval or not included at all .
Fig 4 : Handling non uniformity
Using this result , we can safely “ merge ” all adjacent artificial time points to a single one with the function value −λ · u(t ) , where u(t ) is the number of artificial time points between time index t and t − 1 . Clearly , u(t ) = ∆(t ) − 1 and the number of merged artificial time points is bounded by T − 1 . Overall , we can define a new array f of size 2· T − 1 where f k(y ) = fk( y+1 2 ) −λ · ( ∆( y
2 +1 ) − 1 ) if y is odd if y is even for y ∈ [ 1 , 2 · T − 1 ] . And we now solve the problem
∗
∗
, b
( a
) = arg max ( a,b )
1(y ) f b y=a and set ( L∗ bounded by 2 · T − 1 the runtime complexity is O(T ) .
) . Since the size of f is
1 ) = ( a∗+1
, b∗+1
1 , U∗
2
2 t 1t757755Δ(t)=4 λ λ λ 3λ f(x)f' ( y)845 323 Extension to multiple intervals We now extend our result to multiple different anomalies/intervals . Using multiple anomalies affects the choice of the optimal q3 distribution ( cf . paragraph ’Observation’ in Section 321 ) It is no longer sufficient to know whether t ∈ [ Lk , Uk ] but we also have to know which k we consider.2 Accordingly , for each anomaly k we have to use its individual function fk/f k to measure the gain of adding a time point to the anomaly interval k . Overall , maximizing Eq 8 using multiple intervals corresponds to solving arg max
( a1,b1),,(aK ,bK ) k(y ) with ak ≤ bk < ak+1 f
( 10 ) bk k∈K y=ak
1(t)}
1(1 )
1(t ) , f
We solve the above problem by a dynamic programming technique . The necessary recursions are given by g(1 , t ) = max{g(1 , t − 1 ) + f k(k ) k(t ) , m(k − 1 , t − 1 ) + f g(1 , 1 ) = f g(k , k ) = m(k − 1 , k − 1 ) + f k(t)} g(k , t ) = max{g(k , t − 1 ) + f m(k , t ) = max{g(k , t ) , m(k , t − 1)} m(k , k ) = g(k , k ) Here , m(k , t ) ( for t ≥ k ) denotes the maximal value of Eq 10 when using k intervals and data up to point t . In contrast , g(k , t ) denotes the maximal value of Eq 10 when the kth interval is forced to end at the t th point in time ( using k intervals and data up to t ) . Obviously , g(k , t ) ≤ m(k , t ) holds . The value of m(K , T ) is the optimal value of Eq 10 . We only provide a brief idea of these recursions : Assume we know the optimal intervals when using k − 1 anomalies and data up to time point t − 1 . Let these intervals be denoted ( L1 , U1 ) , . . . , ( Lk−1 , Uk−1 ) . Additionally , assume the optimal intervals for k anomalies and data up to time point t− 1 are given , denoted with ( L k ) . Finally , assume the optimal intervals are given when the last interval is forced to end at time t − 1 ( we call these the g intervals ) . k = t − 1 . Denote these with ( ˆL How will the solution for k intervals and data up to t look like ? We can distinguish the following cases : ( 1 ) The time point t will be included in the optimal intervals . Obviously , since we are at the last point in time , it can only be represented by the kth interval . We can distinguish two subcases : ( 1a ) The time point t is the beginning of the kth interval . In this case , the optimal intervals are ( L1 , U1 ) , . . . , ( Lk−1 , Uk−1 ) , ( t , t ) and m(k , t ) = m(k − 1 , t − 1 ) + f k(t ) . Since the last interval already ends at t , we also have g(k , t ) = m(k − 1 , t − 1 ) + f k(t ) . ( 1b ) The time point t is not the beginning of the kth interval . Thus , the opti1 , ˆU mal solution needs to be ( ˆL k , t ) and we obtain m(k , t ) = g(k , t ) = g(k , t − 1 ) + f
1 ) , . . . , ( ˆL k(t ) . k ) , here ˆU
1 ) , . . . , ( ˆL
1 ) , . . . , ( L k , ˆU k , U
1 , ˆU
1 , U
1 , U k , U
1 ) , . . . , ( L
( 2 ) The time point t will not be included in the optimal intervals . In this case , since we want to find k intervals , the optimal solution is ( L k ) and m(k , t ) = m(k , t − 1 ) . For the g intervals we have to distinguish two cases : ( 2a ) The time point t is the beginning of the kth g interval . In this case , the new g intervals are ( L1 , U1 ) , . . . , ( Lk−1 , Uk−1 ) , ( t , t ) and g(k , t ) = m(k − 1 , t − 1 ) + f ( Note that we use the optimal intervals from m(k− 1 , t− 1 ) , not the g intervals! ) ( 2b ) The time point t is not the beginning of the kth g interval . Thus , leading to the solution ( ˆL k(t ) . 2Technically , we could write φs rameter of q3 assuming t ∈ [ Lk , Uk ] . We omitted k for brevity . k , t ) with g(k , t ) = g(k , t−1)+f t,k to denote the optimal hyperpa
1 ) , . . . , ( ˆL
1 , ˆU k(t ) .
Exploiting the fact g(x , y ) ≤ m(x , y ) and that we want to maximize m(x , y ) , leads to the recursion as defined above . It is easy to add data structures to the method which record the start/end positions of the optimal intervals . Solving the above recursions via dynamic programming , we obtain :
Theorem 1 . The optimal values for L∗ , U∗ and the opti
3 can be computed in time O(K · T ) . mal distributions q∗ 3.3 Optimization of q1 , q2 , q3
Following the principle of [ 5 ] , the optimal distribution for q3 can be determined by i ) = E
∗ 3 ( z(t ) ln q q\z
( t ) i
[ ln p(X , V , Θ ) ] + const i
Here , the constant const absorbs all terms which are independent of z(t ) and , thus , do not affected the optimal distribution of q3 . The term E [ . ] denotes the expectation with respect to the distribution q taken overall all variables . Assuming k(t ) = k = 0 , and using the results except of z(t ) from the appendix , it follows that q\z
( t ) i i i = 1 ) = Eq[ln rk ] + Eq[ln ok,s ] = : x
∗ 3 ( z(t ) ln q i = 0 ) = Eq[ln(1 − rk ) ] + Eq[ln π(b(t))s ] = : y i . Therefore , the optimal value of the variational ln q
∗ 3 ( z(t ) where s = xt parameter is φt,i = φs t = ex ex+ey .
The same principle can be applied for the distributions q1 and q2 , leading to
Uk
αk = ˆα +
S
αk,s = ( ˆα)s + n(t ) s s · φs n(t ) t t=Lk βk = ˆβ + t
· φs
Uk
S s · ( 1− φs n(t ) t )
Uk t=Lk s=1 t=Lk s=1
3.4 Remaining Optimizations
Optimizing the base behavior . The base behavior can be updated for each b(t ) independently . Removing all terms from Equation 8 which are independent of b(t ) leads to Eq[
| ) ] + Eq[ln p(b(t)|b(t−1 ) , R ) ] + H(q4(b(t) ) ) ln p(x(t ) i i
·Eq
2
Eq
=− 1 2
( 11 ) The first term is given in the appendix , and H(q4(b(t) ) ) = S−1 ln(2πev(t) ) . For the second term we derive : ln p(b(t ) | ) b(t)T · R−1 · b(t )
( b(t)−b(t))T · R−1·(b(t)−b(t ) ) b(t)T · R−1 ·b(t )
T r(R−1 · v(t ) ) + µ(t)T · R−1 · µ(t )
+µT t|T · R−1µ(t ) + c3 We absorbed all terms which are independent of b(t ) into the constants ci . Overall , Eq 11 can be written as a function of µ(t ) and v(t ) , which we optimize using gradient ascent .
= − 1 2
= − 1 2
+ Eq
+ c2
Eq
+c1
Optimizing the smoothed base behavior . Since our model corresponds to a linear system , we can use a Kalman filter/smoother to determine the distribution of q5 . We use the Rauch Tung Striebel smoother . Thus , the distribution of q5 can be computed efficiently by a forward and backward pass , leading to an update with runtime complexity O(T ) . Since the outputs b(t ) of the dynamic system are not observations but distributions , we slightly adapt the Kalman
846 update/innovation equations . Following the standard calculus of Kalman filters , the predicted mean and covariance matrix for time t ( given data up to time t − 1 ) are given by
µt|t−1 = µt−1|t−1 and Pt|t−1 = Pt−1|t−1 + ∆(t ) · Q . Given computed as et = Eq[b(t)]−µt|t−1 . Accordingly , the residual the measurement at time t , the measurement residual can be covariance is given by St = Pt|t−1 + R + v(t)· I . Note the increased variance due to the uncertainty of the base behavior . Letting the Kalman gain be defined by Kt = Pt|t−1·S , we see that the Kalman gain is smaller for time points showing a high variance , ie high uncertainty , in the base behavior . These points affect the smoothed base behavior less strongly . Continuing with the standard calculus , the updated mean and covariance areµt|t =µt|t−1+Kt·et and Pt|t = ( I−Kt)· leads to µt|T = µt|t + Ct · ( µt+1|T − µt+1|t ) and Pt|T =
Pt|t−1 · ( I − Kt)T + Kt · R · KT t . Here , we used the Joseph form of the covariance update equation since it holds for any value of Kt . For the backward pass , the RTS smoother Pt|t +Ct·(Pt+1|T −Pt+1|t)·C T −1 t+1|t . Due to space limitations , we kindly refer to the rich literature on Kalman filter/smoother , for details about the derivations . t , with Ct = Pt|t·P
−1 t
Optimization of Q and R . Updating Q and R follows from the properties of the conjugate prior . Note that Eq 5 can also be written in the formb(t)−b(t−1 ) ∼ N ( 0 , ∆(t)·Q ) .
√ Also , by the definition of the normal distribution it holds ∆|0 , Σ ) , where d is the that N ( x|0 , ∆ · Σ ) = ∆−d/2 · N ( x/ dimensionality of the distribution . Since the terms ∆−d/2 are constant when optimizing the log likelihood , they can be ignored . Thus , Q can be seen as the covariance matrix of a Normal distribution with known mean of zero . Correspondingly , we can use the Inverse Wishart distribution as its ( conjugate ) prior . Following the results of conjugacy , the posterior distribution of Q is an Inverse Wishart distribution W−1(Ψq , νq ) with νq = ν0 q + T − 1 and scale matrix b(t ) −b(t−1)T ·b(t ) −b(t−1 )
1
Ψq = Ψ0 q +
Eq
T t=2
∆(t ) which can easily be computed by plugging in the known expectations . Given this distribution , the MAP estimate for Q can efficiently determined by selecting the mode of · Ψq . the Inverse Wishart distribution , ie Q∗ = The same principle can be applied for R . 3.5 Overall Processing and Complexity
( S−1)+1+νq
1
Using the above optimizations and update equations , our method iteratively recomputes the values for Θ and Ω . If the change in Equation 8 is less than 0.1 % we assume convergence and terminate . Based on the previous results , and assuming that K , S << T , each iteration is linear in the number of time stamps , ie we have a complexity of O(T ) . 4 . RELATED WORK
Spotting anomalies in rating data : So far , only [ 7 ] considers the temporal analysis of rating data incorporating potentially anomalous behavior . The work models the rating data as distributions over time . As mentioned in the introduction , it requires an aggregation/binning of the data and it cannot handle intervals of anomalies . We compare our technique against [ 7 ] in the experimental analysis .
Modeling of temporal continuous data : Similar to the work [ 7 ] , traditional time series modeling methods such as vector autoregression [ 14 , 13 ] or Kalman filter/smoother
[ 5 ] , analyze continuous data . They are not directly suited for our scenario of categorical data ( or require a problematic binning ) . Furthermore , traditional approaches for time series modeling are sensitive to outliers . Thus , these models fail to find good approximations of the data corrupted by anomalies . Therefore , robust techniques to handle outliers have been proposed [ 16 ] . These methods are designed to handle outliers which are attributed to mostly independent , random corruptions of the data , while our work is designed to handle anomalies following a specific pattern .
Since in our work the Kalman filter operates on the ( clean ) base behavior , ie the anomalies have been ’removed’ by the other mixture model components , the problem of anomalies is circumvented . We compare our technique against a Kalman filter in the experimental analysis .
Modeling of temporal documents : One might represent the ratings at a certain point in time as a document with the words corresponding to the ratings’ evaluations . Modeling temporal document collections is handled by dynamic topic mining [ 6 , 17 , 2 ] . Applying these methods on the ’documents’ generated via the ratings is questionable since each document most likely would contain only a single ’word’ . Ignoring this issue , further problems for our scenario are : First , [ 6 , 2 ] require a binning of the documents in fixed time slots . Second , [ 6 , 17 ] require that topics exist over the whole lifetime . In our work , however , anomalies exist only in specific time intervals . While [ 2 ] allows topics to appear and disappear , they prefer smooth evolutions . In our case , however , anomalies abruptly appear/disappear in time . Also , all of these techniques are ( of course ) designed to detect multiple topics . In our scenario , however , we want to find a single base behavior which captures the general temporal evolution , enriched by a few number of anomalies .
Related applications : Multiple techniques have been proposed in the area of outlier detection [ 1 ] . While the majority of techniques tackles the case of independently distributed data , time series outlier detection and outlier detection for streaming data are also an active field of research [ 1 ] . Both areas differ from our work since they are designed for continuous data . Also , most existing techniques consider outlier in the sense of independent , random errors in the data . Change detection techniques detect points in time where the state of the underlying system has changed [ 15 ] . A change might not generally indicate anomalous behavior . Indeed , even the base behavior might change over time .
Studying product ratings has been done in multiple research areas , all following different goals and objectives . Recommender Systems incorporate ratings and their temporal information [ 9 , 10 ] to improve the prediction performance . Opinion mining aims at extracting the sentiment of users regarding specific products or features of a product [ 18 ] . Modeling of social rating networks , eg , to compactly describe the underlying mechanism driving the network or to generate synthetic data , has been studied , e.g , in [ 12 ] .
None of the existing methods is designed to detect anomalies and the underlying evolving base behavior in rating data .
5 . EXPERIMENTAL ANALYSIS
We applied our method ( called SpotRate due to its potential to spot anomalies in rating data ) on over six million product ratings representing varies categories : an extract of the Amazon website [ 8 ] evaluating multiple different products , another subset of the Amazon website evaluating
847 Fig 5 : Runtime vs . number of time stamps
Fig 6 : Likelihood and BIC vs . K ( synthetic data )
Fig 7 : Likelihood and BIC vs . K ( real world data ) food products3 , ratings of restaurants in the area of Phoenix based on Yelp , and an extract of the TripAdvisor website4 for hotel ratings . The data consists of tuples representing the ID of a product/service to be rated , the user who evaluated the product , the time stamp when the rating occurred , and a star rating in the range from 1 up to 5 . Additionally , these datasets contain textual reviews , which we used to understand and describe the results of our method .
Besides these real world datasets we used synthetic data generated based on the presented process to analyze the scalability and robustness of our method . 5.1 Runtime Analysis
We briefly analyze the runtime of SpotRate . The runtime is primarily affected by the number of time stamps T and the rating scale S . The actual number of ratings does not affect the runtime ( cf . Sec 311 ) For the runtime analysis , we selected the product B00003TL7P from the Amazon dataset and we extended it to different length ( from 1,000 to 100,000 ) by concatenation . Besides the original rating scale of S = 5 , we used a rating scale of S = 3 by merging 1/2 and 4/5 ratings . All experiments were conducted on commodity hardware with 3 GHz CPU ’s and 4 GB main memory .
The results are shown in Fig 5 . Confirming our study of Sec 3.5 , the runtime increases linearly , showing the method ’s high scalability ( note the slope of 1 in the log log plot ) . The overall runtime for 100,000 time stamps ( which would corresponds to 273 years when measured on a daily basis ) is only about 158 minutes on commodity hardware . A brief study shows that the currently most rated products have around 20,000 ( Amazon : Kindle Fire ) or 8,000 ( Yelp : Bottega Louie ) ratings . Thus , even when considering the finest granularity , we highly exceed this number .
Additionally , we measured the runtime of our method when ignoring the time required for the Kalman smoother ( dashed lines ) . As shown , the Kalman smoother contributs to around 90 % of the absolute runtime . The remaining parts of our method are highly efficient .
We also studied the effect of the number of anomalies K on the runtime . According to Sec 323 , K linearly affects the runtime of the dynamic programming technique . Since the Kalman smoother ( whose runtime is independent of K ) accounts for most of the absolute runtime , we only observed a very small change of only a few seconds . Thus , overall , only T and S influence our method ’s practical applicability . 5.2 Effectiveness
In the following , we analyze the effectiveness of SpotRate considering different aspects . We start with the model se
3http://snapstanfordedu/data/ 4http://sifakacsuiucedu/~wang296/Data/ lection principle . For this experiment , we generated synthetic data according to our model . We used 4000 ratings with 5 anomaly intervals . Figure 6 shows on the ( first ) yaxis the obtained log likelihood of our method when varying the number K of potential anomalies . Obviously , the general trend shows that increasing K also increases the loglikelihood : more flexibility to describe the data is given . A very high increases is obtained until the value of 5 , which corresponds to the true number of anomalies . After this point , the benefit of allowing further anomalies decreases .
This effect is well captured by the BIC score , which is shown on the second y axis of the figure . The minimal BIC value is obtained for the value of 5 . Thus , the model selection principle introduced before can be used as a good indicator how to select the number of anomalies .
The same behavior can be observed for real world data as , eg , shown in Fig 7 . Here we plotted the log likelihood and BIC score for a coconut water sold on Amazon ( cf . Sec 54 ) Again , one sees a clear minimum of the BIC value , indicating that three anomaly intervals describe the data very well .
Next , we analyze our iterative optimization . In Fig 8 we analyze how the log likelihood increases when we increase the number of iterations until convergence . That is , on the x axis we count how often the variables have been updated , while the y axis shows the log likelihood . We plotted the curves for different values of K , again for the product B00003TL7P . As expected , the first iterations lead to the highest improvement in the log likelihood . Still , we see an improvement in the later iterations , showing the effectiveness of the optimization step . As also shown in the previous experiment , a higher value of K leads to a better likelihood . Additionally , for this product , we observed that a smaller number of intervals can lead to a lower number of required iterations . In general , however , the difference in the number of iterations was not as significant as shown for this product . Finally , we analyze the effect of λ . Per default , a value of 0 can be selected to realize a non informative prior . In Figure 9 , we varied the value of λ between 1 and 0 . We selected K = 10 . As shown , for larger values of λ , shorter intervals are preferred . In particular , for λ = 1 the average interval length is close to the shortest possible length of 1 . For λ = 0 larger intervals are captured . Note that λ = 0 does not mean that the whole set of time stamps is represented as an anomaly interval . Even in the case of λ = 0 , we only report time intervals where the behavior is anomalous .
5.3 Comparison with related techniques
We compare SpotRate against the related technique RLA [ 7 ] and a Kalman smoother . Doing a fair comparison between these approaches is challenging since the data they analyze and goals they follow are different . In particular , the
110100100010000100010000100000runtime [sec]number of time stampsS=5 (overall)S=5 (w/o Kalman)S=3 (overall)S=3 (w/o Kalman)1090011000111001120011300‐5500‐5480‐5460‐5440‐5420‐540001020BIClog likelihoodnumber of anomaliesLog likelihoodBIC curve12001300140015001600‐750‐700‐650‐600‐55001020BIClog likelihoodnumber of anomalies Log likelihoodBIC curve848 Fig 8 : Convergence analysis
Fig 9 : Effect of λ
Fig 10 : Comparison with related methods work [ 7 ] requires an aggregation of the data since it operates on the rating distributions . Thus , eg , measuring the likelihood on the categorical data is questionable . Since RLA , however , is the only existing technique which also analyzes dynamic rating data , we try to study some effects .
For comparing the methods , we use two principles : In the first experiment , we compare the base behavior detected by the methods against the known base behavior for synthetically generated data . The base behavior is continuous in our model as well as in RLA , and for the Kalman smoother . Thus , it is fair to , eg , measure the Frobenius norm between the ground truth base behavior and the detected ones . We generated data with 1000 time points and added a varying number of anomaly intervals to it , each covering 10 time points . We ensured that the anomaly intervals exactly match the aggregation required for RLA . Thus , this method gets a huge advantage since this assumption does not necessarily hold for real data . Fig 10 ( left ) shows the results : Our method obtains the lowest error , it is able to detect the hidden base behavior . The Kalman smoother cannot handle anomalies and shows a high deviation to the ground truth . In a second experiment , we evaluated whether the methods are able to detect the anomalous points in time ( this is only possible for SpotRate and RLA ) . As shown in Figure 10 ( right ) , our method almost perfectly detects all anomalous points in time . RLA in contrast is not able to spot all points , which also explains the previously observed higher error to the base behavior . Overall , our method outperforms the competing techniques in detecting the correct base behavior as well as spotting the anomalous points in time . 5.4 Discoveries
( 1 ) We start with the example illustrated in Fig 1 .
In the following , we will demonstrate the application of SpotRate by illustrating some of our interesting discoveries . It represents a hotel in the Caribbean evaluated on TripAdvisor . While understanding the original time stamped data is difficult , the extracted base behavior allows an easy understanding : clearly , the hotel is evaluated with mostly 4 and 5 stars . Our method found anomalous behavior in July and August 2005 . In this time frame , the negative ratings highly increased . Analyzing the reviews at the detected time points , the reviewers criticized “ the restaurants with ridiculous reservation rules’ ’ often showing overbooking and “ the nonfunctional air conditions ” . These reviews indicate that in the given months the service of the hotel has dropped , potentially due to a highly increased number of guests . Our method was able to spot these anomalies , and it successfully smoothed out these points from the base behavior .
( 2 ) Next , we show the result for a coconut water sold on Amazon ( http://wwwamazoncom/dp/B000CNB4LE ) Applying our method leads to the base behavior as shown in Figure 11 . The three detected anomaly intervals appear at the end of 2010 . As shown next to the figure , the detected anomalies are described by distributions ok representing primarily low ratings . They clearly deviate to the base behavior . Inspecting the product ’s reviews during these times , most customers are not satisfied with the “ new plastic bottles ” the manufacturer has introduced , leading to a bad taste . Later time points do not show this anomalous behavior , indicating that the manufacturer has solved this problem ( “ I can understand a lot of the initial bad reviews as I thought the new plastic bottle had a bad after taste . . . . I can say that the taste is much improved . . . ” ) .
( 3 ) Next , we want to show the benefit of extracting an evolving base behavior . Figure 12 shows the base behavior of a baby bouncer ( B00005QI1G ) from the Amazon data . Looking at its evaluation , it is recognizable that the majority of reviewers evaluated this product with 5 stars . At the later time points , however , the number of low and medium ratings increases . Note that these intervals are not classified as anomalies but they represent the general evolution of the product . A closer look at the product ’s reviews at these time points explains that over time the customers were more and more unsatisfied by the product since it is “ nice to play but not long lasting ” and the “ battery simply does not last very long with the vibrating feature ” .
Discoveries via prediction . Finally , we want to show the potential of our method to detect anomalies via prediction . According to Eq 7 , we can predict the base behavior at future points in time . By comparing it against newly arriving ratings , anomalies can be spotted . We removed from all restaurants of the Yelp dataset the last 10 points in time . We applied our method on the remaining data . Figure 13 shows three restaurants whose predicted base behavior ( left bar [ a ] of each diagram ) highly deviates to the observed ratings ( right bar [ b] ) , thus , potentially indicating anomalies .
Inspecting the reviews of the first restaurant , we see comments like “ I’ve been eating at Stacy ’s for over a year so it pains me to kill them but the service [ ] was pathetic . [ ] I don’t know if its a new employee or something going wrong but I’m probably not going back ” . Thus , indicating that the service quality of the ( previously very highly rated ) restaurant has suddenly dropped .
For the second restaurant , we observed comments like “ All the prices have went up ” and “ The picture of the menu and prices is out dated ” , which again indicates a recent deviation to the previous behavior , potentially due to increased prices . Finally , the reason for the abruptly appearing low ratings of the third restaurant seems to be caused by expanding/remodeling the old building . The old atmosphere of the restaurant seems not to be preserved and the larger capacity could not be handled by the service staff : “ the expanded building is nice [ ] but something was lost . we didn’t have
‐1500‐1400‐1300‐1200‐11000100200300log likelihoodnumber of iterations5 intervals15 intervals25 intervals01234100033008002000avg length  of intervalsλ0510151 anomaly5 anom10 anomerrorSpotRateRLAKalman0204060801001 anomaly5 anom10 anomdetected anomaliesSpotRateRLA849 Fig 11 : Base behavior and anomalies ( intervals spotted at the end of 2010 ) .
Fig 12 : Evolving base behavior ( Amazon )
Fig 13 : Spotting anomalies via prediction ( Yelp ) that hometown feel . we miss all the signatures and pictures . ” and “ I think its rad that you expanded , but if you cant handle the customer load then whats the point ? ” .
Overall , by modeling the temporal evolution of the base behavior , our method is able to detect these newly occurring anomalies , which can then , eg , be used to inform the corresponding companies . 6 . CONCLUSION
We developed the method SpotRate for analyzing time stamped rating data . Our method detects the users’ base behavior as well as time intervals representing anomalies . We proposed a sound Bayesian framework which represents the rating data via temporally constrained categorical mixture models . It accounts for the temporal evolution of the base behavior and enables us to predict the rating behavior for newly occurring ratings . We developed an efficient algorithm which exploits principles of variational inference and dynamic programming . Our experimental study has shown the potential of our method to spot anomalies and to use the base behavior for studying the evolution of a product . Acknowledgments . Stephan G¨unnemann has been supported by a fellowship within the postdoc program of the German Academic Exchange Service ( DAAD ) . Research was also sponsored in part by the Army Research Laboratory and was accomplished under Cooperative Agreement Number W911NF 09 2 0053 . Any opinions , findings , and conclusions or recommendations expressed in this material are those of the author(s ) and do not necessarily reflect the views of the funding parties . The US Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on . 7 . REFERENCES [ 1 ] C . C . Aggarwal . Outlier Analysis . Springer , 2013 . [ 2 ] A . Ahmed and E . P . Xing . Timeline : A dynamic hierarchical dirichlet process model for recovering birth/death and evolution of topics in text stream . In UAI , pages 20–29 , 2010 .
[ 3 ] F . Bengtsson et al . Computing maximum scoring segments in almost linear time . In Computing and Combinatorics , pages 255–264 . Springer , 2006 .
[ 4 ] J . Bentley . Programming pearls : algorithm design techniques . Communications of the ACM , 27(9):865–873 , 1984 .
[ 5 ] C . M . Bishop . Pattern recognition and machine learning . Springer , 2007 .
[ 6 ] D . M . Blei and J . D . Lafferty . Dynamic topic models .
In ICML , pages 113–120 , 2006 .
[ 7 ] N . G¨unnemann , S . G¨unnemann , and C . Faloutsos .
Robust multivariate autoregression for anomaly detection in dynamic product ratings . In WWW , pages 361–372 , 2014 .
[ 8 ] N . Jindal and B . Liu . Opinion spam and analysis . In
WSDM , pages 219–230 , 2008 .
[ 9 ] N . Koenigstein , G . Dror , and Y . Koren . Yahoo! music recommendations : modeling music ratings with temporal dynamics and item taxonomy . In RecSys , pages 165–172 , 2011 .
[ 10 ] Y . Koren . Collaborative filtering with temporal dynamics . In KDD , pages 447–456 , 2009 .
[ 11 ] G . F . Lawler . Introduction to stochastic processes .
CRC Press , 2006 .
[ 12 ] K . Lerman . Dynamics of a collaborative rating system . In WebKDD/SNA KDD , pages 77–96 , 2007 .
[ 13 ] R . B . Litterman . Forecasting with bayesian vector autoregressions . Journal of Business & Economic Statistics , 4(1):25–38 , 1986 .
[ 14 ] H . L¨utkepohl . New introduction to multiple time series analysis . Cambridge University Press , 2005 .
[ 15 ] X . Song , M . Wu , C . M . Jermaine , and S . Ranka . Statistical change detection for multi dimensional data . In KDD , pages 667–676 , 2007 .
[ 16 ] J A Ting , E . Theodorou , and S . Schaal . Learning an outlier robust kalman filter . In ECML , pages 748–756 , 2007 .
[ 17 ] C . Wang , D . M . Blei , and D . Heckerman . Continuous time dynamic topic models . In UAI , pages 579–586 , 2008 .
[ 18 ] H . Wang , Y . Lu , and C . Zhai . Latent aspect rating analysis on review text data : a rating regression approach . In KDD , pages 783–792 , 2010 .
[ [z k
( t ) i =0]]]=1 − φt,i
Eq[ln ok,s]=ψ(αk,s ) − ψ(
APPENDIX Let [ [. ] ] denote the Iverson bracket , it holds : Eq[[[z(t ) i =1]]]=φt,i Eq[[[z(t ) j αk,j ) Eq[ln rk]=ψ(αk)−ψ(αk+βk ) Eq[ln(1−rk)]=ψ(βk)−ψ(αk+βk ) Given the definition of the distribution p , it follows : • For k(t ) = k = 0 it holds : • Eq[ln p(z(t ) | . . . ) ] = Eq[ln r • = Eq[[[z(t ) • Eq[ln p(x(t ) • Eq[[[z(t )
· ( 1 − rk)[[z i = 0] ] ] · Eq[ln(1 − rk ) ] i = 1] ] ] · Eq[ln ok(t),s]+ i =1] ] ]
( t ) i =1 ] ]
| ) ] = Eq[[[z(t ) i = 1] ] ] · Eq[ln rk ] + Eq[[[z(t ) S i = 0] ] ] · Eq[ln π(b(t))s ] for x(t ) | ) ] = Eq i ln p(x(t ) s=1 n(t ) t ·Eq[ln ok(t),s ] + n(t ) s · φs s=1 n(t ) µ(t ) 1+S−1 = Eq ln π(b(t))s
• Eq[ • =S ln(1 +S−1 s − ln(1 +S−1
• Eq • Eq • = µ(t ) s=1 eb(t ) i )
( t ) eb s s=1 eb s − ln(Eq s + v(t )
−
1 +S−1 b(t ) s i = s · ln p(x = s | . . . ) s · [ 1 − φs = Eq t ]·Eq [ ln π(bt)s ] s=1 eb(t ) s
)
( t ) s=1 eµ
2 ) s
( t ) s i i i ln
0%50%100%010420100104201101042012detected anomalies0%50%100%04012002040720030401200534%124%226%48%452%223%2325%13252120%2ababab5 stars4 stars3 stars2 stars1 starpredicted base behavior (a) vs spotted anomaly(b)850
