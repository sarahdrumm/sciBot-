Large Scale Adaptive Semi Supervised Learning via
Unified Inductive and Transductive Model
[ Extended Abstract ]
De Wang ∗
Feiping Nie ∗
Heng Huang †
University of Texas at Arlington
University of Texas at Arlington
University of Texas at Arlington
Department of Computer Science and Engineering ,
500 UTA Boulevard
Arlington , Texas,76019 0015 wangdelp@gmail.com
Department of Computer Science and Engineering ,
500 UTA Boulevard
Arlington , Texas,76019 0015 feipingnie@gmail.com
Department of Computer Science and Engineering ,
500 UTA Boulevard
Arlington , Texas,76019 0015 heng@uta.edu
ABSTRACT Most semi supervised learning models propagate the labels over the Laplacian graph , where the graph should be built beforehand . However , the computational cost of constructing the Laplacian graph matrix is very high . On the other hand , when we do classification , data points lying around the decision boundary ( boundary points ) are noisy for learning the correct classifier and deteriorate the classification performance . To address these two challenges , in this paper , we propose an adaptive semi supervised learning model . Different from previous semi supervised learning approaches , our new model needn’t construct the graph Laplacian matrix . Thus , our method avoids the huge computational cost required by previous methods , and achieves a computational complexity linear to the number of data points . Therefore , our method is scalable to large scale data . Moreover , the proposed model adaptively suppresses the weights of boundary points , such that our new model is robust to the boundary points . An efficient algorithm is derived to alternatively optimize the model parameter and class probability distribution of the unlabeled data , such that the induction of classifier and the transduction of labels are adaptively unified into one framework . Extensive experimental results on six real world data sets show that the proposed semi supervised learning model outperforms other related methods in most cases .
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications Data Mining
General Terms Algorithms ∗
Authors contributed equally to this paper .
†
Corresponding author .
Keywords Semi supervised learning ; Large scale semi supervised learning ; Unified inductive and transductive model
1 .
INTRODUCTION
In most data mining applications , the data are generally abundant , however , labeled data is often scarce . Labeling data is a tedious work , and costs huge amount of time and money . In this situation , how to fully utilize the abundant unlabeled data becomes very important .
Semi supervised learning is a learning paradigm that suits for this situation , where both the labeled data and unlabeled data are used to learn the prediction model . There are two types of semisupervised learning models : transductive learning models and inductive learning models . Transductive semi supervised learning methods learn the labels of unlabeled data by propagating the label from labeled data to unlabeled data . The drawback of this kind of methods is that they can not be used for out of sample testing ( ie , new testing data from not included in the unlabeled data ) . So when a new testing data arrives , such methods need to merge those new testing data into the previous data we have , and then reconstruct the whole model based on the merged data . Obviously , such a way is very inefficient for the testing of new out of sample data . Inductive semi supervised learning methods learn a classifier using both labeled data and unlabeled data . Then the learned classifier can be used for the classification of both unlabeled data using for training and also new out of sample testing data . In view of the convenience of out of sample testing , inductive semi supervised learning methods are more attractive in practice .
Many graph based learning approaches have been proposed in recent years [ 1 , 3 , 5 , 6 , 13 , 14 , 15 , 20 ] . Some of the most representative graph based semi supervised learning models are : local and global consistency ( LGC ) [ 18 ] , random walk ( RW ) [ 19 ] , and gaussian field harmonic function ( GFHF ) [ 21 ] , Laplacian regression [ 12 ] , and semi supervised discriminant analysis [ 2 ] . All of these models utilize the Laplacian graph and propagate the labels over the graph . Therefore , in order to use these models , an n × n graph Laplacian matrix has to be built beforehand . However , the computational cost of building the n × n graph Laplacian matrix is at least O(n2 ) . Such computational cost is daunting in the circumstance of large scale data , where the number of data can easily reach billion level . So such graph based algorithms are not scalable to large scale data .
482 50
40
30
20
10
0
−20
0
20
Figure 1 : Data samples with boundary points ( the black circles ) . The red circle point and the blue circle point are the labeled data points . All the rest data are unlabeled .
On the other hand , when we do classification , there are many data points lie around the decision boundary , which we call boundary points in the paper . These data points are very noisy for learning the correct classifier , and thus will deteriorate the classification performance of the learned classifier .
To address the above two challenges , a large scale adaptive semisupervised learning model is proposed in this paper . The proposed semi supervised learning model has many good characteristics which will be discussed in detail after we introduce the model .
2 . NEW LARGE SCALE SEMI SUPERVISED
LEARNING MODEL
2.1 Motivation
Most semi supervised learning methods are based on graph Laplacian matrix , like in the following works : local and global consistency ( LGC ) [ 18 ] , random walk ( RW ) [ 19 ] , and gaussian field harmonic function ( GFHF ) [ 21 ] , Laplacian regression [ 12 ] , and flexible manifold embedding ( FME ) [ 8 ] . One major drawback of such kind of methods is that the computational cost of constructing the n × n graph Laplacian matrix is very high , which is at least O(n2 ) . What ’s more , if we use the gaussian kernel to construct the graph , the bandwidth parameter σ should be tuned carefully in order to achieve good performance . This makes the Laplacian graph based semi supervised learning methods impractical to solve large scale applications in which the number of samples n is often more than billion . If we can develop semi supervised learning methods without constructing the graph Laplacian matrix , most of the computational cost can be avoided , such that the method can be applied to large scale data .
On the other hand , when we do classification , there exist many bad data points that lie around the decision boundary . Consider the situation demonstrated in Figure 1 . The red circle point and the blue circle point are the labeled data points . Considering the distribution of the data , the ideal decision boundary to classify these data points into two classes should be close to the vertical line x = 0 . However , there are many data points lie around the decision boundary ( the black circles in the figure ) . We call these points boundary points in the following . These boundary points will blur the clear distribution of the whole data , and are very noisy for learning the correct classifier . If these boundary points dominated the loss function , the learned classifier maybe distorted far way from the ground truth . In the example of Figure 1 , if we consider too much of these boundary points , the learned classifier may become a horizontal line close to y = 25 . If we do not consider these boundary points , and consider only the remaining clearly classified points , the correct vertical decision boundary can be easily discovered . Therefore , in order to learn the correct classifier , boundary points should be considered less . In fact , boundary points widely existed in all data sets . Especially when the class number of the data is large , boundary points existed around every decision boundary of each two classes . Therefore , it is very important to develop algorithms that is adaptive to boundary points . By adaptive , we mean that the algorithm can automatically distinguish the boundary points and clearly classified points , and pay less attention to boundary points while learning the classifier .
To address the above two important challenges , in this paper , we propose a new semi supervised learning model . 2.2 Adaptive Semi Supervised Learning
In [ 7 ] , the label is obtained by the label propagation procedure and then used in a regression model . Inspired by [ 7 ] , in this paper , we propose a new adaptive semi supervised learning model where the label matrix Y is used as weights and optimized simultaneously with W . Our new model aims to solve the following objective function : 2 X T l W + 1nlbT − Yl i W + bT − tT
xT nfi cfi
2 F + k
F min W,b,Y st ∀i , yik ∈ [ 0 , 1 ] , yik = 1
( 1 ) i=1 k=1 yr ik cfi k=1 where Xl ∈ 'd×nl is the labeled data set , nl is the number of labeled data , d is the number of feature , 1nl is a column vector of size nl whose elements are all one , W ∈ 'd×c is the model parameter matrix that need to be learned , c is the number of classes , b ∈ 'c×1 is the regression bias , Yl ∈ 'nl×c is the label of labeled data ; yik is the probability of the i th unlabeled data belongs to the k th class , which should be in a value between [ 0,1 ] , Y ∈ 'n×c is the matrix formed by yik , which should also be learned along with W , n is the number of unlabeled data , xi ∈ 'd×1 is the i th unlabeled data , tk ∈ 'c×1 is a class indicator vector for k th class , where the k th element of tj : tjk = 1 , and the rest elements are zeros . r is an adaptive parameter that need to be tuned , and r ≥ 1 . The first term in the objective function is the total loss of labeled data , the second term is the loss of unlabeled data , weighted by the probability distribution matrix Y . There is only one parameter , ie r , in our proposed ASL model . In our further analysis in the experiment section , we will show that the range of r can be fixed , and a reasonable interval is suggested .
It is interesting to note that the parameter actually serves for multiple purposes : from the macro level , r balance the two terms in the objective function , decides how much unsupervised information is used ; from the micro level , after r is fixed , the weights of boundary points will be suppressed to make the model robust to boundary points . Following are some more detailed analysis for the conclusion .
From a macro point of view , r serves as a tradeoff between the first term ( supervised part with label information available ) and the second term ( unsupervised part without label information ) . Note that we do not need another tradeoff parameter before the second term because r is able to balance the two terms . When r becomes large , yr ik will become small since it is a probability which is definitely less than 1 , thus the weight of the second term become small .
483 In the extreme case , when r approaches infinity , the second term vanishes to zero , so only the first term counts , which means the objective function reduces to supervised learning . i1 = 0.92 = 0.81 , yr
From a micro point of view , r automatically adjust the importance ( weight : yr ik in the objective function Eq ( 1 ) ) of each data instance . Consider the i th data instance : if it is clearly classified , yik ( k = 1 , , c , the probability for xi belongs to different class ) will show obvious magnitude difference . For boundary points , however , yik ( k = 1 , , c ) will be more likely equal to one another . Without loss of generality , assuming a binary classification problem , and r = 2 . For clearly classified points , yi1 and yi2 would be one large and one small , say yi1 = 0.9 and yi2 = 0.1 , then yr i2 = 0.12 = 001 Thus , those clear classified points still have large weights in total and contribute a lot to the second term in the objective function . For boundary points , however , yi1 and yi2 would be more likely equal . Assuming yi1 = yi2 = 0.5 , then yr i2 = 0.52 = 025 Thus , those boundary points will have small weights and contribute much less to the second term in the objective function . The above analysis is based on binary classification . Actually , in multi class problems , for boundary points , yik ( k = 1 , , c ) may tend to be 1 c . Therefore , when c becomes larger , yik becomes smaller , and yr ik will be much smaller than the case in binary classification . i1 = yr
From the above analysis , we can see that : when r increases , yr ik will decrease for both clearly classified points and boundary points . However , the weights of boundary points are suppressed much more faster than clearly classified points . In this way , our model will relatively pay more attention to the clearly classified points and pay less attention to the boundary points . This makes our model adaptive and robust to boundary points , therefore , a better classifier can be learned .
It is innovative to use the class probability matrix of unlabeled data to do inductive learning . In our model , the induction of the classifier W is dependent on the class probability matrix Y of unlabeled data , and the transduction of labels to unlabeled data is dependent on the classifier W . The two steps are unified together by simultaneously optimize W and Y . This adaptive procedure is expected to benefit both the induction of classifier and also the transduction of labels to unlabeled data . However , in previous inductive semi supervised learning methods , only the feature matrix X of unlabeled data is utilized to learn the classifier . And in those previous methods , an additional step is needed after the model is learned in order to predict the label of unlabeled data .
We summarize the characteristics of our model as following : ( 1 ) Computational efficient : different from common graph based semi supervised learning methods , our model avoids the computational expensive step of constructing the graph Laplacian matrix .
( 2 ) Adaptive and robust to boundary points : our model can adaptively adjust the weights of each data point . Boundary points will get much smaller weights than other points . This makes our model pay less attention to boundary points , and thus robust to boundary points .
( 3 ) Adaptive optimization procedure : we simultaneously optimize the model parameter W and the class probability matrix Y of unlabeled data . This adaptive procedure is expected to benefit both the induction of classifier and also the transduction of labels to unlabeled data .
( 4 ) Only one parameter : there is only one parameter to be tuned in our model , and the single parameter serves for multiple purposes . A reasonable range of the parameter can be given to facilitate the tuning procedure .
In the following section , an efficient iterative algorithm will be derived to solve the proposed objective function .
3 . OPTIMIZATION ALGORITHM
In this section , we derive an efficient iterative algorithm that alternatively optimize over the model parameter W , b and the class probability matrix Y .
( 1 ) When the model parameters W and b are fixed , we solve the class probability matrix Y . Since the first term becomes a constant , the objective function is reduced to : xT ∀i , yik ∈ [ 0 , 1 ] , 2 i W + bT − tT 2 , Eq ( 2 ) can be written as : nfi cfi i W + bT − tT
Denote pik = yik = 1
xT
2 min yr ik nfi cfi i=1 k=1 cfi st k
2
( 2 ) k=1
Y k
Obviously , Eq ( 3 ) can be decoupled between samples . So it is equivalent to solving : min
Y i=1 k=1 yr ikpik cfi k=1 st ∀i , yik ∈ [ 0 , 1 ] , yik = 1 cfi k=1 min yi· yr ikpik cfi k=1 st ∀i , yik ∈ [ 0 , 1 ] , yik = 1
( 3 )
( 4 )
( 5 ) where yi· represents the i th row of Y .
When r = 1 , obviously , the optimal solution of Eq ( 4 ) is : yik = 1 , if k = k∗ yik = 0 , if k = k∗ pik .
;
; where k∗
= arg min k
When r > 1 , we solve Eq ( 4 ) in the following way . The La grangian function of Eq ( 4 ) is : ikpik − β( yr cfi k=1 cfi k=1 yik − 1 )
( 6 ) where β is the Lagrangian multiplier . In order to get the optimal solution of the subproblem , we set the derivative of Eq ( 6 ) with respect to yik to zero . Thus , we get : yik = ( β rpik
)
1 r−1 .
Substituting Eq ( 7 ) into the constraint closed form solution of Y as following : yik = ( 1 pik
)
1 r−1 / cfi k=1
( 1 pik
( 7 ) cfi k=1 yik = 1 , we get the
1 r−1
)
( 8 )
( 2 ) When the class probability matrix Y is fixed , we solve the model parameter W and b . Note that the second term in the objective function in Eq ( 1 ) sums over the number of unlabeled points n and the number of classes c . If we directly taking the derivative of the second term with respect to W , the resulting algorithm would need to iterate over n and c , which would be slow .
It is interesting to note that we can write the objective function in Eq ( 1 ) into compact matrix representation in the following way :
X T l W + 1nlbT − Yl
2
F min W,b
− 2T r(F ( W T X + b1T n )T n )S(W T X + b1T n ) )
( 9 )
+T r(W T X + b1T
484 where F = Y r ∈ 'n×c ( here Y r denotes to perform power operation on each element in Y ) , S ∈ 'n×n is a diagonal matrix with the i th diagonal element equal to sii =
Fik . cfi
Setting the derivative of Eq ( 9 ) with respect to b to zero , we get : b = q[(Y T ( 10 ) l − W T Xl)1nl + ( F T − W T XS)1n ] k=1 where q is a scalar , q =
1 nl+1T n S1n .
Setting the derivative of Eq ( 9 ) with respect to W to zero , and plug in Eq ( 10 ) , we get : [ Xl(Inl − Q1)X T l + X(In − Q2)SX T − XlQ3X T − XQ4X T l ]W
= Xl(Inl − Q1)Y + X(In − Q2)F − XlQ3F − XQ4Y ( 11 ) nl , Q2 = qS1n1T where Q1 = q1nl1T n , Q4 = q1n1T nl . l +X(In−Q2)SX T−XlQ3X T− l , and A = Xl(Inl − Q1)Y + X(In − Q2)F − XlQ3F −
Denote C = Xl(Inl−Q1)X T
XQ4X T XQ4Y , then the optimal solution of W is : n , Q3 = q1nl1T
W = C−1A
( 12 ) The optimization procedure is summarized in Algorithm 1 . The two step optimization procedure alternatively optimize the model parameter W and the class probability matrix Y of unlabeled data . The induction of the classifier W is dependent on the class probability matrix Y of unlabeled data , and the transduction of labels to unlabeled data is dependent on the classifier W . This adaptive procedure is expected to benefit both the induction of classifier and also the transduction of labels to unlabeled data . It is novel to use the class probability matrix of unlabeled data to do inductive learning .
Because the algorithm get the minimum in each updates of W , b and Y , so the objective value decreases in each updates . What ’s more , the objective function is lower bounded by zero . Therefore , it is obvious that our algorithm will converge . We will show in the experiment section that the algorithm actually converges quite fast on all data sets .
Algorithm 1 Algorithm to solve the problem ( 1 ) .
Initialize W . repeat until Converges
Update Y : using Eq ( 5 ) if r = 1 ; using Eq ( 8 ) if r = 1 . Update W and b by Eq ( 12 ) and Eq ( 10 ) , respectively .
3.1 Complexity Analysis and Scalability The major computational cost lies in our algorithm lies in the updating of W = C−1A , where c is a d × d matrix . The computational cost of matrix inverting is O(d3 ) . In fact , the computation of matrix inverse can be avoided . Note that we are aiming to compute C−1A , which is the solution of the following problem :
W T CW − 2W T A min W
( 13 )
The solution of this minimization problem can be get iteratively using gradient descent method using the updating formula : Wt+1 = Wt − α(CWt − A ) , with a computational cost of O(T d2c ) , where T is the number of iterations , c is the number of columns in A . In addition , getting the d × d matrix C cost O(nd2 ) , getting the d × c matrix A cost O(ndc ) . Therefore , the total computational cost of our algorithm is upper bounded by O(nd2 ) + O(ndc ) + O(T d2c ) . Consider in real situation c is always smaller in magnitude compared to d and n , the total computational cost can also be written as O(nd2 ) + O(T d2 ) . This shows that the computational cost of our algorithm is linear with respect to the number of data samples n . Therefore our algorithm is able to scale to large scale data .
However , for those graph based semi supervised learning methods , without taking into consideration of the huge computation cost for tuning the gaussian kernel bandwidth parameter and the running time of the algorithms themselves , constructing the Laplacian graph matrix already takes at least O(n2 ) . In the big data applications , the number of data n can easily reach billion level . So such graph based algorithms are not scalable to large scale data .
4 . EXPERIMENTAL RESULTS 4.1 Data Sets Descriptions
In order to show the effectiveness of the proposed adaptive semisupervised learning method , experiments are conducted on six real world data sets : AR [ 4 ] , YALE B [ 17 ] , MSRC V1 [ 16 ] , PIE [ 11 ] , FERET [ 10 ] and ORL [ 9 ] . These six data sets are all comprising of human faces , represented using gray scale pixel values . Figure 2 demonstrate some sample images from each data set . Important statistics are summarized in Table 1 .
Table 1 : Data Sets Descriptions
# sample
# feature
AR
YALE B MSRC V1 CMU PIE FERET ORL
840 2414 1799 3329 1400 400
768 1024 1024 1024 1296 644
# class 120 38 12 68 200 40
4.2 Experimental Settings
In order to evaluate the effectiveness of the proposed Adaptive Semi supervised Learning ( ASL ) method , we compare it with some most representative state of the art semi supervised learning methods . Since our method is a unified model which can simultaneously perform transduction and induction , we compare our methods with both transductive semi supervised learning methods and inductive semi supervised learning methods .
In this paper , we compare our method with three representative transductive semi supervised learning methods : local and global consistency ( LGC ) [ 18 ] , random walk ( RW ) [ 19 ] , and gaussian field harmonic function ( GFHF ) [ 21 ] .
Inductive semi supervised learning methods learn a classifier using both labeled data and unlabeled data . Then the learned classifier can be used for the classification of both unlabeled data using for training and also new out of sample testing data . Compared with our method which can directly learn the labels of unlabeled data , common inductive semi supervised learning methods need an additional step to get the labels for unlabeled data used in training .
Two representative inductive semi supervised learning methods are compared with our method : flexible manifold embedding ( FME ) [ 8 ] , and Laplacian regression ( LapReg ) [ 12 ] . LapReg aims to solve the manifold regularized problem , which has the following objective function :
||Yl − X T l W||2
F + γ1T r(W T XLX T W ) + γ2||W||2
F ( 14 ) min W where Yl,Xl are the labels and feature matrix of labeled data , respectively , X is formed by both labeled data and unlabeled data , L
485 ( a ) AR
( b ) YALE B
( c ) MSRC V1
( d ) CMU PIE
( e ) FERET
( f ) ORL
Figure 2 : Sample face images from each data set . is the graph Laplacian matrix constructed using X , γ1 and γ2 are regularization parameters to balance the three terms .
Semi supervised discriminant analysis ( SDA ) [ 2 ] is also a commonly used semi supervised learning method . It aims to learn a projection matrix to map the data in high dimension to a lower dimension subspace . Since our method aims to learn a classifier rather than projection matrix for dimension reduction , we do not compare with SDA in this paper .
In our experiment , we repeat every methods 20 times to compute the average classification accuracy and standard deviation . Each data are first preprocessed using PCA such that 95 % of the energy are retained . Different number of labeled points are used for training to study the sensitivity of those methods to the number of labeled data . We randomly choose 1,3,5 labeled points from each class as labeled data , and the remaining as unlabeled data . For transductive methods , since they can not perform out sample testing , only the accuracy for unlabeled data is computed . For inductive methods , 33 % of data are leaved out for out of sample testing , then the remaining training data is splited into labeled data and unlabeled data . The accuracy for both unlabeled data and out ofsample testing are computed .
For our method , the parameter r is tuned from 1 to 2 with a stepsize of 01 For LGC and RW , the tradeoff parameter α is tuned in [ 01:01:09,099 ] ( ie from 0 to 0.99 with a step size of 0.1 , 0.99 is also included ) . GFHF is parameter free . For LapReg and FME , both of them have two regularization parameters , and are tuned in −4 , , 104 , 105} . The classification results of using the {10 best tuned parameter are recorded . 4.3 Demonstration on Synthetic Data
−5 , 10
In this section , we show the working mechanism of the proposed adaptive semi supervised learning model on a synthetic data . The synthetic data ( demonstrated in Figure 3(a ) ) is generated as following : 50 data points ( corresponding to the samples in the left part of the figure ) are sampled from a gaussian distribution with a mean of 15 , and a standard deviation of 5 ; Another 50 data points ( corresponding to the samples in the right part of the figure ) are sampled from a gaussian distribution with a mean of 15 , and also a standard deviation of 5 . Then 20 boundary points are generated around the ideal decision boundary ( ie the vertical line x = 0 ) with a mean of 0 and a standard deviation of 12 . All the above data points are unlabeled data . After that , we add only one labeled point to each part ( ie , red circle box in the left , blue circle box in the right ) . Note that we intentionally place the labeled point in the upper left corner and lower right corner . Compared to randomly label one point from each part , this labeling strategy will make it harder for a model to recover the correct decision boundary . cfi
Figure 3 ( c ) shows the weights/importance of each unlabeled data point learned by our model . The weights of the i th data yr ik . If the weight of a data point is large , it will conpoint is tribute more to the learning of the classifier . The last 20 samples are boundary points ( points in the middle part of Figure 3 ( a) ) . We can see that the weights of those boundary points are very small compared to other point . Therefore , our model can adaptively adjust the weights of each point , and thus robust to boundary points .
Figure 3 ( b ) shows the data after classification using our adaptive semi supervised learning model . The size of each point is proportional to their weights . Boundary points in the middle part get smaller weights . The correct vertical decision boundary ( the green line in the figure ) is perfectly recovered , even with only one selected labeled point from each class , and the one labeled point is intentionally set to make it harder to recover the correct decision k=1
486 50
40
30
20
10
0
−20
0
20
( a ) Original toy data
50
40
30
20
10
0
−20
0
20
( b ) Toy data after classification using our model s t i h g e w e p m a s l
1
0.8
0.6
0.4
0.2
0 0
50 sample index
100
( c ) Sample weights
Figure 3 : Demonstration on Synthetic Data : Figure ( a ) shows the original data distribution , the red circle box and the blue circle box are the labeled points , all other points are unlabeled ; Figure ( b ) shows the data after classification by our model , the vertical green line is the recovered decision boundary . The size of each point is proportional to their weights . Boundary points in the middle part get smaller weights ; Figure ( c ) shows the weights/importance of each sample learned by our model . The last 20 samples are the boundary points in Figure ( a ) boundary . This shows that our model is able to learn the correct classifier with the presence of boundary points . 4.4 Parameter Discussions
There is only one parameter , ie r , in our proposed ASL model . As discussed before , the parameter actually serves for multiple purposes . In order to achieve the best performance , a proper r should be used to balance the supervised part and unsupervised part in the objective function , and in the meantime , make our model robust to boundary points .
If r is too large , the yr ik will tend to be zero , and the unsupervised information is lost . So r can not be too large . That ’s why r is suggested to vary in [ 1,2 ] ( Consider the class number is often larger than 10 , r = 2 is large enough to suppress ; when the class number is small , the range of r can be extended correspondingly . Generally , [ 1,4 ] is totally sufficient for all small class number problems . )
In the following , we will show how the change of parameter r influence the weights of data points , and the influence on classification performance . 441 When r varies , the contribution of each data instance to the model also changes . We define a data point as an important point if the sum of weights over all classes exceeds an threshold value , ie it satisfies the following condition : r and Number of Important Points c' k yr ik > t
( 15 ) where t is a pre specified threshold . In our experiment , t is set as 0.25 since it is reasonable to assume that an important point should belong to one certain class with a probability larger than 0.5 , so cfi yr ik > 0.52 = 0.25 when r < 2 . k
Figure 4 shows the number of important points using different r values . From the figure we can see that : cfi
( 1)When r = 1 , all points are regarded as important points bey1 ik = 1 . In this case , all samples contributes equally to cause the loss function since their sum of weights over all class are the same . k
( 2)When r increases , the number of important points will decrease . In this process , boundary points will become unimportant ( contribute less to the objective function ) with a small r value . This makes our model robust to boundary points . Some non boundary points may become unimportant with a relatively large r value . In the early stage of the increasing of r ( say r < 1.4 ) , the decreasing of important points is mainly because boundary points become unimportant . In the later stage of the increasing of r ( say r > 1.4 ) , the decrease of number of important points may also be due to that some non boundary points also become unimportant . Note that the number of important points on some data sets become zero in the later stage , but this does not mean unlabeled data points are not used in our model . Remember that the threshold we are using is 0.25 , so the unlabeled data information is still utilized in the model with a relatively small weight . In the extreme case , when r approaches to infinity , all points will become totally unimportant with weight 0 . In this case , unlabeled data points are not used in the model , which reduces to supervised learning .
Therefore , in order for the model to utilize the unlabeled data to the best degree , a proper r value should be chosen . The proper r value should : in the macro level , balance the supervised term and unsupervised term in the objective function ; in the micro level , automatically adjust the weights of data points , so that the model become robust to boundary points , and thus learn a better classifier . ( 3 ) On some data sets with large number of classes ( FERET with 200 classes , AR with 120 classes ) , the number of important points will decrease drastically to zero . In the contrary , on some data sets with small number of classes ( MSRC V1 with 12 classes ) , the number of important points will decrease much slower , and still has many important points when r = 2 . This is because when the class number is small , there tend to be less boundary points , and the probability yik will be relatively larger , so even with a large r value , there are still many important points . However , when the class number is large , there tend to be much more boundary points whose yik are small ( tend to be 1 c in the worst case ) . So even with small r value , those boundary points become unimportant . In this case , the benefits of our proposed model , which suppresses the weights of boundary points , will be more obvious .
487 500
400
300
200
100
0 s t i n o p t n a t r o p m i f o r e b m u n
1
1.2
1.4 r
1.6
1.8
2
( a ) AR i s t n o p t n a t r o p m i f o r e b m u n
1600
1400
1200
1000
800
600
400
200
0 i s t n o p t n a t r o p m i f o r e b m u n
1200
1150
1100
1050
1000
950
900
850
800
1
1.2
1.4 r
1.6
1.8
2
1
1.2
1.4
1.6
1.8
2 r
( b ) YALE B
( c ) MSRC V1 i s t n o p t n a t r o p m i f o r e b m u n
2000
1500
1000
500
0 s t i n o p t n a t r o p m i f o r e b m u n
800
700
600
500
400
300
200
100
0 s t i n o p t n a t r o p m i f o r e b m u n
250
200
150
100
50
0
1
1.2
1.4 r
1.6
1.8
2
1
1.2
1.4 r
1.6
1.8
2
1
1.2
1.4
1.6
1.8
2 r
( d ) CMU PIE
( e ) FERET
( f ) ORL
Figure 4 : The number of important points using different r values on six data sets . One labeled data point is used from each class . r and Classification Accuracy
442 After the above analysis , we know that r will influence the learned model from both macro level ( the weight of unsupervised term in the objective function ) and micro level ( suppress the weights of boundary points ) . Different model will lead to different classification results . In this section , we show how the r value will influence the classification performance .
Figure 5 shows the classification accuracy using different r value on the six data sets . From the figure we can see that : the classification accuracy changes with the parameter r , and the best classification results is achieved by different r value on different data sets .
On all the data sets , the best classfication accuracy is not achieved when r = 1 , which is the common practice in many papers . This is because : by setting yr ik as data weights , our model is more robust to boundary points by suppressing the weights of boundary points . Therefore , by setting the weights of data as yr ik rather than the commonly used yik is more meaningful .
4.5 Classification Performance Comparison
In this section , we present extensive empirical study of the classification performance on six real world data sets . The classification accuracy and standard deviation for running different semisupervised learning methods are reported in Table 2 to Table 7 . From these tables , we can conclude that :
( 1 ) The ASL method outperforms other comparing inductive semisupervised learning methods ( LapReg and FME ) in most cases , and is significantly better than other transductive methods ( LGC , RW , and GFHF ) on all the six data sets . This justifies the effectiveness of the proposed ASL method . Because the ASL method is able to automatically balance the supervised part and unsupervised part in macro level . What ’s more , in the micro level , the ASL method can suppress the weights of boundary points , which makes the model robust to boundary points , and thus , learn a better classifier .
( 2 ) In general , the inductive semi supervised learning methods perform better than transductive methods .
( 3 ) The performance of the three transductive methods are comparable to each other . The performance of these transductive methods is pretty good on the MSRC V1 and ORL data sets . The reason maybe that the number of classes is small on these two data sets compared to other data sets . It seems that those transductive methods can not perform well when the data has large number of classes .
( 4 ) The performance of LapReg and FME are comparable to each other . On the AR data set , the performance of FME is slightly better than the ASL method . This shows the effectiveness of using the manifold regularization to utilize the unlabeled data information . However , the drawback of LapReg and FME is that both of them have two regularization parameters , and the parameters’ range is −5 , 105 ] or even larger ) . This pretty large and not fixed ( vary in [ 10 makes it hard to tune the parameter .
( 5 ) When the number of labeled data points from each class(ie kl ) increases , the performance also become better on all data sets . Especially when number of labeled data points increase from 1 to 3 , the classification accuracy improved significantly . Therefore , when the labeled information is scarce , to acquire more labeled data can be very helpful in semi supervised learning . 4.6 Convergence Speed
In this section , we show the converge speed of the proposed algorithm empirically . Figure 6 shows the objective function value versus number of iterations . We can see that the proposed iterative algorithm converges in less than 20 iterations on all data sets .
In our experiment , running our algorithm 20 iterations only takes about 1 second on a laptop with 2.70GHz double core Intel Core i7 cpu , 16GB memory . Since the major computational cost in our al
488 Table 2 : Classification accuracy and standard deviation for running different semi supervised learning methods 20 times on AR Data Set . “ kl ” represents the number of labeled points from each class used . The column “ Unlabeled ” record the classification accuracy of unlabeled data using transductive methods or inductive methods . The column “ Testing ” record the classification accuracy of out of sample testing data using inductive methods . “ NA ” represents the value is not available .
AR
LGC RW GFHF LapReg FME ASL
Unlabeled 2242±157 2040±160 2089±143 7556±164 6702±263 7736±223 kl=1 kl=3 kl=5
Testing
NA NA NA
7365±194 6779±255 7608±281
Unlabeled 3469±199 3309±163 3267±179 9263±172 9258±143 9315±160
Testing
NA NA NA
9292±181 9329±176 9346±167
Unlabeled 4192±353 4040±265 4096±241 9467±189 9567±265 9683±196
Testing
NA NA NA
9477±109 9583±123 9508±133
Table 3 : Classification accuracy and standard deviation for running different semi supervised learning methods 20 times on YALE B Data Set .
YALE B
LGC RW GFHF LapReg FME ASL
Unlabeled 4247±211 4398±243 4375±196 5136±288 5124±201 5935±291 kl=1 kl=3 kl=5
Testing
NA NA NA
5088±311 5090±317 5898±808
Unlabeled 6042±242 6137±238 6305±171 8609±173 8570±255 9606±186
Testing
NA NA NA
8651±192 8554±292 9613±192
Unlabeled 6816±148 6869±142 7032±212 9490±082 9461±203 9914±054
Testing
NA NA NA
9502±107 9483±211 9900±085
Table 4 : Classification accuracy and standard deviation for running different semi supervised learning methods 20 times on MSRCV1 Data Set .
MSRC V1 kl=1 kl=3 kl=5
LGC RW GFHF LapReg FME ASL
Unlabeled 6238±346 6005±378 5573±601 8191±444 8072±434 8255±364
Testing
NA NA NA
8183±465 8045±450 8229±374
Unlabeled 8834±538 8898±529 8973±439 9754±208 9717±264 9839±192
Testing
NA NA NA
9755±190 9728±257 9845±182
Unlabeled 9519±358 9688±294 9707±272 9869±169 9929±088 9936±125
Testing
NA NA NA
9861±161 9922±098 9942±115
Table 5 : Classification accuracy and standard deviation for running different semi supervised learning methods 20 times on CMUPIE Data Set .
CMU PIE kl=1 kl=3 kl=5
LGC RW GFHF LapReg FME ASL
Unlabeled 2636±160 2671±140 2634±162 6218±211 6031±153 6312±220
Testing
NA NA NA
6189±183 6068±173 6280±276
Unlabeled 4476±085 4403±115 4533±172 8608±156 8668±178 9064±173
Testing
NA NA NA
8626±123 8536±166 9070±164
Unlabeled 5467±136 5428±136 5558±117 9144±081 9135±105 9353±084
Testing
NA NA NA
9145±096 9131±142 9338±090
Table 6 : Classification accuracy and standard deviation for running different semi supervised learning methods 20 times on FERET Data Set .
FERET
LGC RW GFHF LapReg FME ASL
Unlabeled 1465±060 1398±049 1207±056 2751±120 2614±142 3179±150 kl=1 kl=3 kl=5
Testing
NA NA NA
2739±174 2664±195 3106±195
Unlabeled 2674±113 2502±114 2157±138 5829±262 5761±263 5975±236
Testing
NA NA NA
5745±166 5658±251 5854±255
Unlabeled 3174±152 3039±165 2952±216 7155±198 7050±261 7370±195
Testing
NA NA NA
7021±201 7110±227 7197±146
489 80
75
70 y c a r u c c a
65
60
55
50
45
70
65
60
55
50
45
40
35
30
25 y c a r u c c a
1
1.2
1.4 r
1.6
1.8
2
( a ) AR
1
1.2
1.4 r
1.6
1.8
2
( d ) CMU PIE y c a r u c c a
65
60
55
50
45
40
35
35
30
25 y c a r u c c a
20
15
10
5
0
1
1.2
1.4 r
1.6
1.8
2
( b ) YALE B
1
1.2
1.4 r
1.6
1.8
2
( e ) FERET y c a r u c c a y c a r u c c a
85
80
75
70
65
60
66
64
62
60
58
56
54
52
50
48
1
1.2
1.4 r
1.6
1.8
2
( c ) MSRC V1
1
1.2
1.4 r
1.6
1.8
2
( f ) ORL
Figure 5 : The classification accuracy using different r values on the six data sets . One labeled data point is used from each class .
Table 7 : Classification accuracy and standard deviation for running different semi supervised learning methods 20 times on ORL Data Set .
ORL
LGC RW GFHF LapReg FME ASL
Unlabeled 7595±248 7586±146 7499±173 7238±244 7365±286 7642±202 kl=1 kl=3 kl=5
Testing
NA NA NA
6858±395 6908±429 7217±301
Unlabeled 8884±278 8655±249 8654±266 8716±254 8772±254 8716±316
Testing
NA NA NA
8450±298 8521±300 8608±333
Unlabeled 9020±254 9065±170 8985±227 9294±241 9300±288 9375±291
Testing
NA NA NA
9029±303 9008±243 9047±254 gorithm lies in the inverse of a d by d matrix , when the algorithm is used on data with high dimensionality , the computational cost can be reduced by using PCA for dimensionality reduction beforehand . 5 . CONCLUSION
In this paper , we propose an adaptive semi supervised learning model . Different from previous semi supervised learning , our proposed model needn’t construct the graph Laplacian matrix . Thus , our method avoids the huge computational cost required by previous methods , and achieves a computational complexity linear to the number of data points . Therefore , our method is scalable to largescale data . Moreover , the proposed model adaptively suppresses the weights of boundary points . This makes our model robust to boundary points . An efficient algorithm is derived to alternatively optimize the model parameter and class probability distribution of unlabeled data , such that the induction of classifier and the transduction of labels are adaptively unified in one framework . Our model only has one parameter need to be tuned , and a fixed range is also suggested . Extensive experimental results show that the proposed semi supervised learning model outperforms other state ofthe art methods in most cases . 6 . ACKNOWLEDGMENTS
This research was partially supported by NSF IIS 1117965 , NSF
IIS 1302675 , NSF IIS 1344152 , NSF DBI 1356628 .
References [ 1 ] M . Belkin , P . Niyogi , and V . Sindhwani . Manifold regularization : A geometric framework for learning from labeled and unlabeled examples . The Journal of Machine Learning Research , 7:2399–2434 , 2006 .
[ 2 ] D . Cai , X . He , and J . Han . Semi supervised discriminant analysis . In Computer Vision , 2007 . ICCV 2007 . IEEE 11th International Conference on , pages 1–7 . IEEE , 2007 . [ 3 ] X . Chang , F . Nie , Y . Yang , and H . Huang . A convex formulation for semi supervised multi label feature selection . In The Twenty Eighth AAAI Conference on Artificial Intelligence , 2014 .
[ 4 ] A . M . Martinez . The ar face database . CVC Technical
Report , 24 , 1998 .
[ 5 ] F . Nie , S . Xiang , Y . Jia , and C . Zhang . Semi supervised orthogonal discriminant analysis via label propagation . Pattern Recognition , 42(11):2615–2627 , 2009 .
[ 6 ] F . Nie , S . Xiang , Y . Liu , and C . Zhang . A general graph based semi supervised learning with novel class discovery . Neural Computing and Applications , 19(4):549–555 , 2010 .
490 127.2
127
126.8
126.6
126.4
126.2
126
125.8
125.6 l e u a v e v i t c e b o j
620
600
580
560
540
520
500
480
460
440 l e u a v e v i t c e b o j
125.4
0
2
4
6
10
8 12 # of iterations ( a ) AR
14
16
18
20
420
0
2
4
6
10
8 12 # of iterations ( b ) YALE B
14
16
18
20 l e u a v e v i t c e b o j
2
4
260
240
220
200
180
160
140
120
100
80
60
0
100
95
90
85
80
75
70
65
60
6
10
8 12 # of iterations ( c ) MSRC V1
14
16
18
20 l e u a v e v i t c e b o j
1150
1100
1050
1000
950
900
850
800
750
0
2
4
135.8
135.6
135.4
135.2
135
134.8
134.6
134.4
134.2
134 l e u a v e v i t c e b o j l e u a v e v i t c e b o j
6
10
8 12 # of iterations ( d ) CMU PIE
14
16
18
20
133.8
0
2
4
6
10
8 12 # of iterations ( e ) FERET
14
16
18
20
55
0
2
4
6
14
16
18
20
10
8 12 # of iterations
( f ) ORL
Figure 6 : Objective value versus the number of iterations .
[ 7 ] F . Nie , D . Xu , X . Li , and S . Xiang . Semisupervised dimensionality reduction and classification through virtual label regression . Systems , Man , and Cybernetics , Part B : Cybernetics , IEEE Transactions on , 41(3):675–685 , 2011 .
[ 8 ] F . Nie , D . Xu , I . W H Tsang , and C . Zhang . Flexible manifold embedding : A framework for semi supervised and unsupervised dimension reduction . Image Processing , IEEE Transactions on , 19(7):1921–1932 , 2010 .
[ 9 ] ORL Face Database , 2007 . http :
//wwwcam orlcouk/facedatabasehtml
[ 10 ] P . J . Phillips , H . Moon , S . A . Rizvi , and P . J . Rauss . The feret evaluation methodology for face recognition algorithms . Pattern Analysis and Machine Intelligence , IEEE Transactions on , 22(10):1090–1104 , 2000 .
[ 11 ] T . Sim , S . Baker , and M . Bsat . The cmu pose , illumination , and expression database . IEEE Transactions on Pattern Analysis and Machine Intelligence , 25(12):1615–1618 , 2003 .
[ 12 ] V . Sindhwani , P . Niyogi , M . Belkin , and S . Keerthi . Linear manifold regularization for large scale semi supervised learning . In Proc . of the 22nd ICML Workshop on Learning with Partially Classified Training Data , volume 28 , 2005 .
[ 13 ] D . Wang , Y . Wang , F . Nie , J . Yan , W . Cai , A . Saykin ,
L . Shen , and H . Huang . Human connectome module pattern detection using a new multi graph minmax cut model . In Med Image Comput Comput Assist Interv , 2014 .
[ 14 ] H . Wang , C . Ding , and H . Huang . Directed Graph Learning via High Order Co linkage Analysis . In Proceedings of the ECML PKDD , pages 451–466 , 2010 .
[ 15 ] H . Wang , H . Huang , and C . Ding . Image annotation using multi label correlated green ’s function . IEEE Conference on Computer Vision , pages 1–6 , 2009 .
[ 16 ] J . Winn and N . Jojic . Locus : Learning object classes with unsupervised segmentation . In Computer Vision , 2005 . ICCV 2005 . Tenth IEEE International Conference on , volume 1 , pages 756–763 . IEEE , 2005 .
[ 17 ] Yale Univ . Face Database , 2002 . http://cvcyale edu/projects/yalefaces/yalefaceshtml
[ 18 ] D . Zhou , O . Bousquet , T . Lal , J . Weston , and B . Schölkopf .
Learning with local and global consistency . Proc . Neural Info . Processing Systems , 2003 .
[ 19 ] D . Zhou and B . Schölkopf . Learning from labeled and unlabeled data using random walks . In Pattern Recognition , pages 237–244 . Springer , 2004 .
[ 20 ] X . Zhu . Semi supervised learning literature survey .
Computer Science , University of Wisconsin Madison , 2:3 , 2006 .
[ 21 ] X . Zhu , Z . Ghahramani , and J . Lafferty . Semi supervised learning using gaussian fields and harmonic functions . Proc . Int’l Conf . Machine Learning , 2003 .
491
