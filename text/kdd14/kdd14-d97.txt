Open domain Quanity Queries on Web Tables : Annotation , Response and Consensus Models
Sunita Sarawagi
IIT Bombay
Soumen Chakrabarti
IIT Bombay
ABSTRACT Over 40 % of columns in hundreds of millions of Web tables contain numeric quantities . Tables are a richer source of structured knowledge than free text . We harness Web tables to answer queries whose target is a quantity with natural variation , such as net worth of zuckerburg , battery life of ipad , half life of plutonium , and calories in pizza . Our goal is to respond to such queries with a ranked list of quantity distributions , suitably represented . Apart from the challenges of informal schema and noisy extractions , which have been known since tables were used for non quantity information extraction , we face additional problems of noisy number formats , as well as unit specifications that are often contextual and ambiguous .
Early “ hardening ” of extraction decisions at a table level leads to poor accuracy . Instead , we use a probabilistic context free grammar ( PCFG ) based unit extractor on the tables , and retain several top scoring extractions of quantity types and numerals . Then we inject these into a new collective inference framework that makes global decisions about the relevance of candidate table snippets , the interpretation of the query ’s target quantity type , the value distributions to be ranked and presented , and the degree of consensus that can be built to support the proposed quantity distributions . Experiments with over 25 million Web tables and 350 diverse queries show robust , large benefits from our quantity catalog , unit extractor , and collective inference . 1 .
INTRODUCTION
Web search engines enhance “ organic ” search results with data from structured knowledge bases ( KBs ) , curated from diverse sources using information extraction [ 13 ] and entity annotation [ 5 ] techniques . With very few exceptions [ 10 , 19 , 3 , 1 ] , a vast majority of work on extracting typed text segments , entities , attributes and relations involve discrete symbols and not measurable quantities 1 . And yet , the Web is as rich a source of quantities as it is of symbolic knowledge . There are hundreds of millions of information rich HTML tables on the Web . In our sample , a full 40 % of their columns exclusively contain quantities . On the other hand , quantityseeking Web queries are well served only in verticals like
1 “ When you can measure what you are speaking about , and express it in numbers , you know something about it . ” — Lord Kelvin . shopping , food and travel where structured databases contain columns that are matched against the query , aggregated , and presented as quantities throughout . With one mild exception [ 20 ] , the emerging Web tables literature [ 17 , 4 , 7 , 11 ] does not regard quantity extractions as any different from discrete symbolic value extractions . 1.1 Our goal
Our goal is to bridge the gap between the noisy presentation of quantities in Web tables and open domain , one off quantity seeking queries . Asking for an attribute of an entity is a common case , eg , average revenue of microsoft , half life of plutonium and mass of pluto . Note that we are primarily interested in seeking quantities with uncertainty and imprecision , unlike “ cosmic truths ” like the number of primes under 1000 , or the value of π . For this reason , our queries cannot be answered from Wikipedia Infoboxes , which mostly offer point answers , and without directly accumulating evidence from the Web . To this end , we present a new , robust , open domain , quantity search system QEWT . QEWT is a large system with very many details . Here we distill three major contributions : a quantity catalog and a new table column unit annotator based on a probabilistic context free grammar ( PCFG ) , a query response model based on ranked value distributions , and a new algorithm for collective consensus inference . Our unit catalog , query workloads , labeled data , and code are publicly available at http://wwwcseiitbacin/~sunita/wwt 1.2 Our contributions Table column unit annotator . Quantity columns in web tables are notoriously challenging to extract . Unit expressions may be ambiguous ( “ m . ” ) or even missing , syntactic clues though present might be noisy , and units may not follow clear cut representations in table headers . We compiled a quantity catalog QuTree , and designed a new unit annotator for table columns based on probabilistic context free grammars ( PCFGs ) . The PCFG technique exploits a diverse set of clues , including co occurrence statistics between quantity types , units and phrases mined from an unlabeled corpus of table headers . Quantity response model . In IR , the response is a list of URLs or documents . In expert or entity search [ 2 ] , it is a list of entities . Queries that seek uncertain quantities require a very different treatment . In particular , extracted quantities can be distinct from each other for extraneous reasons , or there may be systematic variations ( which isotope of Plutonium , or Microsoft revenue in which year ) . A more intuitive uniform output representation is a distribution over quantities . We describe techniques to represent , score and rank such distributions h , given the query .
Earlier work [ 3 , “ QCQ ” ] is a restrictive special case of our framework . Each response in QCQ is a single interval ,
711 and a model was trained to score these using labeled data . Here we can create distributions and intervals in a querydriven manner without labeled data . Since our target is open domain query answering , we ensure that our response model can handle data at arbitrary scales and from arbitrary distributions via a data driven distance metric .
Collective extraction framework . The centerpiece of our approach is a collective model for extracting quantities from raw Web tables . Instead of “ hardening ” extraction decisions on quantity columns eagerly , we keep around multiple uncertain values and units , which are finally collectively resolved over multiple tables at query time . We depend on a consensus model that defines a joint distribution h over all candidate extractions to assign collectively resolved probabilities on the relevance of each snippet and the extracted unit and value . Another valuable input is from a classifier that provides a distribution over the target quantity type . We train the classifier via a novel method of tapping our huge unlabeled table corpus .
Experimental evaluation . We report on experiments with 350 queries spanning three query benchmarks compiled from QCQ [ 3 ] , InfoGather [ 20 ] and World Bank data [ 18 ] . The base corpus tables is accessed through the Google API at researchgooglecom/tables and our corpus of 25 million tables extracted [ 11 ] from a commercial Web crawl of 500 million pages . Our experimental results can be summarized as : • Our PCFG based column annotator is considerably more accurate than rule based ( 82 % vs 40 % ) or simpler baselines ( 74% ) . However , it is not nearly perfect . • As a consequence , choosing locally best extractions from each table independently gives poor accuracy . Collective judgment of snippet relevance and extractions improves answer precision from 28 % to 42 % . • Smooth notions of consensus between contributing quantities is important ; accumulating counts for point estimates ( collapsing distribution h to a histogram over discrete values ) causes a 6 % drop in precision . • The quantity type classifier trained from the unlabeled table corpus provides a 8 % boost in precision .
2 . SYSTEM OVERVIEW AND ROADMAP We present an overview of our system QEWT in Figure 1 . The user submits a quantity query q which has two parts . The first part aq is a sequence of word/s that are a verbal description of the response quantity type ( eg , distance from sun , speed , annual revenue ) . The second part eq is a free form sequence of words that indicate an entity for which a quantity attribute is being sought ( eg Pluto , Concorde , Microsoft ) . The query words are submitted to indices over Web table corpora and a set of tables retrieved . Figure 3 shows sample tables for the query co2 emissions of china with eq = china and aq = co2 emissions . A snippet generator module processes the retrieved tables for potential answer snippets and attaches a relevance score to each . Next , a number/unit extractor parses the noisy snippets to output an uncertain list of values and units based on a unit ontology . The uncertain extractions from all tables are collectively resolved to get a distribution over the target quantity . Finally the response as a continuous distribution or ranked quantity intervals is output to the user . We next present an overview of the main components .
Figure 1 : System sketch of QEWT .
Table corpus . We use two sources of tables : a commercial crawl of 500 million Web pages , from which we extracted and indexed 25 million non decorative HTML tables , and tables collected per query via the API provided by researchgooglecom/tables We extract from each table zero or more top rows as the header for each of its columns , and selected text from the page embedding the table as the context as described in [ 11 ] . A type interpreter labels table columns as numeric or textual ; in our corpus of 25 million tables we recognized 40 % of the table columns as numeric .
Figure 2 : Fragment of QuTree .
Unit catalog QuTree . Starting from Category:Units of measurement in Wikipedia , we created a unit catalog we call QuTree . It has 44 quantity types such as Length , Area , Speed , and 750 units . Each quantity type has a canonical unit , and other units come with conversion factors to/from the canonical unit . A fragment of QuTree is shown in Figure 2 . Each catalog unit is associated with one or more full names ( eg , kilometre per hour ) , one or more symbols ( eg , kmph , km/h ) and an optional list of lemmas to account for the common variant names of a unit , ( eg , metermetre , kmph kilometre per hour ) . QuTree also includes the concept of a “ multiplier ” to denote dimensionless quantities , with unit instances like thousand , million and billion , which capture scales of measurements . QuTree can be downloaded from googl/542L2Y It is to quantities what YAGO [ 14 ] is to discrete entities . Query target type . In the user query , aq is a textual hint at a quantity type t from the quantity catalog . We must build an estimate of the distribution Pr(t|q ) = Pr(t|aq ) . For example , for attribute co2 emissions Pr(t|aq ) is expected to assign high probability to the quantity type mass . In Section 3.1 we show how to train Pr(t|aq ) via an innovative use of the table corpus , and without requiring tedious manual labeling . We will see in Section 6.4 that this signal is very useful to the collective extractor in finding relevant answers .
QuerySnippetGenerationPotentiallyrelevantsnippets Unit + valueextractionCandidate extractionsRsJsvCollectiveinferenceResponserepresentationUserIndexed Table StoreQuTreeValue distributionQuery typeinferenceCandidatetarget types fi'ff()ffff)fifl(ffififffffflfiffff'ff(fifififflfifffffiff'fiffifffi''ffffl)fffiffff(fi) ffi))fifl))ffifi)fffi712 CO2 ) ” — these are different , whereas yet another table saying “ CO2 emissions ( kt ) ” is comparable to the second case . In Section 5 we present our design of a parser based on a context free grammar and a rich feature set . The output of this step is a weighted list of possible extractions of values and units from each snippet . Collective inference . The candidate extractions are inputs to the centerpiece of QEWT : a novel collective inference procedure that simultaneously estimates the relevance of each snippet and each of its possible extractions , and the target quantity type basing its judgement on a global estimate of the distribution of values around the quantity . We describe this in Section 35 Answer representation . As challenging as robustness to input variation is the issue of answer representation . The vast body of work on search has to rank discrete items such as URLs ( pages ) [ 8 ] or entities [ 2 ] . Here our true response is an uncertain quantity , but how best is this shown to the user ? Rather than take an inflexible stand , QEWT supports three different types of answer formats each of which is suited for a different kind of quantitative queries . QEWT ’s internal response representation is a quantity distribution , but QEWT can provide simplified digests in the form of ranked value intervals [ 3 ] , or degenerated to ranked point values ( in case the query has little or no uncertainty , such as physical constants , or are categorical queries in disguise , such as the number of USB ports in a laptop ) . 3 . COLLECTIVE CONSENSUS INFERENCE
As shown in Figure 1 , the collective inference module , which we describe in this section , mediates between the type distribution , snippet relevance uncertainty , and the uncertainty of snippet unit and value extractions . Section 3.1 is about estimating the target type of the query . Section 3.2 discusses scoring snippets wrt the query . Section 3.3 describes how a single snippet can lead to many possible extractions of a unit and value , and how to score these extractions . Section 3.4 deals with the design of distributions for consensus inference . Finally , Section 3.5 describes the consensus inference algorithm itself . 3.1 Query target quantity type Pr(t|q ) Given query q = ( aq , eq ) , its target quantity type , which is uncertain , is modeled as a distribution Pr(t|aq ) . A good estimate of Pr(t|aq ) is vital for generating relevant responses . 311 Dictionary match A baseline method may look for matches of the attribute words with type and unit names/lemmas in our quantity catalog . It may correctly find the target type of queries like length of nile or year of crash . But many queries like usa co2 emissions , walton net worth , miami rainfall , and ebay revenue have no match in the catalog , leading to loss of recall . Surprisingly , there is also precision loss . Eg , query fan speed which matches well the type speed in QuTree but misses the correct target type frequency . 312 Data driven approach Ideally , we would like to recognize that query word revenue targets type money amount and that distance , length , height and width all target the type length ( dimension ) .
This form of association between ( potential query ) words and quantity types is evident in a fraction ( but still a large
Figure 3 : Sample tables responding to query eq = china and aq = co2 emissions .
Snippet relevance . The index search returns a set of candidate tables that match the query keywords q in the header , context , or body . On each candidate table , we use a snippet generation module to match eq to a row header , aq to a column header , and identify candidate cells for quantity extraction . Observe from Figures 3 and 4 the challenges of identifying potentially relevant rows , eg , “ China ” vs . “ China ( mainland ) ” , and relevant columns , eg multiple columns might be relevant and column headers may not match aq at all as shown in Figure 4 . Therefore , relevance of a snippet is uncertain . We use a random variable Rs to denote the uncertainty in the relevance of a snippet s and associate a score rs ∈ [ 0 , 1 ] to denote Pr(Rs = 1 ) . We will describe snippet generation and relevance score assignment further in Section 32 Number and unit extraction . From each snippet s , we extract a float value from the table cell , and annotate it with a unit from QuTree based on signals in the column header and the table cell . Note that a unit node in QuTree belongs to exactly one quantity type , so a unit annotation also gives its quantity type . Both these tasks are highly challenging because of the extreme diversity of writing down quantities on the web : multipliers expressed separately in the column header , locale dependent use of commas , periods , and spaces , scientific notation , etc . can confound any simplistic local extractor . Unit extraction from headers has its own challenges of dealing with drastic formatting differences in both column headers and cell quantities Eg , one column header may say “ Total Emissions ( 1000 ’s tons carbon ) ” whereas another may say “ ( billion metric tons of
713 absolute number ) of tables , with column headers like “ Width in inches ” , “ co2 emissions in kiloton ” , “ Average rainfall in inches ” , “ Fan speed ( rpm ) ” , and “ Annual revenue ( $ million ) ” . To take advantage of these headers , they must be mapped to types , which is , in general , a highly nontrivial job ( see Section 5 ) . However , starting from over 25 million tables , we could find 1.1 million headers which could be mapped with high precision ( 99 % ) to types , using simple rules discussed in Section 5.1 , and restricting to headers with an exact and unique match with a unit in QuTree .
We thus get ( automatically ) labeled instances with observed features consisting of the words xk in the header not included in the unit , and a quantity type tk derived by generalizing its extracted unit . Eg , from the header text “ Annual revenue ( $ million ) ” we extract an instance with bag of words x = {annual , revenue} and type money amount . These labeled instances are used to train a logistic regression classifier for Pr(t|q ) . 3 fold cross validation gave 94 % accuracy , higher than using PMI models [ 16 ] . If the posterior entropy was large ( eg , for a query on refractive index ) , we prefered the “ dimensionless ” type in favor of other types . 3.2 Snippet match with query
Each table retrieved from the index generates one or more snippets . We represent the uncertainty of relevance of a snippet s to q by Rs,q , or Rs if q is fixed . The uncertainty of relevance is reflected in a score rsq or rs ∈ [ 0 , 1 ] . Based on query q , the local evidence in favor of relevance ( Rs = 1 ) is rs , and the local evidence in favor of irrelevance ( Rs = 0 ) is 1 − rqs .
Given query q = ( aq , eq ) and a candidate table T , we describe how we match these two parts of the query to generate one or more snippets from T . We view web tables as vertical stores of entity and their attributes2 . Accordingly , a candidate snippet is generated by matching eq to a row r in a column ce and matching aq to a different column ca and generating the snippet from cell ( r , ca ) . For most queries , we find a single snippet per table but sometimes the entity eq could match multiple rows of a table , or aq could match multiple columns of T . For example , in Figure 3 the first three web tables generate a single snippet for query co2 emissions of china . In contrast , for the query refractive index of flint glass , in Figure 4 , the second table matches the entity “ flint glass ” in the last two rows whereas the first web table provides the attribute “ Refractive index ” in three different forms over columns 2 , 3 , and 4 . We next elaborate on our method for generating such snippets .
Figure 4 : Snippet match with query .
The candidate tables are required to match all high IDF terms in the query in either the body , header , or context .
2We omit a discussion of horizontal tables for simplicity [ 6 ] .
This ensures that our candidate snippets are minimally relevant . Thereafter , we separately measure the similarity score sim(eq , r , ce , T ) of entity eq to cell ( r , ce ) and similarity score sim(aq , ca , T ) of attribute string aq to column ca ’s header . These match scores are custom designed to depend on matches beyond the immediate cells to include T ’s context , title , and other parts of its body . In [ 11 ] we presented the design of a segmented similarity function that shows how to measure the relevance of a table ’s column to a query keyword while combining column specific match with matches from the rest of the table . We use this segmented similarity function as the match function : sim(eq , r , ce , T ) and sim(aq , ca , T ) .
The snippets from a table are generated as follows . We first find the ( r , c ) with the best value of sim(eq , r , c , T ) . Call it ( r∗ , c∗ e ) . Fix c∗ e as the entity column . We then find the column c that is numeric and has maximum similarity sim(aq , c , T ) . Call it c∗ a . Next , as snippets we select those cells rs , cs for which : sim(eq , rs , c e , T ) ≥ max(sim(eq , r ∗ ∗ , c sim(aq , cs , T ) ≥ max(sim(aq , c a , T ) − , 0)} ∗ e , T ) − , σ ) ∗
This criteria makes sure that we select all snippets s = ( rs , cs ) whose match is close enough ( within ) of the best possible match from T , provided the entity match is at least σ . In our experiments we used 0.2 for both and σ . The reason we have a minimum match threshold for entities and not for attributes is because the number of numeric columns in a table is typically much smaller than the number of rows . We depend on the global consensus model to prefer the columns that are correct . We use the entity and attribute match scores to assign the relevance score of a snippet as rs = ( sim(aq , cs , T ) + sim(eq , rs , c∗ 3.3 Possible extractions from snippet e , T ))/2 .
We model each snippet s as extracting exactly one value and one unit ( therefore , type ) . However , given the noise in extraction , we model the extracted value and type as uncertain , but drawn from a finite , usually small set of alternative extractions . This set of possible extractions is indexed by j . For example , consider the web table in Figure 1 obtained in response to query height of washington monument .
Height ( m ) Year Building 168,7 95,8 91,5 87,6 39,2
1884 Washington Monument 1899 Old Post Office 1990 Washington National Cathedral 1892 United States Capitol 1943
Jefferson Memorial
Table 1 : An example web table for query height of washington monument .
We get one snippet from this table at the cell ( 1,1 ) , but we have uncertainty over its value : does 168,7 equal 168.7 , or a list of two numbers 168 and 7 . This gives rise to two possible values : 168.7 and 168 ? Also , from the header Height ( m ) , the unit parser extracts three possible units : meter , million , and mile . So , we consider all six combinations of unit and value as possibilities for j . Each j is attached with a score gsj that we generate as follows : First , parse the value in the cell based on different locale specific parsers and get the set of successful parses of the value vs1 , . . . , vsn . Second , use the CFG unit parser described in Section 5 to get a list of units along with their scores : ( us1 , w1 ) , . . . , ( usm , wsm ) . The set j consists of the mn possible cross product of value,unit combination where each j is associated with : a value vsj , a unit usj and therefore its type tsj , and a confidence score
Refractive index under different light wavelengthsWeb Table 1Web Table 2714
1 Z n ≥ 0 . ( Note that a specific extraction j has an gsj = wsj extracted unit , and each unit maps to a unique quantity type . For convenience , we can convert a specific extraction to some canonical unit that we associate with the quantity type . ) Summarizing , for each snippet s , there are two hidden random variables :
• Rs ∈ {0 , 1} , the snippet relevance bit . • Js , an extraction index .
If Rs = 0 , then Js = ⊥ is undefined . Otherwise , Js tells us what value and unit/type s contributes to the global consensus .
3.4 Value distribution h with the usual constraint
In abstract terms , a quantity query should be answered with a type ( unit ) and a value distribution h . For each potential response type τ , we will build a distribution hτ ( v ) , hτ will be estimated from a set of values {vi} , but , based on the previous sections , each value vi is associated with a probability πi that the value should actually contribute to hτ . This is expressed with the notation hτ ( •|{(vi , πi)} ) .
• hτ ( •)d• = 1 .
Simple parametric distributions are not a good choice for h , given that we have to deal with numbers of arbitrary scale , coming from arbitrary domains , with a high level of extraction noise . Therefore , we focus on non parametric distributions . We briefly mention two standard forms of hτ for the sake of completeness . 341 Kernel density A natural option is a kernel density estimate : hτ ( •;{vi , πi} ) =
πi√ 2πσi e(•−vi)2/2σ2 i ,
( 1 ) i for all i as a special case , and Z = where σi is a kernel width parameter that may be the same i πi . A problem that we faced with a fixed width is that they do not adapt well to numbers of arbitrary scales . Consider the query that seeks to find the half life of Plutonium . Plutonium has many isotopes with extremely diverse half lives : 14 , 88 , 6560 , 24100 , and 376000 years . Naturally these are stated approximately , with errors that are commensurate with the magnitude of the quantity being expressed . We associated a different width σi around each data point that increases with the scale of the point vi as max(10 % of vi , minimum non zero gap between numbers ) . 342 Wavelet Another popular non parametric density estimator is based on Wavelets . Wavelets are believed to outperform kernel density estimators at representing discontinuities and local variations , features rampant in our data . We use the Haar wavelet as our basis function . Due to lack of space , we refer the reader to standard textbooks on the topic and omit further details . 3.5 Consensus inference algorithm 351 Intuition Now we are in a position to verbally express how collective consensus is formed , given a query . We will state that various scores “ should be large ” , with the understanding that , in a collective scheme , we want some sort of aggregate ( say their product ) to be large . We will then translate this description into a formal model . • If τ is a top scoring response quantity type , then Pr(τ|q ) should be large .
• Suppose we choose specific values for all snippet relevance variables Rs . Then the score of that configuration is rs
( 1 − rs ) ,
( 2 ) s:Rs=1 s:Rs=0 and this should be large .
• If a snippet is irrelevant , no specific extraction needs to be chosen for that snippet . If snippet s is relevant , we need to pick one extraction Js = j , for which tsj = τ ( ie , the extracted type matches the query target type ) , and gsj is large . • Consider now all relevant snippets s and their chosen extractions Js . These induce a multiset of values {v} . If we are to propose value distribution h to the user , we want the density at {v} to be large . Assuming iid extraction events3 , this can be written as • If h were an arbitrary distribution of unlimited complexity , it could support an arbitrary set of extracted values , no matter how disparate from each other . Therefore h needs to be regularized , and there needs to be consensus among the values claimed to be contributing to h . s:Rs=1 h(vs,Js ) .
In the rest of this section , we fill in the details of the above framework .
Inputs : Pr(t|q ) ; for all snippets s , rs and tsj , vsj , gsj for all candidate extractions j . Evolving variables : ˇrs , ˇgsj initialize hidden variables ˇgsj ← gsj rs Pr(tsj|q ) for iterations i = 1 , 2 , . . . do for each snippet s do
τ ( • ) be a value distribution estimated from all \s let h snippets except s , using weights ˇgsj for each candidate extraction j do
φsj ← gsj rs Pr(tsj|q ) h
( vsj ) {consensus}
\s tsj end for φs⊥ ← 1 − rs
D ← φs⊥ + ˇrs ← ( 1/D ) for each j do ˇgsj ← φsj/D j φsj j φsj end for end for end for
Figure 5 : Collective inference pseudocode .
Iterative update algorithm
352 Each snippet s that potentially contributes information to the response of a query has the associated hidden variables Rs and Js . The collective inference algorithm will build estimates of the posterior distributions over Rs , Js through the procedure shown in Figure 5 . Specifically , let ˇrs ∈ [ 0 , 1 ] be the posterior probability of relevance of snippet s . If the snippet is relevant , then the probability of extraction j being j ˇgsj = 1 for each s . We also maintain , for all possible response types τ , the value distributions denoted hτ ( • ) . valid is ˇgsj , with
The consensus update φsj ← gsj rs Pr(tsj|q ) h
( vsj ) can also be interpreted as a “ leave one out ” validation of vsj in 3Multiple extractions from a table are not iid . QEWT handles them , but we skip discussing them for simplicity .
\s tsj
715 sion values over all countries . Our goal is to find intervals I = I1 , . . . , Ik to represent the point answers V = {(vi , πi)} . Note that each vi ∈ Sq . Using MDL , we interpret this as a compression problem from a hypothetical sender of V to a receiver , with the intervals serving as a compression model for V . We assume Sq is known to both parties . The cost of sending V has two parts : the cost of the model , which in our case is the set of intervals , and the cost of sending the data V given model . The cost of sending the data V given intervals I = I1 , . . . Ik is the sum of the cost of data in each interval . Let SI denote the set of numbers from Sq that lie in interval I = [ I , uI ] , pI .
As per I , all values within [ I , uI ] have probability rI = pI|SI| data cost over all intervals is − of being relevant . Each entry vi in V is relevant with probability πi , thus the expected number of bits for sending vi is : −πi log rI − ( 1 − πi ) log(1 − rI ) . Other values in SI are irrelevant and require − log(1 − rI ) bits each . Summing up , vi∈I −πi log rI + πi log(1 − rI ) − |SI| log(1 − rI ) .
I∈I1,Ik
The model cost is the cost of sending the parameters rI and the boundary [ I , uI ] . Following MDL , we encode rI by finding a good fit distribution p(rI ) , and setting the cost as − log p(rI ) . Since rI is between 0 and 1 , a natural choice is the Beta distribution whose density is Pr(p ; m , n ) = B(m,n ) pm−1(1 − p)n−1 where B(m , n ) is the Beta function
1 and m , n are parameters . We need to choose m , n so as to minimize the cost of sending rI s over all intervals and all queries . Since Sq is typically larger than V , we choose the prior parameters as m = 1 , n = 2 so that smaller values of rI get lower cost . The set of intervals that minimize the sum of data and model cost can be found in O(|V |2 ) time using the same segmentation algorithm as in Section 41 5 . UNIT EXTRACTOR
So far we have abstracted the role of the extractor that finds likely units from table columns . Given the noise in tables bearing quantities , the module that annotates table columns with units needs to be fairly sophisticated . Problem statement . Given a numeric table column with a textual header x our goal is to extract the units ( if any ) from x that associate with the numbers in the column . Figure 6 the backdrop of other contributing values , as explained below . ( Think of index i here as s , j . ) If vi is dropped while τ ( •|{(vi , πi)} ) . \i making the estimate hτ , we will write it as h A natural notion of consensus among {(vi , πi)} can be obtained by dropping each ( vi , πi ) in turn , forming the estiτ ( •|{(vi , πi)} ) , and evaluating h\i(vi,{(vi , πi)} ) . It \i mate h tells us how strongly vi itself is supported by the rest of the observed values .
The lines after the consensus are to update ˇrs and ˇgsj to normalized posterior probabilities . After ( sufficient ) convergence , we are left with these posterior probabilities , from which we construct hτ on all values ( with their posterior probabilities ) . The resulting hτ and/or values with their posterior probabilities are then sent to the answer interval representation module , described next . 4 .
INTERVAL REPRESENTATION
Expressive densities described in the previous section give us a great deal of power to fit the extracted values . However , the user may wish to view something simpler than such a general density . Following QCQ [ 3 ] , we propose approaches to present ranked intervals as the query response . We associate each interval I with a lower limit I and upper limit uI and a pI that denotes the probability that the answer lies within [ I , uI ] . The final answer is a set I of I∈I pI = 1 . A baseline approach would be to cluster the relevant values , but the relevance of a value is not known for sure , and the number of clusters is also unknown . Therefore , we need more sophisticated methods . non overlapping intervals I1 , . . . , Ik such that
For this part snippet boundaries are not relevant , so we denote our input as a set of ( vi , πi ) pairs where a i corresponds to some sj and πi = ˇgsj . 4.1 Uniform density mixture
I can also be thought of as a mixture of uniform distri bution with density hI(v ) = pI
δ(v ∈ [ I , uI ])dv v
I∈I uI − I
( 3 ) Thus , one method of obtaining I is to approximate a more expressive density ( Section 3.4 ) to a mixture of uniform distributions . Let h(•|{(vi , πi)} ) be a density ( such as a Kernel density ) . We find our desired intervals I1 , . . . , Ik such that within each I = [ I , uI ] , the error of approximating density h by a constant is within a tolerance while minimizing the number of such intervals . We measure error in terms of KL divergence between h and hI ; this makes uI−I h(v ) dv . We allow intervals to be single the error per interval as uI where pI = uI h(v ) log h(v ) dv − pI log v=I pI v=I points I = uI , and assign an error of 0 for such intervals . Since we restrict the interval boundaries to values in V , we can easily find the optimal set of intervals in O(|V |)2 time using a simple range segmentation algorithm . 4.2 MDL intervals
One problem with the above method is the difficulty of finding one tolerance to fit all query types . We next present a method based on the minimum description length ( MDL ) principle [ 12 ] that is more adaptive to per query variations in value distributions .
Assume we have a set Sq of values sampled from all rows of the snippet columns for query q . For example , in Figure 3 the set Sq will be sampled from all distinct co2 emis
Figure 6 : Example unit annotations ( in yellow ) on table columns . shows some example unit annotations to table columns . As shown , we handle four types of unit occurrences :
Metre|footBritish pound [ million]YearUnit listUnit with multiplierAtomic unitPassElevation ( m/ft)Tonalepass1884 ( 6181)ColleManiva1669 ( 5476 ) . . . .Metre|footStorageEnergy density Megajoule/KilogramYear endedNet profit/(loss ) ( £m)2012143201140 . . . .British pound [ million]YearRatio unitRatio with new unitStorageEnergy density by mass in MJ/KgLiquid hydrogen143Energy from the sun645,000,000 . . . .CityDensity ( inh . Per km2)Macau19796Mumbai20694 . . . Inh /square kilometreRatio with new unit716 with a unit node in our quantity ontology QuTree ,
• atomic units like year and meter that have exact match • units with multipliers ( eg million pounds ) , • compound units formed by taking a ratio or product • a list of units ( eg metre|foot ) . of two units ( eg mega joule/kilogram ) , and
5.1 An initial rule based extractor
We initially attempted to extract based on a set of intuitive rules capturing various lexical clues and matches with the unit catalog , since all previous work on quantities have relied on rule based unit extractors [ 3 , 20 ] .
Let the term match refer to the longest sequence of tokens in a header x that matches a unit name , symbol , or lemma in QuTree . Annotating a unit based purely on a match leads to many false positives because there are several words like “ in ” , “ at ” , “ last ” , “ s ” , “ stone ” , “ point ” that are unit names/symbols but are commonly used as non unit words . Therefore , we defined rules that require additional evidences for a match in x to be tagged a unit :
1 . after in : A match after in is a unit eg “ Price in $ ” ,
“ Distance in km ” , “ Wind velocity in miles per hour ” .
2 . bracketed : A match within brackets is a unit e.g
“ Net profit ( $ ) ” , “ CO2 emissions ( kiloton ) ”
3 . type name : A match tied to a unit U whose parent type name appears in the header is unit U . Eg “ Length of race , m ” is annotated U =Metre and “ Cruise Speed , mph . ” is annotated Miles per hour
Figure 7 : An example parse for the column header Windspeed(mph/km/h ) . SimpleUnit is abbreviated to SU , and unary nodes like CUnit and AtomUnit have been deleted for compactness .
Header Unit
Msep CUnit UnitOp SimpleUnit AtomUnit Multiplier
CUnit Msep Multiplier | Msep Multiplier
::= Words ? Unit List Words ? ::= CUnit | Multiplier Msep CUnit | ::= Empty | of | in ::= SimpleUnit | SimpleUnit UnitOp SimpleUnit ::= Empty | per | ’/’ | × ::= AtomUnit | Multiplier AtomUnit ::= Unit in QuTree | New word ::= Multiplier Unit in QuTree | Number
Figure 8 : Grammar used by the CFG
On deploying these rules on our table corpus we were surprised by their large number of errors . For example , the rule after in wrongly labels the header “ Scores in last match ” as unit Last ( a unit of Volume ) , and cannot disambiguate between units Carat , Knot , and Kiloton in header “ Capacity in kt ” since “ kt ” is a symbol for each of these three units . The rule bracketed wrongly labels word “ dec ” in header “ Population ( Dec 2006 ) ” as unit Decade . Also , it cannot differentiate between units in headers “ duration(s ) ” and “ year(s ) ” where the unit annotation of the first should be Seconds but the second should have no unit . The rule type name wrongly annotates header “ Length of song(m:s ) ” as Meter whereas the correct annotation for “ m ” is Minute . The rule is particularly bad for compound units , for example in “ Energy density by volume ( MJ/L ) ” , volume helps annotate L as Liter . Rules for compound units are not easy because they require simultaneous labeling of many different parts of the headers . Finally , these set of rules have poor recall , for example , the header “ CO2 Emissions 2000 thousands of metric tons of carbon ” from the third table in Figure 3 is not covered by any of the three rules .
We therefore explored alternative models that combine multiple soft evidence from additional resources and are more expressive in their modeling of compound units and other unit patterns like multipliers . The rampant ambiguity of a mention with language words and a unit and with other units , implied that just depending on clues derived from the header string may not be adequate . Further , since the system of derived units is based on a well defined grammar , it seemed natural to use a grammar to drive the extraction . Feature based context free grammars seemed perfect for the task . Regular grammars cannot capture patterns that encourage alternative units to be of the same type , as in “ Meters per second ( m/s ) ” . 5.2 Feature based context free grammar
In this approach we use a discriminative Context Free Grammar ( CFG ) with scores attached to each possible production in the grammar [ 15 ] . In Table 8 we show the CFG ( without scores ) for unit extraction from table headers . The grammar supports all four different types of compound units illustrated in Figure 6 . In Figure 7 we present an example parse tree for the header : “ Wind speed ( mph / km/h ) ” that this grammar supports .
In general , the grammar allows many possible parses of a header x . Each parse tree has a score that is additive over each of the productions in the tree . A production P of the form : R ::= R1R2 is scored as : score(P ) = w.f ( P , x , i , j , k )
( 4 ) where ( i , j ) and ( j + 1 , k ) are the text spans in x that R1 and R2 cover , respectively . When R2 is empty ( j = k ) , we have a unary production . The feature vector f can be used to capture various clues that help identify units . We list the set of features we used in sections 521 , . . 525 The weight vector w corresponding to the features could be trained using the discriminative framework of [ 15 ] , but since the number of features in our case was small ( seven ) and easily interpretable , we fixed their values manually .
The task of annotating units in an input x reduces to the task of finding the tree of production with the highest sum of scores , and outputing the list of units under the “ Unit ” nodes in the tree . Since we have scores , as against hard rules , we can output multiple extractions each weighted with a score . The well known Inside outside algorithm [ 13 , 9 ] for inference in PCFGs can be easily extended to output the top K highest scoring extractions in polynomial time .
We next list the features used in f ( P , x , i , j , k ) . We first list the features for leaf level productions where R is a unit name U which could be either a Multiplier or an AtomicUnit , and finally in Section 525 present features for other internal nodes of the parse tree . We will use the short form xij to denote the token subsequence xi . . . , xj of x . 521 Matches with the Unit Catalog The TF IDF similarity of xij to various parts of unit Us entry in QuTree , including U ’s name , lemmas , and symbol is an important feature for productions that tag xij as a unit name . Another feature analogous to the type name rule , is
fi'ffff'()fffiflff'(ff'ff'ff'(ffi')fffiflfflff(fi)fffi)flffflffiffflffffiffi)fffi)flfffifffl)flff717 1 − f ( s ) the TF IDF similarity of the words in the parent quantity type of U and tokens in x excluding xij . For example , in the header “ Length ( m ) ” , this feature will apply for production Meter := x22 since the word “ Length ” matches the parent type of unit Meter . However , it does not fire for production million := x22 . 522 Lexical clues When each of the after in and bracketed rules apply on a xij we fire a feature when R is the unit state “ Unit ” . To allow for occasional extraneous tokens , ( eg length in approx meter ) , we also fire these tokens with a tolerance of one token . 523 Relative frequency We exploit ontologies such as WordNet that provide relative frequency of word usage to get disambiguation clues between units and non units and between different symbols of the same unit . WordNet provides relative frequency of various senses of nouns in its ontology . Each noun sense s is associated with a list of word forms and a frequency f ( s ) . Let S = {s1 , . . . , sm} be the set of senses whose wordforms match xij . If one of these senses , say s , is a descendant of the Quantity type in Wordnet and matches the base name of the unit U in QuTree , we fire a feature with value t∈S f ( t ) . For example , with xij= ” last ” , we found eight matching senses in Wordnet , with more than 99 % frequency on the non unit meaning of last as “ finish ” or “ end ” . This provided strong evidence to not tag “ last ” as a unit . 524 Co occurrence statistics from table corpus Another strong clue for correctly assigning a unit U to x is obtained from the presence of strongly co occurring words in x outside unit words xij . For example , for x = “ width in m ” , and unit U = metre for token x22 , we can exploit the fact that “ width ” often co occurs with Length units . We use the unlabeled corpus of table headers to train a query type classifier as described in Section 312 Using this , we assign a feature with value Pr(tU|x ) where tU is the parent type of unit U . 525 Compound , Multiple , Multiplier units We next add a set of features to handle compound units . For each compound unit type : unit multiplier pair ( eg dollar [ million] ) , and ratio/product of two units ( MJ/L ) we add bias terms so that atomic units are preferentially related via these operations instead of being treated as a list of unrelated units . Finally , when two units belong to the same type , we add a bias term so that they are treated as alternatives ( eg metre|feet ) . 6 . EXPERIMENTS After describing our testbed in Section 6.1 , we perform controlled studies on collective extraction ( §6.2 ) , choice of value distribution h ( §6.3 ) , query type inference ( §6.4 ) , response interval generation approaches ( §6.5 ) , and the effect of various unit extractors ( §66 ) 6.1 Testbed Table corpora . Our corpus of tables was collected from two sources : a commercial Web crawl with 500M pages similar to ClueWeb094 from which we extracted 25 million ta
4lemurproject.org/clueweb09 bles [ 11 ] , and tables returned by researchgooglecom/tables in response to queries . Queries and ground truth . We used three sources of queries : QCQ : 28 diverse queries used in [ 3 ] . WorldBank : 172 queries on four quantity attributes of coun tries : forest area , co2 emissions , land area , and population . Ground truth is from World Bank documents [ 18 ] . InfoGather : 146 queries used in InfoGather [ 20 ] , on three different types of quantities : population of 50 large cities of the world and revenue and profit of 34 large corporations . Land area of countries was also part of this workload , but since we had already included in WorldBank , we dropped it from this set .
Queries with ground truth are at googl/542L2Y Measurements . As in QCQ [ 3 ] , we need to design how the performance of QEWT is measured . Most commonly , ground truth G is available as a set of values , or one or a few ranges . Similar to QCQ , we assume that the query specifies a multiplicative confidence band . Ie , if a true point value is v , the user would be satisfied with a value in [ (1 − )v , ( 1 + )v ] ( Our results are with = 002 ) Meanwhile , QEWT presents a distribution h . The probabilistic precision of h is h(v)δ
[ (1 − )v , ( 1 + )v ] ∈ G dv ,
( 5 ) v which is the total area matching a ground truth value band . Recall is defined as the fraction of G supported by h5 . The probabilistic F1 score is the harmonic mean of probabilistic precision and recall . Given the total area under h is always 1 , any attempt to recall all ground values by “ smearing out ” h will result in large swathes of h never being touched by a ground truth band . Since the form of h shown to the user is a set probability weighted set intervals , we use hI ( Equation 3 ) for h . 6.2 Benefits of collective answer extraction
QEWT potentially improves upon two simpler baselines . In CollectiveHard , for each snippet s , we greedily and locally choose the extraction with the best scoring unit and value . Ie , Js is pinned . The only uncertainty is in relevance Rs , for which h is used . For Pr(t|q ) we use the hard scores from Section 311 which is 1 when aq matches the catalog , and uniform otherwise . In Independent , h is missing but the rest of the setting is exactly the same as in Collective . The default representation for h is variable width kernel density . Figure 9 shows that the full power of QEWT ’s collective extraction is vital . In workload InfoGather , CollectiveHard suffers particularly severely because of the hard method of query type assignment on the corporate tax rate queries . 6.3 Effect of choices of h
With the value of h established , in Figure 10 we compare choices for h : variable and fixed width kernel density , wavelets , and a degenerate distribution with impulse probabilities only at point values seen in the snippets , essentially treating quantities as discrete symbolic extractions . Whereas there is no clear winner among the continuous distributions , they are all better than point histograms , which
5Note , an alternative definition of probabilistic recall is precision times the fraction of G supported by h . But that causes precision to be counted twice in F1 and is redundant .
718 Figure 9 : Comparing collective extraction models . are bad for queries like net worth of a celebrity , or revenue of a company that are often stated approximately ; it gets no benefit of consensus from values that are close by ( eg $ 111.5 billion versus $ 111.05 billion ) .
Attribute Type annual rainfall Length revenue Currency weight Mass CO2 emissions Mass depth distance to sun net worth population half life
Length Length Currency Multiples Time
Pr(τ|aq ) 0.92 0.71 0.96 0.87 0.97 0.97 0.83 0.73 0.67
Figure 11 : Query attributes aq , labeled type τ and Pr(τ|aq ) via logistic classifier .
Figure 12 : Query F1 accuracy under different querytype classification models .
Figure 10 : Comparing different models for h( . )
6.4 Effect of query type prediction Pr(t|q ) We compare the dictionary match approach ( §311 ) with the data driven approach ( §312 ) Out of the 35 distinct attribute names spanning the three workloads , dictionary match correctly identified only ten of the query types . In contrast , our data driven approach correctly classified 34 of them with confidence at least 066 Some example attributes and their type and score from this method appear in Figure 11 . When the data driven unit extractor is plugged into the collective answer extractor , the result is a significant boost to end to end accuracy , as shown in Figure 12 . 6.5 Interval generation methods compared Figure 13 compares MDL intervals ( §4.2 ) vs . uniform mixture approximations ( §41 ) The x axis is proportional to the ( assigned ) marginal cost of an additional segment ( one uniform distribution ) . The y axis shows F1 . At low segment cost , responses are over fragmented , losing recall . At high segment cost , precision is lost .
Part of the reason for MDL ’s superiority is the guidance from the reference value distribution . Figure 14 shows an example , for the query “ corporate tax rate of united states ” . There are 12 distinct correct answers in the tight interval [ 39.05 , 39.34 ] ( % ) , shown as green crosses . The candidate extracted values V = {(vi , pi)} are shown as red squares ( some wide off the mark ) . About 100 reference values forming Sq , sampled from tables on corporate tax rates of several countries are between 20 % and 40 % , shown as blue diamonds . Because of the large reference density in [ 35 , 45 ] , the intervals ( with probabilities ) created by MDL are [ 392,394 ] : 0.61 , [ 35,35 ] : 0.1 , [ 40,40 ] : 0.09 , and [ 390,391 ] : 009 The most likely one is shown as a green box , containing most true values . In contrast , the top response from UniformMix is the low precision interval [ 35.0,45 ] : 0.91 , because it
Figure 13 : Comparing the MDL and UniformMix models for generating final intervals in the answer for increasing Segment cost . has no query specific method of measuring relative distances between points . 6.6 Benefits of PCFG based unit extractor
Here we evaluate different parsers for extracting units discussed in Section 5 and also study their impact on end toend query processing .
In order to estimate the extraction accuracy of unit parsers , we created a dataset of 617 table headers from our corpus that have been manually labeled with the correct units . In Figure 15 we plot the extraction accuracy of different parsers under varying settings . Rule is a rule based parser that includes among others the rules of Section 5.1 refined to label only unambiguous matches . This has an accuracy of only
Figure 14 : An example showing how a reference set of numbers helps the MDL approach select the correct intervals .
fi ' ff(()fffifl)(()fffifl)ffiffl)))ff fi(fflflflfflff*)(cid:30) fi ' ff()()fffi(flflfiffiffl)ffl fifl(fi)fl)fiffl*((cid:30) fi ' ff()ff)fiflffiffl(ffiffffiffl)fl fifflffiflflflfl*fi(cid:30)ffiffffl fi' fiff fi ()(fffiflffifflffl fi fi fi fi'(( ( ( ( )(fflffifl*(cid:30 ) fl fibility'ffffff()fffffifl)fiffffifflffflff flProbabilflfffflffflff*(cid:30)719 40 % , and almost all of this error is due to poor recall — the precision of the rule based parser is 99 % . In contrast , our proposed CFG based parser that we call QuantCFG achieves an accuracy of 82 % . We created another parser called Sequence that uses all the features of QuantCFG but not its grammar , and chooses the unit with highest score w.f among all word sequences that match QuTree . The drop in accuracy to 74 % establishes the importance of the grammar . The next three bars establish the importance of features in Sections 522 , 523 , 524 by reporting accuracy of QuantCFG without various subsets of them .
Figure 15 : Comparing different parsers .
We next assess the impact of the unit extraction accuracy on the final answer quality in the end to end system . In Figure 16 , we compare F1 scores of the answer on the three workloads on three different parsers described above : the rule based parser , the feature based sequence parser , and our QuantCFG parser . We observe that it is necessary to use parsers both with high precision and high recall for getting quality answers .
Figure 16 : Comparing different parsers on their impact on the answer quality in the end to end system .
7 . RELATED WORK
Moriceau [ 10 ] was among the earliest to formalize quantity search , and provide some initial notions of temporal trends and aggregation of values . A more extensive system was built by Wu and Marian [ 19 , “ W&M ” ] . Banerjee et al . [ 3 , “ QCQ ” ] proposed the quantity interval ranking problem . None of these exploited source HTML tables , none delayed per snippet extraction decisions , and none proposed an inference procedure that collectively estimated snippet relevance , snippet extractions , and the value distribution . SCAD [ 1 ] collected quantities while satisfying domain guided numeric constraints between them ( eg , a laptop screen is wider than it is tall ) . But SCAD did not use a unit extractor or consensus inference as in QEWT . Zhang et al . [ 20 , “ InfoGather ” ] also extract units and values from Web tables . They focus on identifying correspondences among tables based on column types . They do not model uncertain value distributions . Their aggregation/consensus is based on exact match of values , which we demonstrate as weaker than QEWT ’s value distribution model . InfoGather extracts column units using rules , which cannot handle compound units and noisy headers , unlike our PCFG unit extractor . Their “ query ” resembles a table completion task , with ∼100 entities , units , and scales explicitly provided . In contrast , QEWT is a robust , open domain system for ad hoc , singleentity queries . Acknowledgments . This work was partly supported by research grants from the Indo German Max Planck Centre for Computer Science ( IMPECS ) and from Yahoo! Research . 8 . REFERENCES
[ 1 ] A . Bakalov , A . Fuxman , P . P . Talukdar , and S . Chakrabarti .
SCAD : collective discovery of attribute values . In WWW Conference , pages 447–456 , 2011 .
[ 2 ] K . Balog , L . Azzopardi , and M . de Rijke . A language modeling framework for expert finding . Information Processing and Management , 45(1):1–19 , 2009 .
[ 3 ] S . Banerjee , S . Chakrabarti , and G . Ramakrishnan . Learning to rank for quantity consensus queries . In SIGIR Conference , 2009 .
[ 4 ] M . J . Cafarella , A . Y . Halevy , and N . Khoussainova . Data integration for the relational web . PVLDB , 2(1 ) , 2009 .
[ 5 ] M . Cornolti , P . Ferragina , and M . Ciaramita . A framework for benchmarking entity annotation systems . In WWW Conference , pages 249–260 , Rio de Janeiro , Brazil , 2013 .
[ 6 ] E . Crestan and P . Pantel . Web scale table census and classification . In Proceedings of the fourth ACM international conference on Web search and data mining , WSDM ’11 , 2011 .
[ 7 ] G . Limaye , S . Sarawagi , and S . Chakrabarti . Annotating and searching web tables using entities , types and relationships . In VLDB , 2010 .
[ 8 ] T Y Liu . Learning to rank for information retrieval . In
Foundations and Trends in Information Retrieval , volume 3 , pages 225–331 . Now Publishers , 2009 .
[ 9 ] C . D . Manning and H . Sch¨utze . Foundations of Statistical Natural Language Processing . The MIT Press , Cambridge , Massachusetts , 1999 .
[ 10 ] V . Moriceau . Numerical data integration for cooperative question answering . In EACL Workshop on Knowledge and Reasoning for Language Processing , pages 42–49 , 2006 .
[ 11 ] R . Pimplikar and S . Sarawagi . Answering table queries on the web using column keywords . In Proc . of the 38th Int’l Conference on Very Large Databases ( VLDB ) , 2012 .
[ 12 ] J . Rissanen . Stochastic complexity in statistical inquiry . In World Scientific Series in Computer Science , volume 15 . World Scientific , Singapore , 1989 .
[ 13 ] S . Sarawagi . Information extraction . FnT Databases , 1(3 ) ,
2008 .
[ 14 ] F . M . Suchanek , G . Kasneci , and G . Weikum . YAGO : A core of semantic knowledge unifying WordNet and Wikipedia . In WWW Conference , pages 697–706 . ACM Press , 2007 .
[ 15 ] B . Taskar , D . Klein , M . Collins , D . Koller , and C . Manning .
Max margin parsing . In EMNLP , July 2004 .
[ 16 ] P . D . Turney . Mining the Web for synonyms : PMI IR versus
LSA on TOEFL . In ECML , 2001 .
[ 17 ] P . Venetis , A . Y . Halevy , J . Madhavan , M . Pasca , W . Shen , F . Wu , G . Miao , and C . Wu . Recovering semantics of tables on the web . PVLDB , 4(9 ) , 2011 .
[ 18 ] Worldbank . http://dataworldbankorg/ , 2014 . [ 19 ] M . Wu and A . Marian . Corroborating answers from multiple web sources . In WebDB : Tenth International Workshop on the Web and Databases , 2007 .
[ 20 ] M . Zhang and K . Chakrabarti . Infogather+ : semantic matching and annotation of numeric and time varying attributes in web tables . In SIGMOD Conference , 2013 .
fi'fffi fi'fififffifffi fifi'fi())fffi)flfifffi()) fi ' ff()fffifffl)ffffifflff)ffi flffiffi*(cid:30)ff720
