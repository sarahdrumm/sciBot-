Factorized Sparse Learning Models with Interpretable High
Order Feature Interactions
Sanjay Purushotham∗ University of Southern
California
Los Angeles , CA , USA spurusho@usc.edu
Martin Renqiang Min†
NEC Labs America Princeton , NJ , USA renqiang@nec labs.com
C C Jay Kuo
University of Southern
California
Los Angeles , CA , USA cckuo@sipiuscedu
Rachel Ostroff‡ SomaLogic , Inc . Boulder , CO , USA rostroff@somalogic.com
ABSTRACT Identifying interpretable discriminative high order feature interactions given limited training data in high dimensions is challenging in both machine learning and data mining . In this paper , we propose a factorization based sparse learning framework termed FHIM for identifying high order feature interactions in linear and logistic regression models , and study several optimization methods for solving them . Unlike previous sparse learning methods , our model FHIM recovers both the main effects and the interaction terms accurately without imposing tree structured hierarchical constraints . Furthermore , we show that FHIM has oracle properties when extended to generalized linear regression models with pairwise interactions . Experiments on simulated data show that FHIM outperforms the state of the art sparse learning techniques . Further experiments on our experimentally generated data from patient blood samples using a novel SOMAmer ( Slow Off rate Modified Aptamer ) technology show that , FHIM performs blood based cancer diagnosis and bio marker discovery for Renal Cell Carcinoma much better than other competing methods , and it identifies interpretable block wise high order gene interactions predictive of cancer stages of samples . A literature survey shows that the interactions identified by FHIM play important roles in cancer development .
Categories and Subject Descriptors J.3 [ Computer Applications ] : Life and Medical Sciences ; ∗Co first author †Co first author , corresponding author ‡To whom data request should be sent
I26 [ Computing Methodologies ] : Artificial Intelligence— sparse learning , feature selection
Keywords Sparse Learning ; High order interactions ; Biomaker Discovery ; Blood based Cancer Diagnosis
1 .
INTRODUCTION
Identifying interpretable high order feature interactions is an important problem in machine learning , data mining , and biomedical informatics , because feature interactions often help reveal some hidden domain knowledge and the structures of problems under consideration . For example , genes and proteins seldom perform their functions independently , so many human diseases are often manifested as the dysfunction of some pathways or functional gene modules , and the disrupted patterns due to diseases are often more obvious at a pathway or module level . Identifying these disrupted gene interactions for different diseases such as cancer will help us understand the underlying mechanisms of the diseases and develop effective drugs to cure them . However , identifying reliable discriminative high order gene/protein or SNP interactions for accurate disease diagnosis such as early cancer diagnosis directly based on patient blood samples is still a challenging problem , because we often have very limited patient samples but a huge number of complex feature interactions to consider .
In this paper , we propose a sparse learning framework based on weight matrix factorizations and 1 regularizations for identifying discriminative high order feature interactions in linear and logistic regression models , and we study several optimization methods for solving them . Experimental results on synthetic and real world datasets show that our method outperforms the state of the art sparse learning techniques , and it provides ‘interpretable’ blockwise highorder interactions for disease status prediction . Our proposed sparse learning framework is general , and can be used to identify any discriminative complex system input interactions that are predictive of system outputs given limited high dimensional training data .
Our contributions are as follows : ( 1 ) We propose a method capable of simultaneously identifying both informative single discriminative features and discriminative block wise highorder interactions in a sparse learning framework , which can be easily extended to handle arbitrarily high order feature interactions ; ( 2 ) Our method works on high dimensional input feature spaces and ill posed problems with much more features than data points , which is typical for biomedical applications such as biomarker discovery and cancer diagnosis ; ( 3 ) Our method has interesting theoretical properties for generalized linear regression models ; ( 4 ) The interactions identified by our method lead to biomedical insight into understanding blood based cancer diagnosis .
2 . RELATED WORK
Variable selection has been a well studied topic in statistics , machine learning , and data mining literature . Generally , variable selection approaches focus on identifying discriminative features using regularization techniques . Most recent methods focus on identifying discriminative features or groups of discriminative features based on Lasso penalty [ 18 ] , Group Lasso [ 21 ] , Trace norm [ 6 ] , Dirty model [ 8 ] and Support Vector Machines ( SVMs ) [ 16 ] . A recent approach [ 20 ] heuristically adds some possible high order interactions into the input feature set in a greedy way based on lasso penalized logistic regression . Some recent approaches [ 2],[3 ] enforce strong and/or weak heredity constraints to recover the pairwise interactions in linear regression models . In strong heredity , an interaction term can be included in the model only if the corresponding main terms are also included in the model , while in weak heredity , an interaction term is included when either of the main terms are included in the model . However , recent studies in bioinformatics has shown that feature interactions need not follow heredity constraints for manifestation of the diseases , and thus the above approaches [ 2],[3 ] have limited chance of recovering relevant interactions . Kernel methods such as Gaussian Process [ 4 ] and Multiple Kernel Learning [ 10 ] can be used to model high order feature interactions , but they can only tell which orders are important . Thus , all these previous approaches either failed to identify specific high order interactions for prediction or identified sporadic pairwise interactions in a greedy way , which is very unlikely to recover the ‘interpretable’ blockwise high order interactions among features in different sub components ( for example : pathways or gene functional modules ) of systems . Recently , [ 14 ] proposed an efficient way to identify combinatorial interactions among interactive genes in complex diseases by using overlapping group lasso and screening . However , they use prior information such as gene ontology in their approach , which is generally not available or difficult to collect for some machine learning problems . Thus , there is a need to develop new efficient techniques to automatically capture the important ‘blockwise’ high order feature interactions in regression models , which is the focus of this paper .
The remainder of the paper is organized as follows : in section 3 we discuss our problem formulation and relevant notations used in the paper . In section 4 , we discuss the main idea of our approach , and in section 5 we give a overview of theoretical properties associated with our method . In section 6 , we present the optimization methods which we use to solve our optimization problem . In section 7 , we discuss our experimental setup and present our results on synthetic and real datasets . Finally , in section 8 we conclude the paper with discussions and future research directions .
3 . PROBLEM FORMULATION Consider a regression setup with a training set of n samples and p features , {(X(i ) , y(i))} , where X(i ) ∈ Rp is the ith instance ( column ) of the design matrix X ( p × n ) , y(i ) ∈ R is the ith instance of response variable y ( n × 1 ) , and i = 1 , . . . , n . To model the response in terms of the predictors , we can set up a linear regression model y(i ) = βT X(i ) + ( i ) ,
( 1 ) or a logistic regression model
1 p(y(i ) = 1|X(i ) ) =
,
1 + exp(−βT X(i ) − β0 )
( 2 ) where β ∈ Rp is the weight vector associated with single features ( also called main effects ) , ∈ Rn is a noise vector , and β0 ∈ R is the bias term . In many practical fields such as bioinformatics and medical informatics , the main terms ( the terms only involving single features ) are not enough to capture complex relationship between the response and the predictors , and thus high order interactions are necessary . In this paper , we consider regression models with both main effects and high order interaction terms . Equation 3 shows a linear regression model with pairwise interaction terms . y(i ) = βT X(i ) + X(i)T WX(i ) + ( i ) ,
( 3 ) where W(p × p ) is the weight matrix associated with the pairwise feature interactions . The corresponding loss function ( the sum of squared errors ) is as follows ( we center the data to avoid an additional bias term ) ,
Lsqerr(β , W ) =
1 2
||y(i ) − βT X(i ) − XT ( i)WX(i)||2
2 . ( 4 ) n i=1 n
We can similarly write the logistic regression model with pairwise interactions as follows , p(y(i)|X(i ) ) =
1 + exp(−y(i)(βT X(i ) + X(i)T WX(i ) + β0 ) )
1
( 5 ) and the corresponding loss function ( the sum of the negative log likelihood of the training data ) is ,
Llogistic(β , W , β0 ) = log(1 + exp(−y(i)(βT X(i ) i=1 +X(i)T WX(i ) + β0) ) .
( 6 )
4 . OUR APPROACH
In this section , we propose an optimization driven sparse learning framework to identify discriminative single features and groups of high order interactions among input features for output prediction in the setting of limited training data . When the number of input features is huge ( eg biomedical applications ) , it is practically impossible to explicitly consider quadratic or even higher order interactions among all the input features based on simple lasso penalized linear regression or logistic regression . To solve this problem , we propose to factorize the weight matrix W associated with high order interactions between input features to be a sum of K rank one matrices for pairwise interactions or a sum of low rank high order tensors for higher order interactions .
K k=1
Each rank one matrix for pairwise feature interactions is represented by an outer product of two identical vectors , and each m order ( m > 2 ) tensor is represented by the outer product of m identical vectors . Besides minimizing the loss function of linear regression or logistic regression , we penalize the 1 norm of both the weights associated with single input features and the weights associated with high order feature interactions . Mathematically , we solve the optimization problem to identify the discriminative single and pairwise interaction features as follows ,
{ ˆβ , ˆak} = arg min ak,β
Lsqerr(β , W )
+ λββ1 +
λakak1
( 7 ) where W = K k=1 ak fi ak , fi represents the tensor product/outer product , and ˆβ , ˆak represent the estimated parameters of our model and let Q represent objective function of ( 7 ) . For logistic regression , we replace Lsqerr(β , W ) in ( 7 ) by Llogistic(β , W , β0 ) . We call our model Factorizationbased High order Interaction Model ( FHIM ) .
Proposition 41 The optimization problem in Equation
7 is convex in β and non convex in ak .
Because of the non convexity property of our optimization problem , it is difficult to propose optimization algorithms which guarantee convergence to global optima . Here , we adopt a greedy alternating optimization methods to find the local optima for our problem . In the case of pairwise interactions , fixing other weights , we solve each rank one weight matrix each time . Please note that our symmetric positive definite factorization of W makes this sub optimization problem very easy . Moreover , for a particular rank one weight matrix akfiak , the nonzero entries of the corresponding vector ak can be interpreted as the block wise interaction feature indices of a densely interacting feature group . In the case of higher order interactions , the optimization procedure is similar to the one for the pairwise interactions except that we have more rounds of alternating optimization . The parameter K of W is generally unknown in real datasets , thus , we greedily estimate K during the alternating optimization algorithm . In fact , the combination of our factorization formulation and the greedy algorithm is effective for estimating the interaction weight matrix W . β is re estimated when K is greedily added during the alternating optimization as shown in algorithm 1 .
Repeat until convergence
Algorithm 1 Greedy Alternating Optimization 1 : Initialize β to 0 , K = 1 and aK = 1 2 : While ( K==1 ) || ( aK−1 = 0 f or K > 1 ) 3 : j+1 , βt−1 4 : k,j−1 , at−1 5 : 6 : 7 : K = K + 1 ; aK = 1 8 : End While 9 : Remove aK and aK−1 from a .
βt j = arg minj Q(βt at k,j = arg minj Q((at j−1 , βt−1
End Repeat k,1 , , at
1 , , βt p
) , ak k,j+1 , at−1 t−1 ) k,p ) , βt )
5 . THEORETICAL PROPERTIES
In this section , we study the asymptotic behavior of FHIM for the likelihood based generalized linear regression models .
The lemmas and theorems proved here are similar to the ones shown in the paper [ 3 ] . However , in their paper the authors make an assumption on the strong heredity ( ie interaction term coefficients are dependent on the main effects ) , which is not assumed in our model since we are interested in identifying all high order interactions irrespective of heredity constraints . Here , we discuss the asymptotic properties wrt to the main effects and factorized co efficients .
Problem Setup : Assume that the data Vi = ( Xi , yi ) , i = 1 , n are collected independently and Yi has a density of f ( g(Xi ) , yi ) conditioned on Xi , where g is a known regression function with main effects and all possible pairwise interactions . Let β∗ k,j denote the underlying true parameters satisfying block wise properties implied by our factorization . Let Qn(θ ) denote the objective with negative log likelihood and θ∗ = ( β∗T , α∗T )T , where α∗ = ( a∗ k ) , k = 1 , , K . We consider the estimates for FHIM as ˆθn : ˆθn = arg min j and a∗
Qn(θ )
( 8 ) n
θ
= arg min
θ
− 1 n
( L(g(Xi ) , yi ) + λβ|β| +
λαk|αk| i=1 k where L(g(Xi ) , yi ) is the loss function of generalized linear regression models with pairwise interactions . In the case of linear regression , g(· ) takes the form of Equation ( 3 ) without the noise term and L(· ) takes the form of Equation ( 4 ) . Now , let us define
A1 = {j : β A2 = {(k , l ) : α A = A1 ∪ A2 j = 0} ∗ k,l = 0} , ∗
( 9 ) where A1 contains the indices of the main terms which correspond to the nonzero true coefficients , and similarly A2 contains the indices of the factorized interaction terms whose true co efficients are non zero . Let us define an = max{λβ bn = min{λβ j , λαk j , λαk l l
: j ∈ A1 , ( k , l ) ∈ A2} : j ∈ Ac 2} 1 , ( k , l ) ∈ Ac
( 10 )
Now , we show that our model possesses the oracle properties for ( i ) n → ∞ with fixed p and ( ii ) pn → ∞ as n → ∞ under some regularity conditions . Please refer to Appendix for proofs of the lemmas & theorems of sections 5.1 and 52 5.1 Asymptotic Oracle Properties when n → ∞ The asymptotic properties when sample size increases and the number of predictors is fixed are described in the following lemmas and theorems . FHIM possesses oracle properties [ 3 ] under certain regularity conditions ( C1) (C3 ) shown below . Let Ω denote the parameter space for θ .
( C1 ) The observations Vi : i = 1 , , n are independent and identically distributed with a probability density f ( V , θ ) , which has a common support . We assume the density f satisfies the following equations : for j = 1 , , p(K + 1 ) ,
∂ log f ( V , θ )
∂θj
Ijk(θ ) =Eθ
=Eθ
= 0
∂ log f ( V , θ ) − ∂2 log f ( V , θ )
∂θj
∂θj∂θk
∂ log f ( V , θ )
∂θk
Eθ and fififififi fififififi ≤ Mjkl(V )
( C2 ) The Fisher Information Matrix
∂ log f ( V , θ )
∂ log f ( V , θ )
T
I(θ ) = E
∂θ
∂θ is finite and positive definite at θ = θ∗ . ( C3 ) There exists an open set ω of Ω that contains the true parameter point θ∗ such that for almost all V the density f ( V , θ ) admits all third derivatives ( ∂3f ( V , θ))/(∂θj ∂θk∂θl ) for all θ ∈ ω and any j , k , l = 1 , , p(K + 1 ) . Furthermore , there exist functions Mjkl such that
∂3
∂θj∂θk∂θl log f ( V , θ ) for all θ ∈ ω where mjkl = Eθ∗ [ Mjkl(V ) ] < ∞ . These regularity conditions are the existence of common support and first , second derivatives for f ( V , θ ) ; Fisher Information matrix being finite and positive definite ; and existence of bounded third derivative for f ( V , θ ) . These regularity conditions guarantee asymptotic normality of the ordinary maximum likelihood estimates [ 11 ] .
Lemma 51 Assume an = o(1 ) as n → ∞ . Then under regularity conditions ( C1) (C3 ) , there exists a local minimizer ˆθn of Qn(θ ) such that || ˆθn − θ∗|| = OP ( n−1/2 + an ) nbn → ∞ and the minimizer ˆθn given in lemma 5.1 satisfies || ˆθn − θ∗|| = OP ( n−1/2 ) . Then under regularity conditions ( C1) (C3 ) , we have
Theorem 52 Assume
√
P ( ˆβAC
1
= 0 ) → 1 ,
P ( ˆαAC
2
= 0 ) → 1
√
Lemma 5.1 implies that when the tuning parameters associated with the non zero coefficients of main effects and pairwise interactions tend to 0 at a rate faster than n−1/2 , then n−consisthere exists a local minimizer of Qn(θ ) , which is tent ( the sampling error is Op(n−1/2) ) . Theorem 5.2 shows √ √ that our model removes noise consistently with high probanbn → ∞ , then lemma 5.1 nan → 0 and bility ( → 1 ) . If √ n−consistent estimator ˆθn and theorem 5.2 imply that the satisfies P ( ˆθAc = 0 ) → 1 . Theorem 53 Assume nbn → ∞ . Then under the regularity conditions ( C1) (C3 ) , the component ˆθA of the local minimizer ˆθn ( given in lemma 5.1 ) satisfies nan → 0 and
√
√
√ n( ˆθA − θ
A ) →d N ( 0 , I ∗
−1(θ
∗ A) ) , where I(θ∗ θ∗ A assuming that θ∗
A ) is the Fisher information matrix of θA at θA =
Ac = 0 is known in advance .
Theorem 5.3 shows that our model estimates the non zero coefficients of the true model with the same asymptotic distribution as if the zero coefficients were known in advance . Based on theorems 5.2 and 5.3 , we can say that our model has the oracle property [ 3 ] , [ 5 ] , when the tuning paramnbn → ∞ . eters satisfy the conditions To satisfy these conditions , we have to consider adaptive weights wβ [ 23 ] for our tuning parameters λβ , λαk ( see appendix for more details ) . Thus , our tuning parameters are :
√ nan → 0 and j , wαk
√ l
λβ j = log n n
λβwβ j ,
λαk l = log n n
λαk wαk l
5.2 Asymptotic Oracle Properties When pn →
∞ as n → ∞
In this section , we consider the asymptotic behavior of our model when the number of predictors pn grows to infinity along with the sample size n . If certain regularity conditions ( C4) (C6 ) ( shown below ) hold , then we can show that our model possesses the oracle property .
We denote the total number of predictors by pn . We denote all the quantities that change with sample size by adding n as their subscript . A1,A2,A are defined as in section 5 and let sn = |An| . The asymptotic properties of our model when the number of predictors increases along with the sample size are described in the following lemma and theorem . The regularity conditions ( C4) (C6 ) are given below : Let Ωn denote the parameter space for θn .
( C4 ) The observations Vni : i = 1 , , n are independent and identically distributed with a probability density fn(Vn , θn ) , which has a common support . We assume the density fn satisfies the following equations :
∂ log fn(Vn , θn )
∂θnj
Eθn and
= 0 for j = 1 , , pn ,
∂ log fn(Vn , θn )
∂θnk
∂ log fn(Vn , θn ) − ∂2 log fn(Vn , θn )
∂θnj
∂θnj∂θnk
Ijk(θn ) =Eθn
=Eθn
∂θn
)( ∂ log fn(Vn1,θn )
( C5 ) In(θn ) = E[( ∂ log fn(Vn1,θn ) )T ] satisfies 0 < C1 < λminIn(θn ) ≤ λmaxIn(θn ) < C2 < ∞ for all n , where λmin( . ) and λmax( . ) represent the smallest and largest eigenvalues of a matrix respectively . Moreover , for any j , k = 1 , , pn ,
∂θn
Eθn < C3 < ∞ , and
Eθn
∂ log fn(Vn1 , θn )
∂ log fn(Vn1 , θn1 )
∂θnj
∂θnk
< C4 < ∞
∂2 log fn(Vn1 , θn )
∂θnj∂θnk
2
( C6 ) There exists a large open set ωn ⊂ Ωn ∈ Rpn which contains the true parameters θ∗ n such that for almost all Vni the density admits all third derivatives ∂3fn(Vni , θn)/∂θnj ∂θnk∂θnl for all θn ∈ ωn . Furthermore , there are functions
Mnjkl such thatfififififi ∂3fn(Vni , θn )
∂θnj∂θnk∂θnl fififififi ≤ Mnjkl(Vni ) for all θn ∈ ωn and
Eθn M 2 njkl(Vni ) < C5 < ∞ for all pn , n , and j , k , l .
Lemma 54 Assume that the density fn(Vn , θ∗ √ n ) satisfies nan → 0 and some regularity conditions ( C4) (C6 ) . n/n → 0 as n → ∞ , then there exists a local minimizer ˆθn p5 √ of Qn(θ ) such that || ˆθn − θ∗ n|| = OP ( pn(n−1/2 + an ) )
If isfies some regularity conditions ( C4) (C6 ) . If
Theorem 55 Suppose that the density fn(Vn , θ∗ n/pnbn → ∞ and p5 ability tending to 1 , the n/pn consistent local minimizer n ) satnpnan → 0 , n/n → 0 as n → ∞ , then with prob
√
ˆθn in Lemma 5.4 satisfies the following :
• Sparsity : ˆθnAc n = 0 • Asymptotic normality : n ( ˆθnAn − θ∗ nAnI
√
1 2 nAn ) →d N ( 0 , G ) where An is an arbitrary m × sn matrix with finite m such n → G and G is a m× m nonnegative symmetric that AnAT matrix and In(θ∗ nAn ) is the Fisher information matrix of nAn . Since the dimension of ˆθnAn → ∞ θnAn at θnAn = θ∗ as sample size n → ∞ , we could consider arbitrary linear combination An ˆθnAn for the asymptotic normality of our model ’s estimates . Similar to section 5.1 , to satisfy oracle nj , wαk property , we have to consider an adaptive weights wβ [ 23 ] for our tuning parameters λβ , λαk as : nl
λβ nj = log(n)pn n
λβwβ nj ,
λαk n,l = log(n)pn n
λαk wαk nl
6 . OPTIMIZATION
In this section , we outline three optimization methods that we employ to solve our objective function ( 7 ) , which corresponds to Line 4 and 5 in Algorithm 1 . [ 15 ] provides a good survey on several optimization approaches for solving 1 regularized regression problems . In this paper , we use the sub gradient and co ordinate wise soft thresholding based optimization methods since they work well and are easy to implement . We compare these methods in the experimental results in section 7 . 6.1 Sub Gradient Methods
Sub gradient based strategies treat the non differentiable objective as a non smooth optimization problem and use sub gradients of the objective function at the non differentiable points . For our model , the optimality conditions wrt parameter vectors β and ak can be written out separately based on the objective function ( 7 ) . Optimality conditions wrt ak is:fl ∇jL(ak ) + λak sgn(akj ) = 0
|∇jL(ak)| ≤ λak
|akj| > 0 akj = 0 where L(ak ) is the loss function of our linear regression model or logistic regression model in Equation ( 7 ) wrt ak . Similarly , optimality conditions can be written for β . The sub gradient ∇s j f ( ak ) for each akj is given by

∇s j f ( ak ) = ∇jL(ak ) + λak sgn(akj ) , ∇jL(ak ) + λak , ∇jL(ak ) − λak , 0 ,
∇jL(ak ) =
( −2X(i )
1 2 − XT ( i)WX(i) ] . i
|akj| > 0 akj = 0,∇jL(ak ) < −λak akj = 0,∇jL(ak ) > λak −λak ≤ ∇jL(ak ) ≤ λak j XT ( i)ak)[y(i ) − βT X(i ) where for our linear regression model . The negation of the subgradient represents the steepest descent direction . Similarly the sub gradients for β ( ∇s j f ( β ) ) can be calculated . Differential of the loss function of the linear regression in Equation ( 7 ) wrt β is given by
∇jL(β ) =
1 2
( −2X(i ) j )[y(i ) − βT X(i ) − XT ( i)WX(i ) ] i
611 Orthant Wise Descent ( OWD ) Andrew and Gao [ 1 ] proposed an effective strategy for solving large scale 1 regularized regression problems based on choosing an appropriate steepest descent direction for the objective function and taking a step like a Newton iteration in this direction ( with an L BFGS Hessian approximation [ 12] ) . The orthant wise learning descent method for our model takes the following form β ← PO[β − γβPS [ H β ∇sf ( β) ] ] −1 ak ← PO[ak − γakPS [ H ak ∇sf ( ak) ] ] −1 where PO and PS are two projection operators and Hβ is the positive definite approximation of Hessian of quadratic approximation of objective function f ( β ) , and γβ and γak are step sizes . PS projects the Newton like direction to guarantee that it is in the descent direction . PO projects the step onto the orthant containing β or ak and ensures that line search does not cross points of non differentiability . 612 Projected Scaled Sub Gradient ( PSS ) Schmidt [ 15 ] proposed optimization methods called Projected Scaled Sub Gradient methods where the iterations can be written as the projection of a scaling of a sub gradient of the objective . Please refer to [ 1 ] and [ 15 ] for more details on OWD and PSS methods . 6.2 Soft thresholding
Soft thresholding based co ordinate descent optimization method can be used to find β , ak updates in the alternating optimization algorithm for our FHIM model . The β updates are ˜βj and are given by
˜βj(λβ ) ←S
˜βj(λβ ) + − n i=1
Xij(yi − k=j
Xjk ˜βk k
XikWXki ) , λβ where W = k ak fi ak , and S is the soft thresholding operator [ 7 ] . Similarly , the updates for ak are ˜akj and given by ˜akj(λak ) ←S n Xjk ˜βk − akrXir)[yi−
XikW∼jXki ] , λak
˜akj(λak ) + p
Xij( r=1 i=1 k k=j k where W∼j is W with jth column and jth row elements are all zero .
7 . EXPERIMENTS
In this section , we use synthetic and real datasets to demonstrate the efficacy of our model ( FHIM ) , and compare its performance with LASSO [ 18 ] , All Pairs Lasso [ 2 ] , Hierarchical LASSO [ 2 ] , Group Lasso [ 21 ] , Trace norm [ 9 ] , Dirty model [ 8 ] and QUIRE [ 14 ] . For all these models , we perform 5 runs of 5 fold cross validation on training dataset ( 80 % ) to find the optimal parameters and evaluate prediction error on a test dataset ( 20 % ) . We search tuning parameters for all methods using grid search and for our model the parameters λβ and λak are searched in the range of [ 0.01 , 10 ] . We also discuss the support recovery of β and W for our model . 7.1 Datasets
We use synthetic datasets and a real dataset for classification and support recovery experiments . We give detailed description of these datasets below .
Synthetic Dataset k=1 akaT matrices ie W =K
711 We generate the predictors of the design matrix X using a normal distribution with mean zero and variance one . The weight matrix W was generated as a sum of K rank one k . β , ak were generated as a sparse vector from a normal distribution with mean 0 and variance 1 , while noise vector is generated from a normal distribution with mean 0 and variance 01 Finally , the response vectors y of the linear and logistic regression models with pairwise interactions were generated using Equations ( 3 ) and ( 5 ) respectively . We generated several synthetic datasets by varying number of instances ( n ) , number of variables/predictors ( p ) , rank of W ie K and sparsity level of β , ak . We denote the combined total predictors ( that is main effects predictors + predictors for interaction terms ) by q , here q = p(p + 1)/2 . Sparsity level ( non zeros ) was chosen as 2 ∼ 4 % for large p(> 100 ) , and 5 ∼ 10 % for small p(< 100 ) for both β , ak . In this paper , we show results for synthetic data in these settings : Case ( 1 ) n > p and q > n ( high dimensional setting wrt combined predictors ) and , Case ( 2 ) p > n ( high dimensional wrt original predictors ) .
2 . Classification experiments using RCC samples : We perform three stage wise binary classification experiments using RCC samples : ( a ) Case 1 : Classification of Benign samples from Stage 1 − 4 samples . ples from Stage 2 − 4 samples .
( b ) Case 2 : Classification of Benign and Stage 1 sam
( c ) Case 3 : Classification of Benign , Stage 1 , 2 sam ples from Stage 3 , 4 samples .
714 Performance on Synthetic dataset We evaluate the performance of our model ( FHIM ) on synthetic dataset by the following experiments : ( i ) Comparison of optimization methods presented in section 6 , ( ii ) Prediction error on the test data for q > n and p > n ( highdimensional settings ) , ( iii ) Support recovery accuracy of β , W and ( iv ) Prediction of rank of W using greedy approach . Table 3 shows the prediction error on test data when different optimization methods ( discussed in section 6 ) are used for our model ( FHIM ) . From table 3 , we see that both OWD and PSS methods perform nearly similar ( OWD is marginally better ) , and are better than the soft thresholding method . This is because , in soft thresholding , co ordinate updates of variables might not be accurate in high dimensional settings ( ie the solution is affected by the path taken during updates ) . We observed that soft thresholding in general is slower than OWD and PSS methods . For all the other experiments discussed in this paper , we choose OWD as the optimization method for FHIM . Table 1 and Table 2 shows
712 Real Dataset To predict cancer progression status directly from blood samples , we generated our own dataset . All samples and clinical information were collected under Health Insurance Portability and Accountability Act compliance from study participants after obtaining written informed consent under clinical research protocols approved by the institutional review boards for each site . Blood was processed within 2 hours of collection according to established standard operating procedures . To predict RCC status , serum samples were collected at a single study site from patients diagnosed with RCC or benign renal mass prior to treatment . Definitive pathology diagnosis of RCC and cancer stage was made after resection . Outcome data was obtained through followup from 3 months to 5 years after initial treatment . Our RCC dataset contains 212 RCC samples from benign and 4 different stages of tumor . Expression levels of 1092 proteins based on a high throughput SOMAmer protein quantification technology are collected . The number of Benign , Stage 1 , Stage 2 , Stage 3 and Stage 4 tumor samples are 40 ; 101 ; 17 ; 24 and 31 respectively .
713 Experimental Design We use linear regression models ( Equation 3 ) for all the following experiments and we only use logistic regression models ( Equation 5 ) for synthetic data experiments shown in table 2 . We evaluate the performance of our method ( FHIM ) by the following experiments :
1 . Prediction error and support recovery experiments on synthetic datasets
Figure 1 : Support Recovery of β ( 90 % sparse ) and W ( 99 % sparse ) for synthetic data Case 1 : n > p and q > n where n = 1000 , p = 50 , q = 1275 .
Figure 2 : Support Recovery of W ( 99.5 % sparse ) for synthetic data Case 2 : p > n where p = 500 , n = 100 . Online supplementary materials contain high quality images for this figure . the performance comparison ( in terms of prediction error on test dataset ) of FHIM for linear and logistic regression models with respect to the state of the art approaches such as Lasso , Fused Lasso , Trace Norm and Hierarchical Lasso ( HLasso is a general version of SHIM [ 3] ) . From tables 1 and 2 , we see that FHIM generally outperforms all the state ofthe art approaches for both linear and logistic pairwise regression models . For q > n , we see that test data prediction n , p , K
1000 , 50 , 1 1000 , 50 , 5
10000 , 500 , 1 10000 , 500 , 5
100 , 500 , 1 100 , 1000 , 1 100 , 2000 , 1
FHIM
3384(145 ) 3437(129 ) 10931(195 )
109076(1221 ) 230.49 ( 50.3 ) 340.1 ( 40.02 ) 907.8 ( 100.1 )
Fused Lasso
4259(207 )
18883(1211 ) 273957(1551 ) 22720(597.8 ) 11572(3550 ) 7709(1276 ) 10223(4062 ) q > n p > n
Lasso
HLasso
Trace norm Dirty Model
4747(153 )
19229(1439 ) 38963(1295 ) 232796(2313 ) 13350(1592 ) 8791(1803 ) 9192(1321 )
354.32 ( 24.82 ) 889.1 ( 112.5 )
4644(363 ) 18226(998 ) 38879(1011 ) 229165(3214 ) 11603(2997 ) 6999(2087 ) 88042(4716 )
6135(076 ) 24538(076 ) 46747(08 ) 29214(0.8 ) 16519(626 )
8081(51 )
19167(634 )
Table 1 : Performance comparison for synthetic data on linear regression model with high order interactions . Prediction Error ( MSE ) and Std . deviation of MSE ( shown inside brackets ) on test data is used to measure the model ’s performance . For p >= 500 , Hierarchical Lasso ( HLasso ) has heavy computational complexity , hence we don’t show it ’s results here . n , p , K
1000 , 50 , 1 1000 , 50 , 5
10000 , 500 , 1 10000 , 500 , 5
100 , 500 , 1 100 , 1000 , 1
FHIM
0.127 ( 0.009 ) 0.189 ( 0.03 ) 0.135 ( 0.002 ) 0.390 ( 0.05 ) 0.325 ( 0.04 ) 0.390 ( 0.056 )
Fused Lasso 0.128 ( 0.017 ) 0.227 ( 0.024 ) 0.265 ( 0.007 ) 0.514 ( 0.006 ) 0.352 ( 0.086 ) 0409(0086 )
Lasso
0.156 ( 0.017 ) 0.292 ( 0.042 ) 0.161 ( 0.012 ) 0507(0108 ) 04323(0054 ) 0458(0083 ) q > n p > n
HLasso
0.136 ( 0.02 ) 0.257 ( 0.022 )
Trace norm 0.128 ( 0.016 ) 0.503 ( 0.027 ) 0.225 ( 0.077 ) 0.514 ( 0.006 ) 040(0079 ) 0438(0011 )
Table 2 : Performance comparison for synthetic dataset on logistic regression model with high order interactions . Misclassification Error on test data is used to measure the model ’s performance error for FHIM is consistently lower compared to all other approaches . For p > n , FHIM performs slightly better than other approaches , however , the prediction error for all the approaches is high since it ’s hard to accurately recover the coefficients of main effects and pairwise interactions in very high dimensional settings .
From figure 1 and table 4 , we see that our model performs very well ( F 1 score close to 1 ) in the support recovery of β and W for the q > n setting . From figure 2 and table 5 , we see that our model performs fairly well in the support recovery of W for p > n setting . We observe that when the tuning parameters are correctly chosen , support recovery of W works very well when W is low rank ( see table 4 and 5 ) , and the F 1 score for the support recovery of W decreases with increase in rank of W . Table 5 shows that for q > n the greedy strategy of FHIM accurately recovers the rank K of W , while for p > n , the greedy strategy might not correctly recover K . This is because the tensor factorization is not unique and slightly correlated variables can enter our model during optimization . the markers selected by Lasso , All Pairs Lasso [ 2 ] , Group Lasso , Dirty model [ 8 ] and QUIRE . We use SLEP [ 13 ] , MALSAR [ 22 ] and QUIRE packages for the implementation of these models . The overall performance of the algorithms are shown in Figure 3 . In this figure , we report average AUC score for five runs of 5 fold cross validation experiments for cancer stage prediction in RCC . In 5 fold cross validation experiments , we train our model on the four folds to identify the main effects and pairwise interactions and we use the remaining one fold for testing prediction . The average AUC achieved by features selected with our model are 0.68 , 0.89 and 0.84 respectively for the three cases discussed in section 713 We performed pairwise t tests for the comparisons of our method vs . the other methods , and all p values are below 00075 From figure 3 , it is clear that our model outperforms all the other algorithms that do not use prior feature group information for all the three classification cases of RCC prediction . In addition , our model has similar performance to the state of the art technique QUIRE [ 14 ] , which uses Gene Ontology based functional annotation for grouping and clustering of genes to identify high order interactions .
Figure 3 : Comparison of the classification performances of different feature selection approaches with our model in identifying the different stages of RCC . We perform five fold cross validation five times and average AUC score is reported . 715 Classification Performance on RCC In this section , we report systematic experimental results on classification of samples from different stages of RCC . The predictive performance of the markers and pairwise interactions selected by our model ( FHIM ) is compared against
Figure 4 : Examples of functional modules for RCC Case 3 , induced by markers and interactions discovered by our model and enriched in pathways and functions associated with RCC
Informative interactions discovered by FHIM 716 An investigation of the pairwise interactions identified by our model on RCC dataset reveals that many of these interactions are indeed relevant to the prediction of cancer . Figure 4 shows some of the interactions associated with higher
0405060708091Benign vs . Stage1 4Benign , Stage 1 vs.Stage 2 4Benign , Stage 1 2vs . Stage 3 4Average area under ROC curve RCC Dataset APLassoLassoTrace normGroup LassoDirty ModelQUIREFHIM ( Ours ) IL5RA ACE2 CHEK2 PES1 AIP CD97 CXC3L1 IL6ST weighted pairwise co efficients for Case 3 of RCC classification experiment . The interactions include CX3CL1 CD97 , CHEK2 IL5RA which are known to be related to proteins in blood . CX3CL1 was recently found to promote breast cancer [ 17 ] , while CD97 was found to promote colorectal cancer [ 19 ] . We believe these protein interactions might lead to renal cell cancer . Further investigations of the interactions identified by our model might reveal novel protein interactions associated with renal cell cancer and thus leading to testable hypothesis . 717 Time Complexity FHIM has O(np ) time complexity for algorithm 1 . In general , FHIM takes more time than the Lasso approach since we do alternating optimization of β , ak . For q ∼ n setting with n = 1000 , q = 1275 , our OWD learning optimization method on Matlab takes around ∼ 1 minute for 5 fold cross validation , while for p > n with p = 2000 , n = 100 , our FHIM model took around 2 hours for 5 fold cross validation . Our experiments were run on intel i3 dual core 2.9 GHz CPU with 8 GB RAM . n , p
OWD Soft thres PSS
100 , 500 100 , 1000 100 , 2000
230.5 340.1 907.8
holding
276.2 710.5 1174.1
239.5 358.7 927.4
Table 3 : Comparison of optimization methods for our FHIM model based on test data prediction error n , p
Sparsity K Support recovery
1000 , 50 1000 , 50 1000 , 50
10000 , 500 10000 , 500 10000 , 500
β , ak 5 , 5 5 , 5 5 , 5
10 , 20 10 , 20 10 , 20
1 3 5 1 3 5
β , W ( F1 score )
1.0 , 1.0 1.0 , 0.95 1.0 , 0.82 0.95 , 0.72 0.80 , 0.64 0.72 , 0.55
Table 4 : Support recovery of β , W n , p true estimated W support recovery
1000 , 50 1000 , 50 1000 , 50 100 , 100 100 , 500 100 , 1000
K 1 3 5 1 3 5
K 1 3 5 2 2 4
F1 score
1.0 1.0 0.8 0.75 0.6 0.5
Table 5 : Recovering K using greedy strategy
8 . CONCLUSIONS
In this paper , we proposed a factorization based sparse learning framework called FHIM for identifying high order feature interactions in linear and logistic regression models , and studied several optimization methods for our model . Empirical experiments on synthetic and real datasets showed that our model outperforms several well known techniques such as Lasso , Trace norm , GroupLasso and achieves comparable performance to the current state of the art method QUIRE , while not assuming any prior knowledge about the data . Our model gives ‘interpretable’ results for high order feature interactions on RCC dataset which can be used for biomarker discovery for disease diagnosis . In the future , we will consider the following directions : ( i ) We will consider factorization of the weight matrix W as
W = k akbT k and higher order feature interactions , which is more general , but the optimization problem is non convex ; ( ii ) We will extend our optimization methods from SingleTask Learning to Multi Task Learning ; ( iii ) We will consider groupings of features for both Single Task Learning and Multi Task Learning .
References [ 1 ] G . Andrew and J . Gao . Scalable training of l1 regularized log linear models . In Proceedings of the 24th international conference on Machine learning , pages 33–40 . ACM , 2007 .
[ 2 ] J . Bien , J . Taylor , and R . Tibshirani . A lasso for hierarchical interactions . The Annals of Statistics , 41(3):1111–1141 , 2013 .
[ 3 ] N . H . Choi , W . Li , and J . Zhu . Variable selection with the strong heredity constraint and its oracle property . Journal of the American Statistical Association , 105(489):354–364 , 2010 .
[ 4 ] D . K . Duvenaud , H . Nickisch , and C . E . Rasmussen .
Additive gaussian processes . In NIPS , pages 226–234 , 2011 .
[ 5 ] J . Fan and R . Li . Variable selection via nonconcave penalized likelihood and its oracle properties . Journal of the American Statistical Association , 96(456):1348–1360 , 2001 .
[ 6 ] R . Foygel , N . Srebro , and R . Salakhutdinov . Matrix reconstruction with the local max norm . arXiv preprint arXiv:1210.5196 , 2012 .
[ 7 ] J . Friedman , T . Hastie , H . H¨ofling , and R . Tibshirani .
Pathwise coordinate optimization . The Annals of Applied Statistics , 1(2):302–332 , 2007 .
[ 8 ] A . Jalali , P . Ravikumar , and S . Sanghavi . A dirty model for multiple sparse regression . arXiv preprint arXiv:1106.5826 , 2011 .
[ 9 ] S . Ji and J . Ye . An accelerated gradient method for trace norm minimization . In Proceedings of the 26th Annual International Conference on Machine Learning , pages 457–464 . ACM , 2009 .
[ 10 ] G . R . G . Lanckriet , N . Cristianini , P . Bartlett , L . E .
Ghaoui , and M . I . Jordan . Learning the kernel matrix with semidefinite programming . J . Mach . Learn . Res . , 5:27–72 , Dec . 2004 .
[ 11 ] E . L . Lehmann and G . Casella . Theory of point estimation , volume 31 . Springer , 1998 .
[ 12 ] D . C . Liu and J . Nocedal . On the limited memory bfgs method for large scale optimization . Mathematical programming , 45(1 3):503–528 , 1989 .
[ 13 ] J . Liu , S . Ji , and J . Ye . SLEP : Sparse Learning with Efficient Projections . Arizona State University , 2009 .
[ 14 ] R . Min , S . Chowdhury , Y . Qi , A . Stewart , and
R . Ostroff . An integrated approach to blood based cancer diagnosis and biomarker discovery . In Proceedings of the Pacific Symposium on Biocomputing , 2014 .
[ 15 ] M . Schmidt . Graphical model structure learning with l1 regularization . PhD thesis , UNIVERSITY OF BRITISH COLUMBIA , 2010 .
[ 16 ] J . A . Suykens and J . Vandewalle . Least squares support vector machine classifiers . Neural processing letters , 9(3):293–300 , 1999 .
[ 17 ] M . Tardaguila , E . Mira , M . A . Garc´ıa Cabezas , A . M .
Feijoo , M . Quintela Fandino , I . Azcoitia , S . A . Lira , and S . Manes . Cx3cl1 promotes breast cancer via transactivation of the egf pathway . Cancer research , 2013 .
[ 18 ] R . Tibshirani . Regression shrinkage and selection via the lasso . Journal of the Royal Statistical Society . Series B ( Methodological ) , pages 267–288 , 1996 .
[ 19 ] M . Wobus , O . Huber , J . Hamann , and G . Aust . Cd97 overexpression in tumor cells at the invasion front in colorectal cancer ( cc ) is independently regulated of the canonical wnt pathway . Molecular carcinogenesis , 45(11):881–886 , 2006 .
[ 20 ] T . T . Wu , Y . F . Chen , T . Hastie , E . Sobel , and
K . Lange . Genome wide association analysis by lasso penalized logistic regression . Bioinformatics , 25(6):714–721 , 2009 .
≥ − Ln(θ − nηn
≥ − Ln(θ − nη2 n(
∗ j∈A1 ∗
)
∗ + ηnδ ) + Ln(θ j |uj| − nηn λβ ( k,l)∈A2
∗ )
+ ηnδ ) + Ln(θ
|uj| +
|vkl| )
|vkl|
λαk l j∈A1 ∗
( k,l)∈A2 ∗
≥ − Ln(θ + ηnδ ) + Ln(θ = − [ ∇Ln(θ )]T ( ηnδ ) − 1 ∗ 2 ( ηnδ)(1 + op(1 ) ) − nη2 n(|A1| + |A2|)d
) − nη2 ( ηnδ)T [ ∇2Ln(θ
∗
) ] n(|A1| + |A2|)d
We used Taylor ’s expansion in above step . We split the above into three parts and we get :
∗
K1 = − ηn[∇Ln(θ = − √ 1√ nηn( n = − Op(nη2
)]T δ ∇Ln(θ
∗
))T δ
1 2 1 2
K2 = nη2
= nη2 K3 = − nη2
∗ n)δ ∇2Ln(θ n{δT [ − 1 n n{δT [ I(θ )δ](1 + op(1))} ∗ n(|A1| + |A2|)d
)δ](1 + op(1))}
[ 21 ] M . Yuan and Y . Lin . Model selection and estimation
Thus , in regression with grouped variables . Journal of the Royal Statistical Society : Series B ( Statistical Methodology ) , 68(1):49–67 , 2006 .
[ 22 ] J . Zhou , J . Chen , and J . Ye . MALSAR : Multi tAsk
Learning via StructurAl Regularization . Arizona State University , 2011 .
[ 23 ] H . Zou . The adaptive lasso and its oracle properties .
Journal of the American statistical association , 101(476):1418–1429 , 2006 .
APPENDIX A . PROOFS FOR SECTION 5.1
Proof of Lemma 51 : Let ηn = n−1/2 + an and {θ∗ + ηnδ : ||δ|| ≤ d} be the ball around θ∗ , where δ = ( u1 , , up , v11 , vKp)T = ( uT , vT )T . Define
Dn(δ ) ≡ Qn(θ
∗
+ ηnδ ) − Qn(θ
∗
)
Where Qn(θ∗ ) is defined in equation ( 8 ) . For δ that satisfies ||δ|| = d , we have
Dn(δ ) = − Ln(θ
∗
) j + ηnuj| − |β ∗
+ ηnδ ) + Ln(θ j ( |β λβ ( |α k,l + ηnvkl)| − |α ∗ j | ) ∗
λαk l k,l| ∗ j
∗ j∈A1 k,l
∗
+ n
+ n
+ n
+ n
( k,l)∈A2
≥ − Ln(θ
∗
)
+ ηnδ ) + Ln(θ j ( |β λβ j + ηnuj| − |β ∗ j | ) ∗
( |α k,l + ηnvkl)| − |α ∗ k,l| ∗
λαk l
Dn(δ ) ≥K1 + K2 + K3
= − Op(nη2
1 2 n)δ + nη2 n(|A1| + |A2|)d
− nη2 n{δT [ I(θ
∗
)δ](1 + op(1))}
We see that K2 dominates the rest of the terms and is positive since I(θ ) is positive definite at θ = θ∗ from regularity condition ( C2 ) . Therefore , for any given > 0 there exists a large enough constant d such that
P{ inf ||δ||=d
∗
Qn(θ
+ ηnδ ) > Qn(θ
∗
)} ≥ 1 −
This implies that with probability at least 1− , there exists a local minimizer in the ball {θ∗ +ηnδ : ||δ|| ≤ d} . Thus , there exists a local minimizer of Qn(θ ) such that || ˆθn − θ∗|| = Op(ηn ) .
Proof of Theorem 52 : Let us first consider P ( ˆαAc
0 ) → 1 . It is sufficient to show that for any ( k , l ) ∈ Ac
2
2
=
∂Qn( ˆθn )
∂αk,l
< 0 for − n < ˆαk,l < 0
( 11 )
∂Qn( ˆθn )
∂αk,l
> 0 for n > ˆαk,l > 0
( 12 ) with probability tending to 1 , where n = Cn−1/2 and C > 0 is any constant . To show ( 12 ) , notice
∂Qn( ˆθn )
∂αk,l
= − Ln( ˆθn ) ∂αk,l = − Ln(θ∗ )
∂αk,l
+ nλαk
− p(K+1 ) j=1 l sgn( ˆαk,l )
∂2Ln(θ∗ ) ∂αk,l∂θj
( ˆθj − θ
∗ j )
( ˆθj − θ j )(ˆθm − θ ∗
∗ m )
Thus , we get ,
√ n
− 1√ n
∇ALn(θ
∗ A )
0 =
√ n(ˆθA − θ
∗ A )
+I(θ
∗ A ) + op(1 )
Therefore , from central limit theorem ,
√ n(ˆθA − θ
A ) →d N ( 0 , I ∗
−1(θ
∗ A ) )
The proofs for lemma and theorem of section 5.2 are along the same lines as above . Please refer to Appendix section C for more details .
Here , we explain how the adaptive weights wβ
B . COMPUTING ADAPTIVE WEIGHTS j , wαk can be calculated for tuning parameters λβ , λαk in Theoretical properties ( Theorems 5.3 & 5.5 ) of Section 5 . Let q be the total number of predictors , let n be total number of instances . j , wαk When n > q , we can compute the adaptive weights wβ for tuning parameters λβ using ordinary least squares ( OLS ) estimates of the training observations . j , λαk l l l
λβ j = log n n
λβwβ j ,
λαk l = log n n
λαk wαk l j = | wβ
| ,
1
ˆβOLS j l = | wαk
| ,
1 OLS l
ˆαk
OLS j
When q > n , the OLS estimates are not available and so we compute the weights using the ridge regression estimates , that is , replacing all the above OLS estimates with the ridge regression estimates . The tuning parameter for ridge regression can be selected using cross validation . Note , we find ˆαk by taking least squares wrt to each αk where k ∈ [ 0 , K ] for some K ≥ trueK . Without loss of generality we can assume K = trueK for proving the Theoretical properties in section 5 . Even if K ≥ trueK , it does not affect the Theoretical properties since the cardinality of A2(|A2| ) does not affect the root n consistency ( see , proof of lemma 51 ) In practice , K is greedily chosen by algo . 1 in our paper .
C . SUPPLEMENTARY MATERIALS
For interested readers , we provide online supplementary materials at http://wwwcstorontoedu/~cuty/FHIM_Supp pdf with detailed proofs for Lemma 5.4 and Theorem 55 These proofs do not affect the understanding of this paper . We also provide high quality images for figure 1 in the supplementary materials . We provide more details about the experimental settings for state of the art techniques used in this paper .
Thus , ˆθA should satisfy ∂Qn(θA )
∂θj fififiθA=ˆθA
= 0
∀j ∈ A
( 13 ) where
− p(K+1 ) p(K+1 ) m=1 j=1 +nλαk l sgn( ˆαk,l )
∂3Ln( ˜θ )
∂αk,l∂θj∂θm where ˜θ lies between ˆθn and θ∗ . By regularity conditions ( C1) (C3 ) and the Lemma 5.1 , we have
∂Qn( ˆθn )
√ n{Op(1 ) + l → ∞ for ( k , l ) ∈ Ac
∂αk,l
=
√ nλαk
2 from the assumption , the is dominated by sgn( ˆαk,j ) . Thus , l sgn( ˆαk,l)}
√ nλαk
As sign of ∂Qn( ˆθn )
∂αk,l
P
∂Qn( ˆθn )
∂αk,l
> 0 for 0 < ˆαk,l < n
→ 1 as n → ∞
= 0 ) → 1 ( 11 ) has identical proof as above . Also , P ( ˆβAc can be proved similarly since in our model β and α are independent of each other .
1
Proof of Theorem 53 Let Qn(θA ) denote the objective function Qn only on the A−component of θ , that is Qn(θ ) with θAc . Based on Lemma 5.1 and Theorem 5.2 , we have P ( ˆθAc = 0 ) → 1 . Thus ,
P arg min θA
Qn(θA ) = ( A − component of
→ 1 arg min
Qn(θ ) )
θ with probability tending to 1 . Let Ln(θA ) and Pλ(θA ) denote the log likelihood function of θA and the penalty function of θA respectively so that we have
Qn(θA ) = −Ln(θA ) + nPλ(θA )
From ( 13 ) , we have
∇AQn(ˆθA ) = −∇ALn(ˆθA ) + n∇APλ(ˆθA ) = 0 ,
( 14 ) with probability tending to 1 . Now , consider by Taylor expansion of first term and second terms at θA = θ∗
A , we get the following :
A ) − [ ∇2ALn(θ ∗
∗ A ) + op(1 ) ]
−∇ALn(ˆθA ) = − ∇ALn(θ ∗ A ) − 1√ n
( ˆθA − θ √ n
= n∇APλ(ˆθA ) =n
∇ALn(θ
∗ A)+
√ n(ˆθA − θ
∗ A )
I(θ
∗ A ) + op(1 )
λβ j sgn(βj ) λαk l sgn(αk,l )
∗ A )
+op(1)(ˆθA − θ √ = nop(1 ) j∈A1,(k,l)∈A2
√ since nan = o(1 ) and ||ˆθA − θ∗
A|| = Op(n−1/2 )
