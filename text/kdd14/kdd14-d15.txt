Batch Discovery of Recurring Rare Classes toward
Identifying Anomalous Samples
∗ Murat Dundar Computer Science
Department
IUPUI
723 W . Michigan St . Indianapolis , IN 46202 dundar@csiupuiedu
Halid Ziya Yerebakan
Computer Science
Department
IUPUI
723 W . Michigan St . Indianapolis , IN 46202 hzyereba@csiupuiedu
Bartek Rajwa
Bindley Bioscience Center
Purdue University
1203 W . State Street W . Lafayette , IN 47907 rajwa@cytopurdueedu
ABSTRACT We present a clustering algorithm for discovering rare yet significant recurring classes across a batch of samples in the presence of random effects . We model each sample data by an infinite mixture of Dirichlet process Gaussian mixture models ( DPMs ) with each DPM representing the noisy realization of its corresponding class distribution in a given sample . We introduce dependencies across multiple samples by placing a global Dirichlet process prior over individual DPMs . This hierarchical prior introduces a sharing mechanism across samples and allows for identifying local realizations of classes across samples . We use collapsed Gibbs sampler for inference to recover local DPMs and identify their class associations . We demonstrate the utility of the proposed algorithm , processing a flow cytometry data set containing two extremely rare cell populations , and report results that significantly outperform competing techniques . The source code of the proposed algorithm is available on the web via the link : http://csiupuiedu/~dundar/ aspirehtm
Categories and Subject Descriptors I53 [ Pattern recognition ] : Clustering—algorithms
General Terms Algorithms
Keywords hierarchical Dirichlet process , random effects , batch clustering , recurring classes , rare classes , anomaly detection
INTRODUCTION
1 . ∗Corresponding author .
Rare class discovery is a difficult machine learning problem that occurs in various practical settings , including visual surveillance and monitoring , quality control , astronomy , physics , and – last but certainly not least – life sciences . A solution to the detection of rare classes is essential for rapid identification of samples with anomalous patterns of data . In this context a normal sample can be considered to be a composition of data points each originating from a predefined , ie , known , class . Unlike normal samples , anomalous samples contain data points originating from classes not known beforehand and thus are considered undefined . An anomalous sample may contain data points from both defined and undefined classes ; however , the points belonging to undefined classes are usually far less frequent than those originating from predefined ones , hence the term rare classes . Predefined classes are recurring and form reproducible patterns ( in terms of class membership proportions ) across all normal samples , whereas rare classes do not necessarily recur , and when they do , they may form varying patterns of class proportions in each anomalous sample . Therefore , anomalous samples can be as different from each other as they are from normal samples , in terms of the specific subset of rare classes present and their membership proportions . We assume that data for each sample are generated by local distributions of classes present in that sample . The total number of classes across all samples is not known . The number and the specific subset of classes locally realized in each sample are also not known . Ideally , local distributions of a given class across all samples should be identical , as they are snapshots of the same underlying model . However , random effects that arise from various sources affecting sample to sample heterogeneity cause local distributions of the same class to vary significantly from one sample to other . This makes automated matching of local distributions across samples an arduous task , which is further complicated when some classes are represented by only a small number of data points . As a result , identifying the subset of classes present in each sample and recovering true class distributions become impractical without modeling random effects . Thus , the main objective of this study reaches beyond clustering on a per sample basis , but addresses the issue of grouping local clusters across multiple samples to identify subsets of classes present in each sample . This goal is achieved under the severe constraint imposing the model in which some classes are rare , local class distributions vary from sample to sample owing to random effects , and classes may disappear altogether from some samples .
1.1 Motivation
Our research has been motivated mainly by a practical problem related to automated clinical diagnostics , involving flow cytometry ( FC ) data analysis .
FC is a single cell screening , analysis , and sorting technology that plays a crucial role in research and clinical immunology , hematology , and oncology . The power of FC lies in its ability to quantify phenotypic characteristics of individual cells in a high throughput manner . This unique capability allows FC to study complex inter cellular networks , such as the immune system as it responds to various external perturbants , including pathogens , chemical compounds ( drugs ) , or vaccination . The cellular phenotypes are defined in FC by combinations of morphological features ( measured by elastic light scatter ) and abundances of surface and intracellular markers . Each biological sample contains multiple , functionally distinct cell types , or “ cell populations ” in FC vernacular . These populations form multidimensional clusters in the space defined by measured biological features . Although the characteristics of cell populations present in normal samples are generally known , the number of populations and the proportions of cells present in them could be substantially different in anomalous ( often diagnostically relevant ) samples .
Given the rapid increase in FC data abundance and the unsatisfactory level of engagement from the machine learning community , FC researchers have been organizing the annual FlowCAP ( Flow Cytometry Critical Assessment of Population Identification Methods ) competition in order to increase awareness and elicit help from data scientists . The problem that our study tackles is related to the rare class classification challenge introduced in FlowCAP 2012 [ 2 ] . The data set used in this challenge was produced by multiple laboratories participating in the External Quality Assurance Program Oversight Laboratory ( EQAPOL ) project [ 1 ] .
The data sets containing two biologically important rarecell populations represent samples that were subject to several potential sources of variation , including natural biological variability , different stimulation levels , and data acquisition in different laboratories . The challenge provided several data sets representing three biological samples , at three levels of stimulation , collected in fifteen FC laboratories across the US . For the purpose of method verification the data points ( individual cells analyzed by FC ) belonging to two rare classes were manually labeled by experts . The remaining data points , considered “ normal ” ( and hence not interesting from the perspective of rare class discovery ) , were all labeled as a single predefined abundant class . A typical sample contained about three hundred thousand data points of which only less than one percent belonged to one of the two rare classes .
The FlowCAP challenge framed this problem in a standard supervised classification setting in which the contestants were provided with half the samples as training data and were required to build classifier models subsequently assessed by the organizers using the remaining test data .
Although we appreciate the complexity and the difficulty of the challenge , we believe it represented the best case scenario and a relatively easy problem setting . Therefore , our problem formulation presented in this report differs significantly from the FlowCAP challenge description . We rec ognize that biologically the rare classes may emerge as a result of various external perturbants , some of which may be unknown a priori . Thus , defining rare classes in an exhaustive fashion may not be realistic . In other words , defining rare classes on the basis of a small available subset present in the training data inevitably leads to classifiers that are biased towards those particular types of rare classes . Such models may not generalize well when applied to future samples in which rare classes may originate owing to other biological mechanisms . Therefore , our problem formulation requires that rare class discovery be performed in the absence of labeled data points representing these classes in the training sets . Herein , we present a nonparametric Bayesian algorithm called ASPIRE ( anomalous sample phenotype identification with random effects ) that identifies biologically significant phenotypes across a batch of samples in the presence of random effects . 1.2 Proposed Approach
We model each sample data by a mixture of potentially infinitely many Dirichlet process Gaussian mixture models ( DPMs ) with each individual DPM modeling the local distribution of a single class . Under fairly weak assumptions and given enough components , finite mixtures of Gaussian distributions can model a given density arbitrarily closely [ 9 ] . The DPM itself is a mixture of potentially infinitely many Gaussian distributions with the actual number of mixture components determined directly from the data during inference . Thus , modeling local class distributions by DPMs offers the flexibility needed to accommodate class data that may arise in samples subjected to significant sources of variations .
As local distributions of a given class are noisy realizations of the true class distribution we introduce a sharing mechanism to create dependencies across DPMs associated with the same class . This is achieved by centering the base distributions of DPMs associated with the same class on a unique global parameter , which itself is distributed according to a higher level DPM . This global DPM not only associates local distributions of a given class with one another but also models the number and proportions of classes in each sample .
We use a collapsed Gibbs sampler to perform inference . Model learning , which is performed in a single unified process , involves three main tasks : recovering DPMs in each sample , finding class associations of DPMs , and identifying the total number of classes and their proportions in each sample .
ASPIRE is capable of identifying recurring classes ( both normal and rare ) in a completely unsupervised way across a batch of samples that are significantly perturbed by random effects and can characterize normal as well as anomalous states given only very weak assumptions regarding sample characteristics and origin . 1.3 Related Work
Existing lines of work that can be adapted to solve the described problem can be broadly grouped into three categories .
The first approach involves pooling data from all samples and applying a standard clustering algorithm to cluster pooled data . Such an approach will have limited success with most real biological data sets because in the presence of random effects , local distributions belonging to one class may significantly overlap with local distributions of another class . The degree of overlap will be more severe in the presence of rare classes . As a result , clusters recovered this way are unlikely to have any meaningful correspondence with the true class distributions .
The second approach involves identifying clusters on a per sample basis and then matching local clusters across samples to recover actual class distributions . Although this technique may perform better than the first solution operating with pooled data , the cluster matching part will remain a big challenge in the presence of random effects and rare classes . As a result , local distributions corresponding to larger classes may not be recovered as a whole and clusters corresponding to rare classes may be incorrectly matched with the distributions of other dominant classes , failing to indicate rare classes . FLAME ( flow analysis with automated multivariate estimation ) [ 11 ] is a well known specialized FC algorithm that can be considered an example belonging to this category . FLAME fits a mixture model into each sample data with four possible choices of density functions ( Gaussian , skewed Gaussian , t distribution , skewed t distribution ) available for individual mixture components . Local modes are pooled and then clustered to obtain a global template of meta clusters . Local clusters are then assigned to these meta clusters using graph matching techniques . FLAME is somewhat similar to ASPIRE in the narrow sense that both techniques model individual sample data by a mixture model . However , there are significant differences in model learning . FLAME divides model learning into three tasks : clustering data in individual samples , finding the optimal number of local clusters in each sample , and matching local clusters across samples to recover classes . These three tasks are performed by FLAME independently in a sequential manner . Unlike FLAME , the model learning by ASPIRE is performed as a single unified process . Thus , ASPIRE can take advantage of recurring patterns of similarities across samples . For example , groups of isolated data points forming rare classes that would be ignored as outliers by clustering followed by cluster matching can be successfully identified as a rare class when these two tasks are performed simultaneously .
The third approach involves performing sample clustering jointly with cluster matching . The proposed ASPIRE model , the hierarchical Dirichlet process Gaussian mixture model ( HDPM ) [ 5 ] , and HDPM with random effects ( HDPMRE ) [ 8 ] all belong to this category . Thanks to their nonparametric nature , the number of local clusters and classes can arbitrarily grow in all three models to better accommodate data as needed . Both HDPM and HDPM RE model individual sample data by a single DPM . HDPM uses the standard hierarchical Dirichlet process prior [ 13 ] , assuming exact sharing of class parameters across all samples and ignoring the presence of random effects . In the presence of random effects this assumption leads to the creation of several extraneous classes . HDPM tackles this problem by postprocessing the results to combine local clusters sharing a common mode . However , such a post processing technique may have limited success , as local clusters of a given class may not necessarily share the same mode . Unlike HDPM , HDPM RE assumes that local clusters are noisy realizations of true class distributions and probabilistically models the deviations of the local cluster means from the mean of the corresponding class distribution .
One key limitation of HDPM RE is the assumption that local class distributions can be effectively captured using a single Gaussian distribution . This assumption is often violated in many real world settings because different sources of variation introduced at different stages of the data collection and processing pipeline create class data that may not be closely approximated by a single Gaussian distribution . In the case of HDPM RE , additional local clusters of a given class are treated as if they belong to another class , thereby splitting a single class into multiple subclasses . Unlike HDPM RE , which uses a single Gaussian distribution for each local distribution of a class , ASPIRE uses a single DPM for each local distribution , allowing for an arbitrarily large number of Gaussian distributions for modeling of local class data . Individual DPMs across samples are linked through class specific global parameters , which are in turn distributed according to a higher level DPM model . In addition to modeling random effects , ASPIRE offers a more flexible data model that can recover class distributions with arbitrary shapes , avoiding the creation of artificial classes .
The rest of this report is organized as follows . In Section 2 we compare data models for DPM , HDPM , HDPM RE , and ASPIRE . In Section 3 we discuss model inference for ASPIRE . In Section 4 we demonstrate the performance of ASPIRE with two experiments and compare results with three other competing techniques . In Section 5 we conclude by summarizing our contributions and offering future research directions .
2 . ASPIRE GENERATIVE MODEL
We describe the technical details of our data model in four incremental stages . In the first stage we assume that each sample is modeled by a single DPM and that DPMs across multiple samples are independent . In the second stage we introduce dependencies across DPMs and impose exact sharing of mixture components corresponding to classes across samples . This is equivalent to the HDPM model . In the third stage we tackle random effects by relaxing the exact sharing of mixture components to allow local clusters to inherit noisy realizations of classes in individual samples . This is equivalent to the HDPM RE model . In the fourth stage we describe the proposed data model for ASPIRE , which models each sample by a potentially infinite mixture of DPMs . 2.1 Independent Modeling of Samples by DPM We denote point i in sample j by xji ∈ d , where i = {1 , . . . , nj.} and j = {1 , . . . , J} , nj . is the number of points in sample j , and J is the total number of samples . In the DPM model xji is associated with a mixture component defined by θji = {µji , Σji} , which in turn is generated iid from a DP as follows : xji ∼ p(·|θji ) θji ∼ Gj
( 1 )
Gj are random probability measures distributed iid according to a DP with a base distribution G0 and a precision parameter α .
Gj ∼ DP ( G0 , α )
( 2 )
Using the stick breaking construction according to [ 7 ] , we can express Gj as
( a ) DPM
( b ) HDPM
( c ) HDPM RE
( d ) ASPIRE
Figure 1 : Plate diagrams for DPM , HDPM , HDPM RE , and ASPIRE . where
Gj = ∞ t−1 l=1 ( 1 − β t=1 βjtδψjt jt
βjt = β jt ∼ Beta(1 , α ) β ψjt ∼ G0 jl )
( 3 )
The points ψjt are called the atoms of Gj . Note that unlike continuous distributions , the probability of sampling the same ψjt twice from Gj is not zero and is proportional to βjt . Thus , Gj is considered a discrete distribution and offers a clustering property , as the same ψjt can be sampled for different θji . In this model α is the parameter that controls the prior probability of assigning a point to a new cluster and thus plays a critical role in the number of clusters generated . For the base distribution G0 , from which ψjt are drawn , we define a bivariate prior : µ|µ0 , p ( µ , Σ ) = N
Σ κ0
× W
−1 ( Σ|Σ0 , m )
( 4 ) where µ0 is the prior mean and κ0 is a scaling constant that controls the deviation of the cluster means from the prior mean . The smaller the κ0 , the larger the separation will be between the cluster means . The parameter Σ0 is a positive definite matrix that encodes our prior belief about the expected Σ , ie , E(Σ ) = Σ0 m−d−1 . The parameter m is a scalar that is negatively correlated with the degrees of freedom . In other words the larger the m , the less Σ will deviate from E(Σ ) , and vice versa . The plate model for independent modeling of samples using one DPM for each sample is available in Figure 1a .
2.2
Introducing dependencies across samples by HDPM
In the previous section we introduced a clustering property across points in an individual sample by placing a DP prior over Gj as in ( 2 ) . Since Gj is a discrete distribution , this prior enables sharing of the same cluster parameter by different points . When dealing with multiple samples , in addition to sharing of clusters by points formed within individual samples , a higher level of sharing occurs . Each local cluster in an individual sample is associated with a class . Thus , as we cluster points in each sample we also need to group local clusters into appropriate classes so that we can identify class associations of local clusters . This grouping can be achieved by introducing dependencies across individual DPMs by placing a hierarchical DP prior over G0 [ 13 ] . The HDPM for joint clustering and cluster matching across multiple samples becomes xji ∼ p ( ·|θji ) θji ∼ Gj Gj ∼ DP ( G0 , α ) G0 ∼ DP ( H , γ )
( 5 ) where γ is the precision parameter for the higher level DP prior and H is defined as in ( 4 ) .
Using the stick breaking construction we can express G0 as
G0 = ∞ k=1 βkδφk
( 6 )
JnjG0GjθjiαxjiJnjHG0GjγθjiαxjiJnjG0G0jHGjγαθjixjiKJnjkG0ϕkHGjkγαθjkixjki where k−1 l=1 ( 1 − β βk = β k k ∼ Beta(1 , α ) β φk = {µk , Σk} ∼ H k )
With this update , instead of letting G0 be distributed according to ( 4 ) as in the independent modeling of samples we let H be distributed according to ( 4 ) and let the atoms of G0 be distributed according to H . The distinct set of parameters φk corresponding to classes is sampled from H and local cluster parameters are sampled from Gj . Since Gj is a discrete distribution with its atoms sampled from G0 , and G0 is a discrete distribution with its atoms sampled from H , each local cluster in turn inherits one of the φk , ie , ψjt ∈ {φk}K t=1 , where K is the number of classes and mj . is the number of local clusters in sample j . k=1 and θji ∈ {ψjt}mj .
Therefore , this model not only groups data points within each sample into clusters , but also groups local clusters across samples into classes . In other words , clustering and cluster matching are simultaneously addressed and depend on one another . The plate model for HDPM is available in Figure 1b . 2.3 Modeling random effects by HDPM RE
In the standard HDPM the same parameters are inherited by all local realizations of a class . However , owing to the potential random effects this surmise may be unrealistic . Therefore , to account for random effects the HDPM RE model [ 8 ] would be more suitable for the discovery of recurring classes . HDPM RE presumes that sample data are generated by noisy versions of parameters defining classes . This change can be incorporated into the data model by updating the model in ( 5 ) as follows : xji ∼ p ( ·|θji ) θji ∼ Gj Gj ∼ DP ( G0j , α ) G0 ∼ DP ( H , γ )
( 7 ) where G0j is a discrete distribution whose atoms are noisy versions of the corresponding atoms in G0 . With this change in the model each individual sample now inherits different noisy realizations of global parameters . The plate model for HDPM RE is available in Figure 1c . 2.4 Modeling individual sample data with mul tiple DPMs
Both HDPM and HDPM RE assumes that local distributions of classes can be closely approximated by a single Gaussian distribution . This assumption is often quite restrictive for many practical settings , as local class data , which are produced subject to random effects , may emerge in the form of skewed as well as multi mode distributions . As a result , fitting a single Gaussian distribution for local class distributions creates artificial classes that may not be easily distinguished from other significant classes .
ASPIRE uses a potentially infinite mixture of DPMs to model each sample data where individual DPMs are linked together through a hierarchical DP prior . This hierarchical prior not only identifies local DPMs associated with the same class through sharing of a global parameter but also models the specific subset of classes present and their proportions in each sample .
We update our indexing notation from previous sections to introduce an additional subscript k to account for multiple DPMs in each sample . We denote point i of class k in sample j by xjki ∈ d , where i = {1 , . . . , njk.} , k = {1 , . . . , K} , and j = {1 , . . . , J} , njk . is the number of points from class k in sample j , K is the total number of classes , and J is the total number of samples . The proposed ASPIRE data model is as follows . xjki ∼ p ( ·|θjki ) θjki ∼ Gjk Gjk ∼ DP ( Fφk , α ) ∼ G0 φk ∼ DP ( H , γ ) G0
( 8 ) where φk are global parameters each of which is associated with a different class . Individual DPMs associated with the same class inherit the same φk across samples . The notation Fφk indicates a distribution F centered at φk and defines class specific base distributions of individual DPMs . Although Fφk is same for all DPMs associated with the same class , local clusters between samples are generated iid given φk of corresponding DPMs . Thus , each local realization of a given class is modeled by a different DPM , allowing for the modeling of sample to sample variations in a systematic manner . The plate model for ASPIRE is available in Figure 1d .
For the sake of simplicity and to preserve conjugacy we assume that the covariance matrices of all local clusters associated with the same class are identical and limit the susceptibility of local clusters to noise with their mean vectors . More specifically , µjki ∼ Gjk , Σjki = Σk , and Fφk is defined as follows .
Fφk={µk,Σk} = N ( µk ,
Σk κ1
)
( 9 )
Note that the covariance matrix of the base distribution Fφk is a function of Σk ; hence conjugacy of the model is preserved . Conjugacy of the model is important as it enables us to implement a collapsed version of the Gibbs sampler as discussed in the next section . The scaling constant κ1 adjusts the degree of deviation of local means from the corresponding global mean . A smaller κ1 results in a situation where local realizations of global means deviate significantly from one sample to another , suggesting significant random effects . On the other hand , a larger κ1 value limits these deviations , resulting in few to no random effects .
3 . ASPIRE MODEL INFERENCE
Posterior inference for the proposed model in ( 8 ) can be performed by a Gibbs sampler by iteratively sampling lo
)K
J i=1 k=1 j=1
,
( {tjki}njk . J ( {cjkt}mjk )K J ( {ψjkt}mjk )K k=1 t=1 t=1 k=1 j=1 j=1
, and lo given the cal cluster indicator variables t = class indicator variables c = cal cluster parameters ψ = state of all other variables . Including ψ in the Gibbs sampler significantly increases the size of the state space and severely retards the convergence of the Gibbs sampler to the equilibrium distribution . Fortunately , our model uses a conjugate pair of H and p(·|ψjkt ) , which allows us to integrate out ψjkt analytically . Thus , in the following we omit the discussion of sampling of ψ and describe sampling for only t and c .
When sampling the local cluster indicator variable tjki for xjki we first remove xjki from its current cluster and update the corresponding predictive distribution p(xjki|Dcjkt , Djkt ) . Then , we evaluate the likelihood of xjki ’s belonging to an existing cluster by evaluating p(xjki|Dcjkt , Djkt ) for all local clusters in sample j , and its likelihood of originating from a new cluster by evaluating the predictive distribution for an empty cluster , ie , p(xjki ) . Finally , we sample tjki based on the normalized values of the product of prior probabilities and the corresponding likelihood values . This can be expressed by the following equation : p(tjki = t|t

−jki , c , D ) αp(xjki ) if jkt p(xjki|Dcjkt , D −jki n if t = mjk + 1 t mjk
∝
−jki jkt )
( 10 )
For the above model the posterior predictive distribution of a local cluster can be derived by evaluating the following integral . first loop tjki are sampled for all points across all samples . In the second loop cjkt are sampled for all local clusters across all samples . The Gibbs sampler is run for a thousand sweeps and the state with the maximum Gibbs likelihood is recorded for final evaluation . 3.1 Deriving predictive distributions
ASPIRE uses the following data model : p(µk|Σk ) p(Σk ) p(µjkt|µk , Σk ) p(xjki|µjkt , Σk ) ∼ N,µjkt , Σk ∼ N,µ0 , κ ∼ N,µk , κ
−1 0 Σk ∼ W −1 ( Σ0 , m ) −1 1 Σk
( 12 ) p(xjki|Dcjkt , Djkt ) = p(xjki|µjkt , Σk ) p(µjkt , Σk|Dcjkt , Djkt)∂µjkt∂Σk
( 13 )
To evaluate the integral in ( 13 ) we need the posterior distribution of the parameters p(µjkt , Σk|Dcjkt , Djkt ) , which can be expressed as follows using the Bayes theorem . p(µjkt , Σk|Dcjkt , Djkt )
∝ p(µjkt , Σk , ¯xjkt , Ajkt|¯xjkt:cjkt=k , Ak )
( 14 ) where ¯xjkt and Ajkt are the sample mean and the scatter matrix for cluster t of class k in sample j , respectively and Ak is the scatter matrix of class k . These statistics are defined as in ( 15 ) .
¯xjkt = n
Ajkt = =
Ak
−1 jkt jki:tjki=t ( xjki − ¯xjkt ) ( xjki − ¯xjkt)T jkt:cjkt=k Ajkt jki:tjki=t xjki
In order to evaluate ( 14 ) we first need to obtain where p(µjkt , Σk , ¯xjkt , Ajkt|¯xjkt:cjkt=k , Ak ) = p(¯xjkt|µjkt , Σk)p(Ajkt|Σk ) p(µjkt|Σk , ¯xjkt:cjkt=k)p(Σk|Ak ) p(¯xjkt|Σk ) = N,µjkt , njkt
−1Σk p(Ajkt|Σk ) = W ( Σk , njkt − 1 ) p(µjkt|Σk , ¯xjkt:cjkt=k ) = N ( ¯µ , ¯κ−1Σk )
= W −1 Σ0 + Ak , m +
( jkt:cjkt=k jkt:cjkt=k jkt:cjkt=k
¯µ =
¯κ = jkt:cjkt=k njktκ1 ( njkt+κ1 ) ¯xjkt + κ0µ0 njktκ1 ( njkt+κ1 ) + κ0 njktκ1 ( njkt+κ1 ) + κ0)κ1 njtκ1 ( njkt+κ1 ) + κ0 + κ1 p(Σk|Ak ) where jkt:cjkt=k(njkt − 1 )
( 15 )
( 16 )
( 17 )
( 18 )
( 19 )
( 20 )
( 21 )
( 22 )
Once the distributions in ( 17) (20 ) are substituted into ( 14 ) a closed form expression for p(µjkt , Σk|Dcjkt , Djkt ) can be where t−jki is the set of all cluster indicator variables , excluding the one for point i of class k in sample j , D denotes the set of all points across all samples , Dcjkt denotes the −jki subset of points sharing class cjkt across all samples , D jkt denotes the subset of points in sample j belonging to cluster t of class k , excluding point i , mjk is the number of clusters associated with class k in sample j , and n is the number of data points in cluster t of class k in sample j , excluding point i .
−jki jkt
As we model local clusters by Gaussian distributions with Gaussian and inverted Wishart priors defined over their mean vectors and covariance matrices , respectively , the predictive distribution p(xjki|Dcjkt , Djkt ) turns out to be in the form of a Student t distribution , the derivation of which is provided in Section 31
When sampling the class indicator variable cjkt for cluster t of class k in sample j we remove points Djkt from Dcjkt and update the parameters of the predictive distribution for class cjkt . Then , we evaluate the joint likelihood of cell data in Djkt for existing classes as well as for a new class . Finally , we sample cjkt based on the normalized values of the product of prior probabilities of classes and the corresponding joint likelihood values . This can be expressed by the following formula : p(cjkt = k|t , c

∝
γ
−jkt , D ) i:tjki=t p(xjki )
−jkt .k if k = K + 1 m if k ≤ K i:tjki=t p(xjki|D
−jkt k
)
( 11 )
−jkt k denotes the subset of points across all samples where D associated with class k , excluding points in cluster t in sample j . The predictive distribution p(x|Dk ) is also obtained in the form of a Student t distribution , and can be readily obtained from p(xjki|Dcjkt , Djkt ) by setting Djkt an empty set . The details are provided in Section 31
Sampling both tjki and cjkt requires evaluating the predictive distribution for a new , ie , an empty , cluster . The predictive distribution for a new cluster is denoted by p(xjki ) in ( 10 ) and ( 11 ) . This distribution can be obtained from p(x|Dk ) by setting Dk an empty set . The details are also provided in Section 31
During a single run of the ASPIRE algorithm one sweep of the Gibbs sampler involves two main iterative loops . In the obtained . When we substitute this solution into ( 13 ) we obtain p(xjki|Dcjkt , Djkt ) in the form of a multivariate Student t distribution with three parameters . p(xjki|Dcjkt , Djkt ) = stu − t( ˆµ , ˆΣ , v )
( 23 )
The location vector ( ˆµ ) , the scale matrix ( ˆΣ ) , and the degrees of freedom ( v ) are given below . Location vector :
ˆµ = njkt ¯xjkt + ¯κ ¯µ njkt + ¯κ
Scale matrix :
Σ0 + Ak + Ajkt + njkt ¯κ njkt+¯κ ( ¯xjkt − ¯µ)(¯xjkt − ¯µ)T
ˆΣ =
( ¯κ+njkt ) v ( ¯κ+njkt+1 )
( 24 )
( 25 )
Degrees of freedom : v = m +
( njkt − 1 ) + njkt − d + 1
( 26 ) jkt:cjkt=k
The predictive distribution of a class can be readily obtained from p(xjki|D.cjkt , Djkt ) by setting Djkt an empty set . This is equivalent to dropping terms related to local clusters in equations ( 24 ) , ( 25 ) , and ( 26 ) . Finally , the predictive distribution of an empty cluster can be obtained from p(xjki|Dk ) by setting Dk an empty set . This is equivalent to dropping terms in p(xjki|Dk ) related to classes .
4 . RESULTS AND DISCUSSIONS
We report results of experiments performed with two different data sets . The first experiment demonstrated the functionality of the algorithms tested using simulated data , while the second experiment utilized real FC data .
Aside from the proposed ASPIRE algorithm , three other techniques were considered : DPM , HDPM , and HDPM RE . In Section 1.3 we described three different approaches to the clustering problem set forth in this study . The first method uses standard clustering algorithms applied to pooled data , the second approach performs clustering and cluster matching in a sequential way , and the third performs clustering jointly with cluster matching . Among the three benchmark techniques DPM belongs to the first category ; HDPM and HDPM RE along with ASPIRE belong to the third category . We chose the well known FC algorithm FLAME to represent the second category . Unfortunately the implementation of FLAME available through GenePattern [ 12 ] produced errors during processing of many of the samples in the two data sets , so we were forced to exclude FLAME from this analysis . For HDPM we used the software provided by the authors in [ 5 ] . For the other three algorithms we used our own implementations . Each algorithm is run for a thousand sweeps , and the state with the best likelihood is recorded for subsequent analysis .
The F1 score is used as the performance measure for comparing performances of these four techniques . As one tomany matchings are expected between true and recovered classes , the F1 score for each class is computed as the maximum of the F1 scores for all recovered classes , similar to [ 3 ] . 4.1 Experiment 1 : Artificial Data Set
We generated twenty samples , each with five thousand data points in a two dimensional feature space , using the
Table 1 : F1 scores achieved and the number of classes recovered by each of the four techniques on the artificial data set .
Class F1 Scores
Method DPM HDPM HDPM RE ASPIRE
1 ( 98.7 % )
2 ( 0.3 % )
3 ( 1 % ) # Classes
1.00 0.84 0.68 1.00
0.75 0.74 0.94 1.00
0.56 0.66 0.85 0.90
5 11 7 3
Table 2 : Number of points available from three classes in the FC data set before and after subsampling . Numbers in parentheses indicate percentage of the total number of points in the corresponding set .
Method Original
Normal 56.2M
( 99.94 % )
# points Rare 1 10.2K ( 0.02 % )
Subsampled
1.9M
9.5K
( 98.23 % )
( 0.50 % )
Rare 2 24.3K ( 0.04 % ) 24.1K ( 1.27 % ) model in ( 8 ) and the following values of the model parameters : κ0 = 0.01 , κ1 = 0.2 , m = 20 , µ0 = [ 0 0]T , Σ0 = I , α = 0.2 , γ = 0.2 , where I denotes the identity matrix . After all data points were sampled , three classes were produced by this model with overall class proportions of 0.987 , 0.003 , and 0.01 , which indicates that two of the three recurring classes can be considered rare . For the pooled data , distributions of local clusters and the true values of the global parameters , ie , φk , are shown in Fig 2a by dashed and solid contours , respectively . The ellipses correspond to data distributions that are at most four standard deviations from the mean . Individual data points are shown by black dots .
We ran all four techniques ( ASPIRE , DPM , HDPM , and HDPM RE ) on this data set and plotted contours representing recovered classes in Figures 2b , 2c , 2d , and 2e , respectively . F1 scores obtained for each class and numbers of classes recovered by all four techniques are included in Table 1 . Results suggest that ASPIRE not only correctly predicts the true number of classes but also estimates global parameters with almost no bias , which in turn produces almost perfect F1 scores for each class . DPM produces a reasonable number of classes but estimates global parameters with a large bias . HDPM fails to consistently match local clusters across samples and substantially overpredicts the actual number of classes . HDPM RE performs better compared to DPM and HDPM but generates several artificial classes , a direct result of modeling local class data by a single Gaussian distribution . 4.2 Experiment 2 : Flow Cytometry Data Set with Two Rare Classes
We evaluated the performance of ASPIRE in discovering rare classes with a FC data set used in the FlowCAP 2012 competition [ 2 ] . The data set contained FC measurements of multiple aliquots of three biological samples exposed to three different stimulation levels . The samples were examined independently by fifteen FC laboratories . In this context the term “ sample ” denotes a tube containing white blood cells .
( a ) Pooled Data
( b ) ASPIRE
( c ) DPM
( d ) HDPM
( e ) HDPM RE
Figure 2 : An illustrative example showing the performance of DPM , HDPM , HDPM RE , and ASPIRE algorithms in estimating global parameters corresponding to classes . Solid color contours plotted using true values of global parameters represent true classes . Dashed color contours indicate true distributions of local clusters with the color identifying the class origin . Solid black contours plotted using estimated values of the global parameters represent recovered classes . Black dots denote data points .
Each cell is separately measured by a flow cytometer . The measurement provides the small angle and large angle lightscatter characteristics as well as four fluorescence intensity values . Thus , each cell is characterized by a six dimensional feature vector . The goal is to recognize the cells belonging to two rare cell populations , manually labeled by experts , without access to information about characteristics of these populations in the training data set . Cells not belonging to one of the two rare populations are considered “ normal ” and were all labeled as a single predefined abundant class . Thus , including the normal class there are three classes in this data set .
The original data set contained data points for about 60 million cells across 202 samples . To obtain a more manageable data set size while preserving cells from rare classes we used a density based subsampling technique and reduced the data size to 1.9 million points . The number of points available from each of the two rare classes as well as the normal class before and after subsampling and their percentages are shown in Table 2 .
As in the previous experiment , we compare ASPIRE against
DPM , HDPM , and HDPM RE . The DPM model has five free parameters ( α , Σ0 , m , κ0 , µ0 ) , the HDPM model has one more parameter ( γ ) than DPM , and HDPM RE and ASPIRE have one more parameter ( κ1 ) than HDPM . These parameters are selected using the following strategy .
Each feature is normalized to have zero mean and unit variance . As the sample batch may contain anomalous samples , prior information about the potential number of local clusters and global classes may not exist for most real world FC data . Thus , for α and γ we use vague priors by fixing their value to one . We set m to the minimum feasible value , which is d + 2 , to achieve maximum degrees of freedom . By doing this we let the actual covariance matrices deviate significantly from the expected covariance matrix , which is E(Σ ) = Σ0 m−d−1 . The prior mean µ0 is set to the mean of the entire data . The scale matrix Σ0 is set to I/s , where I is the identity matrix . This leaves the scaling constant s of Σ0 , κ0 , and κ1 as the three free parameters that require tuning . The parameter κ1 models the deviation of cluster means from their corresponding class mean in the generative model . Thus , increasing κ1 while κ0 and s are fixed
Table 3 : F1 scores achieved and the number of classes recovered by each of the four techniques on the entire FC data set . Results for ASPIRE are averages over ten repetitions . Numbers in parenthesis indicate standard deviations .
Class F1 Scores
Method DPM HDPM HDPM RE ASPIRE
Normal Rare 1 Rare 2 # Classes
0.22 0.23 0.22 0.62 ( 0.02 )
0.20 0.01 0.46 0.59 ( 0.03 )
0.39 0.02 0.63 0.77 ( 0.01 )
175 75 91 38.7 ( 3.20 ) potentially increases the number of classes generated . The parameter κ0 models the deviation of cluster means from the prior mean in the generative model . Thus , increasing κ0 while κ1 and s are fixed potentially increases the number of clusters generated . The parameter s models the expected size of clusters . Increasing s potentially increases the number of clusters generated . These three parameters were coarsely tuned using a generic 5 parameter peripheral blood immunophenotyping data set previously collected and analyzed in our lab as part of an earlier study without retuning them for the FC data used in this experiment . The following values were used : κ0 = 0.05 , κ1 = 0.1 , s = 10 .
F1 scores computed for all three classes are shown in Table 3 . Results for ASPIRE are averages of ten repetitions . As the run time for ten repetitions of the other algorithms would take on the order of weeks , we included results of a single run for these algorithms . Results in Table 3 favor methods modeling random effects ( HDPM RE and ASPIRE ) over those that do not ( DPM and HDPM ) in terms of higher F1 scores achieved for both rare classes . Between techniques that model random effects ASPIRE significantly outperforms HDPM RE in terms of producing a more realistic number of classes and higher F1 scores for all classes . ASPIRE models local realizations of classes by an infinite mixture of Gaussians , which allows for associating multiple clusters with individual classes during inference . The other three techniques use a single Gaussian distribution to model local realization of classes . If a local distribution of a class cannot be effectively modeled by a single Gaussian distribution , these techniques tend to produce multiple local clusters all of which are assigned to a distinct class . As a result ASPIRE tends to generate a fewer number of classes and achieves higher F1 scores compared to the other three techniques .
We also compared ASPIRE with a supervised classifier to find out how F1 scores would improve if a subset of the labeled data were to be used during training . We used all samples belonging to one of the biological samples for training and sequestered all samples for the other two biological samples for testing . The support vector machine toolbox in [ 6 ] was used to train and test a supervised classifier on this data . Parameters of this classifier are extensively tuned to optimize test performance . These results along with the results obtained by ASPIRE on the test data are shown in Table 4 . Results suggest that ASPIRE can predict rare classes with F1 scores comparable to those of a supervised classifier without using any labeled data . The F1 score achieved by ASPIRE for the normal class is worse than that of the
Table 4 : F1 scores achieved by ASPIRE and SVM on the test portion of the FC data set . Results are averages over ten repetitions . Numbers in parenthesis indicate standard deviations .
Class F1 Scores
Method ASPIRE
Supervised
Normal Rare 1 Rare 2
0.62 ( 0.02 ) 1.00 ( 0.00 )
0.54 ( 0.03 ) 0.66 ( 0.01 )
0.75 ( 0.01 ) 0.83 ( 0.01 ) supervised classifier , mainly because the normal class is a combination of multiple uninteresting subclasses for which ASPIRE produces multiple classes to more effectively model the underlying class distribution . However , we do not believe this is a major limitation , as in most practical settings labeled data are present for normal classes as these are classes that are known and predefined . On the other hand , for rare classes , labeled data may not exist because rare classes are usually not known a priori and cannot be predefined . Under such circumstances training a supervised classifier that requires labeled data for all classes may not be very realistic . On the other hand , ASPIRE can cluster data in a fully unsupervised manner and with the help of a limited amount of labeled data from normal classes results can be post processed to distinguish unknown classes from known ones .
For ASPIRE , one sweep of the Gibbs sampler involves two main iterative loops . In the first loop , cluster indicator variables are sampled for all data points across all samples . In the second loop , class indicator variables are sampled for all local clusters across all samples . As the first loop iterates over all points across all samples it is usually more computationally expensive than the second loop . Fortunately , during the sampling of the cluster indicator variables class parameters are fixed . This allows us to sample cluster indicator variables independently for each sample during a single sweep and leads to improvement in processing time on multi processor machines . The actual run time for ASPIRE to process the FC data set containing 1.9 million points is about five and eleven hours with and without parallelization , respectively , on an eight core workstation . The reduction in the overall computational time is not proportional to the number of processors , as the computational gain by parallelizing the first loop will be limited after a certain point by the computational time of the second loop .
5 . CONCLUSIONS
We introduced ASPIRE as a new method for discovering recurring yet significant rare classes in the presence of random effects and showed experimental results that clearly favor ASPIRE over other benchmark techniques . We believe that ability to recover rare classes in FC data sets obtained in fifteen different laboratories convincingly demonstrates that automated identification of anomalous samples in research or diagnostic settings is indeed feasible .
Labeled information about normal , ie , known classes , can be directly incorporated into the learning process by adopting a restricted Gibbs sampler scheme similar to the one introduced in [ 4 ] . Our research was mainly driven by a rare class discovery problem in a clinical setting . However ,
[ 11 ] S . Pyne , X . Hu , K . Wang , E . Rossin , T I Lin , L . M .
Maier , C . Baecher Allan , G . J . McLachlan , P . Tamayo , D . A . Hafler , P . L . De Jager , and J . P . Mesirov . Automated high dimensional flow cytometric data analysis . Proc Natl Acad Sci U S A , 106(21):8519–24 , 2009 .
[ 12 ] M . Reich , T . Liefeld , J . Gould , J . Lerner , T . P . , and
M . JP Genepattern 20 Nature Genetics , 38(5):500–1 , 2006 .
[ 13 ] Y . Teh , M . Jordan , M . Beal , and D . Blei . Hierarchical
Dirichlet processes . Journal of the American Statistical Association , 101(476):1566–1581 , 2006 .
[ 14 ] L . Xiong , B . Poczos , and J . Schneider . Group anomaly detection using flexible genre models . In J . Shawe Taylor , R . Zemel , P . Bartlett , F . Pereira , and K . Weinberger , editors , Advances in Neural Information Processing Systems 24 , pages 1071–1079 . 2011 .
ASPIRE is a general clustering technique that can be used in other disciplines to discover classes with recurring nature irrespective of whether they are rare or normal . ASPIRE can also be utilized for problems involving the detection of group anomalies [ 10 , 14 ] .
ASPIRE is implemented in C++ . The source code is available on the web via the link http://csiupuiedu/~dundar/ aspirehtm
6 . ACKNOWLEDGMENTS
This research was sponsored by the National Science Foundation ( NSF ) under Grant Number IIS 1252648 ( CAREER ) , by the National Institute of Biomedical Imaging and Bioengineering ( NIBIB ) under Grant Number 5R21EB015707 , and by the PhRMA Foundation ( 2012 Research Starter Grant in Informatics ) . The content is solely the responsibility of the authors and does not represent the official views of NSF , NIBIB or PhRMA .
7 . REFERENCES [ 1 ] External quality assurance program oversight laboratory ( EQAPOL ) . http://eqapoldhvidukeedu/
[ 2 ] FlowCAP flow cytometry : Critical assessment of population identification methods . http://flowcapflowsiteorg/
[ 3 ] N . Aghaeepour , G . Finak , FlowCAP Consortium , DREAM Consortium , H . Hoos , T . R . Mosmann , R . Brinkman , R . Gottardo , and R . H . Scheuermann . Critical assessment of automated flow cytometry data analysis techniques . Nature Methods , 10(3):228–238 , mar 2013 .
[ 4 ] F . Akova , Y . Qi , B . Rajwa , and M . Dundar .
Self adjusting models for semi supervised learning in partially observed settings . In IEEE International Conference on Data Mining ( ICDM’12 ) , 2012 . under review .
[ 5 ] A . J . Cron , C . Gouttefangeas , J . Frelinger , L . Lin , S . K . Singh , C . M . Britten , M . J . P . Welters , S . H . van de Burg , M . West , and C . Chan . Hierarchical modeling for rare event detection and cell subset alignment across flow cytometry samples . PLoS Computational Biology , 9:e1003130 , 2013 .
[ 6 ] N . Djuric , L . Lan , S . Vucetic , and Z . Wang .
Budgetedsvm : A toolbox for scalable svm approximations . Journal of Machine Learning Research , 14:3813–3817 , 2013 .
[ 7 ] H . Ishwaran and L . F . James . Gibbs sampling methods for stick breaking priors . Journal of the American Statistical Association , 96(453):pp . 161–173 , 2001 .
[ 8 ] S . Kim and P . Smyth . Hierarchical Dirichlet processes with random effects . In B . Sch¨olkopf , J . C . Platt , and T . Hoffman , editors , Advances in Neural Information Processing Systems 19 , pages 697–704 , Cambridge , MA , 2007 . MIT Press .
[ 9 ] G . McLachlan and D . Peel . Finite Mixture Models .
Wiley Series in Probability and Mathematical Statistics : Applied Probability and Statistics . John Wiley & Sons , 2001 .
[ 10 ] K . Muandet and B . Sch¨olkopf . One class support measure machines for group anomaly detection . CoRR , abs/1303.0309 , 2013 .
