GLAD : Group Anomaly Detection in Social Media Analysis
ROSE YU , XINRAN HE , and YAN LIU , University of Southern California
Traditional anomaly detection on social media mostly focuses on individual point anomalies while anomalous phenomena usually occur in groups . Therefore , it is valuable to study the collective behavior of individuals and detect group anomalies . Existing group anomaly detection approaches rely on the assumption that the groups are known , which can hardly be true in real world social media applications . In this article , we take a generative approach by proposing a hierarchical Bayes model : Group Latent Anomaly Detection ( GLAD ) model . GLAD takes both pairwise and point wise data as input , automatically infers the groups and detects group anomalies simultaneously . To account for the dynamic properties of the social media data , we further generalize GLAD to its dynamic extension d GLAD . We conduct extensive experiments to evaluate our models on both synthetic and real world datasets . The empirical results demonstrate that our approach is effective and robust in discovering latent groups and detecting group anomalies . Categories and Subject Descriptors : H28 [ Database Applications ] : Data Mining General Terms : Anomaly Detection , Social Media Analysis , Hierarchical Bayes Modeling Additional Key Words and Phrases : Group anomaly , topic modeling , community detection ACM Reference Format : Rose Yu , Xinran He , and Yan Liu . 2015 . GLAD : Group anomaly detection in social media analysis . ACM Trans . Knowl . Discov . Data 10 , 2 , Article 18 ( October 2015 ) , 22 pages . DOI : http://dxdoiorg/101145/2811268
1 . INTRODUCTION Social media provide convenient platforms for people to share , communicate , and collaborate . While people enjoy the openness and convenience of social media , many malicious behaviors , such as bullying , terrorist attack planning , and fraud information dissemination , can happen . Therefore , it is extremely important that we can detect these abnormal activities as accurately and early as possible to prevent disasters and attacks .
By definition , anomaly detection aims to find “ an observation that deviates so much from other observations as to arouse suspicion that it was generated by a different mechanism ” [ Hawkins 1980 ] . Several algorithms have been developed specifically for social media anomaly detection such as power law models [ Akoglu et al . 2009 ] , spectral decomposition [ Von Luxburg 2007 ] , scan statistics [ Priebe et al . 2005 ] , and random walk [ Pan et al . 2004 ; Tong et al . 2008 ] . However , these algorithms only detect the
A preliminary version of this paper appeared in the proceedings of the 20th ACM SIGKDD Conference on Knowledge Discovery and Data Mining [ Yu et al . 2014 ] . The research was sponsored by the US Defense Advanced Research Projects Agency ( DARPA ) under the Anomaly Detection at Multiple Scales ( ADAMS ) program , Agreement Number W911NF 11 C 0200 and NSF research grants IIS 1134990 . The views and conclusions are those of the authors and should not be interpreted as representing the official policies of the funding agency , or the US Government . Authors’ addresses : R . Yu , X . He , and Y . Liu , PHE 328 , 3731 Watt Way , Los Angeles , CA , 90089 ; emails : {qiyu , xinranhe , yanliucs}@uscedu Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies show this notice on the first page or initial screen of a display along with the full citation . Copyrights for components of this work owned by others than ACM must be honored . Abstracting with credit is permitted . To copy otherwise , to republish , to post on servers , to redistribute to lists , or to use any component of this work in other works requires prior specific permission and/or a fee . Permissions may be requested from Publications Dept . , ACM , Inc . , 2 Penn Plaza , Suite 701 , New York , NY 10121 0701 USA , fax +1 ( 212 ) 869 0481 , or permissions@acmorg c . 2015 ACM 1556 4681/2015/10 ART18 $15.00 DOI : http://dxdoiorg/101145/2811268
ACM Transactions on Knowledge Discovery from Data , Vol . 10 , No . 2 , Article 18 , Publication date : October 2015 .
18
18:2
R . Yu et al . individual point anomaly . For example , Akoglu et al . [ 2009 ] proposes an “ OddBall ” algorithm to spot anomalous nodes in a graph . The algorithm extracts features from the egonet of the node and declares anomaly node whose features deviate from the power law pattern .
In reality , anomaly may not only appear as an individual point , but also as a group . For instance , a group of people collude to create false product reviews or threat campaign in social media platforms ; in large organizations , malfunctioning teams or insider groups closely coordinate with each other to achieve a malicious goal . Those appear as examples for another type of anomaly : group anomaly , which has not been thoroughly examined in social media analysis . In this work , we focus on group anomaly detection . We are interested in finding the groups which exhibit a pattern that does not conform to the majority of other groups . This problem has found its applications in galaxy identification [ Xiong et al . 2011b ] , high energy particle physics [ Muandet and Sch¨olkopf 2013 ] , anomalous image detection and turbulence vorticity modeling [ Xiong et al . 2011a ] .
We identify three major challenges in group anomaly detection : ( i ) Two forms of data coexist in social media : one is the pointwise data , which characterize the features of an individual person . The other is pairwise relational data , which describe the properties of social ties . In social science , a fundamental axiom of social media analysis is the concept that structure matters . For example , teams with the same composition of member skills can perform very differently depending on the patterns of relationships among the members [ Borgatti et al . 2009 ] . Therefore , it is important to take into account both point wise and pairwise data during anomaly detection . ( ii ) Group anomaly is usually more subtle than individual anomaly . At the individual level , the activities might appear to be normal [ Chandola et al . 2009 ] . Therefore , existing anomaly detection algorithms usually fail when the anomaly is related to a group rather than individuals . ( iii ) Empirical studies in social media analysis suggest the dynamic nature of individual network positions [ Kossinets 2006 ] . People ’s activities and communications change constantly over time and we can hardly know the groups beforehand . Thus , developing a method that can be easily generalized to dynamic setting is critical to anomaly detection in evolving social media data .
In this article , we take a graphical model approach to address those challenges . We propose a hierarchical model , that is GLAD model , to connect two forms of data . To handle the dynamic characteristics of the social media data , we further develop a dynamic extension of GLAD : the d GLAD model . We show that GLAD outperforms existing approaches in terms of group anomaly detection accuracy and robustness . When dealing with dynamic social networks , the dynamic extension of GLAD achieves lower false positive rate and better data fitting . The major contributions of this article can be summarized as follows :
( 1 ) We formulate the problem of group anomaly detection in the context of social media analysis for both static and dynamic settings and articulate the three major challenges associated with the task .
( 2 ) We develop a graphical model called GLAD . GLAD can successfully discover the group structure of social media and detect group anomalies . We also generalize GLAD to its dynamic extension and provide tractable model inference algorithms . ( 3 ) We conduct thorough experiments on both synthetic and real world datasets using anomaly injections . We also construct a meaningful dataset from ACM publication dataset for rigorous evaluation . The dataset is accessible at http://www bcfusc edu/∼liu32/datahtml
This article is an extended version of one of our earlier article [ Yu et al . 2014 ] . We present almost all the contents from Yu et al . [ 2014 ] and an alternative design of the
ACM Transactions on Knowledge Discovery from Data , Vol . 10 , No . 2 , Article 18 , Publication date : October 2015 .
GLAD : Group Anomaly Detection in Social Media Analysis
18:3
GLAD model , which we call GLAD0 . GLAD and GLAD0 share the same design philosophy while GLAD enjoys significant computational efficiency . The extended version includes the GLAD0 model description and its inference in Section 4 , as well as a synthetic data experiment in Section 7 .
2 . RELATED WORK We review the related models on group anomaly detection and illustrate the motivation behind our approach .
The Multinomial Genre Model ( MGM ) proposed in Xiong et al . [ 2011b ] first investigates the problem following the paradigm of Latent Dirichlet Allocation ( LDA ) [ Blei et al . 2003 ] . As a text processing tool , LDA assumes that each word is associated with a topic and a document is a mixture of topics . Similarly , MGM models a group as a mixture of Gaussian distributed topics with certain mixture rate and assumes there exists “ best ” mixture rates , corresponding to the mixture rates of normal groups . Then it conducts group anomaly detection by scoring the mixture rate likelihood of each group . One drawback of MGM is that the set of candidate mixture rates is shared globally by groups . It might lead to poor performance when groups have different sets of mixture rates . Xiong et al . [ 2011a ] further extends MGM to Flexible Genre Model ( FGM ) with more flexibility in the generation of topics . Specifically , the model considers the set of topic mixture rates as random variables rather than model hyper parameters , which would adapt to diverse “ genres ” in groups , each of which is a typical distribution of topic mixture rates .
Another line of work takes a discriminative approach . Muandet and Sch¨olkopf [ 2013 ] uses the same definition of group anomaly from Xiong et al . [ 2011b ] . It considers kernel embedding of the probabilistic distributions and generalizes one class support vector machine from point anomaly detection to group anomaly detection . The proposed support measure machine ( SMM ) algorithm maps the distributions to a probability measure space with kernel methods , which can handle the aggregate behavior of data points .
However , existing approaches separate the group anomaly detection task into two stages : group discovery and anomaly detection . They require the group information to be given before applying the anomaly detection algorithms . For example , in Xiong et al . [ 2011b ] , the Sloan Digital Sky Survey ( SDSS ) dataset needs to be preprocessed before feeding into MGM . The authors first construct a neighborhood graph and then treat the connected components in the graph as groups . For the application on turbulence data , the FGM model [ Xiong et al . 2011a ] considers the vertices in a local cubic region as a group . In SMM [ Muandet and Sch¨olkopf 2013 ] , the authors treat the high energy particles generated from the same collision event as a group .
The two stage approaches identify the groups from the pairwise data and infer the anomalies based on the point wise data . This strategy assumes that the point wise and pairwise data are marginally independent . However , such independence assumption might underestimate the mutual influence between the group structure and the feature attributes . The detected group anomalies can hardly reveal the joint effect of these two forms of data . These motivate us to build an alla prima that can account for both forms of data and accomplish the tasks of group discovery and anomaly detection all at once . Additionally , existing work can only deal with static network and fixed size groups . This is not feasible for the time evolving nature of social media data . For example , in corporate networks , employees may switch teams from one to the other . The organization structure of a team may also change . As the dynamic setting needs to take into account the flexible group size and the changing mixture rates , we further adapt our model to the dynamic setting and formulate the problem as a change point detection task .
ACM Transactions on Knowledge Discovery from Data , Vol . 10 , No . 2 , Article 18 , Publication date : October 2015 .
18:4
R . Yu et al .
Group anomaly detection in social media analysis may shed light on a wide range of real world problems such as corporate restructuring , team job hopping and political inclination shift to which our approach can apply . In Section 3 , we provide a formal definition of group anomaly in social media analysis . We first develop GLAD0 as well as its learning and inference algorithm in Section 4 . Then we present a computationally more efficient model design : GLAD in Section 5 . In Section 6 , we describe the dynamic GLAD model : d GLAD , which can handle the dynamic social networks . Section 7 shows the empirical evaluation results of GLAD and d GLAD on synthetic and real world datasets compared with existing baseline models .
3 . DEFINITION OF GROUP ANOMALY The core of our group anomaly definition lies in the collective behavior of individuals . For example , a document is a mixture of various topics and a team is a mixture of different roles . Therefore , we model the node features of each group as a mixture of components . Each component could be an article topic , a social role or a job title . Specifically , we can describe a component as either a discrete variable such as multinomial distribution or a continues variable like Gaussian distribution , depending on the data type of features . Here we use the term role as a general notion for the component . We assume that there are a fixed number of roles and each of which denotes a particular distribution of node features . All groups share the same set of roles but possibly with different role mixture rates . Normal groups follow the same pattern with respect to their role mixture rates , but the anomalous group has a role mixture rate that deviates from the normal pattern .
For the static GLAD model , we are interested in the distribution of the role mixture rates across the groups . According to our assumption , the mixture rates of normal groups are more likely to appear . For groups with very rare role mixture rates , we treat them as group anomalies . One example of this type of group anomaly comes from particle physics . It is widely accepted that the dynamics of known particles are governed by the Standard Model , which corresponds to the normal pattern . Unknown particles would contaminate the distribution of the Standard Model . Detecting those anomalies could potentially lead to the discovery of new physical phenomenon . In social media such as LinkedIn , users in a group can be clustered into different roles . The role distribution of spam campaign groups would be different from that of normal professional groups . In practice , we first identify the normal mixture rates . Then for each learned group , we evaluate the likelihood of its observations being generated with the normal mixture rates . The lower the likelihood value is , the more anomalous the group would be .
For the dynamic d GLAD model , we emphasize on the temporal aspect of the data and detect the change of the role mixture rate within the groups . For instance , in scientific area , it is valuable to study the evolution of research topics and detect the bursty time periods . In the dynamic setting , since the structure of groups change as well as their role mixture rates , detecting groups with rare mixture rate no long applies . Therefore , we think of the task as a change point detection problem and aim to detect the groups whose mixture rates change drastically from the previous time stamps . Compared with GLAD , we not only need to decide whether a group is anomaly or not , but also need to specify when the group appears anomalous .
Even though we use slightly different definitions of group anomaly for the GLAD model and the d GLAD model , the key ideas behind our definitions are the same . Both definitions build upon the notion of role mixture rate , which essentially requires a precise inference of both the group membership and role identity for each individual in the group .
ACM Transactions on Knowledge Discovery from Data , Vol . 10 , No . 2 , Article 18 , Publication date : October 2015 .
GLAD : Group Anomaly Detection in Social Media Analysis
18:5
4 . GLAD0 Suppose that we are given a social network with N people . Each person p has total of Ap activities . The point wise activities data is X = {X1 , X2 , . . . , XN} . The pairwise communication data is Y = {Y1,1 , Y1,2 , . . . , YN,N} . Xp ∈ RV×Ap . For a particular activity a , Xpa consists of V entries , denoting a feature vector of V dimensions . Yp,q ∈ {0 , 1} is a binary valued variable , indicating the pairwise relationship of nodes . These two forms of data are our inputs . Our goal is to analyze these data jointly and declare the group that has irregular role mixture rate as anomaly . In the following sections , we first describe the motivation for our hierarchical Bayes model and provide its generative process and the plate notation . Then we derive the inference algorithm using the variational Bayesian approach .
41 Model Specification We model a social network with N individuals . From the point wise data aspect , assume that each activity of the person p is associated with a group identity Gpa and a role identity Rpa . Group identity finds the natural cluster of a person influenced by the pairwise observations . Role identify captures the cluster of activities within the group . The two identities assumption is motivated by the controversial viewpoints of what is the right metric for a community . In community detection literature [ Fortunato 2009 ] , some argue that a community is the one that has dense communications within clusters while others suggest that people in the same community should share common activity features . We get around the controversy by recognizing the arguments of both sides . Mathematically , since we model activities as a mixture model , “ role ” is the mixture component that categorizes the feature values of each activity . From the pairwise data perspective , assume that each communication from person p to q has a group membership zp→q . The group membership of person p , zp→ , depends on the recipient of the communication while his group identity Gp is undirected . For simplification , we fix the number of groups as M and the number of roles as K .
For each person p , he joins a group according to the membership probability distribution πp . We impose a Dirichlet prior on the membership distribution . It is well known that the Dirichlet distribution is conjugate to the multinomial distribution . As we will show later , when dealing with latent variables , the Dirichlet prior facilitates the learning and inference of the model . We assume the pairwise link Yp,q between person p and person q depends on the group identities of both p and q with the parameter B . Furthermore , we model the dependency between the group and the role using a multinomial distribution parameterized by a set of role mixture rate {θ1:M} . The role mixture rate characterizes the constitution of the group : the proportion of the population that plays the same role in the group . Finally , we model the activity feature vector of the individual Xpa as the dependent variable of his role with parameter set {β1:K} . Without loss of generality , we assume that the activity data has discrete value and follows the multinomial distribution of single trial , that is the categorical distribution . But we can easily adapt Xpa to other form of activities .
Figure 1 shows the plate representation of the proposed model and summarizes the notations therein . Our model unifies the ideas from both the Mixture Membership Stochastic Block ( MMSB ) model [ Airoldi et al . 2008 ] and the LDA model [ Blei et al . 2003 ] . The blue dashed rectangular on the left side resembles MMSB which models the formation of groups using link information . The red dashed polygon integrates the generating process of LDA which is often used for topic extraction from documents . We denote the current model design as GLAD0 and specify the generative process of GLAD0 in Algorithm 1 . Next , we describe the variational Bayes inference for the GLAD0 model .
ACM Transactions on Knowledge Discovery from Data , Vol . 10 , No . 2 , Article 18 , Publication date : October 2015 .
18:6
R . Yu et al .
Fig 1 . Plate representation for the Group Latent Anomaly Detection ( GLAD0 ) model and the notation descriptions . Shaded circles are observations , blank circles are latent variables and the variables without a circle are model parameters . The blue rectangular resembles MMSB . The red polygon integrates the generating process of LDA .
ALGORITHM 1 : Generative process of the GLAD0 model for individual p = 1 → N do
Draw group membership distribution πp ∼ Dir(α ) for individual q = 1 → N do
Draw group membership zp→q ∼ Multinomial(πp ) Draw group membership zp←q ∼ Multinomial(πq ) Sample communication Yp,q ∼ Bernoulli ( zT p→q Bzp←q ) end for for activity a = 1 → Ap do
Draw group identity Gpa ∼ Multinomial(πp ) Draw role identity Rpa ∼ Multinomial(Rpa|θ1:M , Gpa ) Sample activity Xpa ∼ Multinomial(Xpa|β1:K , Rpa ) end for end for
ACM Transactions on Knowledge Discovery from Data , Vol . 10 , No . 2 , Article 18 , Publication date : October 2015 .
GLAD : Group Anomaly Detection in Social Media Analysis
18:7
42 Model Inference We develop an approximate inference technique based on variational Bayesian methods [ Jordan et al . 1999 ] and an EM algorithm for model inference . Specially , we approximate analytically to the posterior probability of the hidden variables by minimizing the Kullback–Leibler divergence ( KL divergence ) of the variational distribution and the actual posterior . Then we perform the EM procedure to learn the model parameters . Denote the set of model parameters as = {α , B , θ1:M , β1:K} , the set of visible variables as v = {X1:N , Y1:N,1:N} , and the set of the hidden variables as h = {π1;N,Z1:N,1:N,G1:N,R1:N} . Our aim is to estimate the posterior distribution p(h , |v ) . We can first write out the complete joint likelihood of observed and latent variables as follows :
. p(Xpa|Rpa , β)p(Rpa|Gpa , θ)p(Gpa|π ) . p(Ypq|zp→q , zp←q)p(zp→q|πp)p(zp←q|πp )
× p,a
. p(πp|α ) . p(v , h| ) = pq
The marginal likelihood of the data p(v| ) =fi h p(v , h| )dh requires to integrate over all the latent variables in the equation above , which is intractable [ Airoldi et al . 2008 ] . Therefore , we choose a variational distribution q(h ) to approximate the actual posterior distribution , so that the KL divergence between the actual posterior p(h| , v ) and its approximation q(h ) is minimized . Rewriting the marginal log likelihood and plugging in the variational distribution , we have p log p(v| ) = DKL(p||q ) + Eq[log p(v , h| ) ] − Eq[log q(h) ] , where Eq[ f ] represents the expectation of the function f with respect to the distribution q . Since the marginal likelihood log p(v| ) is invariant to the choice of q , minimizing the KL divergence DKL(p||q ) is equivalent to maximizing the last two terms Eq[log p(v , h| )]−Eq[log q(h) ] . In practice , we choose q(h ) to be factorized over the latent ff variables with free parameters = {γ1:N , φ1:N,1:N , μ1:N , λ1:N} as follows : q(h| ) = q(zp→q|φp→q)q(zp←q|φp←q ) q(Gpa|λpa)q(Rpa|μpa ) ff' . ff' . q(πp|γp )
' .
.
. p p,q p a
Finding the optimal set of the variational parameters is equivalent to solving the following optimization problem subject to probability constraints : Eq[log p(v , h| ) ] − Eq[log q(h| ) ] L(v , h , , ) . ff = argmax = argmax
We follow an EM procedure to solve the problem above . We iteratively update the free parameters by taking the derivative of the Lagrange function of the objective L over one parameter at a time given the value of others from the last iteration . The details of the derivation is provided in Appendix A . Since {Yp,q} is symmetric , the objective function will result in a quadratic term with respect to λp . Taking the derivative over the variational parameter would not have a closed form solution . A simple workaround is by assuming constant probability for the generation of {Yp,p} . We omit the tedious derivations and only present the final update formulas of each of the free parameters , as shown in Algorithm 2 . For convenience , we denote f ( Yp,q , Bm,n ) = Yp,q log Bm,n + ( 1 − Yp,q ) log(1 − Bm,n ) . For the parameter estimation , we apply the empirical Bayes method on the variational likelihood . We maximize the Lagrange function of L(v , h , , ) over model
ACM Transactions on Knowledge Discovery from Data , Vol . 10 , No . 2 , Article 18 , Publication date : October 2015 .
18:8
R . Yu et al .
ALGORITHM 2 : Variational Inference for the alternative GLAD randomly initialize B , θ , β normalize θ , β repeat initialize φp→q,g := 1/M initialize φp←q,h := 1/M initialize γp,g := 1/M initialize μpa,r := 1/K initialize λpa,g := 1/M repeat for p = 1 → N do q=1 a=1
λpa,g
( +Ap update γp,g = αg +N φp→q,g + φp←q,g update φp→q,g ∝ eEq(πp)[log πp,g ] ·)M for q = 1 → N , g = 1 → M , h = 1 → M do gh ( 1 − Bgh)1−Ypq update φp←q,h ∝ eEq(πp)[log πp,h ] ·)M BYpq gh ( 1 − Bgh)1−Ypq BYpq update λpa,g ∝ eψ(γp,g ) ·)K end for for a = 1 → Ap , g = 1 → M , r = 1 → K do ·)D update μpa,r ∝)M
θ λpa,g gr
θ μga,r gr d=1
β xpa,d rd g=1 h=1 g=1 r=1 ffφp←q,h ffφp→q,g end for end for until convergence update βrd ∝ ( 1−ρ)· update Bgh = p,q Ypq φp→q,gφp←q,h update θgr ∝ p,q φp→q,gφp←q,h a xpa,d · μpa,r λpa,gμpa,r p p a until convergence parameters = {α , B , θ1:M , β1:K} . Due to the fact that the derivative of the objective function with respect to α depends on α , there is no closed form solution for the − maximizer wrt α . We apply the Newton–Raphson method to reach a numerical solution . Similar to the GLAD model , we score the group anomalousness using p∈G Ep[log p(Rp| ) ] . The most anomalous group will have the highest anomaly − score . We approximate the true log likelihood with the variational log likelihood to get p∈G Eq[log p(Rp| ) ] . GLAD0 jointly models the point wise and pairwise data . It allows mixture of groups and roles by associating each activity with a group identity and a role identity , which implies that each person can have multiple roles and can belong to multiple groups . The GLAD0 model loosely connects the two components of MMSB and LDA via a shared group distribution πp . It distinguishes between the communication group membership z and the activity membership G . However , the number of latent variables in GLAD0 scales linearly with number of activities for each person , thus GLAD0 suffers from high computational cost . The complexity of the model and the difficulty of inference increase significantly when we further consider generalizing to the dynamic setting . Additionally , the loose connection with the shared group membership πp may be restrictive in capturing the interdependencies of point wise and pairwise data . Therefore , we consider a more computationally efficient model design that addresses the above issues .
ACM Transactions on Knowledge Discovery from Data , Vol . 10 , No . 2 , Article 18 , Publication date : October 2015 .
GLAD : Group Anomaly Detection in Social Media Analysis
18:9
Fig 2 . Plate notation for the GLAD model . Shaded circles are observations , blank circles are latent variables and the variables without a circle are model parameters .
5 . GLAD GLAD models a social network of activities X = {X1 , X2 , . . . , XN} and communications Y = {Y1,1 , Y1,2 , . . . , YN,N} , where Xp is the aggregation of the activities for each person . Xp ∈ RV consists of V entries , denoting a feature vector of V dimensions . Each person p joins a group according to the membership probability distribution πp . He is associated with a group identity Gp and a role identity Rp . We draw the pairwise observations of person p {Yp,:} directly from the group identity Gp as Bernoulli random variables . And we further assume that the activities Xp follows a multinomial distribution with Ap trials . GLAD incorporates MMSB and LDA in a more compact way . It not only allows the shared group membership distribution between the two components , but also the group membership identity to emphasize the interdependencies between point wise and pairwise data . Figure 2 depicts the plate representation of the GLAD model and Algorithm 3 describes its corresponding generative process .
ALGORITHM 3 : Generative process of the GLAD model for individual p = 1 → N do
Draw membership distribution πp ∼ Dir(α ) Draw Gp ∼ Multinomial(πp ) for individual q = 1 → N do
Sample Yp,q ∼ Bernoulli ( GT p BGq ) end for Draw Rp ∼ Multinomial(Rp|θ1:M , Gp ) Draw Xp ∼ Multinomial(Xp|β1:K , Rp ) end for
51 Inference and Learning Inference requires us to compute the posterior distributions of the latent variables given the data . The normalizing term of the posterior distribution involves the calculation of the marginal likelihood of the data for which we resort to variational EM algorithms [ Jordan et al . 1999 ] . Denote the set of model parameters as = {α , B , θ1:M , β1:K} , the set of observed variables as v = {X1:N , Y1:N} , and the set of the hidden variables as h= {π1;N,G1:N,R1:N} . Our aim is to estimate the posterior distribution p(h , |v ) . We can first write out the complete joint likelihood of observed and latent variables as follows : p(v , h| ) = p(Yp,q|Gp , Gq , B )
. p,q p(πp|α ) × .
. p
×
ACM Transactions on Knowledge Discovery from Data , Vol . 10 , No . 2 , Article 18 , Publication date : October 2015 . p p(Xp|Rp , β1:K)p(Rp|Gp , θ1:M)p(Gp|πp ) . fi
18:10
R . Yu et al . likelihood of the data p(v| ) = Computing the maximizer for the marginal h p(v , h| )dh requires the integration over all the latent variables in the equation above , which is intractable [ Airoldi et al . 2008 ] . Therefore , we apply the variational Bayesian approach [ Jordan et al . 1999 ] to perform the inference approximately . The essence of the variational Bayesian approach is to choose a variational distribution q(h ) to approximate the actual posterior distribution , so that the KL divergence between p(h , |v ) and its approximation q(h ) is minimized . Rewriting the marginal log likelihood and plugging in the variational distribution , we have log p(v| ) = DKL(p||q ) + Eq[log p(v , h| ) ] − Eq log q(h) ] , similarly , we use Ep[ f ] to represent the expectation of the function f with respect to the distribution p . Since the marginal likelihood log p(v| ) is invariant to the choice of q , minimizing the KL divergence DKL(p||q ) is equivalent to maximizing the last two terms Eq[log p(v , h| ) ] − Eq[log q(h) ] . In practice , we choose q(h ) to be factorized over the latent variables with free parameters = {γ1:N , μ1:N , λ1:N} as follows : q(h| ) = q(πp|γp)q(Rp|μp)q(Gp|λp ) .
. p
Our goal is to find the optimal set of free parameters that provides a variational distribution closest to the actual posterior . Then our problem is to maximize the objective function formulated as follows subject to probability constraints : Eq[log p(v , h| ) ] − Eq[log q(h| ) ] L(v , h , , ) .
The objective function L , by plugging in the joint likelihood and the variational distribution and taking expectations , is given by
L(v , h , , ) = ff = argmax = argmax fi fi Eq[log p(Rp|Gp , θ1:M ) ] Eq[log p(Xp|Rp , β1:K ) ] + fi fi Eq[log p(Yp,q|Gp , Gq , B ) ] Eq[log p(Gp|πp ) ] + fi fi Eq[log q(πp|γp ) ] Eq[log p(πp|α ) ] − fi fi Eq[log q(Rp|μp ) ] − Eq[log q(Gp|λp) ] .
+
+
− p,q p p p p p p p
We follow a variational EM procedure in order to maximize L(v , h , , ) over . Basically we iteratively update the free parameters by taking the derivative of the Lagrange function of the objective L over one parameter at a time given the value of others from the last iteration . Since {Yp,q} is symmetric , the objective function will result in a quadratic term with respect to λp . Taking the derivative over the variational parameter would not have a closed form solution . A simple workaround is by assuming constant probability for the generation of {Yp,p} . We omit the tedious derivations and only present the final update formulas of each of the free parameters , as shown in Algorithm 4 . For convenience , we denote f ( Yp,q , Bm,n ) = Yp,q log Bm,n + ( 1 − Yp,q ) log(1 − Bm,n ) . For the parameter estimation , we apply the empirical Bayes method on the variational likelihood . We maximize the Lagrange function of L(v , h , , ) over model parameters = {α , B , θ1:M , β1:K} . We apply the Newton–Raphson method to reach a
ACM Transactions on Knowledge Discovery from Data , Vol . 10 , No . 2 , Article 18 , Publication date : October 2015 .
GLAD : Group Anomaly Detection in Social Media Analysis
18:11
ALGORITHM 4 : Variational Inference for GLAD initialize γp,m := 1/M initialize μp,k := 1/K initialize λp,m := 1/M repeat λp,m = exp{ γp,m = αm + λp,m μp,k = exp{ end for until convergence for p = 1 → N , m= 1 → M k = 1 → K do v log βv,kXp,v + k log θm,kμp,k + ψ(γp,m ) − ψ( m log θm,kλp,m} n
γp,n ) + q=p n
λq,n · f ( Yp,q , Bm,n)}
βv,k = numerical solution for the maximizer wrt α . The resulting parameter updating functions for α and Bare the same as those of MMSB [ Airoldi et al . 2008 ] and the parameters β and θ can be estimated as follows : p∈G Ep[log p(Rp| ) ] according to our definition of group anomaly in Section 3 . The most anomalous group will have the highest anomaly score . We approximate the true log likelihood with the variational log
θm,k = We score the group anomalousness using − likelihood to get − p Xp,vμp,k v,p Xp,vμp,k p∈G Eq[log p(Rp| ) ] .
A limitation of GLAD is that it only models the static network . This might be restrictive if we want to further consider dynamic networks . Besides the anomaly group whose mixture rate deviates significantly from other groups , we are also interested to study how the mixture rate evolves over time . Fortunately , GLAD can be easily extended to account for this dynamics . This leads to the dynamic extension of the GLAD model , which will be discussed in the next section . p
μp,kλp,m μp,kλp,m
. k,p
6 . DYNAMIC GLAD We now generalize the GLAD model to take into account the dynamics in the social media . We refer the dynamic extension of GLAD as the d GLAD model . To be consistent with our description for GLAD in Section 5 , we start with the model specification and then provide the model inference algorithm using both the variational Bayesian method and the Monte Carlo sampling technique .
61 Model Specification Generalization of GLAD to d GLAD stems from the template models [ Koller and Friedman 2009 ] , which use the model for a particular time stamp as a template , duplicate it over time and connect temporal components sequentially . Similarly , we can adapt GLAD to the dynamic setting by making a copy of GLAD for each time point . To simplify the model , we assume that the latent factors including role Rp , group Gp and mixture rate {θ1:M} change over time but the membership distribution {πp} and model parameters are fixed . We model the temporal evolution of the role mixture rate for each group with a series of multivariate Gaussian distributions . At a particular time point , the Gaussian has its mean as the value of the mixture rate . And the mixture rate of the next time point is a normalized sample from this Gaussian distribution . Since we require the mixture rate to be the parameters of a multivariate distribution over features , we apply a soft max function to normalize the sample drawn from the multivariate Gaussian . The softmax function is defined as S(θm ) = exp θm . When the total time length T equals one , m exp θm
ACM Transactions on Knowledge Discovery from Data , Vol . 10 , No . 2 , Article 18 , Publication date : October 2015 .
18:12
R . Yu et al .
Fig 3 . Plate notation for the d GLAD model and the meaning of notations . The subscript p denotes each person in the social network . The superscript t denotes the network snapshot at time stamp t . d GLAD reduces to the GLAD model . Figure 3 depicts the probabilistic graphical model of d GLAD and the meanings of notations used . We summarize the generative process of d GLAD in Algorithm 5 .
ALGORITHM 5 : Generative Process of DGLAD for t = 1 → T do for m= 1 → M do
Draw θ ( t ) m
∼ Gaussian(θ ( t−1 ) end for for individual p = 1 → N do m
, σ )
Draw membership distribution π ( t ) p Draw G(t ) for individual q = 1 → p − 1 and q = p + 1 → N do p
∼ Multinomial(πp)(t )
∼ Dir(α )
Sample Y ( t ) ( p,q ) p )T BG(t ) q )
∼ Binomial ( (G(t ) |S(θ ( t ) ∼ Multinomial(R(t ) G(t ) |β ∼ Multinomial(X(t ) p ) p
) ) p
R(t ) p end for Draw R(t ) p Draw X(t ) p end for end for
In d GLAD model , since the mixture rate of next time stamp is drawn from a multivariate Gaussian centering around the mixture rate of its previous time stamp , it imposes smoothness on the mixture rates across time , preventing the mixture rate from having drastic changes . The soft max function maps the samples from the multivariate Gaussian to the parameters for the multinomial distribution . Similar idea can be seen from the generalization of LDA to the dynamic topic model [ Blei and Lafferty 2006 ] . While it is true that d GLAD model shares the constraints of GLAD on fixed group/role number and constant self loop , it has certain intriguing advantages over static models . ( i ) d GLAD captures the dynamics of the latent variables Gp and Rp , thus allows an individual to switch groups and roles over time . ( ii ) The smoothness of the mixture rate over time models the behavior of normal groups , so detecting groups whose mixture rates θ t m undergo substantial change becomes easier .
62 Inference and Learning The variational inference of d GLAD is similar to the GLAD model except for the m | ˆθ 1:T ) to approximate longitudinal factor θ ( 1:T ) the original posterior where { ˆθ 1:T} are variational parameters . Then we apply the
1:M . We add a variational distribution p(θ 1:T
ACM Transactions on Knowledge Discovery from Data , Vol . 10 , No . 2 , Article 18 , Publication date : October 2015 .
GLAD : Group Anomaly Detection in Social Media Analysis
18:13
Table I . Two Stage Models in Existing Work
Algorithm Heard 2010 [ Heard et al . 2010 ] Xiong 2011 a [ Xiong et al . 2011b ] Xiong 2011 b [ Xiong et al . 2011a ] Muandet 2013 [ Muandet and Sch¨olkopf 2013 ]
Stage 2
Poisson process
Stage 1 spectrum clustering Mixture genre model clustering Flexible genre model simulator
One class SMM variational Kalman filter technique [ Blei and Lafferty 2006 ] to infer the sequential latent variables and learn the model parameters . The transition for the mixture rate of each group is Gaussian distributed :
θ ( t)|θ ( t−1 ) ∼ N ( θ ( t−1 ) , σ 2 I ) .
We can write the variational distribution for the transition as follows :
ˆθ ( t)|θ ( t−1 ) ∼ N ( θ ( t−1 ) , ˆv2 I ) .
Then we can apply similar variational EM procedure incorporating the transitions to infer the variational parameters . Due to the numerical difficulty of variational Kalman filter method , we also implement a version of the Monte Carlo sampling for d GLAD model , which is used in our empirical evaluations . The algorithm is elaborated in Algorithm 7 . The inference of the transitional part {θ 1:T} is based on the Particle Filtering method [ Doucet and Johansen 2009 ] . Details of the Markov Chain Monte Carlo ( MCMC ) of DGLAD is deferred to Appendix B . The anomaly score of the d GLAD model is measured by ( θ ( t ) m − θ ( t−1 ) m ( .
ALGORITHM 6 : Monte Carlo Sampling of DGLAD
= 1/M , π1:N ∼ Dir(α )
Initialize α , θ0 , β1:K , B R1:T 1:N repeat
= 1/K , G1:T for p = 1 → N do
1:N for t = 1 → T do
Update R(t ) p Update G(t ) p
∼ Mul(S(θ ( t−1 ) ))Mul(X(t ) p ) G(t−1 ) ∼ Mul(π ( t−1 ) )Mul(S(θ ( t−1 ) G(t−1 ) p p
) ) p end for Update πp ∼ Dir(α ) end for for t = 1 → T do
Update θ ( t ) using Particle Filtering end for until Convergence
7 . EXPERIMENTS To evaluate the effectiveness of our model , we conduct thorough experiments on synthetic datasets and real world datasets . We study the applications of our approach by analyzing scientific publications and senator voting records .
71 Baselines To our knowledge , all existing algorithms are two stages approaches : ( i ) identify groups , ( ii ) detect group anomalies . We summarize these algorithms in Table I . We use following approaches as baseline methods in comparison to GLAD and d GLAD :
ACM Transactions on Knowledge Discovery from Data , Vol . 10 , No . 2 , Article 18 , Publication date : October 2015 .
18:14
R . Yu et al .
Fig 4 . The 50 × 50 adjacent matrix rearranged by the group membership discovered by three grouping approaches on a subset of synthetic data of five groups . Dark pixels denote links and white pixels denote no links . Blue block highlights the learned group membership .
( 1 ) MMSB LDA : First use the MMSB model to learn a group membership distribution for each individual node , then assign the node to the group with the highest probability . Finally , for each group , train an LDA model and infer the role identity . ( 2 ) MMSB MGM : Group is learned using the same method as MMSB LDA . For the role inference , train an multimodal MGM instead of LDA .
( 3 ) Graph LDA : Run an off the shelf graph clustering algorithm Min Cut to get group membership and then train a LDA model for each group .
( 4 ) Graph MGM : Get group membership with the graph clustering algorithm Min
Cut and then train a MGM model for each group .
72 Synthetic Dataset We experiment on two type of synthetic datasets . One is a synthetic dataset with injected group anomalies . The other is a benchmark dataset generated by a simulator with individual anomaly labels .
721 Synthetic Data with Anomaly Injection . To justify our approach and evaluate the anomaly detection performance , we generate a network with 500 nodes using GLAD in Algorithm 1 . We set the mixture rates of anomalous groups as [ 0.9 , 0.1 ] and normal groups as [ 0.1 , 09 ] We vary the number of groups from 5 to 50 and inject 20 % anomalous groups . The rest 80 % groups are normal . Since we know the normal and anomalous mixture rates , we calculate the anomaly score of each group by directly computing the differences between the inferred mixture rate and the ground truth normal mixture rate . During the testing procedure , we rank the groups with respect to their anomaly score and retrieve top 20 % groups . For all methods , we set the number of groups and number of roles the same as the ground truth .
We compare the learned groups of three grouping approaches with the ground truth : GLAD , MMSB and Graph , for the case of five groups . The inferred group memberships are shown as adjacent matrices in Figure 4 . For better visualization , we intentionally put the nodes that belong to the same group together . Ideally , we should observe dense
ACM Transactions on Knowledge Discovery from Data , Vol . 10 , No . 2 , Article 18 , Publication date : October 2015 .
GLAD : Group Anomaly Detection in Social Media Analysis
18:15
Fig 5 . Anomaly detection performance of GLAD and baseline methods on synthetic dataset of 500 samples , with 20 % anomalous groups . ( a ) : Anomaly detection accuracy of GLAD and four baselines with respect to number of groups ( small ) over 10 random runs . ( b ) : Mean anomaly detection accuracy with respect to number of groups ( large ) , averaged over 10 random runs . ( c ) : Anomaly detection accuracy of GLAD0 and four baselines with respect to number of groups ( small ) . ( d ) : False positive rate over different thresholds for d GLAD , MMSB MGM , and GLAD for synthetic data . 10 % group anomalies are injected . links within groups and sparse links between groups . Therefore , the dark pixels in the plot would aggregate along the principal diagonal of the matrix . We use blue color to highlight the groups learned . The group discovery result of GLAD is the closest to the ground truth . The high connectivity in the graph and the lack of point wise information could be the reasons for the poor performance of Graph and MMSB .
Figure 5(a ) and ( b ) shows the anomaly detection performance with different number of groups for GLAD and four other baselines . GLAD achieves the highest detection accuracy . It is also more robust over 10 random runs . Note that the differences for the first stage of baselines are more obvious than the second stage . This is because the Bernoulli distribution limits the number of samples in the pairwise data , making the first stage more difficult to learn .
We justify the simplification of GLAD by evaluating the anomaly detection performance of the GLAD0 model . We adopt similar experiment set up for GLAD0 in order to test whether GLAD0 can successfully detect the injected group anomalies . As shown in Figure 5(c ) , for most of the cases ( expect for group number 8 and 9 ) , GLAD0 achieves the highest detection accuracy , while the other two stage approaches are relatively unstable . Given the complexity of the model and the limited observations we feed in , the gain from GLAD0 is less than that from the GLAD model . The performance deterioration with respect to the number of groups is due to the sparsity of the data . As
ACM Transactions on Knowledge Discovery from Data , Vol . 10 , No . 2 , Article 18 , Publication date : October 2015 .
18:16
R . Yu et al .
( a ) : Precision ( b ) : Recall ( c ) : F1 score on the benchmark dataset of GLAD and four baseline methods Fig 6 . over 20 runs . All members in the anomalous groups are treated as individual anomalies and compared with 39 true anomalies . we increase the group number of a fix size network , each group has fewer number of people , thus learning the role mixture for the group becomes more difficult .
We also report the simulation results on group anomaly detection for d GLAD . The data is generated according to Algorithm 5 with five time stamps . We manipulate the mixture rate of 50 % of the groups at time point 4 as injected anomalies . Then we raise alarms if the group ’s mixture rate deviates from the previous time by a certain threshold . In Figure 5(d ) , we display the false positive rate with different threshold values . For comparison , we train MMSB MGM and GLAD at each time independently as baselines . It can be seen that d GLAD achieves the lowest false positive rate , which demonstrates the gain of d GLAD over static models on the dynamic dataset .
722 BenchmarkDatawithAnomalyLabels . The benchmark data set is generated by a simulator from a federal funded program . It contains email communication records and working activities from 258 company employees . Each employee is featured by 6 types of activities . The labeled dataset contains 39 individual anomalies and 5 of them cannot be detected by any existing algorithms . We set the number of groups as 20 as the optimal setting obtained from cross validation and calculate the anomaly score of each group by MCMC sampling . We treat all members in the most anomalous group as individual anomalies and compare them with the anomaly labels . Though the anomaly labels are point anomalies rather than group anomalies , the anomaly detection result reflects the potential of our approach to tackle other type of difficult anomaly detection problems . The precision , recall and F1 score over 20 runs on the benchmark dataset is shown in Figure 6 .
We can see that the GLAD model achieves comparable precision and recall with low variances . In contrast , the detection performances of the two stage models fluctuates significantly . In terms of the F1 scores in Figure 6(c ) , both GLAD and MMSB MGM beat the other algorithms while GLAD has a lower variance than MMSB MGM . One possible explanation is that the point wise features prevent the size of the group to become either too large or too small , thus leading to more robust performance .
73 Real World Datasets
731 ScientificPublications . Researchers study the topics of papers seeking for concise representations of scientific publications , which contain both pairwise data like coauthorship and point wise data such as bag of words features . Detecting anomalous topic distributions in scientific publications can sharpen our understanding of the structure of research communities and possibly reveal unusual research trends . In order to quantify our method , we resort to anomaly injection and construct a dataset with group anomaly labels . One way to construct group anomalies is the scenario that a
ACM Transactions on Knowledge Discovery from Data , Vol . 10 , No . 2 , Article 18 , Publication date : October 2015 .
GLAD : Group Anomaly Detection in Social Media Analysis
18:17
Table II . Group Anomaly Accuracy of GLAD and Four Baselines on DBLP Publications . With KDD Papers
Treated as Normal Groups and Other Conferences are Treated as Group Anomalies Respectively
Methods DBLP:KDD/CVPR DBLP:KDD/ICML DBLP:KDD/SIGMOD DBLP:KDD/CIKM DBLP:KDD/EDBT
GLAD 0.4167 0.2500 0.2875 0.4500 0.2625
Graph LDA Graph MGM MMSB LDA MMSB MGM
0.3333 0.0833 0.0750 0.4000 0.0500
0.3333 0.0833 0.0500 0.3625 0.0875
0.2500 0.1667 0.1625 0.2625 0.2000
0.2500 0.1667 0.1625 0.2625 0.2000
Table III . Key Statistics of the DBLP and ACM Publication Datasets
# of docs # of conf # of links
# of docs # of year
DBLP
ACM
28,569
20
104,962
31,574
10
# of authors # of words # of area
# of authors # of words
28,702 11,771
4
4,474 8,024 conference paper corpus is contaminated by group of papers from conferences in other domains .
We create a dataset from a preprocessed Digital Bibliography and Library Project ( DBLP ) dataset from Deng et al . [ 2011 ] . The dataset consists of conference papers from 20 conferences of four major area : database ( DB ) , data mining ( DM ) , information retrieval ( IR ) and artificial intelligence ( AI ) . Each paper has a bag of words feature vector with a vocabulary size of 11,771 and associated 28,702 authors information . The detailed statistics of the dataset are shown in the top half of Table III . We set up the group anomaly detection scenario as follows : we randomly sample groups of papers from KDD and treat them as normal groups . Then we sample groups of papers from the other conferences ( e.g , CVPR , ICML , SIGMOD ) and inject them into KDD papers as group anomalies . If the two papers have at least one common author , we add a link between them .
Accordingly , all conferences share four topics . But different conferences might have difference point of emphasis , resulting in different mixture rates of topics . Our goal is to pick out the “ anomalous ” papers from the corpus . We sample 50 groups of papers and inject 20 % group anomalies . We apply different models with 50 groups and 4 roles to the data for inference of the membership and role distributions . Then we rank 50 groups with respect to their anomaly scores . We treat the top 20 % groups as the detected anomalies . Table II shows the anomaly detection accuracy by GLAD and four other baselines . GLAD is superior to all four baselines models for different combination of normal/abnormal settings . We also display the topics learned by the GLAD model . In Table IV , we show the top ten most representative words for the four topics , which well reproduce the topic results reported in Deng et al . [ 2011 ] .
Since the DBLP dataset does not contain time specific information which is not suitable for the d GLAD model , we process another ACM dataset downloaded from ArnetMiner [ Tang et al . 2008 ] . The dataset contains the publications from year 2000 to 2009 by 4,474 authors , mainly from the data mining community . In order to study the topic evolution for academic scholars , we extract the abstracts of all publications and group them by authors and publishing years . For each author , we construct a bag of words feature vector out of all the papers he/she has written in one year . And the communication networks we generate are based on the coauthorship of the papers . Whenever two authors have collaborations in a certain year , we create a link between them for the network snapshot in that year .
ACM Transactions on Knowledge Discovery from Data , Vol . 10 , No . 2 , Article 18 , Publication date : October 2015 .
18:18
R . Yu et al .
Table IV . The most Representative Words Learned by GLAD on DBLP Dataset of Four Topics : Database , Data Mining ,
Information Retrieval and Artificial Intelligence
DB
Databases
Object Access
Database Oriented Security Based
Indexing Systems Privacy
DM
Data Mining Efficient Query
Algorithm Queries
Clustering Databases Algorithms
Large
IR Web
Information
Learning Ssearch Retrieval Clustering
AI
Query System
Management Processing
Web
Efficient
Query Text Model
Performance information Distributed Classification Optimization
Table V . Prediction Negative Log Likelihood for GLAD and d GLAD on ACM Dataset Over 9 years
Year GLAD DGLAD
2001
2002
2003
2004
2005
2006
2007
2008
2009
28421.63 28023.68 30184.66 32039.92 28317.67 30539.66 26105.21 34340.53 25967.75 34411.28 33411.14 29935.87 31958.92 30082.65 29696.12 30042.77 34395.68 31683.49
Due to the lack of labels , it is difficult to directly evaluate our model on anomaly detection task . As an alternative , we design a prediction task to compare the modeling performance of GLAD and d GLAD on ACM publications . Specifically , we separate the papers into training and testing sets and measure the predictive model log likelihood on the testing data . For d GLAD , we train our model using a series of publications from previous years , and test on the year immediately after . For the GLAD model , as it is a static model , time independence assumption applies . We train the model using previous year and test on the next year . The model fitting results are shown in Table V . Out of nine training–testing experiments , d GLAD model achieves higher log likelihood than GLAD model for six times , indicating d GLAD as a better fit for the evolving publication modeling .
732 USSenateVoting . We collect the voting records from the government website of United States 109th Congress1 using the New York Time Congress API2 . The records of 109th Congress contain 100 senators’ voting spanning two sessions from Jan 1st 2005 to Dec 31st 2006 . We divide the 24 months records into 8 time slots , where each slot denotes a 3 month interval . Then we apply the method of Kolar et al . [ 2010 ] to construct a network from original yay/nay votes . For the nodes features , we collect the statistics of votes in six dimensions , namely House Joint Resolution(hjres ) , House of Representatives(hr ) , Presidential Nomination(pn ) , Simple Resolution(s ) , sconres(Senate Concurrent Resolution ) and Senate Joint Resolution(sjres ) . We evaluate GLAD on single aggregated network and d GLAD on the 8 time slots time varying data .
We set the number of groups as two and number of roles as three as the Senate consists of two major parties and maintains three types of committees . Figure 7 shows the groups inferred by GLAD . The blue nodes denote Democratic party members and the red ones are Republican . Compared with known facts , the model correctly reveals the party affiliation except for two outliers : Ben Nelson ( Democratic ) and James Jeffords ( Independent ) . The underlying reason is that the votes of these two senators are often at odds with the leadership of his party , leading to false grouping . We conduct an anecdotal investigation and find that the congressional vote rating
1http://wwwsenategov/ 2http://developernytimescom/docs/read/congress_api
ACM Transactions on Knowledge Discovery from Data , Vol . 10 , No . 2 , Article 18 , Publication date : October 2015 .
GLAD : Group Anomaly Detection in Social Media Analysis
18:19
Fig 7 . Common votes graph with party labels inferred by GLAD for 100 senators on the aggregated network . Compared with ground truth , two outliers are highlighted due to their anomalous voting behavior . from the National Journal placed Ben Nelson to the right of five Senate Republicans in 2006 . For James Jeffords , he served as a Republican until 2001 , when he left the party to become an Independent and began caucusing with the Democrats .
Since there are merely two groups , it is impetuous to say one party is more anomalous than the other . Instead , we use d GLAD to detect time points when the role mixture rates change dramatically . In fact , d GLAD raises an alarm at the 7th time step for Democratic . A well known political event happened during this time is that Democratic senator Joseph Lieberman lost the Democratic Party primary election and became an independent Democratic in September 2006 . Though it may be over optimistic to draw the conclusion that this event causes the sudden change of role mixture rates , it serves as an evidence that the dynamics of the voting behavior is closely related to the party affiliation of members .
8 . CONCLUSION In this article , we perform a follow up study of the GLAD model by analyzing an alternative construction of the unified model . We loosely connect the MMSB model and the LDA model assuming the shared group membership distribution for both pointwise and pairwise data . We also provide the variational Bayesian inference algorithm for model inference . We conduct a simulation experiment to verify the benefit of the joint model in comparison with the two stage approaches .
A . VARIATIONAL EM INFERENCE OF GLAD0 The posterior distribution is p(v , h| ) =
. p(Xpa|Rpa , β)p(Rpa|Gpa , θ)p(Gpa|π ) . p(Ypq|zp→q , zp←q)p(zp→q|πp)p(zp←q|πp ) ff' . . ff' .
× p,a pq
' .
The variational distribution takes the following form : q(h ) = q(zp→q|φp→q)q(zp←q|φp←q ) q(πp|γp )
. p p(πp|α ) . ff
. q(Gpa|ηpa)q(Rpa|ξpa ) p p,q p a
We assume that the number of groups is M , number of roles is K and number of activities is D . Thus , θ , the group role distribution is a M by K matrix ; β , the role activity distribution is a K by D matrix .
ACM Transactions on Knowledge Discovery from Data , Vol . 10 , No . 2 , Article 18 , Publication date : October 2015 .
18:20
R . Yu et al .
A1 Posterior Inference Finding the optimal set of the variational parameters ff is equivalent to solving the optimization problem subject to probability constraints : fi ff = argmax fi
Eq[log p(v , h| ) ] − Eq[log q(h| ) ] fi fi st
φp→q,g = 1 ,
φp←q,h = 1 ,
ηpa,g = 1 ,
ξpa,r = 1 . g h g r
Construct Lagrangian for the problem above and set the derivative to zero for each variable separately , we have Update of φp→q and φp←q . ffφp←q,h ffφp→q,g gh ( 1 − Bgh)1−Ypq BYpq gh ( 1 − Bgh)1−Ypq BYpq g=1 h=1 function .
Update for ηpa .
φp→q,g ∝ eEq(πp)[log πp,g ] · M . φp←q,h ∝ eEq(πp)[log πp,h ] · M . where we have Eq(πp)[log πp,g ] = ψ(γp,g ) − M ηpa,g ∝ eψ(γp,g ) · K . · D . ξpa,r ∝ M .
φp→q,g + φp←q,g
Update for ξpa .
Update for γp .
θ ηpa,g gr d=1 g=1 r=1 j=1
θ ξga,r gr
β xpa,d rd
( + Apfi a=1
ηpa,g .
A2 Parameter Estimation Estimation of B . This part is exactly the same to MMSB , where we have q=1 rp,g = αg + Nfi
( 1 − ρ ) ·
ˆBgh = p,q
ˆρ = p,q(1 − Ypq)( fi fi g,h p,q
θgr ∝ p,q Ypqφp→q,gφp←q,h
φp→q,gφp←q,h
.
φp→q,gφp←q,h ) g,h
φp→q,gφp←q,h
ηpa,gξpa,r p a
ψ(γp , j ) , with ψ(· ) as the digamma
Here ρ is for modeling sparsity . More specifically , ρ captures the portion zeros that should not be explained by the block B ( see Section 2.1 of MMSB paper for more detail ) .
Estimation of θ .
ACM Transactions on Knowledge Discovery from Data , Vol . 10 , No . 2 , Article 18 , Publication date : October 2015 .
GLAD : Group Anomaly Detection in Social Media Analysis
18:21
Estimation of β .
B . MCMC OF D GLAD fi fi p a
βrd ∝ xpa,d · ξpa,r .
ALGORITHM 7 : Monte Carlo Sampling of DGLAD
Initialize α , θ0 , β1:K , B R1:T 1:N repeat
= 1/K , G1:T for p = 1 → N do
1:N
= 1/M , π1:N ∼ Dir(α ) ffiffi for t = 1 → T do flSfl fl ∼ Mul ∼ Mul π ( t−1 ) p end for Update πp ∼ Dir(α )
Update R(t ) p Update G(t ) p ffi θ ( t−1 ) G(t−1 ) p
Mul ffi fl X(t ) p θ ( t−1 ) G(t−1 ) p ffiffi
Mul flSfl end for for t = 1 → T do end for until Convergence
Update θ ( t ) using Particle Filtering
B1 Initialization Initialize α = ( 1/M , 1/M , . . . , 1/M ) . For fast convergence , initialize B = diag(0.5 , 0.5 , . . . , 05 ) Randomly initialize θ0 and β1:K .
B2 Gibbs Sampling The posterior is as follows : p(v , h ) =
. . . p t
× p fl p fl ffi × p |α π ( t ) p |R(t ) X(t )
. ffi p,q fl fl p p , β1:K p p,q|G(t ) Y ( t ) p |G(t ) R(t ) p ffi fl ffi p , G(t ) q , B p , θ ( t ) 1:M p p |π ( t ) G(t ) p ffi × p fl
θ ( 1:T ) 1:M ffi
Sample π ( t ) p , G(t ) p , R(t ) p using Gibbs sampler , which is to sample from posterior by fixing all the other variables as the values from the last iteration .
B3 Particle Filtering {θ ( t ) m } are sampled using particle filtering , a sequential importance sampling technique . For each θ ( 1:T ) At t = 1 : m , drop the subscript m for convenience .
Sample the ith particle θ ( 1 ) Compute the weights wq(θ ( 1 )
,S(NG(1 ) is the empirical role distribution for group G(1 ) p .
∼ Gaussian(θ0 , σ I ) . ) = MulPDF(θ ( 1 ) i i i p
) ) and normalize to 1 , where NG(1 ) p
At t ≥ 2 :
Resample ( w(1 ) i
, θ ( 1 ) i
) to obtain N new equally weighted particles ( 1 N
Sample the ith particle θ ( t ) i
∼ Gaussian( ¯θ ( t−1 ) i
, σ I ) and set θ ( 1:t ) i i
, ¯θ ( 1 )
) . ← ( ¯θ ( 1:t−1 ) i
, θ ( t ) i ) .
ACM Transactions on Knowledge Discovery from Data , Vol . 10 , No . 2 , Article 18 , Publication date : October 2015 .
18:22
R . Yu et al .
Compute the weights wq(θ ( t )
,S(NG(t ) is the empirical role distribution for group G(t ) p . i ) = MulPDF(θ ( 1 ) i
Resample ( w(t ) i
, θ ( 1:t ) i
) ) and normalize to 1 , where NG(t ) p p
) to obtain N new equally weighted particles ( 1 N
, ¯θ ( 1:t ) i
) .
REFERENCES Edoardo M . Airoldi , David M . Blei , Stephen E . Fienberg , and Eric P . Xing . 2008 . Mixed membership stochastic blockmodels . Journal of Machine Learning Research 9 , 1981–2014 .
Leman Akoglu , Mary McGlohon , and Christos Faloutsos . 2009 . Anomaly Detection in Large Graphs . In
CMU CS 09 173 Technical Report . Citeseer .
David M . Blei and John D . Lafferty . 2006 . Dynamic topic models . In Proceedings of the 23rd International
Conference on Machine Learning . Pittsburgh , PA , USA , 113–120 .
David M . Blei , Andrew Y . Ng , and Michael I . Jordan . 2003 . Latent dirichlet allocation . Journal of Machine
Learning Research 3 , 993–1022 .
Stephen P . Borgatti , Ajay Mehra , Daniel J . Brass , and Giuseppe Labianca . 2009 . Network analysis in the social sciences . Science 323 , 5916 ( 2009 ) , 892–895 .
Varun Chandola , Arindam Banerjee , and Vipin Kumar . 2009 . Anomaly detection : A survey . ACM Computing
Surveys ( CSUR ) 41 , 3 , 15 .
Hongbo Deng , Jiawei Han , Bo Zhao , Yintao Yu , and Cindy Xide Lin . 2011 . Probabilistic topic models with biased propagation on heterogeneous information networks . In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining . San Diego , CA , USA , 1271–1279 .
Arnaud Doucet and Adam M . Johansen . 2009 . A tutorial on particle filtering and smoothing : Fifteen years later . Handbook of Nonlinear Filtering . Oxford University Press , Oxford , UK .
S . Fortunato . 2009 . Community detection in graphs . Physics Reports 486 , 3 ( June 2009 ) , 75–174 . Douglas M . Hawkins . 1980 . Identification of Outliers . Vol . 11 . Springer . Nicholas A . Heard , David J . Weston , Kiriaki Platanioti , David J . Hand , and others . 2010 . Bayesian anomaly detection methods for social networks . The Annals of Applied Statistics 4 , 2 , 645–662 .
Michael I . Jordan , Zoubin Ghahramani , Tommi S . Jaakkola , and Lawrence K . Saul . 1999 . An introduction to variational methods for graphical models . Machine Learning 37 , 2 , 183–233 .
Mladen Kolar , Le Song , Amr Ahmed , Eric P . Xing . 2010 . Estimating time varying networks . The Annals of
Applied Statistics 4 , 1 , 94–123 .
Daphne Koller and Nir Friedman . 2009 . Probabilistic Graphical Models : Principles and Techniques . MIT press .
G . Kossinets . 2006 . Empirical analysis of an evolving social network . Science 311 , 5757 ( Jan . 2006 ) , 88–90 .
DOI:http://dxdoiorg/101126/science1116869
Krikamol Muandet and Bernhard Sch¨olkopf . 2013 . One class support measure machines for group anomaly detection . In Proceedings of the 29th Conference on Uncertainty in Artificial Intelligence ( UAI’13 ) . AUAI Press .
Jia Yu Pan , Hyung Jeong Yang , Christos Faloutsos , and Pinar Duygulu . 2004 . Automatic multimedia crossmodal correlation discovery . In Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining . ACM , 653–658 .
Carey E . Priebe , John M . Conroy , David J . Marchette , and Youngser Park . 2005 . Scan statistics on enron graphs . Computational & Mathematical Organization Theory 11 , 3 , 229–247 .
Jie Tang , Jing Zhang , Limin Yao , Juanzi Li , Li Zhang , and Zhong Su . 2008 . ArnetMiner : Extraction and mining of academic Social Networks . In KDD’08 . Las Vegas , NV , USA , 990–998 .
Hanghang Tong , Christos Faloutsos , and Jia Yu Pan . 2008 . Random walk with restart : Fast solutions and applications . Knowledge and Information Systems 14 , 3 , 327–346 .
Ulrike Von Luxburg . 2007 . A tutorial on spectral clustering . Statistics and Computing 17 , 4 , 395–416 . Liang Xiong , Barnab´as P´oczos , and Jeff G . Schneider . 2011a . Group anomaly detection using flexible genre models . In NIPS . Vancouver , Canada , 1071–1079 .
Liang Xiong , Barnab´as P´oczos , Jeff G . Schneider , Andrew Connolly , and Jake Vanderplas . 2011b . Hierarchical probabilistic models for group anomaly detection . AI and Statistics , 789–797 .
Rose Yu , Xinran He , and Yan Liu . 2014 . Glad : Group anomaly detection in social media analysis . In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining . New York , NY , USA , 372–381 .
Received January 2015 ; accepted July 2015
ACM Transactions on Knowledge Discovery from Data , Vol . 10 , No . 2 , Article 18 , Publication date : October 2015 .
