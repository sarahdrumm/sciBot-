Safe and Efficient Screening For Sparse
Support Vector Machine
Zheng Zhao , Jun Liu , James Cox
SAS Institute Inc . 600 Research Drive , Cary , NC 27513
{zheng.zhao , jun.liu , jamescox}@sascom
ABSTRACT Sparse support vector machine ( SVM ) is a robust predictive model that can effectively remove noise and preserve signals . Like Lasso , it can efficiently learn a solution path based on a set of predefined parameters and therefore provides strong support for model selection . Sparse SVM has been successfully applied in a variety of data mining applications including text mining , bioinformatics , and image processing . The emergence of big data analysis poses new challenges for model selection with large scale data that consist of tens of millions samples and features . In this paper , a novel screening technique is proposed to accelerate model selection for 1 regularized 2 SVM and effectively improve its scalability . This technique can precisely identify inactive features in the optimal solution of a 1 regularized 2 SVM model and remove them before training . The technique makes use of the variational inequality and provides a closed form solution for screening inactive features in different situations . Every feature that is removed by the screening technique is guaranteed to be inactive in the optimal solution . Therefore , when 1 regularized 2 SVM uses the features selected by the technique , it achieves exactly the same result as when it uses the full feature set . Because the technique can remove a large number of inactive features , it can greatly increase the efficiency of model selection for 1 regularized 2 SVM . Experimental results on five high dimensional benchmark data sets demonstrate the power of the proposed technique .
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications— data mining ; I52 [ Pattern Recognition ] : Design Methodology—feature evaluation and selection
Keywords Screening , sparse support vector machine , feature selection
1 .
INTRODUCTION
Sparse predictive modeling algorithms provide powerful tools to analyze high dimensional data and generate results that have high degree of interpretability and robustness [ 5 , 11 ] . In general , an 1 regularized sparse predictive modeling algorithm can be formulated as minw loss(w)+λw1 . Here w ∈ Rm contains the model coefficients , loss ( w ) is a loss function , and λ ≥ 0 is the regularization parameter that balances between the loss function and the regularizer . When the hinge loss or its square form is used as the loss function , the resulting sparse model is the 1 regularized support vector machine ( SVM ) [ 4 , 18 , 2 , 6 , 16 ] . An 1 regularized SVM model can simultaneously perform model fitting by margin maximization and remove noisy features by softthresholding . It has been successfully applied in a variety of data mining applications that include text mining , bioinformatics , and image processing . Compared to other variances of sparse SVM model [ 15 , 8 , 1 ] , 1 regularized SVM enjoys two major advantages . First , it defines a convex problem ; therefore , an optimal solution can always be achieved without any relaxation of the original problem . Second , its optimization is simple , and a well implemented 1 regularized SVM solver can readily handle large scale problems with tens of millions samples and features [ 6 ] .
The value of the regularization parameter λ is crucial to the performance of an 1 regularized SVM . To achieve good performance , model selection is often used to help choose an appropriate λ value . For example , given a series of regularization parameters , λ1 > λ2 > . . . > λk , the corresponding solutions , w∗ k , can be obtained and the best solution can be chosen by using a prespecified criterion , such as the accuracy or the area under the curve ( AUC ) that is achieved by the resulting models on holdout samples .
2 , . . . , w∗
1 , w∗
Big data analysis requires a higher standard of efficiency for predictive modeling . When data are huge , the computational cost of model selection can be prohibitive . An intuitive question is to ask whether the solution obtained in the kth step of model selection can be used in the ( k + 1)th step to speed up computation . For Lasso [ 11 ] , the answer leads to the state of the art screening techniques to accelerate model selection [ 17 , 7 , 14 , 12 , 10 ] . The key idea is that , given a solution w∗ 1 for λ = λ1 , one can identify many features that are guaranteed to have zero coefficients in w∗ 2 when λ = λ2 . By removing a large number of these inactive features , the cost for computing w∗
2 can be greatly reduced .
Although screening algorithms have been designed for Lasso , very little research has been done for screening for 1 regularized SVMs except in [ 7 ] , which proposes a safe screening tech nique for 1 regularized 1 SVM . This paper presents a novel screening technique that is designed to speed up model selection for 1 regularized 2 SVM . The technique makes use of the variational inequality [ 9 ] for constructing a tight convex set , which can be used to compute bounds for screening features . Features that are removed by this technique are guaranteed to be inactive in the optimal solution . Therefore , the screening is “ safe . ” Experimental results on five highdimensional benchmark data sets demonstrate that the proposed screening technique can dramatically speed up model selection for 1 regularized 2 SVM by efficiently removing a large number of inactive features .
2 .
1 REGULARIZED 2 SVM ples , X = ( x1 , . . . , xn ) , and m features , X =,f
Assume that X ∈ IRm×n is a data set that contains n sam . Assume also that y = ( y1 , . . . , yn ) contains n class labels , yi ∈ {−1 , +1} , i = 1 , . . . , n . Let w ∈ IRm be the mdimensional weight vector , let ξi ≥ 0 , i = 1 , . . . , n be the n slack variables , and let b ∈ IR and λ ∈ IR+ be the bias and the regularization parameter , respectively . The primal form of the 1 regularized 2 SVM is defined as :
1 , . . . , f m n ≥ 1 − ξi , ξi ≥ 0 . i + λ||w||1 , ξ2
1 2 i=1 w st yi min ξ,w xi + b
( 1 )
Eq ( 1 ) specifies a convex problem that has a non smooth 1 regularizer , which enforces that the solution is sparse . Let w(cid:63)(λ ) be the optimal solution of Eq ( 1 ) for a given λ . All the features that have nonzero values in w(cid:63)(λ ) are called active features , and the other features are called inactive . Let α ∈ IRn be the n dimensional dual variable . By applying the Lagrangian multiplier [ 3 ] , the dual of the problem defined in Eq ( 1 ) can be obtained as :
α − 12 2 , min
α
( 2 ) st ˆf j α ≤ λ , n j = 1 , . . . , m , αiyi = 0 , α 0 . i=1
Here , ˆf = Yf , and Y = diag(y ) is a diagonal matrix . By defining α = λθ , Eq ( 2 ) can be reformulated as : st ˆf j θ ≤ 1 , min
θ
||θ − 1 λ
||2 2 , j = 1 , . . . , m , θiyi = 0 , θ 0 . n i=1
[ −1 , +1 ] , fl sign ( wj ) , flflflflfl n
λmax = i=1
The relation between θ and w can be expressed as :
ˆfj =
θ if wj = 0 if wj = 0
, j = 1 , . . . , m .
( 6 )
λmax is defined as the smallest λ value that leads to w = 0 when it is used in Eq ( 1 ) . Given an input data set ( X , y ) , λmax can be obtained in a closed form as : yi − n+ − n− n flflflflfl∞ xi
,
( 7 ) where n+ and n− denote the number of positive and negative samples , respectively . And when λ ≥ λmax , the optimal solution of the problem defined in Eq ( 1 ) can be written as : w(cid:63 ) = 0 , b(cid:63 ) =
.
( 8 )
Denote m = n yi − n+−n− xi . The first feature to enter the model is the one that corresponds to the element that has the largest magnitude in m . i=1 n
( n+ − n− ) n
3 . EFFICIENT AND SAFE SCREENING FOR
1 REGULARIZED 2 SVM
Eq ( 6 ) shows that the necessary condition for a feature f to be active in an optimal solution is |θˆf| = 1 . On the other hand , for any feature f , if |θˆf| < 1 , it must be inactive in the optimal solution . Given a λ value , this condition can be used to develop a screening rule for removing inactive features to speed up training for the 1 regularized 2 SVM . The key is to compute the upper bound of |θˆf| for features . A feature can be safely removed if its upper bound value is less than 1 . The cost of computing the upper bounds can be much lower than training 1 regularized 2 SVM . Therefore , screening can greatly lower the computational cost by removing many inactive features before training . To bound the value of |θˆf| , it is necessary to construct a closed convex set K that contains θ . The upper bound value can be then computed by maximizing |θˆf| over K . 3.1 Constructing the Convex Set K
In the following , Eq ( 3 ) and the variational inequality [ 9 ] are used to construct a closed convex set K to bound |θˆf| . Proposition 3.1 introduces the variational inequality for a convex optimization problem .
In the primal formulation for the 1 regularized 2 SVM , the primal variables are b , w , and ξ . And in the dual formulation , the dual variables are α or θ . When b and w are known , ξ , α , and θ can be obtained as :
ξi = αi = λθi = max
0 , 1 − yi w xi + b
.
( 4 )
The relation between α and w can be expressed as : fl sign ( wj ) λ ,
[ −λ , +λ ] ,
ˆfj = α if wj = 0 if wj = 0
, j = 1 , . . . , m .
( 5 )
( 3 )
Proposition 31 Let θ(cid:63 ) be an optimal solution of the following convex optimization problem : min g(θ ) , st θ ∈ K , where g is continuously differentiable and K is closed and convex . Then the following variational inequality holds :
∇g ( θ(cid:63 ) )
( θ − θ(cid:63 ) ) ≥ 0 ,
∀θ ∈ K .
The proof of this proposition can be found in [ 9 ] . Given λ2 < λmax , assume that there is a λ1 , such that λmax ≥ λ1 > λ2 and its corresponding solution θ1 is known1 . The reason to introduce λ1 is that when λ1 is close to λ2 and θ1 is known , θ1 can be used to construct a tighter convex set that contains θ2 to bound the value of |θ 1When λ1 = λmax , θ1 is given in Eq ( 4 ) .
ˆf| .
2 the optimal solution of the following problem : flflflflθ −
θ t min st ˆf j θ ≤ 1 ,
1 λ n i=1 flflflfl2
2 j = 1 , . . . , m , θiyi = 0 , θ 0 .
+ ( 1 − t ) θ(cid:63 )
,
( 13 )
θ1 −
θ2 −
By applying Proposition 3.1 to the problem defined in Eq ( 13 ) for θ1 , and θ2 , the following results can be obtained : t1
+ ( 1 − t1 ) θ1
1 λ1
( θ − θ1 ) ≥ 0 ,
( 14 )
+ ( 1 − t2 ) θ2
( θ − θ2 ) ≥ 0 .
( 15 ) t2 ≥ 0 . By substituting θ = θ2 and θ = θ1 into Eq ( 14 ) and Eq ( 15 ) , respectively , and then combining the two inequalities , the following equations can be obtained :
Let t = t1 t2
Bt =
θ2 : ( θ2 − c )
,
( 16 )
1 λ2
( θ2 − c ) ≤ l2 flflflfltθ1 − t
1 2 flflflfl2
.
. a1 − θ1 flflfl2 1
+
1 2 c =
, l =
+ θ1
1 λ1
1 λ2 tθ1 − t
− θ1 As the value of t changes from 0 to ∞ , Eq ( 16 ) generates a series of hyperballs . When t = 0 , c = 1 + θ1 ) and 2 1 − θ12 . This corresponds to the hyperball defined l = 1 by Eq ( 12 ) . The following theorems provide some insights about the properties of the hyperballs generated by Eq ( 16 ) :
2 ( 1 λ2
1 λ1
1 λ2
+
λ2
Theorem 32 Let a =
. The radius of the hy perball generated by Eq ( 16 ) reaches it minimum when
1 λ1 flflfl 1
λ1
−θ1 −θ1 flflfl2 flflfl 1
λ1
− 1 λ1
1
λ2 t = 1 +
1
Let ˆc be the center of the ball and l be the radius . When the minimum is reached , they can be computed as : − 1 λ1
Pa ( 1 ) + θ1 , l =
Pa ( 1 ) .
− 1 λ1
ˆc =
1 2
1 2
λ2
λ2
Here , Pu ( v ) = v − vu the null space of u . Since a2 = 1 , Pa ( 1 ) = 1 −,a1 a . u2 u is an operator that projects v to
2
Proof . The theorem can be proved by minimizing the r defined in Eq ( 16 ) .
Theorem 33 Let the intersection of the hyperplane θ1 − 1 be Pt . The following equation holds :
( θ2 − θ1 ) = 0 and the hyperball defined by Eq ( 16 )
λ1
Pt1 = Pt2 , for ∀t1 , t2 ≥ 0 , t1 = t2 .
Proof . The theorem can be proved by showing that Pt is independent of t .
This theorem shows that the intersection between the hy ) ( θ2 − θ1 ) = 0 is perball Bt and the hyperplane ( θ1 − 1 the same for different t values .
λ1
θ1 − 1 λ1 θ2 − 1 λ2
( θ − θ1 ) ≥ 0 ,
( θ − θ2 ) ≥ 0 .
( 9 )
( 10 )
By substituting θ = θ2 into Eq ( 9 ) , and θ = θ1 into
Eq ( 10 ) , the following equations can be obtained :
θ1 − 1 λ1 θ2 − 1 λ2
( θ2 − θ1 ) ≥ 0 ,
( θ2 − θ1 ) ≤ 0 .
( 11 )
( 12 )
In the preceding equations , θ1 , λ1 , and λ2 are known . Therefore , Eq ( 11 ) defines an n dimensional halfspace and Eq ( 12 ) defines an n dimensional hyperball . Because θ2 needs to satisfy both equations , it must reside in the intersection of the halfspace and the hyperball . Obviously , this region is a closed convex set , and it can be used as K to ˆf| . Fig 1 shows an example of the K in a twobound |θ dimensional space . In the figure , Eq ( 11 ) defines the area below the blue line , Eq ( 12 ) defines the area in the red circle , and K is indicated by the shaded area .
2
Besides the n dimensional hyperball defined in Eq ( 12 ) , it is possible to derive a series of hyperballs by combining Eq ( 11 ) and Eq ( 12 ) . Assume that θ(cid:63 ) is the optimal solution of Eq ( 3 ) and t ≥ 0 . It is easy to verify that θ(cid:63 ) is also
Figure 1 : The K in a two dimensional ( 2D ) space when different t values are used . The red circle corresponds to t = 0 , and the blue circle corresponds to t = 1 +
1
− 1
.
λ2
λ1 flflfl 1
λ1 a1 −θ1 flflfl2
Let θ1 and θ2 be the optimal solutions of the problem defined in Eq ( 3 ) for λ1 and λ2 , respectively . Assume that λ1 > λ2 and that θ1 is known . The following results can be obtained by applying Proposition 3.1 to the convex problem defined in Eq ( 3 ) for θ1 and θ2 , respectively :
θ1 θ2 Theorem 34 Let the intersection of the halfspace
) ( θ2 − θ1 ) ≥ 0 and the hyperball defined by Eq ( 16 )
( θ1− 1 be Qt . The following inequality holds :
λ1
In Eq ( 18 ) , θ2 = c + r . Also , r is unknown , and ˆf , a , b , c , and y are known . Since the following equation holds : max|x| = max{− min(x ) , max(x)}
Qt1 ⊆ Qt2 , for ∀t1 , t2 ≥ 0 , t1 ≤ t2 .
Proof . The theorem can be proved by showing that ∀t1 , t2 ≥ 0 and t1 ≤ t2 , if θ2 ∈ Qt1 , then θ2 ∈ Qt2 also holds . max fififi(c + r )
= max{− min(x),− min(−x)} , fififi can be decomposed to two subproblems :
ˆf m1 = − min θ
ˆf = − min rˆf − cˆf st a ( b + r ) ≤ 0 ,r2 − b2 ≤ 0 , ( c + r )
2 y = 0 ,
ˆf = − min r m2 = max θ 2
This theorem shows that the volume of Qt becomes larger when t becomes larger . And Qt1 ⊆ Qt2 if t1 ≤ t2 .
Fig 1 shows two circles in a 2D space . The red circle corresponds to the one obtained by setting t1 = 0 in Eq ( 16 ) . The blue circle corresponds to the one obtained by setting
1
λ2
− 1 λ1 flflfl 1
λ1 a1 − θ1 flflfl2 t2 = 1 + in Eq ( 16 ) . It can be observed that the intersections of the two circles pass the line ( θ1 − 1 ) ( θ2 − θ1 ) = 0 . This is consistent with Theorem 33 Also since t1 ≤ t2 , Qt1 ⊆ Qt2 , which is consistent with Theorem 34
λ1
Thereom 3.4 suggests that Qt=0 should be used to construct K , because when t = 0 , the volume of Qt is minimized . The equality θy = 0 in Eq ( 3 ) of the dual formulation can also be used to further reduce the volume of K .
.
( θ2 − θ1 ) ≥ 0 , θ
2 y = 0
θ2 : ( θ2 − c )
( θ2 − c ) ≤ l2 ,
θ1 − 1 λ1
1 flflfl2 θ2 : θ2 = c + r , r2 ≤ b2 , flflfl 1
, and b = 1 2 and l = 1 2
1
− θ1
+ θ1
λ2
λ2
λ2
− θ1 flflfl2
. Let θ2 = c+r ,
. K can be written as :
Here c = 1 2 −θ1 −θ1 flflfl 1 a =
1 λ1
λ1
K =
K = a
( b + r ) ≤ 0 , ( c + r ) y = 0
.
( 17 )
λ1
Theorem 3.3 shows that when t varies , the intersection of ) ( θ2 − θ1 ) = the hyperball Bt and the hyperplane ( θ1− 1 0 remains unchanged . This suggests that if the maximum value of |θˆf| is achieved with a θ that is in this area , no matter which Bt is used , the maximum value will be the same . This property can be used to simplify the computation . Section 324 will show that when the maximum value of |θˆf| is achieved with a θ on the intersection of the hy ) ( θ2 − θ1 ) = 0 , perball Bt=0 and the hyperplane ( θ1 − 1 flflfl 1 the computation of the maximum value can be simplified by a1 switching to Bt with t = 1 + −θ1 3.2 Computing the Upper Bound fififi can be computed by solving the problem :
Given the convex set K defined in Eq ( 17 ) , the maximum fififiθ
1 value of
− 1 flflfl2
λ2
λ1
λ1
ˆf
λ1
.
2 st a ( b + r ) ≤ 0 ,r2 − b2 ≤ 0 , ( c + r )
( 18 ) y = 0 . fififi(c + r ) fififi
ˆf max
+ c
ˆf
−ˆf fififi = max ( m1 , m2 ) .
( 19 )
( 20 ) fififiθ
2 fififi
ˆf
( b + r ) ≤ 0 ,r2 − b2 ≤ 0 , ( c + r ) y = 0 , st a and fififiθ fififi = max fififi(c + r )
ˆf max
ˆf 2
Eq ( 19 ) and Eq ( 20 ) suggest that the key to bound is to solve the following problem : st a
( b + r ) ≤ 0,r2 − b2 ≤ 0 , ( c + r ) min r
ˆf
( 21 ) y = 0 .
Its Lagrangian L ( r , α , β , ρ ) can be written as :
β,r2
2 − b2
2
+ ρ ( c + r ) y . ( 22 )
ˆf + αa r
( b + r ) +
1 2
( dual feasibility ) ( primal feasibility )
And the Karush Kuhn Tucker ( KKT ) conditions are : α ≥ 0 , β ≥ 0 , 2 − b2 2 ≤ 0 , a ( b + r ) ≤ 0 , ( c + r ) y = 0 , αa ( b + r ) = 0 ,
( complementary slackness ) r2
β,r2
2 − b2
2
= 0 ,
( stationarity ) ∇rL ( r , α , β , ρ ) = 0 .
( 23 ) ( 24 )
( 25 )
( 26 )
( 27 )
( 28 ) ( 29 )
Since r2
2 ≤ b2
2 , the problem specified in Eq ( 21 ) is bounded from below by −b2f2 . Thus , minr L ( r , α , β , ρ ) is also bounded from below .
According to whether the inequality constraints are active , the problem can have different minimum values . This requires a discussion of the following four different cases : ( 1 ) , β = 0 , ˆf + αa + ρy = 0 ; ( 2 ) , β = 0 , ˆf + αa + ρy = 0 ; ( 3 ) , β > 0 , α = 0 ; ( 4 ) , β > 0 , α > 0 . The following sections study these cases in detail . 321 The Case β = 0 , ˆf + αa + ρy = 0 In this case , set r = t ( f + αa + ρy ) , and let t → −∞ . Then L ( r , α , 0 , ρ ) → −∞ . This contradicts the observation that minr L ( r , α , β , ρ ) must be bounded from below . So when ˆf + αa + ρy = 0 , β must be positive . 322 The Case β = 0 , ˆf + αa + ρy = 0 Let Pu ( v ) = v − vuu2 u be the projection of v onto the ˆf null space of u . Given ˆf + αa + ρy = 0 , it is easy to verify that αPy ( a ) = −Py(ˆf ) . This suggests that αPy(a ) and are colinear . Also since α ≥ 0 , the following must Py
2 hold :
Py ( a ) Py Py ( a)Py
ˆf
ˆf
ˆf = −1 . ˆf 2 ˆf
ˆf 2
=
Py Py ( a)2
−
.
Given αPy ( a ) = −Py
, α can be computed by :
Py Py ( a)2 Similarly , the value of ρ can be computed by :
Py Py ( a)2
α = − Py ( a )
2
2
= − ˆfy y2 ˆf 2
Py Py ( a)2
ρ = − ˆfy y2
2
− α ay y2
2 ay y2
2
By plugging β = 0 and the obtained value of α and ρ into Eq ( 22 ) , L ( r , α , 0 , ρ ) can be written as :
L ( r , α , 0 , ρ ) = − a
θ1 − c
ˆf
( 30 )
It can be verified that in this case , all KKT conditions specified in Eq ( 23)–Eq ( 29 ) are satisfied . Since the problem defined in Eq ( 19 ) is convex and its domain is also convex , Eq ( 30 ) defines the minimum value of the problem . The following theorem summarizes the result when β = 0 .
Theorem 35 When its minimum value at β = 0 , which can be computed as :
Py ( a)Py(ˆf ) Py ( a)Py(ˆf ) = −1 , rˆf achieves ˆf 2
θ1 − c
ˆf . a
Py Py ( a)2
ˆf = − r
And in this case , the value of the dual variables are :
, β = 0 , ρ = − ˆfy y2
2
−
ˆf 2
Py Py ( a)2 ay y2
2
. min r
ˆf 2
Py Py ( a)2
Py(ˆf)2 Py ( a)2
α =
> 0 , the minimum value is achieved on
Since α = the hyperplane defined by a ( b + r ) = 0 . |Py ( a)Py(ˆf)| Py ( a)Py(ˆf ) = 1 , rˆf achieves its maximum value at β = 0 . In this case − min θˆf can be computed as :
Corollary 36 When
− min θ
ˆf = − min r
2
ˆf − c
ˆf = a
θ1 .
( 31 )
2 max θ
ˆf can be computed by replacing ˆf with −ˆf in Eq ( 31 ) .
ˆf 2 are independent to λ1 , λ2 ,
In the computation , Py and θ1 . Therefore , it can be precomputed . Py ( a)2 and aθ1 are shared by all features . These properties can be used to accelerate the computation . 323 The Case : β > 0 , α = 0 In this case , since β > 0 and α = 0 , the minimum value of rˆf is achieved on the boundary of the hyperball . In Figure 1 , this corresponds to the arc of the red circle under the blue line . Plugging α = 0 in Eq ( 22 ) results in :
β,r2
2 − b2
2
+ ρ ( c + r ) y ( 32 )
L ( r , 0 , β , ρ ) = r
ˆf +
1 2
ˆf 2
Py Py ( a)2
To obtain this equation , the following facts are used :
The dual function g ( 0 , β , ρ ) can be obtained by setting ∇rL ( r , 0 , β , ρ ) = ˆf + βr + ρy = 0 ⇒ r = − 1 . β Since β > 0 , b2 = r2 . Therefore β can be written as :
ˆf + ρy
β =
ˆf + ρy2 b2
Plugging the obtained r and β into L ( r , 0 , β , ρ ) leads to : g ( ρ ) = min r
L ( r , 0 , β , ρ ) = −b2ˆf + ρy2 + ρc y .
( 33 )
The dual function can be maximized by setting ∂g(ρ ) Also since by = cy , the following equation holds :
∂ρ = 0 .
−b2
ρyy + ˆfy ˆf + ρy2
+ b y = 0 .
( 34 )
Squaring both sides of the equation and solving the ob tained equation leads to the result :
ρ = − ˆfy yy ,by 2 2 ˆfy yy
= yy
= b b −
ˆf − ˆf
= Py ( b)2 2 ,
2
. y
± yy by yy
Py Py ( b)2
ˆf 2 flflflfl2 flflflflb − by flflflflfl2 flflflflflˆf − ˆfy flflfl2 flflflPy ˆf ˆf + ρy
2 ˆf ˆf 2 by yy yy
= y
β
.
2
2
.
, it can be
. To ensure that β is positive ,
Since ( c + r ) y = 0 and r = − 1 ˆfy+ρyy cy verified that β = the following equation must hold : Py Py ( b)2
ρ = − ˆfy yy
+
And in this case , β can be written in the form :
ˆf + ρy2 b2
=
Py Py ( b)2
β =
( 35 )
( 36 )
To compute max
ρ g ( ρ ) , first , Eq ( 34 ) can be rewritten as : b2ˆf + ρy2 = b2
2
ρyy + ˆfy by
.
( 37 )
Plugging Eq ( 35 ) and Eq ( 37 ) into Eq ( 33 ) leads to : g ( ρ ) = −b2
2 max
ρ
+ ρb y
= −Py ( b)2
Since
ˆfyby yy = ˆfb− P y ( b ) Py written in the following form : g ( ρ ) = −Py ( b)2 max
ρ
ρyy + ˆfy by flflfl2 flflflPy ˆf ˆf flflfl2 ˆf
, max
+ Py ( b )
ρ flflflPy
− ˆfyby yy ˆf −ˆf
Py g ( ρ ) can also be b .
It can be verified that in this case , all the KKT conditions specified in Eq ( 23)–Eq ( 24 ) and Eq ( 26)–Eq ( 29 ) are satisfied . The additional condition for Eq ( 25 ) to be satisfied can be derived as follows . First setting the derivative of Eq ( 22 ) to be zero leads to the following equation :
ˆf + αa + ρy r = − 1 β
Plugging this equation to a ( b + r ) ≤ 0 results in :
α ≥ βa b − a f − ρa y .
If βab − af − ρay > 0 , α > 0 must hold . According to the complementary slackness condition , a ( b + r ) = 0 . Therefore , α = βab − af − ρay > 0 . However , this contradicts the requirement that α = 0 . On the other hand , if βab − af − ρay ≤ 0 , α = 0 must hold . Otherwise , α > 0 and α = βab − af − ρay ≤ 0 form a contradiction . Therefore , to satisfy Eq ( 25 ) , the condition βab − af − ρay ≤ 0 must be true . Plugging Eq ( 35 ) and Eq ( 36 ) in βab − af − ρay ≤ 0 leads to : flflflPy flflfl2 ˆf
Py ( a )
ˆf flflfl2 ˆf
− Py flflflPy
Py ( b)2
 Py ( b ) flflfl2 ˆf flflflPy
 ≤ 0
ˆf − ˆf
≤ b by yy
. ( 39 )
Under this condition , the KKT condition a ( b + r ) ≥ 0 must be satisfied . The following theorem summarizes the result for the case β > 0 and α = 0 .
Theorem 37 When Py ( a )
− Py(ˆf ) Py(ˆf)2 0 , rˆf achieves its minimum value at β > 0 and α = 0 :
Py ( b)2
Py ( b )
In this case , the values of the dual variables are :
ˆf = −Py ( b)2 min r r
ˆf 2
Py Py ( b)2
α = 0 , β =
+ Py ( b )
Py
ˆf 2
Py Py ( b)2
−
, ρ = − ˆfy yy
Corollary 38 When Py ( a )
≤ Py ( b)2 0 , rˆf achieves its minimum value at β > 0 and α = 0 . And in this case , − min θˆf can be computed as :
− Py(ˆf ) Py(ˆf)2
Py ( b )
ˆf − ˆf
− Py ( b )
Py
θ1
( 40 )
ˆf = − min r
2
ˆf − c
ˆf
− min θ = Py ( b)2 flflflPy flflfl2 ˆf flflflPy flflfl2 ˆf max θ
ˆf can be computed by replacing ˆf with −ˆf in Eq ( 40 ) . does not rely on λ1 , λ2
In the computation ,
2 and θ1 . Therefore , it can be precomputed . Py ( a ) Py ( b ) and Py ( b)2 , although rely on λ1 , λ2 or θ1 , are shared by all features and only need to be computed once . These properties can be used to accelerate computation . 324 The Case : β > 0 , α > 0 In this case , the minimum value of rˆf is achieved on the intersection of the boundary of the hyperball and the hyIn Figure 1 , this corresponds to the two points perplane . on the intersection of the red circle and the blue line . It turns out that when β > 0 and α > 0 , deriving a closed form solution for the problem specified in Eq ( 19 ) is difficult . Theorem 3.3 suggests that when the minimum value is achieved on the intersection of the hyperball and the hyperplane , one could switch the hyperball used in Eq ( 19 ) to
1 simplify the computation . It turns out that a closed form solution can be obtained by using the hyperball Bt with t = 1 + . This corresponds to the hy
− 1
λ2
λ1 flflfl 1
λ1 a1 −θ1 perball defined in Theorem 32 As proved in Theorem 3.3 , −θ1 ) ( θ2 − θ1 ) = 0 the intersections of different Bt and ( 1 λ1 are identical . Therefore , switching the hyperball Bt does not change the maximum value of |θˆf| . flflfl 1 − 1
When Bt with t = 1 + is used and a1 −θ1
λ2
λ1 assuming that the minimum is achieved on the intersection of the hyperball and the hyperplane , the problem specified in Eq ( 19 ) can be rewritten as : flflfl2
λ1 flflfl2 1
( 41 ) y . st ar = 0 , r2
And its Lagrangian can be written as :
L ( r , α , β , ρ ) = r
ˆf + αa argr min rˆf 2 − l2 ≤ 0 , ( ˆc + r ) y = 0 . r +
1 2
2 − l2 + ρ ( ˆc + r ) β,r2 1
ˆf + αa + ρy
− 1 λ1 l =
1 2
λ2
.
In the preceding equation , c is the center of the hyperfall , and l is the radius of the hyperfall . They are defined as :
1
λ2
ˆc =
1 2
− 1 λ1
Pa ( 1)+θ1 ,
Pa ( 1 ) .
The dual function g ( α , β , ρ ) = minr L ( r , α , β , ρ ) can be obtained by setting ∇rL ( r , α , β , ρ ) = 0 , which leads to : r = − 1 β
Since β = 0 , r2 = l . Therefore , β can be written as :
( 38 )
β =
ˆf + αa + ρy2
Since α = 0 , ar = 0 . Therefore , α can be written as : l
ˆf + ρy
α = −a
Plugging the obtained r , α , and β into L ( r , α , β , ρ ) leads to : g ( ρ ) = min
L ( r , α , β , ρ ) r
= −lˆf + αa + ρy2 + ρˆc y = −lˆf − a ya2 + ρˆc ˆf a + ρy − a
= −lPa y .
+ ρPa ( y)2 + ρˆc
ˆf y
( 42 ) g ( ρ ) can be maximized by setting ∂g(ρ )
∂ρ = 0 , which leads to :
ˆf
ρPa ( y ) l
Pa
ˆf
Pa ( y ) + Pa
+ ρPa ( y)2
Pa ( y )
= ˆc y .
( 43 )
ˆf
Squaring both sides of the equation and solving the resulting problem yields a closed form solution for ρ :
ρ = − Pa
Pa ( y )
Pa ( y )
Pa ( y )
±
PPa(y ) PPa(y ) ( Pa ( 1))2
Pa
Pa ( 1 ) Pa ( y )
Pa ( y ) Pa ( y )
.
ˆf 2
To obtain this equation , the following facts are used :
1 1
λ2
λ2
− 1 λ1 − 1 λ1
ˆc y = l2 =
1 2
1 4
Pa ( y )
Pa ( 1 ) ,
Pa ( 1 )
Pa ( 1 ) .
2
Pa ( ˆc ) + Pa ( r )
Pa ( y ) = 0 . It can
+ ρPa ( y )
. There
. To ensure that β is
ˆf
ˆf 2 y = 0 ,
Since ( ˆc + r ) be verified that Pa ( r ) = − 1
Pa(ˆf )
β
Pa Pa(y)+ρPa(y)Pa(y ) Pa(ˆc)Pa(y ) fore , β = positive , the following equation holds :
ˆf
ρ = − Pa
Pa ( y )
Pa ( y )
−
Pa ( y )
In this case , β can be written in the form :
Pa ( 1 ) Pa ( y )
Pa ( y ) Pa ( y )
Pa
PPa(y ) PPa(y ) ( Pa ( 1))2
−1 PPa(y )
Pa
ˆf 2
PPa(y ) ( Pa ( 1))2
β = 2
− 1 λ1
1
λ2
ρ
To compute max g ( ρ ) , first , Eq ( 43 ) can be rewritten as :
ˆf lPa
+ ρPa ( y)2 = l2
ρPa ( y )
Pa ( y ) + Pa
ˆcy
Pa ( y )
.
ˆf
By plugging this equation and ρ into Eq ( 42 ) , max g ( ρ ) can be written in the following form :
1
λ2
1 2
− 1 λ1
ρ flflPPa(y ) ( Pa ( 1))flfl2
Pa ( y )
Pa ( y )
.
Pa ( y ) Pa ( 1 )
Pa flflfl2 −flflflPPa(y ) ˆf ˆf
− P ˆf
Pa ( y ) a
Since Pa ( 1 )
Pa ( f ) − Pa
Pa ( y ) Pa ( 1 )
Pa ( y )
Pa ( y )
Pa ( y )
ˆf
.
= PPa(y ) ( Pa ( 1 ) )
PPa(y )
Pa max g ( ρ ) can also be written in the form :
ρ
1
1 2
− 1 λ1
λ2
−flflflPPa(y )
Pa flflfl2 flflflPPa(y ) ˆf
ˆf − Pa ( 1 )
Pa flflfl2
Pa ( 1 )
Pa ( f )
.
+PPa(y )
Pa ( 1 )
PPa(y )
Theorem 3.9 summarizes the result when β > 0 and α > 0 .
Theorem 39 When rˆf achieves its minimum value at
β > 0 and α > 0 , it can be computed as : min
1 r
λ2
1 2
ˆf = r
− 1 λ1
−flflflPPa(y )
+PPa(y )
Pa ( 1 )
PPa(y )
Pa flflflPPa(y ) flflfl2 ˆf ˆf
− P
Pa
Pa ( 1 ) flflfl2 a ( 1 ) Pa ( f )
.
Corollary 310 When θˆf achieves its minimum value
1
ˆf = − min r
2 at β > 0 and α > 0 , it can be computed as : − min θ 1 2
ˆf − ˆc
− 1 λ1 flflflPPa(y )
Pa
λ2
ˆf = flflfl2 flflflPPa(y ) ˆf
ˆf flflflPPa(y ) flflfl2
PPa(y )
Pa ( 1 )
Pa max θ
ˆf can be computed by replacing ˆf with −ˆf in Eq ( 44 ) . is shared by all
In the computation ,
2
( 44 ) flflfl2
− PPa(y )
− ˆf
θ1 .
Pa ( 1 )
Pa ( 1 ) features and needs to be computed only once . This property can be used to accelerate computation . 3.3 The Screening Algorithm
Algorithm 1 shows the procedure of screening features for 1 regularized 2 SVM . Given λ1 , λ2 , and θ1 , the algorithm returns a list L , which contains the indices of the features that are potentially active in the optimal solution that corresponds to λ2 . The algorithm first weights all features using It then computes max|ˆfθ| for features in Y in Line 3 . Line 4 and Line 5 . If the value is larger than 1 , it adds the index of the feature to L in Line 7 . The function neg min(ˆf ) computes − min θ ˆf . Since Pu ( −v ) = −Pu ( v ) , the intermediate results computed for neg min(ˆf ) can be used by neg min(−ˆf ) to accelerate its computation .
2
The algorithm needs to be implemented carefully to ensure efficiency . First , each step of the computation needs to be decomposed to many small substeps , so that the intermediate results obtained in the preceding substeps can be used by the following substeps to accelerate computation . Second , the substeps need to be organized and ordered properly so that no redundant computation is performed . It turns out the procedure listed in Algorithm 1 is surprisingly fast . First , y1 , f1 , fy , and ff are independent of θ1 , λ1 , and λ2 . Therefore , they can been precomputed before training , and the cost is O ( mn ) . θ 1 θ1 are shared by all the features . So they can be computed at the begining of screening , and the cost is O ( n ) . Given these intermediate results , most substeps for computing max|ˆfθ| can be obtained in O ( 1 ) . The only expensive substep is to compute θ 1 f , and its cost is O ( mn ) for m features . However , when a solver fits a 1 regularized 2 SVM model , it might have already computed ˆfθ1 as an intermediate result for all the features . In this case , ˆfθ1 can be obtained from the solver for screening features at no cost .
1 1 , and θ
1 y , θ
In summary , in the worst case of the proposed procedure , the total computational cost for screening a data set that has m features and n samples is O ( mn ) . And if ˆfθ1 , f1 , fy , and ff can be obtained from the intermediate results generated by the 1 regularized 2 SVM solver , the total cost can decrease to just O ( m ) . 4 . EMPIRICAL STUDY
The proposed screening method was implemented in the C language and compiled as a library that can be conveniently accessed in a high level programming language , such as the Python or SAS . This section evaluates its power for accelerating model selection for 1 regularized 2 SVM . Experiments are performed on a Windows Server 2008 R2 with two Intel Xeon
L5530 CPUs and 72GB memory .
Input : X ∈ IRn×m , y ∈ IRn , λ1 , λ2 , θ1 ∈ IRn . Output : L , the retained feature list .
1 L = ∅ , i = 1 , Y = diag ( y ) ; 2 for i ≤ m do ˆf = Yfi ; 3 m1=neg_min(ˆf ) , m2=neg_min(−ˆf ) ; 4 m = max{m1 , m2} ; 5 if m ≥ 1 then 6 L = L ∪ {i} ; 7 8 9 end i = i + 1 ;
10 end 11 return L ;
12 Function neg_min(ˆf ) 13
Py ( a)Py(ˆf ) Py ( a)Py(ˆf ) = −1 then if compute m using Eq ( 31 ) ; return m ;
14 15 16
17 end
≤ 0 then if Py ( a )
Py ( b )
Py ( b)2
− Py(ˆf ) Py(ˆf)2 compute m using Eq ( 40 ) ; return m ;
18 19 20 21 22 23 end Algorithm 1 : Screening for 1 regularized 2 SVM . end compute m using Eq ( 44 ) ; return m ;
4.1 Experiment Setup
Five benchmark data sets are used in the experiment . One is a microarray data set : gli 85 . Three are text data sets : rcv1.binary(rcv1b ) , real sim , and news20.binary ( news20b ) . And one is a educational data mining data set : kdd2010 bridge to algebra ( kddb ) . The gli 85 data set is downloaded from Gene Expression Omnibus,2 and the other four data sets are downloaded from the LIBSVM data repository.3 According to the feature to sample ratio ( m/n ) , the five data sets fall into three groups : ( 1 ) the m * n group , including the gli 85 and news20b data sets ; ( 2 ) the m ≈ n group , including the rcv1b and kddb data sets ; and ( 3 ) the m n group , including the real sim data set . Table 1 shows detailed information about the five benchmark data sets .
Table 1 : Summary of the benchmark data sets
Data Set sample ( n ) feature ( m ) m/n gli 85 rcv1b real sim news20b kddb
85
20242
72309
19996
19264097
22283
47236
20958
1355191
29890095
262.15
2.33
0.29
67.77
1.55
2wwwncbinlmnihgov/geo/query/acccgi?acc=GSE4412 3wwwcsientuedutw/ cjlin/libsvmtools/datasets/
A solver based on the coordinate gradient descent ( cgd ) algorithm [ 13 ] is implemented in the C language for training the 1 regularized 2 SVM model . This solver improves the one that is implemented in the liblinear package [ 6 ] . In liblinear , the bias term b is also penalized by the 1 regularizer and is inactive in most cases . In contrast , the improved one solves the problem specified in Eq ( 1 ) exactly . Therefore , the bias term is not penalized and is alway active . Since −ˆfθ1 is the gradient on a coordinate , ˆfθ1 is computed in the solver as an intermediate result . Therefore , in screening , ˆfθ1 values can be obtained from the solver at no cost . k λmax − , k = 1 , . . . , 20 , = 10−8 ) . When
λ values : ( λk = 1 tively . And let m = n
For each given benchmark data set , the cgd solver is used to fit the 1 regularized 2 SVM model along a sequence of 20 λ = λmax − , only one feature is active . Denote n+ and n− as the number of positive and negative samples , respecxi . This feature yi − n+−n− corresponds to the largest element in m . i=1 n
For each given benchmark data set , the cgd solver runs in seven different configurations : ( 1 ) In org , the solver runs without any accelerating technique . ( 2 ) In warm , the solver runs with warm start . In the kth iteration , the wk−1 obtained in the ( k − 1)th iteration is used as the initial wk for fitting the model . When λk and λk−1 are close , warmstart can effectively speed up training by reducing the number of iterations for the solver to converge . ( 3 ) In shr , the solver runs with the shrinking strategy . During each iteration of the cgd solver run , if a feature ’s current weight is 0 and its gradient is very small , the feature is set to be inactive [ 6 ] . ( 4 ) In warm shr , the solver runs with both warmstart and the shrinking strategy . ( 5 ) In scr , the solver runs with the screening technique . ( 6 ) In warm scr , the solver runs with both warm start and the screening technique . ( 7 ) In scr shr , the solver runs with both the shrinking strategy and the screening technique .
Warm start and screening are designed to speed up model selection , and shrinking is designed to speed up training . These techniques can be combined for further performance improvement . The main purpose of running the 1 regularized 2 SVM solver with different configurations is not only to compare different accelerating techniques , but also to provide a sensitivity study for exploring how these techniques can be combined to achieve the best performance .
Both screening and shrinking reduce computational cost by removing inactive features . Their major differences include the following : ( 1 ) Shrinking is performed in each iteration of training to reduce the search space of the solver , whereas screening is performed only once before training . ( 2 ) Shrinking is a heuristic method for removing inactive features . Sometimes it might also remove active features ; when this happens , recovering the true result leads to extra cost . In contrast , the proposed screening technique is safe , because all the removed features are guaranteed to be inactive the optimal solution . ( 3 ) The introduced shrinking technique works only for the cgd solver . In contrast , the proposed screening technique can be applied with any 1 regularized 2 SVM solver to speed up model selection . Therefore , the proposed screening technique is more general .
Table 2 : Total run time of the 1 regularized 2 SVM solver when different combinations of accelerating techniques are used to speed up model selection .
Table 3 : Total number of iterations for the 1 regularized 2 SVM solver to converge when different combinations of accelerating techniques are used .
Tech . org warm shr warm shr scr warm scr scr shr gli 85 rcv1b real sim news20b kddb
328.7
376.6
2.78
3.10
0.78
0.74
1.45
17.92
10.30
4.49
2.31
3.35
1.78
4.00
20.81
13.48
7.25
4.32
6.67
4.30
7.14
943.67
682.08
62.44
32.62
25.53
17.84
48.29
9209.06
7752.80
3374.53
2395.45
1126.05
831.87
2603.94
4.2 Results
Table 2 and Table 3 show the results of the total run time and the total number of iterations for the 1 regularized 2 SVM solver to converge when different combinations of accelerating techniques are used . The total run time and total number of iterations are obtained by aggregating the time and number of iterations used by the 1 regularized 2 SVM solver when it fits models using different regularization parameters . In terms of total running time , screening with warm start ( warm scr ) provides the best performance . Compared to org , for the m * n group , the speedup ratio is about 445 for the gli 85 data and 53 for the news20b data . For the m ≈ n group , the speed up ratio is about 10 for the rcv1b data and 11 for the kddb data . And for the m n group , the speed up ratio is about 5 for the real sim data . The result shows that warm scr is more effective when the number of features is larger than the number of samples . A similar trend is observed on scr and scr shr . In terms of the total iteration number , the best performance is achieved by warm and warm scr . This suggests that warm start can effectively speed up convergence by providing a good start point for optimization . A similar trend is observed when shr is compared to warm shr .
When org is compared to scr , the result suggests that the proposed screening technique can significantly improve the performance of the 1 regularized 2 SVM solver . This justifies that screening can effectively reduce the computational cost of training by removing most inactive features . When shr is compared to scr , the result suggests that screening performs faster . This is because shrinking is a heuristic method for removing inactive features . Sometimes , it might remove active features during training . When this happens , violations can be detected by using the KKT conditions for the optimal solution . However , recovering the optimal solution leads to extra cost . This is supported by the observation that with shr the solver usually takes more iterations to converge than with org and src . When warm is compared to scr and shr , the results suggest that removing inactive features for training is more effective than providing a good starting point for optimization .
The results presented in Table 2 and Table 3 suggest that the performance of screening and shrinking can be further improved by combining them with warm start . This is because warm start can effectively speed up convergence by providing a good starting point for optimization . However , combining screening with shrinking does not improve the performance of screening because that screening has already
Tech . org warm shr warm shr scr
15535
14610
16046
14888
15376 warm scr
14599 scr shr
16150 gli 85 rcv1b real sim news20b kddb
1062
615
1737
713
1059
590
1695
568
373
815
431
596
390
942
2579
1898
4995
2157
2862
1999
4908
755
628
2008
1046
843
569
1901 removed many inactive features before training is performed . When shrinking is used in training , its benefit for removing inactive features becomes insubstantial and is overwhelmed by the cost of recovering the optimal solution when it accidentally removes active features .
Figure 2 shows detailed information about how different combinations of accelerating techniques perform on the news20b data set when λ decreases from λmax to 1 20 λmax . The result shows that screening with warm start is effective for accelerating and its performance is stable . It also shows that when λ decreases , the proposed screening technique can stably select a small set of features for reducing computational cost . Let k be the number of active features . On the news20b data set , when λ decreases from λmax to 1 20 λmax , the proposed screening technique retains about k+430 features for training the 1 regularized 2 SVM model . This number is much smaller than the dimensionality of the news20b data set , which is about 1.3 million . Similar trends are also observed on other data sets and are not presented in the paper because of the space limit .
Table 4 : Comparison of screening to training time
Tech . gli 85 rcv1b real sim news20b kddb scr tr ratio scr tr ratio
0.03
0.75
0.04
0.03
0.70
0.05 scr
0.04
6.63
0.01 warm scr
0.04
4.26
0.01
0.06
3.29
0.02
0.07
1.72
0.04
1.91
23.63
0.08
1.91
15.93
0.12
41.65
1084.40
0.04
41.71
790.15
0.05
Table 4 compares the time used by screening to the time used by training . Notice that for training , the solver uses only the features that are selected by screening . Compared to training , the time used by screening is marginal .
The results presented in this section indicate that the proposed screening technqiue is effective for removing inactive features to improve training efficiency . And with warm start they form the most powerful combination for accelerating model selection for the 1 regularized 2 SVM .
Figure 2 : Detailed information about how different combinations of accelerating techniques perform when λ decreases from λmax to 1 20 λmax . Results are reported for the news20b data set . “ Run time ” is the time that is used for training . For scr , warm scr , and scr shr , run time includes screening time . “ Iterations ” is the number of iterations for the solver to converge . “ Over hits ” is the number of inactive features that are not removed by screening . The results show that the proposed screening technique improves efficiency significantly . And when λ decreases , the number of leftover inactive features is small and stable .
5 . CONCLUSION
Screening is an effective technique for improving model selection efficiency by eliminating inactive features . This paper proposes a novel technique to screen features for 1regularized 2 SVM . The key contribution of this paper is the usage of the variational inequality for deriving closedform criteria to screen features for the 1 regularized 2 SVM model in different situations . Empirical study shows that the proposed technique can greatly improve model selection efficiency by stably eliminating a large number of inactive . Our ongoing work will extend the technique to screen features for the 1 regularized 1 SVM model .
6 . ACKNOWLEDGMENTS
The authors would like to thank Anne Baxter , Russell Albright , and the anonymous reviewers for their valuable suggestions to improve this paper .
7 . REFERENCES [ 1 ] M . T . abd Li Wang and I . W . Tsang . Learning sparse svm for feature selection on very high dimensional datasets . In ICML , 2010 .
[ 2 ] J . Bi , M . Embrechts , C . M . Breneman , and M . Song .
Dimensionality reduction via sparse support vector machines . JMLR , 3:1229–1243 , 2003 .
[ 3 ] S . Boyd and L . Vandenberghe . Convex Optimization .
Cambridge University Press , 2004 .
[ 4 ] P . S . Bradley and L . O . Mangasarian . Feature selection via concave minimization and support vector machines . In ICML , 1998 .
[ 5 ] E . Candes and M . Wakin . An introduction to compressive sampling . IEEE Signal Processing Magazine , 25:21–30 , 2008 .
[ 6 ] R E Fan , K W Chang , C J Hsieh , X R Wang , and C J Lin . Liblinear : A library for large linear classification . JMLR , 9:1871–1874 , 2008 .
[ 7 ] L . Ghaoui , V . Viallon , and T . Rabbani . Safe feature elimination in sparse supervised learning . Pacific Journal of Optimization , 8:667–698 , 2012 .
[ 8 ] I . Guyon , J . Weston , S . Barnhill , and V . Vapnik . Gene selection for cancer classification using support vector machines . Machine Learning , 46:389–422 , 2002 .
[ 9 ] J . L . Lions and G . Stampacchia . Variational inequalities . Communications on Pure and Applied Mathematics , 20 , ( 3):493–519 , 1967 .
[ 10 ] J . Liu , Z . Zhao , J . Wang , and J . Ye . Safe screening with variational inequalities and its application to lasso . In ICML , 2014 .
[ 11 ] R . Tibshirani . Regression shrinkage and selection via the lasso . Journal of the Royal Statistical Society , Series B , 58:267–288 , 1996 .
[ 12 ] R . Tibshirani , J . Bien , J . H . Friedman , T . Hastie ,
N . Simon , J . Taylor , and R . J . Tibshirani . Strong rules for discarding predictors in lasso type problems . Journal of the Royal Statistical Society : Series B , 74:245–266 , 2012 .
[ 13 ] P . Tseng and S . Yun . A coordinate gradient descent method for nonsmooth separable minimization . Mathematical Programming , 117:387–423 , 2009 .
[ 14 ] J . Wang and et al . Lasso screening rules via dual polytope projection . In NIPS , 2013 .
[ 15 ] J . Weston , A . Elisseff , B . Schoelkopf , and M . Tipping .
Use of the zero norm with linear models and kernel methods . JMLR , 3:1439–1461 , 2003 .
[ 16 ] G X Yuan and K L Ma . Scalable training of sparse linear svms . In ICDM , 2012 .
[ 17 ] J . X . Zhen , X . Hao , and J . R . Peter . Learning sparse representations of high dimensional data on large scale dictionaries . In NIPS , 2011 .
[ 18 ] J . Zhu , S . Rosset , T . Hastie , and R . Tibshirani . 1 norm support vector machines . In NIPS , 2003 .
0123456135791113151719news20b : Run Timeshrwarm_shrscrwarm_scrscr_shr050100150200250300350400450135791113151719news20b : Iterationsshrwarm_shrscrwarm_scrscr_shr01002003004005006002468101214161820news20b : Over Hitsnumber of selected featuresover hits
