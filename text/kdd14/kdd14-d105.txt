Predicting Long Term Impact of CQA Posts :
A Comprehensive Viewpoint
Yuan Yao
State Key Laboratory for Novel Software Technology , China yyao@smailnjueducn
Hanghang Tong
Arizona State University , USA htong6@asu.edu
Feng Xu and Jian Lu
State Key Laboratory for Novel
Software Technology , China
{xf,lj}@njueducn
ABSTRACT Community Question Answering ( CQA ) sites have become valuable platforms to create , share , and seek a massive volume of human knowledge . How can we spot an insightful question that would inspire massive further discussions in CQA sites ? How can we detect a valuable answer that benefits many users ? The long term impact ( eg , the size of the population a post benefits ) of a question/answer post is the key quantity to answer these questions . In this paper , we aim to predict the long term impact of questions/answers shortly after they are posted in the CQA sites . In particular , we propose a family of algorithms for the prediction problem by modeling three key aspects , ie , non linearity , question/answer coupling , and dynamics . We analyze our algorithms in terms of optimality , correctness , and complexity . We conduct extensive experimental evaluations on two real CQA data sets to demonstrate the effectiveness and efficiency of our algorithms .
Categories and Subject Descriptors H28 [ Database Management ] : Database applications—Data mining
Keywords Question answering ; long term impact ; impact correlation
1 .
INTRODUCTION
Community Question Answering ( CQA ) sites , such as Stack Over flow1 , Yahoo! Answers2 , AnswerBag3 , and Ask.com4 , have become valuable platforms to create , share , and seek a massive volume of human knowledge . How can we spot an insightful question that would inspire massive further discussions in these CQA sites ? How can we detect an valuable answer that benefits many users ? The long term impact of a question/answer post , which is the size
1http://stackoverflow.com/ 2http://answersyahoocom/ 3http://wwwanswerbagcom/ 4http://wwwaskcom/ of population it benefits in total , is the key quantity to answer these questions . How can we predict the Long term Impact of a Post ( either a question or an answer ) shortly after it is posted on the CQA site ? This , which we refer to as the LIP problem in this paper , is an essential task for the prosperity and sustainability of the CQA ecosystem that benefits all types of its users , including the information producers , spreaders , and consumers .
Despite its importance , it is not an easy task to predict the longterm impact of question/answer posts . Consequently , there are very few dedicated , existing tools for this problem ( see Section 6 for a review ) . We summarize two major challenges below .
The first challenge lies in the multi aspect of the long term impact of question/answer posts . The user rating/voting mechanism within most of the CQA sites often provides a good measure of the long term impact of CQA posts . For example , the voting score in Stack Overflow directly tells how many site users find the corresponding question/answer is beneficial to him/her . This naturally leads us to cast the LIP problem as a ( supervised ) data mining problem . Nonetheless , this problem has its own characteristics , making any off the shelf data mining algorithm sub optimal for this problem . To be specific , while each of the following aspects might affect the long term impact of question/answer posts , they require different treatments in the data mining algorithms . How can we build a comprehensive model to capture all these aspects to maximally boost the prediction accuracy ?
• A1 . Non linearity . Both the contextual features ( eg , the reputation of the user who issues the question , etc . ) and the content of the post ( eg , keywords , etc . ) might affect its longterm impact ; and the effect of each feature might be beyond the simple linear relationship . • A2 . Coupling between questions and answers . Intuitively ( which was also confirmed in our tech report [ 29] ) , the longterm impact of the questions might be correlated with that of its associated answers . Yet , the questions and answers may reside in different feature spaces . • A3 . The dynamics ( of training data sets ) . While CQA sites usually offer a large size of training data set , it may arrive in a dynamic ( stream like ) way for the mining algorithms .
The second challenge is the computation . Each of the above aspects ( non linearity , the question answer coupling , and the dynamics ) will add the extra complexity into the mining process . For example , while many machine learning algorithms ( eg , kernel regression , support vector regression , etc . ) exist to capture the nonlinearity between the features and outputs ( long term impact score in our case ) , they typically require at least O(n2 ) in time and space complexity , where n is the total number of the training examples .
Moreover , when the new training examples arrive in a stream , evergrowing fashion , even an O(n ) algorithm might be too expensive . How can we make our prediction algorithms scalable to millions of CQA posts and adaptive to the newly arrived training examples over time ?
In this paper , we aim to address both challenges by proposing a family of algorithms for predicting the long term impact of question/answer posts . Our algorithms enjoy three key advantages . First , they are comprehensive in the sense that our model naturally captures all the above three key aspects ( ie , non linearity , coupling , and dynamics ) that matter with the long term impact of a post . Second , they are flexible and general , being able to handle the special cases where only a fraction of these aspects are prominent . For example , in some CQA sites , we might be only interested in the prediction of answers posts ( such as Yahoo! Answers ) , and/or the features may have a linear effect on the post impact , etc . Third , they are scalable and adaptive to the newly arrived examples . For example , one of our algorithms ( LIP KIMAA ) has a sub linear complexity in both time and space . On the Stack Overflow data set with more than 3 million posts , this LIP KIMAA algorithm can build and update our model in seconds , while straightforward approaches would take hours .
The main contributions of this paper are summarized as follows : • A family of novel algorithms for the prediction of the longterm impact of questions and answers in CQA sites . • Proofs and analysis , showing the optimality , correctness , and computational efficiency , as well as the intrinsic relationship among different algorithms . • Extensive empirical evaluations , demonstrating effectiveness and efficiency of our algorithms . For example , compared with alternative choices ( eg , KRR [ 25] ) , one of our proposed algorithm ( LIP KIMAA ) ( 1 ) leads up to 35.8 % improvement in prediction accuracy ; ( 2 ) and is up to 390x faster while enjoying sub linear scalability .
The rest of the paper is organized as follows . Section 2 describes the problem definition . Section 3 presents the proposed algorithms . Section 4 discusses some variants . Section 5 presents the experimental results . Section 6 reviews related work , and Section 7 concludes the paper .
2 . PROBLEM STATEMENT
In this section , we first define the LIP problem , and then present its solution space to illustrate the relationship between our proposed algorithms and the existing work . 2.1 Problem Definitions
Table 1 lists the main symbols we use throughout the paper . For convenience , we use bold capital letters for existing matrices/vectors at time t , and bold lower case letters for newly arrived matrices/vectors at time t + 1 . We use superscript ( ie , q or a ) to distinguish questions and answers , and use subscript ( ie , t , t + 1 , etc . ) to indicate time . For example , we use Fq t to denote the feature matrix for questions at time t , and fq t+1 to denote the feature matrix of newly arrived questions at time t+1 . Each row of Fq t+1 contains the feature vector for the corresponding question . Similarly , we use Yq t to denote the vector of impact scores at time t , and yq t+1 to denote the vector of impact scores from new questions at time t + 1 . Following conventions , we use calligraphic letter K q t and K a to denote the kernel matrix for questions and answers at time t . We will omit the subscript when the meaning of matrices/vectors t and fq t
Symbol Fq t , Fa t fq , fa K q t ,K a t+1 t+1 t , hq kq t+1 t+1 ka , ha t+1 t+1 t , Λq Uq t Ua t , Λa t Mt mt+1
Yq t , Ya t yq , ya t+1 t+1 nq , na iq , ia d r th
Table 1 : Symbols . t and Fa t
Definition and Description the features for existing questions/answers at time t the features of new questions/answers at time t + 1 the kernel matrix of Fq the kernel matrix of new questions at time t + 1 the kernel matrix of new answers at time t + 1 the low rank matrices to approximate K q the low rank matrices to approximate K a the row normalized association matrix between existing questions and answers at time t the row normalized association matrix between new questions and answers at time t + 1 the impact score for existing questions/answers at time t the impact score for new questions/answers at time t + 1 the number of existing questions/answers at time t the number of new questions/answers at time t + 1 the feature dimension the rank of Uq t , Ua the threshold for the filtering step t , and Λa t t , Λq t t is clear in the context . We use the row normalized nq × na matrix Mt to denote the association between questions and answers at time t , where non zero element Mt(i , j ) indicates that the jth answer belongs to the ith question . Thus , the matrix Mt is sparse since it contains only na non zero elements . We also use Fq t ( i , : ) to represent the ith row of matrix Fq t ( i ) to represent the ith element of t ) . to represent the transpose of Fq vector Yq t . t , and ( Fq
Based on the above notations , we define the LIP problem in its t , Yq static form as :
PROBLEM 1 . Static LIP Problem
Given : the question/answer feature matrix Fq/Fa , the question/answer impact vector Yq/Ya , and the association matrix M ;
Output : the impact of new questions and their answers .
In real CQA sites where questions and answers continuously arrive , we need to update the model to keep it up to date . To this end , we define the following dynamic form of the LIP problem :
PROBLEM 2 . Dynamic LIP Problem
Given : the question/answer feature matrix Fq t /Fa t and the newly arrived question/answer feature matrix fq t+1/fa t+1 , the question/answer impact vector Yq t and the newly arrived question/answer impact vector yq t+1 , as well as the association matrix Mt and mt+1 ; t /Ya t+1/ya
Output : the impact of new questions and their answers . 2.2 Solution Space
Solution Space . Let us first define the solution space of the LIP problem , which is represented by a genealogy graph in Fig 1 . In this paper , we consider three key aspects that matter with the prediction performance , including ( a ) whether the predication models are linear or non linear ; ( b ) whether we treat the prediction of the questions and the answers separately ( single ) or jointly ( coupling ) ; and ( c ) whether the prediction is static or dynamic . Based on these three aspects , we could have different variants of the prediction algorithms for the LIP problem , whose intrinsic relationship is also summarized in Fig 1 . In the figure , we use the letter K , M , and I to denote non linearity , coupling , and dynamics , respectively .
K : Non linear I : Dynamic M : Coupling A : Approximate
LIP KIMAA
( non linear , coupling , dynamic )
LIP KIMA
( non linear , coupling , dynamic )
LIP KIM
( non linear , coupling , dynamic )
LIP KM
( non linear , coupling )
Recursive Kernel Ridge Regression [ 11 ] ( LIP KI )
LIP IM
( dynamic , coupling )
Kernel Ridge
Regression [ 25 ] ( LIP K )
Linear Co Prediction [ 29 ]
( LIP M )
Recursive Ridge
Regression [ 15 ] ( LIP I )
Non linear
Coupling
Ridge Regression
( linear , static , single )
Dynamics
Figure 1 : The solution space of LIP problem . Shaded boxes are proposed algorithms ; and white boxes are existing work .
For example , LIP KIM means that our model is non linear and dynamic , and it jointly predict the long term impact of questions and answers ; LIP K means that our prediction model is non linear and static , and it treats questions and answers separately , etc .
In Fig 1 , each upward solid arrow makes the model more comprehensive by modeling more aspects in the prediction algorithms , and each dashed arrow makes the algorithms more scalable . For example , starting with the ridge regression algorithm in the bottom layer , we have the kernel ridge regression ( KRR ) [ 25 ] by incorporating non linearity , and we have the recursive ridge regression [ 15 ] by incorporating dynamics . If we incorporate both non linearity and dynamics , we have the recursive kernel ridge regression algorithm ( RKRR ) [ 11 ] in the third layer .
Preliminaries . In Fig 1 , we use shaded boxes to indicate the algorithms proposed in this paper , and while boxes to indicate existing work . Before presenting the proposed algorithms in the next section , we first briefly review some existing work ( eg , white boxes ) , which serves as the building blocks of our proposed algorithms .
.
. min αq,αa
( A ) Linear Co Prediction . In our tech report [ 29 ] , we proposed a regularized optimization formulation to jointly predict the voting score of questions and answers i=1(Fq(i , :)αq − Yq(i))2 + .nq i=1(Fq(i , :)αq − M(i , :)Faαa)2 + λ(||αq||2 i=1(Fa(i , :)αa − Ya(i))2 + ||αa||2 2 )
( 1 ) where parameters λ and θ are used to control regularization and the importance of the coupling between questions and answers , respectively .
+θ nq na
2
( B ) Kernel Ridge Regression . In order to capture the non linearity between the features and outputs , a natural choice is to user kernelized methods . Take question impact prediction as an example , the so called kernel ridge regression [ 25 ] aims to estimate a coefficient βq as follows min βq
.nq i=1(K q(i , :)βq − Yq(i))2 + λ(βq).K qβq
( 2 ) where K q is the kernel matrix of Fq . 3 . THE PROPOSED ALGORITHMS
In this section , we propose our solutions for the LIP problem . We start with presenting two algorithms for Problem 1 ( subsection
3.1 ) and Problem 2 ( subsection 3.2 ) , respectively ; and then address the computational challenges ( subsection 33 34 ) 3.1 LIP KM Algorithm for Problem 1
Here , we address the static LIP problem ( Problem 1 ) . We propose an algorithm ( LIP KM ) to capture both the non linearity and the coupling aspects .
For the non linear aspect , a natural choice is to kernelize a linear prediction model ( eg , linear ridge regression ) . Recall that kernel method aims to produce non linear versions of linear learning algorithms by mapping the data points into a high dimensional Hilbert space H with a non linear function φ [ 4 ] . The key idea behind kernel methods is to use the kernel functions to replace the innerproduct operations in the high dimensional Hilbert space H , and such replacement can be ensured by Mercer ’s Condition [ 6 ] . In other words , for two data points F(i , : ) and F( j , : ) , the inner product of φ(F(i , : ) ) and φ(F( j , : ) ) in the Hilbert space H can be directly computed by a Mercer kernel κ(F(i , : ) , F( j , : ) ) κ(F(i , : ) , F( j , : ) ) =< φ(F(i , :) ) , φ(F( j , : ) ) >= φ(F(i , :))φ(F( j , : ) ) ( 3 ) where < ·,· > indicates the inner product in H . As we can see from Eq ( 3 ) , we can derive the non linear models without any explicit knowledge of either φ or H . Common kernel functions include Gaussian kernel , polynomial kernel , cosine kernel , etc .
For the coupling aspect , LIP KM imposes a so called impact consistency on the prediction space by requiring the predicted impact of a question to be close to that of its answer ( see our tech report [ 29 ] for the detailed explanations about its rationality ) .
.
Putting the non linearity and coupling aspects together , we have the following optimization formulation for Problem 1
. i=1(K q(i , :)βq − Yq(i))2 + nq
. min βq,βa na i=1(K a(i , :)βa − Ya(i))2
+θ
.nq i=1(K q(i , :)βq − M(i , :)K aβa)2 +λ((βq).K qβq + ( βa).K aβa )
( 4 ) where θ is a weight parameter to control the importance of coupling , λ is a regularization parameter , and K q and K a are the kernel matrices of Fq and Fa , respectively . K q and K a can computed as K q(i , j ) = κ(Fq(i , : ) , Fq( j , :) ) , and K a(i , j ) = κ(Fa(i , : ) , Fa( j , :) ) . Eq ( 4 ) can be solved by the closed form solution
β = arg min βq,βa fi +λ((βq ) ( θ + 1)K q + λI
=
−θM.K q
||K qβq − Yq||2 .K qβq + ( βa )
+ ||K aβa − Ya||2 .K aβa )
2
2
−θMK a
K a + θM.MK a + λI
Yq Ya
+ θ||K qβq − MK aβa||2 '−1fi
'
2
( 5 ) where β = [ βq ; βa ] . Once the coefficient vectors βq and βa are inferred from the above equation , the impact of questions/answers can then be predicted as test = K q ˆYq test = K a ˆYa where the kernel matrices on the test set Fq puted as K q Fa( j , :) ) . testβq , testβa test and Fa test(i , : ) , Fq( j , :) ) , K a test can be comtest(i , : ) , test(i , j ) = κ(Fq test(i , j ) = κ(Fa
( 6 )
Algorithm Analysis . Let us analyze the effectiveness and efficiency of the LIP KM algorithm ( ie , Eq ( 5) ) . We first summarize the optimality of LIP KM in the following lemma , which states that LIP KM finds ( at least ) a local minimum for the static LIP problem . LEMMA 1 . Optimality of LIP KM . Eq ( 5 ) finds a local min imum for Eq ( 4 ) .
PROOF . Omitted for brevity .
. can update the model in Steps 3 4 . In these two steps , E1 stands for a permutation matrix to exchange the corresponding rows/columns
⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
E1 =
Inq×nq
0 0 0
0 0
0
Ina×na
0 Iiq×iq 0 0
0 0 0 Iia×ia
⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
( 13 ) where iq and ia denote the number of questions and answers arrived at time t + 1 , respectively .
Algorithm Analysis . The correctness of LIP KIM is summarized in the following theorem , which states that LIP KIM can find the same coefficients ( ie , βt+1 ) as if we apply the LIP KM algorithm whenever we have new training examples .
THEOREM 1 . Correctness of LIP KIM . Let β∗ t+1 be the output of Eq ( 5 ) at time t + 1 , and βt+1 be the output of Alg . 1 updated from time t to t + 1 , we have that βt+1 = β∗ t+1 .
PROOF . Notice that Eq ( 5 ) can be re written as fi
'
Yq t Ya t
βt = S−1 t fi ( θ + 1)K q −θM .
Therefore , we need to first prove the update procedure for S−1 Step 3 in Alg . 1 ) . St+1 can be written as t
+ λI K q t+1
K a t+1 t+1
St+1 =
+ θM . where Mt+1 = [ Mt , 0 ; 0 , mt+1 ] , and K q Eq ( 7 ) . By exchanging the rows and columns of St+1 ( ie , moving all the t+1 t+1 are shown in t+1 t+1
−θMt+1K a t+1Mt+1K a t+1 and K a
' + λI
( ie , features at time t into the upper left corner ) , we have
St+1 = E1
( 14 ) where S1 , S2 , S3 and E1 are specified in Eq ( 9 ) , Eq ( 10 ) , Eq ( 11 ) and Eq ( 13 ) , respectively .
Applying Matrix Inversion Lemma [ 24 , 14 ] to Eq ( 14 ) , we have that
S−1 t+1
= ff fi
' fi−1
1
S1 St E . E1 fi S2 S3 t S1DS2S−1 S−1 t + S−1 −S−1 t ( I + S1DS2S−1 3 S2S−1 t ) = E . 1 . t
= E1 where D is specified in Eq ( 12 ) , and E−1 t+1 , we have that
Based on the updated S−1
1
' E .
1
−S−1 t S1D D fi S1 St S2 S3
' E .
1 t t , fa t+1 , yq t+1 , Mt and mt+1
Algorithm 1 The LIP KIM Algorithm . Input : S−1 t , fq , βt , Fq t+1 , ya t+1 , Fa Output : βt+1 , S−1 t+1 t+1 , hq 1 : compute the new kernels kq t+1 and ha fi 2 : compute S1 , S2 , S3 , D , and E1 as Eq ( 9 ) ( 13 ) ; t S1DS2S−1 S−1 t + S−1 3 : update S−1 fi −S−1 t ( I + S1DS2S−1 3 S2S−1 t ) t S1D(S2βt − [ yq βt + S−1 −S−1 3 S2(βt + S−1 t+1 as E . 4 : update βt+1 as E1 5 : return βt+1 , S−1 t+1 ; t+1 in Eq ( 7 ) ; ' −S−1 t S1D ' E1 ; D t+1 ; ya t+1 ] ) t S1DS2βt ) + D[yq t+1 ; ya t+1 , ka t+1 ]
1 t
;
Next , we summarize the time complexity and space complexity of LIP KM in the following lemma , which basically states that LIPKM requires O((nq + na)3 ) time and O((nq + na)2 ) space .
LEMMA 2 . Complexity of LIP KM . The time complexity of Eq ( 5 ) is O((nq + na)3 + ( nq)2d + ( na)2d ) ; the space complexity of Eq ( 5 ) is O((nq + na)2 + ( nq + na)d ) .
PROOF . Omitted for brevity .
.
3.2 LIP KIM Algorithm for Problem 2
Here , we address the dynamic LIP problem ( Problem 2 ) . We present the LIP KIM algorithm to incrementally update the model in Eq ( 5 ) . The basic idea of LIP KIM is to incorporate the dynamic aspect into LIP KM , and therefore it is adaptive to newly arrived training examples . When new questions and answers arrive at time t + 1 , we first t+1 and K a need to compute the new kernel matrices K q t+1 which are fiK a ' as follows t+1 ) . K q ( ka ha t+1
' , K a t+1 ) . ( kq hq t+1 fiK q kq t+1 ka t+1
( 7 ) t+1 t+1
=
= t t where the kernel matrices involving the newly arrived examples can be computed as : kq t+1(i , j ) = κ(fq t+1(i , j ) = κ(fa t+1(i , : ) , fq t+1(i , : ) , fa To simplify the algorithm description , let us introduce the fol t ( j , :) ) , hq t ( j , :) ) , and ha t+1(i , j ) = κ(fq t+1(i , j ) = κ(fa t+1(i , : ) , Fq t+1(i , : ) , Fa lowing matrices t+1( j , :) ) , ka t+1( j , :) ) . fi ( θ + 1)K q t + λI tK q −θM . fi ( θ + 1)(kq −θM . t(kq fi ( θ + 1)kq −θm . t+1 t+1kq t+1 ka t+1 t+1 ) . t+1 ) . t
St =
S1 =
S2 = fi
S3 =
' t + λI ' t
−θMtK a tMtK a t+1 ) . tMt(ka
K a t + θM . −θMt(ka t+1 ) . + θM . ( ka −θmt+1ka + θm . t+1mt+1ka t+1 −θmt+1ha t+1 t+1mt+1ha t+1
+ θm . t+1 t+1 ) . '
' + λI
( θ + 1)hq + λI −θm . t+1 t+1hq t+1 D = ( S3 − S2S−1 ha t+1
−1 t S1 )
( 8 )
( 9 )
( 10 )
( 11 )
( 12 )
⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
Yq t yq t+1 Ya t ya t+1 t+1
β∗ t+1 = S−1 fi S−1 t + S−1 −S−1 3 S2S−1 fi βt + S−1 −S−1 3 S2(βt + S−1
= E1
= E1 t t S1DS2S−1 t ( I + S1DS2S−1 t ) t S1D(S2βt − [ yq
−S−1 t S1D D
'⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
Yq t Ya t yq ' t+1 ya t+1 t+1 ; ya t S1DS2βt ) + D[yq t+1 ] ) t+1 ; ya t+1 ]
With these extra notations , we present our LIP KIM algorithm for solving Problem 2 , which is summarized in Alg . 1 . As we can see from the algorithm , we re use S−1 t and βt from previous computations . Therefore , we also need to update S−1 t+1 and βt+1 for future iterations . After we compute the new kernels as well as the matrices ( ie , S1 , S2 , S3 and D ) that are based on the new kernels , we
= βt+1 which completes the proof .
. The time complexity and space complexity of our LIP KIM is summarized in the following lemma , which basically states that LIP KIM requires O((nq + na)2 ) time and O((nq + na)2 ) space .
LEMMA 3 . Complexity of LIP KIM . The time complexity of Alg . 1 is O((nq + na)2(iq + ia ) + ( nq + na)(iq + ia)2 + ( iq + ia)3 + ( nqiq + naia + ( iq)2 + ( ia)2)d ) ; the space complexity of Alg . 1 is O((nq + na + iq + ia)2 + ( nq + na + iq + ia)d ) .
. PROOF . Omitted for brevity . Since iq and ia are usually much smaller than nq and na , and the feature dimension d is a fixed constant , the time and space complexity of LIP KIM can be re written as O((nq + na)2 ) . Compared with LIP KM which is cubic in time , LIP KIM is much more efficient . However , it is still quadratic wrt the total number of the training examples . In the next two subsections , we propose two approximate algorithms to further speed up the computation .
3.3 LIP KIMA Algorithm The reason that LIP KIM is quadratic is that we need to maintain two kernel matrices of the size nq × nq and na × na , respectively . In order to avoid quadratic cost for both time and space , we need an efficient way to approximate/compress the full kernel matrices and update them over time . Take the kernel matrix for questions as an example . Notice that K q is symmetric and semi positive definite ; therefore , we can apt ≈ Uq proximate it by eigen decomposition : K q t ) . , where Uq t Λq t is an r × r diagonal matrix is an nq × r orthogonal matrix , and Λq t whose entries are the largest r eigenvalues of K q t . By doing so , we reduce the space cost from O(n2 When new questions arrive at time t + 1 , we have the new kernel matrix K q t+1 by Nyström method [ 10 ] as t+1 as shown in Eq ( 7 ) . We approximate K q q ) to O(nqr ) . t ( Uq t
K q t+1
=
≈
≈
=
' . t t t kq t+1 kq t+1
' t+1 ) . ( kq ' fiK q hq t+1 ( K q −1 t ) ' t ( Λq Uq t ) fiK q fiK q fiK q fi kq t+1 Uq t kq t+1Uq t ( Λq t X . t )−1 t
' . fiK q ' . kq t+1 t
.
−1(Uq t ) kq t+1 −1Λq t ( Λq t ) ' fi Uq t kq t+1Uq t ( Λq
Λq t t )−1
1
= X1Λq
( 15 ) where we define X1 as the ( nq + iq ) × r matrix [ Uq t )−1 ] . t ; kq t+1Uq To make the decomposition of K q t+1 reusable for future updates , we need to find the eigen decomposition form of K q t+1 . To this end , we first perform the Singular Value Decomposition ( SVD ) on 1Q . , where both P and Q are orthogonal . Next , we X1 as X1 = PΛq perform eigen decomposition on an r×r matrix X2 = Λq t QΛq 1 , that is , X2 = VΛqV Based on the above two steps , we have the approximate eigendecomposition of the new kernel matrix K q
1Q.Λq t ( Λq t+1 as follows
K q t+1
1
≈ X1Λq t X . 1Q.Λq = P(Λq t QΛq )P . = P(VΛqV . = Uq t+1(Uq Λq t+1 ) t+1
1)P . .
( 16 )
= PV and Λq t+1 where we define Uq t+1 orthogonal because both P and V are orthogonal . We use the same approach to approximate and update the kernel matrix for answers : K a t+1 ) We further define the
= Λq . Notice that Uq t+1(Ua Λa
≈ Ua t+1 is t+1 t+1
Algorithm 2 The LIP KIMA Algorithm . Input : Uq t , Fq t , fq t+1 , Fa t , Λq t , Ua t , fa t+1 , Yq t , yq t+1 , Ya t , ya t+1 , Mt and mt+1 t , Λa t+1 , Λq t+1 , Ua t+1 , Ua t+1 , Λq t+1 and Λa t+1 t+1 and ka
Output : βt+1 , Uq 1 : compute the new kernels kq 2 : update Uq t+1 and Λa t+1 as Eq ( 16 ) ; 3 : define U , Λ and G as Eq ( 17 ) ; ' 4 : define A and B as [ U , θGU ] and [ ΛU . ; ΛU. ] ; 5 : update βt+1 as 1 ; 6 : return βt+1 , Uq
λ ( I − A(λI + BA)−1B ) t+1 , Λq t+1 and Λa t+1 ; t+1 in Eq ( 7 ) ; fi Yq t+1 Ya t+1 t+1 , Ua following notations to simplify the algorithm description
U = [ Uq , 0 ; 0 , Ua t+1 ] t+1 Λ = [ Λq , 0 ; 0 , Λa t+1 ] t+1 G = [ I,−Mt+1;−M . t+1 , M .
( 17 ) Then , we have the following approximation for the St+1 matrix t+1Mt+1 ]
K a
−θMt+1K a t+1Mt+1K a t+1
+ θM . ' t+1
' + λI defined in Eq ( 8 )
St+1 = fi ( θ + 1)K q −θM . t+1
+ λI K q fiK q t+1 t+1 t+1 0 K a = λI + ( I + θG ) t+1 ≈ λI + UΛU . + θGUΛU . = λI + AB t+1 0 where we define A = [ U , θGU ] and B = [ ΛU . ; ΛU ]
Finally , applying Matrix Inversion Lemma to Eq ( 18 ) , we have the coefficients βt+1
( 18 )
βt+1 = S−1
' fi Yq t+1 Ya t+1 t+1
' fi Yq t+1 Ya t+1 λ ( I − A(λI + BA )
≈ ( λI + AB ) = 1
−1
' fi Yq t+1 Ya t+1
−1B )
( 19 ) t , Ua t , Λq t and Λa t .
The complete algorithm of LIP KIMA is summarized in Alg . 2 . As we can see , in addition to βt+1 , the only variables we need to update are Uq t . Compared to Alg . 1 , we do not need to store βt ; instead we need to store Yq t . More importantly , we do not need to store St ; instead , we only need to store the much smaller matrices of Uq t and Ya t and Λa t , Λq t , Ua
Algorithm Analysis.The effectiveness of LIP KIMA is summarized in Lemma 4 . According to Lemma 4 , there are two possible places where we could introduce the approximation error in the LIP KIMA algorithm , including ( a ) eigen decomposition for the Kt and ( b ) the Nyström method for Kt+1 . Notice that if we only do eigen decomposition at t = 1 , such approximation error might be accumulated and amplified over time . In practice , we could ‘restart’ the algorithm every few time ticks , that is , to re compute ( as opposed to approximate ) the eigen decomposition for the current kernel matrix .
LEMMA 4 . Effectiveness of LIP KIMA . Let β∗ t+1 be the output of Eq ( 5 ) at time t + 1 , and βt+1 be the output of Alg . 2 updated t+1 if Kt = UtΛt(Ut ) . and from time t to t + 1 , we have βt+1 = β∗ ht+1 = kt+1(Kt)−1(kt+1 ) . hold for both questions and answers .
PROOF . Omitted for brevity .
.
The time complexity and space complexity of Alg . 2 is summarized in the following lemma . It basically says that the LIP KIMA algorithm requires linear time and space wrt the total number of questions and answers .
LEMMA 5 . Complexity of LIP KIMA . The time complexity of Alg . 2 is O((nq + na + iq + ia)r2 + r3 + ( nqiq + naia)d ) ; the space complexity of Alg . 2 is O((nq +na +iq +ia)d +(nq +na +iq +ia)r +r2 ) . . PROOF . Omitted for brevity . Since iq and ia are usually much smaller than nq and na , and the low rank r and feature dimension d are fixed constants , the time complexity and space complexity of Alg . 2 can be re written as O(nq + na ) in terms of the total number of questions and answers . 3.4 LIP KIMAA Algorithm
Compared with LIP KIM , LIP KIMA is much more scalable , being linear in terms of both time and space complexity . However , if the new training examples arrive in a stream like , ever growing fashion , a linear algorithm might be still too expensive . To address this issue , we further present the LIP KIMAA algorithm to reduce the complexity to be sub linear .
Our LIP KIMAA is built upon LIP KIMA . The main difference between LIP KIMAA and LIP KIMA is that we add an additional filtering step between Step 1 and Step 2 in Alg . 2 . That is , when new questions and answers arrive at time t +1 , we first treat them as test set and apply the existing model at time t on this test set . Based on the prediction results , we only add the questions and answers whose prediction error is larger than a given threshold th . Notice that the complexity of LIP KIMA is linear wrt the number of questions and answers ; as a result , our LIP KIMAA scales linearly wrt the number of remaining questions and answers after the filtering steps . Therefore , LIP KIMAA scales sub linearly wrt to the total number of questions and answers in both time and space . We omit the detailed algorithm for brevity . 4 . VARIANTS
The proposed LIP KIM and its two approximate algorithms ( LIPKIMA and LIP LIMAA ) are comprehensive . In terms of the modeling power , they capture all the three aspects ( non linearity , coupling , and dynamics ) . In this section , we show that our algorithms are also flexible . That is , if only a subset of these three aspects matter with the prediction performance for some applications , our algorithms can be naturally adapted to these special cases . We briefly discuss some of these variants , and then summarize the algorithms in Fig 1 . 4.1 Variant 1 : Linear Model LIP IM
If we only consider the coupling and dynamic aspects ( ie , ignoring the non linear aspect ) , our LIP KIM can be simplified as the LIP IM algorithm . Essentially , LIP IM aims to efficiently update the solution of Eq ( 1 )
−θ(Fq).MFa
( Fa).Fa + θ(Fa)MMFa + λI
'−1fi
'
( Fq).Yq ( Fa).Ya
⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ fq t+1 0 fq t+1 0 fl
R =
⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ ffi mt+1fa t+1
0
0 t+1 ; mt+1fa fa t+1 t+1 and mt+1 .
As we can see , both L and R are only based on new observations fq t+1 , fa t L(I +RS−1 Next , we further define matrix C = S−1 t L)−1R , and we can show that the model can be updated as follows 'fi fi t+1).yq ( fq t+1).ya ( fa t+1 = ( I − C)S−1 ff S−1 αt+1 = ( I − C ) αt + S−1 t+1 t+1 t t
Remarks . Notice that compared with LIP KIM , LIP IM is much more efficient since it only needs to compute the inverse of a much smaller ( 3iq + ia ) × ( 3iq + ia ) matrix ( see the computation of matrix C ) . By ignoring the smaller terms ( ie , iq and ia ) , the overall time complexity of LIP IM is O(d2 ) where d indicates feature dimension ; on the other hand , directly updating the model would require O((nq +na)d +d3 ) time . In the meanwhile , by a similar procedure as the proof for Theorem 1 , we can show that LIP IM finds the exact solution of Eq ( 1 ) . 4.2 Variant 2 : Approximate LIP KI t+1 t+1 t+1
βq t+1
≈ Uq
−1Yt+1
Λq t+1(Uq
≈ ( Uq = Uq t+1
If we only consider the non linearity and dynamics aspects ( ie , ignoring the coupling aspect ) , our LIP KIMA can be further simplified . Take the prediction for questions as an example . With the approximate eigen decomposition by Eq ( 16 ) : K q t+1) . , we can update the coefficients βq t+1 as follows . + λI ) Λq t+1(Uq t+1 ) .Yt+1 3(Uq Λq t+1 ) 3 is still a diagonal matrix with Λq t+1(i , i ) + λ ) . Remarks . Although sharing the same linear time complexity as LIP KIMA , this variant is even more efficient in practice since it does not need any matrix inversion at all . 4.3 Variant 3 : LIP KIM with Only Questions Next , we discuss a special case of LIP KIM when only new questions arrive at time t + 1 . Compared to Alg . 1 , we can simplify the following notations
3(i , i ) = 1/(Λq where Λq
' t+1 ) . t+1 ) .
S1 = fi ( θ + 1)(kq −θM . fl t(kq ( θ + 1)kq S2 = t+1 ⎡⎢⎢⎢⎢⎢⎢⎢⎣Inq×nq S3 = ( θ + 1)hq t+1
E1 =
0 0
0iq×(nq+na )
+ λI 0 0
Ina×na
⎤⎥⎥⎥⎥⎥⎥⎥⎦
0 Iiq×iq 0 ffi
Then the model can be updated as
S−1 t+1 = E1
βt+1 = E1 fi S−1 t + S−1 −S−1 3 S2S−1 fi βt + S−1 −S−1 3 S2(βt + S−1 t t S1DS2S−1 t ( I + S1DS2S−1 t ) t S1D(S2βt − yq t+1 ) t S1DS2βt ) + Dyq t+1
−S−1 t S1D ' D
' E .
1 fi
( θ + 1)(Fq).Fq + λI
α =
−θ(Fa)MFq where α = [ αq ; αa ] . fi ( θ + 1)(Fq −θ(Fa
St =
In this case , the St matrix becomes t ).Fq t + λI t )M tFq t
−θ(Fq t + θ(Fa t ).MtFa t )M t tMtFa t ).Fa ( Fa
' t + λI
When new questions and answers arrive at time t + 1 , we define the L matrix and the R matrix as fi ( θ + 1)(fq t+1 ) . −θ(fq t+1 ) . 0
0
L =
−θ(fa
0 t+1)m t+1 fl
0 t+1) . , θ(fa t+1)m ( fa t+1 ffi'
Remarks . The time complexity of this variant is O((nq + na)2iq + ( nq + na)(iq)2 + ( iq)3 + ( nqiq + ( iq)2)d ) , which can be re written as O((nq +na)2 ) by ignoring smaller terms of iq and d . The correctness of this variant can also be shown by following a similar procedure as the proof for Theorem 1 .
Table 2 : The summarization of the algorithms from the genealogy graph in Fig 1 . The complexity is the time complexity for updating the model at each time tick , where the n is the totally number of the training examples and those smaller terms are omitted for clarity . The right five columns are the proposed algorithms in this paper .
KRR ( LIP K )
RKRR ( LIP KI )
CoPs ( LIP M )
LIP IM LIP KM LIP KIM LIP KIMA LIP KIMAA
Non linearity
Coupling Dynamics Complexity
SVR fi x x fi x x
' O(n2 )
O(n3 ) fi x fi
O(n2 ) x fi x
O(n ) x fi fi O(1 ) fi fi x fi fi fi
O(n3 )
O(n2 ) fi fi fi O(n ) fi fi fi
< O(n )
Table 3 : The statistics of SO and Math data sets . Votes
Questions 1,966,272
16,638
Answers 4,282,570
32,876
Users 756,695 12,526
14,056,000
202,932
Data SO Math
4.4 Summarization
Finally , we make a comparison of different algorithms in Fig 1 in terms of their modeling power and efficiency . The results are summarized in Table 2 . In this table , we first check whether a given algorithm captures each of the three desired aspects ( nonlinearity , coupling , and dynamics ) . As we can see , only our LIPKIM , LIP KIMA , and LIP KIMAA algorithms meet this requirement . We also summarize the time complexity of the algorithm for updating the model at each time tick , where the smaller terms ( eg , the feature dimensionality d , the number of new training examples iq , ia , etc ) are omitted for clarity . For non linear algorithms like KRR [ 25 ] , support vector regression ( SVR ) [ 9 , 7 ] , RKRR [ 11 ] , LIP KM and LIP KIM , they need at least quadratic time for the model training because they typically need to maintain the kernel matrix . Such a complexity is unaffordable in many CQA sites . On the other hand , the time complexity of the proposed LIP KIMA and LIP KIMAA algorithms is linear and sub linear wrt the total number of questions and answers , respectively .
5 . EXPERIMENTS
In this section , we present the experimental evaluations . The experiments are designed to answer the following questions :
• Effectiveness : How accurate are the proposed algorithms for predicting the long term impact of questions/answers ? • Efficiency : How scalable are the proposed algorithms ?
5.1 Experiment Setup
We use the data from two real CQA sites , ie , Stack Overflow ( SO ) and Mathematics Stack Exchange ( Math ) , to evaluate our algorithms . They are popular CQA sites for programming and math , respectively . The statistics of the two data sets are summarized in Table 3 .
We use both content and contextual features . For content features , we adopt the “ bag of words ” model to extract content features after removing the infrequent words . This model is widely used in natural language processing where the frequency of each word is used as a feature for training . In addition , we adopt some commonly used contextual features in the literature , including the questioner ’s reputation at question creation time , the answerer ’s reputation at answer creation time , the length of the question/answer , the number of questioner ’s previous questions at question creation time , and the number of answerer ’s previous answers at answer creation time . For these features , we can extract them at the moment when the question/answer is posted . For other contextual features , we need to choose a short time window by the end of which the voting score is predicted . With such a fixed time window , we can include some time related features such as the number of comments/answers received during the fixed time window . In this work , we fix this time window as 24 hours . Overall , we have 4,180 and 4,444 features for Math data and SO data , respectively ( ie , d = 4 , 180 for Math data and d = 4 , 444 for SO data ) . For the kernel matrix , we adopt the cosine kernel function due to the sparsity of our feature matrix .
For long term impact , we restrict our attention to predict the impact of a question/answer after it is posted for six months . For each post in the data set , there are several choices to measure impact including the number of pageviews , the number of favorites , and the user voting score ( which is the difference between the number of up votes and the number of down votes on the post ) . In this work , we choose the voting score for the following two reasons . First , we conduct a survey , asking different users about which is the best metric as the long term impact measure of CQA posts ; and most of the users ( 79.4 % ) choose the voting score . Second , to some extent , the voting score of a question/answer resembles the number of the citations that a research paper receives in the scientific publication domain . It reflects the net number of users who have a positive attitude toward it . In addition , we also measured the correlation between the three choices and found that all of them are strongly positive correlated . Thus , we expect that our algorithms could also be used to predict the other two metrics ( ie , pageviews and favorites ) . We normalize the voting scores into the range of [ 0 , 1 ] .
To evaluate the dynamic aspect of our algorithms , we start with a small initial training set , and gradually add new examples into the training set in chronological order . For Math data , we start with 5 % initial data , add 5 % data for each update , and use the latest 10 % data as the test set . For SO data whose size is much larger than the Math data , we start with 0.1 % initial data , add 0.1 % data for each update , and use the latest 0.1 % data as the test set .
For evaluation metrics , we adopt the root mean square error ( RMSE ) between the real impact and the estimated impact for effectiveness , and the wall clock time for efficiency . All the efficiency experiments were run on a machine with eight 3.4GHz Intel Cores and 24GB memory .
Repeatability of Experimental Results . Both data sets are officially published and publicly available5 . Furthermore , we will make the code of the proposed algorithms as well as the extracted feature files publicly available . For all the results reported in this section , the specific parameter settings are as follows . We set λ = θ = 1 for Math data and λ = θ = 0.1 for SO data . For the low rank r in LIP KIMA and LIP KIMAA , we set it as 10 . For LIP KIMAA , we set th = 0.22 for Math data and th = 0.18 for SO data . 5.2 Effectiveness Results
We first compare the effectiveness of the proposed algorithms with two state of the art non linear regression methods , ie , kernel
5http://blogstackoverflowcom/category/cc wiki dump/
0.3
0.28
E S M R
0.26
0.24
KRR SVR LIP−KIM LIP−KIMA LIP−KIMAA
0.22
2000
4000
6000
8000 10000 12000 14000 16000
# of questions and answers ( nq+na )
0.28
KRR SVR LIP−KIM LIP−KIMA LIP−KIMAA
4000
6000
8000 10000 12000 14000 16000
# of questions and answers ( nq+na )
0.26
E S M R
0.24
0.22
2000
( a ) Question impact prediction on Math data
( b ) Answer impact prediction on Math data
KRR SVR LIP−KIM LIP−KIMA LIP−KIMAA
0.24
0.22
E S M R
0.2
KRR SVR LIP−KIM LIP−KIMA LIP−KIMAA
0.4
0.35
E S M R
0.3
0.25
5000
10000
15000
20000
25000
# of questions and answers ( nq+na )
30000
0.18
5000
10000
15000
20000
25000
30000
# of questions and answers ( nq+na )
( c ) Question impact prediction on SO data
( d ) Answer impact prediction on SO data
Figure 2 : The effectiveness comparisons . Lower is better . The proposed algorithms outperform both SVR and KRR .
( a ) Math data
( b ) SO data
Figure 3 : The speed comparisons . The proposed LIP KIMA and LIP KIMAA are much faster . Furthermore , LIP KIMAA scales sub linearly ( in the upper right corner ) . ridge regression ( KRR ) [ 25 ] and support vector regression ( SVR ) [ 7 ] . The prediction results of questions and answers on the two data sets are shown in Fig 2 . On SO data , we only report the first few points because some of the algorithms ( eg , KRR ) cannot finish training within 1 hour . We do not report the results by linear models ( eg , linear ridge regression ) since their performance ( RMSE ) is much worse than SVR .
We make several observations from Fig 2 . First , the proposed LIP KIM algorithm performs the best in most of the cases . For example , when the size of training set increases to 90 % on the Math data , LIP KIM improves the SVR method by 5.7 % for questions and 6.0 % for answers . On SO data , LIP KIM improves the KRR method by up to 35.8 % for questions and 3.6 % for answers . This indicates that the coupling aspect indeed helps in impact prediction . Second , the performance of the proposed LIP KIMA algorithm is close to LIP KIM . This result indicates that while it reduces the time complexity from quadratic to linear , the approximation method introduces little performance loss . Finally , although not as good as LIP KIM and LIP KIMA , the LIP KIMAA algo
E S M R
0.3
0.28
0.26
0.24
0.22
0
CoPs
SVR
KRR
LIP−KIMAA LIP−KIMA
LIP−KIM
50 Wall−clock time ( second )
100
0.4
0.35
0.3
CoPs
0.25
LIP−KIMAA
E S M R
LIP−KM
0.2
SVR
LIP−KIM
KRR
LIP−KIMA
LIP−KM
150
0
500
1000
1500
Wall−clock time ( second )
2000
2500
( a ) Answer impact prediction on Math data
( b ) Answer impact prediction on SO data
Figure 4 : The quality speed balance off . The proposed LIP KIMA and LIP KIMAA achieve a good balance between the prediction quality and the efficiency ( in the left bottom corner ) . Best viewed in color .
Table 4 : Performance gain analysis . Smaller is better . All three aspects of non linearity , coupling , and dynamics are helpful .
Questions/Answers Ridge regression
LIP K LIP KM LIP KIM
SO
Math
04920/04409 04214/02044 02704/01987 02595/01867
02799/03860 02461/02368 02314/02292 02249/02208 rithm is still better than the compared methods for most of the cases .
To further show the effects of all the three aspects ( ie , nonlinearity , coupling , and dynamics ) , we analyze the performance gain in Table 4 . In the table , LIP K incorporates non linearity into ridge regression , LIP KM incorporates coupling into LIP K , and LIP KIM incorporates dynamics into LIP KM . As we can see , all three aspects are helpful to improve the prediction performance . 5.3 Efficiency Results
Next , we compare the efficiency of the proposed algorithms with KRR and SVR in Fig 3 . Notice that the y axis is in log scale . In Fig 3 , we also plot the results of LIP KIMAA with y axis in linear scale in the upper right corner . We only report the results by LIPKIMAA there because it is the only algorithm that can handle the entire SO data set .
As we can see from the figure , our LIP KIMA and LIP KIMAA are much faster than the other algorithms . In the upper right corner , we can observe that the LIP KIMAA scales sub linearly wrt the total number of questions and answers . For instance , it only requires about 60 seconds when there are more than 3,000,000 questions and answers . In contrast , KRR requires more than 2,000 seconds when the size of the training set is about 30,000 .
Finally , we study the quality speed balance off of different algorithms in Fig 4 . In the figure , we show the answer prediction results only . Similar results are observed in question prediction , and we omit the results for brevity . In Fig 4 , we plot the RMSE on the y axis and the wall clock time on the x axis . We also plot the results of the linear co prediction method CoPs [ 29 ] and LIP KM . Ideally , we want an algorithm sitting in the left bottom corner . As we can see , both our LIP KIMA and LIP KIMAA are in the leftbottom corner . For example , for answer impact prediction on the SO data , compared with SVR , LIP KIMAA is 70x faster in wallclock time and 14.0 % better in RMSE . Overall , we recommend LIP KIMAA in practice .
6 . RELATED WORK
In this section , we briefly review related work including mining
CQA sites and mining stream data .
Mining CQA Sites : There is a large body of existing work on mining CQA sites . For example , Li et al . [ 19 ] aim to predict question quality , which is defined as the combination of user attention , answer attempts and the arrival speed of the best answer . Jeon et al . [ 18 ] and Suryanto et al . [ 28 ] evaluate the usefulness of answer quality and incorporate it to improve retrieval performance . To predict the quality of both questions and answers , Agichtein et al . [ 2 ] develop a graph based model to catch the relationships among users , Li et al . [ 20 ] adopt the co training approach to employ both question features and answer features , and Bian et al . [ 5 ] propose to propagate the labels through user question answer graph so as to tackle the sparsity problem where only a small number of questions/answers are labeled . Recently , Anderson et al . [ 3 ] propose to predict the long lasting value ( ie , the pageviews ) of a question and its answers . How to predict the answer that the questioner will probably choose as the accepted answer is also well studied [ 22 , 26 , 1 ] . Overall , our work differs from these existing work at the methodology level . While most of the existing work treats the prediction problem as a single , and/or linear , and/or static problem , we view the problem from a comprehensive perspective and propose to incorporate all these important aspects into the prediction models .
Mining Stream Data : From the dynamic aspect , our LIP problem is related to stream mining [ 13 ] and time series mining [ 12 ] . The main focus of existing stream/time series mining work is on pattern discovery , clustering , and classification tasks . Chen et al . [ 8 ] and Ikonomovska et al . [ 17 ] study the regression problem in data streams ; however , they still focus on a single and linear prediction problem . Several researchers also consider the non linear and dynamic aspects in regression problem [ 11 , 23 ] . Different from these existing work , we consider the coupling between questions and answers , and propose approximation methods to speed up and scale up the computation .
Other Related Work : There are several pieces of interesting work that are remotely related to our work . Liu et al . [ 21 ] propose the problem of CQA site searcher satisfaction , ie , whether or not the answer in a CQA site satisfies the information searcher using the search engines . Shtok et al . [ 27 ] attempt to answer certain new questions by existing answers . The question routing problem ( eg , how to route the right question to the right answerer ) is also an active research area [ 30 , 16 ] .
7 . CONCLUSIONS
In this paper , we have proposed a family of algorithms to predict the long term impact of questions/answers in CQA sites . The proposed algorithms enjoy three key advantages . First , they are comprehensive in the sense that our model naturally captures three key aspects ( ie , non linearity , coupling , and dynamics ) that matter with the long term impact of a post . Second , they are flexible and general , being able to handle the special cases where only a fraction of these aspects are prominent . Third , they are scalable and adaptive to the newly arrived questions and answers . We analyze our algorithms in terms of optimality , correctness , and complexity , and reveal the intrinsic relationship among different algorithms . We conduct extensive experimental evaluations on two real CQA data sets to demonstrate the effectiveness and efficiency of our approaches . 8 . ACKNOWLEDGMENTS
This work is supported by the National 863 Program of China ( No . 2012AA011205 ) , and the National Natural Science Foundation of China ( No . 91318301 , 61321491 , 61100037 ) . This material is partially supported by by the National Science Foundation under Grant No . IIS1017415 , by the Army Research Laboratory under Cooperative Agreement Number W911NF 09 2 0053 , by Defense Advanced Research Projects Agency ( DARPA ) under Contract Number W911NF 11 C 0200 and W911NF 12 C 0028 , and by Region II University Transportation Center under the project number 49997 33 25 .
The content of the information in this document does not necessarily reflect the position or the policy of the Government , and no official endorsement should be inferred . The US Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on . 9 . REFERENCES [ 1 ] L . Adamic , J . Zhang , E . Bakshy , and M . Ackerman .
Knowledge sharing and yahoo answers : everyone knows something . In WWW , pages 665–674 . ACM , 2008 . [ 2 ] E . Agichtein , C . Castillo , D . Donato , A . Gionis , and
G . Mishne . Finding high quality content in social media . In WSDM , pages 183–194 . ACM , 2008 .
[ 3 ] A . Anderson , D . Huttenlocher , J . Kleinberg , and J . Leskovec .
Discovering value from community activity on focused question answering sites : a case study of stack overflow . In KDD , pages 850–858 . ACM , 2012 .
[ 4 ] N . Aronszajn . Theory of reproducing kernels . Transactions of the American mathematical society , 68(3):337–404 , 1950 . [ 5 ] J . Bian , Y . Liu , D . Zhou , E . Agichtein , and H . Zha . Learning to recognize reliable users and content in social media with coupled mutual reinforcement . In WWW , pages 51–60 . ACM , 2009 .
[ 6 ] C . J . Burges . A tutorial on support vector machines for pattern recognition . Data mining and knowledge discovery , 2(2):121–167 , 1998 .
[ 7 ] C C Chang and C J Lin . Libsvm : a library for support vector machines . ACM Transactions on Intelligent Systems and Technology , 2(3):27 , 2011 .
[ 8 ] Y . Chen , G . Dong , J . Han , B . W . Wah , and J . Wang .
Multi dimensional regression analysis of time series data streams . In VLDB , pages 323–334 , 2002 .
[ 9 ] R . Collobert and S . Bengio . Svmtorch : Support vector machines for large scale regression problems . The Journal of Machine Learning Research , 1:143–160 , 2001 .
[ 10 ] P . Drineas and M . W . Mahoney . On the nyström method for approximating a gram matrix for improved kernel based learning . The Journal of Machine Learning Research , 6:2153–2175 , 2005 .
[ 11 ] Y . Engel , S . Mannor , and R . Meir . The kernel recursive least squares algorithm . IEEE Transactions on Signal Processing , 52(8):2275–2285 , 2004 .
[ 12 ] T c Fu . A review on time series data mining . Engineering Applications of Artificial Intelligence , 24(1):164–181 , 2011 . [ 13 ] M . M . Gaber , A . Zaslavsky , and S . Krishnaswamy . Mining data streams : a review . ACM Sigmod Record , 34(2):18–26 , 2005 .
[ 14 ] G . Golub and C . Van Loan . Matrix computations . 1996 . [ 15 ] S . S . Haykin . Adaptive filter theory . 2005 . [ 16 ] D . Horowitz and S . Kamvar . The anatomy of a large scale social search engine . In WWW , pages 431–440 . ACM , 2010 . [ 17 ] E . Ikonomovska , J . Gama , and S . Džeroski . Learning model trees from evolving data streams . Data mining and knowledge discovery , 23(1):128–168 , 2011 .
[ 18 ] J . Jeon , W . Croft , J . Lee , and S . Park . A framework to predict the quality of answers with non textual features . In SIGIR , pages 228–235 . ACM , 2006 .
[ 19 ] B . Li , T . Jin , M . R . Lyu , I . King , and B . Mak . Analyzing and predicting question quality in community question answering services . In WWW , pages 775–782 . ACM , 2012 .
[ 20 ] B . Li , Y . Liu , and E . Agichtein . Cocqa : co training over questions and answers with an application to predicting question subjectivity orientation . In EMNLP , pages 937–946 , 2008 .
[ 21 ] Q . Liu , E . Agichtein , G . Dror , E . Gabrilovich , Y . Maarek ,
D . Pelleg , and I . Szpektor . Predicting web searcher satisfaction with existing community based answers . In SIGIR , pages 415–424 , 2011 .
[ 22 ] Y . Liu , J . Bian , and E . Agichtein . Predicting information seeker satisfaction in community question answering . In SIGIR , pages 483–490 . ACM , 2008 .
[ 23 ] B . Pan , J . J . Xia , P . Yuan , J . Gateno , H . H . Ip , Q . He , P . K .
Lee , B . Chow , and X . Zhou . Incremental kernel ridge regression for the prediction of soft tissue deformations . In Medical Image Computing and Computer Assisted Intervention , pages 99–106 . Springer , 2012 .
[ 24 ] W . W . Piegorsch and G . Casella . Inverting a sum of matrices .
SIAM Review , 32(3):470–470 , 1990 .
[ 25 ] C . Saunders , A . Gammerman , and V . Vovk . Ridge regression learning algorithm in dual variables . In ICML , pages 515–521 , 1998 .
[ 26 ] C . Shah and J . Pomerantz . Evaluating and predicting answer quality in community qa . In SIGIR , pages 411–418 , 2010 .
[ 27 ] A . Shtok , G . Dror , Y . Maarek , and I . Szpektor . Learning from the past : answering new questions with past answers . In WWW , pages 759–768 , 2012 .
[ 28 ] M . Suryanto , E . Lim , A . Sun , and R . Chiang . Quality aware collaborative question answering : methods and evaluation . In WSDM , pages 142–151 . ACM , 2009 .
[ 29 ] Y . Yao , H . Tong , T . Xie , L . Akoglu , F . Xu , and J . Lu . Want a good answer ? ask a good question first! arXiv preprint arXiv:1311.6876 , 2013 .
[ 30 ] Y . Zhou , G . Cong , B . Cui , C . Jensen , and J . Yao . Routing questions to the right users in online communities . In ICDE , pages 700–711 . IEEE , 2009 .
