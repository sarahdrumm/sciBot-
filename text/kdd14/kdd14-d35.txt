Distance Metric Learning Using Dropout :
A Structured Regularization Approach
Qi Qian† , Juhua Hu‡ , Rong Jin† , Jian Pei‡ and Shenghuo Zhu(cid:63 )
†Department of Computer Science and Engineering
Michigan State University , East Lansing , MI , 48824 , USA
‡School of Computing Science
Simon Fraser University , 8888 University Drive , Burnaby , BC , V5A 1S6
( cid:63)NEC Laboratories America , Cupertino , CA , 95014 , USA
{qianqi , rongjin}@csemsuedu , juhuah@sfu.ca , jpei@cssfuca , zsh@nec labs.com
ABSTRACT Distance metric learning ( DML ) aims to learn a distance metric better than Euclidean distance . It has been successfully applied to various tasks , eg , classification , clustering and information retrieval . Many DML algorithms suffer from the over fitting problem because of a large number of parameters to be determined in DML . In this paper , we exploit the dropout technique , which has been successfully applied in deep learning to alleviate the over fitting problem , for DML . Different from the previous studies that only apply dropout to training data , we apply dropout to both the learned metrics and the training data . We illustrate that application of dropout to DML is essentially equivalent to matrix norm based regularization . Compared with the standard regularization scheme in DML , dropout is advantageous in simulating the structured regularizers which have shown consistently better performance than non structured regularizers . We verify , both empirically and theoretically , that dropout is effective in regulating the learned metric to avoid the over fitting problem . Last , we examine the idea of wrapping the dropout technique in the state of art DML methods and observe that the dropout technique can significantly improve the performance of the original DML methods .
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications— Data Mining ; I26 [ Artificial Intelligence ] : Learning
General Terms Algorithms , Experimentation
Keywords Distance Metric Learning , Dropout
1 .
INTRODUCTION
Learning a good distance metric is essential to distance based algorithms and applications , eg , k nearest neighbors , k means clustering , content based image retrieval ( CBIR ) , etc . Distance metric learning ( DML ) aims to learn a linear mapping such that in the mapped space , examples from the same class are closer to each other than those from different classes . Many methods have been developed for DML [ 6 , 7 , 16 , 26 , 27 ] in the past , and DML has been successfully applied in various domains , including information retrieval [ 13 ] , ranking [ 6 ] , supervised classification [ 26 ] , clustering [ 27 ] , semi supervised clustering [ 5 ] and domain adaptation [ 20 ] .
One problem with DML is that since the number of parameters to be determined in DML is quadratic in the dimension , it may overfit the training data [ 26 ] , and lead to a suboptimal solution . Although several heuristics , such as early stopping , have been developed to alleviate the overfitting problem [ 26 ] , their performance is usually sensitive to the setting of parameters ( eg stopping criterion in early stopping ) , making it difficult for practitioners . Another problem with many existing DML methods is their high computational cost since they have to project intermediate solutions onto the Positive Semi Definite ( PSD ) cone at every iteration to ensure that the learned metric is PSD . In [ 6 ] , the authors show that it is possible to avoid the high cost of PSD projection by an one projection paradigm that only needs to project the learned metric onto the PSD cone once at the end of the optimization algorithm . We adopt the one projection paradigm in this paper to alleviate the high computational cost .
Recently , dropout has been found to be helpful in alleviating the over fitting problem in training deep neural networks [ 14 ] . It randomly omits half of the feature detectors to prevent the complex co adaptation between those neurons . The main intuition behind dropout is that it improves the robustness of individual neurons in order to avoid the over fitting problem . Theoretical analysis about the regularization role of dropout in neural network can be found in [ 1 ] . Besides the success in deep learning , dropout has been applied to regression [ 24 ] in order to obtain robust feature representations . Features with artificial noises has been a classic topic in machine learning and data mining , and has been examined by many studies [ 17 , 19 ] with the focus on additive noise that usually leads to a L2 regularizer [ 2 ] . Wager et al . [ 25 ] analyze dropout within the framework of regression and find that dropout is first order equivalent to a L2 regularizer for the dataset scaled by the inverse diagonal Fisher information matrix . Although dropout has received a lot of interests in machine learning and data mining community , to the best of our knowledge , this is the first study that exploits dropout for alleviating the over fitting problem in DML .
In this paper , we , for the first time , introduce dropout to DML . Unlike previous studies on dropout that only apply dropout to training data , we apply dropout to both the learned metrics and the training data . By applying appropriate dropout probabilities to the learned metric , we show that dropout can be equivalent to Frobenius norm and Lp in expectation . In addition , we develop a structured regularizer using the dropout technique . Unlike the conventional regularizer that treats diagonal and off diagonal elements equivalently , the structured regularizer introduces different dropout probabilities for diagonal elements and off diagonal elements . To verify the effect of dropout in DML , we have conducted a comprehensive study that compares the dropout technique to other regularization techniques used in DML . Experimental results show that dropout significantly improves the classification performance on most datasets . In addition , we observe that dropout behaves like a trace norm based regularizer in DML when applied to training data : it controls the rank of the learned metric and leads to a skewed distribution of eigenvalues . This is in contrast to the previous studies that view dropout as a L2 regularizer . Finally , we show that the dropout technique can be easily incorporated into the state of art DML methods and significantly improve their performance for most cases . The main contribution of this work is summarized as follows .
• We , for the first time , introduce dropout to DML and apply it to both the learned metric and the training data .
• We show that it is possible to construct structured regularizers using the dropout technique , and verify its performance , both theoretically and empirically .
• We apply the dropout to the state of art DML methods and show it can significantly enhance their performance .
The rest of the paper is organized as follows . Section 2 introduces the related DML methods and the analysis for dropout . Section 3 describes applying dropout to the learned metric and training data , respectively . Section 4 summarizes the results of the empirical study . Section 5 concludes this work and discusses the future directions .
2 . RELATED WORK
Distance metric learning has been studied sufficiently during the past years [ 6 , 7 , 22 , 26 , 27 ] and detailed investigations could be found in the survey papers [ 15 , 28 ] . The early works focus on optimizing pair wise constraints [ 7 , 27 ] to make sure the distance between examples from the same class is less than a pre defined threshold while that from the different classes is larger than the other threshold . Nowadays , triplet constraints , where the distance of examples from the same class should be marginally smaller than that of examples from different classes , are preferred [ 6 , 26 ] due to their superior performance compared with pair wise constraints .
More analysis shows that triplet constraints have the large margin property and could be explained as learning a set of local SVMs [ 8 ] . However , either taking pair wise constraints or triplet constraints increases the number of training data exponentially , which significantly promotes the risk of overfitting for DML . In fact , over fitting phenomenon is reported by many DML methods [ 6 , 26 ] , and most of them try to alleviate it by PSD constraint . PSD constraint , on one hand , is the feasible set where the optimal metric should live in , and on the other hand , restricts the learned metric in the PSD cone to reduce the complexity of the model . Given a huge number of constraints , stochastic gradient descent ( SGD ) is widely used to learn the metric and PSD projection occurs at every iteration [ 23 ] . Unfortunately , the computational cost of PSD projection is cubic to the dimension of data , which significantly limits the application of DML in high dimensional datasets . Additionally , recent empirical study [ 6 ] demonstrates that one projection paradigm , which only performs PSD projection once at the end of the algorithm , has the similar performance as projecting at every iteration . In this paper , we bring dropout to overcome over fitting in DML . We adopt triplet constraints and oneprojection paradigm setting , and show that dropout significantly improves the performance of existing DML methods . Dropout is a technique developed for training deep neural networks [ 14 ] . Since deep neural network is a very complex model and is easy to overfit a small size of training data , dropout randomly omits half of neurons during training to limit the co adaptation between neurons . Dropout makes sure that each learned neuron is robust . Besides deep learning , dropout has been introduced to more general regression task these years [ 24 , 25 ] . Maaten et al . [ 24 ] use dropout to learn a robust feature representation for bag of words features , which significantly improves the performance over the original features . Although dropout belongs to the classic artificially corrupted features , it is better than other kinds of noises as reported in the work [ 24 ] . Recently , Wager et al . [ 25 ] analyze dropout within the framework of general regression task , and find that dropout is first order equivalent to the L2 regularizer on the rescaled data . The experiments in the study [ 25 ] demonstrate that dropout , as a data dependent L2 regularizer , outperforms the standard L2 norm significantly . In this paper , we introduce dropout for DML . Unlike the previous works that focus on dropout in features [ 24 , 25 ] , we also apply dropout for the learned metric . We observe that dropout is equivalent to Frobenius norm , L1 norm and trace norm with different dropout strategies . Empirical study validates the effectiveness of dropout in DML .
3 . DML USING DROPOUT
Given the dataset X = [ x1,··· , xn ] ∈ Rd×n , distance metric learning is to learn a good Mahalanobis distance metric M , so that for each triplet constraint ( xi , xj , xk ) , where xi and xj are in the same class and xk is from a different class , we have dist(xi , xk ) − dist(xi , xj ) > 1 where dist(xi , xj ) is the squared Mahalanobis distance between xi and xj and is measured by dist(xi , xj ) = ( xi − xj )
M ( xi − xj )
Algorithm 1 SGD for DML 1 : Input : Dataset X ∈ Rd×n , #Iterations T , Stepsize η 2 : Initial M0 as an identity matrix 3 : for t = 1,··· , T do Randomly sample a triplet {xt 4 : 5 : Mt = Mt−1 − η∇ 6 : end for 7 : return Πpsd( ¯M ) k} j , xt i , xt
Therefore , the objective optimization problem based on minimizing empirical risk could be written as ( At , M ) min M∈Sd + t j)(xt k)(xt i − xt i − xt i − xt
+ is the d×d symmetric PSD cone , ( · ) is the convex i − where Sd j ) − ( xt loss function , and At = ( xt k ) . ·,· denotes the dot product for matrix . xt Since the number of triplets is very large ( it can be as high as O(n3) ) , the optimization problem is usually solved by stochastic gradient descent ( SGD ) [ 7 , 23 ] . Instead of projecting the learned metric onto PSD cone at every iteration , which can be an expensive operation , we adopt oneprojection paradigm [ 6 ] , which only projects the learned metric onto the PSD cone once at the end of iterations . Empirical studies have shown that one projection paradigm is significantly more efficient and yields the similar performance as SGD with PSD projection performed at every iteration . Therefore , in this work , we will focus on the following optimization problem without the PSD constraint . nius norm is given by
T t=1 min M∈Sd
1 T
( At , M ) +
M2
F q 2η
( 1 )
The updating rule in SGD for Frobenius norm based regularization is
Mt = Mt−1 − qMt−1 − η∇t(Mt−1 ) where t(M ) = ( At , M ) .
Instead of using the regularizer directly , we could simulate the effect of Frobenius norm based regularization by applying dropout to the learned metric Mt−1 . In particular , the Bernoulli random matrix δ is constructed by sampling each δi,j:i≤j independently from a Bernoulli distribution with Pr[δ = 0 ] = q and setting δj,i = δi,j to ensure that δ is symmetric . It is easy to verify that
E[ ˆMt−1 ] = ( 1 − q)Mt−1 and the updating rule becomes
Mt = ˆMt−1 − η∇t(Mt−1 )
Theorem 1 . Let M∗ be the optimal solution output by Algorithm 1 . Let ¯M be the solution output by Algorithm 1 with dropout in Step 5 and q be the probability that dropout occurs in each item of the learned metric . Assume |x|2 ≤ r and q = 1/T , we have
E[( ¯M ) ] − ( M∗ ) ≤ 1 ηT
M∗2
F + 8ηr2(1 +
1 T
)
The detailed proof can be found in appendix . By setting the stepsize η as t min M∈Sd
( At , M )
η = r
M∗F √ 8 + 8T
Algorithm 1 gives the standard SGD algorithm for DML and dropout will be applied to perturb Step 5 . We will discuss the details in the next section within this framework . In particular , we will discuss two different applications of dropout , ie application of dropout to the learned metric and application of dropout to the training data , in the following two subsections . 3.1 Applying Dropout to Distance Metric
In this section , we focus on applying dropout to the learned metric . Let M be the metric learned from the previous iteration . To apply the dropout technique , we introduce a Bernoulli random matrix δ = [ δi,j]d i,j=1 , where each δi,j is a Bernoulli random variable with δi,j = δj,i . Using the random matrix δ , we compute the dropped out distance metric , denoted by ˆM as
ˆMi,j = δi,jMi,j , i , j = 1 , . . . , d
Note that by enforcing δi,j = δj,i , ˆM is ensured to be a symmetric matrix . Below , we will discuss how to design the dropout probabilities for the Bernoulli random matrix δ to simulate the effect of Frobenius norm based regularization and L1 norm based regularization , respectively .
311 Frobenius norm Frobenius norm is the most widely used regularizer in DML [ 23 , 26 ] , and the standard DML problem with Frobe
T )
√ M∗F = O(1/ we have √ E[( ¯M ) ] − ( M∗ ) ≤ 4r 2 + 2T T √ It is well known that O(1/ T ) is the minimax convergence rate for SGD , when the loss function is Lipschitz continuous [ 11 , 12 , 29 ] . As a result , with the appropriate choice of dropout probabilities , dropout will maintain the same convergence rate as the standard SGD method . We also notice √ that q is suggested to be set as 1/T in order to achieve O(1/ T ) convergence . This result implies that dropout should not be taken too frequently , which is consistent with the analysis of other corrupted feature methods [ 2 , 24 ] . Finally , since the derivation of convergence rates keep the same regardless of the sampling probabilities used in dropout , we thus will omit the analysis on convergence for the following cases . 312 L1 norm Besides Frobenius norm , L1 norm also could be used in
DML as min M∈Sd
1 T t
( At , M ) +
M1 q η fl M
It is known as the composite optimization problem [ 18 ] and could be solved by iterative thresholding method t = Mt−1 − η∇t(Mt−1 )
Mt:i,j = sign(M t:i,j ) max{0,|M t:i,j| − q}
With different design of sampling probabilities , we can apply dropout to the learned metric to simulate the effect of L1 regularization . In particular , we introduce a datadependent dropout probability as
Pr[δi,j = 0 ] = min{1 , q
|Mi,j|}
Figure 1 : Cumulative distribution function for diagonal elements and those in off diagonal .
Now , instead of perturbing Mt−1 , we apply dropout to M t , ie It is easy to verify that the expectation of the perturbed matrix ˆM is given by the matrix after the gradient mapping .
E t]i,j
[ ˆM t]i,j| t]i,j| which is equivalent to the thresholding method stated above . t]i,j − sign([M q ≤ |[M q > |[M
: 0 : t]i,j)q
= fl [ M
It is straightforward to extend the method to Lp norm
Lp(M ) = ( i,j by setting the probability as
Pr[δi,j = 0 ] = min{1 ,
|Mi,j|p)1/p
( q|Mi,j|p−2 i,j M p i,j)1−1/p }
Structured regularizer
Note that when p = 1 , it is equivalent to the probability for L1 norm . 313 Although these conventional regularizers have been applied for DML , they cannot exploit the structure of metrics sufficiently . Given a distance metric , the diagonal elements are more important than those from off diagonal . It is due to the fact that diagonal elements represent the importance of each feature ( eg , linear classifier ) rather than the interactions between different features , and they also control the trace of the learned metric . Therefore , different regularizers should be assigned for diagonal and off diagonal elements , respectively . Fortunately , dropout can serve this purpose conveniently .
Given Q , which is a random matrix with each element from a uniform distribution in [ 0 , 1 ] , we investigate the matrix
R = ( Q + Q
)/2
( 2 )
It is obvious that the diagonal elements of R are still from the same uniform distribution , while elements in off diagonal are from a triangle distribution with cumulative distribution function as fl
F ( q ) =
2q2 1 − 2(1 − q)2
: 0 ≤ q < 0.5 : 0.5 ≤ q ≤ 1 the elements as
Pr[δi,j = 0 ] = Pr[Ri,j ≤ min{1 ,
Algorithm 2 Dropout as Structured Frobenius Norm ( SGD M1 ) 1 : Input : Dataset X ∈ Rd×n , #Iterations T , Stepsize η 2 : Initial M0 as an identity matrix 3 : for t = 1,··· , T do 4 : 5 : Generate random matrix R as in Eqn . 2 6 : Generate dropout parameters δ by Eqn . 3 7 : Dropout : ˆMt−1 = δMt−1 8 : Mt = ˆMt−1 − η∇ 9 : end for 10 : return Πpsd( ¯M )
Randomly sample a triplet ( xt j , xt k ) i , xt since q = 1/T 0.5 as indicated in Theorem 1 . fl M t−1
ˆMt−1 =
Therefore , the expectation of ˆMt−1 is i,j − qM t−1 i,j − 2q2M t−1 d
M t−1 i,j i,j
( At , M ) + min M∈Sd
1 T q 2η i t
: : i = j i = j i,j:i=j
M 2 i,j
M 2 i,i + q2 η which is equivalent to solving the following problem
It is obvious that the L2 norm of diagonal elements in the metric is penalized quadratically more than those from off diagonal . This regularizer seems complex but the implementation by dropout is quite straightforward and Algorithm 2 summarizes the method .
Then , we consider dropout with the probability based on

2(q/|Mi,j|)2 1 − 2(1 − q/|Mi,j|)2 min{1 , q/|Mi,j|} : : : 1 :
( 4 ) q
|Mi,j|} ] i = j i = j , q < 0.5|Mi,j| i = j , 0.5 ≤ q/|Mi,j| ≤ 1 q > |Mi,j|
It seems too complicated to analyze at the first glance , but Figure 1 could help us to understand the dropout strategy clearly . For the diagonal elements , they are actually shrunk by q as the L1 regularizer . For the off diagonal elements , if |Mi,j| > 2q , the red dashed curve is under the blue solid one , which means the shrinkage is less than q . When q ≤ |Mi,j| ≤ 2q , the red dashed curve stands above the blue solid
Figure 1 illustrates the cumulative distribution function for the diagonal elements of R and those living in off diagonal . The dropout probability based on the random matrix R is defined as
=
Pr[δi,j = 0 ] = Pr[Ri,j ≤ q ]
First , we consider dropout with the same probability for each item of the metric as for Frobenius norm . Then , the probability of δ is
Pr[δi,j = 0 ] = Pr[Ri,j ≤ q ] =
: : i = j i = j
( 3 ) fl q
2q2
00204060810020406081Cumulative Distribution Function DiagonalOff−diagonal Randomly sample a triplet ( xt
Algorithm 3 Dropout as Structured L1 Norm ( SGD M2 ) 1 : Input : Dataset X ∈ Rd×n , #Iterations T , Stepsize η 2 : Initial M0 as an identity matrix 3 : for t = 1,··· , T do 4 : t = Mt−1 − η∇ 5 : M 6 : Generate random matrix R as in Eqn . 2 7 : Generate dropout parameters δ by Eqn . 4 8 : Dropout : Mt = δM 9 : end for 10 : return Πpsd( ¯M ) j , xt k ) i , xt t one and the shrinkage on these elements is much faster than the standard L1 norm . Since q is very small , most of the off diagonal elements have relatively larger values and will be shrunk slower than those with extremely small values . Algorithm 3 summarizes this method . Unlike Algorithm 2 , dropout in Algorithm 3 is performed after updating with the current gradient . 3.2 Applying Dropout to Training Data
Besides dropout within the learned metric , in this section we apply dropout to the training data as many pervious studies [ 24 , 25 ] . Since the analysis for data highly depends on the loss function , we take the hinge loss , which is the most widely used loss function in DML [ 6 , 7 , 23 , 26 ] , as an example . Hinge loss is defined as ( z ) = [ 1+z]+ , where z = At , M and
At = ( xt i − xt j)(xt i − xt j )
− ( xt i − xt k)(xt i − xt k )
Obviously , the loss function penalizes At rather than the individual example , so dropout is taken according to the structure of At . To avoid affecting the decision of hinge loss , we perturb At after calculating the hinge loss .
We begin with additive noise as i − xt i − xt j + )(xt
ˆAt = ( xt where ∼ N ( 0 , qId×d ) . So the expectation of ˆAt is k)(xt j + )
− ( xt i − xt
E[ ˆAt ] = E[(xt −(xt i − xt i − xt i − xt j + )(xt i − xt k)(xt k ) j + )
] i − xt k )
( 5 )
Randomly sample a triplet ( xt if ( At , Mt−1 ) > 0 then
Algorithm 4 Additive Noise as Trace Norm ( SGD D1 ) 1 : Input : Dataset X ∈ Rd×n , #Iterations T , Stepsize η 2 : Initial M0 as an identity matrix 3 : for t = 1,··· , T do 4 : 5 : 6 : 7 : 8 : 9 : end if 10 : end for 11 : return Πpsd( ¯M )
Generate a Guassian noise vector Add noise as in Eqn . 5 Mt = Mt−1 − η ˆAt j , xt k ) i , xt
Algorithm 5 Dropout as Trace Norm ( SGD D2 ) 1 : Input : Dataset X ∈ Rd×n , #Iterations T , Stepsize η 2 : Initial M0 as an identity matrix 3 : for t = 1,··· , T do 4 : 5 : 6 : 7 : 8 : end if 9 : end for 10 : return Πpsd( ¯M )
Randomly sample a triplet ( xt if ( At , Mt−1 ) > 0 then Dropout as in Eqn . 7 Mt = Mt−1 − η ˆAt j , xt k ) i , xt where δt is a binary value random variable and i − xt
Pr[δt = 1 + q/(xt j)2 ] = 1/(1 + q/(xt i − xt It is obvious that E[δt ] = 1 and V [ δt ] = 1 + q/(xt j)2 . Similar to the additive noise , we only apply dropout for the first item of At as i − ˆxt j)2 ) i − xt i − xt k )
− ( xt i − ˆxt j )
ˆAt = ( ˆxt i − xt k)(xt j)(ˆxt
Note that when we perform dropout to the training data according to this strategy , we actually drop the rows and the i− corresponding columns in the first component ( xt j ) of At . Since the expectation of random variables in xt diagonal is the variance and it is 1 in off diagonal , the expectation of ˆA is i−xt j)(xt
E[ ˆAt ] = E[(ˆxt i − ˆxt j)(ˆxt i − ˆxt j )
] − ( xt i − xt k)(xt i − xt k )
= At + qI
= At + qI
By replacing At in Prob . 1 with ˆAt , the expectation of the problem becomes
( At , M ) + qMtr
( 6 ) t t:t>0 min M∈Sd
Note that the trace norm stands outside of the hinge loss , since the noise is added only after computing the hinge loss and only active constraints will contribute to the trace norm . We use the trace norm rather than the trace of the metric , because the final metric will be projected onto the PSD cone , where the trace of metric is equivalent to the trace norm . Algorithm 4 describes the details of the method .
Although the Guassian noise could perform as the trace norm , the external noise may affect the solution . Therefore , we consider dropout as
By taking ˆAt back to Prob . 1 , we obtain the same problem as Prob . 6 . Algorithm 5 summarizes this dropout strategy for training data .
Theorem 2 . Let M∗ be the optimal solution output by Algorithm 1 . Let ¯M be the solution output by Algorithm 5 and q be the probability that dropout occurs in each feature of the dataset . Assume |x|2 ≤ r and q = 1/T , we have E[( ¯M )]−(M∗ ) ≤ M∗2
M∗tr
+8ηr2 +
( 2d+1)+
4ηr2
F
2ηT
T
T
The detailed proof is referred in appendix . stepsize η as
If we set the i = δtxt ˆxt i ,
ˆxt j = δtxt j
η =
2r
M∗F √ 4T + 4d + 2 we have ( ¯M ) − ( M∗ ) ≤ 1 T
2r(4T + 4d + 2)M∗F + M∗tr )
√ where O(1/ standard SGD , is also observed as in Theorem 1 .
T ) convergence rate , the well known result for
According to Theorem 2 , applying dropout to training data with the appropriate component and dropout probability does not hurt the convergence performance of standard SGD method too . Furthermore , q is required to be sufficiently small to avoid the suboptimal solution , which is also consistent with the analysis in Theorem 1 .
4 . EXPERIMENTS 4.1 Experiments Setting
Six datasets from different application scenarios are used to verify the effectiveness of the proposed method . Table 1 summarizes the information of these datasets . ta is a social network dataset with 6 different categories of terrorist attacks [ 21 ] . semeion is a handwritten digit dataset downloaded directly from the UCI repository [ 9 ] . caltech10 is a subset of Caltech256 image dataset [ 10 ] with 10 most popular categories and we use the version pre processed by the study [ 6 ] , where each image is represented by an 1 , 000dimension vector . The other datasets are directly downloaded from LIBSVM database [ 4 ] . For dna , protein and sensit , we use the standard training/testing split provided by the original dataset . For the rest datasets , we randomly select 70 % of data for training and use the remaining 30 % for testing . For each dataset , we randomly select T = 100 , 000 active triplets ( eg , incur the positive hinge loss by Euclidean distance ) within the range of 3 nearest same class neighbors as suggested by the study [ 26 ] . K Nearest Neighbor ( k=3 ) classifier is applied after obtaining the metric , since we optimize the triplets from 3 nearest neighbors . All experiments are repeated by 5 trials on different randomly generated triplet sets and the average result with standard deviation is reported . 4.2 Comparison with SGD Methods
Algorithm 1 .
In the first experiment , we compare the standard SGD for DML to five SGD variants including our proposed methods ( ie , SGD M1 , SGD M2 , SGD D1 , and SGD D2 ) . The methods are summarized as follows . • SGD : stochastic gradient descent method as described in • SGD PSD : SGD with PSD projection at every iteration . • SGD M1 : SGD with dropout for the learned metric as • SGD M2 : SGD with dropout for the learned metric as • SGD D1 : SGD with additive Guassian noise in training • SGD D2 : SGD with dropout for training data as trace structured Frobenius norm ( Algorithm 2 ) . structured L1 norm ( Algorithm 3 ) . data as trace norm ( Algorithm 4 ) . norm ( Algorithm 5 ) .
Euclidean distance is also included as the baseline method and denoted as “ Euclid ” .
All of these SGD methods are applied on the same triplet set and take one projection paradigm except SGD PSD that projects the learned metric onto the PSD cone at every iteration . We search the stepsize η in {0.1 , 1 , 10} by cross val
Table 1 : Statistics for the datasets used in the empirical study . #C is the number of classes . #F is the number of features . #Train and #Test represent the number of training data and test data , respectively .
# C # F #Train #Test ta semeion dna caltech10 protein sensit
6 10 3 10 3 3
106 256 180 1,000 357 100
902 1,115 2,000 3,151 17,766 78,823
391 478 1,186 1,338 6,621 19,705 idation and η = 1 shows the best performance , so we fix it for all experiments . The dropout probability parameter q for the proposed methods is searched in {10−i : i = 1,··· , 5} . All SGD methods are started with an identity matrix in the experiment .
Table 2 shows the classification accuracy of different SGD methods . First , it is not surprising to observe that all DML algorithms improve the performance compared to the Euclidean distance . Second , for all datasets , we observe that the proposed SGD methods with dropout ( ie , SGD M1 , SGD M2 , SGD D1 , and SGD D2 ) significantly outperform the baseline SGD methods ( ie , SGD and SGD PSD ) , which is also demonstrated by the statistical significance examined via pairwise t tests at the 5 % significance level . Concretely , on most datasets , the accuracy of SGD with dropout is about 2 % improved compared with that of SGD and it is even 4 % on protein .
Furthermore , we observe that SGD M1 shows the best performance on semeion , caltech10 and protein , while SGDM2 outperforms other methods on ta and dna , and SGD D2 is the best on sensit . It is because dropout in the learned metric and dropout in the training data represent different regularizers , and different dataset prefers different regularizer . SGD D1 and SGD D2 have the similar performance because they optimize the same trace norm . However , SGDD2 is a little bit better than SGD D1 due to the reason that no additional Guassian noise is introduced by SGD D2 . Finally , SGD PSD performs same as if not worse than SGD , which is consistent with the observation in the study [ 6 ] .
Then , we investigate if dropout can perform as the regularizers as we expected . Figure 2 compares the effect of different norms with different weights to that of SGD , where only the parameter q of dropout varies and the others are kept the same . First , since SGD M1 puts more aggressive penalty on the diagonal , Figure 2(a ) shows how L2 norm of the diagonal elements in the metric learned by SGDM1 varies as q decreases . We observe that the smaller the parameter is , the larger the L2 norm is , and even when q = 10−5 , the L2 norm is still less than that of SGD . It demonstrates that dropout as structured Frobenius norm restricts the size of diagonal elements well . Second , Figure 2(b ) compares the sparsity of the learned metric , where sparsity is defined as
Sparsity =
#(Mi,j = 0 ) d2
.
Without the constraint of L1 norm , the sparsity of the metric learned by SGD is small as shown by the blue dashed line .
Table 2 : Comparison of classification accuracy ( % ) for different SGD methods , and the best result is bolded ( statistical significance examined via pairwise t tests at the 5 % significance level between baselines and the proposed methods ) . ta
80.15
Euclid 8314±039 SGD SGD PSD 8294±056 8577±042 SGD M1 8655±022 SGD M2 8433±069 SGD D1 8474±050 SGD D2
91.63 semeion 9414±036 9427±011 9603±039 9561±042 9531±055 9540±039 dna 80.10
9258±039 9261±035 9386±033 9476±059 9336±041 9366±039
62.36 caltech10 6529±029 6488±039 6732±029 6604±024 6585±046 6607±037 protein 50.05
6019±016 5815±047 6405±031 6276±047 6136±057 6286±046 sensit 72.72
7492±009 7454±008 7633±010 7610±008 7682±005 7689±007
Figure 2 : Trend of effect for dropout as regularizers on dataset caltech . Fig ( a ) is the L2 norm of the diagonal elements in the metric learned by SGD M1 . Fig ( b ) is the sparsity of the metric learned by SGD M2 . Fig ( c ) is the rank of the metric learned by SGD D2 .
( a ) SGD M1
( b ) SGD M2
( c ) SGD D2
Figure 3 : Comparison of training error and test error of different SGD methods with different size of triplets . There is no over fitting observed for SGD method with dropout .
( a ) ta
( b ) semeion
( c ) dna legend
However , in SGD M2 as plotted by the red dash dotted line , with the increasing of the probability of dropout as the structured L1 norm , the learned metric becomes more sparse , which confirms the effectiveness of SGD M2 . Finally , trace norm constraint usually leads to a low rank metric [ 3 ] , so we study the rank of the learned metric by SGD D2 in Figure 2(c ) . As expected , when q becomes larger , more stress is put on the trace norm and the lower rank metric is induced . Since dropout is found to be helpful to overcome overfitting in deep learning [ 10 ] , we empirically study the role of dropout for alleviating over fitting problem in DML . We fix all parameters as above except the number of sampled triplets , to study the changes of training error and test error on the training and test set , with the increasing of the number of triplets . Figure 3 shows the training error and test error of SGD , SGD PSD and SGD with best dropout strategy on three small datasets ( ie , SGD M1 on semeion and SGD M2 on the others ) , while the number of sampled triplets increases from 20 , 000 to 100 , 000 . First , we observe that the training error of SGD with dropout is similar to that of conventional SGD methods as we indicate in Theorem 1 . However , over fitting is observed for SGD and SGDPSD when the number of triplets is up to 40 , 000 , while there is no over fitting phenomenon for SGD with dropout . It further demonstrates the overwhelming performance of dropout strategies in Table 2 and confirms that dropout is also helpful to overcome the over fitting problem in DML .
4.3 Comparison with State of Art Methods
Besides the comparison with various SGD methods , we also compare our proposed dropout methods to three stateof art DML algorithms as follows . • SPML [ 23 ] : a mini batch stochastic gradient descent algorithm for DML , which optimizes the hinge loss with Frobenius norm as the regularizer .
010010001000011e−05010203040506070Value of q|diag(MT)|2 SGDSGD−M1010010001000011e−052030405060708090100Value of qSparsity( % ) SGDSGD−M2010010001000011e−050200400600800Value of qRank(MT ) SGDSGD−D2246810x 10424681012141618#TripletsError(%)246810x 1040123456#TripletsError(%)246810x 10402468#TripletsError(%)SGD TrainSGD TestSGD−PSD TrainSGD−PSD TestDropout TrainDropout Test Table 3 : Comparison of classification accuracy ( % ) with state of art DML methods . “ Dropout ” refers to the best result of dropout from Table 2 . Note that LMNN is a batch learning algorithm , and there is no limitation for the triplets it uses and the number of PSD projections . The best result is bolded ( statistical significance examined via pairwise t tests at the 5 % significance level ) . caltech10 6546±050 6506±040 6717±049 6732±029 semeion 9460±027 9406±019 9377±048 9603±039
8356±050 8320±095 8479±065 8655±022
9285±037 8857±028 9513±026 9476±059 protein
5915±037 5883±038 6073±012 6405±031 sensit
7499±010 7350±015 7647±004 7689±007 ta dna
SPML OASIS LMNN Dropout
Table 4 : Comparison of classification accuracy ( % ) for SPML and its variants by wrapping different dropout strategies in . The best result is bolded . ta
SPML
8356±050 8459±038 SPML M1 8446±044 SPML M2 SPML D 8474±056 semeion 9460±027 9548±041 9527±035 9544±027 dna
9285±037 9327±016 9391±032 9341±023 caltech10 6546±050 6668±070 6615±040 6627±036 protein
5915±037 6138±036 6103±048 6214±068 sensit
7499±010 7563±008 7590±023 7684±013
Table 5 : Comparison of classification accuracy ( % ) for OASIS and its variants by wrapping different dropout strategies in . The best result is bolded .
OASIS
OASIS M1 OASIS M2 OASIS D ta
8320±095 8474±086 8479±079 8438±059 semeion 9406±019 9569±043 9473±038 9523±048 dna
8857±028 9335±033 9433±054 9315±028 caltech10 6506±040 6711±031 6612±049 6671±062 protein
5883±038 6244±055 6261±060 6306±055 sensit
7350±015 7547±015 7547±012 7656±015
• OASIS [ 6 ] : an online learning approach for DML and the • LMNN [ 26 ] : a batch learning method with Frobenius symmetric version is adopted in the comparison . norm for DML .
SPML and OASIS use the same triplet set as the proposed methods and also adopt one projection paradigm . LMNN is a batch learning method , and thus there is no limitation for the type and the number of triplets that it could use for each iteration . Specifically , LMNN is not restricted to the set of triplets used by other methods . There is also no constraint for PSD projection in LMNN and it can perform PSD projection whenever it requires . All codes for these methods are from the authors and the recommended parameters are used . Since SPML is a stochastic method , it shares the same setting as the proposed methods , where the parameter for Frobenius norm is searched within the same range as q to obtain the best performance . SPML and OASIS are both initialized with an identity matrix , while LMNN starts with the matrix from PCA without dimension reduction , which usually has a better performance than the identity matrix for the method [ 26 ] .
Table 3 summarizes the classification accuracy of different DML algorithms . “ Dropout ” denotes the best result of dropout methods adopted from Table 2 . It can be easily observed that , although LMNN is a batch learning method and could utilize much more information than our methods , LMNN only has the similar performance on dna and caltech10 , while SGD method with dropout significantly outperforms on all other datasets . It further demonstrates the effectiveness of the proposed methods . SPML and OASIS are slightly better than the standard SGD method , but significantly worse than SGD method with dropout technique . The performance of OASIS could be explained by the fact that it does not include any conventional regularizer and over fitting could be easily induced . Although SPML combines the Frobenius norm as the regularizer , it is worse than SGD M1 and SGD M2 shown in Table 2 , which implies that the proposed structured norm by dropout is more effective than the standard norm . 4.4 Wrap Dropout in Existing DML Methods In this section , we demonstrate that dropout can be easily wrapped in the existing DML methods and help improve the performance . First , we wrap dropout in SPML , which is a state of art mini batch SGD method for DML . Note that SPML has the Frobenius norm as the regularizer , so we drop it first to make sure that there is only one regularizer at one time . Since it is a SGD method , the dropout on M is the same as Algorithm 2 and Algorithm 3 . We denote dropout as the structured Frobenius norm on SPML as “ SPML M1 ” and dropout as the structured L1 norm as “ SPML M2 ” . Instead of randomly selecting one triplet at each iteration , SPML samples b triplets at one time and updates according to the batch of the gradient . Therefore , when applying dropout to the training data , we simply perform the dropout on different matrix A in the mini batch as in Algorithm 5 and the method is denoted as “ SPML D ” .
Table 4 summarizes the results for wrapped SPML methods . First , it is not surprising to observe that all dropout strategies improve the performance of SPML . On almost all datasets , the improvement on accuracy is more than 1 % and it is even about 3 % on protein , which is also consistent with the observation in the comparison for various SGD methods . Although SPML applies the standard Frobenius norm as the regularizer , SPML with different dropout strategies outperforms it significantly according to the statistical sig
Figure 4 : Procedure of wrapping dropout in OASIS . nificance examined via pairwise t tests at the 5 % significance level , which shows the superior performance of the proposed structured regularizers .
Then , we wrap dropout in OASIS , which is a state of art online learning method for DML . Since online learning has the similar process as stochastic gradient descent method , wrapping dropout in is pretty straightforward . Figure 4 illustrates the procedures of wrapping different dropout strategies in OASIS . Let “ OASIS M1 ” , “ OASIS M2 ” , “ OASIS D ” denote dropout as the structured Frobeniuse norm , the structured L1 norm and the trace norm in OASIS , respectively . The comparison of classification accuracy applied by 3 NN is summarized in Table 5 . The similar phenomenon as for SPML is observed , that dropout always helps to improve the performance of OASIS significantly according to pairwise ttest at the 5 % significance level .
In summary , wrapping dropout in existing DML methods is not only convenient but also very helpful for performance improvement .
5 . CONCLUSION
In this paper , we propose two strategies to perform dropout for DML , ie , dropout in the learned metric and dropout in the training data . For dropout in the metric , we propose the structured regularizer , which is simulated with dropout by assigning different dropout probabilities for the diagonal elements and those living in off diagonal . For dropout in the training data , the data dependent dropout probability is adopted to mimic the trace norm . We develop the theoretical guarantees for both dropout scenarios to show that dropout will not affect the convergence rate of SGD with the appropriate dropout probability . Furthermore , we demonstrate that the proposed strategies are very convenient to wrap in the existing DML methods . Our empirical study confirms that the proposed methods have the overwhelming performance compared with the baseline methods , and could significantly improve the classification accuracy for the state of art DML methods . Since we currently apply dropout to the learned metric and the training data separately , we plan to examine the performance of the combination in the near future .
Acknowledgment Qian and Jin are supported by ARO ( W911NF 11 1 0383 ) , NSF ( IIS 1251031 ) and ONR Award ( N000141410631 ) . Hu and Pei ’s research is supported in part by a NSERC Discovery Grant and a BCIC NRAS Team project . All opinions , findings , conclusions and recommendations in this paper are those of the authors and do not necessarily reflect the views of the funding agencies .
6 . REFERENCES [ 1 ] P . Baldi and P . J . Sadowski . Understanding dropout .
In NIPS , pages 2814–2822 , 2013 .
[ 2 ] C . M . Bishop . Training with noise is equivalent to tikhonov regularization . Neural computation , 7(1):108–116 , 1995 .
[ 3 ] E . J . Cand`es and T . Tao . The power of convex relaxation : near optimal matrix completion . IEEE Transactions on Information Theory , 56(5):2053–2080 , 2010 .
[ 4 ] C C Chang and C J Lin . Libsvm : A library for support vector machines . ACM TIST , 2(3):27 , 2011 .
[ 5 ] H . Chang and D Y Yeung . Locally linear metric adaptation for semi supervised clustering . In ICML , pages 153–160 , 2004 .
[ 6 ] G . Chechik , V . Sharma , U . Shalit , and S . Bengio .
Large scale online learning of image similarity through ranking . JMLR , 11:1109–1135 , 2010 .
[ 7 ] J . V . Davis , B . Kulis , P . Jain , S . Sra , and I . S .
Dhillon . Information theoretic metric learning . In ICML , pages 209–216 , 2007 .
[ 8 ] H . Do , A . Kalousis , J . Wang , and A . Woznica . A metric learning perspective of svm : on the relation of lmnn and svm . In AISTATS , pages 308–317 , 2012 . [ 9 ] A . Frank and A . Asuncion . UCI machine learning repository , 2010 .
[ 10 ] G . Griffin , A . Holub , and P . Perona . Caltech 256 object category dataset , 2007 .
[ 11 ] E . Hazan , A . Agarwal , and S . Kale . Logarithmic regret algorithms for online convex optimization . Machine Learning , 69(2 3):169–192 , 2007 .
[ 12 ] E . Hazan and S . Kale . Beyond the regret minimization barrier : an optimal algorithm for stochastic strongly convex optimization . In COLT , pages 421–436 , 2011 .
[ 13 ] X . He , W Y Ma , and H . Zhang . Learning an image manifold for retrieval . In ACM Multimedia , pages 17–23 , 2004 .
Receive a tripletCheck hinge lossReceive a tripletUpdate if misclassifyReceive a tripletIf misclassifyReceive a tripletUpdate if misclassifyDropout on MUpdate if hinge loss is positiveDropout on MDropout on AUpdateOASISOASIS M1OASIS M2OASIS D [ 14 ] G . E . Hinton , N . Srivastava , A . Krizhevsky ,
I . Sutskever , and R . Salakhutdinov . Improving neural networks by preventing co adaptation of feature detectors . CoRR , abs/1207.0580 , 2012 .
[ 15 ] B . Kulis . Metric learning : A survey . Foundations and
Trends in Machine Learning , 5(4):287–364 , 2013 .
[ 16 ] D . K . H . Lim , B . McFee , and G . Lanckriet . Robust structural metric learning . In ICML , 2013 . [ 17 ] K . Matsuoka . Noise injection into inputs in back propagation learning . IEEE Transactions on Systems , Man , and Cybernetics , 22(3):436–440 , 1992 .
[ 18 ] Y . Nesterov et al . Gradient methods for minimizing composite objective function , 2007 .
[ 19 ] S . Rifai , X . Glorot , Y . Bengio , and P . Vincent . Adding noise to the input of a model trained with a regularized objective . CoRR , abs/1104.3250 , 2011 .
[ 20 ] K . Saenko , B . Kulis , M . Fritz , and T . Darrell .
Adapting visual category models to new domains . In ECCV , pages 213–226 . Springer , 2010 .
[ 21 ] P . Sen , G . Namata , M . Bilgic , L . Getoor , B . Gallagher , and T . Eliassi Rad . Collective classification in network data . AI Magazine , 29(3):93–106 , 2008 .
[ 22 ] S . Shalev Shwartz , Y . Singer , and A . Y . Ng . Online and batch learning of pseudo metrics . In ICML , 2004 .
[ 23 ] B . Shaw , B . C . Huang , and T . Jebara . Learning a distance metric from a network . In NIPS , pages 1899–1907 , 2011 .
[ 24 ] L . van der Maaten , M . Chen , S . Tyree , and K . Q . Weinberger . Learning with marginalized corrupted features . In ICML , pages 410–418 , 2013 .
[ 25 ] S . Wager , S . Wang , and P . Liang . Dropout training as adaptive regularization . In NIPS , pages 351–359 , 2013 .
[ 26 ] K . Q . Weinberger and L . K . Saul . Distance metric learning for large margin nearest neighbor classification . JMLR , 10:207–244 , 2009 .
[ 27 ] E . P . Xing , A . Y . Ng , M . I . Jordan , and S . J . Russell . Distance metric learning with application to clustering with side information . In NIPS , pages 505–512 , 2002 .
[ 28 ] L . Yang and R . Jin . Distance metric learning : a comprehensive survery . 2006 .
[ 29 ] M . Zinkevich . Online convex programming and generalized infinitesimal gradient ascent . In ICML , pages 928–936 , 2003 .
APPENDIX A . PROOF OF THEOREM 1
Proof .
F
F = ˆMt−1 − ηAt − M∗2
Mt − M∗2 = δMt−1−(1 − q)Mt−1 +Mt−1−ηAt−M∗−qMt−12 = ( δ−(1 − q))2Mt−12 +q2Mt−12 +2(δ − ( 1 − q))Mt−1 , Mt−1 − ηAt − M∗ − qMt−1 −2qMt−1 , Mt−1 − ηAt − M∗
F − 2ηAt , Mt−1 − M∗
F +Mt−1−M∗2
F +η2At2
F
F
Since the loss function is convex , we have ( Mt−1 − M∗2
( Mt−1)−(M∗ ) ≤ 1 2η
,(δ − ( 1 − q))2Mt−12
F −Mt − M∗2 F ) F + q2Mt−12
F
At2
η 2
1 2η
F +
+ +2(δ − ( 1 − q))Mt−1 , Mt−1 − ηAt − M∗ − qMt−1 −2qMt−1 , Mt−1 − ηAt − M∗ )
Taking expectation on δ , we have
E[(Mt−1)]−(M∗ ) ≤ 1 F −Mt−M∗2 ( Mt−1−M∗2 F ) 2η F −2qMt−1 , Mt−1−ηAt−M∗ ) ( qMt−12
At2
+
F ) +
1 2η
F + F − Mt − M∗2 ( Mt−1 − M∗2 ( 2Mt−1 , ηAt + M∗ − Mt−12 F ) ( Mt−1 − M∗2 F − Mt − M∗2 ( ηAt + M∗2 F ) ( Mt−1 − M∗2
F − Mt − M∗2 F )
+
η 2 ≤ 1 2η q 2η ≤ 1 2η q 2η ≤ 1 2η ( 1 + q)η
+
At2
F +
M∗2
F + q(M∗ )
F ) +
+
2 q 2η
At2
F
At2
F
η 2
η 2
Since |x| ≤ r , AtF ≤ 4r . Adding iterations from 1 to T and setting q = 1/T , we have E[( ¯M ) ] − ( 1 − 1/T )(M∗ ) ≤ 1 ηT
F + 8r2(1 + 1/T )η
M∗2
B . PROOF OF THEOREM 2
Proof .
F = Mt−1 − η ˆAt − M∗2
Mt − M∗2 = Mt−1 − M∗2
F + η2 ˆAt2 Taking expectation on ˆAt , we have
F
F − 2η ˆAt , Mt−1 − M∗
Mt − M∗2 = Mt−1−M∗2
F
F +η2E[ ˆAt2
F ]−2ηAt + qI , Mt−1−M∗
Since the loss function is convex , it is
F −Mt−M∗2 F )
E[(Mt−1)]−(M∗ ) ≤ 1 2η
( Mt−1−M∗2 F ] + q(M∗tr − Mt−1tr ) F − Mt − M∗2 F )
E[ ˆAt2 ( Mt−1 − M∗2
( 16 + 16dq + 8q ) + qM∗tr
η + 2 ≤ 1 2η ηr2 2
+ where q ’s high order items are omitted since q is a small number . Sum over the iteration from 1 to T , we have
E[( ¯M ) ] − ( M∗ ) ≤ M∗2
F
2ηT
+ ηr2(8 + 8dq + 4q ) + qM∗tr
The proof is finished by setting q = 1/T .
