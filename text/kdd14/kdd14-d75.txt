Learning Time Series Shapelets
Josif Grabocka , Nicolas Schilling , Martin Wistuba , Lars Schmidt Thieme
Information Systems and Machine Learning Lab
University of Hildesheim
Samelsonplatz 22 , Hildesheim 31141 , Germany
{ josif , schilling , wistuba , schmidt thieme }@ismlluni hildesheimde
ABSTRACT
Shapelets are discriminative sub sequences of time series that best predict the target variable . For this reason , shapelet discovery has recently attracted considerable interest within the time series research community . Currently shapelets are found by evaluating the prediction qualities of numerous candidates extracted from the series segments . In contrast to the state of the art , this paper proposes a novel perspective in terms of learning shapelets . A new mathematical formalization of the task via a classification objective function is proposed and a tailored stochastic gradient learning algorithm is applied . The proposed method enables learning nearto optimal shapelets directly without the need to try out lots of candidates . Furthermore , our method can learn true top K shapelets by capturing their interaction . Extensive experimentation demonstrates statistically significant improvement in terms of wins and ranks against 13 baselines over 28 time series datasets .
1 .
INTRODUCTION
Time series research has attracted significant interest within the data mining community , due to the fact that series data are present in a wide range of real life domains . Time series data often exhibit inter class differences in terms of small sub sequencies rather than the full series structure [ 17 ] . A recently introduced concept , named shapelet , represents a maximally discriminative sub sequence of time series data . Stated more directly , shapelets identify short discriminative series segments [ 17 , 11 ] . Apart from their high prediction accuracy , shapelets also offer interpretable features to domain experts . Moreover , discovering shapelets has been a hot topic in the time series domain during the last five years [ 17 , 11 , 10 , 19 , 8 , 12 , 9 ] .
State of the art methods discover shapelets by trying a pool of candidate sub sequences from all possible series segments [ 17 , 10 ] and then sorting the top performing segments according to their target prediction qualities . Distances between series and shapelets represent shapelet transformed [ 10 ] classification features for a series of segregation metrics , such as information gain [ 17 , 11 ] , FStat [ 8 ] or Kruskall Wallis [ 9 ] . The brute force candidates search approach , based on an exhaustive search of candidates , suffers from a high runtime complexity , therefore several speed up techniques have aimed at reducing the discovery time of shapelets [ 11 , 12 , 1 ] . In terms of classification performance , the shapelet transformation method constructs qualitative predictors for standard classifiers and has recently shown improvements with respect to prediction accuracy [ 10 , 8 ] .
This paper proposes an entirely new perspective on time series shapelets . For the first time , we propose a mathematical formulation of the shapelet learning task as an optimization of a classification objective function . Furthermore , we propose a learning method that learns ( not searches for ) the shapelets which optimize the objective function . Concretely , we learn shapelets whose distances to series can linearly separate the time series instances by their targets , as shown in Figure 1 . In comparison to existing approaches , our method can learn near to optimal shapelets and true top K shapelet interactions . In a large pool of 28 datasets we demonstrate that the proposed method yields a large and statistically significant improvement over 13 baselines .
2 . RELATED WORK
Shapelets were first proposed by [ 17 ] as time series segments that maximally predict the target variable . All possible segments were considered as potential candidates , while the minimum distances of a candidate to all training series were used as a predictor feature for ranking the information gain accuracy of that candidate on the target variable . Other quality metrics have been proposed for evaluating the prediction accuracy of a shapelet candidate such as F Stats [ 10 ] , Kruskall Wallis or Mood ’s median [ 8 ] . In addition , the minimum distance of a set of shapelets to time series can be perceived as a data transformation [ 10 ] , while standard classifiers have achieved high accuracy over the shapelet transformed representation [ 8 ] .
Due to the high number of candidates , the runtime of brute force shapelet discovery is not feasible . Therefore , a series of speed up techniques such as early abandoning of distance computations and entropy pruning of the information gain metric have been proposed [ 17 ] . Other speed ups rely on the reuse of computations and pruning of the search space [ 11 ] , as well as exploiting projections on the SAX representation [ 12 ] . Alternatively , the training time has been reduced by elaborating the usage of infrequent shapelet candidates [ 7 ] . Moreover , hardware based optimization have assisted the discovery of shapelets using GPUs [ 1 ] . Shapelets have been applied in a series of real life applications . Unsupervised shapelets have also been utilized for clustering time series [ 19 ] . Shapelets have been found useful for identifying humans through their gait data [ 13 ] . Gesture recognition is another application domain that has benefited from the discovery of shapelets [ 5 , 6 ] . In the domain of
1 S
2 S
1
0
−1
2
0
−2
0 30
0 30
2
0
1 T
−2
0
2
0
−2
0
2 T
100
200
100
200
2
0
3 T
−2
0
2
0
−2
0
4 T
100
200
100
200
Shapelet−transformed data
0.09
T3
0.06
0.03
2 | | 2 S −
∗ T
| |
0.1
T4
T2
T1
0.14
0.18
||T∗ − S1||2
Figure 1 : An illustration of two shapelets S1 , S2 ( leftmost plots ) learned on the Coffee dataset . Series’ distances to shapelets can optimally project the series into a 2 dimensional space , called the shapelet transformed representation [ 10 ] ( rightmost plot ) . The middle plots show the closest matches of the shapelets on series of two classes having light blue and black colors . medical and health informatics , interpretable shapelets have been used to enable efficient early classification of time series [ 16 , 15 ] . In comparison to the state or the art methods , we propose a novel method that learns near to optimal shapelets directly , without the need to search exhaustively among a pool of candidates extracted from time series segments .
3 . PROPOSED METHOD
3.1 Definitions and Notations
311 Time Series Dataset
A time series dataset is composed of I training instances and for notation ease we assume that each series contains Q many ordered values , even though our method can operate on variable series lengths . The dataset is defined as T I×Q , while the series target is a nominal variable Y ∈ {1 , . . . , C}I having C categories .
312 Sliding Window Segment
A sliding window segment of length L is an ordered sub sequence of a series . Concretely , the segment starting at time j inside the i th series is defined as ( Ti,j , . . . , Ti,j+L−1 ) . There are totally J := Q − L + 1 segments in a time series provided the starting index of the sliding window is incremented by one .
313 Shapelets
A shapelet of length L is simply an ordered sequence of values from a data structure perspective . Nevertheless , shapelets semantically represent intelligence on how to discriminate the target variable of a series dataset . The K most informative shapelets are denoted as S ∈ RK×L .
314 Distances Between Shapelets and Series
The distance between the i th series Ti and the k th shapelet Sk is defined as the minimum distance Mi,k ( shown in Equation 1 ) among the distances between the shapelet Sk and each segment j of Ti [ 17 , 18 ] . Informally speaking , it is the distance of a shapelet to the most similar series segment , as shown in Figure 1 .
Mi,k = min j=1,,J
1 L
L
Xl=1
( Ti,j+l−1 − Sk,l)2
( 1 )
315 Shapelet Transformation
Minimum distances to shapelets can be characterized as a transformation of the time series data T ∈ RI×Q into a new representation M ∈ RI×K [ 10 ] . Such a transformation reduces the dimensionality of the original time series , because typically K < Q .
General purpose classifiers ( eg : SVMs , Bayesian Network , . . . ) have been recently shown to achieve high prediction accuracy over the new representation M [ 8 ] .
316 Logistic Sigmoid Function
The logistic sigmoid function is an S shaped instance of the lo . We are going to use the sigmoid function for the prediction of target variables via a logistic regression loss . gistic function and is defined as σ(Y ) = ,1 + e−Y −1
3.2 A Novel Principle
In this paper we propose a novel principle in learning time series shapelets . Instead of searching among possible shapelet candidates from the series segments [ 17 , 10 ] , we propose a formal method that can directly learn optimal shapelets without needing to explore all possible candidates . Our principle can be summarized in two steps : ( i ) Start with rough initial guesses for the shapelets , ( ii ) Iteratively learn/optimize the shapelets by minimizing a classification loss function . In order to conduct the shapelet optimization , we define a novel classification model that is differentiable with respect to shapelets . Therefore , shapelets can be updated in a stochastic gradient descent optimization fashion , by taking steps towards the minimum of the classification loss function ( ie towards maximal prediction accuracy ) .
3.3 Objective Function
For the sake of simplicity , the model introduced in this Section will be focused only on binary targets Y ∈ {0 , 1}I and a fixed shapelet length L . A general version of the model , with extended properties , is described Section 5 .
331 Learning Model
Since the minimum distances M are the new predictors in the transformed shapelets space , a linear learning model can predict approximate target values ˆY ∈ RI×K via the predictors M and linear weights W ∈ RK ( plus bias W0 ∈ R ) , as shown in Equation 2 .
ˆYi = W0 +
K
Xk=1
332 Loss Function
Mi,kWk ,
∀i ∈ {1 , . . . , I}
( 2 )
In this paper we are going to exploit the logistic regression classification model , because it provides an option to interpret predicted binary targets as probabilistic confidences . Such a probabilistic interpretation will ensure extending our approach to the multi class case in Section 5 . The logistic regression operates by minimizing the logistic loss , defined in Equation 3 , between true targets Y and estimated ones ˆY .
L(Y , ˆY ) = −Y ln σ( ˆY ) − ( 1 − Y ) ln,1 − σ( ˆY )
333 Regularized Objective Function
( 3 )
The logistic loss function together with regularization terms represent the regularized objective function , denoted as F in Equation 4 . The idea of this paper is to jointly learn the optimal shapelets S and the optimal linear hyper plane W that minimize the classification objective F . argmin
F ( S , W ) = argmin
S,W
S,W
I
Xi=1
L(Yi , ˆYi ) + λW ||W ||2
( 4 )
3.4 Differentiable Soft Minimum Function
In order to compute the derivative of the objective function , all the involved functions of the model need to be differentiable . Unfortunately , the minimum function of Equation 1 is not differentiable and the partial derivative ∂M ∂S is not defined . A differentiable approximation to the minimum function is introduced in this section .
For the sake of organizational clarity we will introduce the distance between the j th segment of series i and the k th shapelet as Di,k,j and define it in Equation 5 .
Di,k,j
:=
1 L
L
Xl=1
( Ti,j+l−1 − Sk,l)2
( 5 )
A differentiable approximation of the minimum function is the popular Soft Minimum function that is depicted in Equation 6 . A parameter α controls the precision of the function and the soft minimum approaches the true minimum for α → −∞ .
Mi,k ≈ ˆMi,k = PJ j=1 Di,k,j eα Di,k,j PJ j ′=1 eα Di,k,j ′
( 6 )
Please note that the soft minimum has the shapelets as the only varying input , which appear embedded inside the distance definition D . We would like to explain the operating principle of the soft minimum with the aid of Figure 2 .
A series from the FaceFour dataset and a shapelet are depicted in the upper plot of Figure 2 . The shapelet is a slightly distorted variant of the series segment starting at time index 51 . If we slide the shapelet over all the series segments and record the distance of shapelets to segments ( ie Equation 5 ) , then the Euclidean distances’ plot in blue is achieved . Two plots in red ( bottommost ) illustrate the operation of the soft minimum function . Each point j of the soft minimum plots correspond to
Di,k,j e
α Di,k,j i,k,j ′ , while the
α D e j ′=1
PJ area under the soft minimum plots sums up to the true minimum distance between the shapelet and the series ( ie Equation 1 ) . It is important to realize in the third plot ( α = −20 ) that the amount by which a segment distance impacts the overall minimum is directly related to how small is that segment ’s distance compared to other segment distances . As can be seen in the bottom plot , if α = −100 , then only the true minimum segment distance is allowed to contribute to the grand total minimum . We found out that α = −100 is small enough to make the soft minumum yield exactly the same
5 2.5 0 −2.5 −5
1
0.5 x 10
−3 x 10
−3
0
6 4 2 0
2
1
0
0
Series
Shapelet
50
100
150
200
250
300
350
Euclidean
50
100
150
200
250
300
350
Soft−min , alpha=−20
50
100
150
200
250
300
350
Soft−min , alpha=−100
50
100
150
200
250
300
350
Figure 2 : Illustration of the soft minimum between a shapelet ( green ) and all the segments of a series ( black ) from the FaceFour dataset results as the true minimum . Therefore , we kept this value fixed throughout all our experiments .
3.5 Per Instance Objective
The optimization we will adopt in this paper is a stochastic gradient descent approach that remedies the classification error caused by one instance at a time . The Equation 7 demonstrates the decomposed objective function Fi , which corresponds to a division of the objective of Equation 4 into per instance losses for each time series .
Fi = L(Yi , ˆYi ) +
λW I
K
Xk=1
2
Wk
( 7 )
3.6 Gradients for Shapelets
The learning algorithm requires the definition of the gradients of the objective function with respect to the shapelets . The gradient of point l in shapelet k with respect to the objective of the i th time series is defined in Equation 8 and is derived through the chain rule of derivation .
∂Fi ∂Sk,l
=
∂L(Yi , ˆYi )
∂ ˆYi
∂ ˆYi ∂ ˆMi,k
J
Xj=1
∂ ˆMi,k ∂Di,k,j
∂Di,k,j ∂Sk,l
( 8 )
Furthermore , the gradient of the loss with respect to the predicted target is defined in Equation 9 , while the gradient of the minimum distances with respect to the estimated target is shown in Equation 10 .
∂L(Yi , ˆYi )
∂ ˆYi
∂ ˆYi ∂ ˆMi,k
= −,Yi − σ , ˆYi
= Wk
( 9 )
( 10 )
In addition , the gradient of the overall minimum distance with respect to a segment distance is presented in Equation 11 and the gradient of a segment distance with respect to a shapelet point is derived in Equation 12 .
Algorithm 1 Learning Time Series Shapelets
Require : T ∈ RI×Q , Number of Shapelets K , Length of a shapelet L , Regularization λW , Learning Rate η , Number of iterations : maxIter
Ensure : Shapelets S ∈ RK×L , Classification weights W ∈ RK ,
1 do
Sk,l ← Sk,l − η ∂Fi ∂Sk,l
Bias W0 ∈ R for k = 1 , . . . , K do Wk ← Wk − η ∂Fi ∂Wk for L = 1 , . . . , L do
1 : for iteration=NmaxIter for i = 1 , . . . , I do 2 : 3 : 4 : 5 : 6 : 7 : 8 : 9 : 10 : 11 : end for 12 : return S , W , W0 end for W0 ← W0 − η ∂Fi end for end for
∂W0
∂ ˆMi,k ∂Di,k,j
∂Di,k,j ∂Sk,l
=
= eα Di,k,j(1+α(Di,k,j − ˆMi,k ) )
PJ j ′=1 eα Di,k,j ′
( Sk,l − Ti,j+l−1 )
2 L
( 11 )
( 12 )
3.7 Gradients for Classification Weights
The hyper plane weights W can also be learned to minimize the classification objective via stochastic gradient descent . Equation 13 shows the partial gradient of updating each weight Wk and Equation 14 presents the bias term W0 .
∂Fi ∂Wk ∂Fi ∂W0
= −,Yi − σ , ˆYi ˆMi,k + = −,Yi − σ , ˆYi
2λW
I
Wk
( 13 )
( 14 )
3.8 Learning Algorithm
After having derived the gradients of the shapelets and the weights , we can introduce the overall learning algorithm . Our approach iterates in a series of epochs and updates the values of the shapelets and weights in the negative direction of the derivative with respect to the classification objective of each training instance .
The steps of the learning process are shown in Algorithm 1 . The pseudo code iterates over all training instances I and updates all K shapelets S and the weights W , W0 by a learning rate η .
3.9 Convergence
The convergence of Algorithm 1 depends on two parameters , the learning rate η and the maximum number of iterations . High values for the learning rate can minimize the objective in less iterations , but pose the risk of divergence , while small learning rates require more iterations . Subsequently , the learning rate and the number of iterations should be learned via cross validation from the training data .
For instance Figure 3 illustrates the convergence of the learning algorithm on the Coffee dataset for 57 shapelets . Both the training and the testing loss converge very smoothly for η = 001 As a consequence of optimizing the training loss , the train and test errors also decrease simultaneously . In addition , the regularization a ) Convergence of Loss b ) Convergence of Error s s o L
20
15
10
5
0
0.6
0.4
0.2 r o r r
E
0
0
Train Loss Test Loss
1000
2000
Iterations
Train Error Test Error
1000
2000
Iterations
Figure 3 : Convergence of the Coffee dataset , Parameters : K = 57 , L = 143 , η = 0.01 , λW = 0.001 , α = −100 weight λW = 0.001 ensures that the train and test loss have small differences , which can be interpreted as a generalization quality without any over fitting effect .
3.10 Model Initialization
Equation 4 is a non convex function in terms of S and W because both are variables that need to be learned . Gradient descent techniques do not theoretically guarantee the discovery of global optima in non linear functions . Unfortunately , non convex optimization techniques are very slow for data mining problems , therefore gradient based approaches are often selected as a compromise between feasibility and optimality [ 14 ] .
Gradient descent optimization requires a good initialization of the parameters when applied to non convex functions . In other words , if the initialization starts the learning around a region where the global optimum is located , then the gradient can update the parameters to the exact location of the optimum .
Initialization can influence a gradient based technique significantly . We are going to illustrate the sensitivity of shapelets initialization through an experiment shown in Figure 4 . For the sake of two dimensional illustration , we initialized one shapelet ( S ) in the Gun Point dataset using two values . The first 15 points of a 30 points long shapelet ( S1:15 ) were given a fixed initial value , while the other half points of the shapelet S16:30 were initialized with another fixed value . Figure 4 demonstrate that different initial values of the shapelet can result in different loss values and error rates over the training instances .
1
0.5
0
0 3 6 1
:
S
−0.5
−1
−1 a ) Train Loss b ) Train Error
40
1
0.18
35
30
25
20
0
1
0.5
0
0 3 6 1
:
S
−0.5
−1
−1
S
1:15
0.16
0.14
0.12
0.1
0.08
0.06
0.04
0.02
0
1
S
1:15
Figure 4 : Sensitivity of Shapelet Initialization , Gun Point dataset , Parameters : L = 30 , η = 0.01 , λW = 0.01 , Iterations= 3000 , α = −100
In order to robustify the initialization guesses we use the KMeans centroids of all segments as initial values for the shapelets . Since centroids represent typical patterns of the data , they offer a good variety of shapes for initializing shapelets and help our method achieve high prediction accuracy . The initialization is conducted before Algorithm 1 starts , while W is also initialized randomly around 0 .
3.11
Illustrating The Mechanism
An illustration of the learning algorithm is depicted in Figure 5 . Two shapelets of length 40 are learned from the Gun Point dataset . Sub figure a ) demonstrates the initialization values of the shapelets and the arrangement of the minimum values of the time series to shapelets . As can be seen , a linear hyper plane W cannot easily separate the two classes . After 400 iterations of our method , the shapelets are updated as shown in sub figure b ) . In addition , the shapelet transformed data representation M becomes almost linearly separable with few exceptions . Finally , the algorithm approaches convergence in sub figure c ) after 800 iterations . The linear hyper plane W separates the shapelet transformed instances of the binary dataset with just a single error ( in red ) .
4 . ANALYSIS OF OUR METHOD
4.1 Algorithmic Complexity
The baseline method which exhaustively tries candidates from series segments [ 17 , 10 ] requires O(I 2Q3 ) running time for discovering the best shapelet of a particular length Q . On the other hand , our method requires O(IQ2 × maxIter ) , therefore our algorithm finds the best shapelet in a faster time , given that usually maxIter << IQ .
4.2 Comparison to State of the Art
421 Learning Near To Optimal Shapelets
The optimal solution of Equation 4 gives the optimal shapelets , while a gradient descent approach can find a near to optimal minimum given an appropriate initialization .
The baseline approaches , on the other hand , provide no guarantee of optimal solutions for two primary reasons . First of all , the baselines are bound to shapelet candidates from the pool of series segments and cannot explore candidates which do not appear literally as segments . Secondly , minimizing the classification objective through candidate guesses has no guarantee of optimality , while a gradient based optimization guarantees at least near to optimal minima .
422 Capturing Interactions Among Shapelets
The baselines find the score of each shapelet independently and then sort the individual quality of each shapelet , in order to select the top performers . However , such an approach does not take into account interactions among patterns . In other words , two shapelets can be individually sub optimal , but when combined together they can improve the results . In fact , this problem is well known in data mining and referred to as variable subset selection [ 2 ] .
0
0
1
2
3
4
5
M 1
0
0
1
2
3
4
5
M 2
2
M
6
5
4
3
2
1
0
0
Pos . Class Neg . Class
1
2
3
4
5
M 1
Figure 6 : Interactions among Shapelets Enable Individually Unsuccessful Shapelets ( left plots ) to Excel in Cooperation ( right plot )
For instance , Figure 6 demonstrates an example on how interactions among shapelets can become a game changing factor . On the left plots , we show the minimum distances of series to two shapelets . As can be observed , the individual discriminative quality of the shapelets is poor . On the other hand , a simple 2 dimensional interaction of exactly the same distances M1 , M2 can yield drastically improved results , as shown on the right plot . When combined together , the distances to those shapelets can create a linearly separable discrimination , ie a perfect classification accuracy .
If the baseline ’s exhaustive discovery approach would attempt to select the true top K interaction of shapelets out of I(Q − L + 1 ) candidates , then it will need to check the interaction of :
I(Q − L + 1 )
K
=
( I(Q−L+1))!
K!(I(Q−L+1)−K)! many combinations of candidates . For instance finding the true top 100 shapelets of length 30 from the Adiac dataset with I = 390 and Q = 176 requires checking 3.42 × 10317 combination trials using the baseline ’s approach . Clearly , the exhaustive search baseline cannot find true top K shapelet interactions within a feasible time frame . On the contrary , our method can find the interactions at a simple linear scale K , due to the property of jointly learning the shapelets and their interactions .
423 Weaker Aspects of Our Paper
Our method relies on more hyper parameters than the baselines , such as the learning rate η , the number of iterations , the regularization parameter λW and the soft min precision α . However , given the high prediction accuracy that will be demonstrated in Section 6 , we argue that the very high accuracy by far out weights the model ’s learning efforts .
The time needed for the baselines to compute the top K shapelets is not significantly large with respect to the time needed to find a single shapelet . Such a behavior comes from the fact that the quality of each candidate is known and ready in the end of the discovery . On the other hand , our method needs K many time units for K shapelets , wrt learning one shapelet . However , such a disadvantage in time for large K is well spent in terms of accuracy , because our method can learn true top K shapelet interactions and significantly improve the classification accuracy . Moreover , we believe that our method may yield to further improvements in efficiency by exploiting "early abandoning" and caching of partial results ( as in [ 7] ) . For brevity we ignore such issues here and focus on forcefully demonstrating the improvements in accuracy .
5 . LEARNING GENERAL SHAPELETS
The model presented in Section 3.3 can be generalized to multiclass labels and multi size shapelets . Basically the model is extended to classify multi class targets and capture interactions among shapelets of various sizes .
5.1 Decomposition of the Multi Class Problem
Into One vs all Subproblems
In order to learn from multi class targets Y ∈ {1 , . . . , C}I with C categories , we will convert the problem into C many one vsall sub problems . Each sub problem will discriminate one class against all the others . The one vs all binary targets Y b ∈ {0 , 1}I×C are defined in Equation 15 .
Y b i,c = fl1 Yi = c
0 Yi 6= c
, ∀i ∈ {1 , . . . , N } , ∀c ∈ {1 , . . . , C} ( 15 ) a ) Iteration 0 b ) Iteration 400 c ) Iteration 800
Y=0 Y=1 W
2
M
0.7
0.5
0.3
0.1
0.2
0.4
S ( It . 400 ) 1
Y=0 Y=1 W
2
M
0.8
0.6
0.4
0.2
0.8
0.6
M 1
S ( It . 400 ) 2
Y=0 Y=1 W
0.2
0.53
S ( It . 800 ) 1
M 1
0.87
1.2
S ( It . 800 ) 2
1
−0.5
−2
2
0
−2
1
−0.5
−2
2
0
−2
0.6
0.8
S ( It . 0 ) 2
0.2
0.4
S ( It . 0 ) 1
M 1
1.5
0
−1.5
2
M
0.6
0.4
0.2
0
0.3
0
−0.3
5 15 25 35
5 15 25 35
5 15 25 35
5 15 25 35
5 15 25 35
5 15 25 35
Time
Time
Time
Time
Time
Time
Figure 5 : Learning Two Shapelets on the Gun Point Dataset : Parameters L = 40 , η = 0.01 , λW = 0.01 , α = −100
In fact , the conversion to one vs all sub problems will be useful for the operation of the logistic regression classifier . The output of the logistic regression for a binary problem can be perceived as a confidence probability . Therefore , the index of the most confident among the C many classifiers is selected as the predicted categorical value of a test instance .
5.2
Interactions among Shapelets having Various Lengths
Capturing interactions among shapelets having various lengths is another aspect of the extended method . Our generalized model learns R different scales of shapelet lengths starting at a minimum Lmin as {Lmin , 2Lmin , . . . , RLmin} . The shapelets therefore will be , and represent K many shapelets for each scale R , ie totally KR shapelets . The length of a shapelet at scale r ∈ {1 , . . . , R} is r · Lmin . Consequently , the number of segments in a time series depends on the scale of the shapelet ’s length to be matched against and is J(r ) = Q − r · Lmin + 1 . defined as S ∈ RR×K×∗ , where S ∈ SR
RK×rLmin r=1 totically smaller values than J(· ) , the big O notation complexity is O(J(·) ) .
ˆYt ← argmax c∈{1,,C}
σ , ˆY b t,c ,
∀t ∈ {1 , . . . , I T est} ( 18 )
5.5 Generalized Soft Minimum
The soft minimum function can be trivially generalized to include the notation for the scales r as shown in Equation 19 . The distance between the k th shapelet at scale r and the j th segment of time series i is denoted as Dr,i,k,j in Equation 20 .
Mr,i,k ≈ ˆMr,i,k = PJ(r ) j=1 Dr,i,k,j eα Dr,i,k,j PJ(r ) j ′=1 eα Dr,i,k,j ′ r·Lmin
( 19 )
Dr,i,k,j =
1 r · Lmin
( Ti,j+l−1 − Sr,k,l)2
( 20 )
Xl=1
5.3 Generalized Objective Function
The objective function of the generalized model is presented in Equation 16 , which is a regularized logistic regression loss between the true targets and the predicted ones shown in Equation 17 . The notation Mr,i,k identifies the minimum distance of the i th series to the k th shapelet of scale r , ie to Sr,k ∈ Rr·Lmin . In addition , the weight Wc,r,k identifies the class c classifier and the weight of the k th shapelet at scale r .
5.6 Gradients of Generalized Objective Func tion
The objective function can be split per each instance i and the loss of each one vs all classifier c and denoted in Equation 21 as Fi,c . argmin
F =
S,W
I
C
Xi=1
Xc=1
ˆY b i,c = Wc,0 +
R
K
Xr=1
Xk=1
L(Y b i,c , ˆY b i,c ) + λW ||W ||2 ( 16 )
Fi,c = L(Y b i,c , ˆY b i,c ) +
λW IC
R
K
Xr=1
Xk=1
Wc,r,k
2
( 21 )
Mr,i,kWc,r,k
( 17 )
The derivative of the per cell objective Fi,c with respect to each shapelet Sr,k,l is shown in Equation 22 .
561 Shapelet Gradients
5.4 Classification of Test Instances
Once the model is learned , a test instance indexed t is classified as the one vs all classifier which yields maximum confidence , as presented in Equation 18 . The algorithmic complexity of classifying a test instance is O(CRKJ(·) ) , but since C , R , K are asymp
∂Fi,c ∂Sr,k,l
= −,Y b i,c − σ , ˆY b i,c ∂ ˆMr,i,k
∂Sr,k,l
Wc,r,k
( 22 )
Moreover , the derivative of the minimum distances with respect to the generalized shapelets is defined in Equations 23 24 .
∂ ˆMr,i,k ∂Dr,i,k,j
∂Dr,i,k,j ∂Sr,k,l
=
= eα Dr,i,k,j ,1 + α,Dr,i,k,j − ˆMr,i,k
PJ(r ) j ′=1 eα Dr,i,k,j ′
2 ( Sr,k,l − Ti,j+l−1 ) r · Lmin
Algorithm 2 Generalized Shapelets Learning
( 23 )
( 24 )
Require : Time series T ∈ RI×Q , Binary labels Y b ∈ RI×C , Number of shapelets K , Learn Rate η , Regularization λW , Scales of shapelet lengths R ∈ N , Minimum Shapelet Length Lmin , Number of Iterations : maxIter
Ensure : Shapelets S ∈ RR×K×∗ , Classification weights W ∈
RR×C×K , W0 ∈ RC
562 Classification Weights’ Gradients
The gradients of the per cell objective with respect to the generalized weights and the bias terms are presented in Equations 25 26 .
∂Fi,c ∂Wc,r,k ∂Fi,c ∂Wc,0
= −,Y b = −,Y b i,c − σ , ˆY b i,c − σ , ˆY b i,c ˆMr,i,k + i,c
λW Wc,r,k
IC
( 25 )
( 26 )
563 Optimized Learning Algorithm
Algorithm 2 summarizes all the steps of the learning process . The first section of the procedure pre computes terms which are used frequently in the gradients of the shapelets , such as ξ , D , ψ , ϑ . The pre computations boost the learning time and avoid computing the same terms repeatedly . The second part of the algorithm updates the weights and the shapelets using the defined gradients and the precomputed terms .
6 . EXPERIMENTAL RESULTS
6.1 Setup And Reproducibility
611 Datasets
For the sake of equivalent comparison , we selected exactly the same set of datasets as the closest baselines [ 10 , 8 ] . A large pool of 28 datasets consisting of time series datasets having various numbers of instances , lengths and number of classes is selected and details are shown in Table 1 . In order to ensure a fair comparison with the baselines , we used the default train and test data splits , same as the baselines [ 10 , 8 ] . The datasets are available through the UCR1 and UEA2 websites .
612 Reproducibility and Hyper parameter Search
Our method ( hereafter denoted as LTS , for Learning Time Series Shapelets ) requires the tuning of a series of hyper parameters , which were found through a grid search approach using cross validation over the training data . The number of shapelets was searched in a range of K ∈ {0.05 , 0.15 , 0.3} , which is a fraction of the series length , eg K = 0.3 means 30 % of Q . Similarly , Lmin ∈ {0.025 , 0.075 , 0.125 , 0.175 , 0.2} × 100 % of Q , while three scales of shapelet lengths were searched from R ∈ {1 , 2 , 3} . The regularization parameter was one of λW ∈ {0.01 , 0.1 , 1} . The learning rate was kept fixed at a small value of η = 0.01 , while the number of iterations is selected from maxIter∈ {2000 , 5000 , 10000} . All our method’ parameters for all datasets are shown in Table 1 . The authors are devoted to promote reproducibility , therefore the source code , datasets and instructions are made publicly available3 .
1http://wwwcsucredu/~eamonn/time_series_ data/ 2http://wwwueaacuk/computing/ machine learning/shapelets/shapelet data 3http://fsismllde/publicspace/ LearningShapelets/ for i = {1 , . . . , I} do
1 : Initialize S , W , W0 2 : for iteration={1 , . . . , maxIter} do 3 : 4 : 5 : 6 : for j = {1 , . . . , J(r)} do
{Pre compute Terms} for r = {1 , . . . , R} , k = {1 , . . . , K} do
7 :
8 : 9 : 10 :
11 : 12 : 13 :
14 :
15 : 16 : 17 : 18 : 19 : 20 : 21 :
22 :
23 : 24 :
Dr,i,k,j := 1
( Ti,j+l−1 − Sr,k,l)2 r·Lmin Pr·Lmin l=1
ξr,i,k,j := eα Dr,i,k,j end for
ψr,i,k := PJ(r )
ˆMr,i,k := 1 j=1 ξr,i,k,j
ψr,i,k PJ(r ) end for for c = {1 , . . . , C} do j=1 Dr,i,k,j ξr,i,k,j
σ( ˆY b i,c ) := 1 + e i,c − σ( ˆY b
ϑi,c := Y b i,c )
−PR r=1PK k=1
ˆMr,i,kWc,r,k−1 end for {Learn Shapelets and Classification Weights} for c = {1 , . . . , C} do for r = {1 , . . . , R} , k = {1 , . . . , K} do
Wc,r,k + = η,ϑi,c ˆMr,i,k − 2λW for j = {1 , . . . , J(r)} do
IC
Wc,r,k
2ξr,i,k,j(1+α(Dr,i,k,j − ˆMr,i,k ) )
φr,i,k,j := for l = {1 , . . . , r · Lmin} do r·Lminψr,i,k
Sr,k,l + = η ϑi,c φr,i,k,j×
( Sr,k,l − Ti,j+l−1 ) Wc,r,k end for Wc,0 + = η ϑi,c end for end for
25 : 26 : 27 : 28 : 29 : 30 : 31 : end for 32 : return S , W , W0 end for end for
613 Baselines
Thirteen different baselines were compared against , which are grouped into the following four clusters .
• Shapelet Tree Methods , constructed from shapelets whose qualities are measured using : i ) Information gain quality criterion ( IG ) [ 17 , 11 ] , ii ) Kruskall Wallis quality criterion ( KW ) [ 8 ] , iii ) F Stats quality criterion ( FST ) [ 10 ] and iv ) the Mood ’s Median Criterion ( MM ) [ 8 ] .
• Basic Classifiers [ 10 ] , learned over shapelet transformed data , such as : Nearest Neighbors ( 1NN ) , Naive Bayes ( NB ) and C4.5 tree ( C45 )
• More Complex Classifiers [ 8 ] , learned over shapelet transformed data , such as : Bayesian Networks ( BN ) , Random
Table 1 : Dataset Information , Parameter Search Results and Running Time for The Best Shapelet [ 8 ]
Dataset Information
Dataset
Train/Test
Length Cls .
Adiac Beef Beetle/Fly Bird/Chicken Chlorine . Coffee Diatom . DP_Little DP_Middle DP_Thumb ECGFiveDays FaceFour GunPoint ItalyPower . Lighting7 MedicalImages MoteStrain MP_Little MP_Middle Otoliths PP_Little PP_Middle PP_Thumb SonyAIBO . Symbols SyntheticControl Trace TwoLeadECG
390/391
30/30 20/20 20/20
467/3840
28/28 16/306 400/645 400/645 400/645 23/861 24/88 50/150 67/1029
70/73
381/760 20/1252 400/645 400/645
64/64
400/645 400/645 400/645 20/601 25/995 300/300 100/100 23/1139
176 470 512 512 166 286 345 250 250 250 136 350 150 24 319 99 84 250 250 512 250 250 250 70 398 60 275 82
37 5 2 2 3 2 4 3 3 3 2 4 2 2 7 10 2 3 3 2 3 3 3 2 6 6 4 2
Parameter Values ( LTS )
Lmin R λW maxIter
Run Time ( Best Shap . ) LTS ( Sec ) F Stat ( Sec )
0.2 0.125 0.125 0.075 0.2 0.075 0.175 0.175 0.025 0.175 0.125 0.175 0.2 0.2 0.075 0.2 0.2 0.2 0.2 0.125 0.125 0.175 0.175 0.125 0.175 0.125 0.125 0.075
3 3 1 1 3 2 2 1 3 3 2 3 3 3 3 2 3 3 2 3 3 2 2 2 1 3 2 1
0.01 0.01 0.01 0.1 0.01 0.01 0.01 1 1 0.1 0.01 1 0.1 0.01 1 1 1 0.01 0.01 0.01 1 0.01 0.1 0.01 0.1 0.01 0.1 0.1
10000 10000 5000 10000 10000 5000 10000 5000 10000 5000 5000 5000 10000 5000 5000 10000 10000 5000 5000 2000 10000 10000 10000 10000 5000 5000 10000 10000
4509.91 1251.21 21496.51 20465.63 15681.39 258.15 53.91 78005.7 91208.52 123766.49 149.1 4556.41 569.42 1.75 14912.74 7742.97 10.76 88071.5 134731.54 55874.19 79993.31 57815.02 91401.49 6.73 8901.28 984.36 54128.53 3.12
3017.23 293.68 131.015 81.405 558.51 90.96 173.1 1525.595 910.33 963.765 29.365 386.45 46.69 10.285 394.44 406.725 16.875 965.27 940.555 407.835 890.925 1574.805 1449.36 11.415 308.99 219.97 275.375 15.415
K
0.3 0.15 0.15 0.3 0.3 0.05 0.3 0.15 0.3 0.05 0.05 0.3 0.15 0.3 0.05 0.3 0.3 0.3 0.05 0.15 0.15 0.15 0.3 0.3 0.05 0.15 0.15 0.3
Forest ( RAF ) , Rotation Forest ( ROF ) and Support Vector Machines ( SVM ) .
• Other Related Methods : The Fast Shapelets ( FSH ) [ 12 ] exploits a fast random projection technique on the SAX representation , while the Dynamic Time Warping ( DTW ) classifier on the raw time series data is also selected due to its reputation as a strong similarity metric [ 4 ] .
6.2 Very High Prediction Accuracy
We compared our method of learning shapelets ( denoted as LTS ) against the selected baselines in terms of classification accuracy ratio ( fraction of correct classifications ) as shown in Table 2 . The best method per dataset is highlighted in bold .
Our method LTS has a very large superiority in terms of Absolute Wins ( 17.28 absolute wins in 28 datasets against 13 baselines ) and 1 to 1 wins , as indicated by the respected rows in the end of the table . Each dataset awards one point , which is split into fractions in case of draws . In addition , we compared the ranks of the classifiers and found out that LTS has a significantly better rank of 1.946 ± 0.536 against the closest baseline ’s ( SVM ) rank 4.554 ± 1180 In order to bullet proof our claim , we ran the well known Wilcoxon signed ranks test [ 3 ] against all baselines and found out that all results are statistically significant at p < 0.05 , as can be deduced by the p values of the most bottom row .
6.3 Competitive Running Time
Since the idea of this paper is entirely novel , our first priority is to evaluate its prediction accuracy rather than elaborating on speedup techniques , as in the fast shapelets approach [ 12 ] . Nevertheless , we would like to show that our method is indeed feasible and competitive in terms of running time and faster than the exhaustive candidate search approach [ 10 , 8 ] . We compared the time needed to find the best shapelet of each dataset against the F Stat metric , which is the fastest quality metric [ 10 , 8 ] . The best shapelet runtime comparison is advocated by our baseline [ 8 ] , in order to ensure that methods can process the same number of candidates . As can be seen from Table 1 , our method can learn the shapelet within a faster time ( 57 times faster in average ) compared to the baseline , which is an indication that our method is practically feasible in terms of running time . Each execution of our method searched over five different shapelet sizes {0.025 , 0.075 , 0.125 , 0.175 , 0.2} × Q and the other parameters were set to η = 0.01 , maxIter= 3000 and λW = 0001
7 . CONCLUSION
In this study we introduced a novel perspective into learning time series shapelets . In contrast to related work which searches for top shapelets from a pool of candidates , we propose a novel mathematical formulation of the task via a classification objective function . In addition , we introduced a learning algorithm which learns near to optimal shapelets by exploring shapelet interactions . An extensive experimentation on 28 time series datasets and 13 base s e n i l e s a B 3 1 d n a s t e s a t a D s e i r e S e m T 8 2 i g n i v l o v n i s o i t a R y c a r u c c A
: 2 e l b a T
S T L
H S F
M V S
F O R
F A R
N B
B N
2 4 5 . 0
0 0 8 . 0
0 5 9 . 0
0 0 0 . 1
3 4 7 . 0
0 0 0 . 1
1 5 9 . 0
7 2 7 . 0
8 5 7 . 0
0 4 7 . 0
0 0 0 . 1
0 0 0 . 1
0 0 0 . 1
2 6 9 . 0
7 7 8 . 0
4 3 7 . 0
3 1 9 . 0
8 5 7 . 0
0 8 7 . 0
6 6 7 . 0
0 1 7 . 0
7 6 7 . 0
5 1 7 . 0
2 5 9 . 0
9 5 9 . 0
0 0 0 . 1
0 0 0 . 1
0 0 0 . 1
8 2 . 7 1
6 4 9 . 1
6 3 5 . 0
8 4 4 . 1
8 5 5 . 0
7 1 5 . 0
2 4 7 . 0
3 4 8 . 0
8 7 5 . 0
5 2 9 . 0
9 6 8 . 0
6 6 5 . 0
1 8 5 . 0
0 8 5 . 0
6 9 9 . 0
9 0 9 . 0
2 3 9 . 0
1 2 9 . 0
1 0 6 . 0
8 0 6 . 0
5 8 7 . 0
5 6 5 . 0
5 0 6 . 0
0 6 5 . 0
9 4 5 . 0
0 8 5 . 0
6 3 5 . 0
8 9 6 . 0
0 3 9 . 0
7 1 9 . 0
0 0 0 . 1
2 2 9 . 0
6 2
5 2 . 1
1
1
6 9 1 . 9
9 1 5 . 1
2 0 1 . 4
0 0 0 . 0
8 3 2 . 0
7 6 8 . 0
5 7 9 . 0
0 5 9 . 0
2 6 5 . 0
0 0 0 . 1
2 2 9 . 0
2 5 7 . 0
6 9 7 . 0
8 9 6 . 0
0 9 9 . 0
7 7 9 . 0
0 0 0 . 1
1 2 9 . 0
9 9 6 . 0
5 2 5 . 0
7 8 8 . 0
0 5 7 . 0
9 6 7 . 0
1 4 6 . 0
1 2 7 . 0
9 5 7 . 0
5 5 7 . 0
7 6 8 . 0
6 4 8 . 0
3 7 8 . 0
0 8 9 . 0
3 9 9 . 0
0 2
0 7 . 4
2
6
4 5 5 . 4
0 8 1 . 1
6 8 1 . 3
3 0 0 . 0
7 0 3 . 0
0 0 7 . 0
0 5 9 . 0
5 2 9 . 0
5 3 6 . 0
3 9 8 . 0
0 3 8 . 0
7 4 7 . 0
8 6 7 . 0
1 7 6 . 0
6 8 9 . 0
9 8 9 . 0
7 8 9 . 0
0 2 9 . 0
8 5 6 . 0
5 1 5 . 0
0 7 8 . 0
2 5 7 . 0
7 4 7 . 0
4 9 5 . 0
8 9 6 . 0
4 5 7 . 0
8 2 7 . 0
0 9 8 . 0
4 4 8 . 0
0 2 9 . 0
0 8 9 . 0
0 8 9 . 0
4 2
0 0 . 0
1
3
7 5 3 . 5
8 9 8 . 0
3 2 4 . 2
0 0 0 . 0
4 0 3 . 0
0 0 6 . 0
0 0 9 . 0
0 5 9 . 0
6 7 5 . 0
0 0 0 . 1
4 0 8 . 0
0 3 7 . 0
5 5 7 . 0
1 4 6 . 0
3 3 9 . 0
5 7 8 . 0
0 6 9 . 0
0 3 9 . 0
4 4 6 . 0
8 0 5 . 0
6 4 8 . 0
4 1 7 . 0
2 5 7 . 0
6 5 6 . 0
6 6 6 . 0
5 0 7 . 0
8 7 6 . 0
2 5 8 . 0
6 4 8 . 0
0 9 8 . 0
0 8 9 . 0
1 6 9 . 0
6 2
0 2 . 0
1
1
1 2 3 . 6
3 4 7 . 0
5 0 0 . 2
0 0 0 . 0
1 5 2 . 0
0 0 9 . 0
5 7 9 . 0
0 5 9 . 0
1 7 5 . 0
4 6 9 . 0
2 0 9 . 0
9 2 7 . 0
7 4 7 . 0
9 3 6 . 0
5 9 9 . 0
0 0 0 . 1
3 9 9 . 0
4 2 9 . 0
8 5 6 . 0
2 8 2 . 0
1 9 8 . 0
5 9 6 . 0
1 1 7 . 0
1 4 6 . 0
1 0 7 . 0
4 1 7 . 0
5 9 6 . 0
7 9 8 . 0
3 2 9 . 0
7 6 7 . 0
0 0 0 . 1
8 8 9 . 0
3 2
8 5 . 1
2
3
8 1 5 . 5
0 5 1 . 1
4 0 1 . 3
0 0 0 . 0
1 8 2
.
0
3 3 7
.
0
5 2 9
.
0
5 7 8
.
0
0 6 4
.
0
9 2 9
.
0
8 8 7
.
0
5 3 7
.
0
0 4 7
.
0
0 3 6
.
0
4 6 9
.
0
7 7 9
.
0
0 2 9
.
0
5 2 9
.
0
5 7 5
.
0
4 7 1
.
0
8 8 8
.
0
8 8 6
.
0
0 2 7
.
0
8 8 6
.
0
2 9 6
.
0
8 9 6
.
0
4 9 6
.
0
0 9 7
.
0
0 8 7
.
0
0 8 7
.
0
0 8 9
.
0
1 9 9
.
0
7 2
0 0
.
0
0
1
4 1 7
.
7
1 9 0
.
1
4 4 9
.
2
0 0 0
.
0
N N 1
3 5 2
.
0
3 3 8
.
0
0 0 0
.
1
5 7 9
.
0
9 6 5
.
0
0 0 0
.
1
5 3 9
.
0
8 2 7
.
0
7 3 7
.
0
7 0 6
.
0
4 8 9
.
0
0 0 0
.
1
0 8 9
.
0
1 2 9
.
0
3 9 4
.
0
7 5 4
.
0
3 0 9
.
0
5 8 6
.
0
9 0 7
.
0
9 1 7
.
0
2 7 6
.
0
5 8 6
.
0
7 7 6
.
0
0 4 8
.
0
6 5 8
.
0
0 3 9
.
0
0 8 9
.
0
5 9 9
.
0
3 2
3 5
.
1
2
3
6 9 1
.
6
5 9 1
.
1
8 2 2
.
3
0 0 0
.
0
5
.
4 C
W T D
M M
T S F
W K
G
I t e s a t a D
#
3 4 2
.
0
0 0 6
.
0
0 5 7
.
0
0 0 9
.
0
5 6 5
.
0
7 5 8
.
0
2 5 7
.
0
9 5 6
.
0
2 1 7
.
0
0 8 5
.
0
2 6 9
.
0
1 6 7
.
0
7 0 9
.
0
0 1 9
.
0
4 3 5
.
0
9 4 4
.
0
4 4 8
.
0
4 3 6
.
0
3 3 7
.
0
6 5 6
.
0
4 7 5
.
0
5 2 6
.
0
5 9 5
.
0
5 4 8
.
0
1 7 4
.
0
3 0 9
.
0
0 8 9
.
0
3 5 8
.
0
8 2
0 0
.
0
0
0
1 8 7
.
0
8 0 1
.
2
0 0 0
.
0
6 3 0
.
0 1
1 9 4
.
0
3 3 4
.
0
0 5 6
.
0
5 2 7
.
0
4 3 6
.
0
4 6 4
.
0
5 2 9
.
0
3 9 4
.
0
6 4 5
.
0
0 3 5
.
0
8 2 8
.
0
0 3 8
.
0
3 1 9
.
0
1 6 9
.
0
6 2 7
.
0
0 9 6
.
0
6 1 8
.
0
8 5 5
.
0
0 7 4
.
0
4 9 5
.
0
5 9 4
.
0
9 9 4
.
0
6 2 5
.
0
9 9 6
.
0
4 3 9
.
0
3 7 9
.
0
0 9 9
.
0
5 9 7
.
0
8 2
0 0
.
0
0
0
4 0 8
.
9
7 6 8
.
1
0 4 0
.
5
0 0 0
.
0
1 7 2
.
0
0 0 3
.
0
0 0 8
.
0
5 7 8
.
0
1 2 5
.
0
7 5 8
.
0
8 4 4
.
0
0 1 7
.
0
7 3 7
.
0
3 0 7
.
0
8 2 9
.
0
9 0 4
.
0
0 2 9
.
0
1 1 9
.
0
4 7 2
.
0
0 9 4
.
0
0 4 8
.
0
3 0 7
.
0
0 2 7
.
0
7 4 5
.
0
3 7 6
.
0
7 9 6
.
0
0 3 7
.
0
9 4 7
.
0
4 7 5
.
0
7 5 8
.
0
0 0 0
.
1
3 5 8
.
0
6 2
5 2
.
0
1
1
0 0 5
.
9
3 7 2
.
1
6 3 4
.
3
0 0 0
.
0
6 5 1
.
0
7 6 5
.
0
0 0 9
.
0
0 0 9
.
0
5 3 5
.
0
0 0 0
.
1
5 6 7
.
0
3 0 6
.
0
9 1 6
.
0
0 6 5
.
0
0 9 9
.
0
0 5 7
.
0
3 5 9
.
0
1 3 9
.
0
1 1 4
.
0
8 0 5
.
0
0 4 8
.
0
8 7 5
.
0
9 0 6
.
0
8 7 5
.
0
6 8 5
.
0
1 8 5
.
0
1 9 5
.
0
3 5 9
.
0
1 0 8
.
0
7 5 9
.
0
0 8 9
.
0
0 7 9
.
0
6 2
0 2
.
1
1
1
7 0 1
.
9
8 1 3
.
1
9 5 5
.
3
0 0 0
.
0
6 6 2
.
0
3 3 3
.
0
0 0 7
.
0
5 7 8
.
0
0 2 5
.
0
7 5 8
.
0
1 2 6
.
0
0 8 6
.
0
3 9 6
.
0
0 2 7
.
0
2 7 8
.
0
3 4 4
.
0
0 4 9
.
0
0 1 9
.
0
0 8 4
.
0
1 7 4
.
0
0 4 8
.
0
7 9 6
.
0
0 5 7
.
0
9 0 6
.
0
0 2 7
.
0
3 8 6
.
0
3 1 7
.
0
7 2 7
.
0
7 5 5
.
0
0 0 9
.
0
0 4 9
.
0
4 6 7
.
0
7 2
0 0
.
0
0
1
2 8 9
.
9
9 5 2
.
1
8 9 3
.
3
0 0 0
.
0
9 9 2
.
0
0 0 5
.
0
5 7 7
.
0
0 5 8
.
0
8 8 5
.
0
4 6 9
.
0
2 2 7
.
0
4 5 6
.
0
5 0 7
.
0
1 8 5
.
0
5 7 7
.
0
1 4 8
.
0
3 9 8
.
0
2 9 8
.
0
3 9 4
.
0
8 8 4
.
0
5 2 8
.
0
4 6 6
.
0
0 1 7
.
0
2 7 6
.
0
6 9 5
.
0
4 1 6
.
0
8 0 6
.
0
5 4 8
.
0
0 8 7
.
0
3 4 9
.
0
0 8 9
.
0
1 5 8
.
0
8 2
0 0
.
0
0
0
8 6 7
.
9
6 1 0
.
1
3 4 7
.
2
0 0 0
.
0 n o i t a r t n e c n o C e n i r o l h C n o i t c u d e R e z i
S m o t a i D e e f f o C e l d d i M _ P D b m u h T _ P D e l t t i
L _ P D s y a D e v i F G C E r u o F e c a F t n i o P n u G d n a m e D r e w o P y l a t I s e g a m
I l a c i d e M
7 g n i t h g i L n e k c i h C / d r i
B y l
F
/ e l t e e B c a i d A f e e B n i a r t
S e t o M e l t t i
L _ P M e l d d i M _ P M e l t t i
L _ P P s h t i l o t O e l d d i M _ P P b m u h T _ P P e c a f r u S t o b o R O B A y n o S
I l o r t n o C c i t e h t n y S
G C E d a e L o w T e c a r T s l o b m y S
1
2
3
4
5
6
7
8
9
0 1
1 1
2 1
3 1
4 1
5 1
6 1
7 1
8 1
9 1
0 2
1 2
2 2
3 2
4 2
5 2
6 2
7 2
8 2 l a v r e t n I e c n e d fi n o C k n a R n o i t a i v e D d r a d n a t S k n a R s e u l a v p t s e T n o x o c l i
W i s n W 1 o t 1 S T L i s n W e t u l o s b A s w a r D 1 o t 1 S T L s e s s o L 1 o t 1 S T L n a e M k n a R lines is conducted . Our method outperforms all the baselines with statistically significant margins in terms of both wins and ranks .
8 . ACKNOWLEDGMENT
This study was partially co funded by the Seventh Framework Programme ( FP7 ) of the European Commission , through project REDUCTION4 ( #288254 ) .
9 . REFERENCES [ 1 ] K W Chang , B . Deka , W . mei W . Hwu , and D . Roth .
Efficient pattern based time series classification on gpu . In M . J . Zaki , A . Siebes , J . X . Yu , B . Goethals , G . I . Webb , and X . Wu , editors , ICDM , pages 131–140 . IEEE Computer Society , 2012 .
[ 2 ] A . Das and D . Kempe . Algorithms for subset selection in linear regression . In Proceedings of the 40th Annual ACM Symposium on Theory of Computing , STOC ’08 , pages 45–54 , New York , NY , USA , 2008 . ACM .
[ 3 ] J . Demšar . Statistical comparisons of classifiers over multiple data sets . J . Mach . Learn . Res . , 7:1–30 , Dec . 2006 .
[ 9 ] J . Lines and A . Bagnall . Alternative quality measures for time series shapelets . In Intelligent Data Engineering and Automated Learning , volume 7435 of Lecture Notes in Computer Science , pages 475–483 . 2012 .
[ 10 ] J . Lines , L . Davis , J . Hills , and A . Bagnall . A shapelet transform for time series classification . In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2012 .
[ 11 ] A . Mueen , E . Keogh , and N . Young . Logical shapelets : an expressive primitive for time series classification . In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2011 .
[ 12 ] T . Rakthanmanon and E . Keogh . Fast shapelets : A scalable algorithm for discovering time series shapelets . Proceedings of the 13th SIAM International Conference on Data Mining , 2013 .
[ 13 ] P . Sivakumar and T . Shajina . Human gait recognition and classification using time series shapelets . In IEEE International Conference on Advances in Computing and Communications , 2012 .
[ 4 ] H . Ding , G . Trajcevski , P . Scheuermann , X . Wang , and E . J .
[ 14 ] E . W . Wild . Optimization based Machine Learning and Data
Keogh . Querying and mining of time series data : experimental comparison of representations and distance measures . PVLDB , 1(2):1542–1552 , 2008 .
[ 5 ] B . Hartmann and N . Link . Gesture recognition with inertial sensors and optimized dtw prototypes . In IEEE International Conference on Systems Man and Cybernetics , 2010 .
[ 6 ] B . Hartmann , I . Schwab , and N . Link . Prototype optimization for temporarily and spatially distorted time series . In the AAAI Spring Symposia , 2010 .
[ 7 ] Q . He , F . Zhuang , T . Shang , Z . Shi , et al . Fast time series classification based on infrequent shapelets . In 11th IEEE International Conference on Machine Learning and Applications , 2012 .
[ 8 ] J . Hills , J . Lines , E . Baranauskas , J . Mapp , and A . Bagnall .
Classification of time series by shapelet transformation . Data Mining and Knowledge Discovery , 2013 .
Mining . ProQuest , 2008 .
[ 15 ] Z . Xing , J . Pei , and P . Yu . Early classification on time series .
Knowledge and information systems , 31(1):105–127 , 2012 . [ 16 ] Z . Xing , J . Pei , P . Yu , and K . Wang . Extracting interpretable features for early classification on time series . Proceedings of the 11th SIAM International Conference on Data Mining , 2011 .
[ 17 ] L . Ye and E . Keogh . Time series shapelets : a new primitive for data mining . In Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2009 .
[ 18 ] L . Ye and E . Keogh . Time series shapelets : a novel technique that allows accurate , interpretable and fast classification . Data Mining and Knowledge Discovery , 22(1):149–182 , 2011 .
[ 19 ] J . Zakaria , A . Mueen , and E . Keogh . Clustering time series using unsupervised shapelets . In Proceedings of the 12th IEEE International Conference on Data Mining , 2012 .
4wwwreduction projecteu
