An Efficient Algorithm For Weak Hierarchical Lasso
Yashu Liu
Arizona State University
Tempe , AZ 85287
YashuLiu@asuedu
Jie Wang
Arizona State University
Tempe , AZ 85287 jiewangustc@asuedu
Jieping Ye
Arizona State University
Tempe , AZ 85287
JiepingYe@asuedu
ABSTRACT Linear regression is a widely used tool in data mining and machine learning . In many applications , fitting a regression model with only linear effects may not be sufficient for predictive or explanatory purposes . One strategy which has recently received increasing attention in statistics is to include feature interactions to capture the nonlinearity in the regression model . Such model has been applied successfully in many biomedical applications . One major challenge in the use of such model is that the data dimensionality is significantly higher than the original data , resulting in the small sample size large dimension problem . Recently , weak hierarchical Lasso , a sparse interaction regression model , is proposed that produces sparse and hierarchical structured estimator by exploiting the Lasso penalty and a set of hierarchical constraints . However , the hierarchical constraints make it a non convex problem and the existing method finds the solution of its convex relaxation , which needs additional conditions to guarantee the hierarchical structure . In this paper , we propose to directly solve the non convex weak hierarchical Lasso by making use of the GIST ( General Iterative Shrinkage and Thresholding ) optimization framework which has been shown to be efficient for solving non convex sparse formulations . The key step in GIST is to compute a sequence of proximal operators . One of our key technical contributions is to show that the proximal operator associated with the non convex weak hierarchical Lasso admits a closed form solution . However , a naive approach for solving each subproblem of the proximal operator leads to a quadratic time complexity , which is not desirable for largesize problems . To this end , we further develop an efficient algorithm for computing the subproblems with a linearithmic time complexity . We have conducted extensive experiments on both synthetic and real data sets . Results show that our proposed algorithm is much more efficient and effective than its convex relaxation .
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications , Data Mining
General Terms Algorithms
Keywords Sparse learning ; non convex ; weak hierarchical Lasso ; proximal operator
1 .
INTRODUCTION
Consider a linear regression model with the outcome vari able y and d predictors x1 , . . . , xd : y = w0 + xiwi + ,
( 1 ) d i=1 d i=1 d d i=1 j=1 where w0 is the bias term , wi , i = 1 . . . , d is the coefficient and ∼ N ( 0 , σ2 ) is the noise term . In many applications , a simple linear regression model is not sufficient for predictive or explanatory purposes . One strategy which has recently received increasing attention in statistics is to include interaction terms into the model to capture the nonlinearity of the data [ 17 , 22 ] . For example , the linear model including terms of order 2 and lower has the following form : y = w0 + xiwi +
1 2 xixjQi,j + ,
( 2 ) where the cross product term xixj , i = j refers to as the interaction variable ( one may view x2 i as a special interaction is and Q ∈ Rd×d are the main effect and invariable ) , and w teraction effect coefficients respectively . Applications with interaction regression models are omnipresent . For example , in psychological study , the effectiveness of using 3 way interactions was demonstrated in testing psychological hypothesis [ 9 ] ; there are strong evidences found in [ 4 ] that genetic environment interactions have significant effects on conduct disorders ; the research in [ 11 ] found a couple of evidences of gene environment interactions in predicting depression status ; in [ 26 ] , the interaction between continuance commitment and affective commitment was found significant in predicting job withdraw intentions and absenteeism ; [ 13 ] discovered that brain derived neurotrophic factor interacts with early life stress in predicting cognitive features of depression and anxiety .
However , the use of higher order terms leads to data of high dimensionality . For instance , for regression model ( 1 ) , if one wants to add all terms of order k and lower , then there will be a total of O(dk ) variables , which is computationally demanding for parameter estimation even when k and d are fairly small . Thus , an efficient approach that is able to deal with huge dimensionality is desired in such cases , and the sparse learning methodology is one promising approach for tackling such problem [ 27 , 18 , 7 , 5 , 32 ] . In this paper , we focus on the model ( 2 ) with pairwise interactions , ie , twofactor interactions . Note that the analysis can be extended to the model with higher order interactions .
In general , not all of the main effects and interactions are of interest , thus it is critical to select the variables of great significance . One simple approach for high dimensional interaction regression is to directly apply the Lasso [ 27 ] , also known as the “ all pairs Lasso ” [ 2 ] in the case of two factor interactions . However , the all pairs Lasso estimator does not account for any structural information which has been shown to be important for prediction and interpretation of the high dimensional interaction regression model [ 2 , 30 , 25 , 29 , 6 ] . In statistics , a hierarchical structure between main effects and interaction effects has been shown to be very effective in constraining the search space and identifying important individual features and interactions [ 2 , 30 , 25 , 29 , 6 ] . Specifically , the hierarchical constraint requires that an interaction term xixj is selected in the model only if the main effects xi and/or xj are included . Strong theoretical properties have been established for such hierarchical model [ 29 , 30 ] . The hierarchical structure is supported by the argument that large main effects may result in interaction of more importance , and it is desired in a wide range of applications in engineering and underlying science . Traditional approaches to fit such a model typically follow the following two step procedures [ 22 ] :
( i ) Fit a linear regression model that only includes the main effects and then select the significant features ;
( ii ) Fit the reformulated model with the identified individual features and the interactions constructed via domain knowledge .
Since even a small d may lead to a huge amount of interaction variables , the two step procedure is still time consuming in many applications . Recently , there have been growing research efforts on imposing the hierarchical structure on main effects and interactions in the regression model with novel sparse learning methods . In [ 2 ] , in order to enable feature selection and impose heredity structures , the authors proposed strong hierarchical Lasso which adds a set of constraints to the Lasso formulation to achieve the strong hierarchy where the interaction effects are non zero only if the corresponding main effects are non zero . In [ 25 ] , a Lasso type penalized least square formulation called VANISH was proposed to achieve the strong hierarchy between the interaction effects and main effects . In [ 29 ] , a type of non negative garrote method was proposed to achieve the heredity structures . In [ 30 ] , the Composite Absolute Penalties were proposed to achieve heredity structures for interaction models . In contrast to the above works which fulfill the hierarchical structure via solving convex problems , Choi et al . in [ 6 ] formulated a non convex problem to achieve the strong hierarchy by assuming that the coefficient of an interaction term is a product of a scalar and main effect coefficients . Different from the strong hierarchy , the weak hierarchy between the main effects and the interaction effects requires that an interaction is included in the model only if at least one of the main effects is included in the model . In mathematical form , Qi,j = 0 only if wi = 0 OR wj = 0 . The weak hierarchy can be considered as a structure in between the strong hierarchy and no hierarchical structure [ 2 , 29 , 30 ] . Specifically , weak hierarchy allows those interactions with only one significant “ parent ” ( main effect ) to be included in the model . Several existing empirical studies have demonstrated the stronger predictive power of weak hierarchical model [ 19 ] . In our study , we mainly focus on the interaction regression model with weak hierarchical structure .
We follow the weak hierarchical Lasso approach recently proposed by [ 2 ] to fit the pairwise interaction regression model with the weak hierarchy . By imposing restrictions of the weak hierarchy and taking advantage of the Lasso penalty [ 27 ] that leads to sparse coefficients , the weak hierarchical Lasso is able to simultaneously attain a hierarchical solution and identify important main effects and interactions . However , the set of constraints restricting hierarchical constraints make the problem non convex ; the algorithm proposed in [ 2 ] aims to solve a convex relaxation . The convex relaxation , however , requires additional conditions to guarantee the weak hierarchy , which is not desirable .
In this paper , we propose to directly solve the weak hierarchical Lasso using the GIST ( General Iterative Shrinkage and Thresholding ) optimization framework recently proposed by [ 15 ] . The GIST framework has been shown to be highly efficient for solving large scale non convex problems . The most critical step in GIST is to compute a sequence of proximal operators [ 23 ] . In this paper , we first show that the proximal operator related to weak hierarchical Lasso admits an analytical form solution by factorizing unknown coefficients into sign matrices and non negative coefficients . However , a naive method of computing the subproblem of the proximal operator leads to a quadratic time complexity , which is not desirable for large size problems . To this end , we further develop an efficient algorithm for solving the subproblems , which achieves a linearithmic time complexity . We evaluate the efficiency and effectiveness of the proposed algorithm and compare it with the convex relaxation in [ 2 ] and other state of the art methods using synthetic and real data sets . Our empirical study demonstrates the high efficiency of our algorithm and the superior predictive performance of weak hierarchical Lasso over the competing methods .
The remaining of the paper is organized as follows : we give a brief review of the weak hierarchical Lasso and its convex relaxation in Section 2 . In Section 3 , we derive the closed form solution to the proximal operator of the original weak hierarchical Lasso by decomposing the unknown coefficients into signs and the non negative coefficients . Then , we show how the associated proximal operator can be computed efficiently . We report the experimental results in Section 4 . We conclude this paper in Section 5 .
2 . THE WEAK HIERARCHICAL LASSO
In this section , we briefly review the weak hierarchical Lasso and its corresponding convex relaxed formulation [ 2 ] . Suppose we are given n pairs of data points {(xi , yi)}n i=1 ⊂ Rd × R . Let Y ∈ Rn×1 be the vector of outcome and X ∈
Rn×d be the design matrix . Let Z ∈ Rn×(d·d ) be the matrix of interactions where
Z ( 1 ) , Z ( 2 ) , . . . , Z ( d )
,
Z =
Z ( i ) ∈ Rn×d and each column of Z ( i ) , i = 1 , . . . d is an interaction , ie , Z ( i)·,j = X·,i fi X·,j ( fi is the operator of elementwise product ) . Thus , Z ( i ) captures the pairwise interactions between the i th feature and all d features . Note that , we include the quadratic terms x2 i in the interaction model for clearer presentation , however our analysis is still applicable if they are not included in the model . By assuming that Y is centered and X , Z are column wise normalized to zero mean and unit standard deviation , we can set the bias term w0 = 0 . Thus , in matrix form , the pairwise interaction regression model can be expressed as
Z · vec(Q ) + ,
1 2
Y = Xw +
( 3 ) where ∼ N ( 0 , σ2I ) and “ vec ” is the vectorization operator that transforms a matrix to a column vector by stacking the columns of the matrix . Thus , the least square loss function of ( 3 ) is given by : flflflflY − Xw − 1
2
L ( w , Q ) =
1 2
Z · vec(Q )
.
( 4 ) flflflfl2
2
Then , the weak hierarchical Lasso formulation takes the form of [ 2 ] : min x,Q where Q1 = st
λ 2
L ( w , Q ) + λw1 + Q·,j1 ≤ |wj| i,j |Qi,j| and λ is the Lasso penalty param
Q1 j = 1 , . . . , d ,
( 5 ) for eter .
Note that the constraints in ( 5 ) guarantee the weak hierarchical structure since the coefficient Qi,j of interaction xixj is non zero only if at least one of its main effects is included in the model , ie , wi = 0 or wj = 0 . However , the imposed hierarchical constraints make problem ( 5 ) nonconvex . Instead of solving ( 5 ) , Bien et al . in [ 2 ] proposed to solve the following relaxed version :
L,w+ − w
− min w,Q st Q·,j1 ≤ w+ j ≥ 0 w+ j ≥ 0 − w
, Q + λ1T ( w+ + w  for
− j + w j
−
) +
Q1
λ 2
( 6 ) j = 1 , . . . , d , where 1 represents a column vector of all ones . In view of ( 6 ) , we can see that w1 is relaxed to w+ + w− . Problem ( 6 ) is convex and can be solved by many efficient solvers such as FISTA [ 1 ] . However , Bien et al . in [ 2 ] showed that problem ( 6 ) needs an additional ridge penalty to guarantee the weak hierarchical structure of the estimator . In this paper , we propose an efficient algorithm which directly solves the non convex weak hierarchical Lasso formulation in ( 5 ) .
3 . THE PROPOSED ALGORITHM
In this section , we propose an efficient algorithm named use of the optimization framework of GIST ( General Iterative Shrinkage and Thresholding ) due to its high efficiency and effectiveness for solving non convex sparse formulations . One of the critical steps in GIST is to compute the proximal operator associated with the penalty functions . As one of our major contributions , we first factorize the unknown coefficients into the product of their signs and magnitudes ; and then show that the proximal operator of ( 5 ) admits a closed form solution in Section 31 Another major contribution is that we present an efficient algorithm for computing the proximal operator associated with the non convex weak hierarchical Lasso in Section 32 The time complexity of solving each subproblem of the proximal operator can be reduced from quadratic to linearithmic . We then summarize our algorithm for computing the proximal operator in Section 32
3.1 The Closed Form Solution to the Proximal
Operator
In this section , we show how to derive the closed form solution of the proximal operator associated with ( 5 ) in detail . Let P =
( a , B ) , a ∈ Rd , B ∈ Rd×d fififi B·,j1 ≤ |aj| , j = 1 , . . . , d
Given a sequence sociated with weak hierarchical Lasso is :
, the proximal operator as and the indicator function be defined by
λ 2
+∞ ,
Q1 ,
R(w , Q ) =
 λw1 + w(k ) , Q(k ) w(k+1 ) , Q(k+1 ) ∇wL L w(k ) , Q(k ) ∇QL w(k ) , Q(k ) , Q − Q(k ) flflflQ − Q(k)flflfl2
+ R(w , Q ) , w , Q
+
+
+ t(k ) 2
F
= arg min
( w , Q ) ∈ P ( w , Q ) /∈ P . if if
( 7 ) w(k ) , Q(k )
, w − w(k ) flflflw − w(k)flflfl2
2
+ t(k ) 2
Simple algebraic manipulation leads to where t(k ) > 0 . w(k+1 ) , Q(k+1 )
= arg min w , Q
1 2
+ where v(k ) =w(k ) − ∇wL U ( k ) =U ( k ) − ∇QL
( 8 ) flflflQ − U ( k)flflfl2
2
+
1 2
( 9 ) flflflw − v(k)flflfl2
2
1 t(k ) R(w , Q ) , w(k ) , Q(k ) w(k ) , Q(k )
/t(k ) ,
/t(k ) .
“ eWHL ” , which stands for “ efficient Weak Hierarchical Lasso ” , to directly solve the weak hierarchical Lasso . eWHL makes
Thus , problem ( 5 ) can be solved by iteratively solving the proximal operator in ( 9 ) . Because R(w , Q ) is an indicator function , we can rewrite the proximal operator ( 9 ) as
Next , we show that ( 13 ) has a closed form solution . Since arg min w , Q
1 2 w − v2
2 + st Q·,j1 ≤ |wj|
Q − U2
F +
1 2 w1 +
λ t
Q1
λ 2t for j = 1 , . . . , d .
=
We omit the superscripts for notational simplicity .
The vector of main effect coefficients can be written as w = S0 ˜w ,
( 10 ) where ˜wj = |wj| , j = 1 , . . . , d and S0 ∈ Rd×d is a diagonal matrix whose j th diagonal element is the sign of wj , ie , S0 j,j = sign(wj ) . We define
 1 if w > 0
−1 if w < 0 0 if w = 0 sign(w ) =
,
( 11 ) and we assume in this paper that the sign operator is applied on vectors or matrices elementwise . Similarly , we factorize each column of the interaction coefficient matrix as Q·,j =
SjQ·,j , j = 1 . . . , d , where Qi,j = |Qi,j| and Sj ∈ Rd×d is the diagonal sign matrix . Then , the proximal operator ( 10 ) is equivalent to
1 2 w − v2 arg min st Q·,j1 ≤ |wj| w , Q
2 +
Q − U2
F + w1 +
λ t
Q1
λ 2t min ˜wj , ˜Q·,j st
1 2
 for j,j ˜wj wj = S0
Q·,j = SjQ·,j Q·,j 0
˜wj ≥ 0 j = 1 , . . . , d ,
,S0 2
2 , ˜wj − S0 jj
=
1 2 jjvj
1 2 1 2
2 jj ˜wj − vj
( wj − vj)2 =
1 2 jj ˜wj − vj
,S0 ,S0 2 will not achieve the minimum . Similarly , and ˜wj ≥ 0 , S0 j,j must have the same sign as vj , that is , wj has the same sign as vj . Otherwise , the value of 1 2 one can show that Sj i,i , ie , the sign of Qi,j , must be the same as the sign of Ui,j . Thus , the diagonal elements diag(S0 ) = sign(v ) , diag(Sj ) = sign(U·,j ) , j = 1 , . . . , d . Next , we
, ˜wj − S0 show how to compute ˜w and Q . jjvj jjvj and Uj = SjU·,j , each subproblem 1TQ·,j flflflQ·,j − U·,j flflfl2
˜wj +
2 +
+
λ 2t
λ t
1 2
2
By letting ˜vj = S0 ( 13 ) is equivalent to ˜wj − ˜vj2 arg min
˜wj ,Q·,j
1 2
1TQ·,j ≤ ˜wj Q·,j 0
. st
( 14 )
( 15 ) flflflQ·,j − qU·,j flflfl2
2
After rearrangement , problem ( 14 ) can be expressed as :
,
1 2
1 2
˜wj − ˇvj2
2 +
1TQ·,j ≤ ˜wj Q·,j 0 t 1 and qU·,j = U·,j − λ flflfl2 flflflQ − qU where ˇvj = ˜vj − λ
2t 1 .
We solve ( 15 ) by deriving its dual problem . Let γ ≥ 0 be the Lagrangian multiplier dual variable of the first inequality constraint . Define the Lagrangian function of ( 15 ) as : l(γ , ˜w , Q ) = 1TQ − ˜w abuse of notation . Since the constraint 1TQ ≤ ˜w is affine , where we omit the subscripts for simplicity with a little
( ˜w − ˇv)2 +
+ γ
1 2
1 2
2 the strong duality holds for the minimization problem ( 15 ) . Thus , the dual problem of ( 15 ) is : max γ≥0 min ˜w , ˜Q0
1 2
( ˜w − ˇv)2 + max γ≥0
1 2 min
( ˜w − ( ˇv + γ))2+
˜w,Q0 where h(γ ) = −ˇvγ − 1
2
1 2
+ γ flflflQ − qU flflfl2
1TQ − ˜w flflflQ −qU − γ1 flflfl2 2 γ2 + γ1TqU − 1
2 γ21T 1 .
1 2
2
+h(γ ) ,
. ( 16 )
( 17 )
By rearranging the terms , ( 16 ) is equivalent to :
For fixed γ , in order to obtain the minimum of the objec tive function in ( 17 ) , we conclude that

ˇv + γ ≥ 0 ⇒ ˜w = ˇv + γ ˇv + γ < 0 ⇒ ˜w = 0 qUi − γ ≥ 0 ⇒ Qi = qUi − γ qUi − γ < 0 ⇒ Qi = 0 due to the constraints ˜w ≥ 0 and Q 0 . Therefore , if we obtain a dual optimal solution γ∗ that maximizes the dual problem ( 17 ) , then we can readily compute the closed form solution to ( 13 ) and thus to ( 12 ) . That is , w∗ =
( 18 )
( 12 ) where Q , ˜w and Sj , j = 0 , . . . , d are the unknown vari ables , is defined as the element wise “ greater than or equal to ” comparison operator , ie , for V , U ∈ Rd×1 , V U ⇔ Vi ≥ Ui , i = 1 . . . , d . Therefore , the solutions of the original weak hierarchical Lasso can be obtained by iteratively solving ( 12 ) . Note that the amounts of l1 penalties on w and Q can be different . Here we use the same penalty parameter λ for notational simplicity and consistency with the original formulation of weak hierarchical Lasso ( 5 ) studied in [ 2 ] . Though the factorization introduces more variables and constraints , we show that the resulting proximal operator admits a closed form solution . More importantly , we show that each sub problem of the proximal operator can be solved by the proposed eWHL algorithm in linearithmic time . Indeed , the factorization of w and Q into their signs and magnitudes is the first key to directly solve the original weak hierarchical Lasso .
It is clear that the proximal operator in ( 12 ) can be de coupled into d subproblems : jj ˜wj − vj arg min jj ,Q·,j ,Sj
˜wj ,S0
1 2
+
2 flfl2 flflS0 1TQ·,j
1TQ·,j ≤ ˜wj Q·,j 0
˜wj +
λ 2t
λ t
,
+ st flflflSjQ·,j − U·,j flflfl2
2
1 2
( 13 ) for j = 1 , . . . , d .
·,j = SjQ∗
S0 ˜w∗ , Q∗
·,j where diag(S0 ) = sign(vj ) , diag(Sj ) = sign(U·,j ) , j = 1 , . . . , d and ˜w∗ , Q∗ are obtained via ( 18 ) at mal solution γ∗ . First , we sort −ˇv and qUi , i = 1 , . . . , d in the optimal dual solution γ∗ . 3.2 The Dual Optimal Solution
Next , we show how to efficiently compute the dual opti ascending order . Without loss of generality , we assume : qU1 ≤ . . . ≤ qUL ≤ −qv ≤ qUL+1 ≤ . . . ≤ qUd .
( 19 )
There are four possible cases about the locations of γ . We discuss how to identify the optimal dual solution γ∗ in each of the four cases .
When . . . ≤ qUG ≤ γ ≤ qUG+1 ≤ . . . ≤ −ˇv ≤ . . . , the objective
Case 1 : in ( 17 ) at γ∗ becomes
2 qUi − γ d qU 2 i + i=G+1
1 2
1 2
=
G G i=1 i=1 d qUi metry point qUG,qUG+1 i=G+1 d−G , we set
1 2
+
( ˇv + γ)2 + h(γ )
2
1 2
ˇv2 .
( 20 )
γ2 +
γqUi − d − G ≥ qUG+1 . Since γ falls in the interval γ = qUG+1
Function ( 20 ) is a quadratic function with respect to γ and the unconstrained maximum is achieved at the axis of sym to achieve the maximum objective value of ( 17 ) . It can be further concluded that , in Case 1 , among all the intervals on the left of −ˇv , the maximum objective value of ( 17 ) is achieved at the qUG . When . . . ≤ qUL ≤ γ ≤ −ˇv ≤ qUL+1 ≤ . . . , it turns out that
Case 2 : the objective value in ( 17 ) at γ is similar to ( 20 ) :
L qU 2 i + d i=1 i=L+1
1 2
γqUi − d − L
2
γ2 +
ˇv2 .
1 2
( 21 )
By a similar argument , we can set γ = −ˇv to achieve the maximum . Combining the results of Case 1 and Case 2 , we conclude that , we may only consider γ in the range
[ max ( −ˇv , 0 ) , +∞ ] . Note that when L = d , that is qUd ≤ γ ≤ i + 1
2 ˇv2 , and thus γ can be
−ˇv , ( 21 ) is a constant 1 2 any value in the interval qU 2 d qUd,−ˇv i=1
.
When . . . ≤ qUL ≤ −ˇv ≤ γ ≤ qUL+1 ≤ . . . , the value of the
Case 3 : objective function in ( 17 ) at γ∗ becomes
L L i=1 qUi − γ qU 2 i + γ
2 d i=1 i=L+1
+ h(γ ) qUi − ˇv
1 2
1 2
=
− d + 1 − L
2
γ2 .
( 22 ) d qUi−ˇv
Again , ( 22 ) is a quadratic function of γ and −ˇv . If i=L+1 d+1−L
≥ i=L+1 d d+1−L qUi−ˇv ≥ qUL+1 , the maximum is achieved at γ = qUL+1 , qUi − ˇv d
γ = i=L+1 d + 1 − L
. otherwise the maximum is achieved at value in ( 17 ) is similar to ( 22 ) :
When . . . ≤ −ˇv ≤ . . . ≤ qUG ≤ γ ≤ qUG+1 ≤ . . . , the objective
Case 4 :
G qU 2 qUi−ˇv i=1 i + γ
1 2 d
If i=G+1 d+1−G qUi−ˇv i=G+1 d+1−G
If d If qUG ≤ d i=G+1 d+1−G
( 23 )
2
γ2 . i=G+1
− p + 1 − G d qUi − ˇv ≥ qUG+1 , then the maximum is achieved at γ = qUG+1 ; ≤ qUG , then the maximum is achieved at γ = qUG ; ≤ qUG+1 , the maximum is achieved at qUi−qv d qUi − ˇv
γ = i=G+1 d + 1 − G
.
Since we know exactly the value of γ for all the four cases , one naive way to find the optimal γ∗ is to enumerate all the possible locations and pick the one that maximizes the objective function value in ( 17 ) . However , evaluating the objectives for all possible locations from max(−ˇv , 0 ) to qUd leads to a quadratic time algorithm for solving ( 17 ) . Interestingly , we show below that the time complexity of solving ( 17 ) can be reduced to O(d log(d) ) .
Let us first list some useful properties as follows :
Given the ordered sequence ( 19 ) :
• Property 1 : i=G
, if i=G+1 and then d d+1−G
• Property 2 :
In Case 4 , for a pair of adjacent intervals
The maximum objective value of ( 17 ) in Case 3 is larger than the one in Cases 1 & 2 ; qUG−1,qUG qUG,qUG+1
≥ qUG+1 for [ qUG,qUG+1 ] , qUi−qv d+1−(G−1 ) ≥ qUG for [ qUG−1,qUG ] ; qUi−qv d qUG,qUG+1
≥ qUG+1 for d • Property 3 : qUG,qUG+1
In Case 4 , if qUG−1,qUG qUG−1,qUG d+1−(G−1 ) ≤ qUG−1 for qUi−qv d qUG,qUG+1
≤ qUG for qUi−qv qUG,qUG+1 qUG−1,qUG
In Case 4 , for a pair of adjacent intervals the maximum objective value of ( 17 ) in larger than or equal to the one in
• Property 4 : qUi−ˇv
, if we have d+1−G d
, then and i=G+1 i=G+1 i=G is
.
,
. d+1−G t 1 ;
Algorithm 1 Computation of the Proximal Operator of Weak Hierarchical Lasso Input : v ∈ Rd×1 , U ∈ Rd×d , t ∈ R+ , λ ∈ R+ Output : w ∈ Rd×1 , Q ∈ Rd×d 1 : ˇv = sign(v ) fi v − λ qU = sign(U ) fi U − λ Sort qU·,j to get a sequence S in ascending order where qQ·,j = 0 ; c = −ˇvj ; S1 ≤ S1 ≤ . . . ≤ Sd ; if Sd < 0 and c < 0 then
2 : for j = 1 : d do 3 : 4 :
2t 11T ;
˜wj = 0 ; else if Sd < c then γ = max(c , 0 ) ; else k = d ; while 1 do c = c + Sk ; k = k − 1 ; if c/(d + 1 − k ) ≥ Sk then
γ = c ; break ; end if end while end if ˜wj = max(ˇvj + γ , 0 ) ; qU·,j − γ , 0
;
Q·,j = max Q = sign(U ) fi Q ;
22 : end if 23 : end for 24 : w = sign(v ) fi ˜w ;
5 : 6 :
7 : 8 : 9 : 10 : 11 : 12 : 13 : 14 : 15 : 16 : 17 : 18 : 19 : 20 : 21 :
, the qUG−1,qUG qUG−1,qUG qUG,qUG+1 qUG,qUG+1 ≤ qUG+1 for qUi−qv d
, is
. and i=G+2 d+1−(G+1 )
,
• Property 5 : In Case 4 , if d+1−(G−1 ) ≤ qUG−1 for d qUi−qv i=G maximum objective value of ( 17 ) in i=G+1 larger than or equal to the one in d+1−G
In Case 4 , if qUG ≤ qUi−qv d • Property 6 : qUG−1,qUG d+1−(G−1 ) ≥ qUG for qUi−qv d qUG+1,qUG+2 ≤ qUG+1 for qUG,qUG+1
( 17 ) in the interval to the ones in its neighbor intervals . then i=G
, and the maximum value of is larger than or equal
−ˇv,qUL+1
Properties 2 6 also apply for adjacent intervals qUL+1,qUL+2 and in Case 3 .
We omit the proof of Properties 1 6 since they are direct applications of 1 D quadratic optimization . Property 1 indicates that it is sufficient for the algorithm to start searching γ∗ from Case 3 . Properties 2 & 3 imply that , for some interval , if the axis of symmetry is on the right hand side of the interval , then one only needs to consider the intervals to the right . Similarly , Properties 4 & 5 indicate that , for some interval , if the axis of symmetry is on the left hand side of the interval , then one only needs to consider the intervals to the left . Property 6 combined with Properties 1 5 imply that , for certain interval , if it contains the axis of symmetry , then γ∗ is the axis of symmetry point . Thus , we can draw the following conclusion : ( 1 ) if max qUd,−v
< 0 , then
( 2 ) if −ˇv > qUd , then ( 3 ) if qUG ≤ d qUG,qUG+1
, then i=G+1 d+1−G
∗
γ
= 0 ;
γ
∗
= max(−ˇv , 0 ) ; qUi−ˇv d i=G+1
ˇUi − ˇv d + 1 − G d
.
∗
γ
=
≤ qUG+1 for a certain interval qUi−ˇv i=G+1
At each move , the axis of symmetry can be calculated by a constant operation based on the value from the last step , and the time complexity of searching γ∗ reduces from quadratic to O(d log(d ) ) as the computation is dominated by the sorting operation . Once γ∗ is determined , we can compute ˜w and Q by ( 18 ) . Note that , the subproblem d+1−G of the proximal operator associated with the convex relaxation in [ 2 ] is solved by searching for the dual variable in a different way with time complexity O(d2 ) .
In summary , we reformulate the proximal operator for the original weak hierarchical Lasso by factorizing the unknown coefficients . The reformulated proximal operator is shown to admit a closed form solution , which enables directly solving the weak hierarchical Lasso problem . Moreover , the subproblem of the proximal operator can be computed efficiently with a time complexity of O(d log(d) ) . The detailed algorithm for solving the proximal operator ( 12 ) is described in Algorithm 1 . We give the details of eWHL algorithm in
Algorithm 2 . Following [ 15 ] , we choose the step size t(k ) by the Barzilai Borwein ( BB ) Rule .
4 . EXPERIMENTS
In this section , we evaluate the efficiency and effectiveness of the proposed algorithm on both synthetic and real data sets . In our first experiment , we compare the efficiency of our proposed algorithm and the convex relaxation of weak hierarchical Lasso [ 2 ] on synthetic data sets where the weak hierarchical structure holds between main effects and interaction effects . In our second experiment , we compare the classification performance of the weak hierarchical Lasso with other classifiers and sparse learning techniques on the data collected from Alzheimer ’s Disease Neuroimaging Initiative ( ADNI)1 . 4.1 Efficiency and Effectiveness Comparison on Synthetic Data Sets
In this experiment , we compare the efficiency of the proposed eWHL algorithm with the convex relaxation on synthetic data sets . Our algorithm is built upon the GIST framework which is available online [ 16 ] . The source code of the convex relaxed weak hierarchical Lasso ( cvxWHL ) was available in the R package “ hierNet ” [ 3 ] where the optimiza
1http://wwwadni infoorg/
Algorithm 2 The Efficient Weak Hierarchical Lasso Algorithm ( eWHL ) Input : X ∈ Rn×d , Z ∈ Rn×(d·d ) , λ ∈ R+ , η > 1 Output : w ∈ Rd×1 , Q ∈ Rd×d
Initialize k ← 0 and starting points w(0 ) and Q(0 ) ;
2 : repeat
Choose the step size t(k ) by the BB Rule repeat
4 : w(k ) , Q(k ) w(k ) , Q(k )
/t(k ) ;
/t(k ) ; v(k ) = w(k ) − ∇wL U ( k ) = U ( k ) − ∇QL w(k+1 ) , Q(k+1 )
Solve v(k ) , U ( k ) , t(k ) , λ t(k ) ← ηt(k ) ;
; by Algorithm 1 with input
6 : until line search criterion is satisfied k ← k + 1
8 : until stop criterion is satisfied
Z ( 1 ) , Z ( 2 ) , . . . , Z ( d ) tion procedure was implemented by C . Since the proposed algorithm in this paper directly solves the non convex weak hierarchical Lasso ( 5 ) , and the eventual goal of the convex relaxed weak hierarchical Lasso is also to find a good “ relaxed ” solution to the original problem , we compare the two algorithms in terms of the objective function in ( 5 ) . In the experiment , entries of X ∈ Rn×d are iid generated from the standard normal distribution , ie , Xi,j ∼ N ( 0 , 1 ) . The matrix of interactions , Z , is then generated via the nor , Z ( i ) ∈ Rn×d , malized X where Z = Z ( i)·,j = X·,i fi X·,j . The ground truths w ∈ Rd×1 and Q ∈ Rd×d are generated based on the weak hierarchical structure Q·,j1 ≤ |wj| , j = 1 , . . . , d . In addition , we vary the ratio of coefficient sparsity , ie , the portion of zero entries in w and Q , from 30 % to 85 % . Then , the outcome 2 Z · vec(Q ) + where vector Y is constructed as Y = Xw + 1 X and Z are normalized to zero mean and unit standard deviation and ∼ N ( 0 , 0.01· I ) . We use sample size n = 100 and 200 and we choose the number of main effects d from {100 , 200 , 300 , 400 , 500 , 600} . The parameter of the l1 penalty , λ , is chosen from {1 , 3 , 5 , 10 , 20} . All algorithms are executed on a 64 bit machine with Intel(R ) Core(TM ) quad core processor ( i7 3770 CPU @ 3.40 GHz ) and 16.0 GB memory . We terminate the algorithm when the maximum relative difference of the coefficients between two consecutive iterations is less than 1e−5 . We run 20 trials for each setting and report the average execution time . The detailed results are shown in Table 1 .
From Table 1 , we observe that eWHL is significantly faster than cvxWHL . Our algorithm is up to 25 times faster than the competing algorithm . As the dimension increases , the running time of cvxWHL increases much faster than our proposed algorithm . Specifically , when the number of individual features increases to 400 ( corresponds to 80200 interactions ) , cvxWHL may take more than one thousand seconds , while the proposed eWHL is reasonably fast even when the number of total variables is around two hundred thousands . To make further comparisons of efficiency , we randomly generate three synthetic data sets where the weak hierarchical structure between main effects and interactions holds . The three data sets are of the same sample size n = 100 and
Figure 1 : Comparison of the running time and the number of iterations by the two algorithms . Three synthetic data sets are generated where the portions of zeros in the ground truth are 85 % , 60 % , 30 % respectively . The plots in the same row correspond to the same data set . The plots in the left column present the running time and those in the right column show the number of iterations . the number of individual features is d = 300 . The ratios of zero entries in the ground truth are 85 % , 60 % and 30 % respectively . The regularization parameters are chosen from {0.5 , 1 , 2 , 4 , 6 , 8 , 16 , 32 , 64} . On each data set , we first run cvxWHL , and then the objective value of ( 5 ) in the final step is recorded . Then , we run the proposed eWHL and terminate the algorithm when the objective value of ( 5 ) is less than the one obtained by cvxWHL . The running time and the number of iterations needed to achieve the same objective value of both algorithms are reported in Figure 1 . We can observe from Figure 1 that the proposed eWHL is much faster than cvxWHL .
Moreover , we also conduct an experiment to compare the recovery performance of eWHL and cvxWHL . We generate synthetic data sets with sample size n = 100 and the number of individual features is d = 50 ( 1225 cross interactions ) . The number of non zero main effects varies from {3 , 4 , 5 , 6 , 7} and the number of non zero interaction effects is from {2 , 4 , 5 , 8 , 10} , respectively . For each setting , ten synthetic data sets are generated with noise ∼ N ( 0 , 0.01 · I ) . We run both eWHL and cvxWHL with parameter selected via 5 fold cross validation . Then we compute the sensitivity and specificity of recovery ( where non zero entries are positive and zero entries are negative ) . The means of sensitivity and specificity are plotted in Figure 2 . We can observe that
Table 1 : Comparison of execution time ( second ) of the proposed algorithm for the non convex weak hierarchical Lasso ( eWHL ) and the one for the convex relaxed formulation ( cvxWHL ) on synthetic data . The penalty parameters used in the experiment are from {1 , 3 , 5 , 10 , 20} . The data is generated under the weak hierarchical constraints where the portion of sparse coefficients is controlled to 85 % , 60 % and 30 % . Two sample sizes , n = 100 and n = 200 , are used and we vary the number of individual features from {200 , 300 , 400 , 500 , 600} corresponding to {20100 , 45150 , 80200 , 125250 , 180300} interactions ( including the self product terms ) .
1 196.5536 15.9318 12.3
336.7086 35.6846 9.4
547.0450 52.8138 10.4
1018.9779 88.0526 11.6
3 54.8801 10.7613 5.1
213.7712 23.3044 9.2
280.6981 35.0482 8.0
757.2096 66.0113 11.5 n = 100
5 43.3018 7.2212 6.0
186.7997 17.9931 10.4
207.8486 29.5107 7.0
524.9644 59.7805 8.8
2543.5021 161.7944 15.7
1594.9729 100.3758 15.9
1517.9605 82.7961 18.3
106.6262 15.1405 7.0
187.7983 33.3861 5.6
418.9647 66.8376 6.3
1501.3934 112.5519 13.3
40.3105 9.6837 4.2
131.7578 22.2763 5.9
276.2089 43.0676 6.4
801.0146 80.5276 9.9
29.1357 6.9516 4.2
106.2882 16.3251 6.5
169.4631 35.7516 4.7
548.8402 60.2488 9.1
1976.0945 159.8307 12.4
1733.8494 112.8232 15.4
1814.1974 80.3703 22.6
85 % Ground Truth Sparsity
10 27.2806 5.6287 4.8
109.7893 11.5569 9.5
170.4894 18.1944 9.4
333.0070 42.2917 7.9
887.8254 71.1211 12.5
20 15.7909 2.6236 6.0
54.9521 10.8269 5.1
85.1425 13.8530 6.1
204.2017 18.6453 11.0
462.2604 40.9529 11.3
1 116.8207 16.4134 7.1
319.6003 38.5045 8.3
921.7651 80.4882 11.5
3 24.6601 9.5164 2.6
147.5044 20.0161 7.4
376.4949 54.1618 7.0
1405.9651 127.0921 11.1
1142.2343 89.1293 12.8
60 % Ground Truth Sparsity
20.8624 5.4949 3.8
61.3653 12.3395 5.0
131.9086 20.5413 6.4
362.7110 38.6862 9.4
815.4298 50.8628 16.0
10.3064 3.3569 3.1
38.3189 9.2993 4.1
84.4169 11.9166 7.1
206.0816 25.5497 8.1
323.4841 34.4373 9.4
113.3342 18.3514 6.2
290.0877 47.8122 6.1
686.8900 69.1413 9.9
1333.8803 114.5243 11.6
44.1169 10.5571 4.2
155.0435 26.1554 5.9
297.7161 41.3634 7.2
861.6311 73.6990 11.7
1622.9061 175.9730 9.2
1205.8489 140.7086 8.6
30 % Ground Truth Sparsity
2826.0083 197.5593 14.3
1558.1431 132.2163 11.8
1332.3515 107.5831 12.4 d
200
300
400
500
600
200
300
400
500
600
200
300
400
500
600
Methods cvxWHL eWHL speedup cvxWHL eWHL speedup cvxWHL eWHL speedup cvxWHL eWHL speedup cvxWHL eWHL speedup cvxWHL eWHL speedup cvxWHL eWHL speedup cvxWHL eWHL speedup cvxWHL eWHL speedup cvxWHL eWHL speedup cvxWHL eWHL speedup cvxWHL eWHL speedup cvxWHL eWHL speedup cvxWHL eWHL speedup cvxWHL eWHL speedup n = 200
5 17.8850 8.3827 2.1
112.5928 16.4566 6.8
256.8054 39.1673 6.6
964.0598 70.0550 13.8
27.2844 8.1777 3.3
131.7942 21.9835 6.0
226.6632 37.3495 6.1
729.1297 63.4899 11.5
987.4595 96.3447 10.2
52.1613 9.7081 5.4
205.4368 25.5594 8.0
560.1970 45.9561 12.2
10 9.1765 5.4922 1.7
59.0820 10.3588 5.7
144.3066 26.6412 5.4
286.2120 42.0014 6.8
873.6990 76.1834 11.5
18.7616 5.1257 3.7
85.8886 13.7322 6.3
166.2235 25.9975 6.4
310.6121 47.9597 6.5
1063.2823 73.7633 14.4
40.8929 6.8616 6.0
142.9171 15.9962 8.9
332.3118 34.6420 9.6
584.2366 63.9484 9.1
20 4.7783 3.9255 1.2
36.2484 6.9153 5.2
81.4817 14.7667 5.5
165.2558 29.0936 5.7
261.6806 45.0594 5.8
11.9756 4.1127 2.9
44.4029 10.5702 4.2
85.4570 16.0270 5.3
202.0412 31.5058 6.4
333.8406 52.2213 6.4
25.3084 4.0905 6.2
97.5106 10.6426 9.2
204.4852 23.9751 8.5
313.8258 35.2531 8.9
139.4226 18.5023 7.5
275.4393 41.4815 6.6
916.5276 75.6108 12.1
1460.8334 114.9244 12.7
116.3866 13.6312 8.5
162.5627 27.6094 5.9
510.0533 47.3789 10.8
900.1424 71.4278 12.6
85.1606 8.7261 9.8
139.4590 23.3748 6.0
342.0358 38.8362 8.8
767.7501 58.4604 13.1
50.2425 7.1346 7.0
79.0609 14.2635 5.5
208.9260 23.9130 8.7
576.6498 37.5124 15.4
2799.5549 186.1483 15.0
2842.9022 119.8450 23.7
2076.6074 85.9816 24.2
1148.0632 65.2515 17.6
23.8680 4.4546 5.4
41.2985 7.9047 5.2
104.5098 17.6649 5.9
242.9080 25.9513 9.4
460.4660 41.2607 11.2
223.2468 20.8344 10.7
575.3758 52.3714 11.0
1688.7030 92.1627 18.3
165.9219 15.0214 11.0
223.1688 33.1059 6.7
814.6646 60.4650 13.5
2003.7611 154.1934 13.0
2040.7488 102.7105 19.9
1632.3245 84.2729 19.4
4067.0519 165.9264 24.5
2795.7589 179.4403 15.6
2128.9981 146.7908 14.5
1946.2750 102.6978 19.0
1140.4244 71.8432 15.9 both algorithms achieve high recovery rate while directly solving the original weak hierarchical Lasso leads to slightly better performance in recovering the non zero effects .
In this experiment , we compare the classification performance of the proposed eWHL with the convex relaxation and other classifiers on the task of discriminating the MCI
4.2 Classification Comparison on ADNI Data In this experiment , we compare the weak hierarchical Lasso with its convex relaxation as well as other classifiers on the Alzheimer ’s Disease Neuroimaging Initiative ( ADNI ) data set .
In Alzheimer ’s Disease ( AD ) research , Mild Cognitive Impairment ( MCI ) is an intermediate state between normal elderly people and AD patients [ 24 ] . The MCI patients are considered to be at high risk of progression to AD . Many recent work focus on how to accurately predict the MCIAD conversion and identifying significant bio markers for the prediction [ 8 , 10 , 12 , 19 , 21 , 28 , 31 , 14 ] .
Figure 2 : Comparison of eWHL and cvxWHL in terms of recovery on synthetic data sets .
Table 2 : The performance of MCI converter vs . MCI non converter classification achieved by random forest ( RF ) , Support Vector Machine ( SVM ) , Sparse Logistic Regression ( spsLog ) , the convex relaxed weak hierarchical Lasso ( cvxWHL ) and the proposed algorithm ( eWHL ) . Classifiers are performed on main effects only ( top ) and on both the main effects and interactions ( bottom ) . The average and standard deviation of accuracy , sensitivity and specificity obtained from 10 fold cross validation are reported .
Main Effects Only
Accuracy ( % ) Sensitivity ( % ) Specificity ( % )
Accuracy ( % ) Sensitivity ( % ) Specificity ( % )
RF
74.23 ± 8.67 78.75 ± 14.00 69.29 ± 11.63
RF
71.26 ± 10.22 83.04 ± 13.18 58.10 ± 23.23
SVM spsLog
74.34 ± 9.56 75.22 ± 8.72 80.18 ± 13.88 80.18 ± 13.89 69.52 ± 13.74 69.76 ± 12.80 Main Effects + Interactions 73.57 ± 10.30 59.45 ± 14.43 74.29 ± 16.22 59.29 ± 17.83 72.86 ± 12.46 60.00 ± 15.42 spsLog
SVM cvxWHL eWHL
NA NA NA
NA NA NA cvxWHL
75.22 ± 11.02 75.71 ± 19.11 74.52 ± 16.84 eWHL
77.42 ± 8.50 77.14 ± 12.05 77.62 ± 15.02 subjects who convert to dementia ( ie , MCI converter ) within a three year period from the MCI subjects who remain at MCI ( ie , MCI non converter ) . The features used in the experiment ( provided by our clinical collaborators ) involve demographic information such as age , gender , years of education , clinical information such as scores of mini mental state examination ( MMSE ) , Auditory Verbal Learning Test ( AVLT ) , and the bio markers including status of Apolipoprotein E , volume of hippocampus , thickness of Mid Temporal Gray Matter . There are 133 samples in total and the number of individual features is 36 ( corresponds to 630 two way interactions ) . The interactions are generated by the normalized individual features and are normalized before entering the model . Since this is a classification task with binary labels , we replace the least square loss with logistic loss in the weak hierarchical Lasso . Besides the non convex and convex weak hierarchical Lasso , we apply random forest ( RF ) , Support Vector Machine ( SVM ) and sparse logistic regression on main effects , and on both main effects and interactions , respectively . We report the means and standard deviations of accuracy , sensitivity and specificity obtained from 10 fold cross validation . The penalty parameters are tuned via 5 fold cross validation in the training procedure . The sample statistics are shown in Table 3 and the classification performance is reported in Table 2 .
Table 3 : The statistics of the ADNI data set used in our experiment . The MCI converters ( MCI cvt ) are characterized as positive samples and the MCI non converters ( MCI non cvt ) are used as negative samples .
Total
( + ) MCI cvt
( ) MCI non cvt
# of samples
# of main effects # of interactions
133 36 630
71
62
From Table 2 , we can observe that , if we only use individual features for classification , then all the classifiers are biased towards the positive class , ie , MCI converter . When interactions are included , we observe that the performances of random forest and SVM become worse . One possible reason is that the large number of variables brought by the interactions weakens their discriminative power . This is not the case for sparse logistic regression , which demonstrates the importance of feature selection . We can observe from the table that the convex relaxed weak hierarchical Lasso and the non convex weak hierarchical Lasso achieve much better classification performance than the competitors . The improvement of the classification performance demonstrates the effectiveness of imposing hierarchical structures in interaction models . In addition , the superior classification performance ( around 77 % accuracy , sensitivity and specificity ) of the proposed eWHL demonstrates that directly solving the non convex weak hierarchical Lasso leads to solutions of higher quality than the convex relaxation . 5 . CONCLUSIONS
In this paper , we propose an efficient algorithm , eWHL , to directly solve the non convex weak hierarchical Lasso . One critical step in eWHL is to compute the proximal operator associated with the non convex penalty functions . As one of our major contributions , we show that the proximal operator associated with the regularization function in weak hierarchical Lasso admits a closed form solution . Furthermore , we develop an efficient algorithm which computes each subproblem of the proximal operator with a time complexity of O(d log d ) . Extensive experiments on both synthetic and real data sets demonstrate the superior performance of the proposed algorithm in terms of efficiency and accuracy .
In the future , we plan to apply the non convex weak hierarchical Lasso to other important and challenging applications such as depression study [ 20 ] . In addition , we plan to extend the proposed techniques to solve the non convex strong hierarchical Lasso formulation . 6 . ACKNOWLEDGEMENT
This work was supported in part by NIH ( R01 LM010730 ) and NSF ( IIS 0953662 , MCB 1026710 , and CCF 1025177 ) . 7 . REFERENCES [ 1 ] A . Beck and M . Teboulle . A fast iterative shrinkage thresholding algorithm for linear inverse problems . SIAM Journal on Imaging Sciences , 2(1):183–202 , 2009 .
[ 2 ] J . Bien , J . Taylor , and R . Tibshirani . A lasso for hierarchical interactions . The Annals of Statistics , 41(3):1111–1141 , 2013 .
[ 3 ] J . Bien and R . Tibshirani . hierNet : A Lasso for
[ 18 ] K . Koh , S J Kim , and S . Boyd . An interior point
Hierarchical Interactions , 2013 . R package version 15
[ 4 ] R . J . Cadoret , W . R . Yates , G . Woodworth , M . A . Stewart , et al . Genetic environmental interaction in the genesis of aggressivity and conduct disorders . Archives of General Psychiatry , 52(11):916 , 1995 .
[ 5 ] E . J . Candes and J . Romberg . Quantitative robust uncertainty principles and optimally sparse decompositions . Foundations of Computational Mathematics , 6(2):227–254 , 2006 .
[ 6 ] N . H . Choi , W . Li , and J . Zhu . Variable selection with the strong heredity constraint and its oracle property . Journal of the American Statistical Association , 105(489):354–364 , 2010 .
[ 7 ] A . d’Aspremont , L . El Ghaoui , M . I . Jordan , and
G . R . Lanckriet . A direct formulation for sparse pca using semidefinite programming . In NIPS , volume 16 , pages 41–48 , 2004 .
[ 8 ] C . Davatzikos , P . Bhatt , L . M . Shaw , K . N .
Batmanghelich , and J . Q . Trojanowski . Prediction of mci to ad conversion , via mri , csf biomarkers , and pattern classification . Neurobiology of aging , 32(12):2322–e19 , 2011 .
[ 9 ] J . F . Dawson and A . W . Richter . Probing three way interactions in moderated multiple regression : development and application of a slope difference test . Journal of Applied Psychology , 91(4):917 , 2006 .
[ 10 ] D . Devanand , G . Pradhaban , X . Liu , A . Khandji ,
S . De Santi , S . Segal , H . Rusinek , G . Pelton , L . Honig , R . Mayeux , et al . Hippocampal and entorhinal atrophy in mild cognitive impairment prediction of alzheimer disease . Neurology , 68(11):828–836 , 2007 . [ 11 ] T . C . Eley , K . Sugden , A . Corsico , A . M . Gregory , P . Sham , P . McGuffin , R . Plomin , and I . W . Craig . Gene–environment interaction analysis of serotonin system markers with adolescent depression . Molecular psychiatry , 9(10):908–915 , 2004 .
[ 12 ] C . Fennema Notestine , D . J . Hagler , L . K . McEvoy ,
A . S . Fleisher , E . H . Wu , D . S . Karow , and A . M . Dale . Structural mri biomarkers for preclinical and mild alzheimer ’s disease . Human brain mapping , 30(10):3238–3253 , 2009 .
[ 13 ] J . Gatt , C . Nemeroff , C . Dobson Stone , R . Paul ,
R . Bryant , P . Schofield , E . Gordon , A . Kemp , and L . Williams . Interactions between bdnf val66met polymorphism and early life stress predict brain and arousal pathways to syndromal depression and anxiety . Molecular psychiatry , 14(7):681–695 , 2009 .
[ 14 ] P . Gong , J . Ye , and C . Zhang . Multi stage multi task feature learning . In NIPS , pages 1997–2005 , 2012 . [ 15 ] P . Gong , C . Zhang , Z . Lu , J . Huang , and J . Ye . A general iterative shrinkage and thresholding algorithm for non convex regularized optimization problems . In ICML , 2013 .
[ 16 ] P . Gong , C . Zhang , Z . Lu , J . Huang , and J . Ye . GIST :
General Iterative Shrinkage and Thresholding for Non convex Sparse Learning . Tsinghua University , 2013 .
[ 17 ] T . Hastie , R . Tibshirani , J . Friedman , T . Hastie , J . Friedman , and R . Tibshirani . The elements of statistical learning , volume 2 . Springer , 2009 . method for large scale 1 regularized logistic regression . Journal of Machine learning research , 8(7 ) , 2007 .
[ 19 ] H . Li , Y . Liu , P . Gong , C . Zhang , J . Ye , A . D . N .
Initiative , et al . Hierarchical interactions model for predicting mild cognitive impairment ( mci ) to alzheimer ’s disease ( ad ) conversion . PloS one , 9(1):e82450 , 2014 .
[ 20 ] Y . Liu , Z . Nie , J . Zhou , M . Farnum , V . A . Narayan ,
G . Wittenberg , and J . Ye . Sparse generalized functional linear model for predicting remission status of depression patients . In Pacific Symposium on Biocomputing . Pacific Symposium on Biocomputing , volume 19 , pages 364–375 . World Scientific , 2013 .
[ 21 ] D . A . Llano , G . Laforet , and V . Devanarayan . Derivation of a new adas cog composite using tree based multivariate analysis : prediction of conversion from mild cognitive impairment to alzheimer disease . Alzheimer Disease & Associated Disorders , 25(1):73–84 , 2011 .
[ 22 ] D . C . Montgomery , E . A . Peck , and G . G . Vining .
Introduction to linear regression analysis , volume 821 . Wiley , 2012 .
[ 23 ] N . Parikh and S . Boyd . Proximal algorithms .
Foundations and Trends in optimization , 1(3):123–231 , 2013 .
[ 24 ] R . C . Petersen . Mild cognitive impairment clinical trials . Nature Reviews Drug Discovery , 2(8):646–653 , 2003 .
[ 25 ] P . Radchenko and G . M . James . Variable selection using adaptive nonlinear interaction structures in high dimensions . Journal of the American Statistical Association , 105(492):1541–1553 , 2010 .
[ 26 ] M . J . Somers . Organizational commitment , turnover and absenteeism : An examination of direct and interaction effects . Journal of Organizational Behavior , 16(1):49–58 , 1995 .
[ 27 ] R . Tibshirani . Regression shrinkage and selection via the lasso . Journal of the Royal Statistical Society . Series B ( Methodological ) , pages 267–288 , 1996 .
[ 28 ] J . Ye , M . Farnum , E . Yang , R . Verbeeck , V . Lobanov ,
N . Raghavan , G . Novak , A . DiBernardo , V . A . Narayan , et al . Sparse learning and stability selection for predicting mci to ad conversion using baseline adni data . BMC neurology , 12(1):46 , 2012 .
[ 29 ] M . Yuan , V . R . Joseph , and H . Zou . Structured variable selection and estimation . The Annals of Applied Statistics , pages 1738–1757 , 2009 .
[ 30 ] P . Zhao , G . Rocha , and B . Yu . The composite absolute penalties family for grouped and hierarchical variable selection . The Annals of Statistics , 37(6A):3468–3497 , 12 2009 .
[ 31 ] J . Zhou , J . Liu , V . A . Narayan , and J . Ye . Modeling disease progression via multi task learning . NeuroImage , 78:233–248 , 2013 .
[ 32 ] H . Zou , T . Hastie , and R . Tibshirani . Sparse principal component analysis . Journal of computational and graphical statistics , 15(2):265–286 , 2006 .
