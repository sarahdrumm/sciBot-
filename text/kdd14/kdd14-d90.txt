Networked Bandits with Disjoint Linear Payoffs
Centre for Quantum Comp . & Intelligent Syst .
University of Technology , Sydney
Meng Fang
235 Jones Street , Ultimo , NSW 2007 , Australia
MengFang@studentutseduau
Dacheng Tao
Centre for Quantum Comp . & Intelligent Syst .
235 Jones Street , Ultimo , NSW 2007 , Australia
University of Technology , Sydney DachengTao@utseduau
ABSTRACT In this paper , we study ‘networked bandits’ , a new bandit problem where a set of interrelated arms varies over time and , given the contextual information that selects one arm , invokes other correlated arms . This problem remains underinvestigated , in spite of its applicability to many practical problems . For instance , in social networks , an arm can obtain payoffs from both the selected user and its relations since they often share the content through the network . We examine whether it is possible to obtain multiple payoffs from several correlated arms based on the relationships . In particular , we formalize the networked bandit problem and propose an algorithm that considers not only the selected arm , but also the relationships between arms . Our algorithm is ‘optimism in face of uncertainty’ style , in that it decides an arm depending on integrated confidence sets constructed from historical data . We analyze the performance in simulation experiments and on two real world offline datasets . The experimental results demonstrate our algorithm ’s effectiveness in the networked bandit setting .
Categories and Subject Descriptors H35 [ Information Systems ] : Social and Information Networks ; 126 [ Computing Methodologies ] : [ Learning ]
General Terms Algorithm , Theory
Keywords Networked bandits ; social network ; exploration/exploitation dilemma
1 .
INTRODUCTION
A multi armed bandit problem ( or bandit problem ) is a sequential decision problem defined by a set of actions ( or arms ) . The term ‘bandit’ originates from the colloquial term for a casino slot machine ( ‘a one armed bandit’ ) , in which a player ( or a forecaster ) faces a finite number of slot machines ( or arms ) . The player sequentially allocates coins ( one at a time ) to different machines and earns money ( or payoff ) depending on the machine selected . The goal is to earn as high a payoff as possible .
Robbins formalized this problem in 1952 [ 20 ] ; in the multiarmed bandit problem , K arms exist that are associated with unknown payoff distributions , and a forecaster can select an arm sequentially . In each round of play , a forecaster selects one arm and then receives the payoff from the selected arm . The forecaster ’s aim is to maximize the total cumulative payoff , ie , the sum of the payoffs of the chosen arms in total . Since the forecaster does not know the process generating the payoffs but has historical payoffinformation , the bandit problem highlights the fundamental difficultly of decision making in the face of uncertainty : balancing the decision of whether to exploit past choices or make new choices with the hope of discovering a better one .
The bandit problem has been studied for many years , with works primarily focusing on the theory and designing different algorithms based on different settings , such as the stochastic setting , adversarial setting , and contextual setting [ 9 ] . In real world applications , the multi armed bandit problem is an effective way of solving situations where one encounters an exploration exploitation dilemma . It has historically been used to decide which clinical trial is better when multiple treatments are available for a given disease and there is a need to decide which treatment to use on the next patient .
Modern technologies have created many opportunities for use of the bandit problem , and it has a wide range of applications including advertising , recommendation systems , online systems , and games . For example , an advertising task may be the choice of which advertisement to display to the next visitor to a web page , where the payoff is associated with the visitor ’s actions . More recently , the bandit algorithm has been used in personalized recommendation tasks [ 17 ] , where a user visits a website and the system collects the user ’s information . The system selectively provides content from a content pool through the user ’s current and past behaviors analyzing to best satisfy the user ’s needs , and the payoff is based on user click feedback .
All the above bandit problems have the major underlying assumption that all the arms are independent , which is inappropriate for web based social network applications . In a network , including social networks , the users are connected by relationships [ 2 , 22 ] . Contextual information can
1106
Round t=n
Historical data
Decision algorithm
Round t=n’
Historical data
Payoff records
Feedback
Payoff records
Arms on network topology Different arms with their relations
Chosen arm with its relations
Payoffs from multiple arms
Content
Contextual information
Round t
Figure 1 : An overview of networked bandits at different rounds . The network is changing over time . An arm ( user ) can invoke other arms ( relations ) and have different relations at different rounds . Given the contextual information an arm is chosen by the decision algorithm for getting multiple payoffs ( feedback ) . The algorithm improves the selection strategy after collecting new payoffinformation . be obtained from other users and can be spread via these relationships . Content promoted to one user provides feedback , not only from that user , but also from his/her relations . For example , a user of Twitter or Facebook can read a tweet/message and can re post someone else ’s tweet/message , allowing the user to quickly share it with his/her followers . Impact can be assessed by counting the number of ‘favorites/likes’ from different users’ pages . Therefore , careful selection of a user for tweet/message posting can maximize the number of ‘favorites/likes’ .
Our study is motivated by the observation that even when a user is randomly selected for promotion , other users close to the selected user in the network will be influenced [ 18 , 22 ] . Specifically , as shown in Figure 1 , in a social network , if we promote a content to a user , the user may share it with others and the payoffs can be collected from the user and its relations . The goal is to gain higher payoffs . The process is similar to share then like , which occurs daily in social networks and needs to be considered for optimized recommendation and advertising tasks , the important point being that the context is extended from the selected user to all other invoked users . There are several challenges to realizing this problem . First , only partial information is available about the chosen users when content is posted , and the information of other users is unknown . Therefore , there is a dilemma of whether the system should select a user with the best payoff history or a new user in order to explore more possibilities . Second , the content may frequently change and few overlapping historical records may exist . Furthermore , relationships exist between users and these relationships may change over time . These challenges inspire us to formalize the networked bandit problem .
The above problem can be considered a balance of the trade off between exploration ( discovering a new user ) and exploitation ( using the current best user ) when network topology is known .
We formalize a well defined but simple setting for the networked bandit problem , in which there exist K arms connected by network topology G . We propose an approach in which a learning algorithm optimally selects an arm at each round based on contextual information and the network topology information of arms . The networked bandit problem can be considered an extension of the contextual multi armed bandit problem , the difference being that in our problem the arm can be connected to other arms and the payoffs come from the multiple arms .
Our contribution is three fold : first , we formalize a new networked bandit problem motivated by real network applications ; second , we provide an algorithm based on confidence sets to solve it along with theoretical analysis ; and third , we design a set of experiments to test and evaluate the algorithm . To the best of our knowledge , we define and solve this problem for the first time and answer the fundamental question of how to define regret when payoffs come from interrelated multiple arms . We design an effective strategy to select arms in order to increase payoffs over time , known as NetBandits , which provides a solution to this problem . Our approach is an ‘optimism in the face of uncertainty’ style algorithm that considers the integrated confidence sets and we prove a regret bound for it . Finally , we analyze empirically the performance , which shows that our algorithm is effective in the networked bandit setting .
2 . RELATED WORK
The traditional multi armed bandit problem does not assume that side information is observed . The forecaster ’s goal is to maximize the sum of payoffs over time based on the historical payoff information . There are two basic settings . In the first , the stochastic setting , the payoffs are iid drawn from an unknown distribution . The upper confidence bound ( UCB ) strategy has been used to explore the exploration exploitation trade off [ 4 , 6 , 16 ] , in which an upper bound estimate is constructed on the mean of each arm at a fixed confidence level , and then the arm with the best estimate is selected . In the second , the adversarial setting , the iid assumption does not exist . Auer et al . [ 7 ] proposed the EXP3 algorithm for the adversarial setting , which was later improved by Bubeck and Audibert [ 3 ] .
The contextual multi armed bandit problem is a natural extension of the original bandit problem . Our setting addresses bandit problem with contextual information . Compared to the traditional K armed bandit problem , the forecaster may use action features to infer the payoff in the contextual setting . This problem largely considers the linear model assumption about payoff of action [ 1 , 5 , 12 , 14 , 21 ] . Auer [ 5 ] proposed the LinRel algorithm , a UCB style algorithm that has a regret of ˜O(√T d ) . Dani et al . [ 14 ] stud
1107 ied the LinRel and provided an ˜O(d√T ) regret bound and proved this upper bound is tight . Chu et al . [ 12 ] provided the LinUCB and SupLinUCB algorithms , and proved an O(!T d log3 ( KT log(T )/δ ) ) regret bound for SupLinUCB that holds with probability 1 − δ . Abbasi Yadkori et al . [ 1 ] proposed an algorithm that modified the UCB style algorithm based on the confidence sets , and showed a regret of O(d log(1/δ)/∆ ) .
Recently , the bandit problem has been used in real life problems , such as recommendation systems and advertising . Li et al . [ 17 ] first introduced the bandit problem to recommendation systems by considering a personalized recommendation as a feature based exploration exploitation problem . This problem was formalized as a contextual bandit problem with disjoint linear payoffs and by focusing on the articleselection strategy based on user click feedback , maximizing the total number of clicks . The features of the users and articles were defined as contextual information , and the expected payoff of an arm was assumed to be a linear function of its contextual features , including the user and article information . Finally , the LinUCB algorithm was proposed to solve this problem and attained a good empirical regret . They further extended the algorithm as SupLinUCB and provided the theoretical analysis [ 12 ] .
There are limited studies that consider the networked bandit problem or that combine bandit problem and network . Buccapatnam et al . [ 10 ] considered the bandit problem in social networks , and assumed that the forecaster can take advantage of side observations of neighbors , except for the selected user ( arm ) . The side observations were used to update the sample mean of other related users and the payoff of the selected arm was collected each time , the goal once again being to maximize the total cumulative payoff of selected arms . Bnaya et al . [ 8 ] considered a bandit view for network exploration and proposed VUCB1 to handle the dynamic changes in arms when crawling the network . More recently , Cesa Bianchi et al . [ 11 ] considered the recommendation problem by taking advantage of the relationships between users in the network . They proposed GOB.Lin , which models the similarity between users and used this similarity to help predict the behavior of other users .
Our work belongs to the contextual bandit setting . However , in contrast to these previous studies we assume that the arms ( actions ) are correlated in the network . The selected arm can invoke other related arms and the forecaster obtains multiple payoffs from these arms . It is a more general setting in networked bandit problem . 3 . NETWORKED BANDITS
We consider a network G . Let V indicate the nodes in the network and E indicate the edges of the network . We can then use G = ( V,E ) to represent the networked bandits where v ∈ V can be considered as an arm and e ∈ E indicates the relationship between arms ; nodes here are correlated . Thus , given the network G and a node v , it is possible to obtain the information for the node v and its relations N ( v ) . In our formulation , we consider a sequential decision problem with contextual information . At round t , except for contextual information xt , we have a network topology of arms denoted by Gt . Given v we let Nt(v ) be its relations and Nt(v ) may change over time . If v is selected then Nt(v ) will also be invoked . We define this setting as networked bandits . Formally , a networked bandit algorithm A proceeds as follows : at each round t , the algorithm observes a set of arms Kt = {1 , 2,··· , k}t , contextual information xt , and the network topology Gt of arms associated with the relationships of arms . The set of relations of arm a is denoted by Nt(a ) . If we also consider the information of arms , we can redefine the context as a set Ct = {x1,t,··· , xk,t} by adding arms’ information . When the algorithm selects an arm at , the at will invoke other related arms Nt(at ) . Nt(at ) can be observed based on the network topology of arms . Before the decision algorithm selects the arm , it observes Gt , Ct , and Kt . Based on historical payoff records , the algorithm selects an arm at and receives a set of payoffs {yat} ∪ {ya|a ∈ Nt(at)} . The algorithm will improve the selection strategy after collecting new payoff information . It then proceeds to the next round t + 1 . Note that traditional contextual bandit problems usually assume that the arms are independent . However , in our problem , we assume that the correlation exists between the chosen arm and its relations . After a total T t=1 gat,t , where rounds the cumulative payoff is defined as"T gat,t = "a∈Nt(at ) ya + yat and ya is the payoff from arm relations . We rewrite gat,t as gat,t ="a∈Nt(at ) ya .
For this networked bandit problem , the algorithm A selects an arm at at each round t = 1 , 2,··· and receives the associate payoff gat,t . After n selections a1 , a2,··· , an we define the regret as follows : a . For simplicity , we use Nt(a ) to indicate both a and its
Rn = max a=1,··· ,k ga,t − gat,t .
( 1 )
The regret can now be used to compare the best decision with the algorithm A . In this problem , Rn is a random variable ; therefore , the goal is to calculate the expectation of Rn with high probability , and it is not easy to obtain expectation directly since its search space is large . Normally we try to bound the pseudo regret , ie ,
Rn = max a=1,··· ,k
E ga,t − E gat,t ,
( 2 ) where the pseudo regret competes against the optimal action in the expectation .
There are two important issues in the networked bandit problem : arms and their network topology . In the context of a social network , the users in the pool may be viewed as arms , the provided message or article as context , and the user ’s information as additional contextual information . The new context vector then summarizes information of both user and context . A payoff of 1 is incurred when a provided message is ‘favorited’ or ‘liked’ ; otherwise , the payoff is 0 . The network topology of a social network naturally constructs the relationships between users . When a message is posted to a user , the message can be seen by relations ( followers ) . The payoff can be collected from the user ’s page ( selected arm ) . Furthermore , any ‘like’ , ‘share’ , or ‘comment’ action by a follower will allow the message to be reposted on the follower ’s page and to be seen by the follower ’s friends . The payoff can then be collected from the followers’ pages ( invoked arms ) . In the special case that the follower does not repost the message , the payoff can be considered as 0 or the arm is not invoked . For simplicity , we only consider the selected arm and its relations . With these definitions of payoff , arm , and invoked arms , the collected payoff after selecting an arm involves the selected user and
1108 n#t=1 n#t=1 n#t=1 n#t=1 his/her relations . Thus , the payoff at round t is defined as gat,t ="a∈Nt(at ) ya .
It is assumed that algorithm A can observe the network topology prior to make a decision . This is intuitive , since network structure information between users can easily be collected or the network structure information can be obtained in advance . In practice , given an arm , we only need concern itself with the invoked arms , and therefore knowledge of full network topology is unnecessary . The invoked arms depend on how we define Nt(a ) . The worst case scenario is that the whole network needs to be searched to find the invoked arms and feedback ; however , we do not concern how to constrict Nt(a ) using such a network propagation model since , as stated above , we only focus on the selection strategy and we simplify the problem by only observing the invoked arms .
4 . ALGORITHM
In this work , we propose an algorithm to solve the networked bandit problem and show that an integrated confidence bound can efficiently be computed in a closed form when the payoff model of an arm is linear . As with previous contextual bandit work [ 17 ] , we assume that the expected payoff of an arm a is linear in context xt and coefficient wa . At round t , for arm a given context xa,t , we assume that the expected payoff of the arm a is a linear function :
E [ ya,t|xa,t ] = x⊤a,twa + ϵa ,
( 3 ) where different arms have different wa and ϵa is conditionally R sub Gaussian when R ≤ 0 is a fixed constant . Formally , this means that ∀λ and we have
2 ’ , E$eλϵa,t|xa,1:t , ϵa,1:t−1 % ≤ exp& λ2R2 where xa,1:t denotes the sequence xa,1 , xa,2,··· , xa,t and , similarly ϵa,1:t−1 denotes the sequence ϵa,1,··· , ϵa,t−1 . The arms therefore have disjoint linear payoffs . The decision of the algorithm lies on w with distribution ϵ . Based on our Rsub Gaussian assumption of the noise , we can obtain meaningful upper bound on the regret . According to this subGaussian condition , we know that E [ ϵa,t|xa,1:t , ϵa,1:t−1 ] = 0 and VAR[ϵa,t|xa,1:t , ϵa,1:t−1 ] ≤ R2 . The conditions therefore show that ϵa,t is bounded by a zero mean noise lying in an interval of length of at most 2R .
( 4 )
As the networked bandit problem , the algorithm faces a set of uncertainties of arms which involve Nt(a ) . We design a new algorithm which is the ‘optimism in the face of uncertainty’ principle , by maintaining confidence of parameter w for each arm . The basic idea is to construct the confidence sets for parameters of each disjoint payoff function and then provide an integrated upper bound .
We use technology from the ‘self normalized bound for vector valued martingales’ [ 19 ] and confidence sets [ 1 ] . For each arm ˆwa is defined as the L2 regularized least squares estimate of w∗a with regularization parameter λ > 0 :
ˆwa = ( X⊤a Xa + λ)−1X⊤a Ya ,
( 5 ) where Xa is the matrix whose rows are x1,··· , xna(t ) corresponding to historical contexts of an arm a and Ya ∈ Rna(t ) is the corresponding historical payoff vector . For a positive definite self adjoint operator V , we define ∥x∥V =!⟨x , V x⟩ as the weighted norm of vector x . It can be proved that ˆw
1109 lies with high probability in an ellipsoid centered at w∗ as follows :
Theorem 1 . [ 1 , 19 ] According to the ‘self normalized bound for vector valued martingales’ , let V = λI , λ> 0 , and V t = V +"t−1 n=1 xn ⊗ xn be the regularized design matrix underlying the covariates . Define yt = x⊤t w∗ + ϵt and assume that ∥w∗∥2 ≤ S . Then , for any 0 < δ < 1 , with probability at least 1−δ , for all t ≥ 1 we can bound w∗ in such a confidence set : Ct =(w∗ ∈ Rd : ∥ ˆwt − w∗∥V t ≤ R)2 log&|V t|1/2|λI|−1/2
’ + λ1/2S* .
( 6 ) In addition , if ∥xt∥ ≤ L then with probability at least 1 − δ , for all t ≥ 1 , we can bound w∗ in a new confidence set : t =(w∗ ∈ Rd : C′ ∥ ˆwt − w∗∥V t ≤ R)d log& 1 + tL2/λ
’ + λ1/2S* .
( 7 )
δ
δ
The above bound provides the confidence region at time t . It shows that with good choice of the right parts of the equation , w∗ always remains inside this ellipsoid for all times t with probability 1 − δ . Next , we show the bound of the arm with a single linear payoff .
Theorem 2 . Let ( x1 , y1),··· , ( xt−1 , yt−1 ) , xi ∈ Rd , yi ∈ R satisfy the linear model assumption . Furthermore , we have the same assumption as Theorem 1 . Then , for any 0 < δ < 1 , with probability at least 1 − δ , for all t ≥ 1 we can have :
∥x⊤ ˆw − x⊤w∗∥ ≤
∥x∥V −1 t ⎛⎝R)2 log&|V t|1/2|λI|−1/2
δ
’ + λ1/2S⎞⎠ .
( 8 ) In addition , if ∥xt∥ ≤ L then for all t ≥ 1 , with probability 1 − δ we can have :
∥x⊤ ˆw − x⊤w∗∥ ≤ t /R)d log& 1 + tL2/λ
δ
’ + λ1/2S0 .
( 9 )
∥x∥V −1
Proof .
∥x⊤ ˆw − x⊤w∗∥ = ∥x⊤( ˆw − w∗)∥ ≤ ∥x∥∥ ˆw − w∗∥ = ∥x∥V −1 t ∥ ˆw − w∗∥V t .
According to ( 6 ) , with probability at least 1−δ , for all t ≥ 1 , we have :
∥x⊤ ˆw − x⊤w∗∥ ≤
∥x∥V −1 t ⎛⎝R)2 log&|V t|1/2|λI|−1/2
δ
’ + λ1/2S⎞⎠ .
According to ( 7 ) , with probability at least 1− δ , for all t ≥ 1 we have :
∥x⊤ ˆw − x⊤w∗∥ ≤ t /R)d log& 1 + tL2/λ
δ
’ + λ1/2S0 .
∥x∥V −1
Lemma 1 . Given an arm a ∈ Kt with the context feature x , let ( xa,1 , ya,1 ) , ( xa,2 , ya,2),··· , ( xa,na(t−1 ) , ya,na(t−1 ) ) be history records of arm a before t and xa ∈ Xa and ya ∈ Ya , and let ˆwa = ( X⊤a Xa + λI)−1X⊤a Ya . We have x⊤w∗a ≤ x⊤ ˆwa+
∥x∥V −1 t ⎛⎝R)2 log&|V t|1/2|λI|−1/2
δ
’ + λ1/2S⎞⎠ .
( 10 )
Figure 2 : An example of the upper bound B in 10 arm networked bandits when t = 120 . Bar denotes the payo ff estimation and vertical line denotes the penalty of the estimation .
As shown in ( 10 ) , we have a possible upper bound of x⊤w∗a , which has two parts . The first term can be deemed as empirical expected estimation of payoff of the arm , and the second term can be considered as a penalty . This penalty is typically a high probability upper confidence bound on the payoff of the arm .
Thus , given an arm a and its relations Nt(a ) , we face the exploitation exploration problem . We use the integrated confidence bound on the payoffs of these invoked arms .
Lemma 2 . In the networked bandits , given an arm a ∈ Kt and the network relationship Nt(a ) , we obtain :
#a∈Nt(a ) #a∈Nt(a ) x⊤a ˆwa+ x⊤a w∗a ≤ #a∈Nt(a ) t ⎛⎝R)2 log&|V t|1/2|λI|−1/2
∥xa∥V −1
δ
’ + λ1/2S⎞⎠ .
( 11 )
We believe that the confidence bound can be successfully applied to this situation with the exploitation exploration trade off . We use the confidence bound generated by the confidence sets of parameters , defined by :
Ba,t = νa,t + ξa(t ) ,
( 12 )
νa,t = "a∈Nt(a ) x⊤ ˆwa,t indicates the expected value , and
ξa(t ) is the last term of ( 11 ) and indicates the penalty of the estimation . Figure 2 shows the upper bound of arms from our illustrative example . Each arm has the empirical payoff and a potential value . Thus , in each round , our algorithm selects an arm based on the estimation from the confidence bound , such that the predicted payoff is maximized . Our algorithm is shown in Algorithm 1 .
5 . REGRET ANALYSIS
We next provide a bound on the regret of our algorithm when run through the confidence sets constructed in Theorem 1 . We assume that the expected estimation of payoffis bounded . We can view this as a bound on parameters and the bound on the arms set . We state a bound on the regret of the algorithm as follows :
Algorithm 1 NetBandits Input : Kt , Gt , Ct , t = 1,··· , T 1 : for round t = 1 , 2,··· , T do 2 :
3 : 4 : 5 :
For each arm we can observe the features xa,t , a ∈ Kt , and the invoked arms Nt(a ) based on Gt for each a ∈ Kt do
Compute ˆwa according to ( 5 ) Compute the quality x⊤a,t ˆwa+
∥xa,t∥V −1 t ⎛⎝R$%%&2 log’|V t|1/2|λI|−1/2
Ba,t = !a∈Nt(a ) !a∈Nt(a ) end for Choose arm at = arg maxa∈Kt Ba,t for each node a ∈ Nt(at ) do end for
6 : 7 : 8 : Observe the multiple payoffs {ya,t|a ∈ Nt(at)} 9 : 10 : 11 : 12 : end for
Update Xa , Ya
δ
( + λ1/2S⎞⎠
Theorem 3 . On the networked bandits , assume that each arm ’s payoff function satisfies the linear model , and assume that the contextual vector is xa,t for each arm a ∈ Kt,|Kt| ≤ K and t = 1,··· , T . Then , for any 0 < δ < 1 , with probability at least 1 − δ , the cumulative regret satisfies
RT ≤ 2K!2βT ( δ)T log |I + XX⊤/λ| ,
( 13 ) where
βT ( δ ) =/R)2 log&|I + XX⊤/λ|1/2
δ
’ + λ1/2S02
.
Proof . Considering the instantaneous regret at round t , we select an optimal arm according to our algorithm . Thus , we have optimistic ( at , ˜wa ) and ˆwa for the N ( at ) . For round
1110 t , we rely on [ 1 ] to have : x⊤a,t ˜wa x⊤a,t ˆwa x⊤a,t ˜wa x⊤a,tw∗a x⊤a,tw∗a − #a∈N ( a∗ ) x⊤a,tw∗a − #a∈N ( at ) x⊤a,tw∗a − #a∈N ( at ) x⊤a,t ˆwa − #a∈N ( at ) x⊤a,t(w∗a − ˆwa ) + #a∈N ( at ) t ∥w∗ − ˆwt∥V −1 rt = #a∈N ( at ) ≤ #a∈N ( at ) = #a∈N ( at ) + #a∈N ( at ) = #a∈N ( at ) = #a∈N ( at ) + #a∈N ( at ) ≤ #a∈N ( at ) ra,t = x⊤a,tw∗a − x⊤a,t ˆwa + x⊤a,t ˆwa − x⊤a,t ˜wa ,
2!βt(δ)∥xt∥V −1 t ∥ ˆwt − ˜wt∥V −1
∥xt∥V −1
∥xt∥V −1
. t t t x⊤a,t( ˆwa − ˜wa )
For each arm a ∈ Nt(a ) , we define and we have
We then rewrite the instantaneous regret ( 14 ) as ra,t ≤ 2!βt(δ)∥xa,t∥V −1 t
. rt ≤ #a∈N ( at ) ra,t .
V −1 t t t t t
V −1
V −1 we have
, 12 , 12 .
Regarding to the fact that ra,t ≤ 2 , we have
Thus , with probability at least 1 − δ , for any T ≥ 1 , ra,t ≤ 2 min1!βt(δ)∥xa,t∥2 ≤ 2!βt(δ ) min1∥xa,t∥2 Given arms Nt(a ) , we define a′ = arg maxa ∥xa,t∥2 rt ≤ |Nt(a)|2!βt(δ)∥xa′ ,t∥V −1 , 12 . |Nt(a)|2!βt(δ)1∥xa′ ,t∥V −1
≤ |Nt(a)|2!βt(δ ) min1∥xa′ ,t∥2 T#t=1 T#t=1!βt(δ)1∥xa′ ,t∥V −1 t ∧ 12 ≤ 2K3445T T#t=11!βt(δ)1∥xa′ ,t∥V −1 t ∧ 1222 ≤ 2K3445T βT ( δ ) t ∧ 1’ .
T#t=1&∥xa,t∥2
T#t=1
RT =
≤ 2K rt ≤
V −1
V −1
According to log ( 1 + z ) ≤ z , we have : t ∧ 12 log |I + XV −1X| ≤ t−1#k=1
∥xk∥2
V −1 k
.
( 21 )
1111
Then according to z ≤ 2 log ( 1 + z ) , z ∈ [ 0 , 1 ] , we have
We choose V = λI , then we rewrite RT as
V −1 t−1#k=11∥xk∥2 k ∧ 12 ≤ 2 log |I + XV −1X| . RT ≤ 2K!2T βT ( δ ) log |I + XX⊤/λ| .
( 22 )
( 23 )
Lemma 3 . Assume that x ∈ Rd and V = λI . Then , for any 0 < δ < 1 , with probability at least 1 − δ , the bound is
RT ≤ 2K)2T d log&λ + δ’ + d log&1 +
·/λ1/2S + R)2 log& 1
( T − 1)L d
’ ’0 . ( 24 )
( T − 1)L
λd
We are mainly interested in the interrelated arms . Our regret bound depends on the number of invoked arms |Nt(a)| or loose K . Figure 3 shows experimentation of our bound applied to the networked bandit problem . Our algorithm keeps the regret as low as possible , and can reach Rt/t → 0 with high probability when t is large enough .
Figure 3 : An example of the regret value in 10 arm networked bandits . The experiments are repeated 100 times and the average regrets are shown . y = x is provided for comparison .
6 . PRACTICAL ISSUES
In real world applications , according to different assumptions about the network topology of arms , we can consider special cases of the networked bandit problem . We focus on Nt(a ) . In our algorithm , we make the very loose assumption that Nt(a ) varies over time . However , the network topology is sometimes stable over a fixed duration . For example , a school social network is stable for the duration of a semester . This means that Nt(a ) = N0(a ) , which is a special case of network bandits . In other cases , for example when inquiring users in the same company , we only need to consider their colleagues or a group of interest . 6.1 Dynamic network
In the networked bandit problem the network topology of arms is usually dynamic over time , which means for each
( 14 )
( 15 )
( 16 )
( 17 )
( 18 )
. Then
( 19 )
( 20 ) round t we have different Nt(a ) . Although we assume that Nt(a ) will be active after the forecaster selects a , we omit how to generate Nt(a ) and how arm a invokes Nt(a ) , which are not our primary concerns . Instead , we simplify our problem using the simple setting of selecting an arm and then receiving payoffs from invoked arms . We assume that we can observe invoked arms Nt(a ) . In practice , we can directly obtain Nt(a ) by predefining the arms , for example as neighbors or groups , or by observing feedback and collecting arms which provide feedback . We focus on how to select arms in order to maximize the total payoff , and therefore we concern the arms of Nt(a ) and the forecaster can obtain the invoked bandits over the course of collecting the payoffs from the network . In particular , at each round the algorithm observes the network topology of arms ; it then decides which arm to select using the knowledge of the network topology and historical payoffinformation .
In Algorithm 2 , we provide a pseudo code for the selection at each round in a dynamic network .
Algorithm 2 Selection at round t in dynamic network 1 : For each arm we have ˆwa and observer the context xa,t 2 : For each arm we collect Nt(a ) 3 : for each a ∈ Kt do 4 : 5 : end for 6 : Select arm at = arg maxa∈Kt Ba,t 7 : Observe the payoffs {ya,t|a ∈ Nt(at)} from the network 8 : For each arm a ∈ Nt(at ) update Xa , Ya and ˆwa
Compute Ba,t
6.2 Static network
We make the simple assumption that the network topology is fixed . In other words , the relationships between arms do not change . For all t , we have Gt = G0 , Kt = K0 and Nt(a ) = N0(a ) . For example , DBLP , Last.FM , and many offline social network datasets are of fixed duration . This is a degenerate version of our problem and can be solved using our algorithm .
In Algorithm 3 , we provide a pseudo code for the selection at each round in a static network .
Algorithm 3 Selection at round t in static network
1 : For each arm we have ˆwa and N0(a ) , and observer the context xa,t
2 : for a ∈ K0 do 3 : Compute Ba,t 4 : end for 5 : Select arm at = arg maxa∈K0 Ba,t 6 : Observe the payoffs {ya,t|a ∈ N0(at)} from the network 7 : For each arm a ∈ N0(at ) update Xa , Ya and ˆwa
6.3 Neighborhood or group
In networks , especially in social networks , a simple yet common assumption is that the node largely influences its neighborhoods or community [ 13 , 15 ] . Moreover , some applications only focus on people who have the same interest or are in a group . This makes it possible to assume that the selected arm only invokes its neighbors or a group ; that is , Nt(a ) = N eigt(a ) , where N eigt(a ) indicates the neighbors of a . We can only collect the payoffs of neighbors of an arm , and therefore Nt(a ) is appropriate . Although there are also two cases in this situation static and dynamic here we focus only on the neighborhood .
In Algorithm 4 , we provide a pseudo code for the selection at each round with specific Nt(a ) .
Compute Ba,t
Algorithm 4 Selection at round t with neighborhood 1 : For each arm we have ˆwa and observer the context xa,t 2 : For each arm we collect N eigt(a ) 3 : for a ∈ Kt do 4 : 5 : end for 6 : Select arm at = arg maxa∈Kt Ba,t 7 : Observe the payoffs {ya,t|a ∈ N eigt(at)} from the net8 : For each arm a ∈ N eigt(at ) update Xa , Ya and ˆwa work
7 . EXPERIMENTS 7.1
Illustrative Example
We first illustrate our model by a synthetic example ( Figure 4 ) , which contains 10 arms ( A0 A9 ) randomly connected at each round . At different rounds , the networks are different . At rounds t = 11 and t = 20 , the upper bound B ( the second row , blue ) is large ; however , the expected estimation is small ( the third row , red ) because the variance is large . Our algorithm selects the arm with maximal upper bound . We also show the real payoffs of all the arms , which are not known to the algorithm . At an early stage , the selection is poor compared to the real payoff ( the fourth row , green ) . At round t = 20 , our algorithm chooses A0 however , the best is A1 , illustrating that the expected estimation is small and the algorithm can try another arm that has potential , but with uncertainty . Later , selection becomes efficient and at t = 120 the algorithm chooses A1 since more information has been learned and the upper bound becomes more stable with lower penalty . This is close to the real situation and provides a good estimation . 7.2 Baselines and Performance Metric
In this section we evaluate the proposed NetBandits strategy on four synthetic datasets and two public real world datasets . We perform two types of experiments : simulation experiments and offline evaluation of two real applications . We compare our proposed method against two baselines : a state of art algorithm for the contextual bandit problem , referred to as TraBandits , and the random strategy . Since there is no existing method for the networked bandit problem , these methods are little altered for networked bandits . The details are follows :
• TraBandits : a state of art method for contextual bandits with linear payoff models [ 17 ] . The algorithm is a UCB style method with the linear payoff assumption that always selects the arm using highest UCB at each round .
• Random : a simple strategy that just randomly selects an arm .
We use two methods to assess the performance of our algorithm . We first analyze the average payoff at each round ,
1112 f f o y a P l a e R n o i t a m i t s E d e t c e p x E d n u o B r e p p U k r o w t e N t=11 t=20 t=40 t=80 t=120 t=200
Figure 4 : Illustrative synthetic example of exploration exploitation trade off . Bottom , arms with networked topology . Second row : the upper bound B for each arm computed using NetBandits . Third row : the expected estimation ν , where bar denotes the estimation and vertical line denotes the penalty of estimation . Fourth row : the real payoff of each arm . and then we analyze the cumulative payoff at each round , which ignores the performance of the algorithm at each fixed round but gives an overall view of the lifetime performance of the algorithm . 7.3 Simulation Experiments
We test our algorithm on a series of synthetic datasets . In contrast to previous work , we need to construct the network topology , which can be either static or dynamic . Static network is a special case of dynamic situation and static network is generated in advance and remains unchanged . We therefore construct the networked bandits based on the dynamic network as follows : we first construct a fixed number of nodes k , which are considered as arms . We then randomly create edges between them , which are used to generate the relations for each arm . Neighborhood is considered as relationship . For every node , we assign different norm random vector ui , ui ∈ R10 and we use the following stochastic model to generate its payoffs : ya(x ) = x⊤ua + ϵa , where ϵa is uniformly distributed in a bounded interval centered around zero and ua and ϵa are not known to the decision algorithm . For contextual information , at each round t we randomly create a set of context vectors {x1,t,··· , xk,t} , xk,t ∈ R10 . The network topology does not have strict assumptions and is created simply : in the dynamic situation , we generate the network topology at each round ( relationships between the nodes change at each round ) . We randomly create k2/3 edges between the nodes , and therefore for most nodes the relations will be no greater than k/3 .
We present the results from k =10 , 100 , 1000 , and 10000 arms with dynamic network topology . In Figure 5 and Figure 6 we present the results of average payoff and cumulative payoff ; our NetBandits outperforms the other baselines . TraBandits does not work well , indicating that best single arm does not always have the best payoff in a network but also depends on its relations . As per the network construction , the average payoff for each node is around k/3 if the node and its relations provide feedback , and the average payoff of TraBandits and Random is around k/3 . For example , as shown in Figure 5 , when k = 10 the payoff ranges from 3.2 to 3.6 ; when k = 100 the payoff ranges from 34 to 35 ; when k = 1000 the payoff ranges from 340 to 350 ; when k = 10000 the payoff ranges from 3450 to 3550 . However , NetBandits usually performs better except the earliest time points , and its value is greater than 4.2 when k = 10 , 40 when k = 100 , 400 when k = 1000 , and 4500 when k = 10000 . This is because NetBandits performs more exploration than exploitation to begin with . Figure 6 shows that our algorithm obtains the best cumulative payoff over all rounds . As average payoff improves , NetBandits also exhibits higher cumulative payoff . This indicates that more early exploration improves later selections , leading to a fairer assessment of the performance of the different algorithms .
The running time of NetBandits according to different numbers of arms and network topology is also shown in Table 1 , and demonstrates the running time increases rapidly as the number of the scale of the networks increase . For example k = 100 is slower than k = 10 by more than k2 but
1113
( a ) k=10
( b ) k=100
( c ) k=1000
( d ) k=10000
Figure 5 : The average payoff at each round in dynamic networks .
( a ) k=10
( b ) k=100
( c ) k=1000
( d ) k=10000
Figure 6 : The cumulative payoff at each round in dynamic networks .
( a ) Average payoff(Del )
( b ) Cumulative payoff(Del )
( c ) Average payoff(LFM )
( d ) Cumulative payoff(LFM )
Figure 7 : The average payoff and cumulative payoff for two real world datasets .
Arms Avg of invoked arms Total round Time ( second )
10 3 100 0.1
100 33
1,000 23.4
1,000 333
10,000 2034.2
10,000 3333
100,000 173,628.3
Table 1 : Running time results of NetBandits on four synthetic datasets . less than k3 . The time taken depends on the size of the network , including the number of nodes and edges . The time complexity of NetBandits is O(T KN Ω ) , where T is the total number of rounds , K is the number of arms , N indicates the average number of invoked arms , and Ω indicates the time taken to compute the parameters ; it is no more than O(T K 2Ω ) where N = K . It can be improved by calculating each arm in parallel for a large number of arms . 7.4 Real world Datasets Experiments
We also test our algorithm on two publicly available realworld datasets1 : Delicious Bookmarks , a dynamic dataset , denoted by Del ; and Last.FM , a static dataset , denoted by LFM .
Delicious Bookmarks is a social network for storing , sharing , and discovering web bookmarks . The Del dataset contains 1,861 nodes and 7,668 edges and 69,226 URLs described by 53,388 tags . Payoffs are created using the in
1http://grouplens.org/node/462 formation about the bookmarked URLs for each user : the payoff is 1 if the user bookmarked the URL , otherwise the payoff is 0 . Pre processing is performed by breaking the tags down into smaller fragile items made up of single words , ignoring the underscores , hyphens , and dashes . Each word is represented using the TF IDF context vector based on the words of all tags , ie , these feature vectors are the context vectors . PCA was performed on the dataset and the first 16 principle components selected as context vectors building a linear function based on payoff records for each user . This linear function generates a payoff when given a new context . At each round t , we provide xk,t ∈ R16 for all users k . The Last.FM dataset is a music website that builds a detailed profile of each user ’s musical taste by recording details of the tracks that the user listened to from a range of digital devices . LFM contains 1,892 nodes and 12,717 edges and has 17,632 artists described by 11,946 tags . We use the listened to artists information to construct payoffs : if the user listened to an artist at least once the payoff is 1 , otherwise the payoff is 0 . Similar pre processing is performed as Delicious Bookmarks . Compound tags are broken down into several corresponding single words resulting in 6,036 words . We represent context features using the TF IDF features , and after PCA the first 16 principle components are selected as context vectors . For each user we then build a linear function based on payoff records . This linear function
1114 can generate a payoff when given a new context . At each round , we provide xk,t ∈ R16 for all users k . We construct the network topology according to the social network of the users . Neighborhood is considered as relationship . The linear payoff function for each user is learned in advance and unknown to the algorithm , which decides its next selection according to previous feedback .
For the Del dataset , there exists the timestamp information that records when contact relationships were created , and we can therefore construct a dynamic network according to the timestamps . Timestamps are from 1146752335000 to 1288104100000 ; we therefore divide them into 14 groups according to the first three numbers ( 114 , 115 , . . . , 128 ) . We set the total rounds T = 14000 and update the network every 1000 rounds . For the Last.FM dataset , there is no time information , thus we construct a static network .
The results of average payoff and cumulative payoffare shown in Figure 7 . Our algorithm outperforms the other baselines . Although the two networks have a similar number of users , LFM has more relationships and the average and cumulative payoff results are higher than Del . For the average payoff of Del , there exist three low intervals marked by ( red ) rectangles in Figure 7(a ) . These occurred at the beginning and close to round t = 1000 and t = 10000 . Since many new nodes and edges are added at these rounds and NetBandits performs more exploration than exploitation . The payoffs improve after exploration . For the average payoffresults of LFM , there is a low interval at the start , denoted by the ( red ) rectangle in Figure 7(c ) , because NetBandits is trying to select possible better arms and perform exploration ; then later the performance improves . Figure 7(b)(d ) show that the cumulative payoff results of NetBandits increase faster , and are much greater than the other algorithms , and demonstrate that exploration does not hurt the total performance .
8 . CONCLUSION AND FUTURE WORK
In this paper we formalize a new bandit problem , termed networked bandits . We presented the novel problem of how to select the arm with multiple payoffs in networked bandits by considering a multi armed bandit of interconnected arms , one of which can invoke other related arms at each round . After selecting an arm , we can obtain payoffs from this arm and its relations .
We consider this approach in the contextual bandit setting and assume disjoint linear payoffs for arms . We propose a new networked bandit algorithm NetBandits that considers the uncertainty of the payoffs using integrated confidence sets . We also provide a regret bound for our solution . Our experiments show that it is better to consider both the network topology and the payoffs of arms , and we observe that our approach performs well in this setting .
The networked bandit problem requires further work . Some interesting problems still remain , such as how to model Nt(a ) . In our work we do not make any assumption about the structure of the network topology ; for example the hub may have higher priority , and it is possible to find a more efficient method for some fixed structures .
Another problem is arm complexity . We assume that one arm invokes other arms , which in turn can invoke other arms sequentially , with processing occurring at the same time . However in some real applications , the structure is possible to be much more complex and evolve over time , which is likely to delay the payoffs .
9 . ACKNOWLEDGMENTS
This work is supported by Australian Research Council
Projects FT 130101457 and DP 140102164 .
10 . REFERENCES [ 1 ] Y . Abbasi Yadkori , C . Szepesv´ari , and D . Tax . Improved algorithms for linear stochastic bandits . In NIPS , pages 2312–2320 , 2011 .
[ 2 ] A . Anagnostopoulos , R . Kumar , and M . Mahdian .
Influence and correlation in social networks . In KDD , pages 7–15 . ACM , 2008 .
[ 3 ] J Y Audibert and S . Bubeck . Regret bounds and minimax policies under partial monitoring . JMLR , 9999:2785–2836 , 2010 .
[ 4 ] J Y Audibert , R . Munos , and C . Szepesv´ari .
Exploration exploitation tradeoff using variance estimates in multi armed bandits . Theor . Comput . Sci . , 410(19):1876–1902 , 2009 .
[ 5 ] P . Auer . Using confidence bounds for exploitation exploration trade offs . JMLR , 3:397–422 , 2003 .
[ 6 ] P . Auer , N . Cesa Bianchi , and P . Fischer . Finite time analysis of the multiarmed bandit problem . Machine Learning , 47(2 3):235–256 , 2002 .
[ 7 ] P . Auer , N . Cesa Bianchi , Y . Freund , and R . E . Schapire .
The nonstochastic multiarmed bandit problem . SIAM J COMPUT , 32(1):48–77 , 2002 .
[ 8 ] Z . Bnaya , R . Puzis , R . Stern , and A . Felner . Social network search as a volatile multi armed bandit problem . HUMAN , 2(2):pp–84 , 2013 .
[ 9 ] S . Bubeck and N . Cesa Bianchi . Regret analysis of stochastic and nonstochastic multi armed bandit problems . Foundations and Trends in Machine Learning , 5(1):1–122 , 2012 .
[ 10 ] S . Buccapatnam , A . Eryilmaz , and N . B . Shroff .
Multi armed bandits in the presence of side observations in social networks . OSU , Tech . Rep , 2013 .
[ 11 ] N . Cesa Bianchi , C . Gentile , and G . Zappella . A gang of bandits . In NIPS , 2013 .
[ 12 ] W . Chu , L . Li , L . Reyzin , and R . E . Schapire . Contextual bandits with linear payoff functions . In AISTAS , pages 208–214 , 2011 .
[ 13 ] D . Crandall , D . Cosley , D . Huttenlocher , J . Kleinberg , and
S . Suri . Feedback effects between similarity and social influence in online communities . In KDD , pages 160–168 . ACM , 2008 .
[ 14 ] V . Dani , T . P . Hayes , and S . M . Kakade . Stochastic linear optimization under bandit feedback . In COLT , pages 355–366 , 2008 .
[ 15 ] T . Iwata , A . Shah , and Z . Ghahramani . Discovering latent influence in online social activities via shared cascade poisson processes . In KDD , pages 266–274 . ACM , 2013 .
[ 16 ] T . L . Lai and H . Robbins . Asymptotically efficient adaptive allocation rules . ADV APPL PROBAB , 6(1):4–22 , 1985 .
[ 17 ] L . Li , W . Chu , J . Langford , and R . E . Schapire . A contextual bandit approach to personalized news article recommendation . In WWW , pages 661–670 . ACM , 2010 .
[ 18 ] S . A . Myers , C . Zhu , and J . Leskovec . Information diffusion and external influence in networks . In KDD , pages 33–41 . ACM , 2012 .
[ 19 ] V . H . Pe˜na , T . L . Lai , and Q M Shao . Self normalized processes : Limit theory and Statistical Applications . Springer , 2008 .
[ 20 ] H . Robbins . Some aspects of the sequential design of experiments . In Herbert Robbins Selected Papers , pages 169–177 . Springer , 1985 .
[ 21 ] P . Rusmevichientong and J . N . Tsitsiklis . Linearly parameterized bandits . MATH OPER RES , 35(2):395–411 , 2010 .
[ 22 ] J . Tang , J . Sun , C . Wang , and Z . Yang . Social influence analysis in large scale networks . In KDD , pages 807–816 . ACM , 2009 .
1115
