Exponential Random Graph Estimation under Differential Privacy
Wentian Lu
University of Massachusetts Amherst wen@csumassedu
Gerome Miklau
University of Massachusetts Amherst miklau@csumassedu
Authors’ note : The original published version of this paper contained a regrettable error in the proof of differential privacy of the “ chain mechanism ” . This version of the paper corrects the error by replacing the chain mechanism with an ( , δ) differentially private mechanism , inspired by [ 17 ] , for estimating the alternating graph statistics discussed herein .
ABSTRACT The effective analysis of social networks and graph structured data is often limited by the privacy concerns of individuals whose data make up these networks . Differential privacy offers individuals a rigorous and appealing guarantee of privacy . But while differentially private algorithms for computing basic graph properties have been proposed , most graph modeling tasks common in the data mining community cannot yet be carried out privately .
In this work we propose algorithms for privately estimating the parameters of exponential random graph models ( ERGMs ) . We break the estimation problem into two steps : computing private sufficient statistics , then using them to estimate the model parameters . We consider specific alternating statistics that are in common use for ERGM models and describe a method for estimating them privately by adding noise proportional to a high confidence bound on their local sensitivity . In addition , we propose an estimation algorithm that considers the noise distribution of the private statistics and offers better accuracy than performing standard parameter estimation using the private statistics .
Categories and Subject Descriptors H27 [ Database Administration ] : Security , integrity , and protection ; H28 [ Database Management ] : Data Mining
Keywords Differential privacy ; Exponential random graph model
921
1 .
INTRODUCTION
The explosion in the collection of networked data has fueled researchers’ interest in modeling networks and predicting their behavior . However , for important application areas such as disease transmission , network vulnerability assessment , and fraud detection ( among others ) , networks contain sensitive information about individuals and their relationships . It is difficult for institutions to release network data and it remains difficult for researchers to acquire data in many important application domains .
Recently , a rigorous privacy standard , differential privacy [ 9 ] was proposed that allows for formal bounds on the disclosure about individuals that may result from computations on sensitive data . Differential privacy provides each participant in a dataset with a strong guarantee and makes no assumptions about the prior knowledge of attackers .
Since its introduction , differentially private algorithms have been developed for a wide range of data mining and analysis tasks , for both tabular data and networked data . For networks , existing work has focused on algorithms for accurately releasing common graph statistics under differential privacy [ 13 , 17 , 25 , 26 , 28 , 31 ] . However , graph statistics are only one aspect of social network analysis and are often most useful in conjunction with some paradigm for modeling structural features of graphs . Privately modeling graph data has only rarely been explored by researchers ; we are aware only of work using the Kronecker model [ 20 ] under differential privacy [ 22 ] .
In this work , we study the differentially private use of the classic exponential random graph model ( ERGM ) [ 21 , 30 , 27 ] . ERGMs are a powerful statistical modeling tool that allows analysts to analyze a network ’s social structure and formation process . In social science and related fields ERGMs have been successfully applied to many scenarios , such as co sponsorship networks [ 5 ] , friendship networks [ 12 ] , and corporate and inter organizational networks [ 21 ] .
Our goal is to accurately support parameter estimation for ERGMs under differential privacy , focusing on a specific set of model parameters of recent interest to researchers : the alternating statistics . These sophisticated statistics represent more structural information than traditional star and triangle counts , and have been shown to lead to much better modeling results [ 30 , 27 , 14 , 12 ] .
Our adaptation of differential privacy to graphs protects relationships of individuals by limiting the influence on the output of any single relationship ( edge ) that is created or removed from the network.1 A standard algorithm that implements this idea is the Laplace mechanism [ 9 ] , which adds random noise to the output . The amount of noise required is related to the maximum difference in the output due to a single edge addition or removal for any possible network ( this is the global sensitivity of the function producing the output ) . For ERGM estimation , this requires calculating the exact change in the ERGM parameter estimates as a result of changing an edge . Unfortunately , the global sensitivity for most ERGM parameters is either hard to compute in general , or too high , so that using noise calibrated to the global sensitivity is not acceptable .
To overcome this obstacle , we decompose private ERGM estimation into two separate steps . We first privately compute the sufficient statistics for ERGM estimation ( typically the model statistics required by model description ) and then estimate the parameters using only these sufficient statistics . Since the estimation process uses only the differentiallyprivate statistics , and there is no additional access to the original graph , the output of estimation is also differentially private . In practice , the estimation algorithm is executed either on the server side ( by the data owner ) or on the client side ( by the analyst ) . In either case , it does not violate the privacy condition to release both the statistics and the derived ERGM parameters .
Challenges arise in both steps of our approach . While prior work has proposed mechanisms for various graph statistics , common ERGM models use unique statistics , eg , alternating graph statistics [ 30 ] , which are a complex aggregation of a series of basic graph statistics . We describe new approaches for privately computing these statistics . The second parameter estimation step could be implemented using standard methods [ 29 , 3 ] while treating the privatelycomputed statistics as if they were the true statistics . Instead , we propose a novel parameter estimation method based on Bayesian inference , which considers the noise distribution from which the private statistics are drawn and produces more accurate parameter estimates .
Contributions • In Section 3 , we describe ( , δ) differentially private algorithms for estimating two key statistics : alternating ktriangle and alternating k twopath . The algorithms add noise proportional to a high likelihood bound on the local sensitivity of the statistics . Unlike global sensitivity , local sensitivity is determined by the current graph instead of worst case graphs and can be much lower . Our algorithms use a technique formalized in [ 17 ] and inspired by the Propose Test Release approach [ 8 ] . • We describe a new Bayesian method for ERGM parameter estimation ( in Section 4 ) that is designed for the noisy sufficient statistics produced by a differentially private algorithm . While it is possible to use a standard algorithm for estimation , our inference takes the unknown network as a hidden variable and can result in estimates with lower error . • We study a set of ERGM models based on model terms consisting of alternating graph statistics [ 30 ] ( in Section 5 ) . Our experiments on both synthetic and real graphs show
1This is one of the most common interpretations of differential privacy for graphs , called edge differential privacy [ 13 ] . Node differential privacy is stronger , but often hurts utility . Our results for edge differential privacy can easily be extended to k edge privacy to protect multiple edges . that our techniques significantly reduce noise over baseline approaches .
2 . BACKGROUND 2.1 Exponential random graph model ( ERGM ) A graph G = ( V , E ) is defined as a set of nodes V and relationships E : V × V → {0 , 1} . A common representation of a graph is as an adjacency matrix x , where xij ∈ {0 , 1} indicating whether there is an edge from node i to j . Let f ( · ) define a vector of graph statistics called the model terms ; the concrete values of f ( x ) are the model statistics . Formally , the ERGM with parameter vector θ defines a probability distribution over graphs in the space X ( typically the set of all simple graphs with n vertices ) : p(x|θ ) = exp(θ · f ( x ) )
Zθ
( 1 )
Zθ is a normalizing constant to make p(x ) a true probability distribution , parameterized by θ . If x0 is the observed graph and X represents the random variable defined by the distribution above , our goal is to tune the parameter vector θ , st the expected value of f ( X ) is equal to observed statistics , meaning Eθ(f ( X ) ) = f ( x0 ) , which intuitively puts the observed graph in the “ center ” of space of possible graphs implied by the model . For example , the simplest ERGM uses the number of edges as the only model term . If m0 is the total number of edges in x0 , the θ , which enables the expected number of edges of ERGM equal to m0 , is given by [ 24 ] :
( 2 )
,n − m0 m0
θ = log
2
Estimating θ . The optimal θ maximizes the likelihood of x0 given θ [ 24 ] , ie , arg maxθ p(x0|θ ) . Unfortunately , most ERGMs do not have an analytical or closed form estimate for the optimal θ . Thus , numerical solutions are proposed in the literature , such as Markov chain monte carlo maximum likelihood estimation [ 29 ] and Bayesian inference [ 3 ] . An interesting property of these inference methods is that the algorithm does not require access to the input graph itself , ie , the sufficient statistics for the parameter estimation are just the model statistics . This feature enables us to decompose the private inference problem into two steps , allowing analysts to see only the sufficient statistics .
Alternating statistics . A model term is usually a counting query of a specific graph pattern . Common patterns include triangles , stars and loops [ 21 ] . Recent research has introduced alternating statistics for k star , k triangle and k twopath , which can represent structural properties of a graph better than traditional star and triangle counts [ 30 ] . Many works have explored these statistics since they were proposed , and they are an active and promising form of ERGM [ 30 , 27 , 14 , 12 ] . Our work is focused on these alternating statistics ( defined precisely in Section 3 ) which have not been studied before under differential privacy . A wide variety of other model terms are used with ERGMs ; our general approach is compatible with other terms but they are beyond the scope of this work .
922
2.2 Differential privacy
Differential privacy is traditionally defined over a tabular based database D consisting of records , each of which describes an individual . When querying the database , differential privacy protects individuals by restricting the impact on the output of any individual who opts into or out of the database . Two such databases that differ by one record are called neighbors . Definition 2.1 ( Differential Privacy[7] ) . Let D and D be neighboring databases and K be any algorithm . For any subset of outputs O ⊆ Range(K ) , the following holds :
Pr[K(D ) ∈ O ] ≤ exp( ) × Pr[K(D
) ∈ O ] + δ
If δ = 0 , K is standard differentially private . Otherwise , K is relaxed ( , δ) differentially private .
The input privacy parameter ( and δ if using the relaxed definition ) are non negative and are used to measure the degree of privacy protection . Smaller means better privacy as exp( ) is close to one .
In this paper , our database is a graph describing relationships among individuals . Our purpose is to protect relationships among individuals so we adapt differential privacy , following [ 13 , 17 , 26 , 31 , 28 ] , by defining a neighboring graph as a graph that differs by one edge . Global sensitivity and the Laplace mechanism Differential privacy can be achieved by adding noise to the output of algorithms according to privacy parameters and query sensitivity . The global sensitivity of a query is the maximum possible difference in the output when evaluating the query on two neighboring graphs . Eg , the query asking for the maximum degree of a graph has global sensitivity 1 , because adding or removing one edge changes any degree by at most 1 . Let Lap(b ) be a Laplace random variable with mean 0 and scale b .
Definition 2.2 ( Laplace mechanism [ 9] ) . Given query f on graph x , the following algorithm K(f , x ) is differentially private :
K(f , x ) = f ( x ) + Lap(GSf / ) where global sensitivity
GSf = max
∀x1,x2 neighbors
|f ( x1 ) − f ( x2)|
A basic property we rely on is that post processing a noisy , differentially private output using any algorithm that does not access the original data cannot alter the privacy guarantee [ 19 ] . Past research has shown that post processing the noisy output can , however , have significant impact on utility . In addition , composition rules for differential privacy allow us to compute the privacy standard that results from the combined release of multiple query answers or releases . Precisely , if each release is i differential privacy , the combined is then i i differential privacy .
In our perturbation step , we will use the composition rule In the parameter to add noise to multiple model terms . estimation step , we run post processing . Local sensitivity and its smooth bound Some common graph analyses have high global sensitivity , requiring the Laplace mechanism to add enormous amounts of noise . For example , consider the simplest ERGM model above where θ is calculated by ( 2 ) . On a graph where m0 = 0
, θ can change drastically with or a graph where m0 = ,n the addition or deletion of one edge . In other words , the global sensitivity is very high for this function . But the fact is that most real graphs are nothing like these extremes . Thus , by only focusing on the input graph ’s neighbors , the local sensitivity [ 25 ] can be much smaller .
2
Definition 2.3 ( Local sensitivity[25] ) . Given query f and graph x , local sensitivity LSf ( x )
LSf ( x ) = max x,xneighbors
|f ( x ) − f ( x
)|
However , one cannot achieve differential privacy by adding noise proportional to the local sensitivity because local sensitivity itself could disclose information . The authors of [ 25 ] proposed using a smooth upper bound on the local sensitivity , the smooth sensitivity . Intuitively , smooth sensitivity tries to “ smooth ” out the difference between local sensitivities of two neighbors , so that it is itself not sensitive . Let d(x , x ) be the distance between two graphs , ie the number of edges in which they differ .
Definition 2.4 ( Smooth bound and smooth sensitivity[25] ) . Function Sf : X ⇒ R defines a β smooth bound of local sensitivity on query f if
∀x : Sf ( x ) ≥ LSf ( x )
The β smooth sensitivity of f is a β smooth bound , and
∀x , x neighbors : Sf ( x ) ≤ exp(β)Sf ( x
( LSf ( x
) · exp,−βd(x , x
)
) )
SSf,β(x ) = max x
Calculating the smooth sensitivity for a function may be easy ( in cases like the median of a list of numbers [ 25 ] ) but could be quite difficult for other functions , requiring complex proofs and nontrivial algorithms [ 17 ] .
3 . PERTURBING MODEL STATISTICS
In this section we provide methods for privately computing alternating graph statistics . In Sec 3.1 we define three alternating statistics and show that one of them ( alternating k star ) has a constant global sensitivity . This means the Laplace mechanism to be applied with relatively small error . However , alternating k triangle and alternating ktwopath both have high global sensitivity . In Sec 3.2 we show that we cannot resort to smooth sensitivity , as calculating the smooth sensitivity is NP hard in both cases . To address this challenge , we adapt a technique which calibrates noise to a private , high likelihood upper bound on the local sensitivity [ 17 ] . That bound is produced using the global sensitivity of the local sensitivity function . If , however , the global sensitivity of that function is high , the technique can be repeatedly applied , using a high likelihood bound on the local sensitivity of the local sensitivity function . We describe these “ first order ” and “ second order ” algorithms with static error calculation in Sec 3.2 , and then analyze the local sensitivity of alternating k triangle and alternating k twopath in Sec 33 3.1 Alternating graph statistics
The three alternating graph statistics , alternating k star , alternating k triangle and alternating k twopath , are essentially complex aggregations of traditional k star , k triangle
923 and k twopath statistics . Instead of considering a vector of k terms , the alternating statistics aggregate over the terms but enforce alternating signs between each consecutive term , to weaken the correlation among different terms and effectively reduce the weight on higher terms near k .
Alternating k star . The k star is a counting query of a star pattern in the graph , where each star contains k edges , ie ,
where di is the degree of node i .
Sk =
,di i k
Definition 3.1 ( Alternating k star [ 30] ) . With parameter λ ≥ 1 , alternating k star S is defined as
S(x ; λ ) = S2 − S3 λ
+ . . . + ( −1)n−1 Sn−1 λn−3
The λ parameter here is a good way to control the geo metrical weights on all k stars .
Alternating k triangle . A k triangle is a graph pattern in which k triangles share a common edge . The k triangle query asks for the total number of k triangles in the graph . Define the shared partner matrix C , where each entry ( i , j ) in C is the count of shared partners between nodes i and j , l xilxlj . Formally , k triangle Tk mathematically Cij(x ) = is defined : i<j
Tk = xij
Cij k
( k ≥ 2 ) , and T1 =
1 3 xijCij i<j
Alternating k triangle is defined similarly as alternating kstar , using parameter λ :
Definition 3.2 ( Alternating k triangle [ 30] ) . With parameter λ ≥ 1 , alternating k triangle T is : λ2 − . . . +
T ( x ; λ ) = 3T1 − T2 λ
−1 n−3
Tn−2
T3
+
λ
Alternating k twopath . A k twopath graph pattern is very similar to k triangle , except it does not require the shared edge required by the k triangle statistic . Using the shared partners matrix C above , the counting query for k twopath Uk is : i<j
Uk =
Cij k
( k = 2 ) , and U2 =
1 2
And alternating k twopath is : i<j
Cij 2
Definition 3.3 ( Alternating k twopath [ 30] ) . With parameter λ ≥ 1 , alternating k twopath U is
U ( x ; λ ) = U1 − 2 λ
U2 + n−2
−1 k−1
λ k=3
Uk
Alternating k star S is the only statistic that can be readily solved using existing privacy mechanisms . Because the degree sequence is a sufficient statistic for S , one natural approach is to use the mechanism described by Hay et al [ 13 ] to compute a private degree sequence from x , and then use it to compute S by Eq ( 3 ) . But , in fact , it can be shown that the global sensitivity of S is at most 2λ . Thus , Laplace noise may be a better choice ( λ is usually set to a small integer in practice ) . We make empirical comparisons between these methods in Section 5 .
924
Lemma 34 The global sensitivity of alternating k star is at most 2λ . 3.2 Bounding local sensitivity
Because the global sensitivity of alternating k triangle and k twopath can be as large as O(n ) , we would like to use a method which adds noise scaled to the local sensitivity , or a quantity close to the local sensitivity . One approach is to compute a smooth bound on the local sensitivity , however , the following lemma shows the NP hardness of computing such a bound for these two statistics :
Lemma 35 Computing the smooth sensitivity for both alternating k triangle and alternating k twopath is NP hard .
We therefore employ a technique inspired by the ProposeTest Release framework [ 8 ] , and formalized by Karwa et al [ 17 ] , where it was used to estimate k triangles . The technique first computes a private over estimate of the local sensitivity , one that is higher than the local sensitivity with high probability . That becomes a safe sensitivity value for calibrating Laplace noise , however , the result satisfies only the weaker notion of ( , δ) differential privacy .
Let f ( x ) be the sensitive function/query . We use LSf,1(x ) to denote the local sensitivity of f , which is a function of the input graph x .
Algorithm 1 Local sensitivity bounding algorithm ( First order )
Require : input graph x , query f and , δ 1 : a = ln(1/δ ) 2 : ˜y1 = LSf,1(x ) + Lap(GS(LSf,1(x))/ ) + a · GS(LSf,1(x ) ) 3 : ˜y = f ( x ) + Lap(˜y1/ ) 4 : return ˜y , ˜y1
In Algorithm 1 , ˜y1 is the private bound on the local sensitivity , computed by adding scaled noise to LSf,1(x ) , as well a positive offset , so that the bound is higher than LSf,1(x ) with high probability . Notice that the scale of the noise is determined by the global sensitivity of the local sensitivity , GS(LSf,1(x) ) .
If GS(LSf,1(x ) ) is large , it may cause ˜y1 to be a significant over estimate of LSf,1(x ) . We can repeat this approach by using a safe upper bound of the local sensitivity of LSf,1(x ) , as presented below . Thus , Algorithm 1 bounds the firstorder local sensitivity and the following algorithm bounds the second order local sensitivity .
Algorithm 2 Local sensitivity bounding algorithm ( Second order )
Require : input graph x , query f and , δ 1 : a = ln(1/δ ) 2 : ˜y2 = LSf,2(x ) + Lap(GS(LSf,2(x))/ ) + a · GS(LSf,2(x ) ) 3 : ˜y1 = LSf,1(x ) + Lap(˜y2/ ) + a · ˜y2 4 : ˜y = f ( x ) + Lap(˜y1/ ) 5 : return ˜y , ˜y1 , ˜y2
Theorem 36 Algorithm 1 is ( 2 , 1 vacy . Algorithm 2 is ( 3 , 1
2 e δ) differential pri2 e2 δ) differential privacy . The step of replacing the global sensitivity by a highlikelihood bound on the local sensitivity can be repeatedly
2 e δ + 1 applied to form more complex higher order algorithms . However , each additional bounding step requires splitting the privacy budget and the combined effects of repeatedly overestimating higher order sensitivities may diminish utility . For the two alternating statistics we consider , and the datasets we tested on , we found that first order and second order is sufficient .
Error analysis
Definition 37 Y0 , Y1 , . . . , Yn is a random variable chain , when the following condition is satisfied : for any i ∈ [ 0 , n − 2 ] , Yi is conditionally independent of Yi+2 , Yi+3 , . . . Yn given Yi+1 .
From conditional independence , an important property of a random variable chain is the following :
Pr(Yi|Yi+1 , Yi+2 , . . . , Yn ) = Pr(Yi|Yi+1 )
It is easy to see that ˜y , ˜y1 , . . . is actually a random variable chain . We use mean squared error ( MSE ) as the measurement of error . In Algorithm 1 and 2 , MSE of ˜y can be written as E[(˜y − f ( x))2 ] = V[˜y ] + ( E[˜y ] − f ( x))2 . Since ˜y is always unbiased ( Laplace noise in the last step with mean zero ) , MSE = V[˜y ] .
Without knowing the true value of the local sensitivities , it is quite hard to compute the MSE . That is to say , we cannot compute the error like we do for the Laplace mechanism , since the noise in the latter is independent of input graph x . But , by exploring properties of the random variable chain , it is possible to utilize the following Lemma as a closed form calculation tool for MSE . In fact , we extend law of total expectation/variance [ 32 ] .
Lemma 38 Y0 , Y1 , . . . , Yn is a random variable chain . Write E[· ] as a shortcut for EYj|Yj+1 [ EYj−1|Yj [ . . . EYi|Yi+1 [ ·] ] ] when j < n and EYj [ EYj−1|Yj [ . . . EYi|Yi+1 [ ·] ] ] when j = n . E[Y0 ] =
E[Y0 ] j,i
V[Y0 ] =
[ Y0 ] ]
E[Y0] ] ]
( cid:71 ) i−2,0
( cid:71 ) n−1,0
+ V Yn
[
E[Y0 ] ]
E[ V
Yi−1|Yi
[
Applying Lemma 3.8 , one can calculate the MSE of Algorithms 1 and 2 . Such error measurement , which needs access to the input graph , can serve as an evaluation tool for privacy researchers when working with our algorithms . From the perspective of data owners , the analytic result of MSE can help them to decide between Algorithm 1 and 2 , ie , with fixed privacy parameters , selecting the algorithm with lower error . 3.3 Alternating k triangle and k twopath
Now we apply the idea of bounding the local sensitivity to alternating k triangle and alternating k twopath . Let β = 1− 1/λ . By binomial coefficients , we can rewrite alternating k triangle T ( x ; λ ) as
1 − βCij i<j
T ( x ; λ ) = λ xij
( 3 )
925 n,0
( cid:71 ) ( cid:71 ) n n,1
+
E[ V Y0|Y1
( cid:71 ) i=2 n,i vj = Cvj − xij . Lemma 39 Set C Let Nij be all shared partners of node i and j and Cmax = maxi<j Cij . The local sensitivity of T is iv = Civ − xij and C
1 − βCij
+ v∈Nij
βC iv + βC vj
LST,1(x ) = max i<j
λ
≤ λ + 2Cmax
As Cmax has global sensitivity 1 , LST,1 has global sensitivity at most 2 . So we can construct a first order local sensitivity bound using LST,1 = λ + 2Cmax to compute private alternating k triangle .
For alternating k twopath U ( x ; λ ) , we can rewrite it as
1 − βCij i<j
U ( x ; λ ) = λ
( 4 )
( 5 )
( 6 )
Lemma 310 Let Ni be the set of neighbors of node i and iv = Civ − xij and dmax be the maximum degree . Set C vj = Cvj − xij . We have local sensitivity C
 v∈Ni,v=j
βC vj +
βC iv v∈Nj ,v=i
LSU,1(x ) = max i<j
≤ 2dmax
LSU,2(x ) ≤ max(4 , 1 + Cmax )
λ
 ( 7 )
( 8 )
( 9 )
From Lemma 3.10 above , ( 8 ) has global sensitivity 2 , since dmax will change by at most 1 by adding or removing an edge . ( 9 ) has global sensitivity 1/λ for Cmax > 3 . Therefore , we can construct either a first order or second order bound . Note that ( 7 ) is the exact local sensitivity of alternating k twopath , but we cannot bound it in Algorithm 1 as the global sensitivity of ( 7 ) is not clear . Instead we use ( 8 ) . When applying Algorithm 2 , ( 7 ) is the local sensitivity to be bounded at Line 4 , as by that step the higher order ( secondorder ) local sensitivity ( 9 ) has already been safely bounded . We compare the resulting error empirically in Section 5 .
4 . ERGM PARAMETER ESTIMATION
The parameter estimation step in our workflow takes the private sufficient statistics ˜y from the previous perturbation step and finds the best parameter vector θ . As stated above , this step is essentially post processing a differentially private output , so the output θ is also differentially private . In this section , we discuss different ways of estimating θ given ˜y . 4.1 Standard estimation
Current estimation techniques [ 29 , 3 ] provide a baseline solution for parameter estimation with private statistics . As these procedures essentially only need access to model statistics , our sufficient statistics in ˜y take the place of the true model terms . The semantics is now to search for θ that defines a probability distribution on graphs with expected model statistics equal to ˜y . Intuitively , the utility of this method depends on the amount of noise added into y0 and how θ reacts to those changes in y0 .
Prior to applying standard estimation , we post process ˜y to cope with some of the difficulties of the perturbed model statistics . As the output of perturbed ˜y might not be graphical ( ie , no graph has statistics equal to ˜y ) , standard estimation may fail to converge . We propose generating a graph that has the closest statistics to ˜y and use the statistics from that graph to replace ˜y , in order to avoid non converging situations and to potentially remove noise from ˜y simultaneously . We use simulated annealing for this purpose and , in practice , we often see big improvements in the accuracy of estimates . 4.2 Bayesian inference
Standard estimation is the direct way of post processing ˜y , but since we know the distribution of the noise added to ˜y , we can “ guess ” the true values and incorporate them into the estimation algorithm . This idea naturally fits into Bayesian inference based post processing . While based on earlier work [ 3 ] on Bayesian inference for non private estimation , our method deals with the extra hidden variable of graph x in our setting . And later we will see , by introducing the unknown x , our method can utilize more information from private statistics , such as the local sensitivity bounds . In particular , we search for θ given ˜y , represented as the posterior distribution of ERGM parameter θ : p(˜y|x)p(x|θ)p(θ ) p(θ|˜y ) ∝ p(˜y|θ)p(θ ) = x
= p(˜y|x)q(x ; θ)p(θ)/Zθ
( 10 ) x where x is our guess about x0 , but the fact is that we need to summarize over all possible x to get to the posterior . In ( 10 ) , p(˜y|x ) is the privacy distribution , defined by the differential privacy mechanism applied on sufficient statistics . p(x|θ ) is the ERGM distribution , as shown in ( 1 ) and q(x ; θ ) represents the unnormalized distribution . q(x ; θ ) = exp(θ · f ( x ) )
( 11 )
The probability distribution ( 10 ) is hard to calculate or even sample from directly due to summarization over all graphs and normalizing constant Zθ . Using the exchange algorithm [ 23 ] , we introduce extra variables x , θ and x to bypass the difficult terms ( 10 ) . By carefully choosing the probability distribution of these new random variables , the posterior distribution is now augmented as shown in ( 12 ) . The key is that the marginal posterior distribution for θ in ( 12 ) is equivalent to ( 10 ) . Thus , if we are able to sample from the distribution in ( 12 ) , the marginal posterior distribution for θ can be obtained by summarizing over all samples .
|θ)p(x
|θ
)
|˜y ) ∝ p(˜y|x)p(x|θ)p(θ)p(θ
, x p(θ , x , θ
( 12 ) θ is sampled from proposal distribution p(θ|θ ) , where , for a given θ , a new θ can be proposed according to p(θ|θ ) . A common choice is a multivariate normal distribution or a multivariate t distribution , with mean equal to θ . x , x are sampled graphs under the ERGM with parameter θ and θ . A MCMC based sampling process for ( 12 ) is shown in Algorithm 3 . In particular , the initial input θ and x could be any parameters and any graph . In Line 3 , we need a separated MCMC chain to sample x ∼ p(x|θ ) . In such MCMC algorithms , at each iteration ( T iterations in total ) , we propose adding or removing edges in the current state of graph , calculate the new model statistics , compare the probability of new state xnew to that of old state xold , and with probability p(xnew|θ)/p(xold|θ ) the change is accepted . This process should be run long enough so that final sample x is truly from p(x|θ ) .
Algorithm 3 ERGM parameter estimation with private model statistics Require : ˜y , initial θ , x 1 : for i in 1 to T do 2 : 3 : 4 :
Sample θ ∼ p(θ|θ ) Sample x ∼ p(x|θ ) Replace θ with θ and x with x , with probability min(1 , H )
//H by ( 13 ) below
5 : return samples of θ .
H in Line 4 is the ratio of accepting the exchange , computed by comparing the probability before and after exchange . That is , we exchange θ with θ and x with x in ( 12 ) and calculate the ratio . Then the complex terms are cancelled out and each remaining term is easy to compute .
H =
= p(˜y|x)p(x|θ)p(θ)p(θ|θ)p(x|θ ) p(˜y|x)p(x|θ)p(θ)p(θ|θ)p(x|θ ) p(˜y|x)p(θ)p(θ|θ ) p(˜y|x)p(θ)p(θ|θ )
( 13 )
In practice , Algorithm 3 usually results in low acceptance rates in the exchange step in Line 4 and thus long mixing times for the MCMC process . We now propose to separate that last step , isolating simultaneously updated θ and x into two different steps , as shown in Algorithm 4 , which improves the acceptance rate significantly .
Algorithm 4 Improved ERGM parameter estimation with private model statistics
Require : ˜y , initial θ , x 1 : for i in 1 to T do 2 : 3 : 4 :
//H1 by ( 14 ) below
Sample θ ∼ p(θ|θ ) Sample x ∼ p(x|θ ) Exchange θ with θ , with probability min(1 , H1 ) Replace x with x , with probability min(1 , H2 )
5 :
//H2 by ( 15 ) below
6 : return samples of θ .
H1 and H2 in Algorithm 4 are defined as follows .
H1 =
= p(˜y|x)p(x|θ)p(θ)p(θ|θ)p(x|θ ) p(˜y|x)p(x|θ)p(θ)p(θ|θ)p(x|θ ) q(x ; θ)p(θ)p(θ|θ)q(x ; θ ) q(x ; θ)p(θ)p(θ|θ)q(x ; θ )
H2 =
= p(˜y|x)p(x|θ)p(θ)p(θ|θ)p(x|θ ) p(˜y|x)p(x|θ)p(θ)p(θ|θ)p(x|θ ) p(˜y|x)q(x ; θ)q(x ; θ ) p(˜y|x)q(x ; θ)q(x ; θ )
( 14 )
( 15 )
The correctness of Algorithm 4 can be proved briefly in terms of a component wise Metropolis Hasting algorithm , with hybrid Gibbs updating steps . In each iteration , θ and x ( Line 2 and 3 ) are drawn based on the full conditional distribution , so the updating probability is always 1 . In Line 4 and 5 , we update θ and x with Hasting ratios . Although we may end up updating θ and x more times in a iteration , we still get to the detailed balance in MCMC [ 10 ] .
926
Figure 1 : Perturbation error on alternating k star on synthetic graphs . Left : p = log(n)/n , varying size of graph n . Right : n = 1000 , varying λ .
Figure 2 : Perturbation error for alternating k triangle . Left : p = log(n)/n . Right : p = 01 Trend lines for LS are non private .
When applying Algorithm 4 to real ERGM models , the key is correctly computing H1 and H2 . Everything in H1 is independent of the privacy mechanism used for the model is terms . mechanism dependent . Here , we illustrate the cases for both Laplace mechanism and local sensitivity bounding algorithms .
In H2 , the ratio of privacy distribution p(˜y|x ) p(˜y|x )
Example 4.1 ( Laplace mechanism ) . If Laplace mechanism is applied on a model term f , and ˜y , and GS are private statistic , privacy parameter and global sensitivities respectively , p(˜y|x ) is then : p(˜y|x ) ∝ exp ( −|˜y − f ( x)| /GS )
( 16 )
Assume we use a symmetric proposal distribution for θ , ie , p(θ|θ ) = p(θ|θ ) . With Algorithm 4 , ratio H1 and H2 can be written as ( after taking logarithm ) p(θ ) p(θ )
+ ( θ − θ
) ·,f ( x ) − f ( x ) ) − f ( x) + ) ·,f ( x ,|˜y − f ( x)| − |˜y − f ( x )| log H1 = log log H2 = ( θ − θ
GS
( 17 )
( 18 )
Example 4.2 ( Local sensitivity bounding ) . Assume a single model term and first order local sensitivity bounding ( multiple model terms and second order can be adjusted accordingly ) , and privacy parameter and δ is the input for Algorithm 1 . Let a = ln(1/δ)/ .
In the process of MCMC , for current sampled graph x , we write l1 as the local sensitivity on x and g1 as the global sensitivity of local sensitivity . The first order local sensitivity bounding ( Algorithm 1 ) returns ˜y , ˜y1 for the observed graph . p(˜y , ˜y1|x ) can be represented as follows by omitting terms that will be cancelled out later in calculating p(˜y,˜y1|x ) p(˜y,˜y1|x ) . p(˜y , ˜y1|x ) = p(˜y|x , ˜y1)p(˜y1|g1 , l1 ) −|˜y − f ( x)|
∝ exp
− |˜y1 − l1 − ag1| g1/
˜y1/
( 19 ) Calculation of p(˜y , ˜y1|x ) deals with not only the private version of local sensitivity ˜y1 , but also more statistics from the sampled graph in each iteration of MCMC ( local sensitivity l1 ) . Recall in the standard estimation , none of them are incorporated in the process . In the next section , we empirically show that such extra information can benefit the estimation . As in the example above , assume a symmetric proposal distribution . With Algorithm 4 , ratio H1 is the same as ( 17 ) . H2 is : log H2 = ( θ − θ
) − f ( x) +
|˜y − f ( x)| − |˜y − f ( x)|
) ·,f ( x
|˜y1 − l1 − ag1| − |˜y1 − l
1 − ag1| g1/
+
˜y1/
( 20 )
Releasing θ . In Algorithm 3 and 4 , we use multiple sampled θ to represent the marginal distribution on θ . A straightforward way to generate a single instance of estimated θ is to calculate the average of those samples . However , in practice , we found that marginal maximum a posterior ( MMAP ) could give analysts better estimates instead . Formally , MMAP p(θ|˜y ) . A fast method we apply is of θ is defined as argmax reusing the samples of θ from Algorithm 4 , and performing approximate MMAP estimation by histogram or density estimation . More sophisticated solutions will require further expanding ( 12 ) before MCMC sampling [ 6 , 16 ] .
θ
5 . EVALUATION
Our evaluation has two goals . First we assess the perturbation error of our privacy mechanisms , particularly the Laplace mechanism on alternating k star and the local sensitivity bounding algorithms on alternating k triangle and k twopath . Second , we evaluate the ERGM parameter estimation with private statistics using different approaches proposed in Section 4 . All our experiments are run on Linux servers with Intel Xeon CPU and 8GB memory . In the experiments , we differ privacy parameters and δ . Note that , whenever we clarify a value for or δ , it always means the overall privacy budget of the entire perturbation process . 5.1 Perturbation error
Our datasets include synthetic and real graphs . Synthetic graphs are generated using a random graph model , G(n , p ) , where parameters n and p control the size of graph and the probability of two nodes connecting , respectively . We iterate from n = 100 to n = 1000 in steps of 100 . p is set to log(n)/n for relatively sparse graphs and then moved to 0.1 and higher . Though we only report the sparse case and p = 0.1 , results for larger p generally agree with the conclusions . Error measurement is root mean square error ( RMSE ) .
Alternating k star As described in Section 3.1 , we can apply the Laplace mech
927
1001011021031042004006008001000NodesRMSEISO , 0.1ISO , 1LAP , 0.1LAP , 1100101102103104246810λ1011015102102510310352004006008001000NodesRMSELSB , 0.1LS , 0.1LSB , 1LS , 11015102102510310352004006008001000Nodes ( 1 ) p = 0.1 , = 0.1
( 2 ) p = 0.1 , = 1
( 3 ) p = 0.1 , = 0.1
( 4 ) p = 0.1 , = 1
Figure 3 : Perturbation error ( 1 ) , ( 2 ) and local sensitivity bounds ( 3 ) , ( 4 ) for alternating k twopath .
Network dolphins lesmis polbooks adjnoun football nodes 62 77 105 112 115 edges 159 254 441 425 613 astar 418.1 756.4 1355.4 1292.9 1992.4 atri 177.5 426.4 715.5 452.1 922.3 atwop 705.4 1565.5 2817.5 3801.0 3675.3
Table 1 : Real networks for ERGM parameter estimation
Model Model terms Perturbation mech
M1 M2 M3
LAP , LAP edges , astar edges , atri LAP , 1 LSB edges , atwop LAP , 1 LSB
Table 2 : Model descriptions will outweigh the gain . In the right two subfigures of Figure 3 , we plot the true local sensitivity and the expected values of private , bounded local sensitivity for both LSB algorithms . We see that 1 LSB results in a bound that is close to the true value but that 2 LSB results in a significant overestimate , especially with a smaller = 01 Although 1 LSB is superior across our tested networks , it remains possible that 2 LSB could outperform 1 LSB for some input graphs or large and λ .
Real graphs For real graphs , we consider several collected networks from the SNAP collection2 to determine if our alternating statistics can be perturbed in a “ meaningful ” way , ie , small relative noise that doesn’t destroy utility . Our metric is relative RMSE , which is RMSE divided by the true statistic . As shown in Figure 4 , with = 0.1 , all three alternating statistics ( with shortened names : astar , atri , atwop ) are estimated with low relative error . In particular , error for alternating k star is between 10−3 and 10−4 , alternating k triangle at 10−1 and alternating k twopath at 10−2 . 5.2 ERGM parameter estimation
For the evaluation of ERGM parameter estimation , we want to compare the algorithms in Section 4 . In practice , the data owner will only perturb each statistic once and then release it to the analysts . As the perturbation is a randomized process , our goal is to understand how good our estimation algorithm is on average . So for each graph and each model description , we perturb the statistics N = 50 times and run the estimation algorithm on each perturbation , finally
2http://snapstanfordedu
Figure 4 : Perturbation error on real graphs anism ( LAP ) directly or compute the degree distribution privately first , by isotonic regression ( ISO ) from [ 13 ] and use it as a sufficient statistic for alternating k star . Figure 1 shows the error of the two methods by varying p and λ , with different settings of = 1 , 0.1 , listed in the legend text . As we do not have analytical RMSE for the ISO case , it is calculated from 100 independent perturbations . We clearly see LAP significantly outperforms ISO , even when λ = 10 at both settings ( and recall that the global sensitivity is 2λ ) . For the rest of this section , if not stated , we set λ = 2 as it is the value normally recommended [ 21 ] and usually plays a minor part in the workflow .
Alternating k triangle The first order local sensitivity bounding algorithm is applied here while setting to 1 and 0.1 and fixing δ = 001 In Figure 2 , we use “ LSB ” to represent Algorithm 1 . For comparison purposes , we plot the non private noisy output resulting from adding Laplace noise based on true local sensitivity , marked as “ LS ” in Figure 2 . We find that LSB can add modest error when compared to this non private baseline , especially when the privacy budget is relatively large .
Alternating k twopath We discussed in Section 3.3 how a first order or secondorder local sensitivity bound can be applied to alternating k twopath . We present these results in Figure 3 , by distinguishing them as “ 1 LSB ” and “ 2 LSB ” . We find that for our test cases with random graphs , 1 LSB is consistantly better than 2 LSB , illustrated by RMSE in the left two subfigures . Referring to Sec 3.3 , recall that in 1 LSB we bound ( 8 ) while in 2 LSB we bound eqrefeq:ktwop1 by using bounded ( 9 ) . If ( 7 ) and ( 9 ) are not small enough compared to ( 8 ) , the fact that we split the budget of privacy one more time
928
103103510410451051055102102510320040060080010002004006008001000NodesRMSE2 LSB1 LSBLS101510210251031035104101610181021022102420040060080010002004006008001000NodesLocal sens2 LSB1 LSBLocal sens 010 410 310 210 1100Relative RMSECA HepThCA HepPhCA AstroPhCA CondMatEmail Enronn=9.9kn=12kn=19kn=23kn=37km=26km=119km=198km=93km=184kastaratriatwop Figure 5 : Parameter estimation with private statistics . Every four bars , from left to right , are θ1 , θ1 , θ2 , θ2 .
1/N measuring their quality by RMSE with respect to estimates i∈[1,N ](ˆθi − θ)2 , where θ is in the non private case , the “ true ” value , calculated from the non private estimation algorithm from [ 15 ] or [ 3 ] , ˆθi is θ from i th perturbation .
As mentioned in [ 3 ] , the estimation using the Bayesian technique has general scalability issues , where it becomes very slow for any graphs beyond a few hundred of nodes . Moreover such time cost also varies with the model terms , eg , alternating k twopath takes much more time than the other two alternating statistics , as calculation of the acceptance ratio in MCMC sampling of x ∼ p(x|θ ) is more complicated . Therefore , here we focus on smaller graphs , and this is the common practice for many ERGM works such as [ 3 , 5 , 21 ] . Our test networks3 include dolphins , lesmis , polbooks , adjnoun and football . Detailed facts are listed in Table 1 . We fix = 0.5 and δ = 001
We experimented with three models , each of which corresponds to one of the alternating statistics , with the purpose of testing estimation by isolating other factors . We include the count of edges as a shared term in all models , as it is very common in ERGM applications . As shown in Table 2 , each model contains two terms , with correspondingly two parameters , θ = ( θ1 , θ2 ) . The estimation algorithms will be standard estimation ( STD ) and Bayesian inference ( BINF ) . In all cases , the privacy budget is distributed evenly in a way such that each generation of noise uses same share of the overall . In Figure 5 , each graph is represented with 4 bars , showing θ1 of STD , θ1 of BINF , θ2 of STD , θ2 of BINF . In M1 and M2 , we see a significant improvement of θ from STD to BINF . Especially in M2 , BINF limits all errors to around 5 or smaller where STD can go up much higher . We believe this is because BINF can utilize the extra information presented by the local sensitivity bound as shown in Example 42 In M3 , compared to the other models , we see that parameters of the model is quite insensitive to the changes due to perturbation , ie , all graphs show much lower errors even under STD . In such situation , theoretically , there is not much room left for the improvement from BINF . This is illustrated in our experiment by showing comparable performance from both methods on M3 . In general , we think BINF can improve the accuracy of parameter estimation significantly by leveraging the privacy distribution , while at the same time , the amount of benefit will vary depending on intrinsic properties of the model .
6 . RELATED WORK
3http://www personalumichedu/˜mejn/netdata/
929
Differential privacy [ 9 ] has been actively studied in many sub areas of computer science . Although the original focus was mainly on tabular data , the definition can be adapted to graph data [ 13 ] as well as other data models . Most research into differentially private analysis of graphs has focused on releasing graph statistics , eg , degree sequence [ 13 ] , triangle/star [ 17 , 25 ] , joint degree distribution/assortativity [ 26 , 28 ] and clustering coefficient [ 31 ] . For modeling graphs privately , we are aware only of a private Kronecker graph modeling approach under differential privacy [ 22 ] . While our work relies on obtaining good private statistics , the ultimate goal is to allow ERGM modeling under differential privacy . All of these works , including ours , protect relationships , ie they support edge differential privacy . A stronger standard is to protect individuals , where neighbors are defined by changing a single node . Recently , researchers have developed some mechanisms for calculating private graph statistics under node differential privacy [ 18 , 2 , 4 ] .
Parameter estimation for ERGMs has also evolved from pseudo likelihood estimation ( MPLE ) [ 1 ] , to Monte Carlo maximum likelihood ( MC MLE ) [ 11 ] to recent stochastic approximation [ 29 ] and Bayesian inference [ 3 ] . These advances have helped ERGMs become central to social network analysis with many successful applications [ 21 ] .
7 . CONCLUSION AND FUTURE WORK
In this work , we consider the problem of estimating parameters for the exponential random graph model under differential privacy . Our solution decomposes the process into two steps : releasing private statistics first and running estimation second . Our local sensitivity based algorithms can offer lower error than common baselines . The redesigned Bayesian parameter estimation is flexible and more accurate than standard methods . For future work , improving scalability is an important direction as well exploring alternative model terms .
8 . ACKNOWLEDGMENTS
This material is based on work supported by the NSF through awards CNS 1012748 and IIS 0964094 .
9 . REFERENCES [ 1 ] J . Besag . Spatial interaction and the statistical analysis of lattice systems . Journal of the Royal Statistical Society . Series B ( Methodological ) , 1974 .
[ 2 ] J . Blocki , A . Blum , A . Datta , and O . Sheffet .
Differentially private data analysis of social networks via restricted sensitivity . In ITCS , 2013 .
M1M2M3010203040dollespoladjfoodollespoladjfoodollespoladjfooRMSESTDBINF [ 3 ] A . Caimo and N . Friel . Bayesian inference for
[ 23 ] I . Murray , Z . Ghahramani , and D . MacKay . Mcmc for exponential random graph models . Social Networks , 33(1):41–55 , 2011 . doubly intractable distributions . arXiv , 2012 .
[ 24 ] M . Newman . Networks : an introduction . Oxford
[ 4 ] S . Chen and S . Zhou . Recursive mechanism : towards
University Press , 2010 . node differential privacy and unrestricted joins . In SIGMOD , 2013 .
[ 5 ] S . J . Cranmer and B . A . Desmarais . Inferential network analysis with exponential random graph models . Political Analysis , 19(1):66–86 , 2011 .
[ 6 ] A . Doucet , S . J . Godsill , and C . P . Robert . Marginal maximum a posteriori estimation using markov chain monte carlo . Statistics and Computing , 2002 .
[ 7 ] C . Dwork , K . Kenthapadi , F . McSherry , I . Mironov , and M . Naor . Our data , ourselves : Privacy via distributed noise generation . EUROCRYPT , 2006 .
[ 8 ] C . Dwork and J . Lei . Differential privacy and robust statistics . In Proceedings of the 41st annual ACM symposium on Theory of computing , pages 371–380 . ACM , 2009 .
[ 9 ] C . Dwork , F . McSherry , K . Nissim , and A . Smith .
Calibrating noise to sensitivity in private data analysis . Theory of Cryptography , 2006 .
[ 10 ] D . Gamerman and H . F . Lopes . Markov chain Monte
Carlo : stochastic simulation for Bayesian inference , volume 68 . CRC Press , 2006 .
[ 11 ] C . J . Geyer and E . A . Thompson . Constrained monte carlo maximum likelihood for dependent data . Journal of the Royal Statistical Society . , 1992 .
[ 12 ] S . M . Goodreau , J . A . Kitts , and M . Morris . Birds of a feather , or friend of a friend ? using exponential random graph models to investigate adolescent social networks* . Demography , 46(1):103–125 , 2009 .
[ 13 ] M . Hay , C . Li , G . Miklau , and D . Jensen . Accurate estimation of the degree distribution of private networks . In ICDM , 2009 .
[ 14 ] D . Hunter . Curved exponential family models for social networks . Social Networks , 29(2):216–230 , 2007 .
[ 15 ] D . Hunter and M . Handcock . Inference in curved exponential family models for networks . Journal of Computational and Graphical Statistics , 2006 .
[ 16 ] A . M . Johansen , A . Doucet , and M . Davy . Particle methods for maximum likelihood estimation in latent variable models . Statistics and Computing , 2008 .
[ 17 ] V . Karwa , S . Raskhodnikova , A . Smith , and
G . Yaroslavtsev . Private analysis of graph structure . In VLDB , 2011 .
[ 18 ] S . P . Kasiviswanathan , K . Nissim , S . Raskhodnikova , and A . Smith . Analyzing graphs with node differential privacy . In TCC , 2013 .
[ 19 ] D . Kifer and B R Lin . An axiomatic view of statistical privacy and utility . Journal of Privacy and Confidentiality , 2012 .
[ 20 ] J . Leskovec , D . Chakrabarti , J . Kleinberg ,
C . Faloutsos , and Z . Ghahramani . Kronecker graphs : An approach to modeling networks . JMLR , 2010 .
[ 21 ] D . Lusher , J . Koskinen , and G . Robins . Exponential Random Graph Models for Social Networks : Theory , Methods , and Applications . Structural Analysis in the Social Sciences . Cambridge University Press , 2012 .
[ 22 ] D . Mir and R . N . Wright . A differentially private estimator for the stochastic kronecker graph model . In EDBT/ICDT Workshops , 2012 .
930
[ 25 ] K . Nissim , S . Raskhodnikova , and A . Smith . Smooth sensitivity and sampling in private data analysis . In STOC , 2007 .
[ 26 ] D . Proserpio , S . Goldberg , and F . McSherry .
Calibrating data to sensitivity in private data analysis . In VLDB , 2014 .
[ 27 ] G . Robins , T . Snijders , P . Wang , M . Handcock , and
P . Pattison . Recent developments in exponential random graph p* models for social networks . Social networks , 2007 .
[ 28 ] A . Sala , X . Zhao , C . Wilson , H . Zheng , and B . Zhao .
Sharing graphs using differentially private graph models . In SIGCOMM , 2011 .
[ 29 ] T . Snijders . Markov chain monte carlo estimation of exponential random graph models . Journal of Social Structure , 2002 .
[ 30 ] T . A . Snijders , P . E . Pattison , G . L . Robins , and M . S . Handcock . New specifications for exponential random graph models . Sociological methodology , 2006 .
[ 31 ] Y . Wang , X . Wu , J . Zhu , and Y . Xiang . On learning cluster coefficient of private networks . Social network analysis and mining , 2013 .
[ 32 ] N . A . Weiss , P . T . Holmes , and M . Hardy . A course in probability . Pearson Addison Wesley , 2006 .
APPENDIX A . PROOF OF THEOREM 3.6
The proof relies on Lemma 4.4 from [ 17 ] , and we restate it as follows :
Lemma A1 If M is ( 1 , δ1) differentially private , and Pr[M ( x ) ≥ LSf ( x ) ] > 1 − δ2 for all x , the following algorithm A , which returns a pair of values ,
A(x ) = ( M ( x ) , Lap(M ( x)/ 2 ) ) is ( 1 + 2 , δ1 + e 1 δ2) differentially private .
Proof of Theorem 36 In Algorithm 1 , ˜y1 is differentially private as it is based on Laplace mechanism and post processing ( adding positive offset ) . Let g1 = GS(LSf,1(x) ) . If the sampled Laplace noise in line 2 is z ,
Pr[˜y1 ≤ LSf,1(x ) ] = Pr[z ≤ −ag1 ]
−ag1
1 exp(−|z| /g1 ) dz
=
=
2g1/
−∞ exp(−ag1 ∗ /g1 ) = 1 2
δ 2
So , Pr[˜y1 ≥ LSf,1(x ) ] > 1 − δ/2 . By applying Lemma A.1 , Algorithm 1 is ( + , 0 + e δ 2 ) differential privacy , which is 2 e δ) differential privacy . ( 2 , 1 In Algorithm 2 , both ˜y1 and ˜y2 are differentially private . Similarly , Pr[˜y2 ≥ LSf,2(x ) ] > 1 − δ/2 . As above , 2 e δ) differential privacy . by applying Lemma A.1 , ˜y1 is ( 2 , 1 Furthermore , applying Lemma A.1 one more time , the final ˜y is ( 2 + , 1 2 ) differential privacy , which is ( 3 , 1
2 e2 δ) differential privacy .
2 e δ + e2 δ
2 e δ + 1
