Large Margin Distribution Machine
Teng Zhang and Zhi Hua Zhou
National Key Laboratory for Novel Software Technology
Nanjing University , Nanjing 210023 , China {zhangt , zhouzh}@lamdanjueducn
ABSTRACT Support vector machine ( SVM ) has been one of the most popular learning algorithms , with the central idea of maximizing the minimum margin , ie , the smallest distance from the instances to the classification boundary . Recent theoretical results , however , disclosed that maximizing the minimum margin does not necessarily lead to better generalization performances , and instead , the margin distribution has been proven to be more crucial . In this paper , we propose the Large margin Distribution Machine ( LDM ) , which tries to achieve a better generalization performance by optimizing the margin distribution . We characterize the margin distribution by the first and second order statistics , ie , the margin mean and variance . The LDM is a general learning approach which can be used in any place where SVM can be applied , and its superiority is verified both theoretically and empirically in this paper .
Categories and Subject Descriptors I26 [ Artificial Intelligence ] : Learning ; I52 [ Pattern Recognition ] : Design Methodology—classifier design and evaluation ; H28 [ Database Management ] : Database Applications—Data mining
Keywords Margin distribution ; minimum margin ; classification
1 .
INTRODUCTION
Support Vector Machine ( SVM ) [ 5 , 26 ] has always been one of the most successful learning algorithms . The basic idea is to identify a classification boundary having a large margin for all the training examples , and the resultant optimization can be accomplished by a quadratic programming ( QP ) problem . Although SVMs have a long history of literatures , there are still great efforts [ 16 , 6 , 25 , 14 , 8 ] on improving SVMs .
It is well known that SVM can be viewed as a learning approach trying to maximize over training examples the minimum margin , ie , the smallest distance from the examples to the classification boundary , and the margin theory [ 26 ] provided a good support to the generalization performance of SVM . It is noteworthy that the margin theory not only plays an important role for SVMs , but also has been extended to interpret the good generalization of many other learning approaches , such as AdaBoost [ 10 ] , a major representative of ensemble methods [ 31 ] . Specifically , Schapire et al . [ 21 ] first suggested margin theory to explain the phenomenon that AdaBoost seems resistant to overfitting ; soon after , Breiman [ 4 ] indicated that the minimum margin is crucial and developed a boosting style algorithm , Arc gv , which is able to maximize the minimum margin but with a poor generalization performance . Later , Reyzin et al . [ 20 ] found that although Arc gv tends to produce larger minimum margin , it suffers from a poor margin distribution ; they conjectured that the margin distribution , rather than the minimum margin , is more crucial to the generalization performance . Such a conjecture has been theoretically studied [ 27 , 11 ] , and it was recently proven by Gao and Zhou [ 11 ] . Moreover , it was disclosed that rather than simply considering a single point margin , both the margin mean and variance are important [ 11 ] . All these theoretical studies , however , focused on boosting style algorithms , whereas the influence of the margin distribution for SVMs in practice has not been well exploited .
In this paper , we propose the Large margin Distribution Machine ( LDM ) , which tries to achieve strong generalization performance by optimizing the margin distribution . Inspired by the recent theoretical result [ 11 ] , we characterize the margin distribution by the first and second order statistics , and try to maximize the margin mean and minimize the margin variance simultaneously . For optimization , we propose a dual coordinate descent method for kernel LDM , and propose an averaged stochastic gradient descent ( ASGD ) method for large scale linear kernel LDM . Comprehensive experiments on twenty regular scale data sets and twelve large scale data sets show the superiority of LDM to SVM and many stateof the art methods , verifying that the margin distribution is more crucial for SVM style learning approaches than minimum margin .
The rest of this paper is organized as follows . Section 2 introduces some preliminaries . Section 3 presents the LDM . Section 4 reports on our experiments . Section 5 discusses about some related works . Finally , Section 6 concludes .
2 . PRELIMINARIES We denote by X ∈ Rd the instance space and Y = {+1,−1} the label set . Let D be an unknown ( underlying ) distribution over X × Y . A training set of size m
S = {(x1 , y1 ) , ( x2 , y2 ) , . . . , ( xm , ym)} , is drawn identically and independently ( iid ) according to the distribution D . Our goal is to learn a function which is used to predict the labels for future unseen instances .
For SVMs , f is regarded as a linear model , ie , f ( x ) = ⊤ ϕ(x ) where w is a linear predictor , ϕ(x ) is a feature mapw ϕ(xj ) . ping of x induced by a kernel k , ie , k(xi , xj ) = ϕ(xi ) According to [ 5 , 26 ] , the margin of instance ( xi , yi ) is formulated as
⊤
⊤
γi = yiw
ϕ(xi),∀i = 1 , . . . , m .
( 1 )
From [ 7 ] , it is shown that in separable cases where the training examples can be separated with the zero error , SVM with hard margin ( or Hard margin SVM ) , min w
1 2
⊤ w st yiw w ϕ(xi ) ≥ 1 , i = 1 , . . . , m ,
⊤ is regarded as the maximization of the minimum margin {min{γi}m i=1} .
In non separable cases where the training examples cannot be separated with the zero error , SVM with soft margin ( or Soft margin SVM ) is posed , y1 , . . . , ym as the diagonal elements . According to the definition in ( 1 ) , the margin mean is
¯γ = yiw
⊤
ϕ(xi ) =
⊤
( Xy ) w ,
1 m and the margin variance is
1 m m∑ m∑ m∑ i=1
( 3 )
( 4 )
ˆγ =
1 m2 i=1 j=1 ⊤
=
2 m2 ( mw
⊤
ϕ(xi ) − yjw
⊤
ϕ(xj))2
( yiw
⊤ w − w
⊤
⊤
⊤
X w ) .
Xyy
XX
Inspired by the recent theoretical result [ 11 ] , LDM attempts to maximize the margin mean and minimize the margin variance simultaneously .
We first consider a simpler scenario , ie , the separable cases where the training examples can be separated with the zero error . In these cases , the maximization of the margin mean and the minimization of the margin variance leads to the following hard margin LDM , min w
1 2 w st yiw
⊤ w + λ1ˆγ − λ2¯γ ϕ(xi ) ≥ 1 , i = 1 , . . . , m , ⊤ where λ1 and λ2 are the parameters for trading off the margin variance , the margin mean and the model complexity . It ’s evident that the hard margin LDM subsumes the hardmargin SVM when λ1 and λ2 equal 0 .
For the non separable cases , similar to soft margin SVM , the soft margin LDM leads to m∑
ξi i=1
( 5 )
( 2 ) min w ;
⊤ w
1 2 w + λ1ˆγ − λ2¯γ + C ϕ(xi ) ≥ 1 − ξi , ⊤ ξi ≥ 0 , i = 1 , . . . , m . st yiw
Similarly , soft margin LDM subsumes the soft margin SVM if λ1 and λ2 both equal 0 . Because the soft margin SVM often performs much better than the hard margin one , in the following we will focus on soft margin LDM and if without clarification , LDM is referred to the soft margin LDM . 3.2 Optimization
We in this section first present a dual coordinate descent method for kernel LDM , and then present an average stochastic gradient descent ( ASGD ) method for large scale linear kernel LDM . 321 Kernel LDM By substituting ( 3) (4 ) , ( 5 ) leads to the following quadrat ic programming problem , w +
⊤
2λ1 m2 ( mw
XX m∑ min w ;
⊤ w
1 2 − λ2 ⊤
⊤
( Xy )
1 w + C m ϕ(xi ) ≥ 1 − ξi , ξi ≥ 0 , i = 1 , . . . , m . st yiw
⊤ w − w
⊤
⊤
⊤
X w )
Xyy
ξi i=1
( 6 )
( 6 ) is often intractable due to the high or infinite dimensionality of ϕ(· ) . Fortunately , inspired by the representer m∑
⊤ min w ;
1 2 st yiw
⊤
ξi w i=1 w + C ϕ(xi ) ≥ 1 − ξi , ξi ≥ 0 , i = 1 , . . . , m . ⊤
∑m
γ0 − ¯C w max st γi ≥ γ0 − ξi , i=1
ξi
ξi ≥ 0 , i = 1 , . . . , m , where = [ ξ1 , . . . , ξm ] measure the losses of instances , and C is a trading off parameter . There exists a constant ¯C such that ( 2 ) can be equivalently reformulated as , where γ0 is a relaxed minimum margin , and ¯C is the tradingoff parameter . Note that γ0 indeed characterizes the top p minimum margin [ 11 ] ; hence , SVMs ( with both hard margin and soft margin ) consider only a single point margin and have not exploited the whole margin distribution .
3 . LDM
In this section , we first formulate the margin distribution , and then present the optimization algorithms and the theoretical guarantee . 3.1 Formulation
The two most straightforward statistics for characterizing the margin distribution are the first and second order statistics , that is , the mean and the variance of the margin . Formally , denote X as the matrix whose i th column is ϕ(xi ) , ie , X = [ ϕ(x1 ) . . . ϕ(xm) ] , y = [ y1 , . . . , ym ] is a column vector , and Y is a m × m diagonal matrix with
⊤ theorem in [ 22 ] , the following theorem states that the optimal solution for ( 6 ) can be spanned by {ϕ(xi ) , 1 ≤ i ≤ m} . for problem ( 6 )
Theorem 1 . The optimal solution w
∗ admits a representation of the form
αiϕ(xi ) = Xff ,
( 7 ) i=1 ⊤ are the coefficients . where ff = [ α1 , . . . , αm ]
Proof . w
∗ can be decomposed into a part that lives in the span of ϕ(xi ) and an orthogonal part , ie , m∑
∗ w
= m∑ i=1 w =
αiϕ(xi ) + v = Xff + v for some ff = [ α1 , . . . , αm ] ⊤ for all j , ie , X ⊤
⊤ v = 0 . Note that
X w = X
( Xff + v ) = X
⊤
Xff ,
⊤ and v satisfying ϕ(xj )
⊤ v = 0 so the second and the third terms of ( 6 ) are independent of v ; further note that the constraint is also independent of v , thus the last terms of ( 6 ) is also independent of v .
As for the first term of ( 6 ) , since X v = 0 , consequently
⊤ we get ⊤ w
⊤
( Xff + v ) = ff
⊤
⊤
X
Xff + v
⊤ v w = ( Xff + v )
≥ ff
⊤
⊤
X
Xff with equality occurring if and only if v = 0 .
So , setting v = 0 does not affect the second , the third and the last term while strictly reduces the first term of ( 6 ) . Hence , w for problem ( 6 ) admits a representation of the form ( 7 ) .
∗
According to Theorem 1 , we have
⊤ w = X ⊤ w = ff
⊤
⊤
⊤
X w ⊤
Xff = Gff ,
X
Xff = ff
Gff , where G = X i th column of G , then ( 6 ) can be cast as
X is the kernel matrix . Let G:i denote the
⊤ m∑
ξi i=1
( 8 ) min ff ;
⊤ ff
1 2 st yiff
⊤ ff + C
Qff + p G:i ≥ 1 − ξi ,
⊤
ξi ≥ 0 , i = 1 , . . . , m , G − ( Gy)(Gy ) ⊤ ⊤ ⊤
)/m2 + G and p = where Q = 4λ1(mG −λ2Gy/m . By introducing the lagrange multipliers fi = [ β1 , . . . , βm ] for the first and the second constraints respectively , the Lagrangian of ( 8 ) leads to and = [ η1 , . . . , ηm ]
⊤ m∑
L(ff , , fi , ) =
ξi ff + C
G:i − 1 + ξi ) − m∑ i=1 i=1
( 9 )
ηiξi .
By setting the partial derivations of {ff , } to zero , we have
⊤
⊤
1 2
⊤ ff
βi(yiff
Qff + p
− m∑ = Qff + p − m∑ i=1
∂L ∂ff
∂L ∂ξi
βiyiG:i , i=1
= C − βi − ηi = 0 , i = 1 , . . . , m .
( 10 )
( 11 )
−1Gy , A = Q
−1GY , hii =
Algorithm 1 Kernel LDM
Input : Data set X , λ1 , λ2 , C Output : ff Initialize fi = 0 , ff = 2 ⊤ e i Y GQ while fi not converge do
−1GY ei ; m Q for i = 1 , . . . m do
( ( [ ∇f ( fi)]i ← e i Y Gff − 1 ; ⊤ i ← βi ; βold βi ← min ff ← ff + ( βi − βold
)Aei ; max
βi − [ ∇f ( fi)]i hii i
)
)
, 0
, C
; end for end while
By substituting ( 10 ) and ( 11 ) into ( 9 ) , the dual 1 of ( 8 ) can be cast as :
(
)⊤
⊤ f ( fi ) =
λ2 min m st 0 ≤ βi ≤ C , i = 1 , . . . , m .
Hfi +
1 2 fi fi
He − e fi ,
( 12 )
−1GY , Q
−1 refers to the inverse matrix where H = Y GQ of Q and e stands for the all one vector . Due to the simple decoupled box constraint and the convex quadratic objective function , as suggested by [ 29 ] , ( 12 ) can be efficiently solved by the dual coordinate descent method . In dual coordinate descent method [ 13 ] , one of the variables is selected to minimize while the other variables are kept as constants at each iteration , and a closed form solution can be achieved at each iteration . Specifically , to minimize βi by keeping the other βj̸=i ’s as constants , one needs to solve the following subproblem , t f ( fi + tei ) min st 0 ≤ βi + t ≤ C ,
( 13 ) where ei denotes the vector with 1 in the i th coordinate and 0 ’s elsewhere . Let H = [ hij]i;j=1;:::;m , we have hiit2 + [ ∇f ( fi)]it + f ( fi ) , f ( fi + tei ) = where [ ∇f ( fi)]i is the i th component of the gradient ∇f ( fi ) . Note that f ( fi ) is independent of t and thus can be dropped . Considering that f ( fi + tei ) is a simple quadratic function of t , and further note the box constraint 0 ≤ αi ≤ C , the minimizer of ( 13 ) leads to a closed form solution ,
1 2
(
( βi − [ ∇f ( fi)]i hii
)
)
, 0
, C
.
βnew i
= min max
Algorithm 1 summarizes the pseudo code of kernel LDM .
For prediction , according to ( 10 ) , one can obtain the co efficients ff from the optimal fi ) ∗ − p ) = Q
−1(GY fi ff = Q
(
−1GY
= Q
∗ e + fi
λ2 m
( as
∗
−1
.
)
GY e + GY fi
∗
λ2 m
1Here we omit constants without influence on optimization .
Hence for testing instance z , its label can be obtained by
(
⊤ sgn w
)
( m∑
ϕ(z )
= sgn
αik(xi , z )
. i=1
)
322 Large Scale Kernel LDM In section 321 , the proposed method can efficiently deal with kernel LDM . However , the inherent computational cost for the kernel matrix in kernel LDM takes O(m2 ) time , which might be computational prohibitive for large scale problems . To make LDM more useful , in the following , we present a fast linear kernel LDM for large scale problems by adopting the average stochastic gradient descent ( ASGD ) method [ 19 ] .
For linear kernel LDM , ( 5 ) can be reformulated as the following form , min w g(w ) =
⊤ w
1 2 w +
− λ2 m
⊤
( Xy ) w + C i=1
2λ1 m2 w m∑
⊤
⊤
( mXX max{0 , 1 − yiw
⊤ − Xyy X xi} ,
⊤
⊤
)w
( 14 ) where X = [ x1 . . . xm ] , y = [ y1 , . . . , ym ] tor .
⊤ is a column vec
For large scale problems , computing the gradient of ( 14 ) is expensive because its computation involves all the training examples . Stochastic gradient descent ( SGD ) works by computing a noisy unbiased estimation of the gradient via sampling a subset of the training examples . Theoretically , when the objective is convex , it can be shown that in expectation , SGD converges to the global optimal solution [ 15 , 3 ] . During the past decade , SGD has been applied to various machine learning problems and achieved promising performances [ 30 , 23 , 2 , 24 ] . unbiased estimation of the gradient ∇g(w ) .
The following theorem presents an approach to obtain an
Theorem 2 . If two examples ( xi , yi ) and ( xj , yj ) are sam pled from training set randomly , then
{ i w − 4λ1yixiyjx ∇g(w,xi , xj ) = 4λ1xix ⊤ ⊤ j w + w − λ2yixi − mC yixi 0 i ∈ I , otherwise ,
( 15 ) where I ≡ {i | yiw ∇g(w ) .
⊤ xi < 1} is an unbiased estimation of
Proof . Note that the gradient of g(w ) is
∇g(w ) = Qw + p − C yixi , i ∈ I , m∑ i=1
Algorithm 2 Large Scale Kernel LDM
Input : Data set X , λ1 , λ2 , C Output : ¯w Initialize u = 0 , T = 5 ; for t = 1 , . . . T m do
Randomly sample two training examples ( xi , yi ) and ( xj , yj ) ; Compute ∇g(w , xi , xj ) as in ( 15 ) ; w ← w − ηt∇g(w , xi , xj ) ; ¯w ← ¯w + µt(w − ¯w ) ; end for between xi and xj , and with ( 16 ) , we have
Exi;xj [ ∇g(w , xi , xj ) ] i ]w − 4λ1Exi [ yixi]Exj [ yjxj ] ⊤ )⊤ = 4λ1Exi [ xix − λ2Exi [ yixi ] − mCExi [ yixi | i ∈ I ]
(
⊤ w + w
1 m
Xy w + w yixi , i ∈ I
= 4λ1
1 m − λ2
1 m
XX
⊤
Xy
1 m w − 4λ1 m∑ Xy − mC m∑
1 m yixi , i ∈ I i=1
= Qw + p − C = ∇g(w ) . i=1
It is shown that ∇g(w , xi , xj ) is a noisy unbiased gradient of g(w ) .
With Theorem 2 , the stochastic gradient update can be formed as wt+1 = wt − ηt∇g(w , xi , xj ) ,
( 17 ) where ηt is a suitably chosen step size parameter in the t th iteration .
In practice , we use averaged stochastic gradient descent ( ASGD ) which is more robust than SGD [ 28 ] . At each iteration , besides performing the normal stochastic gradient update ( 17 ) , we also compute t∑
¯wt =
1 t − t0 wi , i=t0+1 where t0 determines when we engage the averaging process . This average can be computed efficiently using a recursive formula :
¯wt+1 = ¯wt + µt(wt+1 − ¯wt ) , where µt = 1/ max{1 , t − t0} .
⊤ − Xy(Xy ) ⊤ where Q = 4λ1(mXX m∑ −λ2Xy/m . Further note that m∑
Exi [ xix
⊤ i ] =
⊤ i =
1 m xix i=1
Exi [ yixi ] =
1 m yixi = i=1
1 m
Xy .
1 m
⊤
,
XX
( 16 )
According to the linearity of expectation , the independence
Algorithm 2 summarizes the pseudo code of large scale
)/m2 + I and p = kernel LDM . 3.3 Analysis
In this section , we study the statistical property of LDM . Specifically , we derive a bound on the expectation of error for LDM according to the leave one out cross validation estimate , which is an unbiased estimate of the probability of test error .
Here we only consider the linear case ( 14 ) for simplicity , however , the results are also applicable to any other feature mapping ϕ . Following the same steps in Section 321 , one can have the dual problem of ( 14 ) , ie ,
(
)⊤
He − e ff ,
( 18 )
⊤ ff ff
1 2
Hff + f ( ff ) =
λ2 min m st 0 ≤ αi ≤ C , i = 1 , . . . , m . −1XY , Q = 41
⊤ where H = Y X I , e stands for the all one vector and Q verse matrix of Q .
Q m2 ( mXX
⊤−Xy(Xy ) ⊤ )+ −1 refers to the in
Theorem 3 . Let ff denote the optimal solution of ( 18 ) , and E[R(ff ) ] be the expectation of the probability of error , then we have
∑
αi + |I 2| ] i∈I1 m
E[R(ff ) ] ≤ E[h
( 19 ) where I 1 ≡ {i | 0 < αi < C} , I 2 ≡ {i | αi = C} and h = max{diag{H}} . Proof . Suppose
,
∗ ff
= argmin 0≤ff≤C f ( ff ) , ffi = argmin
0≤ff≤C;ffi=0 f ( ff ) , i = 1 , . . . , m ,
( 20 ) and the corresponding solution for the linear kernel LDM are w and wi , respectively .
∗
As shown in [ 17 ] ,
E[L((x1 , y1 ) , . . . , ( xm , ym) ) ]
, m
E[R(ff ) ] =
( 21 ) where L((x1 , y1 ) , . . . , ( xm , ym ) ) is the number of errors in ∗ the leave one out procedure . Note that if α i = 0 , ( xi , yi ) will always be classified correctly in the leave one out procedure according to the KKT conditions . So for any misclassified example ( xi , yi ) , we only need to consider the following two cases :
∗ i < C , according to the definition in ( 20 ) , we
1 ) 0 < α have f ( ffi ) − min f ( ffi ) − f ( ff t f ( ffi + tei ) ≤ f ( ffi ) − f ( ff ) ≤ f ( ff i ei ) − f ( ff ∗ ∗ ∗
∗ − α
) ,
∗
) ,
( 22 )
( 23 ) where ei denotes a vector with 1 in the i th coordinate and 0 ’s elsewhere . We can find that , the left hand side of ( 22 ) is equal to ( 1 − yix ⊤ i wi)2/2hii , and the right hand side of ∗ 2hii/2 . So by combining ( 22 ) and ( 23 ) , ( 23 ) is equal to α i we have i wi)2/2hii ≤ α ( 1 − yix ⊤ ⊤ i wi < 0 , rearranging the above we
2hii/2 .
∗ i
Further note that yix can obtain 1 ≤ α ∗ i hii .
∗ i = C , all these examples will be misclassified in the
2 ) α leave one out procedure .
So we have
L((x1 , y1 ) , . . . , ( xm , ym ) ) ≤ h
∑ i∈I1 i + |I 2| , ∗
α where I 1 ≡ {i | 0 < α i = C} and ∗ h = max{hii , i = 1 , . . . , m} . Take expectation on both side and with ( 21 ) , we get that ( 19 ) holds . i < C} , I 2 ≡ {i | α ∗
4 . EMPIRICAL STUDY
In this section , we empirically evaluate the effectiveness of LDM on a broad range of data sets . We first introduce the experimental settings in Section 4.1 , and then compare LDM with SVM and three state of the art approaches2 in Section 4.2 and Section 43 In addition , we also study the cumulative margin distribution produced by LDM and SVM in Section 44 The computational cost and parameter influence are presented in Section 4.5 and Section 4.6 , respectively . 4.1 Experimental Setup
We evaluate the effectiveness of our proposed LDMs on twenty regular scale data sets and twelve large scale data sets , including both UCI data sets and real world data sets like KDD20103 . Table 1 summarizes the statistics of these data sets . The data set size is ranged from 106 to more than 8,000,000 , and the dimensionality is ranged from 2 to more than 20,000,000 , covering a broad range of properties . All features are normalized into the interval [ 0 , 1 ] . For each data set , half of examples are randomly selected as the training data , and the remaining examples are used as the testing data . For regular scale data sets , both linear and RBF kernels are evaluated . Experiments are repeated for 30 times with random data partitions , and the average accuracies as well as the standard deviations are recorded . For large scale data sets , linear kernel is evaluated . Experiments are repeated for 10 times with random data partitions , and the average accuracies ( with standard deviations ) are recorded . LDMs are compared with standard SVMs which ignore the margin distribution , and three state of the art methods , that is , Margin Distribution Optimization ( MDO ) [ 12 ] , Maximal Average Margin for Classifiers ( MAMC ) [ 18 ] and Kernel Method for the direct Optimization of the Margin Distribution ( KM OMD ) [ 1 ] . For SVM , KM OMD and LDM , the regularization parameter C is selected by 5 fold cross validation from [ 10 , 50 , 100 ] . For MDO , the parameters are set as the recommended parameters in [ 12 ] . For LDM , the regularization parameters λ1 , λ2 are selected by 5 fold cross −2 ] , the parameters ηt validation from the set of [ 2 and t0 are set with the same setup in [ 28 ] , and T is fixed to 5 . The width of the RBF kernel for SVM , MAMC , KM OMD and LDM are selected by 5 fold cross validation from the set −2δ , . . . , 22δ ] , where δ is the average distance between of [ 2 instances . All selections are performed on training sets . 4.2 Results on Regular Scale Data Sets
−8 , . . . , 2
Tables 2 and 3 summarize the results on twenty regular scale data sets . As can be seen , the overall performance of LDM is superior or highly competitive to SVM and other compared methods . Specifically , for linear kernel , LDM performs significantly better than SVM , MDO , MAMC , KM OMD on 12 , 9 , 17 and 10 over 20 data sets , respectively , and achieves the best accuracy on 13 data sets ; for RBF kernel , LDM performs significantly better than SVM , MAMC , KM OMD on 10 , 18 and 15 over 20 data sets , respectively , and achieves the best accuracy on 15 data sets . MDO is not compared since it is specified for the linear kernel . In addition , as can be seen , in comparing with standard SVM which does not consider margin distribution ,
2These approaches will be briefly introduced in Section 5 . 3https://pslcdatashopwebcmuedu/KDDCup/downloadsjsp
Table 1 : Characteristics of experimental data sets .
Scale
Dataset
#Instance #Feature Dataset
#Instance #Feature regular large promoters planning colic parkinsons colic.ORIG sonar vote house heart breast farm ads news20 adult a w8a cod rna real sim
106 182 188 195 205 208 232 232 270 277
4,143 19,996 32,561 49,749 59,535 72,309
57 12 13 22 17 60 16 16 9 9
54,877
1,355,191
123 300 8
20,958 haberman vehicle clean1 wdbc isolet credit a austra australian fourclass german ijcnn1 skin covtype rcv1 url kdd2010
306 435 476 569 600 653 690 690 862 1,000
141,691 245,057 581,012 697,641 2,396,130 8,407,752
14 16 166 14 51 15 15 42 2 59
22 3 54
47,236
3,231,961 20,216,830
Table 2 : Accuracy ( mean±std . ) comparison on regular scale data sets . Linear kernels are used . The best accuracy on each data set is bolded . •/◦ indicates the performance is significantly better/worse than SVM ( paired t tests at 95 % significance level ) . The win/tie/loss counts are summarized in the last row .
Dataset promoters planning relax colic parkinsons colic.ORIG sonar vote house heart breast cancer haberman vehicle clean1 wdbc isolet credit a austra australian fourclass german
Ave . accuracy
LDM : w/t/l
SVM
0723±0071 0683±0031 0814±0035 0846±0038 0618±0027 0725±0039 0934±0022 0942±0015 0799±0029 0717±0033 0734±0030 0959±0012 0803±0035 0963±0012 0995±0003 0861±0014 0857±0013 0844±0019 0724±0014 0711±0030
0.813
12/8/0
MDO
0713±0067 0605±0185◦ 0781±0154 0732±0270◦ 0624±0040 0734±0035 0587±0435◦ 0943±0015 0826±0026• 0710±0031 0728±0029 0956±0012 0798±0031 0966±0010 0501±0503◦ 0862±0013 0842±0055 0842±0020 0377±0238◦ 0737±0014•
MAMC
0520±0096◦ 0706±0034• 0661±0062◦ 0764±0035◦ 0623±0027 0533±0045◦ 0884±0022◦ 0883±0029◦ 0537±0057◦ 0706±0027 0738±0020 0566±0160◦ 0561±0025◦ 0623±0020◦ 0621±0207◦ 0596±0063◦ 0567±0044◦ 0576±0049◦ 0641±0020◦ 0697±0017◦
KM OMD 0736±0061 0479±0050◦ 0813±0028 0814±0024◦ 0635±0045• 0766±0033• 0957±0013• 0957±0020• 0836±0026• 0696±0031◦ 0667±0040◦ 0960±0010 0821±0027• 0968±0009• 0995±0003 0863±0013 0858±0013 0858±0016• 0736±0014• 0729±0017•
0.743
9/10/1
0.650
17/3/0
0.807
10/5/5
LDM
0721±0069 0706±0034• 0832±0026• 0865±0030• 0619±0042 0736±0036 0970±0014• 0968±0011• 0791±0030 0725±0027• 0738±0020 0959±0013 0814±0019• 0968±0011• 0997±0002• 0864±0013• 0859±0015 0866±0014• 0723±0014 0738±0016•
0.823 the win/tie/loss counts show that LDM is always better or comparable , never worse than SVM .
4.3 Results on Large Scale Data Sets
Table 4 summarizes the results on twelve large scale data sets . KM OMD did not return results on all data sets and MDO did not return results on KDD2010 in 48 hours due to the high computational cost . As can be seen , the overall performance of LDM is superior or highly competitive to SVM and other compared methods . Specifically , LDM performs significantly better than SVM , MDO , MAMC on 6 , 7 and 12 over 12 data sets , respectively , and achieves the best accuracy on 8 data sets . In addition , the win/tie/loss counts show that LDM is always better or comparable , never worse than SVM .
4.4 Margin Distributions
Figure 1 plots the cumulative margin distribution of SVM and LDM on some representative regular scale data sets . The curves for other data sets are similar . The point where a curve and the x axis crosses is the corresponding minimum margin . As can be seen , LDM usually has a little bit smaller minimum margin than SVM , whereas the LDM curve generally lies on the right side , showing that the
Table 3 : Accuracy ( mean±std . ) comparison on regular scale data sets . RBF kernels are used . The best accuracy on each data set is bolded . •/◦ indicates the performance is significantly better/worse than SVM ( paired t tests at 95 % significance level ) . The win/tie/loss counts are summarized in the last row . MDO does not have results since it is specified for the linear kernel .
Dataset
SVM
MDO
MAMC
0638±0121◦ 0706±0034 0623±0037◦ 0852±0036◦ 0623±0027 0753±0052◦ 0913±0019◦ 0561±0139◦ 0540±0043◦ 0706±0027◦ 0742±0021• 0924±0025◦ 0561±0025◦ 0740±0042◦ 0994±0004◦ 0542±0032◦ 0560±0018◦ 0554±0015◦ 0791±0014◦ 0697±0017◦
KM OMD 0701±0085 0683±0031◦ 0825±0024 0906±0033◦ 0621±0039 0821±0051◦ 0930±0029◦ 0938±0022◦ 0805±0048 0691±0024◦ 0676±0042◦ 0988±0008◦ 0772±0043◦ 0941±0040 0995±0003◦ 0845±0029◦ 0854±0017 0860±0014• 0838±0014◦ 0742±0017•
0.701
18/1/1
0.822
15/5/0 promoters planning relax colic parkinsons colic.ORIG sonar vote house heart breast cancer haberman vehicle clean1 wdbc isolet credit a austra australian fourclass german
Ave . accuracy
LDM : w/t/l
0684±0100 0708±0035 0822±0033 0929±0029 0638±0043 0842±0034 0946±0016 0953±0020 0808±0025 0729±0030 0727±0024 0992±0007 0890±0020 0951±0011 0998±0002 0858±0014 0853±0013 0815±0014 0998±0003 0731±0019
0.844
10/10/0
N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A
N/A
N/A
LDM
0715±0074• 0707±0034 0841±0018• 0927±0029 0641±0044 0846±0032 0968±0013• 0964±0013• 0822±0029• 0753±0027• 0731±0027 0993±0006 0891±0024 0961±0010• 0998±0002 0861±0013 0857±0014• 0854±0016• 0998±0003 0743±0016•
0.854
Table 4 : Accuracy ( mean±std . ) comparison on large scale data sets . Linear kernels are used . The best accuracy on each data set is bolded . •/◦ indicates the performance is significantly better/worse than SVM ( paired t tests at 95 % significance level ) . The win/tie/loss counts are summarized in the last row . KM OMD and MDO did not return results on some data sets in 48 hours .
Dataset farm ads news20 adult a w8a cod rna real sim ijcnn1 skin covtype rcv1 url kdd2010
SVM
0880±0007 0954±0002 0845±0002 0983±0001 0899±0001 0961±0001 0921±0003 0934±0001 0762±0001 0969±0000 0993±0006 0852±0001
Ave . accuracy
LDM : w/t/l
0.913
6/6/0
MDO
0880±0007 0948±0002◦ 0788±0053◦ 0985±0001• 0774±0203 0955±0002◦ 0921±0002 0929±0003◦ 0760±0003◦ 0959±0000◦ 0993±0006
N/A
0.899
7/3/1
MAMC
KM OMD
LDM
0759±0038◦ 0772±0017◦ 0759±0002◦ 0971±0001◦ 0667±0001◦ 0744±0004◦ 0904±0001◦ 0792±0000◦ 0628±0002◦ 0913±0000◦ 0670±0000◦ 0853±0000•
0.786
12/0/0
N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A
N/A
N/A
0890±0008• 0960±0001• 0846±0003• 0983±0001 0899±0001 0971±0001• 0921±0002 0934±0001 0763±0001 0977±0000• 0993±0006 0881±0001•
0.919 margin distribution of LDM is generally better than that of SVM . In other words , for most examples , LDM generally produce a larger margin than SVM .
4.5 Time Cost
We compare the time cost of LDM and SVM on the twelve large scale data sets . All the experiments are performed with MATLAB 2012b on a machine with 8×2.60 GHz CPUs and 16GB main memory . The average CPU time ( in seconds ) on each data set is shown in Figure 2 . We denote SVM implemented by the LIBLINEAR [ 9 ] package as SVMl and SVM implemented by ASGD4 as SVMa , respectively . It can be seen that , both SVMa and LDM are much faster than SVMl , owing to the use of ASGD . LDM is just slightly slower than SVMa on three data sets ( news20 , real sim and skin ) but highly competitive with SVMa on the other nine data sets . Note that both SVMl and SVMa are very fast implementa
4http://leonbottouorg/projects/sgd
Figure 1 : Cumulative frequency ( y axis ) with respect to margin ( x axis ) of SVM and LDM on some representative regular scale data sets . The more right the curve , the larger the accumulated margin .
Figure 2 : CPU time on the large scale data sets . tions of SVMs ; this shows that LDM is also computationally efficient . 4.6 Parameter Influence
LDM has three regularization parameters , ie , λ1 , λ2 and C . In previous empirical studies , they are set according to cross validation . Figure 3 further studies the influence of them on some representative regular scale data sets by fixing other parameters . Specifically , Figure 3(a ) shows the −8 to influence of λ1 on the accuracy by varying it from 2 −2 while fixing λ2 and C as the value suggested by the 2 cross validation described in Section 41 Figure 3(b ) and Figure 3(c ) are obtained in the same way . It can be seen that , the performance of LDM is not very sensitive to the setting of the parameters , making LDM even more attractive in practice .
5 . RELATED WORK
There are a few studies considered margin distribution [ 12 ] pro in SVM like algorithms [ 12 , 18 , 1 ] . Garg et al . posed the Margin Distribution Optimization ( MDO ) algorithm which minimizes the sum of the cost of each instance , where the cost is a function which assigns larger values to instances with smaller margins . MDO can be viewed as a method of optimizing weighted margin combination , where the weights are related to the margins . The objective function optimized by MDO , however , is non convex , and thus , it may get stuck in local minima . In addition , MDO can only be used for linear kernel . As our experiments in Section 4 disclosed , the performance of MDO is inferior to LDM .
Pelckmans et al . [ 18 ] proposed the Maximal Average Margin for Classifiers ( MAMC ) and it can be viewed as a special case of LDM assuming that the margin variance is zero . MAMC has a closed form solution , however , it will degenerate to a trivial solution when the classes are not with equal sizes . Our experiments in Section 4 showed that LDM is clearly superior to MAMC .
Aiolli et al .
[ 1 ] proposed a Kernel Method for the direct Optimization of the Margin Distribution ( KM OMD ) from a game theoretical perspective . Similar to MDO , this method
Figure 3 : Parameter influence on some representative regular scale data sets . also directly optimizes a weighted combination of margins over the training data , ignoring the influence of margin variances . Besides , this method considers hard margin only , which may be another reason why it behaves worse than our method . It is noteworthy that the computational cost prohibits KM OMD to be applied to large scale data , as shown in Table 4 .
6 . CONCLUSIONS
Support vector machines work by maximizing the minimum margin . Recent theoretical results suggested that the margin distribution , rather than a single point margin such as the minimum margin , is more crucial to the generalization performance . In this paper , we propose the large margin distribution machine ( LDM ) which tries to optimize the margin distribution by maximizing the margin mean and minimizing the margin variance simultaneously . The LDM is a general learning approach which can be used in any place where SVM can be applied . Comprehensive experiments on twenty regular scale data sets and twelve large scale data sets validate the superiority of LDM to SVMs and many state of the art methods . In the future it will be interesting to generalize the idea of LDM to regression and other learning settings .
7 . ACKNOWLEDGMENTS
The authors want to thank anonymous reviewers for helpful comments and suggestions . This research was supported by the National Science Foundation of China ( 61333014 ) and the National Key Basic Research Program of China ( 2014CB340501 ) .
8 . REFERENCES [ 1 ] F . Aiolli , G . San Martino , and A . Sperduti . A kernel method for the optimization of the margin distribution . In Proceedings of the 18th International Conference on Artificial Neural Networks , pages 305–314 , Prague , Czech , 2008 .
[ 2 ] A . Bordes , L . Bottou , and P . Gallinari . Sgd qn :
Careful quasi newton stochastic gradient descent . Journal of Machine Learning Research , 10:1737–1754 , 2009 .
[ 3 ] L . Bottou . Large scale machine learning with stochastic gradient descent . In Proceedings of the 19th
International Conference on Computational Statistics , pages 177–186 , Paris , France , 2010 .
[ 4 ] L . Breiman . Prediction games and arcing classifiers .
Neural Computation , 11(7):1493–1517 , 1999 .
[ 5 ] C . Cortes and V . Vapnik . Support vector networks .
Machine Learning , 20(3):273–297 , 1995 .
[ 6 ] A . Cotter , S . Shalev shwartz , and N . Srebro . Learning optimally sparse support vector machines . In Proceedings of the 30th International Conference on Machine Learning , pages 266–274 , Atlanta , GA , 2013 . [ 7 ] N . Cristianini and J . Shawe Taylor . An Introduction to
Support Vector Machines and Other Kernel based Learning Methods . Cambridge University Press , Cambridge , UK , 2000 .
[ 8 ] H . Do and K . Alexandre . Convex formulations of radius margin based support vector machines . In Proceedings of the 30th International Conference on Machine Learning , pages 169–177 , Atlanta , GA , 2013 .
[ 9 ] R . E . Fan , K . W . Chang , C . J . Hsieh , X . R . Wang , and C . J . Lin . Liblinear : A library for large linear classification . Journal of Machine Learning Research , 9:1871–1874 , 2008 .
[ 10 ] Y . Freund and R . E . Schapire . A decision theoretic generalization of on line learning and an application to boosting . In Proceedings of the 2nd European Conference on Computational Learning Theory , pages 23–37 , Barcelona , Spain , 1995 .
[ 11 ] W . Gao and Z H Zhou . On the doubt about margin explanation of boosting . Artificial Intelligence , 199–200:22–44 , 2013 .
[ 12 ] A . Garg and D . Roth . Margin distribution and learning algorithms . In Proceedings of the 20th International Conference on Machine Learning , pages 210–217 , Washington , DC , 2003 .
[ 13 ] C . J . Hsieh , K . W . Chang , C . J . Lin , S . S . Keerthi , and S . Sundararajan . A dual coordinate descent method for large scale linear svm . In Proceedings of the 25th International Conference on Machine Learning , pages 408–415 , Helsinki , Finland , 2008 .
[ 14 ] C . Jose , P . Goyal , P . Aggrwal , and M . Varma . Local deep kernel learning for efficient non linear svm prediction . In Proceedings of the 30th International Conference on Machine Learning , pages 486–494 , Atlanta , GA , 2013 .
[ 15 ] H . J . Kushner and G . G . Yin . Stochastic
[ 23 ] S . Shalev Shwartz , Y . Singer , and N . Srebro . Pegasos : approximation and recursive algorithms and applications ; 2nd ed . Springer , New York , 2003 .
[ 16 ] S . Lacoste julien , M . Jaggi , M . Schmidt , and
P . Pletscher . Block coordinate frank wolfe optimization for structural svms . In Proceedings of the 30th International Conference on Machine Learning , pages 53–61 , Atlanta , GA , 2013 .
[ 17 ] A . Luntz and V . Brailovsky . On estimation of characters obtained in statistical procedure of recognition ( in russian ) . Technicheskaya Kibernetica , 3 , 1969 .
[ 18 ] K . Pelckmans , J . Suykens , and B . D . Moor . A risk minimization principle for a class of parzen estimators . In J . Platt , D . Koller , Y . Singer , and S . Roweis , editors , Advances in Neural Information Processing Systems 20 , pages 1137–1144 . MIT Press , Cambridge , MA , 2008 .
Primal estimated sub gradient solver for svm . In Proceedings of the 24th International Conference on Machine Learning , pages 807–814 , Helsinki , Finland , 2007 .
[ 24 ] O . Shamir and T . Zhang . Stochastic gradient descent for non smooth optimization : Convergence results and optimal averaging schemes . In Proceedings of the 30th International Conference on Machine Learning , pages 71–79 , Atlanta , GA , 2013 .
[ 25 ] M . Takac , A . Bijral , P . Richtarik , and N . Srebro . Mini batch primal and dual methods for svms . In Proceedings of the 30th International Conference on Machine Learning , pages 1022–1030 , Atlanta , GA , 2013 .
[ 26 ] V . Vapnik . The Nature of Statistical Learning Theory .
Springer Verlag , New York , 1995 .
[ 27 ] L . W . Wang , M . Sugiyama , C . Yang , Z H Zhou , and
[ 19 ] B . T . Polyak and A . B . Juditsky . Acceleration of stochastic approximation by averaging . SIAM Journal on Control and Optimization , 30(4):838–855 , 1992 .
J . Feng . A refined margin analysis for boosting algorithms via equilibrium margin . Journal of Machine Learning Research , 12:1835–1863 , 2011 .
[ 20 ] L . Reyzin and R . E . Schapire . How boosting the
[ 28 ] W . Xu . Towards optimal one pass large scale learning margin can also boost classifier complexity . In Proceedings of 23rd International Conference on Machine Learning , pages 753–760 , Pittsburgh , PA , 2006 .
[ 21 ] R . E . Schapire , Y . Freund , P . L . Bartlett , and W . S . Lee . Boosting the margin : a new explanation for the effectives of voting methods . Annuals of Statistics , 26(5):1651–1686 , 1998 .
[ 22 ] B . Sch¨olkopf and A . Smola . Learning with kernels : support vector machines , regularization , optimization , and beyond . MIT Press , Cambridge , MA , 2001 . with averaged stochastic gradient descent . CoRR , abs/1107.2490 , 2011 .
[ 29 ] G . X . Yuan , C . H . Ho , and C . J . Lin . Recent advances of large scale linear classification . Proceedings of the IEEE , 100(9):2584–2603 , 2012 .
[ 30 ] T . Zhang . Solving large scale linear prediction problems using stochastic gradient descent algorithms . In Proceedings of the 21st International Conference on Machine learning , pages 116–123 , Banff , Canada , 2004 .
[ 31 ] Z H Zhou . Ensemble Methods : Foundations and
Algorithms . CRC Press , Boca Raton , FL , 2012 .
