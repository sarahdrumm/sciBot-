Time varying Learning and Content Analytics via Sparse Factor Analysis
Andrew S . Lan Rice University
Houston , TX 77005 mrlan@sparfacom
Christoph Studer Cornell University Ithaca , NY 14853 studer@sparfa.com
Richard G . Baraniuk
Rice University
Houston , TX 77005 richb@sparfa.com
ABSTRACT We propose SPARFA Trace , a new machine learning based framework for time varying learning and content analytics for educational applications . We develop a novel message passing based , blind , approximate Kalman filter for sparse factor analysis ( SPARFA ) that jointly traces learner concept knowledge over time , analyzes learner concept knowledge state transitions ( induced by interacting with learning resources , such as textbook sections , lecture videos , etc . , or the forgetting effect ) , and estimates the content organization and difficulty of the questions in assessments . These quantities are estimated solely from binary valued ( correct/incorrect ) graded learner response data and the specific actions each learner performs ( eg , answering a question or studying a learning resource ) at each time instant . Experimental results on two online course datasets demonstrate that SPARFA Trace is capable of tracing each learner ’s concept knowledge evolution over time , analyzing the quality and content organization of learning resources , and estimating the question–concept associations and the question difficulties . Moreover , we show that SPARFA Trace achieves comparable or better performance in predicting unobserved learner responses compared to existing collaborative filtering and knowledge tracing methods .
Keywords Expectation maximization , Kalman filter , learning analytics , personalized learning , sparse factor analysis
1 .
INTRODUCTION
The traditional “ one size fits all ” approach to education is a major bottleneck to improving learning outcomes worldwide . Fortunately , significant progress has been made over the past few decades on new technologies that provide timely feedback to learners as they follow personalized learning pathways through nonlinearly interconnected learning contents . Increasingly , these technologies are based on machine learning algorithms that automatically mine data from a potentially large number of learners interacting with learning contents ( see [ 14 , 15 ] for examples ) .
In our view , a personalized learning system ( PLS ) consists of two key components : ( i ) learning analytics ( LA ) , which estimate each learner ’s knowledge state and dynamically trace its changes over time , as they either learn by interacting with various learning resources ( eg , textbook sections , lecture videos , labs ) and questions ( eg , in quizzes , homework assignments , exams , and other assessments ) , or forget ( see [ 30] ) , and ( ii ) content analytics ( CA ) , which provide insight on the quality , difficulty , and organization of the learning resources and questions . 1.1 SPARse Factor Analysis ( SPARFA )
The recently developed sparse factor analysis ( SPARFA ) framework [ 18 ] proposes a set of statistical model and algorithms for machine learning based LA and CA . SPARFA models Yi,j , the binary valued graded response of learner j to question i , as a Bernoulli random variable ( with 1 representing a correct response and 0 an incorrect one ) :
Yi,j ∼ Ber(Φ(Zi,j ) ) with Zi,j = wT i cj − µi .
Here , Φ(· ) is the inverse logit/probit link function , and the slack variable Zi,j depends on three factors : ( i ) the question– concept association vector wi which characterizes how question i relates to each abstract concept , ( ii ) the learner concept knowledge vector cj of learner j , and ( iii ) the intrinsic difficulty µi of question i . Given a dataset of graded learner response data Y , SPARFA jointly estimates cj,∀j to effect LA and wi and µi , ∀i to effect CA .
While powerful , the SPARFA framework has two important limitations . First , it assumes that the learners’ concept knowledge states remain constant over time ; this reduces it ’s efficacy when applied to scenarios , where learners learn ( and forget ) concepts over time ( weeks , months , years , decades ) [ 4 ] . Second , SPARFA models only the learners’ interactions with questions , which measure concept knowledge states , and not other kinds of learning opportunities , such as reading a textbook , viewing a lecture video , or conducting a laboratory or Gedankenexperiment ; this complicates its application in automatically recommending new resources to individual learners for remedial or enrichment studies . 1.2 SPARFA Trace : Time varying LA and CA In this paper , we extend the SPARFA framework to address these limitations . We develop SPARFA Trace , an online estimation algorithm that jointly performs time varying
Figure 1 : The SPARFA Trace framework processes the graded learner response matrix Y ( binary valued , with ‘1’ denoting a correct response , ‘0’ an incorrect one , and ‘?’ indicates an unobserved one ) and the learner activity matrices {R(t)} ( binary valued , with ‘1’ denoting that a learner studied a particular learning resource , and ‘0’ otherwise ) . Upon analyzing this data , SPARFA Trace jointly traces the learner concept knowledge states c(t ) ( a happy face represents high concept knowledge , and a sad face represents low concept knowledge ) j over time , and estimates the learning resource content organization and quality parameters Dm , dm and Γm , together with question–concept association parameters wi and question difficulty parameters µi .
LA and CA . The core machinery is based on blind approximate Kalman filtering . The working principles of SPARFATrace are illustrated in Figure 1 . Time varying LA is performed by tracing the evolution of each learner ’s concept knowledge state vector c(t ) over time t , based on observed j binary valued ( correct/incorrect ) graded learner responses to questions matrix Y and on the learner activity matrices R(t ) . CA is performed by estimating the learner concept knowledge state transition parameters Dm , dm , and Γm , the question–concept associations and the question intrinsic difficulties wi and µi , based on the estimated learner concept knowledge states at all time instances .
Tracing the learners’ concept knowledge states over time is non trivial due to the fact that the observations are noisy , binary valued graded learner responses to questions . To perform this on line estimation process , we develop a novel message passing based algorithm in Section 3 that employs an approximate Kalman filter [ 12 ] . Furthermore , the underlying state transition and observation parameters are , in general , unknown in real educational scenarios . Therefore , in Section 4 , we introduce a set of novel convex optimizationbased algorithms to estimate these parameters directly ( and solely ) from learner response data .
To test and validate the effectiveness of SPARFA Trace , we conduct a series of validation experiments in Section 5 using real world educational datasets collected with OpenStax Tutor [ 3 , 21 ] . We show that SPARFA Trace can effectively trace learner concept knowledge , estimate learner concept knowledge state transition parameters , and estimate the question dependent parameters . Furthermore , we show that it achieves comparable or better performance than existing approaches on predicting unobserved learner responses . 1.3 Related work in knowledge tracing
The closest related work to SPARFA Trace is knowledge tracing ( KT ) , a popular technique for tracing learner knowl edge evolution over time and for predicting future learner performance ( see , eg , [ 5 , 22] ) . Powerful as it is , KT suffers from three drawbacks . First , KT uses binary learner knowledge state representations , characterizing learners as to whether they have mastered a certain concept or not , which provides limited explanatory power . Second , KT assumes that each question is associated with exactly one concept . This restriction limits KT to very narrow educational domains and prevents it from generalizing to typical courses/assessments involving multiple concepts . Third , KT uses a single “ probability of learning ” parameter to characterize learner knowledge state transitions . This limits KT ’s ability to analyze the quality and organization of different learning resources that would lead to different learner knowledge state transitions .
2 . SPARFA TRACE STATISTICAL MODEL We start by extending SPARFA [ 18 ] to model learner concept knowledge evolution over time . Then , in Section 2.2 , we characterize the transitions of a learner ’s concept knowledge states between consecutive time instances as an affine model . 2.1 Model for graded learner responses
The SPARFA Trace statistical model characterizes the probability that a learner answers a question correctly at a particular time instance in terms of ( i ) the learner ’s knowledge on every concept at this particular time instance , ( ii ) how the question relates to each concept , and ( iii ) the intrinsic difficulty of the question . To this end , let N denote the number of learners , K the number of latent concepts in the course/assessment , and T the total number of time instances throughout the course/assessment . We dej ∈ RK , t ∈ {1 , . . . , T} , j ∈ fine the K dimensional vector c(t ) {1 , . . . , N} , to represent the latent concept knowledge state easyeasymedeasyhardmedeasyAssessment 1Assessment 2Assessment 3concept knowledgestate transitionslearnerconceptknowledgequestion conceptassociationquestionintrinsicdifficultytimequestionslearnersreadbookwatchlectureSPARFA of the jth learner at time instance t . Let Q be the total number of questions . We further define the mapping i(t , j ) : {1 , . . . , T} × {1 , . . . , N} → {1 , . . . , Q} , which maps learner and time instance indices to question indices ; this information can be extracted from the learner activity log . We will use the shorthand notation i(t ) j = i(t , j ) to denote the index of the question that the jth learner answers at time instance t as i(t ) j . Under this notation , we define the ∈ RK , i ∈ {1 , . . . , Q} , as the K dimensional vector w question–concept association vector of this question . Fi∈ R to be the intrinsic diffinally , we define the scalar µ
( t ) i j
( t ) i j culty of question i(t ) j , with positive values of µ represent
( t ) i j representing easy ing difficult questions , and negative µ ones . i
( t ) j
Given these quantities , we characterize the binary valued graded response ( where 1 denotes a correct response and 0 an incorrect one ) , of learner j to question i(t ) at time inj stance t as a Bernoulli random variable : j ∼ Ber(Φ(Z ( t ) Y ( t ) j ) ) , j − µ c(t ) Z ( t ) j = wT
( t , j ) ∈ Ωobs , ∀t , j . i
,
( t ) j
( t ) i j
( 1 ) Here , the set Ωobs ⊆ {1 , . . . , T}×{1 , . . . , N} contains the indices associated with the observed graded learner response z data , since some responses might not be observed in practice . Φ(z ) denotes the inverse probit link function Φpro(z ) = exp(−t2/2 ) is the standard −∞N ( t ) dt , where N ( t ) = 1√ normal distribution . ( The inverse logit link function could also be used ; the inverse probit link function is preferred because it simplifies the calculations in Section 32 ) The likelihood of an observation Y ( t ) can , alternatively , be written as
2π j j ) = Φ,(2Y ( t )
| c(t ) p(Y ( t ) j j − 1)(wT j − µ c(t )
( t ) i j
( t ) i j
) , a shorthand expression we will use in what follows .
Following the original SPARFA framework [ 18 ] , we impose the following model assumptions :
( A1 ) The number of concepts is much smaller than the number of questions and the number of learners , K Q , N : This assumption imposes a low dimensional model on the learners’ responses to questions .
( A2 ) The vector wi is sparse : This assumption is based on the observation that each question should only be associated with a few concepts out of all concepts in the domain of a course/assessment .
( A3 ) The vector wi has non negative entries : This assumption enables one to interpret the entries in cj to be the latent concept knowledge of each learner , with positive values representing high concept knowledge , and negative values representing low concept knowledge .
These assumptions are reasonable in most real world educational scenarios and alleviate the common identifiability issue inherent to factor analysis ( if Zi,j = wT i cj , then we have Zi,j = wT Q . Hence , the estimation of wi and cj is non unique up to a unitary transformation ) . The assumptions also improve the interpretability of the variables wi , cj , and µi . icj for any orthonormal matrix i QT Qcj = wT
2.2 Model for state transitions
In this section , we propose a latent state transition model that characterizes the learner concept knowledge evolution between two consecutive time instances . We assume here that the concept knowledge state evolves for two primary reasons : ( i ) A learner may interact with learning resources ( eg , read a section of an assigned textbook , watch a lecture video , conduct a lab experiment , run a computer simulation , etc. ) , all of which are likely to result in an increase of their concept knowledge . ( ii ) A learner may simply forget a learned concept , resulting in a decrease of their concept knowledge . For the sake of simplicity of exposition , we will treat the forgetting effect [ 30 ] as a special learning resource that reduces learners’ concept knowledge over time . We assume that there are a total of M distinct learning resources . We define the mapping m(t , j ) : {1 , . . . , T} × {1 , . . . , N} → {1 , . . . , M} from time and learner indices to learning resource indices ; this information can be extracted from the learner activity log . We will use the shorthand no= m(t− 1 , j ) to denote the index of the learntation m(t−1 ) ing resource that learner j studies between time instance t−1 and time instance t . Armed with this notation , the learner activity summary matrices R(t ) illustrated in Figure 1 are = 1 , ∀(t , j ) , meaning that learner j indefined by R(t ) j,m j
( t ) j teracted with learning resource m(t ) j between time instances t and t + 1 , and 0 otherwise . We are now ready to model the transition of learner j ’s latent concept knowledge state from time instance t − 1 to t as p(c(t ) j N j
| c(t−1 ) ) = | ( IK + D c(t ) j
)c(t−1 ) j
, m m m
, Γ
+ d
( t−1 ) j
( t−1 ) j
( t−1 ) j
( 2 ) where N ( x|µ , Σ ) represents a multivariate Gaussian distribution with mean vector µ and covariance matrix Σ . IK is the K × K identity matrix ; D ( t−1 ) j are latent learner concept knowledge state transition parameters , which define an affine model on the transition of the jth learner ’s concept knowledge state by interacting with between time instances t− 1 and t . learning resource m(t−1 ) is a K × 1 vector . D characterizes the uncertainty is a K × K matrix , and d
The covariance matrix Γ
, and Γ
( t−1 ) j
( t−1 ) j
( t−1 ) j
( t−1 ) j
, d m m m m m j
( t−1 ) j m induced in the learner concept knowledge state transition by interacting with learning resource m(t−1 )
.
In order to reduce the number of parameters and to imand prove identifiability of the parameters D
, d j
( t−1 ) j m
( t−1 ) j m
, we impose three additional assumptions on the
Γ m
( t−1 ) j learner knowledge state transition matrix D
( t−1 ) j
: m
( A4 ) D
( t−1 ) j m is lower triangular : This assumption means that , the kth entry in the learner concept knowledge is only influenced by the the 1st , . . . , ( k−1)th vector c(t ) j . As a result , the upper entries in c(t−1 ) entry in c(t−1 ) represent pre requisite concepts covered early in the course , while lower entries represent advanced concepts covered towards the end of the course . j j
( A5 ) D m
( t−1 ) j has non negative entries : This assumption ensures , for example , that having low concept knowl edge at time instance t − 1 ( negative entries in c(t−1 ) ) does not result in high concept knowledge at time instance t ( positive entries in c(t ) j j ) .
( A6 ) D m
( t−1 ) j is sparse : This assumption amounts for the observation that learning resources typically only cover a small subset of concepts among all concepts covered in a course .
In contrast to the learner concept knowledge transition , we do not impose sparsity or non negativity matrix D
( t−1 ) j m properties on the intrinsic learner concept knowledge state in ( 2 ) ; this enables our framework transition vector d
( t−1 ) j m to model cases of poorly designed , misleading , or off topic learning resources that distract or confuse learners . Note that the forgetting effect can be modeled as a learning resource with negative entries in d . To further reduce
( t−1 ) j m the number of parameters , we assume that the covariance matrix Γ is diagonal .
( t−1 ) m j
3 . TIME VARYING LA
Time varying LA requires an on line algorithm [ 13 ] that traces the evolution of learner concept knowledge over time . Designing such an algorithm is complicated by the fact that the binary valued graded learner responses correspond to a non linear and non Gaussian observation model . The particle filter [ 6 ] is an on line state estimation algorithm in nonlinear and non Gaussian systems , which uses a set of MonteCarlo particles to approximate the latent state distribution . However , its excessive computational complexity prevents it from being applied to personalized learning at large scale ( especially if immediate feedback is required ) . On the contrary , the Kalman filter [ 12 ] is an efficient on line state estimation algorithm for linear dynamical systems ( LDSs ) with Gaussian observations , but it cannot be directly applied to time varying LA because of the non linear , non Gaussian observation model ( 1 ) .
In order to recast the time varying LA problem as an approximate Kalman filter , we next introduce a set of approximations that build upon ideas in expectation propagation [ 20 , 23 ] . We begin in Section 3.1 by reviewing the key elements of the Kalman filtering and smoothing approach , and then detail our approximate Kalman filter in Section 32 For notational simplicity , we will omit the learner index j are in this section , ie , the quantities D and d
( t−1 ) j m
( t−1 ) j m replaced by Dm(t−1 ) and dm(t−1 ) . Moreover , we use the shorthand notation Dm(t−1 ) for the quantity IK + Dm(t−1 ) . 3.1 Kalman filtering and smoothing
The Kalman filter [ 9 , 12 ] solves the problem of state estimation in LDSs , where the system consist of a series of continuous latent state variables that are separated by linear state transitions ; the state observations are corrupted by Gaussian noise . We briefly summarize the main findings from [ 19 ] . Let the LDS consists of a series of T latent state variables c(t ) , t = 1 , . . . , T , and observations y(t ) , t = 1 , . . . , T . The factor graph [ 17 , 28 ] associated to this LDS is visualized in Figure 2 . The latent states ( denoted by dashed circles ) form a Markov chain , meaning that the next state only depends on the current state but not on previous ones . The Kalman filter estimation
Figure 2 : Factor graph message passing algorithm for the estimation of a set of T latent state variables with Markovian transition properties from ( possibly noisy ) observations . procedure of the variables c(t ) , ∀t based on the observations y(t ) , ∀t ( denoted by solid circles ) can be formulated as a message passing algorithm that consists of two phases . First , a forward message passing phase ( ie , the Kalman filtering phase ) is performed . Then , using the estimates obtained during the Kalman filtering phase , a backward message passing phase—often referred to as Kalman smoothing or Rauch Tung Streibel ( RTS ) smoothing—is performed .
In the forward message passing phase ( see Figure 2 ) , the goal is to estimate latent state variables c(t ) based on the previous observations y(1 ) , . . . , y(t ) . In other words , the value of interest is p(c(t ) | y(1 ) , . . . , y(t) ) , ∀t . This quantity can be obtained via a left right message passing algorithm outlined in Figure 2 .
In Figure 2 , the outgoing message α(c(t ) ) from variable node c(t ) is given by [ 19 ]
α(c(t ) ) = α
( c(t ) ) p(y(t)| c(t ) ) t τ =1 b(τ )
= p(c(t)| y(1 ) , . . . , y(t) ) ,
α(c(t ) )
We can see that a scaled version of α(c(t) ) , α(c(t ) ) = where b(t ) = p(y(t ) | y(1 ) , . . . , y(t−1 ) ) is a scaling factor . t τ =1 b(τ ) = p(c(t ) | y(1 ) , . . . , y(t) ) , is exactly the value of p(c(t)| c(t−1))α(c(t−1))dc(t−1 ) . b(t)α(c(t ) ) = p(y(t)| c(t ) ) interest , which can be obtained in recursive fashion via
( 3 )
The key to obtaining a tractable and efficient estimator for p(c(t ) | y(1 ) , . . . , y(t ) ) is that the transition probability p(c(t ) | c(t−1 ) ) and the observation likelihood p(y(t ) | c(t ) ) satisfy certain properties such that the messagesα(c(t ) ) and α(c(t−1 ) ) take on the same functional form , just with differ ent parameters . A LDS satisfies this requirement , in which the transition probability and the observation likelihood are ( multivariate ) Gaussians of the following form : p(c(t)| c(t−1 ) ) = N ( c(t)| Dm(t−1 ) c(t−1 ) + dm(t−1 ) , Γm(t−1 ) ) , p(y(t)| c(t ) ) = N ( y(t)|Wi(t ) c(t ) , Σi(t ) ) . Here , Γm(t−1 ) is the covariance matrix for state transition , Wi(t ) is the measurement matrix , and Σi(t ) is the covariance matrix for the multivariate observation of the system . The functional form of the messages is also Gaussian , ie ,
α(c(t ) ) ∼ N ( c(t ) | m(t ) , V(t) ) . Under these conditions , the
forward message passing recursion ( 3 ) is given by b(t)α(c(t ) ) = N with the parameters b(t ) , m(t ) and V(t ) given by m(t ) = Dm(t−1 ) m(t−1 ) + dm(t−1 ) y(t ) − Wi(t )
+ K(t ) V(t ) =,I − K(t)Wi(t ) b(t ) = N y(t)|Wi(t ) Wi(t ) P(t−1)WT i(t ) + Σi(t )
( 4 )
,
, c(t)| m(t ) , V(t ) ,Dm(t−1 ) m(t−1 ) + dm(t−1 ) P(t−1 ) , , ,Dm(t−1 ) m(t−1 ) + dm(t−1 )
,Wi(t ) P(t−1)WT i(t ) + Σi(t )
,
−1 ,
T m(t−1 ) + Γm(t−1 ) . in which the matrices K(t ) and P(t−1 ) are
K(t ) = P(t−1)WT P(t−1 ) = Dm(t−1 ) V(t−1)D i(t ) c(t )
( 5 )
The recursion starts with p(c(1 ) ) = N ( c(1 ) | m(0 ) , V(0) ) , where we assume c(1 ) to be m(0 ) = 0K and V(0 ) = σ2 0IK . dc(t ) ,
Kalman smoothing uses future observations y(τ ) , τ > t to obtain a better estimate of the latent state at time inIn other words , the value of interest is now stance t . p(c(t ) | y(1 ) , . . . , y(T ) ) . In order to estimate this value , a set of backward recursions similar to the set of forward recursions ( 3 ) can be used : p(c(t)| c(t−1))p(y(t)| c(t))α(c(t))β(c(t ) ) b(t)α(c(t ) )
α(c(t−1))β(c(t−1 ) ) =α(c(t−1 ) ) where β(c(t ) ) = p(y(t+1),,y(T )|c(t ) ) est p(c(t ) | y(1 ) , . . . , y(T ) ) = α(c(t))β(c(t ) ) can be computed
. The quantity of inter recursively as a backward message passing process , given the estimates ( 4 ) following the completion of the forward message passing process detailed above .
τ =t+1 b(τ )
T
Specifically , in an LDS , the recursions take the form :
α(c(t−1))β(c(t−1 ) ) = N ( c(t−1)|m(t−1),V(t−1) ) , with the parameters m(t−1 ) and V(t−1 ) given by m(t−1 ) = m(t−1 ) +J(t−1)m(t)−Dm(t−1 ) m(t−1 ) − dm(t−1 )
V(t−1 ) = V(t−1 ) + J(t−1)V(t ) − P(t−1 ) with m(T ) = m(T ) and V(T ) = V(T ) .
J(t−1 ) = V(t−1)(Dm(t−1 ) )T ( P(t−1 ) )
( J(t−1))T ,
−1 ,
( 6 )
,
3.2 Approximate Kalman filtering for LA
The basic Kalman filtering and smoothing , ie , ( 4 ) and ( 6 ) are only suitable for applications with a Gaussian latent state transition model and a Gaussian observation model , while the forward and backward recursions ( 3 ) and ( 5 ) hold for arbitrary state transition and observation models . When attempting to trace latent learner concept knowledge states under the SPARFA model , it is not possible to make Gaussian observations of these states . Concretely , we have only binary valued graded learner responses as our observations .
= Φ
, ( 7 )
We will now detail approximations that enable the estimation of latent learner concept knowledge states for our model . As introduced in Section 2 , the observation model at time t is given by ( 1 ) and the state transition model is given by ( 2 ) . Therefore , the recursion formula for the forward message passing process ( 3 ) becomes b(t)α(c(t ) ) = p(Y ( t)| c(t ) ) p(c(t)| c(t−1))α(c(t−1))dc(t ) N c(t)|m(t),V(t ) where m(t ) = Dm(t−1 ) m(t−1 ) + dm(t−1 ) and V(t ) = Equation 7 shows that , α(c(t ) ) is no longer Gaussian even ifα(c(t−1 ) ) is Gaussian , under the probit binary observation
Dm(t−1 ) V(t−1)D i(t ) c(t)−µi(t ) wT
T m(t−1 ) +Γm(t−1 ) .
2Y ( t)−1 c(t)| m(t ) , V model . Thus , the closed form updates in ( 4 ) and ( 6 ) can no longer be applied . Therefore , we have to perform an approximate message passing approach within the Kalman filtering framework to arrive at a tractable estimator of c(t ) . A number of approaches has been proposed to approxi ; here , the bar on the variables denote the means and covariances of the approximated Gaussian messages . These approaches include the extended Kalman filter ( EKF ) [ 7 , 10 ] , which uses a linear approximation of the likelihood term mateα(c(t ) ) by a Gaussian distribution N around the point m(t ) , and thus reduce the non Gaussian uses them to approximate the mean and covariance ofα(c(t ) ) mode of α(c(t ) ) and the Hessian at the mode to approxi observation model to a Gaussian one ; the unscented Kalman filter ( UKF ) [ 11 , 27 ] , which uses the unscented transform ( UT ) to create a set of “ sigma vectors ” from p(c(t−1 ) ) and after the non Gaussian observation ; and Laplace approximation [ 23 , 31 ] , which use an iterative algorithm to find the mate the mean and covariance of the approximated Gaussian messages . We will employ an approximation approach first introduced in the expectation propagation ( EP ) literature [ 20 ] .
( t )
It is known that the specific values for m(t ) and V
( t ) that minimize the Kullback Leibler ( KL ) divergence be and a target distribution q(c ) are the first and second moments of q(c ) [ 23 ] . Fortuneately , for the probit observation model p(Y ( t ) | c(t ) ) = and b(t ) have Φ
2Y ( t ) − 1
, m(t ) , V
( t ) closed form expressions ( details omitted for simplicity ) : N ( z ) Φ(z )
2Y ( t ) − 1
,
1 + wT
( t ) c(t)|m(t ) , V tween N m(t ) = m(t ) + i(t ) c(t)−µi(t ) wT i(t)V(t ) = V(t ) − V(t)wi(t ) wT i(t)V(t)wi(t ) wT and m(t ) , V(t ) as given by ( 7 ) .
2Y ( t ) − 1 b(t ) = Φ(z ) ,
1 + wT with z =
V
( t ) i(t)m(t ) − µi(t ) i(t)V(t)wi(t )
1 + wT
V(t)wi(t ) i(t)V(t)wi(t )
N ( z ) Φ(z ) z +
N ( z )
Φ(z )
,
,
( 8 )
The inverse probit link function is preferred over the inverse logit link function , due to the existence of the closedform first and second moments described above . Therefore , we will focus on the inverse probit link function in the sequel . Armed with the efficient approximation ( 8 ) , the forward Kalman filtering and backward Kalman smoothing message passing scheme described in Section 3.1 can be applied to the problem at hand . Using these recursions , estimates of the desired quantities p(c(t ) | y(1 ) , . . . , y(T ) ) can be computed efficiently , providing a way for learner concept knowledge tracing under the model ( 1 ) .
4 . TIME VARYING CA
So far , we have described an approximate Kalman filtering and smoothing approach for learner concept knowl ) , ∀t , j . edge tracing , ie , to estimate p(c(t ) j The method proposed in Section 3 is only able to provide these estimates if all learner initial knowledge parameters , ∀j , all learner concept knowledge state transim(0 ) tion parameters Dm , dm , and Γm , ∀m , and all question parameters , wi and µi , ∀i , are given a priori .
, . . . , y(T )
| y(1 )
, V(0 ) j j j j
However , in a typical PLS , these parameters are unknown and need to be estimated from the observed data . We now detail a set of convex optimization based techniques to estimate the parameters Dm , dm , and Γm , ∀m , and wi , µi , ∀i,1 given the estimates of the latent learner concept knowledge states c(t ) obtained from the approximate Kalman filtering j approach described in Section 3 . The techniques we detail in this chapter allows SPARFA Trace to jointly trace learner concept knowledge and estimate learner , learning resource , and question dependent parameters , using an expectationmaximization ( EM ) approach . 4.1 SPARFA Trace as an EM algorithm
EM has been widely used in the Kalman filtering framework to estimate the parameters of interest in the system ( see [ 2 , Chap . 13 ] and [ 9 ] for more details ) due to numerous practical advantages [ 24 ] . SPARFA Trace performs parameter estimation in an iterative fashion in the EM framework . All parameters are initialized by random values , and then each iteration of the algorithm consist of two phases : ( i ) the current parameter estimates are used to estimate the la ) , ∀t , j , and ( ii ) tent state distributions p(c(t ) j these latent state estimates are then used to maximize the expected joint log likelihood of all the observed and latent state variables , ie ,
, . . . , y(T )
| y(1 ) j j
T N j=1 t=2
E
.logp(Y ( t ) c(t−1 )
,c(t ) j j j
E + ( t,j)∈Ωobs c(t ) j maximize
Dm,dm,Γm,∀m,wi,µi,∀i d
( t−1 ) j m
,Γ m
) ( t−1 ) j logp(c(t ) j |c(t−1 ) j
|c(t ) j ,w i
( t ) j
,µ i
( t ) j m
,D
)fi ,
( t−1 ) j
,
( 9 ) in order to obtain new parameter estimates . SPARFA Trace alternates between these two phases until convergence , ie , a maximum number of iterations is reached or the change in the estimated parameters between two consecutive iterations falls below a given threshold . 4.2 Estimating the state transition parameters 1The estimation of the learner initial knowledge parameters m(0 )
, ∀j is trivial and can be found in [ 2 ] .
, V(0 ) j j
E
( Dmc(t−1 ) j
)T Γ
−1 m
We start by estimating the latent learner concept knowllearning resource ) parameters edge state transition ( ie , Dm , dm , and Γm , ∀m . To this end , define Mm as the set containing time and learner indices ( t , j ) indicating that learner j studies the mth learning resource between time instances t−1 and t . With this definition , we aim to maximize the expected log likelihood ( 9 ) with respect to Dm and dm , subject to the assumptions ( A4)–(A6 ) . We start by estimating Dm and dm given Γm . In order to induce sparsity on Dm to take ( A6 ) into account , we impose an 1 norm penalty on Dm , which is defined as the sum of the absolute values of all entries of Dm [ 8 ] . Taking only the terms containing Dm and dm , we can formulate the following augmented optimization problem : j j j j j
)
,c(t )
)T Γ c(t−1 )
−1 m ( c(t ) t,j:(t,j)∈Mm j − c(t−1 ) j − c(t−1 )
( Pd ) minimize Dm∈L+,dm )−(c(t )
( Dmc(t−1 ) +γDm1 , where L+ denotes the set of lower triangular matrices with written [ Dm dm ] as Dm . We also write the augmented non negative entries . For notational simplicity , we have by Dm , correspondingly . Note that the 1 norm penalty The problem ( Pd ) is convex in Dm , and hence , can be
)T 1]T asc(t−1 ) only applies to the matrix Dm in this notation . latent state vectors [ (c(t−1 )
, when multiplied j j solved efficiently . In particular , we use the iterative fast iterative shrinkage and thresholding algorithm ( FISTA ) framework [ 1 ] . In each iteration = 1 , 2 , . . . , Lmax , the algorithm performs two steps . First , a gradient step that aims to lower the objective function performs
D+1 m ← D m − η∇f ( Dm ) ,
( 10 )
−1 m objective function ( excluding the 1 norm penalty ) in ( Pd ) . The quantity η is a step size parameter for iteration . Details on how to choose η can be found in [ 1 ] . The gradient where f ( Dm ) corresponds to the differentiable part of the ∇f ( Dm ) in ( 10 ) is given by
∇f ( Dm ) =−Γ −V(t−1 ) −m(t−1 ) V(t−1 )
V(t ) j ( m(t−1 ) j +m(t ) j − m(t−1 ) m(t ) m(t−1 ) ( m(t−1 ) j , V(t−1 ) , and V(t ) , m(t ) where the parameters J(t−1 ) are obtained from the backward recursions in ( 6 ) . Next , the FISTA algorithm performs a projection step , which takes into account the sparsifying regularizer γDm1 , and the assumptions ( A4 ) and ( A5 ) :
( m(t−1 ) + m(t−1 ) ( m(t−1 ) , m(t−1 ) t,j:(t,j)∈Mm
[ J(t−1 )
− D j 1
)T
)T
)T
)T m
,
] j j j j j j j j j j j j j j
D+1 m ← PL+ ( max{D+1 m − γη , 0} ) ,
( 11 ) where PL+ ( · ) corresponds to the projection onto the set of lower triangular matrices by setting all entries in the upper triangular part of D+1 m to zero . The maximum operator acts element wise on D+1 m . The updates ( 10 ) and ( 11 ) are repeated until convergence , eventually providing a new esti mate Dnew m for [ Dm dm ] .
Using these new estimates , the update for Γm can be com puted in closed form , which will be omitted for simplicity .
4.3 Estimating the question parameters We next show how to estimate the question dependent parameters wi , µi , ∀i . To this end , we define Qi as the collection set of time and learner indices ( t , j ) that learner j answered the ith question at time instance t . We then minimize the expected negative log likelihood of all the observed binary valued graded learner responses ( 1 ) for the ith question , subject to ( A2 ) and ( A3 ) . In order to impose sparsity on wi , we add an 1 norm penalty to the cost function , which leads to the following optimization problem :
( Pw ) minimize wi:wi,k≥0,∀k
( t,j)∈Qi
− logΦ((2Y ( t ) j − 1)(wT i c(t )
+ λwi1 .
E c(t ) j j − µi ) )
The problem ( Pw ) is convex in wi , thanks to the fact that the negative log likelihood of the observation likelihood is convex and the linearity of the expectation operator ( see [ 18 ] for details ) . However , the inverse probit link function prohibits us from obtaining a simple form of this expectation . In order to develop a tractable algorithm to approximately solve this problem , we utilize the unscented transform ( UT ) [ 27 ] to approximate the cost function of ( Pw ) . sigma vectors {(c(t ) given the mean m(t ) at {(c(t )
Following the paradigms of the UT , we generate a set of j )n} and a corresponding set of weights {un} , n ∈ {1 , . . . , 2K + 1} , for each latent state vector c(t ) j , . The cost function in the optimization problem ( Pw ) can now be approximated by a weighted average of the cost function evaluated j )n} . Once again , ( Pw ) can be solved efficiently by using the FISTA framework [ 1 ] . The gradient step is given by and covariance V(t ) j j
( 12 )
|Qi| i i − η ∇f ( wi ) . i = [ (gq is a ( 2K + 1)|Qi|× 1 vector ri = [ a1 aq i is defined by aq i )1 , . . . , ( gq
The gradient ∇f ( wi ) is given by ∇f ( wi ) = −(cid:102)Ciri , whereri
, in Qi . The K × ( 2K + 1)|Qi| matrix Ci is defined as Ci = .(Gi)1 , . . . , ( Gi)|Qi|fi , where the K × ( 2K + 1 ) matrix ( Gi)q in which ( tq , jq ) represents the qth time–learner index pair i ( c(tq ) i ( c(tq ) i . . . , a i )2K+1 ] , where
− 1)wT − 1)wT i )n = un2(Y ( tq )
N
]T . The vector
2(Y ( tq )
2(Y ( tq )
− 1 ) jq jq jq jq
)n
)n
( gq
Φ jq w+1 i ← w is given by
( Gi)q =
)1 , . . . , ( c(tq ) jq
( c(tq ) i ← max{w+1 jq w+1 i − λη , 0} .
)2K+1
The projection step is given by :
.
( 13 )
For simplicity of exposition , the question intrinsic difficulties µi are omitted in the derivations above , as they can be included as an additional entry in wi as [ wT i µi]T ; the corresponding latent learner concept knowledge state vectors c(t ) j are augmented accordingly as [ (c(t ) j )T − 1]T .
Table 1 : Comparisons of SPARFA Trace against knowledge tracing ( KT ) on predicting responses for new learners using using Dataset 1 . SPARFA Trace slightly outperforms KT on all three metrics .
Performance metric
Prediction accuracy Prediction likelihood Area under the ROC curve
KT 86.42 ± 0.16 % 0.7718 ± 0.0011 0.5989 ± 0.0056
SPARFA Trace 87.49 ± 0.12 % 0.8128 ± 0.0044 0.8157 ± 0.0028
5 . EXPERIMENTAL RESULTS
We now demonstrate the efficacy of SPARFA Trace using real world educational datasets . We begin by comparing SPARFA Trace against two established methods on predicting unobserved binary valued learner response data , namely knowledge tracing ( KT ) [ 5 , 22 ] and SPARFA [ 18 ] . Then , we show how SPARFA Trace is able to visualize learners’ concept knowledge state evolution over time , and the learning resource and question quality and their content organization . The regularization parameters λ and γ are chosen via cross validation [ 8 ] , and all experiments are repeated for 25 independent Monte–Carlo trials . 5.1 Predicting responses for new learners
We now compare SPARFA Trace against the KT method described in [ 22 ] for predicting responses for new learners that do not have previous recorded response history .
The dataset we use for this experiment is from an undergraduate computer engineering course collected using OpenStax Tutor ( OST ) [ 21 ] . We will refer to this dataset as “ Dataset 1 ” in the following experiments . This dataset consists of the binary valued graded response from 92 learners answering 203 questions , with 99.5 % of the responses observed . The course is organized as three independent sections : The first section is on digital logic , the second on data structures , and the third on basic programming concepts . The full course consist of 11 assessments , including 8 homework assignments and an exam at the end of each section ; we assume that the learners’ concept knowledge state transitions can only happen between two consecutive assignments/exams , due to their interaction with all the lectures/readings/exercises .
Since KT is only capable of handling educational datasets that involve a single concept , we partition Dataset 1 into three parts , with each part corresponding to one of the three independent sections . We run KT independently on the three parts , and aggregate the prediction results . We initialize the four parameters of KT ( learner prior , learning probability , guessing probability , slipping probability ) with the best initial value we find over 5 different initializations . For SPARFA Trace , we use K = 3 , with each concept corresponding to one section of the dataset .
For cross validation , we randomly partition Dataset 1 into 5 folds , with each fold consisting of 1/5 of the learners answering all questions . Four folds of the data are used as the training set and the other fold is used as the test set . We train both KT and SPARFA Trace on the training set and obtain estimates on all learner , learning resource and question dependent parameters , and test their prediction performances on the test set . For previously unobserved new learners in the test set , both algorithms make future predictions of Y ( t ) based on these estimates and observations Y ( 1 )
, . . . , Y ( t−1 )
, for t = 1 , . . . , T . j j j
Table 2 : Comparisons of SPARFA Trace against SPARFA M on predicting unobserved learner responses for Dataset 1 .
SPARFA M
SPARFA Trace
Accuracy
Likelihood
Dataset 2
Dataset 1 87.10 ± 0.04 % 86.64 ± 0.14 % 87.31 ± 0.05 % 86.29 ± 0.25 % 0.707 ± 0.003 0.727 ± 0.001
0.730 ± 0.001
0.704 ± 0.002
Dataset 1
Dataset 2
We compare both algorithms on three metrics : prediction accuracy , prediction likelihood , and area under the receiver operation characteristic ( ROC ) curve . The prediction accuracy corresponds to the percentage of correctly predicted responses ; the prediction likelihood corresponds to the average the predicted likelihood of the unobserved responses , ie , obs is the set of unobserved learner responses in the test set ; the area under the ROC curve is a commonly used performance metric for binary classifiers ( see [ 22 ] for details ) .
) where Ωc t,j:(t,j)∈Ωc p(Y ( t ) obs|
|w
1|Ωc
, c(t ) j , µ
( t ) i j
( t ) i j obs j
The means and standard deviations of all three metrics covering multiple cross validation trials are shown in Table 1 . We can see that SPARFA Trace slightly outperforms KT on all performance metrics for Dataset 1 . We also emphasize that SPARFA Trace is capable of achieving superior prediction performance while simultaneously estimating the quality and content organization parameters of all learning resources and questions . 5.2 Predicting unobserved learner responses We now compare SPARFA Trace against the original SPARFA framework [ 18 ] , which offers state of the art collaborative filtering performance on predicting unobserved binary valued graded learner responses .
We will use two datasets in this experiment . The first dataset is the full Dataset 1 with 92 learners answering 203 questions , explained in Section 51 The second dataset we use is from a signals and systems undergraduate course on OST , consisting of 41 learners answering 143 questions , with 97.1 % of the responses observed . We will refer to this dataset as “ Dataset 2 ” in the following experiments . All the questions were manually labeled with a number of K = 4 concepts , with the concepts being listed in Figure 5(b ) . The full course consist of 14 assessments , including 12 assignments and 2 exams . We randomly partition the 143×43 ( or 203×92 ) matrix Y of observed graded learner responses into 5 folds for crossvalidation . Four folds of the data are used as the training set and the other fold is used as the test set . We train both the probit variant of SPARFA M and SPARFA Trace on the training set to obtain estimates of all model parameters and then use these estimates to predict unobserved held out responses in the test set .
The means and standard deviations of the prediction accuracy and prediction likelihood metrics covering multiple cross validation trials are shown in Tables 1 and 2 . We see that SPARFA Trace achieves comparable or better performance than the static SPARFA M on both datasets . 5.3 Visualizing time varying LA and CA
In this section , we showcase another advantage of SPARFA Trace over existing KT and collaborative filtering methods , ie , the visualization of both learner knowledge
( a )
( b )
Figure 3 : Estimated latent learner concept knowledge states for all time instances , for Dataset 1 . ( a ) Learner 1 ’s latent concept knowledge state evolution ; ( b ) Average learner latent concept knowledge states evolution . state evolution over time and the estimated learning resource and question quality and content organization parameters . Figure 3(a ) shows the estimated latent learner concept knowledge states at all time instances for Learner 1 in Dataset 1 . We can see that their knowledge on Concepts 2 and 3 gradually improve over time , while their knowledge on Concept 1 does not . Therefore , recommending Learner 1 remedial material on Concept 1 seems necessary , which is verified by the fact that Learner 1 often responds incorrectly on questions covering Concept 1 towards the end of the course . Hence , SPARFA Trace can enable a PLS to provide timely feedback to individual learners on the their concept knowledge at all times , which reveals the learning progress of the learners . Figure 3(b ) shows the average learner concept knowledge states over the entire class at all time instances for Dataset 1 . Using this information , SPARFA Trace can also inform instructors on the trend of the concept knowledge state evolution of the entire class , in order to help them make timely adjustments to their course plans .
Figure 4(a ) and Figure 4(b ) show the quality and content organization of learning resources 3 and 9 for Dataset 2 . These figures visualize the leaners’ concept knowledge state transitions induced by interacting with Learning Resources 3 and 9 . Circular nodes represent concepts ; the leftmost set of dashed nodes represent the concept knowledge state vector c(t−1 ) , which are the learners’ concept knowledge states before interacting with these learning resources , and the rightmost set of solid nodes represent the concept knowledge state vector c(t ) , which are the learners’ concept knowledge states after interacting with these learning resources . Arrows represent the the learner concept knowledge state transition matrix Dm , the intrinsic quality vector of the learning resource dm , and their transformation effects on learners’ concept knowledge states . Dotted arrows represent unchanged learner concept knowledge states ; these arrows correspond to zero entries in Dm and dm . Solid arrows represent the intrinsic knowledge gain of some concepts , characterized by large , positive entries in dm . Dashed arrows represent the change in knowledge of advanced concepts due to their prerequisite concepts , characterized by non zero entries in Dm : High knowledge level on pre requisite concepts can result in improved understanding and an increase on knowledge of advanced concepts , while low knowledge level on these prerequisite concepts can result in confusion and a decrease on knowledge of advanced concepts .
As shown in Figure 4(a ) , Learning Resource 3 is used in early stage of the course , and we can see that this learn
050100150200−2−15−1−05005115Time Instance tConcept Knowledge Concept 1Concept 2Concept 3050100150200−2−15−1−05005115Time Instance tConcept Knowledge Concept 1Concept 2Concept 3 ( a )
( b )
Figure 4 : Visualized learner knowledge state transition effect of two distinct learning resources for Dataset 2 . ( a ) Learner knowledge state transition effect for Learning Resource 3 ; ( b ) Learner knowledge state transition effect for Learning resource 9 . ing resource gives the learners a positive knowledge gain of Concept 2 , while also helping on the more advanced Concepts 3 and 4 . As shown in Figure 4(b ) , Learning resource 9 is used in later stage of the course , and we can see that it uses the learners’ knowledge on all previous concepts to improve their knowledge on Concept 4 , while also providing a positive knowledge gain on Concepts 3 and 4 .
By analyzing the content organization of learning resources and their effects on learner concept knowledge state transitions , SPARFA Trace enables a PLS to automatically recommend corresponding learning resources to learners based on their strengths and weaknesses . The estimated learning resource quality information also helps course instructors to distinguish between effective learning resources , and poorly designed , off topic , or misleading learning resources , thus helping them to manage these learning resources more easily .
Figure 5 shows the question–concept association graph obtained from Dataset 2 . Circle nodes represent concept nodes , while square , box nodes represent question nodes . Each question box is labeled with the time instance at which it is assigned and its estimated intrinsic difficulty . From the graph we can see time evolving effects , as questions assigned in the early stages of the course cover basic concepts ( Concepts 1 and 2 ) , while questions assigned in later stages cover more advanced concepts ( Concepts 3 and 4 ) . Some questions are associated with multiple concepts , and they mostly correspond to the final exam questions ( boxes with dashed boundaries ) where the entire course is covered .
Thus , by estimating the intrinsic difficulty and content organization of each question , SPARFA Trace allows a PLS to generate feedback to instructors on the underlying knowledge structure of questions , which enables them to identify ill posed or off topic questions ( such as questions that are not associated to any concepts in Figure 5(a) ) .
6 . CONCLUSIONS
We have proposed SPARFA Trace , a novel blind approximate Kalman filtering approach for time varying learning and content analytics . The proposed method jointly traces latent learner concept knowledge evolution over time and simultaneously estimates the quality and content organization of the corresponding learning resources ( such as textbook sections or lecture videos ) and the questions in assessment sets . Being able to trace learners’ concept knowledge evolution over time will enable a PLS to make timely feedback
( a )
Concept 1
Concept 2
Laplace transform and filters
Sampling and reconstruction
Concept 3
Concept 4
Fourier series and Fourier transform Signals and systems basics
( b )
Figure 5 : ( a ) Question–concept association graph and concept labels for Dataset 2 . ( a ) Question– concept association graph . Note that for the visualization to be compact , we show only 1/3 of all questions in the dataset ; ( b ) Label of each concept . to learners on their strengths and weaknesses . Furthermore , the estimated content dependent parameters provide rich information on the knowledge structure and quality of learning resources . Together with the question parameters estimated , a PLS would be able to operate in an autonomous manner , requiring only minimal human input and intervention ; this paves the way of applying SPARFA Trace to MOOC scale education scenarios , where the massive amount of data precludes manual intervention .
We note that SPARFA Trace has potential to be applied to a wide range of other datasets , including ( but not necessarily limited to ) the analysis of temporal evolution in legislative voting data [ 29 ] , and the study of temporal effects in general collaborative filtering settings [ 16 , 25 , 26 , 32 ] .
Acknowledgments Thanks to Kim Davenport and JP Slavinsky for providing the OpenStax Tutor ( OST ) data , and Andrew Waters and Ryan Ning for helpful discussions . Visit the website www . sparfa.com , where you can learn more about the SPARFA project and purchase t shirts and other merchandise .
References [ 1 ] A . Beck and M . Teboulle . A fast iterative shrinkagethresholding algorithm for linear inverse problems . SIAM J . on Imaging Science , 2(1):183–202 , Mar . 2009 .
123434112234t 1 t142341122334t 1 tt=11561t=2151t=3225t=4118t=5064t=61712t=7252t=8192t=91964t=10182t=11180t=12180t=13273t=141993t=15188t=16095t=17184t=18220t=19054t=20103t=21225t=22178t=23107t=24095t=25114t=26142t=27055t=28040t=29045t=30041t=31091t=32 065t=33 005t=34035t=35 043t=36004t=37 049t=38 060t=39 083t=40 049t=41 109t=42 096t=43 104t=44 131t=45 134t=46 176t=47 125t=48 179 [ 2 ] C . M . Bishop and N . M . Nasrabadi . Pattern Recognition and Machine Learning . Springer New York , 2006 .
[ 3 ] A . C . Butler , E . J . Marsh , J . P . Slavinsky , and R . G . Baraniuk . Integrating cognitive science and technology improve learning in a STEM classroom . Educational Psychology Review , 26(1 ) , Feb . 2014 .
[ 18 ] A . S . Lan , A . E . Waters , C . Studer , and R . G . Baraniuk . Sparse factor analysis for learning and content analytics . J . of Machine Learning Research , to appear , 2014 .
[ 19 ] T . P . Minka . From hidden Markov models to linear dynamical systems . Technical report , MIT , 1999 .
[ 4 ] M . Carrier and H . Pashler . The influence of retrieval on retention . Memory & Cognition , 20(6):633–642 , Nov . 1992 .
[ 20 ] T . P . Minka . Expectation propagation for approximate Bayesian inference . In Proc . 17th Conf . on Uncertainty in Artificial Intelligence , pages 362–369 , Aug . 2001 .
[ 5 ] A . T . Corbett and J . R . Anderson . Knowledge tracing : Modeling the acquisition of procedural knowledge . User Modeling and User adapted Interaction , 4(4):253–278 , Dec . 1994 .
[ 6 ] A . Doucet , N . De Freitas , K . Murphy , and S . Russell . Rao Blackwellised particle filtering for dynamic Bayesian networks . In Proc.16th Conf . on Uncertainty in Artificial Intelligence , pages 176–183 , June 2000 .
[ 7 ] G . A . Einicke and L . B . White . Robust extended IEEE Trans . on Signal Processing ,
Kalman filtering . 47(9):2596–2599 , Sep . 1999 .
[ 8 ] T . Hastie , R . Tibshirani , and J . Friedman . The Ele ments of Statistical Learning . Springer , 2010 .
[ 9 ] S . S . Haykin . Kalman Filtering and Neural Networks .
Wiley Online Library , 2001 .
[ 10 ] A . H . Jazwinski . Stochastic Processes and Filtering
Theory . Academic Press , New York , 1970 .
[ 11 ] S . J . Julier and J . K . Uhlmann . New extension of the Kalman filter to nonlinear systems . In Proc . 11th Intl . Symposium on Aerospace/Defense Sensing , Simulation and Controls , pages 182–193 , Apr . 1997 .
[ 12 ] R . E . Kalman . A new approach to linear filtering and prediction problems . ASME J . of Basic Engineering , 82(1):35–45 , 1960 .
[ 13 ] S . P . Kasiviswanathan , H . Wang , A . Banerjee , and P . Melville . Online 1 dictionary learning with application to novel document detection . Advances in Neural Information Processing Systems , pages 2267–2275 , Dec . 2012 .
[ 14 ] G . Kennedy ,
I . Ioannou , Y . Zhou , J . Bailey , and S . O’Leary . Mining interactions in immersive learning environments for real time student feedback . Australasian J . of Educational Technology , 29(2 ) , 2013 .
[ 15 ] Knewton . Knewton adaptive learning : Building the world ’s most powerful recommendation engine for education . online , June 2012 .
[ 16 ] Y . Koren and J . Sill . OrdRec : an ordinal model for predicting personalized item rating distributions . In Proc . of the 5th ACM Conf . on Recommender Systems , pages 117–124 , Oct . 2011 .
[ 17 ] F . R . Kschischang , B . J . Frey , and H A Loeliger . Factor graphs and the sum product algorithm . IEEE Trans . on Information Theory , 47(2):498–519 , Feb . 2001 .
[ 21 ] OpenStaxTutor . https://openstaxtutor.org/ , 2013 .
[ 22 ] Z . A . Pardos and N . T . Heffernan . Modeling individualization in a Bayesian networks implementation of knowledge tracing . In Proc . 18th Intl . Conf . on User Modeling , Adaptation , and Personalization , pages 255– 266 , June 2010 .
[ 23 ] C . E . Rasmussen and C . K . I . Williams . Gaussian Pro cess for Machine Learning . MIT Press , 2006 .
[ 24 ] S . Roweis and Z . Ghahramani . Learning nonlinear dynamical systems using the expectation–maximization algorithm . Kalman Filtering and Neural Networks , 6:175–220 , 2001 .
[ 25 ] J . Silva and L . Carin . Active learning for online Bayesian matrix factorization . In Proc . 18th Intl . Conf . on Knowledge Discovery and Data Mining , pages 325– 333 , Aug . 2012 .
[ 26 ] N . Thai Nghe , T . Horvath , and L . Schmidt Thieme . Factorization models for forecasting student performance . In Proc . 4th Intl . Conf . on Educational Data Mining , pages 11–20 , July 2011 .
[ 27 ] E . A . Wan and R . Van Der Merwe . The unscented Kalman filter for nonlinear estimation . In Adaptive Systems for Signal Processing , Communications , and Control Symposium , pages 153–158 , Oct . 2000 .
[ 28 ] C . Wang , J . Tang , J . Sun , and J . Han . Dynamic social influence analysis through time dependent factor graphs . In Intl . Conf . on Advances in Social Networks Analysis and Mining , pages 239–246 , July 2011 .
[ 29 ] E . Wang , E . Salazar , D . Dunson , and L . Carin . Spatiotemporal modeling of legislation and votes . Bayesian Analysis , 8(1):233–268 , Mar . 2013 .
[ 30 ] B . Weiner and H . Reed . Effects of the instructional sets to remember and to forget on short term retention : Studies of rehearsal control and retrieval inhibition ( repression ) . J . of Experimental Psychology , 79(2):226 , Feb . 1969 .
[ 31 ] R . Wolfinger . Laplace ’s approximation for nonlinear mixed models . Biometrika , 80(4):791–795 , Dec . 1993 .
[ 32 ] H . Yu et al . Feature engineering and classifier ensemble for KDD cup 2010 . JMLR Workshop and Conference Proceedings , 2010 .
