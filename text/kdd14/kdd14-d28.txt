Core Decomposition of Uncertain Graphs
Francesco Bonchi1 Francesco Gullo1 Andreas Kaltenbrunner2 Yana Volkovich2
1Yahoo Labs , Spain
2Barcelona Media Innovation Centre , Spain
{bonchi,gullo}@yahoo inc.com {andreaskaltenbrunner,yanavolkovich}@barcelonamediaorg
ABSTRACT Core decomposition has proven to be a useful primitive for a wide range of graph analyses . One of its most appealing features is that , unlike other notions of dense subgraphs , it can be computed linearly in the size of the input graph .
In this paper we provide an analogous tool for uncertain graphs , ie , graphs whose edges are assigned a probability of existence . The fact that core decomposition can be computed efficiently in deterministic graphs does not guarantee efficiency in uncertain graphs , where even the simplest graph operations may become computationally intensive . Here we show that core decomposition of uncertain graphs can be carried out efficiently as well .
We extensively evaluate our definitions and methods on a number of real world datasets and applications , such as influence maximization and task driven team formation .
Categories and Subject Descriptors H28 [ Database Management ] : [ Database ApplicationsData Mining ] ; G22 [ Discrete Mathematics ] : [ Graph Theory Graph Algorithms ]
Keywords uncertain graphs ; dense subgraph ; core decomposition
1 .
INTRODUCTION
Uncertain graphs , ie , graphs whose edges are assigned a probability of existence ( see an example in Figure 1 ) , arise in several emerging applications [ 24 , 14 , 15 ] . For instance , in biological networks and protein interaction networks vertices represent genes and/or proteins , while edges represent interactions among them . Since the interactions are derived through noisy and error prone laboratory experiments , the existence of each edge is uncertain [ 4 , 26 , 24 ] . In social networks uncertainty arises for various reasons [ 1 ] . Edge probabilities may represent the outcome of a link prediction task [ 20 ] or the influence of one person on another , like in
Figure 1 : An uncertain graph and its ( k , η) core decomposition for η = 004 Vertex 1 has core number 1 , vertices 2 and 7 have core number 2 , and vertices 3 , 4 , 5 and 6 have core number 3 . viral marketing [ 11 ] . Uncertainty can also be intentionally injected for privacy purposes [ 7 ] .
Finding dense subgraphs is a fundamental primitive in many graph analysis tasks [ 21 ] . There exist many different definitions of what a dense subgraph is , eg , cliques , n cliques , n clans , k plexes , f groups , n clubs , lambda sets , most of which are NP hard to compute or at least quadratic in the size of the input graph . In this respect , the notion of core decomposition is particularly appealing as ( i ) it can be computed in linear time [ 5 ] , and ( ii ) it is related to many other definitions of a dense subgraph ( as discussed later ) .
The k core of a graph is defined as a maximal subgraph in which every vertex is connected to at least k other vertices within that subgraph . The set of all k cores of a graph G forms the core decomposition of G [ 25 ] . The fact that core decomposition can be performed in linear time in deterministic graphs does not guarantee efficiency in uncertain graphs . Indeed , in such graphs even the simplest tasks may become hard . As an example , consider the two terminalreachability problem , which asks whether two query vertices are connected . In a deterministic graph the solution to this problem requires a simple scan of the graph . Instead , in uncertain graphs , computing the probability that two vertices are connected is a #P complete problem [ 28 ] .
Thus , a major question we aim at answering in this paper is : can the core decomposition of an uncertain graph be computed efficiently ?
Related work and applications . Existing research on uncertain graphs has mainly focused on querying [ 15 , 33 , 24 , 31 ] and mining , particularly on extracting frequent subgraphs [ 34 ] or subgraphs that are connected with high probability [ 14 ] , and clustering [ 22 , 18 ] .
Core decomposition of deterministic graphs has been exploited to analyse the nature of a network and discover dense substructures [ 2 , 17 ] . It has been applied in many different domains , such as bioinformatics [ 30 ] , software engineering [ 32 ] , and social networks [ 17 ] . Core decomposition has been also used to speed up the computation of more complex definitions of a dense subgraph . For instance , it serves to find maximal cliques more efficiently [ 10 ] , and it is at the basis of linear time approximation algorithms for the densestsubgraph problem [ 19 ] and the densest at least k subgraph problem [ 3 ] . It is also used to approximate betweenness centrality [ 13 ] . A core decomposition tool for uncertain graphs would thus provide a natural extension of all these applications to the context of uncertain graphs . Other direct applications of core decomposition of uncertain graphs include influence maximization and task driven team formation , which we showcase in Section 6 and 7 , respectively .
In influence maximization [ 16 ] , the probability of an edge ( u , v ) represents the influence that u exerts on v , ie , the likelihood that some action/information propagates from u to v . The greedy algorithm [ 12 ] traditionally used to find the users that maximize the information spread over the network requires a number of Monte Carlo simulations that largely limit its efficiency . In Section 6 we show how our probabilistic core decomposition tool can be used to speedup the influence maximization process .
In task driven team formation , the input is a collaboration graph G = ( V , E , τ ) , where vertices are individuals and edges exhibit a probabilistic topic model τ representing the topic(s ) of past collaborations . A query is a pair hT , Qi , where T is a set of terms describing a new task , and Q is a set of vertices . The goal is to find an answer set of vertices A , such that A ⊇ Q is a good team for the task described by T . The given query task T , along with the topic model τ , induces a ( single ) probability value pT for each edge ( u , v ) ∈ E , such that pT ( u , v ) represents the likelihood that u and v collaborate on T . This gives rise to an uncertain graph to which one can naturally apply core decomposition in order to find the desired team ( Section 7 ) .
Challenges and contributions . In this paper we study the problem of core decomposition of uncertain graphs , which , to the best of our knowledge , has never been considered so far . We introduce ( Section 2 ) the notion of ( k , η)core as a maximal subgraph whose vertices have at least k neigbours in that subgraph with probability no less than η ; here η ∈ [ 0 , 1 ] is a threshold defining the desired level of certainty of the output cores .
Let the η degree of a vertex v be the maximum degree such that the probability for v to have that degree is no less than η . We design an algorithm for finding a ( k , η)core decomposition that iteratively removes the vertex having the smallest η degree and prove its correctness ( Section 3 ) . The proposed algorithm resembles the traditional algorithm for computing the core decomposition of a deterministic graph [ 5 ] ; however , as usual when the attention is shifted from the deterministic context to uncertain graphs , the adaptation of that algorithm is non trivial . A major challenge is the capability of handling large graphs .
Two main critical steps affect our algorithm : computing initial η degrees and updating η degrees whenever a vertex is removed from the graph . While the corresponding steps in the deterministic case ( ie , computing and updating the degree of a vertex ) are straightforward , performing them efficiently in uncertain graphs needs a great deal of attention ; approaching them na¨ıvely , indeed , may even lead to intractable ( exponential ) time complexity . We show how to overcome the exponential time complexity by devising a novel yet efficient dynamic programming method to compute η degrees from scratch . We also exploit the same intuition underlying the dynamic programming algorithm so as to efficiently update η degrees after a vertex removal . As a result , we show that computing a ( k , η) core decomposition takes O(m∆ ) time , where m is the number of edges in the input uncertain graph and ∆ is the maximum η degree .
As a further contribution , we devise a novel method to improve the efficiency of the proposed ( k , η) coredecomposition algorithm ( Section 4 ) . The idea is to exploit a fast to compute lower bound on the η degree that can be used as a placeholder during the first iterations while being replaced with the actual η degree only when the vertex at hand is selected and the graph has become smaller .
Finally , we report experiments on efficiency and numerical stability on real world graphs ( Section 5 ) and show our proposal at work in two real life applications ( Sections 6–7 ) .
2 . PROBLEM DEFINITION
Cores of deterministic graphs . Before focusing on uncertain graphs , we briefly recall the problem of computing cores of deterministic graphs . Let G = ( V , E ) be an undirected graph , where V is a set of n vertices and E ⊆ V × V is a set of m edges . For every vertex v ∈ V , let deg(v ) and degH(v ) denote the degree of v in G and in a subgraph H of G , respectively . Also , given a set of vertices C ⊆ V , let E|C denote the subset of edges induced by C , ie , E|C = {(u , v ) ∈ E | u ∈ C , v ∈ C} .
Definition 1
( k core ) . The k core ( or core of order k ) of G is a maximal subgraph H = ( C , E|C ) such that ∀v ∈ C : degH(v ) ≥ k . The core number ( or core index ) of a vertex v , denoted c(v ) , is the highest order of a core that contains v . The set of all k cores of G , for all k , is the core decomposition of G .
The notion of k core is strictly related to the notion of kshell , that is the subgraph induced by the set of all vertices having core number equal to k . Note that neither k cores nor k shells are necessarily connected subgraphs . Also , while these two notions usually refer to subgraphs of the input graph , in the remainder we slightly abuse of notation and denote by k core ( or k shell ) both the subgraph H = ( C , E|C ) itself and the vertex set C that induces H .
All k shells of a graph G form a partition of the vertex set V , while all k cores are nested into each other : G = C0 ⊇ C1 ⊇ · · · ⊇ Ck∗ ( k∗ = maxv∈V c(v) ) . As a result , the core decomposition of G is unique and fully determined by the core number c(v ) of all vertices v in G : the k core of G simply corresponds to ( the subgraph induced by ) the set of all vertices v having core number c(v ) ≥ k .
Batagelj and Zaverˇsnik [ 5 ] show how to compute the core decomposition of a graph G in linear time ( Algorithm 1 ) . The algorithm iteratively removes the smallest degree vertex and sets the core number of the removed vertex accordingly . Vertices are thus required to be ordered based on their degree . Defining the initial vertex ordering and keeping vertices ordered during the execution of the algorithm take O(n ) and O(1 ) time , respectively . The idea is to employ an n dimensional vector D whose single cells D[i ] store all vertices having degree equal to i in the current graph . The overall time complexity of the algorithm is hence O(n + m ) .
Algorithm 1 k cores Input : A graph G = ( V , E ) . Output : An n dimensional vector c containing the core number of each v ∈ V .
1 : c ← ∅ , d ← ∅ , D ← [ ∅ , . . . , ∅ ] 2 : for all v ∈ V do 3 : d[v ] ← deg(v ) 4 : D[deg(v ) ] ← D[deg(v ) ] ∪ {v} 5 : end for 6 : for all k = 0 , 1 , . . . , n do 7 : while D[k ] 6= ∅ do 8 : 9 : 10 : 11 : 12 : 13 : 14 : 15 : 16 : end for end for remove v from G end while pick and remove a vertex v from D[k ] c[v ] ← k for all u : ( u , v ) ∈ E , d[u ] > k do move u from D[d[u ] ] to D[d[u ] − 1 ] d[u ] ← d[u ] − 1
Cores of uncertain graphs . Let G = ( V , E , p ) be an uncertain graph , where p : E → ( 0 , 1 ] is a function that assigns a probability of existence to each edge.1 For the sake of brevity , we hereinafter denote the probabilities p(e ) with pe . For every vertex v ∈ V , let Nv = {(u , v ) ∈ E} denote the set of edges incident to v , and dv = |Nv| its size . To define our notion of core decomposition of an uncertain graph , we resort to the well known possible world semantics , which has been recognized as a sound principle to define queries on probabilistic data [ 9 ] . Broadly , such a principle interprets the probabilistic data as a set of deterministic instantiations , called possible worlds , each of which associated with its probability of being observed . In the context of uncertain graphs , the bulk of the literature assumes the probabilities of existence of the edges independent from one another [ 24 , 14 , 15 ] . Under this assumption , the possibleworld semantics interprets an uncertain graph G with m edges as a set of 2m possible deterministic graphs ( worlds ) , each of which containing a subset of the edges in E . More precisely , an uncertain graph G = ( V , E , p ) yields a set of possible graphs {G = ( V , EG)}EG⊆E , and the probability of observing a possible graph G = ( V , EG ) ⊑ G is :
Pr(G ) = Ye∈EG pe Ye∈E\EG
( 1 − pe ) .
( 1 )
According to the possible world semantics , answering a probabilistic query q means to derive a probability distribution over all possible deterministic answers a to the query q , where the probability of an answer a corresponds to the sum of the probabilities of all worlds where a is the answer to q . As this answer distribution is usually too large and sparse to be explicitly interpreted or computed/stored , the general turnaround adopted is to assign a score to each domain object based on its probability of being part of an answer to the probabilistic query q , and return the objects having highest scores as a final answer to q [ 9 ] .
We cast such a general framework to our context by defining the score of each vertex v to be part of a k core H as
1We consider undirected graphs for the sake of presentation and consistency with the literature on core decomposition . However , all our definitions/methods apply to directed graphs too , by simply replacing the notion of degree with either in degree or out degree . Indeed , in Section 6 , where we focus on influence maximization , the graph is directed and we define probabilistic cores based on out degree . the probability that v has degree no less than k in H , ie , Pr[degH(v ) ≥ k ] . Then , we employ a classic threshold based approach to decide which vertices should actually form a kcore based on their scores . As a result , the notion of probabilistic ( k , η) core we come up with is the following :
Definition 2
( Probabilistic ( k,η) cores ) . Given an uncertain graph G = ( V , E , p ) , and a threshold η ∈ [ 0 , 1 ] , the probabilistic ( k , η) core of G is a maximal subgraph H = ( C , E|C , p ) such that the probability that each vertex v ∈ C has degree no less than k in H is greater than or equal to η , ie , ∀v ∈ C : Pr[degH(v ) ≥ k ] ≥ η .
The notion of η core number immediately follows from the definition of ( k,η) core and is defined as the highest order k of a ( k,η) core containing v .
The problem we address in this work is the following .
Problem 1
( ProbCores ) . Given an uncertain graph G and a probability threshold η ∈ [ 0 , 1 ] , find the ( k , η) core decomposition of G , that is the set of all ( k,η) cores of G .
Our definition of core decomposition of an uncertain graph , has the desirable feature of being unique , as formally shown in the next theorem .
Theorem 1 . Given an uncertain graph G and a probability threshold η , the ( k , η) core decomposition of G is unique .
Proof . We prove the theorem by showing that G cannot have more than one ( k , η) core , for all k . Assume that G has two ( k , η) cores and denote them by H1 and H2 , respectively . According to Definition 2 , it holds that H1 is a maximal subgraph of G such that ∀v ∈ H1 : Pr[degH1 ( v ) ≥ k ] ≥ η , and the same happens for H2 . Combining the ( k , η)core conditions of H1 and H2 leads to the subgraph H1 ∪ H2 to satisfy the ( k , η) core condition too , as ∀v ∈ H1 : Pr[degH1 ( v ) ≥ k ] ≥ η ∧ ∀v ∈ H2 : Pr[degH2 ( v ) ≥ k ] ≥ η clearly implies that ∀v ∈ H1 ∪ H2 : Pr[degH1∪H2 ( v ) ≥ k ] ≥ η . This means that neither H1 nor H2 are maximal , thus contradicting the hypothesis . The theorem follows .
An example of ( k , η) core decomposition of an uncertain graph is provided in Figure 1 .
3 . COMPUTING PROBABILISTIC CORES For a vertex v of the input uncertain graph G , the proba bility Pr[deg(v ) ≥ k ] can be expressed as :
Pr[deg(v ) ≥ k ] =PG⊑G≥k v
Pr(G ) ,
( 2 ) v where G≥k where v has degree ≥ k , ie , G≥k is the set of all possible graphs drawn from G v = {G ⊑ G | degG(v ) ≥ k} . It is easy to see that such a probability value is monotonically non increasing with k , ie , Pr[deg(v ) ≥ 0 ] ≥ Pr[deg(v ) ≥ 1 ] ≥ . . . ≥ Pr[deg(v ) ≥ dv ] . Then , given a threshold η , for every vertex v in the graph , there exists a value ˆk ∈ [ 0dv ] such that Pr[deg(v ) ≥ h ] ≥ η , for all h ≤ ˆk , and Pr[deg(v ) ≥ h ] < η , for all h > ˆk . We call this value the η degree of vertex v .
Definition 3
G = ( V , E , p ) and a threshold η ∈ [ 0 , 1 ] , η deg(v ) of a vertex v ∈ V is defined as
( η degree ) . Given an uncertain graph the η degree
η deg(v ) = max{k ∈ [ 0dv ] | Pr[deg(v ) ≥ k ] ≥ η} .
Let also η degH(v ) be the η degree of v in a subgraph H .
Algorithm 2 ( k,η) cores Input : An uncertain graph G = ( V , E , p ) , a threshold η ∈ [ 0 , 1 ] . Output : An n dimensional vector c containing the η core num note that Pr[deg(v ) ≥ k ] is equal to the sum of the probabilities Pr[deg(v ) = i ] either for all i ∈ [ kdv ] or , equivalently , for all i ∈ [ 0k − 1 ] : ber of each v ∈ V . d[v ] ← η deg(v )
1 : compute η deg(v ) for all v ∈ V 2 : c ← ∅ , d ← ∅ , D ← [ ∅ , . . . , ∅ ] 3 : for all v ∈ V do 4 : 5 : D[η deg(v ) ] ← D[η deg(v ) ] ∪ {v} 6 : end for 7 : for all k = 0 , 1 , . . . , n do 8 : while D[k ] 6= ∅ do 9 : 10 : 11 : 12 : 13 : 14 : 15 : 16 : 17 : 18 : end for end for remove v from G end while pick and remove a vertex v from D[k ] c[v ] ← k for all u : ( u , v ) ∈ E , d[u ] > k do recompute η deg(u ) move u from D[d[u ] ] to D[η deg(u ) ] d[u ] ← η deg(u )
Intuitively , the notion of η degree gives an idea of the degree of a vertex given a specific threshold η . We exploit the notion η degree to adapt the k cores algorithm used for deterministic graphs to the context of uncertain graphs . The proposed algorithm , called ( k,η) cores ( Algorithm 2 ) , follows the same scheme as in the deterministic case with the main difference of the use of the η degree . The soundness of the proposed algorithm is shown in the following theorem .
Theorem 2 . Given an uncertain graph G and a threshold η , Algorithm 2 provides the ( k,η) core decomposition of G .
Proof . For every v ∈ V and every subgraph H = ( C , EC , p|C ) of G , it is easy to see that η degH(v ) ≤ η deg(v ) , as the η degree computation in G relies on more successful events than those encountered in H . This implies that η degH(v ) is a monotonic vertex property function [ 5 ] , where , for every v ∈ V and C ⊆ V , a vertex property function on G is a function φ(v , C ) : V × 2V → R , and the monotonicity property holds if ∀ C1 , C2 ⊆ V : C1 ⊆ C2 implies that ∀ v ∈ V : φ(v , C1 ) ≤ φ(v , C2 ) . The proof is completed by the result by Batagelj and Zaverˇsnik [ 5 ] , who show that , for a monotonic vertex property function φ(v , C ) , the algorithm that repeatedly removes a vertex with the smallest φ value gives the desired core decomposition.2
Instead of computing/updating standard degrees , in the probabilistic case one thus needs to ( i ) compute all η degrees at the beginning of the algorithm ( Line 1 ) , and ( ii ) update the η degree of a neighbour of the currently being processed vertex v ( Line 12 ) . While computing/updating degrees in the deterministic case is straightforward , for the η degrees such steps are non trivial , as shown next .
Computing initial η degrees
Pr[deg(v ) ≥ k ] =
Pr[deg(v ) = i ] = 1−
Pr[deg(v ) = i ] . ( 3 ) dvXi=k k−1Xi=0
Furthermore , we observe that each individual Pr[deg(v ) = i ] can in turn be computed considering all subsets of edges N ⊆ Nv of size i and summing over the probabilities that all and only the edges in these various N exist :
Pr[deg(v ) = i ] = XN⊆Nv ,
|N|=i Ye∈N pe Ye∈Nv \N
( 1 − pe ) .
( 4 )
The sum in the above formula is over all subsets N ⊆ Nv , |N | = i ; thus , a na¨ıve computation would lead to a time complexity exponential in the size of Nv . We can however manage this by rearranging the formula as
Pr[deg(v ) = i ] = PvR(i , Nv ) , where Pv = Qe∈Nv ( 1 − pe ) , R(i , Nv ) = PN⊆Nv , and ˜pe = pe . This rearrangement allows us to exploit the 1−pe next recursive formula , which has originally been introduced in [ 8 ] for sampling from a finite population with unequal probabilities and without replacement :
|N|=i Qe∈N ˜pe ,
R(i , Nv ) =
1 i iXj=1
( −1)j+1T ( j , Nv)R(i − j , Nv ) ,
( 5 ) where T ( j , Nv ) =Pe∈Nv ( ˜pe)j . Now , it is easy to see that
Equation ( 5 ) allows for computing all individual Pr[deg(v ) = i ] values , for all i ∈ [ 0k − 1 ] ( which , according to Equation ( 3 ) , are needed to derive the desired Pr[deg(v ) ≥ k ] ) in polynomial time , precisely in O(kdv ) time . A dynamic programming method . Although the above way of computing Pr[deg(v ) = i ] solves a seemingly exponential time problem , it still has weaknesses due to the recursive formula in Equation ( 5 ) . Firstly , as the formula involves both products and sums of ˜pe values that can be either very large ( when pe → 1 ) or very small ( when pe → 0 ) , it may incur numerical stability issues , which might make the computation of Pr[deg(v ) = i ] problematic when executed by a computer . Secondly , using such a formula , the η degree of a vertex v when one of its incident edges is removed cannot be recomputed faster than a from scratch computation . For the above reasons , we propose here an alternative way of computing Pr[deg(v ) = i ] . Consider a vertex v and an edge e incident to v , and let G\{e} denote the subgraph of G where e is not present . The method is based on the following key observation : the event “ v has degree k in G ” implies that either “ e exists and v has degree k − 1 in G\{e} ” or “ e does not exist and v has degree k in G\{e} ” . This way , the probability for v to have degree k in the original graph G can be computed as a linear combination of the probabilities that v has degree either k − 1 or k in the subgraph G\{e} .
To show how to derive η degrees from scratch , we first focus on the computation of Pr[deg(v ) ≥ k ] for a vertex v , and
2
That work states that the time complexity of such an algorithm is O(m × max{D , log n} ) , where D is the maximum degree . But this is a general result for vertex property functions that can be updated linearly in the degree of a vertex . For any specific vertex property function , such as our η degree , the complexity can be higher or lower .
The above reasoning can be generalised to every subgraph of G and formally expressed in the next theorem ( for which we omit a formal proof due to limited space ) .
Theorem 3 . Given an uncertain graph G = ( V , E , p ) and a vertex v ∈ V , let Nv = {e1 , . . . , edv } be the set of all edges incident to v ordered in some way . Also , given a subset
N ⊆ Nv , let deg(v|N ) denote the degree of v in the subgraph ˆG = ( V , E \ ( Nv \ N ) , p ) . For all h ∈ [ 1dv − 1 ] it holds that :
Pr[deg(v|{e1 , . . . , eh+1} ) = i ] =
= peh+1 Pr[deg(v|{e1 , . . . , eh} ) = i − 1 ] +
( 6 )
+(1 − peh+1 ) Pr[deg(v|{e1 , . . . , eh)} = i ] .
Theorem 3 provides a principled way to efficiently compute Pr[deg(v ) = i ] based on the dynamic programming paradigm . Particularly , we take an arbitrary ordering of the edges incident to the vertex v being currently under consideration and define a proper recursive formula that allows for computing partial solutions relying only on the first i edges . The ultimate score ( ie , the actual value of Pr[deg(v ) = i ] ) is available only when all the edges have been considered ; this makes the overall computation independent from the specific ordering of the edges . Formally , let X(h , j ) = Pr[deg(v|{e1 , . . . , eh} ) = j ] , for all h ∈ [ 0dv ] , j ∈ [ −1i ] We set the following base cases :
X(0 , 0 ) = 1 , X(h , −1 ) = 0 , X(h , j ) = 0 , for all h ∈ [ 0dv ] , for all h ∈ [ 0dv ] , j ∈ [ h + 1i ] ,
 while we exploit Equation ( 6 ) to compute the generic dynamic programming recursive step as
X(h , j ) = peh X(h − 1 , j − 1 ) + ( 1 − peh )X(h − 1 , j ) , for all h ∈ [ 1dv ] , j ∈ [ 0h ] We need to compute all X(· , · ) values so as to get to X(dv , i ) , which corresponds to the desired probability Pr[deg(v ) = i ] . This requires O(idv ) time .
Moreover , one can notice that the values of the entire set {X(dv , j)}i j=0 ( not just X(dv , i ) ) correspond to the actual probability values {Pr[deg(v ) = j]}i j=0 . Thus , employing the proposed dynamic programming method and setting i = k − 1 , the probability values Pr[deg(v ) = 0 ] , . . . , Pr[deg(v ) = k − 1 ] , which are required for computing Pr[deg(v ) ≥ k ] according to Equation ( 3 ) , can all be derived in O(kdv ) time . Thus , the dynamic programming method just described has the same complexity as the method based on Equation ( 4 ) . But , at the same time , it ( i ) alleviates the numerical stability shortcomings , as the numbers involved into Equation ( 6 ) are all probabilities ≤ 1 ( unlike the numbers ˜pe which range from [ 0 , ∞) ) , and ( ii ) can easily be employed for efficiently updating η degrees when an edge is removed from the graph , as described next .
Time complexity . The η degree of a vertex v can be computed incrementally . We start with k = 0 and Pr[deg(v ) ≥ 0 ] = 1 . Then , we increase k one by one and compute Pr[deg(v ) ≥ k ] as Pr[deg(v ) ≥ k−1]−Pr[deg(v ) = k−1 ] . We stop once Pr[deg(v ) ≥ k ] < η , and we set η deg(v ) = k − 1 . This way , we need to compute probabilities Pr[deg(v ) = k ] only for k = 0 , , η deg(v ) + 1 , which , according to the findings reported above , leads to a time complexity of O(η deg(v ) × dv ) . Clearly , in the worst case , such a complexity equals O(d2 v ) , but we expect in practice η deg(v ) reasonably lower than dv , especially for those vertices having very large dv and/or large enough η values . dv ) . Denoting by ∆ the maximum η degree over all vertices in the graph , ie , ∆ = maxv∈V η deg(v ) , the complexity can
Computing all η degrees hence takes O(Pv∈V η deg(v ) × be more compactly expressed as O(Pv∈V dv∆ ) = O(m∆ ) .
Updating η degrees
We now consider the case where the η degree of a vertex v needs to be updated because an edge incident to v has been removed . We recall that this is the other crucial step of our ( k,η) cores algorithm ( Algorithm 2 , Line 12 ) .
As anticipated , we can exploit Theorem 3 to avoid from scratch recomputations . The problem can be reduced to ( efficiently ) updating the probabilities Pr[deg(v ) = 0 ] , . . . , Pr[deg(v ) = η deg(v) ] , whose earlier values are available because of the computation of the earlier η degree . Once all these new probabilities are computed , the new η degree can be derived by the same incremental process described in the previous paragraph “ Time complexity ” . the edge to be
Let e denote removed and let Pr[deg(v|¬e ) = i ] , for all i ∈ [ 0 , . . . , η deg(v) ] , be a shorthand for the new probabilities Pr[deg(v|Nv \ {e} ) = i ] to be computed . Such Pr[deg(v|¬e ) = i ] values can be derived by rearranging Equation ( 6 ) as follows :
Pr[deg(v|¬e ) = i ] =
Pr[deg(v ) = i]−pePr[deg(v|¬e ) = i−1 ]
1 − pe
.
( 7 )
This way , one can set Pr[deg(v|¬e ) = 0 ] = 1 Pr[deg(v ) = 0 ] , and apply Equation ( 7 ) to compute the remaining Pr[deg(v|¬e ) = i ] values , for all i ∈ [ 1η deg(v ) ] Each probability Pr[deg(v|¬e ) = i ] takes constant time . Computing all the new probabilities , and , hence , updating the η degree of v , globally takes O(η deg(v ) ) time , thus improving upon the O(η deg(v ) × dv ) time of a from scratch recomputation .
1−pe
Overall running time of ( k,η) cores
We analyse now the overall time complexity of our ( k,η)cores algorithm . The initialisation phase ( Lines 1–6 ) is dominated by the computation of the initial η degree for all vertices , which takes O(m∆ ) time ( ∆ is the maximum η degree over all vertices ) . In the main cycle ( Lines 7– 18 ) , like the deterministic case , each vertex is visited only once and then removed from the graph . For each visited vertex v , the η degree of all its neighbours has to be updated . As reported above , for a single neighbour u , this takes O(η deg(u) ) . Thus , the main cycle globally takes O(Pv∈V Pu:(u,v)∈Nv
η deg(u ) ) = O(Pv∈V dv∆ ) =
O(m∆ ) . In conclusion , the running time of the ( k,η) cores algorithm is therefore O(m∆ ) .
4 . SPEEDING UP ( k,η) cores
In this section we show how to further speed up our ( k,η) cores algorithm . Our key observation is that the main bottleneck of ( k,η) cores is the computation of initial η degrees ( experimentally confirmed in Section 5 ) : although this step is asymptotically as fast as updating η degrees after a vertex removal , the latter is in practice faster as it is performed on a graph that gets progressively smaller . In this regard , we derive a fast to compute lower bound on the η degree and use it as a placeholder during the first iterations , while replacing it with the actual η degree only when the vertex at hand is going to be processed . This way , the initial η degrees can be computed only when actually needed and on a smaller graph , thus leading to the desired speed up . In the following we provide the details of our lower bound on the η degree and show how to efficiently update this bound after vertex removals . Then , we describe how to incorporate such findings into the enhanced algorithm .
A major feature of the lower bound η lb is its fast fromscratch computation . Here we show that it can also be updated very efficiently ( ie , in constant time ) when an edge is removed from the graph . To this end , we first need to report a couple of results . We start by showing that the η degree of a vertex v can decrease at most by one when an edge incident to v is removed ( Lemma 2 ) .
Lemma 2 . Given an uncertain graph G = ( V , E , p ) and a vertex v ∈ V , let e ∈ Nv be an edge incident to v and let H = ( V , E \ {e} , p ) be the subgraph of G where e is missing . Also , let η degH(v ) be the η degree of v in H . It holds that η degH(v ) > η deg(v ) − 2 .
Proof .
Pr[deg(v ) ≥ k ] =
Pr[degH(v ) = i ] + pePr[degH(v ) = k−1 ] = dv −1Xi=k
{z dv −1Xi=k−1 |
Pr[degH(v)≥k−1 ]
≤ Pr[degH(v ) ≥ k − 1 ] .
}
Pr[deg(v ) ≥ k ] ≥ Ipmin(v)(k , dv − k + 1 ) .
=
Pr[degH(v ) = i ]
−(1 − pe ) Pr[degH(v ) = k − 1 ] ≤
Lower bound on the η degree . We define our lower bound on the η degree in terms of the regularised beta function . Given a real number z ∈ [ 0 , 1 ] and two integers a and b , the regularized beta function Iz(a , b ) is defined as the ratio between the incomplete beta function B(z ; a , b ) and the beta function B(a , b ) [ 29 ] :
Iz(a , b ) =
B(z ; a , b ) B(a , b )
= a+b−1Xi=a a + b − 1 i
!zi(1 − z)a+b−1−i .
Given a vertex v in the input graph , let pmin(v ) denote the minimum probability on the edges incident to v , ie , pmin(v ) = mine∈Nv pe . The next lemma shows how the probability for v to have degree no less than k can be lowerbounded by using the regularised beta function I .
Lemma 1 . Given an uncertain graph G = ( V , E , p ) , for every vertex v ∈ V and for all k ∈ [ 0dv ] it holds that
Proof . Consider a vertex v′ having as many incident edges as v , and assume that each edge incident to v′ has probability pmin(v ) . It is easy to see that Pr[deg(v ) = i ] ≥ Pr[deg(v′ ) = i ] , for all i . Exploiting Equation ( 4 ) we get :
Pr[deg(v ) = i ] ≥ Pr[deg(v′ ) = i ] =
= XN⊆Nv′ , = dv pmin(v ) Ye∈Nv′ \N
|N|=i Ye∈N i!(pmin(v))i(1 − pmin(v))dv −i .
( 1 − pmin(v ) ) =
Combining such a result with Equation ( 3 ) we obtain : dvXi=k
Pr[deg(v ) ≥ k ] =
Pr[deg(v ) = i ] ≥
≥ dvXi=k dv i!(pmin(v))i(1−pmin(v))dv −i = Ipmin(v)(k , dv −k+1 ) .
The lemma follows .
The desired lower bound on η degree can now immediately be derived by exploiting Lemma 1 . We denote such a lower bound by η lb and formally state it in the next theorem .
Theorem 4 . Given an uncertain graph G = ( V , E , p ) , for every vertex v ∈ V it holds that
η deg(v ) ≥ η lb(v ) = max{k ∈ [ 0dv]|Ipmin(v)(k , dv−k+1 ) ≥ η} .
The computation of the above lower bound is very fast . For a fixed z , the values Iz(a , b ) of the regularised beta function are monotonically non increasing as a increases and/or b decreases . Therefore , the lower bounds on Pr[deg(v ) ≥ k ] are monotonically non increasing as k increases and one can thus perform binary search to derive the maximum k such that Ipmin(k , dv −k+1 ) ≥ η , which , according to Theorem 4 , corresponds to the lower bound η lb(v ) . The computation of η lb(v ) requires a logarithmic ( in the number of edges of v ) number of evaluations of Iz . Each evaluation of Iz can be computed in constant time using tables [ 23 ] . Thus , computing η lb(v ) for a vertex v takes O(log dv ) time .
By the definition of η degree we know that Pr[degH(v ) ≥ η degH(v ) + 1 ] < η ; thus , setting k = η degH(v ) + 2 in the above inequality , we get
Pr[deg(v ) ≥ η degH(v)+2 ] ≤ Pr[degH(v ) ≥ η degH(v)+1 ] < η .
Then , η deg(v ) < η degH(v ) + 2 , η degH(v ) > η deg(v ) − 2 . The lemma follows . or , equivalently ,
Based on the above lemma , we can also prove that the lower bound η lb(v ) of a vertex v can decrease at most by one when an edge incident to v is removed .
Theorem 5 . Given an uncertain graph G = ( V , E , p ) and a vertex v ∈ V , let e ∈ Nv be an edge incident to v and let H = ( V , E \ {e} , p ) be the subgraph of G where e is missing . Also , let η lbH(v ) be the lower bound on the η degree of v in H . It holds that η lbH(v ) > η lb(v ) − 2 .
Proof . Consider a vertex v′ having as many incident edges as v , and assume that each edge incident to v′ has probability pmin(v ) . It is easy to see that the η degree η deg(v′ ) of v′ equals the lower bound η lb(v ) . Combining this with Lemma 1 , we get η lbH(v ) = η degH(v′ ) > η deg(v′ ) − 2 = η lb(v ) − 2 . The theorem follows .
Theorem 5 can be exploited for safely updating η lb in constant time . Let e denote again the edge incident to v to be removed and let H be the subgraph of G where e is missing . Thus , η lb(v ) denotes the earlier lower bound of v , while η lbH(v ) denotes the new lower bound to be computed after the e ’s removal . The idea is to compute ( in constant time ) just the value Ipmin(v)(η lb(v ) , ( dv−1)−η lb(v)+1 ) = Ipmin(v)(η lb(v ) , dv −η lb(v) ) . Lemma 1 ensures that
Pr[degH(v ) ≥ η lb(v ) ] ≥ Ipmin(v)(η lb(v ) , dv − η lb(v) ) .
Thus , if Ipmin(v)(η lb(v ) , dv − η lb(v ) ) is still ≥ η , then the lower bound has not changed , ie , η lbH(v ) = η lb(v ) . Otherwise , it means that the lower bound has decreased . According to Theorem 5 , this decreasing can be at most by one , hence we can safely set η lbH(v ) = η lb(v ) − 1 .
A major shortcoming of updating η lb as described above is that , for each vertex v , we need to load/keep in memory O(d2 v ) values of Iz ( ie , all values within {Ipmin(v)(k , h − k + 1 ) | h ∈ [ 0dv ] , k ∈ [ 0h]} ) This would penalize too much both time and space complexity of the algorithm . However , this can be overcome by still relying on Theorem 5 . The idea is to simply set η lbH(v ) = max{0 , η lb(v ) − 1} every time an edge e incident to v is removed , no matter whether Ipmin(v)(η lb(v ) , dv − η lb(v ) ) ≥ η or not . Indeed , Theorem 5 guarantees that η lb(v ) − 1 is still a lower bound for η deg(v ) , even though possibly less tight . This way our algorithm would require only O(dv ) values of Iz for each vertex v , ie , just the values {Ipmin(v)(k , dv − k + 1 ) | k ∈ [ 0dv]} The E (k,η) cores algorithm . We now provide the details of our enhanced (k,η) cores ( for short , E (k,η) cores ) algorithm ( pseudocode omitted for space reasons ) . The algorithm follows the scheme of the basic ( k,η) cores algorithm ( Algorithm 2 ) . The main difference is that , for each vertex v , the lower bound η lb(v ) is computed in the initialisation the vertices for which the exact η degree has not been com phase , rather than the exact η degree . A set eV keeps trace of puted yet . Right after initialisation , eV corresponds to the whole vertex set V . In the main cycle , vertices are processed based on their ( lower bound on ) η degree . When a vertex v is being processed , it is primarily checked whether its exact η degree is already available . If not , the exact η degree of v is computed and v is moved to the proper set of the vector D , so that it can be processed in the correct ( possibly later ) iteration . Otherwise , if the exact η degree of v is available , the η core number of v is set and the η degrees ( either the exact or the lower bounds ) of all v ’s neighbours are updated . The worst case time complexity of E (k,η) cores is the same as the basic ( k,η) cores algorithm , ie , O(m∆ ) . However , smaller running times are expected in practice due to the lazy computation/updating of η degrees in reduced versions of the input graph .
5 . EXPERIMENTS
In this section we report quantitative experiments on efficiency and numerical stability of our ( k,η) cores and E(k,η) cores algorithms ( Sections 3 and 4).3 For this task we use the following real world uncertain graphs .
Flickr ( wwwflickrcom , |V | = 24 125 , |E| = 300 836 ) . We borrowed the dataset from [ 24 ] , where the probability of an edge between two users is defined based on homophily , the principle that similar interests indicate social ties . Particularly , [ 24 ] uses as a measure of homophily the Jaccard coefficient of the interest groups shared by the two users .
|V | = DBLP ( wwwinformatikuni trierde/~ley/db/ , 684 911 , |E| = 2 284 991 ) . The dataset was borrowed from [ 24 , 15 ] . Two authors are connected if they co authored at least once , and the probability on an edge expresses the fact that the collaboration has not happened by chance : the more the collaborations , the larger the probability . Precisely , [ 24 , 15 ] define the probability of each edge based on an exponential function to the number of collaborations .
BioMine ( biomine.org , |V | = 1 008 200 , |E| = 6 742 939 ) . A snapshot of the database of the BioMine project [ 26 ] containing biological interactions . Edges inherently come with
3
We implemented our code in Java and run experiments on a
2.83GHz , 32GB Intel Xeon server .
Table 1 : Times ( secs ) of the proposed methods for computing ( k,η) core decomposition ( precision 64 bits ) . The column “ gain ( % ) ” reports the gain of the E (k,η) cores algorithm over the ( k,η) cores algorithm . initial main initial main η η degrees cycle total η degrees cycle total gain ( % )
Flickr , ( k,η) cores 15.45 13.73 12.56 11.45 9.86
8.88 24.33 7.89 21.61 7.33 19.89 6.64 18.09 5.72 15.58
DBLP , ( k,η) cores 53.81 36.92 90.73 33.16 82.24 49.08 31.14 75.88 44.74 28.40 69.05 40.65 35.54 24.42 59.96
BioMine , ( k,η) cores 4801 1549 6350 1542 6246 4704 1538 6183 4645 1523 6091 4568 4498 1478 5977
0.1 0.3 0.5 0.7 0.9
0.1 0.3 0.5 0.7 0.9
0.1 0.3 0.5 0.7 0.9
Flickr , E (k,η) cores 22.39 14.41 12.90 20.12 18.57 11.86 16.96 10.82 9.34 14.66
7.98 7.22 6.71 6.14 5.32
DBLP , E (k,η) cores 38.23 26.45 64.68 25.21 61.48 36.28 24.45 58.43 33.98 23.07 54.92 31.86 28.40 21.06 49.46
BioMine , E (k,η) cores 4388 4333 4281 4240 4151
1404 1447 1404 1403 1423
5792 5780 5685 5643 5575
7.99 % 6.89 % 6.62 % 6.25 % 5.87 %
28.71 % 25.24 % 23.00 % 20.46 % 17.51 %
8.78 % 7.46 % 8.05 % 7.35 % 6.72 % probabilities . The probability of any edge provides evidence that the interaction actually exists .
Efficiency . Table 1 reports on the running times exhibited by our ( k,η) cores ( left ) and E (k,η) cores ( right ) algorithms on the selected datasets . Times are split by the main phases of computing initial η degrees and running the main cycle . Both algorithms are very fast on Flickr and DBLP . They take on average around 20 and 60 seconds , respectively . On BioMine , which is much larger and denser , clearly the time increases . However , the time required by our algorithms on the latter dataset is in the order of one hour . This is reasonable for networks of such size and testifies the applicability of our methods to very large uncertain graphs . As expected , E (k,η) cores runs faster than the basic ( k,η) cores algorithm , allowing a reduction of the total time up to around 30 % ( DBLP , η = 01 ) The gain is more evident on the larger datasets ( ie , DBLP and BioMine ) and is generally increasing as η decreases . The latter finding is expected because the smaller η , the larger the η degree of a vertex , and , thus , the better the chance for the lower bound to be tighter and lead to better pruning . Larger η degrees for smaller η is also the reason why times ( for both phases and both algorithms ) are increasing with smaller η .
Numerical stability . As discussed in Section 3 , probabilities may lead to numerical instability . To prevent this , one can exploit native solutions provided by modern programming languages to enlarge range and/or precision of the numerical representation . As a side effect , this would slow down the overall computation as larger precision implies slower arithmetic computations . Thus , the goal is to minimise the number of critical operations that may lead to numerical instability , to avoid using a too large precision with the aim of achieving reasonable accuracy . As reported in Section 3 , a major feature of the novel dynamicprogramming method we employ in our algorithms to compute/update η degrees is to alleviate such numerical issues . We next provide experimental evidence on this .
First , we report results by varying the precision used for representing numbers ( we consider 32 , 64 , 128 , and 256 bits
Table 2 : Accuracy of ( k,η) core index for η = 0 wrt deterministic core index ( ground truth ) for different values of precision ( bits ) .
Table 3 : Times ( secs ) of the two proposed methods for computing ( k,η) core decomposition , for η = 0.1 , for different values of precision ( bits ) . dataset pr=32 pr=64 pr=128 pr=256 avg absolute error
Flickr
DBLP
BioMine
6.17 0.27 2.18
5.12 0.1 1.25
3.4 0.03 0.41
2.26 0.01 0.14
% vertices with non zero error
Flickr
DBLP
BioMine
31.69 % 18.91 % 11.92 % 0.51 % 17.48 % 2.27 % 1.51 % 1.11 % 0.47 %
6.00 % 0.18 % 0.09 % as precision levels).4 We note that , for η = 0 , the ( k,η) core decomposition of an uncertain graph G should ideally correspond to the core decomposition of the deterministic graph derived from G by ignoring probabilities . Thus , we measure accuracy by comparing , for each vertex , the 0 core number outputted by our algorithms with the core number returned by the standard k core algorithm ( Algorithm 1 ) on such a deterministic graph .
Tables 2 and 3 show accuracy results ( in terms of pervertex average absolute error and percentage of vertices with core number other than the exact one ) and running times , respectively . We report times separately for ( k,η) cores and E (k,η) cores , while accuracy is the same for both . As expected , larger precision leads to better accuracy and worse efficiency . Particularly , the results show a linear trend : doubling the precision , time doubles while errors get halved . We also compare the results of our algorithms when equipped with the proposed dynamic programming method to the results of our algorithms equipped with the method that computes/updates η degrees using the formula in Equation ( 5 ) . We denote our proposed combination “ ( k,η) cores + dynamic programming method ” simply as ( k,η) cores , while we refer to the “ baseline ” combination “ ( k,η) cores + Equation ( 5) based method ” as Eq5 . These results are summarised in Table 4 ( precision 64 bits ) . Our method outperforms Eq5 in terms of both average absolute error and percentage of vertices with non zero error . Particularly , the average absolute error of the Eq5 method is reduced by 9 % ( Flickr ) , 41 % ( DBLP ) , and 40 % ( BioMine ) .
6 .
INFLUENCE MAXIMIZATION
The influence maximization problem [ 16 ] , has received a great deal of attention over the last decade . It requires to find a set of vertices S , with |S| = s , that maximizes the expected spread , ie , the expected number of vertices that would be infected by a viral propagation started in S , under a certain probabilistic propagation model .
The independent cascade model [ 16 ] is a widely used propagation model ; under this model , the problem of finding a set S of s vertices that maximizes the expected spread σ(S ) is NP hard . However , the submodularity of σ(S ) allows the Greedy algorithm that iteratively adds to S the vertex bringing the largest marginal gain to the objective function to achieve a ( 1 − 1 e ) approximation guarantee . Unfortunately , finding the maximum marginal gain vertex requires to solve a #P complete reliability problem . Hence , existing approaches usually apply sampling methods ( eg , Monte Carlo ) to estimate the best seed vertex at each iteration of the algorithm . This drastically affects the effi
4
In our implementation , we use the BigDecimal Java API , which allows for representing numbers arbitrarily large and/or small , and with arbitrary user defined precision ( up to “ unlimited ” precision ) . prec . ( bits ) η degrees cycle total initial main initial main η degrees cycle total gain ( % )
Flickr , ( k,η) cores 6.96 15.23 25.55 34.35
3.83 10.79 8.89 24.12 14.48 40.03 22.13 56.48
DBLP , ( k,η) cores 20.22 46.93 26.71 39.19 95.92 56.73 59.81 146.5 86.65 128.7 89.14 217.8
Flickr , E (k,η) cores 10.36 6.63 3.73 7.94 14.08 22.02 12.92 36.62 23.69 31.95 19.68 51.63
DBLP , E (k,η) cores 15.51 34.97 19.46 27.17 68.14 40.98 40.40 103.2 62.84 91.15 59.30 150.5
BioMine , ( k,η) cores 704 2 376 3 080 1 693 7 145 5 452 3 146 12 961 9 815 13 296 5 055 18 351
BioMine , E (k,η) cores 2 021 4 738 8 153 11 274
659 2 681 1 390 6 128 2 607 10 760 4 515 15 789
3.94 % 8.72 % 8.53 % 8.59 %
25.48 % 28.96 % 29.51 % 30.93 %
12.97 % 14.24 % 16.98 % 13.96 %
32 64 128 256
32 64 128 256
32 64 128 256
Table 4 : Accuracy of the proposed method in terms of error wrt a ground truth ( precision 64 bits ) . dataset Flickr
DBLP
BioMine avg absolute error Eq5 ( k,η) cores 5.62 0.17 2.07
5.12 0.1 1.25 vertices w . non zero error ( k,η) cores
Eq5
18.91 % 2.27 % 1.11 %
19.91 % 4.42 % 1.36 % ciency of the algorithm , thus limiting its applicability only to moderately sized networks ( the time complexity of the algorithm is O(sT nm ) , where T is the number of Monte Carlo samples , with T ∈ [ 1 000 , 10 000 ] , usually ) . Optimizations of the basic algorithm have been defined which exploit the submodularity of σ to avoid unneeded computations [ 12 ] , but the improvement achieved is typically not enough to handle large graphs ( in the experiment that we show below , on a moderately sized graph a state of the art algorithm such as Celf++ [ 12 ] could not finish after several weeks ) .
Within this view , a useful application of our ( k , η) core decomposition is to provide a way to speed up the execution of the Greedy algorithm . The idea is simple : just reduce the input graph G by keeping only the inner most η shells and run the ( optimized version of the ) Greedy algorithm on such a reduced graph . The rationale here is that , as experimentally observed in [ 17 ] , the core decomposition of the deterministic version of G , is a direct indicator of the expected spread of a vertex : the higher the core index is , the more likely the vertex is an influential spreader . The finding in [ 17 ] however exploits cores derived from a deterministic version of the input graph , thus completely ignoring its probabilistic nature . We conjecture that exploiting a notion of core decomposition defined ad hoc for uncertain graphs can only positively affect the behaviour observed in [ 17 ] . We next empirically show the correctness of our conjecture .
Experiments . We use a small directed graph from Twitter ( |V | = 21 882 , |E| = 372 005 ) , and a set of propagations of URLs in the social graph , which we use as past evidence to learn the influence probabilities ( we employ the traditional method described in [ 11 ] for this ) . Each edge ( u , v ) expresses the fact that v is a follower of u and the corresponding probability provides evidence that an action performed by u will be performed by v as well .
The objective here is to show that running the standard Greedy influence maximization algorithm on a reduced version of the graph given by the inner most ( k , η) shells allows to achieve high quality results while keeping the running time small . We test our method replacing the notion of de
Table 5 : Expected spread achieved by the proposed ( k,η) cores based method vs . some baselines with varying the output set size |S| .
|S| = 10
|S| = 20
|S| = 30
( k,η) cores out degree η degree exp degree k cores
9 570 9 014 9 019 9 012 9 134
9 606 9 016 9 089 9 093 9 192
9 610 9 130 9 125 9 123 9 223 gree with out degree ( given that the graph is directed ) and setting η = 05 We obtain 8 cores and keep the three innermost ( k , η) shells . This gives a reduced graph with 2 064 vertices and 86 142 edges . We run the optimized version of the Greedy algorithm defined in [ 12 ] , ie , the Celf++ algorithm , on such a reduced graph and take the seed vertices S outputted as our result .
For accuracy evaluation , we compute the expected spread achieved by S on the whole graph ( using Monte Carlo sampling with 10 000 samples ) . As criteria for comparison , we use the top K vertices ranked according to the following baseline ranking functions : ( i ) maximum out degree ( ignoring probabilities , as suggested in the seminal work on influence maximization [ 16] ) , ( ii ) maximum η degree , ( iii ) maximum expected degree ( computed by summing the probabilities on the edges outgoing from a vertex ) , and ( iv ) vertices computed by running Celf++ on the graph reduced according to deterministic core decomposition ( ignoring probabilities ) . Note that we could not use the results of the direct execution of Celf++ on the whole graph due to its excessive running time ( it could not finish in several weeks ) . The results reported in Table 5 ( we vary |S| from 10 to 30 ) show how our ( k,η) cores based method evidently outperforms all the baselines , allowing to increase the spread up to 590 ( out degree ) , 551 ( η degree ) , 558 ( exp degree ) , and 436 ( k cores ) . As far as efficiency , we report runtimes in the order of 4–5 hours ( with |S| = 30 ) , which are times clearly affordable—contrast to the unaffordable runtime of the direct execution of Celf++ on the whole graph .
7 . TASK DRIVEN TEAM FORMATION
In task driven team formation we are given a collaboration graph G = ( V , E , τ ) , where vertices are individuals and edges are assigned a probabilistic topic model τ , representing ( a distribution on ) the topics exhibited by past collaborations . The topic model can be produced by standard methods , such as the popular Latent Dirichlet Allocation ( LDA ) [ 6 ] . The input of LDA ( or any other similar method ) is ( i ) a number Z of topics , and ( ii ) for each edge ( u , v ) ∈ E , a document d(u , v ) representing all the past collaborations between u and v . The document d(u , v ) is a bag of terms coming from a finite vocabulary Σ . The output is the topic model τ , that is : the probability pz z=1 pz
• for each edge ( u , v ) ∈ E and each topic z ∈ [ 1Z ] , u,v = ( z|u , v ) that the collaborations u,v = 1 . • for each term t ∈ Σ , a distribution over topics , ie , for each topic z ∈ [ 1 , Z ] , the probability γz t = P ( t|z ) that the term t has been generated by the topic z , with between u and v are on the topic z , withPZ Pt∈Σ γz
A task driven team formation query is a pair hT , Qi , where T ⊂ Σ is a set of terms describing a task , and Q ⊂ V is a set of vertices ( possibly even a single vertex ) . The goal t = 1 . is to find an answer vertex set A , with Q ⊆ A , which is a good team to perform the task described by the terms in T . Being a good team means having a good affinity among the team members with respect to the given task . We report more formal details on this in the following .
The query task T , together with the topic model τ , induce a single probability value pT ( u , v ) for each edge ( u , v ) ∈ E , such that pT ( u , v ) represents the likelihood that T has been generated by a collaboration between u and v : pT ( u , v ) = p(u , v|T ) =Yt∈T
ZXz=1
γz t pz u,v .
( 8 )
Hence , given a task T , the input collaboration graph G yields an uncertain graph GT = ( V , E , pT ) . This way , given GT and a set of query vertices Q ⊆ V , the task of finding a good team for the query at hand directly translates into finding a subgraph of GT that represents a good community for Q . Formally , the goal is to find a connected subgraph H = ( VH , EH ) of GT that ( i ) contains all query vertices ( Q ⊆ VH ) , and ( ii ) maximizes a notion of density . Particularly , as far as the density measure , the minimum degree has been widely recognized as a principled choice for this kind of problem.5 We therefore rely on this notion of density and ask for the subgraph H to maximize the minimum η degree of a vertex in H . The resulting problem statement is :
Problem 2
( Task Driven Team Formation ) .
Given a collaboration graph G = ( V , E , τ ) and a query hT , Qi , let GT be the uncertain graph derived from G and T as described in Equation ( 8 ) . Given a threshold η ∈ [ 0 , 1 ] , we want to find a connected subgraph H = ( VH , EH ) of GT induced by a set of vertices VH such that
VH = arg max Q⊆S⊆V min u∈S
η deg(u ) .
Exploiting ( k,η) cores for team formation . We now show that Problem 2 can be optimally solved by resorting to our notion of ( k,η) core decomposition . This result is stated in the next theorem ( we omit the proof for space reasons ) .
Theorem 6 . Given an uncertain graph GT and a threshold η ∈ [ 0 , 1 ] , let C = {C0 , C1 , . . . , Ck∗} be the ( k,η) core decomposition of GT ( with C0 ⊇ C1 ⊇ · · · ⊇ Ck∗ ) , and , given a set of query vertices Q ⊆ V , let C ∗ Q be the smallestsized core in C such that every q ∈ Q belongs to the same connected component of C ∗ Q .
Then , the solution to Problem 2 is given by the connected component of C ∗
Q that contains Q .
Theorem 6 provides us with a principled way of solving Problem 2 . The solution can be summarized as follows :
1 . Given a collaboration graph G = ( V , E , τ ) and a taskdriven team formation query hT , Qi , derive the uncertain graph GT = ( V , E , pT ) ( Equation ( 8)).6
2 . Compute the ( k,η) core decomposition C of GT ;
5
As argued in [ 27 ] , maximizing the minimum degree provides a better evidence of the goodness of a community than , eg , the maximization of the average degree , which is instead more suitable for dense subgraph discovery . 6
As Equation ( 8 ) can produce very small probabilities , in our implementation we prune GT by removing edges with probability smaller than a threshold ǫ ( ǫ = 10−16 in our experiments ) .
Table 6 : Three examples of task driven team formation queries and corresponding results .
T = {gene , express} , Q = {HVJagadish}
T = {xml , tree} ,
Q = {HVJagadish , S.M uthukrishnan}
T = {auction , model} ,
Q = {S.M uthukrishnan}
Brian D . Athey , Giovanni Scardoni ,
Kathleen A . Stringer , Venkateshwar G . Keshamouni , Jing Gao , Terry E . Weymouth , Vasudeva Mahavisno ,
Charles F . Burant , Christopher W . Beecher ,
Maureen A . Sartor , Alla Karnovsky , Rork Kuick ,
Zach Wright , James D . Cavalcoli , Gilbert S . Omenn ,
H . V . Jagadish , Carlo Laudanna , Tim Hull ,
Barbara R . Mirel , V . Glenn Tarcea
S . Muthukrishnan ,
Panagiotis G . Ipeirotis ,
Lauri Pietarinen H . V . Jagadish , Divesh Srivastava ,
Nick Koudas
Uri Nadav , Noam Nisan , Jon Feldman , Vahab S . Mirrokni , Gagan Aggarwal , Tanmoy Chakraborty , Aranyak Mehta Evdokia Nikolova , S . Muthukrishnan , Martin Pal , Clifford Stein , Eyal Even Dar
Florin Constantin , Yishay Mansour
3 . Visit the cores in C starting from the smallest sized one ( ie , the inner most core ) , until finding C ∗ Q ;
4 . Return the connected component of C ∗
Q containing Q as the solution to Problem 2 .
Experiments . We consider task driven team formation in the context of collaborations among computer science researchers . We build a collaboration network from the DBLP database ( wwwinformatikuni trierde/~ley/db/ ) : vertices are authors and an edge connects two authors if they co authored at least once . The resulting graph has |V | = 1 089 442 and |E| = 4 144 697 . For each edge , we take the bag of words of the titles of all papers coauthored by the two authors ( words are stemmed and stop words are removed ) , and apply LDA to infer the topic model τ ( we set Z = 100 ) . In Table 6 we report the results of three task driven teamformation queries . The first two queries share the query vertex H . V . Jagadish , but the first task is about gene expression while the second one is about xml : as expected the two proposed teams are very different . The third query shares with the second one the vertex S . Muthukrishnan ; but , unlike the previous one that is about xml ( a database topic ) , the third query is about auction models ( an algorithmtheory topic ) : the different teams proposed correctly reflect the difference in the tasks . It is worth noticing that the extraction of these teams , following the process described above and exploiting our efficient ( k,η) core decomposition , takes approximately 2 3 seconds on a commodity laptop .
8 . CONCLUSIONS
In this paper we extend the graph tool of core decomposition to the context of uncertain graphs . We define the ( k , η) core concept , and we devise efficient algorithms for computing a ( k , η) core decomposition . As a future work , we plan to investigate the relationship between ( k , η) cores and other definitions of ( probabilistic ) dense subgraphs , so as to exploit the former as a speeding up preprocessing .
9 . REFERENCES
[ 1 ] E . Adar and C . Re . Managing Uncertainty in Social Networks .
IEEE Data Eng . Bull . , 30(2):15–22 , 2007 .
[ 2 ] J . I . Alvarez Hamelin , L . Dall’Asta , A . Barrat , and
A . Vespignani . Large scale networks fingerprinting and visualization using the k core decomposition . In NIPS , 2005 .
[ 3 ] R . Andersen and K . Chellapilla . Finding dense subgraphs with size bounds . In WAW , 2009 .
[ 4 ] S . Asthana , O . D . King , F . D . Gibbons , and F . P . Roth .
Predicting Protein Complex Membership using Probabilistic Network Reliability . Genome Res . , 14:1170–1175 , 2004 .
[ 5 ] V . Batagelj and M . Zaverˇsnik . Fast algorithms for determining ( generalized ) core groups in social networks . Advances in Data Analysis and Classification , 5(2):129–145 , 2011 .
[ 6 ] D . M . Blei , A . Y . Ng , and M . I . Jordan . Latent dirichlet allocation . JMLR , 3:993–1022 , 2003 .
[ 7 ] P . Boldi , F . Bonchi , A . Gionis , and T . Tassa . Injecting
Uncertainty in Graphs for Identity Obfuscation . PVLDB , 5(11):1376–1387 , 2012 .
[ 8 ] X . H . Chen , A . P . Dempster , and J . S . Liu . Weighted finite population sampling to maximize entropy . Biometrika , 81:457–469 , 1994 .
[ 9 ] N . Dalvi and D . Suciu . Efficient query evaluation on probabilistic databases . In VLDB , pages 864–875 , 2004 .
[ 10 ] D . Eppstein , M . L¨offler , and D . Strash . Listing all maximal cliques in sparse graphs in near optimal time . In ISAAC , 2010 .
[ 11 ] A . Goyal , F . Bonchi , and L . V . Lakshmanan . Learning influence probabilities in social networks . In WSDM , 2010 .
[ 12 ] A . Goyal , W . Lu , and L . V . Lakshmanan . Celf++ : optimizing the greedy algorithm for influence maximization in social networks . In WWW , pages 47–48 , 2011 .
[ 13 ] J . Healy , J . Janssen , E . E . Milios , and W . Aiello .
Characterization of graphs using degree cores . In WAW , 2006 .
[ 14 ] R . Jin , L . Liu , and C . C . Aggarwal . Discovering Highly
Reliable Subgraphs in Uncertain Graphs . In KDD , 2011 .
[ 15 ] R . Jin , L . Liu , B . Ding , and H . Wang . Distance Constraint
Reachability Computation in Uncertain Graphs . PVLDB , 4(9):551–562 , 2011 .
[ 16 ] D . Kempe , J . Kleinberg , and E . Tardos . Maximizing the spread of influence through a social network . In KDD , 2003 .
[ 17 ] M . Kitsak , L . K . Gallos , S . Havlin , F . Liljeros , L . Muchnik ,
H . E . Stanley , and H . A . Makse . Identifying influential spreaders in complex networks . Nature Physics 6 , 888 , 2010 .
[ 18 ] G . Kollios , M . Potamias , and E . Terzi . Clustering large probabilistic graphs . TKDE , 25(2):325–336 , 2013 .
[ 19 ] G . Kortsarz and D . Peleg . Generating sparse 2 spanners . J .
Algorithms , 17(2):222–236 , 1994 .
[ 20 ] D . L. Nowell and J . Kleinberg . The Link Prediction Problem for Social Networks . In CIKM , 2003 .
[ 21 ] V . E . Lee , N . Ruan , R . Jin , and C . C . Aggarwal . A survey of algorithms for dense subgraph discovery . In Managing and Mining Graph Data . 2010 .
[ 22 ] L . Liu , R . Jin , C . Aggrawal , and Y . Shen . Reliable clustering on uncertain graphs . In ICDM , 2012 .
[ 23 ] K . Pearson . Tables of the Incomplete Beta Function .
Cambridge University Press , 1968 .
[ 24 ] M . Potamias , F . Bonchi , A . Gionis , and G . Kollios . k Nearest
Neighbors in Uncertain Graphs . PVLDB , 3(1):997–1008 , 2010 . [ 25 ] S . B . Seidman . Network structure and minimum degree . Social
Networks , 5(3):269–287 , 1983 .
[ 26 ] P . Sevon , L . Eronen , P . Hintsanen , K . Kulovesi , and
H . Toivonen . Link Discovery in Graphs Derived from Biological Databases . In DILS , 2006 .
[ 27 ] M . Sozio and A . Gionis . The community search problem and how to plan a successful cocktail party . In KDD , 2010 .
[ 28 ] L . G . Valiant . The Complexity of Enumeration and Reliability
Problems . SIAM J . on Computing , 8(3):410–421 , 1979 .
[ 29 ] E . W . Weisstein . Binomial distribution . From MathWorld—A
Wolfram Web Resource . Last visited on 16/5/2013 , http://mathworldwolframcom/BinomialDistributionhtml
[ 30 ] S . Wuchty and E . Almaas . Peeling the yeast protein network .
Proteomics , 5(2):444–449 , Feb . 2005 .
[ 31 ] Y . Yuan , G . Wang , L . Chen , and H . Wang . Efficient Subgraph
Similarity Search on Large Probabilistic Graph Databases . PVLDB , 5(9):800–811 , 2012 .
[ 32 ] H . Zhang , H . Zhao , W . Cai , J . Liu , and W . Zhou . Using the k core decomposition to analyze the static structure of large scale software systems . The Journal of Supercomputing , 53(2):352–369 , 2010 .
[ 33 ] L . Zou , P . Peng , and D . Zhao . Top K Possible Shortest Path
Query over a Large Uncertain Graph . In WISE , 2011 .
[ 34 ] Z . Zou , H . Gao , and J . Li . Discovering Frequent Subgraphs over Uncertain Graph Databases under Probabilistic Semantics . In KDD , 2010 .
