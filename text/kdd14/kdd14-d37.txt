Effective Global Approaches for Mutual Information Based
Feature Selection
Nguyen Xuan Vinh
Jeffrey Chan
Simone Romano
James Bailey
Department of Computing and Information Systems , The University of Melbourne , Australia
{ vinh.nguyen | jeffrey.chan | simone.romano | baileyj }@unimelbeduau
ABSTRACT Most current mutual information ( MI ) based feature selection techniques are greedy in nature thus are prone to sub optimal decisions . Potential performance improvements could be gained by systematically posing MI based feature selection as a global optimization problem . A rare attempt at providing a global solution for the MI based feature selection is the recently proposed Quadratic Programming Feature Selection ( QPFS ) approach . We point out that the QPFS formulation faces several non trivial issues , in particular , how to properly treat feature ‘self redundancy’ while ensuring the convexity of the objective function . In this paper , we take a systematic approach to the problem of global MI based feature selection . We show how the resulting NPhard global optimization problem could be efficiently approximately solved via spectral relaxation and semi definite programming techniques . We experimentally demonstrate the efficiency and effectiveness of these novel feature selection frameworks .
Categories and Subject Descriptors I52 [ Pattern Recognition ] : Feature evaluation and selection
Keywords Feature selection ; mutual information ; spectral relaxation ; semi definite programming ; global optimization .
1 .
INTRODUCTION
Mutual information ( MI ) based approaches are an important feature selection paradigm in data mining . Over the years , these methods have gained increasing popularity , thanks especially to their ease of use , effectiveness and strong theoretical foundation rooted in information theory . Seventeen MI based feature selection approaches are listed in a recent comprehensive survey [ 3 ] , summarizing nearly two decades of research in this area . The commonality of these methods lies in the fact that they all employ a greedy scheme to incrementally build the selected feature set , one at a time .
To gain some concreteness to our discussion , let us revisit a very popular mutual information based feature selection family that is centred around the concepts of redundancy and relevancy . A particularly successful and well known instance of this family is the Minimum Redundancy Maximum Relevance ( MRMR ) framework [ 20 ] . Given a set of n features ( which are often referred interchangeably to as variables , or attributes ) X = {X1 , . . . , Xn} and a target class variable C , the relevancy of Xi is measured by its mutual information ( MI ) with the class variable , ie ,
Rel ( Xi ) I(Xi ; C )
P ( Xi , C ) log
P ( Xi , C )
P ( Xi)P ( C )
Xi,C while its redundancy with respect to an already selected feature subset S is defined as Red ( Xi|S ) 1 |S|
I(Xi ; Xj )
Xj∈S
Given these definitions of feature relevancy and redundancy , the MRMR framework [ 20 ] is a greedy scheme to select features one at a time , such that the i th feature is selected maximizing the MRMR objective :
The generalized MRMR family is parameterized as max
Xi∈X\S{Rel(Xi ) − Red(Xi|S)}
I(Xi ; C ) − α
Xj∈S
MRMR : max Xi∈X\S

I(Xi ; Xj )
( 1 ) where α is a weighting factor that balances relevancy and redundancy , which is chosen to be 1/|S| in the case of MRMR . The first member of this family , known as MIFS ( Mutual Information Feature Selection ) [ 2 ] with α = 1 , has been in fact introduced much earlier in the feature selection literature . 1.1 Global MI based feature selection
Most current MI based feature selection approaches are of an incremental nature , similar to the MRMR formulation . As such , these methods are prone to suboptimal decisions , as selected features cannot be deselected at a later stage . Potential performance improvement could be gained by systematically posing MI based feature selection as a global optimization problem , and making a global decision considering the interaction between all features concurrently . The first attempt in this direction is the recently proposed
Quadratic Programming Feature Selection ( QPFS ) approach [ 21 ] . QPFS reformulates the MRMR feature selection problem as the following quadratic program :
QPFS : min x
αxT Hx − xT f st xi = 1 , xi ≥ 0
( 2 ) n i=1 where fn×1 = [ I(X1 ; C ) , . . . , I(Xn ; C)]T is the vector of feature relevancy , Hn×n = [ I(Xi ; Xj)]i,j=1n is the matrix of feature pairwise redundancy , and xn×1 represents relative feature weights . Note that Hii is set to feature selfredundancy , ie entropy , Hii = I(Xi ; Xi ) = H(Xi ) . The most attractive characteristic of the QPFS formulation in ( 2 ) is that if H is positive ( semi)definite , then QPFS is a convex quadratic program which can be solved efficiently in polynomial time for the globally optimal solution . The output x of this program is used for global feature ranking . 1.2 Theoretical issues with the QPFS frame work
The reformulation of the incremental MRMR as a global quadratic program QPFS as proposed in [ 21 ] , although being very attractive , poses several non trivial intriguing questions that we shall elaborate below . • Positive definiteness of H : A pre requisite for QPFS to be a convex quadratic program , thus admitting an efficient polynomial time procedure to find the global minimum , is that the Hessian matrix H of pairwise feature mutual information be positive ( semi)definite . In other words , the mutual information function on the space of features must be a proper kernel function . Our investigation into this problem shows that there is currently little understanding on whether the MI is a proper kernel . While we have not been able to theoretically prove nor disprove the positive definiteness of H , our practical evaluation of QPFS using Matlab quadratic program solver sometimes numerically encounters indefinite H ( for instance , the largest negative eigenvalue could be as large as −50 on a dataset of 2000 features ) , where Matlab solver declares the problem to be non convex and aborts the operation .
In the original paper [ 21 ] , this theoretical issue has been mostly neglected . For problems of a very large number of features , the authors proposed to approximate H using only its largest eigenvalues , so QPFS becomes convex . However , as there exist many small and medium size problems where an approximation might not be needed , establishing the positive definiteness of H is still required to ensure the theoretical soundness of the approach . • How to treat self redundancy ? In QPFS , note that the cost matrix H penalizes features for their redundancy with respect to other features . The ‘self redundancy’ terms Hii = I(Xi ; Xi ) = H(Xi ) , as designated in the original paper [ 21 ] , in fact penalize features for their intrinsic entropy . The question of how to treat self redundancy presents us with the following dilemmas :
– Arguably , features should not be penalized for self redundancy . Unfortunately , if we put Hii = 0 , then the Hessian matrix H becomes indefinite , violating the pre requisite for the QPFS formulation to be convex .
– If we put Hii = H(Xi ) as proposed in [ 21 ] , then there will be selection bias in favor of features with low entropy . In general , discrete features may have higher entropy because of more uniform distribution across its categories , or having more categories . As we will theoretically and empir ically show next in this paper , penalizing features for selfredundancy leads to undesired behaviors .
Example 1 : We use a simple example here to show the counter intuitive behavior of QPFS that penalizing features differently based on their entropy can lead to suboptimal decisions . Let us consider the following scenario , where a quaternary variable S ( Smoking ) takes 4 possible values ( (1 ) none smoker ; ( 2 ) 1 to 5 cigarettes per day ; ( 3 ) 5 to 15 cigarettes per day ; ( 4 ) more than 15 cigarettes per day ) . S causes the binary class variable C ( 0–none , 1–lung cancer ) with joint probability distribution P ( S , C ) . C then in turn causes a binary characteristic feature G ( Coughing , 0–occasionally , 1–frequently ) with joint probability P ( C , G ) . The scenario is denoted by the Bayesian network and joint probability tables in Figure 1 . The joint probability P ( S , G ) can also be calculated as in Fig 1 . In this example , S can be used to perfectly classify C ( using the rule C = 0 if S ∈ {1 , 2} and C = 1 otherwise ) , while if G were used the minimal error rate achievable , ie Bayes error rate , will be 5 % . Thus S–smoking should be clearly preferred over G–coughing as a predictive feature for C–lung cancer , albeit having a higher entropy , ie 2 bits vs . 1 bit . We can compute the following quantities :
I(G ; C ) = 0.7136 bit
I(S ; C ) = 1 bit I(S ; G ) = 0.7136 bit fl
The optimal solution to the QPFS formulation min xT
2
0.7136
1
0.7136 xi>0 , xi=1 ( 3 ) is x∗ = [ 0.42 , 0.58]T , that is , the coughing G ( weight 0.58 ) is ranked higher than smoking S ( weight 0.42 ) , which is incorrect . x − xT
0.7136
1
Figure 1 : The three variable example : QPFS gives preference to the feature with smaller entropy G , while S is a better explanatory variable for C albeit having a higher entropy .
1.3 Contribution
Motivated by the initial success as well as the theoretical gap within the QPFS framework , we set out to systematically investigate the problem of global MI based feature selection . Our first contribution in this paper is to reconsider the QPFS formulation and resolve the theoretical issues associated with its current form , as discussed above . Our second , and principal contribution , is to propose a novel formulation for global MI based feature selection that can be solved effectively via spectral relaxation and semi definite programming techniques . Via extensive experiments on a
CLungCancerSSmokingGCoughingP(S,C)C=0C=1S=10250S=20250S=30025S=40025P(C,G)C=0C=1G=004750025G=100250475P(S,G)G=0G=1S=10237500125S=20237500125S=30012502375S=400125023751 
Xi∈S wide range of data sets , we establish the effectiveness and efficiency of our approach against other successful MI based feature selection techniques . We further show that for large data , low rank approximation can be applied to gain computational advantage to our global algorithm over its greedy counterpart .
2 . PRELIMINARIES
It is worth noting that , in the original paper [ 21 ] , the authors propose the quadratic formulation ( 2 ) without much explanatory detail . While that formulation is intuitively reasonable , let us take a more systematic , step by step derivation process implied behind QPFS , through which the inconsistency within QPFS itself will also be exposed . From a global optimization perspective , the incremental MRMR feature selection problem ( 1 ) can be reconsidered as a global subset selection problem as follows :
Xi,Xj∈S i=j
SS : maxS⊂X |S|=k
I(Xi ; C ) − α
I(Xi ; Xj )
( 4 ) which can in turn be equivalently formulated as a quadratic integer programming problem as
QIP : max x xT f − αxT Hx st x ∈ {0 , 1}n , xi = k
( 5 ) Here , k is the desired size for the final feature set , fn×1 = [ I(X1 ; C ) , . . . , I(Xn ; C)]T is still the vector of feature relevancy , and Hn×n = [ I(Xi ; Xj)]i,j=1n is still the matrix of feature pairwise redundancy , except Hii = 0 , ie zerovalued ‘self redundancy’ terms . Note that this is also the critical difference between our global formulation and QPFS : clearly and naturally , there should be no penalty for feature self redundancy , as evidenced in the MRMR and SS formulations .
Unfortunately , there is no known efficient solution for both SS and QIP . SS is a hard combinatorial problem for which an exhaustive search will cost O(nk ) , ie exponential in the target set size , while similarly QIP is known to be an NPhard problem [ 5 ] . Noting that relaxing the problem to the continuous domain might lead to a more computationally tractable problem , we drop the integral 0–1 constraint , resulting in xT f − αxT Hx st xi = k , xi ≥ 0
( 6 )
 n i=1
With a change of variable yi = xi/k , we arrive at : kαyT Hy − yT f st yi = 1 , yi ≥ 0
( 7 )
Herein , kα plays the role of a dynamic balancing factor . The QPFS formulation ( 2 ) is essentially a simplified variant of ( 7 ) , where one disregards k and fixes the balancing factor to the same constant , ie α . 2.1 The extended MRMR family
Before providing a systematic analysis on how convexity could be ensured and ‘self redundancy’ should be treated in the QPFS framework , let us gain further insight into both max x min y n n i=1 i=1
EMRMR : max Xi
[ I(Xi ; Xj ) − I(Xi ; Xj|C ) ]
MRMR and QPFS by considering an extended MRMR family that incorporates second order dependancy . The material discussed in this section will also serve as building blocks for our new approach , presented in Section 3 . We start by elaborating the theoretical underpinnings behind MRMR and other similar heuristics . The ultimate goal of mutual information ( MI ) based feature selection is to select a subset of features S that shares the highest MI with C , ie maxS⊂X I(S ; C ) . As this is a hard combinatorial problem , a practical approach is to build the feature subset incrementally , so that the i th feature is selected as : arg max Xi∈X\S
I(Xi ; C|S )
I(S ∪ Xi ; C ) ≡ arg max Xi∈X\S
( 8 ) As the high dimensional MI term I(Xi ; C|S ) is still hard to estimate from limited samples , MRMR and many other MIbased heuristics approximate ( 8 ) using low order MI terms as :
I(Xi ; C|S ) I(Xi ; C ) − 1 |S|
I(Xi ; Xj )
( 9 )
Xj∈S
However , the following natural decomposition of I(Xi ; C|S ) ( 10 )
I(Xi ; C|S ) = I(Xi ; C ) − [ I(Xi ; S ) − I(Xi ; S|C ) ] suggests that redundancy is in fact composed of two parts : an unconditional redundancy term I(Xi ; S ) and a class conditional part I(Xi ; S|C ) [ 6 ] . This insight gives rise to the following extended minimal redundancy maximal relevance ( EMRMR ) objective :
Xj∈S
I(Xi ; C ) − α n x
This variant of MRMR has been introduced in the literature and observed to be more effective [ 3 , 13 , 14 ] . Similar to MRMR , EMRMR can be cast as an extended quadratic programming feature selection ( EQPFS ) problem as :
EQPFS : min
αxT [ H1 − H2]x − xT f st xi = 1 , xi ≥ 0
( 12 ) i=1
Here , H1 = [ I(Xi ; Xj)]n×n and H2 = [ I(Xi ; Xj|C)]n×n together make up the ‘total redundancy’ matrix H = H1−H2 . Similar to the QPFS formulation , we are presented with different choices about how to treat the total self redundancy terms , ie the diagonal elements of H1 and H2 .
If we set H1ii = H2ii = 0 , ie no penalty for selftotal redundancy , then H is indefinite , hence EQPFS is nonconvex and a global solution cannot be efficiently located .
If we set H1ii = I(Xi ; Xi ) = H(Xi ) as in the original QPFS formulation [ 21 ] , then analogously , H2ii should be set to I(Xi ; Xi|C ) = H(Xi|C ) . Thus Hii = H1ii − H2ii = H(Xi)− H(Xi|C ) = I(X ; C ) , ie features which share more information with C are penalized more , which is clearly counter intuitive and undesirable . In the next section , we provide a systematic analysis on how self redundancy in the QPFS and EQPFS frameworks should be treated . 2.2 How to properly treat self redundancy ?
We argue that the most proper approach for treating self redundancy is that , there should be no penalty for self redundancy , ie Hii = 0 , as clearly evident in the original
 ( 11 )
MRMR formulation . This choice however leads us to a nonconvex quadratic program . We shall point out here that assigning Hii = H(Xi ) as in [ 21 ] in fact provides a convex approximation to the originally non convex quadratic program . However , setting Hii = H(Xi ) leads to some counterintuitive observation about QPFS ( higher penalty for features with higher entropy ) and EQPFS ( higher penalty for features which share higher MI with C ) as we have pointed out .
We propose that QPFS and EQPFS could be convexified by setting the diagonal elements of H to the same value λ > 0 sufficiently large to ensure the positive ( semi)definiteness of the Hessian matrix . Formally , the general convexified EQPFS is as follows :
αxT [ H1 − βH2 + λI]x − xT f
( 13 ) x,n min i=1 xi=1,xi≥0 where I is the identity matrix , both α and β play the role of balancing factors , and λ is a convexification parameter . β is employed to balance the unconditional redundancy ( in H1 ) and the class conditional redundancy ( in H2 ) , as proposed in [ 14 ] . At β = 0 , EQPFS reduces to the original QPFS . With this approach , all features receive the same penalty for ‘self redundancy’ λ , although the real purpose of λ is to convexify the problem , not to impose a penalty on selfredundancy . It is noted that different choices of {α , β , λ} can lead to different solutions corresponding to different feature rankings .
3 . A NOVEL GLOBAL MI BASED FEATURE
SELECTION PARADIGM
In this section , we set out to design a novel , systematic global approach for MI based feature selection . Our desiderata for such an ideal global framework is two fold : ( i ) ability to handle second order feature dependancy as in EMRMR , ( ii ) strong theoretical foundation , with few or no ad hoc parameters , such as the balancing parameters and convexification parameter as in the ‘remedied’ QPFS framework ( 13 ) . Our first ingredient for such new framework is the following nice theoretical result , which states that the relevancy , unconditional redundancy and class conditional redundancy , can all be combined neatly into a single quantity , namely the conditional mutual information ( CMI ) .
Theorem 1 . We have :
I(Xi ; C|Xj ) = |S|I(Xi ; C )
Xj∈S
−
Xj∈S
( 14 )
[ I(Xi ; Xj ) − I(Xi ; Xj|C ) ]
Proof . The proof is straightforward using the following decomposition of the conditional MI :
I(Xi ; C|Xj ) = I(Xi ; C ) − I(Xi ; Xj ) + I(Xi ; Xj|C )
( 15 )
In fact , now we can see a chain of relationship between the high dimensional conditional relevancy term in ( 8 ) , the CMI and the extended MRMR criteria :
I(Xi ; C|S )
≡ |S|I(Xi ; C ) −
Xj∈S I(Xi ; C|Xj )
Xj∈S[I(Xi ; Xj ) + I(Xi ; Xj|C ) ]
( 16 )
In light of these connections , we propose a global subset selection problem based on the CMI as follows :
SSCMI : maxS⊂X |S|=k
I(Xi ; C ) +
I(Xi ; C|Xj )
Xi,Xj∈S
 ( 17 )
Xi∈S
 which can be equivalently reformulated in the form of a quadratic integer programming problem : st x ∈ {0 , 1}n,x =
√ k ( 18 )
QIPCMI : max xT Qx x i=1 xi = k ⇔ x = that for x ∈ {0 , 1}n , we have n where Qii = I(Xi ; C ) and Qij = I(Xi ; C|Xj ) , i = j . Note √ k . Here we use the norm constraint for set cardinality , as it results in more computationally tractable relaxations , as will be seen in the next sections . It is noted that Q is , in general , asymmetric . However , it could be replaced by the symmetric form ( Q + QT )/2 without changing the objective value . Thus hereafter , Q refers to the matrix with Qij = 2 {I(Xi ; C|Xj ) + I(Xj ; C|Xi)} , i = j and Qii = I(Xi ; C ) . 1
It can be seen that our Hessian matrix Q embodies both the notions of relevancy and total redundancy . With this novel formulation , we have resolved several issues associated with the self redundancy terms , as well as eliminating the need of introducing ( and thus , tuning ) the balancing factors α , β and the convexification parameter λ , as in the general EQPFS formulation .
We now present an interesting geometrical interpretation for the CMI criterion as follows . Besides relevancy , the global subset selection formulations SSCMI and QIPCMI favor features having large total pairwise conditional relevance . It is interesting to note that the quantity dC ( Xi , Xj ) = I(Xi ; C|Xj ) + I(Xj ; C|Xi ) could be regarded as a distance measure in the feature space . Sotoca and Pla [ 22 ] further claimed that this distance measure , named the conditional mutual information distance , is a proper metric , that is , it satisfies the triangle inequality1 . The interpretation of dC ( Xi , Xj ) as a distance measure brings about an interesting insight on SSCMI , which can be rewritten as max S⊂X . This |S|=k
Xi∈S I(Xi ; C ) + 1
Xi,Xj∈S dC ( Xi , Xj )
2 criterion selects k features such that their total relevance and total pairwise distance is maximized . In other words , the criterion aims to choose a set of highly relevant representative features that also provide good coverage over the feature space , ie far apart from each other in CMI distance . As QIPCMI is NP hard , in the next sections we investigate efficient approximation techniques for solving this problem .
3.1 Global MI based feature selection via spec tral relaxation
We propose an efficient yet simple spectral relaxation technique for solving QIPCMI . We shall relax QIPCMI to the continuous domain , by dropping the integral 0–1 constraints which in fact cause NP hardness , while keeping only the
1We recently pointed out that Sotoca and Pla ’s proof is flawed , and that the triangle inequality holds true under the Na¨ıve Bayes assumption , ie all the features are independent given the class variable [ 24 ] . Whether the CMI distance is a proper metric in general is still an open problem . norm constraint , resulting in
√ k , xi ≥ 0 x x max
: max xT Qx xT Qx
( cid:36 ) SPECCMI st x = st x = 1 , xi ≥ 0 ( 19 ) where ( cid:36 ) denotes equivalence in feature ordering , noting that √ k with x = 1 only scales the solution replacing x = √ by a multiplicative constant 1/ k . The non negativity constraints xi ≥ 0 ensure that the relaxed solution can be reasonably interpreted as feature ‘weights’ . Without the non negativity constraints xi ≥ 0 , albeit being a non convex problem in general , SPECCMI admits a simple global solution which coincides with that maximizing the well known Rayleigh quotient of the form xT Qx xT x . The solution to this problem is any unit norm eigenvector corresponding to the dominant eigenvalue of Q [ 10 ] . At optimality , the dominant eigenvalue of Q is also the maximum objective value . When the entries in Q are all non negative , as I(Xi ; C|Xj ) ≥ 0 , then we can prove the following result :
Theorem 2 . If Qij ≥ 0 ∀i , j then : ( i ) the optimal solution x∗ for maxx=1 sign consistent , ie having all x∗ i ’s of the same sign .
( xT Qx ) must be
( ii ) any dominant eigenvector of Q must be sign consistent . ( iii ) if there exists a dominant eigenvector x∗ having x∗ i > 0,∀i , ie strictly positive , then its eigenvalue must be the unique dominant eigenvalue of Q . i x∗
( ii ) We shall note that the critical points and critical val
Proof . ( i ) Assume that x∗ has mixed sign components , as Qij ≥ 0 ∀i , j the value of the quadratic form x∗T Qx∗ = i,j x∗ j Qij can always be increased by assigning the same i ’s ( still satisfying x∗ = 1 ) , contradicting the sign to all x∗ assumption that x∗ is the globally optimal solution .
( xT Qx ) are respectively all the unit norm ( xT Qx ) , ues of maxx=1 eigenvectors of Q and their eigenvectors . In case Q has duplicate dominant eigenvalues , all their associated eigenvectors are globally optimal solution for maxx=1 and therefore must be sign consistent , as per ( i ) . i > 0 , there cannot exist any other sign consistent ( dominant ) eigenvector that is orthogonal to x∗ , thus its eigenvalue must be the unique dominant eigenvalue of Q .
( iii ) As x∗
In view of this result , we can use any unit norm dominant eigenvector of Q with all non negative entries as the solution to SPECCMI . As for the feature ranking purpose , features with higher weights xi will appear higher in the ranking , ie more important features . It is noted that in the SPECCMI formulation , the Hessian Q is not required to be positive semidefinite as in the QPFS formulation .
Example 1 revisited : we have I(G , C|S ) = I(G ; C ) − I(G ; S ) + I(G , S|C ) = 0 bit I(S , C|G ) = I(S ; C ) − I(G ; S ) + I(G , S|C ) = 0.2864 bit The solution to the SPECCMI formulation max x=1,xi≥0 xT
1
0.2864/2
0.2864/2
0.7136 x
( 20 ) is x∗ = [ 0.92 , 0.38]T , that is , smoking S ( weight 0.92 ) is ranked higher than coughing G ( weight 038 ) fl
3.2 Global MI based feature selection via semi definite programming
In this section , we investigate another strategy for solving the integer quadratic programming problem QIPCMI , via semi definite programming . Semi definite relaxation has recently gained increasing interest as an effective approximation tool for solving hard combinatorial problems . This significant interest was sparked by the seminal work of Goemans and Williamson [ 11 ] in approximating the NP hard max cut problem in graph theory ,
MAXCUT : max x
{xT Lx} st x ∈ {−1 , +1}n
( 21 ) where L is the graph Laplacian matrix . As semidefinite programming ( SDP ) is known to generate tighter approximation for MAXCUT over spectral relaxation , here we are interested in seeing whether for the QIPCMI problem , SDP can significantly improve over spectral relaxation as presented above . In order to employ the semidefinite relaxation technique in [ 11 ] , we first transform the binary 0–1 problem QIPCMI into a bipolar −1/+1 problem similar to MAXCUT , via the transformation xi = yi+1
, resulting in
2 fl 1 max y st
( y + 1)T Q(y + 1 )
4 y ∈ {−1 , 1}n , ( y + 1)T I(y + 1 ) = 4k
( 22 ) y st
( 23 ) max
√ where 1n×1 is the vector of all 1 ’s . Note that we could rewrite the norm constraint x = k as xT Ix = k where In×n is the identity matrix , hence ( y + 1)T I(y + 1 ) = 4k . Since the problem ( 22 ) is not in a homogeneous quadratic form , we shall transform it back to an equivalent homogeneous form via simply introducing an additional dummy variable y0 = 1 ( the variable expansion trick [ 27] ) , ie y = {y0 ≡ 1 , y1 , . . . , yn} , resulting in yTQy y0 = 1 , y ∈ {−1 , 1}n+1 , yTIy = 4k where Q(n+1)×(n+1 ) = 1T I1 1T I
1T Q1 1T Q andI(n+1)×(n+1 ) =
. We further note that the constraint y0 = 1 could also be relaxed to y0 ∈ {−1 , 1} . This is because homogeneous quadratic problems are symmetric in y and −y , therefore if y∗ is optimal , then −y∗ will also be optimal , and we simply need to pick the solution with y∗ 0 = 1 as the final solution . Now , note that the quadratic form yTQy can also be rewritten as Q • yyT , where U • V = Q • yyT y ∈ {−1 , 1}n+1,I • yyT = 4k we arrive at : i,j UijVij , max
( 24 )
Q1 y st
I1
I
Q
We next substitute Y = yyT , noting that an arbitrary matrix could only be factorized as such iff Y 0 , ie Y is positive semidefinite , and rank(Y ) = 1 . Also note that for yi ∈ {−1 , 1} we have yi.yi = 1 ⇔ diag(Y ) = 1 , we arrive at ( 25 )
Q • Y I • Y = 4k , diag(Y ) = 1 , Y 0 , rank(Y ) = 1 max st
Y
Until now we have not yet gained any computational advantage , as the problem ( 25 ) is still exactly equivalent to the NP hard QIPCMI problem . The specific constraint that causes NP hardness in this case is the rank 1 constraint , since without it , the following problem can be solved to optimality in polynomial time via semidefinite programming [ 18 ] :
Q • Y I • Y = 4k , diag(Y ) = 1 , Y 0
( 26 )
SDPCMI : max
Y st
After solving SDPCMI we need to recover the discrete {−1 , +1} solution to ( 23 ) , a process known as rounding . Herein , we simply adapt the random projection rounding technique proposed in [ 11 ] , with 100 random projections . In each projection , the top k features are selected as the ones with corresponding rows Yi· having largest cosine similarity to a randomly picked vector uniformly distributed on the unit hypersphere in Rn+1 . Finally , the random projection that results in largest value for the original QIPCMI problem is selected . Interested readers are referred to [ 11 ] for these details . 3.3 Complexity analysis
For all methods , generally there will be time needed for computing the similarity matrix and the time needed for ranking the features . The time for computing MI quantities , such as I(Xi ; Xj ) and I(Xi ; C|Xj ) comprises mainly O(d ) time for computing the joint probability table . Thus , the time for computing the similarity matrix is O(n2d ) . The ranking time complexity for MRMR , SPECCMI and SDPCMI is provided in Table 1 . The dominant time component for MRMR and SPECCMI is in fact , for computing the similarity matrix , rather than ranking . In terms of ranking time , SPECCMI is significantly less expensive than QPFS , while SDPCMI is the most expensive .
Table 1 : Ranking complexity in the number of features n .
Method
Complexity
MRMR SPECCMI O(n2 )
O(n2 )
QPFS SDPCMI O(n3 ) O(n4.5 )
It is noted that greedy algorithms , such as MRMR , fill the similarity matrix gradually and could be stopped at any point to produce a partial ranking . In data mining and knowledge discovery , it is also often desirable to produce a complete ranking of all features . Indeed , while the top ranking features are important for building accurate classifiers , features with low ranks are important for understanding the data generating process . Such knowledge could be used , for example , to improve the data collecting process , where the least important features could be omitted from data collection . Also , a domain expert may be interested in studying how a feature of interest is ranked compared to others , in such case a complete ranking of all features is required . 3.4 Global feature selection for large data
For large data , computing the kernel like matrix Q itself becomes expensive . Herein , we investigate a strategy to reduce this cost via low rank approximation for Q , in particular via the Nystr¨om method . Nystr¨om based methods for large scale data analysis have been successfully applied on numerous problems in the pattern recognition and machine learning literature [ 9 , 15 ] . Without loss of generality , we can assume Q in the SPECCMI formulation ( 19 ) to be positive semi definite . Indeed
≡ arg max x=1,xi≥0 xT ( Q + λI)x
SPECCMI : arg max x=1,xi≥0 xT Qx where λ can be chosen as a sufficiently large positive constant without affecting the ranking . Nystr¨om method approximates the positive semi definite Q as
A
Q =
B
BT BT A−1B using only a subset of p = γn rows of Q , namely those comprising [ Ap×p Bp×(n−p) ] , where the rows are usually randomly sampled without replacement and 0 < γ < 1 is the Nystr¨om sampling rate . A useful characteristic of the Nystr¨om approximation is that the approximated solution to the SPECCMI formulation , namely the dominant eigen vector of Q , could be computed exactly from submatrices BT A−1B and fill in Q . This could be very useful for sitmerely storing Qn×n could already be a problem . Let A fine Ap×p = A + A− 1 vector of Q is simply uations where the number of features is large , such that 1 2 denote the symmetric positive definite square root of A , de2 then the dominant eigen of much smaller size , without explicitly evaluating the block
2 BBT A− 1
A
BT
− 1
2
σ
− 1
A
2 u where σ and u are the dominant eigenvalue and its associ ated eigenvector of A [ 9 ] . The complexity of the Nystr¨om approximated solution is O(γn2d ) for computing the similarity matrix and O(γn2 +γ2n2 ) for ranking . One remaining detail left is that although Q is entry wise positive , it is not guaranteed that this property carries over to its approxima tion Q . Thus , Q can have negative elements and as a results , its dominant eigenvector can have negative entries . In such cases , we induce a global ranking as follows . First , the problem is converted from a binary 0–1 problem to an equivalent bi polar +1/ 1 problem as in ( 22 ) . Then a dummy variable x0 ≡ 1 , which is supposed to be always chosen , is included as in ( 23 ) . The dominant eigenvector of
1TQ1 1TQ Q1 Q with the first entry ( corresponding to x0 ) being positive is chosen for feature ranking where the weights are sorted in descending order .
In [ 21 ] Nystr¨om approximation was also applied for approximating the QPFS formulation ( 2 ) . For QPFS , a second level of approximation was further proposed , where the quadratic programming problem is approximated with one at a lower dimension , using only the largest eigenvalues of ( the Nystr¨om approximated ) H . As opposed to QPFS , for the proposed SPECCMI formulation , only one level of approximation , ie approximating Q , is necessary . In general , Nystr¨om approximation quality improves with increasing p . With a fixed sampling rate , approximation is better when there exists more redundancy in Q , ie there are similar features .
4 . EXPERIMENTAL EVALUATION
We perform a series of experiments to evaluate the efficiency and effectiveness of the two novel MI based feature selection frameworks , namely SPECCMI and SDPCMI . First ,
Table 2 : Average time ( in seconds ) required for solving SDPCMI and SPECCMI at different problem sizes ( given pre computed similarity matrices Q and H ) . SDPCMI #Features n 0.78 ± 0.07 0.005 ± 0.001 21 1.18 ± 0.44 0.005 ± 0.001 57 1.59 ± 1.27 0.005 ± 0.001 64 3.38 ± 0.15 0.005 ± 0.000 166 9.46 ± 0.20 0.006 ± 0.001 257 325 16.92 ± 0.73 0.007 ± 0.005 See Sec 4.2
Dataset Waveform Promoter Optdigits Musk Arrhythmia Lung cancer
SPECCMI n > 700
N/A we compare SPECCMI and SDPCMI in terms of their capability to approximate QIPCMI , and draw the conclusion that SPECCMI is the preferred approach . Second , we test SPECCMI against QPFS in terms of scalability , and draw the conclusion that SPECCMI is much more computationally scalable . Lastly , we compare SPECCMI with other MI based feature selection techniques on an extensive set of 15 small and medium sized real life data sets and 4 large datasets . The experiments were carried out on an Intel Core i7 2.9Ghz PC with 16Gb of main memory . 4.1 SPECCMI vs . SDPCMI : a test of approxima tion tightness and scalability
We select several small size data sets in Table 4 , namely Waveform ( n = 21 ) , Promoter ( n = 57 ) , Optdigits ( n = 64 ) , Musk ( n = 166 ) , Arrythmia ( n = 257 ) and Lung cancer ( n = 325 ) for this experiment . To solve the SDPCMI formulation , we employ the CVX toolbox for convex optimization [ 12 ] , with the underlying solver being SDPT3 [ 23 ] . We set the number of features to be selected k to the range [ 1 , min(n , 100) ] , thus in total there are 442 test cases . The average runtime comparison for SDPCMI and SPECCMI ( for the ranking phase ) is reported in Table 2 . For these small problems , the time required for SPECCMI is negligible , while SDPCMI is orders of magnitude slower , but still acceptable . While SDPCMI running time does not seem a problem , it exhibits a large memory footprint . In fact , for problems with n ∼ 700 , CVX returns an out of memory error2 on our PC . Note that the number of variables in the relaxed space 2 n2 ) . For example , with for semidefinite programming is O( 1 n = 500 , CVX reports problem size of 125,751 variables and employs additionally ∼ 8Gb of memory .
We next compare SDPCMI and SPECCMI in terms of the objective value of the original 0–1 problem QIPCMI in ( 18 ) . Of all the 442 test cases , SDPCMI and SPECCMI return different results in only 63 cases ( ∼ 14% ) , within which SDPCMI ‘wins’ over in 46 cases ( ∼ 73% ) . Thus it can be seen that SDPCMI tends to outperforms SPECCMI . This observation conforms well with previous studies , that semidefinite relaxation provides tighter approximation than spectral relaxation , as in other hard combinatorial problems such as graph max cut . Nevertheless , the difference herein observed , if any , is often minor . More specifically , we have 58/63 cases in which the absolute relative difference , computed as |objSDP− objSPEC|/objSPEC ( where objSDP and objSPEC are the objective values of the SDPCMI and SPECCMI approximated solution respectively ) , is < 05 % Furthermore , a closer in
2Technically , we could employ more virtual memory using hard disk to circumvent memory shortage . But this results in a huge running time due to the high latency of hard disks . spection reveals that in all cases , the feature sets differ in at most 2 features . For the rest 379/442 cases ( 86% ) , SDPCMI and SPECCMI return identical objective value and identical feature sets . For the smallest Waveform data set , we also compute the optimal objective value found by exhaustive enumeration . In this case , the maximum relative difference between the optimal objective and that of SPECCMI is only 0.07 % , confirming the effectiveness of the two approximation schemes . For the other larger data sets , exhaustive enumeration is unacceptably slow , even with k = 5 , ruling out this brute force approach as a practical solution .
From this set of experiment , we draw the conclusion that , while semidefinite programming tends to generate tighter approximation , the difference is negligible . More importantly , the two techniques most often generate identical feature sets . In view of the fact that SPECCMI is significantly simpler and more computationally efficient , we therefore promote SPECCMI as the method of choice . In the next sections , we establish the efficiency and effectiveness of SPECCMI against other popular MI based feature selection approaches . 4.2 SPECCMI vs . QPFS : a test of scalability
To fix a concrete idea about how scalable and speedy dominant eigenvalue computation is , compared to quadratic convex optimization , we generate 10 random positive definite Q matrices for each size ranging from 1,000 to 30,000 ( and also random relevancy vectors f ) , and solve the QPFS and SPECCMI problems using popular off the shelf solvers , specifically those provided by Matlab with default options . The average wall clock time to solve these problems is provided in Table 3 . Note that at n > 16 , 000 , Matlab solver ( quadprog ) returned an out of memory error for QPFS . On average , we observe that QPFS running time is two or more orders of magnitude slower than SPECCMI . For comparison , the ranking time for the incremental MRMR approach on the same similarity matrices is also reported in Table 3 . With a carefully tuned implementation3 , MRMR outpaces SPECCMI in running time , but practically this difference should not be a major concern .
In terms of practical implementation , the solution to SPECCMI amounts to finding the dominant eigenvector of the Hessian matrix Q . Algorithmically , this can be done as simply as repeatedly applying Q to any nondegenerate initial solution ( the power method ) . In practice , dominant eigenvalue finding is a basic and efficient operation , which is built in at the core of most , if not all , numerical packages . On the other hand , QPFS requires the solution to a quadratic convex optimization problem with linear constraints , which is arguably not always readily available as eigenvector computation . A further advantage of the SPECCMI formulation over QPFS is that its solution via eigenvector decomposition is much more amenable to parallel computation , and can be implemented straightforwardly , readily exploiting the benefit of currently popular multi core PC systems . Parallel implementation for quadratic and semi definite programming on the other hand is an advanced research topic [ 26 ] . 4.3 Small and medium data sets
3Implementation details can have considerable effects on the actual run time of the algorithms . Here , we employ our own optimized C++ implementation for MRMR to ensure its competitiveness , given that the same code implemented in Matlab could be 40 100 times slower .
Table 3 : Average time ( in seconds ) required for ranking the features at different problem sizes ( given pre computed similarity matrices Q and H ) .
QPFS
#Features 0.81 ± 0.11 1,000 55.94 ± 5.14 5,000 417.22 ± 25.23 10,000 13,000 1026.73 ± 85.99 16,000 2012.63 ± 157.15 20,000 30,000
N/A N/A
MRMR
SPECCMI 0.03 ± 0.01 0.01 ± 0.00 0.81 ± 0.03 0.17 ± 0.02 2.91 ± 0.26 0.68 ± 0.04 5.12 ± 0.45 1.17 ± 0.06 7.66 ± 0.89 1.97 ± 0.41 10.64 ± 0.30 2.63 ± 0.11 25.03 ± 1.42 6.05 ± 0.25
Table 4 : Dataset summary . n : #features , d : #samples , #C : #classes , Error : average cross validation error rate ( % ) using all features .
Data
NCI60 SRBCT Lung Colon Leukemia Lymphoma Promoter Spambase Musk2 Arrhythmia Multi features Waveform Optdigits Gisette Madelon n
9996 2308 325 2000 7129 4026 57 57 166 257 649 21 64 5000 500 d 60 84 73 62 73 96 106 4601 6598 430 2000 5000 3823 6000 2000
#C Error 43.3 1.2 12.3 17.7 1.4 3.1 16.0 9.5 4.9 21.6 1.6 13.0 1.8 50.0 34.7
10 4 7 2 2 9 2 2 2 2 10 3 10 2 2
Source [ 21 ] [ 20 ]
[ 21 ]
[ 20 ] [ 7 ]
[ 7 ] [ 7 ] [ 7 ] [ 1 ] [ 1 ] [ 1 ] [ 1 ] [ 1 ] [ 1 ] [ 1 ] [ 1 ] [ 1 ]
We compare the proposed SPECCMI method with state of the art MI based feature selection approaches on an extensive set of 15 well known public datasets used in previous research [ 3 , 14 , 20 , 21 ] , covering a wide range of number of features , samples and classes . Feature selection methods are compared in terms of the average cross validation ( CV ) classification error rate on the range of 10 to 100 features in step of 1 ( or 10 to n if n < 100 ) . We employ 10 fold CV for datasets with number of samples d ≥ 100 and leave oneout CV otherwise . Following [ 14 , 21 ] , the based classifier for most data sets is chosen as linear SVM ( with the regularization parameter set to 1 ) , except for the Gisette and Madelon datasets , where a 3 NN classifier was used following [ 3 ] . Details of the datasets used are given in Table 4 . Continuous features were discretized using Fayyad and Irani ’s minimum description length ( MDL ) method [ 8 ] . Feature selection was done on discretized data , while classification was performed on the original feature space .
Apart from the feature selection approaches mentioned herein , namely MRMR , EMRMR , QPFS and SPECCMI , we also compare our approach with other well known MI based methods , namely maximum relevance ( maxRel ) , mutual information quotient ( MIQ ) [ 7 ] and conditional infomax feature extraction ( CIFE ) [ 16 ] . The connections between these methods are presented in great details in [ 3 ] . We note that [ 3 ] recommends the so called joint mutual information ( JMI ) , maxXi Xj∈S I(Xi , Xj ; C ) , as the criterion of choice , for providing ‘the best tradeoff in terms of accuracy , stability and flexibility with small samples’ . We note that the JMI criterion is in fact exactly equivalent to the EMRMR criterion presented herein , and also the ‘average CMIM’ criterion in [ 14 ] . For QPFS the balancing factor α was set as recommended in [ 21 ] . SPECCMI requires no parameter tuning . We implemented and optimized the codes for all the above methods in Malab/C++ , which are made publicly available via our website .
The experimental results for all methods are presented in Table 5 . In order to summarize the statistical significance of the findings , as in [ 14 ] , we employ the one sided paired t test at 5 % significance level to test the hypothesis that SPECCMI or a compared method performs significantly better than the other . Overall , we found the proposed SPECCMI framework to perform strongly against other popular MI based criteria for feature selection . In particular , SPECCMI consistently outperforms the alternative global formulation QPFS . Of the incremental methods , SPECCMI strongly outperforms maxRel , MIQ and CIFE . On the other hand , MRMR and EMRMR are two leading local algorithms , being only narrowly behind SPECCMI . The bold entries in Table 5 indicate the best performing algorithms ( in terms of the average error rate and its standard deviation)—although the difference compared with other methods might not necessarily be statistically significant . The distribution of the ‘bold entries’ seems to suggest that no algorithm is universally dominant—a reminiscence of the no free lunch theorem for machine learning [ 25 ] . Nevertheless , from a practical viewpoint , for the supervised feature selection problem , one can , and should , try multiple feature selection strategies and use , eg the cross validation error rate as a guidance to choose the final set of features . From this perspective , we propose that SPECCMI is a valuable addition to the current literature on feature selection .
4.4 Large data
We employ four datasets from the handwritten Chinese character database [ 17 ] as detailed in Table 6 . These data are characterized by a large number of training samples , and especially a very large number of classes , making classification a challenging task . Indeed , the SVM implementation we employed , namely LibSVM [ 4 ] , does not scale very well with this application where it has to train a large number ( ∼ 3700 ) of one versus all classifiers . We therefore resort to a much simpler and more computationally efficient nearest class mean ( NCM ) classifier [ 19 ] . We select the top two performing greedy algorithms , namely MRMR and EMRMR , from section 4.3 together with QPFS and SPECCMI for this test . In addition , we test the effectiveness of Nystr¨om approximation with both SPECCMI and QPFS . We train the classifier on the train data and test accuracy is estimated on the separate test data . Since the number of samples is large , we expect this performance indicator to be representative . On these data , the MDL algorithm [ 8 ] binarizes most features , ie discretizing to only 2 states . Observing that the very large number of samples can support a finer discretization , we therefore also discretize the data to 5 and 10 equal frequency bins . While finding the optimal discretization strategy is beyond the scope of this paper , we summarize our finding as follows : at lower number of bins , ie 2 and 5 , methods that are based on the MI such as MRMR and QPFS outperform methods based on conditional MI , such that SPECCMI and EMRMR . On the other hand , at higher number of bins , EMRMR and SPECCMI performs
Table 5 : Cross validation error rate comparison of SPECCMI against other methods . W : win ( + ) , T : tie ( = ) , L : loss ( − ) for SPECCMI against the compared method according to the 1 sided paired t test .
MIQ
Data maxRel
14.2 ± 5.7 ( + ) 35.1 ± 8.6 ( + ) 12.8 ± 1.5 ( = ) 3.1 ± 1.0 ( + ) 4.3 ± 2.5 ( = ) 0.8 ± 1.1 ( − ) 8.7 ± 2.9 ( = ) 11.5 ± 1.4 ( + ) 7.8 ± 1.8 ( + ) 22.2 ± 1.0 ( + ) 2.0 ± 0.9 ( + ) 3.3 ± 2.5 ( + ) 13.9 ± 1.5 ( = ) 7.8 ± 2.7 ( + ) 18.7 ± 3.2 ( + )
Lung NCI60 Colon Leukemia Lymphoma SRBCT Promoter Spambase Musk2 Arryth . Multifeat . Optdigits Waveform Gisette Madelon #W/T/L : ∗Also equivalent to the JMI and ave CMIM criteria , see [ 3 , 14 ]
120±27 ( + ) 401±141 ( + ) 128±16 ( = ) 10±16 ( − ) 60±50 ( + ) 21±35 ( + ) 89±34 ( = ) 123±32 ( + ) 71±16 ( − ) 240±35 ( + ) 32±28 ( + ) 33±26 ( + ) 142±13 ( + ) 92±42 ( + ) 374±37 ( + )
10/4/1
CIFE
160±22 ( + ) 649±36 ( + ) 144±27 ( + ) 50±10 ( + ) 166±29 ( + ) 118±37 ( + ) 126±24 ( + ) 179±41 ( + ) 74±11 ( = ) 246±18 ( + ) 28±07 ( + ) 34±25 ( + ) 163±21 ( + ) 67±18 ( + ) 175±34 ( + )
14/1/0
11/2/2
MRMR 98±34 ( + ) 305±107 ( = ) 128±14 ( = ) 24±08 ( − ) 41±21 ( = ) 06±12 ( − ) 94±34 ( + ) 113±15 ( = ) 74±16 ( = ) 223±15 ( + ) 18±04 ( = ) 30±20 ( = ) 137±10 ( = ) 60±25 ( = ) 282±22 ( + )
4/9/2
EMRMR∗ 97±34 ( + ) 302±87 ( − ) 120±10 ( − ) 25±06 ( − ) 40±24 ( = ) 09±12 ( = ) 83±32 ( = ) 114±15 ( + ) 74±10 ( = ) 222±08 ( + ) 18±06 ( = ) 30±22 ( = ) 137±11 ( = ) 64±24 ( + ) 160±31 ( = )
4/8/3
QPFS
104±28 ( + ) 281±94 ( − ) 132±14 ( + ) 30±10 ( = ) 55±37 ( + ) 01±03 ( − ) 87±33 ( = ) 120±19 ( + ) 74±19 ( = ) 228±20 ( + ) 24±13 ( + ) 41±40 ( + ) 137±10 ( = ) 80±23 ( + ) 233±35 ( + )
9/4/2
SPECCMI 94±25 313±84 127±12 29±10 43±39 09±13 83±31 113±15 74±09 217±07 19±06 31±22 137±10 61±22 159±32
Table 6 : Large dataset summary . n : #features , d : #samples , #C : #classes , Error : test error rate ( % ) using all features with NCM classifier .
Data HWDB1.0 HWDB1.1 OLHWDB1.0 OLHWDB1.1 n 512 512 512 512 d ( train ) 1,246,991 897,758 1,256,009 898,573 d ( test ) #C Error 21.93 309,684 26.42 223,991 314,042 15.99 17.18 224,559
3,740 3,755 3,740 3,755 slightly better than QPFS and MRMR . Our hypothesis is that a larger number of bins can leverage the large number of samples such that higher dimensional mutual information quantities , such as the conditional MI , could be estimated at greater resolution . The test error rate on sets of up to 200 features on the 10 bin discretized data are reported in Figure 2(a d ) . It is noted that SPECCMI+Nystr¨om at a sampling rate of γ = 0.2 perform remarkably well on the HWDB1.0 and HWDB1.1 datasets , in fact better than SPECCMI—a somewhat intriguing observation , while being slightly better than QPFS on the OLHWDB1.0 and OLHWDB1.1 data . The effect of different sampling rate for SPECCMI+Nystr¨om on the OLHWDB1.1 is presented in Fig 2(e ) . The wall clock execution time of all algorithms on each data set is presented in Fig 2(f ) . It is observed that methods that make use of the conditional MI such as SPECCMI and EMRMR are more expensive than methods that make use of the MI such as QPFS and MRMR , mainly due to the fact that computing conditional MI is more time consuming . SPECCMI and EMRMR admit similar execution time , while QPFS and MRMR admit similar execution time . Nystr¨om approximation significantly reduces the execution time for both SPECCMI and QPFS .
5 . CONCLUSION
In this paper , we have introduced a novel global optimization framework for the mutual information based feature selection problem . Our criterion for optimization is formulated based on the conditional mutual information , an information theoretic quantity which neatly captures feature relevancy , redundancy as well as class conditional redundancy , leading to a neat homogeneous quadratic optimization criterion . We have demonstrated that this global formulation can be efficiently solved via spectral relaxation , admitting a very simple numerical solution . We also compared the spectral relaxation approach with the more sophisticated semidefinite relaxation , and establish that spectral relaxation returns mostly identical solution at a much cheaper computational cost . Compared to the local formulations MRMR and EMRMR , the global formulations can overcome the issue of local minima faced by local greedy schemes . Compared to the alternative global QPFS formulation , our new SPECCMI framework naturally resolves several theoretical issues associated with the previous global QPFS formulation . Moreover , SPECCMI admits a significantly simpler and much more efficient global solution , yet without any strict condition , such as positive definiteness , on the Hessian matrix .
Acknowledgments : This work is supported by the Aus tralian Research Council via grant number FT110100112 .
6 . REFERENCES [ 1 ] K . Bache and M . Lichman . UCI machine learning repository , 2013 .
[ 2 ] R . Battiti . Using mutual information for selecting features in supervised neural net learning . Neural Networks , IEEE Transactions on , 5(4):537–550 , 1994 .
[ 3 ] G . Brown , A . Pocock , M J Zhao , and M . Luj´an .
Conditional likelihood maximisation : A unifying framework for information theoretic feature selection . J . Mach . Learn . Res . , 13:27–66 , Mar . 2012 .
[ 4 ] C C Chang and C J Lin . LIBSVM : A library for support vector machines . ACM Transactions on Intelligent Systems and Technology , 2:27:1–27:27 , 2011 . Software available at http://wwwcsientuedutw/~cjlin/libsvm
[ 5 ] A . W . Chaovalitwongse , I . P . Androulakis , and P . M .
Pardalos . Quadratic integer programming : complexity and equivalent forms quadratic integer programming : Complexity and equivalent forms . In C . A . Floudas and P . M . Pardalos , editors , Encyclopedia of Optimization , pages 3153–3159 . 2009 .
[ 6 ] H . Cheng , Z . Qin , W . Qian , and W . Liu . Conditional mutual information based feature selection . In Knowledge Acquisition and Modeling , pages 103–107 , 2008 .
[ 7 ] C . Ding and H . Peng . Minimum redundancy feature selection from microarray gene expression data . In Bioinformatics Conference , 2003 , pages 523–528 , 2003 .
[ 8 ] U . M . Fayyad and K . B . Irani . Multi interval discretization of continuous valued attributes for classification learning . In IJCAI , pages 1022–1029 , 1993 .
[ 9 ] C . Fowlkes , S . Belongie , and J . Malik . Efficient spatiotemporal grouping using the nystrom method . In CVPR 2001 , volume 1 , pages I–231–I–238 vol.1 , 2001 .
( a ) HWDB1.0
( b ) HWDB1.1
( c ) OLHWDB1.0
( d ) OLHWDB1.1
( e ) Effect of γ on OLHWDB1.1
( f ) Execution time
Figure 2 : Test accuracy and execution time on the handwritten Chinese character database .
[ 10 ] J . Gallier . Geometric Methods and Applications : For Computer Science and Engineering . Texts in Applied Mathematics . Springer Verlag GmbH , 2001 .
[ 11 ] M . X . Goemans and D . P . Williamson . Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming . J . ACM , 42(6):1115–1145 , Nov . 1995 .
[ 12 ] M . Grant and S . Boyd . CVX : Matlab software for disciplined convex programming , version 2.0 beta . http://cvxr.com/cvx , Sept . 2013 .
[ 13 ] B . Guo and M . Nixon . Gait feature subset selection by mutual information . IEEE Trans . Syst . , Man , Cybern . A , Syst . , Humans , 39(1):36–46 , 2009 .
[ 14 ] G . Herman , B . Zhang , Y . Wang , G . Ye , and F . Chen .
Mutual information based method for selecting informative feature sets . Pattern Recognition , 46(12):3315 – 3327 , 2013 .
[ 15 ] M . Li , J . T . Kwok , and B L Lu . Making large scale nystr¨om approximation possible . In J . F¨urnkranz and T . Joachims , editors , ICML , pages 631–638 . Omnipress , 2010 .
[ 16 ] D . Lin and X . Tang . Conditional infomax learning : an integrated framework for feature extraction and fusion . In ECCV’06 .
[ 17 ] C L Liu , F . Yin , D H Wang , and Q F Wang . Casia online and offline chinese handwriting databases . In Document Analysis and Recognition ( ICDAR ) , 2011 International Conference on , pages 37–41 , Sept 2011 . [ 18 ] Z Q Luo , W K Ma , A C So , Y . Ye , and S . Zhang .
Semidefinite relaxation of quadratic optimization problems . Signal Processing Magazine , IEEE , 27(3):20–34 , 2010 . [ 19 ] T . Mensink , J . Verbeek , F . Perronnin , and G . Csurka .
Distance based image classification : Generalizing to new classes at near zero cost . IEEE TPAMI , 35(11):2624–2637 , Nov 2013 .
[ 20 ] H . Peng , F . Long , and C . Ding . Feature selection based on mutual information criteria of max dependency , max relevance , and min redundancy . IEEE Trans . Pattern Anal . Mach . Intell . , 27(8):1226–1238 , 2005 .
[ 21 ] I . Rodriguez Lujan , R . Huerta , C . Elkan , and C . S . Cruz .
Quadratic programming feature selection . J . Mach . Learn . Res . , 11:1491–1516 , 2010 .
[ 22 ] J . M . Sotoca and F . Pla . Supervised feature selection by clustering using conditional mutual information based distances . Pattern Recognition , 43(6):2068–2081 , June 2010 . [ 23 ] K . C . Toh , M . Todd , and R . H . T¨ut¨unc¨u . Sdpt3 – a matlab software package for semidefinite programming . Optimization methods and software , 11:545–581 , 1999 .
[ 24 ] N . Vinh and J . Bailey . Comments on supervised feature selection by clustering using conditional mutual information based distances . Pattern Recognition , 46(4):1220 – 1225 , 2013 .
[ 25 ] D . H . Wolpert . The lack of a priori distinctions between learning algorithms . Neural computation , 8(7):1341–1390 , 1996 .
[ 26 ] M . Yamashita , K . Fujisawa , M . Fukuda , K . Nakata , and
M . Nakata . Algorithm 925 : Parallel solver for semidefinite programming problem having sparse schur complement matrix . ACM Trans . Math . Softw . , 39(1):6:1–6:22 , Nov . 2012 .
[ 27 ] Y . Zhang , S . Burer , and W . N . Street . Ensemble pruning via semi definite programming . J . Mach . Learn . Res . , 7:1315–1338 , Dec . 2006 .
501001502000405060708091Number of featuresError rate MRMREMRMRQPFSQPFS+Nystrom , γ=0.2SPECCMISPECCMI+Nystrom , γ=02501001502000405060708091Number of featuresError rate MRMREMRMRQPFSQPFS+Nystrom , γ=0.2SPECCMISPECCMI+Nystrom , γ=0205010015020002030405060708091Number of featuresError rate MRMREMRMRQPFSQPFS+Nystrom γ=0.2SPECCMISPECCMI+Nystrom γ=0205010015020002030405060708091Number of featuresError rate MRMREMRMRQPFSQPFS+Nystrom γ=0.2SPECCMISPECCMI+Nystrom γ=0205010015020002030405060708091Number of featuresError rate SPECCMISPECCMI+Nystrom γ=0.1SPECCMI+Nystrom γ=0.2SPECCMI+Nystrom γ=05HWDB11HWDB10OLHWDB11HWDB10100101102103104105Seconds MRMRQPFSQPFS+Nystrom , γ=0.2EMRMRSPECCMISPECCMI+Nystrom , γ=0.2
