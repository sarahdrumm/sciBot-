MenuMiner : Revealing the Information Architecture of
Large Web Sites by Analyzing Maximal Cliques
Matthias Keller
Steinbuch Centre for Computing ( SCC ) Karlsruhe Institute of Technology ( KIT )
D 76128 Karlsruhe , Germany matthiaskeller@kitedu
ABSTRACT The foundation of almost all web sites' information architecture is a hierarchical content organization . Thus information architects put much effort in designing taxonomies that structure the content in a comprehensible and sound way . The taxonomies are obvious to human users from the site's system of main and sub menus . But current methods of web structure mining are not able to extract these central aspects of the information architecture . This is because they cannot interpret the visual encoding to recognize menus and their rank as humans do . In this paper we show that a web site's main navigation system can not only be distinguished by visual features but also by certain structural characteristics of the HTML tree and the web graph . We have developed a reliable and scalable solution that solves the problem of extracting menus for mining the information architecture . The novel MenuMineralgorithm allows retrieving the original content organization of large scale web sites . These data are very valuable for many applications , eg the presentation of search results . In an experiment we applied the method for finding site boundaries within a large domain . The evaluation showed that the method reliably delivers menus and site boundaries where other current approaches fail . Categories and Subject Descriptors Interfaces and H54 Information [ Information Presentation – Hypertext/Hypermedia ; H31 Systems ] : Information Storage and Retrieval – Content Analysis and Indexing General Terms Algorithms , Experimentation , Languages Keywords Web structure mining , Site boundaries , Site hierarchies , Search result presentation 1 . INTRODUCTION Information architecture is crucial for the success of a web site . Engineering the information architecture means labeling hundreds or thousands of resources , organizing them with coherent schemas and developing understandable systems for accessing them . The technical model of content organization are database schemas while information architecture .
[ Information Systems ] : the human model the is
Copyright is held by the International World Wide Web Conference Committee ( IW3C2 ) . Distribution of these papers is limited to classroom use , and personal use by others . WWW 2012 Companion , April 16–20 , 2012 , Lyon , France . ACM 978 1 4503 1230 1/12/04 .
Martin Nussbaumer
Steinbuch Centre for Computing ( SCC ) Karlsruhe Institute of Technology ( KIT )
D 76128 Karlsruhe , Germany martinnussbaumer@kitedu
Information architecture is a way of communication . This is because users must be able to decode the information architecture in order to interact with the information system in the intended way . Decoding the information architecture means that humans are able to understand content hierarchies , to distinguish navigation elements from page content and to learn the purpose of navigation systems . We can recognize entry pages of web sites and we notice if we cross site boundaries . Because of the visual encoding these structures are obvious to humans but not to machines . If a site contains a reasonable number of pages it is usually not very complicated for an average human to model the site map but it is so for machines , even for Google1 . For a site map to be included in the search result presentation the site map has to be provided to Google as XML file . Displaying the position of a page in the content hierarchy is supported by Google ( Figure 1 ) , but it works currently only for a small amount of destination pages that contain a similar presentation ( “ breadcrumbs ” ) , which can be extracted2 .
Figure 1 . Google is able to display the position in the content hierarchy only for a few pages . Though research has focused on extracting hierarchies from web sites ( eg [ 1],[2],[3] ) , no solution has been found yet that could compete with human judgment . The same applies for the problem of mining web site boundaries or subsites ( eg [ 4 ] , [ 5] ) , site entry pages [ 5 ] or compound documents [ 6 ] . There still is a semantic gap between information architecture as perceived by humans and structure information that can be retrieved by current approaches . The seemingly simple problem of extracting the main navigation elements of a site for reverse engineering the information architecture is hard to solve in practice . The straightforward approach of using heuristics for identifying the menus on all pages of a site and then combining frequent structures is difficult and error prone [ 7 ] . Thus up until now very valuable semantic information is not available for machine processing . The hierarchical content organization of a web site is a human made , well designed classification scheme for each page . Mining these taxonomies would not only allow a search result representation as shown in Figure 1 for all sites . For example , it would also allow visualizing all results from a certain site as tree structure . The tools exist to automatically generate
1Some ( eg http://wwwpowermappercom ) , but the quality of the results strongly depends on the examined site 2 The Official Google Blog : New site hierarchies display in search results , http://googleblogblogspotcom/2009/11/new site hierarchiesdisplay in searchhtml sitemaps
WWW 2012 – LSNA'12 WorkshopApril 16–20 , 2012 , Lyon , France1025 original site hierarchy can also be used to improve ranking , eg if a search returns many hits from a certain branch of a site hierarchy , the item at the root of the branch should be ranked first . A method for mining the original content hierarchy of web sites will also open new doors in research topics as web site abstraction and classification , information architecture reverse engineering and automated usability testing . As a first application we demonstrate how the problem of web site boundary detection can be solved by using the MenuMiner approach . 2 . CONTRIBUTION In this paper we show how the global and local menus that represent the central concept of content access and the main hierarchical structure of web sites can be mined . To our knowledge until now no other method exists that solves this problem well enough that the data can be used , eg for the presentation of search results . The presented approach of analyzing maximal cliques for mining menu systems is a novel and applicable solution that is highly accurate and not prone to spam . It scales and the time complexity is linear in the number of processed pages . As first application we used the algorithm to solve a known problem of web site structure mining : the detection of site boundaries ( [4],[5] ) . We used the algorithm to find site boundaries in a collection of 10,000 pages retrieved from a large domain . A method for manually validating and comparing site partitions is described . The evaluation showed that the site boundaries delivered by the MenuMiner algorithm are much more precise than the hierarchy contained in the URLs ( subdomains and folders ) and those retrieved with clustering methods that performed best in a previous experiment [ 4 ] . But the central finding is that the MenuMiner method extracts the main menus that distinguish sites exactly as perceived by humans . In the evaluation section we also point out by examples how the method described in this paper can solve other problems of web site structure mining such as hierarchy extraction , compound document mining and entry page detection . 3 . PROBLEM DEFINITION AND APPROACH The original content organization of web sites as perceived by humans cannot be extracted today because a reliable method for mining menu systems is lacking . This involves a method to distinguish main or sub menus from other page content and a method to identify recurring menus that are shared by a site or sub section . 3.1 Mining Shared Menus The straightforward approach is to first split all pages into smaller content segments and that represent navigation elements . Then an inter page analysis can be conducted to find recurring navigation elements that are shared by a set of pages . That is basically the method used in current works that involve the mining of navigation elements ( cf . Sect . 7 ) . Our previous work on this topic [ 7 ] was based on the same approach . We used an extended set of heuristics to identify navigation elements and conducted an experimental evaluation that was lacking before . Although this method was able to detect most of the shared menus correctly we found that it has inherent limitations that prevent a really satisfying precision . The problem is that the intra page analysis is prone to errors if the page segmentation does not precisely reflect the bounds of individual menus and sub menus . Consider the example in Figure 9 . The same menu has identify segments to additional items ( “ Samples ” , “ Forums ” ) on the page “ Home ” that are missing on the page “ Library ” . By comparing simply the hyperlinks in navigation bars both elements could not be matched . A case like this is not a rare exception but a rather common situation . Thus a better approach is to compute the percentage of shared hyperlinks and applying a threshold . Rodrigues et al . [ 8 ] used a threshold of 0.6 while we were using a threshold of 0.5 in combination with two other metrics . But such a threshold leads to the problem that often elements are matched that do not belong to the same navigation system . Another problem is that sometimes a menu can be distinguished from a submenu only by its visual properties . Then even the most sophisticated algorithm will have difficulties in providing the correct segmentation . With the additional links of a submenu on one page that is not displayed on another page the percentage of shared links can fall below any threshold . Originally we were planning to apply machine learning methods for adjusting the heuristics and improve the precision of the method . Instead we found a different approach that completely avoids the described problems .
Figure 2 . S menus define cliques in the web Graph . The bold edges represent bidirectional links ( example from wwwmicrosoftcom/windowsphone )
3.2 S Menus To avoid transitional volatility ( cf . Sect . 6.2 ) the navigation elements that play the key role for the content access are usually invariable elements in page transitions . These global and local menus allow not only navigating from one page to another but navigating over a group of pages . By this we mean , that we can use a single menu to traverse a group of pages . When we click on an item in the menu and move to the next page the menu is still present . According to [ 8 ] such menus will be referred to as structural menus or s menus in this paper . Of course not all navigation elements are s menus . For example , a list of links related to a certain resource or a group of external links do not have the described characteristics . S menus are the skeleton of the information architecture of sites that are based on menus3 . Because of their invariability the function of s menus is not only to provide paths through the information space but also to communicate the organization of the content . Thus s menus are most suitable for mining the organization of web sites .
3 Other sites are based on search . The MenuMiner approach allows distinguishing both types of sites as shown in Sect . 6.3
WWW 2012 – LSNA'12 WorkshopApril 16–20 , 2012 , Lyon , France1026 The most common model of web structure mining is the web graph whose vertices represent pages and whose edges are given by the hyperlinks . As Rodrigues et al . [ 8 ] have observed before , s menus define cliques in the web graph – complete sub graphs in which every two vertices are connected by an edge . This is because the menu itself is also present on the linked pages . Figure 2 shows an example where all pages that are linked in a menu contain the menu itself and thus have links to the other three pages . Instead of using a set of heuristics to identify navigation elements we propose to mine page segments that form cliques in the Web graph . While the figure only shows the four target pages of the menu , the menu is present on many more pages . In general on almost all sites the main menu is displayed on all pages . Conversely , s menus define the boundaries of the sites . Other s menus that are present only on a subset of the pages consequently represent sub sites . Thus the inclusion relation on the sets of pages that share an smenu reproduces the hierarchical structure of a site . 3.3 Mining S Menus by Analyzing Cliques In this section we describe the problem that has to be solved to recognize s menus by the clique feature . To illustrate that this is not a trivial task we have placed the screenshots in Figure 2 on top of the real web graph . The web graph can be turned into an undirected representation with fewer edges by only keeping bidirectional edges . The graph would have a lower maximum degree ∆ and all maximal cliques – cliques that are not part of larger cliques – could be enumerated in O(∆4 ) time [ 9 ] . Still this would not solve the problem of mining s menus , because these cliques can result from all links on the pages . Instead we need to find page segments that form cliques . We will refer to this kind of cliques as segment cliques , to distinguish them from cliques in the web graph . The page segment k of page pi can be considered as a set of Mi,k target pages of the hyperlinks it contains ( we assume that all hyperlinks that are unidirectional in the web graph are removed in advance ) :
,(cid:3038)={(cid:1868)(cid:3038)(cid:3117),(cid:1868)(cid:3038)(cid:3118),…,(cid:1868)(cid:3038)(cid:3262)(cid:3284),(cid:3286)} ( cid:1829 ) ∈ ( cid:2282)((cid:1831) ) : ∀,(cid:3038),,(cid:3035 ) ∈(cid:1829 )
Let SE be the set of all Si,k that are all segments of all pages of a domain . SE defines a graph GSE whose nodes are the segments . For a target page px ∈ Si,k this graph contains edges from Si,k to all follows : ( cid:1829)=(cid:4682 ) ( cid:3427)(cid:3435)(cid:1868)≠(cid:1868 ) ⇔(cid:1861 ) ≠(cid:3439)∧ ( cid:3435)(cid:1868)∈ ,(cid:3035)(cid:3439)∧(|(cid:1829)|>2)(cid:3431)(cid:4683 ) segments Sx,l of px . We can define the set of candidate cliques as
SC contains sets of segments , each from a different page ( first condition ) . The sets in SC are cliques in GSE because if a segment of a certain page is part of a set , this page also has to be a target page of all other segments in the set ( second condition ) . And finally in the context of s menus we are only interested in cliques with at least three nodes . Two segments on two pages that contain a link to the other page define a clique of two and this is certainly not enough to consider both segments as s menus . Of course not all cliques of page segments in SC represent smenus . Additional considerations are necessary to find a subset SC* of SC that is a good representation of s menus . One consideration is that one certain link can surely be part of only a single menu . On the other hand it should be allowed for a page segment to be part of more than one clique in SC* . Figure 3 shows an example of page segments that represent a simple hierarchical menu . The pages p1 p4 are the top level pages and the pages p5 p7 are part of a submenu under page p2 . In this example the page segmentation algorithm has failed to separate the menu levels , which is what often happens as described above . The light gray edges are the edges of the web graph that have been removed because they are not bidirectional . The menu defines two cliques for each level and by the clique method we are able to separate the s menu of the first level from the s menu of the second level . While the Segment S is part of both cliques , the cliques do not share an edge ( hyperlink ) .
Figure 3 . A menu can define multiple cliques
Another consideration is that larger cliques are more likely to be smenus than smaller cliques . For this reason larger cliques should be preferred over smaller ones . Regarding cliques with the same size we should prefer the clique whose segments are more uniform , because it is more likely that the segments of this clique really belong to the same navigation system . Let r be a scoring function that rates the uniformity of a clique based on the segments that define it . Resulting from the considerations an iterative procedure to find a subset SC* can be derived :
∀(cid:1829)∈(cid:1829 ) ( cid:4672)|(cid:1829)|≥(cid:3627)(cid:1829)(cid:3627)∧(cid:3435)|(cid:1829)|=(cid:3627)(cid:1829)(cid:3627)⇒((cid:1829 ) ) ≥((cid:1829))(cid:3439)(cid:4673 )
1 . Find Ci ∈ SC :
2 . Add Ci to SC* 3 . Update SC by removing Ci and all subsets of Ci :
( cid:1829)≔(cid:1829)∖(cid:2282)((cid:1829 ) )
4 . Let PCi be the set of pages , of which a segment is contained in Ci . Those are the pages that form the clique . Update SC by removing all cliques from SC that share an edge with Ci :
( cid:1829)≔(cid:1829)∖(cid:4676)(cid:1829)∈(cid:1829):(cid:4698)(cid:1842)(cid:3004)(cid:3285)∩(cid:1842)(cid:3004)(cid:3284)(cid:4698)>1(cid:4677 )
If |SC| > 0 , go back to step 1
5 .
In step 1 the largest clique with the highest score is selected . Ci represents a set of page segments that belong to the same menu . Of course all subsets of Ci belong to this menu too and can be removed from SC in step 3 . This is necessary because SC is formulated to contain not only maximal cliques but all cliques . The reason for this is that subsets are relevant because in step 4 all cliques are removed that share at least two nodes , and thus a
WWW 2012 – LSNA'12 WorkshopApril 16–20 , 2012 , Lyon , France1027 one mal the ely be hat for ents the ues SC . t to ent hyperlink can on cliques can now nly belong to o w become maxim h hyperlink , with m menu . Subsets o c cliques and cand T The procedure a m method we used r epresent s menu u used to compute c cannot be achie s solving the cliqu F First the size of o of all pages exc c computational c c consisting of seg T Thus the retrieve fu fulfill this condi a algorithm to gene 4 . PAGE SE 4 T The proposed a d different pages t m method of page d different navigat d does not matter b belonging to the 3 3 ) . Other metho [ 11 ] use heuris b background colo rmalization of ts that most lik orithm that can neration of SC th wn algorithms ustration and fo f page segment it is not an alg roblem is the gen using the know [ 9] ) . e nodes are give en by all segme ph by far and of the web grap nd , only cliqu be high . Seco are allowed in S ifferent pages a have to be split ques would still present an effici In Sect . 5 we p ut computing SC C . ATION
Ci – since a h of the removed c idates for SC* . above is an illu d to find sets o us . In this form the sets . The pr eved by solely e problem ( eg [ the graph whos ceeds the size o costs would b gments from di ed maximal cliq tion somehow . erate SC* withou EGMENTA menus by find approach mines ues in the web that define cliqu s crucial . It is i segmentation is e not merged in tion elements ar hand if a menu r on the other on element are m e same navigatio mentation as de ds of page segm stics in combin nation with vi e . r or element size ding segments graph . Thus , important that tw nto one segment u and a subme merged ( cf . Figu escribed in [ 10 ] sual attributes on the wo . It enu ure or as on on . ical way nks om
Fig T The method we h heuristics and d S Since the DOM o organization of a th hat only the hi r emains . For this u up and four rules B Because we are h hyperlinks are d n node is deleted a 2 2 ) , because these n node has two ch o other is not , the to o the parent nod n nodes with two c a are left unchange R Rule 3 covers sp d destroy the logic a a menu with a su th he third hyperl th hough it logical li inks of the firs c constitute the se gure 4 . DOM tr e implemented does not require tree [ 12 ] reflect a page , our meth ierarchical intra s all nodes of th s are applied to e only interested deleted ( rule 1 ) . and the child is e nodes do not p hildren and the node is deleted de ( rule 3 ) . Nod children who do ed ( rule 4 ) . Thes pecial cases whe al structure . Figu ubmenu beneath link is assigned ly belongs to th st level . The in egments . Though ransformation r in contrast doe e any layout re ts very well the hod transforms t a page structure he DOM tree are each node ( Figur in hyperlinks , l If a node has appended to the provide structura first one is a hy and both hyperl des with more th o not fulfill the c e nodes represen ere rule 2 and ru ure 5 shows a ty the third hyperli d to an additio he same segment nner nodes of t h the segments rules es not depend elated informati visual and logi the DOM in a w of the hyperlin e processed botto re 4 ) . not leaves that are n a single child , the ule e parent node ( r If a al information . I the yperlink while ded links are append n or han two children e 3 condition of rule s . nt page segments and ule 4 interfere a e of ypical DOM tree e 2 ink . Without rul nal segment ev ven ree t as the other thr ree the remaining tr retrieved this w way
Figu 5 . SE EGMENT C 5.1 O Overview lying the page s By appl GSE ( cf . Sect . 3.3 graph G s . A single hype of pages ments of the targ all segm t cliques the sub segment ed concept . describe servation that ca One obs segment cliques is that s Given a set of se graph . G to SC if the pag belong t he maximal cliqu Thus th rds decomposed afterwar cond observation The sec ing page after p processi to , only its n belongs red because onl consider ed the local appr preferre The pag ges indexed by t pages are cach Loaded resource e . mputing SC* For com follows : : 1 . Bu uild local web gra 2 . Red duce local web g the neighboring of t cur rrent page are re que with segmen cliq 3 . Rem move unidirectio 4 . Co ompute maximal erbosch algorithm Ke cliq ques . 5 . Use e the SegmentC dec compose the web 5.2 D Decomposin The Seg gmentCliqueFind containing all the set t of the proce segment maxima al cliques in the are orga the prop created children anized hierarchi posed approach with each segm n . cally , this inform h . Instead , a fla ment containing mation is not ne at list of page g only its direct ecessary for segments is t hyperlink ure 5 . Necessity CLIQUE DE of rule 2 ETECTION N segmentation al 3 ) whose nodes rlink in a segme get page . From bset SC* shall b lgorithm we obt represent segm ent defines multi the set SC that e extracted acco tain a larger ments instead iple edges to contains all ording to the an be utilized for s in SC corresp gments from dif ges do not form ues in the web g d in order to com n is that SC* c age . To find all neighbors in th ly they can be p roach to reduce the crawler are ed to avoid mul r computing SC pond to cliques fferent pages , thi m a clique in the graph can be co mpute SC* .
C* efficiently in the web is set cannot web graph . omputed and an be computed the s menus a c he web graph part of the same e the memory re analyzed one af ltiple requests f d locally by certain page have to be e clique . We equirements . fter another . for the same locally our im mplementation p proceeds as aph from curren graph . Hyperlink pages that do no emoved . Those s nts from the curr onal edges to ob cliques . An imp m [ 13 ] is used to nt page and all its ks that belong to ot contain a hype egments cannot rent page . btain an undirect plementation of t o enumerate all m liqueFinder algo b graph cliques a g Web Grap der algorithm ret cliques represe ssed page belo e partial web gr orithm ( see next and compute the ph Cliques turns the local S enting s menus ongs . Given is raph defined by s neighbors . o segments erlink the form a ted graph . the Bronmaximal section ) to e local SC* .
SC* which is to which a the list of the current
WWW 2012 – LSNA'12 WorkshopApril 16–20 , 2012 , Lyon , France1028 are already assi were processed . s are preferred inal clique and ue is larger . For sizes that a new or target page r the link to p3 in has the size of ques of any size e is compared to he segment j sh j ] and SB[0][j ] from SP[0][j ] th SP and SB algorithm . The e and both segm dered list of pag become end st or page p0 respe is removed from WithMaxScore ) . ages the state a igned to a segm Since accordin d , these links h assigned to a n r this the array w segment clique espectively . In t segment 1 of p1 f 4 . The other l so SB has the v o all segments o hare at least thre become initial s he pages that are are inpu example inclu ments of p0 are a ges represents lev tates . The initia ectively . In each m the list and pro From the state ssociated with t ment cliques ng to 3.3(3 ) have to be new segment SB[i][j][k ] e must have the example 1 is bound to inks can be value 0 . of p0 . If the ee pages , the states of the e not shared . ut of the udes only a added to the vels that the al states are h iteration of ocessed ( line es with the the smallest the p page and all its s successively co d different pages an te erminates if no m multiple segmen a algorithm return c concerning their neighbors . Acc mputes the la nd removes all l segment clique nt cliques with ns the one wh placement in the cording to Sect . rgest clique o links that are par of a minimal siz h maximal size ose segments a e DOM tree .
3.3 the algorith of segments fro rt of that s menu ze of 3 is found are possible are most unifo hm om u . It . If the orm
Figure 6 : Fou s that form a cli ur sample pages gr raph ique in the web b
Figure 7 . Illust O One , multiple or g graph clique . F c contains a segme of SC or not , bec o a all possible comb s shows an exampl g graph . The menu m main menu with page p0 is the pa p c containing links T The segments are th hat has the value o page k and the to tration of the S r no segment c or each page o ent clique SC , a cause SC can be binations a greed le of four pages u in the upper le links to p0 p3 an age that is curre to p0 have to b e encoded as a t e 1 if the segme e value 0 otherw egmentCliqueF lique can be em of a web graph segment on that smaller than WC dy approach can p0 p4 that form eft corner of eac nd an additional l ently processed , be considered o hree dimensiona ent j on page i co wise . The segme m Finder algorithm mbedded in a w web h clique WC t hat part t page is either p C . To avoid testi ing n be used . Figur e 6 a clique in the w web ical ch page is a typi link in p1 . The fi first ents so only segme ges . on the other pag al array SP[i][j][ [ k ] ink ontains a hyperli ents on pages p1 p3 states , ntain links that can con revious pages w when pr segment cliques larger s d from the origi removed if the new cliqu clique i s the minimum contains ude a hyperlink o to inclu in Figure 6 only shown i clique , which another d to segment cliq assigned web graph clique Each w aph clique and th web gra t arrays SP[0][j ] segment hm by removing algorithm in The nitial tCliqueFinder a Segmen web graph clique single w tate list . The ord initial st have to pass to states h associat ted with level 0 orithm one state i the algo nction GetStateW 04 , fun al number of pa maxima selected . level is In Figur re 7 both initial ociated with lev are asso created that re state is the page of the contain re also created states ar the next t level ( line 13 ) . is removed from page it i ding of all links the bind o the state list it added to larger se egment cliques . 1 23 ) . If the seg ( lines 2 han 2 and the larger th to the list State added t dates the state consolid measure es the uniformity ( to fulfill 3.3(3 ) a state ( ted with the sam associat segment uniform higher s he DOM paths o align th attribute es that differ are ds indicated th wildcard ns in the page t position epresentation is visual re e state that is j 7(3 ) the segment ts are more unifo ates that have re New sta end states only i list of e represen nts a larger cliq smaller cliques these e algorithm hm terminates if ize than the curr clique si e highest uniform with the nd states were valid en of SC* in which cliques oving all links th by remo orithm has to be the algo d . be found l states contain vel 0 , so a rand epresents a seg next level ( Algo for each segme If such a segme m the target page s is updated ( lin is tested to see i If that is the ca gment clique rep state has not re es ( line 25 ) . Th list by applyin y of the page se ) ) . If an equal s me level already e mity is kept . To of all segments . e replaced by wi hat the segmen templates and i similar too . In th joined with SP form . eached the max f the list does n que . If there ar end states are r f no other end rent end states ca mity is then retur generated . For h the page is pa hat are bound by executed again u three pages and dom state is pick gment clique th orithm 1 , lines 1 nt of the page r ent does not con es of the state ( l ne17 ) . Before a n if links are alrea ase these links a presented by the eached the final he function Add ng a scoring fu egments that are state ( regarding exists only the st compute the un Node names , c ildcards . A lowe nts are placed it is more likel he example show P[2][0 ] is kept d both states ked . A new at does not 10 12 ) . New representing ntain a target line 16 ) and new state is ady bound to are removed state is still l level , it is dToStateList unction that included in SP and SB ) tate with the niformity we class or ider number of at similar ly that their wn in Figure because its ximal level are a not contain an en re end states th removed ( lines 2 states of at lea an be reached . T rned ( line 06 ) if the finding of art of , SP has to y the returned en until no more en added to the nd state that hat represent 26 30 ) . The ast the same The end state one or more all segment o be updated nd state and nd states can
WWW 2012 – LSNA'12 WorkshopApril 16–20 , 2012 , Lyon , France1029 Algorithm 1 : SegmentCliqueFinder
Input :
States ( initial states ) , SP ( target pages of segments ) , SB ( clique binding of segments )
Output : Maximal clique of page segments
01 : EndStates ← new List ; MaxEndScore ← 0 ; 02 : WHILE ( States.count > 0 ) 03 : NewStates ← new List ; 04 : S ← GetStateWithMaxScore(States ) ; 05 : IF ( ∑j S.pages[j ] < MaxEndScore ) 06 : RETURN BestState(EndStates ) ; 07 : NextLevel = S.level+1 ; 08 : States.remove(S ) ; 09 : IF ( S.level < M 1 ) SN ← S.copy( ) ; 10 : 11 : SN.pages[NextLevel ] ← 0 ; 12 : NewStates.add(SN ) 13 : 14 : 15 : 16 : 17 :
FOR(all segments k of page PNextLevel ) SN ← new State ; FOR(all Pj ) SN.pages[j ] ← min(S.pages[j ] , SP[NextLevel][k][j] ) ; SN.binding[j ] ← max(S.binding [ j ] , SB[NextLevel][k][j] ) ; SN.level ← NextLevel NewStates.add(SN ) ; FOR(all States SN in NewStates ) FOR(i = 0…M ) IF(SN.binding[i ] > ∑j SN.pages[j ] ) SN.pages[i ] ← 0 ; i ← 0 ; IF ( ∑j SN.pages[j ] > 2 ) IF ( SN.level < M 1 ) AddToStateList(States , SN ) ; ELSE IF ( ∑j SN.pages[j ] = MaxEndScore ) EndStates.add(SN ) ; ELSE IF ( ∑j SN.pages[j ] > MaxEndScore ) MaxEndScore ← ∑j SN.pages[j ] ; EndStates ← new List ; EndStates.add(SN ) ;
18 : 19 : 20 : 21 : 22 : 23 : 24 : 25 : 26 : 27 : 28 : 29 : 30 :
6 . EVALUATION To evaluate the proposed method we analyzed 10,000 pages downloaded from microsoftcom This domain was chosen because of its size and the diversity of the content and sub sites . Only pages targeting the US audience were indexed by testing for the substring “ en us ” in the URL . The web crawler retrieved the pages in breadth first order . Since we did not perform a full crawl of the domain and all neighboring pages are necessary for analyzing a page , a total number of 74,198 pages were downloaded . This overhead can be avoided if either a complete crawl of a domain is performed or the boundaries of the crawled space are defined in another way in advance . 6.1 Runtime Performance The algorithms proposed in this paper are low resource consuming . We were able to conduct the experiment with a single Pentium D , 3 GHZ machine equipped with 3 GB RAM . Running the BronKerbosch algorithm to enumerate the maximal local web graph cliques only required a mean execution time of 015ms For the SegmentCliqueFinder algorithm , we measured a mean execution time of 233ms Interestingly there were few pages that required a much longer processing time , up to a maximum of 371ms while for more than 87 % of the pages the execution took no longer than 2ms . We measured the number of input pages , the number of input segments , the number of input cliques and the number of output segment cliques . We found that the number of input cliques correlates most strongly with the execution time ( Figure 8 ) . The mean execution time seems to increase almost linearly with the number of input cliques . In our experiment there were very few pages with more than 100 cliques but this might be different in the general case . However , the algorithm is able to process a higher number of cliques in a reasonable time as the plot shows . If we assume a maximal complexity of the local web graph , the algorithm has a linear time complexity in the number of processed pages because all pages are analyzed independently .
Figure 8 . Number of input cliques vs . SegmentCliqueFinder runtime in ms
6.2 Web Site Boundaries To evaluate the reliability of the method and its potential for solving problems in the field of web structure mining we applied it to detect site boundaries . The result shows that the approach is more accurate than existing methods and that the shared menus found are exactly the site wide navigation systems as perceived by humans . 621 Definition and Approach Often a number of different web ( sub ) sites are hosted under the same domain . In the case of the domain microsoft.com there are for example the Windows site , the Office site , the MSDN site and many more . Identifying site boundaries is useful for processing crawled domains in many ways ( [4 ] , [ 5] ) . But it is also one of the tasks that are easy for humans but difficult for machines . According to Nielson [ 14 ] a sub site is defined by three characteristics : a . A common style b . A shared navigation mechanism c . An entry page
This definition was adopted by Rodrigues et al . [ 5 ] and the same criteria for defining “ site ” was used by Alshukri et al . [ 4 ] . Sites and sub sites refer essentially to the same concepts , except that a sub site is part of another larger site / sub site . The criterion b is obviously the strongest . It is hard to picture a site that does not have some kind of global menu that is shared by all pages . From the usability perspective we can argue that global menus are necessary to avoid navigational volatility leading to disorientation or at least forcing users to reorient [ 15 ] . From a technical
WWW 2012 – LSNA'12 WorkshopApril 16–20 , 2012 , Lyon , France1030 p perspective we c common and thi c can say that two o one menu . can argue that is includes glob o pages that belo today global p bal navigation te ong to the same page templates emplates . Thus e site share at le are we east yles within the s argue that two e site . The comm ws . The visual e4 is clearly di Also an entry pa any special char menu . the MenuMine y identifying clu same site . pages that share mon style criteri design of the tw fferent while th age can always racteristics , exc e a ion wo hey be ept r approach can usters of pages t be hat
9 . Different sty Figure On the other hand , we can a long to the same m menu always bel as Figure 9 show is s not decisive a MSDN website p pages from the global menu . A s share the same does not have a f found . It often d em in the shared b being the first ite considerations , B Based on these used to detect sit te boundaries by u s share an s menu . ment and Res 6 622 Experim A After executing the SegmentCli he extracted seg r epresented by t hat are linked by w with the pages th ent on many mor li ikely to be prese or the finding o o of main menu . F m menu , only thos se pages that lin idered because t h have to be consi entify an s men T To accurately id egment uniform f for measuring se es that the segm u used . This ensure in n the DOM tree and has similar e s menu segme tr ree paths of the h are very robu te emplates which pages that share o obtained sets of p of , eg , a glob d different scopes r etrieved a disjun nctive set of 58 d sets are a p s sets . The joined ur evaluation pro b boundaries as ou ages but we assu m menus on 591 pa c crawl . We placed d these pages in observed that S Since we have this domain v o organization of te segmentation c compare our sit n in this case the s showed that even fr from the URLs . So , we decided e evaluation . H However , we we g group 10,000 p r easonable time . epresentative , th r m much easier and a assessor and ask n not . By this met p pairs compared ere faced with th pages according Even with a s he task would d less error pro k him to decide hod we were ab by one person ults iqueFinder algo gment cliques a y this menu . Ob re pages , at least f all pages that c nk all pages of the links are def u on a candidat mity described in ment is located in style attributes . ents are like sig st because of th e an s menu over bal menu and a clusters by joini precise presenta oves . We were n ume that this due a single generic t the URLs re very well , we n to the folder e site boundaries d to conduct an nus nly u is ind n sque nu . hod be ion OM enu The the We ing Site t slete rithm the s men are associated on bviously a menu t , if it is some ki contain a certain the s menu cliq fined by the men e page the meth n Sect . 5.2 can n a similar positi The aligned DO gnatures for me he wildcards . T rlap because of a local menu . W ing all overlappi ation of the S not able to detect e to the incompl cluster . eflect the cont were planning r structure . But s cannot be deriv additional manu ent to t it ved ual he problem of h g to their site subset that is la be difficult for one to present if both belong t ble to have 845 in approximate having to manua membership in arge enough to an assessor . It two pages to to the same site randomly selec ely 10 hours . T ally n a be t is an or ted The
4 http://msdn . microsoft.com/en
us person w evaluati checked reported The met agreeme and X2 t all tuple the tupl both par the set agreeme was given no fu ion . We gave the d if the linked d that he was abl tric we used is b ent of two partit two partitions of es of elements in les of two elem rtitions or are in of disagreemen ents : urther informatio e hint that in cas d homepages ar le to decide in al based on the Ra tions [ 16 ] . Let S f S . Let ( cid:1829)= {((cid:1876 ) n ( cid:2870 ) . A is the set = ( cid:3002)(cid:3002)(cid:3005 ) ( cid:3005 ) ments that are ei n different clust nts . The Rand in on about the pur e of uncertainty re identical . T ll cases without a and index that m be a set of elem
( cid:1876),(cid:1876)(cid:2870))∈(cid:3400)} of agreements t ither in the sam ters in both part ndex measures rpose of the it should be he assessor ambiguity . measures the ments and X1 be the set of that contains me cluster in titions . D is the ratio of
Instead U of C f disagree clusters that the be even Three o were an level of subdom clusters for the extracte includin RU drop of considering a from our evaluat ement set DU are we computed a remaining error lower for a com other clustering m nalyzing the hie f the hierarchy w mains were split u again by the se segmentation b ed . This proves t ng the folder stru ps to 0.95 and 0.8 all tuples we we ation for which th e known as sam a sample rand in r rate results from mplete crawl . methods were u erarchical structu was given by s up again by the f econd folder . An by subdomains that subdomains ucture , the numb 87 ( Figure 10 ) . ere using the ran he agreement se mple . For our par ndex RU of 0.996 m generic cluste ndom subset et AU and the rtition of 58 6 . It is likely er and would used as benchma ure of the URL separating subdo first folder and t n RU of 0.97 wa but only 9 cl s contain multip ber of clusters in ark . First we Ls . The first omains . The the resulting as computed lusters were ple sites . By ncreases but
Figu ure 10 : Rand in samples for d re also impleme on that perform ng k means algor are large enough ive to the Men has to be know e did not include th uated in [ 4 ] too , be best individual fea valuated on manually ev ds entation method te boundary hods for web sit ent [ 4]5 . A recent experim ed iteratively to s split clusters not really an this method is n e number of hm because the we wanted to see e if a similar ndex RU based o different segme enting the meth med best in a r rithm was applie h . Even though nuMiner algorith wn in advance , w f eight features tha he combination of perform significant ecause it did not p atures in the report ed experiments . at was tly better than
We wer detectio bisectin if they a alternati clusters
5 We evalu the b
WWW 2012 – LSNA'12 WorkshopApril 16–20 , 2012 , Lyon , France1031 high value for RU can be reproduced . As features the internal hyperlinks were used and the bag of words obtained by separating URLs with the delimiters “ . ” and “ / ” . The hyperlink feature performed better with a maximal RU of 0.92 for 26 clusters compared to a maximal RU of 0.86 for four clusters achieved by using the URL word feature . With these methods we were not able to reproduce an RU as high as the one computed for the MenuMiner partition . The results showed the superiority of the proposed approach when applied to detect site boundaries . But even more important , the almost perfect agreement with the human assessor relies on the very good performance of the MenuMiner algorithm in detecting the main navigation systems of a site . This shows the reliability of the method in general . 6.3 Site Analysis Without further analyzing the s menus a basic hierarchical structure is already revealed by the relations of the clusters defined by the s menus . If A and B are sets of pages that share an s menu and A ⊂ B while A ≠ B , then A defines a subsection of a site .
Figure 11 shows the discovered relations together with the sizes of the clusters . The connected components represent sites . One interesting observation is that sites that consist of few , smaller subsections are design oriented sites focused on product presentation , eg , the sites identified by the root clusters J ( Microsoft Windows site ) , K ( Windows Phone site ) and M ( Visual Studio site ) . Large clusters contain resources that cannot be accessed by a single hierarchical structure but can be so by facetted search such as the Pinpoint marketplace ( A – cf . Table 1 ) and the template library of the Microsoft Office site ( B ) . The small uniform nodes under the same root as A are compound documents [ 6 ] that consist of three sections ( Figure 12 ) . These first observations are meant to illustrate the structural information gained with the proposed method based on clustering with s menus . A further analysis of the data including the s menu items promises a more fine grained model of the information architecture .
Figure 12 . Three section of a compound document ( L nodes in
Figure 11 ) 6.4 Main Menu Detection Web site boundary detection is only a first application of our method that we have chosen as a demonstration and to evaluate the method . The s menus delivered by the algorithm provide important information about the site ’s content organization and its menu structure . As a demonstration we have listed all extracted items of the main menu for the 10 largest clusters found in Table 1 . Since the site clusters are defined by overlapping s menus , the s menu that is shared by the largest number of pages in each site can be considered as main navigation . If there were multiple s menus that are shared by the same pages we were selecting one page and picked the s menu that was first found when traversing the DOM tree in depth first order . As Table I shows all main menus , and thus the main categories , were extracted correctly for the listed sites .
Table 1 . Menu items of the then largest clusters Menu Items
Entry Page
A
B
C
#P . 4060
2313
316
D
227
E
164 pinpointmicrosoftcom/enUS/applications/search?q= officemicrosoftcom/en us msdnmicrosoftcom/enus/default technetmicrosoftcom/enus/default windowsmicrosoftcom/enUS/windows vista/help
Applications , Professional Services , Companies home , products , support , images , templates , downloads , more Home , Library , Learn , Samples , Downloads , Support , Community , Forums Home , Library , Wiki , Learn , Downloads , Support , Forum , Blogs Windows Vista Help home , Top solutions , Using Windows Vista , Getting started , Community & forums , Contact support Windows 7 Help home , Getting started , Top solutions , How to videos , Community & forums , Contact support Home , Library , Learn , Downloads , Gallery , Support , Community , Forums Home , 2008 , 2003 , 2000 , Library , Forums
Windows 7 home , What is Windows 7 ? , Compare , Features , Videos Home , Explore Windows , Products , Shop , Downloads , Help & How to
F
137 windowsmicrosoftcom/enus/windows7/help
G
89
H
81
I
J
79
70 msdnmicrosoftcom/enus/windows/aa904944aspx technetmicrosoftcom/enus/windowsserver/ bb250589.aspx windowsmicrosoftcom/enUS/windows7/products/ home http://windowsmicrosoftco m/en US/windows/home
Figure 11 . Clusters of sites and subsites
7 . RELATED WORK Depending on the objectives two different research directions can be distinguished in the field of web structure mining . The first direction aims at generating new structures as rankings or topic hierarchies based on web documents and their structure . Algorithms such as PageRank or HITS and their variations belong to this direction as well as approaches that cluster web documents based on their content . The research presented in this paper belongs to a second research direction that aims at mining existing
WWW 2012 – LSNA'12 WorkshopApril 16–20 , 2012 , Lyon , France1032 structures that are difficult to retrieve as navigational hierarchies or boundaries of web sites . Mining Navigation Elements Some work has been done on mining navigation elements . Li and Kit describe an approach in [ 17 ] that is based solely on the web graph . Frequent item set mining algorithms are applied on the sets of outgoing hyperlinks of the pages to detect repeated menus . However a more comprehensive evaluation of the approach would be interesting . The work of Rodrigues et al . [ 5][8 ] on mining link blocks for representing sites and finding site boundaries has been described above . However the authors do not evaluate how well the page segmentation into link blocks really reproduces the navigation elements as perceived by humans . The ratio of linked text to all text is a common method for recognizing navigation elements or link lists [ 1][18 ] while in [ 19 ] other metrics as the text length and the hyperlink targets are used . The performance of these metrics is not reported . The work presented in [ 20 ] uses several other metrics to find navigation elements on page level . An interpage analysis to find repeated navigation structures is not included . An evaluation was conducted showing a high recall and moderate precision . We used the link text ratio criterion in combination with other metrics in our previous work which included an inter page analysis [ 7 ] . The evaluation showed that achieving a high accuracy with this approach is difficult . There are some approaches that do not mine navigation elements explicitly but do take into account the structural information they provide . For instance , the clustering method described in [ 3 ] considers “ parallel links ” – links that are siblings in the DOM tree of a page . Such links are likely to belong to the same navigation element . Clustering and Hierarchy Detection Several approaches in this direction work solely on the web graph model , whose vertices represent pages and whose arcs represent hyperlinks . One of these is based on hyperlink co citation for web clustering as introduced by Pitkow and Pirolli [ 21 ] . It is based on the idea that hyperlinks that frequently occur on a page together point resources . Other approaches computed additional edge weights for the web graph in order to improve the clustering results . Extracting a hierarchical structure with standard graph algorithms based on the web graph is described in [ 1 ] . The edge weights are computed by machine learning methods that distinguish two link types based on eight link features . In [ 22 ] the edge weights for the web graph are computed based on text similarity and co citation of hyperlinks . Three algorithms ( k means , multilevel METIS and Normalized Cut ) are evaluated to partition up to 3500 documents . Normalized Cut performs best in the evaluation , but the objectives of the experiments are not the detection of Site boundaries but the clustering of topically related documents . Other web graph clustering methods consider hyperlink transitivity to compare pages that are not connected by a direct hyperlink ( eg [ 23] ) . An evaluation of the performance of four clustering algorithms in conjunction with several different features aside from the web graph is described in [ 4 ] . The features include word co occurrences of the complete text as well as of the titles , hyperlinks , script links and the URLs that are split into components using delimiters as “ . ” and “ / ” . A bisecting k means algorithm on the URL components performs best . Instead of using a clustering algorithm , the site segmentation can be retrieved directly from the hierarchical structure of URLs . Using the hierarchical structure of URLs seems to semantically related to be a very common approach ( used eg in [ 1],[2],[24 ] ) but it was not evaluated in [ 4 ] . However , it is well known that the hierarchical structure of URLs does not reflect the Site organization accurately [ 2 ] . An interesting approach of hierarchical web site segmentation is presented in [ 2 ] . The algorithm requires an existing tree structure on the resource , eg , retrieved from the URL hierarchy and knowing the class ( topic ) of each resource . The tree is segmented into topically cohesive regions , representing subsites . Also in the end a similar problem is addressed , the approach is very different from the work presented in this paper , which does not require a given classification and hierarchy . Evaluation The existing work on web pages shows that the evaluation of clustering methods for finding site boundaries and intra site structures is a challenge . The main problem is that a reasonably large data set is necessary for meaningful results but the results have to be evaluated manually . In [ 4 ] four sets of pages from different university departments are used , each representing a site and consisting of 500 pages . Thus the number of Sites is low and they are selected in advance what might bias the results . In [ 5 ] Rodrigues et al . describe an evaluation method that does not measure the aggregation of pages to sites but the precision and recall of detected entry pages . This allows considering sites as well as sub sites . They compare five methods , two of which are based on their own approach . Although the results are mixed and no method achieves a high F measure , the authors show that their approach is able to detect entry pages that are not found by other methods . In the experiments in [ 3 ] and [ 24 ] a large number of pages are clustered , but no metrics are used for evaluating the clustering quality . Instead the resulting clusters themselves are listed in tables and figures . 8 . CONCLUSION We believe that the MenuMiner method proposed in this paper is a contribution that opens new doors for analyzing the structure of domains and sites . The algorithm is fast and its time complexity is linear in the number of pages . It is solely based on analyzing the HTML structure and no additional resources such as CSS style sheets are required . A visual model is not necessary for identifying the s menus of a page . The evaluation shows that the approach allows identifying with high precision the main menu systems that are a common characteristic of all pages of a site and that represent its central organization scheme . Applied to the problem of site boundary detection the presented approach provides almost perfect results in contrast to other current methods . The data obtained in the experiment also gave interesting information about the concepts of content access a site implements , based on which the site can be classified . In our experiment it also allowed the identification of compound documents . The focus of the experiment and evaluation described was to show the reliability of the MenuMiner method . We found that the method is a very solid foundation that is ready to be applied in practice . Thus further research can be done on the interpretation and processing of the obtained data . S menus can be considered as the structural skeleton of web sites . We believe that it is possible to retrieve the complete content hierarchy of web sites based on this skeleton with high precision . This would close the gap between the human perception of a site ’s content structure and the model generated by current structure mining methods . It would bring
WWW 2012 – LSNA'12 WorkshopApril 16–20 , 2012 , Lyon , France1033 improvements in many areas as eg the representation of search results , ranking , automated usability testing or web site reverse engineering . 9 . REFERENCES [ 1 ] C . C . Yang and N . Liu , “ Web site topic hierarchy generation based on link structure , ” Journal of the American Society for Information Science and Technology , vol . 60 , no . 3 , pp . 495508 , 2009 .
[ 2 ] R . Kumar , K . Punera , and A . Tomkins , “ Hierarchical topic segmentation of websites , ” in Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining , Philadelphia , PA , USA , 2006 , pp . 257 266 .
[ 3 ] C . Lin , Y . Yu , J . Han , and B . Liu , “ Hierarchical Web Page Clustering via In Page and Cross Page Link Structures , ” in Advances in Knowledge Discovery and Data Mining , vol . 6119 , Springer Berlin / Heidelberg , 2010 , pp . 222 229 .
[ 4 ] A . Alshukri , F . Coenen , and M . Zito , “ Web site boundary detection , ” in Proceedings of the 10th industrial conference on Advances in data mining : applications and theoretical aspects , Berlin , Germany , 2010 , pp . 529 543 .
[ 5 ] E . Mendes Rodrigues , N . Milic Frayling , and B . Fortuna , “ Detection of Web Subsites : Concepts , Algorithms , and Evaluation Issues , ” in Proceedings of the 2007 IEEE/WIC/ACM International Conference on Web Intelligence , 2007 , pp . 66 73 .
[ 6 ] N . Eiron and K . S . McCurley , “ Untangling compound documents on the web , ” in Proceedings of the fourteenth ACM conference on Hypertext and hypermedia , Nottingham , UK , 2003 , pp . 85 94 .
[ 7 ] M . Keller and M . Nussbaumer , “ Beyond the Web Graph : Mining the Information Architecture of the WWW with Navigation Structure Graphs , ” in Proceedings of 2nd International Conference on Emerging Intelligent Data and Web Technologies ( EIDWT 2011 ) , Tirana , Albania .
[ 8 ] Eduarda Mendes Rodrigues , Natasa Milic Frayling , Martin
Hicks , and Gavin Smyth , “ Link Structure Graphs for Representing and Analyzing Web Sites , ” Microsoft Research , Technical Report MSR TR 2006 94 , Jun . 2006 .
[ 9 ] K . Makino and T . Uno , “ New Algorithms for Enumerating All Maximal Cliques , ” in Algorithm Theory SWAT 2004 , vol . 3111 , T . Hagerup and J . Katajainen , Eds . Springer Berlin / Heidelberg , 2004 , pp . 260 272 .
[ 10 ] D . Cai , S . Yu , J R Wen , and W Y Ma , “ Extracting content structure for web pages based on visual representation , ” in Proceedings of the 5th Asia Pacific web conference on Web technologies and applications , Xian , China , 2003 , pp . 406417 .
[ 11 ] G . Hattori , K . Hoashi , K . Matsumoto , and F . Sugaya , “ Robust web page segmentation for mobile terminal using contentdistances and page layout information , ” in Proceedings of the
16th international conference on World Wide Web , Banff , Alberta , Canada , 2007 , pp . 361 370 .
[ 12 ] “ Document Object Model ( DOM ) Level 2 Core Specification ,
Version 10 W3C Recommendation 13 November , 2000 . ” [ Online ] . Available : http://wwww3org/TR/2000/REC DOMLevel 2 Core 20001113/
[ 13 ] C . Bron and J . Kerbosch , “ Algorithm 457 : finding all cliques of an undirected graph , ” Commun . ACM , vol . 16 , no . 9 , pp . 575 577 , 1973 .
[ 14 ] J . Nielsen , “ Subsite Structure ( Alertbox ) . ” [ Online ] . Available : http://wwwuseitcom/alertbox/9609html [ Accessed : 24 May 2011 ] .
[ 15 ] D . R . Danielson , “ Transitional volatility in web navigation , ”
Information Technology and Society , 1(3),Special Issue on Web Navigation , pp . 131–158 , 2003 .
[ 16 ] W . Rand , “ Objective Criteria for the Evaluation of Clustering
Methods , ” Journal of the American Statistical Association , vol . 66 , no . 336 , pp . 846 850 , 1971 .
[ 17 ] C . Chui and C . Li , “ Navigational Structure Mining for Usability Analysis , ” in Proceedings of the 2005 IEEE International Conference on e Technology , e Commerce and e Service ( EEE’05 ) , 2005 , pp . 126 131 .
[ 18 ] S . Gupta , G . Kaiser , D . Neistadt , and P . Grimm , “ DOM based content extraction of HTML documents , ” in Proceedings of the 12th international conference on World Wide Web , Budapest , Hungary , 2003 , pp . 207 214 .
[ 19 ] J . Chen , B . Zhou , J . Shi , H . Zhang , and Q . Fengwu ,
“ Function based object model towards website adaptation , ” in Proceedings of the 10th international conference on World Wide Web , Hong Kong , Hong Kong , 2001 , pp . 587 596 .
[ 20 ] Z . Liu , W . K . Ng , and E P Lim , “ An Automated Algorithm for Extracting Website Skeleton , ” in In Proceedings of the 9th International Conference on Database Systems for Advanced Applications ( DASFAA 2004 ) , Jeju Island , Korea , 2004 , pp . 799–811 .
[ 21 ] J . Pitkow and P . Pirolli , “ Life , death , and lawfulness on the electronic frontier , ” in Proceedings of the SIGCHI conference on Human factors in computing systems , Atlanta , Georgia , United States , 1997 , pp . 383 390 .
[ 22 ] X . He , H . Zha , C . HQ Ding , and H . D . Simon , “ Web document clustering using hyperlink structures , ” Computational Statistics & Data Analysis , vol . 41 , no . 1 , pp . 19 45 , Nov . 2002 .
[ 23 ] J . Hou and Y . Zhang , “ Utilizing hyperlink transitivity to improve web page clustering , ” in Proceedings of the 14th Australasian database conference Volume 17 , Adelaide , Australia , 2003 , pp . 49 57 .
[ 24 ] W . K . Cheung and Y . Sun , “ Identifying a hierarchy of bipartite subgraphs for web site abstraction , ” Web Intelli . and Agent Sys . , vol . 5 , no . 3 , pp . 343 355 , 2007 .
WWW 2012 – LSNA'12 WorkshopApril 16–20 , 2012 , Lyon , France1034
