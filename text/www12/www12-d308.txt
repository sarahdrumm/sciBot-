Sentiment Classification via Integrating Multiple Feature
Presentations
Yuming Lin , Jingwei Zhang , Xiaoling Wang , Aoying Zhou
Institute of Massive Computing East China Normal University
200062 Shanghai , China
{ymlinbh , gtzhjw}@gmail.com , {xlwang , ayzhou}@seiecnueducn
ABSTRACT In the bag of words framework , documents are often converted into vectors according to predefined features together with weighting mechanisms . Since each feature presentation has its character , it is difficult to determine which one should be chosen for a specific domain , especially for the users who are not familiar with the domain . This paper explores the integration of various feature presentations to improve the classification accuracy . A general two phases framework is proposed . In the first phase , we train multiple base classifiers with various vector spaces and use these classifiers to predict the class of testing samples respectively . In the second phase , the previous predicted results are integrated into the ultimate class via stacking with SVM . The experimental results demonstrate the effectiveness of our method .
Categories and Subject Descriptors I27 [ Natural Language Processing ] : Text Analysis Sentiment Analysis
General Terms Algorithms , Experimentation
Keywords sentiment classification , multiple feature presentations , base classifiers , integrating
1 .
INTRODUCTION
More and more people prefer to write reviews and share their opinions on e commerce sites and microblogging . Determining the overall sentiment of such user generated data is valuable to many applications like recommendation systems , business and government intelligence , etc . In the bag of words framework , the documents are often converted into vectors based on predefined feature presentation including feature type and features weighting mechanism , which is critical to classification accuracy . The major feature types contain unigrams , bigrams and the mixtures of them , etc . The features weighting mechanism mainly includes presence , frequency , tf*idf and its variants . Pang et al . [ 4 ] have compared the performances of different learning methods , and reported that the unigram presence with SVM is superior
Copyright is held by the author/owner(s ) . WWW 2012 Companion , April 16–20 , 2012 , Lyon , France . ACM 978 1 4503 1230 1/12/04 . to the others . Our preliminary experiments ( See Table 1 for more details ) show that none of the feature presentations can always outperform the others for all domains . Thus , it is a challenge to determine which feature presentation should be used , especially for the users who aren’t familiar with the analyzed domains .
In this paper we put forward a general framework to integrate multiple base classifiers for a binary task of classifying sentiment into positive and negative classes . The base classifiers are trained by samples expressed using different feature presentations . Our solution consists of two phases : the training phase and the integrating phase . In the first phase , multiple feature presentations are applied on modeling the samples , and the base classifiers are trained by these transformed samples . In the second phase , we integrate the predictions generated by base classifiers with assembling technology such as voting and stacking[5 ] . Compared with the traditional classification with single feature presentation , our framework offers two major advantages : first , we don’t need to determine which feature presentation should be chosen for classification tasks ; second , the accuracy of our framework is always higher than that of single feature presentation integrated . In our framework , the task of training base classifiers can be processed in parallel easily .
2 . THE PROPOSED METHOD
The n training samples are converted into n vectors according to m feature presentations . Let Xi = ( xi n)T denote such a vectors space using the ith feature presentation , where xi j is the row vector of the jth training sample . lsk is the kth sample ’s class . Yi = ( yi r)T is similar to Xi , excepting y denotes the row vector of testing sample . Our framework includes the following steps :
1 , . . . , yi
1 , . . . ,xi
1 . Converting training samples into m vector spaces with m feature presentations , and using m them to train m base classifiers respectively;(Line 2–4 in Algorithm 1 )
2 . training a integrating classifier based on the predicting results of m base classifiers ; ( Line 5–9 )
3 . Using m base classifiers to predict the classes of r testing samples independently , generating m predicting results for each testing sample;(Line 12–13 )
4 . For each test sample , the predicting results of m base classifiers are integrated into one ultimate result by stacking with SVM . ( Line 14 )
WWW 2012 – Poster PresentationApril 16–20 , 2012 , Lyon , France569 In Algorithm 1 , the parameter k refers to the k fold cross validation procedure is used to generate meta level training samples for the integrating classifier ˜c .
Algorithm 1 . Sentiment Classification by Stacking Input:the training vector set X ={X1,ls1fi , . . ,.Xn,lsnfi} , the testing vector set Y = {Y1 , . . . , Yr} , k for i =1 to k do X i meta = X i generate the base classifier set Ci on X − X i C = C ∪ {Ci} ; for j =1 to n/k do
Output:the ultimate predicted result set L = {l1 , . . . , lr} 1 : meta ∪ {n/k samples never chosen in X} ; 2 : 3 : 4 : 5 : 6 : 7 : 8 : 9 : 10 : select the Ch with the highest accuracy in C ; 11 : for i = 1 to r do 12 : 13 : 14 : 15 : return L ;
//m feature presentations //ct ∈ Ci , . . . , .lm sj for t=1 to m do j ) ; Tmeta = Tmeta ∪ {l1 train the integrating classifier ˜c on Tmeta ; for j =1 to m do i = cj ( yj .lj i ) ; L = L ∪ {˜c(l1 i , . . . , .lm i fi)} ;
//cj ∈ Ch meta ;
.lt sj = ct(xt sj fi , lsjfi} ;
Table 1 : The classification accuracy ( % ) using different feature presentations uni f B 79.90 D 79.15 E 83.80 K 84.95 bi f 75.75 80.45 79.80 74.00 83.85 79.90 80.40 86.65 uni+bi p Δtf*idf uni+bi f uni p bi p 79.50 75.55 74.45 80.25 80.45 85.15 86.50
78.25 78.60 83.10 87.00 80.55
79.85 83.00 84.40 86.20
)
% ( y c a r u c c A
90
89
88
87
86
85
84
83
82
81
80
79
78
SFP weighted voting stacking with SVM
89.35 %
88.75 %
88.20 %
86.60 %
87.00 %
85.15 %
83.95 %
83.80 %
83.15 % 83.00 %
81.95 %
80.45 %
B
D
E
K
Figure 1 : Comparison on accuracies
3 . EXPERIMENT
To validate the effectiveness and robustness of proposed method , a real world dataset reorganized by Blitzer et al.[1 ] is prepared for experiments , which consists of reviews of books ( B ) , DVDs ( D ) , electronics ( E ) and kitchen appliances ( K ) from Amazon . Each domain contains 1000 positive and 1000 negative reviews . Five fold cross validation is applied . The LIBSVM [ 2 ] with a linear kernel is used to implement SVM algorithm . We focus on three main feature types ( unigrams , bigrams and the mixture of them ) and feature weighting mechanisms ( frequency , presence and Δtf*idf [ 3] ) . Multiple feature presentations can be integrated in our framework , but in our experiments we only consider seven combinations described in Table 1 , where “ uni f ” refers to the unigrams with frequency , “ bi p ” to bigrams with presence and “ Δtf*idf ” to unigrams with Δtf*idf , and so on . 3.1 Preprocess
All punctuation marks and the terms occurring less than 5 times are eliminated but the stop words retained . Similar to [ 4 ] , we remove the negatory words and append the tag “ NOT ” to the words following the negatory word in the sentence . For instance , the sentence “ It doesn’t work smoothly . ” would be altered to become “ It NOT work NOT smoothly . ” . 3.2 Results and Discussion
Table 1 reports the classification accuracies using various feature presentations separately in different domains . The accuracy shown in bold font corresponds to the optimal feature presentation in the corresponding domain . For example , the bold 87.00 means the unigrams with presence achieves the highest accuracy in kitchen appliances domain . These results verify our motivation , that is no one feature presentation can outperform the others in all domains . Thus we propose a framework to integrate multiple feature presentations for improving the classification accuracy .
The single feature presentations ( SFP ) with highest accuracy in each domain are treated as baselines . The weighted voting scheme is a simple way to assembling multiple classifiers , and we treat it as a competitor of the stacking with SVM method . Figure 1 illustrates the comparison of above three methods . For sentiment classification in various domains , on the one hand , the proposed method outperforms the single feature presentation used , on the other hand the stacking with SVM is superior to the weighted voting .
4 . ACKNOWLEDGMENTS
This work was supported by the 973 project ( No . 2010CB3 28106 ) , NSFC grant ( No . 61170085 and 60925008 ) , Program for New Century Excellent Talents in China ( No . NCET 100388 ) and Innovation Program of Shanghai Municipal Education Commission .
5 . REFERENCES [ 1 ] J . Blitzer , M . Dredze , and F . Pereira . Biographies , bollywood , boo boxes and blenders : Domain adaptation for sentiment classification . In ACL , pages 440–447 , June 2007 .
[ 2 ] C C Chang and C J Lin . Libsvm : A library for support vector machines . ACM Transactions on Intelligent Systems and Technology , 2:27:1–27:27 , 2011 .
[ 3 ] J . Matineau and T . Finin . Delta tfidf : An improved feature space for sentiment analysis . In ICWSM , pages 258–261 , May 2009 .
[ 4 ] B . Pang , L . Lee , and S . Vaithyanathan . Thumbs up ? sentiment classification using maching learning technique . In EMNLP , pages 79–86 , July 2002 .
[ 5 ] B . ˘Zenko , L . Todorovski , and S . D˘zeroski . A comparison of stacking with mdts to bagging , boosting , and other stacking methods . In ICDM , pages 669–670 , December 2001 .
WWW 2012 – Poster PresentationApril 16–20 , 2012 , Lyon , France570
