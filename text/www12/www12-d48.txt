Group Recommendations Via Multi armed Bandits∗
José Bento∗ Stratis Ioannidis† S . Muthukrishnan‡ Jinyun Yan‡
∗Stanford University †Technicolor
†stratisioannidis@technicolorcom ,
‡Rutgers University
‡{muthu,jinyuny}@csrutgersedu
∗jbento@stanford.edu ,
ABSTRACT We study recommendations for persistent groups that repeatedly engage in a joint activity . We approach this as a multi arm bandit problem . We design a recommendation policy and show it has logarithmic regret . Our analysis also shows that regret depends linearly on d , the size of the underlying persistent group . We evaluate our policy on movie recommendations over the MovieLens and MoviePilot datasets . Categories and Subject Descriptors H3.3 [ Information Search ] : Recommendation—Information Storage Retrieval General Terms Algorithm , Design , Experimentation Keywords Group Recommendation , Multi armed Bandits 1 Humans are social beings and no person is an island unto themselves . People engage in activities as part of one or more groups . At home , families watch TV or plan movie nights , restaurant evenings , day trips , jointly as a group . At work , colleagues plan lunches , weekly socials , off site retreats , and so on , also as a group . In fact , seldom is a person alone in many of their daily activities .
Introduction
While the problem of providing recommendations to individuals is popular both in research and in applications , providing recommendations to groups of individuals has been much less explored . One important aspect one needs to understand is how group membership manifests over time . Our focus is on persistent groups , that is , those whose members are bound together due to some purpose and meet regularly . Examples include families , bands of friends , teams at work and so on . Neverthless , not all members of even a persistent group will be present for each of the group activities . People have conflicting appointments , medical time off , etc .
Our contributions are as follows . First , we formalize the problem of recommendation to persistent groups as a suitable Multiarmed bandit ( MAB ) problem and extend the theory of minimum regret MAB policies . In particular , we design a MAB policy for this problem , Group UCB , and prove a logarithmic upper bound on the regret under suitable conditions . Second , we adapt GroupUCB to a concrete setting of recommending movies . We study a real data set from MovieLens which provides users’ movie ratings and movie genres . 2 Group Multi armed Bandit 2.1 Problem Formulation We formulate group recommendation as a MAB problem as follows . Let t = 1 , 2 , denote the series of times when recommendations are made . Let G be the persistent group of d = |G| users .
∗This material is based upon work supported by the National
Science Foundation under Grant No . 0916782
Copyright is held by the author/owner(s ) . WWW 2012 Companion , April 16–20 , 2012 , Lyon , France . ACM 978 1 4503 1230 1/12/04 .
At any time t , S(t ) ⊂ G is the set of users that are present . Further , each user u has a weight xu(t ) at time t . The vector x(t ) takes values in a finite set X ⊂ Rd + and models users who are not present ( set xu(t ) = 0 ) as well as the relative influence of a user on the overall group ’s satisfaction . Our focus is on designing an algorithm to recommend one of the objects at each time t . Each user u ∈ S(t ) will provide an individual rating ru(t ) ∈ [ 0 , 1 ] for the recommended object ; ru(t ) is a random variable . We seek a recommendation algorithm that maximizes the expectation of u xu(t)ru(t ) , over t . Denote by A , with |A| = K , the set of all possible genres a movie can belong to , for instance , “ horror ” , “ comedy ” , etc . We assume that , conditioned on a movie ’s genre being a ∈ A , the expected value of the rating given user u ∈ G is θu,a ∈ [ 0 , 1 ] . These conditional expectations θu,a consitute a vector θa = [ θu,a]u∈G ∈ [ 0 , 1]d determining every user ’s expected reaction towards genre a . Under this notation , if a movie from genre a ∈ A is suggested and viewed by the users , the expected group rating is given by the inner product x(t ) , θa = u∈G xu(t)θu,a .
2.2 Group UCB Algorithm 1 Group UCB nu ← 0 ; nu,a ← 0 ; ¯θu,a ← 0 for t = 1 to T do
Observe present users S(t ) and weight vector x(t ) for a=1 to K do for u =1 to d do if nu,a = 0 then pu,a ← ∞ else pu,a ← ¯θu,a +2 ln nu/nu,a pa ← u xu(t ) · pu,a end for end for choose arm a ← arg max apa ( break ties arbitrarily ) observe rating ru from each user u ∈ S(t ) nu,a ← nu,a + 1 for all u ∈ S(t ) nu ← nu + 1 for all u ∈ S(t ) ¯θu,a ← ¯θu,a · ( nu,a − 1)/nu,a + ru · 1/nu,a end for the empirical average ¯θu,a(s ) = s
A detailed description of the policy we propose can be found in Algorithm 1 . In short , the recommender maintains estimates of the quantities θu,a , for all u ∈ G and a ∈ A . If u has rated movies from genre a for s times so far , the estimate of θu,a is τ =1 ru(τ )/s where ru is the rating user u gave to τ th movie from genre a . Moreover , the recommender keeps track of how many times a user has participated in the activity and a particular genre has been displayed : it keeps track of nu,a(T ) , the number of times that u has been present and a movie from genre a has been suggested upto session T , as well as nu(T ) , the number of times that u has been present upto session T . Using the above quantities , the recommender selects a genre as follows . At the t th session , the recommender first observes the present composition of group S(t ) and the present weight vector x(t ) . The genre a selected is the one maximizing u∈G xu(t),¯θu,a +2 ln nu/nu,a
. Subsequently , the recom mender suggests a movie from that genre to the users in S(t ) ; the latter react by providing the recommender with ratings , which are
WWW 2012 – Poster PresentationApril 16–20 , 2012 , Lyon , France463 then used to update the estimates ¯θu,a for the arm a = a(t ) and for users u ∈ S(t ) . 2.3 An Upper Bound Given a vector x ∈ X , an optimal genre a∗(x ) is a genre ie , a∗(x ) = that maximizes the expected group rating , arg maxa∈Ax , θa . Given a policy {a(t)}T t=1x(t ) , θa∗(x(t ) ) −T T t=1 , we define the regret of the recommender after T sessions to be R(T ) = t=1x(t ) , θa(t ) , where a∗(x(t ) ) is a genre that is optimal at time t . The regret under Group UCB can be bounded according to the following theorem .
THEOREM 1 . Given x ∈ X , denote by Bx ⊂ A the set of suboptimal genres under x , ie , Bx = {a ∈ A : x , θa∗(x ) > x , θa} Moreover , let ∆a min = inf x∈X :a∈Bxx , θa∗(x)−x , θa . Then , under Group UCB ,
R(T ) ≤ a∈A
8M 3 1 d min)2 ln T + 4KdM1 . ( ∆a
1 /∆a
The bound in Theorem 1 holds for arbirtrary sequences x(t ) ∈ X : irrespectively of which subsets of users show up , and their ratings are weighted , the regret is logarithmic . Compared to the bound of the regret of UCB for the classic bandit problem appearing in min . [ 1 ] , Theorem 1 differs by a multiplicative factor of dM 3 It can be shown that the leading coefficient must be Ω(d ) and , in this sense , the bound is tight in d . The constant M1 is a bound on the sum of weights of all users . For practical purposes , this min capture the gap ought to be small . Moreover , the constants ∆a in expected group rewards between optimal and suboptimal arms . Similar quantities also appear in the bound of [ 1 ] . 3 Experimental Evaluation We evaluated Group UCB on two datasets , MovieLens [ 2 ] and MoviePilot [ 3 ] . For brevity , we only present results for MovieLens . MovieLens Dataset.TheMovieLens dataset consists of 1 000 209 ratings , given by 6 040 users to 3 883 movies . Ratings of a movie range from 1 to 5 . Movies in the dataset are labeled by genres , such as “ Animation ” , “ Children ’s ” , etc . Before simulating Group UCB , we construct a low rank approximation of the MovieLens dataset ; we subsequently use it to predict movie ratings for arbitrary movieuser pairs—ie , perform matrix completion .
Simulation Setup and Evaluation . We simulate Group UCB on two types of groups in MovieLens dataset : ( a ) random groups with size up to 10 , ( b ) a special group of users with shared profile . At each iteration t , the subgroup S(t ) is selected uniformly at random from non empty subset of G . At each session , if Group UCB selects genre a , a movie selected uar among movies tagged with label a is displayed to the present group . The reaction of a user is then the rating that she provided , if the latter is in the dataset ; otherwise , the rating predicted by our low rank approximation model is used .
To form the group , we randomly pick 10 users from the dataset , then build 10 groups by adding one more user to the group each time . Figure 1(a ) plots the regret in semi log scale . It shows that regrets indeed grow logarithmically . We estimate its regret slope as the slope of the final portion of the curve , which is a straight line . Sampling 6 times , we compute average slopes with regards to different group sizes . Figure 1(b ) illustrates the average slope has positive relationship with the group size . We also group users by location : picking users in a same zip code . Profiles of members in the selected group are shown in Table 1 . In Figure 1(a ) , each line represents the regret given a group with random users . We can see that lines can be distinguishable for group size 2 to 6 . However ,
( a )
( b )
Figure 1 : ( a ) Random Groups up to 10 Users . ( b ) Average slope of the regret VS Group Size . ( a )
( b )
Figure 2 : ( a ) Regret for Groups that people share zip code . ( b)Regret for equal weight vector and influence weight vector . in Figure 2(a ) , which depicts the regret lines for a group of users sharing a zip code , lines are indistinguishable from size 2 to 6 .
User ID Gender Male Male Male Female Female Male
1 2 3 4 5 6
Age 18 24 18 24 18 24 25 34 18 24 35 44
Occupation college/grad student programmer college/grad student sales/marketing other academic/educator
Table 1 : member information in the group
Next , we now investigate the scenario in which users have different influence in the group reward . As it is not easy to determine the influence power of each member , we use a simple heuristic : older people and females gain higher weight . Take our 6 user group in Table 1 as an example . Initially , every user has wu = 1 . User 4 gets 1 more weight unit and User 6 gains 2 more units because of their age . User 4 and User 5 will get 1 more unit because of their gender . The final weight vector w for the group is ( 1,1,1,3,2,3 ) . This reflects how each individual affects the group reward.We then apply Group UCB on this group , with other settings not changed . Figure 2(b ) displays the regrets for Equal Weight and our heuristic Influence Weight , on semi log scale . We can see that Group UCB performs well for both types of weight vectors . The final leading constant of regret is 348 for Equal Weight and 380 for Influence Weight . Hence the regret does not vary significantly , indicating that our Group UCB policy works for varying values of x(t ) . 4 Conclusion Our work has initiated the MAB approach to group recommendations . Many extensions remain open . For example , can we work with only a group rating each time , rather than rating from each individual ? How can we extend our policy when genres are correlated ? Last but not least , Group UCB may have other applications . 5 References [ 1 ] P . Auer , N . Cesa Bianchi , P . Fischer , and L . Informatik .
Finite time analysis of the multi armed bandit problem . In Machine Learning , 2002 .
[ 2 ] GroupLens . http://wwwgrouplensorg/node/73 [ 3 ] MoviePilot . http://moviepilotcom/
1001011021031041050500100015002000TRegret 1 user2 users3 users4 users5 users6 users7 users8 users9 users10 users246810150200250300350400450dRegret Slope1001011021031041050500100015002000tRegret 1 user2 users3 users4 users5 users6 users100101102103104105200400600800100012001400160018002000TRegret Equal WeightInfluence WeightWWW 2012 – Poster PresentationApril 16–20 , 2012 , Lyon , France464
