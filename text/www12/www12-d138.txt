Document Hierarchies from Text and Links
Qirong Ho
Machine Learning Department School of Computer Science Carnegie Mellon University qho@cscmuedu
∗
Jacob Eisenstein
School of Interactive Computing Georgia Institute of Technology jacobe@gatech.edu
Eric P . Xing
Machine Learning Department School of Computer Science Carnegie Mellon University epxing@cscmuedu
ABSTRACT Hierarchical taxonomies provide a multi level view of large document collections , allowing users to rapidly drill down to finegrained distinctions in topics of interest . We show that automatically induced taxonomies can be made more robust by combining text with relational links . The underlying mechanism is a Bayesian generative model in which a latent hierarchical structure explains the observed data — thus , finding hierarchical groups of documents with similar word distributions and dense network connections . As a nonparametric Bayesian model , our approach does not require pre specification of the branching factor at each non terminal , but finds the appropriate level of detail directly from the data . Unlike many prior latent space models of network structure , the complexity of our approach does not grow quadratically in the number of documents , enabling application to networks with more than ten thousand nodes . Experimental results on hypertext and citation network corpora demonstrate the advantages of our hierarchical , multimodal approach .
Categories and Subject Descriptors I26 [ Artificial Intelligence ] : Learning ; G.3 [ Probability and Statistics ]
General Terms Algorithms , Experimentation
Keywords hierarchical clustering , Bayesian generative models , topic models , stochastic block models
1 .
INTRODUCTION
As the quantity of online documents continues to increase , there is a need for organizational structures to help readers find the content that they need . Libraries have long employed hierarchical taxonomies such as the Library of Congress System1 for this purpose ; a similar approach was taken in the early days of the Web , with portal sites that present the user with a hierarchical organization of ∗ Jacob Eisenstein ’s contribution to this work was performed at Carnegie Mellon University . 1http://wwwlocgov/catdir/cpso/lcco
Copyright is held by the International World Wide Web Conference Committee ( IW3C2 ) . Distribution of these papers is limited to classroom use , and personal use by others . WWW 2012 , April 16–20 , 2012 , Lyon , France . ACM 978 1 4503 1229 5/12/04 . web pages . The key advantage of such taxonomies is that the logarithmic depth of tree structures permits fine grained distinctions between thousands of “ leaf ” subtopics , while presenting the user with at most a few dozen choices at a time . The user can recursively drill down to a very fine grained categorization in an area of interest , while quickly disregarding irrelevant topics at the coarsegrained level . A partial example of a hierarchical taxonomy for Wikipedia is shown in Figure 1 .
Manual curation of taxonomies was possible when membership was restricted to books or a relatively small number of web publishers , but becomes increasingly impractical as the volume of documents grows . This has motivated research towards inducing hierarchical taxonomies automatically from data [ 39 , 7 ] . However , these existing solutions rely exclusively on a single modality , usually text . This can be problematic , as content is often ambiguous — for example , the words “ scale ” and “ chord ” have very different meanings in the contexts of computer networks and music theory . As a solution , we propose to build taxonomies that incorporate the widely available metadata of links between documents . Such links appear in many settings : hyperlinks between web pages , citations between academic articles , and social network connections between the authors of social media . Network metadata can disambiguate content by incorporating an additional view which is often orthogonal to text . For example , we can avoid conflating two documents that mention “ scales ” and “ chords ” if they exist in completely different network communities ; analagously , we can group documents which share network properties , even if the text is superficially different .
We have incorporated these ideas into a system called TopicBlock , which uses both text and network data to induce a hierarchical taxonomy for a document collection . This requires meeting three technical challenges :
• Challenge 1 : Combining the disparate representations of text and network data . Network and text content have very different underlying representations . We propose a model in which both the text and network are stochastic emissions from a latent hierarchical structure . The inference task is to find the latent structure which is likely to have emitted the observed data . On the text side we use the machinery of hierarchical latent topic models [ 7 ] , a coarse to fine representation in which high level content is generated from shared nodes near the root of the hierarchy , while more technical information is generated from the detailed subtopics at the leaves . On the network side , we employ a hierarchical version of the stochastic block model [ 21 ] , in which links are emissions from Bernoulli distributions associated with nodes in the hierarchy .
WWW 2012 – Session : Entity and Taxonomy ExtractionApril 16–20 , 2012 , Lyon , France739 Figure 1 : An example 4 level topic hierarchy built from Wikipedia Simple English . We annotate each topic with its number of documents , a manually chosen label describing the topic , and a list of highly ranked words according to TF IDF . The dotted lines in the hierarchy show parent and child topics ( only the children of some parents are shown ) . For the bottom level topics , we also provide the names of some Wikipedia documents associated with them . The associated network data is shown in Figure 4 .
• Challenge 2 : Selecting the appropriate granularity . The problem of identifying model granularity is endemic for latent structure models [ 38 ] , but it is particulary vexing in the hierarchical setting . A flat mixture model or topic model requires only a single granularity parameter ( the number of clusters or topics ) , but a hierarchy requires a granularity parameter at each non terminal . Furthermore , the ideal granularity is not likely to be identical across the hierarchy : for example , the nuclear physics topic may demand fewer subtopics than the cephalopods topic . TopicBlock incorporates a Bayesian nonparametric prior which lets the data speak for itself , thus automatically determining the appropriate granularity at each node in the hierarchy .
• Challenge 3 : Scaling the network analysis .
In network data , the number of possible links grows quadratically with the number of nodes . This limits the scalability of many previous techniques [ 2 , 29 ] . In contrast , TopicBlock ’s complexity scales linearly with the number of nodes and the depth of the hierarchy . This is possible due to the hierarchicallystructured latent representation , which has the flexibility to model link probabilities finely where necessary ( at the leaf level ) , while backing off to a coarse representation where possible ( between nodes in disparate parts of the hierarchy ) .
We apply TopicBlock to two datasets . The first is Simple English Wikipedia , in which documents on a very broad array of subjects are connected by hyperlinks . The second is the ACL Anthology [ 5 ] , a collection of scientific research articles , in which documents are connected by citations . TopicBlock yields hierarchies which are coherent with respect to both text and relational structure , grouping documents which share terms and also contain dense relational patterns . In the ACL Anthology data , we evaluate the capability of TopicBlock to recommend citation links from text alone . In the Wikipedia data , we evaluate TopicBlock ’s ability to identify the correct target of a hyperlink that is lexically ambiguous .
2 . RELATED WORK
There is substantial prior work on hierarchical document clustering . Early approaches were greedy , using single link or completelink heuristics [ 39 ] . This yields a dendrogram of documents , in which a root node is decomposed in a series of binary branching decisions until every leaf contains a single document . We prefer flatter trees with fewer non terminals , which are more similar to manually curated hierarchies.2 Other work on hierarchical clustering includes top down techniques for iteratively partitioning the data [ 41 ] , search based incremental methods [ 36 ] , probabilistic modeling of manually created taxonomies [ 31 ] , and interactive exploration [ 11 ] .
The splitting and merging decisions that characterize most hierarchical clustering algorithms can be made on the basis of Bayesian hypothesis tests [ 19 ] . However , our work more closely relates to Bayesian generative models over the document content , as we focus on inducing a latent structure that provides a likely explanation for the observed text and links . Hierarchical latent Dirichlet allocation ( hLDA ) is a prototypical example of such an approach : each document sits on a path through a hierarchy with unbounded treewidth , and the text is generated from a mixture of multinomials along the path . We extend hLDA by incorporating network data , enabling a better understanding of the relationship between these two modalities . Adams et al . [ 1 ] present a hierarchical topic model which differs from hLDA in that documents can be located at any level , rather than exclusively at leaf nodes . Because all content for each document is generated from the hierarchy node at which it sits , the generative distributions must be formed by chaining together conjugate priors , requiring more complex inference .
In network data , clustering is often called “ community discovery ” [ 23 ] . Graph based approaches such as normalized cut [ 37 ] are fast and deterministic , but often require the desired number of clusters to be specified in advance , and do not easily generalize to hierarchical models . SHRINK [ 22 ] induces a hierarchical clustering that prioritizes high modularity , while tolerating hubs and outliers that violate the traditional hierarchical structure . However , our work is more closely related to probabilistic approaches , which
2eg , the Open Directory Project , http://wwwdmozorg
WWW 2012 – Session : Entity and Taxonomy ExtractionApril 16–20 , 2012 , Lyon , France740 provide a principled way to combine content and network structure . Clauset et al . show that hierarchical community discovery can be obtained using a Monte Carlo sampling algorithm ; the generative model assigns a link probability at each node in the hierarchy , and the sampling moves then converge on a stationary distribution centered on a hierarchy with high likelihood of generating the observed links [ 9 ] . However , this model is restricted to dendrograms , or binary trees , which are unlike the flatter hierarchies produced by human curators .
An alternative line of work on network clustering begins with the Stochastic Block Model ( SBM ) [ 21 ] . The SBM is a generative model in which nodes are partitioned into communities , which in turn determine the link probabilities . This idea was extended in the mixed membership stochastic blockmodel ( MMSB ) [ 2 ] , where each node has a mixed membership vector over possible “ roles ” ; an additional pair of latent variables selects the roles that are relevant for each potential network connection . The multiscale community block model ( MSCB ) places this idea in a non parametric hierarchical setting : each document is associated with a path through a hierarchy , and the roles correspond to levels on the path [ 20 ] . Both the MSCB and MMSB assign latent variables to every potential link , so that each sampling pass requires O(N 2 ) complexity in the number of nodes .
A key feature of TopicBlock is that we merge text and network data , with the goal of inducing a more robust hierarchy and enabling applications in which the two modalities can help to explain each other . Mei et al . combine latent topic models with network information by compiling the network into a regularizer that encourages the topic proportions of linked documents to be similar [ 28 ] . This approach encodes the network into the structure of the generative model , so it does not permit probabilistic inferences about the likelihood of additional network connections . Topic sensitive PageRank [ 17 ] takes a different notion of “ topic , ” seeding each topic with documents from the top level categories of the manuallycurated Open Directory Project hierarchy . This method is designed to support information retrieval , and does not permit probabilistic modeling of new content or unseen links . Unlike both of these approaches , TopicBlock is generative over both text and links .
Much of the prior work on joint generative models of text and links falls into two classes . In one class , the identity of the target and/or source of the link is encoded as a discrete random variable [ 10 , 29 , 14 , 27 , 35 ] . Such models permit probabilistic inference within the documents in the training set , but they are closed to outside documents ; it is not possible to use the text of an unseen document to predict who will link to it . In the second class of models , each link is a binary random variable generated from a Bernoulli distribution that is parametrized by the topical similarity of the documents . In the Relational Topic Model ( RTM ) , the link probability is a function of the topical similarity [ 8 ] ( Liu et al . extend the RTM by incorporating a per document “ community ” membership vector [ 25] ) . The RTM treats non edges as hidden data , so its complexity is linear in the number of edges , and thus less than the O(N 2 ) required by the blockmodel variants . Such a model is encouraged to assign arbitrarily high likelihood to the observed links , leading to instability in the parameter estimates , which must be corrected by a regularization heuristic . In contrast , we model both edges and non edges probabilistically , achieving subquadratic complexity by limiting the flexibility of the link probability model .
3 . MODEL DESCRIPTION
TopicBlock treats document text and relational links as emissions from a latent hierarchy , which has fixed depth L but a non
N
(
. m o n
( cid:955 )
( cid:580 ) ( cid:574)h Dirichlet((cid:580 ) ) Beta((cid:585)1 , ( cid:585)2 ) ( cid:560)h ( cid:955 ) ( cid:585)1 , ( cid:585)2
( cid:576 ) i t l u M
( cid:590)i )
Mi Multinom.((cid:574 ) ( ri ,vik ) ) zik wik ri ( cid:590)i nCRP((cid:576 ) ) Dirichlet((cid:573 ) ) N ( cid:238)N Eij Bernoulli(S(cid:560)(ri ,rj ) )
( cid:573 )
Figure 2 : Graphical model illustration parametric branching factor at each non terminal . Each document is represented as a complete path through the hierarchy , with words generated from a mixture across levels in the path , and links generated directly from the paths . We now present the model in detail . A summary of the hypothesized generative process is presented in Table 1 , and a plate diagram is shown in Figure 2 . 3.1 Latent hierarchy Each document ’s position in the hierarchy is denoted by an L×1 vector of integers ri ∈ Z L , which we call a path . The path is interpreted as follows : ri1 denotes the hierarchy branch taken by document i from level 0 ( the implicit root , denoted by r0 ) to level 1 , ri2 denotes the branch taken from level 1 to level 2 relative to ri1 ( the branch just taken ) , and so forth . Example : ri = ( 2 , 1 , 3 , . . . ) says that entity i is reached by taking the 2nd branch from the root , then the 1st branch at the node we just arrived at , followed by the 3rd branch at the next node , etc . The set of all paths ri fully determines the shape of the hierarchy .
The nested Chinese Restaurant Process ( nCRP ) provides a suitable Bayesian prior for non parametric hierarchies [ 7 ] . Each path is obtained by making a series of draws from standard Chinese Restaurant Processes associated with each node in the hierarchy . This prior displays the “ rich get richer ” property : at each level , a draw is likely to follow branches taken by previous documents ; however , there is always a possibility of choosing a new branch which has never been taken before . Blei et al . [ 7 ] show that this model permits collapsed sampling in a way that follows naturally from the original Chinese Restaurant Process . 3.2 Generating words and links
We assume that each document i ∈ {1 , . . . , N} is associated with two kinds of observed data . The first is a collection of words w , where wik denotes the k th word associated with document i , and Mi is the total number of word tokens in document i . The second type of observation is a collection of directed links to other documents , referred to as a network . This network is given as an N×N adjacency matrix E , such that Eij = 1 denotes the presence of a ( directed ) link from document i to document j , whileE ij = 0 denotes its absence . We ignore self links Eii .
Every node in the hierarchy represents a distribution over words and links ; documents whose path contains a hierarchy node h can draw their words and links from the distributions in h . More formally , every hierarchy node h is associated with two distributions . For the text , we define a set of vocabularies βh which generate words wik ; specifically , βh is a V dimensional multinomial parameter representing a distribution over words , as in LDA . For the links , we define a set of link density probabilities Φh . Here , Φh is the probability of generating a link between documents whose
WWW 2012 – Session : Entity and Taxonomy ExtractionApril 16–20 , 2012 , Lyon , France741 paths both contain hierarchy node h . In cases where two document paths share multiple hierarchy nodes , we take h to be the deepest shared node , which may be the root of the tree .
321 Words
Document text is generated from a bag of words model , in which each word is produced by some node along the document ’s path through the hierarchy . On this view , some words will be general and could appear in any document ( these words are drawn from the root ) while others will be specific ( these are drawn from a leaf ) . This encourages a hierarchy in which the most similar documents are grouped at the leaf level , while moderately similar documents are grouped at coarser levels of the hierarchy .
More formally , the words for document i are generated from a mixture of the β distributions found along the path ri , including the implicit root . Each word wik associated with document i can be generated by any of the path nodes ri1 , . . . , riL or the root r0 . The specific path node chosen to generate wik is given by a level indicator zik ∈ {0 , . . . , L} , for example , zik = 3 means that wik is generated from the vocabulary βh associated with the hierarchy node at ( ri1 , ri2 , ri3 ) . These level indicators zik are drawn from ( L + 1) dimensional multinomial parameters πi , which we refer to as level distributions . Intuitively , these represent document i ’s preference for shallower or deeper hierarchy levels .
322 Links
The generative model for links between documents is motivated by the intuition that the non terminals of the hierarchy represent progressively more specific communities of documents . While one might explicitly model the link probability between , say , organic chemistry and ancient Greek history ( as distinct from the likelihood of links from organic chemistry to ancient Roman history ) , a much simpler and more tractable model can be obtained by using the hierarchical structure to abstract this relationship . We make the simplifying assumption that relations between communities in disparate parts of the hierarchy can be summarized by their deepest common ancestor . As a result , the number of parameters grows linearly rather than quadratically with the number of non terminals . More formally , each nonterminal h has an associated Bernoulli parameter Φh , which indicates the link likelihood between documents that share h as their deepest common ancestor . We define S(ri , rj ) as a function that selects the deepest shared Φh between the paths ri , rj :
SΦ(ri , rj ) := Φh h := ( ri1 , . . . , riω ) , so that ,
P ( E | r , Φ ) =
. i,jfi=i
ω := arg max k≥0
I(ri,1:k = rj,1:k ) ,
SΦ(ri , rj )Eij ( 1 − SΦ(ri , rj ))1−Eij .
The likelihood is a product over all N 2 potential links , but as we will see , it can be computed in fewer than O(N 2 ) steps . Note that SΦ(ri , rj ) will select the root parameter Φr0 when ri and rj are completely different . 3.3 Parameters
TopicBlock has four parameter types : the paths ri , level distributions πi , word probabilities βh , and the link probabilities Φh . the paths ri are Each parameter is drawn from a suitable prior : drawn from a depth L nCRP(γ ) ; the level distributions πi are drawn from Dirichlet(α ) ; the topics βh are drawn from a symmetric Dirichlet(ηk ) ( where k is the depth of node h ) ; and the link
( 1 )
( rx1 , . . . , rxzxy ) = ( ri1 , . . . , rizik ) , wxy = v
'ffff .
• Draw the hierarchy — for each entity i :
– Path ri ∼ nCRP(γ ) – Word level distribution πi ∼ Dirichlet(α ) • Draw hierarchy node parameters — for each node h : – Word probabilities βh ∼ Dirichlet(ηdepth(h ) ) – Link probabilities Φh ∼ Beta(λ1 , λ2 ) • Draw text — for each entity i and word k : – Word level zik ∼ Multinomial(πi ) – Word wik ∼ Multinomial(βh ) , where h is the hierarchy node at ( ri,1 , . . . , ri,zik )
• Draw network — for each pair of entities i and j fi= i :
– Link Eij ∼ Bernoulli(SΦ(ri , rj ) ) , where SΦ( ) is defined in Section 3.2
Table 1 : The generative process for TopicBlock ’s model of text and relational connections probabilities Φh are drawn from Beta(λ1 , λ2 ) . The hyperparameter γ > 0 is an L × 1 vector , while α , η > 0 are ( L + 1 ) × 1 vectors , and λ1 , λ2 > 0 are scalars .
4 .
INFERENCE
Exact inference on our model is intractable , so we derive a collapsed Gibbs sampler for posterior inference [ 34 ] . We integrate out π , β and Φ for faster mixing ( collapsed sampling for topic models was introduced in [ 13] ) , so we need sample only the paths r and word levels z . We present the sampling distributions for these parameters now .
Word levels z .
The sampling distribution of zik is P(zik | r , z−(ik ) , E , w )
∝ P(wik , zik | r , z−(ik ) , E , w−(ik ) ) = P(wik | r , z , w−(ik))P(zik | zi,(−k ) )
( 2 ) where zi,(−k ) = {zi·} \ zik and w−(ik ) = {w.} \w ik . The first term represents the likelihood ; for a particular value of zik , it is
P(wik | r , z , w−(ik ) ) = av = |{(x , y ) | ( x , y ) fi= ( i , k ) , zxy = zik , fiV ηzik + awik
V ηzik + v=1 av
,
( 3 )
In plain English , av is the number of words wxy equal to v ( excluding wik ) and coming from hierarchy position ( ri1 , . . . , rizik ) . Thus , we are computing the empirical frequency of emitting word v , smoothed by level zik ’s symmetric Dirichlet prior ηzik .
The second term represents the prior on zik :
P(zik | zi,(−k ) ) =
αzik + #[zi,(−k ) = zik ] fiL fi=1 αfi + #[zi,(−k ) = ) ]
.
Paths r .
The sampling distribution for the path ri is P(ri | r−i , z , E , w )
∝ P(ri , E(i·),(·i ) , wi | r−i , z , E−(i·),−(·i ) , w−i ) = P(E(i·),(·i ) | r , E−(i·),−(·i))P(wi | r , z , w−i)P(ri | r−i ) where wi = {wi·} is the set of tokens in document i , and w−i is its complement . E(i,·),(·,i ) = {Exy | x = i ∨ y = i} is the set of
( 4 )
( 5 )
WWW 2012 – Session : Entity and Taxonomy ExtractionApril 16–20 , 2012 , Lyon , France742 all links touching document i and E−(i,·),−(·,i ) is its complement . In particular , the set E(i,·),(·,i ) is just the i th row and i th column of the adjacency matrix E , sans the self link Eii .
Equation 5 decomposes into three terms , corresponding to link likelihoods , word likelihoods , and the path prior distribution respectively . The first term represents the link likelihoods for all links touching document i . These likelihoods are Bernoulli distributed , with a Beta prior ; marginalizing the parameter Φ yields a Beta Bernoulli distribution , which has an analytic closed form :
.
Γ(A+B+λ1+λ2 ) Γ(A+λ1)Γ(B+λ2 )
· Γ(A+C+λ1)Γ(B+D+λ2 ) Γ(A+B+C+D+λ1+λ2 )
Φ∈Φ(i·),(·i ) Φ(i·),(·i ) ={Φh |∃(x , y)[Exy ∈ E(i·),(·i ) , S A = B = C = D = ffff ( x , y ) | Exy ∈ E−(i·),−(·i ) , S ffff ( x , y ) | Exy ∈ E−(i·),−(·i ) , S ffff ( x , y ) | Exy ∈ E(i·),(·i ) , S ffff ( x , y ) | Exy ∈ E(i·),(·i ) , S
Φ = Φh]} 'ffff xy Φ = Φ , Exy = 1 'ffff xy Φ = Φ , Exy = 0 'ffff xy Φ = Φ , Exy = 1 'ffff xy Φ = Φ , Exy = 0 xy
( 6 ) where Φ(i·),(·i ) is the set of all link probability parameters Φh touched by the link set E(i,·),(·,i ) . Observe that only those Φh along path ri ( or the root ) can be in this set , thus it has size |Φ(i·),(·i)| ≤ L + 1 . Also , note that the terms A , B , C , D depend on Φ . The second term of Equation 5 represents the word likelihoods :
L . fi=1
.V
Γ(V η.+ fiV v=1 G.,v ) v=1 Γ(G.,v +η . )
· fiV v=1 Γ(G.,v +H.,v +η . ) v=1 G.,v +H.,v )
.V
Γ(V η.+
( 7 )
Gfi,v = |{(x , y ) | x fi= i , zxy = ) ,
( rx1 , . . . , rxfi ) = ( ri1 , . . . , rifi ) , wxy = v}|
Hfi,v = |{y | ziy = ) , wiy = v}| where V is the vocabulary size . Gfi,v is just the number of words in w−i equal to v and coming from hierarchy position ( ri1 , . . . , rifi ) . Hfi,v is similarly defined , but for words in wi .
The third term of Equation 5 represents the probability of drawing the path ri from the nCRP , and can be computed recursively for all levels ( , P(rifi = x | r−i , ri,1:(fi−1 ) ) = ⎧⎪⎨ ⎪⎩
|{jfi=i | rj,1:(−1)=ri,1:(−1),rj=x}| |{jfi=i | rj,1:(−1)=ri,1:(−1)}|+γ |{jfi=i | rj,1:(−1)=ri,1:(−1)}|+γ if x is an existing branch , if x is a new branch
( 8 )
γ .
This equation gives the probability of path ri taking branch x at depth ( . At step ( in the path , the probability of following an existing branch is proportional to the number of documents already in that branch , while the probability of creating a new branch is proportional to γfi .
Hyperparameter Tuning .
The hyperparameters γ , α , η , λ1 , λ2 significantly influence the size and shape of the hierarchy . We automatically choose suitable values for them by endowing γ , α , η with a symmetric Dirichlet(1 ) hyperprior , and λ1 , λ2 with an Exponential(1 ) hyperprior . Using the Metropolis Hastings algorithm with these hyperpriors as proposal distributions , we sample new values for γ , α , η , λ1 , λ2 after every Gibbs sampling iteration . 4.1 Linear time Gibbs sampling
To be practical on larger datasets , each Gibbs sampling sweep must have runtime linear in both the number of tokens and the number of 1 links Eij = 1 . This is problematic for standard implementations of generative network models such as ours , because we are modeling the generative probability of all 1 links and 0 links . The sufficient statistics for each Φh are the number of 1 links and
Algorithm 1 Removing document i from sufficient statistics of Φh
Let h0 , . . . , hL be the hierarchy nodes along ri . Let A be a temporary variable . for ( = L . . .0 do if ( < L then uh . ← uh . − ( A − Uh.+1 ) th . ← th . − 1
( Store the original value of Uh . ) end if A ← Uh . Uh . ← Uh . − Uh.,i Th . ← Th . − 1 for j st j ∈ Neighbors(i ) and hfi ⊆ rj do
Uh.,j ← Uh.,j − I(Eij = 1 ) − I(Eji = 1 ) Uh.,i ← Uh.,i − I(Eij = 1 ) − I(Eji = 1 ) end for end for
0 links , and these statistics must be updated when we resample the paths ri . Naïvely updating these parameters would take O(N ) time since there are 2N − 2 links touching document i . It follows that a Gibbs sampling sweep over all ri would require O(N 2 ) quadratic runtime . The solution is to maintain an augmented set of sufficient statistics for Φh . Define h ⊆ ri to be true if path ri passes through node h . Then the augmented sufficient statistics are : fi jfi=i(Eij + Eji)I(h ⊆ ri , h ⊆ rj ) , the number of 11 . Uh,i = fi links touching document i and drawn from Φh and its descendants . i,j Eij I(h ⊆ ri , h ⊆ rj ) , the number of 1 links drawn fi h.∈children(h ) Uh . , the number of 1 links drawn from fi i I(h ⊆ ri ) , the number of documents at h and its descenfi from Φh and its hierarchy descendants .
Φh ’s descendants only .
3 . uh =
2 . Uh = h.∈children(h ) Th . , the number of documents at h ’s de
4 . Th = dants . 5 . th = scendants only .
The number of 0 or 1 links specifically at Φh is given by #[1 links at h ] =U h − uh ( 9 ) #[0 links at h ] = [ (Th)(Th − 1 ) − ( th)(th − 1 ) ] − ( Uh − uh ) Before sampling a new value for document i ’s path ri , we need to remove its edge set E(i,·),(·,i ) from the above sufficient statistics . Once ri has been sampled , we need to add E(i,·),(·,i ) back to the sufficient statistics , based on the new ri . Algorithms 1 , 2 perform these operations efficiently ; observe that they run in O(PiL ) time where Pi is the number of 1 links touching document i . Letting P be the total number of 1 links in E , we see that a Gibbs sampler sweep over all ri spends O(P L ) time updating Φh sufficient statistics , which is linear in P .
The remaining work for sampling ri boils down to ( 1 ) calculating existing and new path probabilities through the hierarchy , and ( 2 ) updating sufficient statistics related to the vocabularies β . Calculating the path probabilities requires O(HLV ) time , where H is the number of hierarchy nodes and V is the vocabulary size ; updating the vocabularies requires O(MiL ) time where Mi is the number of tokens wik belonging to document i . Thus , the total runtime required to sweep over all ri is O(P L + N HLV + M L ) where M is the total number of tokens w . Treating L , H , V as constants and noting that N ≤ M , we see that sampling all ri is indeed linear in the number of tokens M and number of 1 links P . We also need to sample each word level zik , which takes O(L ) time ( including sufficient statistic updates ) for a total of O(M L ) linear work over all z . Finally , the hyperparameter tuning steps require
WWW 2012 – Session : Entity and Taxonomy ExtractionApril 16–20 , 2012 , Lyon , France743 Algorithm 2 Adding document i to sufficient statistics of Φh
There is previous work on modeling the topics underlying Wikipedia
Let h0 , . . . , hL be the hierarchy nodes along ri . Let A be a temporary variable . for ( = L . . .0 do if ( < L then uh . ← uh . + ( Uh.+1 − A ) th . ← th . + 1 end if for j st j ∈ Neighbors(i ) and hfi ⊆ rj do
Uh.,j ← Uh.,j + I(Eij = 1 ) + I(Eji = 1 ) Uh.,i ← Uh.,i + I(Eij = 1 ) + I(Eji = 1 ) end for A ← Uh . Uh . ← Uh . + Uh.,i Th . ← Th . + 1 end for
( Store the original value of Uh . )
Wikipedia ACL Anthology documents tokens links vocabulary
14,675
1,467,500 134,827 10,013
15,032
2,913,665
41,112 2,505
Table 2 : Basic statistics about each dataset us to compute the probability of all tokens w and links E given the paths r and word levelsz , which can be performed in at most linear O(P L + M L ) time . Since we only update the hyperparameters once after every Gibbs sampling sweep , our total runtime per sweep remains linear .
We contrast our linear efficiency with alternative models such as the Mixed Membership Stochastic Block Model ( MMSB [ 2 ] ) and Pairwise Link LDA [ 29 ] . The published inference techniques for these models are quadratic in the number of nodes , so it would be very difficult for serial implementations to scale to the 104 node datasets that we handle in this paper .
5 . DATA
We evaluate our system on two corpora : Wikipedia and the ACL Anthology . The Wikipedia dataset is meant to capture familiar concepts which are easily comprehended by non experts ; the ACL Anthology dataset tests the ability of our model to build reasonable taxonomies for more technical datasets . We expect different network behavior for the two datasets : a Wikipedia page can contain an arbitrary number of citations , while research articles may be space limited , and can only cite articles which have already been published . Thus , the ACL dataset may fail to include many links which would seem to be demanded by the text , but were omitted due to space constraints or simply because the relevant article had not yet been published . The Wikipedia dataset poses its own challenges , as some links are almost completely unrelated to document topical content . For example , the article on DNA contains a link to the article on Switzerland , because DNA was first isolated by a Swiss scientist . 5.1 Simple English Wikipedia
Our first dataset is built from Wikipedia ; our goal is to use the text and hyperlinks in this dataset to induce a hierarchical structure that reflects the underlying content and connections . We chose this dataset because the content is written at a non technical level , allowing easy inspection for non experts . The dataset supports the evaluation of link resolution ( defined in Section 63 ) data [ 14 , 32 ] . Gruber et al . [ 14 ] constructed a small corpus of text and links by crawling 105 pages starting from the page for the NIPS conference , capturing 799 in collection links . Our goal was a much larger scale evaluation ; in addition , we were concerned that a crawl based approach would bias the resulting network to implicitly reflect a hierarchical structure ( centered on the seed node ) and an unusually dense network of links .
Instead of building a dataset by crawling , we downloaded the entire “ Simple English ” Wikipedia , a set of 133,462 articles written in easy to read English . Many of these documents are very short , including placeholders for future articles . We limited our corpus to documents that were at least 100 tokens in length ( using the LingPipe tokenizer [ 3] ) , and considered only articles ( ignoring discussion pages , templates , etc ) This resulted in a corpus of 14675 documents . The link data includes all 152,674 in collection hyperlinks ; the text data consists of the first 100 tokens of each document , resulting in a total of 1,467,500 tokens . We limited the vocabulary to all words appearing at least as frequently as the 10,000th most frequent word , resulting in a total vocabulary of 10,013 . We apply a standard filter to remove stopwords [ 24 ] . 5.2 ACL Anthology
Our second dataset is based on the scientific literature , which contains both text and citations between documents . The ACL anthology is a curated collection of papers published in computational lingusitics venues , dating back to 1965 [ 5 ] . We downloaded the 2009 release of this dataset , including papers up to that year , for a total of 15,032 documents . Taxonomy induction on research corpora can serve an important function , as manually curated taxonomies always risk falling behind new developments which may splinter new fields or unite disparate ones . As noted above , we use the entire ACL Anthology dataset from 1965 to 2009 . We limit the vocabulary to 2,500 terms , and limit each document to the first 200 tokens — roughly equivalent to the title and abstract — and remove stopwords [ 24 ] .
There is substantial previous work on the ACL Anthology , including temporal and bibliometric analysis [ 16 , 33 ] , citation prediction [ 4 ] , and recognition of latent themes [ 15 ] and influence [ 12 , 30 ] . However , none of this work has considered the problem of inducing hierarchical structure of the discipline of computational linguistics .
Our quantitative evaluation addresses the citation prediction task considered by Bethard and Jurafsky [ 4 ] . Following their methodology , we restrict our quantitative analysis to the 1,739 journal and conference papers from 2000 to 2009 . Our version of the corpus is a more recent release , so our data subset is very similar but not identical to their evaluation set .
6 . QUANTITATIVE ANALYSIS
We present a series of quantitative and qualitative evalutions of TopicBlock ’s ability to learn accurate and interpretable models of networked text . Our main evaluations ( sections 6.2 and 6.3 ) test the ability of TopicBlock to predict and resolve ambiguous links involving heldout documents . 6.1 System Details
For all experiments , we use an L = 2 hierarchy ( root plus two levels ) unless otherwise stated . We initialize TopicBlock ’s document paths r by using a Dirichlet Process Mixture Model ( essentially a one level , text only TopicBlock with no shared root ) in a recursive clustering fashion , which provides a good starting hierarchy . From there , we ran our Gibbs sampler cum Metropolis
WWW 2012 – Session : Entity and Taxonomy ExtractionApril 16–20 , 2012 , Lyon , France744 Hastings algorithm for 2,500 passes through the data or for 7 days , whichever came first ; our slowest experiments completed at least 1,000 passes . All experiments were run with 10 repeat trials , and results were always obtained from the most recent sample . We selected the best trial according to experimentally relevant criteria : for the qualitative analyses ( Section 7 ) , we selected according to sample log likelihood ; in the citation prediction task we employed a development set ; in the link resolution task we show the results of all trials . 6.2 Citation Prediction
Our citation prediction evaluation uses the induced TopicBlock hierarchy to predict outgoing citation links from documents which were not seen during training time . For this evaluation , we use the 1,739 paper ACL subset described earlier . Citation prediction has been considered in prior research ; for example , Bethard and Jurafsky present a supervised algorithm that considers a broad range of features , including both content and citation information [ 4 ] . We view our approach as complementary ; our hierarchical model could provide features for such a discriminative approach . He et al . attack the related problem of recommending citations in the context of a snippet of text describing the purpose of the citation [ 18 ] , focusing on concept based relevance between citing and cited documents . Again , one might combine these approaches by mining the local context to determine which part of the induced hierarchy is most likely to contain the desired citation . Metric .
We evaluate using mean average precision , an information retrieval metric designed for ranking tasks [ 26 ] . The average precision is the mean of the precisions at the ranks of all the relevant examples ; mean average precision takes the mean of the average precisions across all queries ( heldout documents ) . This metric can be viewed as an approximation to the area under the precision recall curve .
Systems .
We divided the 1,739 paper ACL subset into a training set ( papers from 2000 2006 ) , a development set ( 2006 2007 ) , and a heldout set ( 2008 2009 ) . For each experiment we conducted 10 trials , using the following procedure :
1 . build a topic hierarchy from the training set using TOPICBLOCK , 2 . fit the development set text to the learnt hierarchy , and predict development links ,
3 . retrieve the trial with the highest mean average precision over development set links ,
4 . fit the heldout set text to that trial ’s hierarchy , and predict heldout links ,
5 . compute mean average precision over heldout set links .
In essence , the development set is being used to select the besttrained model with respect to the citation prediction task . The final predictions were obtained by inferring each test document ’s most appropriate hierarchy path r given only its text , and then using the path r to predict links to training documents according to our network model .
Baselines .
To evaluate the contribution of jointly modeling text with network structure , we compare against hierarchical latent Dirichlet allocation ( HLDA ) [ 7 ] , a closely related model which ignores network structure . We use our own implementation , which is based on the TOPICBLOCK codebase . As HLDA does not explicitly model links , we postfit a hierarchical blockmodel to the induced hierarchy over the training data ; this hierarchy is learnt only from the text .
System TOPICBLOCK HLDA HSBM IN DEGREE TF IDF x x
Text ? Network ? Hierarchical ? MAP 0.137 0.117 0.112 0.0731 0.0144 x x x x x x x
Table 3 : Results on the citation prediction task for the ACL Anthology data . Higher scores are better . Note that HLDA is equivalent to TOPICBLOCK without the network component , while HSBM is equivalent to TOPICBLOCK without text .
Thus , the comparison with HLDA directly tests the contribution of network information to the quality of the hierarchy , over what the text already provides . After postfitting the blockmodel , we fit the development and heldout sets as described earlier .
We can also isolate the contribution of network information to the hierarchy , by learning the shape of the hierarchy based on network contributions but not text . After learning the hierarchy ’s shape ( which is defined by the paths r ) this way , we postfit text topics to this hierarchy by running hLDA while keeping the paths r fixed . Then we fit the development and heldout sets as usual . This approach can be viewed as a hierarchical stochastic blockmodel , so we name the system HSBM .
Next , we consider a simpler text only baseline , predicting links based on the term similarity between the query and each possible target document ; specifically , we use the TF IDF measure considered by Bethard and Jurafsky [ 4 ] . For a fair comparison , we use the same text which was available to TopicBlock and hLDA , which is the first 200 words of each document .
Finally , we consider a network only baseline , where we rank potential documents in descending order of IN DEGREE . In other words , we simply predict highly cited documents first .
Results .
As shown in Table 3 , TOPICBLOCK achieves the highest MAP score of all methods , besting the hierarchies trained using only text ( HLDA ) or only the network ( HSBM ) . This demonstrates that inducing hierarchies from text and network modalities jointly yields quantitatively better performance than post hoc fitting of one modality to a hierarchy trained on the other . In addition , all hierarchybased methods beat the TF IDF and IN DEGREE baselines by a strong margin , validating the use of hierarchies over simpler , nonhierarchical alternatives .
6.3 Link Resolution
Wikipedia contains a substantial amount of name ambiguity , as multiple articles can share the same title . For example , the term “ mac ” may refer to the Media Access Control address , the luxury brand of personal computers , or the flagship sandwich from McDonalds . The link resolution task is to determine which possible reference article was intended by an ambiguous text string . In our Wikipedia data , there were 88 documents with the same base name , such as “ scale_(music)" and “ scale_(map)" , and there were 435 references to such articles . These references were initially unambiguous , but we removed the bracketed disambiguation information in order to evaluate TOPICBLOCK ’s ability to resolve ambiguous references .
Systems .
We run TOPICBLOCK to induce a hierarchy over the training documents , and then learn the best paths r for each of the 88 am
WWW 2012 – Session : Entity and Taxonomy ExtractionApril 16–20 , 2012 , Lyon , France745 Figure 3 : Wikipedia link resolution accuracy , plotted against proportion of links which could be resolved by the hierarchy .
Figure 4 : The network block matrix for the Simple English Wikipedia data . biguous documents according to just their text . Then , for each of the 435 ambiguous references to the 88 target documents , we select the target with the highest link probability to the query document . If two targets are equally probable , we select the one with the highest text similarity according to TF IDF . This experiment was conducted 10 times , and all results are shown in Figure 3 . We also compare against HLDA , which is run in the same way as TOPICBLOCK but trained without network information , using hierarchy path similarity instead of link probability to rank query documents . Finally , as a baseline we consider simply choosing the target with the highest TEXT SIMILARITY .
Metric .
The evaluation metric for this task is accuracy : the proportion of ambiguous links which were resolved correctly . In most cases the ambiguity set included only two documents , so more complicated ranking metrics are unnecessary .
Results .
We performed ten different runs of TOPICBLOCK and HLDA . In each run , a certain number of links could not be resolved by the hierarchy , because the target nodes were equally probable with respect to the query node — in these cases , we use the TF IDF tiebreaker described above . Figure 3 plots the accuracy against the proportion of links which could be resolved by the hierarchy . As shown in the figure , TOPICBLOCK is superior to the TEXT SIMILARITY baseline on all ten runs . Moreover , the accuracy increases with the specificity of the hierarchy with regard to the ambiguous links — in other words , the added detail in these hierarchies coheres with the hidden hyperlinks . In contrast , HLDA is rarely better than the cosine similarity baseline , and does not improve in accuracy as the hierarchy specificity increases . This demonstrates that training from text alone will not yield a hierarchy that coheres with network information , while training from both modalities improves link disambiguation .
7 . QUALITATIVE ANALYSIS
We perform a manual analysis to reveal the implications of our modeling decisions and inference procedure for the induced hierarchies , showcasing our model ’s successes while highlighting areas for future improvement . Note that while the quantitative experiments in the previous section required holding out portions of the data , here we report topic hierarchies obtained by training on the entire dataset .
7.1 Wikipedia
Figure 1 shows a fragment of the hierarchy induced from the Simple English Wikipedia Dataset . Unlike our other experiments , we have used an L = 3 ( root plus 3 levels ) hierarchy here to capture more detail . We have provided the topic labels manually ; overall we can characterize the top level as comprised of : history ( W1 ) , culture ( W2 ) , geography ( W3 ) , sports ( W4 ) , biology ( W5 ) , physical sciences ( W6 ) , technology ( W7 ) , and weapons ( W8 ) . The subcategories of the sports topic are shown in the figure , but the other subcategories are generally reasonable as well : for example biology ( W5 ) divides into non human and human subtopics ; history ( W1 ) divides into modern ( W1.1 ) , religious ( W1.2 ) , medieval ( W1.3 ) , and Japanese ( W14 ) While a manually created taxonomy would likely favor parallel structure and thus avoid placing a region ( Japan ) and a genre ( religion ) alongside two temporal epochs ( modern and medieval ) , TopicBlock chooses an organization that reflects the underlying word and link distributions .
Figure 4 shows the link structure for the Wikipedia data , with the source of the link on the rows and the target on the columns . Documents are organized by their position in the induced hierarchy . Topic 1 has a very high density of incoming links , reflecting the generality of these concepts and their relation to many other documents . Overall , we see very high link density at the finest level of detail ( indicated by small dark blocks directly on the diagonal ) , but we also see evidence of hierarchical link structure in the larger shaded blocks such as culture ( W2 ) and physical science ( W6 ) .
7.2 ACL Anthology
The full ACL anthology hierarchy is shown in Figure 5 , which gives the top words corresponding to each topic , by TF IDF.3 As before , the topic labels are provided by us ; for simplicity we have chosen to focus on an L = 2 level hierarchy . The top level categories include both application areas ( interactive systems ( A1 ) and information systems ( A2 ) ) as well as problem domains ( discourse and semantics ( A4 ) ; parsing ( A6 ) ; machine translation ( A8) ) . These areas are often close matches for the session titles of relevant conferences such as ACL.4 At the second level , we again see coherent topical groupings : for example , the children of information systems include popular shared tasks such as named entity recognition ( A2.1 ) , summarization ( A2.3 ) , and question answering ( A2.4 ) ; the children of discourse and semantics ( A4 ) include well known
3Specifically , we multiplied the term frequency in the topic by the log of the inverse average term frequency across all topics [ 6 ] . 4http://wwwacl2011org/programutf8shtml
WWW 2012 – Session : Entity and Taxonomy ExtractionApril 16–20 , 2012 , Lyon , France746 of the document collection . We also plan to investigate dynamic models , in which temporal changes in the hierarchy may reveal high level structural trends in the underlying data . Finally , in many practical settings one may obtain a partially complete initial taxonomy from human annotators . An interesting future direction would be to apply techniques such as TopicBlock to refine existing taxonomies [ 40 ] .
9 . ACKNOWLEDGMENTS
This work was supported by NSF IIS 0713379 , NSF IIS 1111142 ,
NSF DBI 0546594 ( Career ) , ONR N000140910758 , AFOSR FA9550010247 , NIH 1R01GM093156 , and an Alfred P . Sloan Research Fellowship to Eric P . Xing . Qirong Ho is supported by a graduate fellowship from the Agency for Science , Technology and Research , Singapore .
10 . REFERENCES [ 1 ] R . P . Adams , Z . Ghahramani , and M . I . Jordan . Tree Structured stick breaking processes for hierarchical data . In Neural Information Processing Systems , June 2010 .
[ 2 ] E . M . Airoldi , D . M . Blei , S . E . Fienberg , and E . P . Xing . Mixed membership stochastic blockmodels . Journal of Machine Learning Research , 9:1981–2014 , 2008 . [ 3 ] Alias i . Lingpipe 391 , 2010 . [ 4 ] S . Bethard and D . Jurafsky . Who should I cite : learning literature search models from citation behavior . In Proceedings of CIKM , pages 609–618 , 2010 .
[ 5 ] S . Bird , R . Dale , B . J . Dorr , B . Gibson , M . T . Joseph , M y Kan ,
D . Lee , B . Powley , D . R . Radev , and Y . F . Tan . The ACL anthology reference corpus : A reference dataset for bibliographic research in computational linguistics . In Proceedings of LREC , 2008 .
[ 6 ] D . Blei and J . Lafferty . Topic models . In Text Mining : Theory and
Applications . Taylor and Francis , 2009 .
[ 7 ] D . M . Blei , T . L . Griffiths , and M . I . Jordan . The nested chinese restaurant process and bayesian nonparametric inference of topic hierarchies . Journal of the ACM , 57(2):1–30 , Feb . 2010 .
[ 8 ] J . Chang and D . Blei . Hierarchical relational models for document networks . Annals of Applied Statistics , 2009 .
[ 9 ] A . Clauset , C . Moore , and M . E . J . Newman . Hierarchical structure and the prediction of missing links in networks . Nature , 453(7191):98–101 , May 2008 .
[ 10 ] D . Cohn and T . Hofmann . The missing link a probabilistic model of document content and hypertext connectivity . In Neural Information Processing Systems , 2001 .
[ 11 ] D . R . Cutting , D . R . Karger , J . O . Pedersen , and J . W . Tukey .
Scatter/Gather : a cluster based approach to browsing large document collections . In Proceedings of SIGIR , 1992 .
[ 12 ] S . Gerrish and D . Blei . A language based approach to measuring scholarly impact . In Proceedings of ICML , 2010 .
[ 13 ] T . Griffiths and M . Steyvers . Finding scientific topics . Proceedings of the National Academy of Sciences , 101:5228–5235 , 2004 .
[ 14 ] A . Gruber , M . Rosen zvi , and Y . Weiss . Latent topic models for hypertext . In Proceedings of UAI , 2008 .
[ 15 ] S . Gupta and C . Manning . Analyzing the dynamics of research by extracting key aspects of scientific papers . In Proceedings of IJCNLP , 2011 .
[ 16 ] D . Hall , D . Jurafsky , and C . D . Manning . Studying the history of ideas using topic models . In Proceedings of EMNLP , 2008 .
[ 17 ] T . H . Haveliwala . Topic sensitive pagerank : A context sensitive ranking algorithm for web search . IEEE Transactions on Knowledge Data Engineering , 15(4):784–796 , 2003 .
[ 18 ] Q . He , J . Pei , D . Kifer , P . Mitra , and C . L . Giles . Context aware citation recommendation . In Proceedings of WWW , pages 421–430 , 2010 .
[ 19 ] K . A . Heller and Z . Ghahramani . Bayesian hierarchical clustering . In
Proceedings of ICML . ACM , 2005 .
Figure 6 : The network block matrix for the ACL Anthology Data . Blocks corresponding to links within/between A3 and A6.1 have been delineated by black rectangles . There are 2190 and 2331 citation links within A3 and A6.1 respectively , but only 343 links between them . theoretical frameworks , such as centering theory and propositional semantics ( not shown here ) .
Occasionally , seemingly related topics are split into different parts of the tree . For example , the keywords for both topics A3 and A6.1 relate to syntactic parsing . Nonetheless , the citation links between these two topics are relatively sparse ( see Figure 6 ) , revealing a more subtle distinction : A3 focuses on representations and rule driven approaches , while A6.1 includes data driven and statistical approaches .
As in the Wikipedia data , the network diagram ( Figure 6 ) reveals evidence of hierarchical block structures . For example , A2 contains 4101 links out of 4.4 million possible , a density of 9.3∗ 10 −4 . This is substantially larger than the background density 1.8 ∗ 10 −4 , but less than subtopics such as A2.1 , which has a density of 6.4∗ 10 −3 . We observe similar multilevel density for most of the high level topics , except for interactive systems ( A1 ) , which seems to be more fractured . One of the densest topics is machine translation ( A8 ) , an area of computational linguistics which has become sufficiently distinct as to host its own conferences.5
One could obtain more parallel structure by imposing a domainspecific solution for research papers , such as Gupta and Manning ’s work on identifying the “ focus , technique , and domain ” of each article [ 15 ] ; of course , such a solution would not necessarily generalize to Wikipedia articles or other document collections . While parallel structure is desirable , it is often lacking even in taxonomies produced by human experts . For example , a similar critique might be leveled at the sessions associated with a research conference , or even the ACM taxonomy.6
8 . CONCLUSION
We have presented TopicBlock , a hierarchical nonparametric model for text and network data . By treating these two modalities jointly , we not only obtain a more robust latent representation , but are also able to better understand the relationship between the text and links . Applications such as link prediction , document clustering , and link ambiguity resolution demonstrate the strengths of our approach . In the future we plan to consider richer structures , such as multiple hierarchies which capture alternative possible decompositions
5http://wwwamtaweborg/ 6http://wwwcomputerorg/portal/web/ publications/acmtaxonomy
WWW 2012 – Session : Entity and Taxonomy ExtractionApril 16–20 , 2012 , Lyon , France747 Figure 5 : 3 level topic hierarchy built from the ACL Anthology .
[ 20 ] Q . Ho , A . P . Parkih , L . Song , and E . P . Xing . Multiscale Community
[ 32 ] X . Phan , L . Nguyen , and S . Horiguchi . Learning to classify short
Blockmodel for Network Exploration . In Proceedings of AISTATS , 2011 . and sparse text & web with hidden topics from large scale data collections . In Proceedings of WWW , 2008 .
[ 21 ] P . Holland and S . Leinhardt . Local structure in social networks .
[ 33 ] D . Radev , M . Joseph , B . Gibson , and P . Muthukrishnan . A
Sociological methodology , 7:1–45 , 1976 .
[ 22 ] J . Huang , H . Sun , J . Han , H . Deng , Y . Sun , and Y . Liu . Shrink : a structural clustering algorithm for detecting hierarchical communities in networks . In Proceedings of CIKM , pages 219–228 , 2010 . bibliometric and network analysis of the field of computational linguistics . Journal of the American Society for Information Science and Technology , 2009 .
[ 34 ] C . P . Robert and G . Casella . Monte Carlo Statistical Methods .
Springer , 2004 .
[ 23 ] A . Lancichinetti and S . Fortunato . Community detection algorithms :
[ 35 ] M . Rosen Zvi , T . Griffiths , M . Steyvers , and P . Smyth . The
A comparative analysis . Physical Review E , 80(5):056117+ , Nov . 2009 .
[ 24 ] D . D . Lewis , Y . Yang , T . G . Rose , and F . Li . Rcv1 : A new benchmark collection for text categorization research . Journal of Machine Learning Research , 5:361–397 , December 2004 .
[ 25 ] Y . Liu , A . Niculescu Mizil , and W . Gryc . Topic link lda : joint models of topic and author community . In Proceedings of ICML , 2009 .
[ 26 ] C . D . Manning , P . Raghavan , and H . Schütze . Introduction to
Information Retrieval . Cambridge University Press , Cambridge , UK , 2008 . author topic model for authors and documents . In Proceedings of UAI , pages 487–494 , 2004 .
[ 36 ] N . Sahoo , J . Callan , R . Krishnan , G . Duncan , and R . Padman .
Incremental hierarchical clustering of text documents . In Proceedings of CIKM , 2006 .
[ 37 ] J . Shi and J . Malik . Normalized cuts and image segmentation .
Transactions on Pattern Analysis and Machine Intelligence , 22(8):888–905 , Aug . 2000 .
[ 38 ] Y . Teh , M . Jordan , M . Beal , and D . Blei . Hierarchical dirichlet processes . Journal of the American Statistical Association , 101(476):1566–1581 , 2006 .
[ 27 ] A . McCallum , A . Corrada Emmanuel , and X . Wang . Topic and role
[ 39 ] P . Willett . Recent trends in hierarchic document clustering : A discovery in social networks . In Proceedings of IJCAI , 2005 . [ 28 ] Q . Mei , D . Cai , D . Zhang , and C . Zhai . Topic modeling with network regularization . In Proceedings of WWW , 2008 .
[ 29 ] R . Nallapati , A . Ahmed , E . P . Xing , and W . W . Cohen . Joint latent topic models for text and citations . In Proceedings of KDD , 2008 .
[ 30 ] R . Nallapati , D . McFarland , and C . Manning . Topicflow model :
Unsupervised learning of topic specific influences of hyperlinked documents . In Proceedings of AISTATS , 2011 .
[ 31 ] Y . Petinot , K . McKeown , and K . Thadani . A hierarchical model of web summaries . In Proceedings of ACL , 2011 . critical review . Information Processing & Management , 24(5):577–597 , 1988 .
[ 40 ] F . Wu and D . S . Weld . Automatically refining the wikipedia infobox ontology . In Proceedings of WWW , 2008 .
[ 41 ] Y . Zhao , G . Karypis , and U . Fayyad . Hierarchical clustering algorithms for document datasets . Data Mining and Knowledge Discovery , 10(2):141–168 , Mar . 2005 .
WWW 2012 – Session : Entity and Taxonomy ExtractionApril 16–20 , 2012 , Lyon , France748
