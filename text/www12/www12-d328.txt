S2ORM : Exploiting Syntactic and Semantic Information for Opinion Retrieval
Institute of Computer Science and Technology & The MOE Key Laboratory of Computational Linguistics ,
Liqiang Guo , Xiaojun Wan
Peking University , Beijing 100871 , China {guoliqiang , wanxiaojun}@pkueducn
ABSTRACT Opinion retrieval is the task of finding documents that express an opinion about a given query . A key challenge in opinion retrieval is to capture the query related opinion score of a document . Existing methods rely mainly on the proximity information between the opinion terms and the query terms to address the key challenge . In this study , we propose to incorporate the syntactic and semantic information of terms into a probabilistic language model in order to capture the query related opinion score more accurately . Categories and Subject Descriptors H33 [ Information Storage and Retrieval ] : Information Search and Retrieval – retrieval models General Terms Algorithms , Experimentation Keywords Opinion Retrieval , Syntactic Tree , Semantic Relatedness 1 . INTRODUCTION Blog opinion retrieval is the task of finding blog posts that express an opinion about a given query . In most opinion retrieval systems , two scores are required to be computed for each blog post : the relevance score against the query and the opinion score about the query . The two scores are then combined and the blog posts are finally ranked according to the combined scores . The key challenge of opinion retrieval is how to capture the query related opinion score . The simplest method is to compute the proportion of the opinion terms or sentences in a document ( blog post ) as the query related opinion score , but it is not accurate because the process of calculating the opinion score is independent of the given query . This proximity based method in [ 1 ] assumes that if the distance between an opinion term and a query term is smaller , the opinion term is more likely to be related to this query term . This assumption is not very appropriate , and it may be wrong in several cases . In this study , we propose to make use of the syntactic and semantic information of terms to capture the queryrelated opinion score more accurately . We propose to incorporate the syntactic and semantic information into a probabilistic opinion retrieval model ( S2ORM ) . The experiment results demonstrate the effectiveness of our S2ORM model , which can outperform the state of the art proximity based model [ 1 ] and other baselines . 2 . S2ORM 2.1 Basic Model In this study , our proposed S2ORM model is based on the following generative model [ 1 ] :
( cid:1868)(cid:4666)(cid:1856)|(cid:1867),(cid:1869)(cid:4667)(cid:1503)(cid:1868)(cid:4666)(cid:1856),(cid:1867),(cid:1869)(cid:4667)(cid:1868)(cid:4666)(cid:1856)(cid:4667)(cid:1868)(cid:4666)(cid:1869)|(cid:1856)(cid:4667)(cid:1868)(cid:4666)(cid:1867)|(cid:1869),(cid:1856)(cid:4667 )
Copyright is held by the author/owner(s ) . WWW 2012 Companion , April 16–20 , 2012 , Lyon , France . ACM 978 1 4503 1230 1/12/04 .
( cid:1868)(cid:4666)(cid:1867)|(cid:4667)(cid:1868)(cid:4666)|(cid:1856)(cid:4667 )
( cid:3047)(cid:1488)(cid:3031 ) related opinion probability of the document . Moreover , we use a combination parameter to tune the two parts . and semantic information . With the assumption that all opinion terms appearing in the document are related to the query , the
The first part can be directly obtained from the TREC baselines . The second part can be further refined by smoothing it with the opinion probability of the document as follows [ 1 ] : the term t . Usually the opinion probability of a term can be obtained from an opinion lexicon . 2.2 Syntactic and Semantic Information Based Query Related Opinion Probability
( cid:3045)(cid:3028)(cid:3038)(cid:2880 ) log(cid:4666)(cid:1868)(cid:4666)(cid:1856)(cid:4667)(cid:1868)(cid:4666)(cid:1869)|(cid:1856)(cid:4667)(cid:4667)(cid:3397)log(cid:3435)(cid:1868)(cid:4666)(cid:1867)|(cid:1869),(cid:1856)(cid:4667)(cid:3439 ) The first part ( cid:1868)(cid:4666)(cid:1856)(cid:4667)(cid:1868)(cid:4666)(cid:1869)|(cid:1856)(cid:4667 ) is the relevance probability of the document to the query . The second part ( cid:1868)(cid:4666)(cid:1867)|(cid:1869),(cid:1856)(cid:4667 ) is the query(cid:3045)(cid:3028)(cid:3038)(cid:2880 ) ( cid:2020)log(cid:4666)(cid:1868)(cid:4666)(cid:1856)(cid:4667)(cid:1868)(cid:4666)(cid:1869)|(cid:1856)(cid:4667)(cid:4667)(cid:3397 ) ( cid:4666)1(cid:2020)(cid:4667)log(cid:3435)(cid:1868)(cid:4666)(cid:1867)|(cid:1869),(cid:1856)(cid:4667)(cid:3439 ) ( cid:1868)(cid:3046)(cid:4666)(cid:1867)|(cid:1869),(cid:1856)(cid:4667)(cid:2019)(cid:1868)(cid:4666)(cid:1867)|(cid:1869),(cid:1856)(cid:4667)(cid:3397)(cid:4666)1(cid:2019)(cid:4667)(cid:1868)(cid:4666)(cid:1867)|(cid:1856)(cid:4667 ) In this study , ( cid:1868)(cid:4666)(cid:1867)|(cid:1869),(cid:1856)(cid:4667 ) is captured by making use of the syntactic opinion probability of the document ( cid:1868)(cid:4666)(cid:1867)|(cid:1856)(cid:4667 ) is defined as follows : ( cid:1868)(cid:4666)(cid:1867)|(cid:1856)(cid:4667)∑ where ( cid:1868)(cid:4666)|(cid:1856)(cid:4667)count(cid:4666),(cid:1856)(cid:4667)/|(cid:1856)| denotes the relative frequency of term t in the document , and ( cid:1868)(cid:4666)(cid:1867)|(cid:4667 ) is the opinion probability of Formally , we regard a document as a sentence set , eg , ( cid:1856 ) ( cid:4668),…,,…,(cid:3040)(cid:4669 ) . Each sentence can be regarded as a term set , eg , ( cid:4668),…,,…,(cid:4669 ) . So we have : ( cid:1868)(cid:4666)(cid:1867)|(cid:1869),(cid:1856)(cid:4667)∑ ( cid:1868)(cid:4666)(cid:1867),|(cid:1869),(cid:1856)(cid:4667 ) where ( cid:1868)(cid:4666)(cid:1867),|(cid:1869),(cid:1856)(cid:4667 ) is the query related opinion probability of it can be estimated as : ( cid:1868)(cid:4666)(cid:1867),|(cid:1869),(cid:1856)(cid:4667 ) sentence ∑ ( cid:1868)(cid:3435)(cid:1867),(cid:3627)(cid:1869),(cid:1856)(cid:3439 ) , where ( cid:1868)(cid:3435)(cid:1867),(cid:3627)(cid:1869),(cid:1856)(cid:3439 ) denotes the query related ( cid:2880 ) opinion probability of term in sentence and it can be ( cid:1868)(cid:3435)(cid:1867),(cid:3627)(cid:1869),(cid:1856)(cid:3439)(cid:1868)(cid:3435)(cid:3627)(cid:1869),(cid:1856)(cid:3439)(cid:1868)(cid:3435)(cid:1867)(cid:3627),(cid:1869),(cid:1856)(cid:3439 ) Here we let ( cid:1868)(cid:3435)(cid:3627)(cid:1869),(cid:1856)(cid:3439)(cid:1868)(cid:4666)|(cid:1856)(cid:4667 ) with the assumption that and ( cid:1869 ) are conditionally independent given the document d . We assume with equal probability , ie , ( cid:1868)(cid:3435)(cid:3627)(cid:1856)(cid:3439 ) |(cid:3031)|(cid:3400)|(cid:3023)|(cid:1503 ) |(cid:3031)| . |(cid:1848)| is the size of the term lexicon . And , ( cid:1868)(cid:3435)(cid:1867)(cid:3627),(cid:1869),(cid:1856)(cid:3439 ) can be estimated as follows : ( cid:1868)(cid:4666)(cid:1867)|,(cid:1869),(cid:1856)(cid:4667 ) max(cid:3047)_(cid:3042)(cid:1488)(cid:3046)(cid:3284)(cid:1868)(cid:4666)(cid:1867)|(cid:4667)(cid:1868)(cid:3040)(cid:3042)(cid:3031)(cid:4666),_(cid:1867)(cid:4667)SR(cid:4666)_(cid:1867),(cid:1869)(cid:4667 ) ( cid:1868)(cid:3040)(cid:3042)(cid:3031)(cid:4666),_(cid:1867)(cid:4667 ) denotes the modifying probability between the term ( a potential opinion SR(cid:4666)_(cid:1867),(cid:1869)(cid:4667 ) denotes the semantic relatedness between the noun 221 Estimating ( cid:1868)(cid:3040)(cid:3042)(cid:3031)(cid:4666),_(cid:1867)(cid:4667 ) and estimated as follows : here that each term appears in every position of all the sentences term ) and the noun t_noun . t_noun and the query q .
The modifying probability between an opinion term and a noun in a sentence can be estimated by using the information derived from the syntactic tree structure of the sentence with the tree kernel
( cid:3040)(cid:2880 )
,
WWW 2012 – Poster PresentationApril 16–20 , 2012 , Lyon , France517 Table 1 : Performance of MAP over TREC baselines 1~5 using different semantic relatedness approaches with ( cid:1811)(cid:2778)(cid:2777)(cid:2777)(cid:2777),(cid:2755)(cid:2778),(cid:2756)(cid:2777).(cid:2782 ) b1 b2 b3 b4
0.2781 0.2841 0.2951 0.2965 0.2946
0.2506 0.2730 0.2797 0.2852 0.2785
0.3637 0.3743 0.3787 0.3824 0.3805
0.3222 0.3309 0.3354 0.3431 0.3368 b5
0.2906 0.2893 0.2959 0.3056 0.2953
WN_SR PTM_SR Mark PTM_SR KL PTM_SR Cos PTM_SR Euc
.
We compare our proposed S2ORM model with the following models over the TREC baselines 1~5 : GORM : This model is based on the classical generative model described in section 2.1 and it adopts the generic query related opinion probability method . Laplace : This is a proximity based model proposed by [ 1 ] . It uses the Laplace kernel function to capture the proximity information between the opinion terms and the query terms . LaplaceInt : The model is also proposed by [ 1 ] , which achieves the state of the art performance via smoothing the Laplace model with the document ’s opinion probability . The MAP results of Laplace and LaplaceInt shown in Table 2 are also published in [ 1 ] with its best parameter values . The MAP results of S2ORM are under ( cid:2019)04~05 , ( cid:2020 ) 05~08 and k increase in MAP over the LaplaceInt model . In particular , our
=1000~5000.We find that S2ORM achieves the best performance over TREC baselines 2~5 . On average , S2ORM gets 2.62 %
S2ORM model performs very well over TREC baselines 2 and 3 . The performance is not very high on TREC baseline 1 , because the file of TREC baseline 1 is not complete . method . Thus , a tree kernel based SVM classifier [ 3 ] ( TK_SVM ) is used to capture the modifying probability . The feature of TK_SVM is based on a Path enclosed Tree . It is the smallest common sub tree including two specified terms ( eg an opinion term and a query term ) in a syntactic tree . In order to train TK_SVM , we annotate some training data manually . Finally , we use the sigmoid function to normalize the ranking value into [ 0 ,
1 ] . ( cid:1868)(cid:3040)(cid:3042)(cid:3031)(cid:4666),_(cid:1867)(cid:4667)(cid:1861)(cid:1859)(cid:1865)(cid:1867)(cid:1861)(cid:1856)(cid:4666)TK_SVM(cid:4666),_(cid:1867)(cid:4667)(cid:4667 ) where TK_SVM(cid:4666),_(cid:1867)(cid:4667 ) denotes the output value of TK_SVM for the feature tree structure of and t_noun .
222 Estimating SR ( t_noun , q ) The semantic relatedness is a score that reflects the semantic relationship between the meanings of two concepts . The semantic relatedness between a noun t_noun and a query q can be estimated as : The first popular approach is using WordNet for capturing the semantic relatedness between two terms . 1 ) . WordNet based Semantic Relatedness ( WN_SR ) .
SR(cid:4666)_(cid:1867),(cid:1869)(cid:4667)avg(cid:3044)(cid:3284)(cid:1488)(cid:3292)SR(cid:4666)_(cid:1867),(cid:1869)(cid:4667 ) SR(cid:4666)_(cid:1867),(cid:1869)(cid:4667)(cid:4676)WN_SR(cid:4666)_(cid:1867),(cid:1869)(cid:4667 ) _(cid:1867)(cid:1488)Top ( cid:1863 )
0 else
We rank all the nouns according to their semantic relatedness values in descending order , and Top k denotes the set of the first k nouns in the ranking list . The second popular approach is making use of the probabilistic topic model ( PTM ) [ 4 ] . In PTM , the distribution of a term t can be represented as a vector . Thus , the semantic relatedness can be calculated by the Cosine metric ( Cos ) , the Kullback Leibler divergence metric ( KL ) , the Euclidian distance metric ( Euc ) or a Mark metric which is based on the conditional probability [ 4 ] . 2).Probabilistic Topic Model based Semantic Relatedness
( PTM_SR ( cid:2206) ) . SR(cid:4666)_(cid:1867),(cid:1869)(cid:4667)(cid:4676)PTM_SR(cid:1876)(cid:4666)_(cid:1867),(cid:1869)(cid:4667 ) _(cid:1867)(cid:1488)Top ( cid:1863 ) 0 else
The x can be replaced with ‘Cos’ , ‘KL’ , ‘Euc’ or ‘Mark’ . 3 . EVALUATION RESULTS The BLOG06 collection is used for evaluation , and it includes 150 queries in TREC Blog Track 2006~2008.The data of 2006~2007 is used as training data and the data of 2008 is used as test data . In the preprocessing phase , the stop words and link tables [ 2 ] are removed . The general opinion lexicon used in [ 2 ] is used in this study . We follow the approach in [ 1 ] to normalize the relevance scores . For TREC baselines 2~5 , we use the very simple linear method N1 in [ 1 ] . LRLR in [ 1 ] is used on TREC baseline 1 due to the poor performance of N1 on this baseline . In Section 222 , we propose two different approaches to capture the semantic relatedness between a noun and a query : WN_SR and PTM_SR x . The MAP results of different semantic relatedness approaches over TREC baselines 1~5 are shown in Table 1 . The low performance of WN_SR is attributed to some terms that are not included by WordNet , such as the query term ‘Carmax’ . So , all the following experiments adopt the PTM_SR x approach .
Table 2 : The MAP performance of different models over TREC baselines 1~5 . NoRerank is a baseline without using b1 b2 any re ranking model . b5 b3 b4
0.3239 NoRerank Laplace 0.3960 LaplaceInt 0.4020 GORM 0.3340 S2ORM 0.3975
0.2639 0.2881 0.2886 0.2811 0.3061
0.3564 0.3989 0.4043 0.3964 0.4223
0.3822 0.4267 0.4292 0.3571 0.4300
0.2988 0.3188 0.3223 0.3149 0.3332
( cid:2722)MAP
12.52 % 13.63 % 3.60 % 16.25 % avg 0.3250 0.3657 0.3693 0.3367 0.3778
4 . ACKNOWLEDGMENTS The work was supported by NSFC ( 61170166 ) , Beijing Nova Program ( 2008B03 ) , NCET ( NCET 08 0006 ) and National HighTech R&D Program ( 2012AA011101 ) . 5 . REFERENCES [ 1 ] S . Gerani , M . J . Carman , and F . Crestani . Proximity Based
Opinion Retrieval . In Proceedings of SIGIR '10 , 2010 .
[ 2 ] L . Guo , F . Zhai , Y . Shao and X . Wan . PKUTM at TREC 2010
Blog Track . In Proceedings of TREC'10 , 2010 .
[ 3 ] A . Moschitti . Making tree kernels practical for natural language learning . In Proceedings of EACL’06 , 2006 .
[ 4 ] M . Steyvers and T . Griffiths . Probabilistic Topic Models . In Landauer , T . , McNamara , D . , Dennis , S . , Kintsch , W . , Latent Semantic Analysis : A Road to Meaning . 2006 .
WWW 2012 – Poster PresentationApril 16–20 , 2012 , Lyon , France518
