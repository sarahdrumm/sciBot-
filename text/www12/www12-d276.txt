Unsupervised Extraction of Template Structure in Web
Search Queries
Sandeep Pandey Yahoo! Research 701 1st Avenue
Sunnyvale , CA 94089 spandey@yahoo inc.com
Kunal Punera Yahoo! Research 701 1st Avenue
Sunnyvale , CA 94089 kpunera@yahoo inc.com
ABSTRACT Web search queries are an encoding of the user ’s search intent and extracting structured information from them can facilitate central search engine operations like improving the ranking of search results and advertisements . Not surprisingly , this area has attracted a lot of attention in the research community in the last few years . The problem is , however , made challenging by the fact that search queries tend to be extremely succinct ; a condensation of user search needs to the bare minimum set of keywords . In this paper we consider the problem of extracting , with no manual intervention , the hidden structure behind the observed search queries in a domain : the origins of the constituent keywords as well as the manner the individual keywords are assembled together . We formalize important properties of the problem and then give a principled solution based on generative models that satisfies these properties . Using manually labeled data we show that the query templates extracted by our solution are superior to those discovered by strong baseline methods . The query templates extracted by our approach have potential uses in many search engine tasks ; query answering , advertisement matching and targeting , to name a few . In this paper we study one such task , estimating Query Advertisability , and emprically demonstrate that using extracted template information can improve performance over and above the current state of the art .
1 .
INTRODUCTION
The World Wide Web has grown in size exponentially for many years , and this has been accompanied by search engines becoming the preferred way that users find and access information online . Hundreds of millions of queries are issued to the major search engines everyday , almost all of them in the form of “ keywords ” . Over years of usage , the keyword search functionality has become the standard convention expected by users and supported by all major search engines . This has resulted in users becoming adept at reducing their search needs specification to the bare minimum set of keywords needed to help the search engine find relevant results . There have been some attempts amongst search engine practitioners to induce users to provide queries in natural text , most notably by Powerset [ 2 ] , but by and large all search engines expect users to express their search need via a small set of keywords .
Given these state of affairs , there has been a considerable amount of work on extracting as much information as possible from user queries in order to determine their intent [ 7 , 9 , 23 , 30 ] . Once this intent is extracted it can be used to provide relevant results improving the search experience [ 4 ] , to detect whether a query has a commercial intent [ 23 ] , to select a useful set of advertisements [ 8 ] , and even to learn from the user ’s interaction with the search engine [ 25 ] . Challenges in extracting the intent information from queries arise from scalability issues since any potential solution must scale to hundreds of millions of queries a day , as well as instrumentation issues since the only user actions search engines observe are user clicks [ 17 , 25 ] . However , the main difficulty arises from the brevity , and associated ambiguity , of the keywords in the query . Keywords can sometimes be ambiguous when considered without the surrounding context ; as in the oft cited example of the word “ jaguar ” , which can denote the car as well as the animal . On the other hand , considering all keywords in a query together to maintain the context can result in sparsity issues ; for example , determining the characteristics ( eg , advertisement click through rate ) for tail queries such as “ jaguar xj12 95 6.0l engine mount ” is largely infeasible given the rarity of the query [ 23 ] .
In this paper we seek to solve these issues by enriching search queries with information about the hidden structure underlying them . In particular , our goal is to develop methods that can automatically determine that the tail query “ jaguar xj 12 95 6.0l engine mount ” can be described using the <Brand,Model,Year,Part> pattern . We refer to such patterns as query templates and their constituents , such as Brand and Model , as attributes . Such an enrichment can help in inferring and catering to the user intent behind the query . For example , we can provide custom search experiences for certain templates , such as returning , on the search results page , the availability and prices for vehicles queried via the <Brand,Model,Year> query template . Moreover , these enrichments can help us in dealing with data sparsity by learning query characteristics ( such as clickthrough rates , search difficulty ) at the template level instead of the individual query level . In fact , in this paper we demonstrate that using query templates can improve performance over the state ofthe art method for estimating Query Advertisability .
Many past works have proposed methods to extract information in web search queries . Some of these analyze queries to obtain segmentations [ 5 , 21 ] , while others extract named entities from queries [ 16 , 24 , 22 ] . All these approaches require either direct supervision of the tasks , such as manually labeled seed data , or use ancillary information such web search click through data , both of which might be expensive or difficult to obtain . Some recent works , such as Agarwal et al . [ 3 ] and Sarkas et al . [ 26 ] focus on detecting templates in queries , while Szpektor et al . [ 28 ] use detected templates for improving query recommendations . However , these works as sume that the attribute set as well as the associated vocabularies are given as inputs in form of database relations or entity hierarchies . Given that search engines field queries on a wide array of domains ( with user interests following a long tail [ 12 ] ) and the terminology used in these queries is constantly in flux ( eg . , new movies are released every week ) , it would be prohibitively expensive to create and maintain these attributes and their vocabularies . Hence , in this work we consider the more realistic setting of extracting templates while constructing attributes and their vocabularies at the same time , without requiring any editorial/manual intervention .
At its very core this problem can be looked at as one of grouping keywords into attributes , and hence text mining algorithms such as Spherical k Means and LDA can be used [ 6 , 10 , 11 ] . While these approaches are promising , they are not designed to take into account constraints/properties specific to how users construct queries . For example , for any given domain , users are known to employ only a few configurations of attributes as query templates [ 3 ] , whereas both these methods allow queries to be generated from arbitrary combinations of attributes . Our proposed approach uses this knowledge to reduce the search space of attribute word vocabularies and template attribute configurations . In Section 3 we discuss this and other important properties of the problem and how they are incorporated in our model . While this adds significant complexity to the modeling process , we give a procedure to estimate the parameters of the model . In Section 5 we empirically demonstrate that , without any supervision , manual or external ( eg . click through data ) , our approach is able to extract meaningful templates and attributes from short keyword queries .
OUR CONTRIBUTIONS . 1 ) We formulate the problem of simultaneously extracting the templates in queries as well as learning the attributes and their vocabularies . As far as we know this is the first work to tackle this problem without requiring any seed input , external data ( eg . click through data ) , or manual supervision . 2 ) We describe various characteristics of the way that keyword search queries are formulated by users . We then give a generative model for queries that takes these properties into account . 3 ) We give a procedure based on Gibbs sampling to automatically infer the query templates and attributes in our model from observed search queries . 4 ) In our empirical analysis , we use labeled ground truth to show that our approach finds better query templates and attributes than two state of the art approaches based on LDA and k Means . To ensure representative and robust results we repeat our experiments on real world queries from three different domains . 5 ) In order to showcase the usefulness of the query templates detected by our approach we consider the problem of predicting the Advertisability of tail queries . Our experiments demonstrate that using automatically inferred query templates can significantly improve upon the state of the art [ 23 ] .
ORGANIZATION . In the next section we present our formulation of the problem of extracting templates and attributes from observed web search query data . Our proposed solution for this problem is presented in Section 3 . We postpone a detailed discussion of related work to Section 4 , where we can better contrast our proposed approach to it . In that section we also describe two existing approaches that can be adapted to extract query templates ; these serve as strong baselines in the empirical evaluation of our approach using labeled ground truth in Section 5 . In Section 5.5 we evaluate the templates discovered by our model on the task of predicting query advertisability . Finally , we summarize the paper in Section 6 .
2 . EXTRACTING QUERY TEMPLATES
In this section we formulate the problem of extracting query tem plates from web search queries . 2.1 Motivation for Extracting Query Templates In this paper our goal is to extract , with a completely unsupervised process , a set of domain specific query patterns that most search queries in a domain conform to . For example , we would like our approach to discover that many search queries in the Automobiles domain can be described using the <Brand , Model , Year> pattern . We refer to such patterns as query templates and their constituents as attributes . Here the attribute Brand denotes a placeholder for brand/manufacturer of the vehicle and can stand in for words such as Honda , Toyota , Ford , etc . Similarly , attribute Model can stand in for words such as Accord , Civic , etc . , and , Year denotes the year when the car was manufactured . Extracting attributes from query logs and enriching queries with the templates they conform to can facilitate many search engine operations . For example , due to sparsity issues search engines struggle with obtaining robust estimates of various properties , such as advertisement click through rates , of tail queries . With the ability to discover that the query “ jaguar xj12 95 6.0l engine mount ” corresponds to query template <Brand , Model , Year , Parts> , we can smooth the ad click through estimates of tail queries with the aggregated estimates of the corresponding templates . Other applications include building custom search solutions for some query templates and using the extracted templates for improved query recommendations [ 20 , 28 ] .
Here we note that some past works [ 3 , 5 , 16 , 21 , 24 , 22 , 26 , 28 ] have proposed methods to extract similar information from web search queries . However , these approaches require either direct supervision of the tasks , such as manually labeled seed data , or use ancillary information such web search click through data ( more details in Section 4 ) . The manually labeled seed data takes the form of attributes and their vocabularies in [ 3 , 26 ] and entity classes or hierarchies in [ 5 , 28 ] , all of which are expensive to create and maintain in a dynamically changing environment like web search . While the ancillary information needed in the form of entities [ 24 ] and text of documents [ 21 ] clicked in response to queries , can be difficult to obtain . Therefore , in this work we restrict ourselves to the setting of extracting templates while constructing attributes and their vocabularies at the same time , without requiring any editorial/manual intervention . 2.2 Desiderata for Template Model for Queries From our inspection of search query logs we observed that different users with the same search intent issue variations of a query to the search engine . However , at a fundamental level they follow a common process for generating these queries from a common underlying schema , as shown in Figure 1 . For example , for the search intent “ find information about the 6.0l engine mount of a 1995 jaguar xj12 ” some users might formulate the query “ jaguar xj 12 95 6.0l engine mount ” while others might use “ jaguar xj 95 engine mount ” . Both these queries can be thought of as being generated from the query template <Brand , Model , Year , Parts> with the latter query containing fewer terms from the attributes Model and Parts . Similar observations hold for other domains of web search such as Entertainment , Travel , etc . In each case while we do not know the underlying schema – the templates , attributes , and vocabularies are hidden – and do not observe the underlying generative process , we do see the generated query load . Our approach in this paper is to mathematically construct a realistic generative process for the queries so that we can reconstruct ( infer ) the hidden template structure used to generate them ( as shown in Figure 1 ) .
3 . PROPOSED APPROACH
In this section we formally describe our proposed generative model and give an algorithm to perform inference on it . We also describe how our modeling process satisfies the properties mentioned in Section 31 3.1 Generative Model
Our generative model works as follows . We start with a pool of template configurations,1 called candidate pool , whereby each configuration consists of a set of attributes . For example , say we have 3 attributes , Model , Year and Brand . Then the candidate pool of template configurations can be any subset of {<Model> , <Brand> , <Year> , <Model , Brand> , <Model , Year> , <Brand , Year> , <Model , Brand , Year>} . Note that the empty template configuration is not allowed in the pool . In a real world setting , the candidate pool can also be constructed by a domain expert .
Given the candidate pool we let the model choose T template configurations to construct vector θ where θt denotes the template configuration at index t.2 Then each query q picks a template index tq which leads to its template configuration θtq . This way the queries can pick from only those template configurations which are present in θ ( while the candidate pool has many more configurations ) , helping our model enforce PROPERTY I mentioned above . One way to think of this is that the candidate pool denotes the set of template configurations which are appropriate for the domain . Then the model chooses T template configurations from the candidate pool , θ , to best explain the generation of queries . By varying the value of T we can control the trade off between data likelihood and over fitting .
Lastly , given a template configuration for a query , from each attribute in the configuration we generate a few words and then arrange them to create the query . More specifically , each attribute has an associated Poisson distribution to determine the number of words it contributes ( PROPERTY III ) , and a Multinomial distribution over the vocabulary to decide which words it contributes . As an example , in our learnt model we expect the Poisson parameter for the attribute Year to be smaller than that for the attribute Parts . The priors for all distributions are chosen to be their conjugates ; Dirichlet for Multinomial and Gamma for Poisson . We enforce PROPERTY II within the inference process described in Section 32 Formally , the parameters of the model are :
µ ∼ Dirichlet(α ) {multinomial distribution over all the template configurations in the candidate pool}
θt ∼ Multinomial ( µ ) {configuration for template index t} γ ∼ Dirichlet(δ ) {vector of size equal to the number of template indices , say T , that queries are allowed to chose from tq ∼ Multinomial ( γ ) {template index for query q} φa ∼ Dirichlet(β ) {multinomial word distribution for attribute a} ηa ∼ Gamma(g1 , g2 ) {Poisson parameter for the number of words that attribute a contributes towards a query when the attribute is present in the template for the query}
Given these parameters each search query is generated by the fol lowing process :
1We use the phrases template configurations , attribute configurations , and query templates interchangeably 2For simplicity we construct θ by choosing configurations with replacement , ie , a configuration can make into θ at two different indices . In other words , for t = t , θt may be equal to θt .
Figure 1 : Search queries are assumed to be generated from a common hidden underlying structure . Our goal in this paper is to devise an algorithm to use the observed queries to reconstruct the parameters of the generative process as well as the underlying template structure
The simplest generative process would be single attribute template model whereby each template has a single unique attribute , and each attribute is associated with a set of words ( a word distribution ) . When constructing a query a user picks a template , and then picks words from the associated attribute . This , however , is an unrealistic process since we have seen in the examples above that most queries have words from different attributes of a domain , such as Brand , Year etc . Hence , we consider multi attribute template models whereby each template is associated with multiple attributes . Here , each attribute has its own word distribution from which the words are chosen to instantiate the query .
Besides having multi attribute templates we list some additional properties that we desire in our model . These properties bring the generative model closer to reality . Moreover , they are also critical for the model performance since queries are short and sparse , and so searching a more constrained/restrictive but realistic model space is likely to be more robust and perform better . PROPERTY I : While most queries are different , we believe that a large number of queries in a given domain can be captured by only a few templates . In other words , our model must not allow arbitrary distributions of attributes as templates ; instead we constrain the number of distinct templates that can be formed in the model . PROPERTY II : Once a template for generating a query is picked , we force each attribute in the template to generate at least one word . The intuition behind this is that since queries are short , with 2 3 words on average , if an attribute is not contributing any words towards a query , it is not required in the template . PROPERTY III : Each attribute has a specific word distribution as well as a distinct tendency for the number of words it contributes in a query . For example , the Year attribute in the Automobiles domain is likely to contribute 1 word ( eg , 1995 ) , while the Parts attribute typically generates more words ( eg , 6.0l engine mount ) .
Following this discussion we can formulate our problem as follows :
PROBLEM DEFINITION : Given a set of queries , extract the underlying schema ( templates , attribute , and their vocabularies ) and learn the parameters of the generative process in a completely unsupervised manner while respecting the properties mentioned above . Accounting for these properties significantly increases the technical complexity of our model over the state of the art baseline methods such as LDA [ 6 ] and Spherical k Means [ 10 , 11 ] that are not designed to enforce these properties . However , in Section 3.2 we show that it is still feasible , mathematically and computationally , to perform inference in our model . Moreover , we empirically show in Section 5 that these properties help our approach significantly improve performance over the baselines .
3 . Sample template configurations for each template index from 1 to T : θt ∼ Multinomial ( µ ) .
= where ∆(β ) =
1 . Sample the prior over template indices : γ ∼ Dirichlet(δ ) . 2 . Sample the prior over allowed template configurations : µ ∼
Dirichlet(α ) .
4 . For each attribute a :
( a ) Sample the word distribution prior : φa ∼ Dirichlet(β ) . ( b ) Sample the Poisson parameter : ηa ∼ Gamma(g1 , g2 ) .
5 . To generate words for query q :
( a ) Sample template index tq ∼ Multinomial ( γ ) , which leads to template configuration θtq
( b ) For each attribute a ∈ θtq : i . Sample ni,a ∼ Poisson(ηa ) . ii . Place attribute a at ni,a number of positions in vector zq .
( c ) For each position j in query q , sample word wq,j from attribute zq,j , ie , wq,j|zq,j , φzq,j ∼ Multinomial ( φzq,j ) .
There exist other probabilistic models that could be modified to extract templates ( such as LDA [ 6] ) . In Section 411 we describe these alternative models in detail and show our model differs from them . In Section 5 we empirically compare them against each other . 3.2 Model Learning
Here we describe how we perform inference on our model . Due to paucity of space we skip many intermediate steps of algebra . Interested readers are referred to a longer version of the paper for these detailed derivation of the expressions . Let θ denote the set of T template configurations ( ie , θ = {θt}T t=1 ) and η denote the Poisson parameters for all attributes A , ie , η = {ηa}|A| a=1 . Also , let w , z , t denote the sequence of words , attributes and templates over all queries . Given the hyperparameters , we first derive the joint distribution over the known and latent variables . The joint distribution is then used for inference in Section 321
P ( w , z , t , η , θ , φ , γ , µ | α , β , δ , g1 , g2 )
= ,P ( w|z , φ ) P ( φ|β) · ,P ( t|γ ) P ( γ|δ)·,P ( z|θ , t , η ) P ( η|g1 , g2) · ,P ( θ|µ ) P ( µ|α),P ( w|z , φ ) P ( φ|β) dφ · ,P ( t|γ ) P ( γ|δ) dγ ,P ( z|θ , t , η ) P ( η|g1 , g2) dη ·
Integration over η , φ , γ , µ gives : P ( w , z , t , θ | α , β , δ , g1 , g2 )
,P ( θ|µ ) P ( µ|α) dµ
=
·
= TERM I · TERM II · TERM III · TERM IV We compute each of these terms in turn . with attribute a . Then ,
,P ( w|z , φ ) P ( φ|β) dφ
|A|
∆( na + β )
∆(β ) a=1 dim β Γ(dim β k=1 Γ(βk ) k=1 βk ) where na = {n(w ) a }V w=1 .
TERM II ,P ( t|γ ) P ( γ|δ) dγ =
∆( nt + δ )
∆(δ ) where nt = {nt}T t=1 denotes the vector of counts of queries assigned to each template index , and T denotes the total number of template indices that queries are allowed to choose from .
TERM III
,P ( z|θ , t , η ) P ( η|g1 , g2) dη |Q|
1
=
,Poisson(nq,a|ηa ) nq,a! q nq! a∈θ(tq )
P ( η|g1 , g2 ) dη where nq denotes the length of query q and nq,a denotes the number of words from attribute a in the query .
,P ( z|θ , t , η ) P ( η|g1 , g2) dη |Q|
=
1 nq! q a∈A
ηa q∈all tq that have θ(tq ) with a nq,a!
Gamma(ηa|g1 , g2 )
Poisson(nq,a|ηa ) dηa
Since Gamma is conjugate prior for Poisson , the above integra tion can be simplified to :
,P ( z|θ , t , η ) P ( η|g1 , g2) dη
|Q|
=
1 nq! q a∈A g1 ( g1 + ( g2 + Na)(g1+ g2
Q nq,a − 1)! Q nq,a)(g1 − 1)! where Na denotes the number of queries with attribute a in their templates .
TERM IV ,P ( θ|µ ) P ( µ|α) dµ =
∆( nθ + α )
∆(α )
TERM I
P ( w|z , φ ) =
|A| a=1
{i:zi=a} p(wi = w|zi = a ) =
|A|
V a=1 w=1
φna(w ) a,w where na(w ) denote the number of times word w has been observed where the ith element of nθ is the number of template indices , from 1 to T , pointing to the template configuration θi .
Inference
321 Our goal is to infer the underlying attributes and templates for the given queries . Mathematically speaking , we want to infer the distribution P ( z , θ , t| w ) , which can be written as :
P ( z , θ , t| w ) =
=
P ( z , θ , t , w )
P ( w ) P ( z , θ , t , w ) z,t,θ P ( z , θ , t , w ) where we omit the hyperparameters for convenience . Clearly , the denominator in the above expression is a summation over a large number of combinations and is difficult to compute . Hence , we use Gibbs sampling to perform this inference [ 14 , 15 , 19].3 Under the Gibbs sampling procedure , the full conditional distributions ( P ( zi|z¬i , θ , t , w ) , P ( θi|z , θ¬i , t , w ) , P ( ti|z , θ , t¬i , w ) ) are used to simulate P ( z , θ , t| w ) . To derive these conditionals , we use the joint distribution P ( z , θ , t , w ) ( computed earlier ) : P ( z , θ , t , w|α , β , δ , g1 , g2 ) = P ( w|z , β ) p(θ|α ) p(t|δ ) p(z|θ , t , g1 , g2 )
Next we give derivation of the conditional distributions for different latent variables , ie , zi , θi , ti . As mentioned above , these conditionals are then used to perform the Gibbs sampling for inferring the query templates and attributes . The overview of our complete approach is given in Section 33 322 Computing the Conditional Distributions We start by computing the conditional distribution for z variable .
CONDITIONAL FOR z . Ideally , we would like to compute ( P ( zi|z¬i , θ , t , w ) where zi is the ith element of attribute sequence z . But in our case zi ’s are not sampled independently as this can result in one of the attributes contributing zero words to the query ( we do not allow this under PROPERTY II mentioned above ) . Hence , the sampling is done on per query basis , say zq , which consists of the attributes for every word in the query . Hence , it is convenient to compute the conditional in terms of ( P ( zq|z¬q , θ , t , w ) .
P ( zq = vq|z¬q , θ , t , w ) = P ( w|zq = vq , z¬q )
=
P ( w¬q|z¬q )
P ( zq = vq , z¬q , θ , t , w )
P ( z¬q , θ , t , w ) P ( zq = vq , z¬q|θ , t )
P ( z¬q|θ , t )
P ( t ) P ( t )
P ( θ ) P ( θ )
1 wq where vq denotes a new attribute sequence for query q . Let na denote the vector for attribute a which is of the length equal to the vocabulary size . It consists of the counts of the number of times each word in the vocabulary came from attribute a . In Appendix 8 we derive an expression for ( P ( zq|z¬q , θ , t , w ) and show that :
P ( zq = vq , z¬q|t , θ )
P ( z¬q|t , θ )
=
1 nq
( g1 + ( g1 +
( g2 + Na − 1)(g1+ ( g2 + Na)(g1+ Q ni,a − 1 − nq,a + vq,a)! Q ni,a − 1 − nq,a)! a∈vq
Q ni,a−nq,a ) Q ni,a−nq,a+vq,a ) where Na denotes the number of queries with attribute a in their templates , ni is the length of query i , ni,a is the number of terms in query i from attribute a , and nq,a and vq,a denote the number of
3In the general formulation of a Gibbs sampler , the latent variables , say x , are estimated by computing : P ( x| w ) = P ( xi|x¬i , w ) = P ( x¬i , w ) =
P ( x , w )
P ( x , w )
X P ( x , w)dxi
. terms from attribute a in the old query configuration q and the new configuration v , respectively .
CONDITIONAL FOR t . Recall that t denotes the vector consisting of the template indices of all queries . We compute the conditional by deriving the probability of updating the template index of query j . P ( z|θ , t )
1
P ( tj = k|t¬j , z , w , θ ) =
∝
We computed
P ( z|θ,t )
P ( z¬j|θ,t¬j )
P ( tj = k , t¬j )
P ( t¬j )
=
=
P ( zj|θ )
P ( t ) P ( t¬j ) P ( t ) P ( t¬j )
P ( z¬j|θ , t¬j )
P ( z|θ , t )
P ( z¬j|θ , t¬j ) earlier . Next we look at P ( t )
P ( t¬j ) .
∆( nt + δ )
∆(δ )
∆(nt,¬j + δ )
∆(δ ) n(k ) t,¬j + δk k n(k ) t,¬j + δk t and n(k ) where n(k ) t,¬j denote the number of queries assigned with template index equal to k , with and without including query j . Hence , t = n(k ) n(k ) t,¬j + 1 .
CONDITIONAL FOR θ . Next we compute the conditional for θ , ie , P ( θj|θ¬j , z , w , t ) . Recall that θ denotes the vector of T template configurations selected by the model , from which each query is assigned a configuration . Note that by updating the index j of θ to any configuration c ( ie , set θj = c ) , we indirectly update the template of each query that is pointing to θj ( ie , queries which have tq = j ) . Hence ,
P ( θj = c|θ¬j , z , w , t ) =
∝
P ( θj = c , θ¬j , z , w , t )
P ( θ¬j , z , w , t ) P ( z|θ , t )
P ( z¬j|θ¬j , t )
P ( θ ) P ( θ¬j ) where z¬j denotes the attribute sequence , excluding all the queries whose template is pointing to θj configuration . We can compute similar to how we computed it earlier . Next we look at
P ( z|θ,t )
P ( z¬j|θ¬j ,t ) P ( θ ) P ( θ¬j )
.
P ( θj = c , θ¬j )
P ( θ¬j )
=
=
∆(nθ + α )
∆(α )
∆(nθ,¬j + α )
∆(α ) n(c ) θ,¬j + αc c n(c )
θ,¬j + αc and n(c ) where n(c ) θ
θ,¬j denote the number of elements in vector θ that have attribute configuration c , with and without including element j . Hence , n(c ) 3.3 Overview of the Algorithm
θ = n(c )
θ,¬j + 1 .
Above we have described the model and the update equations ( ie , conditionals ) . Here we summarize how the conditionals are used to perform the inference . The proceduce begins with a random initialization of the queryword attribute assignment z , the querytemplate assignment t , and the set of T template configurations θ .
Then we iterate over queries and the template set using the conditionals derived in Section 322 to update the z , t , and θ vectors . At each iteration we compute the likelihood of the observed query data given the current learnt model parameters ( φ , µ , γ , η ) . The procedure ends with an assignment of each query to a template , and each word in the query to an attribute .
4 . ALTERNATIVE APPROACHES AND
A SURVEY OF RELATED WORK
Some existing methods can be adapted to tackle the problem of discovering templates ; in this section we describe two such approaches , LDA , and k Means , and highlight the ways in which our proposed model differs from them . These approaches also serve as baselines in our experiments ( Section 5 ) . We end this section with a survey of some past works that are broadly related to our problem setting . 4.1 Alternative Approaches 411 Latent Dirichlet Allocation ( LDA ) Latent Dirichlet Allocation is a popular model for unsupervised discovery of document topics [ 6 , 15 ] . Before showing how it can be applied for template detection , we briefly describe LDA generative model here . In the LDA model , each topic has an associated φ distribution over the words in the vocabulary . To construct document d , first a multinomial distribution over the topics , denoted by θd is sampled from a Dirichlet prior . The ith word in the document is picked by choosing a topic from this multinomial distribution , and then sampling a word from the φ distribution associated with the topic . In other words : wi|zi , φ(zi ) ∼ Discrete(φ(zi ) ) ∼ Dirichlet(β ) φ ∼ Discrete(θ(di ) ) zi|θ(di ) ∼ Dirichlet(α ) θ where α and β are the hyperparameters . In our scenario each query can be thought of as a document . By applying LDA we can find the attributes ( ie , the topics ) and their associated vocabularies that have been used to generate the queries . These attributes can then be used to construct the query templates . Here we note that there are some fundamental differences between the LDA model and our proposed generative model ( Section 3 ) . First , LDA allows each query to pick an arbitrary topic distribution , while we constrain the query to conform to one of the finite templates ( ie , attribute distributions ) . This conforms to our intuition about how search engine users generate queries for completing their tasks , and allows us to limit the choices in the query workload generation process to a small set of templates . Or in other words , each template is made to span many queries , which is desired . Second , even if the topic distribution for a query has a high probability for a topic , LDA does not require that topic to contribute a word in the query . In contrast , our model forces each attributes in the template to contribute at least one word to the query . This is important since unlike documents , queries are short and probabilistically sampling the attributes/topics from the topic distribution ( like LDA does ) will lead to a large variance . As we can see our approach is tuned specifically for the constraints observed in search queries , while LDA is not .
Incorporating these properties in our model adds significant technical complexity . For example , for enforcing PROPERTY I from Section 2.2 we maintain a finite pool of allowed configurations ( θ ) . All other latent variables ( eg , z , t ) depend on this θ vector , as a result , in the inference process when θ is updated , all other latent variables have to be updated accordingly . Similar , enforcing PROP
ERTY II means that we cannot sample the attributes for words ( z ’s ) independently . Instead , the sampling is done on a per query basis . Moreover , while performing this sampling , we enforce PROPERTY III as well to allow different attributes to have different tendency of contributing words .
In our experiments we justify this added complexity by comparing LDA with our approach and showing that our approach results in much better performance . k Means
412 The problem of finding attributes can be framed as a problem of grouping the query words , and hence any text clustering approach can be used . In this section we describe how Spherical k Means [ 10 , 11 ] can be applied . We represent each word wu as a vocabularysized vector , where each element v of the vector contains the number of times words wu and wv occur together in queries . By running Spherical k Means on these vectors , we put together those words into a cluster which have similar co occurrence behavior with other words , eg , words such as accord and civic should have similar co occurrence behavior with respect to brand words and year , and should fall into the same cluster . Hence , these clusters act like attributes for our scenario , using which we then construct query templates . 4.2 Survey of Related Work
The field of modeling query keywords has been explored in many past works . While a full survey is not possible due to lack of space , we discuss some key works that can help us put our work into context .
The problem of assigning an attribute to each word in a query has been explored in [ 3 , 26 ] . Agarwal et al . [ 3 ] proposed an approach based on random walk on the tri partite graph of queries , sites , and templates . Sarkas et al . [ 26 ] assumed that “ structured ” data is given in the form of tables . Then queries are annotated by mapping a query to a table and the attributes of this table . Both these works , however , assume that the attributes and their vocabularies are given as input to the algorithms . In contrast our work finds both the query templates as well as the attributes and their vocabularies in a completely unsupervised manner .
The problem of named entity recognition in web queries ( eg . finding movie names ) is related to our problem with entities playing the role of attributes . In [ 16 ] , Guo et al . give a nice semi supervised approach to extract entities from queries while ensuring that the model topics and pre defined classes align . In [ 31 ] the approach learns a topic model using click through data under some supervision from humans and uses the model to resolve ambiguities among named entities . In another related work [ 22 ] a weakly supervised extraction framework is given for extracting named entities from web search queries starting from a seed set . Another set of work seek to obtain useful segmentations of web queries via learning from labeled examples [ 5 ] or using click through data [ 21 ] . Our work differs from these in the focus – we focus assigning all words to attributes , not just named entities – and based on the fact that our proposed approach works with just the query set and does not need manual intervention or data click through interactions .
Finally , there are recent works that seek to exploit query templates for accomplishing search related tasks . In [ 28 ] Szpektor et al . used entity hierarchies to mine query templates , which were used to improve query recommendation algorithms by defining better relationships between queries . Similarly , Jain et al . [ 20 ] use many heuristics to define relationships between queries , and query templates could be used as one such signal . Hence , our work is complementary to these works that seek to improve web mining algorithms via query templates and could act as input into them .
5 . EXPERIMENTS
In this section we analyze the performance of our approach ( QTGEN ) on real world search engine queries . We also provide an empirical comparison with the state of the art alternatives described in Section 41 We begin with a description of the experimental methodology and then proceed to describing the results . We end the section with an application of our approach to the task of predicting query advertisability . 5.1 Experimental Setup
Domain
Automobiles
Travel
Movies
Attribute Brand Model Year Parts Specs
Vehicle_type
Tasks Brand Location Qualifiers
Tasks Names Genres Qualifiers
Vocabulary
Vocab size
Honda , Toyota . . . Civic , Camry . . . 2010 , 2009 . . . engine , tires . . . mpg , 250hp . . . car , sedan . . . purchase , flights . . . hilton , southwest . . . hawaii , SFO . . . cheap , discount . . . tickets , reviews . . . avatar , brad pitt . . . horror , bollywood . . . free , online . . .
56 188 73 148 30 44 42 27 132 27 54 60 25 20
We begin by describing the methodology behind the construction of the query dataset and ground truth . Then we describe the implementation details of our approach , QTGEN , and of the competing baselines . QUERY DATA SET AND GROUND TRUTH .
In order to obtain robust results and reduce the effect of any one topic we perform our empirical evaluation using queries from multiple different domains : Automobiles , Travel , and Movies . This is the standard methodology in this area . Topical classification of queries is a well studied problem and we use a state of the art multilabel classifier [ 29 ] to classify a randomly sampled set of 100 million Yahoo search queries executed in September of 2010 into the above domains . These queries were suitably anonymized and care was taken to remove all personally identifiable information was removed . From these domain specific queries we construct datasets for our two evaluation tasks . First , we construct three domainspecific query sets of roughly 1000 queries each ; the size was picked so that we could manually construct the ground truth . Second , for the large scale automatic evaluation on the query advertisability task we construct three datasets with 43793 , 83387 , and 15050 tail queries from the Automobiles , Travel , and Movies domains , respectively . For this task we also obtain the sponsored search impressions and clicks from the search logs .
In order to construct the ground truth , templates underlying the queries in these query sets were manually extracted . This resulted in the construction of 6 attributes for Automobiles domain and 4 each for the Travel and Movies domains , each of which was populated with the words likely to be generated from them ; some words were labeled as being generated from multiple attributes . Some details about the ground truth are given in Table 1 . Note that the ground truth construction was completed before the outputs of any of the approaches under evaluation were seen by the labelers . OUR APPROACH AND BASELINES .
QTGEN : This is an implementation of the approach outlined in Section 3 . Each run of QTGEN is parameterized by the following settings : number of attributes ( k ) , number of iterations ( N ) , and values of model parameters β , g1 , and g2 . Please see the description of our model for details of these parameters . In our empirical analysis we perform many experiments by varying values for these parameters , but unless mentioned otherwise , the values are set to k = 5 , N = 100 , β = 0.1 , g1 = 4 , and g2 = 0.2 ; these parameter values were tuned using a 10 % validation set .
LDA : For this baseline approach we used the Mallet [ 1 ] implementation of Latent Dirichlet Allocation , which has been described in detail in Section 411 The parameters values are set to k = 5 , α = 50 and β = 001
K MEANS : This baseline approach is described in detail in Section 412 In this implementation the initialization is done via farthest first approach . For our experiments , k was set to 5 ( set using a 10 % validation dataset ) and distance measure between vectors was
Table 1 : Ground Truth : attributes and vocabularies created manually . The learned attributes output by QTGEN and baselines are evaluated in terms of their match to this ground truth . computed using cosine similarity [ 11 ] . 5.2 Evaluation on Manually Labeled Ground
Truth
Above we described the process through which the templates that generate queries were manually extracted for three domains . In this section we evaluate the performance of QTGEN , LDA , and K MEANS in successfully retrieving the attributes that form these templates . Before proceeding to the results we describe the metrics we use to evaluate the outputs of the various approaches . INTERPRETING THE OUTPUT OF ALGORITHMS .
Traditional clustering evaluation measures such as pairs based ones ( eg , Adjusted RAND [ 18 ] ) and entropy based ones ( eg , normalized mutual information [ 27 ] ) are not suited for the tasks we consider in this paper . In designing an evaluation scheme relevant to our problem setting , we first attempt to understand how the extracted templates/attributes are likely to be used . Most applications of interest need the attributes to be returned in some order , and since none of the approaches under evaluation ranks the attributes , we assume each approach returns attributes in the decreasing order of the number of queries that generate words from it . Once the order is set , we want that each learned attribute contain words from only one attribute from ground truth . Moreover , we want that for each attribute in the ground truth , all its words be covered by one learnt attribute . To fulfill these requirements each ground truth attribute must be mapped to a unique learnt attribute . In a real life application , this mapping would be constructed by a human expert when she studies the attributes output by the system . In order to evaluate numerous runs of our approach and baselines objectively , we construct this mapping automatically by evaluating all possible mapping in terms of the total AUC [ 13 ] between the learnt and the ground truth attributes , and picking the one with the maximum score .
Once the mapping is fixed , we go over the learnt attributes in the order established and evaluate them in terms of net PRECISION and CORRECTRECALL . We say that a word is correctly placed if the learnt attribute and the ground truth attribute it belongs to are mapped to each other . Hence , PRECISION(N ) is the fraction of words in the first N learnt attributes ( in the algorithm ’s ordering ) that are correctly placed . Similarly , CORRECTRECALL(N ) , is the fraction of words in ground truth attributes mapped to the first N learnt attributes that are correctly placed . Intuitively , it can be seen that PRECISION measures the accuracy of the system while CORRECTRECALL measures the correct coverage . PRECISION VS . CORRECTRECALL .
We plot PRECISION and CORRECTRECALL of various approaches on the three domains in Figures 2 , 3 , and 4 . The PRECISION is plot
Figure 2 : PRECISION vs . CORRECTRECALL curves of QTGEN and baselines on the Automobiles domain . Each mark on the curves ( from left to right ) is produced by evaluating the top N learnt attributes .
Figure 4 : PRECISION vs . CORRECTRECALL curves of QTGEN and baselines on the Movies domain . Each mark on the curves ( from left to right ) is produced by evaluating the top N learnt attributes .
Figure 3 : PRECISION vs . CORRECTRECALL curves of QTGEN and baselines on the Travel domain . Each mark on the curves ( from left to right ) is produced by evaluating the top N learnt attributes .
Figure 5 : PRECISION vs . CORRECTRECALL curves of QTGEN after different number of iterations on the Automobiles domain . ted on the y axis while the CORRECTRECALL is on the x axis . The markers on each curve indicate the performance values at different N , with markers on the left indicating values for lower N . As we can see , in general , while evaluating at higher values of N , PRECISION tends to decrease while CORRECTRECALL monotonically increases .
First we observe that attributes found by QTGEN match the ground truth very precisely in their word compositions . For example , in the Automobiles domain , the top 3 learnt attributes have > 60 % of the words that were assigned to them and were also manually labeled as belonging to these attributes . Similar results are also seen for the other two domains . Being this precise makes the learnt attributes easier for downstream human users to interpret as well as easier for automated algorithms to use . We also observe that for all domains and operation points , the attributes found by QTGEN dominate those found by LDA and K MEANS in terms of CORRECTRECALL . The runners up in terms of PRECISION is clearly the KMEANS approach ; in fact it equals QTGEN in terms of PRECISION for the Movies domain .
The second observation about the results concerns the CORRECTRECALL of the learnt attributes . As we can see , upon learning 5 attributes our approach consistently finds around 50 % of the words in the matched ground truth attributes . On the Automobiles dataset , QTGEN vastly outperforms the baseline approaches , while on the other two domains the algorithms are roughly comparable in terms of CORRECTRECALL . In fact , LDA , and to a lesser degree KMEANS , consistently operate at a lower PRECISION and slightly higher CORRECTRECALL profile . In many applications producing output with high CORRECTRECALL is very important , however , we feel that the the low PRECISION of attributes learnt by LDA and K MEANS will make them too obscure to be used automatically by algorithms or manually by human editors .
Finally , we note that the results indicate that learnt attributes that are heavily used also score high in terms of PRECISION and CORRECTRECALL . This is completely true in the case of the Automobiles domains and to a slightly lesser degree for the other two domains . This can be seen in the curves of QTGEN in Figures 2 , 3 , and 4 which are constructed by considering attributes in the decreasing order of their usage . As we can see for the performance curve of QTGEN on Automobiles domain ( Figure 2 ) , the PRECISION drops monotonically , indicating that the most precise attributes are the most heavily used , and the gaps between successive marks on the x axis become smaller , indicating the same trend for the CORRECTRECALL of learnt attributes . It is also clear from the figures that the attributes learnt by LDA and K MEANS display this desirable property to a much lesser degree .
To summarize , we have seen that QTGEN outperforms the baselines in terms of obtaining interpretable attributes that cover a large fraction of the query terms correctly . The parameter setting used to report this performance was tuned over a 10 % held out validation dataset . Next , we show the effect of variation of these parameters on the performance of QTGEN in terms of PRECISION and CORRECTRECALL . 5.3 Effect of Parameter Values EFFECT OF NUMBER OF ITERATIONS .
We start by studying the effect of the number of model iterations . In Figure 5 we have plotted the scores of attributes identified by QTGEN after a fixed number of iterations . These performance numbers are plotted by averaging the result of multiple runs with random initializations , but we do not show the confidence intervals to reduce clutter . Note that the results of this experiment on all three domains were very similar and hence we show them only for the Automobiles domain . From the results displayed in Figure 5 we make two observations .
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 0.1 0.2 0.3 0.4 0.5 0.6PRECISION ( N)CORRECT RECALL ( N)Automobile domainQTGENLDAk Means 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 0.1 0.2 0.3 0.4 0.5 0.6PRECISION ( N)CORRECT RECALL ( N)Travel domainQTGENLDAk Means 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 0.1 0.2 0.3 0.4 0.5 0.6PRECISION ( N)CORRECT RECALL ( N)Movies domainQTGENLDAk Means 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0 0.1 0.2 0.3 0.4 0.5 0.6PRECISION ( N)CORRECT RECALL ( N)Automobile domainModelYearBrandParts#iters=100#iters=75#iters=50#iters=25#iters=10 Figure 6 : PRECISION vs . CORRECTRECALL curves of QTGEN using different number of attributes on the Automobiles domain .
Figure 7 : PRECISION vs . CORRECTRECALL curves of QTGEN run with different values of φ on the Automobiles domain .
First , it can be easily seen that while the performance of QTGEN does improve with increasing iterations , after very few iterations ( >50 ) the performance differences become indistinguishable . The only statistically significant differences in performance are between the #iters = 10 and #iters = 25 curves and the rest . This fast convergence of QTGEN can be attributed to the fact that it imposes a lot of structure derived from domain knowledge on the model . For instance , in our model queries are required to be generated from one of a few attribute configurations and each attribute is required to generate at least one word . These restrictions reduce the number of choices that must be evaluated and hence the convergence is achieved in fewer iterations .
Second , we observe that QTGEN converges faster to a precise characterization of attributes that are used more frequently than it does for other less frequently used attributes . In Figure 5 we have annotated each mark of a converged attribute with the ground truth attribute that it maps to . As we can see , QTGEN converges to the final characterization of the attribute that maps to Model first . This is because a model name is specified in nearly every query in our data . Next most commonly used attribute is Year , which , in addition , also has a small vocabulary and is hence found in few iterations as well . Finally , Brand and Parts that participate in fewer queries are found . EFFECT OF NUMBER OF ATTRIBUTES .
In Figure 6 we plot the performance of QTGEN when it learns templates by allowing for different number of attributes . As we can see the performance for most settings are very similar showing that with any given number of attributes QTGEN is able to learn attributes that closely map to ground truth attributes . However , the main difference is in the number of ground truth attributes that are covered by the learnt attributes . This difference is primarily represented in the CORRECTRECALL values that QTGEN is able to achieve with the lower settings of number of attributes .
Often , in unsupervised learning scenarios , one of the hardest pa rameters for a domain expert to set is the true number of clusters/attributes in the data . These results show that QTGEN has the ability to find appropriate attributes as long as the number of desired attributes is set to a reasonable value . EFFECT OF MODEL PARAMETERS .
In these experiments we report on the results of tuning the Dirichlet parameter φ that acts as a prior to the attribute word multinomial distributions and the Gamma distribution parameters g1 and g2 that are used to generate the Poisson distributions for each attribute . The φ parameters controls the extent to which the modeling procedure relies on the observed data as opposed to the priors . The smaller the value of φ , the more the multinomial distribution of words associated with each attribute is tuned to the data . In Figure 7 we plot the performance of QTGEN when run with different settings of φ . It is clear from the results that for a wide range of parameters values the
Attribute Brand Model Year Parts Specs
Words most frequently used in queries honda , toyota , chevy , ford , jeep , nissan , dodge , mustang , ranger truck , camaro , corvette , civic , bmw , accord , silverado , yukon , ram 2010 , 2007 , 2008 , 2006 , 2004 , 2005 , engine , 2011 , 2009 , custom parts , accessories , couple , rims , reviews , problems , tires , manual , seat owners , belt , floor , door , coupe
( a )
Query Template Brand Model Year
Brand Model Year Parts
Brand Model Year Parts Specs
Model Year Parts Brand Model Parts
( b )
Frequency Ad clicks
33.3 % 16.6 % 13.6 %
8 % 7 %
11 % 8.7 % 5.3 % 2.9 % 4.2 %
Table 2 : Structure extracted by QTGEN for the Automobiles domain . Table ( a ) shows the attributes found along with the most popular words in them . The attribute name is the ground truth attribute it matched with in our evaluation . Table ( b ) shows the top 5 frequently used query templates found by QTGEN in the Automobiles domain . The frequency indicates the fraction of the query traffic that is generated from this template . The Ad clicks is fraction of all ad clicks on Automobiles domain queries that are on queries generated from this template . results stay stable . Only in the case when the value of φ is very high do the results deteriorate since the system now relies on the prior too much and ignores the evidence of the data .
The Gamma distribution parameters g1 and g2 control the Poisson parameters which in turn control the propensity of each attribute to generate words in the query . We performed experiments with settings that forced attributes to generate very few words per query and others that allowed attributes to generate more . Our results were remarkably similar across these settings and we do not show them here due to paucity of space .
Our results in this section show that QTGEN is extremely robust to changes in parameters values and performs well for all values in a reasonable range . 5.4 Anecdotal Evidence and Discussion
Here we present a qualitative evaluation of the attributes and tem plates found by QTGEN and discuss potential applications .
In Table 2 we have shown the template structure uncovered by QTGEN . Table 2(a ) shows the attributes found along with the most popular words in their vocabularies . The attribute name in the table is the ground truth attribute that matched it . As is clear most of the words are grouped together into coherent attributes . Moreover , these attributes and their vocabularies closely match those in the manually constructed ground truth in Table 1 .
In Table 2(b ) we have listed the five most popular query templates , and the fraction of queries in our dataset covered by them ,
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 0.1 0.2 0.3 0.4 0.5 0.6PRECISION ( N)CORRECT RECALL ( N)Automobile domaink=4k=5k=6k=7 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 0.1 0.2 0.3 0.4 0.5 0.6PRECISION ( N)CORRECT RECALL ( N)Automobile domain00101110 % of Training data
TERM + SMOOTH
TERM + SMOOTH + QT
Approaches
2 % 5 % 10 % 50 % 100 %
6.26 % 6.57 % 4.29 % 2.81 % 0.83 %
12.04 % 10.41 % 6.20 % 4.18 % 1.35 %
Table 3 : The % improvement in the AUC of predicting the queryadvertisability for different approaches . The two approaches using SMOOTH and QT are explained in this section . The improvements are measured over the TERM baseline . The numbers in bold represent improvements that are statistically significant at α = 0.01 as detected by QTGEN . The results make intuitive sense . The most popular query template found by QTGEN is “ Vehicle ” queries ( that covers 33 % of queries in our set ) and describes the vehicle of interest using the three most important attributes , Brand , Model , and Year . These users are most probably researching an automobile for a purchase or lease . Beyond this template the users are querying for more detailed information on specific vehicles : to do this they first establish the identity of the vehicle using some combination of Brand , Model , and Year , and then express their information need via attributes Part and Spec . These “ Parts ” queries all together are about as frequent as the “ Vehicle ” queries in our dataset .
In the final column of Table 2 we list the fraction of all sponsored search advertisement clicks ( in the Automobiles domain in our dataset ) due to queries that are generated from these templates . As we can see these query templates demonstrate a large difference in their tendency to attract ad clicks from users . For example , the “ Parts ” queries contribute a larger fraction of ad clicks while occurring a smaller fraction of times than the “ Vehicle ” queries . Hence , knowledge gathered by QTGEN on the template from which the query has been generated should be useful for inferring its advertisability [ 23 ] . Other potential applications of extracting the attributes and query templates can be in generating special case search experiences , such as returning on the search results page the price and availability of the automotive part plugged into the query template . 5.5 Case Study : Query Advertisability
The results from Table 2 show that the presence of certain extracted templates and attributes is positively correlated with the clickthrough rate ( CTR ) on sponsored search advertisements . Motivated by these observations , in this section we conduct experiments to validate the usefulness of templates extracted by QTGEN for the problem of predicting Query Advertisability of tail queries [ 23 ] .
In brief , the search query frequencies are skewed as a power law and a large fraction of queries are unique ( or occur very few times ) . This makes predicting the CTR of sponsored search advertisements on these queries very challenging . One way is to predict the Advertisability for these tail queries in an attempt to help the search engine decide whether to show advertisements for them . In [ 23 ] we proposed an approach for this task that was shown to outperform state of the art baselines . Due to paucity of space we refer the reader to the original paper [ 23 ] for details of the proposed approach . Here we only describe how we enhance this approach using query templates learnt by QTGEN . EXPERIMENTAL SETUP .
The approach proposed in [ 23 ] is based on learning word specific scores that are then combined to predict the query advertisability ; we call this approach TERM . We then learn the query templates for the training set of these tail queries and hence the assignment of each term to an attribute . We then augment the query with these learnt attributes and learn the attribute specific scores . Our two enhancements of the TERM baseline are as follows :
( a ) 2 %
( b ) 5 %
( c ) 10 %
( d ) 50 %
Figure 8 : Clicks vs Impressions curves for models trained using varying amounts of data in the Travel domain .
1 ) TERM + SMOOTH : Here each term ’s score is smoothed using the attribute it is assigned to by adding ( a fraction of ) the attribute impression and click counts to the term ’s . This is likely to help for rare terms and in cases where very little data is available to reliably estimate term specific scores . The weight to be given to the attribute impressions is tuned on a validation set and varies with number of training data points . Finally , only the smoothed term specific scores are then combined into the query advertisability score . 2 ) TERM + SMOOTH + QT : In this baseline we perform smoothing as above . In addition , however , we combine term specific scores as well as attribute specific scores to learn the query advertisability score . The methodology for combining these scores is exactly the same as in [ 23 ] .
As before , in order to remove the effects of particular topics and obtain robust results , the experiments were performed on 43793 , 83387 , and 15050 tail queries from the Automobiles , Travel , and Movies domains , respectively , randomly sampled from the Yahoo! query logs . Most of these queries occurred once with less than 5 % occurring twice . Each of the approaches orders the queries in terms of their predicted advertisability scores and we report the AUC [ 13 ] of the clicks vs impressions plots ( see Figure 8 ) . Queries in each domain were evaluated separately since the presence of exclusive templates makes the predicted scores a little difficult to compare . The AUC values reported in Table 3 are weighted average across the domains , with tests in each domain averaged over 100 runs with randomly selected training points . 40 % of queries in each domain were put into the training set , 10 % for tuning the SMOOTH parameter , and rest for testing . Care was taken to ensure that the queries used in the test data had occurred later in time than the ones used for training and validation . To simulate low data situations we subsampled the training data to 2 % , 5 % , 10 % , and 50 % . RESULTS AND DISCUSSION .
The averaged AUC results are in Table 3 and results from the Travel domain are plotted in Figure 8 . The main points to note are that we get huge percentage improvements from the addition of templates found by QTGEN in situations when very little training data is present . This is because attribute specific scores are estimated over a larger number of terms making them more robust than termspecific scores , and using them to smooth the latter improves accuracy significantly . Moreover , note that even the presence of certain attributes ( SMOOTH + QT ) gives the algorithm an additional accuracy boost over just using SMOOTH ; this was as expected from the observations in Table 2 . While the improvements are overwhealming for smaller training set sizes , the effect remains until at least 50 % of the training data is available .
To conclude , the main goal of this experiment was for us to verify that QTGEN finds good query template assignments by showing that these assigned attributes help in estimating query advertisability . As the plots in Figure 8(a ) and 8(b ) show , the very strong baseline [ 23 ] is essentially random in very low data situations . However , the accuracy significantly improves with the addition of query templates . Moreover , even in settings with more data the benefit of using query templates persists .
6 . SUMMARY
In this paper we focused on the goal of automatically enriching short keyword search queries by finding domain specific query templates in a completely unsupervised manner . More specifically , we gave a generative model based approach that finds query templates as well as query attributes and their vocabularies , without any human intervention . We empirically demonstrated the performance of our approach by comparing it against two state of the art approaches on real datasets . Finally , we showed an application of query templates to computational advertising . In particular , we significantly improved the performance of an approach for predicting query advertisability by supplementing query keywords with their attributes and template information .
7 . REFERENCES [ 1 ] Mallet . http://malletcsumassedu/ [ 2 ] Powerset . http :
//enwikipediaorg/wiki/Powerset_(company ) [ 3 ] G . Agarwal , G . Kabra , and K . C C Chang . Towards rich query interpretation : walking back and forth for mining query templates . In 19th WWW , pages 1–10 , 2010 .
=
=
[ 4 ] R . A . Baeza Yates and B . Ribeiro Neto . Modern Information
Retrieval . Addison Wesley Longman , Boston , MA , 1999 . [ 5 ] S . Bergsma and Q . I . Wang . Learning noun phrase query segmentation . In EMNLP CoNLL , pages 819–826 , 2007 .
[ 6 ] D . Blei , A . Ng , and M . Jordan . Latent dirichlet allocation . Journal of
Machine Learning Research , 3:993–1022 , 2003 .
[ 7 ] A . Broder . A taxonomy of web search . SIGIR Forum , 36 , 2002 . [ 8 ] A . Broder , M . Ciaramita , M . Fontoura , E . Gabrilovich , V . Josifovski , D . Metzler , V . Murdock , and V . Plachouras . To swing or not to swing : Learning when ( not ) to advertise . In 17th CIKM , 2008 .
[ 9 ] A . Z . Broder , M . Fontoura , E . Gabrilovich , A . Joshi , V . Josifovski , and T . Zhang . Robust classification of rare queries using web knowledge . In 30th SIGIR , 2007 .
[ 10 ] I . S . Dhillon , J . Fan , and Y . Guan . Efficient clustering of very large document collections . In V . K . R . Grossman , C . Kamath and R . Namburu , editors , Data Mining for Scientific and Engineering Applications . Kluwer Academic Publishers , 2001 .
[ 11 ] I . S . Dhillon and D . S . Modha . Concept decompositions for large sparse text data using clustering . Machine Learning , 42(1):143–175 , Jan 2001 .
[ 12 ] S . Goel , A . Broder , E . Gabrilovich , and B . Pang . Anatomy of the long tail : ordinary people with extraordinary tastes . In 3rd , pages 201–210 , 2010 .
[ 13 ] M . Gonen . Receiver operating characteristic ( ROC ) curves . SAS Users
Group International ( SUGI ) , 31:210–231 , 2006 .
[ 14 ] T . Griffiths . Gibbs sampling in the generative model of latent dirichlet allocation . Technical report , Stanford University , 2002 .
[ 15 ] T . Griffiths and M . Steyvers . Finding scientific topics . Proceedings of the National Academy of Sciences , 101 , 2004 .
[ 16 ] J . Guo , G . Xu , X . Cheng , and H . Li . Named entity recognition in query . In 32nd SIGIR , pages 267–274 , 2009 .
[ 17 ] Q . Guo and E . Agichtein . Exploring mouse movements for inferring query intent . In 31st SIGIR , 2008 .
[ 18 ] L . Hubert and P . Arabie . Comparing partitions . Journal of
Classification , 2:193–218 , 1985 .
[ 19 ] M . S . J . K . Pritchard and P . Donnelly . Inference of population structure using multilocus genotype data . Genetics , 155:945–959 , 2000 .
[ 20 ] A . Jain , U . Ozertem , and E . Velipasaoglu . Synthesizing high utility suggestions for rare web search queries . In 34th SIGIR , pages 805–814 , 2011 .
[ 21 ] Y . Li , B J P . Hsu , C . Zhai , and K . Wang . Unsupervised query segmentation using clickthrough for information retrieval . In 34th SIGIR , pages 285–294 , 2011 .
[ 22 ] M . Pa¸sca . Weakly supervised discovery of named entities using web search queries . In 16th CIKM , pages 683–690 , 2007 .
[ 23 ] S . Pandey , K . Punera , M . Fontoura , and V . Josifovski . Estimating advertisability of tail queries for sponsored search . In 33rd SIGIR , pages 563–570 , 2010 .
[ 24 ] P . Pantel and A . Fuxman . Jigs and lures : Associating web queries with structured entities . In ACL , pages 83–92 , 2011 .
[ 25 ] K . Punera and S . Merugu . The anatomy of a click : modeling user behavior on web information systems . In 19th CIKM , 2010 .
[ 26 ] N . Sarkas , S . Paparizos , and P . Tsaparas . Structured annotations of web queries . In SIGMOD , 2010 .
[ 27 ] A . Strehl and J . Ghosh . Cluster ensembles – a knowledge reuse framework for combining multiple partitions . JMLR , 3 , 2002 .
[ 28 ] I . Szpektor , A . Gionis , and Y . Maarek . Improving recommendation for long tail queries via templates . In 20th WWW , pages 47–56 , 2011 .
[ 29 ] L . Tang , S . Rajan , and V . K . Narayanan . Large scale multi label classification via metalabeler . In 18th WWW , pages 211–220 , 2009 . [ 30 ] X . Wang , D . Chakrabarti , and K . Punera . Mining broad latent query aspects from search sessions . In 15th KDD , 2009 .
[ 31 ] G . Xu , S H Yang , and H . Li . Named entity mining from click through data using weakly supervised latent dirichlet allocation . In 15th KDD , 2009 .
8 . APPENDIX : CONDITIONAL FOR Z and then follow it up by comput
First we look at P ( w|zq =vq ,z¬q )
P ( w|z¬q ) ing P ( zq =vq ,z¬q|θ,t )
P ( z¬q|θ,t )
.
∆( na + β ) ∆(na,¬q + β )
= a∈vq a∈vq
,Γ(n(w ) a + β) Γ , ,Γ(n(w ) a,¬q + β) Γ , ,n(w )
V ( n(w ) a,¬q + β)a + β)− 1 ) . . .
V ( n(w ) w∈q that are assigned to a a,¬q + β + v(a,w ) q
− 1 . . . n(w ) w∈V ( n(w ) a,¬q + β + v(a,w ) q w∈V ( n(w ) a,¬q + β )
P ( w|zq = vq , z¬q )
P ( w¬q|z¬q ) w∈q w∈q a∈vq a∈vq a∈vq a∈vq a,¬q + β
{since if w is not assigned to a , then n(w ) a = n(w ) a,¬q} where v(a,w ) q denotes the number of times word w is assigned to where vq is the new attribute attribute a in attribute sequence vq . Next we compute P ( zq =vq ,z¬q|t,θ ) sequence for query q . On expanding it : P ( zq = vq , z¬q|t , θ )
P ( z¬q|t,θ ) where A is equal to :
|Q|
|A|
1 ni! i=1 a=1
P ( z¬q|t , θ ) g1 ( g1 + ( g2 + Na)(g1+ g2
=
A B
Q ni,a − 1 − nq,a + vq,a)! Q ni,a−nq,a+vq,a)(g1 − 1)! where Na denotes the number of queries with attribute a in their templates , ni is the length of query i , ni,a is the number of terms in query i from attribute a , and nq,a and vq,a denote the number of terms from attribute a in the old query configuration q and the new configuration v , respectively . g1 ( g1 + a)(g1+ g2
Q ni,a − 1)! Q ni,a)(g1 − 1)!
And
|Q|−1
B =
|A|
1 ni! i=1 a=1 where Q = {Q − q} , N terms in Q that come from attribute a . Hence , N
( g2 + N a is the number of queries in Q with Q ni,a is the sum of the number of a = Na − 1 and attribute a in their template ,
Q ni,a = Q ni,a − nq,a .
( g1 + ( g1 +
P ( z¬q|t , θ )
On substituting the values of A and B we get : P ( zq = vq , z¬q|t , θ )
( g2 + Na − 1)(g1+ ( g2 + Na)(g1+ Q ni,a − 1 − nq,a + vq,a)! Q ni,a − 1 − nq,a)!
=
1 nq
Q ni,a−nq,a ) Q ni,a−nq,a+vq,a ) a∈vq
