Towards Minimal Test Collections for Evaluation of Audio Music Similarity and Retrieval Markus Schedl
Julián Urbano
University Carlos III of Madrid
Department of Computer Science
Leganés , Spain
Johannes Kepler University
Department of Computational Perception
Linz , Austria jurbano@infuc3mes markusschedl@jkuat
ABSTRACT Reliable evaluation of Information Retrieval systems requires large amounts of relevance judgments . Making these annotations is quite complex and tedious for many Music Information Retrieval tasks , so performing such evaluations requires too much effort . A low cost alternative is the application of Minimal Test Collection algorithms , which offer quite reliable results while significantly reducing the annotation effort . The idea is to incrementally select what documents to judge so that we can compute estimates of the effectiveness differences between systems with a certain degree of confidence . In this paper we show a first approach towards its application to the evaluation of the Audio Music Similarity and Retrieval task , run by the annual MIREX evaluation campaign . An analysis with the MIREX 2011 data shows that the judging effort can be reduced to about 35 % to obtain results with 95 % confidence . Categories and Subject Descriptors H51 [ Multimedia Information Systems ] : Evaluation/ methodology ; H33 [ Information Search and Retrieval ] ; H34 [ Systems and Software ] : Performance evaluation ( efficiency and effectiveness ) . General Terms Algorithms , Experimentation , Measurement , Performance . Keywords Music information retrieval , evaluation , test collections , relevance judgments . 1 . INTRODUCTION The evaluation of Information Retrieval ( IR ) systems requires a test collection , usually containing a set of documents , a set of task specific queries , and a set of annotations that provide information as to what results a system should return for each query . Depending on the task , the set of queries may comprise the collection of documents itself , and the type of annotations can differ widely . In the field of Music IR ( MIR ) , building these collections is very problematic due to the very nature of the musical information , legal restrictions upon the documents , etc . [ 4 ] . In addition , annotating a test collection is a very timeconsuming and expensive process for some MIR tasks . For instance , annotating a single clip for Melody Extraction can take
Copyright is held by the International World Wide Web Conference Committee ( IW3C2 ) . Distribution of these papers is limited to classroom use , and personal use by others . WWW 2012 Companion , April 16–20 , 2012 , Lyon , France . ACM 978 1 4503 1230 1/12/04 . several hours . As a result , test collections for MIR tasks use to be very small , and they are unlikely to change from year to year , posing serious problems for the proper evolution of the field [ 6 ] . The Music Information Retrieval Evaluation eXchange ( MIREX ) started in 2005 as an international venue to promote and perform evaluation of MIR systems for various tasks [ 5 ] . MIREX was developed following the principles and methodologies that have made the Text REtrieval Conference ( TREC ) such a successful forum for evaluating Text IR systems [ 9 ] . However , since its inception in 2005 , the MIREX campaigns have evolved in parallel to TREC , practically ignoring all recent developments in the evaluation of IR systems [ 6 ] . In fact , the last five years have witnessed several works on reliable and low cost evaluation of IR systems . One of these works is the development of methodologies for evaluation with Minimal Test Collections ( MTC ) [ 3][2 ] . The idea behind MTC is that the results of evaluating IR systems may be estimated with high confidence even if the set of annotations is very incomplete . In a typical Text IR setting , it means that we do not need to judge all documents retrieved for a topic , but only a small fraction of it , to estimate with high confidence which of two systems is better . In this paper we study the application of MTC to the evaluation of Audio Music Similarity and Retrieval ( AMS ) systems , as it is the task that most closely resembles the ad hoc Text IR scenario : for a given audio clip ( the query ) , an AMS system returns a list of music pieces deemed to be similar to it . AMS is one of the most important tasks in MIR , and it has been run in MIREX in five of the seven editions so far ( see Table 1 ) .
Year Teams Systems Queries Results Judgments Overlap 2006 10 % 19 % 2007 10 % 2009 32 % 2010 2011 30 %
5 8 9 5 10 Table 1 . Summary of MIREX AMS editions .
1,800 6,000 7,500 4,000 9,000
1,629 4,832 6,732 2,737 6,322
60 100 100 100 100
6 12 15 8 18
Each edition of the AMS task requires the work of dozens of volunteers to perform similarity judgments , telling how similar two 30 second audio clips are . In the last edition , in 2011 , 6,322 of these judgments were needed , meaning that at least 53 hours of assessor time were needed to complete the judging task . In practice , though , collecting all these judgments takes several days . But AMS is one of the couple of tasks for which a new set of queries and relevance judgments are put together every year . Most of the tasks just use the same collections over and over again because they are too expensive to build , especially in terms of judging or annotation effort . Therefore , the study of low cost
WWW 2012 – AdMIRe'12 WorkshopApril 16–20 , 2012 , Lyon , France917 evaluation methodologies is imperative for the development of proper test collections to reliably evaluate MIR systems and properly advance the state of the art [ 6 ] . Developing low cost evaluation methodologies is essential for private , in house evaluations too . A researcher investigating several improvements of an existing MIR technique is not really interested in knowing how well they perform for the task ( which is highly dependent on the test collection anyway ) , but in which one performs better . That is , she is interested in the comparative evaluation of systems . MTC is specifically designed for these cases : it minimizes the annotation effort needed to find a difference between systems , incrementally selecting for judging those documents that are more informative to figure out the difference between systems , and reusing previous judgments when available . The remainder of the paper is organized as follows . Section 2 details the methodology currently followed to evaluate AMS systems in MIREX . Section 3 develops the methodology based on MTC to evaluate with incomplete judgments , and Section 4 shows the main results . Section 5 discusses the estimation of significance of the system comparisons and Section 6 concludes with final remarks and lines for future work . 2 . AMS EVALUATION Audio Music Similarity systems are evaluated according to an effectiveness measure that assesses how well they would satisfy a user for a given query . In order to generalize the results of an evaluation experiment the MIREX evaluations use a random sample ( cid:2283 ) of 100 queries . Each system is collection ( cid:2270 ) , ranked by their similarity to the query . The ( cid:1863 ) documents retrieved ( (cid:1827)(cid:1833)@(cid:1863) ) , with ( cid:1863)5 . For an arbitrary system A , ( cid:1827)(cid:1833)@(cid:1863 ) is defined as : ( cid:1827)(cid:1833)@(cid:1863)1(cid:1863)(cid:3533)(cid:1833)(cid:3401)(cid:1835)(cid:4666)(cid:1827)(cid:1863)(cid:4667 )   ( cid:1488)(cid:2270 ) where ( cid:1833 ) is the gain of document ( cid:1861 ) , ( cid:1827 ) is the rank at which system A retrieved document ( cid:1861 ) , and ( cid:1835)(cid:4666)(cid:1876)(cid:4667 ) is a boolean indicator function that evaluates to 1 if the expression ( cid:1876 ) is true and to 0 otherwise . collection that were ranked by A in the top ( cid:1863 ) . effectiveness measure used in MIREX is Average Gain of the top run for every query , returning a list of all documents in the
Therefore , the summation adds the gain of all documents in the to an arbitrary query ,
The gain of a document is a measure of how much information the user will gain from inspecting that result . In MIREX , there are two different scales : the BROAD scale is a 3 point graded scale where a document is considered either not similar to the query ( gain 0 ) , somewhat similar ( gain 1 ) or very similar ( gain 2 ) ; and the FINE scale , where the gain of a document ranges from 0 ( not similar at all ) to 100 ( identical to the query)1 . These gain scores are assessed by humans , who make similarity judgments between queries and documents . After all the judging is done , every system gets an ( cid:1827)(cid:1833)@(cid:1863 ) score for each query , and then they are ranked by their mean score across all queries . To minimize random effects due to the particular sample of queries chosen , the Friedman test is run with the Average Gain
1 In some editions of the MIREX AMS task it was defined from 0 to 10 , with one decimal digit . Both definitions are equivalent . every query . However , we may investigate how to compare systems so that we do not need to judge all documents and still be confident on the result of an evaluation experiment . scores of every system to look for significant differences across them . The Tukey ’s HSD test is then used to correct the experiment wide Type I error rate [ 7 ] . The grand results of the evaluation are therefore pairwise comparisons between systems , whether the observed difference was found to be significant . 3 . EVALUATION WITH INCOMPLETE JUDGMENTS The evaluation methodology used in MIREX is expensive in the sense that a complete set of similarity judgments is needed : the telling which one is better for the current set of queries ( cid:2283 ) , and top ( cid:1863 ) documents retrieved by every system have to be judged for Let ( cid:1833 ) be a random variable representing the gain of document ( cid:1861 ) . The distribution of ( cid:1833 ) is multinomial and depends on the similarity scale used : for the BROAD scale ( cid:1833 ) can take one of For now , let us assume that ( cid:1833 ) follows a uniform distribution , that variance of ( cid:1833 ) are as follows : ( cid:3039)(cid:1488)(cid:2278 ) ( cid:3039)(cid:1488)(cid:2278 ) where ( cid:2278 ) is the set of possible relevance levels : ( cid:2278)(cid:3003)(cid:3019)(cid:3016)(cid:3002)(cid:3005)(cid:4668)0,1,2(cid:4669 ) and ( cid:2278)(cid:3010)(cid:3015)(cid:3006)(cid:4668)0,1,…,100(cid:4669 ) . Given this definition of the gain of an arbitrary document , we can now define the ( cid:1827)(cid:1833)@(cid:1863 ) of an arbitrary system as a random variable too . Whenever document ( cid:1861 ) is judged and assigned a gain ( cid:1864 ) , the expectation and variance are fixed to ( cid:1831)(cid:4670)(cid:1833)(cid:4671)(cid:1864 ) and ( cid:1848)(cid:1853)(cid:4670)(cid:1833)(cid:4671)0 ; that is , no uncertainty about ( cid:1833 ) . independent of the others , expectation and variance of ( cid:1827)(cid:1833)@(cid:1863 ) are :
( cid:1831)(cid:4670)(cid:1833)(cid:4671)(cid:3533)(cid:1842)(cid:4666)(cid:1833)(cid:1864)(cid:4667)(cid:3401)(cid:1864 )   ( cid:1848)(cid:1853)(cid:4670)(cid:1833)(cid:4671)(cid:3533)(cid:1842)(cid:4666)(cid:1833)(cid:1864)(cid:4667)(cid:3401)(cid:1864)(cid:2870 ) three values , and for the FINE scale it can take one of 100 values .
Under the assumption that the gain of one document is is , every similarity level is equally likely . The expectation and
( cid:1831)(cid:4670)(cid:1833)(cid:4671)(cid:2870) 
( cid:1831)(cid:4670)(cid:1827)(cid:1833)@(cid:1863)(cid:4671)1(cid:1863)(cid:3533)(cid:1831)(cid:4670)(cid:1833)(cid:4671)(cid:3401)(cid:1835)(cid:4666)(cid:1827)(cid:1863)(cid:4667 )   ( cid:1848)(cid:1853)(cid:4670)(cid:1827)(cid:1833)@(cid:1863)(cid:4671)1(cid:1863)(cid:2870)(cid:3533)(cid:1848)(cid:1853)(cid:4670)(cid:1833)(cid:4671)(cid:3401)(cid:1835)(cid:4666)(cid:1827)(cid:1863)(cid:4667 )
( 1 )
( cid:1488)(cid:2270 ) ( cid:1488)(cid:2270 ) from an incomplete set of judgments . With no judgments at all , the variance of the estimator would be maximum , but as
Having ( cid:1827)(cid:1833)@(cid:1863 ) defined this way allows us to estimate its value judgments are made the variance decreases . With all ( cid:1863 ) documents Using equations ( 1 ) we can estimate the ( cid:1827)(cid:1833)@(cid:1863 ) score of a system . better , that is , the sign of their difference in ( cid:1827)(cid:1833)@(cid:1863 ) . For two ∆(cid:1827)(cid:1833)@(cid:1863)1(cid:1863)(cid:3533)(cid:1833)(cid:3401)(cid:1835)(cid:4666)(cid:1827)(cid:1863)(cid:4667 ) judged , the variance is zero and the estimation equals the actual score . 3.1 Difference in AG@k
1(cid:1863)(cid:3533)(cid:1833)(cid:3401)(cid:1835)(cid:4666)(cid:1828)(cid:1863)(cid:4667 ) 1(cid:1863)(cid:3533)(cid:1833)(cid:3401)(cid:3435)(cid:1835)(cid:4666)(cid:1827)(cid:1863)(cid:4667)(cid:1835)(cid:4666)(cid:1828)(cid:1863)(cid:4667)(cid:3439 ) ( cid:1488)(cid:2270 ) ( cid:1488)(cid:2270 ) ( cid:1488)(cid:2270 )
But we are really interested in knowing which of two systems is arbitrary systems A and B :
( 2 )
WWW 2012 – AdMIRe'12 WorkshopApril 16–20 , 2012 , Lyon , France918
( 3 )
Now that we can compute an estimate of the difference for one than system B ( worse if negative ) for the query . We can see that only documents retrieved by one system and not by the other will randomly3 , queries are independent of each other , so the expectation and variance are : not tell us anything about the difference . Thus , the larger the overlap between the systems’ results , the fewer judgments are necessary to figure which one is better . Because the two systems are independent of each other , the expectation and variance are2 :
If ∆(cid:1827)(cid:1833)@(cid:1863 ) is positive , we can conclude system A performed better contribute to ( cid:1827)(cid:1833)@(cid:1863 ) : documents retrieved by both systems will contribute ( cid:1833)(cid:1833)0 . Therefore , judging these documents will ( cid:1831)(cid:4670)∆(cid:1827)(cid:1833)@(cid:1863)(cid:4671)1(cid:1863)(cid:3533)(cid:1831)(cid:4670)(cid:1833)(cid:4671)(cid:3401)(cid:3435)(cid:1835)(cid:4666)(cid:1827)(cid:1863)(cid:4667)(cid:1835)(cid:4666)(cid:1828)(cid:1863)(cid:4667)(cid:3439 ) ( cid:1848)(cid:1853)(cid:4670)∆(cid:1827)(cid:1833)@(cid:1863)(cid:4671)1(cid:1863)(cid:2870)(cid:3533)(cid:1848)(cid:1853)(cid:4670)(cid:1833)(cid:4671)(cid:3401)(cid:3435)(cid:1835)(cid:4666)(cid:1827)(cid:1863)(cid:4667)(cid:1835)(cid:4666)(cid:1828)(cid:1863)(cid:4667)(cid:3439)(cid:2870 ) query , let us generalize to a set ( cid:2283 ) of queries , computing the mean of the ∆(cid:1827)(cid:1833)@(cid:1863 ) scores for all them . As they are sampled
( cid:1488)(cid:2270 ) ( cid:1488)(cid:2270 ) ( cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3431 ) 1|(cid:2283)|(cid:3533)(cid:1831)(cid:3427)∆(cid:1827)(cid:1833)@(cid:1863)(cid:3044)(cid:3431 ) ( cid:1831)(cid:3427)∆(cid:1827)(cid:1833)@(cid:1863 ) ( cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3431 ) 1|(cid:2283)|(cid:2870)(cid:3533)(cid:1848)(cid:1853)(cid:3427)∆(cid:1827)(cid:1833)@(cid:1863)(cid:3044)(cid:3431 ) ( cid:1848)(cid:1853)(cid:3427)∆(cid:1827)(cid:1833)@(cid:1863 ) documents , we reached a certain confidence on the sign , say 95 % , we could stop judging . 3.2 Distribution of ΔAG@k To compute the confidence in the sign , we need to know the
( cid:3044)(cid:1488)(cid:2283 ) ( cid:3044)(cid:1488)(cid:2283 ) in ( cid:1827)(cid:1833)@(cid:1863 ) . For a given set of judgments , we can compute ( cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)0(cid:3439 ) , that is , the probability of system A performing ( cid:1842)(cid:3435)∆(cid:1827)(cid:1833)@(cid:1863 ) ( cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)0(cid:3439)(cid:2009 ) then we can worse than system B . If ( cid:1842)(cid:3435)∆(cid:1827)(cid:1833)@(cid:1863 ) conclude that system A performs worse than B with ( cid:2009 ) confidence ( 1(cid:2009 ) confidence of B being worse than A ) . If , while judging ( cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364 ) . For a relevance scale with only two levels distribution of ∆(cid:1827)(cid:1833)@(cid:1863 ) ( similar and not similar ) , ( cid:1827)(cid:1833)@(cid:1863 ) is basically the same as ( cid:1842)@(cid:1863 ) ( precision at ( cid:1863) ) , which can be approximated by a normal distribution under a binomial or uniform prior distribution of ( cid:1833 ) Let ( cid:1833 ) be a random variable representing the gain of the top 5 the set ( cid:3419)(cid:1850),…,(cid:1850)|(cid:2283)|(cid:3423 ) be a random sample of size |(cid:2283)| where each ( cid:1850 ) is the mean gain of ( cid:1863 ) documents sampled from ( cid:1833 ) . By the Central Limit Theorem , as |(cid:2283)|(cid:1372)∞ the distribution of the sample average ( cid:1850)(cid:3364)∑(cid:1850 ) |(cid:2283)|⁄ the underlying distribution of ( cid:1833 ) . Every ( cid:1850 ) follows the definition of ( cid:1827)(cid:1833)@(cid:1863 ) for an arbitrary query ( cid:1861 ) , and so ( cid:1850)(cid:3364 ) follows the definition of ( cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364 ) for a set of queries ( cid:2283 ) . Therefore , ∆(cid:1827)(cid:1833)@(cid:1863 ) ( cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364 ) is normally ( cid:1827)(cid:1833)@(cid:1863 )
[ 1 ] . In our case , the BROAD scale has 3 possible levels , and the FINE scale has 101 levels .
With these estimates we can rank all systems by their difference documents retrieved by a system for all possible queries , and let approximates a normal distribution , regardless of distributed for a large number of queries , because it is the sum of two variables distributed normally .
( 4 )
2 The indicator functions are squared in the variance so all documents have a positive contribution to the total variance .
3 Note that this is rarely true in Text Information Retrieval .
BROAD scale
FINE scale
.
0 1
.
8 0
.
6 0
.
4 0
.
2 0 y t i s n e D
0 3 0 0
.
0 2 0 0
.
0 1 0 0
. y t i s n e D
0 0 0 0
.
.
0 0
0
20
40
80
60
0.5
1.5
2.0
1.0
0.0
100
AG@5
AG@5 distribution of gain values for the BROAD ( left ) and FINE ( right ) scales . The red lines are normal distributions with
Figure 1 . Distribution of ( cid:2157)(cid:2163)@(cid:2782 ) assuming a uniform means ( cid:2161)(cid:4670)(cid:2157)(cid:2163)@(cid:2782)(cid:4671 ) and variances ( cid:2178)(cid:2183)(cid:2200)(cid:4670)(cid:2157)(cid:2163)@(cid:2782)(cid:4671 ) . Let us define Γ(cid:3038 ) as the set of all |(cid:2278)|(cid:3038 ) possible assignments of gain that can be made for ( cid:1863 ) documents . Then , the probability of ( cid:1827)(cid:1833)@(cid:1863 ) being equal to a value ( cid:2314 ) is : ( cid:1842)(cid:4666)(cid:1827)(cid:1833)@(cid:1863)(cid:2314)(cid:4667)(cid:1556 ) ( cid:3533 ) ( cid:1842)(cid:4666)(cid:1827)(cid:1833)@(cid:1863)(cid:2314)(cid:3627)(cid:2011)(cid:3038)(cid:4667)(cid:3401)(cid:1842)(cid:4666)(cid:2011)(cid:3038)(cid:4667 )   ( cid:3082)(cid:3286)(cid:1488)(cid:3056)(cid:3286 ) 1|(cid:1985)(cid:3038)| ( cid:3533 ) ( cid:1835)(cid:4684)(cid:3533 ) ( cid:2011)(cid:3038)(cid:1863)(cid:2314 ) ( cid:4685 )   ( cid:3082)(cid:3286)(cid:1488)(cid:3056)(cid:3286 ) the average gain equals ( cid:2314 ) . The left plot in Figure 1 shows the histogram of ( cid:1827)(cid:1833)@5 scores observed in all 35=243 possible normal distributions with means ( cid:1831)(cid:4670)(cid:1827)(cid:1833)@(cid:1863)(cid:4671 ) and variances ( cid:1848)(cid:1853)(cid:4670)(cid:1827)(cid:1833)@(cid:1863)(cid:4671 ) . We can see that the normal distributions do indeed Using the normal cumulative density function Φ we can easily assignments with the BROAD scale ; and the right plot shows the scores observed in a random sample of 1 million assignments out of the 1015 possibilities with the FINE scale . The red lines are that is , the fraction of possible similarity assignments for which approximate very well .
( cid:3082)(cid:3284)(cid:3286)(cid:1488)(cid:3082)(cid:3286 ) compute the confidence on the sign of the difference as :
( cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3431 ) ( cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)0(cid:3439)Φ(cid:1737)(cid:1735)(cid:1831)(cid:3427)∆(cid:1827)(cid:1833)@(cid:1863 ) ( cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3431)(cid:1740)(cid:1738 ) ( cid:1842)(cid:3435)∆(cid:1827)(cid:1833)@(cid:1863 ) ( cid:3495)(cid:1848)(cid:1853)(cid:3427)∆(cid:1827)(cid:1833)@(cid:1863 )
( 5 )
3.3 Document Selection Equations ( 3 ) and ( 4 ) can be used to estimate the difference between two systems with an incomplete set of judgments , but the problem is : what documents should we judge ? Ideally , we want to judge only those that are most informative to know the sign of the difference in ( cid:1827)(cid:1833)@(cid:1863 ) . For just two systems it is obvious from equation ( 2 ) : only documents retrieved by one system and not by the other one are informative . For an arbitrary number of queries , we can just refer to a query document pair as a single document ( ie the gain of a document for a particular query ) . For an arbitrary number of systems , a particular document could be informative for more than just one of the system comparisons .
Therefore , we can assign a weight ( cid:1875 ) to every query document ( cid:1861 ) , judging query document ( cid:1861 ) would affect the estimate of ( cid:1986)(cid:1827)(cid:1833)@(cid:1863)(cid:3044 ) . equal to the number of pairwise system comparisons for which
At any given time , we will want to judge the query documents with largest weight because they will have the largest effect .
WWW 2012 – AdMIRe'12 WorkshopApril 16–20 , 2012 , Lyon , France919 FINE scale
BROAD scale s m e t s y s f i o g n k n a r n i e c n e d i f n o C correct bins 2482 judgments ( 39 % )
99 % confidence 4297 judgments ( 68 % )
95 % confidence 1967 judgments ( 31 % )
90 % confidence 1415 judgments ( 22 % )
0 0 1
5 9
0 9
5 8
0 8
5 7 s m e t s y s f i o g n k n a r n i e c n e d i f n o C
99 % confidence 4254 judgments ( 67 % ) correct bins 1611 judgments ( 25 % )
95 % confidence 2234 judgments ( 35 % )
90 % confidence 1304 judgments ( 21 % )
0 0 1
5 9
0 9
5 8
0 8
5 7
0
10
20
30
40
50
60
70
80
90
100
0
10
20
30
40
50
60
70
80
90
100
Figure 2 . Confidence in the ranking of systems as the number of judgments increases , with the BROAD ( left ) and FINE ( right ) similarity scales . “ correct bins ” marks the point at which all true significant pairwise comparisons have a correct
Percent of judgments
( cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364 ) . estimation of the sign of ∆(cid:2157)(cid:2163)@(cid:2193 )
Percent of judgments
BROAD scale
FINE scale
0 0 1
5 9
0 9
5 8 y c a r u c c A
1
.
9 0
8
.
0
7
.
0 i g n k n a r e u r t e h t o t n o i t l a e r r o c
0 0 1
5 9
0 9
5 8 y c a r u c c A
1
9
.
0
.
8 0
7
.
0 i g n k n a r e u r t e h t o t n o i t l a e r r o c
Correct bins Correct signs
.
6 0
Correct bins Correct signs
6
0 8
80
Figure 3 . Accuracy of the sign of ∆(cid:2157)(cid:2163)@(cid:2193 )
Confidence in ranking of systems
90
95
85 increases , for the BROAD ( left ) and FINE ( right ) similarity scales . Pairs with wrong sign estimates are considered correct under “ correct bins ” if they are not significantly different .
0 8
.
0
85
95
90
80
100
100
Confidence in ranking of systems
( cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364 ) estimates and Kendall ’s τ correlation to the true ranking as confidence Algorithm 1 : MTC for ( cid:1986)(cid:1827)(cid:1833)@(cid:1863 ) 1 : while |(cid:2285)|(cid:3532 ) ( cid:1829)(cid:3002),(cid:3003 ) 1(cid:2009 ) do ( cid:1861)(cid:1499)(cid:1370)(cid:1853)(cid:1859)(cid:1865)(cid:1853)(cid:1876)(cid:1875 ) for all unjudged query document pairs ( cid:4666)(cid:3002),(cid:3003)(cid:4667)(cid:1488)(cid:2285 ) judge query document ( cid:1861)(cid:1499 ) ( obtain true ( cid:1859)(cid:1853)(cid:1861)(cid:1499 ) ) ( cid:1831)(cid:4670)(cid:1833)(cid:1499)(cid:4671)(cid:1370)(cid:1859)(cid:1853)(cid:1861)(cid:1499 ) ( cid:1848)(cid:1853)(cid:4670)(cid:1833)(cid:1499)(cid:4671)(cid:1370)0
2 : 3 : 4 : 5 : 6 : end while For the stopping condition we compute the mean confidence across all system pairs . If it is sufficiently large , we stop judging altogether . We call this the confidence on the ranking . Equation ( 6 ) ensures that the judging effort will be put into the less confident pairs . 4 . RESULTS We simulated the use of MTC to evaluate all systems from the MIREX 2011 Audio Music Similarity and Retrieval task . This is
Ignoring these , the weight of every query document is :
But if we were already highly confident about the difference between two systems , we would not need to judge another one of their query documents . For two arbitrary systems A and B , let us define the confidence on the sign of their difference , as per
Equation ( 5 ) , as ( cid:1829)(cid:3002),(cid:3003 ) . Being ( cid:2285 ) the set of all system pairs , at any point we can compute the subset ( cid:2284)(cid:1599)(cid:2285 ) as the subset of pairs for ( cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364 ) . which we are already highly confident on the sign of ∆(cid:1827)(cid:1833)@(cid:1863 ) ( cid:1875 ) ( cid:3533 ) ( cid:3435)1(cid:1829)(cid:3002),(cid:3003)(cid:3439)(cid:3401)(cid:3435)(cid:1835)(cid:4666)(cid:1827)(cid:1863)(cid:4667)(cid:1835)(cid:4666)(cid:1828)(cid:1863)(cid:4667)(cid:3439)(cid:2870 ) ( cid:4666)(cid:3002),(cid:3003)(cid:4667)(cid:1488)(cid:2285)(cid:2284 ) At this point , we can define the MTC algorithm for ( cid:1986)(cid:1827)(cid:1833)@(cid:1863 ) :
That is , the contribution of a pairwise system comparison to the weight of a query document is inversely proportional to the confidence in the sign of their difference .
( 6 )
WWW 2012 – AdMIRe'12 WorkshopApril 16–20 , 2012 , Lyon , France920 the largest edition so far , where 18 systems were evaluated with 100 queries for a total of 6,322 judgments ( see Table 1 ) . There are thus 153 pairwise comparisons between systems . Figure 2 shows how the confidence in the ranking of systems increases as more judgments are made : with no judgments confidence is 50 % ( ie one system is equally likely to be better than another one than it is to be worse ) , and with a complete set of judgments confidence is 100 % . The pattern is quite similar for both similarity scales : 90 % confidence is reached with about one fifth of the total judgments , 95 % confidence with one third of the judgments ; and 99 % confidence with two thirds . This ranking confidence can be interpreted as the confidence in ∆(cid:1827)(cid:1833)@(cid:1863 ) the sign of ∆(cid:1827)(cid:1833)@(cid:1863 )
( cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364 ) of any ( cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364)(cid:3364 ) ? Figure 3 plots the accuracy of the estimates two systems picked at random . We can see that high confidence levels can be achieved with considerably fewer judgments , but how good are the estimates of as a function of the ranking confidence . Accuracy is defined as the ratio of correct sign estimates across all 153 system comparisons . An estimate is considered correct under “ correct bins ” if it has the same sign as the true difference or the true difference is not statistically significant anyway . An estimate is considered correct under “ correct signs ” only if it has the same sign as the true difference regardless of the significance . The accuracy of the estimated bins is always better than the ranking confidence , and for more than 90 % confidence on the ranking most significant differences seem to be identified . If we look at the accuracy regardless of true significance ( “ correct signs ” ) , it is again highly correlated with the ranking confidence , but it is sometimes lower than expected . This is caused by pairs of systems that are very similar , making the estimates swap from positive to negative values with very few judgments . Note that these swaps were considered correct under “ correct bins ” . A traditional way of comparing the estimated ranking and the true ranking is to compute the Kendall ’s τ correlation coefficient between the two . Rankings with correlations above 0.9 are usually considered equivalent if we account for the effect of having one or another person make the judgments [ 8 ] . Formally , 0.9 Kendall correlation corresponds to 95 % accuracy under “ correct signs ” . We can see in Figure 3 that rankings with more than 95 % confidence do indeed have a very high correlation with the true ranking . Virtually all ranking estimates have a correlation over 0.9 , although in the case with the FINE judgments a higher confidence seems necessary , as there are some lower observations around 95 % confidence . 5 . STATISTICAL SIGNIFICANCE The results in the previous section show that we can estimate the ranking of systems with a fraction of the total judgments , and that this estimated ranking is very similar to the true ranking or equivalent for all practical purposes . The next problem would be to figure out whether differences between systems are statistically significant . With the traditional methodology , after completing all the judgments and computing the true differences in ( cid:1827)(cid:1833)@(cid:1863 ) scores , systems are compared to each other to see whether the difference observed with the current set of queries would be expected with a different set of queries [ 7 ] . So far , the MTC algorithm allows us to estimate the difference for the current set of queries , but it does not allow us to generalize to a different set . The main problem at for every query . Estimating this sample variance and adapting a statistical test accordingly is far from trivial . Instead , we work levels permitted by the scale ( 2 and 0 for BROAD ; and 100 and 0 for FINE ) . For an arbitrary pair of systems A and B , we compute this point is that we have an estimate of ∆(cid:1827)(cid:1833)@(cid:1863 ) for each query , but we do not have the variance of the sample of true ∆(cid:1827)(cid:1833)@(cid:1863 ) ’s with the best and worst cases for each ∆(cid:1827)(cid:1833)@(cid:1863 ) . Let ( cid:2024 ) and ( cid:2024)(cid:3364 ) be the sets of all judged and unjudged documents so far ; and let ( cid:1864 ) and ( cid:1864 ) be the maximum and minimum similarity the upper and lower bounds on ∆(cid:1827)(cid:1833)@(cid:1863 ) : ( cid:1727)∆(cid:1827)(cid:1833)@(cid:1863)(cid:1728)1(cid:1863)(cid:3533)(cid:1833)(cid:3401)(cid:3435)(cid:1835)(cid:4666)(cid:1827)(cid:1863)(cid:4667)(cid:1835)(cid:4666)(cid:1828)(cid:1863)(cid:4667)(cid:3439 ) ( cid:3397)1(cid:1863)(cid:3533)(cid:1864)(cid:3401)(cid:1835)(cid:4666)(cid:1827)(cid:1863)(cid:4667 ) 1(cid:1863)(cid:3533)(cid:1864)(cid:3401)(cid:1835)(cid:4666)(cid:1828)(cid:1863)(cid:1512)(cid:1827)(cid:3408)(cid:1863)(cid:4667 ) ( cid:1488)(cid:3095 )   ( cid:1729)∆(cid:1827)(cid:1833)@(cid:1863)(cid:1730)1(cid:1863)(cid:3533)(cid:1833)(cid:3401)(cid:3435)(cid:1835)(cid:4666)(cid:1827)(cid:1863)(cid:4667)(cid:1835)(cid:4666)(cid:1828)(cid:1863)(cid:4667)(cid:3439 ) ( cid:1488)(cid:3095)(cid:3365 ) ( cid:1488)(cid:3095)(cid:3365 ) ( cid:3397)1(cid:1863)(cid:3533)(cid:1864)(cid:3401)(cid:1835)(cid:4666)(cid:1828)(cid:1863)(cid:4667 ) 1(cid:1863)(cid:3533)(cid:1864)(cid:3401)(cid:1835)(cid:4666)(cid:1827)(cid:1863)(cid:1512)(cid:1828)(cid:3408)(cid:1863)(cid:4667 ) ( cid:1488)(cid:3095 )   ( cid:1488)(cid:3095)(cid:3365 ) ( cid:1488)(cid:3095)(cid:3365 ) The upper bound corresponds to the ∆(cid:1827)(cid:1833)@(cid:1863 ) score in the best
( cid:3397)  ( cid:3397)  case for system A : the first summation accounts for the information due to documents that have already been judged ; the second summation assumes that all unjudged documents retrieved by A have the highest gain allowed by the scale ; and the third summation assumes that all unjudged documents retrieved by B , but not by A , have the lowest gain allowed by the scale . The same ( opposite ) rationale follows for the lower bound . In our case , the third summations can be ignored , as the minimum gain in the
BROAD and FINE scales is 0. 
At any iteration of the algorithm we can compute the upper and lower bounds for every pair of systems and follow these rules : 1 . If in the upper bound ( best case for A ) , A would still be significantly worse than B , it does not matter which judgments we do next : we can conclude that A is significantly worse than B .
2 . If in the lower bound ( best case for B ) , B would still be significantly worse than A , we can similarly conclude that B is significantly worse than A .
3 . If in the upper bound A would still not be significantly better than B , and in the lower bound B would still not be significantly better than A , we conclude they are not significantly different . the upper and naively conclude the systems are significantly different .
These rules only become useful with a relatively large amount of lower bounds are very high judgments : overestimations because the true performance of systems is far from the assumed perfect cases . With a large degree of incompleteness we can use another heuristic :
4 . If the estimated difference is larger than a threshold we particular number of queries , variance of the sample of ( cid:1986)(cid:1827)(cid:1833)@(cid:1863 ) ’s variance based on previous MIREX data ( (cid:2026)0.248 ) , set the rates to typical values ( cid:2009)0.05 and ( cid:2010)0.15 ( power = 0.85 ) ,
This threshold may be fixed based on power analysis . We can compute the effect size detectable by a paired 1 tailed t test for a sample size to 100 queries , and set the Type I and Type II error and permitted Type I and Type II error rates . We estimate the
WWW 2012 – AdMIRe'12 WorkshopApril 16–20 , 2012 , Lyon , France921 t s e a m i t s e e c n a c i f i i n g s e h t f o y c a r u c c A s e t a m i i t s e e c n a c i f i n g s e h t f o y c a r u c c A
0 0 1
8 9
6 9
4 9
2 9
0 9
0 0 1
8 9
6 9
4 9
2 9
0 9
Threshold t = 0
Threshold t = 0.033 t s e a m i t s e e c n a c i f i i n g s e h t f o y c a r u c c A
Precision Recall F−measure
0 0 1
8 9
6 9
4 9
2 9
0 9
Precision Recall F−measure
90
92
94
96
98
100
90
92
94
96
98
100
Confidence in ranking of systems
Threshold t = 0.067
Confidence in ranking of systems
Threshold t = 0.1 s e t a m i i t s e e c n a c i f i n g s e h t f o y c a r u c c A
0 0 1
8 9
6 9
4 9
2 9
0 9
Precision Recall F−measure
Precision Recall F−measure
90
92
94
96
98
100
90
92
94
96
98
100
Confidence in ranking of systems
Confidence in ranking of systems
Figure 4 . Accuracy of the significance estimates in terms of precision , recall and F measure as confidence increases , for thresholds t = 0 ( top left ) , t = 0.033 ( top right ) , t = 0.067 ( bottom left ) and t = 0.1 ( bottom right ) .
≈ 0067 This is the value to which we fix the threshold . respectively . The observable true difference in this case would be
We can evaluate the accuracy of these rules with typical precision recall ratios . If the rules estimate that a pair of systems is significantly different we count it as a positive result , negative otherwise . If the prediction is correct ( according to the complete set of judgments ) , we count a true estimate , false otherwise . Figure 4 shows precision , recall , and F measure for various thresholds .
The effect of the threshold is clear : the larger the minimum difference between systems ( large ) , the fewer estimates turn out significant , so precision is high and recall is low . With 0 ( ie the magnitude of the estimated differences is ignored ) , precision is above expected and recall is below ; but when 95 % confidence is
( over ) estimated as significant because 0 in rule 4 . As the reached they begin to swap . At this point , we are confident of the sign of about 95 % of the pairs , so about 95 % of them will be confidence increases , more comparisons are overestimated , but when approaching the complete set of judgments ( over 99 % confidence ) , rules 1 to 3 reduce the amount of false estimates again . The overall accuracy of the significance estimates , as per the Fmeasure , corresponds fairly well with the confidence in the ranking until 95 % confidence is reached . Nonetheless , it is always above 90 % . When using the threshold 0.067 computed above , the F measure improves considerably for very high confidence levels ( bottom left plot ) , and gradually diminishes as the threshold gets larger ( bottom right ) .
WWW 2012 – AdMIRe'12 WorkshopApril 16–20 , 2012 , Lyon , France922 can be estimated
In general , we can see that the statistical significance of the differences the incompleteness of judgments and the uncertainty on the true fairly well despite
∆(cid:1827)(cid:1833)@(cid:1863 ) scores . similarity judgments , we showed that the distribution of ( cid:1827)(cid:1833)@(cid:1863 )
6 . CONCLUSIONS We have shown how to adapt the Minimal Test Collections ( MTC ) family of algorithms for the evaluation of the Audio Music Similarity and Retrieval task . Assuming a uniform distribution of scores is normally distributed , which allows us to look at it as a random variable whose expectation may be estimated with a certain level of confidence . This confidence is proportional to the number of similarity judgments available , and MTC ensures that the set of judgments we make to reach some confidence level is minimal . Using the data from the MIREX 2011 AMS evaluation , we simulated MTC and found that with just one third of the judgments the correct ranking of systems can be estimated with 95 % confidence , and with no swap between significantly different systems . We also showed some simple rules to estimate the significance of the differences , which work reasonably well for 95 % confidence but tend to overestimate significance when higher confidence is achieved in the ranking of systems . Three clear lines for future work can be identified . First , this paper assumed that the distribution of similarity judgments was uniform , that is , that all assignments of similarity were equally likely . However , it is clear this assumption does not hold in reality : the distribution of similarity judgments is skewed towards the highly similar or the not similar side , depending on how well the system performs . Having better estimates of the true distribution would make the algorithm perform better in terms of effort and accuracy of the estimates . These estimations of the true distribution could be approximated from previous MIREX AMS data , or with a model fitted with the judgments we make for the very systems we are evaluating , learning the true distribution on a per system or per query basis . Second , our estimates of the significance of the differences were based on very simple rules that assumed the ( very unrealistic ) best cases . Developing a comprehensive mathematical framework for testing significance is another clear line for future work . The most important direction for further research is the study of low cost evaluation methodologies for other MIR tasks . In accordance with previous work [ 7 ] , we have shown that the effort in evaluating a set of AMS systems can be greatly reduced , leaving open the possibility of building brand new test collections for other tasks for which creating annotations is very expensive . For instance , the group of volunteers requested by MIREX for the yearly evaluation of the AMS and SMS tasks could be better employed if some of them were instead dedicated to incrementally add new annotations for the other tasks . low cost Another clear methodologies is that of a researcher evaluating a set of systems with a private document collection , a scenario very common in MIR given the legal restrictions on sharing music corpora . Those researchers , and in most cases public forums too , do not have the possibility of requesting large pools of external volunteers for annotating their collections . Thus , being able to evaluate systems the application of setting for with the minimal effort is paramount . To this end , low cost evaluation methodologies must be investigated for the wealth of MIR tasks . In most of these tasks researchers rely on test collections annotated a priori , which can be very expensive and time consuming to build . However , we have seen that not all annotations are necessary to evaluate systems . For instance , if two Audio Melody Extraction algorithms predict the same F0 ( fundamental frequency ) in a given frame , whether that F0 prediction is correct or not is not useful to know which of the two systems is better . The adoption of a posteriori evaluation methodologies such as MTC can take advantage of this to greatly reduce the annotation cost or allow the use of larger collections . Getting to that point , though , requires a shift in the current evaluation practices . But given the benefits of doing so , both in terms of cost and reliability , we strongly encourage the MIR community and progressively adopt them for a more rapid and stable development of the field . ACKNOWLEDGEMENTS This research is supported by the Spanish National Plan of Scientific Research , Development and Technological Innovation through grants TSI 020110 2009 439 and HAR2011 27540 and the Austrian Science Funds ( FWF ) : P22856 N23 . REFERENCES [ 1 ] B . Carterette . Low Cost and Robust Evaluation of Information Retrieval Systems . PhD dissertation , Department of Computer Science , University of Massachusetts Amherst , 2008 . alternatives evaluation study these to
[ 2 ] B . Carterette . Robust Test Collections for Retrieval
Evaluation . In International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 55 62 , 2007 .
[ 3 ] B . Carterette , J . Allan , and R . Sitaraman . Minimal Test
Collections for Retrieval Evaluation . In International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 268 275 , 2006 .
[ 4 ] JS Downie . The Scientific Evaluation of Music Information Retrieval Systems : Foundations and Future . Computer Music Journal . 28(2 ) : 12 23 , 2004 .
[ 5 ] JS Downie , AF Ehmann , M . Bay , and MC Jones . . The Music Information Retrieval Evaluation eXchange : Some Observations and Insights . In Advances in Music Information Retrieval , WR Zbigniew and AA Wieczorkowska , eds . Springer . 2010 , 93 115 .
[ 6 ] J . Urbano . Information Retrieval Meta Evaluation :
Challenges and Opportunities in the Music Domain . In International Society for Music Information Retrieval Conference , pages 609 614 , 2011 .
[ 7 ] J . Urbano , D . Martín , M . Marrero , and J . Morato . Audio
Music Similarity and Retrieval : Evaluation Power and Stability . In International Society for Music Information Retrieval Conference , pages 597 602 , 2011 .
[ 8 ] EM Voorhees . Variations in Relevance Judgments and the
Measurement of Retrieval Effectiveness . Information Processing and Management . 36(5 ) : 697 716 , 2000 .
[ 9 ] EM Voorhees and DK Harman . . TREC : Experiment and
Evaluation in Information Retrieval . MIT Press , 2005 .
WWW 2012 – AdMIRe'12 WorkshopApril 16–20 , 2012 , Lyon , France923
