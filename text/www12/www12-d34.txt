A Unified Approach to Learning Task Specific Bit Vector
Representations for Fast Nearest Neighbor Search
Vinod Nair
Yahoo! Labs Bangalore vnair@yahoo inc.com
Dhruv Mahajan
Yahoo! Labs Bangalore dkm@yahoo inc.com
S . Sundararajan
Yahoo! Labs Bangalore ssrajan@yahoo inc.com
ABSTRACT Fast nearest neighbor search is necessary for a variety of large scale web applications such as information retrieval , nearest neighbor classification and nearest neighbor regression . Recently a number of machine learning algorithms have been proposed for representing the data to be searched as ( short ) bit vectors and then using hashing to do rapid search . These algorithms have been limited in their applicability in that they are suited for only one type of task – eg Spectral Hashing learns bit vector representations for retrieval , but not say , classification . In this paper we present a unified approach to learning bit vector representations for many applications that use nearest neighbor search . The main contribution is a single learning algorithm that can be customized to learn a bit vector representation suited for the task at hand . This broadens the usefulness of bit vector representations to tasks beyond just conventional retrieval .
We propose a learning to rank formulation to learn the bit vector representation of the data . LambdaRank algorithm is used for learning a function that computes a task specific bit vector from an input data vector . Our approach outperforms state of the art nearest neighbor methods on a number of real world text and image classification and retrieval datasets . It is scalable and learns a 32bit representation on 1.46 million training cases in two days .
Categories and Subject Descriptors H33 [ Information Storage and Retrieval ] : Information Search and Retrieval ; I26 [ Artificial Intelligence ] : Learning
Keywords nearest neighbor search , hashing , learning to rank
1 .
INTRODUCTION
Nearest neighbor ( NN ) methods are widely used in several web applications such as web objects ( eg , documents , and images ) classification , and retrieval . In retrieval applications , availability of large volume of web data makes it possible to retrieve very similar objects for almost any given query object , and these similar objects are nearest neighbors under some suitably defined similarity measure . Similarly , NN methods such as k Nearest Neighbor ( kNN ) classifiers achieve high accuracy as very similar objects often be
Copyright is held by the International World Wide Web Conference Committee ( IW3C2 ) . Distribution of these papers is limited to classroom use , and personal use by others . WWW 2012 , April 16–20 , 2012 , Lyon , France . ACM 978 1 4503 1229 5/12/04 . long to the same category . These classifiers are attractive due to their simplicity and speed independence in the number of classes1 . Since NN methods work well in very large data set scenarios , the ability to handle large volume of data during training and testing is a necessity . For example , in a naive implementation , linear search over the entire training data is needed , and this is expensive for web scale applications . Therefore , development of algorithms , data structures ( eg , KD Trees ) and representations ( eg , bit vectors ) that facilitate fast search are of paramount importance . Furthermore , task specific performance ( eg , accuracy in classification tasks , and ranking measures such as normalized discounted cumulative gain ( NDCG ) , precision@k in retrieval tasks ) achieved is dependent on distance metric or similarity measure that is used in finding the neighbors . Although Euclidean distance is a useful metric in some applications , it is often not optimal . Hence , learning a task specific distance metric is also necessary .
Keeping above requirements in mind , we propose a unified framework in which task specific fast NN methods that achieve high performance can be developed . We demonstrate our method on classification and retrieval tasks , and experimental results show superior performance over state of the art NN classification and retrieval methods .
1.1 Nearest Neighbor Approaches
In recent years , there has been a flurry of research activity on nearest neighbor methods for web classification and information retrieval applications . To motivate our approach , we briefly discuss several popular methods and differentiate them along three key aspects ( capturing the above mentioned requirements ) : 1 ) speed ( training and testing ) , 2 ) learning ( representations/similarity measure with or without additional information ( eg , labels of objects , pairwise similar or dissimilar objects ) , and 3 ) optimizing a taskspecific performance measure during learning . In Table 1 we present a summary of various methods , and Figure 1 shows different configurations to which these methods adhere to . We observe that all the existing methods fall short in covering at least one of the aspects , and we attempt to address this issue .
Hashing methods are approximate nearest neighbor search methods , and are popular due to their speed . In these methods data points ( objects ) are represented as bit vectors , and such a representation helps in finding the neighbors quickly2 . Similarity between two objects is measured by the Hamming distance , and nearest
1Although standard classifiers such as support vector machines or logistic regression models have been quite successfully used in the web context , the dependence of model and test time complexities on the number of classes makes them less attractive when the number of classes is very large . 2An important advantage of using a bit vector representation is that the search time can be made constant with respect to the size of the search set . For example , with 32 bit representation , one can build an inverted index with 232 = 4294967296 bins
WWW 2012 – Session : Data and Content Management 2April 16–20 , 2012 , Lyon , France929 Bits
Real vector
Bits
Bits
Bits
Random projections
Learned projections
Random projections
Real vector
Learned projections
Learned projections
Task specific learned projections
Data vector
Data vector
Data vector
Data vector
Data vector
( a ) LSH
( b ) Metric learning
( c ) Metric learning
+ LSH
( d ) Spectral Hashing
( e ) This paper
Figure 1 : Comparison of various approaches with respect to our algorithm . neighbors are identified as the points that are within a user specified Hamming distance from a given query .
Locality Sensitive Hashing ( LSH ) [ 1 ] is a simple method ( figure 1(a ) ) in which bit vector representation for a data point ( object ) is obtained from projecting the data vector on several random directions , and converting the projected values to {0 , 1} by thresholding . This method does not make use of data to learn the representation . Learning is important because it is useful to find compact and better representations , which in turn results in improved speed and taskspecific performance . Hence , attempts have been made to develop better hashing methods . Sophisticated hashing methods such as Semantic Hashing [ 19 ] and Spectral Hashing [ 23 ] ( figure 1(d ) ) learn compact bit vector representations ( codes ) with the desirable property that similar objects have similar codes . In Semantic Hashing , each object in the training database is represented by a compact binary code , and the code is computed using a feed forward neural network , with the network weights learned by minimizing data reconstruction error . In Spectral Hashing [ 23 ] , k bit compact binary codes are obtained by minimizing the sum of weighted Hamming distance between data points that are similar , and the weight represents a similarity score computed using the euclidean distance in a suitable input feature space . A key disadvantage of these methods is that they do not make use of any task specific information such as object labels , query document pair relevancy score , etc . ( when available ) ; and , they do not explicitly optimize task specific performance measures . So these methods suffer on actual task specific performance such as classification accuracy , NDCG , etc .
There are several popular methods that learn a distance metric ( eg , Mahalanobis distance ) . These methods make use of additional information and learn the distance metric by a optimizing task specific objective function . Methods such as Neighborhood component analysis ( NCA ) [ 9 ] and large margin nearest neighbor ( LMNN ) [ 22 ] ( figure 1(b ) ) classification learn the metric using class label information , keeping nearest neighbor classification as the goal . Metric learning has also been seen as a special case of ranking problem , from the view point that good neighbors appear at the top of the list and bad neighbors appear at the bottom . McFee et al . [ 16 ] propose a structural SVM learning framework ( figure 1(b ) ) for learning the metric where various ranking measures such as precision@k , NDCG , etc . , can be optimized ( referred as MLR in Table 1 ) . Chechik et al . [ 6 ] propose an online algorithm where each bin contains the data points that were mapped to the corresponding bit vector . for scalable image similarity ( OASIS ) that learns a similarity measure ( figure 1(b ) ) with the property of scoring the more relevant pair of objects higher ; it uses labeled information in the form of relative similarity of different pairs of objects . A major drawback with these methods is that they do not provide bit vector representations . Therefore , finding nearest neighbors is expensive unless the input representation is sparse ( which is not the case in many applications ) . Also , learning is computationally expensive with NCA , LMNN and MLR approaches3 . Therefore , they are not suitable for very large scale problems .
Jain et al . [ 12 ] propose a method ( figure 1(c ) ) that applies LSH on a learned metric ( referred as M+LSH in Table 1 ) . However , this method does not use task specific objective function for learning the metric ; more importantly , it does not learn the bit vector representation directly . Although LSH can be applied on the projected data using a metric learned via NCA or LMNN , any such independent two stage method will be sub optimal in getting a good bit vector representation . 1.2 Our Contributions
We propose a unified framework ( Section 3 ) to develop fast nearest neighbor methods for various applications such as classification , retrieval , regression , and user recommendations . In our method we learn the bit vector representation directly by explicitly optimizing task specific performance . This is done by solving a learning torank problem . Our approach has several advantages .
• Using bit vector representation helps in finding nearest neigh bors fast using Hamming distance as the measure .
• High performance is achieved by direct bit vector learning and explicit optimization of task specific performance measure .
• It is easy to adapt the method for different applications via choices of performance measure and ranking . LambdaRank [ 5 ] can be used to optimize various non smooth task specific performance measures ( eg , precision@k and nearest neighbor classification accuracy ) ( Section 3 ) .
3In LMNN and MLR , computationally complex matrix constraints such as positive definiteness ( PD ) are used . Chechik et al . [ 6 ] argue that PD constraints are not necessary for large datasets , and they do not use such constraints . In NCA , the computational cost is quadratic in the number of training points ; there are no positive definite constraints since the projection matrix is learned directly .
WWW 2012 – Session : Data and Content Management 2April 16–20 , 2012 , Lyon , France930 Bit vector
Learned function
Query / Document
( a )
( b ) h
Weight matrix W v
+
W2j
W1j v1 v2 bj
WDj vD
( c )
Figure 2 : ( a ) The algorithm we are proposing learns a function that takes a query or a document as input and computes a bit vector as output . ( b ) The function multiplies the input vector v by the matrix W , adds the biases bj to the result , and then applies the sigmoid function to compute a vector h whose components are real values between 0 and 1 . The elements of W and the biases bj are the learnable parameters of the function . ( c ) The sigmoid is applied component wise to the result of the linear projection .
Learning Representation Task Specific No bit vector
No
Yes
Yes
Method LSH Fig 1(a ) Spectral Fig 1(d ) Semantic Yes Fig 1(d ) NCA Fig 1(b ) LMNN Fig 1(b ) OASIS Fig 1(b ) MLR Fig 1(b ) M LSH Fig 1(c ) Ours Fig 1(e )
Yes
Yes
Yes bit vector bit vector
No
No real vector
Yes ( C ) real vector
Yes ( C ) real vector
Yes ( IR ) real vector
Yes ( IR ) bit vector
No bit vector
Yes ( C , IR , R )
Yes
Yes classification and regression ( Section 6 ) . We also show how the speed of our method can be significantly improved further with a multi stage ( cascaded ) implementation ; here , speed improvement is achieved by allocating fewer bits in earlier stages , and reducing the nearest neighbor search space progressively ( Section 6 ) .
2 . OVERVIEW
Our algorithm learns a function that takes a query or document vector as input and computes its corresponding bit vector representation as output ( see figure 2a ) . Here we describe at a high level how the function is learned , and after learning , how it is used for nearest neighbor search . Training data : As mentioned before , we adopt a learning to rank approach . So each training example has the form ( query , document list ) , where document list specifies how its elements should be ordered with respect to that query . In this paper we assume that both the query and the document are the same type of data , using the same representation ( eg , both are images of the same size , or both are text documents that use the same bag of words representation)4 . Therefore the same learned function can be applied to both a query and a document to compute the corresponding bit vector . Parameterization of the function : The function applies linear projections to the input vector , followed by the sigmoid function to map the output of the linear projections into real numbers in the range [ 0 , 1 ] ( see figure 2b and 2c ) . Making the function differentiable allows for gradient descent learning . At test time , when a proper bit vector is required for fast nearest neighbor search , the output of the sigmoid function is thresholded at 0.5 to convert its value from a real number in [ 0 , 1 ] into 0 or 1 . The linear projection coefficients are the learnable parameters of the function . They are represented as a matrix of size ( number of inputs ) × ( number of bits ) . See section 3 for more details . Learning : LambdaRank [ 5 ] is the algorithm we use for learning . It is based on the RankNet algorithm [ 4 ] , which learns a pairwise ranking function . Given a triplet ( query , document1 , document2 ) , the ranking function computes the probability that document 1 should be ranked higher than document 2 for that query . The parameters of the function are learned iteratively using the gradient of the log probability of the true pairwise ranking for a set of training triplets .
While RankNet learning is limited to pairwise ranking , Lamb
Table 1 : Comparison of Methods ( see text for details ) . The abbreviations C , IR and R stand for classification , information retrieval and regression tasks respectively , and PD stands for positive definite matrix constraint . Compact bit vector representation is preferable for faster nearest neighbor search during training and testing . Constraints such as positive definite affect training speed . Taskspecific performance optimization is important to get improved performance . Jain et al . [ 12 ] use information theoretic criterion for distance metric learning . However , LSH can be combined with any other task specific distance metric learning methods such as NCA , LMNN , etc .
• From an optimization view point , our formulation and implementation ( Section 4 ) are simple and scalable . We observed that our algorithm , parallelized over 8 cores , completes learning in 2 days on a 1.46 million image dataset . At test time on the same dataset , 100000 query searches over 1.46 million vectors with a 32 bit representation using an inverted index are completed in 1.37 seconds ( 13.7 microseconds per query on average ) on a single core of an Intel Xeon 2.33GHz machine with 4GB RAM .
We conduct detailed experimental study ( Section 5 ) on several benchmark datasets for two applications : 1 ) classification and 2 ) information retrieval . Comparisons with state of the art NN methods show that our method performs significantly better . We discuss applicability of our framework to other problems such as hierarchical daRank can optimize more general , listwise ranking evaluation scores . Suppose LambdaRank is optimizing a ranking function for some
4It is easy to modify our algorithm to handle the case where the query and the document are two distinct types of inputs , but we do not consider it here .
WWW 2012 – Session : Data and Content Management 2April 16–20 , 2012 , Lyon , France931 evaluation score S ( eg NDCG ) . Given a query , the basic version of LambdaRank defines the gradient to be that function ’s RankNet gradient for a pair of documents , multiplied by the change in the score ’s value |ΔS| if those two documents swapped positions in the full listwise ordering computed by the current ranking function . Optimizing with the LambdaRank gradient has been shown empirically to converge to a local optimum of various non smooth IR evaluation scores [ 7 ] .
Adapting LambdaRank to learn a bit vector representation requires several modifications . These are 1 ) the parameterization of the ranking function , 2 ) the procedure for finding pairwise swaps that give nonzero |ΔS| , and 3 ) the definition of appropriate evaluation scores for various tasks . In addition , we show how to apply LambdaRank to non retrieval tasks such as classification and regression with bit vectors . Details are in section 3 . 2.1 Nearest neighbor search with bit vectors After learning , the bit vector representation is used to find nearest neighbors for a query vector within a set of documents . The query is first converted into a bit vector using the learned function , with the output of the sigmoid function thresholded at 05 The bit vectors for the documents to be searched can be precomputed and stored in memory . The query bit vector is then compared against the document bit vectors in the search set with Hamming distance as the metric . Documents with the lowest Hamming distances are returned as neighbors .
Two important choices that need to be made in bit vector based nearest neighbor search are 1 ) the type of thresholding to use on the Hamming distance , and 2 ) how to resolve ties in the ranking . These choices are task specific , so we discuss them in section 3 .
3 . PROBLEM FORMULATION
Now we explain how the training data for different tasks are constructed to fit the learning to rank framework . Then we define the function for computing the bit vector representation , how it is used for ranking , and how it is learned from data using the RankNet and LambdaRank algorithms . 3.1 Training data for different tasks
Learning requires triplets , each consisting of a query and two documents , along with the desired pairwise ranking . Depending on the task ( eg retrieval , nearest neighbor classification , nearest neighbor regression , etc. ) , how these triplets are created differs . NN Classification : Given a classification dataset consisting of ( input , label ) pairs , we define triplets as follows : the input for which the class label needs to be predicted is the query . Inputs in the training set that are used as candidate neighbors are the documents . Given a query belonging to a particular class , we want documents of the same class to get ranked higher than documents of all other classes . We will denote input vectors by v and their labels by l . So three class labeled inputs ( v1 , l1 ) , ( v2 , l2 ) , and ( v3 , l3 ) , where l1 = l2 and l1 .= l3 are turned into a triplet ( v1 , v2 , v3 ) where v1 is the query , v2 and v3 form the document pair . Retrieval : A retrieval dataset may already be in the desired querydocument format , where each query has a corresponding list of documents ordered by a relevance label . In such a case , it is clear how to construct triplets . In image retrieval applications ( such as the ones used in our experiments ) we may simply be given for each query a set of documents that are equally relevant ( ie the relevance label is either 0 or 1 ) . Here we can follow the same strategy as in the classification setting and create triplets of the form ( v1 , v2 , v3 ) where v1 is the query , v2 is a relevant document for the query , and v3 is an irrelevant document .
3.2 Computing the bit vector representation Let v ∈ 'D be the D dimensional input vector ( either a query or a document ) for which we want to compute the bit vector . First we define a function for computing a real valued B dimensional vector h whose components have values between 0 and 1 , h ∈ [ 0 , 1]B , from v . The bit vector is subsequently computed from h by rounding . The jth component of h is given by hj =
1 + exp
“
“ −
1 bj +
P
D i=1 Wijvi
” ” ,
( 1 ) where vi is the ith component of h , Wij is a learnable weight parameter , bj is a learnable bias , and 1/(1+exp(−(· ) ) is the sigmoid function . The function for computing the entire h vector is parameterized in terms of a D × B matrix W and the B bias parameters ( figure 2b ) . To simplify notation we omit bias terms from the equations in the rest of the paper without loss of generality5 The bit vector b is computed by rounding each component of h to 0 or 1 . Note that h is differentiable with respect to W , but b is not because of the non smoothness of rounding .
3.3 Smooth ranking function for learning
We want to formulate an algorithm for learning W within a learning to rank framework . To do this we first need to define a ranking function suited for learning a bit vector representation . It takes a query and a document as inputs and computes a scalar score with which the document can be ranked . Given a query vq and a set of documents {vd1 , vd2 , , vdn} ( all in 'D ) , the score can be computed for each query document pair , followed by a sort , to rank the documents with respect to the query . Let bq ∈ {0 , 1}B be the bit vector for the query and bi ∈ {0 , 1}B be the bit vector for the ith document . The ranking score si for the ith document is the Hamming distance between bq and bi . Documents are ranked in ascending order of the score .
For gradient based learning we need the ranking score to be differentiable with respect to W . The score si does not meet this requirement because rounding is not a differentiable function . So we compute an alternative score ˆsi using h ( from equation 1 ) instead . Since h is real valued , Hamming distance is no longer appropriate . One alternative is to use Euclidean distance , ie let the score for the ith document be the Euclidean distance between hq and hi . When the components of hq and hi are exactly binary , the score will be the same as Hamming distance . One drawback is that the score can also be zero even when hq and hi are not binary , but still identical . For example , if the components of both vectors are all 0.5 , then the score will be zero .
A second alternative is the following :
ˆsi =
NX j=1 hqj(1 − hij ) + ( 1− hqj)hij ,
( 2 ) where hqj and hij are the jth component of hq and hi , respectively . It ‘relaxes’ Hamming distance to non binary values , and becomes equal to Hamming distance for binary values . It is not a proper distance metric – two identical vectors will not give a score of zero unless they are binary . We use this relaxed Hamming score in our experiments .
5The biases can be put into W by appending an extra constant element 1 to the input vector and defining W to be a ( D + 1 ) × B matrix where the extra row corresponds to the biases .
WWW 2012 – Session : Data and Content Management 2April 16–20 , 2012 , Lyon , France932 3.4 Learning with RankNet gradient as follows :
As mentioned before , LambdaRank is based on RankNet . Now we describe RankNet in some detail because it is required for LambdaRank . Also it serves as an important baseline in our experiments to demonstrate the usefulness of task specific optimization .
So far we have formulated a ranking function that incorporates the smooth version of the bit vector representation and is differentiable with respect to W . Now we plug this ranking function into the RankNet algorithm to learn W .
Given a query vq and a pair of documents , vd1 and vd2 , let vd1 vd2 denote the event that vd1 is ranked higher than vd2 . RankNet adopts a probabilistic view that allows for uncertainty in the pairwise ranking . The ranking scores of the two documents do not deterministically imply an ordering . Instead they define a probability that vd1 is ranked higher than vd2 :
P ( vd1 vd2 ) =
1
1 + exp(ˆs1 − ˆs2 )
,
( 3 ) where ˆs1 and ˆs2 are the ranking scores for vd1 and vd2 , respectively ( as computed by equations 1 and 2 ) .
Note that if ˆs1 < ˆs2 , then P ( vd1 vd2 ) > 05 Intuitively if one document is closer to the query than the other , it has a higher probability of being ranked higher than the farther one . And if ˆs1 << ˆs2 ( document 1 is much closer to the query than document 2 ) , then P ( vd1 vd2 ) ≈ 1 .
Consider the simple case where our training set consists of only one triplet ( vq , vd1 , vd2 ) . Let T1 2 be the target probability that vd1 is ranked higher than vd2 with respect to vq . For example , T1 2 = 1 if vd1 should be ranked higher than vd2 , and 0 if the opposite is true . The cost function used by RankNet for learning is : C = −T1 2 log P ( vd1 vd2 )−(1−T1 2 ) log(1−P ( vd1 vd2 ) ) . ( 4 ) C is the cross entropy between the two Bernoulli distributions P ( vd1 vd2 ) and T1 2 . The parameters of the ranking function are learned via gradient descent on C . C is a smooth differentiable function of W , and an expression for ∂C ∂W can be derived analytically . Equation 4 considers only a single training triplet , but it can be applied to the case where multiple training triplets are available by simply minimizing the average cost of all the triplets .
Note that with RankNet only the way in which the triplets are defined differs from task to task . The training cost function and the optimization procedure stay the same across tasks . In contrast , LambdaRank allows a different cost function for each task , which makes it possible to customize the learning . 3.5 Learning with LambdaRank
We begin with a general description of LambdaRank , and then describe the adaptations specific to learning a bit vector representation for classification and retrieval in sections 351 , 352 , and 353
We keep the parameterization of the ranking function and the definition of the ranking score the same as before , but change the learning algorithm to LambdaRank ( figure 3 ) . This affects two things – 1 ) the objective function used for learning , and 2 ) the procedure for computing its gradient .
For simplicity we present LambdaRank here as a modified version of RankNet , but the underlying ideas are more general . Consider again a RankNet training triplet ( vq , vd1 , vd2 ) where vq is a query , and vd1 and vd2 are two documents to be ranked with respect to vq . Suppose that we want the learning to optimize the ranking function for an evaluation score S . S can be a listwise ranking score , eg NDCG . Then LambdaRank modifies the RankNet
∂C ∂W
|ΔS|
( 5 ) where C is the RankNet cost function ( equation 4 ) and W is the weight matrix to be learned . |ΔS| is the absolute difference in the value of S due to swapping the positions of vd1 and vd2 in the ordering of all documents , with respect to vq , computed by the current ranking function .
Note that LambdaRank learns on triplets , as before , but now only those triplets that produce a non zero change in S by swapping the positions of the documents contribute to the learning . Given a query , first all documents are ordered with respect to it using the ranking function given by the current W . Evaluating this ordering will give some score S1 . Now pick any two documents and swap their positions in the ordering . Evaluating this new ordering will give some score S2 . LambdaRank then computes the RankNet gradient for those two documents and the query , and multiplies it by |S1−S2| . Only the score function S needs to be changed from task to task in LambdaRank .
We now describe aspects of LambdaRank training that are spe cific to our approach .
Score function for classification :
351 Suppose that for a query vq , L documents {vd1 , vd2 , , vdL} are selected as neighbors ( as explained later ) . Let lq and ldi be the class labels of the query and the ith document , respectively . The score function SC for nearest neighbor classification of a single query is :
SC = [ M AJORIT Y ( ld1 , ld2 , , ldL ) =l q ] ,
( 6 ) where the M AJORIT Y ( · ) function picks the most frequent label in {ld1 , ld2 , , ldL} and [ · ] is Iverson notation denoting an output 1 if the argument is true and 0 otherwise .
One disadvantage of the above score is that it does not reflect incremental progress towards the correct answer . For example , if the correct label is three votes short of becoming the majority , then a swap that increases its vote count by 1 will not change the above score . But such a swap moves the result closer to the correct answer and therefore should be used . To allow for such swaps , we use the following the score :
LX
[ ldi = lq ] .
SC =
( 7 ) i=1
Note that without the M AJORIT Y ( · ) function , even swaps that would make all the neighbors have the same label as the query would be allowed . This is more stringent than necessary for nearest neighbor classification . A score function with a looser requirement may give better accuracy , but we have not yet explored that . Thresholding for neighbor selection : There are two types of thresholding that can be used on the Hamming distance . An absolute threshold k selects all documents with Hamming distance ≤ k from the query as neighbors . A relative threshold k selects only those documents in the k nearest non empty Hamming distance bins from the query as neighbors . Consider an example where the nearest documents from the query appear at say , distances 4 , 7 , 12 , 17 , 20 , etc . and k = 3 . With an absolute threshold , no documents would be selected , which makes nearest neighbor classification ambiguous . But with a relative threshold , documents at distances 4 , 7 , and 12 would be selected . Since a relative threshold guarantees that the set of neighbors will always be non empty , we use it for nearest neighbor classification .
WWW 2012 – Session : Data and Content Management 2April 16–20 , 2012 , Lyon , France933 Resolving ties : If we use a B dimensional bit vector representation , then Hammming distance can take on only B + 1 possible values ( 0 to B ) . If the number of documents being ranked is much greater than B + 1 , then there will be a lot of ties in the ranking . For nearest neighbor classification , we do not break ties . Instead we use all the documents from the selected bins to vote on the class label .
352 Score function for retrieval :
In section 5 we consider two retrieval tasks where the relevance label of a document is binary ( either 0 or 1 ) , and accuracy is measured using precision ( # of retrieved documents that are relevant divided by # of retrieved documents ) . We define the score function for this particular setting . Again consider a query vq , and L documents {vd1 , vd2 , , vdL} retrieved for that query with an absolute threshold k . Let ldi be the relevance label for the ith document . The score function SR for retrieval is : LX
SR = ldi .
( 8 ) i=1
We do not normalize the score by L in order to prevent queries that have a small number of retrieved documents from contributing disproportionately to the gradient . Resolving ties : During learning , we do not break ties in the ranking . But during testing , tie breaking may be needed . For example , consider a task that requires retrieving exactly 100 documents for a given query . If the nearest Hamming distance bin to the query contains 110 documents , then some re ranking procedure would be needed to select exactly 1006 . In such an application a bit vector representation is still useful for rapidly shortlisting a small set of documents for the re ranker . Note that tie breaking is needed for any approach that uses bit vectors , not just ours . 353 Procedure for finding swaps with |ΔS| > 0 A key step in the per query gradient computation of LambdaRank is to identify only those pairwise swaps that have nonzero |ΔS| ( steps 3 and 4 in figure 3 ) . At first glance , finding such swaps may appear to be a very expensive computation that requires sorting all the documents in the training set for every query . But in the case of learning a bit vector representation , a fast approximation turns out to be possible .
Given a query , there are only a fixed number of Hamming distance bins that the documents can belong to . For efficiency , we use swaps only among those documents that belong to a small subset of bins closest to the query . Finding documents that belong to the nearest bins can be done efficiently using a heap structure , without having to compute the full Hamming distance and sorting over the entire training set .
IMPLEMENTATION DETAILS
4 . Subsampling : To significantly speed up the per query gradient computation , we subsample the set of documents from which document pairs are selected for each query . A noisy estimate of the gradient can be computed cheaply from a small subset of the full document set . This is helpful when the training set is large and computing the gradient from the full set is too expensive .
In the case of RankNet , for each query we use only a small subset of the full training set to generate pairs for that query . Most of our experiments ( section 5 ) use only 100 randomly chosen documents per query to generate triplets .
6One possibility is to sort by the real valued score ˆsi ( equation 2 ) and select the top 100 .
LambdaRank learning algorithm : Training inputs : A ranking metric M ( eg NDCG ) to be optimized . A set of training examples where the ith example contains :
∈ fiD ,
1 . Query vi q 2 . List Li of N documents {vi
, , vi dN
} all in fiD . d1
Number of bits B . Learning rate parameters : step size η , momentum m . Training outputs : D × B weight matrix W . Initialization : Wij are sampled from zero mean Gaussian with small ( 10−3 ) variance , ΔW = zero matrix .
Weight update computed using the ith training case :
1 . Compute real valued representation hq for vi 2 . For the kth document in Li compute : q using equation 1 .
( a ) Compute real valued representation hk for vi dk using equation 1 .
( b ) Compute score ˆsk using equation 2 from hq and hk .
3 . Compute an ordering O of the documents in Li by sorting them in ascending order of the scores ˆsk . Let SO be the value of the evaluation score S for O . For an efficient approximation , see sections 353 and 4 .
4 . Find the set V of pairs of documents ( vi a such that swapping the positions of ( vi a and vi for which |SO − SO.| > 0 . a new ordering O . b ) ∈ V compute P ( vi
5 . For each element ( vi a
, vi b
, vi b equation 3 .
) selected from Li ) in O results in vi b
) using a with respect to W .
∂W of the cost function C ( equation 4 )
6 . Compute the gradient ∂C 7 . ΔW ← mΔW − η(|SO − SO.| ∂C 8 . W ← W + ΔW . Figure 3 : Summary of LambdaRank learning algorithm .
∂W
) .
For LambdaRank , we restrict the number of bins from which documents are considered for swaps to be one third of the number of bits . For the tasks considered here , swaps only happen between a document in one of the top k bins and a document outside of the top k bins , but still within the restricted set . We can make this even more efficient by considering only a subset of the full training set ( Li in step 2 of figure 3 ) to populate these bins . How much subsampling can be done without degrading accuracy depends on the dataset , but in our experiments we have seen that good results can be achieved even with 10× subsampling . Gradient descent : The weight matrix W is updated by averaging gradient estimates given by a set of queries and taking a step along the average gradient . Averaging can reduce the noise in the updates . The average is typically computed over 100 queries . One can easily parallelize this computation by splitting the set of queries across multiple cores and then averaging together the gradients computed at all cores .
−1 , 10
We use a fixed step size to update W . The best value depends on the dataset – for the datasets in section 5 we have tried values in the −4 ] . On some datasets we have observed significant range [ 10 sensitivity in the accuracy to the step size value . We set the step size to the highest possible value that does not produce large oscillations in the objective function value during optimization .
We have not yet tried any second order optimization methods like conjugate gradient to improve convergence speed . Such methods may help in RankNet training , but are unlikely to be useful
WWW 2012 – Session : Data and Content Management 2April 16–20 , 2012 , Lyon , France934 for LambdaRank since the actual objective function is implicit and cannot be directly evaluated .
As in [ 11 ] , we maintain an exponentially decaying sum of the previous gradients which is added to the current gradient to compute the weight update . The decay factor is set to 0.8 , so the effect of the gradient computed at a particular step persists for several steps afterwards .
5 . EXPERIMENTAL EVALUATION
Performance is evaluated on two types of tasks : 1 ) nearest neighbor classification and 2 ) retrieval . We present results for four classification and two retrieval datasets . 5.1 Metrics
The evaluation metric for classification is the number of incorrect label predictions on a test set . Given a test case , its bit vector is compared against the bit vectors for all the training cases . Neighbors are selected using a relative threshold k – those training cases that fall within the nearest k non empty Hamming distances to the test case bit vector are returned . The predicted label is then picked by a majority vote among the neighbors .
For retrieval we use the same precision metric as in Weiss et al . [ 23 ] . Given a query , its bit vector is compared against the bit vectors for all the documents in the search set . Documents are selected using an absolute threshold – those documents that are less than a pre specified Hamming distance threshold from the query bit vector are retrieved . We consider binary relevance labels here7 – a document is either relevant for a query or not . So precision for a single query is computed as follows :
Precision =
# of relevant documents in the retrieved set
# of documents in the retrieved set
.
( 9 )
This quantity is then averaged over a held out set of queries to get a single precision value .
5.2 Datasets
Tables 2 and 3 summarize the datasets . The datasets span a range of training set sizes ( 60K to 1.45 million ) , input dimensionality ( 128 to 47K ) , and in the case of classification , number of classes ( 7 to 101 ) . They include a number of different types of data – images ( MNIST , INRIA SIFT 1M , Tiny Images Subset ) , text documents ( MCAT , RCV ) , and geospatial measurements ( Covertype ) .
We briefly describe each dataset :
MNIST8 is a set of handwritten images of the digits 0 to 9 . The images are grayscale and of size 28 × 28 . MCAT contains text documents that belong to a subtree of the Reuters Corpus Volume 1 ( RCV1 ) dataset [ 14 ] . A document is represented as a bag of words with a vocabulary size of 11429 . Only about 0.58 % of the word counts are nonzero , so the representation is sparse . Covertype9 [ 3 ] is a dataset of geospatial measurements that are used to predict the forest covertype at various locations in the US . RCV1 Subset10 contains only those documents in the RCV1 dataset that do not have multiple labels associated with them . As in MCAT , a document is represented as a bag of words , but with a much
7The two retrieval datasets we use supply binary relevance labels . 8http://yannlecuncom/exdb/mnist/ 9http://archiveicsuciedu/ml/datasets/Covertype 10http://wwwcsientuedutw/~cjlin/libsvmtools/datasets/ binaryhtml\#rcv1binary
Dataset
Train set Test set
MNIST MCAT Covertype RCV1 size 60000 150344 522911 531742 size 10000 4362 58101 15913
Input Number of dim . 784 11429 classes
10 7 7 101
54
47236
Table 2 : Classification datasets .
Dataset
INRIA SIFT1M Tiny Images Subset
Train set size
100000
# of test queries 10000
# of test docs to search 1000000
Input dim . 128
1458356
100000
1458356
512
Table 3 : Retrieval datasets . larger vocabulary size of 47236 and 101 classes . The representation is 0.14 % sparse . INRIA SIFT1M11 is a web image dataset designed for evaluating retrieval algorithms . It consists of 128 dimensional SIFT features [ 15 ] computed for images collected from the web . Given a query image ’s SIFT feature vector , the relevant images to retrieve are defined to be its 50 nearest neighbors , according to Euclidean distance , in the SIFT feature space . Tiny Images Subset is derived from the Tiny Images dataset [ 21 ] , which contains 80 million images collected from the web . Various text queries were given to popular image search engines and the results were downloaded . The subset we use here contains only the top ranked images for the query terms , so it is likely to be less noisy than the full 80 million set . The images are represented using 512 dimensional GIST feature vectors [ 17 ] . The retrieval task is defined in the same way as in INRIA SIFT1M : the goal is to retrieve the 50 nearest neighbors , according to Euclidean distance , of a query image ’s GIST feature vector .
5.3 Baselines
We compare against other methods that compute a bit vector representation . For retrieval , Spectral Hashing is a state of the art method . Binary LSH is a commonly used baseline in the literature , so we compare against it as well . We use the Matlab implementation of Spectral Hashing by the authors of that paper12 .
For classification , we again use Spectral Hashing and LSH as baselines . However these methods were not intended to be used for classification , so they cannot be taken as strong baselines . Therefore we decided to compare also against nearest neighbor methods that do not use bit vectors . The simplest one is kNN classification with L2 distance . State of the art learning methods are NCA and LMNN . LMNN does not scale to datasets with more than a few tens of thousands of training cases [ 22 ] , so we cannot run it on Covertype , MCAT , and RCV1 Subset . For MNIST we quote the LMNN classification error from [ 22 ] . For NCA , we use the implementation in the Matlab Toolbox for Dimensionality Reduction13 .
Note that one can always apply binary LSH on top of a metric learning method like NCA or LMNN to construct bit vectors . But such a two stage approach will at best give the same accuracy as the
11http://corpus texmexirisafr/ 12wwwcshujiacil/~yweiss/SpectralHashing/ 13http://homepagetudelftnl/19j49/Matlab\_Toolbox\_for\ _Dimensionality\_Reduction.html
WWW 2012 – Session : Data and Content Management 2April 16–20 , 2012 , Lyon , France935 underlying real valued metric since binary LSH is only an approximation of it . So we compare against the accuracy of the real valued metric directly , ie the best case result for the two stage approach . On the RCV1 Subset , Spectral Hashing required doing an eigenvalue decomposition of the 47236 × 47236 data covariance matrix and selecting the top eigenvectors . For 32 and 64 bits , Matlab was not able to perform this computation due to excessive memory use , even with 32GB of RAM . Those results are not given . In NCA , the dimensionality of the output of the linear projection matrix can be made less than the input dimensionality . The dimensionality we chose is 60 for MNIST , 500 for MCAT , 54 for Covertype , and 500 for RCV . On MNIST 60 gave the same accuracy as bigger values . On Covertype the input dimensionality is already small ( 54 ) , so a lower number was not tried . On MCAT and RCV , we chose 500 to keep the training times reasonble . 5.4 Classification Results
Table 4 shows the test set error rates for the four classification datasets . We use a relative threshold of k = 3 for all methods that use bit vectors . LambdaRank achieves the lowest classification error on three out of four datasets . Among bit vector methods , LambdaRank is 48.9 % better than others ( averaged over four datasets ) , with RankNet being the closest competitor . LambdaRank is on average 26.8 % better than the best RankNet result . Only on the Covertype dataset does RankNet perform comparably to LambdaRank . So task specific optimization by LambdaRank improves accuracy , and as MNIST and Covertype results show , the improvement can be substantial ( +54 % and +42 % , respectively ) .
Recall from section 351 that there can be ties in the Hamming distance ranking . A relative threshold k only guarantees that the number of neighbors used to classify a given test case is at least k . The actual number can be much larger than k if there are many ties . All the four bit vector methods compared in table 4 share this property . The better accuracy of LambdaRank despite this commonality implies that simply having a large number of neighbors to classify a test case is not sufficient for high accuracy , and that learning a good bit vector representation is also crucial .
The benefit of LambdaRank can also be measured in terms of the bit “ compression ” it gives with respect to the other methods while matching their best error . In many cases LambdaRank needs significantly fewer bits . To get a rough idea of how big the compression factor is , we linearly interpolate between the datapoints in table 4 to determine the number of bits needed by LambdaRank to achieve a particular error rate . On average the ratio of the number of bits needed by LSH , Spectral Hashing and RankNet to achieve their best error rate divided by the number of bits needed by LambdaRank to achieve the same error rate is 7.1× , 4.1× , and 2.0× , respectively .
5.5 Retrieval Results
Table 5 shows the precision results computed on the test sets of INRIA SIFT1M and Tiny Images Subset . Again , LambdaRank gives the best results on both datasets . It gives 1.2× and 2.66× better precision than Spectral hashing on the two datasets .
The improvements over RankNet are again substantial for both datasets : +6.5 % and +136 % This adds further evidence to the usefulness of task specific learning . 5.6 Training and Testing Times
The training time of our algorithm scales well to large datasets . On Tiny Images Subset ( 1.46 million training cases ) , learning with 8 core parallelization converges in approximately 2 days on an Intel Xeon 2.50GHz machine . Figure 4 shows the convergence be
No . of bits
LSH Spectral Rank Net hashing
Lambda
Rank
Non bit vector methods
8 16 32 64 128 256
8 16 32
8 16 32 64 128 256
8 16 32 64
58.73 52.15 24.61 13.20 7.71 4.64
67.35 60.39 42.85
42.07 33.96 20.65 12.33 9.55 7.64
84.63 75.20 63.79 50.95
MNIST 12.64 8.50 5.54 4.20 4.01 3.55 MCAT 1.70 1.38 1.42
Covertype
29.00 26.54 21.62 18.50 15.40 9.58 RCV1 45.60 18.56 16.84 13.34
12.32 7.57 4.85 4.02 2.37 1.63
1.54 1.47 1.31
30.36 21.60 14.24 10.88 7.74 5.52
25.21 15.93 14.41 12.58
33.63 19.67 10.10 6.82 5.74 4.63
24.97 8.02 5.36
42.17 34.30 27.29 14.68 9.46 7.44
75.24 59.46 kNN , L2 : 3.09
NCA : 2.45 LMNN : 1.72 kNN , L2 : 3.67
NCA : 7.66 kNN , L2 : 6.25
NCA : 4.01 kNN , L2 : 29.67
NCA : 45.79
Table 4 : Classification error ( % ) on the test sets of MNIST , MCAT , Covertype , and RCV1 .
No . of
LSH
Spectral hashing
Rank Net INRIA SIFT1M ( Precision ×10 15.00 184.61 1687.21 Tiny Image Subset ( Precision ×10
14.26 200.04 1462.76
6.11 44.70 476.29 bits
8 16 32
8 16 32
Lambda
Rank
−4 )
17.16 266.63 1805.25 −5 )
6.93 34.58 410.41
28.73 211.39 3396.62
36.85 430.98 7979.02
42.93 578.99 9065.89
Table 5 : Precision at Hamming distance < 2 from the query on the test sets of INRIA SIFT1M and Tiny Images Subset . haviour . Even after one pass through the data , the precision on the test set has already reached 90 % of its final value . The memory needs of the learning algorithm are minimal . The weight matrix , its gradient , and the bit vectors for the training set ( computed using the current weight matrix ) account for most of the memory use , and all of these together takes up much less memory than the training data itself ( eg for Tiny Images Subset these variables take up 0.2 % of the memory occupied by the training data ) .
Table 6 shows the CPU times needed for finding the nearest neighbors for the test set of different datasets by the different algorithms . Note that nearest neighbor classification with bit vectors is substantially faster than with real valued representations , even without using an inverted index . This is because the distance calculation for bit vectors is done using bitwise XOR on chunks of 8 bits , and the 8 bit result is converted into a Hamming distance with a lookup table . The distances from the 8 bit chunks are then added together to get the full distance . So computing the Hamming dis
WWW 2012 – Session : Data and Content Management 2April 16–20 , 2012 , Lyon , France936 y r e u q m o r f
2 < e c n a t s i d g n i m m a H t a n o i s i c e r P
14.5 hours
2 days
0.1
0.08
0.06
0.04
0.02
0 0
1
2
Epochs
3
4
5
Figure 4 : Precision on the test set as a function of the number of passes through the Tiny Images Subset training data . Note that most of the improvement happens in the first pass .
Dataset
Lambda
MNIST MCAT
Covertype
RCV1 Subset
Rank 23.99 8.85 765.83 127.78
L2 kNN 756.81 385.13 1744.83 5457.93
NCA
121.19 707.84 1785.33 10706.11
Table 6 : CPU time ( seconds ) required to find the nearest neighbors for the entire test set by different methods on the various datasets . tance between two 256 bit vectors requires only 256 bitwise XORs , 32 accesses into a lookup table and 32 adds .
For INRIA SIFT1M and Tiny Images Subset , the number of bits in the learned representation is small enough to build an inverted index that fits into 4GB RAM . We use subroutines from the Spectral Hashing software ( see footnote 11 ) to build the inverted index and use it for search . The CPU times for finding the nearest neighbors on the test sets of INRIA SIFT1M and Tiny Images Subset with a 32 bit representation and an inverted index are 0.10s and 1.37s , respectively , when run on a single core of an Intel Xeon 2.33GHz machine with 4GB RAM . This translates to an average search time per query of 10 microseconds for INRIA SIFT1M and 13.7 microseconds for Tiny Images Subset . Note that linear search on the same 32 bit representation is much slower than the inverted index . The total CPU times for finding the nearest neighbors on the test sets of INRIA SIFT1M and Tiny Images Subset are 63.09s and 1674.40s , respectively .
6 . EXTENSIONS AND FURTHER APPLICA
TIONS
Learning nonlinear features : Currently the ranking function does not contain any learnable nonlinear features of the input vector . Including such features is easy and can make the function more flexible and accurate . For example , one can use hidden units from the neural network literature to learn nonlinear features . Cascade architecture : A common way to speed up search is to use a multi stage , cascade architecture where a fast first stage search rules out a large fraction of the search set , followed by increasingly slower stages that only need to search the set retrieved by the previous stage . Such an architecture can also be used in our case – different ranking functions can be cascaded in increasing order of the number of bits , with each stage searching only among the neighbors found by the previous stage . We have tried a two stage classifier on the MNIST dataset with an 8 bit ranking function first and then a 256 bit classifier . The search set size for the 256 bit classifier reduces by an order of magnitude with almost no loss in accuracy .
Note that with LambdaRank it is possible to modify the training of the initial filtering stages such that they are explicitly trained to filter ( and not classify ) . We have not yet explored this option . Hierarchical classification : If a hierarchy over classes is given in a classification task , then the score function for LambdaRank can be modified to incorporate this information . Instead of binary relevance ( 1 for a document with the same label as the query , 0 otherwise ) , now we use multiple relevance levels to represent varying degrees of similarity between two classes . The similarity between two classes can be defined in many ways , eg with a monotonically decreasing function of the height of the common ancestor of the two classes in the hierarchy . For a query vq and L documents {vd1 , vd2 , , vdL} retrieved from the top K non empty Hamming distance bins , we can modify the original classification score function ( equation ) as follows :
SHier =
F ( lq , ldi ) .
( 10 )
LX i=1 where lq and ldi are the labels of the query and the ith document , respectively , and F ( a , b ) is a function that specifies the similarity between classes a and b . Regression : As mentioned before , a bit vector representation for nearest neighbor regression can be learned using our approach . In the case of RankNet , pairwise ranking of a document pair with respect to a query can be done using the absolute difference between the query target and a document target . The document with the closer target to the query in the pair should get ranked higher . For LambdaRank we need to define a score function . Consider a query vq , and L documents {vd1 , vd2 , , vdL} retrieved from the top K bins for that query . Let tq and tdi be the regression target values for the query and the ith document . The score can be the mean squared error between tq and the document targets :
SRegression =
1 L
( tq − tdi ) 2 .
LX i=1
( 11 )
As a preliminary experiment , we have trained a 64 bit RankNet model on the SARCOS dataset14 ( see section 2.5 of [ 18] ) . It achieves a better standardized mean squared error than a linear regression model . Experiments with LambdaRank is left as future work .
One potential application of nearest neighbor regression is in Collaborative Filtering . Neighborhood based models have been shown to be useful for predicting the rating a user would give to an item [ 2 ] . For example , in a user based neighborhood model , given a user item pair for which to predict a rating , a bit vectorbased representation can be used to retrieve similar users who have rated the same item . The predicted rating is then computed as a weighted average of the ratings of the retrieved users .
7 . OTHER RELATED WORK
We discussed several popular methods closely related to our work in the introduction , and empirically compared with representative methods . Here , we present other related work . Hashing Methods Kernel LSH [ 13 ] is a recent scheme that generalizes LSH , and is useful when similarity measure is given directly via a kernel function ( without explicit knowledge of the underlying transformation ) , or the underlying transformation is infinitely 14http://wwwgaussianprocessorg/gpml/data/
WWW 2012 – Session : Data and Content Management 2April 16–20 , 2012 , Lyon , France937 dimensional ( hence , incomputable ) . He et al . [ 10 ] propose a joint optimization method to optimize the codes for both preserving similarity as well as minimizing search time .
The main drawback of these hashing approaches is that they cannot be directly used in applications where we are not given a similarity metric but rather class/relevance labels that indicate which data points are similar or dissimilar to each other . Metric Learning Methods There are other methods that learn the distance metric by optimizing task specific performance measure . Classification task oriented methods such as NCA and LMNN discussed before fall under this category . Other approaches include Fisher ’s linear discriminant analysis ( FLDA ) , maximally collapsing metric learning algorithm ( MCML ) [ 8 ] , relevance component analysis ( RCA ) [ 20 ] , etc . While FLDA and MCML use class label information , RCA assumes that a set of chunklets is available , where each chunklet is a set of examples which belong to the same class ( but , the class information is unknown ; hence , RCA can be seen as a weaker form of supervised learning ) . Both FLDA and RCA involve matrix inversion ( in the dimension of input space ) , and use projections on eigen vectors for nearest neighbor classification . Therefore , they are computationally expensive for high dimensional data . MCML method tries to collapse all examples belonging to the same class into a single point , and keep examples belonging to other class far away . Like LMNN , MCML uses positive definite matrix constraints during training . All these methods are not scalable , and are limited to classification application .
To summarize , our work can be seen as merging two streams of work , one for learning a task specific metric from labeled data , and the other for learning bit vector representations for doing fast search . To our knowledge this is the first attempt that combines the two types of learning into a unified framework .
8 . CONCLUSIONS
We have presented a unified approach to learning a bit vector representation for nearest neighbor search in different tasks . The key contribution is the ability to customize the learning algorithm to the task at hand by modifying the score function used in LambdaRank training . What makes the unified approach possible is that the differences across the tasks are abstracted out of the core nearest neighbor search problem and pushed into the score function in LambdaRank . As a result the same algorithm can be easily adapted to different tasks to learn accurate representations for each . Experimental results clearly demonstrate that our algorithm 1 ) outperforms other methods for nearest neighbor classification and retrieval , and 2 ) scales well to large text and image datasets , making it particularly useful for web applications .
9 . REFERENCES [ 1 ] A . Andoni and P . Indyk . Near optimal hashing algorithms for approximate nearest neighbor in high dimensions . In In FOCS 2006 , pages 459–468 . IEEE Computer Society , 2006 . [ 2 ] R . Bell , Y . Koren , and C . Volinsky . Modeling relationships at multiple scales to improve accuracy of large recommender systems . In SIGKDD , KDD ’07 , pages 95–104 , New York , NY , USA , 2007 . ACM .
[ 3 ] J . A . Blackard and D . J . Dean . Comparative accuracies of artificial neural networks and discriminant analysis in predicting forest cover types from cartographic variables . Computers and Electronics in Agriculture , vol.24:131–151 , 1999 .
[ 4 ] C . Burges , T . Shaked , E . Renshaw , A . Lazier , M . Deeds ,
N . Hamilton , and G . Hullender . Learning to rank using gradient descent . In ICML , ICML ’05 , pages 89–96 , New York , NY , USA , 2005 . ACM .
[ 5 ] C . J . Burges , R . Ragno , and Q . V . Le . Learning to rank with nonsmooth cost functions . In B . Schölkopf , J . Platt , and T . Hoffman , editors , NIPS 19 , pages 193–200 . MIT Press , Cambridge , MA , 2007 .
[ 6 ] G . Chechik , V . Sharma , U . Shalit , and S . Bengio . Large scale online learning of image similarity through ranking . J . Mach . Learn . Res . , 11:1109–1135 , March 2010 .
[ 7 ] P . Donmez , K . M . Svore , and C . J . Burges . On the local optimality of lambdarank . In SIGIR , pages 460–467 , New York , NY , USA , 2009 . ACM .
[ 8 ] A . Globerson and S . T . Roweis . Metric learning by collapsing classes . In NIPS , pages –1–1 , 2005 .
[ 9 ] J . Goldberger , S . Roweis , G . Hinton , and R . Salakhutdinov .
Neighbourhood components analysis . In NIPS 17 , pages 513–520 . MIT Press , 2004 .
[ 10 ] J . He , R . Radhakrishnan , S F Chang , and C . Bauer .
Compact hashing with joint optimization of search accuracy and time . In IEEE Computer Society Conference on Computer Vision and Pattern Recognition ( CVPR ) , June 2011 .
[ 11 ] G . Hinton and R . Salakhutdinov . Reducing the dimensionality of data with neural networks . Science , 313(5786):504 – 507 , 2006 .
[ 12 ] P . Jain , B . Kulis , and K . Grauman . Fast image search for learned metrics . CVPR , 0:1–8 , 2008 .
[ 13 ] B . Kulis and K . Grauman . Kernelized locality sensitive hashing for scalable image search . In ICCV , 2009 .
[ 14 ] D . D . Lewis , Y . Yang , T . G . Rose , F . Li , G . Dietterich , and
F . Li . Rcv1 : A new benchmark collection for text categorization research . JMLR , 5:361–397 , 2004 .
[ 15 ] D . G . Lowe . Distinctive image features from scale invariant keypoints . Int . J . Comput . Vision , 60:91–110 , November 2004 .
[ 16 ] B . Mcfee and G . Lanckriet . Metric learning to rank . In
ICML , 2010 .
[ 17 ] A . Oliva and A . Torralba . Modeling the shape of the scene :
A holistic representation of the spatial envelope . IJCV , 42:145–175 , 2001 .
[ 18 ] C . E . Rasmussen and C . K . I . Williams . Gaussian Processes for Machine Learning . MIT Press , 2006 .
[ 19 ] R . Salakhutdinov and G . Hinton . Semantic Hashing . In
SIGIR workshop on Information Retrieval and applications of Graphical Models , 2007 .
[ 20 ] N . Shental , T . Hertz , D . Weinshall , and M . Pavel .
Adjustment learning and relevant component analysis . In ECCV , ECCV ’02 , pages 776–792 , London , UK , UK , 2002 . Springer Verlag .
[ 21 ] A . Torralba , R . Fergus , and W . T . Freeman . 80 million tiny images : A large data set for nonparametric object and scene recognition . PAMI , 30:1958–1970 , November 2008 .
[ 22 ] K . Q . Weinberger and L . K . Saul . Distance metric learning for large margin nearest neighbor classification . JMLR , 10:207–244 , June 2009 .
[ 23 ] Y . Weiss , A . Torralba , and R . Fergus . Spectral hashing . In
NIPS , pages 1753–1760 , 2008 .
WWW 2012 – Session : Data and Content Management 2April 16–20 , 2012 , Lyon , France938
