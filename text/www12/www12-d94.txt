Sparse Linear Methods with Side Information for Top N Recommendations
4 192 EE/CS Building , 200 Union Street SE
4 192 EE/CS Building , 200 Union Street SE
George Karypis
Computer Science & Engineering University of Minnesota , Twin Cities
Minneapolis , MN 55455 , USA karypis@csumnedu
Xia Ning
Computer Science & Engineering University of Minnesota , Twin Cities
Minneapolis , MN 55455 , USA xning@csumnedu
ABSTRACT This paper focuses on developing effective algorithms that utilize side information for top N recommender systems . A set of Sparse Linear Methods with Side information ( SSLIM ) is proposed , that utilize a regularized optimization process to learn a sparse item to item coefficient matrix based on historical user item purchase profiles and side information associated with the items . This coefficient matrix is used within an item based recommendation framework to generate a size N ranked list of items for a user . Our experimental results demonstrate that SSLIM outperforms other methods in effectively utilizing side information and achieving performance improvement . Categories and Subject Descriptors H4m [ Information Systems ] : Miscellaneous ; J.7 [ Computer Applications ] : Computers in other systems—Consumer products Keywords Recommender Systems , Sparse Linear Methods , Side Information 1 .
INTRODUCTION
Top N recommender systems are used in E commerce applications widely to recommend ranked lists of items so as to help the users in identifying the items that best fit their personal tastes . Over the years , many conventional collaborativefiltering based top N recommendation algorithms [ 2 ] have been developed that primarily focus on utilizing user item purchase profiles to generate recommendations . Recently , with the increasing availability of additional information associated with the items , referred to as side information , there is a greater interest in taking advantage of such information in order to improve the qualities of top N recommender systems . As a result , a number of approaches have been developed for incorporating side information , including hybrid methods [ 3 ] , matrix/tensor factorization [ 5 ] , and other regression methods [ 1 ] .
In this paper , we propose a set of Side information utilized Sparse LInear Methods ( SSLIM ) for top N recommendation . SSLIM learns a sparse coefficient matrix for the items , which is used to do top N recommendation , by leveraging both
Copyright is held by the author/owner(s ) . WWW 2012 Companion , April 16–20 , 2012 , Lyon , France . ACM 978 1 4503 1230 1/12/04 . user item purchase profiles and side information on items within a regularized optimization process . These methods learn recommendation models that explicitly incorporate the relation between the side information of an item and the historical purchase information of that item so as to improve recommendation accuracy . Sparsity is introduced into the coefficient matrix , which allows SSLIM to generate recommendations efficiently , and thus makes it better suitable for real time applications . Our experimental results show that SSLIM produces better recommendations than the state ofthe art methods . 2 . METHODS 2.1 SLIM : Sparse Linear Methods
SSLIM is an extension to the Sparse LInear Method [ 4 ]
( SLIM ) . In SLIM , the recommendation score on an un purchased item tj of a user ui is calculated as a sparse aggregation of items that have been purchased by ui , that is ,
˜mij = mT i sj ,
( 1 ) where mT is the purchase profile vector for ui on all the i n items with mik = 1 if ui has purchased item tk and 0 otherwise . In Equation 1 , mij = 0 and sj is a sparse size n column vector of aggregation coefficients . Top N recommendation for ui is done by sorting ui ’s non purchased items based on their recommendation scores in decreasing order and recommending the top N items .
SLIM views the purchase activity of user ui on item tj ( ie , mij ) as the ground truth item recommendation score , and learns the sparse n × n matrix S = [ s1,··· , sj,··· , sn ] as the minimizer for the following regularized optimization problem :
1 2 minimize subject to S ≥ 0
S
M − M S2
F +
S2
F + λS1
β 2 diag(S ) = 0 , i=1 and S1 =n n where M = [ m1,··· , mi,··· , ml]T ( ie , the user item purchase profile matrix ) , · F is the matrix Frobenius norm , j=1 |sij| is the entry wise 1 norm of S . The regularization on 1 norm ( ie , λS1 ) will introduce sparsity in the solution . The S ≥ 0 constraint is applied such that the learned S represents non negative relations between items . The constraint diag(S ) = 0 is also applied so as to avoid trivial solutions ( ie , the optimal S is an identical matrix ) and to ensure that mij is not used to compute ˜mij .
WWW 2012 – Poster PresentationApril 16–20 , 2012 , Lyon , France581 hhhhhhhh method dataset
SLIM SSLIM1 SSLIM2 itemSI CWRMF
Table 1 : Comparison of Top N Recommendation Algorithms
ML100K
HR ARHR 0.147 0.150 0.146 0.124 0.144
0.343 0.347 0.344 0.284 0.331
NF
HR ARHR 0.018 0.021 0.017 0.016 0.015
0.045 0.050 0.047 0.045 0.045
CrossRef HR ARHR 0.203 0.213 0.212 0.209 0.089
0.388 0.401 0.412 0.397 0.208
Lib
Pubmed
DrugSE
HR ARHR 0.266 0.287 0.279 0.264 0.224
0.407 0.444 0.438 0.418 0.385
HR ARHR 0.141 0.134 0.141 0.077 0.065
0.247 0.245 0.250 0.152 0.129
HR ARHR 0.157 0.137 0.158 0.034 0.084
0.280 0.263 0.283 0.090 0.193
Bold numbers are the best performance in terms of HR for each dataset . N in this table is equal to 10 .
2.2 SSLIM : SLIM with Side Information
We have developed two SSLIM approaches . Common to these approaches is that they utilize the side information during learning to bias the sparse coefficient matrix S .
The first approach imposes an additional requirement on S , that is , if the item purchase profile vector mj for item tj from all the l users is estimated by ˜mj = M sj , then the side information feature vector fj of tj should also be well approximated by
˜fj = F sj .
( 2 ) where F = [ f1,··· , fj,··· , fn ] . In order to satisfy this requirement , the corresponding S matrix can be learned as the minimizer of the following optimization problem , S2
M − M S2
F − F S2
F + λS1
F +
F +
α 2
1 2 minimize subject to S ≥ 0
S
β 2 diag(S ) = 0 ,
F − F S2
α 2 where the term F regularizes the coefficient matrix S to be learned such that it also fits a model on the side information ( ie , Equation 2 ) . The parameter α controls the importance of the side information F during learning . Larger α indicates more importance/emphasis on side information F . This method is denoted as SSLIM1 .
The second approach also tries to reproduce the feature vector for each tj , but it utilizes a different linear combination of the feature vectors via another aggregation coefficient vector qj , that is
˜fj = F qj .
( 3 )
However , it is required that sj and qj should not be very different . Note that the requirement in Equation 2 is a special case of the one in Equation 3 , when the penalty associated with the difference of sj and qj is very high . The matrix S and the matrix Q = [ q1,··· , qj,··· , qn ] in Equation 3 can be learned as the minimizer of the following optimization problem , minimize
S,Q
+ subject to
F
α 2
F +
S − Q2
M − M S2
F − F Q2 F + λ(S1 + Q1 )
1 2 β 2 S ≥ 0 , Q ≥ 0 diag(S ) = 0 , diag(Q ) = 0 . where the parameter β controls how S and Q are different from each other . This method is denoted as SSLIM2 .
3 . RESULTS
We evaluated the performance of SSLIM1 and SSLIM2 and compared them with other popular top N recommendation methods with side information incorporated ( ie , itemSI and CWRMF , modified from [ 2 ] and [ 5 ] , respectively ) . The performance was evaluated on six real datasets ( Table 1 ) following 5 time Leave One Out cross validation protocol , and measured by the Hit Rate ( HR ) and the Average Reciprocal Hit Rank ( ARHR ) [ 2 ] . Comparison of top N recommendation algorithms is presented in Table 1 . Comparing SLIM based methods ( ie , SSLIM1 and SSLIM2 vs SLIM ) , for all the datasets , SSLIM1 and SSLIM2 always improve the topN recommendation performance over SLIM in terms of both HR and ARHR . Comparing SSLIM1 and SSLIM2 with the other methods , either SSLIM1 or SSLIM2 achieves the best performance for all the datasets . In term of HR , the best of SSLIM1 and SSLIM2 is on average 8.5 % better than the best of any other comparison methods . For different values of N for top N recommendation , SSLIM consistently outperforms the other methods for the different values of N . We also conducted a density study by keeping the testing set and side information unchanged , but randomly select a certain percentage of non zero values from each user so as to construct training sets of different information density . The results on such training sets show that in general , SSLIM1 and SSLIM2 perform better than the other methods as the density decreases . Acknowledgement This work was supported in part by NSF ( IIS 0905220 , OCI1048018 , IOS 0820730 ) , NIH ( RLM008713A ) , and the Digital Technology Center at the University of Minnesota . Access to research and computing facilities was provided by the Digital Technology Center and the Minnesota Supercomputing Institute . 4 . REFERENCES [ 1 ] D . Agarwal and B C Chen . Regression based latent factor models . In Proceedings of the 15th ACM International Conference on Knowledge Discovery and Data Mining , pages 19–28 , 2009 .
[ 2 ] M . Deshpande and G . Karypis . Item based top n recommendation algorithms . ACM Transactions on Information Systems , 22:143–177 , January 2004 .
[ 3 ] Z . Gantner , L . Drumond , C . Freudenthaler , S . Rendle , and L . Schmidt Thieme . Learning attribute to feature mappings for cold start recommendations . IEEE International Conference on Data Mining , pages 176–185 , 2010 .
[ 4 ] X . Ning and G . Karypis . Slim : Sparse linear methods for top n recommender systems . In Proceedings of 11th IEEE International Conference on Data Mining , pages 497–506 , 2011 .
[ 5 ] A . P . Singh and G . J . Gordon . Relational learning via collective matrix factorization . In Proceeding of the 14th ACM International Conference on Knowledge Discovery and Data Mining , pages 650–658 , 2008 .
WWW 2012 – Poster PresentationApril 16–20 , 2012 , Lyon , France582
