Heterogeneous Web Data Search
Using Relevance based On The Fly Data Integration
Daniel M . Herzig
Institute AIFB
Karlsruhe Institute of Technology
76128 Karlsruhe , Germany herzig@kit.edu
Thanh Tran Institute AIFB
Karlsruhe Institute of Technology
76128 Karlsruhe , Germany ductran@kitedu
ABSTRACT Searching over heterogeneous structured data on the Web is challenging due to vocabulary and structure mismatches among different data sources . In this paper , we study two existing strategies and present a new approach to integrate additional data sources into the search process . The first strategy relies on data integration to mediate mismatches through upfront computation of mappings , based on which queries are rewritten to fit individual sources . The other extreme is keyword search , which does not require any upfront investment , but ignores structure information . Building on these strategies , we present a hybrid approach , which combines the advantages of both . Our approach does not require any upfront data integration , but also leverages the fine grained structure of the underlying data . For a structured query adhering to the vocabulary of just one source , the so called seed query , we construct an entity relevance model ( ERM ) , which captures the content and the structure of the seed query results . This ERM is then aligned on the fly with keyword search results retrieved from other sources and also used to rank these results . The outcome of our experiments using large scale real world data sets suggests that data integration leads to higher search effectiveness compared to keyword search and that our new hybrid approach consistently exceeds both strategies .
Categories and Subject Descriptors H33 [ Information Storage and Retrieval ] : Information Search and Retrieval—Relevance feedback General Terms Experimentation Keywords vertical search , structured web data , data integration , rdf 1 .
INTRODUCTION
A rapidly increasing amount of structured data can be found on the Web today . This development is triggered by the Linked Data movement , Semantic Web community efforts , and recently , also enjoys strong support from large companies including Google , Yahoo! and Facebook , and governmental institutions . The amount of Linked Data alone is in the order of billions of RDF triples , residing in hundreds of data sources [ 11 ] . In this paper , we aim at supporting the
Copyright is held by the International World Wide Web Conference Committee ( IW3C2 ) . Distribution of these papers is limited to classroom use , and personal use by others . WWW 2012 , April 16–20 , 2012 , Lyon , France . ACM 978 1 4503 1229 5/12/04 . exploitation of these structured Web data . In particular , we aim at extending vertical search capabilities beyond internal data to also incorporate external Web data into the retrieval process . We illustrate the problem behind it based on the following scenario :
There is a company running a movie shopping website . Users can search for movies on this website via form based interfaces , and their requests are internally executed as structured queries against the company ’s dataset . Now , the company aims to exploit the numerous Web data sources available as Linked Data , including data provided by a partner company with similar offerings and an encyclopedia dataset that contains additional movie related information . The goal is to incorporate data from these external sources into the search processes . However , the vocabularies and structure exhibited by these target data sources are different such that issuing the same structured queries ( called seed queries ) against these external sources may not produce any results . Results satisfying the information needs behind these seed queries may exist but due to mismatches in structural and syntactical representation , they cannot be found .
In this paper , we study three different strategies that are applicable to this search scenario :
( 1 ) There are Information Retrieval ( IR ) solutions , which treat both the data and queries as bags of words [ 5 , 20 ] . Because structure information is ignored during query processing , this strategy ( called keyword search ) often leads to non empty results – albeit with varying quality .
( 2 ) The alternative is to employ database solutions , where information needs are expressed as structured queries . Given the richer representation of the information needs , the structure of the underlying data can be exploited and incorporated into the matching process . While this can improve the quality of the results , this type of solutions requires upfront investment in data integration , ie computation of ontology and schema mappings and consolidation of data instances that refer to the same object ( entity mappings ) [ 8 , 9 , 13 , 4 ] . Based on these mappings , results from external sources can be obtained via query rewriting [ 3 , 23 ] . Integration efforts are needed whenever the data changes . Clearly , integration on the Web is hard due to the large number of sources and their scale as well as their heterogeneity regarding differences at the schema and data level , which is illustrated for our scenario in Figure 1 . Here , entities representing movies are displayed . One can observe that three different representations of “ Steven Spielberg ” are used for the same real world object . Also , different labels are used to express the same attribute .
( 3 ) As the third category , we elaborate on a hybrid solution , which combines the flexibility of unstructured IR solu
WWW 2012 – Session : Semantic Web Approaches in SearchApril 16–20 , 2012 , Lyon , France141 Figure 1 : Data heterogeneity on the Web . Entities from three different Web datasets are represented differently at the schema level ( eg actors vs . starring ) and data level ( eg Spielberg , Steven vs . Steven Spielberg ) . tions ( in the sense that no prior data integration is needed ) and the expressiveness of database style querying by incorporating the structure of the underlying data . The idea is to start with a structured seed query specified for one particular source . Based on the content and structure of the results obtained from this source , we construct an Entity Relevance Model ( ERM ) that can be seen as a compact representation of relevant results mirroring the underlying information need . Instead of relying on up front computed mappings for rewriting the structured seed query , we treat the seed query as a keyword query and submit it against external data sources to obtain additional results . These candidates are obtained using a standard IR based search engine . Then , we create mappings between the structure of each candidate result and the structure of the ERM on the fly . These mappings are used for an additional round of matching and ranking . Candidates which more closely match the content as well as the structure captured by the ERM are ranked higher . Thereby the structure of the ERM and of the result candidates are incorporated into the search process . Since , the same similarity metrics for creating the mappings are reused for ranking , this on the fly integration comes for free . As a result , this hybrid strategy not only takes structure information into account for more effective search , but also provides on the fly computed mappings that can support a pay as you go integration paradigm where data integration is tightly embedded into the search process [ 16 ] .
Contributions . The contributions of this work can be summarized as follows : ( 1 ) We perform a systematic study of the two main prevailing strategies towards searching external heterogeneous data sources . In particular , we show how to adopt the data integration approach to our scenario where the computation of entity mappings is challenging . ( 2 ) To achieve the best of both worlds , we elaborate an a hybrid approach that does not rely on upfront data integration , but uses a query specific Entity Relevance Model ( ERM ) for searching as well as for computing mappings on the fly . ( 3 ) Based on large scale experiments using real world datasets , we observe that the data integration approach consistently provides better results than keyword search . The hybrid approach yields best results , outperforming keyword search by 120 % and the data integration baseline by 54 % on average in terms of Mean Average Precision . Further , this hybrid approach is able to leverage upfront integration results , leading to additional quality improvement when precomputed mappings are considered . The qualitative differences between these approaches are : Keyword search and the hybrid approach do not require upfront data integration . Additionally , the hybrid approach provides on the fly computed mappings that can be used for a pay as you go in tegration process that can exploit user feedbacks for quality improvement ( as discussed in [ 16] ) .
Outline . Section 2 defines the research problem and gives an overview of existing solutions and briefly sketches our new approach . This approach of relevance based on the fly mappings is presented in detail in Section 3 . Evaluation results are presented in Section 4 . Section 5 discusses the related work before we conclude in Section 6 .
2 . OVERVIEW
In this section , we present the setting of the addressed problem , and provide an overview of three different solutions . 2.1 Data Heterogeneity on the Web
The problem we address is situated in a Web data scenario . The kind of Web data that is of most interest is RDF data . For reasons of generality and simplicity , we employ a generic graph based data model that omits specific RDF features such as blank nodes . In this model , entity nodes are RDF resources , literal nodes correspond to RDF literals , attributes are RDF properties , and edges stand for RDF triples :
Data Graph . The data is a directed and labeled graph G = ( N , E ) . The set of nodes N is a disjoint union of entities NE and literals NL , ie N = NE NL . Edges E can be conceived as a disjoint union E = EE EL of edges representing connections between entities , ie a(ei , ej ) ∈ EE , iff ei , ej ∈ NE , and connections between entities and literals , ie a(ei , ej ) ∈ EL , iff ei ∈ NE and ej ∈ NL . Given this graph , we call the set of edges A(ei ) = {a(ei , ej ) ∈ E} the description of the entity ei ∈ NE , and each member a(ei , ej ) ∈ A(ei ) is called an attribute of ei . The set of distinct attribute labels of an entity ei , ie A(ei ) = {a|a(ei , ej ) ∈ A(ei)} , is called the model of ei .
It is clear that this notion of data graph is sufficiently general to capture not only RDF but also other types of Web data . For instance , data in a relational database can be mapped to this model by representing tuple ids as entity nodes , other tuple values are literal nodes that are connected to the corresponding ids that are in the same tuple , and foreign key relationships are captured as connections between entity nodes .
Data Heterogeneity . Web data reside in different datasets , each represented by a data graph . Typically , real world Web datasets exhibit heterogeneity at the schema and the data level . At the data level , entities in different datasets , which refer to the same real world object , may have different descriptions . Differences at the schema level occur when the same entity is represented in different datasets using attributes with different labels ( different models ) . As mentioned in the introduction , Figure 1 exemplifies this het a:Movie type Steven Spielberg a:Directors ea Amazon a : DBpedia db : IMdb i : Munich a:Title Daniel Craig , Eric Bana DVD a:Actors a:Binding 2005 a:ReleaseDate i:movie type Spielberg , Steven ( I ) i:directors ei ET ( 1994 ) i:title Coyote , Peter i:actors i:producer Spielberg , Steven ( I ) db:Film type db:director ed 1941 ( film ) rdfs:label db:starring db:John_Candy_(actor ) db:Steven_Spielberg WWW 2012 – Session : Semantic Web Approaches in SearchApril 16–20 , 2012 , Lyon , France142 erogeneity exhibited by real world datasets . Dealing with these types of heterogeneity requires data integration . For this , a large body of work on schema alignment and entity consolidation ( record linkage ) can be leveraged to compute mappings between data sources [ 8 ] . While mappings of varying semantics have been proposed , the most basic and commonly used one asserts that two elements ( schema elements or entities ) are the same ( ie same as mappings ) . 2.2 Research Problem
Given this model of Web data , structured queries can be specified to search over such datasets . The most commonly used language for querying RDF data on the Web is SPARQL [ 1 ] . One essential feature of SPARQL is the Basic Graph Pattern ( BGP ) . Basically , a BGP is a set of conjunctive triple patterns , each of the form predicate(subject , object ) . They represent patterns because either predicate , subject or object might be a variable , or is explicitly specified as a constant . Answering these queries amounts to the task of graph pattern matching , where subgraphs in the data graph matching the query pattern are returned as results . Predicates are matched against edges in the data graph , whereas bindings to subjects and objects in the query are entity or literal nodes .
One particular form of BGP with high importance are so called entity queries . Essentially , they are star shaped queries with the node in the center of the star representing the entity ( entities ) to be retrieved . Figure 6 provides 3 examples . According to a recent Web query logs study performed by Yahoo! researchers , queries searching for entities constitute the most common type on the Web [ 18 ] . Also , most of the current Semantic Web search engines such as Sig.ma 1 and Falcons [ 5 ] focus on answering these queries . For the sake of clarity , we also focus on this type of queries in this paper to illustrate the main ideas underlying our approach . Later , we will point out how our approach can be extended towards supporting general graph patterns . This however requires more complex algorithms for searching paths between entity nodes matching the query keywords ( ie matching the seed query represented as keywords ) .
Problem . Based on her knowledge about the schema and data of one particular source ( eg the one owned by the company in our scenario ) , it is possible for a programmer or expert user to specify complex entity queries that specifically ask for information from this source . It is however not trivial to exploit external datasets for this kind of entity search when they exhibit heterogeneity at the schema and data level as discussed before . The problem we tackle is finding relevant entities in a set of target datasets Gt given a source dataset Gs and an entity query qs adhering to the vocabulary of Gs . 2.3 Solutions
Clearly , if all datasets exhibit the same schema and data representation , then qs can directly be used to retrieve information from Gt . When this is not the case , the following different solutions can be applied .
Keyword Search ( KW ) . The first and most widespread solution to this end is to use keyword search over so called ‘bag of words’ representations of entities [ 20 , 5 ] . That is , the description of an entity is simply a bag of terms . A query is also represented as terms , which is then matched against the term based representation of the entities . Clearly , this
Figure 2 : KW : A structured query ( 1 ) transformed into a keyword query ( 2 ) and matched against bag of words representations of entities ( 3 ) .
Figure 3 : QR : A query for Amazon is rewritten into a query for DBpedia with constants being replaced with variables , and the missing mapping results in an “ empty ” triple pattern . approach is simple but also flexible in the sense that the same keyword query specified for Gs can also be used for Gt because results from Gt can be obtained when there are matches at the level of terms . As illustrated in Figure 2 , this approach ignores structure information and vocabulary mismatches .
Structured Query Rewriting ( QR ) . Another view on this retrieval problem is the database perspective . Here , structure information in the entity descriptions is taken into account . However , this also requires the query to be fully structured . The strategy to query over multiple datasets and to deal with data heterogeneity here is to rewrite the structured seed query qs into a query qt that adheres to the vocabulary of the target dataset Gt ∈ Gt . For this , same as mappings are computed using entity consolidation and schema mapping tools [ 8 , 9 , 13 , 4 ] . Then , predicates and constants in qs referring to attributes and entities in Gs are replaced with predicates and constants representing corresponding attributes and entities in Gt . While this strategy can exploit the fine grained structure of data and query , it relies on upfront data integration , which is problematic in the Web scenario because Web datasets are heterogeneous and evolve quickly . In our experiment on the datasets prepared for the Billion Triple Challenge2 for instance , we observe that state of the art entity consolidation approaches [ 4 ] do not scale well to large datasets [ 21 ] . In particular , they are focused on the single domain setting such that for these heterogeneous datasets ( where many of them exhibit only small pairwise overlaps at the schema level ) , only a relatively small amount of correct mappings could be produced . Thus , rewriting constants using entity mappings is especially challenging in this scenario .
In fact , it has been recognized that integration at the Web scale is too complex and resource intensive to be performed completely upfront [ 16 ] . A more practical strategy to deal with this dynamic and large scale environment is to perform integration as you go [ 16 ] , ie at usage time as the system evolves . In this regard , an alternative solution is to precompute schema mappings only . Then , entity mappings
1http://sig.ma/
2http://challengesemanticweborg/ a:Movie  type   “ Rainer  Werner  Fassbinder ”  a:Directors  ?x  Amazon  a :  1982  a:Theatrical  ReleaseDate  directors  rainer  werner  fassbinder  theatrical  release  date  1982  type  movie  e1  1982  IMdb  i :  Rainer  Werner  Fassbinder  Veronika  Voss  i:released                Itle  veronika  voss  director  rainer  werner  fassbinder  released  1982  i:Itle  i:director  e2  i:movie  Spielberg ,  Steven  (I )  Schindlers  Liste  (1994 )  type                    Itle  schindlers  liste  1994  director  spielberg  steven  type  movie  i:Itle  i:director  (3 )  (2 )  (1 )  e1  e2  db:director  ?x  DBpedia  db :  type  ?y  ?z  Amazon  a :  Dbpedia  db :  a:Directors  =  db:director  a:Title  =  db:name  A:Actor  =  db:starring  …  =  …  Schema    Alignment  Tool  Schema  Amazon  Schema  DBpedia  a:Movie  type   “ Rainer  Maria  Fassbinder ”  a:Directors  ?x  Amazon  a :  1982  a:Theatrical  ReleaseDate  WWW 2012 – Session : Semantic Web Approaches in SearchApril 16–20 , 2012 , Lyon , France143 that are needed for a specific query are obtained at runtime . Figure 3 illustrate this : Schema mappings are used to rewrite the query , triple patterns for which no corresponding schema level mappings exist are omitted , and constants are replaced with variables ( instead of being replaced with constants that adhere to the vocabulary of the target source ) . The resulting query captures only structure constraints of the original query and thus , produces possibly much more results than a query where constants are also rewritten . To achieve that , a standard IR search engine can be leveraged to limit the results to only those , which match the constants expressed as keyword queries . That is , the constants that have been replaced by variables in the first step , act as a keyword query in the second step to perform on the fly entity consolidation , ie to find entities in Gt , which match the entities in Gs as represented by the constants ( such as “ Rainer Maria Fassbinder 1982 ” in the example ) .
Our approach . In this paper , we present a framework to address this problem of querying heterogeneous Web data using on the fly mappings computed in a pay as you go fashion based on entity relevance models . This framework is instantiated involving the following four steps . ( 1 ) First , we compute an ERM from the results returned from the source dataset Gs using qs . ( 2 ) Second , we treat qs as keywords and using a standard IR based search engine , we obtain result candidates from the target datasets Gt . ( 3 ) Then , a lightweight on the fly integration technique is employed , which maps the structure of result candidates to the structure of the ERM . ( 4 ) Finally , the result candidates are ranked according to their similarity to the ERM using the mappings computed at runtime .
( 1 ) ERM as label starring director released language type ( 2 ) et at i:title i:actors i:directors i:producer type k(as )
1 0.5 1 1 0.5 1 w : Ps(w|as ) world:0.2 , on:0.2 , wires:0.2 , . . . klaus:0.25 , l¨owitsch:0.25 , barbara:025 , . . rainer:0.33 , werner:0.33 , fassbinder:0.33 1973:0.5 , 1982:0.5 german:1 film:1 w : Pt(w|at ) e:0.33 , t:0.33 , 1994:0.33 coyote:0.5 , peter:0.5 spielberg:0.33 , steven:0.33 , i:0.33 spielberg:0.33 , steven:0.33 , i:0.33 movie:1
Figure 5 : ( 1 ) ERM constructed from the entities e1 , e2 of Figure 4 . The ERM has a field for each attribute with label as . Each field is weighted with k(as ) and has a language model Ps(w|as ) defining the probability of w occurring in field as . ( 2 ) Representation of the entity ei of Figure 1 with language models for each attribute labeled at . a probability to every word w in the vocabulary . Considering the underlying statistical process that leads to the generation of query and document samples , the corresponding query and document language models can be reconstructed when the samples are large and representative . A Maximum Likelihood Estimator is often used for this . For example , given a document corpus C , the vocabulary of terms V , the language model P ( w|D ) representing document D ∈ C can be estimated as follows :
Figure 4 : Example set Rs of two entities e1 , e2 obtain for query qs n(w , D )
P ( w|D ) = λ
|D| + ( 1 − λ)P ( w|C )
( 1 ) where n(w , D ) is the count of word w in D , |D| is the document length , and P ( w|C ) is a background probability , which is used for smoothing controlled by the parameter λ . While a query Q may be too short as a sample , it has been shown that pseudo relevance feedback ( PRF ) results obtained for it can serve as a representative sample of the information need , from which a query model ( called relevance model ) can be reconstructed . Thus , instead of the query , a relevance model P ( w|Q ) is reconstructed of PRF results [ 14 ] . For ranking , a document D is considered relevant to a given query Q , if their probability distributions are close in “ distance ” . One way to achieve this is using the negative cross entropy −H :
X w∈V
3 . SEARCH OVER HETEROGENEOUS DATA
H(Q||D ) =
P ( w|Q ) log P ( w|D )
( 2 )
In this section , we present how the entity relevance model is constructed and discuss how this model can be exploited for ranking and relevance based on the fly data integration . 3.1 Entity Relevance Model
We aim at building a model that captures the structure and content of entities relevant to the information need , which is expressed in the seed entity query qs . The proposed model is called the Entity Relevance Model ( ERM ) . The ERM builds upon the concept of language model , a statistical modeling technique frequently applied in Information Retrieval tasks . We start with a brief overview of language models for IR ( see [ 17 ] for more details ) .
Language Model . The main idea here is to see documents and queries as samples from different probability distributions also called language models . More precisely , a language model is a multinomial distribution , which assigns
We adopt this modeling approach to the problem of searching structured Web data . Our goal is to model both the structure and the content of entities . The idea behind the ERM is to represent the attribute structure of entities by a set of language models , and each language model captures the content of the respective attribute . Hence , instead of using language models to represent entire documents , we use them for modeling attribute values . Entity Relevance Model . The ERM = ( Rs , As,Ps ) is a composite model consisting of a set of entities Rs ⊆ NE , a set of attributes As ⊆ E , and a set of language models Ps . Each Ps ∈ Ps is associated with a weight defined through the function k : Ps → [ 0 , 1 ] . The entities Rs are obtained by submitting the query qs against the source dataset Gs and used as pseudo relevance feedback . As denotes the set of all distinct attribute labels that are associated with the entities
Film  type  Rainer  Werner  Fassbinder  director  e1  1973  label  World  on  Wires  released  Klaus  Löwitsch  starring  type  director  e2  1982  Veronika  Voss  released  label  German  language  Barbara  ValenEn  starring  WWW 2012 – Session : Semantic Web Approaches in SearchApril 16–20 , 2012 , Lyon , France144 Rs , ie As = {a|a ∈ A(e ) , e ∈ Rs} . For each distinct attribute label as ∈ As , we compute a corresponding language model Ps(w|as ) ∈ Ps and its weight k(as ) . The language model Ps(w|as ) specifies the probability of any word w ∈ V occurring in the nodes of data graph edges with label as , where V is the vocabulary of all words . Let N ( ei , as ) be the set of nodes that is connected with ei through edges with label as , ie N ( ei , as ) = {ej|as(ei , ej ) ∈ E} , we compute Ps(w|as ) from all entity descriptions for ei ∈ Rs as follows :
P P ei∈Rs
P P ej∈N ( ei,as ) n(w , ej ) ej∈N ( ei,as ) |ej| ei∈Rs
( 3 )
Ps(w|as ) = where n(w , e ) denotes the count of word w in the node e and |e| is the length of e ( the number of words contained in e ) . The outer sum goes over the entities ei ∈ Rs and the inner sum goes over all values ej of attributes with labels as . Thus , entity descriptions , which do not have the attribute as , do not contribute to Ps(w|as ) . In order to capture the importance of these attribute specific language models , we compute k(as ) as the fraction of entities having an attribute with label as : k(as ) = n(as , Rs )
|Rs|
( 4 ) where the numerator denotes the number of entities having an attribute with label as and the denominator is the total number of entities in Rs . In summary , an ERM can be seen as a query specific model built from pseudo relevance feedback entities retrieved for the seed query qs . An example for an ERM constructed from two entities is illustrated in Figure 5 ( 1 ) .
3.2 Search Using ERM
We tackle the problem of searching over heterogeneous data in a way similar to entity consolidation . That is , given the results es ∈ Rs from the source dataset obtained for the seed query , we aim at finding entities in the target datasets which are similar to Rs . We use the ERM as the model of those relevant results . In particular , we estimate which entities et of Gt are relevant for the query qs by measuring their similarity to the ERM and rank them by decreasing similarity . We model a candidate entity et analogously to the ERM : et = ( At,Pt ) where At = A(et ) is the set of attributes of et and Pt is a set of language models . Similar to the ERM , Pt contains a language model Pt(w|at ) for each distinct attribute label at ∈ At . Let N ( at ) be the set of value nodes of the attribute at , ie N ( at ) = {ej|at(et , ej ) ∈ E} , Pt(w|at ) is estimated as follows :
Pt(w|at ) = ej∈N ( at ) n(w , ej ) ej∈N ( at ) |ej|
( 5 )
Here , the sum goes over all values e of attributes with label at , n(w , e ) denotes that number of occurrences of w in e , and |e| denotes the length of e . Figure 5 ( 2 ) illustrates an example .
We calculate the similarity between the ERM and a candidate entity et by measuring the “ distance ” between a language model of ERM and a language model of et using the ( negative ) cross entropy −H . We sum over these “ distances ” and weight each summand by k(as ) and the parameter β(as ) :
P P
Sim(ERM , et ) =
X as∈As
β(as ) · k(as ) · H(Ps(w|as)||Pt(w|at )
( 6 ) The parameter β gives us the flexibility to boost the importance of attributes that occur in the query qs as follows :
 1 b
β(as ) = if as /∈ qs if as ∈ qs , b ≥ 1
( 7 )
X w∈V
In particular , we apply this similarity calculation only when we know which attribute label as of ERM should be matched against which attribute at of et . We address this problem in the next section and show how the ERM can be exploited to create on the fly schema mappings , ie mappings between an attribute at and a field as of ERM . Equation 6 applies to corresponding pairs of attribute and field . If there is no mapping between as and at , then we use a “ maximum distance ” . This distance is computed as the cross entropy between Ps(w|as ) and a language model that contains all words in the vocabulary but the ones in Ps(w|as ) .
For constructing the language models of the ERM and of the candidate entities , a maximum likelihood estimation has been used , which is proportional to the count of the words in an attribute value . However , such an estimation assigns zero probabilities to those words not occurring in the attribute value . In order to address this issue , Pt(w|at ) is smoothed using a collection wide model cs(w ) , which captures the probability of w occurring in the entire dataset Gs . This smoothing is controlled by the Jelinek Mercer parameter λ . As a result , the negative cross entropy −H is calculated over the vocabulary V of field as as :
H(Ps||Pt ) =
Ps(w|as ) · log( λ · Pt(w|at ) + ( 1 − λ ) · cs(w ) )
( 8 )
3.3 On The Fly Integration Using ERM
We want to determine which attribute of an entity needs to be compared to a given field of the ERM constructed for qs . The ERM is not only used for search , but also exploited for this alignment task . The details for computing mappings between entity attributes at ∈ At and ERM fields as ∈ As are presented in Algorithm ( 1 ) . The rational of the algorithm is that a field as is aligned to an attribute at when the cross entropy H between their language models is low , ie a mapping is established , if H is lower than a threshold t ( normalized based on the highest cross entropy , line 12 ) . The algorithm iterates over n · r comparisons in worst case for an ERM with n fields and an entity with r = |A(et)| attribute labels . Note that n and r are relatively small ( see Table 1 and Table 3 ) because this algorithm operates only on entities that are requested as part of the search process compared to full fledge upfront integration that takes the entire schema into account . Further , ranking requires the same computation ( Equation 8 ) and thus the entropy values computed here are kept and subsequently reused for ranking . Moreover , for a faster performance , ERM fields having a weight of k(as ) < c can be pruned due to their negligible influence ( see Section 4.6 and 47 ) In addition , existing mappings can be reused to reduce the number of comparisons even further .
WWW 2012 – Session : Semantic Web Approaches in SearchApril 16–20 , 2012 , Lyon , France145 if at /∈ A.values then h ← H(Ps(w|as)||Pt(w|at ) ) candM appings.add(at , h ) candM appings := new OrderedByV alueM ap for all at ∈ A(et ) do
Algorithm 1 On the fly Alignment Input : ERM , Entity et , Threshold t ∈ [ 0 , 1 ] Output : M appings A := {(as , at)|as ∈ As , at ∈ At ∪ null} 1 : A := new M ap 2 : for all as ∈ As do 3 : 4 : 5 : 6 : 7 : 8 : 9 : 10 : 11 : 12 : 13 : 14 : 15 : 16 : 17 : end if 18 : end for 19 : return A end for bestA ← candM appings.f irstV alue worstA ← candM appings.lastV alue if bestA < t · worstA then at ← candM appings.f irstKey A.add(as , at ) else
A.add(as , null ) end if
// If not already aligned // see equation ( 8 )
// no mapping found
4 . EXPERIMENTS
In this section , we report on the experiments conducted with the three solutions discussed in Section 2 . We experimented with different parameter settings and observed that performance is stable when the employed parameters are in certain ranges ( will be discussed in Section 46 ) Results reported in the following are obtained using the configuration : b = 10 , c = 0.8 , t = 075 The smoothing parameter λ , whose effect on retrieval performance has been studied extensively for IR tasks , was set to 0.9 , a common value used in literature . We follow the Cranfield [ 6 ] methodology for the experiments on the search effectiveness and adopt the same methodology to analyze the effectiveness of mapping computation . 4.1 Datasets
Our experiments were conducted with 3 RDF Web datasets ,
DBpedia 351 , IMdb , and Amazon . In every experiment , one of them serves as the source dataset and the other two represent the target datasets . DBpedia is a structured representation of Wikipedia , which contains more than 9 million entities of various types , among them about 50k entities typed as films . The IMdb and Amazon datasets are retrieved from wwwimdbcom and wwwamazoncom [ 23 ] , and then transformed into RDF . The IMdb dataset contains information about movies and films , whereas the Amazon dataset contains product information about DVDs and VHS Videos . These three datasets are representative for our Web scenario because a vertical search application running one of these datasets ( eg the one owned by the company in our scenario ) could benefit from incorporating the other two into the search process . Further , the datasets exhibit the heterogeneity previously illustrated in Figure 1 . Table 1 gives details about each dataset .
Dataset
#Entities
Amazon IMdb DBpedia
115K 859K 9.1M
#Distinct Attribute
Labels
28 32
39.6K
|A(e)|
±StdDev . 184±38 114±64 9±18.2
Table 1 : Dataset statistics
4.2 Queries and Ground Truth
Our goal is to find relevant entities in the target datasets Gt for a given query qs . In this setting , we can determine the relevant entities in Gt by manually rewriting the query qs to obtain a structured query qt adhering to the vocabulary of Gt ∈ Gt . Figure 6 shows such a set of queries , one of the queries serves as the source query qs and the results of the other two queries capture the ground truth for the retrieval experiments .
We created three query sets , each containing 23 SPARQL BGP entity queries of different complexities , ranging from 2 to 4 triple patterns that produce a varying number of results , see Table 2 . The queries represent information needs like retrieve “ movies directed by Steven Spielberg ” , “ movies available in English and also in Hebrew ” , or “ movies directed by Rainer Werner Fassbinder , which were released in 1982 ” . The last query is illustrated in Figure 6 .
Rel . Entities
Amazon
IMdb DBpedia max avg . median min
153 32.2 18 1
834 114.9
21 1
47 10.9
5 1
Table 2 : Results per query and dataset .
Source Dataset
Amazon IMdb DBpedia
|ERM| ± StdDev .
141±36 158±67 23±5.4
Table 3 : Average number of fields of an ERM .
4.3 Systems
We implement the strategies as discussed previously in
Section 2 .
Keyword Query ( KW ) . IR style keyword search on Web data has been proposed [ 20 , 5 ] and implemented as an adoption of Lucene3 , an IR engine , which applies a document and query length adjusted TF/IDF based ranking function . We use the Semplore implementation [ 5 ] , which uses a virtual document for every entity description and use the concatenations of attribute labels and attribute values as document terms . In the same way , we transform the structured query into a keyword query by using the concatenations of predicates and constants of the structured query as terms . The resulting keyword query retrieves all virtual documents representing entity descriptions , which contain some of the corresponding terms .
Query Rewriting ( QR ) . This system is based on query rewriting using precomputed schema mappings . We created same as mappings with the tools Falcon AO [ 13 ] and Aroma [ 7 ] using their default configurations . Table 4 shows the number of mappings between the datasets . Then , to rewrite constants at runtime as discussed , we apply the KW baseline on top to limit the search results produced by the rewritten query to those that match constants formulated as a keyword query .
Datasets
Falcon AO[13 ] Aroma[7 ]
Amazon IMdb Amazon DBpedia IMdb DBpedia
8 11 4 Table 4 : Number of mappings .
5 11 12
Hybrid ( ERM ) . 3 different versions are employed : ( 1 ) ERM computes mappings on the fly . ( 2 ) ERMa relies entirely on the alignment computed upfront by Falcon AO . This version of ERM can be seen as a combination of our approach and query rewriting that mimics the QR baseline . The precomputed mappings are used
3http://luceneapacheorg
WWW 2012 – Session : Semantic Web Approaches in SearchApril 16–20 , 2012 , Lyon , France146 Figure 6 : Example of manually created queries that serve as ground truth . to obtained a rewritten query , which is processed to obtain results . However instead of using keyword search on top , we use the ERM and apply our approach for ranking .
( 3 ) ERMq combines these two approaches . It uses precomputed mappings and creates additional mappings on the fly for those attributes , which could not be mapped upfront .
4.4 Search Effectiveness
We use the standard IR measures precision , recall , mean average precision ( MAP ) and mean reciprocal rank ( MRR ) . We retrieve the top five thousand entities using the initial keyword search , rank them , and compute the metrics based on the top one thousand entities returned by each system . The results for six different retrieval settings are shown in Figure 7 :
First , we examine the scenario without prior data integration . Here , finding relevant entities in the target dataset is only possible with KW or ERM . When comparing their results ( Figure 7 ) , we observe that ERM outperforms KW across all metrics and retrieval settings and improves over KW by 120 % on average in terms of MAP . Looking at the different retrieval settings , we can see that ERM performs best between IMdb and Amazon ( ie when IMdb or Amazon are either source or target dataset ) , where MAP are 0.8 and 0.95 , respectively . The reason for this is that both datasets hold only entities from similar domains , movies and DVD/Videos , and describe them using similar attributes . DBpedia seems to be the most difficult one , mainly due to its schema complexity : It is very heterogeneous , containing information about different types of entities . Thus , whereas only one type have to be considered in the other datasets , identifying the relevant types out of a much larger set of possible candidates is also part of the retrieval problem here . Further , entities in DBpedia often exhibit redundant attributes with same values , eg name , title and rdfs:label , which leads to higher ambiguity during the computation of mappings . Across all retrieval settings , ERM yields MAP above 05 Also similarly good performance could be achieved for MRR and P@10 , which consider the top of the ranked results . The robustness of the retrieval performance of ERM can be observed in Figure 8 , which shows the interpolated precision across the recall levels . It can be observed that precision is fairly stable over different recall levels . One exception is the setting with IMdb as the target and DBpedia as the source dataset ( Figure 8(e) ) . Here , performance decreases notably at recall levels above 03 This is because there are some outlier queries , which have much more relevant entities than others , and the rank of some entities obtained for these queries were relatively very low . However , P@R , where R is the number of relevant entities , is still above 0.5 even for this setting ( Figure 7(c) ) . In the next scenario , we examine the performance in the presence of precomputed alignments . Now , applying QR to retrieve entities is possible . This system considerably outperforms KW . Using pre computed alignments with the hybrid approach , ERMa , yields slightly better performance than ERM on average ( see Figure 7 ) . Both , ERM and ERMa outperform QR on average by 54 % , respectively 59 % in terms of MAP . The performance of ERMa diverges from ERM most notably in two cases : ERMa is worse if IMdb and Amazon are involved . It is better in the retrieval setting with IMdb and DBpedia . This is because in the latter , the alignment problem that has to be solved as part of searching is more difficult due to the higher ambiguity and complexity introduced by DBpedia . Thus , applying the rewritten query using precomputed mappings to produce candidate results yields better performance . This effect can be observed in Figure 8(e ) . The strategy of combining the advantages of pre computed mappings and computing alignments on the fly implemented by ERMq outperforms the others across all metrics ( see Figure 7 ) . 4.5 On The Fly Mappings
We assessed the mappings computed on the fly during the previously discussed experiments . First , we collected all mappings and manually determined the ground truth based on the pooled mappings . Since we operate on heterogeneous datasets , multiple correct mappings for one attribute are possible , eg title in one dataset might correctly corresponds to title , name and label in another dataset . Given this ground truth , we computed precision and recall of the mappings created between the fields of an ERM and the attributes of an entity . Table 3 shows the average size of an ERM and Table 1 provides the average description size of an entity . Precision and Recall are here defined as follows :
P recision =
( 9 )
|{correct mappings}| |{created mappings}| |{correct mappings}|
Recall =
|{possible , correct mappings}|
( 10 ) where {possible , correct mappings} is the set of mappings , which could be established between the ERM and an entity as captured by the ground truth . We computed precision and recall for each individual entity considered during search , averaged over the query and finally over the entire query set . Overall , mappings obtained for 115k entities and the ERMs are taken into account . Figure 9(a ) shows precision and recall for the different retrieval settings . Averaging over all entities , precision is 0.46 and recall is 012 However , we are primarily interested in the entities , which are actually relevant . Therefore , we examine precision and recall only for these relevant entities . Here , the average over all scenarios is 0.70 for precision and 0.30 for recall , as shown in Figure 9(a ) . Figure 9(b ) gives the average number of actual mappings created between the ERM and entities , and be a:Movie  type   “ Rainer  Werner  Fassbinder ”  a:Directors  ?x  db:director  ?x  db:Rainer_Werner_Fassbinder  i:directors  ?x   “ Fassbinder ,  Rainer  Werner ”  Amazon  a :  DBpedia  db :  IMdb  i :  1982  a:Theatrical  ReleaseDate  db:Film  type  1982  db:released  i:movie  type  1982  i:year  WWW 2012 – Session : Semantic Web Approaches in SearchApril 16–20 , 2012 , Lyon , France147 ( a ) Mean Average Precision ( MAP )
( b ) Mean Reciprocal Rank ( MRR )
( c ) R Precision ( P@R )
( d ) Precision at 10 ( P@10 )
Figure 7 : Retrieval performance between source dataset ( S ) and target dataset ( T ) . tween the ERM and relevant entities . Clearly , better results can be achieved for relevant entities . This is important for our search task , which is focused on finding these relevant entities .
Intuitively , the search performance depends on the quality of the alignment . We verified this by computing the Pearson correlation coefficient ρ between the search performance of the different settings captured by MAP , as reported in Figure 7(a ) , and the alignment quality in terms of precision and recall for relevant entities , as reported in Figure 9(a ) . This yields ρ(MAP , Precision Rel ) = 0.98 and ρ(MAP , Recall Rel ) = 0.97 , indicating strong dependency between quality of the mappings and search performance . 4.6 Parameter Analysis
The hybrid approach relies on three parameters : b for boosting fields ( attributes ) in the seed query ( Equation 7 ) , the alignment threshold t and the threshold c for pruning fields of the ERM ( Section 33 ) We analyze the robustness of search effectiveness in terms of MAP for the six retrieval scenarios by varying one parameter while keeping the others fixed at the levels we used for the experiments . The results are shown in Figure 10 . We observed that boosting helps to improve the performance when dealing with similar datasets ( ie Amazon and IMDB ) but has a negative effect when a different and diverse dataset like DBpedia is involved . However , performance is rather insensitive to this parameter when b > 10 ( thus we chose b = 10 ) . Regarding the alignment threshold t , we observed that performance is fairly stable when t is within the range [ 0.2 , 08 ] Pruning fields has almost no effect on effectiveness . 4.7 Runtime Performance
To analyze the performance of the hybrid approach , we measured query execution time for ERM across all six re trieval scenarios , ie for a total of 138 queries . Figure 11(a ) shows the min , max and average time in seconds for each retrieval scenario . The times reported cover all steps of the retrieval process , ie executing qs to obtain results for the source dataset , computing ERM , retrieving results for target datasets , computing models for each candidate entity , establishing mappings and ranking . Such a retrieval process takes less than 13 secs on average for the above configuration . The performance can be improved by increasing the pruning parameter c as shown in Figure 11(b ) , which shows the min , max , and average query execution time over all six scenarios for different values of c . For these runtime experiments , we use a standard laptop with Intel Core 2 Duo 2.4 GHz CPU , 4 GB RAM , Serial ATA HDD@5400rpm , MacOS 10.6 , and implemented our approach using Java 6 and Lucene 3.0 for indexing and retrieval . Computing the language models from the term frequency vectors was performed at runtime . These tasks can also be performed at indexing time . Still , these preliminary results suggest that the hybrid approach is promising , given that not only search results but also on the fly mappings are obtained during the process .
( a ) Runtime performance ERM
( b ) Prunning c
Figure 11 : Runtime performance analysis
0 0.2 0.4 0.6 0.8 1 S=Amazon T=IMDB S=Amazon T=DBpedia S=IMDB T=Amazon S=IMDB T=DBpedia S=DBpedia T=IMDB S=DBpedia T=Amazon Average MAP KW  QR  ERM  ERMa  ERMq  0 0.2 0.4 0.6 0.8 1 S=Amazon T=IMDB S=Amazon T=DBpedia S=IMDB T=Amazon S=IMDB T=DBpedia S=DBpedia T=IMDB S=DBpedia T=Amazon Average MRR KW  QR  ERM  ERMa  ERMq  0 0.2 0.4 0.6 0.8 1 S=Amazon T=IMDB S=Amazon T=DBpedia S=IMDB T=Amazon S=IMDB T=DBpedia S=DBpedia T=IMDB S=DBpedia T=Amazon Average P@R KW  QR  ERM  ERMa  ERMq  0 0.2 0.4 0.6 0.8 1 S=Amazon T=IMDB S=Amazon T=DBpedia S=IMDB T=Amazon S=IMDB T=DBpedia S=DBpedia T=IMDB S=DBpedia T=Amazon Average P@10 KW  QR  ERM  ERMa  ERMq  0  5  10  15  20  25    S=Amazon  T=Dbpedia    Amazon  IMdb  IMdb  Dbpedia  IMdb  Amazon  DBpedia  Amazon  DBpedia  IMdb  secs  0  5  10  15  20  25  0.1  0.4  0.7  1  secs  c  max  avg  min  WWW 2012 – Session : Semantic Web Approaches in SearchApril 16–20 , 2012 , Lyon , France148 ( a ) S=Amazon T=IMdb
( b ) S=Amazon T=DBpedia
( c ) S=IMdb T=Amazon
( d ) S=IMdb T=DBpedia
( e ) S=DBpedia T=IMdb
( f ) S=DBpedia T=Amazon
Figure 8 : Precision recall curves for source dataset ( S ) and target dataset ( T ) .
( a ) Precision and recall of the mappings created on the fly between ERM and entities , respectively relevant entities ( Rel ) .
( b ) Average number of mappings created on the fly between ERM and entities , respectively relevant entities ( Rel ) .
Figure 9 : Evaluation of the mappings created on the fly .
5 . RELATED WORK
We have discussed related work throughout the paper . Basically , there are two existing lines of approaches , one that is based on keyword search [ 2 , 5 , 20 ] and the other one is structured query rewriting [ 3 , 23 , 10 ] . The latter type of approaches uses precomputed mappings , finds duplicates [ 23 ] or uses precomputed relaxations of the query constraints [ 10 ] to bridge differences in syntactical representation . The keyword search approaches rely on matches on the level of terms . Beside the pure ‘bag of word’ approaches [ 5 , 20 ] , a recent study showed that using a minimal structure by classifying attributes into important and unimportant fields improves keyword search for entities [ 2 ] .
Our approach represents a novel combination which combines the flexibility of keyword search with the power of structured querying . Just like keyword search , it does not rely on precomputed mappings . However , it is able to exploit the fine grained structure of query and results , which is the primary advantage of structured query rewriting . In addition , it can leverage existing mappings created by alignment tools like [ 13 , 7 ] . We presented the general idea of our approach and preliminary results in [ 12 ] .
Our work leverages several ideas that are have been pro posed for IR tasks . In fact , the model underlying our approach originates from the concept of language models [ 17 ] , which have been proposed for modeling resources and queries as multinomial distributions over the vocabulary terms , and for ranking based on the distance between the two models , eg using KL divergence [ 22 ] or cross entropy [ 14 ] as measures . More precisely , the foundation of our work is established by Lavrenko et al.[14 ] , who propose relevance based language models to directly capture the relevance behind document and queries . Also , structure information has been exploited for constructing structured relevance models [ 15 ] ( SRM ) . This is the one mostly related to ERM . The difference is that while the goal of SRM is to predict values of empty fields in a single dataset scenario , ERM targets searching in a completely different setting involving multiple heterogeneous datasets . Thus , we build on well studied concepts and investigate them in a scenario different from the traditional IR settings . Instead of searching documents using keyword queries , we show how to use structured language models to process structured queries against structured data residing in external Web datasets . In this scenario , we also need to take structure mismatches ( ie differences at the schema level ) into account and thus , propose on the fly integration to deal with this problem .
000204060810000204060810PrecisionRecallERMERMaERMqKWQR000204060810000204060810PrecisionRecallERMERMaERMqKWQR000204060810000204060810PrecisionRecallERMERMaERMqKWQR000204060810000204060810PrecisionRecallERMERMaERMqKWQR000204060810000204060810PrecisionRecallERMERMaERMqKWQR000204060810000204060810PrecisionRecallERMERMaERMqKWQR0 0.2 0.4 0.6 0.8 1 S=Amazon T=IMDB S=Amazon T=DBpedia S=IMDB T=Amazon S=IMDB T=DBpedia S=DBpedia T=IMDB S=DBpedia T=Amazon Average Recall  Recall ­‐Rel  Precision  Precision ­‐Rel  0 1 2 3 4 5 S=Amazon T=IMDB S=Amazon T=DBpedia S=IMDB T=Amazon S=IMDB T=DBpedia S=DBpedia T=IMDB S=DBpedia T=Amazon Average Avg .  Align./En+ty  Avg .  Align./  Rel  En+ty  WWW 2012 – Session : Semantic Web Approaches in SearchApril 16–20 , 2012 , Lyon , France149 ( a ) Field boosting b
( b ) Alignment threshold t
( c ) ERM pruning c
Figure 10 : Parameter sensitivity analysis . The legend ‘A2D’ stands for source dataset=Amazon ( A ) , target dataset=DBpedia ( D ) , ‘I2A’ stands for source=IMdb ( I ) , target=Amazon , etc . 8 . REFERENCES
The proposed technique is in principle similar to existing work on schema matching , eg [ 9 ] , to the extent that it relies on the same features , ie values of attributes . However , the use of language models for representing these features as well as the similarity calculation based on entropy is common for retrieval tasks , but we have not seen them applied to the schema mapping problem before . We consider this as a promising approach for embedding the pay as you go data integration paradigm [ 16 ] into the search process .
6 . CONCLUSION
We have proposed a novel approach for searching heterogeneous Web datasets using one single structured seed query that adheres to the vocabulary of just one of the datasets . We have introduced the entity relevance model which captures the structure and content of relevant results obtained for a seed query . The entity relevance model is used for matching and ranking results from external datasets , as well as for performing data integration on the fly . Our approach combines the flexibility of keyword search in the sense that no upfront integration is required , with the power of structured querying that comes from the use of the fine grained structure of query and results . Extensive experiments conducted with real world datasets show the effectiveness and feasibility of our approach .
Using our approach allows to take advantage of the structured data available numerously as Linked Data on the Web by incorporating these datasources into existing vertical search capabilities .
As future work , we will extend this approach to allow for more general queries representing graph patterns . The main idea will remain the same : building a relevance model from the seed query , querying external structured data using the seed query as keywords , and finally , mapping the structure of the results to the relevance model to rank them . However , results here are more complex , involving entities of different types that are possibly connected over long paths . We will employ existing techniques for keyword search on structured data ( eg [ 19 ] ) to retrieve these results , ie , subgraphs in the data , which connect entities matching the query keywords . Also , we will extend our ideas for on the fly mapping and ranking to deal with the more complex structure of the queries and results .
7 . ACKNOWLEDGEMENTS
We thank our colleagues Philipp Sorg and G¨unter Ladwig for helpful discussions . Also , we thank Julien Gaugaz and the L3S Research Center for providing us their versions of the IMdb and Amazon datasets . This work was supported by the German Federal Ministry of Education and Research ( BMBF ) under the iGreen project ( grant 01IA08005K ) .
[ 1 ] W3C Recommendation 15 January 2008 ,
SPARQL Query Language for RDF . http://wwww3org/TR/rdf sparql query/
[ 2 ] R . Blanco , P . Mika , and S . Vigna . Effective and Efficient Entity Search in RDF data . In ISWC , pages 83–97 , 2011 .
[ 3 ] A . Cal`ı , D . Lembo , and R . Rosati . Query rewriting and answering under constraints in data integration systems . In IJCAI , pages 16–21 , 2003 .
[ 4 ] S . Chaudhuri , B C Chen , V . Ganti , and R . Kaushik .
Example driven Design of Efficient Record Matching Queries . In VLDB , pages 327–338 , 2007 .
[ 5 ] G . Cheng and Y . Qu . Searching Linked Objects with Falcons : Approach , Implementation and Evaluation . Int . J . Semantic Web Inf . Syst . , 5(3):49–70 , 2009 .
[ 6 ] C . Cleverdon . The CRANFIELD Tests on Index Langauge
Devices . Aslib , 1967 .
[ 7 ] J . David . AROMA results for OAEI 2009 . In Ontology
Matching Workshop , ISWC , 2009 .
[ 8 ] A . Doan and A . Y . Halevy . Semantic Integration Research in the Database Community : A Brief Survey . AI Magazine , 26(1):83–94 , 2005 .
[ 9 ] S . Duan , A . Fokoue , and K . Srinivas . One size does not fit all :
Customizing Ontology Alignment Using User Feedback . In ISWC , pages 177–192 , 2010 .
[ 10 ] S . Elbassuoni , M . Ramanath , and G . Weikum . Query
Relaxation for Entity Relationship Search . In ESWC , pages 62–76 , 2011 .
[ 11 ] T . Heath and C . Bizer . Linked Data : Evolving the Web into a
Global Data Space . Morgan & Claypool , 1st edition , 2011 . [ 12 ] D . Herzig and T . Tran . One Query To Bind Them All . In
COLD2011 , CEUR Workshop Proceedings , Vol . 782 , 2011 .
[ 13 ] W . Hu and Y . Qu . Falcon AO : A practical ontology matching system . J . Web Sem . , 6(3):237–239 , 2008 .
[ 14 ] V . Lavrenko and W . B . Croft . Relevance based language models . In SIGIR , pages 120–127 , 2001 .
[ 15 ] V . Lavrenko , X . Yi , and J . Allan . Information retrieval on empty fields . In HLT NAACL , pages 89–96 , 2007 .
[ 16 ] J . Madhavan , S . Cohen , X . L . Dong , A . Y . Halevy , S . R .
Jeffery , D . Ko , and C . Yu . Web scale data integration : You can afford to pay as you go . In CIDR , pages 342–350 , 2007 .
[ 17 ] J . M . Ponte and W . B . Croft . A Language Modeling Approach to Information Retrieval . In SIGIR , pages 275–281 , 1998 .
[ 18 ] J . Pound , P . Mika , and H . Zaragoza . Ad hoc Object Retrieval in the Web of Data . In WWW , pages 771–780 , 2010 .
[ 19 ] T . Tran , H . Wang , S . Rudolph , and P . Cimiano . Top k
Exploration of Query Candidates for Efficient Keyword Search on Graph Shaped ( RDF ) Data . In ICDE , pages 405–416 , 2009 .
[ 20 ] H . Wang , Q . Liu , T . Penin , L . Fu , L . Zhang , T . Tran , Y . Yu , and Y . Pan . Semplore : A scalable IR approach to search the Web of Data . J . Web Sem . , 7(3):177–188 , 2009 .
[ 21 ] H . Wang , T . Tran , P . Haase , T . Penin , Q . Liu1 , L . Fu , , and
Y . Yu . SearchWebDB : Searching the Billion Triples! In Semantic Web Challenge , ISWC , 2008 . http://wwwcsvunl/~pmika/swc 2008/SearchWebDB paperpdf
[ 22 ] C . Zhai and J . D . Lafferty . A Risk Minimization Framework for Information Retrieval . Inf . Process . Manage . , 42(1):31–55 , 2006 .
[ 23 ] X . Zhou , J . Gaugaz , W T Balke , and W . Nejdl . Query
Relaxation Using Malleable Schemas . In SIGMOD Conference , pages 545–556 , 2007 .
0 0.2 0.4 0.6 0.8 1 1 10 100 1000 MAP b A2D A2I D2A D2I I2A I2D 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 MAP t A2D A2I D2A D2I I2A I2D 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 MAP c A2D A2I D2A D2I I2A I2D WWW 2012 – Session : Semantic Web Approaches in SearchApril 16–20 , 2012 , Lyon , France150
