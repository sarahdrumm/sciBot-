Good Abandonments in Factoid Queries
Aleksandr Chuklin
Yandex & Moscow Institute of Physics and
Technology
Moscow , Russia chuklin@yandex team.ru
Pavel Serdyukov
Yandex
Moscow , Russia pavser@yandex team.ru
ABSTRACT It is often considered that high abandonment rate corresponds to poor IR system performance . However several studies suggested that there are so called good abandonments , ie situations when search engine result page ( SERP ) contains enough details to satisfy the user information need without necessity to click on search results . In those papers only editorial metrics of SERP were used , and one cannot be sure that situations marked as good abandonments by assessors actually imply user satisfaction . In present work we propose some real world evidences for good abandonments by calculating correlation between editorial and click metrics .
Categories and Subject Descriptors H33 [ Information Storage and Retrieval ] : Information Search and Retrieval
General Terms Human Factors , Verification
Keywords good abandonments , factoid queries
1 .
INTRODUCTION
Abandonment rate is one of the most broadly used online user satisfaction metrics to evaluate the quality of information retrieval ( IR ) system . It is often considered that high abandonment rate corresponds to poor IR system performance . It was also shown that conventional editorial document relevance metrics show remarkable negative correlation with abandonment rate ( see [ 2])1 . However in [ 5 ] it was suggested that some abandonments might be good : user decided not to click on search results because her information need were satisfied with SERP itself . Such situations were further studied in [ 4 ] where cursor movements were proposed to measure user satisfaction when no clicks had been issued . In [ 3 ] it was found that presence of specially decorated search results ( Answers ) may lead to higher abandonment rate for several query types .
1They showed positive correlation with UCTR which is the opposite of abandonment .
Copyright is held by the author/owner(s ) . WWW 2012 Companion , April 16–20 , 2012 , Lyon , France . ACM 978 1 4503 1230 1/12/04 .
In all previous works related to good abandonments authors usually perform human assessments of query SERP pairs by answering the following question : ” Does the search engine result page p contain an answer to the query q ? ” . However , it is not completely evident that an average user will notice such an answer and will decide to abandon her habit to click on search results . Current work is the first to measure correlation of snippet editorial metrics with user clicks . Here we focused on factoid queries : short questions that can be answered in search result snippet ( annotation ) . Our idea was to gather judgements about snippets quality , build various editorial metrics that represents SERP quality and then calculate correlation with click metrics .
2 . METHOD
We started with extracting large amount of data from Yandex daily query log . Then we extracted queries with Encyclopaedic intent , namely queries of the form ” X definition ” , ” what is X ” , ” meaning of X ” , etc . After that we asked a group of assessors to judge each query snippet ( short annotation shown to users by Yandex ) with respect to the query :
• Snippet was marked as ” Yes ” if it represents a full and easy to read answer to the question .
• Snippet was marked as ” No ” if it does not contain an answer to the user ’s query ( even if an acceptable answer can be found after following the URL ) .
• Snippet was marked as ” Partial ” if it contains only partial answer to the query . Eg the question was to define some ambiguous term and the snippet explains only one of its possible meanings . Another example : answer is correct , but not detailed enough or is not easy to read . After performing some filtering we had 8830 judged snippets and correspondingly 883 queries . In order to calculate whole SERP relevance we computed several editorial metrics frequently used to measure IR system performance2 :
Precision at N . We calculated P @10 and P @5 by converting editorial grades to binary ( only ” yes ” snippets were treated as relevant ) .
Average Precision . Again , only ” yes ” snippets were treated as relevant . r=1 γr−1R(gr ) Qr−1
ERR . We used ERR with abandonment probability ( [2 , Section 7.2] ) : P10 i=1 ( 1 − R(gi) ) . We converted relevance grades to probabilities using the mapping : R(g ) = 2g 2gmax , as in [ 2 ] . In our case g ∈ {0 , 1 , 2} — editorial grade , gmax = 2 . In our setup we used γ = 0.9 as was suggested in [ 1 ] .
−1
2we replaced document relevance by snippet relevance
WWW 2012 – Poster PresentationApril 16–20 , 2012 , Lyon , France483 CTR Abandonments ( whole dataset ) Abandonments ( best 40 % )
Figure 1 : CTR
Table 1 : Correlations
P@10 0.190 0.154 0.010 +0.023 +0.150 +0.074 +0.006 +0.114
CG@10 CG@5 DCG ERR 0.186 0.182 0.195 +0.022 +0.012 0.021 +0.084 +0.084 +0.078
AP 0.111 0.041
P@5 0.142 0.003
ERR DCG on Figures 1 and 2 . From these plots we can make several conclusions . First , we can see that with better snippets’ quality overall page CTR decreases , ie better SERPs need less clicks . Second , we conclude that users tend to abandon more on SERPs containing very informative snippets .
In order to verify these hypothesis we calculated correlation between editorial and click metrics using the following formula :
Corr(x , y ) = i=1(xi − µx)(yi − µy )
Pn i=1(xi − µx)2pPn pPn Pn i=1 xi n
Pn i=1 yi n
, µy =
µx = i=1(yi − µy)2 where x and y are vectors , i th vector component is the value of some editorial or click metric for the query qi , µz is the average value of z . For CTR we used the whole dataset and managed to find remarkable negative correlation . For abandonment rate we compared above whole datased approach with another one : we leave only well performing queries , ie queries with large value of particular editorial metric ( best 40 % the queries ) . Results are summarized in Table 1 .
3 . DISCUSSION
In this work we focused on factoid queries and studied correlations of two commonly used online user satisfaction metrics with editorial metrics representing SERP quality ( in terms of presence of the answer ) . We found that correlation with SERP CTR is negative . We also found that correlation with abandonment rate is positive for well performing queries while negligible for the whole dataset . These results might be considered as a justification of the existence of real good abandonments in the real Web .
As a next step we are going to build a machine learned algorithm that approximates some editorial metric and see how it can be applied to predict good user abandonments in real search sessions .
4 . REFERENCES [ 1 ] O . Chapelle . A dynamic bayesian network click model for web search ranking . WWW’09 .
[ 2 ] O . Chapelle , D . Metlzer , Y . Zhang , and P . Grinspan . Expected reciprocal rank for graded relevance . CIKM ’09 .
[ 3 ] L . Chilton and J . Teevan . Addressing people ’s information needs directly in a web search result page . In WWW’11 .
[ 4 ] J . Huang , R . R . W . White , and S . Dumais . No clicks , no problem : using cursor movements to understand and improve search . CHI’11 .
[ 5 ] J . Li , S . Huffman , and A . Tokuda . Good abandonment in mobile and PC internet search . SIGIR’09 .
1.00
0.95
0.90
R T C
0.85
0.80
0.75
0.70 1 e t a r t n e m n o d n a b a
0.57
0.56
0.55
0.54
0.53
0.52 1
2
3 bin number
4
5
Figure 2 : Abandonment rate
ERR DCG 2
3 bin number
4
5
Cumulative Gain . Non discounted metric :
CG@N = PN i=1 R(gi ) . i=1
2gi −1 log ( i+1 ) .
DCG . Classic DCG model : P10 When having quality measures for SERP we compared them with click metrics3 . The hypothesis was that users tend to click less when SERP is sufficiently good . First , we performed the following procedure : for each editorial metric we sorted queries according to this metric ( in ascending order ) and split our data set into M bins of equal size ( each bin contains 1/M of all instances ) . For that experiment we used only 2 editorial metrics : ERR , DCG . Other metrics were too discrete and hard to split to equal bins . We examined two click metrics , namely abandonment rate and total clickthrough rate ( CTR ) ( total number of SERP clicks divided by the number of search requests ) . Results are presented
3Clicks were gathered from Yandex query log for the period of 3 months . In total , we have 137010 search requests used for our analysis .
WWW 2012 – Poster PresentationApril 16–20 , 2012 , Lyon , France484
