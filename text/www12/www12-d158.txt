Recommendations to Boost Content Spread in Social
Networks
Vineet Chaoji#1
Sayan Ranu†2
Rajeev Rastogi#3
Rushi Bhatt#4
†Dept . of Computer Science UCSB , Santa Barbara , USA
#Yahoo! Labs Bangalore , India
2sayan@csucsbedu ,
{1chaojv ,
3rrastogi ,
4rushi}@yahoo inc.com
ABSTRACT Content sharing in social networks is a powerful mechanism for discovering content on the Internet . The degree to which content is disseminated within the network depends on the connectivity relationships among network nodes . Existing schemes for recommending connections in social networks are based on the number of common neighbors , similarity of user profiles , etc . However , such similarity based connections do not consider the amount of content discovered .
In this paper , we propose novel algorithms for recommending connections that boost content propagation in a social network without compromising on the relevance of the recommendations . Unlike existing work on influence propagation , in our environment , we are looking for edges instead of nodes , with a bound on the number of incident edges per node . We show that the content spread function is not submodular , and develop approximation algorithms for computing a near optimal set of edges . Through experiments on real world social graphs such as Flickr and Twitter , we show that our approximation algorithms achieve content spreads that are as much as 90 times higher compared to existing heuristics for recommending connections .
Categories and Subject Descriptors H34 [ Information Storage and Retrieval ] : Systems and Software—information networks
General Terms Algorithms,Experimentation
Keywords content spread , recommendation , social networks
1 .
INTRODUCTION
Social networks are increasingly becoming a powerful medium for disseminating and discovering useful content . In popular social networking sites like Google+ and Twitter , users share activity updates with their neighbors or followers . The updates typically include recently uploaded photos , comments on photos and news articles , reviews and ratings that the user has assigned to a movie or restaurant , or simply an article or game on the web that the user has liked . Each neighbor recursively shares received updates within its own
Copyright is held by the International World Wide Web Conference Committee ( IW3C2 ) . Distribution of these papers is limited to classroom use , and personal use by others . WWW 2012 , April 16–20 , 2012 , Lyon , France . ACM 978 1 4503 1229 5/12/04 . neighborhood , and content generated by a user propagates through the network to a wide user population . Thus , social networks enable users to share content at an unprecedented scale , and discover new content of interest to them .
On friendship networks such as Facebook , the content spread is confined since connections are typically made to a close group of friends1 . On the contrary , content centric networks such as Twitter and Google+ promote content spread by allowing users to connect with people having common interests , who are most likely not their friends . The extent to which a social network spreads content is a key metric that impacts both user engagement and network revenues . The more content spreads , the more novel content users end up discovering , and the more value users derive from being part of the social network . The effective dissemination of generated content also helps users build their “ online social reputation ” . For instance , on microblogging sites such as Twitter , the number of active followers is indicative of a user ’s online reputation [ 22 ] . Building an active following is contingent on the content reaching the right set of interested users on Twitter .
From the social network ’s perspective , higher content spread helps drive up user engagement which in turn leads to improved user retention and audience growth . Furthermore , as users spend more time accessing diverse content in the form of photos , news articles , games etc . , there are increased opportunities for monetizing the content via online ads , sale of virtual goods , subscriptions , and so on . As a result of the above benefits , it is crucial for social networks to maximize the dissemination of interesting content across the entire social graph .
One way to boost content spread in a social network is by increasing the connectivity among users . Social networking sites like Twitter and Google+ already offer “ people recommendations ” to users to increase connectivity . These people recommender implementations , however , focus primarily on making relevant recommendations without an explicit effort towards increasing content availability . For instance , the “ People You May Know ” feature employs the Friend ofFriend ( FoF ) algorithm [ 1 ] that recommends users based on the number of common friends with the user receiving the recommendation . Other recommender algorithms , for eg , in Twitter [ 2 , 12 ] , suggest users whose profiles , interests , or updates have substantial overlap with the receiver of the recommendation . In a typical social network , the number of relevant recommendations that qualify based on criteria
1The recently introduced Subscribe Button now allows content to spread beyond friends on Facebook .
WWW 2012 – Session : Information Diffusion in Social NetworksApril 16–20 , 2012 , Lyon , France529 B
A
C
D
E
G
F
Figure 1 : Example illustrating that connecting users with the maximum mutual friends does not maximize content availability . such as FoF or interest similarity can be significantly large . However , different subsets of these relevant recommendations may have very diverse content spread characteristics . Consequently , simply recommending connections based on the number of mutual friends or similarity between profiles and posted content may not maximize content spread in the social network . This is illustrated in the following example .
Example 1 . Consider the simple social network in Figure 1 with users A , . . . , G . Suppose that user G generates a piece of content c , and each user ( except for E which does not share content ) shares content with its neighbors with a probability of 1 2 . Then , users D and F each receive c with probability 1 2 , E receives c through D or F with probability ( 1 − ( 1 − 1 2 · 1 2 ) · ( 1 − 1 16 , C receives with probability 1 4 , and A and B receive with probability 1
2 ) ) = 7
8 each .
2 · 1
Let us assume that the set of two hop neighbors form the set of relevant recommendations for a user . Now , suppose that we are looking to recommend a user to G to connect with . The two relevant candidates for G are C and E . If we use the FoF heuristic , then we would end up recommending user E since it has the maximum number of mutual friends with G . With the additional connection ( G , E ) , it can be shown that content c reaches from G to E with probability ≈ 3 2 along edge ( G , E ) and with probability 1 4 along the two paths passing through D and F ; thus , the probability that c reaches E along one of the paths is 1 − 1 4 ) . Furthermore , there is no change in the probability with which c reaches users C , D and F . Thus , with edge ( G , E ) , the only change in content spread is that c reaches E with 3
4 ( c reaches E with probability 1
16 more probability .
16 = 5
4 − 7
4 ≈ 3
2 · 3
4 · 3
On the other hand , if G connects with C instead , then c reaches C with probability 5 8 ( c reaches C with probability 2 along edge ( G , C ) and with probability 1 1 4 along the path through D ) . Furthermore , c reaches users A and B with probability 5 16 . Thus , with edge ( G , C ) , c reaches C with 8 more probability , and users B and A each with 3 3 16 more probability .
Consequently , even though both E and C are two hops from G , and E has the maximum number of common friends with G , connecting G with C results in content spreading to users with 3 16 as a result of connecting G with E .
16 ) higher probability compared to 5
4 ( = 3
16 + 3
8 + 3
✷
In this paper , we consider the problem of recommending connections that maximize content spread in a social network while ensuring that the recommendations are relevant . For each user u , our approach first identifies a candidate set Nu of similar users based on the number of common neighbors , proximity in the social graph , similarity of profiles and posted content , etc . It then selects up to k users Ru from each user ’s candidate set Nu such that if every u connects with users in Ru then the content spread in the network is maximized . The k users in Ru are recommended for connection to user u .
Observe that by recommending a subset of Nu to user u , we ensure that the connections recommended to each user are relevant . In typical social networks , the set Nu can be very large ( in the order of hundreds or thousands ) . By using the content spread objective to select the subset Ru , our recommendation scheme essentially balances the needs of both users ( by recommending relevant users ) and the social network provider ( by boosting content spread in the network ) . In addition , the constraint k on the maximum number of new connections per user prevents deluging active or highly connected users with an unreasonable number of recommendations . Presenting each user with a bounded number k of relevant connections ensures a good user experience . For the same reason , social networking sites such as Twitter typically limit the number of recommendations to about 10 .
Our work differs from prior research on influence maximization in social networks [ 14 ] . The objective in [ 14 ] is to select the top k influential nodes in the network that can be targeted to maximize influence spread in the network . The authors show that even though influence maximization is NP hard , the influence spread function on nodes is submodular , and thus a greedy strategy yields influence spreads that are within ( 1− 1 e ) of the optimum . In contrast , our content spread maximization problem looks to add up to k new connections per node so that content spread is maximized . As edges are added to the social network , its structure itself changes , and so the content spread function on edges is no longer submodular – this precludes simple greedy solutions . Furthermore , in our problem setting , there are complex constraints requiring that the number of new edges incident on any node of the social graph is at most k . Thus , we have millions of local constraints on selected edges as opposed to a single global constraint in [ 14 ] on the number of selected nodes . The lack of submodularity coupled with complex local constraints make our content spread maximization problem a lot more challenging .
To summarize , our main contributions are as follows :
• We formally define the content maximization problem that seeks to add up to k connections per user such that the ( probabilistic ) propagation of content in the social network is maximized . To the best of our knowledge , the people recommendation problem with the explicit goal of maximizing content availability in the social network is new .
• We show that our content maximization problem is NPhard . Moreover , our content spread function lacks desirable properties like submodularity that allow for efficient approximation algorithms . We propose a restricted variant that is submodular and closely approximates our original content spread function .
• For our restricted content spread function , we devise an approximation algorithm that computes an edge set satisfying constraints and whose content spread is provably close to the optimum .
• We conduct an extensive experimental study with reallife social networks from Twitter , Flickr , etc . In our experiments , the connections recommended by our algorithm achieve content spreads that are as much as 90 times higher compared to the FoF heuristic and 4 times more than simple greedy strategies .
WWW 2012 – Session : Information Diffusion in Social NetworksApril 16–20 , 2012 , Lyon , France530 2 . THE CONTENT MAXIMIZATION PROB
LEM
We model the social network as an undirected graph G = ( V , E ) where nodes represent users and edges are the connections between them . Furthermore , we denote the pieces of content ( eg , photos , comments , articles ) that nodes share with their neighbors over a fixed time period ( eg , a month ) by C . Each node i in the graph has the following three parameters : ( 1 ) pi , the probability with which node i shares content independently with each of its neighbors , ( 2 ) ci ⊆ C , the content generated or discovered by node i , and ( 3 ) Ni , the set of nodes in G that is compatible with node i . The parameter pi can be empirically estimated by observing the fraction of content that a node shares with its neighbors . Also , Ni = {j : sim(i , j ) > α ∧ j ∈ V } . Here , sim(i , j ) is the similarity between nodes i and j computed based on the number of hops between the nodes , the number of common neighbors , node profiles ( eg , preferences , educational background ) , and posted content . The user defined parameter α ensures that nodes in Ni are fairly similar to i , and are thus relevant candidates for recommendation to i .
Observe that the parameters ci and pi determine the flow of content through the network . We define the content spread within the network as : Pc Expected number of nodes with content c . Our objective in this paper is to compute a set of relevant recommendations X such that the content spread is maximized . Each recommendation in X is a node pair ( i , j ) , and indicates that node i is recommended to j , and vice versa . Now , suppose that PX ( i , c ) is the probability of content c reaching node i over the edge set E ∪ X . Then , the expected number of nodes with content c is given by Pi PX ( i , c ) , and the content spread with new edges X is f ( X ) = Pc Pi PX ( i , c ) . We formally define our content maximization problem below .
Definition 1 . Content Maximization Problem :
Given a graph G = ( V , E ) and a constant k , find an edge set X ⊆ {(i , j ) : i , j ∈ V } such that : ( 1 ) At most k edges from X are incident on any node in V , ( 2 ) For each ( i , j ) ∈ X , i ∈ Nj and j ∈ Ni , and ( 3 ) f ( X ) is maximum . ✷
The term PX ( i , c ) within the content spread expression f ( X ) depends on the specific content propagation model [ 10 , 9 , 14 , 4 ] employed . A popular model for the spread of information or influence through a social network is the Independent Cascade ( IC ) model [ 14 ] . In this model , when a node receives or generates a new piece of content c ( that it has not seen before ) at step t , it shares the content with its neighbors in the subsequent step t + 1 . Thus , each node shares specific content with its neighbors only once .
Our content spread function f ( · ) under the IC model has two main drawbacks . First , computing the expected number of nodes with specific content c is #P hard [ 4 ] , and so accurately estimating the content spread requires running expensive simulations for a large number of times . To overcome the high computation cost , Chen et al . [ 4 ] propose an efficient heuristic that restricts influence propagation between a pair of nodes to be only along the maximum probability path ( MPP ) between the nodes . This propagation model can also be applied to our setting , thus allowing the content spread function f ( · ) to be efficiently computed . We will refer to this model as the MPP model in this paper . Interestingly , even though the MPP model is more restrictive compared to the IC model , Chen et al . [ 4 ] empirically show that the
1
2
3
4
5
Figure 2 : Example illustrating that f ( · ) is not submodular under the MPP model .
MPP model closely matches the IC model in terms of the influence spread .
Unfortunately , the set cover problem can be reduced to our content maximization problem resulting in the following theorem .
Theorem 1 . The content maximization problem is NP hard under both the IC and MPP models .
Proof . Follows from a reduction of the Set Cover problem to the content maximization problem . Details omitted due to space constraints .
The second drawback is that under both the IC and MPP models , f ( · ) lacks properties that would allow us to devise good approximation algorithms . One such property is submodularity – a function h on subsets of edges is submodular if h(S ∪ {e} ) − h(S ) ≥ h(T ∪ {e} ) − h(T ) for all edges e and all pairs of edge subsets S ⊆ T . Kempe et al . [ 14 ] and Chen et al . [ 4 ] show that influence spread is submodular under the IC and MPP models , thus enabling a simple greedy strategy to yield a solution that is within a factor of ( 1 − 1 e ) of the optimum . However , our content spread function f ( · ) defined on edges is very different from influence spread which is defined on nodes . Specifically , when computing the content spread f ( X ) , the new edge set X gets added to the underlying graph G and this changes the structure of G . In the following example , we show that f ( · ) is not submodular under the IC and MPP models .
Example 2 . Consider the social network graph with 5 nodes and 2 edges depicted in Figure 2 . Let each node have propagation probability 1 and let only node 1 contain a single piece of content c . Let S = ∅ , T = {(2 , 3)} and e = ( 1 , 2 ) . Now f ( S ∪ {e} ) − f ( S ) = 2 − 1 = 1 since with edge e , content c from node 1 reaches node 2 with probability 1 . On the other hand , f ( T ∪ {e} ) − f ( T ) = 5 − 1 = 4 since with edges ( 1 , 2 ) and ( 2 , 3 ) , content c from node 1 reaches every node j with probability 1 along the unique path from 1 to j . Thus , since f ( S ∪ {e} ) − f ( S ) < f ( T ∪ {e} ) − f ( T ) for S ⊆ T , f ( · ) is not submodular . ✷
In the next section , we propose a restricted content propagation model that closely approximates the MPP model but in which content spread f ( · ) is submodular . This allows us to develop efficient approximation algorithms for content maximization . Although , in our problem setting , we have more complex constraints that require the number of edges incident on each node to be no more than k . The constraints preclude simple greedy approaches , and necessitate more involved approximation algorithms .
3 . A SUBMODULAR CONTENT SPREAD
FUNCTION
We now present our new content propagation model in which content spread is both submodular and efficient to compute .
WWW 2012 – Session : Information Diffusion in Social NetworksApril 16–20 , 2012 , Lyon , France531 3.1 Restricted Maximum Probability Path
Model
For a path hi = u1 , u2 , . . . , ur = ji through nodes u1 , u2 , . . . , ur , we define the propagation probability of the path as pu1 · pu2 · · · pur−1 . This is essentially the probability that content from node i reaches j along the path . Our new model transmits content along paths that are restrictions of MPPs – we define these paths below .
Definition 2 . Restricted Maximum Probability Path
( RMPP ) : For edge set X , the restricted maximum probability path RM P PX ( i , j ) from node i to node j is the path with the maximum probability among all paths from i to j containing at most one edge from X . Ties are broken arbitrarily . ✷ In our new RMPP model , content from node i flows to node j only along the path RM P PX ( i , j ) . These RMPPs between nodes are used to compute the content spread f ( X ) for an edge set X . Thus , the RMPP model restricts content propagation paths to contain at most one edge from X , and this ensures submodularity of content spreads . In Section 5 , we show that considering these restricted propagation paths has little effect on content spread values ; this is because a bulk of the probability mass is concentrated along such paths . Note that RMPPs can be efficiently computed using a variant of Dijkstra ’s shortest path algorithm ; omitted here due to space considerations .
To illustrate the submodularity property in the RMPP model , let us revisit the social graph in Figure 2 . Assume that all nodes have propagation probabilities of 1 and content c is only at node 1 . Let us compute the content spread for edge sets S = ∅ , T = {(2 , 3)} and e = ( 1 , 2 ) in the RMPP model . The content spread is 1 for edge sets S and T since node 1 is completely disconnected from the rest of the graph . For edge set S ∪ {e} , the content spread is 2 because content c reaches node 2 with probability 1 along path h1 , 2i . The content spread for edge set T ∪ {e} is also 2 because the content c from node 1 can only reach node 2 . It cannot reach other nodes since this would require the content to traverse a path containing both edges in T ∪{e} which is not allowed . Thus , f ( S ∪ {e} ) − f ( S ) = f ( T ∪ {e} ) − f ( T ) in the RMPP model . This is in contrast to the MPP model under which f ( S ∪ {e} ) − f ( S ) < f ( T ∪ {e} ) − f ( T ) ( see Example 2 ) .
In addition to submodularity , the content spread in the RMPP model can also be efficiently computed . We make the following two simplifying assumptions to speed up computation and design effective algorithms : ( 1 ) Similar to the MPP model in [ 4 ] , we use a threshold θ to prune paths with too small propagation probabilities , and ( 2 ) We assume that content propagates along each path independent of other paths . Note that the MPP model in [ 4 ] does not assume path independence . In Section 5 , we show that the path independence assumption minimally impacts the computed content spread values . Thus , the content spreads for the RMPP and MPP models are very close .
Now , for our RMPP model , we can compute the probability PX ( i , c ) of content c getting to node i for a new edge set X as follows . Let V ( c ) denote the nodes containing content c . Further , for j ∈ V ( c ) , let qX ( j , i ) be the probability of the path RM P PX ( j , i ) from j to i if it is above threshold θ . On the other hand , if the probability of path RM P PX ( j , i ) is less than θ , then qX ( j , i ) = 0 . Since the propagation of content to node i along the individual paths RM P PX ( j , i ) are independent , we get that PX ( i , c ) = 1−Qj∈V ( c)(1−qX ( j , i) ) .
Thus , the content spread function in the RMPP model is given by : f ( X ) = X
PX ( i , c ) = X
( 1 − Y
( 1 − qX ( j , i) ) )
X
X i c c i j∈V ( c )
( 1 ) Our content maximization problem in the RMPP model is then to find an edge set X in graph G that maximizes the content spread f ( X ) in Equation ( 1 ) above subject to constraints ( 1 ) and ( 2 ) in Definition 1 . This problem can also be shown to be NP hard using a reduction similar to the one used for content maximization under MPP in Theorem 1 earlier . The following example illustrates content spread computation under the RMPP model .
Example 3 . Consider the social graph in Figure 2 . Let the propagation probabilities for all nodes be 1 2 . Furthermore , let node 1 contain content c , and nodes 4 and 5 contain content c′ . Finally , let X = {(1 , 2 ) , ( 2 , 3)} . Now , PX ( 2 , c ) = p1 = 1 2 since c can flow from 1 to 2 along path ( 1 , 2 ) . However , PX ( j , c ) = 0 for j ≥ 3 since there is no path from 1 to j containing at most one edge from X . Content c′ can reach node 3 from 4 and 5 along two paths h4 , 3i and h5 , 3i , respectively . Thus , PX ( 3 , c′ ) = 1 − ( 1 − p4)(1 − p5 ) = 3 4 . Similarly , since content c′ can reach 2 along paths h4 , 3 , 2i and h5 , 3 , 2i , we get PX ( 2 , c′ ) = 1−(1−p4 ·p3)(1−p5 ·p3 ) = 7 16 . However , content c′ cannot reach node 1 because paths to 1 from 4 and 5 involve two edges from X . Thus , content spread f ( X ) = Pi PX ( i , c ) +Pi PX ( i , c′ ) = ( 1 + 1 16 ) = 46875 16 above , we assumed that the paths h4 , 3 , 2i and h5 , 3 , 2i are independent . Without the path independence assumption , we would have obtained PX ( 2 , c′ ) = PX ( 3 , c′ ) · p3 = 3 8 . Thus , the PX ( 2 , c′ ) values with and without path independence are fairly close . ✷
4 + 7 Observe that in our derivation of PX ( 2 , c′ ) = 7
2 ) + ( 2 + 3
2 = 3
4 · 1
It is straightforward to see that f ( · ) is monotonic . The following theorem proves submodularity .
Theorem 2 . The content spread function f ( X ) under the
RMPP model is submodular .
Proof . See Appendix A .
4 . APPROXIMATION ALGORITHM FOR
CONTENT MAXIMIZATION
We are now ready to present our approximation algorithm for the content maximization problem in the RMPP model . Let Z = {e1 , e2 , . . . , em} be the set of edges between similar nodes in V corresponding to compatible users . We are looking for a set X ⊆ Z such that at most k edges from X are incident on any node and f ( X ) as defined in Equation ( 1 ) is maximum .
Since f ( · ) is submodular , one option is to use a simple greedy strategy that ( in each step ) selects the edge that provides the maximum marginal increase in function value . However , this does not give good approximation bounds because of the constraint k on the number of edges incident on a node . Specifically , an edge that results in the maximum increase in content spread might violate node incidence constraints and thus be ineligible for selection . Consequently , to handle these feasibility constraints , we adopt a different approach that considers a continuous relaxation of our problem , and computes a fractional ( approximate ) solution for edge membership in set X using the algorithm of [ 23 ] . We
WWW 2012 – Session : Information Diffusion in Social NetworksApril 16–20 , 2012 , Lyon , France532 then use randomized rounding to convert our fractional solution into an integral solution , and incur a constant factor reduction in the benefit due to rounding .
Continuous relaxation . Let ¯y = ( y1 , . . . , ym ) be an mdimensional vector of variables yi ∈ [ 0 , 1 ] . The semantics here are that edge ei ∈ X with probability yi . We define F ( · ) to be the following continuous extension of f ( · ) . Let X ⊆ Z be a random variable such that ei ∈ X with probability yi . Then ,
F ( ¯y ) = E[f ( X ) ] = X
X f ( X ) Y ei∈X yi Y ei6∈X
( 1 − yi )
( 2 )
The continuous relaxation of our content maximization problem then is to find ¯y such that F ( ¯y ) is maximized with yi ≤ k for all j ∈ V
X j∈ei yi ∈ [ 0 , 1 ]
( 3 )
( 4 )
Note that Equation ( 3 ) enforces the constraint that each node j has at most k incident edges in the discrete case . Now , let F ( ¯yopt ) be the maximum value of F ( · ) subject to the constraints , and Xopt be the edge set satisfying constraints for which f ( Xopt ) is maximum . Also , let ¯z be defined as follows : zi = 1 if ei ∈ Xopt , and 0 otherwise . Then , observe that F ( ¯z ) = f ( Xopt ) , and ¯z is feasible . Thus , we have that F ( ¯yopt ) ≥ F ( ¯z ) = f ( Xopt ) .
Algorithm 1 : ContinuousGreedy Input : Graph G = ( V , E ) , candidate edge set Z ; Output : ¯y satisfying Equation ( 3 ) and
F ( ¯y ) ≥ ( 1 − 1 e ) · f ( Xopt ) ;
1 ¯y = 0 ; l = 0 ; 2 while l < δ do 3
Generate r samples X1 , X2 , . . . , Xr , where ei ∈ Xj with probability yi . Set wi =
Pj f ( Xj ∪ei)−f ( Xj ) r
.
4
5
6
Compute a subset of edges Y such that no node has more than k incident edges and Pei∈Y wi is maximum . This is an instance of the graph matching problem and can be solved using the algorithm of [ 13 ] in O(m3 ) steps ; foreach ei ∈ Y do yi = yi + 1/δ ; l = l + 1 ;
7 return ¯y ;
Continuous greedy algorithm . We use the continuous greedy algorithm of Vondrak [ 23 ] ( see Algorithm 1 ) to find a ¯y satisfying Equation ( 3 ) above such that F ( ¯y ) ≥ ( 1 − e ) · F ( ¯yopt ) ≥ ( 1 − 1 1 e ) · f ( Xopt ) . Algorithm 1 considers δ intervals of width 1/δ , and in each iteration , it increments yi values of edges ei in a feasible edge set Y with the maximum sum of gradients Pei∈Y can be approximated as E[f ( X ∪ ei ) − f ( X ) ] which is estimated by averaging over r samples Xj . The graph matching algorithm of [ 13 ] is then used to compute the optimal set Y with at most k edges per node and the maximum sum of gradient estimates . Note that since the yi values of only edges ei ∈ Y are incremented by 1/δ in each iteration , it follows that the final ¯y satisfies Equation ( 3 ) . In fact , [ 23 ] proves the following theorem .
. Each gradient ∂F ∂yi
∂F ∂yi
Theorem 3 . [ 23 ] For δ = m2 and r = m5 , Algorithm 1 e ) · ✷ returns ¯y satisfying Equation ( 3 ) and F ( ¯y ) ≥ ( 1 − 1 f ( Xopt ) .
Randomized rounding procedure . Once we have computed ¯y satisfying Pj∈ei yi ≤ k for all j ∈ V and F ( ¯y ) ≥ ( 1 − 1 e ) · f ( Xopt ) , we use randomized rounding [ 23 ] to compute the final set X of edges . Essentially , we add element ei to X with probability yi . Note that E[f ( X ) ] = F ( ¯y ) and so E[f ( X ) ] ≥ ( 1− 1 e )·f ( Xopt ) . However , the result of rounding X may no longer be feasible , that is , the number of edges in X that are incident on a node j may exceed k . So we need to delete edges from X to ensure that it is feasible – we do this by partitioning X into a small number of feasible sets Xi and returning the Xi for which f ( Xi ) is maximum .
Our partitioning scheme starts with X1 = X and for each node j with k′ > k incident edges in X1 , it deletes ( an arbitrary set of ) k′ − k edges incident on j from X1 and inserts them into a new ( overflow ) set X2 . Thus , X1 now becomes feasible , and the procedure is repeated for X2 , . . . , Xs until we get an overflow set Xs that is feasible . Analysis of Approximation Algorithm . We can show the following approximation guarantee for our algorithm .
Theorem 4 . Let |V | = n , δ = m2 and r = m5 . Further , let our partitioning scheme generate edge sets X1 , . . . , Xs . Then whp E[maxi f ( Xi ) ] ≥ 1 e ) · f ( Xopt ) , where ǫ = q 8
3+2ǫ · ( 1 − 1 k log(n ) .
Proof . See Appendix B .
Note that Theorem 4 provides worst case bounds . In practice , our experimental results indicate that our approximation algorithm returns edge sets with good content spreads for much smaller values of parameters δ ( set to 2000 ) and r ( set to 30 ) . The time complexity of our approximation algorithm is dominated by the matching procedure in Step 3 of Algorithm 1 . The matching algorithm has time complexity O(m3 ) and is run δ times ; so the overall time complexity of our approximation algorithm is O(m3 · δ ) . To overcome the computation cost , for large m , we can cluster the edges in Z and run our recommendation algorithm on the smaller clusters . We can also achieve further speedup using approximate matching based on greedy heuristics instead of exact matching . In our experiments in Section 6 , computing recommendations on a one million node Twitter graph took a little over 11 hours on a stand alone PC .
5 . DISCUSSION
In this section , we present intuitive arguments to show that the three models – MPP with path dependence assumption [ 4 ] , MPP with path independence assumption , and RMPP – result in similar content spreads for realistic graphs .
5.1 Closeness of Content Spreads in the Dependent and Independent Path MPP Models
To show that the path independence assumption does not affect content spread values significantly , we compute PX ( i , c ) for a node i with and without the path independence assumption in the MPP model . For simplicity , let the propagation probability of all nodes be p . Furthermore , let the paths ( over the edge set E ∪ X ) that carry content c to node i form a tree of depth l = ⌊logp θ⌋ and degree d at each node ( thus , there are dl paths ) . Recall ( from Section 3.1 )
WWW 2012 – Session : Information Diffusion in Social NetworksApril 16–20 , 2012 , Lyon , France533 d p p c l p c i p p 0 p c p c p =1 l
Figure 3 : Propagation tree of depth l for carrying c from content nodes to node i .
MPP ( independent ) MPP ( dependent ) t n e t n o c e g a r e g v A
1.2
1
0.8
0.6
0.4
0.2
0
2
4
6
8
1
1
1
1
2
2
2
2
0
0
0
0
0
2
4
6
2
4
6
8
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
Tree size
Figure 4 : Histogram of average content at a node in the Twitter graph under the dependent and independent path MPP models . that θ is the threshold to prune paths with low propagation probabilities . This is pictorially depicted in Figure 3 . In general , the degree d of a node in the propagation tree is much smaller compared to its degree in the graph G due to the limited number of sources with content c . Thus , since propagation probability is typically small ( ˜0.05 ) , p · d ≪ 1 . Under the independent path assumption , PX ( i , c ) = 1 − ( 1 − pl)dl ≈ pl · dl . Now , let us compute PX ( i , c ) considering dependencies between paths as in [ 4 ] . Let px be the probability of content reaching an intermediate node in the content propagation tree at depth x . Then , the probability of content reaching a node at depth ( x−1 ) can be recursively computed as px−1 = 1−(1−px ·p)d . Since the probability of content at the leaf nodes pl = 1 , we get pl−1 = 1−(1−p)d ≈ p·d . Similarly , pl−2 = 1 − ( 1 − p · d · p)d ≈ p2 · d2 . Computing recursively , the probability of content reaching the root ( node i ) p0 = PX ( i , c ) ≈ pl · dl . Thus , the content spread with and without path independence is approximately the same .
Figure 4 empirically compares the average content at a node in a one million node Twitter graph ( described in Section 6.1 ) under the independent and dependent path assumptions . Content c is randomly assigned to 1 % of the nodes in the graph which in turn spreads through paths of the propagation trees . The propagation trees are grouped into bins ( x axis ) based on the number of nodes in the tree . The average content at root nodes in a bin is shown on the yaxis . As can be seen , the content spreads with and without the path independence assumption are very close .
The path independence assumption in our setting is essential for submodularity in Theorem 2 . In fact , it can be shown that the submodularity property does not hold for the dependent path MPP model in [ 4 ] .
5.2 Closeness of Content Spreads in the RMPP and Independent Path MPP Models
Another natural question arises – what is lost when the restriction of at most one new edge per path is imposed in the RMPP model ? For a simplified yet representative setting , we present informal arguments to show that the content spread along paths containing at most one edge from X ( RMPP model ) matches the spread along paths containing an arbitrary number of edges from X ( MPP model ) .
Our arguments rely on two basic observations : ( 1 ) In social networks , the average degree of nodes is typically much larger than the number of social recommendations k per node , and ( 2 ) With each additional hop , the probability of content traversing a path decreases by a factor equal to the propagation probability of the hop . Thus , shorter paths tend to have higher probabilities , and so content is more likely to spread along shorter paths compared to longer paths .
We will illustrate this in the context of a simple scenario where a single node i contains a piece of content c and we trace the spread of c from i along different types of maximum probability paths . Consider the tree T of maximum probability paths over edges in E ∪ X originating from i . To keep our analysis simple , let us assume that each node in T has h + k children connected to it by h edges from E and k edges from X . Also , let us assume that each node shares content with its neighbors with probability p . Since we ignore paths with probability less than θ , the depth of tree T is at most l = ⌊logp θ⌋ . See Figure 5 for an illustration .
Now , it is easy to see that the number of paths in T originating at root i of length r with zero edges from X is hr . Similarly , the number of paths in T of length r with a single edge from X is r · hr−1 · k . The reason for the factor r is that the single edge from X can occur in one of r positions , and the factor k is there because each node has k incident edges from X . Also , the total number of paths starting at root i of length r in T is ( h + k)r . Suppose that P0 , P1 and P∞ are the content spread values from node i along paths with at most zero , one , and unlimited edges from X , respectively . P∞ is essentially the total probability of all the paths , and so we get that P∞ = Pl r=1(h + k)r · pr . Similarly , P1 is the probability of paths with at the most one edge from X , and so P1 = Pl r=1 r · hr−1 · k · pr . And finally , P0 = Pl Typical values of parameters are h = 100 , k = 10 , p = 0.05 , and l = 3 . For these values , P0 = 155 , P1 = 198 and P∞ = 202125 Thus , the content spread along paths with at most one edge from X is very close to the content spread along paths with no restrictions on the number of edges from X . In fact , if we only consider paths containing at least one edge from X , then the bulk of the probability mass is concentrated in paths with exactly one edge from X . This is because the probability of paths with exactly one edge from X is P1 − P0 = 43 while the probability of paths with more than one edge from X is P∞ − P1 = 4125 Emperical results comparing content spreads in the RMPP r=1 hr · pr + Pl r=1 hr · pr . and the MPP models are presented in Section 63
6 . SIMULATION EXPERIMENTS
Through simulations on real life social network data , we show the superior performance of the continuous greedy algorithm compared to other popular approaches – simple greedy , degree based heuristics , and Friend of Friend ( FoF ) based selection . We also ( empirically ) substantiate our claim about the closeness of content maximization under the RMPP and MPP models .
WWW 2012 – Session : Information Diffusion in Social NetworksApril 16–20 , 2012 , Lyon , France534 h+k p c i p h+k l
Figure 5 : Propagation tree for spread of content c from node i along h ( solid ) edges from E and k ( dashed ) edges from X .
Table 1 : Statistics for the datasets . ( SCC : Strongly Connected Components , WCC : Weakly Connected Components ) .
Wikipedia Flickr Epinions Twitter
#Nodes #Edges
Average Degree Maximum Degree
# WCC # SCC
Largest SCC size Avg . SCC Size
# groups
7.1 K 104 K 14.6 893 24
5.9 K 1.3 K 1.22 NA
81 K 5.9 M 72.8 4714
1 1
81K 81K 195
76 K 508 K
6.7 1801
1
42 K 3.2 K 1.79 NA
1 M
3088467
3.1 216 15
896749 103113
1.12 27
6.1 Experimental Setup
To simulate content dissemination , we need realistic graph datasets , and models for content generation and propagation . Datasets : We use the Wikipedia , Flickr , Epinions , and Twitter social graphs . Between them , these graphs capture a wide variety of social relations ( eg , trust relations , follower following relations , and friendship relations ) . Important statistics of these graphs are summarized in Table 1 . Twitter : This dataset is obtained by crawling the twitter.com site starting from a randomly chosen set of popular personalities on Twitter . A directed edge from node X to Y indicates that X is following Y ’s tweets . Due to Twitter ’s limit on the number of web server requests , only a subset of users followed by X could be obtained . Based on the content of their tweets , users were assigned to zero or more groups from a set of 27 predefined groups ( eg , politics and sports ) . Flickr : The Flickr ( undirected ) graph [ 24 ] consists of friendship relations between users on the popular image site flickrcom In addition to the friendship connections , each user belongs to one or more Flickr groups ( eg , wildlife and nature ) from a set of 195 groups . Wikipedia : The Wikipedia(directed ) graph [ 16 ] is generated using the voting activity in elections for granting administrator rights to Wikipedia users . Each node in the graph represents a Wikipedia user with voting rights . A directed edge from i to j denotes user i ’s vote for user j . Epinions : Directed social network captures the who trustswhom relation on the consumer reviews site epinionscom
We consider the similarity sim(i , j ) = 1 if the pair of users ( i , j ) shares a common group and sim(i , j ) = 0 otherwise . The similarity function determines the candidate set Nu of nodes with which a user u can connect . In the absence of groups in a dataset , an edge can be added between any pair of nodes in the network . Content generation model : In the absence of informa tion related to content at the nodes , we make the following assumptions for simulation purposes – our algorithm is independent of the exact values assumed here . We assume that a single content type c is generated by a set of seed nodes , S . Moreover , the rate of content generation is assumed to be uniform across all nodes in S . Unless specified otherwise , |S| is 1 % of the node set size . The set S is selected randomly for all datasets other than Twitter . For Twitter , S consists of about 38K users who tweet about soccer . To maintain parity , the same set of randomly generated seed nodes is used for all the algorithms .
Propagation model : Since we do not have the data to infer propagation probabilities , we instead consider two propagation probability assignments that are simple , yet illusIn the uniform assignment ( henceforth UNI ) , all trative . nodes have the same probability p of sharing content . In the weighted ( henceforth WT ) assignment , inspired by the weighted cascade model [ 15 ] , pairwise propagation probabilities are inversely proportional to the degree of the originating node . These assignments have been considered elsewhere for studying the MPP model [ 4 ] . We use the setting p = 0.05 , unless stated otherwise .
611 Performance Evaluation
To compare the different edge selection methods , we define the lift metric as follows :
Lif t(X ) = f ( E ∪ X ) − f ( E ) f ( E )
× 100 ,
( 5 ) where E is the set of edges in the original graph G and f ( . ) is the content spread function . X is the set of recommendations computed by the following edge selection methods . Since the initial set S of nodes where content originates is determined randomly , all the Lift results reported here are averages over 10 independent runs .
In this paper , we do not focus on evaluating the quality of the recommendations , since the candidate set Ni selected using traditional recommendation methods is itself likely to be highly relevant .
6.2 Edge Selection Algorithms
We compared the following edge selection strategies .
Greedy , where edges with the largest lift , given the current set of edges in the graph , are added one at a time . This process continues until no further edges can be added as a result of the maximum edge constraint for each node . Continuous Greedy ( CG ) , where edges are added to the original graph based on Algorithm 1 followed by randomized rounding . Unless stated otherwise , we use δ = 2000 and r = 30 as parameters ( see Section 4 for parameter description ) . Threshold θ for pruning paths is set to 001 Degree based selection adds edges between high degree node pairs , wherein one of the nodes has content c . This heuristic is intuitively competitive because it exploits the high degree of nodes to maximize content spread . Friend of Friend ( FoF ) based selection , where node pairs are ranked by the number of common neighbors . Edges are added between unconnected node pairs in this rank order .
In our experimental setup , k is set to 10 . In principle , all of the above algorithms terminate when the maximum recommendations limit k is reached for all nodes in the graph . To highlight key insights , we instead terminate the simulations after a certain fixed number of new edges are added .
WWW 2012 – Session : Information Diffusion in Social NetworksApril 16–20 , 2012 , Lyon , France535 All simulations were performed on a 64 bit Intel Xeon
2.5GHz processor with 32 GB of main memory .
6.3 Simulation Results
Flickr,CG,MPP Flickr,CG,RMPP Epinions,GREEDY,MPP Epinions,GREEDY,RMPP
)
%
( t f i L
100
80
60
40
20
0
Flickr,CG,MPP Flickr,CG,RMPP Epinions,GREEDY,MPP Epinions,GREEDY,RMPP
)
%
( t f i L
70
60
50
40
30
20
10
0
0
2000
4000
6000
8000 10000
0
2000
4000
6000
8000 10000
#edges added
#edges added
( a ) U N I propagation
( b ) W T propagation
Figure 6 : Lift for RM P P and M P P models under uniform and weighted propagation .
Comparison between RM P P and independent path M P P . For the Epinions and Flickr datasets , Figures 6(a ) and 6(b ) show the percentage lift as edges are added to the initial graph under the UNI and WT probability assignment models , respectively . In both models , the M P P and RM P P lifts are almost identical . However , the RM P P and M P P propagation models tend to deviate minimally under the W T probability assignment since W T has higher likelihood of longer paths in graphs with low degree nodes . Since long paths have a higher chance of more than one new edge , we would expect M P P and RM P P to diverge in such situations . Subsequent results consider the RM P P model alone .
)
%
( t f i
L
)
%
( t f i
L
120
100
80
60
40
20
0
140
120
100
80
60
40
20
0
CG UNI CG WT GREEDY UNI GREEDY WT
CG UNI CG WT GREEDY UNI GREEDY WT
)
%
( t f i
L
100
80
60
40
20
0
0
2000
4000
6000
8000 10000
0
2000
4000
6000
8000 10000
#edges added ( a ) Wikipedia
Epinions CG UNI Epinions CG WT Epinions GREEDY UNI Epinions GREEDY WT
#edges added
( b ) Flickr
CG UNI CG WT GREEDY UNI GREEDY WT
)
%
( t f i
L
5
4
3
2
1
0
0
2000
4000
6000
8000 10000
0
2000
4000
6000
8000
10000
#edges added
( c ) Epinions
#edges added
( d ) Twitter
Figure 7 : Lift as a function of number of edges added to the initial graph .
Greedy versus Continuous Greedy ( CG ) maximization . The Greedy approach selects edges with the highest immediate lift . On the contrary , CG takes an approach that captures the correlation between edges with high marginal gain . The iterative computation in CG has the following benefits : ( 1 ) Each iteration selects a locally optimal edge set ( to maximize sum of gradients ) , and ( 2 ) Each iteration takes into account edge sets selected in previous iterations , thus capturing the interplay between edges .
Figure 7 shows the lift in content propagation for 10K edges added to the initial graphs . The combination of con tinuous greedy with weighted propagation outperforms other algorithms by a factor of 1.75 to 2 for all datasets other than Flickr . For Flickr,CG UNI outperforms other algorithms by as much as a factor of 6 . The lower lift for CG WT on the dense Flickr graph can be attributed to the propagation probability , which is inversely proportional to node degree in the WT assignment . In contrast , for low density datasets ( eg Wikipedia and Epinions ) WT propagation achieves maximum marginal gain by connecting low degree nodes .
Comparison with edge recommendation heuristics . We now compare Greedy and CG , under UNI propagation , with two commonly used heuristics – Degree and FoF . In Fig
CG UNI GREEDY UNI DEGREE FoF
)
%
( t f i L
100
75
50
25
0
CG UNI GREEDY UNI DEGREE FoF
)
%
( t f i L
50
40
30
20
10
0
1000
5000
10000
1000
5000
10000
#edges added
( a ) Flickr
#edges added ( b ) Epinions
Figure 8 : Comparison with Degree and FoF based heuristics . ure 8 , both Degree and FoF have insignificant lifts compared to CG and Greedy , which shows the merit of applying optimization techniques instead of generally accepted heuristics . To highlight the difference , CG has a lift 80–95 times that of FoF and Degree .
)
%
( t f i L
100
80
60
40
20
0
CG UNI CG WT GREEDY UNI GREEDY WT
CG UNI CG WT GREEDY UNI GREEDY WT
95 80 60 40 20 0
CG UNI GREEDY UNI
50
40
30
20
10
0
0
2
4
6
8
10
0
10
20
30
Initial nodes with content ( % ) k Maximum edge additions per node
0.02 0.04 0.06 0.08 0.1 UNI Propagation Prob
( a ) Varying F
( b ) Varying k
( c ) Varying UNI p
Figure 9 : Lift versus varying parameters for the Epinions dataset .
Varying CG algorithm parameters . Figure 9(a ) shows lift as the fraction of nodes with original content varies . We show results for the Epinions dataset alone ; other datasets show similar trends . 10K edges are added for each F = |S| |V | , varying from 1 % to 10 % . As expected , with increasing F the lift decreases as the fraction of yet to be reached nodes decreases . Again , CG optimization yields the best results over the entire range of F considered .
Figure 9(b ) shows the impact of varying the number of recommended edges per node . Initially , the lift increases for all the models . Beyond k = 20 , however , the lift for the UNI models stabilizes whereas the lift for WT propagation decreases . For the W T propagation probability assignment , as the degree of a node increases with additional edges its propagation probability decreases ; beyond a point the overall spread begins to get impacted .
Finally , Figure 9(c ) shows the impact of varying the propagation probability p for the UNI model . As expected , the lift increases with p . At p = 0.1 , the network has ample content prior to any edge additions . As a result , we observe the lift beginning to drop .
WWW 2012 – Session : Information Diffusion in Social NetworksApril 16–20 , 2012 , Lyon , France536 Comparing UNI and WT . Figure 10 shows how UNI and WT probability assignments result in different kinds of nodes being selected for edge recommendations . The WT probability assignment results in new links originating from low degree nodes , since those nodes have the highest peredge propagation probabilities . Thus , low degree nodes are preferred until such nodes become increasingly rare . On the other hand , UNI is biased towards picking high degree nodes , which have the highest expected number of neighbors receiving content . s e d o n f o e e r g e d
. g v A s e g d e w e n h t i w
CG UNI CG WT
300
250
15
10
5
200
0
0 2000 4000 6000 8000 10000
#edges added ( a ) Flickr s e d o n f o e e r g e d
. g v A s e g d e w e n h t i w
175
150
125
100
CG UNI CG WT
40
30
20
10
75
0
0 2000 4000 6000 8000 10000
#edges added ( b ) Wikipedia
Figure 10 : UNI versus WT – Average degree of nodes with new edges . Note : Right axis is for CGWT and left is for CG UNI . Runtime performance of continuous greedy . Computing 5K recommendations for the Twitter , Flickr and Epinions datasets took 40913 , 2169 and 1581 seconds , respectively , under the U N I propagation model . Clearly , the sparse one million node Twitter graph takes comparatively much longer due to the larger number of missing edges that are potential recommendations . It is important to note that in practice , recommendations are computed offline , thus allowing for the higher computation cost . Observe that we can also speed up computation by parallelizing parts of our algorithms using a Map Reduce framework . Nevertheless , efficient methods based on greedy heuristics is a direction for further exploration .
7 . RELATED WORK
Following categories of research relate to the current work .
Recommendations and link prediction in social networks : As mentioned in Section 1 , a large number of interests or profiles ( ie college attended , current city , etc . ) based and FoF based recommendation algorithms have been proposed in the literature [ 3 , 1 , 11 ] . Link prediction algorithms have also been developed for friend recommendations [ 21 , 18 ] . These algorithms , unlike the proposed work , do not consider content spread as an explicit objective while recommending links . Recent work by Roth et al . [ 20 ] defines an Interactions Rank ( IR ) metric based on email interactions between users . The IR score is used within a Friend Suggest algorithm to recommend connections similar to a seed set of users . Another work , Twittomender [ 12 ] , recommends users to follow on Twitter based on a combination of content and collaborative filtering type features . Influence propagation and maximization : A large number of social contagion models have been proposed for explaining the diffusion of information and ideas through social connections . The linear threshold model [ 10 ] and the independent cascade ( IC ) model [ 9 ] are the most widely studied probabilistic models of diffusion . Variations of these models – decreasing cascade model [ 15 ] , triggering model [ 14 ] , and non progressive models such as the Susceptible/Infected/Sus ceptible ( SIS ) model [ 19 ] have also been studied . Most of these models are compute intensive to simulate .
The influence maximization problem , also known as the target set selection problem [ 7 , 14 , 15 ] addresses the maximization of social contagion within the propagation models mentioned above . Recent work [ 17 , 5 , 4 ] has focused on efficient techniques for influence maximization . These techniques identify a set of nodes as opposed to edge recommendations in our setting . Edge augmentation : Adding edges to graphs has been explored with other objectives – minimizing the network diameter [ 6 ] and maximizing algebraic connectivity for robustness [ 8 ] , to list a few .
8 . CONCLUSION
We introduced the problem of recommending connections in a social network with the explicit objective of maximizing content spread in the network . Our content maximization problem is interesting in two ways . First , the problem is NP hard and non submodular . Second , we impose per node constraints on the maximum number of new links as opposed to a global constraint on the number of selected nodes as in the influence maximization problem . We proposed a novel RMPP model that admits submodularity leading to computationally feasible approximation algorithms in the presence of the above constraints . Simulation results on realistic graphs demonstrate the superiority of our approach in comparison with commonly used heuristics .
The proposed content maximization framework has interesting extensions . The model currently assumes that the content generated at each node is independent and noncompeting . Adapting the model to overcome these assumptions is a direction worth exploring . Scalability , alternate models for propagation ( eg SIS diffusion model ) and effectiveness on live web scale networks are aspects that also need further investigation .
9 . REFERENCES [ 1 ] Official Facebook Blog : . http://blogfacebookcom/blogphp?post=15610312130
[ 2 ] Discovering who to follow . Official Twitter Blog . http://blogtwittercom/2010/07/ discovering who to followhtml
[ 3 ] J . Chen , W . Geyer , C . Dugan , M . Muller , and I . Guy . Make new friends , but keep the old : recommending people on social networking sites . In CHI ’09 , pages 201–210 , Boston , MA , USA , 2009 .
[ 4 ] W . Chen , C . Wang , and Y . Wang . Scalable influence maximization for prevalent viral marketing in large scale social networks . In KDD ’10 , pages 1029–1038 , Washington , DC , USA , 2010 .
[ 5 ] W . Chen , Y . Wang , and S . Yang . Efficient influence maximization in social networks . In KDD ’09 , pages 199–208 , New York , NY , USA , 2009 .
[ 6 ] E . D . Demaine and M . Zadimoghaddam . Minimizing the diameter of a network using shortcut edges . In SWAT , pages 420–431 , 2010 .
[ 7 ] P . Domingos and M . Richardson . Mining the network value of customers . In KDD ’01 , pages 57–66 , San Francisco , California , 2001 .
[ 8 ] A . Ghosh and S . Boyd . Growing well connected graphs . In Growing Well connected Graphs , pages 6605 – 6611 , 2006 .
[ 9 ] J . Goldenberg , B . Libai , and E . Muller . Talk of the network : A complex systems look at the underlying process of word of mouth . Marketing Letters , pages 211–223 , August 2001 .
WWW 2012 – Session : Information Diffusion in Social NetworksApril 16–20 , 2012 , Lyon , France537 [ 10 ] M . Granovetter . Threshold models of collective behavior .
American Journal of Sociology , ( 83):1420–1443 , 1978 .
[ 11 ] I . Guy , I . Ronen , and E . Wilcox . Do you know ? : recommending people to invite into your social network . In IUI ’09 , pages 77–86 , 2009 .
[ 12 ] J . Hannon , M . Bennett , and B . Smyth . Recommending twitter users to follow using content and collaborative filtering approaches . In RecSys ’10 , pages 199–206 , 2010 .
[ 13 ] B . Huang and T . Jebara . Loopy belief propagation for bipartite maximum weight b matching . In M . Meila and X . Shen , editors , AISTATS ’07 , volume 2 of JMLR : W&CP , March 2007 .
[ 14 ] D . Kempe , J . Kleinberg , and E . Tardos . Maximizing the spread of influence through a social network . In KDD ’03 , pages 137–146 , Washington , DC , 2003 .
[ 15 ] D . Kempe , J . M . Kleinberg , and ´E . Tardos . Influential nodes in a diffusion model for social networks . In ICALP’05 , pages 1127–1138 , 2005 .
[ 16 ] J . Leskovec , D . Huttenlocher , and J . Kleinberg . Predicting positive and negative links in online social networks . In WWW ’10 , pages 641–650 , 2010 .
[ 17 ] J . Leskovec , A . Krause , C . Guestrin , C . Faloutsos ,
J . VanBriesen , and N . Glance . Cost effective outbreak detection in networks . In KDD ’07 , pages 420–429 , San Jose , California , USA , 2007 .
[ 18 ] D . Liben Nowell and J . Kleinberg . The link prediction problem for social networks . In CIKM ’03 , pages 556–559 , New Orleans , LA , USA , 2003 .
[ 19 ] M . E . J . Newman . The structure and function of complex networks . SIAM Review , 45:167–256 , 2003 .
[ 20 ] M . Roth , A . Ben David , D . Deutscher , G . Flysher , I . Horn ,
A . Leichtberg , N . Leiser , Y . Matias , and R . Merom . Suggesting friends using the implicit social graph . In KDD ’10 , pages 233–242 , Washington , DC , USA , 2010 .
[ 21 ] R . Schifanella , A . Barrat , C . Cattuto , B . Markines , and F . Menczer . Folks in folksonomies : social link prediction from shared metadata . In WSDM ’10 , pages 271–280 , New York , NY , USA , 2010 .
[ 22 ] B . Suh , L . Hong , P . Pirolli , and E . H . Chi . Want to be retweeted ? Large scale analytics on factors impacting retweet in twitter network . In SOCIALCOM ’10 , pages 177–184 , Washington , DC , USA , 2010 . IEEE Computer Society .
[ 23 ] J . Vondr´ak . Optimal approximation for the submodular welfare problem in the value oracle model . In STOC , pages 67–74 , 2008 .
[ 24 ] R . Zafarani and H . Liu . Social computing data repository at ASU , 2009 .
APPENDIX
A . PROOF OF THEOREM 2
THEOREM : The content spread function f ( X ) under the RMPP model is submodular .
We show that PX ( i , c ) = 1−Qj∈V ( c)(1−qX ( j , i ) ) is submodular . Since the sum of submodular functions is also submodular , we get that f ( X ) = Pc Pi PX ( i , c ) is submodular . For edge e and edge subsets S ⊆ T , we are looking to show that PS∪{e}(i , c ) − PS ( i , c ) ≥ PT ∪{e}(i , c ) − PT ( i , c ) . Without loss of generality , let V ( c ) = {1 , 2 , . . . , n} . Recollect that V ( c ) is the set of nodes with content c . Furthermore , for edge set T ∪ {e} , let the RMPPs from only nodes 1 , . . . , r to i pass through edge e . We rely on the following three observations for proving that PX ( i , c ) is submodular . 1 . ( a ) qT ( j , i ) ≥ qS ( j , i ) , ( b ) qT ∪{e}(j , i ) ≥ qS∪{e}(j , i ) , and ( c ) qS∪{e}(j , i ) ≥ qS ( j , i ) . Probability of RMPPs cannot decrease with the addition of new edges . 2 . qS∪{e}(j , i ) = qT ∪{e}(j , i ) , j ≤ r . With the restriction of at the most one new edge per RMPP , the RMPPs from j to i for edge set S ∪ {e} also pass through e and are identical to those for T ∪ {e} . 3 . qT ∪{e}(j , i ) = qT ( j , i ) , j > r . RMPPs from j to i for T ∪ {e} that do not contain edge e must be identical to the ones for T .
Now , PT ∪{e}(i , c ) − PT ( i , c ) n n
( 1 − qT ∪{e}(j , i) ) ) − ( 1 −
Y j=1
( 1 − qT ( j , i) ) )
Y j=1
= ( 1 − n
=
( 1 − qT ( j , i ) ) −
Y j=1 r
≤ (
( 1 − qT ( j , i ) ) −
Y j=1 n n
Y j=1
( 1 − qT ∪{e}(j , i))/* Applying 3 */ r
Y j=1
( 1 − qT ∪{e}(j , i) ) )
( 6 )
·
Y j=r+1
( 1 − qT ∪{e}(j , i ) )
/* Applying 1(a ) , 1(b ) and 2 */ r r
≤ (
( 1 − qS ( j , i ) ) −
Y j=1 n
( 1 − qS∪{e}(j , i) ) )
( 7 )
Y j=1
·
Y j=r+1
( 1 − qS∪{e}(j , i ) )
/* Applying 1(c ) */ n n
≤
( 1 − qS ( j , i ) ) −
Y j=1
( 1 − qS∪{e}(j , i ) )
Y j=1
≤ PS∪{e}(i , c ) − PS ( i , c )
Thus , f ( · ) is submodular .
B . PROOF OF THEOREM 4
THEOREM : Let |V | = n , δ = m2 and r = m5 . Further , let our partitioning scheme generate edge sets X1 , . . . , Xs . Then whp E[maxi f ( Xi ) ] ≥ 1 k log(n ) . e ) · f ( Xopt ) , where ǫ = q 8
3+2ǫ · ( 1 − 1
Consider the set X obtained as a result of our randomized rounding procedure . Due to Theorem 3 , we have that E[f ( X ) ] ≥ ( 1 − 1 e ) · f ( Xopt ) .
Now , let Yj be the number of edges in X incident on node j . Also , let ǫ = q 8 k log(n ) . Recall that an arbitrary edge ei is included in X with probability yi . Further , since ¯y is feasible , Pj∈ei yi ≤ k for all nodes j . Thus , E[Yj ] ≤ k . Applying Chernoff Bounds , we get
P ( Yj ≥ ( 1 + ǫ ) · E[Yj ] ) < exp(−
Since E[Yj ] ≤ k , we get
P ( Yj ≥ ( 1 + ǫ ) · k ) < exp(−
E[Yj ] · ǫ2
4
) k · ǫ2
4
)
Now define Ymax = maxj {Yj } . By the union bound , we get that
P ( Ymax ≥ ( 1 + ǫ ) · k ) < n · exp(− k · ǫ2
)
4 n ) for ǫ = q 4 The above probability is extremely small ( ≤ 1 Thus , we get that whp Yj ≤ ( 1 + ǫ ) · k for all nodes j . k log(n2 ) .
Next , we show that our partioning scheme divides set X into at most 2ǫ + 3 feasible sets Xi . This is because whp , for any edge ( u , v ) ∈ X , at most 1 + ǫ sets Xi can have k edges incident on each of the vertices u and v ( since Yj ≤ ( 1 + ǫ ) · k ) . Thus , whp , in at least one of the 2ǫ + 3 sets Xi , both u and v must have fewer than k incident edges , and so Xi ∪ {(u , v)} is feasible .
Now , since f ( · ) is submodular , we have that Pi f ( Xi ) ≥ f ( ∪iXi ) .
Furthermore , there are at most 2ǫ + 3 feasible sets Xi . Thus , we get that maxi f ( Xi ) ≥ f ( X ) 3+2ǫ · ( 1 − 1 e ) · f ( Xopt ) .
2ǫ+3 , and so E[maxi f ( Xi ) ] ≥ 1
WWW 2012 – Session : Information Diffusion in Social NetworksApril 16–20 , 2012 , Lyon , France538
