Correcting for Missing Data in Information Cascades
Eldar Sadikov
Montserrat Medina eldar@csstanfordedu mmmedina@stanford.edu
Jure Leskovec jure@csstanfordedu
Hector Garcia Molina hector@csstanfordedu
August 1 , 2010
Abstract
Transmission of infectious diseases , propagation of information , and spread of ideas and influence through social networks are all examples of diffusion . In such cases we say that a contagion spreads through the network , a process that can be modeled by a cascade graph . Studying cascades and network diffusion is challenging due to missing data . Even a single missing observation in a sequence of propagation events can significantly alter our inferences about the diffusion process .
We address the problem of missing data in information cascades . Specifically , given only a fraction C ′ of the complete cascade C , our goal is to estimate the properties of the complete cascade C , such as its size or depth . To estimate the properties of C , we first formulate ktree model of cascades and analytically study its properties in the face of missing data . We then propose a numerical method that given a cascade model and observed cascade C ′ can estimate properties of the complete cascade C . We evaluate our methodology using information propagation cascades in the Twitter network ( 70 million nodes and 2 billion edges ) , as well as information cascades arising in the blogosphere . Our experiments show that the k tree model is an effective tool to study the effects of missing data in cascades . Most importantly , we show that our method ( and the k tree model ) can accurately estimate properties of the complete cascade C even when 90 % of the data is missing .
1 Introduction
Social and information networks are a fundamental medium for the spread of information , ideas , viruses and behavior . A cascade graph can be used to represent the contagion across the network . For example , if Alice is connected to Bob in a social network and Bob participates in the “ Fight Against Cancer ” campaign , he may influence Alice to do the same . Or similarly , Bob may spread information to Alice , if Bob reads some article and shares it with Alice . As information or actions spread from a node to node through the social network , a cascade is formed . Nodes of the cascade are the nodes of the network that performed an action of interest and edges represent influence relations [ 4 , 8 , 9 ] . Thus , when Alice joins the campaign under Bob ’s influence , we observe a directed edge from Bob to Alice in the cascade . We define social networks and cascades formally in Section 2 , but to illustrate now , Figure 1 gives a network and two types of cascades .
We may not observe all actions performed by the nodes of interest , and hence our cascades may be incomplete , ie , have missing data . For example , Figures 1(d , e ) show cascades when some of the data ( ie , actions of node s ) is missing . The cascades with missing data may no longer have the same properties ( eg , depth , the number of edges ) as the original cascade , and may not even
1
( a ) Network
( b ) Influence cascade Ci
( c ) Network cascade Cn
( d ) Sampled influence cascade C′i
( e ) Sampled network cascade C′n
Figure 1 : Missing data in cascades . ( a ) A social network . ( b ) Influence cascade : we observe edges over which the information propagated . ( c ) Network cascade : we only observe participating nodes , not propagation edges ; edges are inferred from the network based on time order . ( d , e ) Influence and network cascade , respectively , with missing data ( information about node s is missing ) . be connected . Here , we address the problem of estimating properties of a complete cascade C from a small observed part C′ of the complete cascade . Specifically , can we infer properties , like size and depth , of the complete cascade , when data is missing ? There are a number of reasons why cascades may have missing data . Most social networks do not provide full information about their user activity and thus we only observe a subset of users participating in the cascade . For example , Twitter does not provide public access to its full stream of tweets and most Facebook users keep their activity and profiles private . Furthermore , there have been growing concerns about Facebook ’s privacy policy , which indicates that users are generally reluctant to share their data . Finally , full information may not be available because of the costs of collecting it . Overall , the rapid growth of the social networks themselves , the increasing volume of their generated data , and the growing concerns of users over privacy will likely to only exacerbate the problem of missing data over time .
Why estimate properties of complete cascades ? Processes that form cascades in a social network have been studied in a number of domains , including the diffusion of medical and technological innovations [ 23 ] , adoption of strategies in game theoretic settings [ 7 ] , product adoption , promotion , and viral marketing [ 8 , 15 ] . Diffusion and cascades have been studied in the context of Facebook [ 26 ] , Twitter [ 13 ] , Flickr [ 5 ] , blogs [ 18 ] , and email chain letters [ 19 ] . To study diffusion processes underlying the cascades , one needs accurate knowledge of the cascade properties , such as node out degree , in degree , or cascade depth . However , observed properties may differ from the properties of the complete cascade which highly biases inferences about the diffusion processes .
Cascades are also essential for selecting trendsetters for viral marketing [ 22 , 11 ] , finding inoculation targets in epidemiology [ 21 ] , and explaining trends in blogosphere [ 10 ] . Missing data in information cascades can have large effect on these applications . Consider , for example , the problem of influence maximization for viral marketing . The task here is to select a set of most influential nodes in the network where the influence of a node could be the average size of the cascades it creates . However , a cascade can become disconnected with missing data , so the size cannot be reliably estimated . Accordingly , the influence maximization algorithm will perform poorly and the targeted marketing campaign will likely fail .
Related work on missing data . Missing data in networks is a longstanding but relatively poorly
2 understood problem . Related to our work here are the works that study the effects of missing data on measured properties of social networks [ 12 ] and the study of biases when obtaining a graph of the Internet based on measurements [ 14 , 1 ] . Another related line of work is on sampling in large networks [ 16 , 20 , 25 ] , where given a large network we would like to find some procedure to sample a small set of nodes such that important structural properties of the network are preserved .
In terms of the effects of missing data in information cascades prior work is practically nonexistent . The exception is the recent work by Choudhury et al . [ 6 ] that considers the effect of various sampling strategies on the measured properties of diffusion series ( similar notion to cascades ) . While this work tries to find a sampling strategy that least distorts the observed properties , our work here differs . We work under consideration of uniform random sampling , where each node is missing independently with probability 1 − σ . However , we are able not only to both analytically and empirically understand the distortion created by sampling ( ie , missing data ) but also to correct for the distortion ( ie , infer properties of the complete cascade ) . To our knowledge this is the first attempt to analytically understand the distortions under missing data and , more importantly , to correct for them . This is especially challenging as cascades , tree like graphs , are very fragile , easily disconnected even with a small fraction of missing nodes .
Outline . In the following we first propose a k tree model of cascades and derive properties of the resulting cascades , such as size , number of edges , etc . Then , given an observed cascade C′ with missing data , we show how to select a “ proxy ” k tree model that best approximates C′ . The model can then be used to estimate the properties of the complete cascade C . We experimentally show that the properties estimated via a proxy cascade are much closer to the true properties of C than the observed properties on C′ for any sample ratio σ less than 07 Hence , we can effectively correct for missing data . We evaluate our findings on a Twitter social network of 70 million nodes and 2 billion edges . We run our experiments on more than 1 billion tweets . In addition , we also study information diffusion cascades formed on the blogosphere . We show that our methodology can reliably infer structural properties of complete cascades with as much as 90 % of missing data .
2 Problem Statement
We model a social network , over which cascades unfold , as a directed graph G(V , E ) , where nodes V represent entities ( eg , people , web sites , blogs ) and edges E represent directed interactions . For example , in network in Fig 1(a ) , nodes r and s interact with t .
We focus on nodes of G that have performed a particular type of action , eg , joined the “ Fight against cancer ” campaign , participated in an online poll , or bought a camera . The process starts with an initially active node r ( the root ) and the decision to perform an action can be seen as an infection transmitted over the edges of G from a node to node as a result of their interaction . An action sequence A is a sequence of pairs ( s , t ) , one pair for each node t that performed the action of interest , where s influenced t . For example , if s bought a camera under the influence of r , then ( r , s ) appears in the action sequence . The initially active node r is not influenced by anyone , denoted by ( ⊥ , r ) . The order of the pairs in A represent the order in which nodes performed the action of interest . For the scenario so far , we have A = h(⊥ , r ) , ( r , s)i . We assume a node can be influenced by at most one other node , much like a disease is transmitted to a person from a specific individual in epidemic models [ 3 ] . If a node performs the action multiple times , we only consider the first action .
The subgraph of G defined by the influence relations in the action sequence forms an influence cascade Ci . The nodes in Ci are the nodes in the action sequence and an edge ( r , s ) is in Ci if ( r , s ) ∈ A ( since actions only spread along the edges of G then ( r , s ) ∈ E ) . Figure 1(b ) shows one possible influence cascade , where r is the initially active node , which then influenced node s , which
3
Figure 2 : Methodology . in turn influenced nodes v and then t . Note , there is only one root node , which is not influenced by any other nodes – the first node in the action sequence . Influence cascades are trees because nodes cannot repeat in the action sequence and each non root node s has one incoming edge ( from the influencer of s ) .
In many real world scenarios it may be hard to identify the influencer node . We may only observe action sequence pairs of the form ( ∅ , u ) where we know that node u performed an action but do not know which node influenced the action . In this case , we construct a network cascade Cn . The nodes in Cn are the nodes in the action sequence and an edge ( r , s ) is in Cn if ( r , s ) ∈ E and r appears before s in the action sequence . Intuitively , there is an edge between r and s in the network cascade if r performed the action before s and r is connected to s in the social network G .
For example , Figure 1(c ) shows a network cascade . In particular , note that t is now connected to all nodes that could have possibly influenced it . We call the edges that are in the network cascade but not in the influence cascade spurious , eg , edge ( r , t ) is spurious . Since each node may have more than one incoming edge , network cascades are not trees but rather directed acyclic graphs ( DAGs ) .
As discussed in Section 1 , we may not observe the complete action sequence , so we may have missing data in our cascades . In particular , say , we have a sample of the action sequence . Then if we use the sampled action sequence instead of the complete action sequence in the definitions above , we obtain a sampled influence cascade or a sampled network cascade . For example , Figures 1(d ) and ( e ) show the sampled influence and sampled network cascades for the sampled action sequence where information about node s is missing . Note how cascades become disconnected in both cases . In this paper , we assume that missing data is a result of uniform random sampling . Specifically , each node in the complete action sequence is included in the sample at random with probability σ , independent of other nodes . We call σ the sample ratio .
Methodology . Our goal is to obtain a set of properties X of a given complete ( influence or network ) cascade C . For example , size and depth are two such properties . However , we do not have access to the cascade C itself but to a sample C′ . Thus , we can only compute the properties X′ of the sample C′ . Note that the properties X′ can be very different from the properties X of C . For example , in Figure 1 , the depth of the influence cascade is 2 , while the depth of its sample is 0 .
Figure 2 illustrates our approach . To estimate the properties X of C , we first propose a k tree model of cascades . The box labeled “ k tree cascade ” represents a parameterized family of cascades . The samples of these cascades are represented by the box labeled “ sampled k tree cascade . ” We can compute the properties XM of the complete k tree cascade and X′M of the sampled k tree cascade .
4
Figure 3 : k tree cascade with branching factor b = 2 , number of parents k = 2 , and depth h = 3 .
Our strategy now is to find a sampled k tree cascade with properties X′M similar to the properties X′ of the sampled cascade C′ . Then we can approximate the properties X of the complete target cascade C by the properties XM of the complete k tree cascade . We start in Section 3 by defining our k tree model of cascades and analytically derive their important properties . Then in Section 4 , we discuss how to estimate model parameters so that X′M and X′ are similar . Finally , in Section 5 we experimentally show the soundness of our approach . For our evaluation , we need complete target cascades in order to check whether X matches XM . We consider two types of target cascades : ( a ) synthetic cascades obtained from a simulated action propagation process , ( b ) actual cascades obtained from Twitter and blogs . In addition , through our experiments , we show that the k tree model is an effective tool to study the sensitivity of cascade properties to sampling .
3 Cascade Model
Next we introduce k tree model of cascades . The model allows for mathematical analysis of cascade properties without the need for asymptotic analysis . We cannot assume cascades of infinite size or depth as real cascades are rather shallow . Obtaining precise constant factors in the expressions describing cascade properties is essential in order to be able to reconstruct the complete cascade . A k tree Γ(b , h , k ) is generated from a balanced tree of height h and branching factor b . We then augment each node of the tree with k − 1 edges from its k − 1 closest ancestors , starting from its grandparent . Thus nodes have k parents , except for nodes near the root which do not have enough ancestors . Figure 3 shows a k tree with b = 2 , h = 3 , and k = 2 . Original edges of the balanced tree model influence edges while the k − 1 additional edges per node model spurious edges . In Figure 3 , influence edges are darker and spurious edges are lighter . As noted earlier , influence cascades are trees , so we model influence cascades by k trees with k = 1 , equivalent to regular balanced trees . Network cascades , on the other hand , are modeled by k trees with k > 1 , since each node of a network cascade can have more than one parent . Even though cascades are often imbalanced , we will see in Section 5 , that the effect of missing data on cascades can be reliably modeled by assuming they are balanced .
When a k tree cascade has missing data , we refer to it as sampled k tree . We use Γ(p , b , h , k ) to refer to a k tree Γ(b , h , k ) with p fraction of its nodes observed . Each node is included in the sampled k tree with probability p , independently of other nodes .
In what follows , we derive structural properties X1 , . . . , X6 of sampled k trees Γ(p , b , h , k ) as a function of the four parameters p , b , h , and k . Some of the properties we study are important in their own right . Others make it easier to match a k tree cascade to the target cascade , as described in Section 2 . Table 1 provides a reference for the symbols used in the theorems and proofs .
X1 : Number of nodes . We first derive an expression for the expected number of nodes m in a sampled k tree Γ(p , b , h , k ) .
Theorem 1 The expected number of nodes m in a sampled k tree Γ(p , b , h , k ) is p bh+1−1 b−1 .
5
Symbol Description
σ p b h k n m
Fraction of C nodes observed in C′ ( sample ratio ) Probability of observing a node in the k tree model Number of children ( out degree ) via influence edges Height of the tree on influence edges Number of parents ( in degree ) of non root nodes Number of nodes in the complete k tree , n = bh+1−1 b−1 Number of nodes in the sampled k tree , m = p · n
Table 1 : Table of symbols i=0 bi = bh+1−1
Proof : Let n be the number of nodes in the complete k tree . By summing the geometric series b−1 . We know that m = n · p ( expectation of binomial random variable we get n =Ph with parameters ( n , p) ) . Then m = p bh+1−1 b−1 . X2 : Number of edges . Observe that any tree ( ie , k = 1 ) has ( n− 1 ) edges ( one incoming edge per each non root node ) , while any k tree generally has close to k(n − 1 ) edges ( k incoming edges per each non root node ) . More formally , the number of observed edges is given by the following theorem :
Theorem 2 The expected number of edges in a sampled k tree Γ(p , b , h , k ) is equal to : p2 b − 1 , b(1 − bk ) b − 1
+ kbh+1
Proof : Let Zi be the random variable representing the number of nodes at level i and Wi be the random variable representing the number of observed parents of a node at level i . Then the number of edges is equal toPh i=0 E[Zi· Wi ] . Furthermore , since Zi is independent of Wi ( because each node is observed independently of other nodes ) , Ph i=0 E[Zi]E[Wi ] . Since Zi is a binomial random variable with parameters ( bi , p ) and Wi is a binomial random variable with parameters ( min({i , k} ) , p ) , E[Ph i=0 E[Zi · Wi ] = Ph i=0 Zi · Wi ] = p2bi · min({i , k} ) = p2 i=0 Zi·Wi . By linearity of expectation , E[Ph
X3 : Number of isolated nodes . A node becomes isolated if and only if its parents and its children are not observed . Thus , to derive the number of isolated nodes , let ’s first derive the number of children each node has . i=0 Zi·Wi ] =Ph b−1 , b(1−bk ) b−1 + kbh+1 .
Lemma 1 The expected number of children of a node at level i(i ≤ h ) in a sampled k tree Γ(p , b , h , k ) is p bl+1−b b−1 , where l = min(k , h − i ) . j=1 bj = ( bk+1−1 j=1 bj = bh−i+1−b
Proof : Any non leaf node at level i ≤ ( h − k ) has outgoing edges to all of its descendants at the next k levels . Hence , each non leaf node at level i ≤ ( h − k ) has the following number of outgoing edges in the complete tree : Pk b−1 . If i > ( h − k ) , then k > ( h − i ) , and accordingly , the node can only connect to ( h− i ) levels of descendants . Hence , such node will have Ph−i children . Combining both cases , a node at level i in the complete tree has bl+1−b children , where l = min(k , h − i ) . Since each node is included in the sample independently b−1 of other nodes with probability p , the expected number of children in the sampled tree is p bl+1−b b−1 . Now using Lemma 1 , we can derive the expected number of isolated nodes in a sampled k tree : b−1 − 1 ) = bk+1−b b−1
6
Theorem 3 In a sampled k tree Γ(p , b , h , k ) the expected number of isolated nodes is equal to : h bip(1 − p)l+ bc+1−b b−1 i=0 Zi · Wi ] =Ph
Xi=0 where l = min{i , k} and c = min{h − i , k} . Proof : Let Zi be the random variable representing the number of nodes at level i and Wi be the indicator random variable for any node at level i , equal to 1 if all of the node ’s parents and i=0 Zi · Wi and , children are not observed and 0 otherwise . The number of isolated nodes is thenPh by linearity of expectation , E[Ph For a node at level i , the probability that all of its parents are excluded from the sample is ( 1 − p)l where l = min{i , k} . On the other hand , for the same node the probability that all its children are excluded from the sample is given by ( 1 − p ) b−1 where c = min{h − i , k} ( since a node at level i has bc+1−b children ) . Hence , Wi is a Bernoulli random variable with success b−1 probability ( 1 − p)l+ bc+1−b . On the other hand , Zi is a binomial random variable with parameters ( bi , p ) . Noting that Zi and Wi are independent ( because parents and children of a node are at different levels and each node is observed independently of other nodes ) : Ph i=0 E[Zi · Wi ] = Ph i=0 E[Zi]E[Wi ] =Ph
X4 : Number of weakly connected components . A new weakly connected component is formed in a sampled k tree if and only if all parents of a given node are not observed . Hence , the number of weakly connected components of a sampled k tree is equal to the number of roots of such tree , ie , nodes with no incoming edges . i=0 bip(1 − p)l+ bc+1−b i=0 E[Zi · Wi ] . bc+1 −b b−1 b−1
Theorem 4 The expected number of connected components of a sampled k tree Γ(p , b , h , k ) is equal to : p +
( 1 − p)aba − 1 ( 1 − p)b − 1
+( p(1 − p)k bh+1−ba b−1
0 if if h > k h ≤ k where a = min({k , h} ) . Proof : Let Zi be the random variable representing the number of nodes at level i and Wi be the indicator random variable for any node at level i , equal to 1 if all of the node ’s parents are not observed and 0 otherwise . The number of weakly connected components is then Ph i=0 Zi · Wi . By i=0 Zi · Wi ] =Ph linearity of expectation , E[Ph i=0 E[Zi · Wi ] . Furthermore , since Zi is independent of Wi ( because parents of a given node are not among the nodes at the current level and each node is observed independently of other nodes),Ph i=0 E[Zi·Wi ] =Ph i=0 E[Zi]E[Wi ] . Now we know that Zi is a binomial random variable with parameters ( bi , p ) and Wi is a Bernoulli random variable with success probability ( 1 − p)min({k,i} ) . Hence , the number of weakly connected components is in expectation Ph i=0 pbi(1 − p)min({k,i} ) . Simplifying this expression , we obtain : p +
( 1 − p)aba − 1 ( 1 − p)b − 1
+( p(1 − p)k bh+1−ba b−1
0 if if h > k h ≤ k where a = min({k , h} ) X5 : Out degree of a non leaf node . The expected out degree of a non leaf node is equal to : number of edges number of nodes − number of leaves
7
( 1 )
By noticing that a node is a leaf if it has no children we derive the number of leaves in a sampled k tree :
Theorem 5 In a sampled k tree Γ(p , b , h , k ) the expected number of leaves is equal to : bip(1 − p ) bc+1 −b b−1 h
Xi=0 where c = min{h − i , k} . Proof : Let Zi be the random variable representing the number of nodes at level i and Wi be the indicator random variable for any node at level i , equal to 1 if all of the node ’s children are not observed and 0 otherwise . The number of leaves is then Ph i=0 Zi · Wi and , by linearity of expectation , E[Ph i=0 E[Zi · Wi ] . The number of children a node at level i has is in the general form : bc+1−b b−1 where c = min{h − i , k} . Hence , Wi is a Bernoulli random variable with success probability ( 1 − p ) . On the other hand , Zi is a binomial random variable with parameters ( bi , p ) . Since Zi and Wi are independent ( because children of a node are not among the nodes at the current level and each node is observed independently of other nodes ) , we have : i=0 Zi · Wi ] = Ph bc+1−b b−1 i=0 E[Zi]E[Wi ] =Ph
Ph i=0 E[Zi · Wi ] =Ph
Now , using Theorems 1 , 2 , 5 , and assuming independence between the number of nodes , the number of edges , and the number of leaves , we find an approximation for the out degree of non leaves . The approximation for arbitrary k follows from expression ( 1 ) above . However , in the theorem that follows , we consider the case of k = 1 in particular , because it yields an expression for b that does not depend on h , as further discussed in Section 4 . i=0 bip(1 − p ) b−1 bc+1−b
Theorem 6 The expected out degree of a non leaf node in a sampled k tree Γ(p , b , h , k ) for k = 1 is approximately : pb
Proof : Assuming independence between the number of nodes , the number of edges , and the number of leaves , using Theorems 1 , 2 , 5 , the expected out degree of non leaves is equal to :
1 − ( 1 − p)b p2 bh+1−b b−1 p bh+1−1 b−1 − ( p(1 − p)b bh−1 b−1 + pbh )
= pb
1 − ( 1 − p)b
X6 : Average node degree . Assuming independence between the number of nodes and the number of edges , we can derive an approximation for the average node degree : Theorem 7 Average node degree in a sampled k tree Γ(p , b , h , k ) with h ≫ k , is approximately pk .
Proof : The average degree of a node in a k tree can be approximated by making independence assumption between the number of nodes and the number of edges . Then using Theorem 1 and 2 , we get : p2 b−1 + kbh+1If h ≫ k , the above expression approaches pk . to p . b−1 , b(1−bk ) p bh+1−1 b−1
= p( b(1−bk ) b−1 + kbh+1 ) bh+1 − 1
When k = 1 and h ≫ 1 , the theorem above shows that the average node degree is proportional Although expressions in both Theorems 6 and 7 give approximate results , we have experimen tally observed that both expressions are accurate in practice .
8
Figure 4 : Fitting a k tree model . Two alternative k tree models with respect to the observed cascade property .
4 Model Estimation
Recall from Figure 2 that we observe a sample C′ of the target cascade C . We aim to estimate parameters of the sampled k tree Γ(p , b , h , k ) , such that its properties X′M closely resemble the properties X′ of the sampled cascade C′ . The premise is that if X′M matches X′ , then XM will match X . Here we show how we estimate k tree parameters from X′ using the expressions for X1–X6 ( ie , X′M ) we derived in Section 3 . We estimate model parameters in two steps . We first obtain p and then obtain b , h , and k .
Obtaining p . For influence cascades , we use property X6 , and specifically Theorem 7 , to obtain an estimate for p , ˆp . Recall that for k = 1 ( which is always the case for influence cascades ) , average node degree is equal to the fraction of observed nodes p . Accordingly , ˆp equals to the average node degree measured on C′ . For network cascades , on the other hand , we cannot solve analytically for p . Thus , we obtain ˆp by other means . If the sample ratio σ used to obtain C′ is known , ˆp = σ . If σ is unknown , we estimate σ and set ˆp to the estimated σ . For example , if we have multiple cascades C′ , all of which were obtained with the same σ , one can estimate σ as the fraction of cascades where the root node is observed ( granted that we can identify the root of each cascade ) .
Obtaining b , h and k . Setting p to ˆp from the previous step and equating analytical expressions for X1–X4 to the measured X1–X4 on C′ yields a set of equations with 3 unknowns : b , h , k . For example , if m′ is the number of observed nodes ( property X1 ) , then equating the expression for m from Theorem 1 yields the following equation : ˆp bh+1−1 b−1 = m′ .
Since the derived equations are non linear , we cannot directly solve this system . Instead , we solve for the unknowns by finding the set of parameters with the minimum sum of the squares of the errors made in solving every single equation . Specifically , if x′ is the measured value of one property on C′ and x′M is the corresponding value predicted by the model Γ(ˆp , b , h , k ) , then the squared error for that value is ( x′ − x′M )2 . We have experimentally observed that minimizing the errors in this fashion gives poor results . To explain why , consider Figure 4 . The solid curve corresponds to the value x′(σ ) of some property x , eg , the number of nodes in the cascade , as a function of the sample ratio σ . Of course , in reality we do not see this curve ; we only see the value x′(σ∗ ) at the sample ratio σ∗ used to obtain C′ ( and in many cases we do not even know the value of σ∗ ) . Our goal is to estimate the value of x′(σ ) at σ = 1 . The two dashed lines illustrate the value x′M ( p ) of the same property x for two k tree models Γ1 and Γ2 . Even though at p = ˆp model Γ1 fits better than Γ2 ( point B is closer to the solid line than A ) , model Γ2 may be preferred . At σ = 1 point D of Γ2 is closer to E than C of Γ1 .
In order to prefer models like Γ2 over Γ1 , we should look for models that match the x′(σ )
9
Figure 5 : Parameter estimation of k tree model . We subsample the observed cascade to obtain more accurate parameters . curve better – more than just at the point σ = σ∗ . But how can we do this fitting when we do not know the shape of x′(σ ) ? We can discover other points on the x′(σ ) curve by further subsampling the sampled cascade C′ . Say we re sample the cascade C′ with rate α ( 0 < α ≤ 1 ) and evaluate the properties . The effect is the same as if we had sampled the original cascade C with sample rate α·σ∗ . Thus by using multiple values of α we obtain multiple points along the x′(σ ) curve . Each α yields a new error term of the form : ( x′(ασ∗ ) − x′M )2 , where x′M is the value predicted by a model when p = α · ˆp . Minimizing the sum of the error terms for all α values , we fit the model not only at a single point σ∗ , but along the whole interval ( 0 , σ∗ ) . We found it best to generate several samples at the same α , and then average the measured x′ values .
Figure 5 summarizes the parameter estimation procedure : 1 . Subsample observed cascade C′ for multiple values of α ∈ ( 0 , 1 ] . For each α , generate multiple subsamples of C′ and average measured properties X1 through X4 . 2 . For each α ( and for α = 1 ) , generate an error term : the squared difference between the measured ( averaged ) value and the value predicted by the k tree model ( function of αˆp , b , h , and k ) .
3 . Apply the least squares method ( we use grid based search ) to find parameters ˆb , ˆk , ˆh that minimize the sum of the errors .
Influence cascades . In degree of non roots in influence cascades is always 1 . So when estimating parameters for influence cascades , k is explicitly set to 1 and we only have to solve for b and h .
Furthermore , we found that better k tree models can be found by first solving for b using X5 . Specifically , we measure the out degree of non leaves on C′ , say it is x′ , and equate it to the expression from Theorem 6 to obtain : ˆpb/(1 − ( 1 − ˆp)b ) = x′ . Then we solve this equation for b numerically using bisection . Having found ˆb estimate this way , we then estimate h using X1–X4 and subsampling as described above ( X1 alone would also suffice ) .
Integer valued vs . real valued parameters . Our k tree model assumes integer values for all three parameters . However , in real cascades nodes have varying branching factor b and in degree k , and different leaf nodes are at different heights . Hence , we allow real valued parameters for k trees .
10
Real valued parameters have natural interpretation in our k tree model . Real valued b can be interpreted as an average number of direct children ( not counting children attached via spurious edges ) , eg , if b is 2.5 , half of the nodes have 2 children and half of the nodes have 3 children . Similarly , real valued k is interpreted as an average in degree of non roots , eg if k is 2.5 , half of the nodes have connections from 2 closest ancestors while half have connections from 3 closest ancestors . Finally , modulo of h can be interpreted as the fraction of nodes with children at level ⌊h⌋ , eg if h is 3.5 , half of the nodes at level 3 have children , while half do not . However , the expressions for X1–X5 do not allow non integer values for h and k ( because bounds of summations need to be integer valued ) . To address this , we linearly interpolate the function value between two integer values . For example , if y0 = f ( x0 ) for integer x0 and y1 = f ( x0 + 1 ) , then the value of f ( x ) for x ∈ [ x0 , x0 + 1 ] is y0 + ( x − x0)(y1 − y0 ) . In our case we linearly interpolate between f ( h , k ) and f ( h + 1 , k + 1 ) in 2 dimensions .
5 Experiments
In this section , we evaluate our cascade model and our method for correcting for missing data in the cascades . We first evaluate whether k tree cascades’ properties are affected by the missing data in the same way as the properties of the target cascades . In other words , is there a k tree for each target cascade such that the properties of the target cascade are similar to the properties of the k tree at each sample ratio ? Next , we evaluate the soundness of the method . Specifically , do the properties XM of the complete k tree parameterized based on C′ match the properties X of the complete target cascade C ? Finally , we study the parameters of the model itself . b , k , and h can themselves be viewed as properties of the original cascade . Hence , we look at how well k tree parameters match the corresponding properties of the target cascade . If k tree parameters match the corresponding cascade properties then each parameter indeed has an intuitive meaning .
5.1 Experimental Setup
For our evaluation , we need complete target cascades . We consider two types of target cascades : ( a ) synthetic cascades generated on real and synthetic networks , ( b ) actual cascades obtained from Twitter and blogs which we refer to as real cascades . Each complete cascade , in our experiments , can actually be sampled at several ratios , not just at one ratio σ∗ , as it is the case for the target cascade of Figure 2 . Hence , in this section we refer to the sample ratio of the observed cascade using variable σ , and not σ∗ .
Synthetic cascades . Synthetic cascades are generated using an action propagation model simulated on a given network . The model takes as input a network and action sequence size , set to 127 in our simulations , and generates an action sequence which specifies influences . We use both synthetic networks and the real network of Twitter users to simulate our action propagation model . We use a variant of the Susceptible Infected ( SI ) model [ 3 ] , commonly used in studies of virus and information diffusion . Here are the steps of the simulation . First , we select at random a root node r with non zero out degree . Then , r is added to the initially empty list of infected nodes I and all of its outgoing edges ( r , s ) are added to the initially empty FIFO queue of infected susceptible node pairs S . After that , we repeat the following steps until 127 elements of the action sequence A are produced :
1 . Remove the first pair ( r , s ) from the queue S . 2 . With probability β , output an action sequence element ( r , s ) . Then add s to I and add all edges ( s , u ) where u /∈ I to S . Otherwise ( ie , with the remaining probability 1 − β ) , push ( r , s ) back into S .
11
Our action propagation model requires a network as input . We used two types of networks : synthetic networks and real social network of “ who follows whom ” of Twitter .
Synthetic networks were generated using three network models : Erd˝os R´enyi random graph , Scale Free random graph and Forest Fire model [ 17 ] . All networks were generated with 106 nodes . Erd˝os R´enyi graph was generated with an average degree of 10 , Forest fire network was generated with parameters pf = 0.36 and pb = 0.315 ( yielding average degree of 10 ) . And Scale Free graph was generated with power law degree distribution exponent α = 2.0 ( roughly corresponding to the power law degree exponent of the Twitter network ) .
The Twitter “ who follows whom ” network was collected via the Twitter API in a breadth first manner from June through December 2009 with the set of seed user IDs taken from the public stream of tweets ( Twitter status updates ) monitored in that period . In other words , for every user u for which we observed a tweet , we collected friends of u followed by their friends of friends , etc . in a breadth first manner . The network we obtained has 71,804,410 nodes and 2,040,072,198 directed edges ( average degree of 28.4 ) , where each edge corresponds to the “ follows ” relationship among users ( if A follows B , A receives B ’s tweets ) .
We also consider real cascades that are constructed from action sequences extracted from traces of human activity . We use Twitter retweets as natural action sequences that when combined with the Twitter network form network cascades . In addition , we also use the action sequences of link creation between blog posts that naturally form influence cascades .
Retweet Cascades . Tweets are Twitter status update messages and retweets are re postings of the previously posted tweets . We focus on how a given URL x propagates through the Twitter social network by people reposting ( ie , forwarding ) the original Tweet .
We took a complete set of tweets , collected by Topsy , for the most popular URLs posted on Twitter between June and December 2009 . From the set of tweets with URLs , we then extracted retweets . If user u posts a tweet with URL x , any tweet of the form “ RT @u t ” , where t contains URL x , is a retweet of x . For example , suppose user A posts a tweet with URL x , then user B who follows A posts “ RT @A x ” and another user C who follows B ( but may not follow A ) posts : “ RT @A x ” . This sequence of tweets forms an action sequence h(⊥ , A ) , ( ∅ , B ) , ( ∅ , C ) i . This action sequence , combined with the network of who follows whom , can then be used to construct a network cascade ( if a node retweets more than once , we consider only the first retweet ) .
Note that there is no way to tell which node influenced which other node from retweets . Using the same example , if another node D retweets A ’s x and D follows both B and C ( but does not follow A ) , then both B and C could have influenced D . Thus to obtain influence cascades from retweet network cascades , we select a single incoming edge for each node giving credit to the last neighbor to retweet . In our example , if C retweeted after B , we say ( C , D ) .
Although we had to drop some of the tweets due to changed and deleted usernames , our final cascades were nearly 95 % complete with the experiments performed on their largest connected components . We considered only cascades of more than 100 nodes with the total of 250 such cascades .
Blog Cascades . Blog posts and links in them to other blog posts provide action sequences with explicit influence relations . Specifically , each post with no outgoing links starts an action sequence . Suppose blog A makes a post a and blog B links to post a in one of its posts b , then we can infer action sequence ( A , B ) . If blog C then makes a post c linking to b , our action sequence becomes h(⊥ , A ) , ( A , B ) , ( B , C)i . Accordingly , we can then construct an influence cascade . If C has links to say both x and y , we arbitrarily pick one of the links . In our dataset , blog posts linking to more than one post in the same cascade were extremely rare ( less than 0.01% ) , validating our model of influence cascades as trees .
For our experiments , we extracted influence cascades from the set of blog posts collected by Spinn3r between August and November 2008 . This data set includes essentially a complete
12 snapshot of the English blogosphere . We considered only cascades of 100 and more nodes with the total of 100 such cascades . We did not consider network cascades in the context of blogs because there is no explicit blog network ( although implicitly created blog networks have been studied before [ 18] ) .
5.2 Soundness of the k tree Model
One of the assumptions underlying our methodology is that sampling ( ie , missing data ) has the same effect on k trees as on target cascades . We test this assumption to validate our method and demonstrate that k tree is a useful model for real cascades .
In this experiment we work with complete target cascades so σ = 1 ( ie , C′ = C ) . We sample each target cascade at rates α = 0.1 , 0.2 , . . . , 1.0 and measure properties X1–X4 at each rate . Then we estimate a k tree model Γ(ˆb , ˆh , ˆk ) as discussed in Section 2 . Estimated parameters ˆb , ˆh , ˆk are the same for all four properties .
Number of Nodes 140
Number of edges 300
Number of isolated nodes Number of components
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
) 1 X
( s e d o N f o r e b m u N
) 1 X
( s e d o N f o r e b m u N
) 1 X
( s e d o N f o r e b m u N
120
100
80
60
40
20
0
110 100 90 80 70 60 50 40 30 20 10 0
550 500 450 400 350 300 250 200 150 100 50 0
α
α
α
) 2 X
( s e g d E f o r e b m u N
) 2 X
( s e g d E f o r e b m u N
) 2 X
( s e g d E f o r e b m u N
250
200
150
100
50
0
120
100
80
60
40
20
0
1000 900 800 700 600 500 400 300 200 100 0
α
α
α
) 3 X
( s e d o N d e a o s I f t l o r e b m u N
) 3 X
( s e d o N d e t l a o s I f o r e b m u N
) 3 X
( s e d o N d e t l a o s I f o r e b m u N
30
25
20
15
10
5
0
1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0
120
100
80
60
40
20
0
α
α
α
) 4 X
( s t n e n o p m o C f o r e b m u N
) 4 X
( s t n e n o p m o C f o r e b m u N
) 4 X
( s t n e n o p m o C f o r e b m u N
35
30
25
20
15
10
5
0
1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 0.1
140
120
100
80
60
40
20
0
α
α
α
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Network Cascade
Influence Cascade
Network Cascade ode
Influence Cascade del
 
Figure 6 : X1 X4 properties on estimated k tree and observed cascades . First row : synthetic cascades on Twitter network . Second row : a single blog cascade . Third row : a single Twitter retweet cascade . Error bars correspond to 95 % confidence interval . Note agreement between the properties of the target cascade ( solid line ) and the properties of the k tree cascade ( dashed line ) as we vary the fraction of missing data .
Figure 6 shows a grid with 12 graphs . The rows correspond to synthetic cascades on the Twitter network , a single blog cascade , and a single retweet cascade1 , respectively . Each column of the grid corresponds to one of the X1–X4 properties . Consider the bottom rightmost graph for a single retweet network cascade and its corresponding influence cascade . The dark dashed curve shows X4 ( number of weakly connected components ) measured on the network cascade as a function of subsample rate α . The dark solid curve shows analytically calculated X4 for the
1We estimate model parameters for each real cascade individually , so we are showing results for only one cascade as an example for blogs and retweets .
13 t n e n o p m o C t s e g r a L f o e z S i t n e n o p m o C t s e g r a L f o e z S i
1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1
1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1
α
α t n e n o p m o C t s e g r a L f o e z S i t n e n o p m o C t s e g r a L f o e z S i
1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1
1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1
α
α t n e n o p m o C t s e g r a L f o e z S i t n e n o p m o C t s e g r a L f o e z S i
1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1
1.2
1
0.8
0.6
0.4
0.2
0
0.2
α
α network cascade ’s k tree model as a function of α . Similarly , the light dashed curve shows X4 measured on the influence cascade and the light solid curve shows analytically calculated X4 for its k tree model . Observe that the model predicts the actual values fairly well . The same close fits can be observed in the second row of Figure 6 , which shows X1 X4 for a single blog influence cascade . In blogs , we can only observe influence cascades , so we do not show network fits in this row .
Now consider the top row . Here , for the target influence and network cascades , each measured property value at each sample rate is an average across 1000 synthetic cascades simulated on the Twitter network ( all of the same size ) . Accordingly , the k tree models for both influence and network cascades are fitted to the average of measured values . Again , the fits are fairly close .
Note that the model curves in each row correspond to the same k tree ( estimated for either network or influence target cascade ) , so the curves in each graph were not fitted individually . Yet the analytical model values match closely the measured values for all the properties . The same close fit for all properties was observed for synthetic cascades simulated on synthetic graphs , as shown in Appendix A4
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Network Cascade
Influence Cascade
Network Cascade ode
Influence Cascade del
Figure 7 : Size of the largest connected component divided by the cascade size on estimated k tree and observed cascades . First row : synthetic cascades on Erdos Renyi , Forest Fire , and Scale Free networks . Second row : synthetic cascades on Twitter network , a single blog cascade and a single Twitter retweet cascade . Error bars correspond to 95 % confidence interval .
Finally , we performed the same experiment with cascade properties that are not explicitly fitted . Specifically , consider Figure 7 , which shows the size of the largest weakly connected component ( divided by the cascade size ) measured on synthetic and real cascades and their k tree models . Here , solid lines are not analytical expressions , but rather empirically measured values on a k tree model . We estimate k tree parameters from X1–X4 , generate a k tree cascade with these parameters , and sample it at different sample rates . The solid curve is then extrapolated from the size of the largest connected component measured on each generated sample . Observe that the k tree model ( estimated on X1–X4 ) yields relatively good fits for the observed size of the largest connected component . In the Appendices A.2 and A.3 , we also derive tight analytical bounds on the size of the largest connected component .
14
Overall , we conclude that for every target cascade there is a k tree model such that their properties are alike at each sample rate .
5.3 Estimating Target Cascade Properties
Next , we evaluate to what degree do the properties of the estimated k tree model approximate the properties of the complete target cascade .
We reconstruct the following cascade properties : ( 1 ) number of nodes , ( 2 ) number of edges , ( 3 ) width , which is defined as the maximum number of nodes at any depth/level [ 6 ] , and ( 4 ) participation , which is defined as the number of non leaf nodes [ 6 ] .
1
0.8
0.6
0.4
0.2 s e d o N
, r o r r
E e v i t l a e R
Observed Estimated
1
0.8
0.6
0.4
0.2 s e d o N
, r o r r
E e v i t l a e R
Observed Estimated
1
0.8
0.6
0.4
0.2 s e d o N
, r o r r
E e v i t l a e R
Observed Estimated
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Figure 8 : Observed vs . estimated properties . First column : blog influence cascades . Second column : retweet influence cascades . Third column : retweet network cascades . All errors are averaged over a set of cascades , error bars correspond to 95 % confidence interval .
15
0
0
0
0
Observed Estimated
1
0.8
0.6
0.4
0.2
Observed Estimated
1
0.8
0.6
0.4
0.2
Observed Estimated
1
0.8
0.6
0.4
0.2
σ
σ
σ
σ s e g d E
, r o r r
E e v i t l a e R n o i t i a p c i t r a P
, r o r r
E e v i t l a e R t h d W i
, r o r r
E e v i t l a e R
Observed Estimated
1
0.8
0.6
0.4
0.2
Observed Estimated
1
0.8
0.6
0.4
0.2
Observed Estimated
0
0
0
1.4 1.2 1 0.8 0.6 0.4 0.2 0 s e g d E
, r o r r
E e v i t l a e R n o i t i a p c i t r a P
, r o r r
E e v i t l a e R t h d W i
, r o r r
E e v i t l a e R
σ
σ
σ
σ
0
0
0
0
Observed Estimated
1
0.8
0.6
0.4
0.2
Observed Estimated
1
0.8
0.6
0.4
0.2
Observed Estimated
1
0.8
0.6
0.4
0.2
σ
σ
σ
σ s e g d E
, r o r r
E e v i t l a e R n o i t i a p c i t r a P
, r o r r
E e v i t l a e R t h d W i
, r o r r
E e v i t l a e R
Figure 8 shows a grid of 12 plots . The top row corresponds to blog cascades , the middle row corresponds to retweet influence cascades , and the bottom row corresponds to retweet network cascades . Each column corresponds to one of the four properties . The results are averaged across 250 retweet and 100 blog cascades ( the results for synthetic cascades are similar to those presented here ) .
For example , consider the top leftmost graph for the number of nodes measured on blog cascades . Say x is the number of nodes in C , x′(σ ) is the number of nodes in C′ obtained with sample ratio σ and xM is the number of nodes in the k tree model estimated from C′ . The relative error of the k tree model estimate at sample ratio σ is then ˆe = |xM−x| , shown by a dark curve in the plots ( k tree model may differ at each σ ) . Similarly , the relative error of the observed value at sample ratio σ is e′ = |x′(σ)−x| , shown by a light curve in the plots . As expected , the light curve is a straight line because the observed cascade size , and its relative error , is linear with σ . x x
In all of the plots of Figure 8 , except for the width on retweet influence cascades , the error of the estimated properties is better than the error of the observed properties for almost all σ values . In general , with 70 % or less of the target cascade C , our method provides a significantly better estimate of C ’s properties than what is observed on C′ . However , for σ > 0.9 , our method does worse than working with C′ directly and ignoring the missing data . This suggests that if estimated sample ratio ˆp is high , one is better off measuring properties on C′ directly . But , of course , most properties are not perturbed by missing data at such such high σ values and one would not bother to correct for missing data in such case . Distortions due to missing data become a bigger issue at lower σ values and this is where our method is most effective and significantly outperforms measurements made on C′ . 20 30 % relative error is especially encouraging for such low values of σ as 01 As seen in the rightmost plot of the second row of Figure 8 , our method performs poorly on the width of retweet influence cascades . We found that while blog influence cascades are mostly shallow balanced trees ( star shaped ) , retweet influence cascades resemble more imbalanced trees , possibly because they were artificially generated from the network cascades . This is the reason we believe we are unable to estimate well the width of retweet influence cascades .
Finally , observe that while the number of nodes and edges are explicitly fitted during parameter estimation , participation and width are not fitted . Yet , our model predicts these properties fairly well . Similar results , demonstrated in Appendix A.5 , were observed for synthetic cascades .
5.4 Estimated vs . Observed Parameters Parameters of the k tree model can themselves be viewed as cascade properties . For example , parameter k naturally maps to the average node in degree in a cascade . We next evaluate how well the model parameters match the corresponding cascade properties .
As described in Section 3 , k accounts for the spurious edges ( k − 1 spurious edges per node ) , while b and h are branching factor and height of the tree without the spurious edges . A network cascade is essentially an influence cascade with spurious edges . Accordingly , if a network cascade has a corresponding influence cascade ( which is always the case , given our experimental settings ) , p , b and h values must be the same for both cascades . Because k is trivially 1 for influence cascades , the estimated parameter k is specific to a network cascade .
Now let ’s define how model parameters map to cascade properties . p corresponds to the sample ratio σ of C′ , so we say the true value of p is σ . For b , h , and k , the true value of each will be its corresponding property value measured on C . The true value of b is the average out degree of non leaves in the influence cascade . The true value of h is the average weighted depth over all leaves of the influence cascade . The weight of each leaf is the number of its descendants at the max level of the tree had the tree been balanced ( consistent with real valued h in Section 4 ) . Finally , the true value of k is the average in degree of non roots . Recall that we only consider k for network cascades .
16
Network Cascade
Influence Cascade estimated error ( ˆe ) observed error ( e′ ) estimated error ( ˆe ) observed error ( e′ ) p b k h
–
0.03 0.14 0.00
–
0.29 0.21 0.39
0.02 0.03
–
0.00
–
0.32
–
0.46
Table 2 : Relative errors for estimated and observed parameters averaged over synthetic cascades on Twitter network , σ∗ = 0.5
To have a baseline when comparing estimated parameters to the true ones , we also measure properties corresponding to b , h , and k on C′ . We will refer to these measured properties as to observed parameters . Observed parameters are defined similarly to the true ones but unlike the true parameters these are measured on C′ as opposed to C 2 . For example , observed b is defined as the average out degree of non leaves measured on C′ . In this experiment , the goal is to compare estimated model parameters to the true parameters . As a baseline , we also compare the observed parameters to the true parameters . To make comparison direct , as in the previous experiment , we use relative errors ˆe and e′ . For example , if b∗ is the true value of branching factor , ˆb is its estimate , and b′(σ ) is the observed value at sample ratio σ , then ˆe = |ˆb−b∗| Performance on synthetic cascades . We generated synthetic cascades on synthetic networks and Twitter network . We show results here only for Twitter network ; the results for other networks are similar to the ones we present here . Synthetic cascades were generated with 127 nodes each with a total of 1000 cascades simulated on each network . and e′ = |b′(σ)−b∗| b∗
. b∗
Table 2 shows results averaged over all simulations on Twitter network with sampled cascades C′ at σ = 05 Each row corresponds to one of the four parameters . There are two sets of columns , one for the network cascades and one for the influence cascades . Each set has two columns : the left one corresponds to the estimated error ˆe ( with respect to one of the four parameters ) , the right one corresponds to the observed error e′ . Note that we are not showing the observed error for p because this parameter cannot be directly measured from observed data . Also we cannot analytically estimate p for network cascades , so we omit values for p ’s estimated error for network cascades .
Observe that the errors of the estimated parameters are very low : almost all below 5 % ( similar low errors were observed for real cascades and synthetic cascades on other networks ; see Appendix A.6 for details ) . Contrast them with 20 40 % errors for observed parameters . Finally , note how accurately we are able to infer p for influence cascades . This demonstrates the effectiveness of X6 as an estimate of σ .
Recall that both influence and network cascades , in our setting , are constructed from the same action sequence . Accordingly , b and h are the same for the network and influence cascades , as discussed above . Interestingly enough , although model parameters for network and influence cascades are estimated independently , the estimated errors for b and h are low in both cases . Hence , similar b and h values are predicted for both influence and network cascades . This is yet another strength of our method : we are able to detect close similarity between the equivalent network and influence cascades .
2We cannot assume to have access to both influence and network cascades , so the parameters are measured on whatever C ′ we are given ( either network or influence ) .
17
Spurious Edges estimated error ( ˆe ) observed error ( e′ )
No Spurious Edges observed error ( e′ ) estimated error ( ˆe ) p b k h
–
0.03 0.02 0.05
–
0.86 0.43 0.66
0.02 0.02
–
0.10
–
0.31
–
0.43
Table 3 : Relative errors for estimated and observed parameters for k trees with 127 nodes , b ∼ Normal(2 , 1 ) , k = 3.5 , σ∗ = 0.5
Robustness of parameter estimation . Finally , we test the robustness of parameter estimation . By generating and sampling a k tree cascade with constant integer b , h , k parameters and σ sample ratio , we experimentally verified that we can match all parameters precisely ( ˆe = 0 for all parameters ) . So we added variance to parameter b and used real valued k . Specifically , we generated cascades with 127 nodes , b ∼ Gaussian(2 , 1 ) and k = 3.5 ( since b is stochastic , the actual height varied ) . All cascades C were generated 1000 times and sampled to obtain C′ with σ = 05 Table 3 shows the results . The table is similar to Table 2 with the two sets of columns referring to cascades with and without the spurious edges instead of network and influence cascades , respectively . Although we added variance to the branching factor and used a real valued k , the estimated parameters have a very small error with respect to the true values . This result demonstrates the robustness of our parameter estimation .
6 Discussion and Conclusion
In this paper , we addressed the problem of estimating properties of a target cascade C , given only its fraction C′ obtained by uniform sampling of C nodes . This is the first attempt to our knowledge to analytically study the effect of missing data on cascade properties and , most importantly , the first attempt to correct for missing data in cascades . In summary , our contributions are as follows : • Proposed an analytical k tree model of cascades and rigorously derived a number of their important properties .
• Experimentally showed that the k tree model is an effective proxy to study the effect of missing
• Proposed a method that , given a cascade model ( k tree , in our case ) , estimates properties of C data on the observed properties of C′ . given C′ . • Experimentally demonstrated that the estimated properties of C using our method are significantly more accurate than the observed properties on C′ ( effectively correcting for the property distortions due to missing data in C′ ) .
• Experimentally showed that the k tree model parameters have an intuitive meaning : they roughly correspond to the properties of the target cascade C . Our methodology for estimating cascade properties in the face of missing data is a practical necessity . For instance , Twitter provides public access to less than 10 % of its stream of tweets , whereas Facebook users are becoming more concerned about privacy of their data . Current algorithms and methods for designing viral marketing campaigns or studying information diffusion
18 processes assume access to complete cascade data . These algorithms fail miserably given incomplete data and , thus , distorted cascade properties . The method we propose can address these issues . Many cascade properties important for influence maximization such as cascade size and node participation can be recovered using our framework . Likewise , we can recover many properties important for studying information diffusion such as node out degree , in degree , and cascade width .
There are a number of future directions for this work . The k tree model for cascades , although simple and thus relatively easy to analyze , may not work for all types of cascades . As we have seen for retweet influence cascades , cascade trees which are severely imbalanced , may create challenges for our model . Hence , we may need more sophisticated models of cascades , possibly stochastic in nature based on Galton Watson trees [ 27 ] ( see Appendix A1 ) Given a different model , however , our method to correct for missing data could still be applied . Finally , in this paper we worked in the regime of sampled action sequences but complete knowledge of the underlying network . One interesting venue for future work could be studying the effect of missing data in both action sequences and the network .
Acknowledgments We thank Topsy and Spinn3r for providing us with the data that facilitated the research .
References
[ 1 ] D . Achlioptas , A . Clauset , D . Kempe , and C . Moore . On the bias of traceroute sampling : Or , power law degree distributions in regular graphs . JACM , volume 56 , pages 1–28 , 2009 .
[ 2 ] T . Aven . Upper ( lower ) bounds on the mean of the maximum ( minimum ) of a number of random variables . Journal of Applied Probability 22 , pages 723–728 , 1985 .
[ 3 ] N . Bailey . The Mathematical Theory of Infectious Diseases . Griffin , London , 1975 .
[ 4 ] S . Bikhchandani , D . Hirshleifer , and I . Welch . A theory of fads , fashion , custom , and cultural change in informational cascades . J . of Polit . Econ . , 100(5):992–1026 , October 1992 .
[ 5 ] M . Cha , A . Mislove , and K . P . Gummadi . A measurement driven analysis of information propagation in the flickr social network . In WWW ’09 , pages 721–730 , 2009 .
[ 6 ] M . de Choudhury , Y R Lin , H . Sundaram , K . S . Candan , L . Xie , and A . Kelliher . How does the data sampling strategy impact the discovery of information diffusion in social media ? In ICWSM ’10 , 2010 .
[ 7 ] G . Ellison . Learning , local interaction , and coordination . Econometrica , 61(5):1047–71 ,
September 1993 .
[ 8 ] J . Goldenberg , B . Libai , and E . Muller . Talk of the network : A complex systems look at the underlying process of word of mouth . Marketing Letters , 3(12):211–223 , 2001 .
[ 9 ] M . S . Granovetter . Threshold models of collective behavior . American Journal of Sociology ,
83(6):1420–1443 , 1978 .
[ 10 ] D . Gruhl , R . Guha , D . Liben Nowell , and A . Tomkins .
Information diffusion through blogspace . In WWW ’04 , 2004 .
[ 11 ] D . Kempe , J . M . Kleinberg , and E . Tardos . Maximizing the spread of influence through a social network . In KDD ’03 , pages 137–146 , 2003 .
19
[ 12 ] G . Kossinets . Effects of missing data in social networks . Social Networks , 28:247–268 , 2006 .
[ 13 ] H . Kwak , C . Lee , H . Park , and S . Moon . What is Twitter , a Social Network or a News Media ?
In WWW’10 , April 2010 .
[ 14 ] A . Lakhina , J . W . Byers , M . Crovella , and P . Xie . Sampling biases in ip topology measure ments . In INFOCOM , 2003 .
[ 15 ] J . Leskovec , L . A . Adamic , and B . A . Huberman . The dynamics of viral marketing . ACM
TWEB , 1(1):2 , 2007 .
[ 16 ] J . Leskovec and C . Faloutsos . Sampling from large graphs . In KDD ’06 , pages 631–636 , 2006 .
[ 17 ] J . Leskovec , J . Kleinberg , and C . Faloutsos . Graph evolution : Densification and shrinking diameters . ACM TKDD , 2007 .
[ 18 ] J . Leskovec , M . McGlohon , C . Faloutsos , N . Glance , and M . Hurst . Cascading behavior in large blog graphs . In SDM ’07 : SIAM Conference on Data Mining , 2007 .
[ 19 ] D . Liben Nowell and J . Kleinberg . Tracing information flow on a global scale using Internet chain letter data . PNAS , 105(12):4633–4638 , 2008 .
[ 20 ] A . Maiya and T . Berger Wolf . Sampling community structure . In WWW ’10 , 2010 .
[ 21 ] M . E . J . Newman . The spread of epidemic disease on networks . Phys . Rev . E , 66:016128 ,
2002 .
[ 22 ] M . Richardson and P . Domingos . Mining knowledge sharing sites for viral marketing . In KDD
’02 , pages 61–70 , 2002 .
[ 23 ] E . M . Rogers . Diffusion of Innovations . Free Press , New York , fourth edition , 1995 .
[ 24 ] S . Ross . A First Course in Probability . 7th ed . , Prentice Hall , 2006 .
[ 25 ] D . Stutzbach , R . Rejaie , N . G . Duffield , S . Sen , and W . Willinger . Sampling techniques for large , dynamic graphs . In INFOCOM , 2006 .
[ 26 ] E . Sun , I . Rosenn , C . Marlow , and T . Lento . Gesundheit! modeling contagion through facebook news feed . In ICWSM ’09 , 2009 .
[ 27 ] H . W . Watson and F . Galton . On the probability of extinction of families . Journal of the
Anthropological Institute of Great Britain and Ireland , ( 4):138–144 , 1875 .
20
A Appendix
A.1 k Trees in Relation to Galton Watson Trees
We start by observing that each connected component of a sampled influence cascade of depth h is a Galton Watson ( GW ) tree of height h . GW trees have been extensively studied in the theory of Branching Processes [ 27 ] and have found applications in evolutionary genetics and epidemiology . Each GW tree is generated by starting from the root node and generating the number of children of each node independently of other nodes , distributed per some random variable Y ( in GW literature , usually referred to as Z1 ) . It is conventional to denote the number of nodes in level i of such tree as Zi . The tree grows infinitely long if Zn > 0 for all n > 0 or stops at the first level n such that Zn = 0 . It has been proven that if the mean of Y , ie number of each node ’s children , is less than 1 , the tree dies out , and if the mean of Y > 1 , the tree grows infinitely long .
We assume that each influence cascade is a balanced tree with some constant branching factor b . Such balanced tree influence cascades are essentially equivalent to GW trees with Y ∼ Gaussian(b , 0 ) . Then if we sample such trees , ie obtain their corresponding sampled influence cascades , each of their connected components can be viewed as GW tree with Y ∼ Binomial(b , p ) . Indeed , the number of children of each node can be viewed as b independent Bernoulli trials with success probability p .
A.2 Size of the largest connected component in a balanced tree ( k = 1 )
Xh = Ph
Let Xh be a random variable representing the size of the weakly connected component formed by the root of a tree of height h , conditioned on the root appearing in the sample . Following the standard literature on Galton Watson trees , let Zi denote the number of nodes at level i in the connected component formed by the root ( number of individuals in ith generation ) . Then i=0 Zi . Let Y be a random variable representing the number of children each node has . Since we assume that a tree is balanced with a constant branching factor b and each node is included in the sample independently with probability p , Y is a binomial random variable with parameters ( b , p ) . Note that Y does not depend on the tree level , hence , is independent of any Zi ’s or Xh . Therefore , Zi = Y Zi−1 Lemma 2 The expected number of nodes at level i ( i ≤ h ) in the weakly connected component formed by the root of the tree is E[Zi ] = ( bp)i
Proof : By induction : Base case : the expected number of nodes at level 0 ( root ) is E[Z0 ] = 1 by definition as we are conditioning on the root being present in the sample . Moreover , in expectation the root will have ( bp ) children . Equivalently , E[Z1 ] = E[Y ] = bp Inductive hypothesis : E[Zn ] = ( bp)n Inductive step : E[Zn+1 ] = E[Y Zn ] as noted above . As Y is independent of Zn , then E[Y Zn ] = E[Y ]E[Zn ] . Thus , E[Zn+1 = ( bp)(bp)n = ( bp)n+1 .
Theorem 8 The expected size of the weakly connected component formed by the root of a tree of height h , E[Xh ] , given that the root is included in the sample , is E[Xh ] = ( pb)h+1−1
. pb−1 i=0 E[Xi ] . By Lemma 2 , we know that E[Zi ] = ( bp)i . Thus ,
Now , in general , any node at level i of the original tree is a root of a subtree of height ( h − i ) . Hence , if a node at level i forms a new weakly connected component , the expected size of its component is given by ( pb)h−i+1−1
.
Proof : E[Xh ] = E[Ph E[Xh ] =Ph i=0(bp)i = ( pb)h+1−1 i=0 Zi ] = Ph
. pb−1 pb−1
21
Corollary 1 The size of the weakly connected component formed by the root of a tree of height h , as a fraction of m , is Θ(ph ) for large h .
Proof : From Theorem 8 , as h → ∞ , the expected size of the weakly connected component as a fraction of m :
F = lim h→∞
( pb)h+1−1 p bh+1−1 pb−1 b−1
= lim h→∞
( b − 1).(pb)h+1 − 1fi p(pb − 1)(bh+1 − 1
= b − 1 p(pb − 1 ) lim h→∞
( pb)h+1 − 1 bh+1 − 1
Applying L’Hopital ’s rule we obtain :
F = b − 1 p(pb − 1 ) lim h→∞
( ln pb)(pb)h+1 ln b · bh+1 →
( ln pb)(b − 1 ) ln b · ( pb − 1 ) ph lim h→∞
Lemma 3 The variance of the number of nodes at level i in the weakly connected component formed by the root is V ar[Zi ] = ( bp)i(1 − p ) ( bp)i−1 bp−1 .
Proof : By prove this by induction on i .
Base case : the variance the number of nodes at level 0 ( root ) is V ar[Z0 ] = 0 by definition as we are conditioning on the root being present in the sample . Moreover , V ar[Z1 ] = V ar[Y ] = bp(1− p )
Inductive hypothesis : V ar[Zn ] = ( bp)n(1 − p ) ( bp)n−1 bp−1 Inductive step : By the Law of total variance [ 24 ] ( conditional variance ) V ar[Zn+1 ] = E[V ar[Zn+1|Zn]]+ As noted above , Zn+1 = Y Zn =PZn
V ar[E[Zn+1|Zn] ] . i=1 Y . Thus ,
Zn
V ar[Zn+1 ] = E[V ar[
Y |Zn ] ] + V ar[E[Y Zn|Zn ] ]
Xi=1
V ar[Zn+1 ] = E[
Zn
V ar[Y ] ] + V ar[E[Y ]Zn ] = E[Zn]V ar[Y ] + E[Y ]2V ar[Zn ]
Xi=1 V ar[Zn+1 ] = ( bp)n(bp)(1 − p ) + ( bp)2E[Zn ]
Using the inductive hypothesis ,
V ar[Zn+1 ] = ( bp)n+1(1 − p ) + ( bp)n+2(1 − p )
( bpn − 1 bp − 1
V ar[Zn+1 ] = ( 1 − p )
( bp)n+2 − ( bp)n+1 + ( bp)n+2(bp)n − 1
V ar[Zn+1 ] = ( bp)n+1(1 − p ) bp − 1 ( bp)n+1 − 1 bp − 1
Lemma 4 The covariance of the number of nodes at two different levels is given by Cov[Zi , Zj ] = ( 1 − p)(bp)max(i,j ) ( bp)min(i,j)−1
. bp−1
22
Proof : Cov[Zk , Zm ] = E[ZkZm ] − E[Zk]E[Zm ] . Using the identity E[XY ] = E[E[XY |Y ] ] . Let i = min(k , m ) and j = max(k , m ) thus , we can condition on Zi as Zj is dependent on Zi . Then : Cov[Zi , Zj ] = E[E[ZiZi|Zi ] ] − ( bp)i+j . Recall that Zi+1 = Y Zi , thus , one can recursively obtain that Zj = Y j−iZi . Thus ,
Cov[Zi , Zj ] = E[E[Y j−iZiZi|Zi ] ] − ( bp)i+j
Now notice that E[ZiZi ] = V ar[Zi ] + ( E[Zi])2 . Then ,
= ( bp)j−iE[ZiZi ] − ( bp)i+j
Cov[Zi , Zj ] = ( bp)j−i(V ar[Zi ] + ( E[Zi])2 ) − ( bp)i+j
= ( bp)j−i((bp)i(1 − p )
+ ( bp)2i ) − ( bp)i+j
= ( bp)j((1 − p )
( bp)i − 1 bp − 1 ( bp)i − 1 bp − 1 = ( bp)j(1 − p )
+ ( bp)i − ( bp)i ) ( bp)i − 1 bp − 1
Lemma 5 The variance of the weakly connected component formed by the root of a tree of height h , given that the root is included in the sample is
V ar[Xh ] =
Proof :
( 1 − p ) bp − 1
(
( bp)2(h+1 ) − 1 ( bp)2 − 1 −
( bp)h+1 − 1 bp − 1
)+
2(1 − p ) ( bp − 1)3 (
( bp)2(h+1 ) − 1 bp + 1
−h(bp−1)(bp)h+1−(bp−1 ) )
V ar[Xh ] = V ar[
Zi ] = h
Xj=0 h
Xi=0
V ar[Zi ] + 2 h
Xj=1 j−1
Xi=0
Cov(Zi , Zj )
We will first expand the first term due to the variance of each Zi h
Xi=0
V ar[Zi ] =
=
1 − p bp − 1
( h h
( bp)i − 1 bp − 1
( bp)i(1 − p )
Xi=0 Xi=0 Xi=0 ( bp)2i − ( bp)h+1 − 1 ( bp)2(h+1 ) − 1 ( bp)2 − 1 − bp − 1
( bp)i ) h
)
=
( 1 − p ) bp − 1
(
Now let ’s expand the second term due to the covariance between different Zi ’s
2 h
Xj=1 j−1
Xi=0
Cov(Zi , Zj ) = 2 h
Xj=1
= 2
1 − p bp − 1 h
Xj=1
( bp)j
23 j−1
Xi=0 Xi=0 j−1
( 1 − p)(bp)j ( bp)i − 1 bp − 1
( (bp)i − 1 )
= 2
1 − p bp − 1 h
Xj=1
( bp)j(
( bp)i − 1 bp − 1 − j )
Notice that for j = 0 the above term becomes 0 , therefore , we can extend the sum to start at j = 0 : j−1
Xi=0
1 h
2
Xj=1 1 − p bp − 1
(
= 2
Cov(Zi , Zj ) = 2
(
( bp)2(h+1 ) − 1 ( bp)2 − 1 − bp − 1
1 − p bp − 1
1
(
Xj=0 bp − 1 ( bp)h+1 − 1 ) − bp − 1 h h
( (bp)2j − ( bp)j ) −
Xj=0 ( h + 1)(bp)h+1 − 1
+ bp − 1
( bp)jj )
( bp)h+2 − 1 ( bp − 1)2
With some basic algebra we obtain :
2 h
Xj=1 j−1
Xi=0
Cov(Zi , Zj ) =
2(1 − p ) ( bp − 1)3 (
( bp)2(h+1 ) − 1 bp + 1
− h(bp − 1)(bp)h+1 − ( bp − 1 ) )
Theorem 9 The expected size of the largest weakly connected component , Xmax in the cascade sample in a tree of height h is upper bounded by :
E[Xmax ] ≤ O((bp)h ) +qO(bh ) + O(bp)2h + O(h ) + O(h(bp)h ) + C + O(bp)h + O(p2b)h where C is a constant
Proof : Aven , et . al . [ 2 ] proposed an upper bound on the expected value of the highest order statistic given by : Eθ[Xl:l ] ≤ max1≤i≤l µi +q l−1 l Pl i where in our context , l is the expected number of weakly connected components ( ie roots ) in a tree of height h , µi is the expected size of the connected component formed by a node at level i and σi is the standard deviation of the connected component formed by a node at level i . It is clear that : i=1 σ2
µi = max 1≤i≤n
( bp)h+1 − 1 bp − 1
Moreover , let now Wi be the number of roots ( nodes that start a new connected component ) at level i in the tree . Then , from Theorem 4 , it follows that :
Also , from Theorem 4 :
On the other hand , p for for i = 0 i > 0
E[Wi ] =fl ( 1 − p)pbi l = p(1 − p ) bh+1 − 1 b − 1 − 1 + p
σ2 i = l
Xi=1 h
Xi=0
WiV ar[Xh−i ]
24
With some basic algebra one can obtain that : i=1 σ2
( 1−p)2p i = ( 1−p)p2
( bp−1)2 ( bp)2(h+1 )
( bp)2−1 − ( bp)h+1−1 bp−1 ( bp)2(h+1)−1 bp−1 + 1−(bp2)2(h+1 ) p−1 b−1 − ( bp)h+1 ph+1−1 bh+1−1 ( bp2)h(1−bp2 ) + bp bp+1 1−p2b − ( bp)h+1−1 1−(p2b)h+1 ( bp−1)(b−1 ) + ( bp−1)3 h(bp)h(1 − ( bph+1 ) − bp(h + 1 ) + ( bp)h+2 + bp 1−(bp)h+2 + h(bp − 1)(bp)h+1 − ( bp − 1 )
Pn ( bp−1)3 ( bp)2bh ( bp−1)3 ( bp)2(h+1)−1 bp+1 2(1−p)2p
2(1−p)p
2(1−p)2p bp+1 bp+1 i=1 σ2 i = O(bh ) + O(bp)2h + O(h ) + O(h(bp)h ) + C + O(bp)h + O(p2b)h
1−bp − h(bp − 1 ) +
Thus , Pl as :
Notice on the other hand , that l ≥ 1 thus , l−1 E[Xmax ] ≤ O((bp)h ) +qO(bh ) + O(bp)2h + O(h ) + O(h(bp)h ) + C + O(bp)h + O(p2b)h l ≤ 1 therefore , we can rewrite the upper bound
Corollary 2 For a large h , the size of the largest weakly connected component as a fraction of nodes included in the sample approaches 0 with at least a convergence rate of ( O(ph ) bh/2 )
O( 1 if if p ≥ 1√b p ≤ 1√b
Proof : Dividing the size of the largest weakly connected component from Theorem 9 over the expected number of nodes included in the sample m = p bh+1−1 b−1 = O(bh ) , we obtain :
Xmax m ≤
O((bp)h ) +pO(bh ) + O(bp)2h + O(h ) + O(h(bp)h ) + C + O(bp)h + O(p2b)h )h/2 ) + O( p √b
√h bh ) + O(√h(
1 bh ) + O(
)h/2 ) + O(
O(bh ) p b p b
)h b )h/2 ) ≤ O( 1 bh/2 ) , also , O( 1 bh ) ≤ O( 1 bh/2 ) , thus , we can simplify
Xmax m ≤ O(ph ) + O( Notice that O( p√b the above expression to :
1 bh/2 ) + O(p)h + O( )h ≤ O( p
Xmax m ≤ O(ph ) + O(
1 bh/2 ) + O(
√h bh ) + O(√h( p b
)h/2 )
Now , we need to find a relation between the terms that depend linearly with h and the other h , this will be terms that exponentially decay . To do this , we will first study the function f ( h ) = h useful for future discussion . Limit when h → ∞ :
1
X = lim h→∞
1 h h log X = lim h→∞ Thus we conclude that X → 1 and therefore : lim h→∞ log h h
1 h
1 → 0
= lim h→∞ h
1 h → 1
25
We will next solve for the h for which f ( h ) is maximized and value of max f ( h ) . The maximum value of f ( h ) satisfies the condition that f′(h ) = 0 . To satisfy this condition , we will first take logarithms to find the derivative of the function : log f ( h ) =
1 h log h
( log f ( h))′ =
1 f ( h ) f′(h ) =
1 h2 − log h h2 =
1 − log h h2
1 f′(h ) = h h ( 1 − log h ) h2
= 0 then log h = 1 → h = e
This solution is assuming continuity . Therefore , the result should be rounded . For discontinuous h , the maximum of f ( h ) is found at h = 3 and the maximum value is : max f ( h ) = 3
3 .
1
Let ’s now focus on the term of O(√h( p this term is smaller than O( 1 conditions would guarrantee convergence at the rate of the other terms . b )h/2 ) . In what follows , we will study for which contidion bh/2 ) or smaller than O(ph ) . Notice that satisfying any of these
Thus , the above condition is true iff h 069 We will refer to this as condition 1 . On the other hand , h ≤ 1
1
√h(
1 bh/2 p )h/2 ≤ b hph ≤ 1 p , and thefore , the condition is satisfied if p ≤ 3
−1
3 ≈
√h( p b
)h/2 ≤
1 bh/2
−1
1 From here we get that the condition is satisfied if bp ≥ 3 3 . Combining this result with that of 2 condition 1 , we obtain that if 3 3 to guarrantee faster convergence rate . As b is discrete , then we would require b ≥ 3 . However , with further analysis we obtain that the condition on b is relaxed to be b ≥ 2 for h ≥ 5 . Given that we are considering large h and b = 1 is the trivial case where a chain is formed , then we can conclude that the term O(√h( p
3 ≤ p ≤ 1 then we would require that b ≥ 3 b )h/2 ) will always decay faster than either O( 1 bh/2 ) or O(ph ) .
√h bh ) and find a condition that would guarrantee this term to be
Similarly , we can bound O( bh/2 ) or O(ph ) . smaller than O( 1
√h bh ≤
1 bh/2
1 h ≤ b , this condition will always be satisfied for b ≥ 2 . Therefore , we have The above is true iff h already proven that this term is always going to decay faster than 1 bh/2 . Thus , we can conclude that as h → ∞ then Xmax m → 0 at a convergence rate of O(ph ) + O( 1 bh/2 ) . Therefore , if p > 1 bh/2 then the rate will be O(ph ) , otherwise , O( 1 bh/2 ) .
26
A.3 Size of the connected components when k > 1
We are going to use the same notation as we used in Section A.2 with a few adjustments . Specifically , instead of Y , we are going to have k independent random variables Y1 , . . . , Yk representing the number of nodes included in the sample at level i . Each Yi is a binomial random variable with parameters ( bi , p ) , because each i’th level has bi nodes and each one is included in the sample with probability p . Otherwise , as before , let Xh be a random variable representing the size of the weakly connected component formed by the root of a tree of height h , conditioned on the root appearing in the sample . And as before , Zi is the number of nodes at level i in the connected component .
First observe that Z0 = 1 , Z1 = Z0Y1 = Y1 , Z2 = Z0Y2 = Y2 , , Zk = Z0Yk = Yk . In other words , the number of nodes at the first k levels is equal to the number of nodes included in the sample , because each of these nodes have a connection to the root . Moreover , because each node at i’th level connects to its ancestor at ( i − k)’th level , the number of nodes at any i’th level Zi can be lower bounded by Zi−kYk . In what follows , we are going to derive a lower bound on the expected size of the weakly connected formed by the root of a tree , ie E[Xh ] .
Lemma 6 The expected number of nodes at level i in the weakly connected component formed by k ⌋+1bi for any i > 0 . k ⌋ = j + 1 , E[Zi′ ] > pj+2bi′
Base Case : For 0 < i ≤ k , ie , j = ⌊ i−1 Inductive Case : Assume for level any level i such that ⌊ i−1 k ⌋ = 0 , we know that Zi = Yk . Hence , E[Zi ] = E[Yk ] = k ⌋ = j , E[Zi ] ≥ pj+1bi . We need to . We know that Zi′ ≥ Zi′−kYk k ⌋ = j + 1 , ⌋ = j . Hence , using inductive hypothesis , E[Zi′−k ] ≥ pj+1bi′−k . Since E[Yk ] = pbk , the root of a k tree is E[Zi ] ≥ p⌊ i−1 Proof : We are going prove by induction on j = ⌊ i−1 k ⌋ . pbi . Indeed , our formula for any i ≤ k gives us the same : p0pbi = pbi . prove that for any level i′ , such that ⌊ i′−1 and , accordingly , E[Zi′ ] ≥ E[Zi′−k]E[Yk ] ( since Yk is independent of Zi′−k ) . If ⌊ i′−1 then ⌊ i′−k−1 E[Zi′ ] = E[Zi′−k]E[Yk ] ≥ pj+1bi′−kpbk = pj+2bi′ Theorem 10 The expected size of the weakly connected component formed by the root of a k tree of height h , E[Xh ] , given that the root is included in the sample , is E[Xh ] ≥ 1 + pb p Proof : E[Xh ] = E[Ph i=0 E[Zi ] ≥ 1 +Ph 1 + pbPh i=0 E[Zi ] . Using Lemma 6,Ph k bh−1 k bi = 1 + pb p k b−1 p i=0 Zi ] =Ph k bi−1 = 1 + pbPh−1
1 k bh−1 p k b−1 i=1 p⌊ i−1
Corollary 3 The size of the weakly connected component formed by the root of a k tree of height h , as a fraction of m , is Ω(p h k ) for large h . k ⌋+1bi ≥ i=0 p h
1 i−1 i=1 p h
. k i
Proof : From Theorem 10 , as h → ∞ , the expected size of the weakly connected component , as a fraction of m , is at least :
Applying L’Hopital ’s rule , we obtain : lim h→∞ h k bh−1 1 + pb p k b−1 p
1 p bh+1−1 b−1 ln(p
1 k b )
1 b k b−1 ln b b−1 b h k p lim h→∞ pb 1 k b−1 p lim h→∞ ln(p
1 k b)(p
1 k b)h p ln b·bh+1 b−1 p
→
27
A.4 X1–X4 Properties on Synthetic Cascades
Number of Nodes 140
Number of edges 140
Number of isolated nodes Number of components
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
) 1 X
( s e d o N f o r e b m u N
) 1 X
( s e d o N f o r e b m u N
) 1 X
( s e d o N f o r e b m u N
120
100
80
60
40
20
0
140
120
100
80
60
40
20
0
110 100 90 80 70 60 50 40 30 20 10
α
α
α
) 2 X
( s e g d E f o r e b m u N
) 2 X
( s e g d E f o r e b m u N
) 2 X
( s e g d E f o r e b m u N
120
100
80
60
40
20
0
160 140 120 100 80 60 40 20 0
250
200
150
100
50
0
α
α
α
) 3 X
( s e d o N d e t l a o s I f o r e b m u N
) 3 X
( s e d o N d e t l a o s I f o r e b m u N
) 3 X
( s e d o N d e t l a o s I f o r e b m u N
25
20
15
10
5
0
25
20
15
10
5
0
20 18 16 14 12 10 8 6 4 2 0
α
α
α
) 4 X
( s t n e n o p m o C f o r e b m u N
) 4 X
( s t n e n o p m o C f o r e b m u N
) 4 X
( s t n e n o p m o C f o r e b m u N
35
30
25
20
15
10
5
0
35
30
25
20
15
10
5
0
30
25
20
15
10
5
0
α
α
α
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Network Cascade
Influence Cascade
Network Cascade ode
Influence Cascade del
Figure 9 : X1 X4 properties on estimated k tree and observed cascades . First row : synthetic cascades on Erdos Renyi graph , Second row : synthetic cascades on Scale Free network . Third row : synthetic cascades on Forest Fire network . Error bars correspond to 95 % confidence interval .
28
A.5 Estimated vs . Observed Properties on Synthetic Cascades s e d o N
, r o r r
E e v i t l a e R s e g d E
, r o r r
E e v i t l a e R n o i t i a p c i t r a P
, r o r r
E e v i t l a e R h t i d W
, r o r r
E e v i t l a e R
Observed Estimated
1
0.8
0.6
0.4
0.2
Observed Estimated
1
0.8
0.6
0.4
0.2
Observed Estimated
1
0.8
0.6
0.4
0.2
Observed Estimated
1
0.8
0.6
0.4
0.2
0
0
0
0
σ
σ
σ
σ s e d o N
, r o r r
E e v i t l a e R s e g d E
, r o r r
E e v i t l a e R n o i t i a p c i t r a P
, r o r r
E e v i t l a e R h t i d W
, r o r r
E e v i t l a e R
Observed Estimated
1
0.8
0.6
0.4
0.2
0
Observed Estimated
1
0.8
0.6
0.4
0.2
Observed Estimated
1
0.8
0.6
0.4
0.2
Observed Estimated
1
0.8
0.6
0.4
0.2
0
0
0
σ
σ
σ
σ s e d o N
, r o r r
E e v i t l a e R s e g d E
, r o r r
E e v i t l a e R n o i t i a p c i t r a P
, r o r r
E e v i t l a e R h t i d W
, r o r r
E e v i t l a e R
Observed Estimated
1
0.8
0.6
0.4
0.2
Observed Estimated
1
0.8
0.6
0.4
0.2
Observed Estimated
1
0.8
0.6
0.4
0.2
Observed Estimated
1
0.8
0.6
0.4
0.2
0
0
0
0
σ
σ
σ
σ s e d o N
, r o r r
E e v i t l a e R s e g d E
, r o r r
E e v i t l a e R n o i t i a p c i t r a P
, r o r r
E e v i t l a e R h t i d W
, r o r r
E e v i t l a e R
Observed Estimated
1
0.8
0.6
0.4
0.2
Observed Estimated
1
0.8
0.6
0.4
0.2
Observed Estimated
1
0.8
0.6
0.4
0.2
Observed Estimated
1
0.8
0.6
0.4
0.2
0
0
0
0
σ
σ
σ
σ
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Figure 10 : Observed vs . estimated properties . First column : influence cascades on Erdos Renyi graph . Second column : network cascades on Erdos Renyi graph . Third column : influence cascades on Scale Free graph . Forth row : network cascades on Scale Free graph . All errors are averaged over a set of cascades , error bars correspond to 95 % confidence interval .
29 s e d o N
, r o r r
E e v i t l a e R s e g d E
, r o r r
E e v i t l a e R n o i t i a p c i t r a P
, r o r r
E e v i t l a e R h t i d W
, r o r r
E e v i t l a e R
Observed Estimated
1
0.8
0.6
0.4
0.2
0
0
0
0
Observed Estimated
1
0.8
0.6
0.4
0.2
Observed Estimated
1
0.8
0.6
0.4
0.2
Observed Estimated
1
0.8
0.6
0.4
0.2
σ
σ
σ
σ s e d o N
, r o r r
E e v i t l a e R s e g d E
, r o r r
E e v i t l a e R n o i t i a p c i t r a P
, r o r r
E e v i t l a e R h t i d W
, r o r r
E e v i t l a e R
Observed Estimated
1
0.8
0.6
0.4
0.2
0
0
0
0
Observed Estimated
1
0.8
0.6
0.4
0.2
Observed Estimated
1
0.8
0.6
0.4
0.2
Observed Estimated
1
0.8
0.6
0.4
0.2
σ
σ
σ
σ s e d o N
, r o r r
E e v i t l a e R s e g d E
, r o r r
E e v i t l a e R n o i t i a p c i t r a P
, r o r r
E e v i t l a e R h t i d W
, r o r r
E e v i t l a e R
Observed Estimated
1
0.8
0.6
0.4
0.2
0
0
0
0
Observed Estimated
1
0.8
0.6
0.4
0.2
Observed Estimated
1
0.8
0.6
0.4
0.2
Observed Estimated
1
0.8
0.6
0.4
0.2
σ
σ
σ
σ s e d o N
, r o r r
E e v i t l a e R s e g d E
, r o r r
E e v i t l a e R n o i t i a p c i t r a P
, r o r r
E e v i t l a e R h t i d W
, r o r r
E e v i t l a e R
Observed Estimated
1
0.8
0.6
0.4
0.2
0
0
0
0
Observed Estimated
1
0.8
0.6
0.4
0.2
Observed Estimated
1
0.8
0.6
0.4
0.2
Observed Estimated
1
0.8
0.6
0.4
0.2
σ
σ
σ
σ
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Figure 11 : Observed vs . estimated properties . First column : influence cascades on Forest Fire network . Second column : network cascades on Forest Fire network . Third column : influence cascades on Twitter network . Fourth column : network cascades on Twitter network . All errors are averaged over a set of cascades , error bars correspond to 95 % confidence interval .
A.6 Estimated vs . Observed Parameters : Extended Results
Network Cascade
Influence Cascade estimated error ( ˆe ) observed error ( e′ ) estimated error ( ˆe ) observed error ( e′ ) p b k h
–
0.11 0.00 0.05
–
0.25 0.00 0.64
0.01 0.00
–
0.04
–
0.03
–
0.09
Table 4 : Relative errors for estimated and observed parameters averaged over synthetic cascades on Erdos Renyi network , σ∗ = 0.5
30
Network Cascade
Influence Cascade estimated error ( ˆe ) observed error ( e′ ) estimated error ( ˆe ) observed error ( e′ ) p b k h
–
0.07 0.02 0.11
–
0.15 0.06 0.56
0.00 0.05
–
0.09
–
0.19
–
0.58
Table 5 : Relative errors for estimated and observed parameters averaged over synthetic cascades on Scale Free network , σ∗ = 0.5
Network Cascade
Influence Cascade estimated error ( ˆe ) observed error ( e′ ) estimated error ( ˆe ) observed error ( e′ ) p b k h
–
0.08 0.03 0.10
–
0.12 0.23 0.51
0.01 0.00
–
0.04
–
0.03
–
0.51
Table 6 : Relative errors for estimated and observed parameters averaged over synthetic cascades on Forest Fire network , σ∗ = 0.5
Network Cascade
Influence Cascade estimated error ( ˆe ) observed error ( e′ ) estimated error ( ˆe ) observed error ( e′ ) p b k h
–
0.04 0.07 0.11
–
0.13 0.09 0.22
0.08 0.13
–
0.11
–
0.33
–
0.13
Table 7 : Relative errors for estimated and observed parameters averaged over retweet cascades , σ∗ = 0.5
Influence Cascade estimated error ( ˆe ) observed error ( e′ ) p b h
0.01 0.15 0.03
–
0.37 0.14
Table 8 : Relative errors for estimated and observed parameters averaged over blog cascades , σ∗ = 0.5
31
