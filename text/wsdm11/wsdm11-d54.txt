Collective Extraction from Heterogeneous Web Lists
Ashwin Machanavajjhala
Arun Iyer†
Philip Bohannon
Srujana Merugu
Yahoo! Research , Santa Clara , USA †Yahoo! Research , Bangalore , India
{mvnak,aruniyer,plb,srujana}@yahoo inc.com
ABSTRACT Automatic extraction of structured records from inconsistently formatted lists on the web is challenging : different lists present disparate sets of attributes with variations in the ordering of attributes ; many lists contain additional attributes and noise that can confuse the extraction process ; and formatting within a list may be inconsistent due to missing attributes or manual formatting on some sites .
We present a novel solution to this extraction problem that is based on i ) collective extraction from multiple lists simultaneously and ii ) careful exploitation of a small database of seed entities . Our approach addresses the layout homogeneity within the individual lists , content redundancy across some snippets from different sources , and the noisy attribute rendering process . We experimentally evaluate variants of this algorithm on real world data sets and show that our approach is a promising direction for extraction from noisy lists , requiring mild and thus inexpensive supervision suitable for extraction from the tail of the web .
Categories and Subject Descriptors H28 [ Database Management ] : Data Mining
General Terms Algorithms
1 .
INTRODUCTION
Content portals powered by databases of extracted data are now in wide use for a number of domains [ 10 , 18 ] . As such portals compete for audience engagement , each will naturally seek to enhance its database with data from lists found on a variety of smaller , structurally diverse sites . This is partially because the smaller sites are often more authoritative , but also because larger sites are often competitors . In this paper we focus on the extraction challenges faced on such smaller sites ; in particular , those that contain lists of entity information ; for example , lists of businesses , books , movies , etc . While some of these lists are script generated , some are not , and traditional extraction from these lists via site specific supervision is cost prohibitive [ 5 , 13 ] .
To simplify the problem , we assume that lists have been correctly segmented into snippets by an unsupervised technique ( see , eg , [ 2 ] ) and that each snippet contains at least some attribute data from a given entity schema . In this context , we informally define the Snippet Extraction Problem as : Segment each snippet , and label the resulting segments with the correct attribute label , or Other if none applies . Given the assumption that snippets contain exactly one record , correct attribute labeling yields entity extraction . We now illustrate the snippet extraction problem with an example : Example . Figure 1 shows fragments of three web pages with information about schools in California . Across the pages , information about a variety of attributes is available , including officialURL , name , city , state , phone , team and address . Each of these three web pages have been segmented to yield snippets ( separated by horizontal lines ) , each associated with a single school .
An example of snippet t2,1 , correctly segmented and labeled , is shown between sources 2 and 3 in Figure 1 . Note that portions of the snippet not corresponding to the schema have the distinguished label “ Other ” .
Given the snippet extraction problem and the requirement of little or no supervision , two classes of techniques might generally be applied . First , an unsupervised wrapper ( eg [ 1 , 3 , 9 , 11 ] ) technique might be used to further segment each snippet into likely attribute segments , and to align these segments across snippets . These techniques are particularly effective when strong HTML or punctuation signal is present on the site . Recently , these techniques have been generalized to collectively segment multiple sites [ 7 ] . Note that by segmenting and aligning snippets , these techniques fall short of full Snippet Extraction since labeling is not done . Second , database supervision [ 5 , 12 , 13 , 15 , 17 ] might be used to label some snippets on the site using a seed entity database , and a sequential model learned from these examples . This model can then be applied to segment and label , usually skipping the alignment step . Unlike unsupervised wrappers , databases supervision may also leverage sequential signal , for example , that certain attribute orderings are used on a particular site . Challenges Two challenges arise when state of the art techniques are applied to diverse and noisy web data :
1 . Attribute boundaries from HTML or punctuation are often inconsistent or missing , leading to poor segmentation ( and thus alignment and labeling errors ) .
( cid:127 ) 42nd Street Elementary School– www42StSchoolcom – Los Angeles ( cid:127 ) Collins Elementary–wwwcollinselementarycom– Cupertino ( cid:127 ) Z Zamorano Elementary School – wwwzamoranoedu– San Diego
Source 1
Entity e3 e2 e4
† wwwlincolnHSedu , Lincoln HS – Pirates – ( 650 ) 343 2321 – ( next game Wednesday ) † wwwsmmmcom , Memphis MS – Grizzlies – ( 408 ) 330 1247 – San Mateo † wwwmontavistaedu , Monta Vista HS – Matadors – ( 408 ) 366 7777 – Santa Clara
Source 2
Entity e5 e6 e1
Attributes officialURL name addr city state zip phone team
Collins Cougars – L . P . Collins Elementary – wwwcollinselementarycom – Cupertino , CA · 408 366 5555 Monta Vista Matadors – Monta Vista High – 21840 McClellan Rd · Cupertina , CA · 408 366 7777 Cupertino Union School District – wwwCupertinoSDedu – 10301 Vista Drive · Cupertino , CA
Source 3
Entity e2 e1 e7 t1,1 t1,2 t1,3 t2,1 t2,2 t2,3 t3,1 t3,2 t3,3
† wwwlincolnHSedu officialURL t2,1 , correctly segmented and labeled
,
Lincoln HS
– Pirates
–
( 650 ) 343 2321
– ( next game on Wednesday ) name team phone
Other
Figure 1 : Example sources and snippets . fi(cid:30)(cid:30 ) ) fi* • (cid:45)fl *(cid:45) ** )fl (• (cid:45) fl fl * (cid:36)** ffl fi (*(cid:43) ff fl• fi* • (cid:45) fl fl * (cid:36)**(cid:43) fi (* ffl (cid:61) (cid:59)(cid:63)• (cid:58)(cid:45) fl (cid:36)(cid:43) 'fl fi* ff* ff* (cid:58)(cid:59) (cid:59)• ffi(cid:30 ) fi
.'ff'(ff'fi
.())'ff fi* • (cid:45)fl *(cid:45) ** )fl (• (cid:45) fl fl * (cid:36)** ffl fi (*(cid:43) ff fl• fi* • (cid:45) fl fl * (cid:36)**(cid:43) fi (* ffl (cid:61) (cid:59)(cid:63)• (cid:58)(cid:45) fl (cid:36)(cid:43) 'fl fi* ff* ff* (cid:58)(cid:59) (cid:59)• ffi(cid:30) . )*fl fi (cid:50) (fl. fi(cid:30)(cid:30 ) fl)*fl fi (cid:50) (fl. (cid:30)(cid:30 ) ffi(cid:30) .fi* )*fl fi fi
Figure 2 : Collective Extraction Algorithm
2 . Confusing attributes or junk segments cause a variety of problems . For example , consider team name and name , as shown in Source 3 of Figure 1 , if team name is not in the extractor ’s schema then it can cause false matches to name . This can cause trouble for both attribute model based techniques [ 7 ] and automatic labeling [ 12 , 13 ] .
Obviously these challenges interact , and in conjunction can lead to extremely difficult extraction scenarios .
Collie In this paper , we introduce collie , a novel system developed at Yahoo! for snippet extraction . The goals of collie are : i ) To work flexibly with a wide variety of HTML and non HTML data sets , even with poor signal for attribute separation , ii ) To improve quality in the presence of confusing junk segments , and iii ) To work with no explicit supervision , just a small database of seed entities .
Following [ 7 ] , collie can collectively extract from multiple sites , and introduces two key improvements over prior work :
1 . We provide a collective extraction technique that leverages sequential models as in [ 13 ] , but we are significantly more resilient to noisy data .
2 . We make explicit use of ( approximate ) entity match information throughout the process , rather than using it only initially to perform noisy labeling as in [ 12 , 13 ] . For example , in Figure 1 , snippets t1,2 and t3,1 both correspond to entity e2 , and it may be possible to identify this match prior to extracting information from t1,2 and t3,1 . In particular , the match information is leveraged in our adaptation of Viterbi decoding for sequential labeling based on a hidden Markov model .
3 . Rather than proceeding source by source as in [ 11 , 12 , 13 ] or using all sources at once as in [ 7 ] , we carefully maintain active sets of sources , entities and attributes . This helps us cope with poor initial knowledge of a particular site or entity , especially when sites are noisy .
Figure 2 outlines our extraction algorithm . The three parallelograms are key initialization steps of the algorithm : first a matching of snippets is found , using an algorithm like [ 8 ] . Second , an unsupervised segmentation algorithm is applied ( [16 ] and [ 7] ) . Third , our various models for entities , attributes and sources are initialized based on the seed entities and the unsupervised segmentation . Once initialization is complete , we iteratively perform four steps : 1 ) re segment and relabel snippets , 2 ) update models of sources , 3 ) resegment and relabel again , then 4 ) update entity models . After each iteration through these steps , our set of active entities , snippets and sources is revised . This is critical to avoiding garbage labelings in noisy sources .
We experimentally evaluate collie against two baseline algorithms , and the recent wwt [ 13 ] algorithm on a number of challenging , real data sets taken from “ Bestseller ” book lists on the web . Our experimental evaluation highlights the importance of leveraging a variety of cues and collective extraction from multiple sources in order to extract from noisy data with low supervision . collie achieves good attribute segmentation and labeling significantly better ( achieving at least 50 % error reduction in labeling on average ) than the state of the art . We show that collie is able to tolerate attribute confusion by leveraging sequential models , and confusing non attribute junk segments by estimating the complete set of attributes presented on each source . Finally , while collie assumes perfect knowledge of which snippets are from the same entity , we show that collie can also lever
Entity e2
η(t1,6 ) e2.a2 e2.a3
Collins Elementary School
Cupertino t1,1 t1,3 t1,2
Source s1 snippets : T(s1 )
L . P . Collins Elementary Cupertino , CA · 408 366 5555 12 1
10
3
8
9
2
4
5
6
L . P . Collins Elementary Cupertino , CA · 408 366 5555 b_5 = a1= name b6 = a3 a4 a5 b1 b2 b3
Figure 4 : Source snippet entity associations . age imperfect matching . In fact , we show that approximate matching schemes that match snippets with very similar attribute values can actually boost collie ’s performance over perfect entity matching .
In summary , collie is a promising new direction for collective extraction of structured entity records from a large number of small web sites , with zero per site supervision , and even limited supervision in the form of seed entities . Outline : In Section 2 , we formally state the multi site attribute extraction problem . In Section 3 we introduce the component attribute and sequential models needed in collie . In Section 5 , we present our experimental results . We discuss related work in Section 6 , and conclude in Section 7 .
In Section 4 , we present our detailed algorithm .
2 . PROBLEM DEFINITION
In this section , we introduce sufficient notation to formally define the collective record extraction problem . Sets are represented using upper case calligraphic letters ( eg , X ) with the corresponding lower case letter ( eg , x ) denoting an element of X and NX the cardinality . Schema . Let A denote a fixed set of attributes that should be recovered by extraction . In the running example , each entity has seven such attributes – officialURL , name , city , state , phone , and address . Each attribute a ∈ A takes values from a domain denoted by dom(a ) . For example , the attribute a1 = officialURL takes values only among valid URLs while a7 = state takes string values that refer to one among the fifty US states . Seed Entities . Let E denote the set of entities . Each entity e ∈ E is associated with a unique attribute value e.a for each attribute a ∈ A from dom(a ) or a distinguished value null ( which indicates that the attribute does not exist for that entity ) . Let Eseed ⊂ E denote a set of seed entities whose attribute values are already known prior to the extraction . Figure 3 shows two seed entities for the scenario described in the running example . Snippets . Let T be a set of text or HTML snippets each containing information on an entity of interest along with some potentially irrelevant information . Each snippet t ∈ T is modeled as a sequence of |t| tokens . We use t[x ] to refer to the xth token in t , and t[x1 : x2 ] , x2 > x1 to refer to a subsequence of tokens in t , t[x1 ] + t[x1 + 1 ] + . . . + t[x2 − 1 ] . Example snippets are shown in Figure 1 .
Sources . Each snippet t ∈ T is derived from a source s = S(t ) . Abusing notation , we use S(T ) or simply S to denote the set of all sources from which snippets T are drawn , and T ( s ) to denote the snippets from source s , ie , {t|S(t ) = s} . Figure 1 shows a set of snippets derived from three sources . In this example , each snippet embeds information about a school in the United States . Segmentation . A segmentation function χ maps snippets to a sequence of increasing integers , χ(t ) = ( x1 , x2 , . . . , x|χ(t)| ) . χ breaks t into |χ(t)|−1 subsequences , t[xr : xr+1 ] , [ r ] |χ(t)|−1 where x1 = 1 and x|χ| = |t| . We overload [ ] to define 1 t[χ(t)[r ] ] as t[xr : xr+1 ] . Figure 4 shows snippet t1,2 = “ L . P . Collins Elementary Cupertino , CA ˚u 408 366 5555 ” with tokens being partitioned into 4 segments corresponding to χ(t1,2 ) = {1 , 5 , 6 , 7 , 8 , 9 , 13} , and χ(t1,2)[4 ] = ( 7 : 8 ) . Labels . Let B be a set of labels that includes the attributes in A , and other auxiliary labels helpful for extraction ( eg , attribute separators and unknown attributes ) . In particular , let Bi ⊂ B be a set of labels that may be assigned to segments on site si , a non empty subset of which are from A . The remaining labels in Bi are assigned to punctuation and other separators , junk segments , unknown attributes , etc . In addition Bi always contains Other . Labeling . A labeling , l(t ) , maps a snippet t to a function that maps segments to labels . Usually , l(t)(· ) is defined only on the sequences defined by a segmentation function χ , in which case l(t ) may be subscripted by lχ(t ) . If t is from source si , then the range of l(t ) is Bi . For example , in Figure 4 , the first two segments of snippet t1,2 are labeled as b5 = a1 = name , b1 = other1 b6 = a3 = city respectively , so we would write lχ(t)[χ(t)[1 ] ] = a1 . Since this notation is cumbersome , the ( t ) may be omitted following l or χ when clear from context .
The Collective Record Extraction Problem Given a set of snippets T , over sources S(T ) along with A , B , and Eseed as defined above , to produce a solution ( χ , lχ ) that agrees with the correct segmentation and labeling , up to a given equivalence ( such as white space ) .
3 . COMPONENTS
In this section , we introduce our base algorithms for seg mentation , labeling and snippet matching . 3.1 Attribute and Field Models
Content models are widely used [ 1 , 4 , 5 , 7 , 11 ] to estimate the two relationships between segments ( t[x : y ] ) and labels , bi . First , we may wish to estimate P ( l | t[x : y] ) , the probability that a label applies given the contents of the segment ( classification ) , and second , we may wish to estimate P ( t[x : y ] | bi ) , the probability that the value t[x : y ] appears in a segment correctly labeled bi ( emission ) . Attribute Models For each attribute that appears in Eseed , we create a model {αm : am ∈ A} . This model is not updated . We use Pα(t[x : y ] | am ) and Pα(am | t[x : y ] ) to indicate the emission and classification probabilities estimated from αm , respectively . Source Field Models For each label bm,i ∈ Bi , we define βm,i as a content model of bm,i . The βm,i models always come from a segmentation and alignment of active snippets on a source si . We describe what these models correspond to in more detail in the next section . We use Pβ(t[x : y ] | bi ) eid a1 : officialURL e1 e2 a3 : city wwwmontavistaedu Cupertino CA wwwcollinselementarycom Collins Elementary School Cupertino CA a2 : name Monta Vista High School a4 : state
Seed Entities a5 : phone 408366777 4083665555 a6 : addr 21840 McClellan Rd null
Figure 3 : A set of seed entities or Pβ(bi | t[x : y ] ) to indicate the emission and classification probabilities estimated from βm , respectively . Implementation A variety of techniques have been used for content models including bag of words [ 8 ] , ngram language models [ 5 , 11 ] , statistical feature based models [ 7 ] , and HMM based models [ 1 , 4 ] . We implement our content models by combining ( a ) a bag of words model P1 , ( b ) Poisson models for character and token lengths P2 , and ( c ) a few multinomial models capturing different binary features , such as presence/absence of alphabets , digits , punctuation , and html tags , P3 . The models are smoothed appropriately to account for unseen values . Finally , the emission probability P ( t[x : y ] | bi ) is computed as , with manually tuned weights ( we used a uniform assignment ) . The implementation of P ( bi | t[x : y ] ) is discussed in the next section . 3.2 Entity Models
P wi
Q i i
Another way to estimate the emission probability arises when an entity , ej , is known for the snippet , t . In this case , we use the token level Jaccard similarity between ej .am and a segment t[x : y ] as an estimate of P ( t[x : y]|am ) . In particular , if jac(x1 , x2 ) represents the ratio of intersecting tokens between strings x1 and x2 to total tokens in both , then we estimate Pe(t[x : y]|am ) ∝ ejac(x1,x2 ) . 3.3 Labeling with a Sequential Model
With each source si , we associate a first order Markov model that models the distribution over the segment labellings of the snippets in T ( si ) . This model is parameterized by ( πi , λi ) as follows :
• πi[m ] , [ m ]
NB,i 1 the first segment for any snippet in T ( si ) is bm,i . denote the probability that the label of
• λ[m1][m2 ] , [ m1 ]
NB,i 1
NB,i+1 [ m2 ] 1 denotes the conditional probability of label bm2,i following label bm1,i in a snippet labeling . Note that bNB,i+1,i is interpreted as a dummy terminal state .
Given a segmentation χ(t ) for a snippet t , the probability of observing the snippet t can be written as P ( t|χ , Mi , πi , λi ) =
( P ( t|l(t ) , Mi , χ ) · P ( l(t)|πi , λi))(1 )
X
P ( t|l(t ) , Mi , χ ) = l(t ) |χ(t)|Y r=1
P ( t[χ(t)[r ] ] | l(r ) )
( 2 )
P ( l(t)|πi , λi ) =πi[l(t)[1 ] ] ·
|χ(t)|Y
λi[l(t)[r − 1]][l(t)[r]](3 ) r=2 where , l(t)[|χ(t)| ] = terminal . We use the standard Viterbi [ 21 ] algorithm in order to find the labeling lχ that maximizes P ( t|χ , Mi , πi , λi ) , the probability of generating the snippet t given segmentation χ . As input , Viterbi takes a given a segmentation χ(t ) , ( πi , λi ) and an emission probability function that estimates
Segmentation and labeling ( χ , lχ ) of
Algorithm 1 Collective Extraction Algorithm Input : Schema A , set of text snippets T over sources S(T ) , seed entity set Esup Output : snippets in T , Entity attributes {ek.am : am ∈ A , ek ∈ E} 1 : initEntitiesAndModels 2 : initSnippetToEntityMapping 3 : segmentSnippets 4 : while repeat till convergence do 5 : 6 : 7 : 8 : 9 : end while relabelActiveSnippets updateSourceTemplates relabelActiveSnippets updateEntityModels
P ( t[χ(t)[r ] ] | bm,i ) , with bm,i drawn from a family of labels Bi . The way the emission probability is computed in collie is one of our contributions , and it carefully depends on the currently active snippets , entities , where in most prior work only the attribute models , αm are used , not source specific “ junk ” models βm , nor entity specific models . The former is critical for dealing with noisy sites , and the latter is critical for capitalizing on small Eseed . 3.4 Active Sets
Given a key goal of working with very low supervision , we assume that no snippets have been labeled , and that in general Eseed is small . Given these constraints , very few snippets will correspond to entities in Eseed , and there may be some sources for which no snippets are associated with an entity in Eseed . To avoid estimating field models , transition probabilities , or entity values with insufficient evidence , we limit inference to active entities , and carefully grow this set as the algorithm progresses . We maintain sets of active snippets , Tactive , sources Sactive and entities Eactive .
4 . EXTRACTION ALGORITHM
In this section , we present the collie algorithm for the snippet extraction problem . The algorithm takes as input the set of snippets T and the seed set of entities Eseed , and outputs a pair ( χ , lχ ) that map each snippet t ∈ T to a segmentation , χ(t ) and a labeling χ(t ) . Algorithm 1 provides an overview of the key steps ( also illustrated in Fig 2 ) , and the steps are described in detail in this section . 4.1 Initialization
The algorithm begins by initializing entities , models , snippet to entity mappings , and by performing an unsupervised segmentation of each source . 411 initEntitiesAndModels In this step , a variety of variables and model parameters are initialized , as follows .
Active Sets Eactive is initialized to Eseed , and Tactive is initialized to {t|η(t ) ∈ Eactive} ( snippets mentioning active objects ) , and Sactive to ∅ . Global Models First , the entity attribute values , ei.am , are initialized using the attribute values in Eseed , and the global models αm are initialized with the am values for each ei . Sequential Models The parameters ( hπ , hλ ) that determine the Dirichlet priors for the source specific initial label and transition distributions are initialized using domain knowledge ( eg , name is the first attribute , state follows city ) where available , and are otherwise uniformly initialized .
S(t ) ∈ Sactive}
W if S(t ) ∈ Sactive then
Algorithm 2 relabelActiveSnippets 1 : Tactive ← {t : η(t ) ∈ Eactive 2 : for t ∈ Tactive do 3 : 4 : 5 : 6 : 7 : 8 : 9 : end for
( lt , χt ) ← viterbiNeighborhood(t ) ( lt , χt ) ← directLabel(t , η(t ) ) end if l(t ) ← lt(t ) ; χ(t ) ← χt(t ) else is then refined and labeled using our probabilistic model . To be specific , for each source si , we obtain a segmentai ( · ) ( allowing for empty segments ) of the snippets in tion χ0 T ( si ) such that |χ0 i ( t)| is invariant across all snippets in si and the aligned segments ( ie , rth segments of all snippets ) are maximally similar . Currently , there exist many such techniques [ 7 , 16 , 22 ] and we use both [ 7 ] and [ 16 ] in our experiments . 4.2 Iterative Inference
The main portion of the algorithm alternates between updating the source template model ( the Bi labels and ( πi , λi ) transition probabilities ) and the entity models ( the current estimates for ei.am for entities not in Eseed ) . After each such update , either to templates or entity models , relabelActiveSnippets is called . This is actually required , since both the entity models and source templates are updated from the active snippets and their current estimated segmentation and labeling . The whole process is repeated until convergence ; ie , when the total change to the source templates and entity models is below a low threshold.1
Pe(t[χ(t)[r]]|m ) // object model
Algorithm 3 directLabel(t , e ) 1 : lt,← new labeling ; 2 : for r ∈ χ0 3 : m∗ ← argmaxm i ( t ) , S(t ) = si do lt(t)[r ] ← m∗
;
4 : 5 : end for 6 : // Remove multiple labels 7 : for r ∈ χ0 8 : m ← l(t)[r ] 9 : i ( t ) do if m exists as label of any other segment r . ∈ χ0 with higher score then i ( t ) lt(t)[r ] ← Other
10 : end if 11 : 12 : end for 13 : return ( lt , χ0 i ) ; // doesn’t change χ0 i
412 initSnippetToEntityMapping An important benefit of our collective extraction approach relative to previous work on synchronized extraction [ 7 ] is the ability to effectively exploit overlap in entities across sources . To do so , we need to first infer the mapping , η , from snippets to entities . This is done in two steps , given a similarity function ∼ that is defined on both ( a ) pairs of snippets ( t1 ∼ t2 ) ( b ) pairs consisting of one snippet and one entity ( t1 ∼ e ) ( see [ 8 ] for simple but effective snippet matching techniques ) . For a snippet t , let ei be the entity that maximizes t ∼ ei . If this match is over a threshold , then we set η(t ) ← ei . However , due to the assumption that ηsup is small , the bulk of entities will not be assigned , and for the remaining snippets , they are clustered using ∼ with the constraint that two snippets from the same site are not in the same cluster . A new “ dummy entity ” ec is created , and for each cluster c , and for each t in c , η(t ) ← ec . 413 segmentSnippets We first perform collective unsupervised segmentation of snippets within each source , and the resulting segmentation
Algorithm 4 viterbiNeighborhood ( t ) Xneigh = {χ(t)} // initialize neighborhood for r ∈ χ(t ) do
( t)}
( t ) ← χ(t ) with segment r and r + 1 merged χ . Xneigh ← Xneigh ∪ {χ . for p ∈ χ(t)[r ] . . . χ(t)[r + 1 ] each token in χ(t ) do ( t ) ← χ(t ) with segment r split after token p χ . Xneigh ← Xneigh ∪ {χ . // find best segmentation amongst Xneigh return argmaxχ(t)viterbiLabel(χ(t),collieEmission )
( t)} end for end for
Algorithm 5 collieEmission ( t[x : y ] , bm,i ) Input : segment t[x : y ] of a snippet , and candidate label bi Output : P ( t[x : y]|bm,i ) if bi is a non attribute model then return Pβ(t[x : y]|bm,i ) using βm,i else let am be the attribute corresponding to bm,i p ← Pα(t[x : y]|am).75 · Pβ(t[x : y]|bm,i).25 ; if η(t ) ∈ Eactive then p ← p · Pe(t[x : y]|am ) end if return p end if relabelActiveSnippets ( Algorithm 2 ) As a first step , the active set of snippets is set to snippets that are either a ) part of active sources or b ) associated by η with an active entity . Next , in order to relabel active snippets with the latest entity and source template information , we iterate through each snippet as shown in Algorithm 2 . For sites that are inactive , we call directLabel , which uses entity models Pe to label columns based on the initial aligned segmentation χ0 i .
1With reasonably connectivity , we find that our algorithm quickly converges to a solution in about 5 10 iterations .
Algorithm 6 updateLabelPool Input : source si , snippets T = T ( si ) Output : new current label pool , Bi , for si , training label function ltmp 1 : B ← ∅ 2 : n ← 0 // junk labels 3 : L ← l(t)∀t ∈ T 4 : χi ← align NW(L ) 5 : for each column r in alignment of χi do 6 : 7 : 8 : 9 : 10 : 11 : 12 : end if 13 : 14 : end for 15 : return Bi , ltmp add new state bn to B ∀t∈T ltmp(t)[r ] = bn n + + else if r is an attribute am then add am to B ∀t∈T ltmp(t)[r ] = am
For active sites , the re labeling is more sophisticated , and is accomplished by viterbiNeighborhood , shown in Algorithm 4 . The idea of this routine , following [ 7 ] , is to perform a limited search around the current estimated segmentation of t , χ(t ) ( but [ 7 ] does not use a sequential model at all ) . This involves all new segmentations that can be formed by merging a single adjacent pair of segments , or splitting a single existing segment . Among the resulting candidate segmentations , the best labeling is chosen using the standard Viterbi decoding algorithm [ 21 ] , but with using a novel emission probability shown in Algorithm 5 . The idea is to carefully trade off between the use of attribute models from the initial seed set of entities Eseed , unsupervised label models , and information from known entity correspondences . updateSourceTemplates ( Algorithm 7 ) This routine updates two aspects of the template for a source , si . First , the set of source specific labels , Bi , is re estimated . Second , the Markov parameters ( πi , λi ) are updated . The label set Bi is computed by the subroutine updateLabelPool , shown in Algorithm 6 . This routine actually embodies the key subtleties of the template update logic . First , the set of label sequences from all active snippets on the source are aligned by Needleman Wunsch [ 16 ] , with the constraint that segments having two different attribute labels cannot be in the same column . The result stored in Xi , and due to the alignment , the r’th entry in each segment is expected to receive the same label – columns that contain at least one attribute label a are given that label ; other columns are given a distinct source specific label . This generates a temporary labeling , ltmp of all the active snippets of si . Once updateLabelPool finishes , ltmp is used to train an HMM , and the parameters of that HMM are stored as the new values of ( πi , λi ) . The use of ltmp only for training is in fact critical – the assumptions made by the alignment routine are too strong to actually use for labeling . By using it for training , the next relabelActiveSnippets will be influenced more softly , also allowing the entity models will also be taken into account . Finally the source is added to Sactive . Updating Entity Parameters As in the case of sources , in each iteration , any entity that is associated with at least one active snippet is added to Eactive . For each entity ek
Algorithm 7 updateSourceTemplates 1 : ( Bi , ltmp ) ← updateLabelPool ( si,T ( si ) ) 2 : if first call to updateSourceTemplates for si then 3 : ignore ltmp , set ( πi , λi ) to uniform probabilities over Bi 4 : else 5 : HM M ← hidden Markov model with states Bi 6 : 7 : 8 : 9 : end if train HM M with ltmp ( pii , λi ) ← model from HM M add sj to Sactive and attribute am ∈ A , we first collect the snippet fragments with label am and fairly high normalized marginal likelihood values , and pick the fragment which has the highest average Jaccard distance to all other choices to be ekam 4.3 Algorithm Efficiency
We now briefly discuss the computational complexity of our algorithm . Let NB denote the maximum number of labels across the sources and Lmax the maximum snippet length . The initialization costs are linear in the size of the dataset and the data structures required for model , ie , B + NE · NB ) . Snippet O((NS + 1)(NB + 1)Lmax + NS · N 2 labeling using the Viterbi algorithm as well as the marginal probability computation has a complexity that is O(|t|2·N 2 B ) for a snippet t . Updating the source attribute ordering profiles requires at most O(N 2 B ) operations for each source . The estimation of the entity attribute values , on the other hand , is linear in the number of the candidate snippet fragments ( due to the simplification via the mean model ) , which is usually fairly small ( < 10 ) . Therefore , the total time complexity of our algorithm for a single iteration is given by O(|Tactive|· max · N 2 B + |Eactive| · NA ) . In L2 the beginning , the number of active snippets,entities and sources are fairly small and eventually cover the entire connected portion of the entity snippet source graph . With reasonably connectivity , we find that our algorithm converges quickly to a solution ( in about 5 10 iterations ) .
B + NT · Lmax + |Sactive| · N 2
5 . EXPERIMENTAL EVALUATION
In this section , we report on comparing collie to state ofthe art techniques on a collective extraction task involving real web datasets . After describing our experimental set up , we show the following results :
• On a number of real list pages , our algorithm , collie , achieves good attribute segmentation and labeling even with a small seed set of examples entities in Eseed . • Baselines that use unsupervised segmentation fail to accurately label the segments in the presence of confusing attributes . Sequential models ( like wwt ) fail to correctly segment snippets in the presence of extra junk attributes . Our algorithms are able to overcome both these challenges as evidenced by collie ’s superior performance over unsupervised segmentation baselines as well as wwt .
• Our collective inference algorithm shows gain in performance compared to a variant of our algorithm that segments and labels each source independently .
100
80
60
40
20
1 F l e b a L
0
100
80
60
40
20 l l a c e R t n e m g e S
0
Collie Baseline A Baseline C WWT
Collie Baseline A Baseline C WWT
100
80
60
40
20
1 F l e b a L
0
100
80
60
40
20 l l a c e R t n e m g e S
0
Collie Baseline A Baseline C WWT
Collie Baseline A Baseline C WWT
100
80
60
40
20
1 F l e b a L
0
100
80
60
40
20 l l a c e R t n e m g e S
0
Collie Baseline A Baseline C WWT
Collie Baseline A Baseline C WWT
0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Seed Size Parameter
Seed Size Parameter
Seed Size Parameter
0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Seed Size Parameter
Seed Size Parameter
Seed Size Parameter
Figure 5 : Performance of collie and competing algorithms on HTML 1 b ( left ) , HTML S2 ( middle ) and HTML S3 ( right ) , with increasing seed size parameter .
• Finally , we study the impact of approximate entity match information utilized by collie . We experimentally show that approximate matching schemes that ensure that matching snippets have very similar attribute values boost collie ’s performance .
5.1 Experimental Setup Dataset . We use challenging datasets consisting of real HTML web pages for our experiments . Each of these pages contains a list of bestselling books from different sites – ( a ) amazon.com , ( bn ) barnesandnobles.com , ( p ) powells.com , 2 , ( pw ) publishersweekly.com , ( u ) usato(b ) bookweb.org daycom Text snippets were extracted from these HTML pages using an unsupervised list segmentation algorithm [ 2 ] . Snippets from each source contained a subset of attributes from bookname , author list , list price , publisher , isbn , excerpt , type in addition to other site specific fields . Moreover , snippets from a few of the sources ( eg , amazon.com ) had strong HTML structure , a few had some inline HTML formatting ( eg , a <strong> tag around the book title ) , while others were plain textual snippets with little or no delimiters between attributes . Finally , every source had some internal variance in the number of attributes on snippets .
Since all the pages describe bestselling books , some books appear on multiple sources . We associated objectIDs with snippets ( thus “ matching ” snippets across sources ) in three ways – perfect , bookname and author . Snippets share the same perfect ID if they contain information about exactly the same entity . They share the same bookname ( author ) ID if they have the same bookname ( author , respectively ) . Our extraction schema is A = {bookname , author list , list price , publisher} . Eseed is generated by manually extracting attributes from an s % of the records on one of the sources , which we call the database source . This is reasonable , since one may wish to find overlapping sources based on database content , or to seed extraction with a few records from one source to supervise further extraction . We call s as seed size parameter . We will use 3 variants of this dataset . HTML 1 x consists of a single source ( x ∈ {a , b , bn , p , pw , u} ) , and seed entities are picked from the same source . HTML S2 consists of 111 snippets from 4 sources ( b , p , pw , u ) and mentions 94 unique books . Eseed is chosen by picking s % of the 16 entities from b . HTML S3 consists of 220 snippets from 5 sources ( a , bn , p , pw , u ) mentioning a total of 159 books ; here Eseed is chosen as a s % fraction of 30 entities listed on u . Out of the 94 books mentioned HTML S2 , 14 appear on more than one source . Similarly for HTML S3 , 38 out of the 159 books appear on multiple sources . Evaluation . All snippets were semi automatically segmented and labeled , then manually reviewed to generate ground truth . We evaluate the output of our algorithms using precision and recall metrics defined as follows . For each snippet t , let St denote the set of segments which have been labeled by an algorithm with some a ∈ A . Similarly , let Gt denote the set of ground truth segments labeled with some a ∈ A . A segment in x ∈ St is considered a true positive if its label and string value matches the ground truth segment xg with the same attribute label ( we ignore any leading and trailing white space and html tags ) ; such segments are given a score score(x ) = 1 . Segments that intersect the ground truth segment , but miss or add some tokens are scored as score(x ) = number of tokens appear in x and xg number of tokens appear in x or xg
We now define precision and recall as follows . X
X
X precision = ( score(x ) ) / ( recall = (
|St| ) |Gt| ) t∈T X t∈T t∈T X x∈St X x∈St score(x ) ) / ( t∈T 2 ∗ precision ∗ recall precision + recall
( 4 )
( 5 )
( 6 )
( 7 )
2American Booksellers Association
F 1 =
We analogously define recall for segmentation ( number of ground truth segments in the predicted segmentation ignoring labels ) . To measure labeling performance , we use F 1 score since both precision and recall matter . For segmentation we measure only recall , as over segmentation of “ junk ” segments does not affect final quality . Algorithms . We refer to our algorithm as collie . We perform the initial unsupervised segmentation in collie using a variant of a well known alignment algorithm [ 16 ] . The same algorithm is used to align label sequences while updating source parameters . We compare our algorithms to wwt , baseline c and baseline a .
The baseline algorithms have the following structure . First , snippets are segmented in an unsupervised manner ; we use alignment [ 16 ] in baseline a , and Chuang et al . ’s context aware wrapping algorithm [ 7 ] based on initial alignments of sources in baseline c . Attribute models αm are built for each attribute am ∈ A , and each segment is labeled by the attribute whose model gives the segment the highest probability . Since this might result in multiple segments being labeled by the same attribute , we clean up labels by retaining only the highest scoring label for each attribute . The rest of the segments are labeled Other . wwt is the attribute extraction component from Gupta et al ’s [ 13 ] implementation of a list augmentation framework3 ( described in detail in Section 6 ) . 5.2 Results l l a c e R t n e m g e S
100
80
60
40
20
0 a b bn p pw u
Collie Baseline A
Baseline C WWT
( a ) Segmentation Recall on HTML 1 x with 20 % seed size
1 F l e b a L
100
80
60
40
20
0 a b bn p pw u
Collie Baseline A
Baseline C WWT
( b ) Labeling F 1 scores on HTML 1 x with 20 % seed size
Figure 6 :
Figures 6(a ) and 6(b ) compare the segmentation recall and labeling accuracy of the HTML 1 x datasets ( x =a , b , bn , p , pw , u ) each seeded with entities extracted from 20 %
3We thank Gupta et al . for sharing their source code , and Chuang et al . for clarifying implementation details . of the snippets on those sources ( 5 random runs ) . Note that most of these sources have about 15 30 snippets , hence 20 % is about 3 6 seed entities . Even with this small amount of supervision , collie achieves higher accuracy both the baseline algorithms as well as wwt . In the rest of the section , we present more detailed results on only one of the single source datasets , HTML 1 b , and the multisource datasets HTML S2 , HTML S3 .
521 Effect of database seed size In Figure 5 , we compare the accuracy of collie to that of the competing algorithms for attribute extraction on HTML1 b , HTML S2 and HTML S3 . In each of the datasets , we varied the seed size parameter ( s % of the entities picked at random ) , and compared labeling F 1 score and segmentation recall ( averaged over 5 random runs ) . On all three datasets , we see that collie has superior labeling F 1 over all the competing algorithms , and that collie ’s segmentation is comparable to the unsupervised segmentation techniques , and has superior recall to to wwt ’s segmentation . Even a few examples are enough for collie to get to its best performance ; baseline algorithms show a mild increase in labeling F 1 scores with increasing seed size . We explain the dip in wwt ’s performance at higher supervision next .
522 Effect of attribute confusion As mentioned in the introduction , wwt and the baseline algorithms have poor labeling F 1 scores due to two reasons : confusing attribute values , and confusing non attribute junk segments . Figure 5 illustrates that for small Eseed , attribute models alone are insufficient to distinguish between confusing attributes , like eg , author and publisher . To illustrate the effect of junk segments , in Figure 7(a ) , we modified the snippets HTML 1 b to only retain segments corresponding to attribute values ( we removed serial number , isbn number , description , and rank ) . collie still has close to perfect labeling and segmentation . The baseline algorithms still have trouble for small Eseed due to attribute confusion . However , we see that wwt performs remarkably well . This was because in the original dataset , some author and book names also appeared in the description field . wwt initializes the segmentation , by finding string matches between attribute values in supervision and the snippets – this step wrongly matches some of the attribute values to mentions in junk segments ( like description ) . We avoid this problem , since we leverage the unsupervised segmentation to identify both the known and unknown attribute segments on the page , and then use object models to label whole segments .
523 Impact of Collective Extraction Figure 7(b ) illustrates the impact of collective simultaneous extraction from multiple sites with seed size parameter set to 20 % . We created a modified non collective variant of our algorithm wherein extraction proceeds on each source independent of the others . Consequently , non collective cannot leverage the values extracted on other sites , and solely depends on the amount of overlap between the snippets on the source and the initial seed set supervision . We compare the labeling accuracy of this algorithm to that of our collective algorithm and wwt . wwt is not a fully collective algorithm – it performs extraction one source at a time , but incorporates extracted values from each source to supervise the next source .
100
95
90
85
80
75
70
65
1 F l e b a L
60
Collie Baseline A Baseline C WWT
0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Seed Size Parameter
( a ) Labeling F 1 scores on HTML 1 b without non attribute segments
1 F l e b a L
120
100
80
60
40
20
0
WWT Non Collective Collie
HTML S2
HTML S3
1 F l e b a L
100
95
90
85
80
75
70
65
Keymatch Author Bookname
HTML 1 b
HTML S2
HTML S3
( b ) Labeling accuracy of collie , wwt and a non collective variant on HTML S2 and HTML S3 with 20 % seed size
( c ) collie accuracy under different matching keys on HTML 1 b , HTML S2 , HTML S3 datasets with 20 % seed size
Figure 7 :
As expected on both HTML S2 and HTML S3 collie has higher accuracy than non collective . While the only a few seed entities may match each source , collective extraction helps improve extraction by sharing object models across sources . Surprisingly , non collective is significantly more accurate than wwt on the HTML S3 dataset . 524 Impact of key quality An important cue leveraged by collie is that the same entity is mentioned on multiple sources , and that we know η(t ) , the true entity associated with snippet t . While this perfect matching is seldom available , approximate matches between snippets can be easily found . To study how collie behaves with imperfect keys , we tested the accuracy of extraction when using bookname ID or author ID instead of the perfect ID . As can be seen from Figure 7(c ) , even with imperfect keys , extraction accuracy is quite good . Such imperfect keys are quite prevalent , eg , URLs and phone numbers indicating the wide applicability of our approach .
In some cases , like bookname ID , we get better labeling accuracy than with perfect keys . Segmenting a snippet t is helped the most if an algorithm knows that the attribute value extracted from a different snippet t . matches that attribute on t . Knowing the perfect key does not guarantee this ; eg the title of the book “ Titan ’s curse ” is represented as “ The Titan ’s Curse ( Percy Jackson and the Olympians #03 ) ” in one source and as “ Percy Jackson and the Olympians , Book 3:The Titan ’s Curse ” in another source . Combined with a small seed set , this variation can lead to a drop in accuracy . In fact , we computed the average pairwise Jaccard distances between attribute values for snippets sharing the same key , which we will call keyP urity . For instance , in HTML S3 , we found that keyP urity(perfect ) = 0.79 while the keyP urity(bookname ) = 0.88 ; this might explain the better accuracy . 5.3 Discussion
The study on the books data sets supports four key points about our approach to collective extraction : First , by leveraging a variety of cues , collie , achieves good attribute segmentation and labeling even with small seed sets significantly better than state of the art ( Section 521 ) Second , our algorithm is able to label accurately despite attribute confusion , and confusing non attribute segments ( Section 522 ) Third , our collective extraction algorithm outperforms noncollective baselines ( Section 523 ) Finally , while collie assumes perfect knowledge of η(t ) , the true entity associated with the snippet , collie can also use imperfect keys . We show that approximate matching schemes which ensures that matching snippets have very similar attribute values boost collie ’s performance ( Section 524 )
6 . RELATED WORK
Recently , there has been considerable interest in large scale extraction of multi attribute entity records from list pages on the web . Such extraction entails discovery of list page sources , segmentation of lists into snippets that correspond to entity descriptions , and finally , extraction of entities themselves . Our current work focuses on the last step , ie , entity extraction given multiple segmented lists from different sources . Most existing techniques for addressing this problem follow three main paradigms discussed below . The first class of approaches make use of source specific supervision ( ie , a small number of segmented and labeled records on each list page ) to learn attribute ordering and layout patterns via wrapper induction [ 6 , 14 ] or conditional random fields [ 20 , 23 ] , which are then used to label the rest of the snippets . In contrast , we use supervision strictly from an external database . There exist variants [ 5 ] that make use of annotations generated from a large seed database to learn source specific models even without explicit source specific supervision . In either case , the labeled snippets have to be separately deduped [ 19 ] to finally identify the unique entities . This approach is , however , prohibitively expensive to scale due to the excessive supervision requirements and does not take any advantage of the content overlap between sources . Recent techniques [ 15 ] add information from the database and perform deduping , but still require sourcespecific supervision , perhaps due to the use of a discriminative technique ( CRFs ) .
The second class of approaches [ 7 , 11 ] perform attribute extraction using both within source structural regularity ( eg , HTML , delimiter cues , lexical features ) , as well as global attribute distributional properties . Most of these techniques involve two core steps where the first step collectively segments all the snippets in a source into aligned fields in an unsupervised fashion and the second step involves matching ( or labeling given a reference database ) the fields across different sources using the global distributional properties . There also exists an alternative approach [ 23 ] that makes use of the same predictive signals , but in a coupled fashion via a conditional random field with features chosen carefully to capture the local structural similarity and global commonalities . This CRF based technique , however , requires a fair bit of supervision , typically 1/3 of the sources in the test set in experiments,4 and thus cannot be used if site specific supervision is not cost effective . Though all these approaches account for global distributional commonalities , these techniques do not involve any entity deduping and cannot make use of any fine grained dependencies arising from multiple snippets describing the same entity . These methods also tend to be not as effective in the presence of confusing attributes , eg , phone and fax numbers , and moderate structural variations within a source .
Gupta and Sarawagi [ 13 ] recently proposed a third approach called World Wide Tables ( WWT ) that is closest in spirit to our current work . Like collie and unlike most of the related work , WWT can handle lists with little or no HTML structure . WWT is an end to end system for expanding a query table using unstructured lists on the web that addresses source discovery , extraction , and ranking . Since the initial query is only a few example tuples , the size of supervision required is modest . The WWT approach labels records on a new site based on the current database , and adds to that database after extraction .
Unlike our collective extraction approach , each inference step is performed independently without adequately accounting for the uncertainty involved in the preceding steps and hence , there is a considerable potential for errors to propagate . The choice of discriminative model in WWT allows for inclusion of a wide variety of features , but places the onus on the system designer to specify these features . It also requires extremely high quality initial annotations since it operates in a supervised fashion . On the other hand , our sequential generative model models the snippet labeling as a latent variable allowing us to capture the uncertainty in the annotations in a principled fashion . Our model also allows discovery of unknown attributes , and can capture the layout patterns fairly well even in the presence of such latent fields , while the semi Markov CRF has to be provided additional long range features to obtain a comparable performance . Furthermore , our active set based incremental inference is significantly more powerful than WWT ’s source prioritization ( based on the number of annotations ) .
7 . CONCLUSIONS
In this paper , we consider the problem of multi attribute record extraction from unstructured lists on the web . This is a challenging task due to the need to extract from a variety of small sites with different formatting templates . We present a novel approach to jointly extract attribute values for these records based on collective extraction with a sequential model , use of unsupervised segmentation and local search to handle additional attributes and junk segments , and careful active set management to better cope with noisy sources and confusing attributes . We evaluate our algorithms on real , noisy data sets and obtain superior performance relative to some of the existing state of art techniques indicating that our methodology is able to effectively exploit multiple types of predictive cues , and adapt
4We thank the authors for this clarification via private communication . to difficult , noisy extraction cases where site specific supervision is cost prohibitive .
8 . REFERENCES [ 1 ] E . Agichtein and V . Ganti . Mining reference tables for automatic text segmentation . In KDD , pages 20–29 , 2004 . [ 2 ] M . Alvarez , A . Pan , J . Raposo , F . Bellas , and F . Cacheda .
Extracting lists of data records from semi structured web pages . Data Knowl . Engg . , 2008 .
[ 3 ] A . Arasu and H . Garcia Molina . Extracting structured data from web pages . In SIGMOD , 2003 . ACM , 2003 .
[ 4 ] V . Borkar , K . Deshmukh , and S . Sarawagi . Automatic segmentation of text into structured records . SIGMOD Rec . , 30(2 ) , 2001 .
[ 5 ] S . Canisius and C . Sporleder . Bootstrapping information extraction from field books . In EMNLP , pages 827–836 , 2007 .
[ 6 ] C . Chang , M . Kayed , M . R . Girgis , and K . F . Shaalan . A survey of web information extraction systems . IEEE Trans . Knowl . Data Eng . , 2006 .
[ 7 ] S L Chuang , K . C C Chang , and C . Zhai . Context aware wrapping : Synchronized data extraction . In VLDB , 2007 .
[ 8 ] W . W . Cohen . Data integration using similarity joins and a word based information representation language . ACM Trans . Inf . Syst . , 18(3 ) , 2000 .
[ 9 ] V . Crescenzi , G . Mecca , and P . Merialdo . Roadrunner :
Towards automatic data extraction from large web sites . In VLDB , 2001 .
[ 10 ] P . DeRose , W . Shen , F . Chen , A . Doan , and
R . Ramakrishnan . Building structured web community portals : A top down , compositional , and incremental approach . In VLDB , pages 399–410 , 2007 .
[ 11 ] H . Elmeleegy , J . Madhavan , and A . Halevy . Harvesting relational tables from lists on the web . In Proceedings of the VLDB Endowment ( PVLDB ) , pages 1078–1089 , 2009 . [ 12 ] P . Gulhane , R . Rastogi , S . Sengamedu , and A . Tengli .
Exploiting content redundancy for web information extraction . In VLDB , 2010 .
[ 13 ] R . Gupta and S . Sarawagi . Answering table augmentation queries from unstructured lists on the web . In VLDB , 2009 .
[ 14 ] N . Kushmerick , D . Weld , and R . Doorenbos . Wrapper induction for information extraction . In IJCAI , 1997 .
[ 15 ] I . R . Mansuri and S . Sarawagi . Integrating unstructured data into relational databases . In ICDE ’06 : Proceedings of the 22nd International Conference on Data Engineering , page 29 , Washington , DC , USA , 2006 .
[ 16 ] S . B . Needleman and C . D . Wunsch . A general method applicable to the search for similarities in the amino acid sequence of two proteins . J Mol . Bio . , 1970 .
[ 17 ] P . Papotti , V . Crescenzi , P . Merialdo , M . Bronzi , and
L . Blanco . Redundancy driven web data extraction and integration . In WebDB , 2010 .
[ 18 ] A . Rajaraman . Kosmix : Exploring the deep web using taxonomies and categorization . IEEE Data Eng . Bull . , 32(2):12–19 , 2009 .
[ 19 ] P . Ravikumar and W . Cohen . A hierarchical graphical model for record linkage . In UAI ’04 : Proceedings of the 20th conference on Uncertainty in Artificial Intelligence , pages 454–461 , 2004 .
[ 20 ] C . Sutton and A . Mccallum . An introduction to conditional random fields for relational learning . In Introduction to Statistical Relational Learning , chapter 4 . MIT Press , 2007 . [ 21 ] A . J . Viterbi . Error bounds for convolutional codes and an asymptotically optimum decoding algorithm . IEEE Transactions on Information Theory , 13(2 ) , 1967 .
[ 22 ] Y . Zhai and B . Liu . Web data extraction based on partial tree alignment . In WWW . ACM , 2005 .
[ 23 ] J . Zhu , Z . Nie , J . Wen , B . Zhang , and W . Ma .
Simultaneous record detection and attribute labeling in web data extraction . In KDD , 2006 .
