Using Graded Relevance Metrics for Evaluating
Community QA Answer Selection
Tetsuya Sakai
Microsoft Research Asia , tetsuyasakai@acm.org
PRChina
Yohei Seki
University of Tsukuba , Japan yohei@slistsukubaacjp
Daisuke Ishikawa National Institute of Informatics , Japan dais@niiacjp
Noriko Kando National Institute of Informatics , Japan kando@niiacjp
Kazuko Kuriyama
Shirayuri College , Japan kuriyama@shirayuriacjp
Chin Yew Lin
Microsoft Research Asia , cyl@microsoft.com
PRChina
ABSTRACT Community Question Answering ( CQA ) sites such as Yahoo! Answers have emerged as rich knowledge resources for information seekers . However , answers posted to CQA sites can be irrelevant , incomplete , redundant , incorrect , biased , ill formed or even abusive . Hence , automatic selection of “ good ” answers for a given posted question is a practical research problem that will help us manage the quality of accumulated knowledge . One way to evaluate answer selection systems for CQA would be to use the Best Answers ( BAs ) that are readily available from the CQA sites . However , BAs may be biased , and even if they are not , there may be other good answers besides BAs . To remedy these two problems , we propose system evaluation methods that involve multiple answer assessors and graded relevance information retrieval metrics . Our main findings from experiments using the NTCIR 8 CQA task data are that , using our evaluation methods , ( a ) we can detect many substantial differences between systems that would have been overlooked by BA based evaluation ; and ( b ) we can better identify hard questions ( ie those that are handled poorly by many systems and therefore require focussed investigation ) compared to BAbased evaluation . We therefore argue that our approach is useful for building effective CQA answer selection systems despite the cost of manual answer assessments .
Categories and Subject Descriptors H33 [ Information Storage and Retrieval ] : Information Search and Retrieval ; H35 [ Information Storage and Retrieval ] : Online Information Services General Terms Experimentation Keywords best answers , community question answering , evaluation , graded relevance , NTCIR , test collections
1 .
INTRODUCTION
Community Question Answering ( CQA ) sites such as Yahoo! Answers1 , Baidu Zhidao2 and Microsoft Answers3 have emerged as very rich knowledge resources for information seekers . For example , Yahoo! Answers announced in May 2010 that they have collected one billion answers since their launch in December 20054 . However , answers posted to CQA sites can be irrelevant , incomplete , redundant , incorrect , biased , ill formed or even abusive ( eg spam ) . An answer is irrelevant if it does not discuss what the asker has asked ; it is incomplete if it does discuss the right topic but lacks some information ; on the other hand , it is redundant if it contains not only sufficient information but also unnecessary information . An answer is incorrect if its content is not true or no longer true ( ie obsolete ) . For questions that elicit a variety of opinionated answers ( where there may be no single truth ) , for example , an answer can be biased , in the sense that it reflects only one view and does not represent different people ’s views . In addition to these content aspects , style and language of answers may also be important for some applications : by ill formed we casually mean “ badly written ” answers regardless of the content .
Given the rapid growth of CQA contents and these different ways posted answers can fail to solve the posted questions , automatic selection of “ good ” answers for a given posted question is a practical research problem that will help us manage the quality of accumulated knowledge . One way to evaluate answer selection systems for CQA would be to use the Best Answers ( BAs ) that are readily available from the CQA sites . However , while the BA may be what the original asker of the question “ liked , ” if we want to reuse the question for a wider user community , relying on the BA may not be an optimal solution [ 9 ] . To be more specific , a BA selected by the asker5 may be biased , and even if it is not , there may be other good answers besides the BA [ 17 ] . To remedy these bias and nonexhaustiveness problems , we propose system evaluation methods that involve multiple answer assessors and graded relevance information
1http://answersyahoocom 2http://zhidaobaiducom 3http://answersmicrosoftcom 4http://yanswersblogcom/indexphp/archives/ visited 2010/05/03/1 billion answers served/ November 3 , 2010 . 5In general , a BA may be selected by the asker or through a voting mechanism . This study considers asker selected BAs only , as vote based BAs are not available in the CQA data set that we use . Gyöngyi et al . [ 4 ] report that vote based BAs of Yahoo! Answers are often influenced by self votes . retrieval ( IR ) metrics . ( In this paper , exhaustiveness means whether a set of answers fully covers different correct answers , while completeness means whether a single answer fully covers the information need of the asker . ) Our proposed methods were inspired by the pyramid method used in text summarisation [ 10 ] and automatic question answering evaluation [ 8 ] .
We use data from the Yahoo! Chiebukuro ( Japanese Yahoo! Answers ) site6 and the recent NTCIR 8 CQA task that was designed on top of the Yahoo! Chiebukuro data . Our main findings from experiments using the NTCIR 8 CQA data are that , using our evaluation methods , ( a ) we can detect many substantial differences between systems that would have been overlooked by BA based evaluation ; and ( b ) we can better identify hard questions ( ie those that are handled poorly by many systems and therefore require focussed investigation ) compared to BA based evaluation . We therefore argue that our approach is useful for building effective CQA answer selection systems despite the cost of manual answer assessments . The remainder of this paper is organised as follows . Section 2 discusses existing studies that are related to CQA evaluation . Section 3 describes the NTCIR 8 CQA task and its data , which we use as our testbed . Section 4 describes our proposed evaluation methods for the CQA answer selection task . Section 5 discusses the results of our experiments on ranking systems in terms of performance and also on ranking questions in terms of hardness . Finally , Section 6 concludes this paper .
2 . RELATED WORK
In response to the rapid advent of CQA sites over the past five years or so , researchers have recently identified several tasks for extracting and utilising knowledge from the CQA data . However , these tasks have been evaluated using existing methods , all of which are based on binary matching between system ’s output items and the gold standard , as we shall illustrate below .
Agichtein and Liu [ 2 ] defined a task to predict , given a question , whether the asker will eventually be satisfied with one of the responses . Agichtein et al . [ 1 ] defined the task of separating highquality questions and answers from low quality ones , ie a binary classification problem . In these studies , standard metrics like precision , recall and F1 measure were used .
Jeon et al . [ 7 ] discussed another practical task : in response to a newly submitted question , find existing question and answer ( Q A ) pairs and rank them . They constructed three separate gold standard Q A files : the first one considers only the relevance between the new question and existing questions ; the second one is similar to the first but contains only “ good ” and “ medium ” answers ; the third one is similar but contains only “ good ” answers . The evaluation metrics employed were traditional binary relevance metrics such as Average Precision .
Wang et al . [ 18 ] used the BAs of Yahoo! Answers as the gold standard , and computed Precision and Reciprocal Rank . Hence no manual answer assessments were done .
In a task setting similar to that of Jeon et al . , Suryanto et al . [ 17 ] hired multiple assessors to assess the BAs of retrieved questions in terms of ( binary ) quality based on correctness , readability , usefulness , objectivity and sincerity as well as ( binary ) relevance . If at least n assessors judged an answer as relevant or good quality , then that answer was treated so . They then computed Precision based on good quality answers , that based on relevant answers , and that based answers that satisfy both of these criteria . While their gold standard data sets were based on BAs only , they noted that answer ranking systems can perform better if not only BAs but also other answers are utilised .
Liu et al . [ 9 ] reported that less than half of the BAs from Yahoo! Answers data that they analyzed were unique answers . While they did not assess non BAs , their results also suggest that BA based evaluation may have bias and nonexhaustiveness problems .
Other CQA tasks include question ranking [ 3 ] and expert ranking [ 19 ] , but again , traditional metrics such as Average Precision , R Precision and rank correlation were used in these studies .
The task we examine in this paper is closely related to the aforementioned task of finding good Q A pairs given a newly posted question , but different . Our task , which we defined for the NTCIR8 CQA task , is : given one existing question , rank all answers posted in response to this question or select a good one from them . While this simpler task has its own potential applications such as presenting answers in order of utility on an CQA site or summarising the entire CQA data , it can be regarded as a special case of the Q A pairs task , where the new question coincides with exactly one existing question . Hence we believe that implications from our study on the use of non BA answers are also useful to the Q A pairs task . Unlike previous work on CQA evaluation , we utilise the notion of graded relevance [ 6 , 12 ] . The idea of using multiple assessors to construct graded relevance answer data was inspired by the pyramid method used in text summarisation [ 10 ] and automatic question answering [ 8 ] . The premise of this method is that different people have different views on which evaluation units ( summary content units [ 10 ] or nuggets [ 8 ] ) are correct or not , and the basic idea is to view this fact as a pyramid , where the top represents data with high inter assessor agreements , while the bottom represents those with low inter assessor agreements . However , our task and methods are simpler than these “ real ” pyramid methods , in that our evaluation unit is exactly the entire answer . By automatically constructing graded relevance data from multiple assessor judgments which disagree with one another substantially , we reduce the problem of CQA answer ranking into an IR evaluation problem .
The present study partially overlaps with the NTCIR 8 CQA task overview paper ( an unrefereed publication ) , but the overview paper primarily discusses a preliminary version of our proposed method , which involved a manual mapping of answer assessments to relevance levels [ 14 ] . In contrast , the methods proposed in this paper define relevance levels systematically . We discussed the effect of the manual mapping and that of the choice of gain values [ 6 ] on the NTCIR 8 CQA task evaluation in another recent paper [ 15 ] . To our knowledge , our work is the first to demostrate the advantages of evaluation based on multiple assessors and graded relevance over BA based evaluation .
3 . NTCIR 8 CQA TASK AND DATA
The NTCIR 8 CQA task was designed on top of the Yahoo! Chiebukuro ( Japanese Yahoo! Answers ) data . The task procedure started in March 2010 and a concluding session was held at the NTCIR 8 workshop meeting in June 2010 [ 5 , 14 ] . Four teams participated in the task , and the official evaluation results were released to them in April , prior to the concluding session .
The Yahoo! Chiebukuro data used for the task ( and for this study ) contains 3,116,009 resolved questions [ 4 , 5 ] , the Best Answer ( BA ) selected by the asker for each question7 , and 10,361,777 additional posted answers . Each question has exactly one question category tag such as “ entertainment and hobbies ” and “ internet , PC and appliances , ” and the NTCIR 8 CQA task organisers sampled 1,500 questions from the entire question set by taking into
6http://chiebukuroyahoocojp/
7One BA is missing from the data , so there are 3,116,008 BAs . account the frequency distribution across question categories . Thus the NTCIR 8 CQA formal evaluation set contains 1,500 questions covering 14 categories . This question set has a total of 7,443 answers , including 1,500 BAs . Hence the number of answers per question is 4.96 on average , ranging between 2 and 19 .
Participants were asked to submit runs of the following format :
<Q_ID>,<A_ID at rank 1>,<A_ID at rank 2> , where Q_ID and A_ID are questions IDs and answer IDs . Thus , for every question , the participating systems were to rank all the answers in decreasing order of answer quality . As there are 1,500 formal run questions , each run file contains exactly 1,500 lines . The organisers informed participants in advance that the run files were to be evaluated from at least three viewpoints :
1 . Whether the top ranked answer is the BA ;
2 . Whether the top ranked answer is a “ good ” answer based on judgments of multiple assessors ;
3 . Whether the entire ranked list of answers contain many “ good ” answers at high ranks based on judgments of multiple assessors .
Four teams , which we call A , B , L and M , participated in the NTCIR 8 CQA task and submitted 2 , 3 , 3 and 5 runs , respectively . Hence a total of 13 runs were evaluated at NTCIR 8 . The actual team/run names and their algorithms can easily be recovered by referring to the NTCIR 8 CQA overview paper part II [ 14 ] . However , we exclude the fifth run from Team M ( “ M 5 ” ) in our experiments as it is unsuitable for the purpose of discussing the difference between BA based evaluation and our proposed evaluation methods : more specifically , this run inadvertently tuned itself using the BA data of the formal run question set and does not represent a practical performance level in terms of BA based evaluation [ 14 ] .
The three runs from Team B were baseline runs generated by organisers : B 1 ranks answers at random ; B 2 ranks answers by length ( the longer the better ) ; and B 3 ranks answers by timestamp ( the newer the better ) .
4 . PROPOSED EVALUATION METHODS
One of the evaluation metrics used at the NTCIR 8 CQA task was hit at 1 ( or precision at 1 ) based on the BA data , denoted by BA Hit@1 . For a given question , let I(r ) = 1 if the answer at rank r is “ relevant ” according to a gold standard , and let I(r ) = 0 otherwise . In the case with the BA data , an answer is relevant to the question if and only if it is the BA . Then
BA Hit@1 = I(1 ) .
( 1 )
Hence , Mean BA Hit@1 over the question set is simply the number of questions for which the system correctly identified the BA .
The advantage of BA based evaluation is that the gold standard is “ already there ” – no manual answer assessments are required . However , as was discussed earlier , for the purpose of reusing and sharing posted questions and answers among a wide range of users , BA data may be biased , in the sense that each BA reflects the view of only one user , and nonexhaustive , in the sense that there may be other good answers besides the BA . Hence , BA data may not be ideal for evaluating systems designed for selecting “ good ” answers . We therefore propose an alternative evaluation methodology as follows .
In order to remedy the bias and the nonexhaustiveness problems at the same time , we hired four assessors to independently assess every answer for the CQA question set containing 1,500 questions :
Table 1 : Mapping relevance patterns to relevance levels based on judgment weights ( A:B=2:1 ) .
( a ) pattern AAAA AAAB AABB AAA ABBB AAB BBBB ABB AA BBB AB BB A B ( C ’s only ) total
( b ) #answers 1301 1505 1525 2 1385 14 1241 76 1 231 7 105 1 32 17 7443
( c ) weight 8 7 6 6 5 5 4 4 4 3 3 2 2 1 0
( d ) level
L8 L7 L6
( e ) #answers 1301 1505 1527
L5
L4
L3
L2
L1 L0 total
1399
1318
238
106
32 17 7443 we call them J1 , J2 , J3 and J4 ( where “ J ” stands for judge ) . They are all undergraduates in their 20s/30s who regularly visit the Yahoo! Chiebukuro site , but have never posted any questions/answers to it . Two were male and two were female . Each assessor labeled every answer using the following 3 point scale8 : A the answer completely satisfies the information need ; B the answer only partially satisfies the information need ; C the answer is irrelevant .
Each assessor spent approximately 16 25 hours in total ( 39 61 seconds per question on average ) for labelling the 1,500 questions and 7,443 answers9 . As a result , we obtained a relevance pattern for each answer as shown in Table 1(a ) . For example , “ AAAB ” means that the answer was rated A by three assessors and rated B by one assessor , and “ AAA ” means that the answer was rated A by three assessors and rated C by one assessor . The C ’s are not shown explictly in the table as we do not regard them as votes of confidence . Table 1(b ) shows the number of answers for each relevance pattern .
As discussed in detail in [ 5 ] , the inter assessor agreement is not high ( Kappa coefficient : 0249 0390 ) , even though the assessors did not report any serious difficulty in assessing the answers ( eg lack of enough background knowledge for technical questions ) . This illustrates how subjective answer assessments can be , and suggests that evaluation based on one person ’s judgments , such as BAbased evaluation , may not be reliable .
Based on the relevance patterns , we constructed graded relevance answer data as follows . First , we assigned a judgment weight to each label A , B and C . In this study , we simply let A : B : C = 2 : 1 : 0 . Then , we defined the weight of each relevance pattern as the sum of these weights : for example , the weight of “ AAAA ” is 4 ∗ 2 = 8 , as shown in Table 1(c ) . Finally , we defined relevance levels of the answers based on these weights : as Table 1(d ) shows , we obtained a 9 point scale graded relevance data set as a result . Table 1(e ) shows the total number of of answers for each relevance level . We shall refer to this gold standard data set as the Good Answers with Weights ( GAW ) data . Thus , according to our methodology , a highly relevant answer is one that was rated highly by many 8According to these criteria , an incomplete answer should be rated B , while a redundant answer should be rated A . But we did not explicitly introduce these concepts to the assessors . 9This includes the time used to label the questions with A or B [ 5 ] and to caterogise them into 13 question types .
Table 2 : Number of BAs that were judged to be Lx relevant .
L1 L2 L3 L4 99
22
1
7
L5 156
L6 245
L7 372
L8 598 total 1500 assessors . As mentioned earlier , this was inspired by the pyramid method used in text summarisation [ 10 ] and automatic question answering evaluation [ 8 ] .
Table 2 shows the relationship between the BA data and the GAW data : for example , for 598 questions , the BAs are treated as L8 relevant in the GAW data . It can be observed that , while the relevance grade of a BA is likely to be very high , there are some exceptions .
Using the GAW data , we propose to compute three graded relevance evaluation metrics : GAW nG@1 , GAW nDCG and GAW Q .
A naïve approach with the GAW data would be to use the binaryrelevance Hit@1 just like we do with the BA data . However , we verified at NTCIR 8 that this is not very useful [ 14 ] . If we simply count all answers except those judged L0 as “ relevant ” and compute Hit@1 , then it would be very easy for systems to achieve high performance , since all except 17 of the 7,443 answers are relevant ( See Table 1 ) . On the other hand , if we introduce a relevance level threshold , for example by considering only L4 or higher as relevant , we would lose some questions as not all questions have answers with very high relevance levels . This would be a waste of assessment effort .
Let g(r ) denote the gain of the answer at rank r in a system ’s ranked list : for the GAW data , simply let the gain values be 1 8 for ( r ) denote L1 L8 relevant answers , respectively . Similarly , let g the gain of the answer at rank r in an ideal ranked list , obtained by listing up all relevant answers in decreasing order of the relevance levels . Then , normalised gain at 1 based on GAW is defined as :
∗
GAW nG@1 = g(1)/g
∗
( 1 ) .
( 2 )
This is in fact the same as normalised discounted cumulative gain ( nDCG ) at rank 1 , since neither discounting nor gain cumulation applies at rank 1 [ 6 ] . Suppose a question has at least one L8relevant answer . Then a system that returns an L8 relevant answer at rank 1 receives GAW nG@1 = 8/8 = 1 , while one that returns an L1 relevant answer at rank 1 receives only GAW nG@1 = 1/8 . Note that a “ flat ” gain value setting ( ie giving a gain of 1 to all relevance levels ) would reduce nG@1 to Hit@1 .
Since multiple good answers are possible with the GAW data , we also evaluate the entire ranked lists of answers using two wellstudied graded relevance IR metrics , namely a version of nDCG [ 6 ] and Q measure ( Q ) [ 12 , 13 ] .
With the GAW data , we define GAW nDCG as follows :
∗
( 3 )
GAW nDCG = r=1 g(r)/ log(r + 1 ) r=1 g ( r)/ log(r + 1 ) where l is a document cut off value . In our evaluation , we use l = 20 for convenience since the number of answers per question lies between 2 and 19 . But this is actually the same as using l = 2 for questions with two answers , while using l = 20 for those with twenty answers , and so on , as both the system ’s ranked list and the ideal ranked list rank all answers to a given question and nothing else10 . 10As the task is to rank all answers , many of which are at least somewhat relevant , even a very bad system would receive some credit , ie a score greater than zero . If we want to ensure that the system performances range fully between 0 1 , an alternative normalization method could be used . Let sysDCG and bestDCG denote the numerator and the denominator in Eq 3 , respectively ,
.l .l
.r
∗
.r
∗
Let cg(r ) = i=1 g(i ) and cg
( i ) . These are known as the cumulative gains [ 6 ] for the system ’s ranked list and for the ideal one . Let I(r ) be 1 if the answer at rank r is Lx relevant ( x > 0 ) and 0 otherwise . Moreover , let C(r ) = .r
( r ) = i=1 g i=1 I(i ) . We define GAW Q as follows :
GAW Q =
1 R
I(r )
C(r ) + βcg(r ) r + βcg∗(r )
( 4 ) fi r where R is the total number of relevant items , and β is a persistence parameter for penalising late arrival of relevant items : following a recommendation in the literature [ 12 ] , we let β = 1 . ( β = 0 reduces Q to the binary relevance Average Precision . ) Like nDCG , Q quantifies how the system ’s ranked list deviates from the ideal one .
To sum up our proposal : we use four assessors to assess answers using a 3 point scale ; we aggregate the result to form a 9point scale graded relevance data set ; and then we use three gradedrelevance metrics to utilise this pyramid like gold standard data . GAW nG@1 is for the task of returning exactly one good answer to a given question . GAW nDCG and GAW Q are for the task of ranking all answers to a given question .
5 . EXPERIMENTS 5.1 Ranking Runs
This section discusses ranking different runs based on the GAW data as well as the BA data to demonstrate the usefulness of our evaluation methods . In addition to 12 runs submitted to the NTCIR8 CQA task ( after excluding one anomalous run as mentioned in Section 3 ) , we also evaluate four “ human performances ” : as each judge rated every answer as either A , B or C , ( s)he can be regarded as an answer ranking system that returns all answers rated A first , then those rated B , and then finally those rated C . When regarded as a system , we denote them by j1 j4 . 511 BA vs . GAW Table 3 summarises the results of our run ranking experiments using the GAW data as well as the BA data , where 16 “ runs ” have been sorted by mean performance in terms of BA Hit@1 , GAWnG@1 , GAW nDCG and GAW Q . We conducted a two sided sign test for every pair of runs ( 16*15/2=120 pairs ) , but only the significant differences between adjacent pairs in the table are shown : throughout this paper , ∗ and ∗∗ indicate that a run significantly outperforms the one below at α = 0.05 and α = 0.01 , while † and ‡ indicate that a run significantly underperforms the one below at α = 0.05 and α = 0.01 ( contrary to the ranking in terms of mean performance ) , respectively . The arrows indicate the rank changes when compared to the BA based ranking . For example , j3 is ranked at 8 in the BA Hit@1 ranking , but is ranked at 1 in the GAW nG@1 ranking , so “ ↑ 7 ” is shown to indicate “ up 7 ranks . ”
Figure 1 visualises the rankings shown in Table 3 : the runs on the x axis have been sorted by the GAW nG@1 performance . Table 4 shows the Kendall ’s τ rank correlations between two run rankings based on two different metrics . It can be observed that :
1 . The run rankings by GAW nG@1 , GAW nDCG and GAW Q are similar to one another . In particular , Table 4 shows that GAW nG@1 and GAW nDCG produce very similar rankings ( as they are both nDCG ) , and that GAW nDCG and and let worstDCG denote a similar summation for a reversed ideal list , obtained by listing up all answers in ascending order of relevance levels . Then ( sysDCG − worstDCG)/(bestDCG − worstDCG ) should have the desired property .
Table 3 : Mean performances based on the BA and GAW data . The runs are sorted by each performance metric , and the rank changes relative to the BA Hit@1 ranking is indicated by ↑ ’s and ↓ ’s . “ ∗ ” and “ ∗∗ ” indicate that a run significantly outperforms the one shown immediately below according to a two sided sign test ( α = 0.05 and α = 0.01 , respectively ) . Whereas , “ † ” and “ ‡ ” indicate that a run significantly underperforms the one shown below ( α = 0.05 and α = 0.01 , respectively ) , contrary to the mean performance ranking . Note that statistical significance is not transitive .
↑ 7 ↑ 7 ↓ 2 ↓ 2 ↓ 2 ↓ 1 ↓ 3 ↓ 1 ↓ 3 ↑ 1 ↑ 1 ↓ 2
( b ) GAW nG@1 j3 j2 M 2 M 1 M 4 A 2 B 2 A 1 M 3 j1 j4 B 3 B 1 L 3 L 2 L 1
( c ) GAW nDCG j3 0.9567 0.9446 M 2 0.9288 M 4 j2 0.9278 0.9276 M 1 0.9251 A 2 0.9242 A 1 0.9238∗∗ B 2 0.9076∗∗ M 3 j1 0.8916 0.8814∗ j4 0.8460∗∗ B 3 0.8057∗∗ B 1 L 2 0.7354 L 3 0.7354 0.7354 L 1
↑ 7 ↓ 1 ↑ 5 ↓ 3 ↓ 1 ↓ 4 ↓ 3 ↑ 1 ↑ 1 ↓ 2 ↑ 1 ↓ 1
( d ) GAW Q ↑ 7 j3 0.9857 ↓ 1 0.9797 M 2 0.9795∗∗ M 4 ↑ 1 0.9794‡ A 2 ↓ 3 0.9791† M 1 0.9790∗∗ A 1 ↑ 1 ↓ 3 B 2 0.9785 ↑ 1 0.9784∗∗ j2 ↓ 3 0.9744∗∗ M 3 ↑ 1 j1 0.9724 0.9699∗∗ ↑ 1 j4 ↓ 2 0.9576∗∗ B 3 0.9455∗∗ B 1 ↑ 1 0.9365∗∗ L 2 0.9325∗∗ ↓ 1 L 3 0.9291 L 1
( a ) BA Hit@1 0.4980 M 2 M 1 0.4980 0.4847 M 4 0.4847 B 2 0.4840 A 2 M 3 0.4813 0.4813† A 1 j3 0.4353 j2 0.4187 B 3 0.3820 0.3280∗ j1 j4 0.3020 0.2713∗∗ B 1 0.1767 L 3 0.1767 L 2 L 1 0.1767
0.9760 0.9688 0.9687 0.9683∗ 0.9679 0.9674 0.9673∗∗ 0.9646 0.9609∗∗ 0.9573 0.9538∗∗ 0.9366∗∗ 0.9172 0.9094∗∗ 0.9015∗∗ 0.8944
GAW Q also produce very similar rankings ( as they are both graded relevance metrics that evaluate the entire answer list ) ;
2 . The run ranking based on BA Hit@1 also resembles the GAWbased rankings , except that the performances of the four judges are underestimated ( assuming the GAW results are more reliable ) ;
3 . The performances of the four judges vary widely : in particular , note that j1 and j4 underperform the simple , length based baseline B 2 in terms of all metrics ;
4 . According to the GAW based metrics , all runs except B {1 , 3} and L {1 , 2 , 3} do “ as well as the humans ” in that they lie between the judges’ performances .
The second observation is not altogether surprising , as the GAW data were constructed based on the judgments of these very judges . We shall later discuss the evaluation of humans that did not contribute to the gold standard . The third observation suggests , as the aforementioned Kappa values did , that answer assessments are very subjective , and that relying on one person ’s judgments for evaluation may not be reliable .
We also compared our GAW based metrics and BA Hit@1 in terms of of discriminative power [ 11 ] . Ideally , we want evaluation metrics that are robust to variation across questions , so that the same conclusion can be reached as to which of two given systems is better , regardless of the choice of the question set . More precisely , we measure discriminative power by conducting a statistical significance test for every pair of runs , and counting the number of significantly different pairs at α = 005 As was mentioned earlier , we tested 120 run pairs with the two sided sign test . Note that discriminative power is not about whether the metrics are right or wrong ; it is about how metrics can be consistent across experiments and as a result how often differences between systems can be detected with high confidence .
Sanderson [ 16 ] argues that discriminative power is not useful when , for example , the “ metric ” in question sorts systems alphabetically by the system name as this can make consistent judgments regardless of the data used . However , we are interested in metrics that are strictly functions of a ranked list of items ( ie system output ) and a set of judged items ( ie right answers ) . We are not in
BA Hit@1 underestimates j1 j4
GAW nG@1
GAW nDCG
GAW Q
BA Hit@1
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1 j3 j2 M 2 M 1 M 4 A 2 B 2 A 1 M 3 j1 j4
B 3 B 1 L 3 L 2 L 1
Figure 1 : System ranking based on GAW nCG@1 vs . based on GAW nDCG , GAW Q and BA Hit@1 . those terested in a “ metric ” that knows that one ranked list is from ( say ) Google and that the other is from Bing , and uses this knowledge to consistently decide which is better than the other .
BA Hit@1 detected 59 significantly different run pairs ( 49 % of all system pairs ) . In contrast , our GAW based metrics were substantially more discriminative : GAW nG@1 detected 95 significant differences ( 79% ) ; GAW nDCG and GAW Q each detected 100 significant differences ( 83% ) . Table 5 breaks down these significantly different pairs by comparing across two metrics . For example , BA Hit@1 and GAW nG@1 have 58 significantly different pairs in common ; for only 1 pair of runs BA Hit@1 detected a sigfinicant difference while GAW nG@1 did not ; for as many as 37 run pairs GAW nG@1 detected a significant difference while BA Hit@1 did not . Looking across the first row of Table 5 , it is clear that GAW based metrics can detect many substantial differences between systems that BA Hit@1 fails to see . The table also shows that the different GAW metrics suggest similar conclusions : in particular , GAW nDCG and GAW Q yield almost identical set of significantly different run pairs .
Table 4 : Kendall ’s τ between BA and GAW metrics ( ranking 16 systems ) .
BA Hit@1 GAW nG@1 GAW nDCG
0.700
GAW nG@1 GAW nDCG GAW Q 0.733 0.833 0.917
0.683 0.917
Table 5 : Comparison of significantly different pairs between two metrics : significant with Metric 1 but not with Metric 2 / significant with both metrics / significant with Metric 2 but not with Metric 1 .
BA Hit@1 GAW nG@1
GAW nDCG
GAW nG@1 GAW nDCG GAW Q 1/58/37 7/52/48 9/86/14 ( 1 conflict ) 1/99/1
6/53/47 8/87/13 ( 1 conflict )
Table 6 : Distribution of answers over relevance levels for the leave one assessor out data .
LOO1 1366 1647 1963 2081 272 82 32 7443
L6 L5 L4 L3 L2 L1 L0 total
LOO2 2091 2015 1574 1406 268 70 19 7443
LOO3 1808 2180 1689 1501 171 68 26 7443
LOO4 1446 1737 2077 1786 280 93 24 7443
Interestingly , there was exactly one conflict between GAW nG@1 and GAW nDCG , and between GAW nG@1 and GAW Q , as indicated in Table 5 : that is , j2 significantly outperformed A 1 according to GAW nG@1 , but significantly underperformed A 1 according to GAW nDCG and GAW Q . This is not unnatural , since GAW nG@1 is a metric for systems designed to return exactly one good answer , while the other two are for systems designed to rank answers . There was no other conflict in our experiments . 512 Leaving Out One Judge The “ systems ” j1 j4 were evaluated with the GAW data in Table 3 , but we may have overestimated their performances since the GAW data set was constructed based on judgments from these very judges . In order to investigate this effect , we conducted additional leave one out experiments as described below .
We created four new gold standard data sets , by ignoring the assessments of one judge at a time . Thus , we used a table similar to Table 1 , but with relevance patterns “ AAA ” , “ AAB ” , “ ABB ” etc . , and relevance levels L0 through L6 . Gain values of 1 6 were assigned to L1 L6 . The gold standard data thus built without using the assessments from J1 is referred to as LOO1 ( Leave One Out Judge 1 ) . The other three data sets were named similarly . Furthermore , for example , nG@1 based on the LOO1 data will be referred to as LOO1 nG@1 . Table 6 shows the distribution of relevant answers over relevance levels for our LOO data sets .
Table 7 summarises the results of our LOO experiments . For each metric , the entire ranking has been compared to the case with the GAW data , and the rank changes are indicated by “ ⇑ 1 ” and so on . Statistically significant differences are indicated as before . Figures 2 4 compare the rankings based on each LOO data with those based on GAW for each metric . Table 8 quantifies the comparisons in terms of Kendall ’s τ .
GAW nG@1 LOO1 nG@1 LOO2 nG@1 LOO3 nG@1 LOO4 nG@1
1
0.95
0.9
0.85
0.8
0.75
0.7
The left out judge performs poorly for each leave one out data j3 j2 M 2 M 1 M 4 A 2 B 2 A 1 M 3
L 1 L 2 Figure 2 : System ranking based on GAW nCG@1 vs . those based on LOO1 nCG@1 , LOO2 nCG@1 , LOO3 nCG@1 and LOO4 nCG@1 .
B 3 B 1 L 3 j1 j4
1
GAW nDCG LOO1 nDCG LOO2 nDCG LOO3 nDCG LOO4 nDCG
0.99
0.98
0.97
0.96
0.95
0.94
0.93
0.92
The left out judge performs poorly for each leave one out data j3 M 2 M 4 j2 M 1 A 2 A 1 B 2 M 3
L 1 L 3 Figure 3 : System ranking based on GAW nDCG vs . those based on LOO1 nDCG , LOO2 nDCG , LOO3 nDCG and LOO4 nDCG .
B 3 B 1 L 2 j1 j4
GAW Q LOO1 Q LOO2 Q LOO3 Q LOO4 Q
1
0.98
0.96
0.94
0.92
0.9
0.88
The left out judge performs poorly for each leave one out data j3 M 2 M 4 A 2 M 1 A 1 B 2 j2 M 3 j1 j4
B 3 B 1 L 2 L 3 L 1
Figure 4 : System ranking based on GAW Q vs . those based on LOO1 Q , LOO2 Q , LOO3 Q and LOO4 Q .
⇑ 1 ⇓ 1
⇑ 1 ⇓ 1
⇑ 1 ⇓ 1
⇑ 3
Table 7 : Mean performances based on the LOO data , presented in a way similar to Table 3 . The rank changes relative to the corresponding GAW ranking is indicated by ⇑ ’s and ⇓ ’s .
⇑ 1 ⇓ 1
⇑ 1 ⇓ 1
⇑ 2 ⇓ 1 ⇓ 1
⇑ 1 ⇑ 2 ⇑ 1 ⇑ 2 ⇓ 6
( a1 ) LOO1 nG@1 j3 j2 M 2 M 1 M 4 B 2 A 2 A 1 M 3 j4 j1 B 3 B 1 L 3 L 2 L 1 ( b1 ) LOO1 nDCG j3 j2 M 2 M 4 M 1 A 2 B 2 A 1 M 3 j4 j1 B 3 B 1 L 2 L 3 L 1 ( c1 ) LOO1 Q j3 j2 M 4 M 2 A 2 M 1 B 2 A 1 M 3 j4 j1 B 3 B 1 L 2 L 3 L 1
( a2 ) LOO2 nG@1 j3 0.9565 0.9438∗∗ M 2 M 4 0.9162 M 1 0.9151 A 2 0.9149 0.9124 A 1 0.9115 B 2 0.9108∗∗ j2 0.8965∗∗ M 3 0.8729∗∗ j1 j4 0.8440 0.8302∗∗ B 3 0.7867∗∗ B 1 L 3 0.7158 L 2 0.7158 0.7158 L 1 ( b2 ) LOO2 nDCG 0.9865∗∗ j3 0.9791† M 2 M 4 0.9763 0.9761 M 1 A 2 0.9756 0.9754∗ A 1 0.9753 B 2 0.9750∗∗ j1 0.9711∗∗ M 3 0.9684∗∗ j4 j2 0.9582 0.9532∗∗ B 3 0.9398 B 1 0.9312∗∗ L 2 0.9269∗∗ L 3 0.9234 L 1 ( c2 ) LOO2 Q 0.9789∗∗ j3 0.9664‡ M 4 M 2 0.9660 A 2 0.9659 0.9653 M 1 0.9650 A 1 0.9649 B 2 0.9645∗∗ j1 0.9587∗∗ M 3 0.9547∗∗ j4 j2 0.9399 0.9344∗∗ B 3 0.9140 B 1 0.9076∗∗ L 2 0.8993∗∗ L 3 0.8924 L 1
⇑ 6 ⇓ 2 ⇓ 1 ⇓ 1 ⇓ 2 ⇑ 1 ⇓ 1
⇑ 1 ⇑ 1 ⇑ 1 ⇑ 1 ⇑ 2 ⇑ 1 ⇓ 7
⇑ 1 ⇓ 1 ⇑ 1 ⇓ 1
⇑ 2 ⇑ 1 ⇓ 3
⇑ 1 ⇓ 1
⇑ 1 ⇓ 1
⇑ 2 ⇑ 4
0.9543 0.9406 0.9243 0.9241 0.9235 0.9202 0.9195 0.9184∗∗ 0.8989∗∗ 0.8819∗∗ 0.8339∗ 0.8317∗∗ 0.7912∗∗ 0.7104 0.7104 0.7104 0.9857∗ 0.9789∗∗ 0.9788‡ 0.9787 0.9784† 0.9780 0.9774 0.9773∗∗ 0.9723∗∗ 0.9706∗∗ 0.9553† 0.9546∗∗ 0.9413∗∗ 0.9303∗∗ 0.9258∗∗ 0.9218
⇑ 1 ⇑ 1 ⇑ 1 ⇑ 1 ⇑ 1 ⇓ 5 ⇑ 1 ⇓ 1 ⇑ 1 ⇓ 1
( a3 ) LOO3 nG@1 0.9724∗∗ j2 0.9293 M 2 0.9281 M 1 0.9276 M 4 0.9255 A 2 j3 0.9247 0.9246∗∗ A 1 B 2 0.9139 0.9086∗∗ j1 M 3 0.9079 0.8985∗∗ j4 0.8502∗∗ B 3 0.8123∗∗ B 1 L 3 0.7427 L 2 0.7427 0.7427 L 1 ( b3 ) LOO3 nDCG 0.9885∗∗ j2 0.9789 M 2 0.9788∗∗ M 4 0.9783‡ A 2 0.9783∗ M 1 j1 0.9780 0.9777∗∗ A 1 0.9758‡ B 2 0.9739∗∗ j3 j4 0.9734 0.9700∗∗ M 3 0.9573∗∗ B 3 0.9461∗∗ B 1 0.9369∗∗ L 2 0.9332∗∗ L 3 0.9301 L 1 ( c3 ) LOO3 Q 0.9805∗∗ j2 0.9685 M 2 0.9684† A 2 0.9681∗∗ M 4 M 1 0.9675 0.9674 A 1 0.9670∗∗ j1 0.9633‡ B 2 0.9614∗∗ j4 0.9601∗∗ j3 0.9521∗∗ M 3 0.9378∗∗ B 3 0.9205∗ B 1 0.9126∗∗ L 2 0.9053∗∗ L 3 0.8991 L 1
( a4 ) LOO4 nG1 0.9643∗∗ j3 j2 0.9285 0.9283 M 2 0.9266 M 4 0.9265∗∗ M 1 0.9261† A 2 0.9245 B 2 0.9234∗∗ A 1 0.9157‡ M 3 0.9102∗∗ j1 0.9071∗∗ B 3 0.8550∗∗ j4 0.8188∗∗ B 1 L 3 0.7602 L 2 0.7602 0.7602 L 1 ( b4 ) LOO4 nDCG 0.9846∗ j3 0.9793 M 2 j2 0.9790 0.9790∗ M 4 0.9789∗∗ M 1 0.9788‡ A 2 0.9784 B 2 0.9780∗∗ A 1 0.9772 M 3 0.9765† j1 0.9747∗∗ j4 0.9597∗∗ B 3 0.9490∗ B 1 0.9416∗∗ L 2 0.9382∗∗ L 3 L 1 0.9352 ( c4 ) LOO4 Q j3 M 2 M 4 A 2 M 1 B 2 A 1 j2 M 3 j1 B 3 j4 B 1 L 2 L 3 L 1
0.9741 0.9700 0.9697 0.9694 0.9693 0.9688∗∗ 0.9683‡ 0.9683∗∗ 0.9653 0.9645‡ 0.9633∗∗ 0.9423∗∗ 0.9256 0.9192∗∗ 0.9128∗∗ 0.9068
⇑ 3 ⇓ 1 ⇑ 2 ⇓ 9 ⇓ 2
0.9776 0.9697 0.9696 0.9692∗ 0.9689† 0.9684 0.9682∗∗ 0.9659 0.9605∗∗ 0.9574∗∗ 0.9365∗ 0.9364∗∗ 0.9165 0.9073∗∗ 0.8987∗∗ 0.8908
⇑ 7 ⇑ 1 ⇓ 1
⇓ 8 ⇑ 1 ⇓ 2
⇑ 1 ⇓ 1
⇑ 1 ⇓ 1
Table 8 : Kendall ’s τ between GAW and LOO metrics ( ranking 16 systems ) .
GAW nG@1 GAW nDCG GAW Q
LOO1 0.967 0.933 0.850
LOO2 0.867 0.867 0.917
LOO3 0.883 0.750 0.683
LOO4 0.967 0.967 0.967
From these results , we can observe that :
1 . When evaluated based on a LOO data set , the judge who has been left out performs relatively poorly . In particular , Table 7(a2 ) , ( b2 ) and ( c2 ) show that leaving out J2 makes j2 go down 6 , 7 , 3 ranks in terms of nG@1 , nDCG and Q , respectively ; and Table 7(a3 ) , ( b3 ) and ( c3 ) show that leaving out J3 makes j3 go down 5 , 8 , 9 ranks in terms of nG@1 , nDCG and Q , respectively . The fall is not so severe for j1 and j4 as they perform relatively poorly even in terms of GAWbased evaluation .
2 . As shown in Figures 2 4 , apart from the above rank changes , the LOO based rankings are generally similar to the original GAW based rankings . It is still true that many runs lie between different human performances .
The second observation is also interesting in that while the GAW data set uses four judges and 9 point scale graded relevance , the LOO data sets use three judges and 7 point scale graded relevance . While our approach remedies the bias and nonexhaustiveness problems of BA based evaluation , it is not clear how many judges and how many relevance levels should be used for cost effective evaluation . These questions should be addressed in future work .
The bottom line of these system ranking experiments is that human performances do vary widely , and by combining different views of these humans and yet preserving them in the form of gradedrelevance data , we can observe meaningful system differences which BA based evaluation often overlooks . 5.2 Ranking Questions
This section discusses ranking questions by hardness , where hardness of a question is defined as the performance averaged over the submitted runs . Ranking questions in this way will help researchers to identify hard questions for failure analysis and system improvement . In this paper , to rank the 1,500 questions from the NTCIR 8 CQA task , we take the average over the 12 runs that were submitted to the task ( again , after excluding one anomalous run ) . We examine the GAW based metrics as well as BA Hit@1 .
Figures 5 8 visualise the correlation between two question rankings based on two different metrics . ( The graphs showing the correlation between BA Hit@1 and GAW Q and that between GAWnG@1 and GAW Q are omitted due to lack of space . They look very similar to that for BA Hit@1 and GAW nDCG and that for GAW nG@1 and GAW nDCG , respectively . ) For example , in Figure 5 , each dot represents a question , and the x and y axes represent BA Hit@1 and GAW nG@1 , respectively . ( Note that Average BAHit@1 can either be 1 , 11/12 , 10/12 , . . . , 0 as we are averaging a binary flag over 12 runs . ) It can be observed that :
1 . Questions that are easy according to BA Hit@1 are also easy according to the GAW based metrics . ( High Average BAHit@1 values imply high Average GAW nCG@1 values , etc . )
2 . Questions that are hard according to BA Hit@1 may not necessarily be hard according to the GAW based metrics . ( Low Average BA Hit@1 values do not say anything about Average GAW nCG@1 values , etc . )
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
BA Hit@1 ( x axis ) vs GAW nG@1 ( y axis )
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Figure 5 : Scatterplot of question hardness : average BA Hit@1 vs . average GAW nG@1 .
1
0.95
0.9
0.85
0.8
0.75
0.7
BA Hit@1 ( x axis ) vs GAW nDCG ( y axis )
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Figure 6 : Scatterplot of question hardness : average BA Hit@1 vs . average GAW nDCG .
3 . The GAW based question rankings are highly correlated with one another . ( Kendall ’s τ between the GAW nG@1 and GAWnDCG rankings are 0.825 ; that between the GAW nG@1 and GAW Q rankings are 0.742 ; that between the GAW nDCG and GAW Q rankings are 0891 )
The first observation suggests that if systems have found the BA for a question , they would also have found ( other ) highly relevant answers successfully . The second observation suggests that systems often find highly relevant answers that are not the BA .
The above observations essentially mean that using the GAWbased evaluation , we can identify hard questions that are worth investigating for failure analysis and improvement . Whereas , a hard question according to BA based evaluation may not necessarily be important , because the system may have already found highly relevant answers that are in fact at least as good as the BA . To further discuss the differences between BA based and GAWbased evaluation from the viewpoint of question hardness , let us define easy , medium and hard questions for each metric as the top 500 , medium 500 and bottom 500 questions after a sort by average performance . Moreover , we utilise the question categories [ 5 , 14 ] in order to discuss which categories tend to be easy/hard .
Figures 9 12 visualise the frequency of easy/medium/hard questions for each question category for each evaluation metric . For convenience , we define an easy category as one in which easy
1
0.95
0.9
0.85
0.8
0.75
0.7
100
BA Hit@1
“ easy ” categories “ hard ” categories
80
60
40
20
0
GAW nG@1 ( x axis ) vs GAW nDCG ( y axis ) easy medium hard
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Figure 7 : Scatterplot of question hardness : average GAWnG@1 vs . average GAW nDCG .
1
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6
Figure 9 : Question hardness by category : BA Hit@1 .
100
GAW nG@1
“ easy ” categories “ hard ” categories
80
60
40
20
0
GAW nDCG ( x axis ) vs GAW Q ( y axis ) easy medium hard
0.7
0.75
0.8
0.85
0.9
0.95
1
Figure 8 : Scatterplot of question hardness : average GAWnDCG vs . average GAW Q . questions are more frequent than medium questions and medium questions are more frequent than hard questions . Similarly , we define an hard category as one in which hard questions are more frequent than medium questions and medium questions are more frequent than easy questions . The arrows in the figures visualise these trends , from which we can observe , for example , that while the LOVE category is hard according to BA Hit@1 , it is easy according to GAW nG@1 and GAW Q11 . This may because many LOVE questions often elicit different opinions and there is no one single true answer . Thus , even if the system fails to find the BA ( the answer that the asker liked ) , it can find other good answers . We have actually looked at LOVE questions for which Average BA Hit@1 was 0 while Average GAW nCG@ was 1 ( ie all systems returned a most highly relevant non BA at rank 1 ) : there were three such questions among the 120 LOVE questions12 . All of these questions indeed elicited different opinions , eg “ What do you women think about a man bragging about , ” “ Do you think I’m being bullied ? , ” “ Don’t you think that that woman [ who appeared in a TV show ] is scary ? ” This suggests that BA based evaluation is insufficient especially for questions that elicit subjective answers .
11Similar discrepancies between BA and GAW exist for TRAVEL and EDUCATION categories [ 14 ] . 12Q_ID=383311 , 1395988 , 5442340 .
Figure 10 : Question hardness by category : GAW nG@1 .
6 . CONCLUSIONS
This paper proposed new evaluation methods for the task of selecting/ranking answers for a given question , and demonstrated the advantages over zero cost evaluation using the BA data . To remedy the bias and nonexhaustiveness problems of BA based evaluation , we hired four assessors to assess every answer in a 3 point scale ( A , B , C ) , and automatically constructed a pyramid like gradedrelevance data set with a 9 point scale ( L0 L8 ) . Finally , we applied graded relevance metrics for IR evaluation . Our main findings from experiments using the NTCIR 8 CQA task data are that , using our evaluation methods , ( a ) we can detect many substantial differences between systems that would have been overlooked by BA based evaluation ; and ( b ) we can better identify hard questions ( ie those that are handled poorly by many systems and therefore require focussed investigation ) compared to BA based evaluation . We therefore argue that our approach is useful for building effective CQA answer selection systems despite the cost of manual answer assessments .
Although we have observed that human assessments and answer ranking performances vary widely and therefore that relying on multiple assessors for evaluation is worthwhile , we know neither the optimal number of assessors for cost effective evaluation , nor the effect of the degree of inter assessor disagreements on evaluation . Also , the effects of using different answer assessment criteria ( eg redundancy , style and language ) on inter assessor agreement
[ 5 ] Ishikawa , D . , Sakai , T . and Kando , N . : Overview of the NTCIR 8 Community QA Pilot Task ( Part I ) : The Test Collection and the Task , NTCIR 8 proceedings , pp . 421 432 ( 2010 ) .
[ 6 ] Järvelin , K . and Kekäläinen , J . : Cumulated Gain Based Evaluation of IR Techniques , ACM TOIS , Vol . 20 , No . 4 , pp . 422 446 ( 2002 ) .
[ 7 ] Jeon , J . , Croft , W . B . , Lee , J . H . and Park , S . : A Framework to Predict the Quality of Answers with Non Textual Features , ACM SIGIR 2006 Proceedings , pp . 228 235 ( 2006 ) .
[ 8 ] Lin , J . and Demner Fushman , D . : Will Pyramids Built of
Nuggets Topple Over ? HLT/NAACL 2006 Proceedings , pp . 383 390 ( 2006 ) .
[ 9 ] Liu , Y . , Li , S . , Cao , Y . , Lin , C Y , Han , D . and Yu , Y . :
Understanding and Summarizing Answers in Community based Question Answering Services , COLING 2008 Proceedings , pp . 497 504 ( 2008 ) .
[ 10 ] Nenkova , A . , Passonneau , R . and McKeown , K . : The
Pyramid Method : Incorporating Human Content Selection Variation in Sumarization Evaluation , ACM Transactions on Speech and Language Processing , Volume 4 , Number 2 , Article 4 ( 2007 ) .
[ 11 ] Sakai , T . : Evaluating Evaluation Metrics based on the Bootstrap , ACM SIGIR 2006 Proceedings , pp . 525 532 ( 2006 ) .
[ 12 ] Sakai , T . : On Penalising Late Arrival of Relevant Documents in Information Retrieval Evaluation with Graded Relevance , Proceedings of the First Workshop on Evaluating Information Access ( EVIA 2007 ) , pp . 32 43 ( 2007 ) .
[ 13 ] Sakai , T . and Robertson , S . : Modelling A User Population for Designing Information Retrieval Metrics , Proceedings of the Second Workshop on Evaluating Information Access ( EVIA 2008 ) , pp . 30 41 ( 2008 ) .
[ 14 ] Sakai , T . , Ishikawa , D . and Kando , N . : Overview of the NTCIR 8 Community QA Pilot Task ( Part II ) : System Evaluation , NTCIR 8 Proceedings , pp . 433 457 ( 2010 ) .
[ 15 ] Sakai , T . , Ishikawa , D . , Seki , Y . , Kando , N . and Kuriyama ,
K . : Selecting Good Answers for Community QA : A Note on Evaluation Methods ( in Japanese ) , Forum on Information Technology 2010 , pp . 13 20 ( 2010 ) .
[ 16 ] Sanderson , M . : Test Collection Based Evaluation of
Information Retrieval Systems , Foundations and Trends in Information Retrieval , Vol . 4 , No . 4 , pp . 247 375 ( 2010 ) . [ 17 ] Suryanto , M . A . , Lin , E P , Sun , A . and Chiang , R . H . L . :
Quality Aware Collaborative Question Answering : Methods and Evaluation , ACM WSDM 2009 Proceedings , pp . 142 151 ( 2009 ) .
[ 18 ] Wang , X J et al . : Ranking Community Answers by
Modeling Question Answer Relationships via Analogical Reasoning , ACM SIGIR 2009 Proceedings , pp . 179 186 ( 2009 ) .
[ 19 ] Zhang , J . , Ackerman , M . S . and Adamic , L . : Expertise
Networks in Online Communities : Structure and Algorithms , ACM WWW 2007 Proceedings , pp . 221 230 ( 2007 ) .
100
GAW nDCG
“ easy ” categories “ hard ” categories
80
60
40
20
0 easy medium hard
Figure 11 : Question hardness by category : GAW nDCG .
100
GAW Q
“ easy ” categories “ hard ” categories
80
60
40
20
0 easy medium hard
Figure 12 : Question hardness by category : GAW Q . and evaluation outcome may be worth investigating . Moreover , while we used the Yahoo! Chiebukuro Japanese CQA data with the NTCIR 8 CQA task data , it would be interesting to explore how far our findings can extend to different data and to other related CQA tasks . These questions should be pursued in our future work .
7 . ACKNOWLEDGMENTS
Yahoo! Chiebukuro Data , provided to National Institute of Informatics by Yahoo Japan Corporation , was used in the implementation of this research .
8 . REFERENCES [ 1 ] Agichtein , E . et al . : Finding High Quality Content in Social Media , ACM WSDM 2008 Proceedings , pp . 183 194 ( 2008 ) .
[ 2 ] Agichtein , E . , Liu , Y . and Bian , J . : Modeling
Information Seeker Satisfaction in Community Question Answering , ACM TKDD , Volume 3 , Issue 2 , Article No.10 ( 2009 ) .
[ 3 ] Cao , Y . et al . : Recommending Questions Using the
MDL based Tree Cut Model , ACM WWW 2008 Proceedings , pp . 81 90 ( 2008 ) .
[ 4 ] Gyöngyi , Z . , Koutrika , G . , Pedersen , J . and Garcia Molina , H . : Questioning Yahoo! Answers , QAWeb 2008 Proceedings ( 2008 ) .
