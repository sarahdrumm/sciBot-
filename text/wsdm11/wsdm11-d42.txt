Multidimensional Mining of Large Scale Search Logs : A
Topic Concept Cube Approach
Dongyeop Kang1 Daxin Jiang2 Jian Pei3 Zhen Liao4 Xiaohui Sun2 Ho Jin Choi1 Hang Li2
1Korea Advanced Institute of Science and Technology
2Microsoft Research Asia
3Simon Fraser University
4Nankai University
Email : 1{dykang,hojinc}@kaistackr
2{djiang , xiaos , hangli}@microsoft.com
3jpei@cssfuca
4liaozhen@mailnankaieducn
ABSTRACT In addition to search queries and the corresponding clickthrough information , search engine logs record multidimensional information about user search activities , such as search time , location , vertical , and search device . Multidimensional mining of search logs can provide novel insights and useful knowledge for both search engine users and developers . In this paper , we describe our topic concept cube project , which addresses the business need of supporting multidimensional mining of search logs effectively and efficiently . We answer several challenges . First of all , search queries and click through data are well recognized sparse , and thus have to be aggregated properly for effective analysis . At the same time , there is often a gap between the topic hierarchies in multidimensional aggregate analysis and queries in search logs . To address those two challenges , we develop a novel topic concept model which learns a hierarchy of concepts and topics automatically from search logs . Enabled by the topic concept model , we construct a topic concept cube which supports online multidimensional mining of search log data . A distinct feature of our approach is that , in addition to the standard dimensions such as time and location , our topic concept cube has a dimension of topics and concepts , which substantially facilitates the analysis of log data . To handle a huge amount of log data , we develop distributed algorithms for learning model parameters efficiently . We also devise approaches for computing a topic concept cube . We report an empirical study verifying the effectiveness and efficiency of our approach on a real data set of 1.96 billion queries and 2.73 billion clicks .
1 .
INTRODUCTION
Search logs in search engines record rich information about user search activities . In addition to search queries and the corresponding click through information , the related information is also recorded on multiple attributes , such as search time , location , vertical , and search device . Multidimensional mining of such rich search logs can provide novel insights and useful knowledge for both search engine users and developers . As a concrete motivation example , let us consider the following two multidimensional analysis tasks .
A multidimensional lookup ( lookup for short ) specifies a subset of user queries and clicks using multidimensional constraints such as time , location and general topics , and requests for the aggregation of the user search activities . For example , by looking up “ the top 5 electronics that were most popularly searched by the users in the US in December , 2009 ” , a business analyst can know the common interests of search engine users on topic “ Electronics ” . Moreover , search engine developers can use the results from the lookup to improve query suggestion , document ranking , and sponsored search . Multidimensional lookups can be extended in many ways to achieve advanced business intelligence analysis . For example , using multiple lookups with different multidimensional constraints , one may compare the major interests about electronics from users in different regions such as the US , Asia , and Europe .
A multidimensional reverse lookup ( reverse lookup for short ) is concerned about the multidimensional groupbys where one specific object is intensively queried . For example , using reverse lookup “ What are the group bys in time and region where Apple iPad was popularly searched for ? ” , an iPad accessory manufacturer can find the regions where the accessories may have a good market . Using the results from the reverse lookup , a search engine can improve its service by , for example , locality sensitive search . Again , reverse lookups can be used to compose advanced business intelligence analysis . For example , by organizing the results from the reverse lookup about iPad , one may keep track of how iPad becomes popular in time and in region , and also compare the trend of iPad with those of iPod and iPhone . This is interesting to both business parties and users .
As search engines have accumulated rich log data , it becomes more and more important to develop a service which supports multidimensional mining of search logs effectively and efficiently . To answer multidimensional analytical queries online , a data warehousing approach is a natural choice , which pre computes all multidimensional aggregates offline . However , traditional data warehouse approaches only explore a series of statistical aggregates such as MIN , MAX , and AVG ; they cannot summarize the semantic information of user queries and clicks . In particular , multidimensional analysis on search log data presents two special challenges .
Challenge 1 : sparseness of queries in log data . Queries in search engine logs are usually very sparse , since users may formulate different queries for the same information need [ 8 ] . For example , to search for Apple iPad , users may issue queries such as “ ipad ” , “ apple ipad ” , “ ipad 32g ” , “ i pad apple ” , and so on . Aggregating only on individual queries cannot summarize user information needs recorded
1 2 3 4 5 ipad apple ipad ipad 32g kindle amazon kindle
( a )
1 2 3 4 5 ipad kindle iphone xbox 360 wii
( b )
Table 1 : Answers to “ the top 5 electronics that were most popularly searched by the users in the US in December , 2009 ” by ( a ) individual queries and ( b ) concepts . in logs comprehensively . For example , when a business analyst asks for “ the top 5 electronics that were most popularly searched by the users in the US in December , 2009 ” , a na¨ıve method may simply count the frequency of the queries in the topic of “ Electronics ” and return the top 5 most frequently asked queries . Due to the sparseness of queries in the logs , the analyst may get an answer with many redundant queries , such as the one shown in Table 1(a ) . Instead , if we can summarize the various query formulations of the same information need and provide non duplicate answers ( eg , Table 1(b) ) , the user experience can be improved greatly . Similarly , in reverse lookup , when an iPad accessary manufacturer asks the question “ What are the group bys in time and region where Apple iPad was popularly searched for ? ” , the system should consider not only aggregates of the query “ Apple iPad ” but also its various formulations . To address the sparseness of log data , we have to aggregate queries and click through data in logs .
Challenge 2 : mismatching between topic hierarchies used in analytics and learned from log data . More often than not , people use different topic hierarchies in searching detailed information and summarizing analytic information . For example , when users search electronics on the web , often the queries are about specific products , brand names , or features . A query topic hierarchy automatically learned from log data in a data driven way depends on the distribution and occurrences of such queries . “ Apple products ” may be a popular topic . When an analyst explores a huge amount of log data , she may bear in her mind a product taxonomy ( eg , a well adopted ontology ) , such as TV & video , audio , mobile phones , cameras & camcorders , computers , and so on being the first level categories . The analytic topic hierarchy may be very different from the query topic hierarchy learned from log data . For example , the “ Apple products ” in the query topic hierarchy corresponds to multiple topics in the analytic topic hierarchy . This mismatching in topic hierarchies is partly due to the different information needs in web search and web log data analysis . Web searches often opt for detailed information , while web log analysis usually tries to summarize and characterize popular user behavior patterns . To bridge the gap , we need to map the aggregates from logs to an analytic topic hierarchy . In this paper , we describe our topic concept cube project which builds a multidimensional service on search log data . In this project , we answer a few challenges such as the two just mentioned , and make the following contributions .
First , we tackle the sparseness of queries in logs and the gap between concept taxonomy in analytics and queries in logs by a novel concept topic model . Figure 1 illustrates our ideas . We first mine click through information in search logs and group similar queries into concepts . Intuitively , users with the same information need tend to click on the same URLs . Therefore , various query formulations , for example ,
Figure 1 : The hierarchy of topics , concepts , queries , and clicks . of Apple ipad , such as “ ipad ” , “ apple ipad ” , “ ipad 32g ” , and “ i pad apple ” can be grouped into the same concept since all of them lead to clicks on the web page wwwapplecom/ipad More interestingly , some misspelled queries , such as “ apple ipda ” and “ apple ipade ” , can also be clustered into this concept , since they also lead to clicks on the ipad page . Once we summarize queries and clicks into concepts , we will answer lookups and reverse lookups by concepts instead of individual queries . For each concept , we use the most frequently asked query as the representative of the concept . In this way , we can effectively avoid redundant queries in lookup answers . At the same time , we can effectively cover all relevant queries in reverse lookup answers .
Our concept topic model further maps concepts to topics in a given taxonomy , which is essentially a query classification problem . For example , suppose a concept consists of queries “ apple ipad ” , “ ipad 32g ” , etc , we want to classify them into the topic “ Electronics ” . Compared with classifying individual queries to topics , mapping concepts has several advantages . For example , for a misspelled query “ apple ipda ” , the classification problem becomes much easier once we know this query belongs to a concept which also contains other queries such as “ apple ipad ” . Moreover , through the content of the web pages that are commonly clicked as answers to the queries in the concept , we may further enrich the features to classify “ apple ipda ” .
Our concept topic model provides the “ semantic ” aggregates for search log data . Those concepts and topics not only provide us a meaningful way to answer lookups and reverse lookups , but also serve as an important dimension for multidimensional analysis and exploration .
Second , to handle large volumes of search log data , which may contain billions of queries and clicks , we develop distributed algorithms to learn the topic concept models efficiently . In particular , we develop a strategy to initialize the model parameters such that each machine only needs to hold a subset of parameters much smaller than the whole set .
Third , to serve online multidimensional mining of search log data , we build a topic concept cube . In addition to the standard dimensions such as time and location , a topicconcept cube has a dimension of topics and concepts . We devise effective approaches for computing a topic concept cube . In particular , queries are assigned to a hierarchy of concepts and topics in the materialization of the cube .
Finally , we conduct extensive experiments on a real log data set containing 1.96 billion queries and 2.73 billion clicks . We examine the effectiveness of the topic concept model as well as the efficiency and scalability of our training algorithms . We also demonstrate several concrete examples of lookups and reverse lookups answered by our topic concept cube system . The experimental results clearly show that our approach is effective and efficient .
The rest of the paper is organized as follows . We review
Topic TaxonomyConceptsQuery & Clicks the related work in Section 2 , and present the framework of our system in Section 3 . We describe the topic concept model in Section 4 , and develop the distributed algorithms for learning the topic concept model from large scale log data in Section 5 . Section 6 discusses our approaches to computing the topic concept cube . We report the experimental results in Section 7 , and conclude the paper in section 8 .
2 . RELATED WORK
Supporting multidimensional analysis of large scale search log data online is a new problem . To the best of our knowledge , the most related work to our project is a query traffic analysis service provided by a major commercial search engine1 . The service allows users to look up and compare the hottest queries in specified time ranges , regions , verticals , and topics . However , the service organizes the user interests at only two levels : the lower individual query level containing individual queries , and the higher topic level consisting of 27 topics such as “ Health ” and “ Entertainment ” .
As will be illustrated in our experiment results , using only 27 topics seems insufficient to summarize user interests from time to time . Instead , a richer hierarchical structure of topics learned from search logs , as implemented in our project , is more effective in multidimensional analysis . For example , after browsing the hottest queries in topic “ Entertainment ” , a user may want to drill down to a subtopic “ Entertainment/Film ” . The current two layer structure in the existing project can only provide limited analysis power .
Moreover , using individual queries to represent user interests seems ineffective . It is well recognized that users may formulate various queries for the same information need . Therefore , the search log data at the individual query level may be sparse . For example , the system returns queries “ games ” , “ game ” , “ games online ” , and “ free games ” as the 1st , 2nd , 7th , and 8th hottest queries , respectively , on topic “ Game ” in the US . Clearly , those queries carry similar information needs . To make the analysis more effective , as achieved by the topic concept model in our project , we need to summarize similar queries into concepts and represent user interests by concepts instead of individual queries .
To a broader extent , our project is related to the previous studies on search query traffic patterns , user interest summarization , and data cube computation .
Several previous studies explored the patterns of query traffic with respect to various aspects , such as time , locations , and search devices . For example , Beitzel et al . [ 7 ] investigated how the web query traffic varied hourly . Backstrom et al . [ 4 ] reported a correlation between the locations referred in queries and the geographic focus of the users who issued those queries . Kamvar et al . [ 16 ] presented a log based comparison on the distribution and variability of search tasks that users performed from three platforms , namely computers , iPhones , and conventional mobile phones . However , those studies mainly focused on the general trends of user query traffic without mining user interests from the log data .
Previous approaches to summarizing user search queries can be divided into two categories : the clustering approaches and the categorization approaches . A clustering approach groups similar queries and URLs in an unsupervised way . For example , Zhao et al . [ 20 ] identified events in a timeseries of click through bipartites derived from search logs . Each event consists of a set of queries and clicked URLs
1Due to our company policy , we do not reveal the name of the search engine mentioned here .
Uid Time Stamp U1 U2 U1 . . .
Type 100605110843 Query 100605110843 Vancouver , BC , CA Query 100605110846 . . .
Location Seattle , WA , US
Seattle , WA , US . . .
Click . . .
Value “ wsdm 2011 ” “ you tube ” wsdm2011.org . . .
Table 2 : A search log as a stream of query and click events with multidimensional information . which evolve synchronously along the time series . In [ 5 , 6 , 8 , 18 ] , the authors clustered the click through bipartites and grouped similar queries into concepts . A categorization approach classifies queries into a set of pre defined topics in a supervised way . For example , Shen et al . [ 17 ] leveraged the search results returned by a search engine and converted the query categorization problem into a text categorization problem . Both the clustering and categorization approaches are effective to summarize user interests into events , concepts , or topics . However , they do not consider how the interests vary with respect to various dimensions such as time and locations . Consequently , those methods cannot be directly used to support lookups and reverse lookups as well as advanced online multidimensional exploration .
Grey et al . [ 12 ] developed data cubes as the core of data warehouses and OLAP systems . A data cube contains aggregated numeric measures with respect to group bys of dimensions . Zhang et al . [ 19 ] proposed a topic cube which extends the traditional data cube with an extra hierarchy of topics . Each cell in the cube stores the parameters learned from a topic modeling process . Users can apply the OLAP operations such as roll up and drill down along both standard dimensions and the topic dimension . The system was built on a single machine . There are several critical differences between our topic concept cube and the topic cube . First of all , the topic model pLSA [ 13 ] applied in [ 19 ] targets at modeling documents , which involves only two types of variables , namely the terms as observed variables and the topics as hidden variables . However , to summarize the common interests in search log data , we have to consider more variables , especially , queries and clicked URLs as observed variables , and concepts and topics as hidden variables . Therefore , the traditional pLSA model cannot be applied in our project . Consequently , the methods to materialize our topic concept cubes are very different from those to materialize the topic cubes . Finally , we reported an empirical study on a much larger set of real data , containing billions of queries and clicks , and processed in a distributed environment .
3 . OUR FRAMEWORK
When a user raises a query to a search engine , a set of URLs are returned by the search engine as the search results . The user may browse the snippets of the top search results and selectively click on some of them . A search log can be regarded as a sequence of query and click events by users . For each event , a search engine may record the type and content of the event as well as some other information such as the time stamp , location , and the device associated with the event . Table 2 shows a small segment of a search log .
Some dimensions of the search events have a hierarchical structure . For example , the location dimension can be organized into levels of country → state → city , and the time dimension can be represented at levels of year → month → day → hour . Therefore , the multi dimensional , hierarchical log data can naturally be organized into a raw log data
( a )
( b )
Figure 2 : An example of ( a ) click through bipartite and ( b ) QU matrix . cube [ 12 ] , where each cell is a group by using the dimensions . For example , a cell may contain all query and click events of time “ February , 2010 ” and location “ Washington State ” .
We can aggregate the query and click events in a cell and derive a click through bipartite , where each query node corresponds to a unique query in the cell and each URL node corresponds to a unique URL , as demonstrated in Figure 2(a ) . An edge eij is created between query node qi and URL node uj if uj is a clicked URL of qi . The weight wij of edge eij is the total number of times when uj is a clicked result of qi among all events in the cell .
A click through bipartite can be represented as a queryURL matrix ( QU matrix for short ) , where each row corresponds to a query node qi and each column corresponds to a URL node uj . The value of entry nij is simply the weight wij between qi and uj , as shown in Figure 2(b ) .
The QU matrix at a cell is often sparse . Moreover , QU matrix represents information at the level of individual queries and URLs . As discussed before , we need to summarize and aggregate the information in a QU matrix to facilitate online multidimensional analysis . This will be achieved by the topic concept model to be developed in Section 4 .
Figure 3 shows the framework of our system . In the offline stage , we first form a raw log data cube by partitioning the search log data along various dimensions and at different levels . For each cell of the raw log data cube , we construct a click through bipartite and derive the QU matrix . Then , we materialize the cube by learning topic concept models which summarize the distributions of topics and concepts on the QU matrix for each cell . The resulting data cube is called the topic concept cube . In the online stage , we use the learned model parameters to support multidimensional lookups , reverse lookups , as well as advanced analytical explorations .
4 . TOPIC CONCEPT MODEL
We propose a novel topic concept model ( TC model for short ) , a graphical model as shown in Figure 4 , to describe the generation process of a QU matrix . Essentially , we assume that a user bears some search intent in mind when interacting with a search engine . The search intent belongs to certain topics and focuses on several specific concepts . Based on the search intent , the user formulates queries and selectively clicks on search results .
From the search log data , we can observe user queries q and clicks u . Following the convention of graphical models , these two observable variables are represented by black circles in Figure 4 . Since user search intents cannot be observed , the topics t and concepts c are latent variables , which
Figure 3 : The framework of our system .
Figure 4 : A graphical representation of TC model . are represented by white circles .
Let Q and U be the sets of unique queries and unique URLs in a QU matrix , respectively . Let C and T be the sets of concepts and topics to model user interests . The training process of the topic concept model is to learn four groups of model parameters Θ = ( Φ , ∆ , ΥQ , ΥU ) . Here , the prior topic distribution Φ = {P ( tk)} , where tk ∈ T and P ( tk ) is the prior probability that a user ’s search intent involves topic tk . The concept generation distribution ∆ = {P ( cl|tk)} , where cl ∈ C , tk ∈ T , and P ( cl|tk ) is the probability that topic tk generates concept cl . The query generation distribution ΥQ = {P ( qi|cl)} , where qi ∈ Q , cl ∈ C , and P ( qi|cl ) is the probability that concept cl generates query qi . The URL generation distribution ΥU = {P ( uj|cl)} , where uj ∈ U , cl ∈ C , and P ( uj|cl ) is the probability that concept cl generates a click on URL uj .
Given that a user bears a search intent on specific concepts c , we assume that ( 1 ) the formulation of queries is conditionally independent of the clicks on search results , ie , P ( q , u|c ) = P ( q|c ) · P ( u|c ) ; and ( 2 ) both the formulation of queries and the clicks on search results are conditionally independent of the topics t of the search intent , ie , P ( q , u|t , c ) = P ( q , u|c ) . Then , the likelihood for each entry ( qi , uj ) in the QU matrix can be factorized as follows .
( cid:180)nij ( cid:179 ) ( cid:180)nij cl∈C P ( qi , uj , cl , tk ; Θ ) cl∈C P ( tk)P ( cl|tk)P ( qi|cl)P ( uj|cl )
( cid:179 )
= tk∈T
L(qi , uj ; Θ ) = tk∈T
( 1 ) where nij is the value of entry ( qi , uj ) in the QU matrix . The likelihood for the whole QU matrix D is L(D ; Θ ) =
P ( qi , uj ; Θ ) . qi,uj Since the data likelihood is hard to be maximized analytically , we apply the Expectation Maximization ( EM ) algorithm [ 11 ] . The EM algorithm iterates between the E step and the M step . The E step computes the expectation of the log data likelihood with respect to the distribution of the latent variables derived from the current estimation of the model parameters . In the M step , the model parameters are estimated to maximize the expected log likelihood q1URLsQueriesq2q3q4u1u2u3u4u5231300790102805301050u1u2u3u4…q123013000q2079000q300101050q300101050q4000530…Offline ProcessSearch logsRaw Log Data CubeTopic Concept CubeClick Through BipartiteTopic Concept Model14 . Cube materializationOnline Process Look up Comparison Reverse look up TracingTCQUQQQUUUULocationTime32For each cellLocationTimeQUTC found in the E step . We have the following equations for the E step in the r th iteration .
P r(cl|qi , uj ) ∝
P r(tk|qi , uj ) ∝ tk
( cid:161 ) ( cid:161 ) cl
P r−1(tk ) · P r−1(cl|tk )
·P r−1(qi|cl ) · P r−1(uj|cl ) P r−1(tk ) · P r−1(cl|tk )
·P r−1(qi|cl ) · P r−1(uj|cl )
( cid:162 ) ( cid:162 )
In the M step of the r th iteration , the model parameters are updated by the following equations . tk qi,uj qi,uj nijP r(tk|qi , uj ) nijP r(tk|qi , uj ) nijP r(cl|qi , uj ) nijP r(cl|qi , uj ) qi ,uj nijP r(cl|qi , uj ) qi,uj nij P r(cl|qi , uj ) qi,uj uj qi cl qi,uj
P r(tk ) =
P r(qi|cl ) =
P r(uj|cl ) =
P r(cl|tk ) = nijP r(cl|qi , uj)P r(tk|qi , uj ) nijP r(cl|qi , uj)P r(tk|qi , uj )
5 . LEARNING LARGE TC MODELS
Although the EM algorithm can effectively learn the parameters in TC models , there are still several challenges to apply it on huge search log data . In Section 5.1 , we will develop distributed algorithms for learning TC models from a huge amount of data . In Section 5.2 , we will discuss the model initialization steps . Last , in Section 5.3 we will develop effective heuristics to reduce the number of parameters to learn in each machine . 5.1 Distributed Learning of Parameters
Search logs typically contain billions of query and click events involving tens of millions of unique queries and URLs . It is impractical to learn a TC model from a huge amount of data using a single machine . To address this challenge , we develop distributed algorithms for the E step and M step .
In our learning process , a QU matrix is represented by a set of ( qi , uj , nij ) tuples . Since a query usually has a small number of clicked URLs , a QU matrix is very sparse . We only need to record the tuples where nij > 0 . We first partition the QU matrix into subsets and distribute each subset to a machine ( called a process node ) . Then we carry out the E step and the M step .
In the E step of the r th iteration ( Algorithm 1 ) , each process node loads the current estimation of the model parameters and scans the assigned subset of training data once . For each tuple ( qi , uj , nij ) , the process node enumerates all the concepts cl such that P r−1(qi|cl ) > 0 and P r−1(uj|cl ) > 0 . For each enumerated concept cl , the process node further enumerates each topic tk such that P r−1(cl|tk ) > 0 and evaluates the value v = P r−1(tk)P r−1(cl|tk)P r−1(qi|cl)P r−1(uj|cl ) . The values of v are summed up to estimate P r(cl|qi , uj ) and P r(tk|qi , uj ) according to Equations 2 and 3 , respectively . Finally , we output the probabilities for the hidden variables . Those results will serve as the input of the M step .
In the M step , we estimate the model parameters based on the probabilities of the hidden variables . According to Equations 4 6 , the estimation for each parameter involves a sum over all the queries and/or URLs . Since the matrix is
( 2 )
( 3 )
( 4 )
( 5 )
( 6 )
( 7 )
σij = 0 ; for each topic tk ∈ T do σt let Cij = {cl|P r−1(qi|cl ) > 0 && P r−1(uj|cl ) > 0} ; for each concept cl ∈ Cij do
Algorithm 1 The r th round E step for each process node . Input : the subset of training data S ; the model parameters Θr−1 of the last round 1 : Load model parameters Θr−1 ; 2 : for each tuple ( qi , uj , nij ) in S do 3 : 4 : 5 : 6 : 7 : 8 : 9 : 10 : 11 : 12 : 13 :
σc ijl = 0 ; for each topic tk ∈ T such that P r−1(cl|tk ) > 0 do v = P r−1(tk)P r−1(cl|tk)P r−1(qi|cl)P r−1(uj|cl ) ; σc ijl+ = v ; σt for each topic tk ∈ T such that P r−1(cl|tk ) > 0 do for each concept cl ∈ Cij do output(qi , uj , cl , tk , nij , σc ijk+ = v ; σij + = v ; ijk = 0 ; ijl/σij , σt ijk/σij ) ;
Key tk cl , tk
Value nij · σt nij · σc ijk/σij ijl · σt ijk/σ2 ij
Key qi , cl uj , cl
Value nij · σc nij · σc ijl/σij ijl/σij
Table 3 : The key/value pairs at the map stage of the r th round of M step . distributed on multiple machines , the summation involves aggregating the intermediate results across machines , which is particularly suitable for a Map Reduce system [ 10 ] .
In the map stage of the M step , each process node receives a subset of tuples ( qi , uj , cl , tk , nij , σc ijk/σij ) . For each tuple , the process node emits four key value pairs as shown in Table 3 . In the reduce stage , the process nodes simply sum up all the values with the same key and update the model parameters using Equations 4 6 . 5.2 Model Initialization ijl/σij , σt
The Topic Concept model consists of four sets of parameters , Φ , ∆ , ΥQ and ΥU . We first initialize the query andclick generation probabilities ΥQ and ΥU by mining the concepts from the click through bipartite . We then initialize the prior topic probabilities Φ and the concept generation probabilities ∆ by assigning concepts to topics .
To mine concepts from a click through bipartite , we first apply an existing clustering algorithm [ 8 ] and derive a collection of query clusters . The clustering algorithm regards queries sharing many clicked URLs similar to each other , and thus groups them to the same cluster . However , the clustering method assigns each query to only one cluster , which may not be suitable for ambiguous queries that involve multiple concepts . To address this challenge , we follow the method in [ 9 ] and conduct two steps of propagation along the edges in the click through bipartite . That is , for each query cluster Ql , we find the set of URLs Ul such that each URL u ∈ Ul is connected with at least one query in Ql . In the first step of propagation , Ql is expanded to Q l such that each query q ∈ Q l is connected with at least one URL u ∈ Ul . In the second step of propagation , Ul is expanded to U l is connected with at least one query q ∈ Q l . Finally , we represent each concept cl by the pair of query and URL sets ( Q l ) , and initialize the query and URL generation probabilities by nij ; P 0(uj|cl ) ∝ l such that each URL u ∈ U
P 0(qi|cl ) ∝ l , U nij , uj∈U l qi∈Q l where nij is the value of entry ( qi , uj ) in the QU matrix .
After deriving the set of concepts C , we consider the set of topics T . Although we may automatically mine topics by clustering concepts , in practice , there are several wellaccepted topic taxonomies , such as Yahoo! Directory [ 3 ] , Wikipedia [ 2 ] , and ODP [ 1 ] . We use the ODP topic taxonomy in this paper , though others can be adopted as well . mated by ˆP ( cl ) ∝
The ODP taxonomy is a hierarchical structure where each parent topic subsumes several sub topics , and each leaf topic is manually associated with a list of URLs by the ODP editors . Given a set of topics at some level in the taxonomy , we can initialize the concept generation probabilities P ( cl|tk ) as follows . According to Bayes Theorem , P ( cl|tk ) ∝ P ( cl)P ( tk|cl ) . The prior probability P ( cl ) indicates the popularity of concept cl and the probability P ( tk|cl ) indicates how likely cl involves topic tk . Suppose cl is represented by the queryand URL sets ( Q l ) . The popularity of cl can be estinij , where nij is the value qi∈Q of entry ( qi , uj ) in the QU matrix . To tell how likely cl involves topic tk , we merge the text content of the URLs u ∈ U l into a pseudo document dl . Then , the problem of estimating P ( tk|cl ) is converted into a text categorization problem , and P ( tk|cl ) can be estimated by applying any text categorization techniques ( eg , [ 14 , 15 ] ) on the pseudodocument dl . Based on the estimated ˆP ( cl ) and ˆP ( tk|cl ) , we initialize the parameters by P 0(cl|tk ) ∝ ˆP ( cl ) ˆP ( tk|cl ) ; P 0(tk ) ∝
ˆP ( cl ) ˆP ( tk|cl ) . l,uj∈U l l , U cl
Why do we still need the EM iterations given that we can estimate all the model parameters in the initialization stage ? The EM iterations can improve the quality of concepts and topics by a mutual reinforcement process . In the TC model , the probabilities {P ( q|c)} and {P ( u|c)} assign queries and URLs to concepts , while the probabilities {P ( c|t)} assign concepts to topics . In the initialization stage , those two types of probabilities are estimated independently . If two queries/URLs belong to the same concept , it is more likely that they belong to the same topic , and vice versa . Therefore , if we jointly consider those two types of probabilities , we may derive more accurate assignments of concepts and topics . In the EM iterations , the relationship between the concepts and topics is embedded in the latent variables {P ( c|q , u)} and {P ( t|q , u)} , which contributes to the increase of the data likelihood . In our experiments on a real data set , the data likelihood increased by 11 % after the EM iterations . 5.3 Reducing Re estimated Parameters As described in Section 5.1 , in the E step , each process node estimates the latent variables P ( cl|qi , uj ) and P ( tk|qi , uj ) on the basis of the last round estimation of parameters Φ , ∆ , ΥQ , and ΥU . Let Nt , Nc , Nq , Nu be the numbers of topics , concepts , unique queries , and unique URLs , respectively . The sizes of the parameter sets are |Φ| = Nt , |∆| = Nt · Nc , |ΥQ| = Nq · Nc , and |ΥU| = Nu · Nc . In practice , we usually have tens of millions of unique queries and URLs in the search log data , which may form millions of concepts . For example , in the real data set in our experiments , we have 11.76 million unique queries , 9.5 million unique URLs , 4.71 million concepts , and several hundred topics . The total size of the parameter space reaches 1014 . Consequently , it is infeasible to hold the full parameter space into the main memory of a process node .
To reduce the number of parameters to be re estimated , we analyze the cases when the model parameters remain zero during the EM iterations . Suppose a process node receives a subset S of training data in the E step , we give a tight superset Θ(S ) of the nonzero model parameters which need to be accessed by the process node in the E step . In our experiments , |Θ(S)| for each process node is several orders of magnitudes smaller than the size of full parameters space . Each process node only needs to process a subset of Θ(S ) .
Lemma 1 . The query generation probability at the r th iteration P r(qi|cl ) = 0 if P 0(qi|cl ) = 0 .
Proof . Let U be the whole set of unique URLs . From Equation 2 , if P r−1(qi|cl ) = 0 , then P r(cl|qi , uj ) = 0 holds for every uj ∈ U . According to Equation 5 , if P r(cl|qi , uj ) = 0 holds for every uj ∈ U , then P r(qi|cl ) = 0 . Therefore , we have P r−1(qi|cl ) = 0 ⇒ P r(qi|cl ) = 0 . Using simple induction , we can prove P 0(qi|cl ) = 0 ⇒ P 0(qi|cl ) = 0 .
Similarly , we can prove the following lemma .
Lemma 2 . The URL generation probability at the r th it eration P r(uj|cl ) = 0 if P 0(uj|cl ) = 0 .
Let us consider the concept generation probabilities P ( cl|tk ) .
We call a pair ( qi , uj ) belongs to concept cl , denoted by ( qi , uj ) ∈ cl , if nij > 0 , P 0(qi|cl ) > 0 , and P 0(uj|cl ) > 0 . Two concepts cl and cl are associated if there exists a pair ( qi , uj ) belonging to both concepts . Trivially , a concept is associated with itself . Let A(cl ) be the set of concepts associated with cl , and QU ( cl ) be the set of pairs ( qi , uj ) which belong to at least one concept associated with cl , ie , QU ( cl ) = {(qi , uj)|∃cl ∈ A(cl ) , ( qi , uj ) ∈ cl} . We have the following .
Lemma 3 . The concept generation probability at the r th iteration P r(cl|tk ) = 0 if ∀cl ∈ A(cl ) , P r−1(cl|tk ) = 0 .
Proof . According to the definitions , for any ( qi , uj ) ∈ cl , one of the following three predicates holds ( 1 ) nij = 0 ; ( 2 ) P 0(qi|cl ) = 0 ; or ( 3 ) P 0(uj|cl ) = 0 . If nij = 0 , from Equation 7 , ( qi , uj ) does not contribute to P r(cl|tk ) . Otherwise , if P 0(qi|cl ) = 0 or P 0(uj|cl ) = 0 , according to Lemmas 1 and 2 , we have either P r−1(qi|cl ) = 0 or P r−1(uj|cl = 0 ) . From Equation 2 , if either P r−1(qi|cl ) = 0 or P r−1(uj|cl ) = 0 , then P r(cl|qi , uj ) = 0 . Therefore , Equation 7 can be re written as
P r(cl|tk ) ∝ nijP r(cl|qi , uj)P r(tk|qi , uj ) .
( 8 )
( qi,uj )∈cl
Now we only need to focus on P r(tk|qi , uj ) for pairs ( qi , uj ) ∈ cl . According to the definition of A(cl ) , for any pair ( qi , uj ) ∈ cl and concept cl /∈ A(cl ) , either P 0(qi|cl ) = 0 or P 0(uj|cl ) = 0 holds . Using Lemmas 1 and 2 , we can rewrite Equation 3 for every pair ( qi , uj ) ∈ cl as
P r(tk|qi , uj ) ∝
P r−1(tk ) · P r−1(cl|tk ) cl∈A(cl )
· P r−1(qi|cl ) · P r−1(uj|cl ) .
( 9 ) According to Equation 9 , if ∀cl ∈ A(cl ) , P r−1(cl|tk ) = 0 , then P r(tk|qi , uj ) = 0 holds for every ( qi , uj ) ∈ cl . Further according to Equation 8 , if P r(tk|qi , uj ) = 0 holds for every ( qi , uj ) ∈ cl , then P r(cl|tk ) = 0 . Therefore , if ∀cl ∈ A(cl ) , P r−1(cl|tk ) = 0 , then P r(cl|tk ) = 0 .
Lemma 3 suggests that at each round of iteration , a concept cl propagates its nonzero topics tk ( ie , topics such that P ( cl|tk ) > 0 ) one step further to all its associated concepts . To further explore the conditions for P r(cl|tk ) = 0 , we build a concept association graph G(V , E ) , where each vertex v ∈ V corresponds to a concept c , and two concepts ca and cb are directly connected by an edge eab ∈ E if they are associated . In the association graph , two concepts ca and cb are connected if there exists a path between ca and cb . The connected component N∗(ca ) of concept ca consists of all concepts cb which are connected with ca . The distance between two concepts ca and cb is the length of the shortest path between ca and cb in the graph . If ca and cb are not connected , the distance is set to ∞ . The set of m step neighbors N m(ca ) ( 1 ≤ m < ∞ ) of concept ca consists of the concepts whose distance from ca is smaller than or equal to m . We can easily prove the following lemma by recursively applying Lemma 3 .
Lemma 4 . The concept generation probability at the rth iteration P r(cl|tk ) = 0 if ∀cl ∈ N m(cl ) ( 1 ≤ m ≤ r ) , P r−m(cl|tk ) = 0 . Moreover , P r(cl|tk ) = 0 if ∀cl ∈ N∗(cl ) , P 0(cl|tk ) = 0 .
Using Lemmas 1 4 , we can give a tight superset of the parameters needed in the E step for any subset S of training data . Let ( qi , uj , nij ) be a training tuple in S . In the E step , we enumerate the concepts cl such that P r−1(qi|cl ) > 0 and P r−1(uj|cl ) > 0 . According to Lemmas 1 and 2 , to process ( qi , uj , nij ) , we can safely enumerate only those concepts ij = {cl|(qi , uj ) ∈ cl} . C We consider the nonzero parameters for each concept cl . Using Lemmas 1 and 2 , the nonzero query and URL generaQ(cl ) = {P ( qi|cl)|P 0(qi|cl ) > tion probabilities are simply Υ+ 0} and Υ+ U ( cl ) = {P ( uj|cl)|P 0(uj|cl ) > 0} , respectively . Furthermore , let T ( cl ) = {P ( cl|tk)|P 0(cl|tk ) > 0} and T ∗(cl ) = cl∈N∗(cl ) T ( cl ) . Using Lemma 4 , the nonzero concept generation probabilities are ∆+(cl ) = {P ( cl|tk)|tk ∈ T ∗(cl)} . Let C S be the set of concepts that are enumerated for the training tuples in S , ie , C S = ij . We summarize the above discussion as follows . sij∈S C
Theorem 1 . Let S be a subset of training data , the set of nonzero parameters need to be accessed in the E step for S is a subset of Θ(S ) , where
 .
{P ( tk)} , cl∈C
S
Θ(S ) =
Υ+
Q(cl ) ,
Υ+
U ( cl ) ,
∆+(cl ) cl∈C
S cl∈C
S
In practice , a concept association graph can be highly connected . That is , for any two concepts ca and cb , there likely exists a path ca , cl1 , . . . , clm , cb . In some cases , although each pair of adjacent concepts on the path are related to each other , the two end concepts ca and cb of the path may be about dramatically different topics . As discussed before , in the EM iterations , each concept propagates its nonzero topics to its neighbors . Consequently , after several rounds of iterations , two totally irrelevant concepts ca and cb may exchange their nonzero topics through the path ca , cl1 , . . . , clm , cb . To avoid over propagation of the nonzero topics , we may constrain the propagation up to ς steps . Specifically , for each concept cl , let T ( cl ) = {P ( cl|tk)|P 0(cl|tk ) > 0} and T ς ( cl ) = cl∈N ς ( cl ) T ( cl ) , we constrain the concept generation probability P ( cl|tk ) = 0 if
( a ) On standard dimension
( b ) On TC dimension
Figure 5 : The cube construction approaches on ( a ) standard dimension and ( b ) TC dimension . tk ∈ T ς ( cl ) . In our experiments , we find that the nonzero topics propagated from the neighbors of more than one step away are often noisy . Therefore , we set ς to 1 .
Theorem 1 greatly reduces the number of parameters to be re estimated in process nodes in practice . For example , when we use 50 process nodes in our experiments , each process node only needs to re estimate 62 million parameters , which is about 10−7 of the size of the total parameter space . In practice , 62 million parameters may still be expensive for a machine with small memory , eg , less than 2G . In this case , the process node can recursively split the assigned training data Sn into smaller blocks Snb ⊂ Sn until the necessary nonzero parameters Θ(Snb ) for each block can be loaded into the main memory . Then , the process node can carry out the E step block by block . We report the details of the experiment in Section 71
6 . CUBE CONSTRUCTION AND REQUEST
ANSWERING
Similar to a traditional data cube , a topic concept cube ( TC cube for short ) contains some standard dimensions such as time and locations . However , a TC cube differs from a traditional data cube in several critical aspects . First , for each cell in a TC cube , we learn the TC models from the training data in the cell and use the model parameters as the measures of the cell . Those parameters allow us to answer lookups and reverse lookups introduced in Section 1 . Second , a TC cube contains a special topic concept dimension ( TC dimension for short ) as shown in Figure 1 . Therefore , to materialize a TC cube , we need to address three questions . First , how to materialize the standard dimensions ? Second , how to materialize the TC dimension ? Finally , how to materialize a TC cube which consists of both standard dimensions and the TC dimension ? In the following , we will briefly address these three questions . The full technical details can be found in Appendices A and B .
As illustrated in Figure 5(a ) , in a standard dimension , the training data in a upper level cell C1 is split into its child cells C21 , . . . , C2M . For example , C1 may contain the set of training tuples D1 from the US , while each child cell C2m ( 1 ≤ m ≤ M ) may contain the set of training tuples D2m from one state of the US . In general , D21 , . . . D2M form a partition of D1 . A na¨ıve method to materialize the standard dimension is to follow the initialization steps in Section 5.2 for each cell and learn the TC models from scratch . However , since the training data D2m in a child cell is a subset of D1 , the topics and concepts may not differ dramatically between a child cell and a parent cell . Hence , we may develop two approaches . In the top down approach , we may inherit the trained parameters Θ1 for the parent cell C1 to
D1;θ1D22;θ22Top downBottom upD21;θ21D2M;θ2MParent cell C1Child cells C21 , C22 , , C2MBottom upT1,ΘT2,ΘTop down initialize the parameters for a child cell C2m . Alternatively , in the bottom up approach , we may aggregate the trained parameters Θ21 , . . . , Θ2M of the child cells to initialize the parameters for the parent cell C1 .
Next , we materialize the TC dimension . Recall that the topic concept model assigns the concepts to a set of topics . Given a taxonomy of topics , such as ODP [ 1 ] , the TCdimension organizes the queries and clicks into a hierarchy of topics and concepts ( see Figure 1 ) . To materialize the TC dimension , we need to learn the model parameters with respect to each level of topics in the hierarchy .
Different from the standard dimensions , the TC dimension has the same set of training data at different levels ( Figure 5(b) ) . Without loss of generality , let T1 = {t1k} be the set of topics at some level of a given topic taxonomy , and T2 = {t2kn} be the set of topics one level lower than T1 . In particular , t2kn is a sub topic of t1k , where 1 ≤ n ≤ N1k and N1k is the number of sub topics of t1k . Again , we have three alternative options to materialize the TC dimension . First , a na¨ıve method materializes different levels of topics separately . Second , the top down approach inherits the model parameters Θ1 with respect to T1 for the materialization of parameters Θ2 with respect to the sub topics T2 . Finally , the bottom up approach initializes the model parameters for a higher level topic t1k by aggregating those of its sub topics t2k1 , . . . , t2kN1k .
We have two alternative approaches to materialize the whole TC cube which consists of both standard dimensions and the TC dimension . The standard dimension first approach materializes a raw log data cube using the standard dimensions , and then materializes along the TC dimension for each cell in the raw log data cube . The TC dimensionfirst approach processes the topic hierarchy level by level . For each level , it materializes the cells formed by the standard dimensions .
After materializing the whole TC cube , we answer the lookups and reverse lookups using the model parameters in the TC cube . Since the number of model parameters can be large , we store the parameters distributively on a cluster of process nodes , where each node contains the parameters for a set of cells . When the system receives a lookup request , for example , “ ( time=Dec . , 2009 ; location=US ; topic=Games ) ” , it will delegate the query to the process node where the model parameters of the corresponding cell are stored . Then the process node will select the top k concepts c with the largest concept generation probabilities P ( c|t = Games ) . For each top concept , the process node will use the query q with the largest P ( q|c ) as the representative query . Finally , the system returns a list of representative queries of the top concepts as the answer to the lookup request .
To answer the reverse lookups , we build inverted lists which map key words to concepts . The inverted list can be stored distributively on a cluster of process nodes , where each node takes charge of a range of key words . Suppose a user requests a reverse lookup about “ hurricane Bill ” . The system will delegate the key words to the corresponding node which stores the inverted list for “ hurricane Bill ” . The node retrieves from the inverted list the concepts C = {c} which consist of “ hurricane Bill ” . The system then broadcasts the concepts C to all the nodes which store the model parameters . Each node checks the measures of all its cells and reports ( Dval , Count ) for each cell , where Dval consists of the corresponding values of the standard dimensions of the cell , and Count is the frequency of the concepts C in the cell , ie , Count = qi,uj∈c nij , where nij is the value of c∈C
( a ) Data likelihood
( b ) Parameter changes
Figure 6 : The data likelihood and the average percentage of parameter changes during EM iterations . entry ( qi , uj ) in the QU matrix of the cell . If the user has specified the levels of the standard dimensions , for example , time@day ; location@country , the system returns the Dvals of the top k cells which match the specified levels of the standard dimension . If the user does not specify the levels , the system will answer the request at the default levels . The user can further drill down or roll up to different levels .
7 . EXPERIMENTS
In this section , we report the results from a systematic empirical study using a large search log from a major commercial search engine . The extracted log data set spans for four months and contains 1.96 billion queries and 2.73 billion clicks from five markets , ie , the United States , Canada , United Kingdom , Malaysia , and New Zealand . In the following , we first examine the efficiency and scalability of our distributed training algorithms for the TC model . We briefly report our findings about the alternative approaches for the materialization of the TC cube . Finally , we demonstrate the effectiveness of our approach by several examples of the lookup and reverse lookup requests . 7.1 Training TC models
The TC model was initialized as described in Section 52 We derived 4.71 million concepts , which involve 11.76 million unique queries and 9.5 million unique URLs . On average , a concept consists of 4.68 unique queries and 6.77 unique URLs . We further chose the second level of the ODP [ 1 ] taxonomy and applied the text classifier in [ 14 ] to categorize the concepts into the 483 topics . For each concept , we kept the top five topics returned by the classifier .
From the raw log data , we derived 23 million training tuples where each training tuple is in the form ( qi , uj , nij ) and nij is the number of times URL uj was clicked on as answers to query qi .
Figures 6(a ) and ( b ) show the data likelihood and the average percentage of parameter changes with respect to the number of EM iterations . The iteration process converges fast ; the data likelihood and parameters do not change much ( less than 0.1 % ) after five iterations . The results suggest that our initialization methods are effective to set the initial parameters close to a local maximum . Moreover , the data likelihood increases by 11 % after ten iterations . As explained in Section 5.2 , this indicates that the EM algorithm is effective to improve the quality of the TC model by jointly mining the assignments of concepts and topics in a mutual reinforcement process .
Figures 7(a ) and ( b ) show the runtime of the E step and the M step with respect to the percentage of the full data set with 50 , 100 , and 200 process nodes , respectively . Each
0246810−18−175−17−165−16−155x 1010IterationLog L(Q,U)0246810005115225IterationAvg percentage of param changes(% ) % rameter space , and the number of blocks processed by each process node . Table 4 suggests the following . First , the average size of Θ(S ) over the size of the whole parameter space is very small , in the order of 10−7 . This means Theorem 1 can greatly reduce the number of parameters to be held by each process node . Moreover , the size of the estimated nonzero parameters is close to that of nonzero parameters during the iterations . This indicates that the superset of nonzero parameters given by Theorem 1 is tight . 7.2 TC Cube Materialization
We conducted an empirical study on the alternative meth ods to materialize the standard dimensions , the TC dimension , and the whole TC cube , and obtained the following observations . First , in standard dimensions , both the bottom up and top down approaches achieved higher initial likelihoods than that by the na¨ıve method after initialization . However , all the three methods needed about five iterations to converge , and thus took similar runtime . Moreover , all of them converged to comparable likelihoods . Therefore , we may choose any of them to materialize the standard dimensions . Second , in the TC dimension , the top down method was much slower than the other two methods . The reason is that when we inherit the model parameters from the upper level topics , most of the concept generation probabilities P ( c|t ) for the lower level topics are nonzero . In this case , the superset of nonzero parameters estimated by Theorem 1 can still be very large . Consequently , each process node needs to partition the assigned training tuples into many blocks and scan the large parameter file many times . Therefore , in the TC dimension , we may consider either the bottomup method or the na¨ıve method . Finally , it does not make much difference to materialize the standard dimensions first or the TC dimension first . The detailed experiment report can be found in Appendix C . 7.3 Examples of lookups and reverse lookups In this subsection , we show some real examples for the lookups and reverse lookups answered by our system . We use the query traffic analysis service by a major commercial search engine as the baseline . Please refer to Section 2 for a more detailed description of the baseline .
Table 5 compares the results for the lookup request “ ( time = ALL ; location = US ; topic = Games ) ” returned by our system and the baseline . Since the baseline does not group similar queries into concepts , the top 10 results are quite redundant . For example , the 1st , 2nd , 7th , and 8th queries are similar . Our system summarizes similar queries into concepts and selects only one query as the representative for each concept . Consequently , the top 10 queries returned by our system are more informative . We further request the top results for four sub topics of “ Games ” , namely “ card games ” , “ gambling ” , “ party games ” , and “ puzzles ” . The queries returned by our system are informative ( Table 6 ) . However , the baseline only organizes the user queries by a flat set of 27 topics ; it does not support drilling down to sub topics .
As an example for reverse lookup , we asked for the groupbys where the search for Hurricane Bill was popular by a request “ ( time@day , location@state , keyword=‘hurrican bill’ ) ” . Purposely we misspelled the keyword “ hurricane ” to “ hurrican ” to test the summarization capability of our TCmodel . Our system can infer that the keyword “ hurrican bill ” belongs to the concept which consists of queries “ hurricane bill ” , “ hurrican bill ” , “ huricane bill ” , “ projected path of hurricane bill ” , “ hurricane bill 2009 ” and some other variants . Therefore , the system sums up the frequencies of all
( a ) E step
( b ) M step
Figure 7 : The scalability of the E step and the Mstep .
# pn
50 100 200
|S|
|Θ(S)|
460,062 230,031 115,015
62,325,884 35,368,823 18,656,725
# nonezero parameters 56,682,113 30,370,194 15,821,818
Ratio # B
5.7 e 7 3.0 e 7 1.6 e 7
4 2 1
Table 4 : The effectiveness of Theorem 1 . process node has a four core 2.67GHz CPU and 4G main memory . We observe the following in Figure 7(a ) . First , the more process nodes used , the shorter runtime for the E step . The runtime needed for the E step on the full data by 50 , 100 , and 200 process nodes is approximately in ratio 4:2:1 . This suggests that our algorithm scales well with respect to the number of process nodes . Second , the more process nodes are used , the more scalable is the E step . For example , when 50 process nodes were used , the runtime increases dramatically when 40 % , 70 % , and 100 % of the data was loaded . As explained in Section 5.3 , if the training data for a process node involves too many parameters to be held in the main memory , the algorithm recursively splits the training data into blocks until the parameters needed by a block can be held in the main memory . Therefore , the runtime of the E step mainly depends on the number of disk scans of the parameter file , ie , the number of blocks to be processed . When we used 50 process nodes , each node split the assigned training data into 2 , 3 , and 4 blocks when 40 % , 70 % , and 100 % of the full data set was used for training , respectively . This explains why the runtime increases dramatically at those points . When we used 200 nodes , each node can process the assigned data without splitting even for the full data set . Consequently , the runtime increases linearly and mildly from 30 % to 100 % of the data .
In Figure 7(b ) , the runtime of M step increases almost linearly with respect to the data set size , indicating the good scalability of our algorithm . Interestingly , the runtime of the M step does not change much with respect to the number process nodes . This is because the major cost of the mapreduce process of the M step is the merging of parameters , which is done on a single machine . This bottleneck costs the M step much longer time than that of the E step .
Table 4 evaluates the effectiveness of Theorem 1 . We executed the E step on the full data set with 50 , 100 , and 200 process nodes , respectively . For each setting , eg , using 50 nodes , we recorded the average number of training tuples S assigned to each process , the average number of the estimated nonzero parameters Θ(S ) by Theorem 1 , the average number of nonzero parameters after ten iterations , the ratio of the average size of Θ(S ) over the size of the whole pa
3040506070809010020040060080010001200140016001800Percentage of full data(%)Runtime(s ) of E−step 50 nodes100 nodes200 nodes30405060708090100150020002500300035004000Percentage of full data(%)Runtime(s ) of M−step 50 nodes100 nodes200 nodes No . 1 2 3 4 5 6 7 8 9 10 baseline games game cheats wow lottery xbox games online free games wii runescape
TC cube games pogo maxgames aol games wow heroes killing games addicted games age of war powder game monopoly online
P ( c|t ) 0.020 0.013 0.012 0.011 0.010 0.009 0.008 0.008 0.008 0.008
Table 5 : The top ten queries returned by our TCcube and the baseline for lookup “ ( time=ALL ; location=US ; topic=Games ) ” .
Figure 8 : The top five states of US where Hurricane Bill was most intensively search in Aug . , 2009 . card games pogo gogirlsgames solitaire aol games scrabble blast msn spades party games tombola oyunlar fashion games drinking games evite beer pong
P ( c|t ) 0.020 0.004 0.004 0.003 0.003 0.002 P ( c|t ) 0.003 0.003 0.003 0.002 0.002 0.002 gambling sun bingo wink bingo tombola skybet ladbrokes ny lotto puzzles pogo sudoku meriam webster thesaurus com mathgames online crossword puzzles
P ( c|t ) 0.004 0.004 0.003 0.003 0.002 0.002 P ( c|t ) 0.006 0.004 0.003 0.003 0.002 0.002
Table 6 : The top queries returned by TC cube for four sub topics of “ Games ” in the US .
Figure 9 : The trajectory of Hurricane Bill . the queries in the concept and answers the top five states during the days in August , 2009 ( Figure 8 ) . Figure 9 visualizes the trend of the popularity of the whole concept according to the output of the reverse lookup . The dates in the figure indicate when the concept was most intensively searched in different states in the US . Interestingly , the trend shown in Figure 9 reflects well the trajectory and the influence of the hurricane geographically and temporally , which indicates that the real world events can be reflected by the popular queries issued to search engines . However , when we sent the same request to the baseline , it answered that the search volume was not enough to show trend . The reason is that the baseline may only consider the query that exactly matches the misspelled keyword “ hurrican bill ” , which may not be searched often .
8 . CONCLUSION
In this paper , we described our topic concept cube project which supports online multidimensional mining of search logs . We proposed a novel topic concept model to summarize user interests and developed distributed algorithms to automatically learn the topics and concepts from large scale log data . We also explored various approaches for efficient materialization of TC cubes . Finally , we conducted an empirical study on a large log data set and demonstrated the effectiveness and efficiency of our approach . A prototype system which can provide public online services is under development .
9 . REFERENCES
[ 1 ] ODP : http://wwwdmozorg [ 2 ] Wikipedia : http://enwikipediaorg
[ 3 ] Yahoo! Directory : http://diryahoocom [ 4 ] Backstrom , L . , et al . Spatial variation in search engine queries .
In WWW’08 , 2008 .
[ 5 ] Baeza Yates , RA , et al . Query recommendation using query logs in search engines . In EDBT’04 Workshop , 2004 .
[ 6 ] Beeferman , D . and Berger , A . Agglomerative clustering of a search engine query log . In KDD’00 , 2000 .
[ 7 ] Beitzel , SM , et al . Hourly analysis of a very large topically categorized web query log . In SIGIR’04 , 2004 .
[ 8 ] Cao , H . , et al . Context aware query suggestion by mining click through and session data . In KDD’08 , 2008 .
[ 9 ] Cao , H . , et al . Towards context aware search by learning a very large variable length hidden markov model from search logs . In WWW’09 , 2009 .
[ 10 ] Dean , J . , et al . MapReduce : simplified data processing on large clusters . In OSDI’04 , 2004 .
[ 11 ] Dempster , AP , et al . Maximal likelihood from incomplete data via the EM algorithm . Journal of the Royal Statistical Society , Ser B(39):1–38 , 1977 .
[ 12 ] Grey , J . , et al . Data cube : a relational aggregation operator generalizing group by , cross tab , and sub totals . Data Mining and Knowledge Discovery , 1:29–53 , 2007 .
[ 13 ] Hofmann , T . Probabilistic Latent Semantic Analysis . In
UAI’99 , 1999 .
[ 14 ] Joachims , T . Text categorization with support vector machines : learning with many relevant features . In ECML’98 , 1999 .
[ 15 ] Joachims , T . Transductive inference for text classification using support vector machines . In ICML’99 , 1999 .
[ 16 ] Kamvar , M . et al . Computers and iphones and mobile phones , oh my! : a logs based comparison of search users on different devices . In WWW’09 , 2009 .
[ 17 ] Shen , D . et al . Q2c@ust : our winning solution to query classification in kddcup 2005 . KDD Exploration , 7(2 ) , 2005 . [ 18 ] Wen , J . , et al . Clustering user queries of a search engine . In
WWW’01 , 2001 .
[ 19 ] Zhang , D . , et al . Topic cube : Topic modeling for olap on multidimensional text databases . In SDM’09 , 2009 .
[ 20 ] Zhao , Q . , et al . Event detection from evolution of click through data . In KDD’06 , 2006 .
51015202530050100150200250300August , 2009 ( date)Number of search times FloridaGeorgiaNew JerseyMassachusettsVirginia                                                 
APPENDIX A . MATERIALIZATION ON STANDARD DI
MENSIONS
A.1 A Top down approach
As illustrated in Figure 5(a ) , in the top down approach , high level cells are materialized before the low level ones . Without loss of generality , suppose we already compute the model parameters Θ1 for a cell C1 and want to materialize the parameters Θ2 for a cell C2 which is a child cell of C1 . Let D1 and D2 be the sets of query and click events in C1 and C2 , respectively . Obviously , D2 ⊆ D1 .
One way to materialize C2 is to follow the initialization steps in Section 5.2 and learn the TC model from scratch . Can we find a better initialization method such that the number of EM iterations can be reduced ? Since D2 ⊆ D1 , the topics and concepts in D2 only partially differ from those in D1 . Therefore , we may consider using the trained parameters for C1 to initialize the parameters for C2 . In the following , we first describe how to project the concepts in C1 to C2 , and then discuss how to initialize the parameters for C2 using those in C1 .
Let c1l be a concept in C1 , which is represented by a pair of query and URL sets ( Q1l , U1l ) . Let Q2 and U2 be the sets of queries and URLs in D2 , respectively . We can project c1l on C2 by calculating Q2l = Q1l ∩ Q2 and U2l = U1l ∩ U2 . Then , the concept c2l can be represented by the projected pair ( Q2l , U2l ) . Correspondingly , the initial query and URL 2 ( qi|c2l ) and generation probabilities in cell C2 , denoted by P 0 2 ( uj|c2l ) , respectively , can be estimated by P 0 2 ( qi|c2l ) ∝ P 0 n2,ij , where n2,ij is uj∈U2l
2 ( uj|c2l ) ∝ n2,ij and P 0 qi∈Q2l the count of ( qi , uj ) pairs in D2 .
2 ( c2l|tk ) and the topic prior probability P 0
Let us consider how to initialize the concept generation probability P 0 2 ( tk ) in cell C2 . We assume the projection of concepts from C1 to C2 does not change the meaning of the concepts . In other words , if a concept c1l in C1 involves a topic tk , so does its projected image c2l in C2 . Based on this assumption , the probability P2(tk|c2l ) of concept c2l belonging to topic tk can be inherited from P1(tk|c1l ) ∝ P1(tk)P1(c1l|tk ) . Then the initial concept generation probability in cell C2 , 2 ( c2l|tk ) ∝ denoted by P 0 P2(c2l)P2(tk|c2l ) , and the initial topic prior probability in 2 ( tk ) ∝ cell C2 , denoted by P 0 P2(c2l)P2(tk|c2l ) , where the concept prior probability P2(c2l ) can be estimated by the count of ( qi , uj ) pairs in n2,ij . After initializing the parameters , we carry out the EM algorithm for C2 as described in Section 5 . A.2 A Bottom up approach
D2 , ie , P2(c2l ) ∝
2 ( c2l|tk ) , can be estimated by P 0
2 ( tk ) , can be estimated by P 0 qi∈Q2l,uj∈U2l c2l
In the bottom up approach , we want to estimate the parameters Θ1 for a higher level cell C1 ( Figure 5(a) ) . In the similar spirit of the top down approach , we initialize Θ1 based on the trained parameters Θ21 , . . . Θ2M for C1 ’s child cells C21 , . . . , C2M .
Let the concepts c2l1 , . . . , c2lM be the projected images of cell c1l in child cells C21 , . . . , C2M of parent C1 , respectively . The query generation probability of concept c1l , denoted by P1(qi|c1l ) , can be initialized by aggregating those of its prom n2lmP ( qi|c2lm ) , where jected images , ie , P 0 n2,ij,m and n2,ij,m is the count of ( qi , uj ) n2lm = pairs in cell C2m ( 1 ≤ m ≤ M ) . Similarly , we can initialize
1 ( qi|c1l ) ∝ qi,uj the URL generation probabilities .
For concept generation probabilities P1(c1l|tk ) , we again assume that the meaning of concepts does not change across different level of cells . We initialize P1(c1l|tk ) in three steps . First , we estimate the probability P2lm(tk|c2lm ) for concept c2lm in cell C2m to involve topic tk by P2lm(tk|c2lm ) ∝ P2m(tk)P2m(c2lm|tk ) . In the second step , we estimate the probability Pl(c1l|tk ) for concept c1l to involve topic tk by aggregating those probabilities from c1l ’s projected images , m n2lmP2m(tk|c2lm ) . Finally , the con1 ( c1l|tk ) ∝ n1,ij and n1,ij is the count of ( qi , uj ) pairs in c1l . The topic prior probability can be initialized by P 0 ie , P1(tk|c1l ) ∝ P1(c1l)P1(tk|c1l ) , where P1(c1l ) ∝ 1 ( tk ) ∝ cept generation probability can be initialized by P 0
P1(c1l)P1(tk|c1l ) . qi,uj c1l
B . MATERIALIZATION ON TC DIMENSION
The topic concept model assigns the concepts to a set of topics . Given a taxonomy of topics , such as ODP [ 1 ] , the TC dimension organizes the queries and clicks into a hierarchy of topics and concepts ( see Figure 1 ) . To materialize the cube on the TC dimension , we can first learn a topic concept model with respect to any level of topics in the hierarchy . Then , we can materialize the model parameters with respect to other levels of topics by either a top down approach or a bottom up approach .
Different from the standard dimensions , the topic concept dimension has the same set of training data at different levels . As a result , the initial concepts derived from the clustering results on the training data are the same for all the levels . Therefore , when we roll up or drill down along the TC dimension , it is reasonable to assume that only the topic prior probabilities P ( tk ) and the concept generation probabilities P ( cl|tk ) change substantially with respect to the different sets of topics at different level , while the query and URL generation probabilities , ie , P ( qi|cl ) and P ( uj|cl ) do In the following , we not change much at different levels . only focus on the initialization of P ( tk ) and P ( cl|tk ) . Without loss of generality , let T1 = {t1k} be the set of topics at some level of a given topic taxonomy , and T2 = {t2kn} be the set of topics one level lower than T1 . In particular , t2kn is a sub topic of t1k , where 1 ≤ n ≤ N1k and N1k is the number of sub topics of t1k .
2 ( t2kn ) and P 0
As illustrated in Figure 5(b ) , the top down approach along the TC dimension materialize the model parameters Θ1 with respect to T1 before the materialization of parameters Θ2 with respect to T2 . Given the parameters P1(t1k ) and P1(cl|t1k ) 2 ( cl|t2kn ) with respect to T1 , the initialization of P 0 is straightforward as follows . First , we can distribute the mass of prior probability P1(t1k ) evenly to its children t2kn ( 1 ≤ n ≤ N1k ) , ie , the initial topic prior probability P 0 P1(t1k)/N1k . Second , we inherit the concept generation probability of topic t2kn from that of its parent t1k , ie , 2 ( cl|t2kn ) = P1(cl|t1k ) . P 0 In the bottom up approach , we initialize the parameters for a higher level topic t1k by aggregating those of its sub topics t2kn ( Figure 5(b) ) . To be specific , the topic prior probability can be initialized by P 0 n P2(t2kn ) and the concept generalization probability can be initialized by P 0
1 ( cl|t1k ) ∝ n P2(cl|t2kn ) .
1 ( t1k ) =
2 ( t2kn ) =
C . EMPIRICAL STUDY ON CUBE CONSTRUC
TION
In this subsection , we report the results of an empirical study on different options to materialize the standard di
( a ) Date
( b ) Location
Figure 10 : The top down method vs . the na¨ıve method on the standard dimensions : ( a ) date and ( b ) location .
( a ) Date
( b ) Location
Figure 11 : The bottom up method vs . the na¨ıve method on the standard dimensions : ( a ) date and ( b ) location .
21 , . . . , Dt mensions , the TC dimension , and the whole TC cube . In this study , we used the time and location as two standard dimensions . The top cell C1 consists of the full data set D1 . Along the standard time dimension , the full data set splits into four subsets Dt 24 , where each subset consists of one month data and constitutes one child cell C t ( 1 ≤ nt ≤ 4 ) . Along the standard location dimension , each 2nt 2nl ( 1 ≤ nl ≤ 5 ) corresponds to one of the five child cell C l countries , ie , the United States , Canada , United Kingdom , Malaysia , and New Zealand , and contains the corresponding subset Dl 2nl of training data . We adopted the top two levels of topics of ODP [ 1 ] in the TC dimension , which consists of 16 and 483 topics , respectively .
Figure 10 compares the top down method and the na¨ıve method on the standard dimensions . As described in Appendix A.1 , in the top down method , we inherit the model parameters from the parent cell , while in the na¨ıve method , we initialize the model parameters from scratch ( ie , using the initialization method in Section 52 ) Figure 10(a ) shows the log data likelihood with respect to the number of EM iterations summed over the four month cells C t 24 . We can see the top down method achieves higher initial likelihoods than that by the na¨ıve method after initialization . However , both methods needed about five iterations to converge , and thus took similar runtime . Moreover , both methods converged to comparable likelihoods . Therefore , we may choose any of them to materialize the standard time dimension . Figure 10(b ) shows the log data likelihood summed over the five country cells C l 25 with respect to the number of EM iterations . This figure illustrates similar trends with those in Figure 10 .
21 , . . . , C t
21 , . . . , C l
Figure 11 compares the bottom up method and the na¨ıve method on the standard dimensions . As described in Appendix A.2 , in the bottom up method , we aggregate the model parameters of the child cells to initialize those for the parent cell , while in the na¨ıve method , we initialize the
( a ) Full data cell
( b ) Month cells
( b ) Country cells
Figure 12 : The bottom up method vs . the na¨ıve method on the TC dimension in ( a ) full data cell , ( b ) month cells , and ( c ) country cells . model parameters from scratch . Figures 11 shows the log data likelihoods in the parent cell C1 with respect to the number of EM iterations . The model parameters were initialized by aggregating those of month cells ( Figure 11(a ) ) and country cells ( Figure 11(b ) ) by the bottom up method , respectively . The bottom up method achieves higher initial likelihoods than that by the na¨ıve method after initialization . Again , both methods needed about five iterations to converge , and they converged to comparable likelihoods . Therefore , these two methods have comparable performance . Considering both Figures 10 and 11 , we may choose any of the bottom up , top down , and na¨ıve methods to materialize the standard dimensions .
Figures 12(a ) , ( b ) , and ( c ) compare the bottom up method and the na¨ıve method on the TC dimension in ( a ) the full data cell , ( b ) the month cells , and ( c ) the country cells , respectively . As described in Appendix B , in the bottom up method , we initialize the model parameters for the parent topic level by aggregating those from the child topic level , while in the na¨ıve method , we initialize the model parameters from scratch . In all the three figures , we observe similar trends . That is , the bottom up method achieves higher initial data likelihood than that of the na¨ıve method . However , both methods have comparable performance in efficiency and effectiveness .
We also compared the top down method and the na¨ıve method on the TC dimension . The top down method was much slower than the na¨ıve method . The reason is that when we inherit the model parameters from the upper level topics , most of the concept generation probabilities P ( c|t ) for the lower level topics are nonzero . In this case , the superset of nonzero parameters estimated by Theorem 1 can still be very large . Consequently , each process node needs to partition the assigned training tuples into many blocks and scan the large parameter file many times . Therefore , in the TC dimension , we may consider either the bottom up method or the na¨ıve method .
Finally , we compare the two options to materialize the whole TC cube , ie , the standard dimension first method and the TC dimension first method . In our study , the parent cell C1 consists of the full data set D1 . There are two types of child cells : the four month child cells C t 2nt ( 1 ≤ nt ≤ 4 ) and the five country child cells C l 2nl ( 1 ≤ nl ≤ 5 ) . For each cell , there are two topic levels T1 and T2 , which are the top two topic levels in the ODP taxonomy [ 1 ] . According to the above experimental results , we may choose any of the three options , ie , the bottomup , top down , or na¨ıve method , to materialize along the standard dimensions . Moreover , we may choose either the bottom up or the na¨ıve method to materialize along the TCdimension . Therefore , to materialize the whole TC cube , a possible standard dimension first approach consists of four
12345−165−16−155−15−145x 1010IterationLog L(Q,U ) Top−DownNaive12345−148−146−144−142−14−138−136−134−132−13x 1010IterationLog L(Q,U ) Top−DownNaive12345−176−174−172−17−168−166−164−162−16−158x 1010IterationLog L(Q,U ) Bottom−UpNaive12345−18−175−17−165−16−155x 1010IterationLog L(Q,U ) Bottom−UpNaive01234−176−174−172−17−168−166−164−162−16−158x 1010IterationLog L(Q,U ) Bottom−UpNaive12345−165−16−155−15−145−14x 1010IterationLog L(Q,U ) Bottom−UpNaive12345−145−14−135−13−125x 1010IterationLog L(Q,U ) Bottom−UpNaive ( a ) Month cells
( b ) Country cells
Figure 13 : The standard dimension first method vs . the TC dimension first method in ( a ) month cells and ( b ) country cells . steps : ( 1 ) materialize the second topic level T2 of the parent cell C1 from scratch ( ie , using the initialization method in Section 5.2 ) ; ( 2 ) materialize the second topic level T2 of the child cells C2 by the top down method along the standard dimensions ; ( 3 ) materialize the first topic level T1 of the parent cell C1 by the bottom up method along the TC dimension ; and ( 4 ) materialize the first topic level T1 of the child cells C2 by the bottom up method along the TC dimension . Analogously , a possible TC dimension first approach consists of the following four steps : ( 1 ) materialize the second topic level T2 of the parent cell C1 from scratch ; ( 2 ) materialize the first topic level T1 of the parent cell C1 by the bottom up method along the TC dimension ; ( 3 ) materialize the second topic level T2 of the child cells C2 by the top down method along the standard dimensions ; and ( 4 ) materialize the first topic level T1 of the child cells C2 by the top down method along the standard dimensions . The only difference between the two approaches is in step ( 4 ) . Therefore , we only focus on the fourth step of the two approaches .
Figures 13(a ) and ( b ) show the log data likelihoods with respect to the number of EM iterations by the standarddimension first approach and the TC dimension first approach in month cells and country cells , where the log data likelihoods are the sums over those of the four month cells and the five country cells , respectively . Both figures show similar trend . First , the standard dimension first approach has higher initial log data likelihood than that of the TCdimension first approach . This suggests the aggregation of the model parameters learned from the same data set is more accurate than those directly inherited from the larger parent data set . Second , both approaches need the same number of iterations to converge , and they converge to comparable likelihoods . Therefore , it does not make much difference to materialize the standard dimensions first or the TC dimension first .
12345−16−158−156−154−152−15−148−146−144x 1010IterationLog L(Q,U ) Standard−dimension−firstTC−dimension−first12345−144−142−14−138−136−134−132−13−128x 1010IterationLog L(Q,U ) Standard−dimension−firstTC−dimension−first
