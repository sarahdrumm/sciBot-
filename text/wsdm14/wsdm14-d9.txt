Predicting Response in Mobile Advertising with
Hierarchical Importance Aware Factorization Machine
Richard J . Oentaryo
Living Analytics Research Centre Singapore Management University
80 Stamford Road , Singapore roentaryo@smuedusg
Ee Peng Lim
Jia Wei Low
Living Analytics Research Centre Singapore Management University
80 Stamford Road , Singapore
Living Analytics Research Centre Singapore Management University
80 Stamford Road , Singapore eplim@smuedusg jwlow@smuedusg
David Lo
Living Analytics Research Centre Singapore Management University
80 Stamford Road , Singapore davidlo@smuedusg
Michael Finegold
Department of Statistics
Carnegie Mellon University
229K Baker Hall , Pittsburgh , PA mfinegol@andrewcmuedu
ABSTRACT Mobile advertising has recently seen dramatic growth , fueled by the global proliferation of mobile phones and devices . The task of predicting ad response is thus crucial for maximizing business revenue . However , ad response data change dynamically over time , and are subject to cold start situations in which limited history hinders reliable prediction . There is also a need for a robust regression estimation for high prediction accuracy , and good ranking to distinguish the impacts of different ads . To this end , we develop a Hierarchical Importance aware Factorization Machine ( HIFM ) , which provides an effective generic latent factor framework that incorporates importance weights and hierarchical learning . Comprehensive empirical studies on a real world mobile advertising dataset show that HIFM outperforms the contemporary temporal latent factor models . The results also demonstrate the efficacy of the HIFM ’s importance aware and hierarchical learning in improving the overall prediction and prediction in cold start scenarios , respectively .
Categories and Subject Descriptors H33 [ Information Storage and Retrieval ] : Information Search and Retrieval ; H.4 [ Information Systems Applications ] : Miscellaneous
General Terms Algorithms , Experimentation
Keywords Factorization machine , hierarchy , importance weight , mobile advertising , response prediction
1 .
INTRODUCTION
The proliferation of mobile phones and devices has made mobile advertising an effective means for businesses to target the desired market on the fly . Global mobile advertising revenue is projected to hit $11.4 billion by 2013 , creating new opportunities in many countries [ 7 ] . A mobile advertising ecosystem is typically governed by an ad network , which mediates all transactions between advertisers and content publishers . The advertiser plans a budget , provides ads , and agrees on commission for customer actions ( eg , clicking an ad , or bidding in an auction ) . The publisher contracts with the ad network to display ads on their ( web)pages , and earns commission based on the traffic driven to the advertisers .
The most popular monetary paradigm in mobile advertising is the cost per click ( CPC ) scheme . An advertiser places a bid for an ad to be placed on some publisher ’s webpage , and pays the bid amount to the publisher only if the ad is displayed ( ie , an exposure ) and chosen by a user ( ie , a click ) . Among the competing ads , the publisher chooses to display the ad with the highestexpected revenue , which is the ad ’s bid price times the probability it is clicked . This probability is known as clickthrough rate ( CT R ) , and accurately estimating it is important for maximizing revenue . The task of estimating CT R ( or similar metrics such as conversion rate ) falls under the term response prediction [ 11 , 15 ] .
Response prediction tasks present several challenges . First , ad response data are dynamic and change over time . There is a need for a method that can account for the temporal aspects of the data . Second , most ads have limited or no past history in a page–also known as the cold start issue . That is , the response estimation is highly unreliable if we have very low ad exposures . Robust estimation in cold start cases is important for the advertisers , as it provides a means to measure/explore the potential of investing in new pages and ads . To increase robustness , we can plausibly leverage on prior knowledge in the form of hierarchical structure of pages and ads . Such hierarchy contains useful information about correlations between responses at various levels , which can smoothen the estimation for limited historical data .
From the modeling standpoint , response prediction tasks involve two requirements : good regression and good ranking performances [ 16 ] . In real time auctions for mobile adver
123 tising , ads are often ranked based on bid × CT R . It is thus important that CT R gives good ranking that distinguishes the impacts of different ads . Also , ads are usually priced via a next price auction , ie , the price of a click on an ad at rank i is based on the ad ’s expected bid× CT R at the next lower rank [ 3 ] . In this , the CT R ( regression ) estimates must be as accurate as possible , especially for ads that are displayed many times . That is , high importance weights should be assigned to ads with high exposures , as the cost of inaccurately predicting the response of such ads is high .
To address all the above challenges and requirements , we present in this paper a novel latent factor model for response prediction , termed Hierarchical Importance aware Factorization Machine ( HIFM ) . Our HIFM is built upon the state of the art factorization machines ( FM ) [ 12 , 13 ] , with new extensions to utilize importance weights and hierarchical knowledge for handling temporal/dynamic ad response data . We summarize our key contributions as follows :
• We develop a new latent factor framework that utilizes importance aware and hierarchical learning for handling temporal ad response data . These two ideas help improve response prediction by putting priorities based on ads’ exposures and by making more informative prediction in cold start cases , respectively . Deviating from the existing approaches , which are often very specialized ( eg , only works for dyads ) and not able to cater for temporal data , HIFM offers a generic approach to simultaneously address importance weight , hierarchical knowledge , and temporal dynamics .
• We adapt and generalize two learning algorithms , the stochastic gradient descent ( SGD ) and coordinate descent ( CD ) , to facilitate importance aware and hierarchical learning in HIFM . We adopt roulette wheel sampling strategy and weighted least square update to cater for the importance weight in SGD and CD respectively , while hierarchical learning is achieved in both SGD and CD by means of hierarchical fitting and regularization . For each algorithm , we develop several variants with ( the same ) linear runtime complexity . These extensions in turn facilitate comprehensive studies for evaluating different methods .
• Extensive experiments on real world mobile advertising data show that HIFM outperforms contemporary temporal latent factor models . We also find statistically significant improvements due to the HIFM ’s importance aware and hierarchical learning , in terms of the overall ( weighted ) prediction and prediction for cold start cases , respectively . This shows the applicability of HIFM in complex response prediction tasks .
2 . RELATED WORK
Traditionally , the data mining approaches to ad response feature based and prediction broadly fall into two camps : maximum likelihood based [ 11 ] . In feature based methods , prediction models are constructed based on explicit features of a page and/or an ad . These features–also termed as side information–may include textual content of an ad , its placement on the ( web)page , etc . Feature based methods typically utilize prediction models from logistic regression family [ 8 , 15 ] . However , constructing these models requires extensive manual intervention or domain knowledge . On the other
Table 1 : Global count statistics
Minimum exposure ( emin )
Entity #records #(web)pages #publishers #countries #channels #ads #advertisers #banner types
1
10
100
24,172,134
10,535,658
3,587,160
244,341
138,351
3,945 243
8
23,500 1,989
5
3,539 239
8
18,365 1,406
4
55,260 2,654 226
8
15,600 1,245
3
1000
931,032 16,374 1,643 199
8
10,877 1,124
3 hand , maximum likelihood based methods try to smoothen the response estimation using statistical models of ad clicks and exposures , eg , the Gamma Poisson model [ 1 , 2 ] . These methods , however , are based on simple linear models and lack the ability to capture rich latent structure in data .
In this light , Menon et al . [ 11 ] proposed a hybrid approach by combining matrix factorization ( MF ) with explicit page and ad features as well as hierarchical information about pages and ads . This method learns a set of latent features from data through MF , and at the same time allows side information–typical of traditional feature based methods–to be incorporated for improving the prediction . However , the MF representation used in this method is only restricted to dyadic relations ( eg , page vs . ad ) , and does not cater for higher order relations such as those that include temporal dynamics ( eg , page vs . ad vs . day ) . Similar hierarchical MF methods were proposed in [ 23 , 18 ] , but like [ 11 ] all of them assume a restrictive dyadic representation .
On the other hand , several time aware factorization approaches have been proposed , which take into account the temporal dynamics in data . Comon et al . presented a tensor factorization method trained using an alternative least square algorithm [ 5 ] . Koren developed TimeSVD++ to address temporal dynamics via specific parameterization with factors drifting from a central time [ 9 ] . More recently , Shen et al . [ 19 ] proposed a tensor factorization model to address the issue of personalization in click modeling . Other models have been developed that combine MF and Kalman filter to simultaneously model the spatial and temporal patterns in data [ 10 , 21 ] . Regardless , all these methods use restrictive representation and cannot be easily extended to augment side information such as hierarchical prior knowledge .
Recently , Rendle [ 12 , 13 ] proposed a generic factorization machine ( FM ) framework , which can efficiently handle arbitrary relationships ( ie , dyad , triad , etc ) The approach can thus be used to address temporal data and can easily incorporate side information . As noted in [ 13 ] , the FM approach can mimic many state of the art factorization models ( eg , pairwise tensor factorization , TimeSVD++ , attributeaware models , and neighborhood models ) , and exhibits good performances on many competitive tasks [ 13 ] . Nonetheless , the current FM does not yet cater for hierarchical structure and/or instance importance in its model . In light of response prediction task , we extend and generalize the FM framework in this work by combining importance aware learning and hierarchical fitting and regularization .
3 . PROBLEM FORMULATION 3.1 Dataset
In our study , we use the data provided by our industry partner , a global mobile advertising network based in Sin
124 0.0020
0.0015
R T C f o n o i t
0.0020
0.0015
R T C f o n o i t
0.0020
0.0015
R T C f o n o i t i a v e D d r a d n a S t
0.0010
0.0005 i a v e D d r a d n a S t
0.0010
0.0005 i a v e D d r a d n a S t
0.0010
0.0005
0.0000
0.0000
0.0000
5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31
5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31
5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31
( b ) Publisher × advertiser
Day
Day
( a ) Page × advertiser
( c ) Channel × advertiser
Day
0.0020
0.0015
R T C f o n o i t
0.0020
0.0015
R T C f o n o i t i a v e D d r a d n a t S
0.0010
0.0005 i a v e D d r a d n a t S
0.0010
0.0005
0.0000
0.0000
5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31
Day
( d ) Publisher × ad
5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31
Day
( e ) Channel × ad
Figure 1 : Temporal distribution of click through rates grouped at various hierarchy levels gapore . The supplied data consist of a campaign table and an ad click table . The latter contains records of clicks and exposures for various ( web)pages and ads at different days . The data were taken from a 27 day period , 05 31 October 2012 , and have 24,172,134 records . In this period , there are over 17 billion ad exposures and over 42 million ad clicks .
The ad click table has several fields : day ( yyyy mm dd format ) , page ( page ID ) , publisher ( publisher ID ) , ad ( ad campaign ID ) , channel ( publisher ’s channel type ) , and numclick and numexpose ( number of clicks and exposures for a page ad pair at a given day , respectively ) . Each record in the table is uniquely identified by a ( page , ad , day ) triplet . There are five ( ie , channel ) types , each denoting the publisher category ( eg , adult sites , entertainment and lifestyle , glamour and dating , etc ) From numclick and numexpose , we can then estimate CT R = numclick , though precaution numexpose needs to be exercised when numexpose is low . Also note that all records in the ad click table have numexpose ≥ 1 . The campaign table , on the other hand , complements the ad click table and comprises several fields : ad ( as with the ad click table ) , advertiser ( advertising company of the ad ) , and bannertype ( type of ad banners ) . That is , the table maintains a list of ads with their corresponding advertisers and banner types ; the latter involve five categories : {image , remote , splash , tablet , text} . It is also worth noting that all fields ( except for day ) in the ad click and campaign tables are anonymized for security protection .
Table 1 summarizes the count statistics derived from the ad click and campaign tables . It shows the counts of various entities as the minimum exposure threshold emin is varied from 1 to 1000 . Here the number of records refers to that of ( page , ad , day ) triplets in the ad click table . As emin gets increased , we can see rather drastic reductions in the counts ( except for channel and bannertype ) , suggesting a heavytail distribution where most pages have small number of ad exposures and thus low confidence CT R estimates . 3.2 Response Prediction
We now formulate our response prediction task : Given a pair of page p and ad a at a specific day d , we predict the click probability CT R . That is , we wish to compute the probability Pp,a,d = P r(Click|Exposure , p , a , d ) , where p , a and d denote a particular page , ad , and day respectively . ep,a,d p,a,d = cp,a,d
A simple solution to this task is to use the maximum likeli,∀ep,a,d > 0 , hood estimate ( MLE ) [ 11 ] , ie , P M LE where cp,a,d and ep,a,d are the numbers of ad clicks and exposures respectively . However , when ep,a,d is very small , the MLE estimate would be highly noisy . For instance , if an ad has been exposed 3 times on a page and received no click , a CT R estimate of 0 would make no sense . As this extreme case is common in practice , we need an alternative estimate ˆP that smoothens the MLE so as to make it more reliable . As advocated in [ 11 ] , a good predictive model for estimating ˆP should fulfill several criteria . First , it should provide smoothened estimates in the face of very few exposures ep,a,d . Second , it should output meaningful probabilities , within the range [ 0 , 1 ] . Finally , the MLE should be consistent , meaning that as ep,a,d → ∞ , ˆPp,a,d → P M LE p,a,d . In other words , when the number of exposures is large enough , any estimator ˆP should converge to the MLE . 3.3 Page and Ad Hierarchies
Further investigations on the ad click and campaign tables reveal that page and ad hierarchies exist in the data . That
125 is , a page is associated with a publisher , and a publisher belongs to a unique channel . Similarly , an ad belongs to an advertiser . Hierarchical structure encodes useful prior knowledge for improving CT R estimation when we have little data at granular level . For instance , if a ( page , ad ) pair has only very small numexpose , but the siblings of the ad has large numexpose , we can increase the confidence of its estimate by leveraging the siblings’ estimates . This allows a “ back off ” mechanism for borrowing data from coarse level to smoothen the estimation at the granular level [ 11 ] .
For our hierarchy analysis , we compute the CT R distribution for different pairs ( Cartesian products ) of entities in the page and ad hierarchies : page × advertiser , publisher×ad , channel×ad , publisher×advertiser , and channel × advertiser . Figure 1 shows the box plots of the CT R variations ( ie , first quantile , median , and third quantile ) for each entity pair over 27 days . To obtain Figure 1(a ) , for instance , we first group each ad by the advertiser , and then look at the CT R variation under the page advertiser pair for a given day . Specifically , we calculate the standard deviation of the CTR for each page advertiser pair , which measures the homogeneity of the CT R under the group . We repeat the procedure for the remaining ( four ) combinations , the results of which are shown in Figures 1(b) (d ) .
From these plots , we can make several observations . First , the CT R values vary across different days , which justifies the need to model temporal dynamics . Second , comparing Figure 1(a) (c ) and 1(d) (e ) , we can see that the CTR variability increases as we go up in the hierarchy , implying that the CT R values are less ( more ) homogeneous at the higher ( lower ) level . This suggests the feasibility of the “ back off ” mechanism , ie , when a child node lacks data , we can leverage on the estimates from its parent(s ) , and then those from its grandparent(s ) , and so on . 4 . FACTORIZATION MACHINE
The HIFM model we proposed to predict CT R is built upon a generic latent factor model called factorization machine ( FM ) [ 12 , 13 ] . We first describe the basic FM in this section , and then new extensions that we develop in house for HIFM in Section 5 , based on different importance aware learning methods and use of page and ad hierarchies . 4.1 Model Overview The FM takes as its inputs a real valued feature matrix ( also called design matrix ) X ∈ RN×J . For an input vector xi ( i ∈ {1 , . . . , N} , xi ∈ RJ ) , the output ˆyi of an ( order 2 ) FM is defined in ( 1 ) :
J .
ˆy(xi ) = w0 + wjxi,j + j=1
J . j.=j+1
J . fi j=1 xi,jxi,j .
'ff
= ˆp(xi )
K . k=1 vk,jvk,j .
( 1 ) where K is the dimensionality of the interaction factors , and the model parameters Θ = {w0 , w1 , . . . , wJ , v1,1 , . . . , vK,J} consist of wo ∈ R , w ∈ RJ , andV ∈ RK×J .
According to [ 12 ] , we can efficiently compute the term
ˆp(xi ) in ( 1 ) based on its equivalent formulation ( 2 ) :
K .
⎡ ⎣ )
J .
2
− J .
⎤ ⎦
ˆp(xi ) =
1 2 vk,jxi,j v2 k,jx2 i,j k=1 j=1 j=1
This makes it possible to quickly process all N instances of a design matrix X in linear time O(KNz(X) ) , where Nz(X ) is the number of non zero entries in X . 4.2 Multilinearity Property for each model parameter θ ∈ Θ , the FM output ˆy(xi ) is a linear combination of two functions rθ(x ) and gθ(x ) , as per ( 3 ) :
The FM has an appealing multilinearity trait :
ˆy(xi ) = rθ(xi ) + θgθ(xi ) ,
∀θ ∈ Θ where gθ(xi ) corresponds to the gradient term ( 4 ) :
⎧⎪⎨ ⎪⎩1 xi,j xi,j ( qi,k − vk,jxi,j ) if θ = w0 if θ = wj if θ = vk,j
( 3 )
( 4 ) gθ(xi ) =
∂ ˆy(xi )
=
∂θ
J and qi,k = j=1 vk,jxi,j is the inner product term that can be precomputed ( cached ) for efficient learning ( cf . Section 53 ) Note that in our learning procedures , the residual term rθ(xi ) need not be computed ; we only use the term gθ(xi ) .
5 . THE PROPOSED HIFM FRAMEWORK We extend the basic FM model to incorporate importance weights and hierarchical structure for response prediction tasks , which results in the proposed HIFM model . We first discuss how importance weights and hierarchy can be incorporated into our model in Sections 5.1 and 5.2 , respectively . Section 5.3 then presents our several variants of optimization algorithm for learning the HIFM ’s model parameters . 5.1 Importance Aware Loss Functions We consider the problem of predicting the click probability ˆyi(x|Θ ) ∈ [ 0 , 1 ] ( ie , CTR ) , given a design matrix X and model parameters Θ . The goodness of fit between the predicted and actual probabilities can be expressed using the two objective functions in ( 5 ) and ( 6 ) :
Lsquare(Θ ) =
N . Llogistic(Θ ) = − N . i=1 ei(
− ˆy(xi|Θ))2 + Ω(Θ ) ci ei ( ci log σ(ˆy(xi|Θ))+
( 5 ) i=1
( ei − ci ) log ( 1 − σ((ˆy(xi|Θ) ) ) + Ω(Θ ) )
( 6 ) where ci ≥ 0 and ei ≥ 1 denote the numbers of observed ad clicks and exposures ( impressions ) for the ith instance xi ∈ X , respectively , ˆyi(x|Θ ) ∈ [ 0 , 1 ] is the click probability to be predicted by FM , Ω(Θ ) is the regularization term , and σ(x ) = 1 1+e−x is the logistic function . These formulae are also known as the weighted square and weighted logistic loss functions , using ei as the importance weights .
The regularization term Ω(Θ ) is useful to reduce the risk of data overfitting , due to a large number of model parameters Θ ( especially when K is large ) . Typically , L2 regularization is used [ 20 ] as given in ( 7 ) :
Ω(Θ ) =
1 2
λθθ2
( 7 )
.
θ∈Θ
( 2 ) which is equivalent to imposing a Gaussian prior on θ , ie , θ ∼ N ( 0 , λ −1 θ ) . This regularization essentially tries to penalize model parameters that have large ( squared ) norm , thus driving the parameter values to be small .
126 ƌŽŽƚƉ
,PSRUWDQFH
ZHLJKW
3DJH
$G
'D\
3XEOLVKHU &KDQQHO $GYHUWLVHU
5RRWV
Ɖϭ ƉϮ Ɖϯ Ăϭ ĂϮ Ăϯ Ěϭ ĚϮ Ɖďϭ ƉďϮ
Ϭ
ϭ
ϭ
Ϭ
Ϭ
Ϭ
Ϭ
ϭ
Ϭ
Ϭ
ĐŚϭ Ϭ
ĐŚϮ Ăǀϭ ĂǀϮ Ϭ Ϭ
Ϭ
&KDQQHO
OHYHO
FK
FK
URRWD
3XEOLVKHU
OHYHO
$GYHUWLVHU
OHYHO
SE
SE
DY
DY
URRWG
[
S D G
[
S D G
[
S D G
[
S D G
[
S D G
[
S D G
Figure 2 : Example of page and ad hierarchies
5.2 Hierarchical Learning
Inspired by [ 11 , 18 ] , we utilize hierarchical information in our FM model via two ways : hierarchical fitting and regularization . For illustration , we consider the page and ad hierarchies in Figure 2 . There are six instances at the bottom level , each corresponding to a page ad day ( p , a , d ) triplet . For the page hierarchy , each page belongs to a publisher node ( pb ) and then a publisher belongs to a unique channel ( ch ) . Meanwhile , for the ad hierarchy , each ad belongs to an advertiser node ( av ) . We implement the hierarchies as a directed graph ( DG ) . It should be noted that the hierarchy structure need not be a tree ; the DG representation allows us to work with non tree hierarchy as well .
Ğŝ ϭϬϬ
ϮϬϬ
ϭϬϬ
ϭϬϬ
ϮϬϬ
ϭϬϬ
ϭϬϬ
ϮϬϬ
ϭϬϬ
ϮϬϬ
ϮϬϬ
ϭϬϬ
ϮϬϬ
ϭϬϬ
ϮϬϬ
ϮϬϬ
ϭϬϬ
ϯϬϬ
ϭϬϬ
ϮϬϬ
ϭϬϬ
ϮϬϬ
ϭϬϬ
ϭϬϬ
ϮϬϬ
ϭϬϬ
ϯϬϬ
ϭϬϬ
ϭϬϬ
ϯϬϬ
ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
ϭ
ϭ
Ϭ
Ϭ
Ϭ
Ϭ
ϭ
Ϭ
Ϭ
Ϭ
Ϭ
ϭ
ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
ϭ
ϭ
Ϭ
Ϭ
Ϭ
ϭ
ϭ
Ϭ
Ϭ
Ϭ
Ϭ
ϭ
ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
ϭ
ϭ
Ϭ
Ϭ
Ϭ
ϭ
Ϭ
Ϭ
Ϭ
ϭ
Ϭ
ϭ
Ϭ
Ϭ
Ϭ
ϭ
ϭ
Ϭ
Ϭ
Ϭ
ϭ
ϭ
Ϭ
Ϭ
ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
ϭ
ϭ
Ϭ
Ϭ
Ϭ
Ϭ
ϭ
ϭ
Ϭ
Ϭ
Ϭ
ϭ
ϭ
Ϭ
Ϭ
Ϭ
ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
ϭ
ϭ
ϭ
Ϭ
Ϭ
Ϭ
ϭ
ϭ
Ϭ
Ϭ
Ϭ
ϭ
ϭ
Ϭ
Ϭ
ϭ
ϭ
Ϭ
Ϭ
Ϭ
ϭ
ϭ
ϭ
Ϭ
Ϭ
ϭ
ϭ
ϭ
ϭ
Ϭ
Ϭ
Ϭ
ϭ
ϭ
ϭ
Ϭ
Ϭ
ϭ
ϭ
ϭ
Ϭ
Ϭ
ϭ
ϭ
Ϭ
Ϭ
ϭ
ϭ
ϭ
Ϭ
Ϭ
Ϭ
ϭ
ϭ
Ϭ
Ϭ
Ϭ
Ϭ
ϭ
Ϭ
ϭ
Ϭ
Ϭ
Ϭ
ϭ
Ϭ
Ϭ
Ϭ
Ϭ
ϭ
Ϭ
Ϭ
Ϭ
ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
ƌŽŽƚƉ
ƌŽŽƚĂ
ƌŽŽƚĚ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
ϭ
ϭ
ϭ
ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
ϭ
ϭ
ϭ
ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
7DUJHW dZ
Ϭ͘ϬϬϭϬ
Ϭ͘ϬϬϮϬ
Ϭ͘ϬϬϮϱ
Ϭ͘ϬϬϭϯ
Ϭ͘ϬϬϬϴ
Ϭ͘ϬϬϭϬ
Ϭ͘ϬϬϭϬ
Ϭ͘ϬϬϮϬ
Ϭ͘ϬϬϮϱ
Ϭ͘ϬϬϭϮ
Ϭ͘ϬϬϬϴ
Ϭ͘ϬϬϭϬ
Ϭ͘ϬϬϮϬ
Ϭ͘ϬϬϮϱ
Ϭ͘ϬϬϬϴ
Ϭ͘ϬϬϭϬ
Ϭ͘ϬϬϭϬ
Ϭ͘ϬϬϮϮ
Ϭ͘ϬϬϬϴ
Ϭ͘ϬϬϭϬ
Ϭ͘ϬϬϭϬ
Ϭ͘ϬϬϮϬ
Ϭ͘ϬϬϮϱ
Ϭ͘ϬϬϭϯ
Ϭ͘ϬϬϬϴ
Ϭ͘ϬϬϭϬ
Ϭ͘ϬϬϭϯ
Ϭ͘ϬϬϮϱ
Ϭ͘ϬϬϭϯ
Ϭ͘ϬϬϴϳ
SDJH î DG î GD\
SXEOLVKHU î DG î GD\
FKDQQHO î DG î GD\
URRWS î DG î GD\
SDJH î DGYHUWLVHU î GD\
SDJH î URRWD î GD\
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
ϭ
ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
ϭ
ϭ
ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
ϭ
ϭ
ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
ϭ
ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
ϭ
Ϭ
Ϭ
Ϭ
ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
Ϭ
ϭ
ϭ
ϭ
Ϭ
ϭ
Ϭ
Ϭ
Ϭ
Ϭ dŚĞ ƌĞŵĂŝŶŝŶŐ ĐŽŵďŝŶĂƚŝŽŶƐ ͙
.
θp∈P ar(θ )
1
|P ar(θ)| 2
θ − ¯θP ar
To incorporate hierarchical constraints into FM , we let every node in the hierarchy to possess its own latent factors ( ie , {wj} and {vk,j} ) . Using this setup , we impose hierarchical regularization on the latent factors , so that each factor has a prior that makes it more similar to its parent in expectation . This is expressed in ( 8 ) :
θ0 ∼ N ( 0 , λ
θ0 ),∀θ0 ∈ Hroot −1 θj ),∀θj ∈ H − Hroot −1
θj ∼ N ( ¯θP ar , λ
( 8 )
( 9 ) where ¯θP ar is the average of the parents’ latent factors as defined in ( 10 ) :
¯θP ar =
θp
( 10 )
1 2 1 2 where P ar(θ ) is the set of all parents of θ .
The resulting hierarchical regularization is given in ( 11 ) : if θ ∈ H − Hroot if θ ∈ Hroot
θ λθ θ λθθ2
Ω(Θ ) =
( 11 ) where H is the set of all nodes in the hierarchy , and Hroot is the set of root nodes sitting at the top of the hierarchy ( ie , with no incoming edges ) . In other words , every node is expected to be similar to its parents ( if any ) . Note that our regularization approach is more general than that in [ 11 , 18 ] , which is only restricted to dyads ( pages and ads in this case ) and does not model temporal dynamics . Also , in our approach , the interactions between pages , ads ( and days ) are governed by shared latent factors Vk,j , instead of separate latent factors for pages and ads as in [ 11 , 18 ] . Together with hierarchical regularization , this enables our method to handle sparse data more effectively .
Figure 3 : Example of feature representation for FM
While hierarchical regularization alone can help constrain the parameters of the nodes , the final derived priors may not conform directly to the underlying click and exposure data . More informative priors can be obtained by fitting the data directly , that is , by agglomerating the click/expose data over various pages and ads . As an example , for a publisher node pb , we agglomerate the clicks/exposures of all page ad day triplets ( p , a , d ) wherep belongs to publisher . In this way , a reasonable prior for the children ’s latent factors can be learned . Formally , the aggregated clicks and exposures of a non leaf node u in the hierarchy are defined in ( 12 ) :
.
. cu = v:u∈P ar(v ) cv , eu = v:u∈P ar(v ) ev
( 12 )
The base case is when v is the leaf node , such that cu ( or eu ) is the standard click ( or expose ) value ci ( or vi ) .
Figure 3 illustrates the FM compatible feature representation for the example in Figure 2 . We use binary categorical features to represent various entities in the hierarchy . Each row represents a data instance , and is associated with a target response ( CT R ) and importance weight ( exposures ) ei . The base instances ( ie , page × ad × day ) correspond to the first six rows . In the next level publisher × ad × day , we aggregate the exposures ei ( and similarly the clicks ci ) , yielding the next five rows . We repeat this for the remaining combinations to obtain the full design matrix X for FM .
One may notice from Figure 3 that , due to the agglomeration process , the exposures and clicks of the base cases page× ad× day will be outweighed by the ones in the parent
127 level ( eg , publisher× ad× day ) . As our model is weighted by the exposures , the contribution of the base cases would be “ ignored ” in our HIFM training . To remedy this , we rescale the importance weights of all non base cases using ( 13 ) : ei ← ei 2 p∈page×ad×day ep qfi∈page×ad×day eq
,∀i )∈ page × ad × day
( 13 )
That is , we rescale based on the ratio of the sum of exposures of the base cases over that of the non base cases . 5.3 Optimization Algorithms
To support hierarchical and importance aware learning in HIFM , we develop two classes of efficient iterative methods : stochastic gradient descent ( SGD ) and coordinate descent ( CD ) , as described in Sections 531 and 532 respectively . 531 Stochastic Gradient Descent The standard , unweighted SGD update for a specific model parameter θ is given in ( 14 ) :
θ ← θ − η
∂l(yi , ˆy(xi|Θ ) )
∂θ
+
∂Ω(Θ )
∂θ
( 14 ) where yi is the target value of the ith instance , and l(a , b ) = ( b − a)2 for square loss or l(a , b ) = −a log(σ(b ) ) − ( 1 − a ) log(1 − σ(b ) ) for logistic loss . The ( sub)gradients of the square and logistic losses translate to ( 15) (16 ) respectively :
∂lsquare(yi , ˆy(xi|Θ ) ) ∂llogistic(yi , ˆy(xi|Θ ) )
∂θ
∂θ
= ( ˆy(xi|Θ ) − yi ) gθ(xi ) = ( σ(ˆy(xi|Θ ) ) − yi ) gθ(xi )
( 15 )
( 16 )
A simple way to incorporate importance weights is to oversample the instances as many as the number of exposures ei . However , for large ei , the oversampled data would be much larger than the original data , leading to inefficient computation . Another tempting approach is to multiply the gradient term gθ(xi ) by ei . However , such approach violates the principle that an instance xi with importance weight ei should be treated as if it appears ei times in the data .
Algorithm 1 Importance aware SGD with regularization Input : Design matrix X , targets y = {yi} , regularization parameters {λθ} , initial width σ , learning rate η
∂w0
+ ∂Ω(Θ ) ∂w0
∂l(ˆy(xi|Θ),yi )
Output : Model parameters Θ 1 : w0 ← 0 ; w ← ( 0 , . . . ,0 ) ; V ∼ N ( 0 , σ2 ) 2 : repeat 3 : for i ∈ {1 , . . . , N} do
4 : i ← RouletteWheelSelection( ) 5 : w0 ← w0 − η
6 : for j ∈ {1 , . . . , J} ∧x j )= 0 do 7 : wj ← wj − η 8 : for k ∈ {1 , . . . , K} do 9 : vk,j ← vk,j − η 10 : end for 11 : end for 12 : end for 13 : until stopping criterion is met
∂l(ˆy(xi|Θ),yi )
∂l(ˆy(xi|Θ),yi )
+ ∂Ω(Θ ) ∂vk,j
+ ∂Ω(Θ ) ∂wj
∂vk,j
∂wj given by p(xi ) = ei.N
. i.=1 e i
, and subsequently a random sam pling ( with replacement ) is carried out according to p(xi ) until N instances are obtained1 . In this way , we can avoid explicitly oversampling the data , while achieving asymptotically the same effect of presenting an instance ei times , given sufficient number of iterations . Note that , for each instance , the runtime complexity of our importance aware SGD remains the same as the original SGD , ie , O(KNz(X) ) .
Our importance aware SGD method ( with generic regularization term ) is summarized in Algorithm 1 . With respect to the different regularizations ( 7 ) and ( 11 ) , the term ∂Ω(Θ ) evaluates to ( 17 ) and ( 18 ) respectively :
∂θ
∂Ω(Θ )
∂θ
∂Ω(Θ )
∂θ
θ − ¯θP ar
( 17 )
( 18 )
= λθθ
= λθ where we have a special case ¯θP ar = 0 if θ ∈ Hroot .
Finally , in cold start situations , we perform a special procedure when dealing with a new column j in unseen ( test ) data . Specifically , whenever we have hierarchy information , we augment our prediction using the latent factors of the ancestors . This is realized via the “ back off ” formula ( 19 ) : wj ← wj . , vk,j ← j.∈P ath(j)−{j} j.∈P ath(j)−{j} vk,j .
( 19 )
.
. where P ath(j ) refers to the path from the root node to node j in the hierarchy . For a non tree hierarchy , we utilize the average latent factor of the parent nodes level by level . 532 Coordinate Descent The basic idea of the CD algorithm is to start with an initial ( random ) guess of Θ , and then iterate over and update each model parameter θ ∈ Θ , assuming all the other parameters are fixed . We propose an extended importance aware CD with generic regularization . For the weighted square loss in ( 5 ) , the importance aware CD algorithm performs an update of the form ( 20 ) : ei(ˆy(xi|Θ ) − yi)2 + Ω(Θ )
( 20 )
)
N .
θ = arg min
θ i=1
Note that , for logistic loss , there is no closed form solution for the update , so we focus only on square loss in this paper . Consequently , the update steps for the regularization in
( 17 ) and ( 18 ) are given by ( 21 ) and ( 22 ) respectively :
N i=1 eig2
θ ← θ
N i=1 eigθ(xi)ri
θ ( xi ) + λθ
θ ( xi ) + N i=1 eig2 θ ( xi ) +
N
N
N i=1 eig2 i=1 eig2
θ ← θ i=1 eigθ(xi)ri + λθ ¯θP ar θ ( xi ) + λθ
( 22 ) where ri = yi − ˆy(xi|Θ ) is the residual term and , again , ¯θP ar = 0 if θ ∈ Hroot . The update for standard ( L2 ) regularization is thus a special case of ( 22 ) , where P ar(θ ) =∅ . In the case of a cold start column such that all gθ(xi ) = 0 , hierarchical regularization gives us θ ← ¯θP ar , which should lead to a more informative prediction .
( 21 )
In light of these drawbacks , we propose to use a simple roulette wheel selection mechanism [ 4 ] . That is , the probability of selecting instance xi with importance weight ei is
Algorithm 2 outlines our importance aware CD method with generic regularization . Here , linear time learning can 1The standard , unweighted SGD refers to a special case of roulette wheel selection with uniform probability p(xi ) = 1 N
128 Algorithm 2 Importance aware CD with regularization Input : Design matrix X , targets y = {yi} , regularization parameters {λθ} , initial width σ
Output : Model parameters Θ 1 : w0 ← 0 ; w ← ( 0 , . . . ,0 ) ; V ∼ N ( 0 , σ2 ) 2 : repeat 3 : ˆy ← predict all instances X 4 : r ← y − ˆy 5 : Update w0 using ( 21 ) or ( 22 ) 6 : Update cache r using ( 25 ) 7 : for j ∈ {1 , . . . , J} do 8 : Update wj using ( 21 ) or ( 22 ) 9 : Update cache r using ( 25 ) 10 : end for 11 : for k ∈ {1 , . . . , K} do 12 : Initialize cache q.,k using ( 24 ) 13 : for j ∈ {1 , . . . , J} do 14 : Update vk,j using ( 21 ) or ( 22 ) 15 : Update cache r using ( 25 ) 16 : Update cache q using ( 26 ) 17 : end for 18 : end for 19 : until stopping criterion is met be achieved using r cache and q cache , which store the results of ( pre)computing the residuals {ri} and inner product terms {qi,k} . They are given in ( 23 ) and ( 24 ) respectively : ( 23 ) ri = yi − ˆy(xi|Θ )
J . qi,k = vk,jxi,j j=1
( 24 )
After each update of model parameter θ , we update the e cache accordingly . Similarly , for every update of Vk,j , we update the q cache . The cache update steps are summarized in ( 25 ) and ( 26 ) respectively : ri ← ri − qi,k ← qi,k +
θnew − θold k,j − vold vnew k,j gθ(xi ) xi,j
Referring to ( 22 ) , the bulk of the CD computations lies on N i=1 eigθ(xi)ri . By caching two terms : residuals {ri} and inner products {qi,k} , each full iteration over all θ ∈ Θ can be done quickly in O(KNz(X) ) . i=1 eig2
θ ( xi ) and
N
We also note that Algorithm 2 actually corresponds to the cyclic coordinate descent , which traverses all columns j in a fixed order ( ie , from j = 1 to j = J ) . In a similar vein to [ 17 ] , we also develop a stochastic variant called stochastic coordinate descent , which iterates over the columns in a random order instead . In our experiments , we found that this stochastic version exhibits faster convergence , albeit producing similar overall prediction quality ( cf . Section 63 )
6 . EXPERIMENTS 6.1 Setup
We conducted 10 runs of experiments to evaluate our algorithms , based on the ad response dataset supplied by our partner for the period of 05 31 October 2012 ( cf . Section 3 ) . For each run , we take 9 days and split the data by time into
( 25 )
( 26 )
Table 2 : Experiment setup
Evaluation Training period Trial 1 05 11 October 07 13 October Trial 2 09 15 October Trial 3 11 17 October Trial 4 13 19 October Trial 5 15 21 October Trial 6 Trial 7 17 23 October 19 25 October Trial 8 21 27 October Trial 9 Trial 10 23 29 October
Test period
12 13 October 14 15 October 16 17 October 18 19 October 20 21 October 22 23 October 24 25 October 26 27 October 28 29 October 30 31 October a training set ( 7 days ) and a test set ( 2 days ) . For our evaluations , we use the average and standard deviation of the prediction results on the 10 test sets . Table 2 summarizes the configuration of our training and test sets .
For all algorithms , we set the number of interaction factors as K = 5 , as we find it gives good overall results . We also fix the regularization parameters as λθ = 0 for θ = w0 , and λθ = 0.001 for θ = wj or θ = Vk,j . As our stopping criterion , we fix a total number of iterations for each algorithm . For the SGD algorithm , we set the total iterations to 100 , while we use 10 for the CD algorithm ( since CD generally requires fewer iterations than SGD to reach convergence ) . 6.2 Evaluation Metrics
As mentioned in Section 1 , a desirable model for response prediction should give good regression and ranking results . Accordingly , we evaluate the prediction performances of our algorithms using several regression and ranking metrics . For regression , we use weighted root mean square error ( wRMSE ) and weighted negative log likelihood ( wNLL ) , as defined in ( 27 ) and ( 28 ) respectively , for a test set with N instances : wRM SE = wN LL = −
N i=1 ei ( yi − ˆyi)2 i=1 ei(yi log σ(ˆyi ) + ( 1 − yi ) log ( 1 − σ(ˆyi ) )
N i=1 ei
N
( 27 )
N i=1 ei
( 28 )
Here yi and ˆyi denote the target and predicted outputs respectively , and ( again ) ei is the number of exposures . The wRM SE and wN LL can be thought as complementary metrics quantifying the importance weighted “ distance ” between predicted and actual probabilities . They also correspond to the loss functions ( 5 ) and ( 6 ) respectively .
For ranking , on the other hand , we use the weighted area under the receiver operating characteristic curve ( wAUC ) [ 6 ] . Essentially , this metric reflects the probability that a predictive model will rank a randomly chosen positive instance higher than a randomly chosen negative one . In this , we treat the problem as a binary classification task , where a click ( no click ) indicates a positive ( negative ) instance . For example , if the number of exposures ei = 10 and the number of clicks ci = 3 ( ie , CT R = yi = 0.3 ) , we treat this as 3 positive instances and 7 negative instances . By “ unrolling ” all CT R entries this way , we can compute AUC weighted by ad exposures so as to measure the ranking quality . 6.3 Overall Results
The overall prediction results , evaluated using the three performance metrics , are summarized in Tables 3 , 4 and 5 ,
129 Method TensorALS TimeSVD++ FM SGD
Square
FM SGD
Logistic
FM CD
Cyclic
FM CD
Stochastic
FM SGD
Logistic
FM CD
Cyclic
FM CD
Stochastic
No No Yes Yes No No Yes Yes No No Yes Yes No No Yes Yes
No Yes No Yes No Yes No Yes No Yes No Yes No Yes Yes Yes
No No Yes Yes No No Yes Yes No No Yes Yes No No Yes Yes
No Yes No Yes No Yes No Yes No Yes No Yes No Yes Yes Yes wRMSE
0.0245347 ± 0.0006238 0.0230917 ± 0.0005482 0.0105449 ± 0.0040673 0.0092290 ± 0.0034284(* ) 0.0043385 ± 0.0003909(* ) 0.0044152 ± 0.0006106(* ) 0.0067266 ± 0.0003933 0.0068068 ± 0.0008265 0.0052925 ± 0.0001996(* ) 0.0048635 ± 0.0001841(* ) 0.0061768 ± 0.0001934 0.0063543 ± 0.0001996 0.0044806 ± 0.0002127(* ) 0.0047210 ± 0.0001874(* ) 0.0062104 ± 0.0001935 0.0063543 ± 0.0001996 0.0044815 ± 0.0002135(* ) 0.0047006 ± 0.0001915(* ) wNLL
0.1141281 ± 0.0031088 0.0369275 ± 0.0009549 0.0190836 ± 0.0033935 0.0190393 ± 0.0040543 0.0139876 ± 0.0021322(* ) 0.0143682 ± 0.0024797(* ) 0.0154207 ± 0.0013011 0.0149345 ± 0.0014383(* ) 0.0139813 ± 0.0014130(* ) 0.0136824 ± 0.0014207(* ) 0.0145682 ± 0.0015581 0.0151101 ± 0.0013793 0.0135587 ± 0.0017265(* ) 0.0141638 ± 0.0021363 0.0146287 ± 0.0015349 0.0151101 ± 0.0013793 0.0135992 ± 0.0017595(* ) 0.0138494 ± 0.0021000 wAUC 0.5 ± 0.0
0.7046705 ± 0.0101725 0.7600722 ± 0.0348881 0.7588117 ± 0.0426068 0.8263158 ± 0.0327539(* ) 0.8240209 ± 0.0343667(* ) 0.7730542 ± 0.0314904 0.7804171 ± 0.0339882(* ) 0.8136070 ± 0.0334504(* ) 0.8152107 ± 0.0340372(* ) 0.7617378 ± 0.0441938 0.7629488 ± 0.0455320 0.8262493 ± 0.0283374(* ) 0.8212097 ± 0.0321086(* ) 0.7617537 ± 0.0441836 0.7629527 ± 0.0455303 0.8255169 ± 0.0290100(* ) 0.8233171 ± 0.0326712(* ) wRMSE
0.0148571 ± 0.0007870 0.0133219 ± 0.0009000 0.0066994 ± 0.0039766 0.0063514 ± 0.0036174 0.0035637 ± 0.0002817(* ) 0.0036013 ± 0.0003965(* ) 0.0052633 ± 0.0006386 0.0048462 ± 0.0009245 0.0048071 ± 0.0002476(* ) 0.0043588 ± 0.0002261(* ) 0.0052848 ± 0.0002765 0.0054034 ± 0.0002760 0.0035659 ± 0.0002768(* ) 0.0037860 ± 0.0002213(* ) 0.0053125 ± 0.0002773 0.0054034 ± 0.0002760 0.0035798 ± 0.0002810(* ) 0.0037806 ± 0.0002244(* ) wNLL
0.0898490 ± 0.0033757 0.0300316 ± 0.0011584 0.0159641 ± 0.0030365 0.0155720 ± 0.0030424 0.0136790 ± 0.0022342(* ) 0.0143439 ± 0.0028064(* ) 0.0144573 ± 0.0013365 0.0139555 ± 0.0014297(* ) 0.0137598 ± 0.0014451(* ) 0.0134459 ± 0.0014542(* ) 0.0139085 ± 0.0017059 0.0141956 ± 0.0015021 0.0131528 ± 0.0016376(* ) 0.0134757 ± 0.0020551 0.0139288 ± 0.0016660 0.0141957 ± 0.0015021 0.0131411 ± 0.0016117(* ) 0.0133138 ± 0.0019167 wAUC 0.5 ± 0.0
0.7104049 ± 0.0104965 0.7906827 ± 0.0399742 0.7947463 ± 0.0436061 0.8263953 ± 0.0345580(* ) 0.8216659 ± 0.0377304(* ) 0.7831293 ± 0.0378750 0.7940811 ± 0.0385131(* ) 0.8085024 ± 0.0353346(* ) 0.8119756 ± 0.0355739(* ) 0.7725149 ± 0.0481081 0.7735657 ± 0.0484978 0.8246543 ± 0.0288612(* ) 0.8253329 ± 0.0321349(* ) 0.7725211 ± 0.0481107 0.7735673 ± 0.0484932 0.8248637 ± 0.0276094(* ) 0.8262658 ± 0.0313433(* )
Table 3 : Consolidated results averaged over 10 trials for emin = 10
Type
Importance Hierarchy
( * ) indicates statistically significant improvements over the baseline based on Wilcoxon signed rank test at significance level of 0.01
Table 4 : Consolidated results averaged over 10 trials for emin = 100
Type
Importance Hierarchy
Method TensorALS TimeSVD++ FM SGD
Square
( * ) indicates statistically significant improvements over the baseline based on Wilcoxon signed rank test at significance level of 0.01
Table 5 : Consolidated results averaged over 10 trials for emin = 1000
Type
Importance Hierarchy
Method TensorALS TimeSVD++ FM SGD
Square
No No Yes Yes No No Yes Yes No No Yes Yes No No Yes Yes
No Yes No Yes No Yes No Yes No Yes No Yes No Yes Yes Yes wRMSE
0.0105002 ± 0.0011214 0.0093541 ± 0.0012304 0.0035179 ± 0.0006461 0.0033768 ± 0.0005637 0.0027669 ± 0.0003007(* ) 0.0027779 ± 0.0002821(* ) 0.0041532 ± 0.0003815 0.0035746 ± 0.0004055(* ) 0.0043950 ± 0.0003841 0.0038005 ± 0.0003306(* ) 0.0043290 ± 0.0004144 0.0044521 ± 0.0003917 0.0025551 ± 0.0003352(* ) 0.0027313 ± 0.0002812(* ) 0.0043391 ± 0.0004133 0.0044521 ± 0.0003917 0.0025507 ± 0.0003444(* ) 0.0027390 ± 0.0002842(* ) wNLL
0.0698346 ± 0.0037189 0.0244543 ± 0.0009942 0.0140867 ± 0.0031550 0.0141022 ± 0.0031252 0.0127455 ± 0.0023743(* ) 0.0136180 ± 0.0027904 0.0133235 ± 0.0014580 0.0128660 ± 0.0014666(* ) 0.0130108 ± 0.0015346(* ) 0.0126841 ± 0.0015381(* ) 0.0131398 ± 0.0022141 0.0129831 ± 0.0016401 0.0124212 ± 0.0017798(* ) 0.0123107 ± 0.0018302(* ) 0.0129884 ± 0.0020158 0.0129832 ± 0.0016401 0.0124773 ± 0.0018751(* ) 0.0123266 ± 0.0018478(* ) wAUC 0.5 ± 0.0
0.7066218 ± 0.0113939 0.8032424 ± 0.0479115 0.8029501 ± 0.0465531 0.8253467 ± 0.0383887(* ) 0.8191953 ± 0.0401579(* ) 0.7802751 ± 0.0425936 0.7920499 ± 0.0432757(* ) 0.7984587 ± 0.0402334(* ) 0.8057917 ± 0.0396685(* ) 0.7795976 ± 0.0543661 0.7811009 ± 0.0542086(* ) 0.8232776 ± 0.0334076(* ) 0.8268533 ± 0.0333732(* ) 0.7797752 ± 0.0543298 0.7811113 ± 0.0542079(* ) 0.8223575 ± 0.0337542(* ) 0.8262701 ± 0.0335686(* )
FM SGD
Logistic
FM CD
Cyclic
FM CD
Stochastic
( * ) indicates statistically significant improvements over the baseline based on Wilcoxon signed rank test at significance level of 0.01 for different minimum exposures emin = 10 , 100 and 1000 , respectively . For our SGD algorithm , we employ two variants that optimize for square and logistic loss functions respectively . Similarly , we utilize the cyclic and stochastic variants of our CD algorithm . For comparison , we also experiment with two popular time aware factorization algorithms : TensorALS [ 5 ] and TimeSVD++ [ 9 ] , which should serve as good reference methods . For fairness , similar configurations were used in these methods , eg , 5 latent factors ( K = 5 ) and 100 training iterations ( as with our SGD ) .
To investigate the effects of importance weighted and hierarchical learning , we conduct experiments by turning on and off the importance weight and hierarchy , resulting in four configurations for each algorithm variant . Subsequently , to
130 see how significant the result improvements are due to importance weight and/or hierarchy , we use the non parametric Wilcoxon signed rank statistical test2 [ 22 ] . All the tests were performed at a significance level of 1 % , with the baseline being the “ no importance weight and no hierarchy ” case .
Several observations can accordingly be made based on the overall prediction results in Tables 3 , 4 and 5 :
• All our FM based algorithms outperform the TensorALS and TimeSVD++ methods , in terms of both regression and ranking results . ( Separate Wilcoxon tests using TensorALS and TimeSVD++ as baselines also show that all our algorithms are significantly better . ) Note also that TensorALS gives “ flat ” predictions on the test set ( where all the time indices never appear in the training set ) , leading to a constant wAU C = 05 • As the minimum exposure threshold emin increases , we see improvements in all performance metrics . This is expected , since a higher emin implies a higher confidence in the CT R estimation and thus cleaner data for training and testing ( recall that CT R = numclick ) . numexpose • We observe large improvements when we incorporate importance weights ( ie , importance aware learning ) , while incorporating hierarchical information does not necessarily improve the overall prediction results . This is reasonable nonetheless , since we can expect that hierarchical learning helps in ( near ) cold start cases , which would likely have small importance weights in our ( weighted ) evaluation on the test data . Further investigation on the performance of our methods in cold start scenarios is presented in Section 64
• In general , the CD algorithm produces similar performance to the SGD procedure , though the former needs fewer iterations to converge and has fewer user parameters ( ie , no learning rate η ) . Comparing the square and logistic SGD variants , we observe that the former generally exhibits better results in terms of wRM SE and wAU C , while the latter is superior in terms of wN LL , which is expected . Meanwhile , the cyclic and stochastic CD variants yield very similar performances , though the stochastic approximation in the latter is expected to give faster training convergence .
6.4 Cold Start Results
We carried out further studies on how hierarchical learning in our HIFM approach can help address the cold start situations . In this study , we concentrate on the extreme cold start scenarios whereby we deal with new pages and ads . We define new pages and ads as those that only appear in the test data , and never in the training set . We conducted the experiments as before , and averaged the results for these cold start cases over 10 trials . We can expect that , when no hierarchical regularization or fitting takes place , our algorithms will produce a “ flat’ prediction ( ie , wAU C = 05 ) In this case , we may obtain reasonably good regression performance , but the ranking produced will be meaningless . Due to this reason ( and space constraint ) , we shall focus on evaluating the ranking performance ( ie , wAU C ) here . 2The Wilcoxon test can be used as an alternative to the ttest for matched pairs , or the t test for dependent samples when they cannot be assumed to be normally distributed .
C U A w e g a r e v A
C U A w e g a r e v A
C U A w e g a r e v A
0.8
0.7
0.6
0.5
0.4
0.8
0.7
0.6
0.5
0.4
0.8
0.7
0.6
0.5
0.4
SGD−Square
SGD−Logistic
CD−Cyclic
CD−Stochastic
Method
Importance = No Hierarchy = No
Importance = No Hierarchy = Yes
Importance = Yes Hierarchy = No
Importance = Yes Hierarchy = Yes
( a ) Minimum exposure emin = 10
SGD−Square
SGD−Logistic
CD−Cyclic
CD−Stochastic
Method
Importance = No Hierarchy = No
Importance = No Hierarchy = Yes
Importance = Yes Hierarchy = No
Importance = Yes Hierarchy = Yes
( b ) Minimum exposure emin = 100
SGD−Square
SGD−Logistic
CD−Cyclic
CD−Stochastic
Importance = No Hierarchy = No
Importance = No Hierarchy = Yes
Importance = Yes Hierarchy = No
Importance = Yes Hierarchy = Yes
Method
( c ) Minimum exposure emin = 1000
Figure 4 : Results for cold start cases
Our experimental results are presented in Figure 4 , with the minimum exposure emin varied from 10 to 1000 . It can be seen that the ranking results always improve as we incorporate hierarchy information in our algorithms . In addition , we observe that the result improvements get amplified as emin is increased , which can ( again ) be attributed to the higher confidence in the CT R estimates ( ie , cleaner data ) . It is also worth noting that , for these cold start cases , the results of importance aware learning and those of unweighted learning are not directly comparable . In fact , the cold start cases tend to have low exposures in the training data , for which the importance aware learning would give lower priority . Nevertheless , the results illustrate that the importanceaware learning and hierarchy information are complementary in handling complex response prediction tasks .
131 7 . CONCLUSION
In this paper , we put forward a latent factor model , termed the Hierarchical Importance Aware Factorization Machine ( HIFM ) , for predicting dynamic ad response . Using the factorization machine as the base generic framework , we develop new importance aware and hierarchical learning mechanisms to improve the model ’s predictive abilities in complex response prediction tasks where cost varying instances and cold start cases are ubiquitous . Several variants of efficient learning methods have been developed to explore different ways of incorporating importance weight and hierarchy information in HIFM . The efficacy of our HIFM model has been exemplified through extensive empirical studies on realworld mobile advertising data from a global ad network .
Moving forward , we consider several avenues for future research . First , there is a need to further scale up the HIFM for large datasets , either by means of parallelization or a more efficient data representation ( eg , [ 14] ) . Second , we can further enhance the HIFM ’s predictive power by incorporating hierarchical Bayesian learning on top of a deep , multi layer representation . Last but not least , we wish to explore the applicability of the HIFM in other task domains , such as temporal item adoption or rating prediction .
Acknowledgment This research is supported by Singapore National Research Foundation under its International Research Centre @ Singapore Funding Initiative and administered by the IDM Programme Office , Media Development Authority ( MDA ) .
8 . REFERENCES [ 1 ] D . Agarwal , R . Agrawal , R . Khanna , and N . Kota .
Estimating rates of rare events with multiple hierarchies through scalable log linear models . In Proceedings of the ACM SIGKDD Knowledge Discovery and Data Mining , pages 213–222 , 2010 .
[ 2 ] D . Agarwal , B C Chen , and P . Elango .
Spatio temporal models for estimating click through rate . In Proceedings of the International World Wide Web Conference , pages 21–30 , New York , NY , 2009 .
[ 3 ] G . Aggarwal , A . Goel , and R . Motwani . Truthful auctions for pricing search keywords . In Proceedings of the ACM Conference on Electronic Commerce , pages 1–7 , 2006 .
[ 4 ] T . B¨ack . Evolutionary algorithms in theory and practice : evolution strategies , evolutionary programming , genetic algorithms . Oxford University Press , Oxford , UK , 1996 .
[ 5 ] P . Comon , X . Luciani , and A . L . F . de Almeida .
Tensor decompositions , alternating least squares and other tales . Journal of Chemometrics , 23:393–405 , 2009 .
[ 6 ] T . Fawcett . ROC graphs : Notes and practical considerations for researchers . Technical report , 2004 .
[ 7 ] Gartner . Gartner says worldwide mobile advertising revenue to reach $11.4 billion in 2013 . Available : http://wwwgartnercom/newsroom/id/23062152013
[ 8 ] T . Graepel , J . Q . Candela , T . Borchert , and
R . Herbrich . Web scale Bayesian click through rate prediction for sponsored search advertising in Microsoft ’s Bing search engine . In Proceedings of the International Conference on Machine Learning , 2010 .
[ 9 ] Y . Koren . Collaborative filtering with temporal dynamics . In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 447–456 , New York , NY , 2009 .
[ 10 ] Z . Lu , D . Agarwal , and I . S . Dhillon . A spatio temporal approach to collaborative filtering . In Proceedings of the ACM conference on Recommender systems , pages 13–20 , 2009 .
[ 11 ] A . K . Menon , K P Chitrapura , S . Garg , D . Agarwal , and N . Kota . Response prediction using collaborative filtering with hierarchies and side information . In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 141–149 , 2011 .
[ 12 ] S . Rendle . Factorization machines . In Proceedings of the IEEE International Conference on Data Mining , Sydney , Australia , 2010 .
[ 13 ] S . Rendle . Factorization machines with libFM . ACM Transactions on Intelligent Systems and Technology , 3:57:1–57:22 , 2012 .
[ 14 ] S . Rendle . Scaling factorization machines to relational data . In Proceedings of the International Conference on Very Large Data Bases , pages 337–348 , 2013 . [ 15 ] M . Richardson . Predicting clicks : Estimating the click through rate for new ads . In Proceedings of the International World Wide Web Conference , pages 521–530 , 2007 .
[ 16 ] D . Sculley . Combined regression and ranking . In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 979–988 , New York , NY , USA , 2010 .
[ 17 ] S . Shalev Shwartz and A . Tewari . Stochastic methods for l1 regularized loss minimization . In Proceedings of the International Conference on Machine Learning , pages 929–936 , 2009 .
[ 18 ] H . Shan , J . Kattge , P . B . Reich , A . Banerjee ,
F . Schrodt , and M . Reichstein . Gap filling in the plant kingdom–trait prediction using hierarchical probabilistic matrix factorization . In Proceedings of the International Conference on Machine Learning , Edinburgh , 2012 .
[ 19 ] S . Shen , B . Hu , W . Chen , and Q . Yang . Personalized click model through collaborative filtering . In Proceedings of the ACM International Conference on Web Search and Data Mining , pages 323–332 , 2012 .
[ 20 ] N . Srebro , J . D . M . Rennie , and T . S . Jaakola .
Maximum margin matrix factorization . In Proceedings of the Advances in Neural Information Processing Systems , volume 17 , pages 1329–1336 , 2005 .
[ 21 ] J . Z . Sun , K . R . Varschney , and K . Subbian . Dynamic matrix factorization : A state space approach . In Proceedings of the IEEE International Conference on Speech and Signal Processing , pages 1897–1900 , Kyoto , Japan , 2012 .
[ 22 ] F . Wilcoxon . Individual comparisons by ranking methods . Biometrics Bulletin , 1(6):80–83 , 1945 .
[ 23 ] E . Zhong , W . Fan , and Q . Yang . Contextual collaborative filtering via hierarchical matrix factorization . In Proceedings of the SIAM International Conference on Data Mining , pages 744–755 , 2012 .
132
