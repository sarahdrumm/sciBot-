Active Learning for Networked Data Based on
Non progressive Diffusion Model
Zhilin Yang
Department of Computer Science and Technology
Tsinghua University kimiyoung@yeah.net
Jie Tang and Bin Xu Department of Computer Science and Technology
Tsinghua University
{jietang , xubin}@tsinghuaeducn
Chunxiao Xing
Department of Computer Science and Technology
Tsinghua University xingcx@tsinghuaeducn
ABSTRACT We study the problem of active learning for networked data , where samples are connected with links and their labels are correlated with each other . We particularly focus on the setting of using the probabilistic graphical model to model the networked data , due to its effectiveness in capturing the dependency between labels of linked samples .
We propose a novel idea of connecting the graphical model to the information diffusion process , and precisely define the active learning problem based on the non progressive diffusion model . We show the NP hardness of the problem and propose a method called MaxCo to solve it . We derive the lower bound for the optimal solution for the active learning setting , and develop an iterative greedy algorithm with provable approximation guarantees . We also theoretically prove the convergence and correctness of MaxCo .
We evaluate MaxCo on four different genres of datasets : Coauthor , Slashdot , Mobile , and Enron . Our experiments show a consistent improvement over other competing approaches . Categories and Subject Descriptors H28 [ Database Management ] : Database Applications— Data Mining ; I26 [ Artificial Intelligence ] : Learning General Terms Algorithms , Experimentation Keywords Active learning , Non progressive model , Factor graph model
1 .
INTRODUCTION
One challenge for a machine learning task is how to collect sufficient labeled samples for training an accurate classification model . Active learning is a method to alleviate this problem by actively querying experts to obtain the desired labels of a few samples . For example , Hoi et al . [ 12 ] studied active learning on text categorization problem . Cohn et al . [ 5 ] proposed two methods : mixtures of Gaussians and locally weighted regression for efficiently selecting samples in neural networks . Settles et al . [ 22 ] provided a survey for various sample selection strategies . The underlying idea for most of these methods is to measure the informativeness of each unlabeled sample and finally select an “ informative ” sample to query each time . The problem becomes more difficult with the increase of the complexity of the input data . First , the samples in the input data may be connected and correlated with each other ( ie , networked data ) , which implies that selecting the most informative ( but isolated sample ) may be not that helpful for classifying the other samples . Second , in practice , to avoid frequently querying the experts , it is usually desirable to select a set of samples and query the users in a batch mode .
In this paper , we try to systematically address the above questions . The problem can be formally defined as follows :
Definition 1 . Batch Mode Active Learning for Networked Data . Given a network G = ( VU , VL , yL , E , X ) , where VU denotes a set of unlabeled samples , VL denotes a set of labeled samples , yL corresponds to labels of the labeled samples , E is the set of edges between samples in the network G , and X is an ( |VU| + |VL| ) × d attribute matrix in which each row xi represents the vector of attributes for sample vi , our goal is to query a subset of k unlabeled samples so as to maximize the following utility function : max VS⊆VU
Q(VS ) , with
|VS| ≤ k
( 1 )
In the formulation , the utility function Q(VS ) is a general definition on the subset VS , and can be instantiated in different ways . Such a definition of active learning for networked data has been extensively used in the literature [ 2 , 23 , 31 ] . To model the correlation between labels of linked samples , we consider the probabilistic graphical model . In the setting of graphical model , we connect the active learning problem to the theory of non progressive diffusion [ 8 ] , and develop an instantiation model for the above problem . The active learning problem based on non progressive diffusion is proved to be NP hard . We present an efficient method named MaxCo to solve the problem with provable approximation guarantee . Theoretically , we prove the convergence and correctness of the proposed method and also provide its approximation ratio .
Empirically , we verify the proposed method on four different genres of datasets : Coauthor , Slashdot , Mobile , and
Figure 1 : Performance Comparison : Five algorithms on four datasets with accuracy and F1 measure . X axis denotes the number of labeled samples while y axis represents the measure score .
Enron . We compare MaxCo with several competing methods ( Cf . §5 for definitions of the competing methods ) . The experimental results indicate that the proposed method can significantly improve the active learning performance over the other methods . Figure 1 shows the performance comparison of the comparison methods on the four datasets . On average , our method MaxCo outperforms the other methods by 5% 10 % in terms of F1 ( t test , p value < 001 ) the following aspects :
To summarize , the major contribution of this work lies in • Precisely define the problem of active learning for networked data via the non progressive diffusion model , which provides an elegant way to model the diffusion probabilities from the available labeled samples to the unlabeled samples .
• Prove the NP hardness of the problem and develop an efficient algorithm with provable approximation guarantee to solve the problem .
• Theoretically analyze the proposed algorithm , prove its convergence and correctness , and provide an upper bound and a lower bound of the proposed algorithm . • Empirically evaluate the proposed algorithm on several real world datasets to demonstrate the effectiveness of our algorithm .
Organization . Section 2 presents the factor graph framework and instantiates the problem in the settings of nonprogressive diffusion model ; Section 3 proposes the algorithm ; Section 4 provides the theoretical analysis ; Section 5 evaluates the algorithm ; Section 6 discusses related work , and finally Section 7 concludes the work .
2 . MODEL
The first question we want to address is how to leverage the link information to improve the effectiveness of active learning . In a network setting , for example a social network , users are connected with each other and their behaviors are strongly correlated . To deal with this , we consider the probabilistic graphical model as our basic framework . We utilize Loopy Belief Propagation ( LBP ) [ 17 ] to learn parameters in the probabilistic graphical model .
For active learning , a frequently used method is to select the most informative samples [ 23 , 31 ] . However , they do not consider the fact that in the graphical model learning process , an instance classified to have label yi after iteration τ may be classified to have label yj after the next iteration τ + 1 . To this end , we propose a novel idea of connecting message passing in LBP to the theory of non progressive diffusion and instantiate the active learning problem based on non progressive diffusion model . 2.1 Factor Graph Model
Factor graph is one type of probabilistic graphical models . It leverages factorization of probabilistic distribution to capture dependency and correlation among random variables .
In particular , we consider a partially labeled setting for the factor graph model , denoted as G = ( VL , VU , yL , E , X ) , which is consistent with our definition in active learning problem for networked data . We associate each sample vi ∈ VU ∪ VL with a random variable yi taking value in a discrete space Y . Basically , there are two categories of nodes in a factor graph , variable nodes and factor nodes . If a factor function is defined over a clique , a factor node will be added into the graph and all variable nodes in the clique will be connected to the factor node respectively . Given this , let V = VU ∪ VL , we can define the joint probability over all the labeled and unlabeled samples : P ( y|yL ; Θ )
θif ( yi , xi ) +
θi,jg(yi , yj )
 vi∈V
=
1 Z exp
( vi,vj )∈E
 ( 2 )
020406080100#Label03040506070809AccuracyCoauthorMaxCoIMSBMALMURAN020406080100#Label060062064066068070072074076AccuracySlashdotMaxCoIMSBMALMURAN020406080100#Label050055060065070075080085AccuracyMobileMaxCoIMSBMALMURAN0102030405060#Label055060065070075080085090095AccuracyEnronMaxCoIMSBMALMURAN020406080100#Label050055060065070075080F1CoauthorMaxCoIMSBMALMURAN020406080100#Label070072074076078080082084F1SlashdotMaxCoIMSBMALMURAN020406080100#Label02030405060708F1MobileMaxCoIMSBMALMURAN0102030405060#Label070075080085090095F1EnronMaxCoIMSBMALMURAN where f ( yi , xi ) represents the factor function defined on variable node yi with attribute vector xi and g(yi , yj ) denotes the factor function defined on a factor node that connects yi and yj ; Θ = ( {θi},{θij} ) are parameters to be estimated and Z is a normalization factor .
Model Learning . Given a training dataset , we aim to estimate the parameter Θ in the factor graph model . One challenge here is how to leverage unlabeled data for parameter estimation . When training the parameter , we maximize the likelihood of labeled samples by summing up all possible distributions over the unlabeled samples . For the sake of simplicity , we first rewrite the conditional probability ( Eq 2 ) as follows :
P ( y|yL ; Θ ) =
1 Z exp{ΘT S(y)}
( 3 ) where S(y ) denotes all the factor functions defined on the graph G related to variable y . For maximum likelihood estimation , the log likelihood objective function can be written as :
O = P ( yL|G ) y|yL
= log exp{ΘT S(y)} − log Z
( 4 ) where y|yL indicates a label configuration inferred from the available labels yL . Then we can write the gradient of the objective function wrt to the parameter Θ :
∂O ∂Θ
= log
∂ ∂Θ exp{ΘT S(y)} − ∂ ∂Θ y|yL y|yL exp{ΘT S(y)} · S(y ) y|yL
− = Ep(y|yL;Θ)S(y ) − Ep(y;Θ)S(y ) exp{ΘT S(y)}
= log exp{ΘT S(y)} y exp{ΘT S(y)} · S(y ) y exp{ΘT S(y)} y
( 5 )
We can apply gradient descent method to solve the objective function . However , Ep(y|yL;Θ ) and Ep(y;Θ ) are intractable when the graph contains cycles [ 29 ] . A stateof the art approximate solution is Loopy Belief Propagation(LBP)[17 ] .
Loopy Belief Propagation . LBP utilizes message passing to calculate marginal probability . More specifically , for each iteration , message passing is performed according to the following update rules .
µτ yi→f ( xi ) =
µτ f→yi ( xi ) =
∼yi f∗∈N B(yi)\f f ( xf )
( xi )
µτ−1 f∗→yi yj∈N B(f )\{yi}
µτ−1 yj→f ( xj )
( 6 )
For iteration τ , µτ y→f represents the message passed from a variable node y to a factor node f , while µτ f→y denotes the message passed in the reverse direction . N B(y ) is the set of neighbor factor nodes of a variable node y . f ( xf ) denotes the factor function associated with certain factor node f . means to sum up the factor function over all variables
∼yi
Figure 2 : Modeling LBP with information diffusion : Streams of label information are diffused from labeled samples while flows of statistical bias are intrinsically distributed in the network . except yi . After LBP converges to a fixed point , the belief probability of each variable node yi can be obtained as the product of all messages passing toward yi . 2.2 Active Learning on Non progressive Dif fusion Model
We propose a novel idea of connecting the LBP process to the information diffusion model and develop an instantiation model for the active learning for networked data .
In the information diffusion theory , there are two state ofthe art diffusion models : linear threshold model and independent cascaded model [ 13 ] . We consider the linear threshold model , where a sample will be activated as long as the number of its active neighbors exceeds its threshold . According to the diffusion process , the linear threshold model can be further classified into two categories : progressive model and non progressive model .
In the progressive model , a node in the network can only be activated once and remains its status in the following diffusion process . More formally , let fτ ( v ) denote the status of node v at time τ . If f ( v ) = 1 then v is activated , otherwise not . Let N B(v ) denote the neighbor set of v , the formal definition of f is as follows . u∈N B(v ) fτ−1(u ) ≥ t(v ) or fτ−1(v ) = 1 u∈N B(v ) fτ−1(u ) < t(v ) and fτ−1(v ) = 0 fl 1 if 0 if fτ ( v ) =
In the non progressive model , a node in the network can reverse its status in both directions , ie , either from active to inactive , or vice versa , depending on the status of its neighbors at the last time step . More formally , fl 1 if 0 if fτ ( v ) = u∈N B(v ) fτ−1(u ) ≥ t(v ) u∈N B(v ) fτ−1(u ) < t(v )
Which model is more suitable for modeling the message passing process in an LBP ? As illustrated in figure 2 , we can imagine that there are two types of information diffusing across the social network . One is statistical bias , which may be caused by insufficiency of labeled data , imbalance of labeled data or redundancy of selected features . This kind of information is distributed randomly and intrinsically in the given network and may cause overfitting of the factor graph model . The other type is labeled information , which indicates the ground truth , counterbalances the statistical bias and thus enhances the capability of generalization of factor graph model . Suppose statistical bias and labeling information are spreading across the network simultaneously , our goal is to maximize the influence of labeling information and minimize the side effects of statistical bias . In this perspective , as for an uncertain variable node , whose “ belief ” is around 0.5 , it may be alternatively influenced by statistical bias and labeling information , and thus sway its “ belief ” probability between 0.49 and 051 For this reason , during LBP message passing , except the initially labeled nodes , a node predicted to have label yi after iteration τ will be probably predicted to have label yj after the next iteration τ + 1 . Therefore , apparently , it is more acceptable to select non progressive model because a variable node in the factor graph , as we demonstrated above , can reverse its status in both directions .
Now we utilize non progressive linear threshold model to instantiate the utility function in Eq 1 .
Problem 1 . Active Learning on Non progressive Diffusion Model . Given a factor graph model trained on the partially labeled network G = ( VL , VU , yL , E , X ) , the threshold values and uncertainty values for each sample , denoted as {t(v)|v ∈ VL ∪ VU} and {µ(v)|v ∈ VL ∪ VU} , and a budget k , we aim to query a subset of k unlabeled samples , such that the performance of factor graph model will be best improved . Suppose the uncertainty of variable v grows as µ(v ) increases , the utility function can be defined as : max VS⊆VU with the constraints :
{ max VT ⊆VU
|VT|} ,
|VS| ≤ k f0(v ) = 1 ⇐⇒ v ∈ VS ∃τM st ∀v ∈ VT ∀τ > τM fτ ( v ) = 1 ∀v ∈ VU\VT ,∀u ∈ VT , µ(v ) ≤ µ(u ) fτ ( v ) = 1 ⇐⇒ fτ−1(u ) ≥ t(v ) u∈N B(v )
( 7 ) ( 8 )
( 9 )
( 10 )
Here we interpret the constraints we made above . Constraint ( 7 ) indicates that we initially label all samples in VS . By constraint ( 8 ) we require all samples in VT to be eventually activated , because we assume that only an activated sample can be effectively influenced by label information . By constraint ( 9 ) , we also require VT is made up of top k uncertain nodes in VU , instead of simply counting the number of eventually infected nodes . In this way of definition we are only interested in those samples that lie on the edge of right and wrong , which is the key point to the active learning problem , rather than treating every unlabeled sample equally . Constraint ( 10 ) indicates that we consider the active learning problem under non progressive diffusion model . Because there may be more than one VT satisfying the constraints , we select the maximum one in the utility function .
3 . ALGORITHMS
In this section , we solve problem 1 by two steps . First we reduce the problem to Minimum Source Set problem . Second we give an approximate solution to Minimum Source Set problem . 3.1 Reduction
In problem 1 , we fix the number of source set VS and try to maximize the size of target set VT . However , in this section , we define a Minimum Source Set problem where we fix the number of target set VT and aim to minimize the number of queried samples .
Problem 2 . Minimum Source Set
Given a factor graph model trained on the networked data G = ( VL , VU , yL , E , X ) , the threshold values for each sample , denoted as {t(v)|v ∈ VL ∪ VU} , and the target set VT ⊆ VU , we aim to find a minimal source set VS ⊆ VU to eventually activate all samples in VT , ie , min|VS| with the constraints that if f0(v ) = 1 for all v ∈ VS , then ∃τM st ft(v ) = 1 for all v ∈ VT and t > τM . The samples in the graph are updated with the non progressive rule : fτ ( v ) = 1 ⇐⇒ fτ−1(u ) ≥ t(v ) u∈N B(v )
We are now to introduce the equivalence of problem 1 and problem 2 .
Theorem 1 . The problem of Active Learning on Nonprogressive Diffusion Model(problem 1 ) and Minimum Source Set problem(problem 2 ) are equivalent .
S such that |V ∗
Proof . First , we introduce a reduction from problem 1 to problem 2 . For problem 1 , given |VS| = k , we aim to find an instance of VS to maximize |VT| . We build a family of instances of problem 2 to solve problem 1 . We enumerate |VT| from its maximum possible value |VU| and count down until |VT| can be achieved with |VS| ≤ k . We build an instance of problem 2 to find out the optimal solution VS,opt given |VT| . If |VS,opt| ≤ k , then we can quit the algorithm and VS,opt is the optimal solution for the problem 1 , with which the maximum size of |VT| is the one being enumerated , denoted as |VT|m . Now we prove the reduction above by contradiction . SupS | ≤ k pose that there exists another solution V ∗ T | > |VT|m , where V ∗ and |V ∗ T is the target set corresponding to V ∗ S . Therefore , when we build an instance of probT | , the optimal solution VS,opt will satisfy lem 2 with |V ∗ S | ≤ k . Thus , the algorithm will quit and |VS,opt| ≤ |V ∗ |VT|m will not be enumerated , which leads to contradiction . Therefore , the reduction above gives optimal solution to problem 1 . Next , we prove that problem 2 can be reduced to problem 1 . For problem 2 , given |VT| = l , we aim to find a smallest source set VS to eventually activate all samples in VT . Similar to the reduction in the reverse direction , we can enumerate the value of |VS| in this case . Because we want to minimize |VS| , we enumerate |VS| from 0 and count upwards until VT can be activated by VS . Again we leverage an instance of problem 1 to find the maximum size of target set |VT|m with source set of size |VS| . If |VT|m ≥ l , we terminate the algorithm and the optimal solution is the one returned by problem 1 , denoted as VS,opt . Now we prove the reduction above by contradiction . SupS | < pose that there exists another solution V ∗ |VS,opt| and the size of eventually activated sample set |V ∗ S | , we can leverage an inT | ≥ l . When we enumerate |V ∗ stance of problem 1 to obtain a target set VT with |VT| ≥ l . Therefore , |VS,opt| will not be enumerated , which contra
S such that |V ∗ dicts the assumption .
It follows two problems are equivalent .
Algorithm 1 : MaxCo Algorithm part 1 : Reduction Input : G = ( VU , VL , yL , E , X ) , k , threshold function t Output : VS : the selected set of nodes to be labeled .
1 l ← min value for |VT| 2 r ← max value for |VT| 3 VS,opt ← ∅
2
7
8
6
4 while l < r do |VT| ← l+r 5 VS ← MinimumSourceSet(G,|VT| ) if |VS| ≤ k then l ← |VT| + 1 VS,opt ← VS r ← |VT| else
9
10
11
12 return VS,opt
Lemma 1 . For problem 2 , if either µ(v1 ) < µ(v2 ) or µ(v2 ) < µ(v1 ) ∀v1 , v2 ∈ VU ∪ VL and v1 = v2 , |VS,opt| is monotonically non decreasing with respect to |VT| .
Proof . To prove the lemma , we only need to prove that for two target sets VT 1 and VT 2 and their optimal solutions VS1 and VS2 , if |VT 1| > |VT 2| , then we have |VS1| ≥ |VS2| . According to the definition of minimum source set problem , the target set should be top l uncertain nodes . Because samples can be strictly ordered with respect to uncertainty , we can infer VT 2 ⊂ VT 1 from the assumption that |VT 1| > |VT 2| . Suppose |VS1| < |VS2| . Because VS1 can activate all samples in VT 1 and VT 2 ⊂ VT 1 , VS1 can activate all samples in VT 2 as well . It contradicts the assumption that VS2 is the optimal solution for VT 2 . Therefore , |VS1| ≥ |VS2| .
In practice , we can force any two uncertainty values to be strictly ordered even if they are arithmetically equal . For example , we can assume that vi is uncertain than vj if µ(vi ) = µ(vj ) but i > j . Therefore , because |VS,opt| is monotonically non decreasing with respect to |VT| , we can apply bisection method to optimize the reduction algorithm introduced in the proof of theorem 1 . When reducing from problem 1 to problem 2 , instead of enumerating the value of |VT| one by one , we can leverage bisection method to bisect the interval and select a subinterval recursively , which will largely speed up the algorithm from O(n ) to O(log n ) , where n indicates the size of domain of |VT| . The complete procedure for reduction from problem 1 to problem 2 is illustrated in algorithm 1 . 3.2 Threshold and Uncertainty
Because the problem of Active Learning on Nonprogressive Diffusion Model can be reduced to Minimum Source Set problem in polynomial time , we now focus on solving the Minimum Source Set problem .
Before we solve Minimum Source Set problem , we need to give a definition of threshold function t(v ) .
Threshold Definition . The threshold value of a sample reflects how sensitive it is to the status of its neighbors . In the factor graph model , if the “ belief ” probability of a sample v is quite close to 1|Y| , ie , variable node v is very uncertain , then it is easily activated by the messages passed from its neighbors . For this reason , samples with higher uncertainty should obtain a lower threshold . Moreover , because a sample with high degree will receive a great number of messages , which may contain both labeling information and statistical bias , its threshold should be relatively high . Considering both degree and uncertainty factors , we define the threshold function as follows . t(v ) = min{(cid:100)η(µmax − µ(v))d(v ) , d(v)}
( 11 ) where d(v ) is the degree , µmax is the maximum value of uncertainty measure and η is a global constant factor to adjust the distribution of thresholds . We apply ceiling operation to the expression to avoid some extreme cases where t(v ) = 0 . Also , of course , the value t(v ) should not be greater than d(v ) , otherwise v will never be activated whatsoever .
Given Eq 11 , it is a nontrivial job to tune the parameter η because it varies in a wide range in different data sets . To have a unified representation , we further define η to be
η =
γ
Avg(µmax − µ )
( 12 ) where Avg means the average value over the given data set , and γ is a constant factor . Under the definition , we can instead tune γ between 0 and 1 , which is relatively irrelevant to the specific data set .
Uncertainty Definition . Now what remains to be defined is an uncertainty function . There are several measures corresponding to the uncertainty of a variable node in a factor graph . One is to define it as the sum of difference between expected belief and calculated belief of each class [ 21 ] .
µ(v ) = − y∈Y fifififiBv(y ) − 1
|Y| fifififi + 2
1 − 1 |Y|
( 13 ) where Bv(y ) is obtained by LBP , indicating the “ belief ” probability that v is classified to have label y . We add a constant term to ensure that µ(v ) is always nonnegative .
Another extensively used measure of uncertainty is en tropy(aka TTE in [ 22] ) . More formally , y∈Y
µ(v ) =
Bv(y ) log
1
Bv(y )
( 14 )
We reserve two ways of definition here and judge them through experiments in § 5 . 3.3 MinSS
After defining the threshold function and uncertainty function , in this section , we aim to solve Minimum Source Set problem . First we claim that Minimum Source Set is an NP hard problem , which will be proved in § 4 . We design an approximate algorithm MinSS to find the source set VS iteratively and greedily .
MinSS . The basic idea of MinSS is to find an extended target set Vτ . An extended target set should satisfy two constraints . First , Vτ is a superset of VT . Second , Vτ can be eventually activated if we initially label a subset of Vτ .
Algorithm 2 : MaxCo Algorithm part 2 : MinSS Input : G = ( VU , VL , yL , E , X ) , k , threshold function t Output : VS : the selected set of nodes to be labeled .
1 calculate Bv(y ) and µ(v ) by LBP 2 Vτ ← VT ← {top k uncertain nodes in VU} 3 VS ← ∅ 4 while VS = ∅ do VP ← VU\Vτ sort nodes in VP in descending order of t(v ) as v1 , v2 , , vp foreach v ∈ Vτ do w(v ) ← 0
7
8
5
6 for i ← 1 to p do if w(u ) < d(u ) − t(u ) ∀u ∈ N B(vi ) ∩ Vτ then foreach u ∈ N B(vi ) ∩ Vτ do
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24 w(u ) ← w(u ) + 1
VP ← VP\{vi} if VP = ∅ then sort nodes in Vτ in ascending order of d(v ) as v1 , v2 , , vm foreach v ∈ Vτ do w(v ) ← 0 foreach i ← 1 to m do if ∃u ∈ N B(vi ) ∩ Vτ st . w(u ) = d(u ) − t(u ) then
VS ← VS ∪ {vi} else foreach u ∈ N B(vi ) ∩ Vτ do w(u ) ← w(u ) + 1
Vτ ← Vτ ∪ VP
25 return VS
MinSS is an iterative algorithm . In each iteration , we greedily select samples to expand the size of Vτ until Vτ satisfies the constraints made above .
The procedure of MinSS is illustrated in Algorithm 2 . We first select top k uncertain nodes in VU to form the target set Vτ . Here we may define the uncertainty with either Eq 13 or Eq 14 . Then we expand the target set Vτ iteratively . For each iteration , we sort all unselected samples v ∈ VP in descending order of threshold value because a smaller threshold value indicates less active neighbor samples required . From line 9 to line 13 , we greedily remove samples from VP with large threshold value while satisfying the constraint that each node v ∈ Vτ have at least t(v ) neighbors in Vτ ∩VP . In line 14 , if VP = ∅ , then Vτ is an extended target set . This time we greedily remove samples from Vτ with small degree and all samples left form the source set VS , which is done from line 15 to line 23 . In algorithm 2 , w(v ) is used to count the unselected neighbors of sample v . So far , we solve the problem of Active Learning on Nonprogressive Diffusion Model by two steps . First , the problem is reduced to Minimum Source Set problem . Second , we solve Minimum Source Set problem by algorithm MinSS . We combine these two steps and refer to the algorithm as MaxCo .
In this section , we theoretically analyze the problems de
4 . THEORETICAL ANALYSIS fined in § 2 and the MinSS algorithm proposed in § 3 . 4.1 NP hardness
Now we introduce the tools for proofs in this section .
Lemma 2 . Suppose VS,opt problem 2 , if α|VU| ≤ |VS,opt| for every bipartite graph H , then α|VU| ≤ |VS,opt| for every graph G . is an optimal solution for
Lemma 3 . Minimum Source Set problem is NP hard when VT = VU .
The proofs of lemma 2 and lemma 3 could be found in [ 8 ] . It is trivial to show that Minimum Source Set problem is NP hard .
Lemma 4 . Minimum Source Set problem is NP hard .
Proof . The theorem follows directly from lemma 3 because the problem in lemma 3 is a special case of Minimum Source Set problem .
Corollary 1 . The problem of Active Learning on Non progressive Diffusion Model is NP hard .
Proof . By lemma 4 and theorem 1 , the conclusion fol lows . 4.2 Convergence and Correctness
Now we analyze the convergence issue for MinSS algo rithm .
Lemma 5 . Convergence . converge within O(|VU| − |VT| ) time .
The MinSS algorithm will
Proof . Denote Vτ after each round of iteration as a sequence Vτ 0 , Vτ 1 , , Vτ γ . For each iteration i , if VP = ∅ , the loop of iteration will be terminated since VS will be a nonempty set . Therefore , if i < γ , then VP = ∅ . It follows that |Vτ i+1| > |Vτ i| for i ∈ {0 , 1 , , τ − 1} . That is , |Vτ i| is strictly monotonically increasing . It is trivial to show that |Vτ i| ≤ |VU| and |Vτ 0| = |VT| . Therefore , the length of sequence {Vτ i}γ i=0 is finite . More precisely , |γ| ≤ |VU| − |VT| , which yields the conclusion .
Then we formally prove that MinSS algorithm will return a feasible solution once it converges .
Theorem 2 . Correctness .
If MinSS algorithm converges , VS is a feasible solution . That is , if we initially label all samples in VS , there exists a τH such that fτ ( v ) = 1 for all v ∈ VT and τ ≥ τH .
Proof . The MinSS algorithm converges if and only if VP = ∅ . In addition , after each iteration , v ∈ VP iff ∃u ∈ NB(v ) ∩ Vτ st . w(u ) ≥ d(u ) − t(u ) . At the end of the algorithm , because VP is empty , we have w(v ) < d(v ) − t(v ) for all v ∈ Vτ . That is ,
|NB(v ) ∩ Vτ| ≥ t(v ) for all v ∈ Vτ
Now we prove fξ(v ) = 1 for all v ∈ Vτ and ξ ≥ 1 by induction . For ξ = 1 , since w(v ) < d(v ) − t(v ) for all v ∈ Vτ , u∈NB(v ) f0(u ) ≥ t(v ) . Therefore , f1(v ) = 1 for all v ∈ Vτ . For ξ > 1 , fξ−1(u ) ≥ fξ−1(u ) u∈NB(v ) u∈NB(v)∩Vτ
By induction hypothesis , fξ−1(v ) = 1 for all v ∈ Vτ .
Therefore , for all v ∈ Vτ , fξ−1(u ) ≥ |NB(v ) ∩ Vτ| ≥ t(v ) u∈NB(v )
Then we have fξ(v ) = 1 for all v ∈ Vτ . Because VT ⊆ Vτ , the conclusion follows . 4.3 Approximation Ratio
Now we show a lower bound for the optimal solution to problem 2 when VT = VU .
T ( V ) =
Theorem 3 . Lower Bound . v∈V d(v ) , v∈V t(v ) , and suppose t(v ) ≤ βd(v ) for all v ∈ V . If 2T ( VU ) − D(VU ) > 0 and VT = VU , we have an lower bound for optimal solution
Let D(V ) = fififi to problem 2 . fififiVS,opt fififi ≥ 2T ( VU ) − D(VU )
β∆ fififiVS,opt
( 15 )
The proof of Theorem 3 is given in the appendix . Because MinSS is an approximate algorithm , we give an upper bound for it as follows . We denote ∆ as the maximum degree among nodes in the graph G .
Theorem 4 . Upper Bound .
Suppose t(v ) ≤ βd(v ) for all v ∈ V , we can derive an upper bound for MinSS algorithm .
|VS| ≤
β∆
1 − β + β∆
|VU|
The detailed proof of Theorem 4 is given in the appendix . With an upper bound and a lower bound , we can prove an approximation ratio when VT = VU .
Corollary 2 . Approximation Ratio .
Let VS,g denote the solution given by MinSS algorithm , VS,opt represent the optimal solution and ∆ be the maximum degree in the graph . Suppose t(v ) ≤ βd(v ) for all v ∈ V , if VT = VU and 2T ( VU ) > D(VU ) , we have
( β∆)2
( 1 − β + β∆ ) · Avg[2t(v ) − d(v ) ]
( 16 )
|VS,g| fififiVS,opt fififi ≤ where Avg[ . ] represents the expectation over all samples in the network .
Proof . By Theorem 3 , we have fififiVS,opt fififi ≥ 2T ( VU ) − D(VU )
β∆
Avg[2t(v ) − d(v ) ]
β∆
|VU|
=
By Theorem 4 , we have |VS,g| ≤
β∆
1 − β + β∆
|VU|
Dividing ( 18 ) by ( 17 ) yields the conclusion .
( 17 )
( 18 )
The approximation ratio given in equation ( 16 ) depends on two variables β and Avg[2t(v ) − d(v) ] . Both of them will be affected if we adjust the value of η given in equation ( 11 ) . To optimize the approximation ratio , we need to tune β as small as possible and set Avg[2t(v)− d(v ) ] as large as possible . However , it is contradictory . Suppose we tune up the value of η , then according to definition of threshold value , in general , β and Avg[2(v ) − d(v ) ] will both be tuned up . And for another thing if we tune down η , both β and Avg[2t(v ) − d(v ) ] will be tuned down as well . Thus , for practical application , we need to carefully choose the value of η such that we can balance the effects of β and Avg[2t(v)− d(v) ] . This issue will be further discussed in § 5 by experiments .
5 . EXPERIMENTAL RESULTS
The proposed active learning framework is general and can be applied to arbitrary networked data . In this section , we evaluate the proposed algorithm ( MaxCo ) and compare with existing methods . All codes and datasets used in this paper can be found at http://arnetminerorg/maxco/ 5.1 Datasets and Comparison Methods
We consider four social network datasets in our evaluation : Coauthor , Slashdot , Mobile and Enron . In all these datasets , we aim to infer the type of social relationships in the different networks . Regarding the partially labeled factor graph model , we use the implementation from [ 28 , 31].1 In the factor graph model , we view each relationship as a variable node and define factor functions according to the specific properties of each network .
• Coauthor . The dataset is a subgraph extracted from ArnetMiner [ 27 ] . In this dataset , we aim to infer advisor advisee relationship from the given network . The factor graph built upon this dataset consists of 6096 variable nodes and 24468 factor nodes .
• Slashdot . The dataset is crawled from Slashdot website . We try to infer friendship on this network . The factor graph built upon this network contains 370 variable nodes and 1686 factor nodes .
• Mobile . This dataset contains logs of call , blue tooth scanning and location collected by mobile applications on 107 phones in a span of 10 months . We aim to infer friendship on this dataset . The factor graph built upon the data consists of 314 variable nodes and 513 factor nodes .
• Enron . The dataset consists of 136,329 emails among 151 Enron employees . We aim to infer manager subordinate relationship from the network . The factor graph model built has 100 variable nodes and 236 factor nodes .
We the compare our algorithm ( MaxCo ) with the following methods for the problem of batch mode active learning for networked data .
• Random(RAN ) . In this method , each time we ran domly select given number of samples to label .
1The source code is available at http://kegcstsinghuaeducn/ jietang/software/OpenCRF PartiallyLabeledFGM.rar
Table 1 : Factor Graph Size
Data Coauthor Slashdot Mobile Enron
#Variable Node #Factor Node 6,096 370 314 100
24,468 1,686 513 236
Table 2 : Average Accuracy( % )
Data Coauthor Enron Slashdot Mobile
IMS MaxCo MU 74.24 71.17 66.95 67.83
82.70 85.33 69.62 76.15
46.99 85.67 66.11 59.68
RAN BMAL 79.72 83.67 67.00 63.73
44.92 84.67 66.16 55.86
• Maximum Uncertainty ( MU ) . This method greedily se lects samples with great entropy(Eq 14 ) .
• Batch Mode Active Learning ( BMAL ) . This method is proposed by [ 23 ] , which aims to maximize the following quality function of selected sample set VS .
Q(VS ) = αC(VS ) + ( 1 − α)H(VS ) , 0 ≤ α ≤ 1 where the definition of H(VS ) and C(VS ) is as follows .
H(VS ) = i∈VU
C(VS ) = i∈VS
H(i )
1−β
( H(i))β max j∈VL∪VS wij
Here H(i ) is the entropy of a variable node i and β is a constant parameter ; wij denotes the similarity between variable i and j , and can be calculated by − xi−xj2 e each variable .
, where x represents an attribute vector for
σ2
• Influence Maximization Selection ( IMS ) . IMS is proposed by [ 31 ] , which also utilizes information diffusion model to solve the active learning problem . However , it is based on progressive diffusion model .
• Maximum Coverage ( MaxCo ) . We use entropy to measure uncertainty and empirically set γ = 0.7(Eq 12 ) .
For each dataset , we randomly label 10 samples to form yL at the beginning . Then we iteratively apply the active learning algorithm , by selecting 10 samples to query each time . After each round , we train the factor graph to test the accuracy and F1 score . 5.2 Performance Analysis
Table 3 : Average F1 score( % )
Data Coauthor Enron Slashdot Mobile
IMS MaxCo MU 69.55 79.50 76.90 69.51
76.15 87.94 80.04 75.31
59.68 86.30 77.85 51.23
RAN BMAL 63.73 85.85 77.60 50.17
55.86 85.59 77.74 53.57
( a ) Coauthor : #Label=100
( b ) Mobile : #label=100
Figure 3 : Labeled Data Balance : Blue bars above 0 represent the number of positive selected samples and red bars below 0 indicate the number of negative selected samples with different active learning strategies .
Figure 1 shows F1 score and accuracy performance of each algorithm on each dataset . We also calculate the average accuracy performance and F1 score for all selection strategies , which is shown in Table 2 and Table 3 respectively .
Performance Comparison . According to Figure 1 and Table 2 and 3 , we see that MaxCo significantly outperforms other competing methods on the four datasets . In Coauthor , MaxCo is better than the random selection method by 2.98 % and 12.42 % improvements in terms of average accuracy and F1 score . Other methods such as MU and BMAL seems to amplify the side effects of label imbalance , which will be discussed later . In Enron , MaxCo is 0.43 % worse than MU by average accuracy , but 1.64 % better in terms of average F1 . These two methods strongly outperform other methods . In Slashdot and Mobile , MaxCo achieves outstanding performance while other methods perform relatively closely to each other .
From the results , we Imbalance of Labeled Data . found a phenomenon : for some active learning strategies , performance will decrease as the number of labels grows . By further investigation , we discover that the phenomenon is related to labeled data imbalance . From figure 1 we can see that , Coauthor and Mobile datasets are relatively more sensitive to the balance of labeled data . For these two datasets , the curves representing MU and BMAL turn down sharply as the number of labeled instances increases . When the number of labeled data comes to 100 , the F1 score falls to around 50 % in the Coauthor data and 30 % in the Mobile data , the accuracy performance also suffers sharp decrease . We observe the ratio of positive and negative instances and find that the ineffectiveness of these two models in this case is correlated to imbalance of labeled data .
Figure 3 plots the number of positive and negative labeled samples when the number of labeled samples is 100 . Figure 3(a ) is for the Coauthor dataset and figure 3(b ) is for the Mobile dataset . On the Coauthor dataset , IMS and MU tend to label positive instances while BMAL labels significantly more negative instances . On the Mobile dataset , also , BMAL and MU suffer from labeled data imbalance ,
−60−40−20020406080100MaxCoRANIMSBMALMUCoauthorPositiveNegative−60−40−200204060MaxCoRANIMSBMALMUMobilePositiveNegative sion to solve the problem , which is demonstrated to be less effective than non progressive one in this paper . In [ 23 ] , Shi et al . proposed a general framework on batch mode active learning . They used three criteria for instance selection . We also compare to their algorithm in §5 . Also , extensive literature focused on active learning on social network [ 15 , 14 , 2 , 20 ] .
Our work is also related to information diffusion model . In [ 13 ] , Kempe et al . solved the influence maximization problem on progressive diffusion model and show a reduction to non progressive one . However , their definition of nonprogressive model cannot be applied to the active learning problem . There are several works studying the spread of social influence with various propagation models [ 9 , 18 , 30 , 6 ] . Besides , the diffusion models are widely used in real world applications such as viral marketing [ 19 , 7 ] . Progressive diffusion models have been extensively studied in the literature [ 25 , 4 , 11 , 3 , 26 ] . Fazli et al . [ 8 ] proposed a greedy algorithm for non progressive model and proved approximation ratio on power law graph . However , their works cannot be directly applied to active learning problem for the network data .
7 . CONCLUSION
In this paper , we study the problem of batch mode active learning for networked data , which aims to query k unlabeled samples in a network such that we can achieve best performance improvement . We utilize factor graph model as our basic framework so as to leverage link formation of networked data . We leverage Loopy Belief Propagation to learn the parameter in factor graph model . We propose a novel idea of connecting the graphical model to the information diffusion process . Therefore , we precisely instantiate the active learning problem on a non progressive diffusion model . We solve the problem of active learning on non progressive diffusion model by MaxCo , which includes two steps . First we prove a reduction to Minimum Source Set problem and then we propose an iterative greedy algorithm MinSS to solve the Minimum Source Set problem . We theoretically show the NP hardness of our problem , analyze the convergence and correctness of MinSS , and finally provide approximation guarantees for our algorithm with an upper bound and a lower bound . We empirically evaluate MaxCo algorithm in comparison with several baseline methods on several datasets . The experimental results demonstrate that our approach significantly outperforms the competing methods .
Acknowledgements . The work is supported by National Basic Research Program of China ( No . 2011CB302302 ) and Natural Science Foundation of China ( No . 61222212 , No . 61073073 ) , and a research fund supported by Huawei Inc .
8 . REFERENCES [ 1 ] A . Arasu , M . G¨otz , and R . Kaushik . On active learning of record matching packages . In SIGMOD’10 , pages 783–794 , 2010 .
[ 2 ] M . Bilgic , L . Mihalkova , and L . Getoor . Active learning for networked data . ICML , pages 79–86 , 2010 .
[ 3 ] C . Chang and Y . Lyuu . Spreading messages . Theor Comput
Sci , 410:2714–2724 , 2009 .
[ 4 ] W . Chen , C . Wang , and Y . Wang . Scalable influence maximization for prevalent viral marketing in large scale social networks . In KDD’10 , pages 1029–1038 , 2010 .
( a ) Slashdot
( b ) Enron
Figure 4 : Parameter Sensitivity : D denotes distance measure of uncertainty and E represents entropy measure ; γ is the constant factor in threshold function . where negative instances take up to 60 percent of all labeled samples . The results show that for some datasets , the performance of factor graph model will be influenced by the balance of labeled samples . The results also demonstrate that MaxCo can produce relatively balanced results .
Significance Test . We perform significance test for the results of the comparison methods . Pairing the measure scores(F1 or accuracy ) of two models with the same number of labeled instances on a dataset , we assume that the difference of the two models is random and symmetric around the median . Therefore , we can perform Wilcoxon signed rank test [ 24 ] to demonstrate the significance level of the difference between the two models . The result shows that p value is less than 0.01 , which indicates that the improvements of MaxCo over the competing methods are statistically significant .
Parameter Sensitivity . We further study the parameter sensitivity of γ in MaxCo . We also consider the effects of uncertainty definition . According to § 3 , we compare distance measure ( Eq 13 ) with entropy measure ( Eq 14 ) . We repeat our experiment with different ways of threshold definition and uncertainty definition . The results are plotted in Figure 4 . We can see that MaxCo is insensitive to the threshold parameter and uncertainty definition . Entropy measure is slightly better than distance measure , with a relatively larger γ .
6 . RELATED WORK
Active learning is a very important topic in the study of social network and web mining because of exponentially growing size of data and high cost of data labeling . Settles et al . [ 22 ] surveyed query selection strategies for sequence models and proposed novel algorithms . There have been several works designing active learning algorithms to specific problems . For example , Arasu et al . [ 1 ] present novel algorithms for the problem of record matching packages . Hoi et al . [ 12 ] studied active learning on text categorization problem . Different from existing methods , we propose a general framework , which can be applied to different problems . There are also several works based on specific models . Martinez et al . [ 16 ] formulated the active learning problem under for the conditional random field ( CRF ) model . Golovin et al . [ 10 ] developed a greedy algorithm for Bayesian active learning with noisy observations . A similar work [ 31 ] by Zhuang et al . also studied active learning problem on factor graph model . However , they utilize progressive information diffu
102030405060708090100#Label068070072074076078080082084F1SlashdotD γ=0.2D γ=0.5D γ=0.8E γ=0.2E γ=0.5E γ=08102030405060#Label070075080085090095F1EnronD γ=0.2D γ=0.5D γ=0.8E γ=0.2E γ=0.5E γ=0.8 [ 5 ] D . A . Cohn , Z . Ghahramani , and M . I . Jordan . Active
[ 31 ] H . Zhuang , J . Tang , W . Tang , T . Lou , A . Chin , and learning with statistical models . J . Artif . Int . Res . , 4(1):129–145 , Mar . 1996 .
[ 6 ] Z . Dezs¨o and A . Barab´asi . Halting viruses in scale free networks . Phys Rev , 2002 .
[ 7 ] P . Domingos and M . Richardson . Mining the network value of customers . In KDD’01 , pages 57–66 , 2001 .
[ 8 ] M . Fazli , M . Ghodsi , J . Habibi , P . J . Khalilabadi ,
V.Mirrokni , and S . S . Sadeghabad . On the non progressive spread of influence through social networks . LATIN 2012 : Theoretical Informatics , 7256:315–326 , 2012 .
[ 9 ] L . Freeman . The development of social network analysis .
Empirical Press Vancouver , British Columbia , 2004 .
[ 10 ] D . Golovin , A . Krause , and D . Ray . Near optimal bayesian active learning with noisy observations . CoRR , 2010 .
[ 11 ] A . Goyal , F . Bonchi , and L . V . Lakshmanan . Learning influence probabilities in social networks . In WSDM’10 , pages 241–250 , 2010 .
[ 12 ] S . C . H . Hoi , R . Jin , and M . R . Lyu . Large scale text categorization by batch mode active learning . In WWW’06 , pages 633–642 , 2006 .
[ 13 ] D . Kempe , J . Kleinberg , and E . Tardos . Maximizing the spread of influence through a social network . In KDD’03 , pages 137–146 , 2003 .
[ 14 ] M . Kimura , K . Satio , R . Nakano , and H . Motoda .
Extracting influential nodes on a social network for information diffusion . DMKD , 20:70–97 , 2010 .
[ 15 ] A . Kuwadekar and J . Neville . Relational active learning for joint collective classification models . In ICML’11 , pages 385–392 , 2011 .
[ 16 ] O . Martinez and G . Tsechpenakis . Integration of active learning in a collaborative crf . In CVPRW’08 , pages 1–8 , 2008 .
[ 17 ] K . P . Murphy , Y . Weiss , and M . I . Jordan . Loopy belief propagation for approximate inference : An empirical study . In UAI’99 , pages 467–475 , 1999 .
[ 18 ] R . Pastor Satorras and A . Vespignani . Epidemic spreading in scale free networks . Phys Rev , 65 , 2002 .
[ 19 ] M . Richardson and P . Domingos . Mining knowledge sharing sites for viral marketing . In KDD’02 , pages 61–70 , 2002 .
[ 20 ] N . Roy and A . McCallum . Toward optimal active learning through sampling estimation of error reduction . ICML , pages 441–448 , 2001 .
[ 21 ] T . Scheffer , C . Decomain , and S . Wrobel . Active hidden markov models for information extraction . In CAIDA’01 , pages 309–318 , 2001 .
[ 22 ] B . Settles and M . Craven . An analysis of active learning strategies for sequence labeling tasks . In EMNLP , pages 1070–1079 , 2008 .
[ 23 ] L . Shi , Y . Zhao , and J . Tang . Batch mode active learning for networked data . ACM Transactions on Intelligent Systems and Technology ( TIST ) , 2011 .
[ 24 ] Siegel and Sidney . Non parametric statistics for the behavioral sciences . New York : McGraw Hill , 1956 .
[ 25 ] J . Tang , J . Sun , C . Wang , and Z . Yang . Social influence analysis in large scale networks . In KDD’09 , pages 807–816 , 2009 .
[ 26 ] J . Tang , S . Wu , and J . Sun . Confluence : Conformity influence in large social networks . In KDD’13 , pages 347–355 , 2013 .
[ 27 ] J . Tang , J . Zhang , L . Yao , J . Li , L . Zhang , and Z . Su . Arnetminer : Extraction and mining of academic social networks . In KDD’08 , pages 990–998 , 2008 .
[ 28 ] W . Tang , H . Zhuang , and J . Tang . Learning to infer social ties in large networks . In ECML/PKDD’11 , pages 381–397 , 2011 .
[ 29 ] M . J . Wainwright and M . I . Jordan . Graphical models , exponential families , and variational inference . Found . Trends Mach . Learn . , 1(1 2):1–305 , Jan . 2008 .
[ 30 ] D . Wilson . Levels of selection : An alternative to individualism in biology and the human sciences . Soc Networks , 11:257–272 , 1989 .
X . Wang . Actively learning to infer social ties . Data Mining and Knowledge Discovery , 25(2):270–297 , 2012 .
APPENDIX
Proof of Theorem 3 .
Proof . First , suppose G = ( X , Y ) is a bipartite graph . Let eA denote the number of edges in A and eAB denote the number of edges across A and B . Let BX = VS ∩ X , BY = VS ∩ Y and W = VU\VS . Following the lemma in [ 8 ] , we have eW BX + eW ≤ eW BY + eW ≤ Because d(v ) ≤ v∈BY ∪W v∈BX∪W v∈W
( d(v ) − t(v ) )
( d(v ) − t(v ) ) v∈W v∈W d(v ) = 2eW + eW BX + eW BY , it follows ( d(v ) − t(v ) ) v ∈ VU ( d(v ) − t(v ) ) +
Therefore ,
T ( W ) ≤ D(VU ) − T ( VU )
Because T ( VS ) = T ( VU ) − T ( W ) , we have T ( VS ) ≥ T ( VU ) − ( D(VU ) − T ( VU ) ) = 2T ( VU ) − D(VU ) Because t(v ) ≤ βd(v ) for each v ∈ V , we can derive
Therefore , it yields
T ( VS ) ≤ βD(VS ) ≤ β∆|VS| fififiVS,opt fififi ≥ 2T ( VU ) − D(VU )
β∆
By Lemma 2 , the inequality holds as well for any general graph G .
Proof of Theorem 4 .
Proof . Let Q = {v|w(v ) = d(v ) − t(v ) ∧ v ∈ VU} , and
W = VU\VS . By definition we have , v∈Q
( d(v ) − t(v ) ) ≤ d(v ) ≤ v∈W v∈Q v∈W
( 1 − β ) d(v ) d(v )
Therefore ,
Referring to the procedure of MinSS algorithm , a sample v is inserted into VS if and only if NB(v ) ∩ Q = ∅ . Because for v ∈ Q , w(v ) = d(v ) − t(v ) , we have
|VS| ≤ t(v ) ≤ β d(v ) ≤ β
1 − β d(v ) ≤ β
1 − β
∆|W| v∈Q v∈Q v∈W Because |W| = |VU| − |VS| , it follows , |VU|
|VS| ≤
β∆
1 − β + β∆
