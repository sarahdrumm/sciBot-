Going Beyond Corr LDA for Detecting Specific Comments on News & Blogs
Mrinal Das† mrinal@csaiiscernetin
Chiranjib Bhattacharyya† chiru@csaiiscernetin
Trapit Bansal† trapit@csaiiscernetin
† Department of Computer Science and Automation , Indian Institute of Science , India
ABSTRACT Understanding user generated comments in response to news and blog posts is an important area of research . After ignoring irrelevant comments , one finds that a large fraction , approximately 50 % , of the comments are very specific and can be further related to certain parts of the article instead of the entire story . For example , in a recent product review of Google Nexus 7 in ArsTechnica ( a popular blog ) , the reviewer talks about the prospect of “ Retina equipped iPad mini ” in a few sentences . It is interesting that although the article is on Nexus 7 , but a significant number of comments are focused on this specific point regarding “ iPad ” . We pose the problem of detecting such comments as specific comments location ( SCL ) problem . SCL is an important open problem with no prior work .
SCL can be posed as a correspondence problem between comments and the parts of the relevant article , and one could potentially use Corr LDA type models . Unfortunately , such models do not give satisfactory performance as they are restricted to using a single topic vector per article comments pair . In this paper we propose to go beyond the single topic vector assumption and propose a novel correspondence topic model , namely SCTM , which admits multiple topic vectors ( MTV ) per article comments pair . The resulting inference problem is quite complicated because of MTV and has no off the shelf solution . One of the major contributions of this paper is to show that using stick breaking process as a prior over MTV , one can derive a collapsed Gibbs sampling procedure , which empirically works well for SCL .
SCTM is rigorously evaluated on three datasets , crawled from Yahoo! News ( 138,000 comments ) and two blogs , ArsTechnica ( AT ) Science ( 90,000 comments ) and AT Gadget ( 160,000 comments ) . We observe that SCTM performs better than Corr LDA , not only in terms of metrics like perplexity and topic coherence but also discovers more unique topics . We see that this immediately leads to an order of magnitude improvement in F1 score over Corr LDA for SCL .
Categories and Subject Descriptors G.3 [ Probability and Statistics ] : Probabilistic algorithms ; I27 [ Artificial Intelligence ] : Natural Language Processing—Text Analysis
Keywords specific , correspondence , comments , news , blogs
1 .
INTRODUCTION
Comments on news and blogs naturally promote user participation and play a pivotal role in increasing the popularity of host websites . In this paper we investigate the correlation structure between the content , such as news or blogpostings , and user generated comments .
Consider the example in Figure 1 that shows an excerpt from the review on Google Nexus 7 , dated July 31 20131 . It is one of the well commented recent reviews with more than 200 comments . Some comments are related to the entire article , like . . . good to see tablets are becoming cheaper ( black colored in Figure 1 ) , which are general comments . Some comments are irrelevant , for example I work on a laptop ( blue in Figure 1 ) .
In the same article , in about 8 % of the sentences , the author talks about retina display and compares with “ iPad ” ( colored green in Figure 1 ) . It was found that , leaving irrelevant comments , around 22 % comments ( colored green in Figure 1 ) are made only on that specific point . The fact of 22 % comments on a topic covered in 8 % of sentences , and those sentences being far different from the central theme of the article , is quite astonishing . These comments are not useless , rather they provide valuable user feedback . We call such comments as specific comments . Discovering specific comments and the associated part of the content which is specific to the comment will be called the specific comment location ( SCL ) problem .
Solving SCL will open up many opportunities such as accumulating user feedback , analysing market trends , improving user experience etc . There is no prior work related to SCL , furthermore there are no off the shelf techniques which could be adapted to solve this problem . Discriminative techniques are often accurate but require lot of labeled training data which in this case is not available . Past approaches such as [ 11 ] on comment understanding demonstrates that accuracy of discriminative approaches depend
1arstechnica.com/gadgets/2013/07/the 2013 nexus 7review meet the new standard for android tablets/ on cleverly crafted features . Keeping this in mind we pursue topic models , an unsupervised approach , to the SCL problem . Correspondence LDA ( Corr LDA ) [ 1 ] is an interesting topic model which could be a potential candidate for SCL . Existing topic models , including Corr LDA , assign a single topic vector to each document . This makes Corr LDA more suitable for discovering general comments and as empirical evidence suggests it is not suitable for SCL . In this paper we address the issue of SCL and make the following contributions .
Contributions : We introduce specific correspondence topic models ( SCTM ) , based on the notion of multiple topic vectors ( MTV ) as opposed to single topic vector in the state of the art models . Moreover , in order to handle wide variety of comments we enhance the diversity among topics through sparsity . The inference becomes non standard due to MTV and sparsity . We explore a stick breaking process ( SBP ) as a prior over MTV . One major contribution of this paper is a collapsed Gibbs sampling inference procedure for SCTM . The resultant algorithm converges fast and also leads to simple update equations which are easy to implement . Using three real world datasets , we evaluate the proposed approach in two aspects . ( i ) Using perplexity and topic coherence we show that SCTM models data better than the state of art . Then , we demonstrate that SCTM discovers more diverse set of topics and converges faster in terms of likelihood . ( ii ) Using precision recall we compare SCTM with the baseline on the task of discovering specific comments as well as aligning them to the respective sentences in the article . Finally , we show various use cases of SCTM on some interesting practical applications .
The paper is organized as follows . We formulate the problem of SCL and discuss the challenges and related works in section 2 . Then in section 3 we present the proposed model and the Gibbs sampling inference algorithm . We describe the algorithm for SCL in section 4 . In section 5 , we provide empirical evaluation . Finally , section 6 presents some use cases of SCL .
2 . THE PROBLEM OF SPECIFIC COMM
ENT LOCATION
In this section we introduce the problem of specific comment location ( SCL ) and discuss the difficulties in resolving SCL . We begin by introducing relevant notation .
V
Notation : The set of element wise positive d dimen+ . ∼ means “ distributed sional vectors will be denoted by Rd as ” , 1x is a x dimensional vector with all entries as 1 . K is the number of topics and V is the number of words in the vocabulary . βk is a V dimensional vector such that j=1 βkj = 1 , popularly called as a “ topic ” . x.y is element wise product of two vectors x and y of same dimension . Dir denotes Dirichlet distribution , U denotes discrete uniform distribution and mult represents multinomial distribution . [ n ] = {1 , 2 , . . . , n} and |R| : cardinality of set R . ˜x means a set of variables of same type . I[ . ] is the indicator function .
Definition : A news or a blog article Ad , indexed by d , is a collection of Sd number of sentences . More explicitly Ad = {sda|a ∈ [ Sd]} , where each sentence is denoted by sda = {wdai|i ∈ [ nda]} . The ith word in the ath sentence
Figure 1 : Example from ArsTechnica . For specific comments ( in green ) , the corresponding sentences in the article ( in green , referred as Rde ) are few and are not contiguous . The box shows hot spot ( most commented sentences ) . Irrelevant comment ( in blue ) does not have any corresponding sentence in the article , whereas general comments ( in black ) are related to the entire article . sda is denoted by wdai and the number of words in sda is denoted by nda . Corresponding to each article , Ad there is a set of comments denoted by Cd = {cdej|j ∈ [ nde ] , e ∈ [ Ed]} where cdej is the jth word in the eth comment related to article Ad . Number of comments on Ad is denoted by Ed and nde is the number of words in the eth comment . Furthermore wdai , cdei ∈ [ V ] , where V is the number of unique words in the entire corpus . Let Rde ⊆ Ad denote the set of sentences related to the eth comment of dth article . In Figure 1 the boxes show the sentences indexed by Rde for the green colored comment . The eth comment can be a general , irrelevant or specific comment depending on the size of Rde . It is a general comment when |Rde| is almost equal or equal to Sd , and is an irrelevant comment when Rde is empty . Comment e is a specific comment if 0 < |Rde| ≤ Nd , where the parameter Nd is preset by the user and determines the granularity of specific comments . Notice that general comments and irrelevant comments are two extreme cases .
SCL formulation : The problem of specific comment location can now be formulated as identifying Rde given Ad and the eth comment whenever |Rde| ≤ Nd .
For a specific comment e , the set Rde can be understood as specific correspondence between the comment and the article Ad . In this paper we concentrate on articles and comments but it is conceivable that similar problems exist in many other domains such as technical paper and bibliography , image and tags etc .
The difficulty in resolving SCL : It is not easy to discover Rde . Specific comments and general comments look very similar , and there are no distinguishing features such as length ( number of words ) , presence of proper nouns , or selection of words in comments . Absence of distinguishing features makes it impractical to apply rule based approaches .
Cheaper than most , better than all : the 2013 Nexus 7 reviewed Just over a year ago , Google released its first Nexus tablet . The 2012 Nexus 7 wasn't perfect by along shot , but it was the kick in the pants that the Android tablet ecosystem needed at the timeThe 1920×1200 display has much brighter colors than last year's 1280×800 panel , and at 323 PPI,it outdoes both the 300 PPI Nexus 10 and 264 PPI Retina iPads . Until there's a Retina iPad mini , no other small tablet screen comes closeIn our previous article , we noted that the 2013 Nexus 7 was consistently faster than the Nexus 4 despite the fact that the two ostensibly share the same system on a chip ( SoC)If Apple responds with a Retina equipped iPad mini in the fall , the balance of power may shift back in the other direction . But if it sticks with its current display , it will become more difficult to recommend.CommentsAs always , an authoritative review . I picked one up after getting tired of Apple's iPad Mini Retina Display nonsense they want to prolong the agony before releasing it and eating away their iPad revenue . I need the high resolution screen because It's good to see mainstream tablets are getting cheaper with better technology.I just got a 10 inch Chinese tablet for about the same as this 7 in mainstream one . I agree , but your very example just proved that apple *didn't* care about the user experience andjust wanted to release an ipad with a retina screen in the early part of 2012 without regard for the"experience" . Yup.I read , play games , do email , check quick facts and goof off on my tablet . I surf on a laptop . I work on a laptop.SpecificGeneralIrrelevant In addition , due to lack of labeled data , supervised models are inapplicable . For example , [ 11 ] made an attempt to align comments to paragraphs in an article , however their method being supervised is restrictive and does not apply generally . An immediate alternative could be the use of keyword based searches .
√ ndae nda nde
Very low textual overlap between specific comments and the article : To measure the overlap in words between a specific comment and the main article , we define , ∆dae = √ , where ndae is the number of words in common between ath sentence and eth comment for article indexed by d . ∆dae can be interpreted as cosine distance or correlation coefficient which computes the normalized similarity between a sentence in the main article with a comment . nda and nde are as defined before . Let D be the total number of ( Ad , Cd ) pairs then the average textual overlap over all sentences and the articles can be measured by ∆ = 1 D a∈{a|sda∈Rde} ∆dae .
1|Rde| e
1 Ed d
From our study based on ArsTechnica gold standard ( see section 5 ) , we find the following . For specific comments , considering all sentences of the article ( ie using Ad instead of Rde in ∆ above ) , ∆ is 0.07 and for non specific comments it is 006 However , considering only the relevant sentences for the specific comments , the value of ∆ is 008 Notice that , not only the value is very small but also they are very similar for both specific and non specific comments . For example in Figure 1 , “ iPad ” is the only word which probably can link but due to other words the overlap becomes low . This makes keyword based approaches difficult to apply .
Topic models : Over the last decade Topic modeling has become ubiquitous in text analysis . It is an unsupervised approach rooted in Bayesian modeling . In the following we review some of the related topic models and discuss their suitability for SCL . There has been an increasing interest in using topic models such as [ 9 , 15 , 6 ] for understanding user generated content . [ 6 ] considered the problem of finding episodic tweets about an event , tweets which are related to one of the segments of an event . Their model is similar in spirit to [ 15 ] , where event and tweet are exchangeable . As noted in [ 1 ] these models [ 15 , 6 ] are not suited for understanding the dependency between a news article and comments . MG LDA [ 12 ] has been developed to model local topics . A set of sliding windows are used across the sentences in a document which uses local topics . MG LDA is inappropriate because : ( i ) local topics scatter across the corpus which is not the case here , and ( ii ) every segment or a set of contiguous sentences may not correspond to a comment . Among the existing models the most suited seems to be Corr LDA [ 1 ] and we will use it as a baseline .
Review of Corr LDA : Corr LDA [ 1 ] is a topic model for understanding correspondences . It was initially proposed for modeling annotations on images . Corr LDA uses a bagof words and there is no notion of sentences , ie Sd = 1 . The generative process of Corr LDA is as follows .
For each ( Ad , Cd ) • Sample a topic vector θd ∼ Dir(α1K ) • For each word wdi , i ∈ [ nd ]
– sample topic zdi ∼ mult(θd ) – sample word wdi ∼ mult(βzdi )
• For each comment e ∈ [ Ed ] – For each word i ∈ [ nde ]
∗ sample topic ydei ∼ U ( ˜zd ) ∗ sample word cdei ∼ mult(βydei )
Notice that comment topic indices are sampled uniformly at random from article topic indices . Hence , yde corresponds to the complete set ˜zd and comment e becomes related to the entire article d . This lacks the specific correspondence focus of the current paper . Using Algorithm 1 ( to be described in section 4 ) Corr LDA can be used for SCL .
Limitations of Corr LDA for SCL : We summarize some major limitations of Corr LDA for SCL as below .
A . ) Very low correspondence : Corr LDA models Rde as the entire article , however Rde is not known a priori and is very small for specific comments . Based on the goldstandard on ArsTechnica ( see section 5 ) , we observe that on average only 3 % of the sentences in an article are related to a specific comment , ie |Rde| is very small . In Figure 1 , for the specific comment on “ iPad ” , is 008
|Rde| Sd
B . ) Topical difference in Rde : A specific comment e is related to a small part Rde and is less relevant to the rest of the article . Therefore , Rde has a different topic proportion than the rest of the article . For example , in Figure 1 , Rde should have high probability for “ iPad ” topic , although that is a topic with low overall probability in the article .
Note that in Corr LDA there is a single topic vector θd for document d . Hence topic proportions in Rde is also θd , ie probability of “ iPad ” topic , for example , in Figure 1 is also low in Rde . This is a major drawback which we address in our proposed model SCTM .
3 . SCTM : A CORRESPONDENCE TOPIC
MODEL
In this section we define specific correspondence topic mod els ( SCTM ) and the associated inference algorithm . 3.1 SCTM
The need for multiple topic vectors ( MTV ) : As pointed out in the last section , existing correspondence topic models use a single topic vector θd for each pair of article and the associated comments . This makes the topic proportions constant throughout the article and comments . Though this maybe suitable for general comments , where one might have similar topic proportions , but specific comments have different proportion over topics than the main article . Precisely , a specific comment e and Rde should have similar topic proportions . It is thus clear that one needs to go beyond the single topic vector assumption to resolve SCL . To this end we introduce the novel concept of using multiple proportions over topics per article comments pair to model specific correspondence .
Modeling topical difference in Rde using MTV : Note that the challenge is that Rde is not known a priori , so that we can extract Rde from the main article and model the correspondence between Rde and comment e following CorrLDA . We resolve this by introducing multiple topic proportions to be called as MTV ( multiple topic vectors ) . Recently , [ 5 ] has used the concept of MTV in a different context to find subtle topics .
For each vocabulary term v ∈ [ V ]
• Sample κv ∼ Beta( νλ
V , λ )
For each topic k ∈ [ K ] ,
• For each term of the vocabulary v ∈ [ V ]
– Sample word selector φkv ∼ Bernoulli(κv )
• Draw topic distribution βk ∼ Dir(η1V +1.φk ) For each article comments pair ( Ad , Cd ) , d ∈ [ D ] ,
• For j ∈ [ Jd ] , draw θdj ∼ Dir(α1K ) • For each article sentence a ∈ [ Sd ] ,
– Draw ρda ∼ stick(τ , ι ) – For each word i ∈ [ nda ] ,
∗ Draw bdai ∼ mult(ρda ) ∗ Draw zdai ∼ mult(θdbdai ) ∗ Draw word wdai ∼ mult(βzdai )
• For each comment , e ∈ [ Ed ]
– Sample πde ∼ Beta( 1 , 2 ) – Sample de ∼ Beta(ς1 , ς2 ) – For each sentence a of the article ∗ Draw ξdea ∼ Bernoulli(πde )
– Set ϕde = {zdai , ∀ ( a , i ) | ξdea = 1} – For each comment word i ∈ [ nde ] ,
∗ Draw tdei ∼ Bernoulli( de ) ∗ If tdei = 1 , ydei = K +1 ( irrelevant topic ) ∗ Else ydei ∼ U ( ϕde ) ∗ Draw word cdei ∼ mult(βydei )
Figure 2 : Generative process of SCTM . θdj : topicproportion vector , ρda : distribution over topicvectors , bdai : topic vector index , zdai : topic assignment , ξdea : sentence selector , tdei : irrelevant topic selector . stick refers to construction as in ( 1 ) .
Therefore , per article comments pair there are Jd ≥ 1 topic vectors denoted by {θdj}Jd j=1 . While generating a word in a sentence , one θdj is selected randomly . So , proportions over topics in each sentence can be different and a random selection of Rde from article d can have very different proportions over topics than that of article d . For example , even if “ iPad ” topic has low probability in the entire article in Figure 1 , it may have high probability in some Rde .
Choice of a suitable prior for MTV : The choice of a suitable prior for MTV is an important question . The challenge is that , ( i ) in the SCL problem , it may occur that some topic has an extremely low probability in the entire document but might have a very high probability in a specific region and can be useful in detecting specific comments . ( ii ) Topic vectors should be shared across the sentences in an article because for a specific comment e on article d , Rde may span over multiple non contiguous sentences . Important to note that , over estimation of Jd will try to model many general comments as specific in nature , whereas underestimation will try to model specific comments as general ones . Nonparametric priors such as dependent Dirichlet process can mitigate this issue . One could potentially use hierarchical Dirichlet process ( HDP ) which is suited to share topics as required in ( ii ) . However the difficulty with HDP is its rich getting richer property which tends to make a widely appearing topic vector highly probable in all the sentences , contradicting the requirement ( i ) . We will explore SBP as an alternative prior which could address these issues . Stick breaking process ( SBP ) : SBP[7 ] is defined as follows . Let τ , ι ∈ RJ + and Γ is a diffuse probability measure on a measurable space ( Ω,B ) . A random probability measure G on ( Ω,B ) is a stick breaking prior if G = j=1 ρjδθj , θj iid∼ Γ and ( ρj ) are constructed as
J
ρ1 = v1 , ρj = vj
( 1 − vl ) , j > 1
( 1 ) j−1 l=1 struction J iid∼ Beta(τj , ιj ) , j < J , and vJ = 1 . To keep where vj the exposition simple we assume J to be finite . δθj denotes an atomic distribution where the entire probability mass is concentrated at θj . Γ is a measure defined on vectors θj , commonly referred as atoms of G . Furthermore by conj=1 ρj = 1 . For each sentence sda in article Ad we use a distribution Gda with topic vectors ( θdj ) as atoms . In our case the topic vectors are sampled from a Dirichlet prior Dir(α1K ) . For a fixed d assume that ( ρda ) is defined as in equation ( 1 ) then
Jd iid∼ Dir(α1K ) , Gda =
θdj
ρdaδθdj
( 2 ) j=1
Notice that , the proportion over the topic vectors for a sentence does not depend on those of other sentences , nor does it depend on any document wide property . Unlike Dirichlet distribution , Jd acts as an upper limit on number of topic vectors , where higher indexed topic vectors get lower prior probabilities . Thus SBP allows to provide high weight on a topic vector for some sentences , whereas for most of the sentences that topic vector has low probability . Unfortunately though SBP is suitable , as it is not commonly known in machine learning , makes the inference non standard .
Generation of Rde We randomly select a set of sentences in article Ad as Rde . Random selection of Rde and use of MTV vary the topic proportions in Rde making SCTM appropriate for SCL . Note that , alternatively , Rde could be sampled first and then corresponding to each Rde a topic vector could be used . But this has several problems : ( i ) even if Rde is same for two comments it will have two different θ , ( ii ) articles with large number of comments will create too many topic vectors , ( iii ) modeling article will become dependent on comments . So we have avoided this choice .
Enhancing diversity in topics : Note that , topics related to specific comments contain words which are very rare in the article . For example , “ iPad ” has rarely appeared in the article . Now considering a web scale data we need to consider a vast and diverse set of article comments . Hence , it is required that we detect a diverse set of topics which have high probability for different sets of words .
We model this fact by modifying the definition of topic slightly . Instead of defining a topic over all the words in the vocabulary , similar to the selection of Rde we randomly select a subset of words using φ and define a topic over that subset of words . φ is a Bernoulli random variable and when φkv = 0 , βkv = 0 with probability one , where βkv is the probability of word indexed by v in the kth topic . To avoid accidental situations , we use φk of length V + 1 , and i=1 φki = 0 ] . Similar concept has been explored by [ 13 ] for decoupling sparsity and smoothness in topics . Unlike [ 13 ] , we have two parameters λ and ν , where λ is a repulsion parameter . When λ increases , diversity across φ increases , in turn increasing diversity across β , ie topics . set φk,V +1 = I[V
Irrelevant topic : Irrelevant topic is used for modeling comments to take care of words which generally appear in comments but not in main articles . These words are uninformative words , motivating the name of the topic . For example in Figure 1 , “ Yup ” in the irrelevant comment is one such word . Irrelevant topic models a topic which is irrelevant to any type of correspondence between article and comment . It has been observed that irrelevant comments mostly contain this topic . Similar technique has been explored in [ 9 ] . 3.2 Collapsed Gibbs sampling inference
In this section we describe an efficient and easy to implement Gibbs sampling based inference procedure for SCTM . In order to achieve accelerated convergence , we need to marginalize out the real valued random variables β , θ , , ρ , π and κ , in the generative process ( Fig 2 ) and infer the latent variables z , y , b , ξ and φ . The major challenges are : ( 1 ) sampling b , due to SBP the inference becomes non standard and ( 2 ) ξ , φ are binary random variables demanding novel inference mechanisms . We address the first challenge by noting the relationship between SBP and generalized Dirichlet distribution GD [ 4 ] , and conjugacy of GD with the multinomial distribution . That allows us to integrate out ρ , see appendix for details . We address the second challenge by integrating out the parameters using the Beta Bernoulli conjugacy and directly sampling the binary random variables . Following counting notations will be used in the inference .
Count notation : ( i ) “ dot ” in the suffix represents marginal ization at the corresponding index , ( ii ) ( −x ) in the superscript means counting without x . nda k , and ¯mk . is number of times topic k has occurred .
¯mkv denotes number of times word v is assigned to topic ˙mdeak = r=1 I[zdar = k , ξdea = 1 ] , ie number of times topic k has occurred in the article considering only those sentences selected by ξdea . ˆmdjk is number of times topic k has been used from topic vector j of article d . ˇmdaj is number of times topic vector j has appeared in sentence a in document i=1 I[ydei = k ] is the number of times d . ˚mdk = Ed nde e=1 topic k has appeared across all the comments .
Sampling b : We compute the conditional probability p(bdai = j|b−dai , τ , ι ) , for j < Jd , as
τj + ιj +Jd
τj + ˇm
−dai daj r=j ˇm
−dai dar
ιl +Jd τl + ιl +Jd s=l+1 ˇm s=l ˇm
−dai das −dai das l<j
( 3 )
& p(bdai = Jd|b−dai , τ , ι ) = 1 −Jd−1 j−1 l=1 p(bdai = l|˜b−dai , τ , ι ) . l=1 ( 1− For any j the above equation can be expressed as uj ul ) for suitably defined u . The probability of bdai = j = l , l < j . This directly depends on probability of bdai property is absent in standard priors eg finite dimensional Dirichlet distribution , nonparametric DP . Finally , p(bdai = j|b−dai , z ) is computed as :
∝ p(zdai|bdai = j , ˜z
=
α + ˆm
Kα + ˆm
−dai djzdai −dai dj .
−dai)p(bdai = j|˜b p(bdai = j|˜b
−dai )
−dai )
( 4 )
Sampling φ : Note that , φkv is a binary selector , so if ¯mkv > 0 then φkv = 1 as , otherwise we compute p(φkv = 1| ˜w , ˜z , ˜φ−kv ) as below .
∝ p(w|φkv = 1 , ˜z , ˜φ
−kv)p(φkv = 1| ˜φ
Γ( Γ(
= u=v φkuη + η ) u=v φkuη + η + ¯mk . ) where p(φkv = 1| ˜φ−kv ) can be computed as dκv p(φkv = 1|κv)p(κv| ˜φ
−kv ) =
−kv ) p(φkv = 1| ˜φ
−kv )
V +
νλ j=k φjv
νλ V + λ + K
( 5 )
( 6 )
∝ nde nde i=1
= i=1
Sampling ξ : The inference equation is derived from uniform ( first part ) and Beta Bernoulli conjugacy ( second part ) . We compute p(ξdea = 1|˜yde , ˜ξ−dea , ˜zd , ) as below . −dea ) p(ξdea = 1| ˜ξ p(ydei|˜zd , ξdea = 1 , ˜ξ
−dea , )
˙m
+ ˙mdeaydei
−a de.ydei −a ˙m de + ˙mdea .
1
1 + 2
( 7 )
Sampling z : Sampling topic indices z follows from Dirichlet multinomial conjugacy as follows . p(zdai = k| ˜w , ˜z−dai ) is computed as below . ∝ p(wdai|zdai = k , ˜φ , η)p(zdai = k|˜z−dai , ˜b , α )
× p(˜yd|˜z−dai , zdai = k )
=
φkwdai
η+ ¯m v φkv η+ ¯m
−dai kwdai −dai k .
α+ ˆm
αK+ ˆm
−dai dbdaik −dai dbdai .
˙m˚mdl de.l ( ˙mde.k + 1)˚mdk ( 8 ) l=k
Sampling y : Comment indices depend on topic word Dirichlet multinomial conjugacy ( first part ) and uniform distribution ( second part ) for the comments article correspondence . We compute p(ydei = k|˜c , ˜y−dei , ˜zd , ˜ξde ) as below .
∝ p(cdei|ydei = k , ˜y −dei kcdei −dei k .
φkcdei η + ¯m v φkvη + ¯m
=
−dei , ˜φ , η ) p(ydei = k|˜y
−dei , ˜zd , ˜ξde )
˙mde.k ˙mde
( 9 )
Inference algorithm : Equations ( 4 ) , ( 5 ) , ( 7 ) , ( 8 ) and ( 9 ) together form the inference algorithm . Inference of all variables depend on others , so we need to solve iteratively . The procedure starts by initializing the variables randomly .
Relationship with other algorithms : [ 7 ] and [ 5 ] have given two different algorithms for solving SBP . The algorithm in [ 7 ] samples v explicitly , while the algorithm in [ 5 ] is similar to that used here . On the other hand , [ 13 ] uses a different mechanism for inducing sparsity over words , they do empty empty cosine distance insert a cos & p(ξdea|yde , zd ) ≥ tξ then insert a cos , tξ
Algorithm 1 Classification & alignment Input : y , z ; thresholds : tcos , t 1 : Compute ˜y , ˜z from y , z . 2 : for d ← 1 , . . . , D do 3 : 4 : 5 : 6 :
Initialize Φd ← {} , Ψd ← {} , Γd ← {} for e ← 1 , . . . , Ed do de ˜zda
Sde ← Sde ∪ {a} end if if ∆(a , e ) ≥ t
Rde ← {} , Sde ← {} for a ← 1 , . . . , Sd do ∆(a , e ) ← ˜yT ˜yde˜zda , if ∆(a , e ) ≥ tcos then Rde ← Rde ∪ {a}
7 : 8 : 9 : 10 : 11 : 12 : 13 : 14 : 15 : 16 : 17 : 18 : 19 : 20 : 21 : 22 : end for Output : Φd , Ψd , Γd , Rde , Sde end for if |Rde| == 0 then Ψd ← Ψd ∪ {e} else if |Rde| ≤ Nd then Φd ← Φd ∪ {e} else Γd ← Γd ∪ {e} end if end for end if cardinality is 0 Irrelevant
Specific General not sample the binary random variables φ explicitly . The algorithm given here is novel and much simpler to implement , yet efficient . Use of ξ has not been studied before .
4 . ALGORITHM FOR SCL
In this section we propose an algorithm based on SCTM for SCL . The algorithm considers the inferred latent variables z , y and ξ obtained from SCTM inference procedure and uses them for labeling each comment as general , specific or irrelevant and recover the set Rde for specific comments . z and y are topic indices for the article and comment respectively 2 ˜z , ˜y are topic frequency vectors3 for article and comment respectively . Following the definition in section 2 , we need to compute Rde for each comment e corresponding to article d . Computation of Rde follows from cosine similarity , between the topic assignment counts of the comment ˜y and the sentence ˜z , above a fixed threshold . The decision will depend on threshold Nd , ie specific if |Rde| ≤ Nd . The details are given in Algorithm 1 . For SCTM we can also use the posterior probability of ξ , ie p(ξdea = 1|yde , zd ) , to improve the alignment to specific sentences . Thus , for SCTM , we have a separate set Sde in the algorithm which is obtained by using this information . Selecting thresholds : We set the threshold as Nd = min(Ng,0.6 ∗ Sd ) . Ng limits the threshold for very large documents . Ng ∈ [ 6 , 10 ] is found to work well .
5 . EMPIRICAL EVALUATION
In this section we will evaluate the proposed model SCTM empirically on various aspects using real life datasets4 . 2z = {{zdai|i ∈ [ nda ] , a ∈ [ Sd]}D 3˜z = {{˜zda|a ∈ [ nda]}D d=1} , ˜zda = ( ˜zdak)K i=1 I[zdai = k ] , ˜y is defined similarly nda 4Relevant resources at : mllabcsaiiscernetin/sctm nda d=1} k=1 ∈ RK , ˜zdak =
1
Table 1 : Properties of the datasets .
AT Science AT Gadgets Yahoo! News
#(Articles )
#(Comments )
1,369 90,654
2,186
160,761
730
138,538
5.1 Datasets
We show some basic properties of the datasets in Table 1 and provide a description below . Note that the data consists mostly of articles with a large number of comments which introduces further challenges in discovering specific comments as their number will be expected to be very small compared to the total comments .
ArsTechnica Science ( AT Science ) : The dataset consists of articles and comments crawled from Science section of the site ArsTechnica5 . ArsTechnica is a science and technology blog whose writers consist mostly of academicians . Its articles and readership leads to opinionated discussions in the comments which makes it a perfect testbed for our problem . We crawled 1500 articles and their comments over approximately a two year timeline ( June 2011 to March 2013 ) and removed articles with less than 5 comments .
Gold standard :
In order to quantitatively evaluate our model , we developed a gold standard by manually annotating articles over a one year timeline ( March 2012 to March 2013 ) after filtering out articles which either had very few comments or had only general and irrelevant comments . For each of these article we manually labeled specific and nonspecific comments and also created the alignment of specific comments to the relevant sentences . The gold standard consists of 501 articles with a total of 3176 comments , in which there are 1443 specific comments with an average of 2.9 specific comments per article .
ArsTechnica Gadgets ( AT Gadgets ) : The dataset consists of articles and comments crawled from Gadgets section of the site ArsTechnica6 . This dataset consists mostly of product reviews on latest gadgets . We crawled about 2200 articles and their comments over a two year timeline ( August 2011 to August 2013 ) and removed articles with less than 5 comments .
Yahoo! News : The dataset consists of articles and comments crawled from the most commented7 and archive section8 of the site Yahoo! News , one of the most popular news site . We crawled about 1150 articles along with their comments , going chronologically backwards from 31 March 2013 . We then removed all those articles in the dataset which had fewer than 5 lines or had fewer than 5 comments . We were left with 730 articles with more than 100,000 comments . 5.2 Experimental setup
We have used the same pre processing of the dataset and exactly similar parameter settings for Corr LDA as well as SCTM . We have removed stop words and transformed all
5arstechnica.com/science 6arstechnica.com/gadgets 7newsyahoocom/all sections most commented/mostpopular 8newsyahoocom/archive
Table 2 : Perplexity on test data ( lower is better ) . Model SCTM Corr LDA
AT Science AT Gadgets Yahoo! News
12,327 14,029
6,880 9,673
6,445 6,443
Table 3 : Topic coherence for top 50 topics ( greater is better ) .
Model SCTM Corr LDA
AT Science AT Gadgets Yahoo! News
52.08 88.22
50.47 87.27
65.83 93.68
Table 4 : Topic diversity ( greater is better ) .
Model SCTM Corr LDA
AT Science AT Gadgets Yahoo! News
96.34
9.0
96.78 23.34
99.45 21.5
Table 5 : Precision , recall and F1 score for discovering specific comments .
Model SCTM Corr LDA
Precision Recall 0.61 0.21
0.60 0.23
F1 0.60 0.22
Perplexity : Perplexity is a standard quantitative measure in topic modeling literature to compare the performance of various topic models [ 2 ] . A lower value of perplexity indicates better generalizability of the topic model . As shown in Table 2 , SCTM performs far superior than Corr LDA . i≤σ
D(wj ) j<i log D(wi,wj )+
Topic coherence : By approximating the user experience of topic quality on top σ words of a topic , [ 10 ] proposed that topic coherence ( TC ) can be measured as : T C(σ ) = . D(w ) is the document frequency of any word w , and D(wi , wj ) is the document frequency of wi and wj together . is a small constant to avoid log zero . Values closer to zero indicate better coherence . We have used σ = 5 to compute coherence of a topic . Table 3 contains the comparison between SCTM and Corr LDA which shows supremacy of the proposed approach .
Figure 3 : Convergence of inference : change in negative log likelihood with number of iterations of the sampling algorithm on Yahoo! News ( lower is better ) . SCTM converges faster and better . characters into small case . Blank spaces are treated as the delimiter between words . However , we have not used stemmer or POS taggers . We used the following parameter setting : α = 1 , η = 1 , τj = 0.01 , ιj = 0.1 ( ∀j ) , ν = 1 , λ = 10 , 1 = 1 , 2 = 4 , and Jd = Sd . For α , η we use uninformative value . For other parameters specific to SCTM , low held out data perplexity ( Table 2 ) and high training likelihood ( Figure 3 ) indicate that model is less sensitive to these hyperparameters . We used 300 topics and 20,000 vocabulary in our experiments .
Evaluation criteria : First we evaluate SCTM against Corr LDA in terms of fitness to the dataset . We use perplexity , topic coherence , topic diversity on all three datasets and convergence of the algorithm on Yahoo! News . Then we focus on the main question of this paper , by evaluating the performance for SCL on AT Science ( subset with goldstandard ) and a limited evaluation on Yahoo! News .
5.3 Comparison of SCTM and Corr LDA
We quantitatively evaluate SCTM in the task of modeling article comments dataset , comparing with the baseline Corr LDA . For this task , we use perplexity , topic coherence , diversity among discovered topics and convergence of the inference algorithm as evaluation metrics . The results are presented in Table 2 , 3 , 4 and Figure 3 .
Topic diversity : We want to evaluate the ability of SCTM in discovering diverse set of topics . We consider top 5 words for topics which have maximum probability for a word greater than 001 Then compute textual difference between topic i and topic j as T divij = 1 − |twi ∩ twj|/5 , where twi contains top 5 words for topic i . Averaging over all pairs we report in Table 4 . Note that higher value of T div is better and empirical results show that SCTM is superior .
Convergence of inference : We plot negative log likelihood of SCTM and Corr LDA against iterations in Figure 3 . Note that SCTM mixes much better than Corr LDA . Both the models , SCTM as well as Corr LDA , converge almost in the same time . However , as fitness of Corr LDA is worse , the likelihood of the dataset remains low throughout .
Discussion : Interestingly , SCTM outperforms Corr LDA in modeling aspects commonly used in topic modeling literature . The main reason behind this is that SCTM finds more unique topics , whereas Corr LDA finds relatively mixed topics by mixing up many unique topics . On the other hand , supremacy of SCTM over Corr LDA in topic diversity is expected as that is explicitly ensured in the modeling . Low perplexity on held out dataset and high likelihood on training data affirms that although SCTM is a far more complex model than Corr LDA , it is able to learn from the dataset appropriately .
5.4 Accuracy of SCTM over Corr LDA on SCL We evaluate the main task of this paper in this section , ie the task of discovering specific comments . We use the manually annotated gold standard from AT Science dataset for evaluation . Using Algorithm 1 we classify comments into specific and non specific ( general and irrelevant ) for both Corr LDA and SCTM . Using the gold standard we compute precision recall and report them in Table 5 . SCTM outperforms Corr LDA significantly .
020040060080014161822224x 106Number of Iterations− log likelihoodArticles CorrLDASCTM02004006008001112131415x 107Number of IterationsComments CorrLDASCTM Table 6 : Precision , recall and F1 score for aligning specific comments to the sentences . Precision Recall 0.442 0.038
Model SCTM Corr LDA
0.365 0.019
F1
0.400 0.025
Figure 4 : Sensitivity analysis : F1 score variation with respect to number of topics and vocabulary size for SCTM .
Similarly , from Algorithm 1 we get the article sentences corresponding to each specific comment detected by the models , for both Corr LDA and SCTM . Then using gold standard we compute precision , recall and F1 . The results are reported in Table 6 . SCTM is far superior than Corr LDA .
We can see that Corr LDA is unable to give any satisfactory result in discovering specific comments . This is due to the fact that there is one topic vector per article comments pair which fails to model specific comments properly .
Evaluation on Yahoo! News : Due to the unavailability of ground truths and the large number of comments in the dataset , we have done a limited evaluation on all articles which had upto 50 predicted specific comments ( a total of 604 articles out of 730 ) . Out of a total of 138,538 comments in the dataset , SCTM predicted 23,624 comments to be specific comments and the per document average accuracy is 63 % . In comparison , Corr LDA discovers only a total of 4,704 comments in the dataset which is even less than 50 % of the correctly discovered specific comments by SCTM .
Sensitivity analysis of SCTM : We analyse the sensitivity of SCTM towards number of topics and vocabulary size , two important hyperparameters , in Figure 4 . F1 score is observed to be not sensitive to the vocabulary size , however it gets better with the number of topics and stabilizes gradually . This is justified as we expect to get large number of topics due to diversity across the articles and comments .
6 . USE CASES OF SCL
Comments are a great source of public response . Whereas specific comments are more useful to mine information from comments . To demonstrate that SCTM is useful in a wide variety of applications , we do a detailed analysis of two specific articles . ( i ) Regarding President Obama ’s visit to Middle Eastern countries and ( ii ) a product review on Google Nexus . The first article was the most commented article in our Yahoo! News dataset and the second article was the most recent well commented article in ArsTechnica ’s Gadget section . The two articles demonstrate the efficacy of SCTM in the diverse scenarios of large and small number of comments , different categories of politics and product reviews
Figure 6 : Popularity of topics based on number of comments . The first topic ( gray colored ) corresponds to general comments . Except that , all other topics correspond to specific comments and are missed by the state of the art methods . and different hosting platforms of news and blogs . Note that , such analysis is subject to efficacy in SCL and hence is beyond the scope of the existing methods . 6.1 Analysing comments using topics discov ered by SCTM
We analyse the comments on the news reporting President Obama ’s visit to the Middle East during March 19 23 , 20139 . The event was widely covered in the media and was highly commented upon . Figure 5 shows excerpts from one such article at Yahoo! News . At the time of crawling , the article had 1919 comments . Analysing the comments one can accumulate a survey of opinion .
Note that the article in example touches upon various issues like Iranian nuclear program , Syria etc in different segments of the article . The number of such issues covered in the article is large but the amount of text contributing to each of them is small . Interestingly , many comments focus on such issues minorly described in the article .
Figure 5 shows some of the topics discovered by SCTM on this event and the comments corresponding to those topics . Sentences , topics and the comments are color coded such that they signify a link among them . Note that , SCTM is able to discover fine topics like “ Obama ’s MidEast Visit ” , “ Israel Palestine Peace ” and “ Iran Nuclear Dispute ” where Corr LDA mixes them together . SCTM is able to retrieve comments which are related to such topics described in specific parts of the article .
Using the ability to detect precise topics SCTM can analyze the public response quantitatively , on the basis of the topic being discussed in the comment . We categorize comments based on their major topic . Figure 6 shows the number of comments classified in this way into the different topics . As expected , we find that most of the comments are general and about the topic “ Obama ’s MidEast Visit ” ( gray colored ) . The remaining comments are specific and related to other topics . Among the specific comments , the most discussed topic is “ Obama Visit Expectations ” ( red colored ) with 195 comments marking the “ hot spot ” in the article ( box in Figure 5 ) .
9newsyahoocom/obama heads middle east lowexpectationshtml
50100150200250300025035045055065Number of TopicsF1 Score ClassificationAlignment5101520035045055065Vocabulary ( in thousands ) ClassificationAlignment0100200300Comment TopicNumber of Comments Obama ’s MidEast VisitObama Visit ExpectationsObama Spending , BudgetJew , Arab , IslamIsrael− Palestine PeaceAmerica ’s MidEast EffortsIran Nuclear DisputeUS Politics , ElectionsOthers Figure 5 : An example from Yahoo! News . SCTM is able to find issues under reported in the article and specific comments on them . SCTM also detects hot spot ( red box ) , although “ Obama Visit Expectations ” topic has appeared only in those two sentences , it has received the most number of comments leaving general ones . Corr LDA finds only one mixed topic failing to analyse user response effectively .
It is important to note that sentences in the main article related to a specific comment can be very few and may not be contiguous ( which can be seen in Figure 5 and Figure 1 ) . This demonstrates the elegance of SCTM in performing such a hard task of SCL . 6.2 Capturing surprising market trends
We analyse the comments on the review of a recently launched tablet , Google Nexus 7 , published in AT Gadget . An excerpt is given in Figure 1 . This article is one of the well commented reviews in the recent times with 180 comments at the time of crawling . In review articles , the story generally covers a wide range of aspects and many of them will be less described . However , people might be sensitive to a certain aspect which is difficult to guess a priori .
The author discussed only in approximately 8 % of the sentences the “ screen display quality ” , or more precisely , the comparison with “ iPad Retina display ” and the future prospect of “ iPad mini ” equipped with such retina display . SCTM correctly found that this point received a great deal of attention from users with 22 % of comments leaving the irrelevant ones . It is surprising because an article about a new Google tablet has attracted a large number of comments related to another tablet , “ iPad mini ” , which has not even been released . Examples of such specific comments can be seen in Figure 1 , all of these were correctly discovered by SCTM . We hope , this provides an important feedback to both Google as well as Apple . Note that none of the existing methods are capable to capture such trends . 6.3 Enhancing user satisfaction
We demonstrate two applications of SCL which can improve user experience . In these cases accuracy on SCL is critical and hence beyond the scope of the existing methods . the hot spots discovered by SCTM . Similarly , in Figure 1 the portions in green box are the hot spots in that article . SCL enables one to find such hot spots in articles automatically . This can help in summarization , advertisements , better user engagement etc .
The ability of SCTM in categorizing comments into various issues covered in the article and connecting with the sentences in the article is key to discover hot spots , where existing methods fail .
Comment cleaning : Comment cleaning is one of the important tasks today for online media sites . Being equipped with SCL , we propose a novel method of cleaning up irrelevant comments without using any external resource or supervision . Existing methods of comment cleaning focus on special features like presence of URL or certain keywords . However , even after filtering out such comments , there will be some comments which are absolutely irrelevant and in many cases indecent ( see Figure 7 ) . But it is difficult for existing methods to clean up such irrelevant comments as they look very much similar to the normal comments , many of them may contain words present in the article . Moreover , most of the existing methods are either supervised or semi supervised ( refer to [ 8 ] for a complete review ) or use text enrichment [ 14 ] .
The algorithm for detecting irrelevant comments is in Algorithm 1 . The comments shown in Figure 7 on two articles of AT Science10 are all examples of irrelevant comments correctly detected by SCTM , where existing methods failed .
We evaluated the performance of SCTM in finding irrelevant comments on 100 randomly selected articles from the Yahoo! News dataset . For each of these articles , the algorithm was applied to predict top 20 irrelevant comments
Hot spot detection : A hot spot is the set of sentences in the article which have received most attention in the comments . For example , in Figure 5 the portions in red box are
10arstechnica.com/science/2013/03/first planckresults the universe is still weird and interesting/ , arstechnica.com/science/2013/03/voyager probes keytransition remains mysterious/
Obama heads to Middle East with low expectationsWhen President Barack Obama steps into the Middle East's political cauldron this coming week , he won't be seeking any grand resolution for the region's vexing problemsHis goal will be trying to keep the troubles , from Iran's suspected pursuit of a nuclear weapon tothe bitter discord between Israelis and Palestinians , from boiling over on his watch"This is not about accomplishing anything now . This is what I call a down payment trip," said Aaron David Miller , an adviser on Mideast peace to six secretaries of state For much of Obama's first term , White House officials saw little reason for him to go to the region without a realistic chance for a peace accord between the Israelis and PalestiniansOfficials now see the lowered expectations as a chance to create space for frank conversations between Obama and both sides about what it will take to get back to the negotiating tableNetanyahu , in a speech to the United Nations in September , said Iran was about six months away from being able to build a bomb . Obama told an Israeli television station this past week that the US thinks it would take "over a year or so for Iran to actually develop a nuclear weapon"Traveling to the West Bank , Obama will meet with Palestinian Authority President Mahmoud Abbasand Prime Minister Salam Fayyad in Ramallah . Obama and Fayyad will visit a Palestinian youth center , another attempt to reach the region's young peopleObama will make a 24 hour stop in Jordan , an important US ally , where the president's focus will be on the violence in neighboring Syria . More than 450,000 Syrians have fled to Jordan , crowding refugee camps and overwhelming aid organizations . israel , obama , jews , peace , palestinians , jewish , israeli , land , president , eastArticle ( sentences are color coded by topic , red box is hot spot)Comments(color coded with parts of article and topic)I am not sure why he is going . To go under the gloom of 'low expectations' indicates a wasted trip . Though Obama seems more comfortable in some Middle Eastern coutnries than in US States . Sure spend more money we do not haveWho is John Galt?THEY have vexing problems.If you want to solve the Palestinian problem it's simple , create jobs and security for the people . When a man has a job and family to provide for , he doesn't think of war . He wants to live and let live . The Palestinian leadership are getting rich off the conflict between Isreal and Palestine! Arafat died with a BILLION DOLLARS IN THE BANK!!!I wonder if Netanyahu plays golf?This problem with Iran's nuclear weapons program should have been handled long before the tipping point of CrisisObama waited too long.obama , east , america , israeli , netanyahu , trip,americans , american , visit , countryisrael , peace , palestinians , land , palestinian , muslim , netanyahu , zionist , world , hate , israelisiran , israel , nuclear , world , military , iranian , peace , weapons , destroyer , attackexpectations , middle , obama , money , president,east , office , barry , hope , waste , biden , troublesmuslim , stay , middle , muslims , syrians , lebanon,east , egypt , arab , israel , obama , sunniObama Middle East VisitIsrael Palestine Peace TalksIran Israel Nuclear DisputeObama Vist ExpectationsSyria & Arab SpringCorr LDA Topic ( mixed topic)SCTM Topics ( color coded with article and comments)Obama Visit & Mideast Issues Figure 7 : Examples of irrelevant comments on two articles from AT Science ( showing only headline and irrelevant comments ) . Notice that these comments look normal and do not have any specific feature except lack of correspondence to the main article making it challenging for existing methods . These comments are correctly marked by SCTM to be irrelavnt , whereas the existing methods failed . which were manually annotated as correct or incorrect . The mean average precision at 20 ( MAP@20 ) was found to be 0.87 which is quite high . Moreover , most of the mistakes that the model makes fall in a grey area , where humans will differ in their opinion of the comment being irrelevant .
7 . CONCLUSION
In this paper we explored an interesting problem of specific comment location which is beyond the scope of the state of the art . A novel correspondence topic model , namely SCTM , has been proposed which admits an efficient collapsed Gibbs sampling algorithm . Following [ 3 ] the proposed inference can be easily modified to be scalable . On three real life datasets we evaluated SCTM against Corr LDA to demonstrate efficacy of the proposed approach . Finally , we demonstrated four different use cases with practical importance . We believe similar study can be done using SCTM on other datasets like image tag , paper bibliography etc .
8 . ACKNOWLEDGMENTS
We are thankful to all the reviewers for their valuable comments . The authors were partially supported by DST grant ( DST/ECA/CB/1101 ) .
9 . REFERENCES [ 1 ] D . M . Blei and M . I . Jordan . Modeling annotated data . In SIGIR , pages 127–134 . ACM , 2003 .
[ 2 ] D . M . Blei , A . Y . Ng , and M . I . Jordan . Latent
Dirichlet allocation . Journal of Machine Learning Research , 3:993–1022 , Jan . 2003 .
[ 3 ] K . R . Canini , L . Shi , and T . L . Griffiths . Online inference of topics with latent Dirichlet allocation . In AISTATS , volume 5 , 2009 .
[ 4 ] R . J . Connor and J . E . Mosimann . Concepts of independence for proportions with a generalization of the Dirichlet distribution . Journal of the American Statistical Association , 64(325):194–206 , 1969 .
[ 5 ] M . Das , S . Bhattacharya , C . Bhattacharyya , and
G . Kanchi . Subtle topic models and discovering subtly manifested software concerns automatically . In ICML , pages 253–261 , 2013 .
[ 6 ] Y . Hu , A . John , D . D . Seligmann , and F . Wang . What were the tweets about ? Topical associations between public events and twitter feeds . In ICWSM , 2012 .
[ 7 ] H . Ishwaran and L . F . James . Gibbs sampling methods for stick breaking priors . Journal of the American Statistical Association , 96(453):161–173 , 2001 .
[ 8 ] R . Kant , S . H . Sengamedu , and K . S . Kumar .
Comment spam detection by sequence mining . In WSDM , pages 183–192 . ACM , 2012 .
[ 9 ] Z . Ma , A . Sun , Q . Yuan , and G . Cong . Topic driven reader comments summarization . In CIKM ’12 , pages 265–274 . ACM , 2012 .
[ 10 ] D . Mimno , H . M . Wallach , E . Talley , M . Leenders , and A . McCallum . Optimizing semantic coherence in topic models . In EMNLP , pages 262–272 . Association for Computational Linguistics , 2011 .
[ 11 ] D . K . Sil , S . H . Sengamedu , and C . Bhattacharyya . Supervised matching of comments with news article segments . In CIKM , pages 2125–2128 . ACM , 2011 . [ 12 ] I . Titov and R . McDonald . Modeling online reviews with multi grain topic models . In WWW , pages 111–120 . ACM , 2008 .
[ 13 ] C . Wang and D . Blei . Decoupling sparsity and smoothness in the discrete hierarchical Dirichlet process . In NIPS , pages 1982–1989 . 2009 .
[ 14 ] J . Wang , C . T . Yu , P . S . Yu , B . Liu , and W . Meng .
Diversionary comments under political blog posts . In CIKM , pages 1789–1793 . ACM , 2012 .
[ 15 ] T . Yano , W . W . Cohen , and N . A . Smith . Predicting response to political blog posts with topic models . In NAACL , pages 477–485 . ACL , 2009 .
APPENDIX
Collapsing SBP random vector ρ .
From the relation between SBP and GD [ 4 ] we get that , if ρdas are constructed as equation ( 1 ) , then they are equivalently distributed as GD [ 7 ] . The density of ρda is :
Jd−1 daj ( 1 −j
ρτj−1 l=1 ρdal)κj fρda = j=1
B(τj , ιj )
( 10 ) where B(τj , ιj ) = Γ(τj )Γ(ιj )
1 −Jd−1
Γ(τj +ιj ) . κj = ιj − ιj+1 − τj+1 for j = 1 , 2 , . . . , Jd − 2 and κJd−1 = ιJd−1 − 1 . Note that , ρdaJd = l=1 ρdal . Note that , by setting ιj−1 = τj + ιj , 2 ≤ j < Jd , GD reduces to standard Dirichlet distribution . Like Dirichlet distribution , GD is also conjugate to the multinomial distribution , and hence we can integrate out ρ ’s and v ’s . If ρda ∼ GDJd−1(τ1 , . . . , τJd−1 , ι1 , . . . , ιJd−1 ) , and bdajs are sampled from mult(ρda ) , then the posterior distribution of ρda given ( bdal)s ( l = i ) is again a GD with density GDJd−1(¯τ1 , . . . , ¯τJd−1 , ¯ι1 , . . . , ¯ιJd−1 ) , where ¯τj = τj + −dai ˇm daj , ¯ιj = ιj +Jd l=j+1 ˇm
−dai dal
.
