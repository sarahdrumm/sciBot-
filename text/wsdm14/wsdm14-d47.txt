Nonparametric Bayesian Upstream Supervised
Multi Modal Topic Models
Renjie Liao
Department of Computer Science & Engineering
The Chinese University of
Hong Kong rjliao@csecuhkeduhk
Jun Zhu
State Key Lab of Intell . Tech &
Sys . ; TNLIST Lab
Dept . of Comp . Sci & Tech Tsinghua University , Beijing ,
100084 , China dcszj@mailtsinghuaeducn
Zengchang Qin
Intelligent Computing and
Machine Learning Lab
School of ASEE
Beihang University , Beijing ,
100191 , China zcqin@buaaeducn
ABSTRACT Learning with multi modal data is at the core of many multimedia applications , such as cross modal retrieval and image annotation . In this paper , we present a nonparametric Bayesian approach to learning upstream supervised topic models for analyzing multi modal data . Our model develops a compound nonparametric Bayesian multi modal prior to describe the correlation structure of data both within each individual modality and between different modalities . It extends the hierarchical Dirichlet process ( HDP ) through incorporating upstream supervised response variables and values of latent functions under Gaussian process ( GP ) . Upstream responses shared by data from multiple modalities are beneficial for discriminatively training and GP allows flexible structure learning of correlations . Hence , our model inherits the automatic determination of the number of topics from HDP , structure learning from GP and enhanced predictive capacity from upstream supervision . We also provide efficient variational inference and prediction algorithms . Empirical studies demonstrate superior performances on several benchmark datasets compared with previous competitors .
Categories and Subject Descriptors G.3 [ Probability and Statistics ] : Nonparametric statistics ; H33 [ Information Search and Retrieval ] : Retrieval models ; H.4 [ Information System Applications ] : Miscellaneous
Keywords Multi modal Learning ; Nonparametric Bayesian ; Topic Model ; Cross modal Retrieval
1 .
INTRODUCTION
Nowadays , large collections of data on the web consist of various modalities , such as images , texts , and audio or video clips . Extracting useful knowledge from these growing multimodal data has become increasingly important in many application areas . Multi modal learning , sometimes referred to as multi view learning [ 8 ] or multi field learning [ 26 ] , aims at modeling collections of such kind of data and making predictions when data of some modalities is missing . One typical example is to model pairs of images and associated texts ( eg , captions , paragraphs or articles ) , which lays the foundation of many valuable applications , including crossmodal retrieval , image annotation and so on . However , this multi modal learning problem is rather challenging , since it requires analyzing not only the characteristics of data in single modality but also the relevances of data across different modalities .
Previous works in this area have primarily focused on looking for latent representations shared by the multi modal data . Based on the different techniques they used , these works can be roughly divided into three categories : subspace learning , undirected probabilistic graphical models ( PGMs ) , and directed PGMs . Representative works of the first class include canonical correlation analysis ( CCA ) and its variants [ 31 , 28 ] . By maximizing the correlation between different modalities , it aims to find a low dimensional subspace representation for multi modal data . Another ones are based on distance metric learning [ 33 , 34 ] and hash function learning [ 39 ] , which try to discover optimal nonlinear subspace endowed with an expected similarity measure . For the second class , Markov random fields ( MRF ) based methods have commonly been used . For example , the dual wing harmonium model [ 35 ] describes the latent representations shared by images and texts through tying the latent variables of two basic harmonium models . The above two types of models are capable of effectively discovering the desired latent representations , but often lack intuitive explanations and do not perform well in terms of prediction .
The main line of the third class of work are multi modal topic models ( mmTM ) , such as multi modal latent Dirichlet allocation ( mmLDA ) [ 2 ] , correspondence latent Dirichlet allocation ( CorrLDA ) [ 2 ] and the multi modal aspect model [ 22 ] . Most of them describe the single modal data via standard topic models , like latent Dirichlet allocation ( LDA ) [ 4 ] . Thereafter , [ 36 ] extends the Dirichlet prior in mmTM by using a hierarchical Dirichlet process ( HDP ) [ 29 ] which is able to automatically choose the number of topics . These models introduce shared latent variables which either indicate the topic proportions as in mmLDA or the
493 indexes of topics as in CorrLDA , thus enforcing strong correlations between topics from different modalities . This kind of strong correlation is inappropriate since usually data in one modality contains a considerable amount of modalityprivate information that is unrelated to the other . Therefore , relying on the logistic normal distribution adopted in correlated topic model ( CTM ) [ 3 ] , authors in [ 26 ] relax this strong correlation and allow structure learning of the correlation matrix . Beyond that , the discrete infinite logistic normal ( DILN ) [ 23 ] distribution is utilized in [ 30 ] to further keep the private topics inside each modality . All the above mmTMs learn the latent representations of the multi modal data in an unsupervised manner and are able to offer intuitive probabilistic interpretations .
Recently , upstream supervised response variables , like categorical labels , which are available in many scenarios , are utilized for enhancing the performance of prediction . Upstream supervised models are common for understanding scenes in computer vision community [ 11 , 41 ] , since image categories are cheaply accessible on the web . In contrast to downstream supervised models , like supervised topic model and its variants [ 5 , 40 ] , upstream ones assume that response variables are directly or indirectly involved in generating latent variables . In the setting of multi modal learning , these supervised response variables are usually shared by data from different modalities , thus help describing the relation between data from different modalities precisely . Moreover , with this kind of supervised guidance , latent representations learned by the model are presumably more discriminative . For example , it has been shown in [ 24 ] that applying logistic regression to class label , is helpful for improving the predictive capacity of CCA in cross modal retrieval . At the same time , similar categorical information has been exploited in [ 8 , 7 ] to extend the MRF based multi view models through a supervised max margin approach and boosted predictive results are also demonstrated .
In this paper , we propose a novel nonparametric Bayesian upstream supervised ( NPBUS ) multi modal topic model by combining the above advantages of previous works . Specifically , our model first inherits the two merits of having an explicit probabilistic explanation and automatic determination of the number of topics from the HDP based mmTM . Then our model introduces a Gaussian process ( GP ) [ 25 ] to flexibly capture correlation structures both within each individual modality and between multiple different modalities . Moreover , upstream supervised response variables shared by data from different modalities are incorporated into a normalized gamma representation of HDP , thus making our method possess remarkable predictive ability . We also derive an efficient variational inference algorithm for training . The proposed NPBUS model demonstrates superior experimental results than various competitors in the predictive tasks of cross modal retrieval and image annotation .
The rest of the paper is structured as follows . Sec 2 introduces the background works related to our model . Sec 3 elucidates our NPBUS multi modal topic model . Sec 4.1 presents our variational inference and prediction algorithms . Sec 5 presents our empirical results on cross modal retrieval and image annotation . Finally , Sec 6 concludes .
2 . BACKGROUND WORKS
In this section , we review the hierarchical Dirichlet process ( HDP ) [ 29 ] based topic model for single modal data and the discrete infinite logistic normal ( DILN ) distribution [ 23 ] , which lays the foundation of our model to be presented later . Terminologies from text modeling , eg “ words ” , “ documents ” and “ vocabulary ” , are used throughout the paper , since they can be well generalized in modeling data of other modalities . For example , in the context of bag of words model for image classification , “ words ” , also referred to as “ visual words ” , are clustering centers of some low level visual descriptors ( eg SIFT [ 19] ) , and “ documents ” correspond to images . 2.1 Hierarchical Dirichlet Process Topic Mod els
We first introduce latent Dirichlet allocation ( LDA ) [ 4 ] which assumes that documents are represented as mixtures of latent topics η and words appearing in a document are drawn independently from their corresponding topics . Specifically , a topic mixing proportion θ for each document is first sampled from a Dirichlet prior and thereafter the topic for each word is chosen through sampling a topic index z ∼ M ult(θ ) , where M ult(· ) is a multinomial distribution . At last , word x is drawn from its corresponding topic ηz which is a multinomial distribution over the words in the vocabulary . Note that , in a fully Bayesian treatment , we may place a prior for topics η . To further model the topic mixing proportion , hierarchical Dirichlet process ( HDP ) [ 29 ] , a nonparametric Bayesian prior , is widely adopted . Formally , HDP is a Dirichlet process ( DP ) [ 13 ] that has another Dirichlet process as its base probability measure . In this paper , we focus on two level HDPs , though it may be extended to arbitrary levels . Its hierarchical representation is
G ∼ DP ( αG0 ) ˜G ∼ DP ( βG ) ,
( 1 ) where G0 is the aforementioned Dirichlet prior for η , and α and β are the first and second level concentration parameters respectively . The main advantage of HDP formulation is the automatical determination of the number of topics owing to its nonparametric nature . Moreover , due to the almost sure discreteness property of DP [ 1 ] , the HDP prior enables the implicit sharing of atomic probability measures between the repeatedly sampled topic proportions θ . And it will largely ease our work of specifying multiple priors , since there will be multiple different sets of topics in the multi modal setting . 2.2 Discrete Infinite Logistic Normal ( DILN )
Distribution
Though HDP is a popular prior for modeling topic proportions , it is insufficient to capture the correlation structure between different topics since its construction is a random measure which means all random variables as well as all summations of subsets are independent . To overcome this weak point , discrete infinite logistic normal ( DILN ) model [ 23 ] is proposed . It takes advantages from both HDP and CTM , and offers an effective prior for structure learning of topics . Specifically , DILN first introduces an auxiliary variable for each topic η . It can be interpreted as the latent location of a topic in some latent space . As and η are associated , the base distribution used in the first level HDP is augmented as a product measure G0 × L0 , where G0 is still the prior distribution for topics and L0 is a distribution over latent locations . To construct DILN , a hierarchical sampling process is
494 )m ( t(cid:75 )
)m tv (
( cid:102 )
(
)m
(
)m
) ) i jx( i j,x m ( ,
) i jz m ( ,
(
)mN
,i tC
( cid:102 )
)m
( i(cid:84 )
(
)m if iy
M
D
K
Figure 1 : A graphical illustration of our NPBUS model . The red rectangle ( single dot dashed lines ) indicates the part of GP , the green rectangle ( dashed lines ) indicates the part of HDP topic model and the blue rectangle ( double dots dashed lines ) indicates the part of upstream supervised response variable . implemented as HDP . In the first level , a random measure G is drawn from Dirichlet process , G ∼ DP ( αG0×L0 ) . And in the second level , a random measure ˜G is drawn from another DP , ˜G ∼ DP ( βG ) . Meanwhile , a random function f ( ) is drawn from a Gaussian process , f ( ) ∼ GP ( μ( ) , k( , ) ) , where f ( · ) and GP are defined on the latent location , μ and k are the mean function and kernel function respectively . Finally , a sample from DILN , ie , the topic mixing proportion , is constructed via multiplying the random measure ˜G by the exponentiated value of the random function f ( ) . Hence , a topic assignment z could be drawn as ,
. z ∼ exp(f ( ) ) ˜G .
( 2 )
Since is explicitly correlated via a GP , topic proportions drawn from DILN are also correlated . And , due to the discreteness of ˜G , the sampled topic mixing proportion is discrete .
3 . NPBUS MULTI MODAL TOPIC MODEL We now develop the nonparametric Bayesian upstream supervised ( NPBUS ) multi modal topic model and elaborate its merits in modeling multi modal data . Without loss of generality , we focus on the two modal dataset which contains collection of images and texts . We emphasize that the generalization to more and different modalities is straightforward . For clarity , we denote the two modal observable dataset by X = {x , yi|i = 1 , 2 , , D , m = 1 , 2} , where and yi are the ith word of mth modality and ith rex sponse variable respectively , and D is the size of the dataset . Note that in our problem setting , each pair of image and text shares an extra response variable which plays an supervision role . Here we highlight the key challenges dealing with such a type of dataset : ( i ) modeling correlations of topic proportions both within modality and between different modalities ; and ( ii ) exploiting upstream supervising information of response variable . Then we will explain in detail our NPBUS model and how it tackles the above challenges .
( m ) i
( m ) i
3.1 Compound Nonparametric Bayesian Multi
Modal Prior
In the context of multi modal learning , words in each modality are drawn independently from the modality specific topic given their corresponding topic assignments . For the ith document of mth modality , we denote the vector of topic mixing proportion as θ and the probability of choosing ( m ) i,t . The topic assignment of the jth the tth topic is thus θ ( m ) i,j and the auxiliary variable of tth topic appeared in DILN is f is the value of a random function modeled by GP and we omit its input latent location t here and clarify the reason later . is defined as z
. Note that , f word x
( m ) i,t
( m ) i,t
( m ) i
( m ) i,j
Similar with HDP topic models , the first level Dirichlet process of our model is represented via a stick breaking process [ 27 ] ,
( m )
( m ) t ∼ G0 η .t−1 t ∼ Beta(1 , α ˜v ( m ) ( m ) t = ˜v v t j=1
∞fi
G = t=1
( m ) v t
δη(m ) t
,
( m )
) ( 1 − ˜v
( m ) j
)
( 3 )
( m ) where G0 is the Dirichlet prior for topics , v proportion and Beta(· ) is a beta distribution . t is the stick
In the second level , we first draw a random measure ˜G from another Dirichlet process , DP ( β(m)G ) . Note that we here focus on presenting our prior and leave the detailed construction of this DP in next section . As aforementioned , different topics are just weakly correlated owing to the normalization property of probability measure ˜G . To capture flexible correlation structures , we then concatenate the auxiliary variables from each modality into an single vector , like below , fi = [ f
( 1 ) i,1 , f
( 1 ) i,∞ , f
( 2 ) i,1 , f
( 2 ) i,∞ , , f
( M ) i,1 , , f
( M ) i,∞ ] .
( 4 )
( m ) it
Then we use a GP to model the whole vector fi , which is similar with what [ 26 ] and [ 30 ] have done . Finally , as in DILN , we could multiply the samples from G by the exponentiated value of f , which fulfills the introduction of correlation to topic proportions . Specifically , in the covariance matrix of GP , the diagonal blocks describe the correlation of topic proportions within the same modality , and off diagonal ones describe the correlation of topic proportions between different modalities . Note that , to sample f , we merely need ( m ) to sample fi and obtain f immediately according to its it position in the whole vector .
( m ) it
We now turn to model the upstream supervised response variable y which can be categorical labels , rating scores and so on . Here we only consider the discrete case , ie , y ∈ {1 , 2 , , K} . Since these responses induce a supervised grouping of multi modal data , we could conduct correlation structure learning within each group . Therefore , in this way , the whole multi modal dataset would be modeled more precisely and the generated topic proportions would be more discriminative . Specifically , we introduce another random scaling factor Ci,t which is drawn from a gamma distribution Gamma(at,yi , bt,yi ) . Then we divide exponentiated value ( m ) exp(f i,t ) by Ci,t to obtain a response dependent random scaling factor . By doing so , the compound nonparametric
495 Bayesian multi modal prior distribution of topic proportions is formulated as ,
∞fi
( m ) i ∝
θ exp(f
( m ) i,t ) t=1
Ci,t
˜G .
( 5 )
Now the correlation structures of multi modal data is cap(m ) tured in the weight term exp(f i,t ) . And , multi modal data within the same group , ie , having the same value of yi , would share the same hyperparameters at,yi and bt,yi . Thus the group specific random weight Ci,t has introduced the upstream supervising information . As a result , with this prior at hand , we have solved the two challenges mentioned above . 3.2 Normalized Gamma Representation
Following the normalized gamma process construction of HDP in [ 23 ] , we now develop a representation of our NPBUS multi modal topic model . It will be clear that this kind of representation incorporates the correlation structure via the second parameter of gamma distribution and simplifies our posterior inference .
First of all , a normalized gamma process construction of the second level HDP could be expressed as ,
( m )
( m ) v t
, 1 ) i,t ∼ Gamma(β ( m ) ˜θ i,t'∞ ( m ) ˜θ
∞fi
( m ) i =
θ t=1 j=1
( m ) ˜θ i,j
'∞
δη(m ) t
.
( 6 ) where the normalizing constant is almost surely finite [ 23 ] . Building upon Eq ( 6 ) , a topic mixing proportion θ drawn from the second level of our prior could be constructed below ,
( m ) i j=1
( m ) ˜θ i,j
( m ) i,t ∼ GP ( μ , K ) , f Ci,t ∼ Gamma(ayi,t , byi,t ) , i,t ∼ Gamma(β ( m ) ˜θ i,t'∞ ( m ) ˜θ
∞fi
( m ) θ i =
( m ) v t
δη(m )
( m )
. t=1 j=1
( m ) ˜θ i,j t
, exp(−f
( m ) i,t )/Ci,t ) ,
( 7 ) where the third line is derived by absorbing the scaling fac(m ) tor exp(f i,t )/Ci,t of Eq ( 5 ) into the second parameter of gamma distribution .
Once the topic mixing proportion θ is obtained , we are capable to generate multi modal data . The overall generative process of our model is summarized below : • For each modality m = 1 , , M
( m ) i
• For each latent topic t = 1 , 2 , , ∞ draw {v
( m ) t
( m ) , η t
} according to Eq ( 3 )
• For each document i = 1 , 2 , , D according to Eq ( 7 )
( m ) draw θ i • For each word j = 1 , 2 , , N ( m ) draw z draw x
( m ) i,j ∼ M ult(θ i,j ∼ M ult(η
( m )
( m ) i
)
( m ) z(m ) i,j
)
Graphically , the PGM of our model is illustrated as in Fig 1 in which we separately annotate the part of HDP topic model , the part of GP prior and the part of upstream supervised response variable . Here we omit the prior G0 of topics and hyperparameters of Ci,t in the figure for simplicity . As can be seen from the figure , both within modality and between modality correlation of topic proportions are captured and the upstream supervised response variable is also exploited .
3.3 The Correlation Structure
We now exploit the correlation structure of topic proportions in our model . Owing to the normalized gamma process representation above , we could analytically calculate the first two order central moments and the covariance of ( m ) the unnormalized topic proportions ( ie , ˜θ i,t ) . These moments essentially contain the desired correlation structures , since the correlation matrix can be obtained by merely normalizing the covariance matrix . The mean function μ of GP is assumed to be zero in deriving the following equations . After some integration procedures , we could obtain the results below ,
( m )
( m )
E[˜θ i,t |Θ ] = i,t |Θ ] = ayi,t
V[˜θ ( m ) β(m)v t byi,t
( m ) β(m)v t byi,t ayi,t eK(m,m ) t,t
/2 ff
( m ) β(m)v t byi,t ayi,t
( 1 −
1 eK(m,m ) t,t
) e2K(m,m ) t,t
1 + i,s |Θ ] =
( n )
( m ) Cov[˜θ i,t , ˜θ ( m ) β(m)β(n)v t byi,tbyi,s t,t s,s
)/2
( m ) t
+K(n,n )
( n ) v s ayi,tayi,s
( eK(m,n ) e(K(m,m ) t,s − 1 ) ( 8 ) , at,yi , bt,yi , K|t = where the conditional set is Θ ={β(m ) , v 1 , 2 , , ∞} . Recall that the diagonal block sub matrices of K denote the correlation of topic proportions within each modality and off diagonal ones denote the correlation between different modalities . To make expressions clear , we use the superscripts of K to denote the row and column indexes of block sub matrices in K . For example , K(1,1 ) may denote the covariance matrix of topics from image modality , and K ( 1,2 ) may denote the covariance matrix between topics of image modality and text modality . Then we use the subscripts of K to denote the row and column indexes in the corresponding sub matrix .
Note that the correlation structures shown in the above equations are distinct from ones in [ 26 ] and [ 30 ] . As dis(m ) cussed in [ 23 ] , the term v indicates how sparsity is ent forced in the first level DP . Moreover , in our model , it is clear that the covariance depends both on the kernel matrix K of GP and hyperparameters at,yi and bt,yi of gamma distribution . We could control the correlation structure flexibly through learning these parameters .
4 .
INFERENCE AND PREDICTION
In this section , we focus on the computational problems of our NPBUS model and present an efficient variational posterior inference algorithm and a corresponding prediction algorithm .
496 4.1 Variational Inference
To infer the posterior distributions of the latent variables , especially the unnormalized topic mixing proportions ˜θ , and to learn the model parameters , we employ a truncated meanfield variational inference algorithm , which has been shown to be effective in dealing with nonparametric Bayesian models [ 29 ] . First of all , we denote the previous set of observed variables as X and the set of all latent variables as V . By introducing a variational distribution q(V ) , we can write the general variational lower bound for the log evidence log p(X ) as
L = Eq[log p(V , X ) ] − Eq[log q(V ) ] ,
( 9 ) where Eq[· ] means the expectation is calculated with respect to the distribution q . According to the PGM in Fig 1 ,
V = {Ci,t , α(m ) , β(m ) , v i,t , fi,t , μ , K| m = 1 , 2 , i = 1 , , D , j = 1 , , N ( m ) , t = 1 , , T} ,
( m ) i,j , ˜θ
( m ) , η t
( m ) t
, z
( m )
( m ) ( m ) T = 1 ) = 1 , and ˜v T
( 10 ) where N ( m ) is the maximum number of words per document in mth modality . T is the truncation level , which means q(˜v is defined as in Eq ( 3 ) . The joint probability distribution p(V , X ) can be obtained according to the generative process mentioned before . As for the variational distribution q(V ) , it can be factorized according to the mean field assumption ,
M
D
N ( m )
T q(V ) = m=1 ( m ) q(η t i=1
)q(Ci,t)q(z j=1 ( m ) i,j )q(˜θ t=1 q(α(m))q(β(m))q(v ( m ) i,t )q(fi)q(μ)q(K ) .
( m ) t
)
( 11 ) Note that here we exploit the variational distribution of ( m ) fi for approximation . And q(f i,t ) thus can be obtained straightforwardly via marginalization of q(fi ) , due to the construction in Eq ( 4 ) . Next , distributions in the right hand side of Eq ( 11 ) are further defined as ,
( m ) q(η t
) ∼ Dir(π i,j ) ∼ M ult(φ
( m )
( m ) t
) , ( m ) i,j ) , q(z q(fi ) ∼ N ormal(˜μi , diag(˜σi) ) , q(Ci,t ) ∼ Gamma(˜ayi,t , ˜byi,t ) , ( m ) q(˜θ i,t ) , i,t ) ∼ Gamma(ˆa
( m )
( m )
( m )
)q(β
( m ) q(α )q(v t q(μ)q(K ) = δμ · δK ,
( m ) i,t , ˆb ) = δα(m ) · δβ(m ) · δv(m ) t
( 12 )
, where δ(· ) is the Kronecker delta function and is used for tractability of inference as in [ 17 ] . Moreover , Dir(· ) and N ormal(· ) are the Dirichlet distribution and multivariate normal distribution respectively . Here ˜σi is a vector and q(fi ) thus has a diagonal covariance matrix .
Now we have set up the variational lower bound , which up to a constant is equivalent to the negative KullbackLeibler ( KL ) divergence of the true posterior of latent variables p(V |X ) from the approximated variational distribution q(V ) [ 15 ] . We now derive a coordinate ascent algorithm for maximizing the lower bound by taking derivatives of Eq ( 9 ) with respect to variational parameters . Since our variational inference algorithm is partly related to the one given by [ 23 ] , here we only list the different updates . Specifically , the update equation or gradients of variational parameters are listed as below :
( ˜μi − μ ) t,t − 1 ˜σi,t
= λi − γ − K−1 = − 1 D . 2 δ(yj = i){log
( λi,t + K−1
=
)
∂L ∂ ˜μi ∂L ∂ ˜σi,t
∂L ∂ai,t j=1 ai,t˜bi,t bi,t =
∂L ∂˜ai,t
=
∂L ∂˜bi,t
=
˜ai,t
D . D . j=1 j=1
δ(yj = i )
δ(yj = i ) byj ,t ˜byj ,t
+ ψ(˜ayj ,t ) − ψ(ayj ,t)} fi 1 − ( γt + ˜ayj ,t)ψ . ⎧⎨ ⎩ γt − ˜ayj ,t − λyj ,t
˜byj ,t
+
( ˜ayj ,t ) − byj ,t ⎫⎬ ˜byj ,t ⎭ , byj ,t
˜ayj ,t ˜b2 yj ,t
+
'
λyj ,t ˜ayj ,t
( 13 )
.
( · ) are the digamma function and trigamma where ψ(· ) and ψ function respectively . λi is a vector which has the same size as fi . And its tth element is given as ,
λi,t = exp(−˜μi,t +
1 2
˜σi,t )
ˆayi,t˜byi,t ˆbyi,t˜ayi,t
.
( 14 )
γ is a vector which is also of the same size as fi and similarly constructed as in Eq ( 4 ) ,
( 1 )
( M )
( M ) v 1
( 1 ) v 1 , . . . , β
( 1 )
( 1 ) v T , . . . , β
γ = [ β
] . ( 15 ) Based on above derivations , we thus are able to perform efficient gradient based techniques , eg Newton Raphson , stochastic gradient descent , to find the optimized variational parameters .
, . . . , β
( M ) v T
( M )
To update the parameters of GP , we maximize the marginal likehood , which provides equations of μ and K :
˜μi
Dfi Dfi i=1 i=1
μ =
K =
1 D
1 D
{(˜μi − μ)(˜μi − μ )
T
+ diag(˜σi)} ,
( 16 )
We update the kernel matrix directly rather than the kernel function , since the latter one will increase the computational burden by inferring latent locations which are as many as latent topics of both image and text modality . Moreover , in this way , low rank approximations of the gram matrix K , like Nystrom approximation [ 25 ] , could be applied for speeding up the inference . 4.2 Prediction
( m ) j
Prediction tasks under the upstream supervised multimodal setting usually involve two objectives , ( i ) given test|m = 1 , 2 , j = 1 , , N} , preing multi modal samples {¯x dicting their corresponding response variables {¯yj} ; ( ii ) given testing samples of one modality {¯x j } , predicting samples j } and the corresponding reof the missing modality {¯x sponse variable {¯yj} . Here we focus on achieving the second objective , since ( ii ) is generally harder than ( i ) and is more closely related to the scenarios of cross modal retrieval and image annotation .
( 2 )
( 1 )
497 |¯x
Specifically , the aim of ( ii ) is to obtain the predictive posterior distribution p(¯yj , ¯x , X ) , where X is the aforementioned set of training dataset . Relying on the Bayes theorem , the desired probability could be expressed as , , ¯yj , V )p(¯yj)dp(V |X ) , p(¯yj , ¯x
, X ) =
(
|¯x
|¯x p(¯x
( 2 ) j
( 1 ) j
( 2 ) j
( 1 ) j
( 2 ) j
( 1 ) j
( 17 ) where V is the set of all latent variables as in Sec 4.1 and p(¯yj ) is the prior of response variable . Note that , in the training process , response variable is observable , whereas it is a latent variable to be inferred during testing . In our experiments , we just use the empirical distribution of y in the training dataset . Analogous to the empirical approximation of MCMC sampling , we calculate the predictive distribution using variational posterior as , , X ) ≈ Eq∗ [ p(¯x
, ¯yj , V )]p(¯yj ) , p(¯yj , ¯x
( 18 )
|¯x
|¯x
( 2 ) j
( 1 ) j
( 2 ) j
( 1 ) j
∗ where q ( V ) stands for the approximated variational distribution inferred during training process according to Sec 41 , ¯yj , V ) ] , we could first infer
To compute Eq∗ [ p(¯x
|¯x
( 2 ) j
( 1 ) j the topic mixing proportion vector θ
( 1 ) j and the vector of
( 1 ) auxiliary latent variables f through maximizing a lower j bound as in Eq ( 9 ) . Then , based on the conditional expectation of multivariate Gaussian distribution , we can obtain the auxiliary variables for the missing modality as ,
( 2 ) f j = μ
( 2 )
+ K
( 2,1 )
( K
( 1,1 )
−1
)
( 1 ) j − μ
( f
( 1 )
) ,
( 19 ) where μ(1 ) and μ(2 ) are factorized from μ according to the construction in Eq ( 4 ) , and superscripts of K denote the row and column indexes of its block sub matrices as in Sec 33 ( 2 ) With f at hand , we now are capable to calculate the exj pectation .
( 2 ) j
Further more , if we are only interested in estimating ¯yj , in the situation of cross modal retrieval , we can inteeg grate ¯x from the predictive posterior distribution Eq ( 18 ) which provides the maximum a posterior ( MAP ) estimation . When the prior for ¯yj is a uniform distribution , MAP is equivalent to the maximum likelihood estimation which is what common upstream models [ 11 , 41 ] do . Therefore , we could make our prediction algorithm flexible and powerful by imposing a proper application dependent prior for response variable . In turn , ¯x could be obtained in a similar manner for situations where ¯yj is less cared , like image annotation .
( 2 ) j response variable of category label is offered for each training pair . These labels cover 10 semantic categories , like art , biology , music and so forth . For the experiments on image annotation we use the public Corel5K [ 10 ] as the benchmark dataset , which contains around 5,000 images that are only accompanied by 1 to 5 annotations of keywords . We use a fixed set of 499 images for testing and the rest for training , following the setup in [ 14 ] . Since there is no human labeled category information for training data , we cluster the tags through hierarchical Dirichlet process ( HDP ) topic model and automatically find 21 clusterings in total . Then the obtained clustering labels are regarded as response variables . Note that tags for each image are represented as a vector of words distribution on the vocabulary and response variable adopts 1 of W vector representation . Besides , all visual features and ground truth annotated text tags are available on the web2 . 5.2 Correlation Structure
We first investigate how our NPBUS model chooses topics and how topics learned by our model are correlated on the wiki dataset . First , we demonstrate the stick propor(m ) tions v obtained through inference . We notice that , in t Fig 2 , the stick proportions of image vary differently with ones of text , which motivates our incorporation of separate HDPs for the modalities . Also , from Fig 2 , we see that the last stick proportions are close to zero which justifies the truncation level T = 100 is sufficient .
0.04
0.035
0.03 n o i t r o p o r P k c i t
S
0.025
0.02
0.015
0.01
0.005
0
0 image text
10
20
30
40
50
60
70
80
90
100
5 . EXPERIMENTS
In this section , we present in detail the experiments for evaluating the performance of our NPBUS model in capturing correlation structures of multi modal data and its predictive capacity . 5.1 Data & Experimental Settings
We evaluate the performance of our model on two predictive tasks—cross modal retrieval and image annotation . For the first one , we use the public wiki dataset1 contributed by N . Rasiwasia et al . [ 24 ] . The dataset consists of 2,866 image text pairs which are collected from 2,700 articles selected and reviewed by Wikipedia , of which 2,173 pairs are randomly chosen to be the training set and the other 693 image text pairs are chosen to be the test set . Moreover , a 1 http://wwwsvclucsdedu/projects/crossmodal/
Figure 2 : Visualization of stick proportions learnt from our model for image modality ( red ball ) and text modality ( green triangle ) .
Next , we turn to illustrate the learned correlation structures of topics . For saving space , we only show the correlation matrix between topics of image modality and topics of text modality in Fig 3 . We can see that some correlation values are negative which may suggest absence of specific topics from the other modality . Moreover , only a few topics of image are strongly correlated with topics of text . This is as expected , since every pair of image and text in this wiki dataset is extracted owning to their close positions on the webpage which does not imply close similarity of their contents . To further inspect whether the correlation learned by
2 http://learinrialpesfr/people/guillaumin/data php
498 our model matches our own intuition , we are supposed to present the content of a pair of correlated topics . However , it is not intuitive for visualize the topics of image , since they are distributions over SIFT vocabulary . In order to overcome this problem , we adopt the same visual relevance measurement ρ as in [ 30 ] . For each topic of text modality , ρ is defined to be the mean value of absolute correlations with all image topics . Note that , this measurement captures the co occurrences of text topics and combined image topics . Then we can rank topics according to its value of visual relevance . In Table . 1 , ranked topics and their text contents are summarized . We see that topics with strong visual relevance , ie , large values of ρ , contain clear and concrete visual counterparts . For example , the first topic is about birds and their living environments , which are easily depicted by pictures . Nevertheless , topics with weak visual relevance are difficult to be described by visual contents , like the fifth topic in the last row . Therefore , our NPBUS model does capture the correlation structures of multi modal data .
0
0
10
20
30
Image Topics 40 60
50
70
80
90
100 i s c p o T t x e T
10
20
30
40
50
60
70
80
90
100
0.01
0.005
0
−0.005
−0.01 e r o c S P A M
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0 e r u t c e t i h c r a & t r
A
Ours SCM n o i t a e r c e r
& t r o p S e r a f r a W y t i l i b o n & y t l a y o R i c s u M i a d e M y g o o B i l l s e c a p & y h p a r g o e G y r o t s H i e r t a e h t
& e r u a r e t t i
L
1
2
3
4
5
6
7
8
9
10
Figure 4 : MAP scores per category with text query . e r o c S P A M
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0 e r u t c e t i h c r a & t r
A y g o o B i l l s e c a p & y h p a r g o e G y r o t s H i e r t a e h t
& e r u a r e t t i
L
Ours SCM e r a f r a W n o i t a e r c e r
& t r o p S i c s u a M d e M i y t i l i b o n & y t l a y o R
1
2
3
4
5
6
7
8
9
10
Figure 3 : Visualization of correlation structure between topics of image modality ( columns ) and topics of text modality ( rows ) .
Figure 5 : MAP scores per category with image query .
5.3 Cross Modal Retrieval
Table 2 : MAP scores of cross modal retrieval
Image Query
Method SCM [ 24 ] GMA [ 28 ] CMTC [ 38 ] MLBE [ 39 ] NPBUS
Text Query
Average
0.277 0.272 0.293 0.381 0.408
0.226 0.232 0.232 0.496 0.544
0.252 0.253 0.266 0.439 0.476
We then conduct cross modal retrieval which contains two sub tasks—text retrieval through image queries and image retrieval through text queries . We first execute our variational inference algorithm for the training dataset . Then , given a query in one modality , we predict its corresponding response variable y by the prediction algorithm mentioned in Sec 42 Finally , we use the probability vector p(y ) to calculate the ranking of all retrieved data . More specifically , the response variables of training data are encoded in 1 of K vector representation and here K equals 10 since there are 10 categories . Therefore , the distance between two response variables is established as distance of two vectors in vector space . Note that we experimented with different distance measures , including L1 , L2 , normalized correlation ( NC ) and chi squared χ2 distance , and we found that the L1 distance outperforms all others . To make the comparison fair , we take the same feature settings as in [ 24 ] , that is , we use topic mixing proportions to represent text documents and use bag of SIFT features to represent images . In the training stage , we set the initial values of two concentration parameters in HDP as : α = 15 and β = 5 , and the truncation level as T = 100 for each modality . 34 and 66 topics are learnt automatically for training texts and images , respectively . We compare our NPBUS model with semantic correlation matching ( SCM ) [ 24 ] , generalized multi view analysis ( GMA ) [ 28 ] , cross modal topic correlations ( CMTC ) [ 38 ] and multi modal latent binary embedding ( MLBE ) [ 39 ] . The performance is measured with mean average precision ( MAP ) which has good discrimination and stability and is widely used in the literature of information retrieval [ 9 ] .
The overall MAP scores are reported in Table 2 . It is clear from this table that our NPBUS outperforms other models
499 Topics with top 5 visual relevances population species north birds forest area males females found breeding trees year largest high conservation film music production movie scene play musical studio role american sound hollywood pictures performance ship navy fleet naval guns war sea british battleship royal squadron world hitler enemy iowa coast aircraft king family george prince life wife duke earl father died royal princess marriage lord son daughter married book works published novel story life writing critics stories popular characters literary history author volume
Topics with least 5 visual relevances up base out hit second single two followed third cardinals left home inning pearl fly field down yankees through wall sculpture neilston placed although models agricultural model turrets flight entire having computer gold richard california order bangladesh nails amp due entire jews raised hotel videos 1976 far observer hours canadian adams states united tournament international revolutionary gold winter world people old historical people female eggs number american african minnesota many census 2001 chicks days rate around home life
Table 1 : Visualization of topic contents . Each row of words indicates a topic . From top row to bottom row , the corresponding visual relevances are decreasing . Words from left to right in each topic are ranked in descending order according to their probabilities .
Query Image
Image corresponding to the top retrieved text
Table 3 : Comparison of performance for image annotation
Methods CorrLDA [ 2 ] CRM [ 16 ] InfNet [ 21 ] NPDE [ 37 ] SML [ 6 ] MBRM [ 12 ] TGLM [ 18 ] MSC [ 32 ] JEC [ 20 ] TagProp [ 14 ] NPBUS
P 6 16 17 18 23 24 25 25 27 33 29
R 9 19 24 21 29 25 29 32 32 42 44
N+ 59 107 112 114 137 122 131 136 139 160 187
Figure 6 : Examples of cross modal retrieval with image query . Images corresponding to retrieved texts with top 4 rank scores are shown . in both sub tasks on the wiki dataset . This improvement of performance validates the benefits of incorporating supervising information of semantic abstracts . Note that the boost of performance is larger in sub task with text query than the one with image query . It is perhaps because that the SIFT feature is not expressive enough for generally describing visual objects with semantic meaning . Since only SCM published their per category MAP scores , we compare the histograms of both text query and image query with SCM in Fig 4 and Fig 5 respectively . And our NPBUS achieves better results for almost all categories . More instances of cross modal retrieval are demonstrated in Fig 6 and Fig 7 . We consider the top 4 ranked retrieved texts for image queries in Fig 6 . And in Fig 7 , we show the top 5 ranked images for text queries . Note that , for the ease of display , we only show the images corresponding to the retrieved texts in Fig 6 and contents of text queries have been fragmented in Fig 7 . 5.4 Image Annotation
In experiments of image annotation , variational inference is first conducted to obtain the approximated variational distribution for training dataset . For testing images , we compute the desired conditional probability of tags given response variables and image feature according to the prediction algorithm . Following the aforementioned convention , captions with first 5 highest conditional probabilities are drawn as the final annotation results . As for image features , we exploit the same types of ones as in [ 14 ] , and normalize them by their L1 norm separately before the combination step . Since the overall features they used are of more than 30,000 dimensions , we reduce the dimension to 500 by principle component analysis ( PCA ) for computational efficiency . Moreover , 3 standard measures are adopted as in [ 6 ] : the mean precision per word ( ie , P ) , the mean recall per word ( ie , R ) and number of keywords with non zero recall value ( ie , N+ ) . We compare our NPBUS with 10 other methods which publicly report their results on this dataset . The experimental comparisons are listed in Table 3 . From this table , we can find that NPBUS ranks 2nd with respect to P and achieves the best results in terms of R and N+ . The causes for less precision than the state of art may be the reduced dimension visual features used in our experiments . Also , since some training images have too few attached tags , the topic model may not perform better compared to some simple model , like logistic regression in [ 14 ] . Finally , the supervising information obtained via unsupervised HDP clustering is somewhat limited .
500 On 31 January , the effort to retake the city began anew . The attack was launched at 08:30 hours , and was met by inaccurate Iraqi fire which knocked out two Saudi V 150 wheeled vehicles.Stanton , p . 9 , claims that two vehicles were destroyed , while Westermeyer , p . 31 , claims that three were knockedout . The 8th battalion of the Saudi brigade was ordered to deploy to the city by 10:00 hours , while 5th Battalion to the north engaged another column of Iraqi tanks attempting to reach the city . The latter engagement led to the destruction of around 13 Iraqi tanks and armored personnel carriers , and the capture of 6 more vehicles and 116 Iraqi soldiers , costing the Saudi battalion two dead and two wounded……
''Honoured members : the Hockey Hall of Fame'' , p . 91 . On March 30 , 1993 , it was announced that Gil Stein , who at the time was the president of the National Hockey League , would be inducted into the Hall of Fame . There were immediate allegations that he had engineered his election through manipulation of the hall's board of directors . Due to these allegations , NHL commissioner Gary Bettman hired two independent lawyers , Arnold Burns and Yves Fortier , to lead an investigation . They concluded that Stein had "improperly manipulated the process" and "created the false appearance and illusion" that his nomination was the idea of Bruce McNall……
Figure 7 : Two examples of cross modal retrieval with text query . Left parts are fragmented queries and right parts are corresponding retrieved images with top 5 rank scores . The content of the top text describes a war and the below one is about hockey .
6 . CONCLUSIONS
In this paper , we have presented a nonparametric Bayesian upstream supervised ( NPBUS ) multi modal topic model . Our NPBUS model allows flexible learning of correlation structures of topics within individual modality and between different modalities . And it becomes more discriminative via incorporating upstream supervising information shared by multi modal data . Last , it is capable to automatically determine the number of latent topics in each modality . We also devise efficient variational inference and prediction algorithms . Extensive experiments demonstrate the above advantages in terms of cross modal retrieval and image annotation .
In future work , we intend to develop truncation free algorithms to improve our approximated inference , or utilize sampling methods , like MCMC . And we also will exploit low rank approximations to accelerate the kernel learning . Moreover , we will apply our model for mining more different kinds of multi modal data , eg , multi lingual webpages , multi source perception data of robotics .
7 . ACKNOWLEDGMENTS
2012CB316301 ) , and National Natural Science Foundation of China No . 61322308 . Zengchang Qin is supported by the National Science Foundation of China No . 6130504 .
8 . REFERENCES
[ 1 ] D . Blackwell and J . MacQueen . Ferguson distributions via P´olya urn schemes . The Annals of Statistics , 1(2):353–355 , 1973 .
[ 2 ] D . Blei and M . Jordan . Modeling annotated data . In
ACM SIGIR , pages 127–134 , 2003 .
[ 3 ] D . Blei and J . Lafferty . A correlated topic model of science . The Annals of Applied Statistics , pages 17–35 , 2007 .
[ 4 ] D . Blei , A . Ng , and M . Jordan . Latent Dirichlet allocation . JMLR , 3:993–1022 , 2003 .
[ 5 ] D . M . Blei and J . D . McAuliffe . Supervised topic models . In NIPS , 2007 .
[ 6 ] G . Carneiro , A . Chan , P . Moreno , and N . Vasconcelos .
Supervised learning of semantic classes for image annotation and retrieval . IEEE Trans . Pattern Anal . Mach . Intell . , 29(3):394–410 , 2007 .
Jun Zhu is supported by the National Basic Research 2013CB329403 ,
Program ( 973 Program ) of China ( Nos .
[ 7 ] N . Chen , J . Zhu , F . Sun , and E . Xing . Large margin predictive latent subspace learning for multiview data
501 analysis . IEEE Trans . Pattern Anal . Mach . Intell . , 34(12):2365–2378 , 2012 .
[ 8 ] N . Chen , J . Zhu , and E . Xing . Predictive subspace learning for multi view data : A large margin approach . In NIPS , 2010 .
[ 9 ] W . B . Croft , D . Metzler , and T . Strohman . Search engines : Information retrieval in practice . Addison Wesley Reading , 2010 .
[ 27 ] J . Sethuraman . A constructive definition of Dirichlet priors . Statistica Sinica , 4:639–650 , 1994 .
[ 28 ] A . Sharma , A . Kumar , H . Daume , and D . Jacobs . Generalized multiview analysis : A discriminative latent space . In CVPR , pages 2160–2167 . IEEE , 2012 . [ 29 ] Y . Teh , M . Jordan , M . Beal , and D . Blei . Hierarchical
Dirichlet processes . Journal of the American Statistical Association , 101(476):1566–1581 , 2006 .
[ 10 ] P . Duygulu , K . Barnard , J . De Freitas , and
[ 30 ] S . Virtanen , Y . Jia , A . Klami , and T . Darrell .
Factorized multi modal topic model . In UAI , 2012 .
[ 31 ] S . Virtanen , A . Klami , and S . Kaski . Bayesian CCA via group sparsity . In ICML , 2011 .
[ 32 ] C . Wang , S . Yan , L . Zhang , and H . Zhang . Multi label sparse coding for automatic image annotation . In CVPR , pages 1643–1650 . IEEE , 2009 .
[ 33 ] P . Wu , S . C H Hoi , P . Zhao , and Y . He . Mining social images with distance metric learning for automated image tagging . In WSDM , pages 197–206 . ACM , 2011 .
[ 34 ] H . Xia , P . Wu , and S . C . Hoi . Online multi modal distance learning for scalable multimedia retrieval . In WSDM , pages 455–464 . ACM , 2013 .
[ 35 ] E . Xing , R . Yan , and A . Hauptmann . Mining associated text and images with dual wing harmoniums . In UAI , 2005 .
[ 36 ] O . Yakhnenko and V . Honavar . Multi modal hierarchical Dirichlet process model for predicting image annotation and image object label correspondence . In SIAM SDM , 2009 .
[ 37 ] A . Yavlinsky , E . Schofield , and S . R¨uger . Automated image annotation using global features and robust nonparametric density estimation . In CIVR , 2005 . [ 38 ] J . Yu , Y . Cong , Z . Qin , and T . Wan . Cross modal topic correlations for multimedia retrieval . In ICPR , 2012 .
[ 39 ] Y . Zhen and D . Yeung . A probabilistic model for multimodal hash function learning . In ACM SIGKDD , 2012 .
[ 40 ] J . Zhu , A . Ahmed , and E . P . Xing . Medlda : maximum margin supervised topic models . Journal of Machine Learning Research , 13:2237–2278 , 2012 .
[ 41 ] J . Zhu , L J Li , L . Fei Fei , and E . P . Xing . Large margin learning of upstream scene understanding models . In NIPS , pages 2586–2594 , 2010 .
D . Forsyth . Object recognition as machine translation : Learning a lexicon for a fixed image vocabulary . In ECCV , pages 349–354 . Springer , 2002 .
[ 11 ] L . Fei Fei and P . Perona . A Bayesian hierarchical model for learning natural scene categories . In CVPR , volume 2 , pages 524–531 . IEEE , 2005 .
[ 12 ] S . Feng , R . Manmatha , and V . Lavrenko . Multiple
Bernoulli relevance models for image and video annotation . In CVPR , volume 2 , pages II–1002 . IEEE , 2004 .
[ 13 ] T . Ferguson . A Bayesian analysis of some nonparametric problems . The Annals of Statistics , pages 209–230 , 1973 .
[ 14 ] M . Guillaumin , T . Mensink , J . Verbeek , and
C . Schmid . Tagprop : Discriminative metric learning in nearest neighbor models for image auto annotation . In ICCV , pages 309–316 . IEEE , 2009 .
[ 15 ] M . Jordan , Z . Ghahramani , T . Jaakkola , and L . Saul . An introduction to variational methods for graphical models . Machine Learning , 37(2):183–233 , 1999 .
[ 16 ] V . Lavrenko , R . Manmatha , and J . Jeon . A model for learning the semantics of pictures . In NIPS , 2003 .
[ 17 ] P . Liang , S . Petrov , M . I . Jordan , and D . Klein . The infinite PCFG using hierarchical Dirichlet processes . In EMNLP/CoNLL , 2007 .
[ 18 ] J . Liu , M . Li , Q . Liu , H . Lu , and S . Ma . Image annotation via graph learning . Pattern Recognition , 42(2):218–228 , 2009 .
[ 19 ] D . G . Lowe . Distinctive image features from scale invariant keypoints . International journal of computer vision , 60(2):91–110 , 2004 .
[ 20 ] A . Makadia , V . Pavlovic , and S . Kumar . A new baseline for image annotation . In ECCV , volume 8 , pages 316–329 , 2008 .
[ 21 ] D . Metzler and R . Manmatha . An inference network approach to image retrieval . In CIVR , 2004 .
[ 22 ] F . Monay and D . Gatica Perez . Modeling semantic aspects for cross media image indexing . IEEE Trans . Pattern Anal . Mach . Intell . , 29(10):1802–1817 , 2007 .
[ 23 ] J . Paisley , C . Wang , and D . M . Blei . The discrete infinite logistic normal distribution . Bayesian Analysis , 7(4):997–1034 , 2012 .
[ 24 ] N . Rasiwasia , J . Costa Pereira , E . Coviello , G . Doyle ,
G . Lanckriet , R . Levy , and N . Vasconcelos . A new approach to cross modal multimedia retrieval . In ACM Multimedia , pages 251–260 , 2010 .
[ 25 ] C . Rasmussen and C . Williams . Gaussian processes for machine learning , volume 1 . MIT press Cambridge , MA , 2006 .
[ 26 ] K . Salomatin , Y . Yang , and A . Lad . Multi field correlated topic modeling . SIAM SDM , 2009 .
502
