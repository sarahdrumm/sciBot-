A Few Good Predictions : Selective Node Labeling in a
Social Network
Gaurish Chaudhari
IIT Bombay gscchaudhari@gmailcom vas1089@gmail.com
Vashist Avadhanula
IIT Bombay
Sunita Sarawagi
IIT Bombay sunita@iitbacin
ABSTRACT Many social network applications face the following problem : given a network G = ( V , E ) with labels on a small subset O ⊂ V of nodes and an optional set of features on nodes and edges , predict the labels of the remaining nodes . Much research has gone into designing learning models and inference algorithms for accurate predictions in this setting . However , a core hurdle to any prediction effort is that for many nodes there is insufficient evidence for inferring a label .
We propose that instead of focusing on the impossible task of providing high accuracy over all nodes , we should focus on selectively making the few node predictions which will be correct with high probability . Any selective prediction strategy will require that the scores attached to node predictions be well calibrated . Our evaluations show that existing prediction algorithms are poorly calibrated . We propose a new method of training a graphical model using a conditional likelihood objective that provides better calibration than the existing joint likelihood objective . We augment it with a decoupled confidence model created using a novel unbiased training process . Empirical evaluation on two large social networks show that we are able to select a large number of predictions with accuracy as high as 95 % , even when the best overall accuracy is only 40 % .
Categories and Subject Descriptors I26 [ Artificial Intelligence ] : Parameter Learning
Keywords well calibrated probabilities ; graphical models ; high confidence predictions
1 .
INTRODUCTION
The problem of predicting labels for users in a partially labeled social network has many applications . For example , in the Twitter follower network , a recent problem [ 17 ,
9 , 12 , 10 , 22 ] is predicting a user location starting from a small 1–2 % known locations obtained from users who allow geo tagging their tweets . These predictions are used to improve user experience through location based services like recommendations , advertisements , and automatic language selection . As another example consider a friendship network like Facebook , where a small subset of users provide profile attributes such as age , gender , education , and , hobbies . A problem of recent interest [ 28 , 19 , 31 ] is to predict the missing attributes of other users based on known attributes of their friends .
A core problem faced by all prediction algorithms is that for most users there is just not sufficient signal to infer their labels . For example , existing efforts on location prediction of Twitter users report accuracy in the range of 50–60 % [ 17 , 9 , 12 , 10 ] . This means that almost half of the predicted locations are wrong , leading to misplaced recommendations sometimes bordering on to downright absurd . Our goal is to abstain from making such prediction . Our motto is pauca sed matura1 — make a few predictions but ensure that they are correct with high probability . More formally , our problem statement is as follows . Given a social network G = ( V , E ) , known labels for a small subset O ⊂ V , and a confidence threshold σ , predict the labels of the largest possible V such that the predictions are correct with at least σ probability . Equivalently , attach a well calibrated probability pu with the predicted label of each node u where well calibrated pu values have the property that the accuracy over nodes u with pu ≥ σ is at least σ .
Such a selective labeling problem is well motivated in large social networks settings because such networks exhibit very high diversity in accuracy in different regions arising out of differences in the level of connectivity of users , the degree of homophily of the predicted labels , and the fraction of known labels . A node labeling algorithm just needs to translate the diversity to correctness probability of the predicted label . Algorithms for labeling nodes in a partially labeled graph have been extensively researched across multiple communities ( see [ 24 , 21 , 16 , 3 , 14 ] for a survey ) . Many techniques have been harnessed , including , iterative classification [ 5 , 15 , 24 ] , Markov Random Fields ( MRFs ) [ 29 , 4 , 27 ] , random walks [ 2 , 25 ] , label propagation [ 26 ] , semi supervised learning [ 6 , 32 ] , communities [ 19 ] and co citations [ 3 ] . When we compared many of these on their ability to provide wellcalibrated probabilities , we found that labeling probabilities were poorly calibrated in all methods , including MRF based methods which create a joint distribution over labels in a
1few but ripe Carl Friedrich Gauss graph . While many studies [ 24 , 16 , 18 , 21 ] have compared methods on overall accuracy , ours is the first comparison on calibration quality .
In this paper we propose a method for labeling nodes in a partially labeled graph and attaching well calibrated probabilities to the predicted label . Our approach is based on MRFs , chosen because of their popularity in the graph labeling community and their elegant modeling of the joint distribution over all labels in a graph . We make two key modifications to MRFs to make them better calibrated .
First , we propose a novel training objective : maximize likelihood of each known label conditioned on every other known label . This objective is more directly linked to our calibration goal than the conventional objective : maximize joint likelihood of all known labels . Also , it is both theoretically and empirically superior to another popular objective : maximize pseudo likelihood . We address the computational challenge of training our objective through a theoretically sound graph pruning algorithm that guarantees an approximation on trees . Second , we propose a decoupled stage of confidence estimation based on marginals obtained from inference on the MRF . A challenge in training such two stage models is providing unbiased labeled data to both stages without hard partitioning the data across them . We propose a novel leave one out inference strategy on graphs to tackle this problem and provide an efficient algorithm for implementing the strategy .
Experiments on two large social networks show that our two stage strategy is much better calibrated than any of the existing techniques and provides a practical solution to the selective prediction problem on graphs . On a one million node Twitter location prediction problem , with only 2 % observed locations ( a typical situation on the real Twitter dataset ) , the best overall accuracy by any graph prediction algorithm was only 30 % whereas our method could select 60,000 users with accuracy over 80 % . The best existing approach could select only one fifth of this number .
The rest of the paper is organized as follows . Section 2 sets up the problem and notations . Section 3 discusses existing algorithms . Section 4 describes our approach . Theoretical results on pruning are in Section 5 . Empirical comparisons appear in Section 6 and conclusions in Section 7 . its neighbors , because most such features can be rewritten as sum of edge features . Our goal is to infer a label ˆcu for each remaining user u ∈ V − O based on G and the features , and attach a probability pu that the prediction is correct . Typically , this task is performed in two steps . First we train parameters of a model using observed labels . We will generically call these parameters as w . Second , we deploy the trained model to get ( ˆcu , pu ) for each u ∈ R .
In this paper we use bold faced symbols for vectors ( eg c , µj ) and normal font for their members indexed either as a subscript ( eg ci ) or as an argument ( eg µj(i) ) .
3 . EXISTING/RELATED APPROACHES
We consider two types of existing solutions to this problem : iterative classification based ( Section 3.1 ) and graphical model based ( Section 32 ) Readers familiar with these approaches can directly skip to our approach in Section 4 . We do not consider methods based on random walks [ 2 , 25 ] , label propagation [ 6 , 26 ] , and other pure inference algorithms [ 19 ] because these either do not support learning with both node and edge features , or are subsumed by the graphical model based approach . Another promising approach based on Gaussian Markov Models is Copula Latent Markov models [ 30 ] . These do not fit our setup because they require the labels to be binary and do not model features on edges . Our datasets span thousands of labels and heavily depend on edge features .
Algorithm 1 Framework : training and deployment
Input : G = ( V , E ) , f ,O , c Initialize w . while w not converged do
E step Call inference(G , w , c ) and obtain prediction ˆcu and marginals µu( ) for u ∈ R , ∈ Y . M step Re estimate w using ( ˆcu , µu ) for each u ∈ R and cu for each u ∈ O . end while Deployment Call inference(G , w , c ) to get ( ˆcu , µu(ˆcu ) ) for u ∈ R .
2 . PROBLEM SETUP AND NOTATIONS
3.1
Iterative classification
Let G = ( V , E ) be our social network where V is the set of n users and E the set of pairwise homophilic relationships among them . Let O ⊂ {1 , . . . , n} denote the set of k users each of whose label is known and let R = V − O . Without loss of generality assume O = {1 , . . . , k} . Let Y denote the space of possible labels , and cu ∈ Y for each u ∈ O denote user u ’s label . Let c denote the vector c1 , . . . , ck , and m denote the cardinality |Y| .
In general , each node and edge in G is associated with a set of features which is either user provided ( eg the education of a user may help predict age ) or derived from properties of G ( e.g , the strength of an edge may depend on the log(degree ) of its endpoints ) . We use f ( , u ) to denote the vector of node features when user u takes label , and f ( , , u , v ) to denote the edge feature vector on edge ( u , v ) when label of u is and v is . Often the node feature vector is empty , and the edge features may only depend on whether and are equal . We do not consider features over larger subsets of variables , for example a node and all
This popular approach from relational learning [ 5 , 15 , 24 , 20 ] consists of repeatedly creating a classifier to predict a node ’s label using features of the node and labels of its neighbors in the graph , only some of which may be observed . In Algorithm 1 we present a common framework to describe the many existing variants of this approach . The first step is to initialize the parameters w of the classifier by training on only the observed node and its observed neighbors . Next , we enter a EM kind of loop that trains the classifier Pr(xu|cN ( u ) , ˆcN ( u ) , µN ( u ) ) which predicts xu conditioned on known label cv , or predicted label ˆcv and/or distribution µv over possible labels of each neighbor v of u . The loop consists of two steps : E step : First , using the existing parameters w infer the unknown labels ˆcu and marginal probability µu( ) that an unobserved node u takes label . We call this the inference step . Inference is also iterative and is known by several names , including ICM , relaxation labeling , and mean field inference [ 21 ] .
M step : Second , we retrain the classifier so as to maximize likelihood of the observed nodes conditioned on the ( re)estimated labels/marginals of all its neighbors . This is like normal classifier training except features are aggregate over predicted/true labels/marginals of neighbors . We compare with four variants : the ICA method that ignores the marginals µN ( u ) in Pr(xu|cN ( u ) , ˆcN ( u ) , µN ( u ) ) and the ICA Soft method that includes them and ignores the hard predictions ˆcN ( u ) , the ICA NoLoop method which after initialization , skips the training loop ; and the LR method that uses only the observed nodes for both training and deployment and is thus a simple logistic regression classifier . 3.2 Graphical model based approach
The repeated classification approach does not create a consistent global distribution , and expresses variable dependence in ad hoc terms . An undirected graphical model or a Markov Random Field provides a more principled joint distribution among interacting variables in a graph and has served as another popular choice for label prediction in social networks [ 29 , 4 , 27 , 24 ] . Let x = x1 , . . . , xn be variables denoting the labels of all n users and let F(x ) denote the sum of the feature vectors over the entire graph , that is
F(x ) = f ( xi , i ) + f ( xi , xj , i , j )
( 1 ) nX i
X
( i,j)∈E
A graphical model expresses the joint distribution as
Pr(x1 , . . . , xn|w ) =
1
Z(w ) ewF(x1,,xn )
( 2 ) is equal toP where Z(w ) is a normalizer called the partition function and x1,,xn ewF(x1,,xn )
Many different methods have been proposed to train w . We discuss one of the most popular of these : the joint likelihood method . This method maximizes the joint probability of all observed labels as follows log Pr(xO = c|w ) max w
X xR
= max w log ew.F(c,xR ) − log Z(w )
( 3 )
The objective is not convex when R is non empty because of the summation within the log . Therefore , typically the EM algorithm is used for finding a local optima . E step : The E step computes marginal probability of each node and edge conditioned on the observed labels . That is , for each node compute µu( ) = Pr(xu = |xO = c , w ) and for each edge compute µu,v( , ) = Pr(xu = , xv = |xO = c , w ) by marginalizing the joint distribution in Equation 2 . These can be computed simultaneously for all nodes and edges using one of the many existing algorithms for inference in graphical models [ 13 ] . However , on general graphs , exact inference is intractable , and many approximations exist , including Gibbs Sampling , Mean field , Belief Propagation ( BP ) and its convergent variants such as TRWS [ 13 ] . M step : The M step solves for the expected log likelihood of the observed node which becomes w.Eµ(F(c , xR ) ) − log Z(w ) where Eµ(F(c , xR ) =
X
µij( ,
)f ( ,
, i , j ) +
( i,j)∈E , ,
X i∈V ,
( 4 )
µi()f ( , i )
This objective is convex in w and can be solved using gradient descent . However , a new challenge is that computation of log Z(w ) is intractable and approximate inference via methods like BP or sampling does not guarantee a convergent gradient descent loop . Recent research [ 23 , 11 ] provides an elegant solution to this problem by approximating log Z(w ) as a convex optimization over message variables and jointly solving for the message variables and w as one convergent program .
After training parameters w , a final inference step ( same as in the E step of training ) is used to compute the marginals µu( ) = Pr(xu = |xO = c , w ) for each u ∈ R , and predict label ˆcu = argmaxµu( ) , ∀u ∈ R . The confidence pu of the prediction is just µu(ˆcu ) .
Since the confidence is marginal probability of a distribution trained via a statistically sound training objective , we expected the confidence values to be well calibrated . But , when we deployed them on two real life social networks , we did not find that to be the case at all . Therefore , in the next section we present the steps we took to solve this problem .
4 . OUR APPROACH
We address the problem of getting well calibrated probabilities through fixes on two fronts . First , we link the graphical model training objective more directly to our calibration goal ( Section 41 ) Second , we decouple the confidence estimation problem from label prediction and develop a method of collecting an unbiased labeled set for training a well calibrated estimator of confidence ( Section 42 ) 4.1 Modified training of graphical models
We observed an impedance mismatch in the graphical model approach ( Section 3.2 ) between the training objective of maximizing joint probability of all observations Pr(xO = c|w ) , and the predicted single variable marginals conditioned on all observation Pr(xu = ˆcu|xO = c ) . We propose to fix this mismatch through a training objective that maximizes conditional likelihoods where for each observed label ci we maximize its probability conditioned on all other observed labels . Our revised training objective then becomes :
Y i∈O log
X max w
= max w log
X i∈O
Pr(xi = ci|xO−i = c−i , w )
X ew.F(c,xR ) − log ew.F(c−i,xR∪{i} ) xR xR∪{i} where the condition xO−i = c−i implies that we are fixing the labels of all observed nodes except the ith one . We call this the node conditional likelihood method or NCL in short . Likewise , we use the short form JL for the Joint Likelihood method . We next elaborate on the steps required for solving this objective efficiently . This objective is also non convex but this is only due to the first term which is the same as in JL ( except for being repeated k times ) . Therefore we use the same EM framework to solve for a local optimum . E step : Same as in JL . M step : The M step is different because of difference in the second term . We need to solve for expected conditional likelihood of each observed node which becomes : w.Eµ(F(c , xR ) ) − log ew.F(c−i,xR∪{i} )
( 5 )
X
X i xR∪{i} where Eµ(F(c , xR ) ) is same as in Equation 4 . The second term looks like a partition function and we denote it as log Z(w|c−i ) . In order to compute log Z(w|c−i ) we need to separately run inference for each observed node because the conditioning variables are different for each i . There is no easy way to reuse computation across different i values and requires inference on the full graph k times for each optimization loop . We propose an approximation to reduce this cost . For computing Pr(xi = ci|xO−i = c−i , w ) ( henceforth denoted as Pr(ci|c−i ) ) we select a mini graph Gi from G such that the approximate probability PrGi ( ci|c−i ) computed only using features of variables in Gi is within a usergiven tolerance of Pr(ci|c−i ) . In Section 5 we present how such mini graphs are selected . Let Ri denote the subset of R in Gi and let FGi ( x ) denote the sum of features over nodes and edges in Gi . The modified M step now becomes :
X
X w.Eµ(FGi ( c , xR ) ) − log i xRi∪{i} ew.FGi
( c−i,xR∪{i} )
After training parameters w , we use inference to compute
Computationally , this objective for each i is not too different from the M step of joint likelihood except that it is performed over a smaller graph Gi . As in the joint likelihood case , even though the objective is convex , the computation of log Z(w|c−i ) is intractable on arbitrary Gis . Therefore , here also we rely on the convex approximation of [ 23 , 11 ] . This provides us an efficient and convergent M step . ( ˆcu , µu(ˆcu ) ) ∀u ∈ R exactly as in JL . 411 Asymptotic statistical guarantees We can prove that like JL , the NCL objective is also consistent . This means that if the model is faithful to the true data distribution , and if labeled data is infinite , then the NCL objective will find the true parameters . We skip a proof due to lack of space but the broad steps are the same as in Theorem 20.3 of [ 13 ] . Also , our EM algorithm is guaranteed to find a locally optimum solution as in JL . This is easy to see because EM only modifies the first non convex term in the NCL objective and this term is identical to the first term of JL up to a positive multiplicative constant . 412 Relationship to other training objectives Some readers might find our NCL objective similar to another popular training objective [ 29 , 13 ] called Pseudolikelihood ( PL ) . PL is based on approximating the joint u=1 Pr(xu|xNu ) , the product of node level probability conditioned on the node ’s neighbors . When all neighbors are known , PL is identical to NCL since Pr(xi = ci|xO−i = c−i , w ) = Pr(xi = ci|xNi = cNi , w ) . However , for the general case with unknown neighbors , the only known extension of PL we are aware of is the PL EM method of [ 29 ] which is very different from NCL . PL EM proposes an approximate EM based solution of the PL objective where in the E step mean field inference is used to compute the marginals µu for each u ∈ R . The M step X then solves the following convex objective : probability Pr(x1 , . . . , xn ) asQn
X
Y
µu(xu ) log Pr(xi|xRN i
, xOi = cOi , w )
( 6 ) i∈O x u∈RN i
RN i where RN i denotes the unobserved neighbors of node i and Oi denote the observed neighbors of i . Unlike our approach , PL EM is not statistically consistent because its M step at tempts to maximize expected likelihood with respect to the mean field distribution which is not guaranteed to match the true distribution . We will also contrast the two approaches empirically in Section 6 . 4.2 Decoupled confidence estimation
We now describe a second step that we took to better calibrate the confidence of predictions ˆcu from inference on the graphical model . While the marginals µu(ˆcu ) of NCL are better calibrated than of JL , our experiments show that they are still not good enough for selective node labeling . Both methods tended to provide overly inflated values of µu(ˆcu ) for immediate neighbors of observed nodes , and there was no easy way to offset that while also ensuring effective label propagation in the graph . We therefore chose to train a decoupled model C that works on ˆcu and marginals µu( . ) produced by the graphical model and outputs a confidence pu of the prediction ˆcu being correct for each u ∈ R . Assume that we know true labels of a subset of nodes D ⊂ R that does not overlap with O . Then C can be a probabilistic binary classifier , like a logistic regression model trained using D as follows . For each j ∈ D , create an instance with a label yj = 1 if ˆcj = cj and yj = 0 otherwise , and a set of features zj derived from observed and predicted labels/marginals in j ’s neighborhood . Examples of such features include , the smoothed fraction of j ’s neighbors that have label ˆcj , the marginal µj(ˆcj ) , the largest fraction of nodes having a label other than ˆcj in j . The complete list of features we used can be found in [ 7 ] . A major shortcoming of the above method is that we need a labeled set D to train C in addition to the set O for prediction . Since our goal is to maximize the number of nodes correctly predicted , we want O to be as large as possible . Setting aside a portion of O as D will compromise this goal , and/or could lead to a poorly trained C . We propose a novel method of creating an unbiased training set for C without any additional labeled data . When we perform inference on the full graph to get predictions ˆcu on the unobserved set R , the labels of nodes in O are pinned to c , and therefore get no predicted label . Our key idea is to use the mini graphs created during NCL training , to run inference on each Gi with all but the ith observed label c−i in u( ) = Pr(xu = |c−i , w ) , and preGi to obtain marginals ¯µi u( ) for each i ∈ O and u ∈ {i}∪N ( i ) . diction ¯ci Note , we used a different symbol for these marginals computed without ci to distinguish from the marginal obtained using all of c . This step is efficient because Gi is small and in Section 5 we will show how to obtain such a Gi . Now , for each i ∈ O create a labeled instance with label yi = 1 if ¯ci i = ci else yi = 0 , and features zi created as above . The yi labels of the observed nodes can also be used to create additional features such as the fraction of wrongly predicted observed nodes in a node ’s neighborhood . u = argmax ¯µi
Algorithm 2 presents our overall approach .
421 Related work on confidence estimation The use of a decoupled binary model to get confidence with predictions is not new , for example , [ 4 ] uses one such for selecting nodes for active labeling . The novelty of our approach is in the way we create an unbiased labeled dataset for training while using the same data for predictions . In contrast , [ 4 ] assumes a separate labeled dataset . We show in Section 631 how that impacts selective accuracy .
Algorithm 2 Our approach .
Input : G = ( V , E),O , c w = Trained parameters using NCL ( Section 41 ) Call inference(G , w , c ) and get ( ˆcu , µu ) for u ∈ R for i ∈ O do
Gi = Prune G to approximate Pr(ci|c−i ) ( Section 5 ) u ) u ∈ {i} ∪ N ( i ) Call inference(Gi , w , c−i ) , get ( ¯ci zi = Make Features(¯ci N ( i ) , cN ( i ) ) yi = 1 if ¯ci i = ci , yi = 0 otherwise . u , ¯µi N ( i ) , ¯µi i , ¯µi i , ¯ci end for C = logistic model trained using {(zi , yi ) : i ∈ O} for u ∈ R do zu = Make Features(ˆcu , µu , ˆcN ( u ) , µN ( u ) , cN ( u ) ) pu = C(zu ) end for return {(u , pu ) : u ∈ R}
Figure 1 : An example of graph pruning to obtain Pr(c1|c−1 ) . The shaded nodes are observed . The dotted nodes in the left are nodes pruned via the Markov condition . The dotted nodes in the right are nodes pruned using = 0.02 via the method of Section 53
5 . PRUNING THE GRAPHICAL MODEL We show how to select a sub graph Gi of G such that the approximate probability PrGi ( ci|c−i ) computed only using Gi is within of Pr(ci|c−i ) . First , based on the Markov property of graphical models , we can prune any node u from G all of whose paths to i are through observed nodes without modifying Pr(ci|c−i ) . For example , in Figure 1 , we can prune nodes 10 and 11 when i = 1 because for both of them the only paths to node 1 are through observed nodes ( shown shaded ) . Call the pruned graph Gi . Gi might still be large and we present recipes for pruning further nodes from it while maintaining an approximation . For ease of notation , we will drop the subscript i , use G instead of Gi , assume i = 1 , ci = 1 , and assume that the nodes in G are numbered 1 , . . . , n . Also , we denote the node potentials as ψu(xu ) and define ψu(xu ) = ew.f ( xu,u ) for u ∈ ( R ∩ Gi ) ∪ {i} , ψu(xu ) = M if u ∈ ( O − i ) ∩ Gi , xu = cu and ψu(xu ) = 1 otherwise where M * 1 is a large constant . We denote edge potentials as ψuv(xu , xv ) = ew.f ( xu,xv ,u,v ) . We assume that edge features f ( xu , xv , u , v ) depend only on whether xu = xv . Therefore , they can be expressed as ψuv(xu , xv ) = ( αuv if xu = xv , 1 else ) where αuv is a constant . Further , since we assume homophily , αuv ≥ 1 . Let α be an upper bound on the values of αuv . During parameter training when the potentials are unknown , we assume that the user can guess a suitably tight bound for α as pruning is performed in terms of α . This is not too difficult in our experience , particularly since the penalty for over estimate is just reduced pruning .
With these notations we can cast the conditional probability Pr(x1 = 1|xO−1 = c−1 ) as this marginal probability ( u,v ) ψuv(xu , xv ) ( u,v ) ψuv(xu , xv ) u ψu(xu)Q Q Q u ψu(xu)Q
P P x2,,xn:x1=1
Pr(x1
1 ) = x1,x2,,xn where we use the shorthand x1 1 for x1 = 1 . We introduce a convenient normalization operator N ( z ) that when applied on a vector z normalizes its entries so they sum to one . Using this , we can write Pr(x1 = ) = N (
ψuv(xu , xv))( )
X
Y
Y
ψu(xu ) x2,,xn u
( u,v )
Our goal is to remove nodes from G so as to approximate Pr(x1 1 ) within a given of the unpruned value . We develop the method in three stages : first assume that G is a chain with x1 at one end , then generalize to the case when G is a tree , and finally to arbitrary graphs .
The proofs in the rest of the section are quite technical and assume knowledge of message based computations in graphical models . Readers unfamiliar with the topic can skip the proofs and just see the main pruning results in Theorems 5.1 , 5.2 , and 53 5.1 Single chain
Assume the chain of nodes is x1 , . . . , xn with an edge between each xu and xu+1 . In a chain if we remove a node xt , all nodes xj for j > t are also removed . Let P ( x1|ψ1t ) denote the marginal calculated with potentials up to node t . Then P ( x1|ψ1t ) =
!
N
ψ1(x1 )
X tY x2,,xt u=2
ψu(xu)ψu−1,u(xu−1 , xu )
( 7 )
1|ψ1n ) − P ( x1
1|ψ1n ) = P ( x1
Thus , our task reduces to identifying an index t such that |P ( x1 1|ψ1t)| ≤ . For a chain , the impact of all potentials on node 1 after t can be expressed as a simplex message , say θt that node t+1 sends to t and we can rewrite 1|ψ1t , θt ) . The principle we follow for P ( x1 pruning is to find the maximum swing in this value over all possible values of θt ∈ ∆m , the m dimensional simplex . Let ˆθ and ˇθ denote the two extreme values of θt that maximize this difference , that is , ( ˆθ , ˇθ ) = argmaxˆθ
∈∆m
,ˇθ
1|ψ1t , ˆθ
) − P ( x1
1|ψ1t , ˇθ
)
˛˛˛P ( x1
˛˛˛
Now , we will derive what ( ˆθ , ˇθ ) should be and for what t will the difference be ≤ . We first introduce some notations . Let ˆMt r denote a message at distance r from node 1 , when a message ˆMt = ˆθ is injected from node t + 1 to t . Likewise define ˇMt r with message θt from t+1 . Let h(ψ , α , M ) denote the outgoing message at a node with node potential ψ when a message M is injected into it via an edge with Potts potential α . That is , r with message ˇθ from t + 1 and ¯Mt h(ψ , α , M ) = N ( ψγ ) where γ = ( α − 1)M + 1m ,
( 8 ) where 1m is a length m vector of all ones . Using this we can express ¯Mt r , recursively as follows : r = h(ψr , α , ¯Mt r+1 ) , if r < |t| ,
¯Mt
¯Mt t = θt ,
( 9 )
1|ψ1t , θt ) = ¯M t First , we assume no node potentials and derive a closed and similarly ˇMt , ˆMt . From the above , it is easy to see that P ( x1 form expression for Pr(x1 = |ψ1t , θt ) .
1(1 ) .
1234751068911.019123475689 Lemma 5.1 If G is a chain , αuv = α , ψu( ) = 1 ∀u , , then
Pr(x1 = |ψ1t , θt ) =
θt( ) − 1/m α−1 + 1)t−1 + ( m
1 m
( 10 )
Proof . When node potential is 1 for all labels , Equations 8 and 9 gives that ¯M t . To get 1( ) which is equal to Pr(x1 = |ψ1t , θt ) we repeatedly ¯M t apply this formula t times . After simplifying the result , we get the RHS . r−1( ) = ( α−1 ) ¯M t m+α−1 r ( )+1
#
"
A corollary from the above that we will find useful is : Corollary 5.1 If αuv ≤ α ∀(u , v ) ∈ E ( other conditions same as in Lemma 5.1 ) , then
1 m
θt( ) − 1/m α−1 + 1)t−1 + ( m
Pr(x1 = |ψ1t , θt ) −
≤ 0 if θt( ) ≥ 1 m ≥ 0 otherwise . α−1 +1 ) , αuv ≤ α log 1 Theorem 5.1 If G is a chain , t > log ( m ψu( ) = 1 ∀u , then |P ( x1 1|ψ1n ) − P ( x1 1|ψ1t)| ≤ . In other words , pruning potentials ψt,t+1 . . . , ψn−1,n changes Pr(x1
1 ) by at most .
1|ψ1n ) − P ( x1
1|ψ1t)|
Proof . |P ( x1 ≤ max ˆθ,ˇθ∈∆m
˛˛˛P ( x1
≤
≤ max
0≤ ˇθ(1)≤1/m≤ ˆθ(1)≤1
1
α−1 + 1)t−1 ≤
( m
˛˛˛
1|ψ1t , ˆθ ) − P ( x1
1|ψ1t , ˇθ )
˛˛˛˛˛ ˆθ(1 ) − ˇθ(1 )
α−1 + 1)t−1 ( m
˛˛˛˛˛
( Corollary 5.1 )
Now let us extend the result to include node potentials . Recall that since training is not over , we do not know the value of the node potentials . Unlike edge potentials , it is difficult to bound their values and without those we cannot obtain a closed form expression for Pr(x1 ) in terms of θt unlike what we did earlier . So , we take an alternative approach and bound the difference between the upper and lower bound of r(1 ) − ˇM t label 1 in the message at stage r ˆM t r(1 ) in terms r+1(1 ) − ˇM t of the corresponding difference ˆM t r+1(1 ) at stage r + 1 . The following lemma gives the bound . r(1 ) ≤ κ(m , α)( ˆM t Lemma 5.2 ˆM t where ( α − 1)(m + α − 1 ) r+1(1)− ˇM t r(1)− ˇM t r+1(1 ) )
κ(m , α ) =
α(m + α − 1 ) + 3m + α − 5
( 11 )
Proof . The proof is involved and we provide a brief sketch . Let ψr denote the unknown node potential at r . Since , r(1 ) − ˇM t
ˆM t r(1 ) = h(ψr , α , ˆMt r+1)(1 ) − h(ψr , α , ˇMt r+1)(1 )
≤ max h(ψ , α , ˆMt r+1)(1 ) We prove in [ 1 ] that the solution ψ∗ for the optimization problem above is
ψ r+1)(1 ) − h(ψ , α , ˇMt p(1 − ˆγ1)(1 − ˇγ1 ) p(1 − ˆγ1)(1 − ˇγ1 ) + ( m − 1 )
ψ∗(1 ) =
, ψ∗(l > 1 ) =
√
ˆγ1 ˇγ1
1 − ψ∗(1 ) m − 1
Figure 2 : Error bounds ( log scale ) vs distance t for m = 100
Figure 3 : An example tree . Pruned nodes are dotted : = 0.05 , m = 100 , α = e05
( α−1 ) ˆM t r+1(1)+1 where ˆγ1 = this solution we can show that : m+α−1 and likewise ˇγ1 . Plugging in max
ψ h(ψ , α , ˆMt r+1)(1 ) − h(ψ , α , ˇMt ˆγ1 − ˇγ1
2p(1 − ˆγ1)(1 − ˇγ1)ˆγ1ˇγ1 + ˆγ1(1 − ˇγ1 ) + ˇγ1(1 − ˆγ1 ) r+1)(1 ) =
= r+1(1)− ˇM t
Next , we use a set of inequalities to show that this expression can be upper bounded by κ(m , α)( ˆM t r+1(1) ) . The details of this can be found in [ 1 ] . Theorem 5.2 If G is a chain where αuv ≤ α and t > log ( κ(m,α ) ) + 1 , then |P ( x1 Proof . The proof follows by repeatedly applying Lemma 5.2 1|ψ1t , ˇθ)| = 1(1 ) ≤ maxˆθ,ˇθ κ(m , α)t−1(ˆθ(1)−ˇθ(1 ) ) = κ(m , α)t−1 . 1(1)− ˇMt t−1 times until we get that |P ( x1 ˆMt
1|ψ1t , ˆθ)−P ( x1
1|ψ1n ) − P ( x1
1|ψ1t)| ≤ . log
In Figure 2 we plot the worst case bound on | Pr(x1
1|ψ1n)− 1|ψ1t)| on a chain when we prune nodes after distance Pr(x1 t from node 1 for α = e0.5 , m = 100 and α = e and for the two cases : without node potentials ( Theorem 5.1 ) and with node potentials ( Theorem 52 ) The bounds are tighter without node potentials but in all cases we achieve good approximation within a small t . With α = e0.5 , a typical value in our datasets , we approximate to within 0.0001 and 0.019 respectively at t = 2 . 5.2 Tree
Now consider the case where G is a tree rooted at x1 . Pruning now amounts to choosing a frontier of G so that the size of the pruned tree is smallest while ensuring that the maximum swings in the potentials outside the frontier do not impact Pr(x1
1 ) by more than .
Suppose we decide to prune all nodes below a node xt with parent p . The contribution of all potentials under xt ( inclusive of its own node potential ) in computing Pr(x1 1 ) can be expressed as a message Mt from t to p . We compute what happens to Mt along the path P(t ) to x1 . Unlike for chains , the path has incoming messages from other children of intermediate nodes . For example , in Figure 3 M9 has to multiplied with when the message from node 10 passes through node 4 to node 1 . During message passing , all incoming messages at a node u are multiplied with the node potential at u , so the messages can be treated as a modified node potential . Our bounds in the previous section assumed the most adverse node potentials . These same bounds can be used unchanged on the path ignoring all other branches on the path , we only have to modify Equation 9 to work with
1E 0500001000100101101234567Approximation RangeDistance from source nodew/o NP α=exp(0.5 ) w/o NP α=exp(1.0)with NP α=exp(0.5)with NP α=exp(10)1234765890190191011 non continuous indices along a path P(t ) . We use notation ˆMP(t ) instead of ˆMt therefore .
Thus , we know how to compute message bounds for single path in the tree . If we prune below a frontier of multiple nodes , we can simply add the bounds for each frontier node because our per path bounds are with respect to the worst effect of messages from the rest of the tree . The final theorem for pruning a tree based on frontier of nodes appears below ( We skip a formal proof due to lack of space ) .
# Nodes # Edges % Uni directional edges # Labels(m )
Twitter 1,071,254 3,863,698 42.87 2,113
Pokec 1,136,049 10,773,722 61.24 10
Table 1 : Twitter and Pokec graph statistics
Theorem 5.3 Let G be a tree rooted at 1 with potentials as per Theorem 52 If GT is a subset of G obtained by removing all nodes below a frontier T = t1 , . . . , tf such that
1|ψG ) − P ( x1
1|ψGT )| ≤ .
P j κ(m , α )
|P(tj )| ≤ then |P ( x1
An example of a pruned tree obtained by using α = e0.5 , m =
100 and = 0.05 is shown in Figure 3 . The weights on a node t is κ(m , α ) 5.3 Arbitrary graph and the frontier is T = 2 , 4 .
|P(t)|
For arbitrary graphs , inference is intractable and it is difficult to efficiently bound the influence that a message from some node u has on node 1 when there are multiple paths between them . Recently , [ 8 ] addressed this problem by choosing the shortest path between two nodes to bound this influence . We follow the same strategy . Let SP(t ) denote the shortest path from node t to 1 . Define the frontier nodes of a pruned graph G ⊂ G as the set of unobserved nodes in G with at least one pruned neighbor . We choose G with fron|SP(tj )| ≤ . For example , the right graph in Figure 1 is obtained by pruning with = 002 Node 2 is the only frontier node here ; node 7 is not a frontier node because it is observed . tier nodes T = t1 , . . . , tf such that P j κ(m , α )
Unlike for trees , we have no approximation guarantees on Pr(x1 1 ) with this pruning but since inference algorithms on general graphs are also not exact , we cannot hope to do much better . Empirically , we will show in Section 633 that our pruning method is effective .
6 . EXPERIMENTS
We present an evaluation of different methods on their ability to do selective node prediction and an analysis of our method along different dimensions . 6.1 Social Networks
Our experiments were performed on two large real life social networks , Twitter and Pokec , each with more than one million nodes . We summarize their key statistics in Table 1 . Twitter : We crawled 82 million geo tagged tweets from all over the world over four weeks in June and July 2012 . We extracted from these 3.5 million geo tagged users and assigned to each his most frequent tweet location . We then crawled for followers of users . We removed users with more than 1000 neighbors because these are typically celebrity or media accounts , and not useful for location prediction . This left us with 3 million users . The raw user locations were in ( latitude , longitude ) format . Since our focus is discrete prediction , we mapped these to one of 2,113 top populated world cities using Google ’s Geo coding API2 . Users outside these 2113 cities were removed giving rise to our final graph of 1.07 million users and 3.86 million edges .
Figure 4 : Histograms of fraction of followers of a user sharing his label for Twitter(left ) and Pokec(right ) . Blue ( darker ) bars are for histogram and red ( lighter ) bars for cumulative probabilities .
Pokec : As a representative of the attribute prediction problem in a friendship network , we used the publicly3 available Pokec social network ( crawled in May 2012 ) , a popular social network in Slovakia . Each user has a profile in Slovak spanning attributes like gender , age , hobbies , marital status , children , profession , and education . Our task is to predict a user ’s age , available in only 68 % user profiles . We retained these 68 % users , giving rise to a graph of 1.13 million nodes and 10.8 million edges . We segmented age into bins of five years , obtaining 10 age groups . 6.2 Setup and Methods
We compared eight methods : four methods from the iterative classification family ( Section 3.1 ) : LR , ICA NoLoop , ICA , and ICA Soft ; three methods based on graphical models : JL trained on joint likelihood ( Section 3.2 ) and PLEM trained on pseudo likelihood ( Section 412 ) , and NCL trained on node conditional likelihood ( Section 4.1 ) ; and finally our two stage method NCL+Conf that uses the decoupled training of Section 42 The base classifier for LR , ICA NoLoop , ICA , and ICA Soft was a multi class linear logistics regression model . Methods JL and NCL used BP during deployment and convergent TRWS during training . For both our datasets an edge could be bi directional or uni directional . We designed a set of nine edge features based on intuitive clues such as edges between high degree nodes indicate less homophily , bi directional edges are stronger , and high entropy of observed labels implies less homophily . These are summarized in Table 2 and fire only for the case when xu = xv . Thus all our edge potentials are Potts . For Pokec we experimented with node features derived from a user ’s profile attributes like education and marital status which seemed indicative of age . However , we obtained no gain in accuracy from them . One reason could be that these were both highly unstructured text fields . Our best effort at picking high signal phrases provided little gain . In contrast , we found that our edges were highly homophilic . We ascertain this via Histograms in Figure 4 where X axis is the fraction su of a user ’s neighbors that have the same label , bucketed into steps of 0.1 , and Y axis is the proportion of users with su fraction co labeled friends . On Twitter , for more than 55 % of the users , majority of their graph neighbors share his label ; and for Pokec it is 51 % . The edge
2developersgooglecom/maps/documentation/geocoding
3http://snapstanfordedu/data/soc pokechtml
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Fraction of Users Fraction of followers sharing city Twitter Fraction Cumulative 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Fraction of Users Fraction of followers sharing age group Pokec Fraction Cumulative Figure 5 : Precision recall curves comparing different methods on Twitter(up ) and Pokec(below ) for k = 2 % ( left ) , 5 % ( middle ) , 10 % ( right ) . The legend is the same for all charts and dropped in some charts to reduce clutter . [ Best seen in color ] f0(u , v ) = 1 ( This is a bias feature ) f1(u , v ) = 1 if edge ( u , v ) is bi directional f2(u , v ) = f3(u , v ) = f4(u , v ) = f5(u , v ) = log(#followers(u)+1 ) if u has a follower v log(#followees(v)+1 ) if u follows v log(degree(u)+1 ) + log(#followers(u)+1 ) + if u and v are follower followee log(#followees(u)+1 ) + if u and v are follower followee log(#followees(v)+1 ) log(#followers(v)+1 ) f6(u , v ) = log(degree(v)+1 )
1
1
1
1
1
1
1
1 f7(u , v ) = 1 if u or v is observed node N(u))−entropy(O entropy(O
N(v ) ) f8(u , v ) = 2 log m where O
N ( i ) are labels of i ’s observed neighbors
Table 2 : Edge features used by our models . The label argument is missing because they only fire when u , v get the same label . affinity values as defined in [ 19 ] for the two datasets are 7.7 and 2.8 respectively , which compares favorably to affinities in other social networks [ 19 ] . Thus , both our datasets were trained on the nine edge features alone . The parameter vector w represents weight of these nine features trained via different methods . For methods from the iterative classifier family , the features at a node u were a sum of features in Table 2 over all neighbors v of u . Thus , all our methods used the same set of features and were trained with L2 regularization . We select various subsets O of observed nodes in a clustered manner to simulate homophily in the level of privacy that a user prefers . Starting from a random node , we select its neighbor with probability β and teleport to some other node with probability 1 β . We set β as 085 We found that relative performance was not too sensitive to β and even for β as small as 0.5 we got similar curves . To select a set O of size k , we take k steps in this random walk . We used k values for 2 % , 5 % and 10 % of the graph size . For each k , we select 10 random O sets and average over these 10 runs . We measure statistical significance using a student ’s t test over the 10 seeds and accuracy at the top p % most confident predictions for p=1,2 , . . . ,10 % . Since our focus is accuracy at the top , we do not consider larger p . 6.3 Results
In Figure 5 we plot the PR curves for six4 methods for Twitter on the first row and Pokec on the second and for k = 2 % on the left , k = 5 % in the middle , and k = 10 % on the right . The Y axis is the accuracy on the top X % predictions sorted on the confidence pu output by each method . We used a log scale on the X axis because our focus is accuracy on the top few most confident predictions . The overall accuracy of a method is the Y axis value when X value is 100 % . We make the following observations from these plots .
1 . The overall accuracy of different methods ranges from 26%–41 % for Twitter and 27–47 % for Pokec . The variation in the overall accuracy of different methods is not of consequence because even the maximum is too low to be useful in practice .
2 . If we focus on accuracy among the top 1 % to 20 % predictions we get a more positive story . Many methods provide high accuracy in this selected set , which touches 95 % with the top 1–3 % predictions . As we compare graphs from left to right for increasing k , we see that the top accuracy increases with k .
3 . The best selective accuracy is provided by our decoupled approach NCL+Conf . In all cases , NCL+Conf dominates all other methods for top 20 % predictions even though its overall accuracy is the same . For example , with 5 % observed nodes if our goal is 85 % accuracy we can find four and 2.1 times more predictions than any other method on Twitter and Pokec respectively . The gain of NCL+Conf is statistically significant with p value < 10−27 for all six cases .
4 . Between the two graphical model approaches , NCL provides better calibration than JL , even though the
4The ICA NoLoop and ICA methods were superseded by ICA Soft and LR and were not plotted to reduce clutter .
30405060708090110100Accuracy % UsersTwitter : 2%405060708090110100Accuracy % UsersTwitter : 5%LRPL EMICA SoftJLNCLNCL+Conf405060708090110100Accuracy % UsersTwitter : 10%LRPL EMICA SoftJLNCLNCL+Conf30405060708090110100Accuracy % UsersPokec : 2%405060708090110100Accuracy % UsersPokec : 5%405060708090110100Accuracy % UsersPokec : 10%LRPL EMICA SoftJLNCLNCL+Conf O % NCL+Conf
2 5 10
2 5 10
00/60 15/181 43/248
35/153 88/231 8.0/23
NCL Twitter 00/06 00/51 00/102 Pokec 00/01 00/70
002/187
ICA Soft
LR
00/14 008/35 02/77
00/26 00/003 002/144
00/12 015/46 03/101
00/46 21/144 68/226
Table 3 : % of nodes selected at accuracy 90 % ( above the slash ) & at accuracy 80 % ( below the slash ) for Twitter and Pokec .
Figure 7 : Comparing two different methods of training the second stage model : ( 1 ) our leave one out strategy ( NCL+Conf ) and ( 2 ) 70 30 split of labeled data across the two stages ( NCL+Split ) for k=5 % and 2 % .
Method
LR ICA Soft PL EM JL NCL NCL+Conf
2 % Twitter Infer Learn 42.9 0.1 3971 601 1265 16 227 2378 4584 846 1414 4627
5 % Twitter Infer Learn 48.4 0.2 216 19 4380 3416 69 2749 4994 696 2701 5042
5 % Pokec
Learn 0.2 163 4533 198 5334 11771
Infer 42.1 14717 13115 14621 909 950
Figure 6 : PR curves to compare gain of second stage on JL and NCL on Twitter and Pokec datasets with k = 5 % .
Table 4 : Learning and inference time in seconds . overall accuracies are comparable . See for example , Pokec 5 % . This confirms that conditional likelihood is a better objective for selective prediction . The gain of NCL over JL is statistically significant with p value < 10−4 in all cases except Pokec 10 % where it is 004 5 . The method closest to NCL from the existing literature , PL EM , does not provide good calibration for k = 2 , 5 % but is okay for larger k , corroborating the conclusion of [ 29 ] that PL EM is good with many observed nodes . On overall accuracy PL EM is competive with JL as observed in [ 29 ] , but on the top 10 % accuracy , JL scores over PL EM in all cases .
6 . Among the iterative classification methods , ICA Soft that conditions on marginal probability provides steadily better results than ICA that conditions on hard predictions . ICA Soft is comparable to JL — slightly better than it for 2 % but worse for 5 % and 10 % . ICA Soft is worse than NCL in most cases .
7 . The LR method is good in a narrow range but degrades rapidly , particularly for 2 % observations . This is because for the few nodes with many observed neighbors the classifier is able to assign high confidence . But , there is no generalization beyond this narrow set .
As further evidence of the effectiveness of NCL+Conf for selective prediction , in Table 3 we show the fraction of nodes selected at two target accuracy values ( 80 % and 90 % ) via four methods for 2 , 5 , and 10 % observed nodes . We see that NCL+Conf is able to make selective predictions on significantly more nodes than others , particularly when size of O is small . For example , on Pokec 2 % when the target accuracy is 90 % , we are able to select 3.5 % nodes whereas none of the other approach can select any node . For a target accuracy of 80 % , NCL+Conf can select between 2k and 7k nodes where k is the size of O . 631 Efficacy of our confidence estimator Since our main gain is from the second stage confidence model , we perform two experiments to analyze this stage further . First , we check whether the lift in top accuracy of NCL due to the second stage , holds for other methods too , eg JL . In Figure 6 we show PR curves for JL and JL+Conf for Twitter and Pokec with k = 5 % . We observe that JL+Conf is indeed much better than JL although on Pokec it continues to be worse than NCL+Conf because on that dataset even JL is much worse than NCL . Second , we check the gain due to our specific method of creating an unbiased training set over all observed nodes through our novel mini graph inference method . We compare with the standard practice of splitting the observed nodes over the two stages . In Figure 7 we measure accuracy of NCL+Conf with NCL+Split which uses 70 % of O as observed nodes for NCL inference and 30 % for training the second stage . We observe that NCL+Conf shows a statistically significant gain over NCL+Split for both k = 2 % and 5 % . 632 Running time In Table 4 we show the time taken for learning and inference using various methods on a sequential program on a 2.5 GHz Intel Xeon linux server with 8 GB RAM . As expected , the plain LR model is the fastest because it only looks at the small number of observed nodes during training . The running time for all graphical model based methods show a lot of variability because the convergence of inference is dependent on the potentials , the observed nodes , and edge density . Roughly , learning+inference on Twitter takes an hour , whereas on Pokec it is three hours . The training time for NCL+Conf is two to four times more than NCL and most of it goes in computing the mini graph instances . However , this part of the code is trivially parallelizable . 633 Pruning efficiency In this section we analyze the running time versus accuracy trade offs due to our pruning strategy . In Figure 8 we plot average error in the probability of the true label and average inference time ( log scale ) over hundred nodes for increasing sizes of the mini graph . The X axis varies from a minigraph size of 5 nodes to the full graph . We observe that
405060708090110100Accuracy % UsersTwitter : 5%JLJL+ConfNCLNCL+Conf405060708090110100Accuracy % UsersPokec : 5%JLJL+ConfNCLNCL+Conf60708090110Accuracy % UsersTwitterNCL+Conf 5%NCL+Split 5%NCL+Conf 2%NCL+Split 2%708090110Accuracy % UsersPokecNCL+Conf 5%NCL+Split 5%NCL+Conf 2%NCL+Split 2 % Figure 8 : Error and inference time for increasingly pruned graph for Twitter and Pokec with 2 % and 5 % observed nodes . the running time drops by several orders of magnitude when we prune mini graphs so that the error is ≤ 0002 This shows that our pruning strategy is effective for the leave one out inference needed for well calibrated probabilities .
7 . CONCLUSION AND FUTURE WORK
In this paper we defined and motivated the problem of selective prediction of labels in a social network . Our study over two large social networks revealed that existing graphbased prediction algorithms do not provide well calibrated probabilities — a pre requisite for selective prediction . We proposed a new node conditional likelihood objective for training a graphical model based prediction algorithm , and a decoupled model for confidence estimation based on a novel unbiased training process . We provided theoretically sound graph pruning strategies for training the new objective . These ideas together provide an accurate and practical mechanism for selective predictions in social networks .
An important outcome of our work is that it raises even graver privacy concerns , than the ones raised in earlier studies based on overall accuracy [ 31 ] . An interesting area of future work is understanding the relationship between information blurring models and selective prediction models .
8 . REFERENCES [ 1 ] V . Avadhanula . Selective node labeling in social networks .
Master ’s thesis , IIT Bombay , 2013 .
[ 2 ] A . Azran . The rendezvous algorithm : Multiclass semi supervised learning with markov random walks . In ICML , 2007 .
[ 3 ] S . Bhagat , G . Cormode , and S . Muthukrishnan . Node classification in social networks . In SNDA . 2011 .
[ 4 ] M . Bilgic and L . Getoor . Effective label acquisition for collective classification . In Proc . ACM SIGKDD , 2008 .
[ 5 ] S . Chakrabarti , B . Dom , and P . Indyk . Enhanced hypertext categorization using hyperlinks . SIGMOD Rec . , 27(2):307–318 , 1998 .
[ 6 ] O . Chapelle , B . Sch¨olkopf , and A . Zien , editors .
Semi Supervised Learning . MIT Press , 2006 .
[ 7 ] G . Chaudhari . High confidence predictions in social networks . Master ’s thesis , IIT Bombay , 2013 .
[ 8 ] A . Chechetka and C . Guestrin . Focused belief propagation for query specific inference . JMLR , 9:89–96 , 2010 .
[ 9 ] Z . Cheng , J . Caverlee , and K . Lee . You are where you tweet : a content based approach to geo locating twitter users . In CIKM , 2010 .
[ 10 ] J . Eisenstein , B . O’Connor , N . A . Smith , and E . P . Xing . A latent variable model for geographic lexical variation . In Proc . EMNLP , 2010 .
[ 11 ] T . Hazan and R . Urtasun . A primal dual message passing algorithm for approximated large scale structured prediction . In NIPS , 2010 .
[ 12 ] L . Hong , A . Ahmed , S . Gurumurthy , A . J . Smola , and
K . Tsioutsiouliklis . Discovering geographical topics in the twitter stream . In Proc WWW , 2012 .
[ 13 ] D . Koller and N . Friedman . Probabilistic Graphical Models :
Principles and Techniques . MIT Press , 2009 .
[ 14 ] D . Koutra , T Y Ke , U . Kang , D . H . Chau , H K K . Pao , and C . Faloutsos . Unifying guilt by association approaches : Theorems and fast algorithms . In ECML/PKDD ( 2 ) , 2011 . [ 15 ] Q . Lu and L . Getoor . Link based classification . In Machine
Learning , ICML , pages 496–503 , 2003 .
[ 16 ] S . A . Macskassy and F . J . Provost . Classification in networked data : A toolkit and a univariate case study . JMLR , 8:935–983 , 2007 .
[ 17 ] J . Mahmud , J . Nichols , and C . Drews . Where is this tweet from ? inferring home locations of twitter users . In Proc . ICWSM , 2012 .
[ 18 ] L . K . McDowell , K . M . Gupta , and D . W . Aha . Cautious collective classification . JMLR , 10:2777–2836 , 2009 .
[ 19 ] A . Mislove , B . Viswanath , P . K . Gummadi , and
P . Druschel . You are who you know : inferring user profiles in online social networks . In WSDM , 2010 .
[ 20 ] J . Neville and D . Jensen . Relational dependency networks .
Journal of Machine Learning Research , 8 , 2007 .
[ 21 ] J . Neville and F . Provost . Predictive modeling with social networks . ICWSM , Tutorial , 2009 .
[ 22 ] A . Sadilek , H . Kautz , and J . P . Bigham . Finding your friends and following them to where you are . In WSDM , 2012 .
[ 23 ] A . G . Schwing , T . Hazan , M . Pollefeys , and R . Urtasun .
Efficient structured prediction with latent variables for general graphical models . In ICML , 2012 .
[ 24 ] P . Sen , G . Namata , M . Bilgic , L . Getoor , B . Gallagher , and T . Eliassi Rad . Collective classification in network data . AI Magazine , 29(3):93–106 , 2008 .
[ 25 ] M . Szummer and T . Jaakkola . Partially labeled classification with markov random walks . In NIPS , 2001 . [ 26 ] P . P . Talukdar , J . Reisinger , M . Pasca , D . Ravichandran , R . Bhagat , and F . Pereira . Weakly supervised acquisition of labeled class instances using graph random walks . In EMNLP , 2008 .
[ 27 ] B . Taskar , E . Segal , and D . Koller . Probabilistic classification and clustering in relational data . In IJCAI , 2001 .
[ 28 ] U . Weinsberg , S . Bhagat , S . Ioannidis , and N . Taft .
Blurme : inferring and obfuscating user gender based on ratings . In RecSys , 2012 .
[ 29 ] R . Xiang and J . Neville . Pseudolikelihood em for within network relational learning . In ICDM , 2008 .
[ 30 ] R . Xiang and J . Neville . Collective inference for network data with copula latent markov networks . In WSDM , 2013 .
[ 31 ] E . Zheleva and L . Getoor . To join or not to join : the illusion of privacy in social networks with mixed public and private user profiles . In Proc . WWW , 2009 .
[ 32 ] D . Zhou , O . Bousquet , T . N . Lal , J . Weston , and
B . Sch¨olkopf . Learning with local and global consistency . In NIPS , 2003 .
0000100111001000000000005001000150020002555000Time in sec ( log)ErrorMini graph SizeTwitter : 2%ErrorTime0000100111001000000000002000400060008001000120014550050000Time in sec ( log)ErrorMini graph SizeTwitter : 5%ErrorTime000010011100000000050010001500200025550050000Time in sec ( log)ErrorMini graph SizePokec : 2%ErrorTime000010011100100000000000500100015002000250030550050000Time in sec ( log)ErrorMini graph SizePokec : 5%ErrorTime
