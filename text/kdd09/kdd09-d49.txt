Correlated Itemset Mining in ROC Space :
A Constraint Programming Approach
Siegfried Nijssen
Tias Guns
Luc De Raedt
KULeuven , Celestijnenlaan 200A , Leuven , Belgium
{siegfriednijssen,tiasguns,lucderaedt}@cskuleuvenbe
ABSTRACT Correlated or discriminative pattern mining is concerned with finding the highest scoring patterns wrt a correlation measure ( such as information gain ) . By reinterpreting correlation measures in ROC space and formulating correlated itemset mining as a constraint programming problem , we obtain new theoretical insights with practical benefits . More specifically , we contribute 1 ) an improved bound for correlated itemset miners , 2 ) a novel iterative pruning algorithm to exploit the bound , and 3 ) an adaptation of this algorithm to mine all itemsets on the convex hull in ROC space . The algorithm does not depend on a minimal frequency threshold and is shown to outperform several alternative approaches by orders of magnitude , both in runtime and in memory requirements .
Categories and Subject Descriptors H28 [ Database Management ] : Database applications— Data Mining ; F41 [ Mathematical Logic and Formal Languages ] : Mathematical Logic—Logic and Constraint Programming
General Terms Algorithms , Theory
Keywords Itemset Mining , Constraint Programming , ROC Analysis
1 .
INTRODUCTION
Correlated pattern mining is amongst the most popular data mining tasks . As opposed to frequent itemset mining , correlated pattern mining involves transactions that belong to two different classes and the task is to find those patterns that are correlated with the class attribute , that is , those patterns that are indicators of one of the two classes . In this paper , we formalize this task as that of finding the patterns that score high wrt a correlation measure , such as χ2 , information gain , etc . , without applying a support threshold . A large number of publications is concerned with correlated pattern mining [ 21 , 24 , 22 ] , but the problem is also known under the names of interesting itemset mining [ 3 ] , contrast set mining [ 1 ] , emerging itemset mining [ 12 ] , subgroup discovery [ 26 , 18 , 16 ] and discriminative itemset mining [ 8 , 9 , 27 , 13 ] . Further publications have extended these settings to structured domains , such as graphs , trees and sequences [ 5 , 17 , 6 , 30 ] . The popularity of correlated patterns is in part due to their use for classification , where they have been used both as classification rules [ 30 , 5 , 9 , 27 , 13 ] and as features for building classifiers [ 6 , 11 ] . The key difference between traditional rule learning and correlated pattern mining approaches is that the former are typically heuristic and the latter provide guarantees concerning completeness and optimality of the computed solutions .
We revisit the problem of correlated pattern mining using the principles of ROC analysis and Constraint Programming ( CP ) . First , by reformulating correlation measures in PN space ( a rescaling of ROC space common in ROC analysis ) , building on [ 22 , 15 ] , we contribute a theoretical framework that allows a better understanding and comparison of different bounds used for pruning in correlated pattern mining . More specifically , we investigate various n support bounds that are used for pruning in correlated pattern mining ; these are bounds that are based on n different support values . We show that the 2 support bound of [ 21 ] is tighter than the 1 support bound proposed by [ 8 ] , and also show how the 2 support bound can be generalized into a novel 4 support bound , which allows for extra pruning .
Secondly , we formulate the task of correlated itemset mining as a constraint programming problem ( cf . [ 10] ) . The main argument for this formalisation is that it allows us to incorporate additional constraints in the mining process in an easy and flexible manner , among which closure constraints . This extends earlier results in the application of constraint programming to data mining , by allowing to deal with optimization problems and correlation measures . The resulting approach is incorporated in both an off the shelf constraint programming solver and a specialized itemset miner . Employing the 4 support bound , both implementations are shown to be highly efficient and orders of magnitude faster than the state of the art systems of [ 21 , 9 ] .
Finally , we study how to find all optimal itemsets independent of the correlation measure . We do this by mining the itemsets on the convex hull in ROC space . We relate this approach to an algorithm by Bayardo et al . for mining interesting itemsets [ 3 ] . We show that the hull can be mined effectively and that this set of patterns is surprisingly small . This paper is organized as follows . First we review earlier work on branch and bound search for correlated patterns ( Section 2 ) , and review our approach for formalizing itemset mining in constraint programming ( Section 3 ) . Subsequently , we show how to formalize correlated pattern mining in constraint programming , and prove the correctness of our formalization in Section 4 . We perform experiments in Section 5 which explain in detail the reasons for the significant improved performance of our method compared to other algorithms for the basic setting . In Section 6 we study how to mine the convex hull of itemsets . We conclude in Section 7 .
2 . CORRELATED PATTERN MINING
2.1 Problem Setting
While introducing the problem of correlated pattern mining , we assume that the reader is familiar with the standard formulation of frequent itemset mining , and focus on the differences between frequent and correlated itemset mining . We assume we are given a set of possible items I =
{1 , 2 , . . . , m} and a dataset D consisting of n examples {(X1 , y1 ) , . . . , ( Xn , yn)} , where yt ∈ {+ , −} is a class label and Xt ⊆ I . It will sometimes be convenient to represent the database as a binary matrix , where Dti = 1 if and only if i ∈ Xt and Dti = 0 otherwise . Furthermore , T denotes the transaction identifiers T = {1 , 2 , . . . , n} . Given a set of transactions T , T c denotes the set of transaction identifiers in T having class c , {t | t ∈ T , yt = c} . We use ϕ(I ) = {t | t ∈ T , I ⊆ Xt} to denote the set of transactions in database D in which itemset I occurs and ϕc(I ) as shorthand notation for ( ϕ(I))c . The vector σ(T ) = ( |T +| , |T −| ) is called the stamp point of transaction set T ; similarly , we define the stamp points of an itemset I as σ(I ) = σ(ϕ(I ) ) = σ(|ϕ+(I)| , |ϕ−(I|) ) . So the stamp point of an itemset is a vector ( p , n ) where p is the frequency of this itemset in T + and n is the frequency of this itemset in T − .
In correlated itemset mining , we are given a function which computes a correlation score for an itemset , that is , a function f ( σ(I) ) , or short handed f ( I ) . One such measure is information gain :
IG(p , n ) = H(|T +| , |T −| ) − p + n |T |
· H(p , n ) −
|T | − ( p + n )
|T |
· H(|T +| − p , |T −| − n ) , where
H(a , b ) = − a a + b
· log2( a a + b
) − b a + b
· log2( b a + b
) .
Theorem 1 . Fisher score , information gain , gini index , and chi square are ZDC correlation measures .
Definitions , as well as independent , alternative proofs of this theorem , can be found in [ 8 , 20 , 21 ] . A plot of χ2 is given in Figure 1 , and illustrates these two properties . A characteristic of these correlation measures is that they are symmetric : both f ( |T +| , 0 ) and f ( 0 , |T −| ) are local maxima . Asymmetric scoring functions , that is , functions in which only one of f ( |T +| , 0 ) and f ( 0 , |T −| ) is a local maximum , are also common ; examples include Laplace value , confidence and conviction ; furthermore , the growth rate used in emerging patterns is also asymmetric [ 12 ] . We focus our attention in this paper on the harder , symmetric case , even though our method can be extended to the asymmetric case .
The top k correlated itemset mining problem can now be defined as :
Given a database D over a set of items I , an integer k , and a correlation measure f ; Find the k highest ranked itemsets I ⊆ I according to their f ( I ) values .
In this paper we focus mostly on this optimization version of the correlated itemset mining problem , for the case that k = 1 . An alternative problem is to find all itemsets I ⊆ I for which f ( I ) ≥ θ , given a threshold θ ; our technique can easily be adapted towards these other cases .
Notice that none of these formulations involves a minimum frequency threshold . Some authors have defined discriminative patterns as those patterns that are frequent in one class and infrequent in another , given two support thresholds [ 19 ] ; in this work we do not consider approaches that require the explicit specification of frequency thresholds .
It is often useful to restrict one ’s attention to closed itemsets [ 25 , 29 ] , which are itemsets whose supersets have a different frequency . Given a set of transactions T , the largest itemset in common between these transactions is ψ(T ) = ∩t∈T Xt . An itemset I is closed iff I = ψ(ϕ(I) ) . Closed itemsets form a condensed representation of the set of all frequent itemsets : if there is an itemset with correlation score θ , there also is a closed itemset with this score . Usually closed itemsets can be found faster . Taking closedness into account we obtain the problem of closed top k correlated itemset mining .
2.2 Bounds
Most correlated pattern miners employ a branch and bound algorithm to avoid having to enumerate all possible itemsets . They realize this by using bounds on correlation . In our approach , the following result will be essential .
Other , similar measures are χ2 and Fisher scores . These scoring functions have two important properties .
Theorem 2 . Let f be a ZDC correlation measure and
0 ≤ p1 ≤ p2 and 0 ≤ n1 ≤ n2 . Then
Definition 1 . A scoring function is zero diagonal con vex ( ZDC ) if it has the following two properties : max
( σ1,σ2)∈[p1,p2]×[n1,n2 ] f ( σ1 , σ2 ) = max{f ( p1 , n2 ) , f ( p2 , n1)}
• the function is convex , ie , for every pair of stamp points σ 6= σ′ it holds that ∀0 ≤ α ≤ 1 : f ( ασ + ( 1− α)σ′ ) ≤ αf ( σ ) + ( 1− α)f ( σ′ ) .
• the function reaches its minimum in all stamp points on the diagonal in PN space , ie ,
∀0 ≤ α ≤ 1 : f ( α|T +| , α|T −| ) = 0 .
The bound states that to find the highest possible score in a rectangle of stamp points , it suffices to check two corners of the rectangle . A proof can be found in the appendix . The bound generalizes earlier bounds , eg , those of [ 21 , 8 ] which cover cases where p1 = n1 = 0 .
We will now first review the bounds that were used in earlier methods and then show how our bound improves on them .
80
0 40
20 n
0
10
20 p
30
40
Figure 1 : A plot of the χ2 scoring function , and a threshold on χ2 .
Figure 2 : The 1 support bound in PN space .
Figure 3 : The 2 support bound in PN space .
Figure 4 : The 4 support bound in PN space .
In discriminative itemset mining [ 8 , 9 , 27 , 13 ] , the search is performed depth first , using similar data structures as traditional frequent itemset miners ( such as FP Trees in the DDPMine algorithm [ 9] ) . The aim is to prune those itemsets I for which it can be shown that no J ⊇ I can have f ( J ) ≥ f ( I ∗ ) , where I ∗ is the best itemset found so far . The main limitation of discriminative itemset miners is that during the search , only the total support values of itemsets are determined . The supports on the classes separately are not taken into account . Hence , given an itemset I , we need to derive a bound on f ( J ) for all J ⊇ I from the value |ϕ(I)| only . Given this information , the stamp points ( p , n ) of the itemsets J which can possibly be reached satisfy
As a consequence a correlated itemset miner that uses 1support pruning can never be more efficient than a frequent itemset miner run with support threshold min{p∗ , n∗} .
The restriction that only |ϕ(I)| is available during the search is not present in the pruning strategy proposed in [ 21 ] and used in [ 21 , 24 , 22 , 6 , 30 , 5 , 17 , 27 ] . This method assumes that both |ϕ+(I)| and |ϕ−(I)| are available during the search , and , hence , it can be determined that an itemset J ⊇ I can only have stamp points ( p , n ) with
0 ≤ p ≤ |ϕ+(I)| ,
0 ≤ n ≤ |ϕ−(I)| .
This yields the following pruning strategy .
Theorem 4
( 2 Support Pruning ) . Given an itemset
0 ≤ p ≤ |T +| ,
0 ≤ n ≤ |T −| , p + n ≤ |ϕ(I)| ,
I and a ZDC correlation measure f , if where p = |ϕ+(J)| and n = |ϕ−(J)| . This leads to the following result , which can be used for pruning itemsets .
Theorem 3
( 1 Support Pruning ) . Given an itemset
I and a ZDC correlation measure f , if max{f ( min{|ϕ(I)| , |T +|} , 0 ) , f ( 0 , min{|ϕ(I)| , |T −|})} < f ( I ∗ ) then ∀J ⊇ I : f ( J ) < f ( I ∗ ) .
In Figure 2 we visualize this in PN space [ 15 ] . Each point in PN space corresponds to a stamp point . An isometric in PN space is a line that connects stamp points with the same score according to a correlation function . For a given optimal itemset I ∗ with score f ( I ∗ ) , we have plotted the isometric f ( p , n ) = f ( I ∗ ) ; stamp points that improve the score f ( I ∗ ) are found in the upper left and lower right corner ( see Figure 1 ) . Furthermore , we have drawn the isometrics p + n = |ϕ(Ik)| for three given itemsets I1 , I2 , I3 . Itemset I1 can be pruned using the 1 support bound but itemsets I2 and I3 cannot be pruned , because given only the total support information |ϕ(Ik)| ( k = 1 , 2 ) , we cannot conclude whether the score of an itemset J ⊇ Ik can exceed f ( I ∗ ) or not . The area of itemsets that can be pruned is indicated in gray .
The importance of this bound is that it relates minimum correlation to minimum support , and hence makes it possible to apply traditional itemset miners to correlated itemset mining . As can be seen in PN space , given an itemset I ∗ , there is a stamp point ( p∗ , 0 ) in which f ( p∗ , 0 ) = f ( I ∗ ) and a stamp point ( 0 , n∗ ) in which f ( 0 , n∗ ) = f ( I ∗ ) . We can reformulate the pruning condition as follows : max{f ( min{|ϕ(I)| , |T +|} , 0 ) , f ( 0 , min{|ϕ(I)| , |T −|})} < f ( I ∗ )
⇐⇒ |ϕ(I)| < min{p∗ , n∗} .
( 1 ) max{f ( |ϕ+(I)| , 0 ) , f ( 0 , |ϕ−(I)|)} < f ( I ∗ ) then ∀J ⊇ I : f ( J ) < f ( I ∗ ) . This bound is visualized in Figure 3 . In our example we can now also prune itemset I2 , as the additional support information allows us to determine that the best stamp point we can reach from I2 is either ( 0 , |ϕ−(I2)| ) or ( |ϕ+(I2)| , 0 ) , and these scores are lower than f ( I ∗ ) . Also in general the 2 support bound is more powerful than the 1 support bound . Both 1 pruning and 2 pruning assume that the best reachable itemset J ⊇ I covers zero examples in one of the two classes . We will now drop this assumption . Assume that we know for all J ⊇ I enumerable in the depth first search below I , there is a set U ⊆ ϕ(J ) of unavoidable transactions ( thus the unavoidable transactions are those that are covered by all refinements of I ) . Then the additional support information |U +| and |U −| can be used to obtain an improved 4 support pruning condition : max{f ( |ϕ+(I)| , |U −| ) , f ( |U +| , |ϕ−(I)|)} < f ( I ∗ ) .
This is illustrated in Figure 4 , where we are computing a bound for itemsets J with U ⊆ ϕ(J ) . We have indicated the stamp point σ(U ) . Observe that I3 is now also pruned , as any itemset I ′ ⊇ I3 is assumed to cover at least |U +| positive and |U −| negative examples , and both f ( |ϕ+(I)| , |U −| ) < f ( I ∗ ) and f ( |U +| , |ϕ−(I)| ) < f ( I ∗ ) . The area of stamp points pruned , as indicated by the gray box in Figure 4 , is largest for the 4 support bound . A practical complication is however the need to compute the set U . The simplest solution would be to choose U = ∅ , which reduces the 4 pruning bound to the 2 support bound . Ideally , U should be as large as possible , without affecting the optimality of the search . To solve this problem , we will employ the principles of constraint programming that we introduce in the next section .
Algorithm 1 Constraint Search(Dom(V ),C ) 1 : Dom(V ) :=propagate till fix point(Dom(V ) ) 2 : if ∃v ∈ V : dom(v ) = ∅ then 3 : 4 : end if 5 : if ∃x ∈ V : |dom(x)| > 1 then 6 : 7 : 8 : 9 : 10 : else 11 : 12 : end if select such an x using some heuristic for all d ∈ dom(x ) do
Output solution end for return
Constraint Search(Dom(V ) , C ∪ {x = d} )
3 . CONSTRAINT PROGRAMMING
Constraint programming ( CP ) is a flexible programming paradigm in which problems are specified declaratively using constraints on variables . More formally , a problem is specified by a set of variables V , for each variable v an associated domain dom(v ) , and a set of constraints C on possible assignments of values to these variables . In this paper , we shall only employ binary domains , that is , for all variables dom(v ) = {0 , 1} . The problem then is to find an assignment of values to all the variables in V that satisfies all constraints in C . Solutions are found using general purpose solvers that combine two main ideas . First , solvers employ constraint propagation pervasively . This means that the domains of variables are iteratively reduced by exploiting the constraints amongst the variables . By shrinking the domains of the variables , one effectively reduces the number of possible variable assignments to consider . To compute the reduced domains , constraint programming techniques employ propagators . Secondly , when it is no longer possible to reduce the domains ( and hence a fix point of the propagation is reached ) , the solver branches , which means that it selects a variable v ( according to some heuristic ) and recursively calls the solver while assigning a value to this variable . This process is continued until a domain becomes empty and the solver backtracks , or a solution is found . The resulting algorithm is summarized in Algorithm 1 , where Dom(V ) represents the set of all domains of the variables V , and C the set of constraints .
In [ 10 ] we studied how to formalize constrained itemset mining in CP systems . We will illustrate this for the problem of frequent itemset mining . In its CP formulation we search in parallel for an itemset I and a transaction set T . We use as variables V = {I1 , . . . , Im} ∪ {T , . . . , Tn} ; Ii represents in a final solution , Ii = 1 corresponds to i ∈ I ; item i ; Tt represents transaction t ; Tt = 1 corresponds to t ∈ T . Moreover , two constraints need to be satisfied :
T = ϕ(I )
|T | ≥ θ
( 2 ) ( 3 )
The combination of these constraints expresses the traditional support constraint that |ϕ(I)| ≥ θ must hold . The problem of frequent itemset mining can be reformulated in terms of constraint programming as follows ; cf . [ 10 ] . Given are a database D and a threshold θ . The goal is to find
Ii , Tt ∈ {0 , 1} such that
∀t ∈ T : Tt = 1 ↔ X
Ii(1 − Dti ) = 0 i∈I
∀i ∈ I : Ii = 1 → X
TtDti ≥ θ t∈T
( 4 )
( 5 )
The first constraint is called the coverage constraint , the second the support constraint . The coverage constraint states that a transaction Tt = 1 if and only if t is covered by I , that is , it belongs to ϕ(I ) . The support constraint basically states that if an item Ii = 1 then the support of i in T should be larger than θ . To see why these CP constraints correspond to itemset mining constraints ( 2 ) and ( 3 ) , consider that Pi∈I Ii(1 − Dti ) = 0 iff ∀i ∈ I : Ii = 0 ∨ Dti = 1 iff I ⊆ Xt iff t ∈ ϕ(I ) . Also , knowing that T = ϕ(I ) , we can deduce for I 6= ∅ that |T | ≥ θ iff ∀i ∈ I : |ϕ({i} ) ∩ T | ≥ θ iff ∀i ∈ I : Ii = 1 → Pt∈T TtDti ≥ θ .
A constraint of the form
Ii = 1 → X
TtDti ≥ θ , t∈T
( 6 ) containing a sum within an implication , is called a reified summation constraint . Reified constraints are readily available in many CP systems and in our case allow for pervasive propagation . The propagation for the constraint of equation ( 6 ) is performed as follows . Let us define the minimum value of Tt according to its domain as T min = mind∈dom(Tt ) d and the maximal value as T max = maxd∈D(Tt ) d . Then the propagator for constraint ( 6 ) is the following : t t if X
T max t Dti
<
θ then
I max i
=
0 . t∈T
This propagator states that when the highest possible value the sum can take is too low , dom(Ii ) is modified to not contain the value 1 . In a similar way , propagators for other reified summation constraints can be defined ; the CP system will execute a propagator whenever one of its variables is changed ( even though the order of execution is often system dependent ) . Using these constraints and propagators in Algorithm 1 results in a search that is similar to many specialized depth first itemset miners . Indeed , once we set dom(Ii ) = {1} in line 8 of Algorithm 1 , we include item i in the itemset , and recurse for the resulting itemset ; in line 1 the propagation ensures that first transactions that do not include item i are set to T max = 0 ; then , all items which are no longer frequent in T are set to I max = 0 , and the search recurses over those items that have not been fixed yet . t i
The key advantage of constraint programming is that it is very general and flexible . In an itemset mining context this manifests itself in that it is easy to incorporate further constraints and also to combine them in a complex way . All that is needed to realize this is a formulation of the constraint and a propagator . The CP system takes care that solutions are found . In [ 10 ] it has been shown that the above CP formulation of frequent itemset mining can be extended to cope with anti monotonic , monotonic , succinct , closed , free , fault tolerant patterns , etc . One example of such a constraint that will be used later on in this paper is that for obtaining closed sets :
∀i ∈ I : Ii = 1 ↔ X
Tt(1 − Dti ) = 0 ;
( 7 ) t∈T
This closedness constraint is very similar to the coverage constraint and the propagation of this constraint can be shown to emulate the search strategy of LCM [ 25 ] .
Also for other constraints , other itemset miners are emulated ; for instance , for monotonic constraints , CP emulates the pruning strategies of ExAnte and DualMiner [ 4 , 7 ] .
4 . CORRELATED ITEMSET MINING IN CP While previously we have shown how to tackle a wide variety of constraint based itemset mining problems with constraint programming [ 10 ] , the questions were left open as how to tackle top k queries or to find correlated patterns . This section shows how the optimization version of correlated itemset mining can be realized using constraint programming . It can be specified as follows . Given is a database D and a ZDC correlation measure f ; the problem is to find Ii , Tt ∈ {0 , 1} and the maximal value θ ∈ R such that the following constraints are satisfied .
∀t ∈ T : Tt = 1 ↔ X
Ii(1 − Dti ) = 0
( 8 ) i∈I
∀i ∈ I : Ii = 1 → f ( X t∈T +
TtDti , X
TtDti ) ≥ θ ( 9 ) t∈T −
This formulation is called the CP version of the top 1 correlated itemset mining problem . The first constraint is again the coverage constraint , the second one is the correlation constraint . It is shown in the appendix that this reified formulation is correct , cf . Theorem 5 . As argued before , it is easy to add further constraints to the CP engine in a flexible manner . One constraint that is useful in the context of correlated pattern mining is the closedness constraint , which shall be used in our experiments . Furthermore , using the 4 support bound , the following propagator can be shown to be sound and complete for the correlation constraint ( 9 ) , cf . Theorem 6 and its proof in the appendix . if max {f ( X t∈T + t Dti , X T max
T min t Dti ) , t∈T − t Dti , X T min
T max t Dti)} < θ t∈T − f ( X t∈T + I max i
= 0 .
( 10 ) then
To deal with the optimization criterion in CP , the threshold θ is increased during the search whenever a solution is found . Most CP systems provide functionality for this .
An interesting question is now how the propagation of this propagator differs from the search strategies used in traditional correlated itemset miners . It is important to see that the propagator also considers the T min if T min = 1 , transaction t is unavoidable and included in all transaction sets deeper down the search . To see how T min is set we need to consider the propagators for the coverage constraint ( 8 ) . One propagator is the following : variables ; t t t if X
( 1 − Dti)I max i
= 0 then
T min t
= 1 .
( 11 ) i∈I
Following [ 7 ] , let us denote by Itail = {i | I max = 1} the largest itemset that can still be reached at a specific point in the search space . This propagator states that if for all i
Figure 5 : Example of how CP prunes the search . items i ∈ Itail we have that Dti = 1 , ie Itail ⊆ Xt , then we know that T min
= 1 , that is , transaction t is unavoidable . t i
CP systems propagate constraints whenever changes in a domain occurs . This leads to two types of propagation that are uncommon in existing correlated itemset miners . First , if the search branches over an item i and excludes it by setting I max = 0 , the data is scanned again , as the removal of this item might make certain transactions unavoidable , which may improve the bounds for remaining items ( using the 4 support bound ) . This contrasts with most itemset miners that only scan the data when an item is added to an itemset . Second , propagation is repeated as long as changes in domains occur , as illustrated in the following example , which is visualized in Figure 5 .
In this example we assume that the best itemset found so far is the itemset {1} . This itemset reaches a certain score θ ; the curved lines indicate the isometrics for this score . The itemset which we are refining is the itemset {2} , the possible items which we can add to this itemset are 3 , 4 , 5 and 6 . Hence , we evaluate the support of itemsets {2 , 3} , {2 , 4} , {2 , 5} , {2 , 6} and {2 , 3 , 4 , 5 , 6} ( step 2 ) . Itemset {2 , 3 , 4 , 5 , 6} is Itail in the initial situation and is the largest itemset that can still be reached ; it determines the set of unavoidable transactions by propagating the coverage constraint using propagator ( 11 ) . Assume that we find that the support of {2 , 3 , 4 , 5 , 6} is zero in both classes . The gray box indicates the stamp points of itemsets that will be pruned . In our example , {2 , 6} falls within this box . Consequently we know that no itemset containing {2 , 6} will achieve sufficient correlation . Hence we remove 6 from consideration . This means that the largest itemset we can reach now is itemset {2 , 3 , 4 , 5} , whose supports in the classes are evaluated ( step 3 ) to determine the number of unavoidable transactions . Even though these supports may be small , the isometrics of many correlation measures are such that the set of prunable itemsets becomes significantly larger . In our example we now find that {2 , 4} cannot achieve significant correlation . Consequently , we remove item 4 from consideration and we evaluate the support of {2 , 3 , 5} ( step 4 ) . As a result , the prunable region is increased , and we find that no pattern can reach the desired correlation score ; we prune the search for all itemsets below {2} .
5 . EXPERIMENTS
In this section , we report on experiments for top 1 correlated itemset mining . We compare several variations of our approach and other state of the art algorithms to get insight in the differences in performance between the approaches .
Experimental setup .
We use data from the UCI repository1 . To deal with missing values we preprocessed each dataset by first eliminating all attributes having more than 10 % of missing values and then removing all instances ( transactions ) for which the remaining attributes still had missing values . Numerical attributes were discretized in a number of binary splits , setting thresholds for these splits using equal frequency binning . Nominal attributes are represented using one item for every value . For multi class problems we combined all smallest classes together in one class . Experiments were run on PCs with Intel Core 2 Duo E6600 processors and 4GB of RAM , running Ubuntu Linux . The code of our implementation and the datasets used are available on our website2 . To make an empirically fair comparison of the three different bounds , we implemented them all in a Constraint Programming system . The system we use is Gecode3 , which is an open and efficient CP solver . Because of its generality , it was sufficient to plug in a propagator for each of the bounds , and model the problem as defined in Section 4 . The resulting system is called cimcp . The correlation measure used is information gain , although any ZDC correlation measure can be supplied . As variable selection heuristic the mostconstrained heuristic was used ; this is comparable with an item ordering by increasing frequency . In our earlier work [ 10 ] we noticed that the Gecode system incurs extra overhead on very sparse data . In a specialized itemset mining implementation , based on the same principles , this overhead would not occur . To show this , and to show that any principle discovered in CP can be straightforwardly added to existing itemset miners , we implemented the specialized correlated itemset miner corrmine . It implements the same propagation as the CP system , including iterative 4 support pruning and closedness pruning ; it is based on ECLAT [ 28 ] , a memory efficient depth first miner . Also this algorithm is provided on our website . In all experiments we only mine for closed itemsets . We found that this is always more efficient . We will compare our approach with the state of the art ddpmine algorithm , presented in [ 9 ] . ddpmine is based on an implementation of FPGrowth by Zhu and Grahne [ 29 ] , which stores closed itemsets in memory during the search to prevent duplicate itemsets from being found ; it uses 1support pruning . In our experiments the algorithm often ran out of memory ; this will be indicated by a “ − ” in our result tables . In our experiments we answer the following questions .
Q1 : How many discretization bins should be used ? .
In Figure 6 we compare the runtime of ddpmine , cimcp and corrmine on the same dataset , but discretized with a different number of bins . Using more bins , one can expect the correlation score of the correlated itemsets to increase ;
1http://archiveicsuciedu/ml/ 2http://wwwcskuleuvenbe/∼dtai/CP4IM/ 3http://gecode.org
Figure 6 : Runtime for the anneal dataset , having 812 transactions . The number of items ranges from 53 to 147 .
Name anneal australian cr breast wisc diabetes german cr heart clevel hypothyroid ionosphere kr vs kp letter mushroom pendigits primary tum segment soybean splice 1 vehicle yeast
Dense Trans 812 653 683 768 1000 296 3247 351 3196 20000 8124 7494 336 2310 630 3190 846 1484
0.45 0.41 0.50 0.50 0.34 0.47 0.49 0.50 0.49 0.50 0.18 0.50 0.48 0.50 0.32 0.21 0.50 0.49
Item 4 sup 0.22 0.30 0.28 2.45 2.39 0.19 0.71 1.44 0.92 52.66 14.11 3.68 0.03 1.45 0.05 30.41 0.85 5.67
93 125 120 112 112 95 88 445 73 224 119 216 31 235 50 287 252 89
2 sup 24.09 0.63 13.66 128.04 66.79 2.15 10.91 > 46.20 > 13.48 > 0.13 > 0.07 31.11 > 781.63
1 sup 72.71 17.52 228.08 >
> 29.58 >
> 713.35 > 27.31 > 0.85 > 0.38 35.02 >
>
Table 1 : Statistics of UCI datasets , and runtimes , in seconds , of the CP model for the different bounds . however , the size of the transaction database will also increase , as will the runtimes . Using 20 bins , the ddpmine algorithm is no longer able to find the optimal solution because of memory problems . To keep runtimes at reasonable levels for all algorithms we use 8 bins in further experiments .
Q2 : How efficient are the 1,2 and 4 support bounds ? .
Table 1 shows the difference in runtime when using the different bounds . The density of a dataset , listed in column 2 , is the fraction of 1 ’s in the binary representation of the transaction database . “ > ” indicates that no solution was found within 900 seconds . Using the 1 support bound , we can only find the optima for 10 out of the 18 datasets . Using the 2 support bound , the same optima are found significantly faster , and for even more datasets . The 4 support bound finds these even faster , and for all datasets . For the mushroom dataset , the 4 support bound takes slightly more time than the 2 support bound , but the next experiment will show that this is due to the overhead incurred in CP systems on data of low density . In general it is clear that using the 4 support bound results in the most pruning power , reducing the search space enough to make exhaustive search feasible , and even fast .
Q3 : Are 4 support bound miners faster than the stateof the art ? .
Table 2 shows the result of comparing the corrmine , cimcp and ddpmine implementations on the different datasets . We also compare the runtimes with LCM , one of the fastest , most memory efficient closed itemset mining implementation from the FIMI challenge [ 25 ] . We ran LCM with the
Name anneal australian credit breast wisconsin diabetes german credit heart cleveland hypothyroid ionosphere kr vs kp letter mushroom pendigits primary tumor segment soybean splice 1 vehicle yeast avg . when found : corrmine 0.02 0.01 0.03 0.36 0.07 0.03 0.02 0.24 0.02 0.65 0.03 0.18 0.01 0.06 0.01 0.05 0.07 0.80 0.15 cimcp 0.22 0.30 0.28 2.45 2.39 0.19 0.71 1.44 0.92 52.66 14.11 3.68 0.03 1.45 0.05 30.41 0.85 5.67 6.55 ddpmine 22.46 3.40 96.75 lcm 7.92 1.22 27.49 − 697.12 − 30.84 2.87 9.49 − > − > 25.62 125.60 − > 0.03 0.09 − > 0.08 0.26 − > 0.02 0.05 1.86 0.02 − > − 185.28 28.88+ 81.54+
Table 2 : Runtimes , in seconds , of 3 correlated and one frequent itemset miner . minimum support threshold of equation ( 1 ) ( see Section 2 ) , obtained by first running our system to find the optimal itemset . It therefore represents the minimal amount of time a 2 phase algorithm would need , which would first enumerate all frequent closed itemsets , and in a second step would select the most correlated one . corrmine , our specialized implementation using 4 support pruning , is by far the most efficient on all datasets . The difference in runtime between corrmine and cimcp illustrates the overhead of using the general CP system . ddpmine does not find a solution for many of the datasets ( indicated by “ − ” ) . This is a limitation of the pruning of the 1 support bound , as our implementation of the 1 support bound in Table 1 is not able to find the solution in a reasonable amount of time either . For the mushroom and splice datasets , ddpmine is faster than cimcp , but the results of our specialized implementation show convincingly that this is due to the overhead incurred in the Gecode system on sparse data . The runtimes of LCM confirm that a 2 phase algorithm can never be faster than a specialized 1 step algorithm like corrmine . In general , using the 4 support bound results in more effective algorithms , with some extra efficiency to be gained by creating a specialized implementation .
6 . MINING THE CONVEX HULL
In the problem setting of correlated itemset mining we assumed we were given a ZDC correlation measure f . The correlation measure can be seen as a parameter of the method . In exploratory data mining parameter free methods are often preferred . We can obtain a parameter free setting by informally formulating the following problem setting : “ can we find all itemsets for which there exists a ZDC correlation measure under which the itemset is optimal ? ” In ROC analysis it is well known that the problem of finding such itemsets ( rules , classifiers ) can be solved by determining which ones are on the convex hull in ROC space [ 14 ] . Exploiting the aforementioned relation between correlated itemset mining and ROC analysis , we can now apply this result to correlated itemsets .
Due to lack of space , we here skip a formal definition of a
Figure 7 : Comparison of an isometric , a convex hull and the border of Bayardo et al . : an isometric is a continuous curve ; the convex hull is piece wise linear ; Bayardo ’s border also contains points not on the convex hull . convex hull ; intuitively , given a set of points in P N space , the convex hull consists of all stamp points on a rubber band that surrounds all stamp points as tightly as possible ( including the ( 0 , 0 ) and ( |T +| , |T −| ) points ) . For symmetric measures , the hull has two sides , one for each of the two classes . An example is given in Figure 7 . We are interested in finding all ( closed ) itemsets on the convex hull .
To find itemsets on the convex hull we need to implement a new propagator . The main idea behind this propagator is that the convex hull is a curve in P N−space , similar to an isometric for a correlation score ; this curve is piece wise linear between the stamp points on this curve ( see Figure 7 ) . Hence , instead of modifying the isometric during the search , by updating the correlation threshold , we will update the piece wise linear curve when a new solution is found . When adding a new stamp point to the hull , previous hull points that are no longer on the convex hull are removed .
To find all itemsets on the hull , we replace propagator ( 10 ) with the following propagator : if in hull( X t∈T + X t∈T + t Dti , X T max
T min t Dti , t∈T − t Dti , X T min
T max t Dti ) t∈T − then
I max i
= 0 .
( 12 )
Here , in hull(p2 , n1 , p1 , n2 ) checks whether we can still reach a stamp point outside the current convex hull . This test is performed as follows . We split the convex hull in two sets of stamp points : one set of stamp points S − above the diagonal , ie stamp points ( p , n ) in which n/|T −| > p/|T +| , and one set of remaining stamp points below the diagonal , S + . Then we check the following :
• let ( p+
↓ , n+
↓ ) ∈ S + be the largest stamp point for which ↑ ) ∈ S + be the smallest stamp p+ ↓ < p2 and ( p+ point for which p+ ↑ − p+
↑ , n+ ↑ > p2 ; we propagate if ↓ ) < ( n1 − n+
( p2 − p+
↓ )/(n+
↓ )/(p+
↑ − n+
↓ ) ;
• let ( p−
↓ , n−
↓ ) ∈ S + be the largest stamp point for which ↑ ) ∈ S − be the smallest stamp
↓ < n2 and ( p− n− point for which n− ↑ − n−
↑ , n− ↑ > n2 ; we propagate if ↓ ) < ( p1 − p−
( n2 − p−
↓ )/(n−
↓ )/(p−
↑ − p−
↓ ) .
In Table 3 we compare the runtime of finding the top 1 most correlated itemset , according to the information gain measure , with the runtime needed to find all itemsets on the
Name anneal australian credit breast wisconsin diabetes german credit heart cleveland hypothyroid ionosphere kr vs kp letter mushroom pendigits primary tumor segment soybean splice 1 vehicle yeast average : cimcp time ( s ) 0.22 0.30 0.28 2.45 2.39 0.19 0.71 1.44 0.92 52.66 14.11 3.68 0.03 1.45 0.05 30.41 0.85 5.67 6.55 cimcp convex hull time ( s ) 0.44 1.33 0.83 11.9 3.93 0.37 3.01 8.69 1.75 405.14 32.45 45.79 0.07 8.96 0.09 40.13 4.12 25.51 33.03 size of hull 17 22 20 30 21 20 19 15 17 34 10 19 16 6 9 10 22 28 18.61
Table 3 : Mining the top 1 correlated itemset versus all itemsets on the convex hull in ROC space . convex hull . Both are implemented in Gecode . Although more time is always required to mine the entire convex hull , we believe most runtimes are within acceptable boundaries . Except for datasets that incur extra overhead in Gecode , mining all itemsets on the hull is even faster than existing approaches to finding the top 1 itemset . The results show that mining all itemsets on the convex hull is a realistic alternative to having to choose a correlation measure up front . Furthermore , the results in Table 3 also show that the number of patterns is reasonably small , on average 19 itemsets on the entire convex hull .
This algorithm for mining itemsets on the convex hull is related to the algorithm of Bayardo et al . for finding interesting correlated itemsets [ 2 ] . Bayardo et al . ’s method for finding interesting correlated itemsets is an indirect method , in which a border set of itemsets is computed even if the correlation function is fixed ; the optimal pattern is always chosen from this border set . Bayardo et al . ’s border is similar to the convex hull in ROC space . The main difference is that they define a border of non dominated itemsets in support confidence space , which is a transformation of PN space . Restricting ourselves to one half of PN space , one itemset I dominates another itemset I ′ iff it dominates both in support and confidence , ie |ϕ+(I)| > |ϕ+(I ′)| and |ϕ+(I)|/|ϕ(I)| > |ϕ+(I ′)|/|ϕ(I ′)| . This border representation is illustrated in PN space in Figure 7 . In this figure we have highlighted several isometrics for confidence . The border is along iso support and iso confidence lines ; we see that the border of Bayardo et al . includes redundant stamp points compared to the convex hull representation .
7 . CONCLUSIONS
We have revisited the problem of correlated pattern mining starting from an analysis of correlation measures in PNspace and principles of constraint programming . This has allowed us to clarify the relationship between different bounds used for pruning and has also led to the introduction of a new bound that allows for significantly more pruning . The approach has been empirically evaluated and shown to outperform state of the art methods , either in terms of speed or memory requirements . Finally , we have adapted our technique for finding the set of all itemsets on the convex hull , a problem of relevance to the machine learning and classification literature .
Several challenges still remain . First , it is unclear in how far our results can be extended towards structured domains , as the approach depends on an upper bound which may not be present in some structured pattern domains . Second , even though our algorithm can be plugged into many existing classification algorithms , an interesting question is whether it can be integrated in some algorithms exploiting ROC spaces [ 23 ] .
Acknowledgements .
We are grateful to Albrecht Zimmermann for discussions , and to Hong Cheng for providing the implementation of DDPMine . This work was supported by a Postdoc and a project grant from the Research Foundation—Flanders , project “ Principles of Patternset Mining ” .
8 . REFERENCES [ 1 ] S . D . Bay and M . J . Pazzani . Detecting change in categorical data : Mining contrast sets . In KDD , pages 302–306 , 1999 .
[ 2 ] R . J . Bayardo Jr . and R . Agrawal . Mining the most interesting rules . In KDD , pages 145–154 , 1999 .
[ 3 ] R . J . Bayardo Jr . , R . Agrawal , and D . Gunopulos .
Constraint based rule mining in large , dense databases . In ICDE , pages 188–197 , 1999 . [ 4 ] F . Bonchi and C . Lucchese . Extending the state of the art of constraint based pattern discovery . Data Knowl . Eng . , 60(2):377–399 , 2007 .
[ 5 ] B . Bringmann and A . Zimmermann . Tree2 decision trees for tree structured data . In PKDD , pages 46–58 , 2005 .
[ 6 ] B . Bringmann , A . Zimmermann , L . De Raedt , and
S . Nijssen . Don’t be afraid of simpler patterns . In PKDD , pages 55–66 , 2006 .
[ 7 ] C . Bucila , J . Gehrke , D . Kifer , and W . M . White . DualMiner : A dual pruning algorithm for itemsets with constraints . Data Min . Knowl . Discov . , 7(3):241–272 , 2003 .
[ 8 ] H . Cheng , X . Yan , J . Han , and C W Hsu .
Discriminative frequent pattern analysis for effective classification . In ICDE , pages 716–725 , 2007 .
[ 9 ] H . Cheng , X . Yan , J . Han , and P . S . Yu . Direct discriminative pattern mining for effective classification . In ICDE , pages 169–178 , 2008 .
[ 10 ] L . De Raedt , T . Guns , and S . Nijssen . Constraint programming for itemset mining . In KDD , pages 204–212 , 2008 .
[ 11 ] M . Deshpande , M . Kuramochi , N . Wale , and
G . Karypis . Frequent substructure based approaches for classifying chemical compounds . IEEE Trans . Knowl . Data Eng . , 17(8):1036–1050 , 2005 .
[ 12 ] G . Dong and J . Li . Efficient mining of emerging patterns : Discovering trends and differences . In KDD , pages 43–52 , 1999 .
[ 13 ] W . Fan , K . Zhang , H . Cheng , J . Gao , X . Yan , J . Han ,
P . S . Yu , and O . Verscheure . Direct mining of discriminative and essential frequent patterns via model based search tree . In KDD , pages 230–238 , 2008 .
[ 14 ] T . Fawcett . An introduction to ROC analysis . Pattern
Recognition Letters , 27(8):861–874 , 2006 .
[ 15 ] J . F¨urnkranz and P . A . Flach . ROC ’n’ rule learning – towards a better understanding of covering algorithms . Machine Learning , 58(1):39–77 , 2005 . [ 16 ] H . Grosskreutz , S . R¨uping , and S . Wrobel . Tight optimistic estimates for fast subgroup discovery . In ECML/PKDD ( 1 ) , pages 440–456 , 2008 .
[ 17 ] M . Hirao , H . Hoshino , A . Shinohara , M . Takeda , and
S . Arikawa . A practical algorithm to find the best subsequence patterns . Theor . Comput . Sci . , 292(2):465–479 , 2003 .
[ 18 ] B . Kavsek , N . Lavrac , and V . Jovanoski .
APRIORI SD : Adapting association rule learning to subgroup discovery . In IDA , pages 230–241 , 2003 . [ 19 ] S . Kramer , L . De Raedt , and C . Helma . Molecular feature mining in HIV data . In KDD , pages 136–143 , 2001 .
[ 20 ] Y . Morimoto , T . Fukuda , H . Matsuzawa ,
T . Tokuyama , and K . Yoda . Algorithms for mining association rules for binary segmentations of huge categorical databases . In VLDB , pages 380–391 , 1998 .
[ 21 ] S . Morishita and J . Sese . Traversing itemset lattice with statistical metric pruning . In PODS , pages 226–236 , 2000 .
[ 22 ] S . Nijssen and J . N . Kok . Multi class correlated pattern mining . In KDID , pages 165–187 , 2005 .
[ 23 ] R . C . Prati and P . A . Flach . ROCCER : An algorithm for rule learning based on ROC analysis . In IJCAI , pages 823–828 , 2005 .
[ 24 ] J . Sese and S . Morishita . Answering the most correlated n association rules efficiently . In PKDD , pages 410–422 , 2002 .
[ 25 ] T . Uno , M . Kiyomi , and H . Arimura . Lcm ver . 2 :
Efficient mining algorithms for frequent/closed/maximal itemsets . In FIMI , 2004 .
[ 26 ] S . Wrobel . An algorithm for multi relational discovery of subgroups . In PKDD , pages 78–87 , 1997 .
[ 27 ] X . Yan , H . Cheng , J . Han , and P . S . Yu . Mining significant graph patterns by leap search . In SIGMOD Conference , pages 433–444 , 2008 .
[ 28 ] M . J . Zaki , S . Parthasarathy , M . Ogihara , and W . Li . New algorithms for fast discovery of association rules . In KDD , pages 283–286 , 1997 .
[ 29 ] J . Zhu and G . Grahne . Reducing the main memory consumptions of FPmax* and FPclose . In FIMI , 2004 .
[ 30 ] A . Zimmermann and B . Bringmann . CTC correlating tree patterns for classification . In ICDM , pages 833–836 , 2005 .
APPENDIX
Theorem 2 . Let f be a ZDC correlation measure and
0 ≤ p1 ≤ p2 and 0 ≤ n1 ≤ n2 . Then max
( σ1,σ2)∈[p1,p2]×[n1,n2 ] f ( σ1 , σ2 ) = max{f ( p1 , n2 ) , f ( p2 , n1)} .
Proof . The proof is similar to that of [ 21 ] . First , we observe that the function is convex . Hence , we know that the maximum in a space [ p1 , p2 ] × [ n1 , n2 ] is reached in one of the points ( p1 , n1 ) , ( p1 , n2 ) , ( p2 , n1 ) and ( p2 , n2 ) . Next , we need to show that we can ignore the corners ( p1 , n1 ) and ( p2 , n2 ) . For this we observe that the minimum is reached on the diagonal . We can distinguish several situations . diagonal . We know for the stamp point ( |T +| diagonal that f ( |T +| know then that f ( |T +|
If n1/|T −| < p1/|T +| , the point ( p1 , n1 ) is ‘below’ the |T −| n1 , n1 ) on the |T −| n1 , n1 ) = 0 . Due to the convexity we |T −| n1 , n1 ) = 0 ≤ f ( p1 , n1 ) ≤ f ( p2 , n1 ) . Similarly , we can show that if ( p1 , n1 ) is above the diagonal that f ( p1 , n1 ) ≤ f ( p1 , n2 ) ; that f ( p2 , n2 ) ≤ f ( p2 , n1 ) if ( p2 , n2 ) is below the diagonal ; and that f ( p2 , n2 ) ≤ f ( p1 , n2 ) if ( p2 , n2 ) is above the diagonal .
Theorem 5 . The CP version of top 1 correlated itemset mining is equivalent to the top 1 correlated itemset mining problem .
Proof . We show that the itemset I = {i | Ii = 1} satisfying the CP version of the problem is a solution to the original top 1 correlated itemset mining problem . We already showed that the two versions of the coverage constraint ( 4 ) and ( 2 ) are equivalent . Hence , we only need to show that the correlation constraint ( 9 ) ensures that f ( |ϕ+(I)| , |ϕ−(I)| ) ≥ θ . To see this , observe that we can rewrite ϕ(I ) as follows :
ϕ(I ) = {t ∈ T |∀i ∈ I : Dti = 1} = \
ϕ({i} ) i∈I
Using this observation , it follows that : f ( |ϕ+(I)| , |ϕ−(I)| ) ≥ θ
⇐⇒ f (
ϕ+({j} )
\ j∈I
⇐⇒ ∀i ∈ I :
,
˛˛˛˛˛
˛˛˛˛˛
\
ϕ−({j} ) j∈I
) ≥ θ
˛˛˛˛˛
˛˛˛˛˛
˛˛˛˛˛
˛˛˛˛˛
,
˛˛˛˛˛ f (
ϕ+({i} ) \
ϕ+({j} ) j∈I
ϕ−({i} ) \
ϕ−({j} ) j∈I
) ≥ θ
⇐⇒ ∀i ∈ I : f ( ˛˛ϕ+({i} ) ∩ ϕ+(I)˛˛ , ˛˛ϕ−({i} ) ∩ ϕ−(I)˛˛ ) ≥ θ ⇐⇒ ∀i ∈ I : Ii = 1 → f ( X t∈T +
TtDti , X
TtDti ) ≥ θ . t∈T −
˛˛˛˛˛
Finally , because θ is required to be maximal in the CP formulation , an itemset reaching at least score θ is a top 1 itemset .
Theorem 6
( Propagator for Reified Correlation ) .
The propagator in equation ( 10 ) is sound and complete for the correlation constraint ( 9 ) . t
Proof . ( Soundness ) Given the current values for T max , We know that the stamp points σ that we can and T min still reach are in [ Pt∈T + T min t Dti , Pt∈T − T max [ Pt∈T − T min orem 2 , the best score we can still reach is given by the two corners checked in the propagator . If the condition of the propagator is no longer satisfied , we know that the condition Ii = 1 can no longer be satisfied , and change the domain . t Dti , Pt∈T + T max t Dti ] . Hence , according to The t Dti]× t
( Completeness ) We need to show that if all variables are fixed , the constraint is not satisfied iff the propagator derives a contradiction . This follows from the fact that if all variables are fixed , the two stamp points checked in the propagator are the same stamp point , ie , the stamp point occurring in the constraint .
