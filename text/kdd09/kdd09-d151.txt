Tell Me Something I Don’t Know :
Randomization Strategies for Iterative Data Mining
Sami Hanhijärvi , Markus Ojala , Niko Vuokko , Kai Puolamäki , Nikolaj Tatti , Heikki Mannila
Helsinki Institute for Information Technology HIIT Department of Information and Computer Science
Helsinki University of Technology , Finland fistnamelastname@tkkfi
ABSTRACT There is a wide variety of data mining methods available , and it is generally useful in exploratory data analysis to use many different methods for the same dataset . This , however , leads to the problem of whether the results found by one method are a reflection of the phenomenon shown by the results of another method , or whether the results depict in some sense unrelated properties of the data . For example , using clustering can give indication of a clear cluster structure , and computing correlations between variables can show that there are many significant correlations in the data . However , it can be the case that the correlations are actually determined by the cluster structure .
In this paper , we consider the problem of randomizing data so that previously discovered patterns or models are taken into account . The randomization methods can be used in iterative data mining . At each step in the data mining process , the randomization produces random samples from the set of data matrices satisfying the already discovered patterns or models . That is , given a data set and some statistics ( eg , cluster centers or co occurrence counts ) of the data , the randomization methods sample data sets having similar values of the given statistics as the original data set . We use Metropolis sampling based on local swaps to achieve this . We describe experiments on real data that demonstrate the usefulness of our approach . Our results indicate that in many cases , the results of , eg , clustering actually imply the results of , say , frequent pattern discovery .
Categories and Subject Descriptors H28 [ Database management ] : Database Applications— Data mining ; G.3 [ Probability and Statistics ] : Markov processes
General Terms Algorithms , experimentation , theory
Keywords Statistical significance , matrix randomization , inverse frequent set mining
1 .
INTRODUCTION
The data mining research of the past 15 years has produced a wide collection of algorithms for exploratory data analysis .
In this paper we consider a simple question . Suppose we have a dataset D , and we first run an analysis using algorithm A1 on D , obtaining interesting results A1(D ) . Then we use another method A2 on D , and again obtain fine results A2(D ) . How do we know whether the second result is actually just a consequence of the first , or does it somehow increase our information about the data ? So does A2(D ) tell us something we don’t know , or is it just rephrasing the result that we already saw when A1(D ) was given to us ?
For example , using clustering can give indication of a clear cluster structure , and computing correlations between variables can show that there are many significant correlations in the data . However , it can be the case that the correlations are actually determined by the cluster structure .
In this paper , we consider the problem of randomizing data so that previously discovered patterns or models are taken into account . The randomization methods can be used in iterative data mining . At each step in the data mining process , the randomization produces random samples from the set of data matrices satisfying the already discovered patterns or models .
Example .
Consider the toy task of analyzing the dataset shown in Figure 1 . We can first look at the row and column margins ( sums of rows and columns ) in the data , observing for instance that the column margins vary between 3 and 7 and the row margins between 1 and 6 .
The next step in the analysis could be to find the frequent itemsets using minimum frequency of , say , 3 . The resulting itemsets are
A , B , C , D , E , F , H , AB , AC , AH , BC , BH , CD , CE ,
CF , CH , DE , EF , ABC , ABH , ACH , BCH , ABCH , all with frequency 3 expect itemsets A , E , F , H , AC , CH with frequency 4 , itemset D with frequency 5 and itemset C with frequency 7 .
The result contains multiple itemsets . Thus it is natural to ask which of them are interesting ?
379 A B C D E F G H 1 1 1 0 0 0 1 1 1 0 1 0 0 1 0 0 1 1 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 1 1 1 0 0 1
Figure 1 : Example 0–1 dataset
A possible method is to use randomization . Given the row and column margins , we can [ 5 , 9 ] generate datasets that have these margins but are otherwise random . Then we can see how often the frequencies of the sets above are higher in the real data than in the generated datasets , ie , we can compute empirical p values for the frequencies of the dataset.1
By preserving row and column margins in randomization , the frequencies of the itemsets
AB , BH , ABC , ABH , BCH , ABCH are found to be statistically significant with significance level α = 005 Thus even given the information about the row and column margins the frequencies of these itemsets are interesting . The corresponding empirical p values are
0.044 , 0.041 , 0.023 , 0.004 , 0.015 , 0003
The other empirical p values are notably larger .
Now the question is , are the significances of the itemsets independent of each other ? In other words , are the larger itemsets ABC , ABH , BCH , ABCH statistically significant only because the smaller itemsets AB , BH are ?
To answer the question , we would like to compute empirical p values in a way that takes the already known information into account . That is , we would like to constrain the sampling of datasets so that each dataset will have the same frequencies for the itemsets AB , BH . This effectively forms a new null hypothesis , which states that the dataset is a random dataset with specific row and column margins and frequencies of the itemsets AB , BH . Using this randomization and assessing the statistical significances of the itemsets ABC , ABH , BCH , ABCH , we discover that none of these itemsets are statistically significant with this new null hypothesis . The corresponding empirical p values are
0.229 , 0.683 , 0.222 , 0170
We can therefore conclude that in this example , the frequencies of the smaller itemsets AB , BH and the marginal distributions explain the frequencies of the larger itemsets . We continue analyzing the dataset by clustering the rows into two clusters using k means . The first four rows are found to form the first cluster and the last five rows the second cluster . To assess the significance of the clustering given the row and column information , we compare the original k means clustering error to clustering errors on randomized datasets where the row and column margins are preserved .
1As there are several patterns , we should also correct for multiple testing [ 1 , 11 ] , but for simplicity we omit that consideration in the example .
We obtain a p value of 0.011 implying that the dataset contains a clustering structure which is independent of the row and column margins .
Now it would be tempting to conclude that the dataset contains some significant itemsets and an interesting clustering structure . But are the frequent itemsets and the clusters independent of each other ? If we again preserve the frequencies of the itemsets AB , BH and the row and column margins in randomization , we get an empirical p value of 0.096 for the clustering structure of the dataset! That is , the cluster structure does not seem that interesting , given also the data about the two itemsets .
We can also do the reverse : We can preserve the clustering structure in addition to the row and column margins in randomization and test the significances of the frequencies of the itemsets AB , BH , ABC , ABH , BCH , ABCH . In this case we obtain the empirical p values
1.000 , 0.239 , 1.000 , 0.239 , 0.239 , 0.239 , respectively , thus implying that the clusters and the frequent itemsets depend on each other .
Contents of the paper .
In this paper we consider the approach described in the above example in a general form . We give a general problem statement , show hardness of the randomization problem and give simple approximate algorithms for the task . We describe experiments on real data that demonstrate the usefulness of our approach . Our results indicate that in many cases , the results of , eg , clustering actually imply the results of , say , frequent pattern discovery .
The rest of this paper is organized as follows . In Section 2 we give the basic definitions . The general problem of randomizing datasets given the information of some other analyses is stated in Section 3 . Section 4 gives some complexity results for this problem , showing that it is in most cases NPhard . The randomization algorithms based on Metropolis techniques are given in Section 5 , while the empirical results are described in Section 6 . Section 7 is a short conclusion .
Related work .
Randomization is a widely used method in statistics [ 10 , 20 ] . The main benefit is that the user is releaved from the often difficult , and sometimes impossible , task of defining an analytical distribution for the test statistic . It is sometimes easier to devise a way of sampling from the null hypothesis than to actually define it analytically . And integrating over the analytical distribution , which is needed for the p value calculation , may not be straightforward . MCMC methods are constructed to this purpose , which are also methods for randomization .
Based on a wide body of work in ecology ( see [ 5] ) , [ 9 ] discusses the randomization of binary matrices when the size of the matrix as well as the column and row margins are to be maintained . The authors use a Markov chain using local swaps that respect the marginal distributions . The paper [ 16 ] discusses the same idea for real matrices , where the marginal distributions are no longer single integers for rows or columns , but distributions . They use the MetropolisHastings method to define transition probabilities for their local changes to maintain the desired marginal distributions . We take influence from these papers to produce our methods . Our concepts are closely related to the concept on non
380 derivability [ 4 ] . An itemset X is derivable if we can reduce the exact support of X from the supports of its sub itemsets . For example , if the support of A is 0 , then the support of AB must also be 0 . This means that if we preserve the supports of the sub itemsets , then the support of X will be constant , thus its p value will be 1 . Hence , our method can be seen as a generalization of non derivability in which we can still remove insignificant X even though we do not know the exact support of X .
A closely related idea for iterative knowledge discovery is presented in [ 13 ] , where randomization is used in model selection to test if a candidate model is significantly better than the current model . The current model is replaced by a significantly better candidate model and the process is repeated , effectively carrying out model selection iteratively . The pattern ordering problem considered in [ 15 ] tries to order a collection of patterns so that each pattern gives as much information about the data as possible , given the earlier patterns . However , in that approach there is no consideration of randomization or significance , and the approach is only applicable to simple patterns . A related idea for closed itemsets , where statistical significance is used to order the patterns , is presented in [ 7 ] . For another type of approach to significance of patterns , see [ 12 , 18 , 19 ] .
2 . PRELIMINARIES
In this section , we describe how randomization approach is used in significance testing , give the definition of empirical p values and discuss the method by Besag and Clifford for calculating MCMC p values . First , however , we introduce the notation used in the paper . 2.1 Notation
Let D be a 0–1 dataset with m rows and n columns . The approach we describe is not limited to 0–1 data , but for simplicity we consider just such data in the paper . The rows of D correspond to transactions and the columns to attributes . The notation Dtx refers to the element at row t and column x in the matrix D . An itemset X is a subset of the attributes , ie , X ⊂ {1 , . . . , n} . A transaction t covers an itemset X if Dtx = 1 for all x ∈ X . The frequency of an itemset X in the dataset D is the number of transactions t that cover the itemset X , denoted by fr(D ; X ) . A family of itemsets F is a set of itemsets , F = {X1 , . . . , Xk} . A clustering C of the rows of D is a partition of the set {1 , . . . , m} . The row and column margins of a dataset D are the row and column sums of D . 2.2 Randomization Approach
Consider a 0–1 dataset D with m rows and n columns . Assume that some data mining task , such as frequent itemset mining , is performed on D . Let S(D ) be the result of the data mining task . We assume that the result S(D ) can be described with a single number , and we call such a result the structural measure of D . It can be , eg , the frequency of a given itemset , the number of frequent itemsets or the clustering error of the dataset . Any measure can be used as long as larger ( or smaller ) values mean stronger presence of the structure . To assess whether the result S(D ) is explained by certain characteristics of the original dataset , we generate random m × n sized 0–1 datasets ˆD , which share the given characteristics with D , and compare the original result S(D ) with the results S( ˆD ) on the randomized datasets . We can , eg , preserve the row and column margins in randomization and assess the number of frequent itemsets . 2.3 Empirical p Values
Let ˆD = { ˆD1 , . . . , ˆDk} be a collection of independent randomized versions of the original dataset D . The one tailed empirical p value of S(D ) for S(D ) being large is
|{ ˆD ∈ ˆD | S( ˆD ) ≥ S(D)}| + 1 k + 1
,
( 1 )
This gives the fraction of randomized datasets whose structural measure is larger than the original S(D ) . The onetailed empirical p value when small values of S(D ) are interesting , and the two tailed empirical p value are defined similarly . If the obtained p value , adjusted for multiplicity if needed , is less than a given threshold α , say , α = 0.05 , we can regard the result to be independent of the characteristics preserved in randomization . 2.4 MCMC p Values
We will use Markov chain Monte Carlo methods to produce the randomized datasets . The samples produced by Markov chains are generally not independent thus breaking the validity of the empirical p value . However , we will use the approach by Besag and Clifford [ 3 ] to guarantee the exchangeability of the samples . In the approach the chain is started from D and run backwards for K steps to produce a new starting state ˆD0 . Each randomized dataset ˆD is produced by starting a new chain from ˆD0 and running the chain K steps forwards . This produces an exchangeable set of samples {D , ˆD1 , . . . , ˆDk} which ensures the validity of the empirical p value regardless of the independence . If the samples are dependent , we obtain just more conservative pvalues , see [ 2 , 3 ] for more details . Our methods turn out to be time reversible , ie , running the chain backwards is the same as running the chain forwards .
3 . PROBLEM STATEMENT
In this section we formulate the general specific random ization problems discussed in the introduction . 3.1 General Problem Statements
In randomization we want to preserve certain characteristics of the original 0–1 dataset D , eg , the row and column margins . Let f ( D ) be a statistics function which calculates these statistics of the dataset D . To measure the similarity between two datasets D and ˆD in the corresponding statistics f ( D ) and f ( ˆD ) , we define a difference measure h(f ( D ) , f ( ˆD ) ) which is a positive function between two sets of statistics f ( D ) and f ( ˆD ) , where D and ˆD are 0–1 datasets with the same size . Any difference measure can be used as long as a smaller difference means that the statistics f ( D ) and f ( ˆD ) are closer to each other and a zero difference that the statistics are equal .
We introduce two general randomization problems . In the first problem the statistics are preserved exactly where as in the second problem datasets with small difference in the statistics are sampled with high probability . The problem statements are as follows .
Problem 1
( ExactRand ) . Given a 0–1 dataset D and a statistics function f , generate a dataset ˆD chosen indepen
381 dently and uniformly from the set of 0–1 datasets having the same size and the same statistics as D , ie , f ( D ) = f ( ˆD ) .
Problem 2
( SoftRand ) . Given a 0–1 dataset D , a statistics function f , a difference measure h(f ( D ) , f ( ˆD ) ) and a scaling constant w > 0 , generate a dataset ˆD chosen with a probability p ∝ exp{−wh(f ( D ) , f ( ˆD))} from all 0–1 datasets having the same size as D . Note that when w = ∞ , SoftRand reduces to ExactRand . 3.2 Specific Problem Statements
Next we give the problem statements of the three specific randomization tasks discussed in the introduction . The problems Margins , ClusterMargins and ItemsetMargins are examples of the problem ExactRand .
Problem 3
( Margins ) . Given a 0–1 dataset D , generate a dataset ˆD chosen independently and uniformly from the set of 0–1 datasets having the same row and column margins as the dataset D .
Problem 4
( ClusterMargins ) . Given a 0–1 dataset D and a clustering C of the rows of D , generate a dataset ˆD chosen independently and uniformly from the set of 0–1 datasets having the same row and column margins as well as the same cluster centers and variances for each cluster in C as the dataset D .
Problem 5
( ItemsetMargins ) . Given a 0–1 dataset D and a family of itemsets F , generate a dataset ˆD chosen independently and uniformly from the set of 0–1 datasets having the same row and column margins as well as the same frequencies for the itemsets in F as the dataset D .
It turns out that the problem ItemsetMargins is much harder than Margins and ClusterMargins , see Section 4 . Thus we introduce SoftRand version of ItemsetMargins . Let D be the original dataset , ˆD a randomized dataset and F a family of itemsets which we are trying to preserve in randomization . We define the difference in the itemset frequencies of F between the datasets D and ˆD as | fr(D ; X ) − fr( ˆD ; X)| , hF ( D , ˆD ) =
X
X∈F
( 2 ) where we have combined the statistics function directly into the difference measure .
Problem 6
( ItemsetMarginsSoft ) . Given a binary dataset D , a family of itemsets F and a scaling constant w > 0 , generate a dataset ˆD chosen with a probability p ∝ exp{−whF ( D , ˆD)} from the set of 0–1 datasets having the same row and column margins as the dataset D .
4 . COMPLEXITY RESULTS
In this section we prove that the ItemsetMargins variant defined in Section 3.2 is intractable in general case . We will do this by reducing the HamiltonCycle to ItemsetMargins . This negative result shows the need for ItemsetMarginsSoft where we allow some variation in the frequencies .
Theorem 1 . Assume that there is a random polynomial algorithm for ItemsetMargins . Then RP = NP even if the algorithm is provided with an example of such dataset .
A random polynomial algorithm is a Turing machine that is bounded by a polynomial time with an oracle producing random independent numbers . A language is said to be in RP if there is a non deterministic Turing machine such that , given a ’yes’ instance , at least half of the computational paths end up with ’yes’ . It is easy to see that RP ⊆ NP , and the usual conjecture is that RP = NP .
Our reduction is based on Hamiltonian cycles . A Hamiltonian cycle is a cycle in a graph such that every node of the graph is visited only once . Alternatively we can see the cycle as a permutation of the nodes such that adjacent nodes ( including the first and the last nodes ) are connected . The problem HamiltonCycle is FNP complete [ 17 ] .
To prove the main result we will need a couple of lemmae . The first lemma states that we can connect Hamiltonian cycles and the datasets satisfying some specific constraints .
Lemma 2 . Assume that we are given a graph G . There are column and row margins , itemset frequency constraints , and a function m that maps a dataset satisfying the constraints into a Hamiltonian cycle of G .
Proof . Assume that we are given a graph G with M nodes and N edges . Our goal is to construct appropriate constraints . At first , we will focus only on itemsets and column margins . The row margins will be discussed later . In our construction we will have 6 different attribute groups . The first attribute group O = {o1 , . . . , oM} contains M attributes . We impose the frequencies fr(oi ) = 1 , fr(oioj ) = 0 for i , j = 1 , . . . , M , i = j . These frequencies will force that for each oi there is a row on which the attribute value is 1 and that there are no other active oj on that row , ie , if we stack the rows into a matrix we will have a permutation matrix . The second group V = {v1 , . . . , vM} is similar to the first , that is , we have fr(vi ) = 1 and fr(vivj ) = 0 . This construction gives us also a permutation matrix . Also note that for each oi there is a unique vj such that there is a row in which both oi and vj are present . This allows us to define a permutation of σ(i ) = j . The main idea is that σ represents the Hamiltonian cycle . This can be achieved if we can force that the nodes in G corresponding to σ(i ) and σ(i + 1 ) are connected . We will achieve this with the rest of the attributes . The third group B = {b1 , . . . , bM} contains the attributes satisfying bi = oi ∨ oi+1 . This is done by imposing the itemsets fr(bi ) = 2 , fr(bioi ) = 1 , and fr(bioi+1 ) = 1 for i = 1 , . . . , M − 1 and fr(bM ) = 2 , fr(bM oM ) = 1 , and fr(bM o1 ) = 1 . We see that the attribute bi is present on two rows and the rows are precisely those in oi and oi+1 are present . group . We denote the group by C = ˘c1 , . . . , cM ( M−1)/2
Our fourth group of items resembles greatly the third The group contains M ( M − 1)/2 attributes , where each ci correspond to a pair of nodes ( j , k ) in the graph G . We define the frequencies such that ci is exactly vj ∨ vk . This is done by setting fr(ci ) = 2 and fr(civj ) = fr(civk ) = 1 .
¯ .
Now let us consider what happens if σ(i ) and σ(i + 1 ) are not connected in G . There is a cj corresponding to the node pair ( σ(i ) , σ(i + 1 ) ) such that the columns bi and cj are equivalent . In other words , we have fr(bicj ) = 2 . Thus , to guarantee that σ is indeed a valid Hamiltonian cycle we need to make sure that for all bi and cj corresponding to unconnected pairs in G we have fr(bicj ) < 2 .
382 Since we do not have inequality constraints in our setup we will have to simulate it . Let L = M ( M − 1)/2− N be the number of pairs of unconnected nodes . We define the fifth set of attributes , denoted by S = {s1 , . . . , sLM} , to contain LM attributes , one sij for each pair ( bi , cj ) , where cj represent a pair of unconnected nodes . The needed inequality constraint is simulated by setting fr(sij ) = 1 , fr(sijbi ) = 1 , and fr(sijcj ) = 0 . This construction forces bi and cj to be unequal so that the frequency fr(bicj ) = 2 .
We see now that using the defined itemsets and column margins from all 5 groups forces the permutation σ to be a valid Hamiltonian cycle .
Let us now turn the attention to the row margins . The number of 1s in a single row is as follows : one 1 from group O , one 1 from group V , two 1s from group B and M − 1 1s from group C . Group S is problematic since the number of 1 varies per row . We remedy this by defining the sixth group E = {e1 , . . . , eLM} . This group contains LM attributes . We set ei to be the negation of si . We achieve this by setting fr(ei ) = M −1 and fr(siei ) = 0 . Now the common number of 1s on single row in group S and E is LM . Hence by setting all the row margins to 1 + 1 + 2 + M − 1 + LM we have created a set of constraints such that a dataset satisfying the constraints corresponds to a Hamiltonian cycle .
Our second lemma states that for each Hamiltonian cycle in a given graph there is exactly same number of datasets satisfying the constraints .
Lemma 3 . Given a graph G and a Hamiltonian cycle H . Let m be the map given in Lemma 2 . Define X = {D ; m(D ) = H} to be the datasets which m maps into H . Then |X| is a constant not depending on the Hamiltonian cycle H .
Proof . To prove the lemma first note that a dataset satisfying the constraints has M unique rows . Thus there are M ! datasets obtained by permuting the rows . Assume now that the group O has the shape of the identity matrix . There are exactly 2M different permutations for a given Hamiltonian cycle , ie , exactly 2M different configurations for V . Note that the groups B and C are determined completely by the groups O and V and that the group E is determined by the group S . The theorem is proved if we can show that there are a constant number of configurations for S .
Note that there are 2U configurations for S where U is the number of sij for which fr(bicj ) = 0 . The frequency fr(bicj ) is 0 when both the σ(i ) and σ(i+1 ) do not contain the nodes corresponding to the cj . For a fixed j there are M − 4 of such sij . Hence we have U = ( M − 4)L . Combining the numbers together we have that for a fixed Hamiltonian cycle we have exactly M !2M 2(M−4)L datasets which is a constant . This proves the theorem .
Proof of Theorem 1 . We start the proof by considering a related NP complete problem called SecondHamilton [ 17 ] . In this problem we are given a Hamiltonian graph , a Hamiltonian cycle , and we are asked if there is a second Hamiltonian cycle different from the given one .
Now let us consider a problem RandomHamilton having the same input as SecondHamilton in which we are asked to give a random Hamiltonian cycle of the graph . Assume that we have a random polynomial algorithm H for this problem . Now consider the problem of SecondHamilton . We can replace H with a non deterministic machine by considering all the possible random outputs of the oracle .
··· 1 ··· 0··· ··· 0 ··· 1··· s t x y
⇐⇒ s t
··· 0 ··· 1··· ··· 1 ··· 0··· x y
Figure 2 : A swap in a 0–1 matrix .
Then we compare the random Hamiltonian cycle returned by H with a given Hamiltonian cycle . If the cycles differ , then we return ’yes’ , otherwise ’no . This means that if there is another cycle in the graph , then at least 1/2 of the computation paths ends up with ’yes’ . Thus we have shown that SecondHamilton is in RP . But SecondHamilton is NP complete and this proves that NP = RP .
Now assume that there is a polynomial algorithm for ItemsetMargins . Such an algorithm is given a set of constraints and an example dataset satisfying the constraints . By using the construction given in Lemma 2 we can use that algorithm for sampling Hamiltonian cycles in polynomial time . The given Hamiltonian cycle can be also easily transformed into a dataset needed for the input of the algorithm .
The only thing we need to show is that this reduction produces Hamiltonian cycles from the uniform distribution . Since we have assumed that the datasets are coming from the uniform distribution , it suffices to prove that there are same number of datasets for each Hamiltonian cycle in a fixed graph . But this is exactly the statement of Lemma 3 .
5 . ALGORITHMS
Next we introduce algorithms for solving the problems Margins , ClusterMargins and ItemsetMarginsSoft . First we introduce the method by Gionis et al . [ 9 ] for solving Margins . Our methods for ClusterMargins and ItemsetMarginsSoft extend the method for solving Margins . 5.1 Preserving Row and Column Margins
The method for producing random datasets ˆD having the same row and column margins as the original dataset D is based on swaps . In each swap two rows s , t and two columns x , y are selected such that Dsx = Dty = 1 and Dsy = Dtx = 0 . In a swap the four elements are swapped as shown in Figure 2 . A swap preserves the row and columns sums . A randomized dataset ˆD is produced by performing K attempts of swaps . This is given in Algorithm 1 . The existence of self loops guarantees that the stationary distribution is uniform , see [ 9 ] for more details .
Algorithm 1 Swap Input : Dataset D , num . of swap attempts K Output : Randomized dataset ˆD 1 : ˆD ← D 2 : for i ← 1 to K do 3 : 4 : 5 : 6 : 7 : end for 8 : return ˆD
Pick s , t and x , y such that ˆDsx = 1 , ˆDty = 1 if ˆDsy = 0 and ˆDtx = 0 then ˆD ← swapped version of ˆD end if
383 5.2 Preserving Clustering Structure
To obtain an algorithm for the problem ClusterMargins , we modify Algorithm 1 to preserve the given clustering C . At each step we pick a cluster C ∈ C and attempt a swap inside that cluster . When the rows s , t belong to the same cluster , the cluster centers and variances do not change . The pseudocode of this approach is given in Algorithm 2 .
Algorithm 2 Cluster Swap Input : Dataset D , partition C of D , num . of swap at tempts K
Output : Randomized dataset ˆD 1 : ˆD ← D 2 : for i ← 1 to K do 3 : 4 : 5 : 6 : 7 : 8 : end for 9 : return ˆD end if
Pick a cluster C ∈ C Pick s , t ∈ C and x , y such that ˆDsx = 1 , ˆDty = 1 if ˆDsy = 0 and ˆDtx = 0 then ˆD ← swapped version of ˆD
5.3 Preserving Itemset Frequencies
As was mentioned in Section 3 , preserving itemset frequencies exactly in general is hard . Thus we give an algorithm for solving ItemsetMarginsSoft where the itemset frequencies are preserved approximately . We use the Metropolis algorithm [ 14 ] to produce random samples from the probability distribution
π( ˆD|D,F ) ∝ exp{−whF ( D , ˆD)} , where hF is defined in Equation ( 2 ) . At each step in the Metropolis algorithm , a proposal modification D of the current state ˆD is formed . The proposal is accepted as the new state with a probability min(1 , π(D)/π( ˆD) ) . A direct implementation of the Metropolis algorithm with swaps is given in Algorithm 3 .
Algorithm 3 Itemset Swap Input : Dataset D , itemsets F , num . of swap attempts K Output : Randomized dataset ˆD 1 : ˆD ← D 2 : for i ← 1 to K do 3 : 4 : 5 : 6 : 7 : 8 : 9 : 10 : 11 : end for 12 : return ˆD
Pick s , t and x , y such that ˆDsx = 1 , ˆDty = 1 if ˆDsy = 0 and ˆDtx = 0 then D ← swapped version of ˆD a ← Uniform(0,1 ) if a < exp{−w(hF ( D , D ) − hF ( D , ˆD))} then
ˆD ← D end if end if
The same approach can be used to solve the problem SoftRand in general . However , if too many characteristics are preserved at the same time , the chain may not mix well enough . In such cases parallel tempering can be used to overcome the problem [ 8 ] . In our experiments , we consider only a reasonable amount of small itemsets as constraints .
Data set # of rows # of cols # of 1 ’s density
Paleo Courses
124 2405
139 5021
1978 65152
11.48 % 0.54 %
Table 1 : Basic characteristics of the datasets .
6 . EXPERIMENTS
In this section , we carry out several data mining experiments to demonstrate the use of our framework . We will first present a method to automatically analyze the convergence of the Markov chain . The method is then used in the subsequent experiments with two real data sets . 6.1 Setup for the Experiments
We use two real data sets : Paleo and Courses , whose basic characteristics are presented in Table 1 . The Paleo2 data contains paleontological absence/presence information about species in excavation sites [ 6 ] . The Courses data set records the courses individual students have taken in the Department of Computer Science in Helsinki University of Technology .
We used the method by Webb [ 18 ] to ensure correct treatment of multiple hypothesis . We split both data sets randomly half on rows , and used the other part for mining frequent itemsets of size 2 and 3 . The other part was used for randomization , ie , p value calculation.3
The Paleo data set was mined with minimum support 4 , and Courses with 200 . It turned out that only 41 columns of Courses are covered by the found frequent sets . We choose to ignore the columns of Courses that are not present in any of the frequent sets . We chose to consider only the frequent sets of size 2 and 3 , since it may be fairly easy to understand the co occurrence of 2 or 3 variables , but understanding itemsets of larger size is increasingly difficult .
When calculating the p values of itemsets , we use the support as the test statistic , where a higher value is more extreme , ie , more interesting . We always sample 1000 matrices and use them for p value calculation . The unadjusted , raw p values are always adjusted for multiplicity with Benjamini Hochberg method [ 1 ] , which controls the false discovery rate ( FDR ) . The level of FDR is set to 0.05 , and thus , if the p value of a pattern does not exceed this threshold value , the pattern is considered statistically significant .
We also cluster the data sets in some of the experiments . We use the traditional k means with 3 clusters for Paleo and 20 clusters for Courses . Since k means may get stuck in local optima , we run k means 10 times and use the clustering with the smallest error . When calculating the p value of a clustering , our test statistic is the sum of the square of the L2 distances between rows and their respective centroids . This is almost the same as optimized by k means , but we use this slightly modified version since we know the ClusterMargins maintains the value of this test statistic . We use the Algorithm 3 to solve ItemsetMarginsSoft , with w = 4 . That is , a swap which would increase the total error in itemset frequencies by one is accepted with probability exp(−4 ) ≈ 0018
2NOW public release 030717 available from [ 6 ] . 3We also tried not using Webb ’s method and use the complete data sets for mining itemsets as well as randomization . However , the results did not differ significantly .
384 Data set M IMS CM
Paleo Courses
4.9 64
360 5200
15 200
Table 2 : Average execution times in milliseconds to produce a single random matrix with Margins ( M ) , ItemsetMarginsSoft ( IM ) and ClusterMargins ( CM ) . The implementations are written in Java and the randomizations run on a 2.5GHz machine .
Paleo
M
N
IM
N 8832 S
60
S 145 544
Paleo
CM N
IM
N 8977 S 614
S 0 0
M
Courses
N IM N 402 41
S
S 47 330
Courses
CM
N IM N 437 326
S
S 6 51
Table 3 : Some of the contingency tables of the significance of itemsets in randomizations Margins ( M ) , ItemsetMarginsSoft ( IM ) and ClusterMargins ( CM ) . S represents statistical significance and N the opposite . All pairs of columns in a data set were used as itemsets .
The execution times to produce a single random matrix with different constraints and data sets are listed in Table 2 . Table 3 depicts several interesting contingency tables of the significance of itemsets in two randomizations at a time . We also measured the significance of the clustering result . Using the defined test statistic , the clustering was statistically significant in Margins and ItemsetMarginsSoft , but naturally not in ClusterMargins .
When comparing the contingency table of ItemsetMarginsSoft and Margins in Paleo , it is evident that many of the itemsets were not significant in either randomization . Still , several itemsets were statistically significant in both cases . One example of such an itemset with two species is Brachyodus and Bunolistriodon . Both were contained in separate itemsets constrained in ItemsetMarginsSoft . Apparently , having both species in separate itemset constraints did not require to maintain the support of their combined itemset , and hence , it was statistically significant .
Other interesting itemsets are the ones that were statistically significant in ItemsetMarginsSoft but not in Margins . One example of such itemset is Hyainailouros and Amphimoschus . Again both species were contained in separate itemset in the constraints , but their combined itemset was not . These constraints had a clear decreasing effect in the support of the itemset , and therefore , it was found significant when randomizing with ItemsetMarginsSoft , but not with Margins .
The contingency table between ClusterMargins and ItemsetMarginsSoft shows that no itemset was statistically significant when constraining the randomization with clustering . Even though the Paleo data set is known to have three clear clusters , the extent of this constraint was surprising . We expected to see at least few itemsets significant in ClusterMargins . Evidently the clustering result fully explains the pairwise correlations of the species in the data set .
Figure 3 : Boxplot of distances between the original matrix and 5 ItemsetMarginsSoft randomized matrices of Courses data with different number of swap attempts . The x axis depicts the multiplier to use with the number of ones in Courses to get the number of swap attempts .
6.2 Convergence Analysis
In our experiments we used an automatic convergence analysis method that is based on the distance between the original and the randomized matrix . As a distance we used the square of the Frobenius norm between these two matrices [ 16 ] . With 0–1 matrices , this is essentially the number of cells where the two matrices differ . When swapping , this distance usually first increases rapidly , until after some number of swaps it tends to settle around a certain value . The intuition is that when the distance does not change much , the Markov chain is considered to have converged .
We calculate the number of swap attempts K needed as follows . We first assign to K the amount of ones in the matrix , as it gives an indication of the number of swaps possible in that matrix , given that the matrix is sparse . We then randomize the original matrix separately 5 times with K swaps and calculate the mean of the matrix distances between the randomized data and the original data . If the mean distance with the current K has changed less than 1 % with respect to the previous step , we consider the chain to have converged , stop iterating , and use the final value of K in the sampling . Otherwise , we multiply K by 2 and repeat the step .
As an example , we run the automated convergence analysis on Courses with 40 frequent sets of size 2 as constraints . The development of the matrix distances is given in Figure 3 . Note that since we use backward forward sampling , the chain is first traversed backwards with K swap attempts , and the resulting matrix is used as a basis for further randomizations . Eventually , the number of swap attempts from the original matrix to a matrix sample is 2K . 6.3 Significant Itemsets of Size 2
In the first experiment , we are interested in the covariances between the columns when constraining the randomization in different ways . Therefore , we focus on all possible itemsets of size 2 , and randomize with Margins , ItemsetMarginsSoft and ClusterMargins . For the constraints of ItemsetMarginsSoft we used itemsets of size 2 containing adjacent items . Notice that we did not use Webb ’s method here since we consider all possible patterns .
121416182222426283x 104distancemultiplier20212223242526272829210211385 Similar results were found from Courses . An example of a statistically significant itemset in Margins and ItemsetMarginsSoft is Computer Organization and Database Systems I . This was expected , since 90 % of students who took Database Systems I also took Computer Organization . One example of a course pair that was significant in ItemsetMarginsSoft but not significant in Margins is Models for Programming and Computing and Introduction to the Use of Computers . One possible reason for this is that this pair of courses is taken by very different kind of people . The itemsets of these groups introduce anti correlation between the courses in the example , and therefore , that itemset becomes statistically significant in ItemsetMarginsSoft .
The clustering results are again similar to the ones in Paleo , with the exception that now some itemsets are significant in ClusterMargins . An example of a pair of courses that is significant in ItemsetMarginsSoft and ClusterMargins is Reading Comprehension in English and English Oral Test . These courses are not specific to any study programme in computer science , and therefore , are not affected by clustering . The clustering mostly separates students with respect to computer science courses , which makes these general courses poorly clustered . However , this allows them to be statistically significant even when constraining the randomization with the clustering .
6.4 Using Significant Itemsets as Constraints The motivation of the second experiment is to simulate an actual data mining scenario , where the patterns are discovered by a data mining algorithm and these are then used to assess which are significant in different randomization scenarios . Essentially , the idea is to find out the possible interrelations between itemsets and clustering .
We focus on frequent sets of size 2 and 3 mined from the used data sets . Here we use again the same randomizations Margins , ItemsetMarginsSoft and ClusterMargins , but with different constraints for ItemsetMarginsSoft .
The first set of constraints in ItemsetMarginsSoft is constructed by adding the 40 most significant itemsets from Margins , ie , the ones with the smallest p values . The interest is how the significance of the other itemsets change when constraining with the most significant itemsets .
The second set of itemset constraints is used for ItemsetMarginsSoft , but now the set contains the itemsets that had the largest increase in their p value from Margins to ClusterMargins . The intuition is that these itemsets may explain the clustering since they were explained by it . We use ICM for short for this randomization .
Table 4 depicts a few interesting contingency tables of the significance of itemsets in two randomizations at a time . The clustering was statistically significant in Margins , ItemsetMarginsSoft and ICM , but not in ClusterMargins . The Paleo results between ItemsetMarginsSoft and ClusterMargins are very similar to the previous section . None of the itemsets were significant in ClusterMargins , and a portion was significant in ItemsetMarginsSoft . The strength of the clustering result clearly also affect the itemsets of size 3 .
The contingency table between ICM and ItemsetMarginsSoft displays the fact that neither of the constraint sets completely explain the data since roughly 500 itemsets were significant in both , and few hundred in either . Still , 60 % of the itemsets were explained by both constraint sets along
Paleo
CM N IM N 1882 687
S
S 0 0
Paleo
ICM
N IM N 1572 162
S
S 306 535
Courses
CM
N IM N 297 809
S
S 13 146
Courses
ICM
N CM N 265 2
S
S 841 157
Table 4 : Some of the contingency tables of the significance of itemsets in randomizations Margins ( M ) , ItemsetMarginsSoft ( IM ) , ClusterMargins ( CM ) and ItemsetMarginsSoft constrained by itemsets found using ClusterMargins ( ICM ) . S represents statistical significance and N the opposite . Frequent itemsets of size 2 and 3 were used as itemsets . with the margins . However , 1419 itemsets were never significant even with Margins , which means that the expressive power of both constraint sets is very limited .
ClusterMargins is very different from ICM . Although we tried to construct the set of itemsets to constrain in ICM in such a way that the results would be the same as with ClusterMargins , the numbers of significant itemsets do not show this .
The results for Courses display similar behavior between ItemsetMarginsSoft and ClusterMargins as in previous experiment , and different from Paleo . Clearly , the clustering of Courses does not describe the data as well as for Paleo . One example of an itemset of courses significant in both is Programming Project and Programming in Java . Evidently , these have much in common and are most likely taken both instead of just either of them .
The comparison between ClusterMargins and ICM expresses the same as with ClusterMargins and ItemsetMarginsSoft . We can conclude also here that ICM with these constraints is not as strict , and conversely , does not explain as much , as ClusterMargins . An example itemset significant in ICM but not in ClusterMargins is Information Systems Project , Programming Project and Data Communications . The last two was an itemset constrained in ICM , and the first was also a part of a separate constraint . This itemset had high frequency in some clusters but very low in others . Because of this , the randomization had little room to break the itemset in ClusterMargins . We conjecture that these courses are most likely all required in some study programme . Additionally , Programming Project and Data Communications together did not explain the occurrence of the triplet , and therefore it was significant in ICM . 6.5 Discovering Significant Itemsets Iteratively In our final experiment we conduct an iterative data mining process in which the itemsets are added iteratively to constraints . We used again the frequent sets of size 2 and 3 . First , the data is randomized with Margins and the itemset with the smallest p value is inserted to the set of itemset constraints . The data is then randomized at each iteration with ItemsetMarginsSoft and always the itemset with the smallest p value is added to the set of constraints . This resembles the situation where the user iteratively tries to understand one pattern at a time and wants to find which patterns are not explained by the already understood patterns .
386 s n r e t t a p t n a c fi n g s i i
#
( a ) Paleo
( b ) Courses
Figure 4 : The number of significant itemsets at each iteration . Notice the different vertical scale and that the lower parts of the bars have been cropped to better see the difference between iterations .
We carried out a total of 10 iterations . Figure 4 displays the number of itemsets found significant at each iteration , including the initial Margins randomization .
The results follow almost with no fail the intuition that when constraints are added , the number of significant itemsets decreases . However , this may not always be the case , as seen in the previous experiments . Sometimes adding constraints increases the statistical significance of some patterns by introducing anti correlation . Still , the intuitive results may be due to how the itemset constraints were selected . At any time , the constrained itemsets do not explain the itemsets found statistically significant . However , the significant itemsets are still likely to have some correlation between them , and adding one of them to the set of constraints will restrict the rest . Selecting itemsets in another fashion may produce very different results .
7 . CONCLUSIONS
Our focus in the paper was to study the concept of iterative data mining . The idea behind this approach is the question whether the results of one analysis explains or implies the results of another analysis . This approach can be then refined into iterative data mining process in which the user iteratively selects interesting patterns or models that are then used for updating the significance of the rest of the patterns or models .
Our approach is to produce random data sets having the same selected statistics as the original dataset . As constraining statistics we used row and column margins , itemsets , and clustering structure . Using these random data sets we computed empirical p values for our test statistics . Our experiments demonstrated that our method works in practice .
8 . REFERENCES [ 1 ] Yoav Benjamini and Yosef Hochberg . Controlling the false discovery rate : A practical and powerful approach to multiple testing . Journal of the Royal Statistical Society . Series B ( Methodological ) , 57(1):289–300 , 1995 .
[ 2 ] Julian Besag . Markov chain Monte Carlo methods for statistical inference . http://wwwimsnusedusg/ Programs/mcmc/files/besag\_tl.pdf , 2004 .
[ 3 ] Julian Besag and Peter Clifford . Generalized Monte Carlo significance tests . Biometrica , 76(4):633–642 , 1989 .
[ 4 ] T . Calders and B . Goethals . Non derivable itemset mining . Data Mining and Knowledge Discovery , 14(1):171–206 , 2007 .
[ 5 ] G . W . Cobb and Y P Chen . An application of markov chain monte carlo to community ecology . American Mathematical Monthly , ( 110):264–288 , 2003 .
[ 6 ] Mikael Fortelius and Jussi Eronen . Now – neogene of the old world . http://wwwhelsinkifi/science/now/ , 2005 . Database of fossil mammals .
[ 7 ] Arianna Gallo , Tijl Bie , and Nello Cristianini . Mini : Mining informative non redundant itemsets . In Proc . of the 11th European conference on Principles and Practice of Knowledge Discovery in Databases , 2007 .
[ 8 ] Charles J . Geyer . Markov chain monte carlo maximum likelihood . In Computing Science and Statistics : Proc . of 23rd Symposium on the Interface Foundation , 1991 . [ 9 ] Aristides Gionis , Heikki Mannila , Taneli Mielik¨ainen , and Panayiotis Tsaparas . Assessing data mining results via swap randomization . ACM Transactions on Knowledge Discovery from Data , 1(3 ) , 2007 .
[ 10 ] Phillip Good . Permutation Tests : A Practical Guide to Resampling Methods for Testing Hypotheses . Springer Verlag , 2000 .
[ 11 ] S . Holm . A simple sequentially rejective multiple test procedure . Scandinavian Journal of Statistics , 1979 . [ 12 ] Szymon Jaroszewicz . Interactive hmm construction based on interesting sequences . In Local Patterns to Global Models ( LeGo’08 ) Workshop at the the European Conference on Principles and Practice of Knowledge Discovery in Databases , 2008 .
[ 13 ] David Jensen . Knowledge discovery through induction with randomization testing . In Proc . of the 1991 Knowledge Discovery in Databases Workshop , 1991 .
[ 14 ] N . Metropolis , A . W . Rosenbluth , M . N . Rosenbluth ,
A . H . Teller , and E . Teller . Equation of state calculation by fast computing machines . Journal of Chemical Physics , 21(1):1087–91 , 1953 .
[ 15 ] Taneli Mielik¨ainen and Heikki Mannila . The pattern ordering problem . Lecture Notes in Computer Science , 2838:327–338 , 2003 .
[ 16 ] Markus Ojala , Niko Vuokko , Aleksi Kallio , Niina Haiminen , and Heikki Mannila . Randomization of real valued matrices for assessing the significance of data mining results . In Proc . of the 2008 SIAM International Conference on Data Mining , 2008 .
[ 17 ] Christos H . Papadimitriou . Computational
Complexity . Addison Wesley , 1994 .
[ 18 ] Geoffrey I . Webb . Discovering significant patterns .
Mach . Learn . , 68(1):1–33 , 2007 .
[ 19 ] Geoffrey I . Webb . Layered critical values : A powerful direct adjustment approach to discovering significant patterns . Machine Learning , 71(2–3):307–323 , 2008 .
[ 20 ] Peter H . Westfall and S . Stanley Young .
Resampling based multiple testing : examples and methods for p value adjustment . Wiley , 1993 .
0123456789109501000105011001150iteration0123456789101080110011201140iteration387
