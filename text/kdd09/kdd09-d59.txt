DynaMMo : Mining and Summarization of Coevolving
Sequences with Missing Values
Lei Li , James McCann , Nancy Pollard , Christos Faloutsos
School of Computer Science , Carnegie Mellon University
{leili , jmccann , nsp , christos}@cscmuedu
ABSTRACT Given multiple time sequences with missing values , we propose DynaMMo which summarizes , compresses , and finds latent variables . The idea is to discover hidden variables and learn their dynamics , making our algorithm able to function even when there are missing values .
We performed experiments on both real and synthetic datasets spanning several megabytes , including motion capture sequences and chlorine levels in drinking water . We show that our proposed DynaMMo method ( a ) can successfully learn the latent variables and their evolution ; ( b ) can provide high compression for little loss of reconstruction accuracy ; ( c ) can extract compact but powerful features for segmentation , interpretation , and forecasting ; ( d ) has complexity linear on the duration of sequences .
Categories and Subject Descriptors : H28 Database applications : Data mining I26 Artificial Intelligence : Learning parameter learning
General Terms : Algorithms ; Experimentation .
Keywords : Time Series ; Missing Value ; Bayesian Network ; Expectation Maximization ( EM ) .
1 .
INTRODUCTION
Time series data are abundant in many application areas such as motion capture , sensor networks , weather forecasting , and financial market modeling . The major goal of analyzing these time sequences is to identify hidden patterns so as to forecast the future trends . There exist many mathematical tools to model the evolutionary behavior of time series ( eg Linear Regression , Auto Regression , and AWSOM [ 15] ) . These methods generally assume completely available data . However , missing observations are hardly rare in many real applications , thus it remains a big challenge to model time series in the presence of missing data . We propose a method to handle the challenge , with occlusion in motion capture as our driving application . However , as shown in the experiments , our method is capable of han e n o b t o o r f o i t e a n d r o o c
0
−0.02
−0.04
−0.06
−0.08
−0.1
−0.12
−0.14
−0.16
0
DynaMMo
MSVD
Spline original missing Linear Spline MSVD DynaMMo
50
100
150
200
250
300
350 frame # the original signal
Figure 1 : Reconstruction for a jump motion with 322 frames in 93 dimensions of bone coordinates . Blue line : for root bone zcoordinate the dash portion indicates occlusion from frame 100 to 200 . The proposed DynaMMo , in red , gets very close to the original , outperforming all competitors . dling missing values in diverse settings : sensor data , chlorine levels in drinking water system , and other similar coevolving sequences .
Motion capture is a technique to produce realistic motion animation . Typical motion capture system use cameras to track passive markers on human actors . However , even when multiple cameras are used , some markers may be out of view – especially in complex motions like handshaking or modern dance . Handling occlusions is currently a manual process , taking hours/days for human experts to fill in the gaps . Figure 2 illustrates a case of motion capture data , with occlusions : A dark cell at row j , and column t denotes a missing value , for that specific time ( t th frame/column ) and for that specific joint angle ( j th row ) .
The focus of our work is to handle occlusions automatically . Straightforward methods like linear interpolation and spline interpolation give poor results ( see Section 4 ) . Ideally we would like a method with the following properties :
1 . Effective : It should give good results , both with respect to reconstruction error , but primarily agreeing with human intuition .
2 . Scalable : The computation time of the method should grow slowly with the input and the time duration T of the motion capture . Ideally , it should be O(T ) or O(T log(T ) ) , but below ( T 2 ) .
3 . Black outs : It should be able to handle “ black outs ” , when all markers disappear ( eg , a person running behind a wall , for a moment ) .
In this paper , we propose DynaMMo , an automatic method
Figure 2 : Occlusion in handshake motion . 66 joint angles ( rows ) , for ≈ 200 frames . Dark color indicates a missing value due to occlusion . Notice that occlusions are clustered . to learn the hidden pattern and handle missing values . Figure 1 shows the reconstructed signal for an occluded jumping motion . Our DynaMMo gives the best result close to the original value . Our main idea is to simultaneously exploit smoothness and correlation . Smoothness is what splines and linear interpolation exploit : for a single time sequence ( say , the left elbow x value over time ) , we expect successive entries to have nearby values ( xn ≈ xn+1 ) . Correlation reflects the fact that sequences are not independent ; for a given motion ( say , “ walking ” ) , the left elbow and the right elbow are correlated , lagging each other by half a period . Thus , when we are missing xn , say , the left elbow at time tick n , we can reconstruct it by examining the corresponding values of the right elbow ( say , yn−1 , yn , yn+1 ) . This two prong approach can help us handle even “ black outs ” , which we define as time intervals where we lose track of all the time sequences .
The main contribution of our approach is that it shows how to exploit both sources of redundancy ( smoothness and correlation ) in a principled way . Specifically , we show how to set up the problem as a Dynamic Bayesian Network and solve it efficiently , yielding results with the best reconstruction error and agreeing with human intuition . Furthermore , we propose several variants based on DynaMMo for additional time series mining tasks such as forecasting , compressing , and segmentation .
The rest of the paper is organized as follows : In Section 2 , we review the related work ; the proposed method and its discussion are presented in Section 3 ; the experimental results are presented in Section 4 . Section 5 discusses additional benefits of our method : interpretation , compression and segmentation . Finally , Section 6 concludes the paper .
2 . RELATED WORK
Interpolation methods , such as linear interpolation and splines , are commonly used to handle missing values in time series . Both linear interpolation and splines estimate the missing values based on continuity in a single sequence . While these methods are generally effective for short gaps , they ignore the correlations among multiple dimensions .
Singular Value Decomposition ( SVD ) and Principal Component Analysis ( PCA ) [ 20 ] are powerful tools to discover linear correlations across multiple sequences , with which it is possible to recover missing values in one sequence based on observations from others . Srebro and Jaakkola [ 18 ] have proposed an EM approach ( MSVD ) to factor the data into low rank matrices and approximate missing value from them . We will describe MSVD in appendix , and we show that it is a special case of our model . Brand [ 1 ] further develop an incremental algorithm to fast compute the singular de composition with missing values . Similar to the missing value SVD approach , Liu and McMillan [ 13 ] have proposed a method that projects motion capture markers positions into linear principal components and reconstructs the missing parts from the linear models . Furthermore , they proposed an enhanced Local Linear method from a mixture of such linear models . Park and Hodgins [ 16 ] have also used PCA to estimate the missing markers for skin deformation capturing . In another direction , Yi et al [ 21 ] have proposed a online regression model over time across multiple dimension that is in extension to Autoregression ( AR ) , thus could handle missing values .
There are several methods specifically for modeling motion capture data . Herda et al [ 5 ] have used a human body skeleton to track and reconstruct the 3 d marker positions . If a marker is missing , it could predict the position using three previous markers by calculating the kinetics . Hsu et al [ 6 ] have proposed a method to map from a motion control specification to a target motion by searching over patterns in existing database . Chai and Hodgins [ 2 ] uses a small set of markers as control signals and reconstruct the full body motion from a pre recorded database . The subset of markers should be known in advance , while our method does not assume fixed subsets observed or missing . As an alternative non parametric approach , Lawrence and Moore [ 9 ] model the human motion using hierarchical Gaussian processes . [ 13 ] provides a nice summary of related work on occlusion for motion capture data as well as of techniques for related tasks such as motion tracking .
There are many related work in time series representation [ 14 , 12 , 17 ] , indexing [ 8 ] , classification [ 3 , 19 ] and outlier detection [ 10 ] . Mehta et al [ 14 ] proposed a representation method for time varying data based on motion and shape information including linear velocity and angular velocity . With this representation , they track the tangible features to segment the sequence trajectory . Symbolic aggregate approximation ( SAX ) [ 12 ] is a symbolic representation for time series data , and later generalized for massive time series indexing ( iSAX ) [ 17 ] . Keogh et al use uniform scaling when indexing a large human motion database [ 8 ] . Lee et al [ 10 ] proposed the TRAOD algorithm to identify outliers in a trajectory database . In their approach , they first partition the trajectories into small segments and then use both distance and density to detect abnormal sub trajectories . Gao et al [ 3 ] proposed an ensemble model to classify the data streams with skewed class distributions and concept drifts . Their approach is to undersample the dominating class , oversample or repeat the rare class and then partition the data set and perform individual training . The trained models are then combined evenly into the resulting classification function . However , none of these methods can handle missing values .
Our method is also related to Kalman Filters and other adaptive filters conventionally used in tracking system . Jain et al [ 7 ] have adapted Kalman Filters for reducing communication cost in data stream . Tao et al [ 19 ] have proposed a recursive filter to predict and index moving objects . Li et al [ 11 ] used Kalman filter to stitch motions in a natural way . While our method includes Kalman Filter as a special case , DynaMMo can effectively cope with missing values .
3 . PROPOSED METHOD : DYNAMMO
Given a partially observed multi dimensional sequence , we
Table 1 : Symbols and Definitions
Symbol Definition X a multi dimensional sequence of observations with missing values ( x1 , xT ) the observed values in the sequence X variables for the missing values in the sequence X dimension of X duration of X missing value indication matrix with the same duration and dimension of X a sequence of latent variables ( z1 , . . . zT ) dimension of latent variables ( z1 · · · zT )
Xg Xm m T W
Z H propose DynaMMo , to identify hidden variables , to mine their dynamics , and to recover missing values . Our motivation comes from noticing two common properties of time series data : temporal continuity and spatial correlation . On one hand , by exploiting continuity as many interpolation methods do , we expect that missing values are close to observations in neighboring time ticks and follow their moving trends . On the other hand , by using the correlation between difference sequences as SVD does , missing values can be inferred from other observation sources . Our proposed approach makes use of both , to better capture patterns in coevolving sequences .
3.1 The Model
We will first define the problem of time series missing value recovery , and then present our proposed DynaMMo . Table 1 explains the symbols and annotations .
Definition 1 . Given a time sequence X with duration T in m dimensions , X = {x1 , . . . , xT} , to recover the missing part of the observations indicated by W . wt,k = 0 whenever X ’s k th dimensional observation is missing at time t , and otherwise wt,k = 1.Let us denote the observed part as Xg , and the missing part as Xm .
We build a probabilistic model ( Figure 3 ) to estimate the expectation of missing values conditioned on the observed parts , E[Xm|Xg ] . We use a sequence of latent variables ( hidden states ) , zn , to model the dynamics and hidden patterns of the observation sequence . Like SVD , we assume a linear projection matrix G from the latent variables to the data sequence ( both observed and missing ) for each time tick . This mapping automatically captures the correlation between the observation dimensions ; thus , if some of the dimensions are missing , they can be inferred from the latent variables . For example , the states could correspond to degrees of freedom , the velocities , and the accelerations in human motion capture data ( although we let DynaMMo determine them , automatically ) ; while the observed marker positions could be calculated from these hidden states.To model temporal continuity , we assume the latent variables are time dependent with the values determined from the previous time tick by a linear mapping F . In addition , we assume an initial state for latent variables at the first time tick . Eq ( 1 3 ) give the mathematical equations of our proposed model , with the
(z0 , Γ )
(F∙z1 , Λ )
Z2
Z1
(F∙z2 , Λ )
Z3
(F∙z3 , Λ )
Z4
(F∙z4 , Γ )
(G∙z1 , Σ )
(G∙z2 , Σ )
(G∙z3 , Σ )
(G∙z4 , Σ )
…
X1
X2
X3
X4
Figure 3 : Graphical Illustration of the Model . z1···4 : latent variables ; x1,2,4 : observations ; x3 : partial observations . Arrows denote Gaussian distributions . parameters θ = {F , G , z0 , Γ , Λ , Σ} . z1 = z0 + ω0 zn+1 = Fzn + ωn xn = Gzn + ǫn
( 1 ) ( 2 ) ( 3 ) where z0 is initial state of the latent variables . F implies the transition and G is the observation projection . ω0 , ωi and ǫi(i = 1 . . . T ) are multivariate Gaussian noises with the following distributions :
ω0 ∼ N ( 0 , Γ ) ωi ∼ N ( 0 , Λ ) ǫj ∼ N ( 0 , Σ )
( 4 )
The model is similar to Linear Dynamical System except that it includes an additional matrix W to indicate the missing observations . The joint distribution of Xm , Xg and Z is given by
P ( Xm , XgandZ ) = P ( z1 ) ·
T
Yi=2
P ( zi|zi−1 ) ·
T
Yi=1
P ( xi|zi ) ( 5 )
3.2 The Learning Algorithm
Given an incomplete data sequence X and the indication sequence W , we propose DynaMMo method to estimate :
1 . the governing dynamics F and G , as well as other pa rameters z0 , Γ , Λ and Σ ;
2 . the latent variables ˆzn = E[zn ] , ( n = 1 . . . T ) ; 3 . the missing values of the observation sequence E[Xm|Xg ] .
The goal of parameter estimation is achieved through maximizing the likelihood of observed data , L(θ ) = P ( Xg ) . However , it is difficult to directly maximize the data likelihood in missing value setting , instead , we maximize the expected log likelihood of the observation sequence . Once we get the model parameters , we use belief propagation to estimate the occluded marker positions . We define the following objective function as the expected log likelihood Q(θ ) with respect to the parameters θ = {F , G , z0 , Γ , Λ , Σ} :
Q(θ ) = EXm,Z|Xg ,W [ P ( Xg , Zm , Z ) ]
( 6 )
= EXm,Z|Xg ,W [ −D(z1 , z0 , Γ ) −
T
D(zt , Fzt−1 , Γ ) ( 7 )
Xt=2
T
−
D(xt,Gzt,Σ ) −
Xt=1 log|Γ|
2
−
( T − 1 ) log|Λ|
2
−
T log|Σ|
2
](8 ) where D( ) is the square of the Mahalanobis distance D(x , y , Σ ) = ( x − y)T Σ−1(x − y )
Our proposed DynaMMo searches for the optimal solution using Expectation Maximization [ 4 ] . The optimization algorithm is actually an iterative , coordinate descent procedure : estimating the latent variables , maximizing with respect to parameters , estimating the missing values , and iterating until convergence .
To estimate the parameters , taking the derivatives of Eq 78 with respect to the components of θnew and setting them to zero yield the following results :
0
= E[z1 ]
µnew Γnew = E[z1zT
1 ] − E[z1]E[zT 1 ] N−1
Fnew = (
E[znzT n−1])(
N
Xn=2
1
N − 1 n ])−1
E[znzT
Xn=1 n ] − Fnew E[zn−1zT n ]
N
Xn=2
( E[znzT n−1](Fnew)T )(12 )
( 9 )
( 10 )
( 11 )
( 13 )
Λnew =
−E[znzT
Σnew =
Gnew = ( n ])(
N
N n ])−1
E[znzT
Xn=1 xnE[zT n−1](Fnew)T + Fnew E[znzT Xn=1 Xn=1 n ](Gnew )T + Gnew E[znzT n − Gnew E[zn]yT
( ynyT
1 N
N n
−ynE[zT n ](Gnew)T )(14 )
The calculation of optimal parameters in Eq 9 14 requires estimation of latent variables , which includes our second goal . We use a belief propagation algorithm to estimate the posterior expectations of latent variables , similar to message passing in Hidden Markov Model and Linear Dynamical Systems . The general idea is to compute the posterior distribution of latent variables tick by tick , based on the computation of previous time tick .
Finally , the missing values are easily computed from the estimation of latent variables using Markov property in the graphical model ( Figure 3 ) . We have the following equation :
E[Xm|Xg , Z ; θ ] = G · E[Z]{i,j}({i , j} ∈ W )
( 15 )
The overall algorithm is described in Algorithm 1 , omit ting details explained in Appendix A1 3.3 Discussion Model Generality : Our model includes MSVD , linear interpolation , and Kalman filters as special cases :
• MSVD : If we set F and z0 to 0 , and Γ = Λ , then the model becomes MSVD , We describe MSVD in Section 4
• Linear interpolation : For one dimensional data , we obtain the linear interpolation by setting Λ = 0 and the rest of the parameters to the following values :
F = „ 1 1
0 1 «
G = „ 1
0 «
• Equations ( 1) (3 ) of DynaMMo become the equations of the traditional Kalman filters if there are no missing values . In that case , the well known , singlepass Kalman method applies .
Penalty and Constraints : In the algorithm described above , Eq ( 7 ) is error term for the initial state . Eq ( 8 ) is trying to estimate the dynamics for the hidden states ,
Algorithm 1 : DynaMMo
Input : Observed data sequence : X = Xg , Missing value indication matrix:W the number of latent dimension H Output :
• Estimated sequence : ˆX • Latent variables ˆz1 · · · ˆzT| • Model parameters θ
Initialize ˆX with Xg and the missing value filled by linear interpolation or other methods ; Initialize F , G , z0 ; Initialize Γ , Λ , Σ to be identity matrix ; θ ← {F , G , z0 , Γ , Λ , Σ} ; repeat
1
2
Estimate ˆz1···T = E[Z| ˆX ; θ ] using belief propagation ( see details in Appendix A.1 ) ; Maximizing Eq ( 7) (8 ) with E[Z| ˆX ; θ ] using Eq 9 14 ,
θnew ← arg maxθ Q(θ ) ; forall i,j do
// update the missing values if Wi,j = 0 then Xi,j is missing i,j ← ( Gnew · E[Z| ˆX ; θ])i,j
ˆX new until converge ; while Eq ( 8 ) is getting the best projection from observed motion sequence to hidden states . Eq ( 8 ) is penalty for the covariance , similar to model complexity in BIC . It is easy to extend the model by putting a further penalty on the model complexity through a Bayesian approach . For example , we could constraint the covariance to be diagonal σ2I , which is used in our experiments , since it is faster to compute . Time Complexity : The algorithm needs time linear on the duration T , and specifically O(#(iterations ) · T · m3 ) . Thus , we expect it to scale well for longer sequences . As a point of reference , it takes about 6 to 10 minutes per sequence with several hundreds time ticks , on a Pentium class desktop .
4 . EXPERIMENTAL RESULTS
We evaluate both quality and scalability of DynaMMo on several datasets . To evaluate the quality of recovering missing values , we use a real dataset with part of data treated as “ missing ” so that it enables comparing the real observations with the reconstructed ones . In the following we first describe the dataset and baseline methods , and then present the reconstruction results . 4.1 Experiment Setup
411 Baseline Methods
We use linear interpolation and Missing Value SVD ( MSVD ) as the baseline methods . We also compare to spline interpolation .
Missing Value SVD involves iteratively taking the SVD and fitting the missing values from the result [ 18 ] . This method is very easy to implement and already used on motion capture datasets in [ 13 ] and [ 16 ] . In our implementation ( appendix A.2 ) , we initialized the holes by linear interpolation , and use 15 principal dimensions ( 99 % of energy ) .
412 Datasets Chlorine Dataset ( Chlorine ) : The Chlorine dataset ( see sample in Figure 6(a ) ) was produced by EPANET 21 that models the hydraulic and water quality behavior of water distribution piping systems . EPANET can track , in a given water network , the water level and pressure in each tank , the water flow in the pipes and the concentration of a chemical species ( Chlorine in this case ) throughout the network within a simulated duration . The data set consists of 166 nodes ( pipe junctions ) and measurement of the Chlorine concentration level at all these nodes during 15 days ( one measurement for every 5 minutes , a total of 4310 time ticks ) . Since the water demand pattern during the 15 days follows a clear global periodic pattern ( daily cycle , dominating residential demand pattern ) , EPANET would correctly reflect the pattern in the Chlorine concentration with a few exceptions and slight time shifts . Full body motion set ( Motion ) : This data set contains 58 full body motions of walking , running , and jumping motions from subject #16 of mocap database2 . Each motion spans several hundred of frames with 93 features of bone positions in body local coordinates . The total size of the dataset is 17MB . We make random dropouts and reconstruct the missing values on the data set .
0.025
0.02
0.015
0.01
0.005 r o r r e e g a r e v a
0
10 spline linear spline MSVD DynaMMo
20
30
40
50
λ
60
70
80
90
100
Figure 5 : Average error for missing value recovery on a sample mocap data ( subject#1622 ) Average rmse over 10 runs , versus average missing length λ(from 10 to 100 ) . Randomly 10.44 % of the values are treated as “ missing ” . DynaMMo ( in red solid line ) wins . Splines are off the scale . average of squared differences between the actual ( X ) and reconstructed ( ˆX ) missing values formally :
M SE(X , W , ˆX ) =
||(1 − W ) ⊗ ( X − ˆX)||2
||1 − W ||2
413 Simulate Missing Values
=
( 1 − Wt,k)(Xt,k − ˆXt,k)2
( 16 )
We create synthetic occlusions ( dropouts ) on the Motion data and evaluate the effectiveness of reconstruction by DynaMMo . To mimic a real occlusion , we collected the occlusion statistics from handshake motions . For example , there are 10.44 % of occluded values in typical handshake motions , and occlusions often occur consecutively ( Figure 2 ) . To create synthetic occlusions , we randomly pick a marker j and the starting point ( frame ) n for the occlusion of this marker ; we pick the duration as a Poisson distributed random variable according to the observed statistics , and we repeat , until we have occluded 10.44 % of the input values . 4.2 Experimental Results
We present three sets of results , to illustrate the quality of reconstruction of DynaMMo .
421 Qualitative result
Figure 1 shows the reconstructed signal ( root bone zcoordinate ) for a jump motion . Splines find a rather smooth curve which is not what the human actor really did . Linear interpolation and MSVD are a bit better while still far from the ground truth . Our proposed DynaMMo ( with 15 hidden dimensions ) captured both the dynamics of the motion as well as the correlations across the given inputs , and achieved a very good reconstruction of the signal .
422 Reconstruction Error
For each motion in the data set , we create a synthetic occluded motion sequence as described above , reconstruct using DynaMMo , then compare the effectiveness against linear interpolation , splines and MSVD . To reduce random effects , we repeat each experiment 10 times and we report the average of MSE . To evaluate the quality , we use the MSE : Given the original motion X , the occlusion indication matrix W and the fitted motion ˆX , the MSE is the 1http://wwwepagov/nrmrl/wswrd/dw/epanethtml 2http://mocapcscmuedu
1
Pt,k(1 − Wt,k ) Xt,k
Both MSVD and our method use 15 hidden dimensions ( H = 15 ) . Figures 4(a) 4(c ) show the scatter plots of the average reconstruction error over 58 motions in the Motion dataset , with 10 % missing values and 50 average occlusion length . It is worth to noting that the reconstruction grows little with increasing occlusion length , compared with other alternative methods ( Figure 5 ) . There is a similar result found in experiments on Chlorine data as shown in Figure 6(b ) . Again , our proposed DynaMMo achieves the best performance among the four methods .
423 Scalability
As we discussed in Section 3 , the complexity of DynaMMo is O(#(iterations)·T ·m3 ) . Figure 7 shows the running time of the algorithm on the Chlorine dataset versus the sequence length . For each run , 10 % of the Chlorine concentration levels are treated as missing with average missing length 40 . As expected , the wall clock time is almost linear to sequence duration .
5 . ADDITIONAL BENEFITS
In addition to recovering the missing observations , our
1
0.8
0.6
0.4
0.2
0
500
0 2000 ( a ) Sample Chlorine data .
1000
1500
Linear
Spline
MSVD
DynaMMo
0
0.01 0.02 0.03 0.04 error
( b ) Reconstruction error .
Figure 6 : Reconstruction experiment on Chlorine with 10 % missing and average occlusion length 40 .
0.2
0.15
0.1
0.05 o M M a n y D f o r o r r e
0
0
DynaMMo loses
DynaMMo wins
0.05 error of Linear Interpolation
0.15
0.1
0.2 o M M a n y D f o r o r r e
6
5
4
3
2
1
0
0
DynaMMo loses
DynaMMo wins
0.2
0.15
0.1
0.05 o M M a n y D f o r o r r e
DynaMMo loses
DynaMMo wins
2
4
6 error of Spline
0
0
0.05
0.1
0.15
0.2 error of MSVD
( a ) DynaMMo vs Linear
( b ) DynaMMo vs Spline
( c ) DynaMMo vs MSVD
Figure 4 : Scatter plot of missing value reconstruction error for Mocap dataset .
5000
4000
3000
2000
1000 e m i t i g n n n u r
0
0
1000
2000
3000
4000 sequence length
Figure 7 : Running time versus the sequence length on Chlorine dataset . For each run , 10 % of the values are treated as “ missing ” . proposed DynaMMo method can be easily extended for further data mining tasks . Here we propose several DynaMMo extensions for time series compression , segmentation , and forecasting . 5.1 Interpretation and Forecasting
One of the advantages of our proposed DynaMMo is that its learnt representation of a data stream contains information about behavior of patterns and trends , as illustrated in the following examples . As shown in top of Figure 8 , we create a sinusoid sequence with a cycle of 32 , then learn the parameters θ = {F , G , z0 , Γ , Λ , Σ} using DynaMMo with hidden dimension of 6 , and then generate a simulated signal ˆx using only the parameter z0 , F , G : ˆz1 = z0 , ˆzn+1 = Fˆzn , ˆxn = Gˆzn . The simulated signal in Figure 8 presents an identical periodic pattern to the original signal , with negligible difference in amplitude . Note it is easy to forecast the future if we continue ticking the time . As this simple case demonstrates , DynaMMo is able to learn dynamics from a data stream and reproduce its behavior even without observations . This suggests that it could be used to compress time sequences , as we will discuss in the next section . 5.2 Compression
Time series data are usually real valued which makes it hard to achieve a high compression ratio using lossless methods . However , lossy compression is reasonable if it gets a high compression ratio and low recovery error . As described in Section 3 , DynaMMo produces three outputs :
5
0
−5
0
5
0
−5
0
20
40
60
80
100
20
40
60
80
100
Figure 8 : Simulated signal versus original signal . The top is the original signal ( blue curve ) , while the bottom ( red curve ) is the generated using parameters learnt from original with a hidden dimension of 6 . Note the almost identical pattern in both generated and original signal , and will continue if prolonged . model parameters , latent variables ( posterior expectation ) and missing values . To compress , we record some of the hidden variables learned from DynaMMo instead of storing direct observations . By controlling the hidden dimension and the number of time ticks of hidden variables to keep , it is easy to trade off between compression ratio and error . We provide three alternatives for compression .
Here we first present the decompression algorithm in Al gorithm 2 .
521 Fixed Compression : DynaMMof
The fixed compression will first learn the hidden variables using DynaMMo and store the hidden variables for every k time ticks . In addition , it stores the matrix F , Λ , G and Σ . Both covariance Λ and Σ are constrained to λ2I and σ2I respectively . It also stores the number k .
The total space required for fixed compression is Sf = k · H + H 2 + H · m + 3 , where k is the gap number given .
T
522 Adaptive Compression : DynaMMoa
The adaptive compression will first learn the hidden vari
Algorithm 2 : DynaMMo Decompress Input : ˆzS , hidden variables , indexed by S ⊆ [ 1 · · · T ] ,
F , G .
Output : The decompressed data sequence ˆx1···T y ← ˆz1 ; for n ← 1 to T do if n in S then y ← ˆzn ; else y ← F · y ;
ˆxn ← G · y ; ables using DynaMMo and store the hidden variable only for the necessary time ticks , when the error is greater than a given threshold . Like fixed compression , it also stores the matrix F , Λ , G and Σ . Both covariance Λ and Σ are constrained to λ2I and σ2I respectively . For each stored time tick , it also records the offset of next storing time tick .
The total space required for adaptive compression is Sa = l · ( H + 1 ) + H 2 + H · m + 2 where l is the number of stored time ticks . 523 Optimal Compression : DynaMMod
The optimal compression will first learn the hidden variables using DynaMMo and store the hidden variables for the time ticks determined by dynamic programming , so as to achieve the smallest error for a given number of stored time ticks . Like the fixed compression , it also stores the matrix F , Λ , G and Σ . Both covariance Λ and Σ are constrained to λ2I and σ2I respectively .
The total space required for optimal compression is Sd = l · ( H + 1 ) + H 2 + H · m + 2 where k is the number of stored time ticks . 524 Baseline Method
We use a combined method of SVD and linear interpolation as our baseline . It works as follows : given k and h , it first projects the data into h principle dimensions using SVD , then records the hidden variables for every k time ticks . In addition , it will also record the projection matrix from SVD . When decompressing , the hidden variables are projected back using the stored matrix and the gaps filled with linear interpolation .
T
The total space required for baseline compression is Sb = k · h + h · m + h + 1 , where k is the gap number given , and h is the number of principle dimensions .
For all of these methods , the compression ratio is defined as r o r r e
1
0.8
0.6
0.4
0.2
0
0 baseline
Baseline DynaMMof DynaMMoa DynaMMod
50
100
150
200
250
300
350 compression ratio
Compression for Chlorine dataset : Figure 9 : RMSE versus compression ratio . Lower is better . DynaMMod ( in red solid ) is the best . to have different model parameters and latent variables . We use the reconstruction error as an instrument of segmentation . Note that since there might be missing value in the data sequences , a normalization procedure with respect to the number observation at each time tick is required . We present our segmentation method in Algorithm 3 .
Algorithm 3 : DynaMMo Segment
Input : Data sequence : X , With or without missing value indication matrix:W the number of latent dimension H Output : The segmentation position s
{G , ˆz1···m} ← DynaMMo(X , W , H ) ; for n = 1 to m do
Reconstruct the data for time tick n : ˆxn ← G · ˆzn ; Computer the reconstruction error for time tick n :
||wT n ⊗ ( ˆxn − xn)||2
△n ← find the split : s ← arg max k
P wn
△k ;
To illustrate , Figure 10 shows the segmentation result on a sequence composed of two pieces of sinusoid signals with different frequencies . Our segmentation method could correctly identify the time of frequency change by tracking the spikes in reconstruction error . Figure 11 shows the reconstruction error from segmentation experiment on a real human motion sequence in which an actor running to a complete stop . Two ( y coordinates of left hip and femur ) of 93 joint coordinates are shown in the top of the plot . Note the spikes in the error plot coincide with the slowdown of the pace and transition to stop .
R∗ =
Total uncompressed space compressed space
=
T · m
S∗
6 . CONCLUSIONS
Figure 9 shows the decompression error ( in terms of RMSE ) with respect to compression ratio compared with the baseline compression using a combined method SVD and Linear Interpolation . DynaMMod wins especially in high compression ratio . 5.3 Segmentation
As a further merit , our DynaMMo is able to segment the data sequence . Intuitively , this is possible because DynaMMo identifies the dynamics and patterns in data sequences , so segments with different patterns can be expected
Given multiple time sequences , we propose DynaMMo ( Dynamics Mining with Missing values ) , which includes a learning algorithm and its variant extensions to summarize , compress and find latent variables . The idea is to automatically discover a few , hidden variables and to compactly describe how the hidden variables evolve by learning their transition matrix F . Our algorithm can even work when there are missing observations , and includes Kalman filters as special case .
We presented experiments on motion capture sequences and chlorine measurements and demonstrated that our pro
5
0
−5
0 x 10−8 r o r r e
2
1
0
0
50
100
150
200
250
300
50
100
150
200
250
300
Figure 10 : Segmentation result on one dimensional synthetic data . Top is a sequence composed of two pieces of sinusoid signals with different frequencies 64 and 128 respectively . Bottom is the reconstruction error per time tick . Note the spike in the middle correctly identify the shifting of frequencies . posed DynaMMo method and its extensions ( a ) can successfully learn the latent variables and their evolution , ( b ) can provide high compression for little loss of reconstruction accuracy , and ( c ) can extract compact , but powerful features , for sequence forecasting , interpretation and segmentation , ( d ) scalable on duration of time series .
Acknowledgments . This material is based upon work supported by the National Science Foundation under Grants No.DBI 0640543 , by the iCAST project sponsored by the NSC , Taiwan , under the Grants No . NSC97 2745 P 001001 and by the Ministry of Economic Affairs , Taiwan , under the Grants No . 97 EC 17 A 02 R7 0823 , also , under the auspices of the US Department of Energy by University of California Lawrence Livermore National Laboratory under contract DE AC52 07NA27344 ( LLNL CONF 404625 ) , subcontracts B579447 , B580840 . The data used in this project was obtained from mocapcscmuedu supported by NSF EIA0196217 . This work is also partially supported by an IBM Faculty Award , a Yahoo Research Alliance Gift , a SPRINT gift , with additional funding from Intel , NTT and HP . Any opinions , findings , and conclusions or recommendations expressed in this material are those of the author(s ) and do not necessarily reflect the views of the funding parties .
7 . REFERENCES [ 1 ] M . Brand . Incremental singular value decomposition of uncertain data with missing values . In Proceedings of the 7th European Conference on Computer Vision , pages 707–720 , London , UK , 2002 . Springer Verlag . [ 2 ] J . Chai and J . K . Hodgins . Performance animation from low dimensional control signals . In SIGGRAPH ’05 : ACM SIGGRAPH 2005 Papers , pages 686–696 , New York , NY , USA , 2005 . ACM .
[ 3 ] J . Gao , B . Ding , W . Fan , J . Han , and P . S . Yu .
Classifying data streams with skewed class distributions and concept drifts . IEEE Internet Computing , 12(6):37–49 , 2008 .
[ 4 ] Z . Ghahramani and M . I . Jordan . Supervised learning from incomplete data via an EM approach . In J . D . Cowan , G . Tesauro , and J . Alspector , editors , Advances in Neural Information Processing Systems , volume 6 , pages 120–127 . Morgan Kaufmann Publishers , Inc . , 1994 . p h i t f e l
1.1
1
0.9
0 r u m e f t f e l
0.8
0.6
0.4
2
1 r o r r e
0 x 10−3
0
0
50 run
100 slow down
150
200 stop
250
50
100
150
200
250
50
100
150
200
250 frame #
Figure 11 : Reconstruction error plot for segmentation on a real motion capture sequence in 93 dimensions ( subject#16.8 ) with 250 frames , an actor running to a complete stop , with left hip and femur y coordinates shown in top plots . The spikes in bottom plot coincide with the slowdown of the pace and transition to stop .
[ 5 ] L . Herda , P . Fua , R . Pl¨ankers , R . Boulic , and
D . Thalmann . Skeleton based motion capture for robust reconstruction of human motion . In CA ’00 : Proceedings of the Computer Animation , page 77 , Washington , DC , USA , 2000 . IEEE Computer Society .
[ 6 ] E . Hsu , S . Gentry , and J . Popovi´c . Example based control of human motion . In Proceedings of the 2004 ACM SIGGRAPH/Eurographics symposium on Computer animation , pages 69–77 , Aire la Ville , Switzerland , 2004 . Eurographics Association .
[ 7 ] A . Jain , E . Y . Chang , and Y F Wang . Adaptive stream resource management using kalman filters . In SIGMOD ’04 : Proceedings of the 2004 ACM SIGMOD international conference on Management of data , pages 11–22 , New York , NY , USA , 2004 . ACM .
[ 8 ] E . Keogh , T . Palpanas , V . B . Zordan , D . Gunopulos , and M . Cardle . Indexing large human motion databases . In VLDB ’04 : Proceedings of the Thirtieth international conference on Very large data bases , pages 780–791 . VLDB Endowment , 2004 .
[ 9 ] N . D . Lawrence and A . J . Moore . Hierarchical gaussian process latent variable models . In ICML ’07 : Proceedings of the 24th international conference on Machine learning , pages 481–488 , New York , NY , USA , 2007 . ACM .
[ 10 ] J G Lee , J . Han , and X . Li . Trajectory outlier detection : A partition and detect framework . IEEE 24th International Conference on Data Engineering , pages 140–149 , April 2008 .
[ 11 ] L . Li , J . McCann , C . Faloutsos , and N . Pollard .
Laziness is a virtue : Motion stitching using effort minimization . In Short Papers Proceedings of EUROGRAPHICS , 2008 .
[ 12 ] J . Lin , E . Keogh , S . Lonardi , and B . Chiu . A symbolic representation of time series , with implications for streaming algorithms . In DMKD ’03 : Proceedings of the 8th ACM SIGMOD workshop on Research issues in data mining and knowledge discovery , pages 2–11 , New York , NY , USA , 2003 . ACM Press .
[ 13 ] G . Liu and L . McMillan . Estimation of missing markers in human motion capture . Vis . Comput . , 22(9):721–728 , 2006 .
[ 14 ] S . Mehta , S . Parthasarathy , and R . Machiraju . On trajectory representation for scientific features . IEEE International Conference on Data Mining , 2006 .
[ 15 ] S . Papadimitriou , A . Brockwell , and C . Faloutsos .
Adaptive , hands off stream mining . In VLDB ’2003 : Proceedings of the 29th international conference on Very large data bases , pages 560–571 . VLDB Endowment , 2003 .
[ 16 ] S . I . Park and J . K . Hodgins . Capturing and animating skin deformation in human motion . ACM Trans . Graph . , 25(3):881–889 , 2006 .
[ 17 ] J . Shieh and E . Keogh . isax : indexing and mining terabyte sized time series . In KDD ’08 : Proceeding of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining , pages 623–631 , New York , NY , USA , 2008 . ACM .
[ 18 ] N . Srebro and T . Jaakkola . Weighted low rank approximations . In 20th International Conference on Machine Learning , pages 720–727 . AAAI Press , 2003 .
[ 19 ] Y . Tao , C . Faloutsos , D . Papadias , and B . Liu . Prediction and indexing of moving objects with unknown motion patterns . In SIGMOD ’04 : Proceedings of the 2004 ACM SIGMOD international conference on Management of data , pages 611–622 , New York , NY , USA , 2004 . ACM Press .
[ 20 ] M . E . Wall , A . Rechtsteiner , and L . M . Rocha .
Singular value decomposition and principal component analysis . In D . P . Berrar , W . Dubitzky , and M . Granzow , editors , A Practical Approach to Microarray Data Analysis , pages 91–109 , Norwell , MA , Mar 2003 . Kluwel .
[ 21 ] B K Yi , N . D . Sidiropoulos , T . Johnson , H . V .
Jagadish , C . Faloutsos , and A . Biliris . Online data mining for co evolving time sequences . In ICDE ’00 : Proceedings of the 16th International Conference on Data Engineering , page 13 , Washington , DC , USA , 2000 . IEEE Computer Society .
APPENDIX A . APPENDIX
A.1 Algorithm 1 details
In Algorithm 1 , the estimation ( line 1 ) is to find the marginal distribution for hidden state variables given the data , eg ˆzn = E[zn | Xg , Xm](n = 1 , . . . , T ) . Since both prior and conditional distributions in the model are Gaussian , the posterior up to current time tick p(zn|x1 , . . . , xT ) should also be Gaussian , denoted by ˆα(zn ) = N ( µn , Vn ) . Let p(xn|x1 , . . . , xn−1 ) denoted as cn , We have the following propagation equation : cn ˆα(zn ) = p(xn|zn)Z ˆα(zn−1)p(zn|zn−1)dzn−1
( 17 )
From Eq 17 we could obtain the following forward passing of the belief . The messages here are µn , Vn and Pn−1(needed in later backward passing ) .
Pn−1 = FVn−1FT + Λ
Kn = Pn−1GT ( GPn−1GT + Σ)−1 µn = Fµn−1 + Kn(xn − GFµn−1 ) Vn = ( I − Kn)Pn−1 cn = N ( GFµn−1 , GPn−1GT + Σ )
The initial messages are given by :
K1 = ΓGT ( GΓGT + Σ)−1 µ1 = µ0 + K1(x1 − GFµ0 ) V1 = ( I − K1)Γ c1 = N ( Gµ0 , GΓGT + Σ )
( 18 )
( 19 ) ( 20 ) ( 21 )
( 22 )
( 23 ) ( 24 ) ( 25 )
( 26 )
For the backward passing , let γ(zn ) denote the marginal posterior probability p(zn|x1 , . . . , xN ) with the assumption :
γ(zn ) = N ( ˆµn , ˆVn )
The backward passing equations are :
Jn = VnFT ( Pn)−1 ˆµn = µn + Jn(ˆµn+1 − Fµn ) ˆVn = Vn + Jn( ˆVn+1 − Pn)JT n
( 27 )
( 28 ) ( 29 )
( 30 )
Hence , the expectation for Algorithm 1 line 1 are com puted using the following equations :
E[zn ] = ˆµn n−1 ] = Jn−1 ˆVn + ˆµn ˆµT n−1
E[znzT
E[znzT n ] = ˆVn + ˆµn ˆµT n
( 31 )
( 32 )
( 33 ) where the expectations are taken over the posterior marginal distribution p(zn|y1 , . . . , yN ) .
From these estimations , the new parameter θnew is obtain by maximizing the Equations 7 8 with respect to the components of θnew given the current estimate of θold , yield the Algorithm 1 line 2 . A.2 Missing Value SVD Algorithm 4 : Missing Value SVD
Input : Observed data matrix : Xg , Occlusion indication matrix:W the number of hidden dimensions H Output : Estimated data matrix : ˆX Initialize ˆX with Xg and the missing value filled by linear interpolation ; repeat taking SVD of ˆX , ˆX ≈ UH ΛH V T H ; // update estimation Y ← UH ΛH V T H ; forall i,j do if Wi,j = 0 then Xi,j is missing
ˆX new i,j ← Yi,j until converge ;
