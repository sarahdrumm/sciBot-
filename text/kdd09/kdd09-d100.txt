MetaFac : Community Discovery via Relational Hypergraph
Factorization
Yu Ru Lin1 Jimeng Sun2 Paul Castro2 Ravi Konuru2 Hari Sundaram1 Aisling Kelliher1
1Arts Media and Engineering
Program , Arizona State University ,
Tempe , AZ 85281 USA
2IBM TJ Watson Research Center ,
Hawthorne , NY 10532 USA
{yu ru.lin , hari.sundaram , aislingkelliher}@asuedu , {jimeng , castrop,rkonuru}@usibmcom
ABSTRACT This paper aims at discovering community structure in rich media social networks , through analysis of time varying , multi relational data . Community structure represents the latent social context of user actions . It has important applications in information tasks such as search and recommendation . Social media has several unique challenges . ( a ) In social media , the context of user actions is constantly changing and co evolving ; hence the social context contains time evolving multi dimensional relations . ( b ) The social context is determined by the available system features and is unique in each social media website . In this paper we propose MetaFac ( MetaGraph Factorization ) , a framework that extracts community social contexts and interactions . Our work has three key contributions : ( 1 ) metagraph , a novel relational hypergraph representation for modeling multirelational and multi dimensional social data ; ( 2 ) an efficient factorization method for community extraction on a given metagraph ; ( 3 ) an on line method to handle time varying relations through factorization . Extensive experiments on real world social data collected from the Digg social media website suggest that our technique is scalable and is able to extract meaningful communities based on the social media contexts . We illustrate the usefulness of our framework through prediction tasks . We outperform baseline methods ( including aspect model and tensor analysis ) by an order of magnitude . incremental metagraph structures from various
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications—Data mining ; H33 [ Information Storage and Retrieval ] : Information Search and Retrieval—Information filtering ; I53 [ Pattern Recognition ] : Clustering ; J.4 [ Computer Applications ] : Social and Behavioral Sciences—Economics
General Terms Algorithms , Experimentation , Measurement , Theory
Keywords MetaFac , metagraph factorization , relational hypergraph , nonnegative tensor factorization , community discovery , dynamic social network analysis
1 . INTRODUCTION This paper aims at discovering community structure in rich media social networks , through analysis of the time varying multirelational data from social media websites . Social media websites
( a )
( c )
( b )
( d )
Figure 1 : The social context of user actions vary across social media websites – we propose a metagraph representation to model various social context ; ( a ) primary actions and related media objects in Diggs ; ( b ) primary actions and related objects in Flickr ; ( c ) a metagraph representation for Digg ; ( c ) a metagraph for Flickr . such as Flickr , Digg and Facebook allow a wide array of actions on media objects – eg uploading photos , submitting and commenting on news stories , bookmarking and tagging , posting documents , creating web links , as well as actions with respect to other users ( eg sharing media and links with a friend ) . The key to social media information tasks such as media recommendation relies in understanding the context of these actions – how they relate to other actions , users and media objects . For example , a user might be motivated to search a story after viewing her friend ’s bookmarks .
The problem has two challenges : ( 1 ) in social media , the context of user actions is constantly changing and co evolving , eg with respect to other users’ actions , emergent concepts and users’ historic preferences . Hence the social context contains timeevolving multi dimensional relations ; ( 2 ) the social context is determined by the available system features that allow interactions on media objects and among people . Hence the social context is unique in each social media website . For example , Figure 1 shows the main actions available in Digg and Flickr , as well as related media objects . In Digg , users might submit / vote ( digg ) / comment on a story , reply to a comment , reply to a reply , etc . Flickr users might post , tag and comment on a photo , make friend
527 contacts , label a photo as a favorite , join a photo sharing group ( pool ) , etc . There are some common actions , but more sitespecific actions cater to the purpose of each site . The discovery of social context needs to deal with the diverse and dynamic nature of actions in social media .
In this paper we propose MetaGraph Factorization ( MetaFac ) , a framework that extracts community structures ( ie the latent social context ) from various social interactions . Our work has three key contributions : ( 1 ) metagraph , a novel relational hypergraph representation for modeling multi relational and multi dimensional social data ; ( 2 ) an efficient factorization method for community extraction on a given metagraph ; ( 3 ) an on line method through incremental metagraph factorization . time varying to handle relations
Extensive experiments on real world social media data suggest that our technique is scalable and is able to extract meaningful communities based on social media context . We illustrate the usefulness of our framework through prediction tasks – to predict users’ future interests on voting or commenting on Digg stories . Our prediction significantly outperforms baseline methods ( frequency counts , tensor analysis , etc. ) , suggesting the utility of leveraging metagraphs to handle time varying social relational contexts .
The rest of the paper is organized as follows . Section 2 reviews the related work . Section 3 introduces preliminaries and section 4 formalizes the problem . Section 5 and 6 presents our community extraction method on both static and dynamic multi relational data . Section 7 presents experiments and section 8 concludes .
2 . RELATED WORK Community discovery in rich media social networks deals with a constantly changing ―mishmash‖ of interrelated users and media objects . The problem has three aspects : ( 1 ) evolutionary characterization of communities in time varying social networks , ( 2 ) analysis of multi dimensional data , and ( 3 ) relational learning adaptable to different social contexts . To the best of our knowledge , our work is the first unified attempt to address all three aspects within a single problem .
Evolutionary community characterization . Social interactions among people have been studied through a unipartite or bipartite graph , in which the community structure can be characterized by clustering methods [ 14 ] , and the evolution of community structure is captured in terms of various criteria . Kumar et al . [ 9 ] study the evolution of the blogosphere in terms of the change of graph statistics and the burstiness of extracted communities . Sun et al . [ 14 ] use the Minimum Description Length principle to extract communities and to detect their changes . Lin et al . [ 11 ] use an evolutionary clustering criterion [ 4 ] to extract community structures based on both observed networked data and historic community structure . All these works restrict themselves to pairwise relations between entities ( eg user user or user paper ) . In rich online social media , networked data consists of multiple coevolving dimensions , eg users , tags , feeds , comments , etc . Collapsing such multi way networks into pairwise networks results in the loss of valuable information , and the analysis of temporal correlation among multi dimensions is difficult .
In multi dimensional network Multi dimensional mining . analysis , networks have more than two types of entities . Existing techniques include tensor based analysis [ 5,14 ] or multi graph mining . Tensor factorization is a generalized approach for analyzing multi way interactions among entities . Note that a tensor represents complete interactions among all involved entities , which is a very strong assumption in social media since there might be events involving some but not all dimensions . Multi graph mining considers joint factorization over two or more matrices . The combination of such matrices is domain specific , eg in text mining , Zhu et al . [ 15 ] propose a joint matrix factorization combining both linkage and document term matrices to improve the hypertext classification . In social media , relations depend on the system features , which might vary across websites . Moreover , the system features may change over time in a social media website , which requires flexible relational learning .
Relational learning . Relational techniques such as PRMs [ 6 ] extend generative models to deal with various combinations of probabilistic dependency among entities . Such techniques can be computationally expensive , and may not scale to the large amount of data typically collected by social media websites . There have been relational learning techniques through pairwise relationships among entities , eg [ 3,12 ] , which involve loss of information when data has higher order interactions . Our work shares the same advantages as Kemp el at . [ 8 ] and Banerjee et al . [ 2 ] , which deal with multiple tensors , but their static settings are different from our problem .
In sum , social media analysis requires a flexible and scalable framework that exploits relational context defined by the system features of individual social media sites . Such relational context is multi dimensional , sparse ( not all dimensions are involved in an event ) , specific , and evolving over time . We propose a unified approach to analyze the dynamics of rich media social networks .
3 . PRELIMINARIES ON TENSOR This section provides notations and minimal background on tensors and some basic operations used in this work . We refer readers to [ 1 ] for a more comprehensive review on tensors .
3.1 Tensors A tensor is a mathematical representation of a multi way array . The order of a tensor is the number of modes ( or ways ) . A firstorder tensor is a vector , a second order tensor is a matrix , and a higher order tensor has three or more modes . We use x as a vector , X as a matrix , and  as a tensor . The dimensionality of a mode is the number of elements in that mode . We use Iq to denote the dimensionality of mode q . Eg , the tensor has 3 modes with dimensionalities of I1 , I2 and I3 , respectively . + indicates all elements of the tensor  have nonnegative values , which is usually the case for a data tensor . The ( i1,i2,i3) element of a third order tensor is denoted by . Indices typically range from 1 to their capital version , eg i1=1,…,I1 . 3.2 Basic Operations Mode d matricization or unfolding : Matricization is the process of reordering the elements of an M way array into a matrix . The mode d matricization of a tensor is denoted by X(d ) , ie
. Unfolding a tensor on mode d results in a matrix with height Id and its width is the product of dimensionalities of all other modes .
The inverse operation is denoted as
.
In general the unfolding operation can be defined on multiple modes . For example , we can define mode (c,d ) unfolding as
123III123iiix1MII{1,,},()(,)dqqMqdIIdunfolddX1()()MIIdfoldX528 Figure 2 : CP decomposition of a three way tensor .
. Unfolding a tensor on two modes c and d results in a cube ( three way tensor ) . Similarly , we can define a vectorization operation x=vec( ) , which linearizes the tensor into a vector . Mode d product : The mode d matrix product of a tensor is denoted by dU and results in a tensor of size I1…Id 1J Id+1…IM . Elementwise , with a matrix we have
.
Mode d accumulation : A mode d accumulation or summation is defined as
. The operation sums up all entries across all modes except for mode d , which results in a vector of length Id . Accumulating a tensor on mode d can be obtained by unfolding the tensor on mode d into a matrix and then multiplying the matrix with an all one vector . Like unfolding operation , accumulation can be defined on multiple modes , eg a mode (c,d ) accumulation
. This will result in a matrix of size IcId . Tensor decomposition or factorization is a form of higher order principal component analysis . It decomposes a tensor into a core tensor multiplied by a matrix along each mode . Thus , in the threeway case where IJK , we have 1A2B3C , which means each element of the tensor  is the product of the corresponding matrix elements multiplied by weight zpqr , ie
. Here , AIP , BJQ and
CKR are called factor matrices or factors and can be thought of as the principal components of the original tensor along each mode . The tensor PQR is called the core tensor and its elements show interaction between different components . A special case of tensor decomposition is referred as CP or PARAFAC decomposition [ 1 ] , where the core tensor is level of the superdiagonal and P=Q=R . ( A tensor is diagonal if only if i1=…=iM . ) The CP decomposition of a third order tensor is then simplified as
, as illustrated in Figure 2 . We use [ z ] to denote a superdiagonal tensor , where the operation [ · ] transforms a vector z to a superdiagonal tensor by setting tensor element zk…k=zk and other elements as 0 . Thus the CP decomposition of a three way tensor can be written as [z]1A2B3C , where [ z ] denotes a corresponding superdiagonal core tensor .
4 . PROBLEM FORMULATION This section defines the problem of discovering latent community structure that represents the context of user actions in social networks . The problem has three parts : ( 1 ) how to represent multi relational social data ( section 4.1 ) , ( 2 ) how to reveal the latent communities consistently across multiple relations , and ( 3 ) how to track the communities over time ( section 42 )
4.1 Metagraph Representation We introduce metagraph , a relational hypergraph for representing multi relational and multi dimensional social data . We use a metagraph to configure the relational context specific to the system features – this is the key to making our community analysis adaptable to various social media contexts , eg Digg and Flickr ( Figure 1 ) . We shall use the Digg example to illustrate three concepts : facet , relation , and relational hypergraph .
As shown in Figure 1(a ) , Digg allows various actions for news sharing – users might submit ( indicated by the line labeled ―S‖ ) a news story associated with a particular topic . They might vote ( or digg , line ―D‖ ) or comment ( line ―C‖ ) on the submitted story , reply ( line ―R‖ ) to a comment created by other users , or even reply to a reply ( not shown in the figure ) , etc . To describe the context of actions , we call a set of objects or entities of the same type a facet , eg a user facet is a set of users , a story facet is a set of stories , etc . We call the interactions among facets a relation ; a relation can involve two ( ie binary relation ) or more facets , eg the ―digg‖ relation involves two facets ( user , story ) , and the ―make comment‖ is a 3 way relation ( user , story , comment ) . A facet can be implicit , depending on whether the facet entities interact with other facets , eg the set of digg objects might be omitted due to no interactions with other facets . Formally , we denote the q th facet as v(q ) and the set of all facets as V . A set of instantiations of an M way relation e on facets v(1 ) , v(2),… , v(M ) is a subset of the Cartesian product v(1)…v(M ) . We denote a particular relation by e(r ) where r is the relation index . The observations of an M way relation e(r ) is represented as an Mway data tensor (r ) .
Now we introduce a multi relational hypergraph ( denoted as metagraph in this paper ) to describe the combination of relations and facets in a social media context . A hypergraph is a graph where edges , called hyperedges , connect to any number of vertices . The idea is to use an M way hyperedge to represent the interactions of M facets : each facet as a vertex and each relation as a hyperedge on a hypergraph . A metagraph defines a particular structure of interactions among facets , not among facet elements . Formally , for a set of facets V={v(q)} and a set of relations E={e(r)} , we construct a metagraph G=(V,E ) . To reduce notational complexity , V and E also represent the set of all vertex and edge indices respectively . A hyperedge/relation e(r ) is said to be incident to a facet/vertex v(q ) if v(q)e(r ) , which is represented by v(q)~e(r ) or e(r)~v(q ) . Eg , in Figure 1(c ) v(1 ) represents the user facet , e(5)={v(1),v(2),v(3)} represents the ―make comment‖ relation . We summarize our notations in Table 1 .
Symbol x X 
I1,…,IM v(q ) e(r ) V E
G
K , L
Description a vector ( boldface lower case letter ) a matrix ( boldface capital letter ) a tensor ( boldface Euler script letter ) the dimensionality of mode 1 , … , M a vertex v(q)V represents the facet v(q ) a hyperedge e(r)V represents the relation e(r ) the set of all facets V={v(q)} , or the set of all vertex indices the set of all relations E={e(r)} or all hyperedge indices a metagraph G=(V,E ) , where V is a set of facets/vertices and E is a set of relations/hyperedges constants
Table 1 : Description of notations .
{1,,},,(,)(,(,))cdqqMqcdIIIcdunfoldcd1MIIdJIU111121()dddMMddIdiijiiiiijiixuU()(,)dIdaccdX1(,)3(,(,))cdIIcdacccd1111PQRijkpqripjqkrpqrxzabc1MII10Miix1Rijkrirjrkrrxzabc529 4.2 Community Discovery on Metagraph We formalize the community discovery problem as latent space extraction from multi relational social data represented by a metagraph . Our goal is to discover latent community structures that represent the context of user actions in social media networks . We are interested in clusters of people who interact with each other in a coherent manner . Some of the interaction can be implicit , eg two users may comment on the same stories , and the interactions can be further enhanced by other interactions . Hence we consider a community as a latent space of consistent interactions or relations among users and objects . interactions in a community ,
By assuming consistent the interaction between any two entities ( users or media objects ) i and j in a community k , written as xij , can be viewed as a function of the relationships between community k with entity i , and k with j . If we consider the function to be stochastic , ie let pki indicate how likely an interaction in the k th community involves the i th entity and pk is the probability of an interaction in the k th community , we can express xij by xijk pkipkjpk . Likewise a entity 3 way is . A set of such interactions interaction among i3 i1 , i2 and among entities in facet v(1 ) , v(2 ) and v(3 ) can be written by :
<1> where three way interactions among facet v(1 ) , v(2 ) and v(3 ) . is the data tensor representing the observed is written as an ( iq,k) element of U(q ) for q=1,2,3 . U(q ) is a IqK matrix , where Iq is the size of v(q ) . The probabilities of communities are elements of z , ie pk=zk . This is similar to the CP decomposition of a tensor ( section 3.2 ) , except that the core tensor [ z ] and the factor matrices {U(q)} are constrained to contain nonnegative probability values . Under the nonnegative constraints , the 3 way tensor factorization is equivalent to the three way aspect model in a three dimensional co occurrence data [ 13 ] . The nonnegative tensor decomposition can be viewed as community discovery in a single relation . The interactions in social media networks are more complex – usually involving multiple two or multi way relations . By using metagraphs , we represent a diverse set of relational contexts in the same form and define the community discovery problem on a metagraph , with the following two technical issues .
The first issue is how to extract community structure as coherent interaction latent spaces from observed social data defined on a metagraph , which is formally stated as follows .
Problem ( Metagraph Factorization , or MF ) : given a metagraph G=(V,E ) and a set of observed data tensors {(r)}rE defined on G , find a nonnegative core tensor [ z ] and factors {U(q)}qV for corresponding facets V={v(q)} . ( Since E also represents the set of all edge indices , the notations rE and e(r)E are exchangeable . Likewise , qV and v(q)V are exchangeable . )
The second issue concerns the dynamic nature of human activities – those interactions might be consistent during a short time period but are unlikely to be consistent all the time . The problem , how to extract community structure as coherent interaction latent spaces from time evolving data given a metagraph , is defined as follows .
Problem ( Metagraph Factorization for Time evolving data , or MFT ) : given a metagraph G=(V,E ) and a sequential set of
Figure 3 : An example of the metagraph factorization ( MF ) . Given observed data tensors {(a),(b)} and a metagraph G that describes the interaction among facets {v(1 ) , v(2 ) , v(3)} , find a consistent community structure expressed by core tensor [ z ] and facet factors {U(1 ) , U(2 ) , U(3)} . observed data tensors {t find a nonnegative core tensor [ zt ] and factors {Ut corresponding to facets V={v(q)} for each time t .
( r)}rE defined on G for time t=1,2,… , ( q)}qV
We will present our method in two steps : ( 1 ) present a solution to MF ( section 5 ) ; ( 2 ) extend the solution to solve MFT ( section 6 ) .
5 . METAGRAPH FACTORIZATION This section presents our solution to the metagraph factorization problem ( MF ) . Our method relies on formulating MF as an optimization problem ( section 51 ) We then provide an algorithm to solve the optimization objective ( section 5.2 ) and discuss its computational complexity ( section 53 )
5.1 Optimization Objective The MF problem can be stated in terms of optimization . Let us first consider a simple metagraph case . Assume we are given a metagraph G=(V,E ) with three vertices V={v(1 ) , v(2 ) , v(3)} and two 2 way hyperedges E={e(a),e(b)} that describe the interactions among these three facets , as shown in Figure 3 . The observed data corresponding to the hyperedges are two second order data tensors ( ie matrices ) {(a),(b)} with facets {v(1 ) , v(2)} and {v(2 ) , v(3)} respectively . The facet v(2 ) is shared by both tensors .
The goal is to extract community structure from data tensors , through finding a nonnegative core tensor [ z ] and factors {U(1 ) , U(2 ) , U(3)} corresponding to the three facets . The core tensor and factors need to consistently explain the data , ie we can approximately express the data by (a)[z]1U(1)2U(2 ) and (b)[z]2U(2)3U(3 ) , as in eq<1> . The core tensor [ z ] and facet U(2 ) are shared by the two approximations , and the length of z is determined by the number of latent spaces ( communities ) to be extracted . Since both the left and the right hand side of the approximation are probability distributions , it is natural to use the KL divergence ( denoted as D(|| ) ) as a measure of approximation cost . To simultaneously reduce two approximation costs we can define a cost function as :
,
<2> where D(||)=i ( ai log ai/bi – ai + bi ) is the KL divergence between tensor  and  and a = vec( ) , b = vec( ) .
The solution to eq.<2> will be an MF solution for the metagraph in Figure 3 . We observe three things in this example : In eq.<2> , each D(|| ) correspond to a hyperedge , each tensor product corresponds to how facets are incident to an hyperedge and the summation corresponds to all hyperedges on the graph . We can generalize eq.<2> to any metagraph G , as follows .
123123iiikkikikikxpppp3(1)(2)(3)()11[],KmkkkkmkmpuuuzU123IIIqkip()(1)(2)()(2)(3)1223(||[])(||[])abDDzUUzUU530 Given a metagraph G=(V,E ) , the objective is to factorize all data tensors such that all tensors can be approximated by a common nonnegative core tensor [ z ] and a shared set of nonnegative factors {U(q)} , ie to minimize the following cost function : can be rewritten as follows . First , for each e(r ) , compute a tensor (r) 
+ by :
<7>
<8>
<3> where  denotes the element wise division , and  denotes the where K is the number of communities , and D(|| ) is the KLdivergence as described above . The constraint that each column of {U(q)} must sum to one is added due to the modeling assumption that the probability of an occurrence of a relation on an entity is independent of other entities in a community . Eq <3> can be easily extended to incorporate weights on relations .
5.2 Algorithm We provide a solution to the objective function defined in eq<3> It is difficult to guarantee a global minima solution , as eq.<3> is not convex in all variables . By employing the concavity of the log function ( in the KL divergence ) , we derive a local minima solution to eq<3> The solution can be found by the following updating algorithm :
,
,
<4>
<5> where z is a length K vector , L=|E| denotes the total number of hyperedges on G , Lq=|{l:e(l)~v(q)}| denotes the number of hyperedges incident to v(q ) , and
.
<6>
After updates , each column of U(q ) are normalized to sum to one . Because of this normalization step , we can omit dividing by Lq in eq<5> This iterative update algorithm is a generalization of the algorithm proposed by Lee et al . [ 10 ] for solving the single nonnegative matrix In metagraph factorization , the update for core tensor [ z ] depends on all hyperedges on the metagraph , and the update for each facet factor U(q ) depends on the hyperedges incident to the facet . The proof for the convergence of our algorithm is omitted due to space limit . factorization problem .
The computation in eq.<4>–<6> can be time consuming due to the high dimensionalities of tensors . We now discuss an efficient implementation of the update rules . In eq.<4>–<6> , is an element of a tensor . Let (r) 
+ denote this tensor , where  denotes the dimensionalities in short . Because (r ) is expensive to compute and operate , we want to reduce computation that involves (r ) . By observing the shared part for updating the core tensor and all facet factors in eq.<4> and <5> , we can use the following strategy to achieve efficient computation : Instead of computing (r ) explicitly , we compute an intermediate tensor (r ) of the same dimensionalities as (r ) . (r ) will save the repeating part of multiplication of (r ) with {U(q)} and z in eq.<4> and <5> . Thus , the above update rules
Khatri Rao product . The Khatri Rao product of two matrices A by and , where ak and bk are denoted defined
B , by
AB , is the kth column vectors of A and B respectively , and where ab is the Kronecker product of a and b . The second step is to update z and {U(q)} by :
<9>
<10> where Mr+1 is the last mode of (r ) . The multiplication of (r ) and (r ) in eq.<4> and <5> is now pre computed in eq.<7> and <8> by utilizing the Khatri Rao product . To obtain z and {U(q)} , we only need to accumulate (r ) on the corresponding modes . {U(q)} obtained from eq.<10> will be equivalent to those from eq.<5> after normalization . Eq <7>–<10> yield exactly the same results as eq<4>–<6> The algorithm shares the same form of the expectation maximization algorithm , where eq.<7> and <8> correspond to the E step and eq.<9> and <10> correspond to the M step . Note that the information contained in each data tensor with respect to a hyperedge is aggregated through the E step and is shared by the core tensor and all facet factors in the M step , thus the extracted communities will be coherent latent spaces . Table 2 summarizes the whole process to solve an MF problem .
Algorithm 1 : MF Input : metagraph G = ( V,E ) and data tensors {(r)} on G Output : z and {U(q)} Method : Initialize z , {U(q)}
Repeat until convergence For each rE , compute (r ) by eq.<7> and <8> update z by eq . <9> For each qV , update U(q ) by eq . <10>
Table 2 : The MF ( metagraph factorization ) algorithm .
We refer the solution core tensor and facet matrices as a community model , from which we infer the probabilistic ( soft ) membership of entities in each facet . As described in section 4.2 , each ( i,k) element of a facet matrix U is p(i|k ) ( ie pki , how likely an interaction in the community k involves entity i ) , and each element zk=p(k ) ( ie pk , the probability of an interaction in community k ) . Thus we compute the conditional probability p(k|i ) to indicate the soft membership of entity i with respect to community k by p(i|k)p(k)/p(i ) , where p(i)=k’p(i|k’)p(k’ ) is the probability of an interaction involving entity i .
5.3 Computational Complexity We now discuss the time complexity for the updates . The most time consuming step in the algorithm is to compute (r ) for each hyperedge e(r ) . As can be seen in eq.<7> , we can take advantage of the sparseness of the data tensor (r ) and compute only the
( )()()()(),{}:~1()()()min(||[] ) . , ,1 qmrqrmmrEmveIKKqqikiJGDstqqkzUzUzUU111()()1MMrrMrrrkiiiikrEiiLz11()()111()()():~1qMMlllqqqMlqllikiiiikiiiilevqLU()()()1()()1():~():~([])mmrmMrmrMrkikrmveiikmmiimvezUzU1()Mrriik1rMIIK1rMIIK()()()()():~(([]))mrrrmmmvevecμzU()()()(1)T(())rMrrfoldμzUU1122KKABababab()1(,1)rrrEaccMLz()()()():~(,(1,))lqqlllevaccMqU531 non zero elements ( total number of tuples ) in (r ) . Let n denote the largest number of non zero elements of the involved data tensors . This step has time complexity O(nKML ) , where K is the number of clusters , M is the maximal number of incident facets of a relation , and L is the total number of input relations . Usually , K , M , L are much smaller than n . If we consider K , M and L are bounded by some constants , the time complexity per iteration is linear in O(n ) , the number of non zero elements in all data tensors .
6 . TIME EVOLVING EXTENSION This section presents our solution to the problem of metagraph factorization with time evolving data ( MFT ) .
6.1 Optimization Objective In the MFT problem , the relational data is constantly changing as evolving tensor sequences . We propose an online version of MF to handle dynamic data . Since historic information is contained in the community model extracted based on previously observed data , the new community structure to be extracted should be consistent with previous community model and new observations , which is similar to the evolutionary clustering discussed in [ 11 ] . To achieve this , we extend the objective in eq.<3> as follows .
A community model for a particular time t is defined uniquely by ( q)} and core tensor [ zt ] . ( To avoid notation clutter , the factors {Ut we omit the time indices for t . ) For each time t , the objective is to factorize the observed data into the nonnegative factors {U(q)} and core tensor [ z ] which are close to the prior community model , [ zt 1 ] ( q)} . We introduce a cost lprior to indicate how the new and {Ut 1 community structure deviates from the previous structure in terms of the KL divergence . The new objective is defined as follows :
<11> where α is a real positive number between 0 and 1 to specify how much the prior community model contributes to the new community structure . lprior is a regularizer used to find similar pairs of core tensors and pairs of facet factors for consecutive times . The new community structure will be a solution incrementally updated based on a prior community model .
6.2 Algorithm Based on a derivation similar to the discussion in section 5 , we provide a solution to eq . <11> as follows :
,
<12>
,
<13>
<14>
<15> where Mr+1 is the last mode of (r ) . The whole process of finding solutions to the MFT problem is summarized in Table 3 .
Algorithm 2 : MFT
Input : hypergraph G = ( V,E ) , the data tensors Output : new model z and {U(q)} time t , previous model zt 1 , and {Ut 1 Method : Initialize z,{U(q)}
( q)}
{(r)} on G observed at
Repeat until convergence For each rE , compute (r ) by eq.<7> and <8> update z by eq . <14> For each qV , update U(q ) by eq . <15> Table 3 : The MFT algorithm . time evolving social data , changes might happen
For in interactions among entities , or even in interactions among facets ( eg due to the evolution of system features ) which lead to changes in metagraph . One advantage of our MFT algorithm is it only requires new observed data defined on any given metagraph , so it is straightforward to incorporate the changes of a metagraph .
7 . EXPERIMENTS This section reports our experimental study on a real world social media dataset collected from Digg . We first describe the dataset ( section 7.1 ) and present the extracted communities ( section 72 ) We evaluate our technique through prediction tasks ( section 73 ) Finally , we evaluate the scalability of our factorization method on synthetic datasets ( section 74 )
7.1 Digg Dataset We have collected data from a large set of user actions from Digg . Digg is a popular social news aggregator that allows users to submit , vote ( ie digg ) and comment on news stories . It also allows users to create social networks by designating other users as friends and tracking friends’ activities . The dataset used in our experiments include stories , users and their actions ( submit , digg , comment and reply ) with respect to the stories , as well as the explicit friendship ( contact ) relation among these users . To analyze users’ topical interests , we also retrieve the topics of the stories and extract keywords from the stories’ titles .
Relation
Tensor / incident facets
( R1 ) content dynamic ( story , keyword , topic )
( R2 ) contact static ( user , user )
( R3 ) submit dynamic ( user , story )
( R4 ) digg dynamic ( user , story )
( R5 ) comment dynamic ( user , story , comment )
( R6 ) reply dynamic ( user , comment )
#Tuples
151,779
56,440
44,005
1,157,529
241,800
94,551
Table 4 : Summary of the relations in Digg dataset . where is defined as in eq<6> After updates , each column of U(q ) and the vector z are normalized to sum to one . Because of this normalization step , we have dropped the scaling constant for updating z and U(q ) . It can be shown that the parameters in the previous model ( zt 1 and ( q)} ) act as Dirichlet prior distribution to inform the solution {Ut 1 search ( ref . [ 11] ) , thus the solution is consistent with previous community structure . The update rules can be rewritten as the following operations with (r ) pre computed by eq.<7> and <8> :
From this dataset , we select 5 facets ( user , story , comment , keyword and topic ) and build 6 relations among them . The relations are summarized in Table 4 , which correspond to the metagraph shown in Figure 1(c ) . Except for the contact relation , all relations have timestamps . We assume the contact relation is static and consider the other relations as dynamic . For dynamic relations , we extract tuples with timestamps ranging from August 1 to August 27 , 2008 . To study the data evolution , we segment the duration into 9 time slots ( ie every three days ) , and construct a sequence of data tensors for each dynamic relation . In the
( )()()()()2,{}:~()()111()()()min(1)(||[])(||)(|| ) . , ,1 qmrqrmmpriorrEmveqqpriorttqIKKqqikiJGDllDDstqqkzUzUzzUUzUU111()();1(1)MMrrMrrrkiiiikktrEiizz11()()111()()()();1:~(1)qMMqlllqqqMlqllqikiiiikiktiiiilevUU1()Mrriik()1(1)(,1)rrtrEaccMzz()()()()()1:~(1)(,(1,))lqqlqltlevaccMqUU532 ( a ) 2 community
( b ) 4 community
( c ) 12 community
C1
C2
C3
C4
Figure 4 : Community extracted based on user digging activities , for time t=3 ( August 6 9 , 2008 ) and number of communities K=2 , 4 and 12 . The most likely keyword and topic terms ( shown within brackets ) in each community are projected based on their soft membership . The size of each term indicates its probability and each term is colored based on its most likely community . The results show coherent topical preference in communities , as the terms with the same colors are located closely . following we shall use t[1,9 ] to denote a time slot index . The total number of tuples in each tensor sequence per relation is listed in Table 4 . Our dataset and code are available online ( http://wwwpublicasuedu/~ylin56/kdd09suphtml )
7.2 Community Analysis We present a qualitative analysis of the communities extracted by our method , which demonstrates an advantage of probabilistic interpretation given by our method . We first show all communities extracted for a particular time and then examine the community evolution within two of these communities .
To illustrate what kinds of stories are ―dugg‖ by what kind of communities , we track the latent communities based on the digging activities which involve relation R1 and R4 . Figure 6(a ) and ( e ) shows the corresponding metagraph and the number of tuples in the two relations . In our factorization algorithm , we assume that the number of communities , K , is given beforehand . Here we show communities extracted given K=2 , 4 and 12 .
Based on relation R1 and R4 , four facets are involved : user , story , keyword and topic . We present the keyword and topic facets because they are more informative to the readers than other facets . Figure 4 shows the most likely keywords and topics in each community . We present the results of t=3 ( August 6 9 , 2008 ) . We project those keyword and topic ( shown within brackets ) terms onto a 2D plane . The location of the i th keyword or topic term indicates its relative proximity to other terms and is computed based on its soft membership p(k|i ) . ( The position is determined by standard multidimensional scaling with the soft membership as input . ) The size of the i th term indicates how likely the term appears in a story and is determined based on the probability p(i ) . Each term is colored based on its most likely community , ie by choosing k with maximal p(k|i ) . In the figure we can see the communities based on users’ digging activities have coherent topical preference , as the terms with the same colors are located closely . The 2 , 4 and 12 community results show the communities at different resolution . The 2 community result distinguishes political interests from the Olympics news ( Figure 4(a) ) . The 4 community shows in communities : C1 : gaming industry news , C2 : US election news , C3 : world news , and C4 : general political news ( Figure 4(b) ) . The two major topics ( ―olympics‖ and ―georgia‖ ) in C3 are further split in the 12 community result ( Figure 4(c) ) . interests four topical
( a ) community popularity
( b ) concept evolution
Figure 5 : The community evolution characterized based on ( a ) change in size of communities , and ( b ) change in keyword distribution in each community . four communities over time , and Figure 5(b ) shows the keyword dissimilarity across time where the dissimilarity is computed based on the cosine similarity of keyword distribution in each community of consecutive timestamps . ( We use a cosine similarity measure in order to emphasize the differences at the ―head‖ of the distributions . ) We observe two critical times in Figure 5(b ) : for community C2 and C3 , the keywords distribution change drastically at t=3 ( August 6 9 ) and t=8 ( August 21 24 ) . To examine the events occurring during these times , we look at the keyword distributions of the two communities . Table 5 lists the top 10 keywords that are mostly likely to appear in C3 and C2 , at t=2,3 and t=7,8 respectively . At t=3 , the new popped keywords ―olympics‖ and ―georgia‖ reflect users’ attention two significant world news items : the 2008 Summer Olympics began on August 8 and the 2008 Russia Georgia conflict started on August 7 . At t=8 , the new popped keywords ―joe‖ , ―biden‖ , ―vp‖ correspond to the time when presidential candidate Barack Obama announced that Joe Biden would be his running mate ( on August 22 ) . Another critical time is captured by the change in community size . In Figure 5(a ) , we see the community C3 keeps growing until t=6 , when the Russia Georgia conflict ended with a ceasefire agreement signed on August 15 and 16 . to
Community Evolution . We select the 4 community result and examine its evolution . Figure 5(a ) shows the probabilities of the
Table 5 : The keyword distribution of community C2 and C3 during two critical times , t=3 and t=8 .
533 The characterization of community evolution based on change in the probability of a cluster and on change in the distribution of entities such as keywords ( Figure 5 and Table 5 ) demonstrates the advantage of our soft clustering method . The presented case study suggests that our method is able to generate meaningful mining results from dynamic multi relational social networks .
7.3 Evaluation via Prediction We use a prediction task to demonstrate the utility of our techniques . Based on the Digg scenario , we design two prediction tasks – to predict users’ future interests on digging ( ie voting ) and commenting on Digg stories . We study three aspects of our method tasks : ( 1 ) How does our community discovery framework help predict users’ future interests ? ( 2 ) How much historic information do we need ? ( 3 ) Which relation is relevant to the prediction ? the prediction through
Prediction setting . There are two tasks : ( a ) digg prediction – what stories a user will digg , and ( b ) comment prediction – what stories a user will comment on . Both tasks are evaluated on data from each time slot . We use stories that have digging or commenting events in time slot ts[2,9 ] as testing sets and the available relational data ( ref . Table 4 ) in time slot ts 1 as training sets . The prediction results are compared with the actual diggs and comments occurring in slot ts . This is a constrained setting because there might be more digging or commenting activities occurring after ts . In our prediction experiments we only consider diggs and comments in each single slot ts as ground truth .
Evaluation metrics . We use two metrics adopted in Information Retrieval : ( 1 ) ―P@10‖ ( the precision of the top 10 results ) : For each user we compute the precision based on the top 10 stories retrieved for the user . The overall P@10 for the set of users is computed by taking the mean of P@10 per user , per time slot . ( 2 ) ―NDCG‖ ( Normalized Discount Cumulative Gain [ 7] ) : One advantage of the measure is its sensitivity to the prediction order . The NDCG is proportional to i(i)/log(1+i ) , where i is the rank of predicted stories , δ(i)=1 if the prediction of the rank i story is correct and 0 otherwise .
Our prediction method . We generate predictions based on the community structure extracted by our method , denoted by MF and MFT . The MF algorithm outputs community structure from relational data of each time slot ts 1 . The MFT algorithm uses the same data as MF , with the aid of a community model extracted for time ts 2 as an informative prior . Hence MFT gives results incrementally . From an extracted community model we obtain the probability of a community k , p(k ) , and the probability of a user u , a keyword w and a topic j , given community k , ie p(u|k ) , p(w|k ) and p(j|k ) . To predict if a user u will digg or comment on a story r , we first use a folding in technique ( ref . eg [ 13 ] ) to compute p(r|k ) , the probability of a story given each community k , based on the story topic and keywords . Then a prediction is made based on the condition probability p(r|u)p(u,r)k’p(k)p(u|k’)p(r|k’ ) .
Baseline methods . Three baseline methods are used : ( 1 ) Frequency based heuristics ( FREQ ) – predicting stories based on the frequencies of story topic and keywords at ts 1 . ( 2 ) Standard tensor analysis ( PARAFAC ) – predicting stories by using the CP/PARAFAC tensor decomposition [ 1 ] for data in slot ts 1 . The stories to be predicted are first projected on the latent spaces , and the prediction is made based on the dot product of the user and story projected vectors . ( 3 ) Multi way aspect model ( MWA ) – predicting stories by using the multi way aspect model [ 13 ] , a special case of our model ( ref . section 42 )
( e )
( a )
( c )
( b )
( d )
Figure 6 : Relations used by different methods for digg and comment prediction : ( a ) R1 and R4 used in our method for digg prediction ; ( b ) R1 and R5 used in our method for comment prediction ; ( c ) TD tensors used in PARAFAC and MWA for digg prediction ; ( d ) TC tensors used in PARAFAC and MWA for comment prediction ; ( e ) number of tuples in each relation over time .
The ability to handle relational contexts is the key to our comparison . We choose specific relations to illustrate the utility of leveraging a specific context by a metagraph – relation R1 and R4 for digg prediction and R1 and R5 for comment prediction ( ref . Figure 6(a ) and ( b) ) , and we shall evaluate the effect of other relations later in this section . Since PARAFAC and MWA only deal with a single high dimensional relation , we construct two 4way tensors per time that contains digg actions and comment actions with respect to stories . The two tensors , denoted by TD and TC are shown in Figure 6(c ) and ( d ) . Figure 6(e ) shows the number of non zero entries ( tuples ) of these data tensors over time . The number of tuples in an R5 tensor corresponds to the number of stories per time .
Except for the FREQ method , all methods are tested with number of clusters or latent spaces K=4… 20 . For MFT , we use α=02 metric digg prediction comment prediction method
FREQ
PARAFAC
MWA
MF
MFT
P@10
NDCG
P@10
NDCG
01750061
00350016
00150007
00040002
03690004
01450002
00490002
00180001
01950002
00690001
00670001
00200000
05290008
02120002
01170001
00380000
05430007
02150004
01350001
00430000
Table 6 : The average prediction performance for digg and comment prediction , evaluated by P@10 and NDCG metrics .
Results and Discussion . The overall prediction performance is obtained by taking the average of prediction performance on data for each time slot ( t=1…8 for training and t=2…9 for testing ) over different K values . The results ( mean and standard deviation ) are given in Table 6 . There are several observations . First , our method significantly outperforms all baseline methods . In digg prediction , our MF method outperforms the baselines by 43 % to 5X on the average . In comment prediction , the MF method outperforms the baselines by 73 % to 10X . Second , the MFT performs the best . It slightly outperforms MF in digg prediction and improves MF by 15 % in comment prediction . Next we show how our prediction can be further improved by ( a ) incorporating a historic model and ( b ) leveraging other relations through a metagraph .
Effect of historic information . We vary the weight of the prior model in MFT and report the average P@10 over α values ( Figure 7(a) ) . The results suggest that incorporating historic information as prior knowledge works better than no prior ( α=0 ) . Note for comment prediction , the performance increases until α08 This suggests that the comment activities are more consistent with the historic community structure than the digg activities .
534 ( a )
( b )
Figure 7 : Effect of ( a ) prior community model ( historic information ) and ( b ) different input relations .
Effect of various relational context . For comment prediction , we evaluate the prediction performance over different relational contexts . Figure 7(b ) shows the average prediction results . The label R* indicates which relations are used in the training set , eg R125 denotes relation R1 , R2 and R5 . We observe that different combinations of the relations affect the prediction performance . For example , incorporating the contact relation R2 with R1 and R5 significantly helps predict users’ comment activities .
7.4 Scalability Evaluation We use synthetic datasets to illustrate the scalability of our algorithms . We study how the computational time of our algorithm increases with four variables , including different types of data growth – ( a ) non zero elements in a data tensor , ( b ) number of tensor modes ( dimensions ) , ( c ) number of relations ( tensors ) on a given metagraph , as well as ( d ) the algorithm parameter , ie number of clusters . In the simulation we randomly generate tensors by varying one of the above variables ( eg the number of non zero elements ) and fixing all remaining variables .
( a )
( b )
( c )
( d )
Figure 8 : Running time per iteration ( sec . ) for different types of data growth ( let n denote the value on the x axis of each plot ) : ( a ) number of non zero elements ( one 3 way tensor with n non zero elements ) , ( b ) number of tensor modes ( one n way tensor ) , ( c ) number of relations ( n 3 way tensors ) in a metagraph , and ( d ) for different algorithm parameter , the number of clusters ( K ) .
Figure 8 shows the simulation results , indicating that the running time per iteration scales linearly with the data size , the number of tensor modes , the total number of relations and the number of clusters . Note that the slope for increasing tensor modes is steeper than increasing relations . Empirically the non zero elements in a higher mode tensor are usually much more than lower mode tensors ( as in Figure 6(e) ) . Therefore by leveraging a metagraph we can efficiently combine multiple low dimensional relations instead of constructing a high dimensional tensor . The experimental results on the synthetic datasets correspond to our analysis in section 5.3 and suggest that our algorithm can efficiently deal with large sparse multi relational data .
8 . CONCLUSION We proposed the MetaFac framework to extract community structures from various social contexts and interactions . There to handle were three key ideas : ( 1 ) metagraph , a relational hypergraph for representing multi relational social data ; ( 2 ) MF algorithm , an efficient non negative multi tensor factorization method for community extraction on a given metagraph ; ( 3 ) MFT , an on line factorization method time varying relations . We conducted extensive experiments on real world data collected from Digg . Our case study demonstrated that meaningful mining results can be generated by our method . We evaluated our method by predicting digg / comment actions on stories . We generated the predictions based on the extracted community models and compare results with baselines . Our method outperformed baseline measures up to an order of magnitude . Our method can be further improved by ( a ) incorporating a historic model and ( b ) leveraging other relations through a metagraph . As part of our future work , we plan to investigate the following open issues : ( 1 ) the resolution ( including number of communities ) of community structures ; ( 2 ) kernel based representation of facet relationships .
9 . REFERENCES [ 1 ] B . BADER and T . KOLDA ( 2006 ) . Algorithm 862 : MATLAB tensor classes for fast algorithm prototyping . TOMS 32(4 ) : 635 653 .
[ 2 ] A . BANERJEE , S . BASU and S . MERUGU ( 2007 ) . Multi way
Clustering on Relation Graphs , SDM , 2007 .
[ 3 ] R . BEKKERMAN , R . EL YANIV and A . MCCALLUM ( 2005 ) .
Multi way distributional clustering via pairwise interactions , ACM Intl . Conf . Proc . Series , 41 48 ,
[ 4 ] D . CHAKRABARTI , R . KUMAR and A . TOMKINS ( 2006 ) .
Evolutionary clustering , SIGKDD , 554 560 ,
[ 5 ] Y . CHI , S . ZHU , Y . GONG and Y . ZHANG ( 2008 ) .
Probabilistic Polyadic Factorization and Its Application to Personalized Recommendation , CIKM , 2008 .
[ 6 ] N . FRIEDMAN , L . GETOOR , D . KOLLER and A . PFEFFER
( 1999 ) . Learning probabilistic relational models , IJCAI , 1300 1309 , 1999 .
[ 7 ] K . JÄRVELIN and J . KEKÄLÄINEN ( 2000 ) . IR evaluation methods for retrieving highly relevant documents , SIGIR , 41 48 , 2000 .
[ 8 ] C . KEMP , J . TENENBAUM , T . GRIFFITHS , T . YAMADA and N .
UEDA ( 2006 ) . Learning Systems of Concepts with an Infinite Relational Model , Proc . of the Natl . Conf . on AI , 381 ,
[ 9 ] R . KUMAR , J . NOVAK and A . TOMKINS ( 2006 ) . Structure and evolution of online social networks , SIGKDD , 611 617 , 2006 .
[ 10 ] D . LEE and H . SEUNG ( 2001 ) . Algorithms for non negative matrix factorization , NIPS , 556–562 , 2001 .
[ 11 ] Y . LIN , Y . CHI , S . ZHU , H . SUNDARAM and B . TSENG ( 2008 ) . Facetnet : a framework for analyzing communities and their evolutions in dynamic networks , WWW , 2008 .
[ 12 ] B . LONG , Z . ZHANG and P . YU ( 2007 ) . A probabilistic framework for relational clustering , SIGKDD , 470 479 ,
[ 13 ] A . POPESCUL , L . H . UNGAR , D . M . PENNOCK and S .
LAWRENCE ( 2001 ) . Probabilistic Models for Unified Collaborative and Content Based Recommendation in Sparse Data Environments , UAI 2001 , 437 444 ,
[ 14 ] J . SUN , C . FALOUTSOS , S . PAPADIMITRIOU and P . YU ( 2007 ) . GraphScope : parameter free mining of large time evolving graphs , SIGKDD , 687 696 , 2007 .
[ 15 ] S . ZHU , K . YU , Y . CHI and Y . GONG ( 2007 ) . Combining content and link for classification using matrix factorization , SIGIR , 487 494 , 2007 .
535
