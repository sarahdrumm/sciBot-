Extracting Discriminative Concepts for Domain Adaptation in Text Mining∗
Bo Chen†
Wai Lam†
Ivor Tsang‡
Tak Lam Wong†
† The Chinese University of Hong Kong
{bchen , wlam}@secuhkeduhk , wongtl@csecuhkeduhk
‡ School of Computer Engineering , Nanyang Technological University
IvorTsang@ntuedusg
ABSTRACT One common predictive modeling challenge occurs in text mining problems is that the training data and the operational ( testing ) data are drawn from different underlying distributions . This poses a great difficulty for many statistical learning methods . However , when the distribution in the source domain and the target domain are not identical but related , there may exist a shared concept space to preserve the relation . Consequently a good feature representation can encode this concept space and minimize the distribution gap . To formalize this intuition , we propose a domain adaptation method that parameterizes this concept space by linear transformation under which we explicitly minimize the distribution difference between the source domain with sufficient labeled data and target domains with only unlabeled data , while at the same time minimizing the empirical loss on the labeled data in the source domain . Another characteristic of our method is its capability for considering multiple classes and their interactions simultaneously . We have conducted extensive experiments on two common text mining problems , namely , information extraction and document classification to demonstrate the effectiveness of our proposed method .
Categories and Subject Descriptors H.4 [ Information Systems Applications ] : Miscellaneous ; I52 [ Design Methodology ] : Feature evaluation and selection ∗ The work described in this paper is substantially supported by grants from the Research Grant Council of the Hong Kong Special Administrative Region , China ( Project No : CUHK4128/07 ) and the Direct Grant of the Faculty of Engineering , CUHK ( Project Codes : 2050391 and 2050442 ) . This work is also affiliated with the MicrosoftCUHK Joint Laboratory for Human centric Computing and Interface Technologies . This research is in part supported by Singapore MOE AcRF Tier 1 Research Grant ( RG15/08 ) .
General Terms Algorithms
Keywords Domain Adaptation , Text Mining , Feature Extraction
1 .
INTRODUCTION
Traditional statistical learning techniques rely on the basic assumption that the training data and the operational ( testing ) data are drawn from the same underlying distribution . However , in many text mining applications involving high dimensional feature space , it is difficult to collect sufficient training data for different domains . For example , consider a text information extraction problem whose objective is to automatically extract precise job information such as job title , duty , requirement , etc . from recruitment Web sites in different industries supporting intelligent analysis of employment information . Usually we may just have few experts who can accurately annotate the information in one specific industry like accounting for preparing the training data . The learned model deployed obviously cannot perform well in other domains ( industries ) such as logistic or health care due to the distribution of the terms in each domain is different . One strategy to tackle this problem is to adapt the trained model from one domain known as the source domain with sufficient labeled data to another domain known as the target domain where only unlabeled data is available .
It can be observed that domain adaptation is reasonable and practical if the distributions between the source domain and the target domain is related , which is mainly based on the fact that there exists a shared concept space in which the embedded distribution of each domain is close enough . Consequently it is very reasonable to believe that a good feature representation is able to encode this concept space and provide strong adaptive power from the source domain to the target domain . On the other hand , such a changed representation may encode less information leading to an increase of the empirical loss on the labeled data . To cope with this problem , we try to learn the ideal shared concept space with respect to two criteria : the empirical loss in the source domain , and the embedded distribution gap between the source domain and the target domain . Consider again the job information extraction example . For the task of extracting the job requirement information in the domain of accounting , the most representative terms are “ qualified ” ,
179 “ year ” , “ experience ” , “ CPA ” , “ CA ” , “ ACCA ” , etc . Similarly for the domain of health care , the representative terms shift to “ qualified ” , “ degree ” , “ year ” , “ CCP ” , “ physiology ” , “ experience ” , etc . If we can extract the shared domain independent features such as “ qualified ” , “ year ” , “ experience ” for the specific task , then the learned extractor can be effectively adapted to the domain of health care .
In this paper we propose a domain adaptation method which directly minimizes both the distribution gap between the source domain and the target domain , as well as the empirical loss on the labeled data in the source domain by extracting the low rank concept subspace . Maximum Mean Discrepancy ( MMD ) [ 5 ] is adopted to measure the embedded distribution difference between the source domain with sufficient but finite labeled data and the target domain with sufficient unlabeled data . Then our objective is to minimize the empirical loss and the MMD measurement with respect to the parametric family ( linear transformation ) which parameterizes the embedded feature subspace . Furthermore , we apply the graph Laplacian [ 1 ] to exploit the predictive power for some domain dependent representative features in the target domain based on the co occurrence with the shared features . This technique can help improve the performance especially when the common features are not sufficient in the target domain .
Several domain adaptation methods have been proposed to learn a reasonable representation so as to make the distributions between the source domain and the target domain closer [ 3 , 12 , 13 , 11 ] . However , none of them can automatically learn the concept space where the prediction power in the source domain and the adaptive power from the source domain to the target domain are both considered .
Our main contributions can be summarized as follows :
( 1 ) We propose a domain adaptation method to extract the low rank concept space shared by the source domain and the target domain , which can ensure both the predictive power and adaptive power are maximized . ( 2 ) We can transfer the predictive power from the extracted common features to the characteristic features in the target domain by the feature graph Laplacian . ( 3 ) We theoretically analyze the expected error in the target domain showing that the error bound can be controlled by the expected loss in the source domain , and the embedded distribution gap , so as to prove that what we minimize in the objective function is very reasonable for domain adaptation . ( 4 ) Our domain adaptation method is capable of considering multiple classes and their interactions simultaneously . It can be applied to high dimensional text mining applications due to two major properties of text : latent semantic and sparseness . The first property ensures that low rank concept space can still preserve enough information , and the second property contributes to the computation speed .
We have conducted extensive experiments on two common text mining problems , namely , information extraction and document classification to demonstrate the effectiveness of our proposed method . Experiment results show that our method can get better performance than other existing competitive methods . 2 . DOMAIN ADAPTATION 2.1 Related Work
Domain adaptation is a widely studied area . It addresses a common situation when applying the trained model to a different domain . Many works try to learn a new representation which can bridge the source domain and the target domain . Blitzer et al . [ 3 ] proposed a heuristic method to select some domain independent pivot features to learn an embedded space where the data coming from both domains can share the same feature structure . Daum´e III [ 4 ] proposed the Feature Augmentation method to augment features for domain adaptation . The augmented features are used to construct a kernel function for kernel methods . Raina et al . [ 12 ] learned the sparse basis from the unlabeled data which may not come from the same domain as the labeled data . Then it represents the labeled data by those learned high level basis for further classification . Yang et al.[16 ] proposed Adaptive SVM ( A SVM ) to enhance the prediction performance of video concept detection , in which the new SVM classifier is adapted from an existing classifier trained from the auxiliary domain . Several domain adaptation methods [ 6 , 14 , 15 , 8 , 2 ] suggested to apply the instance weighting technique for domain adaption in various applications . Recently , Pan et al . [ 11 ] applied the Maximum Mean Discrepancy ( MMD ) to learn the embedded space where the distribution between the source domain and the target domain is minimized .
2.2 Problem Statement and Preliminaries
.
In this paper , we focus on the setting where the operational ( testing ) samples come from another domain , which is different from the training set . In the sequel , we refer the training set to as the source domain DS = {(xi , yi)}n1 i=1 , where xi ∈ Rd is the d dimensional input space , and yi is the output label . We also assume that the testing samples i}n2 are available . Denote the testing set as DT = {x . i=1 and ) ( or P and Q for i ∈ Rd is the input . Let P(x ) and Q(x . x short ) be the marginal distributions of the input sets {xi} and {x i} from the source and target domains , respectively . . In general , P and Q can be different . The task of domain . adaptation is to predict the labels y i ’s corresponding to the . inputs x i ’s in the target domain . Note that domain adaptation is different from Semi Supervised Learning ( SSL ) . SSL methods employ both labeled and unlabeled data for better classification , in which the labeled and unlabeled data are assumed to be drawn from the same domain . Unlike SSL , the key assumption in domain adaptation is that P '= Q , but the class conditional distribution of the source and target domains remains unchanged , ie , P ( y|x ) = P ( y
.|x
.
) .
2.3 Maximum Mean Discrepancy
Recall that , in domain adaptation , the fundamental question is how to evaluate the difference in distribution between two domains given finite observations of {xi} and {x i} . There exists many criteria ( such as the Kullback . Leibler ( KL ) divergence ) that can be used to measure their distance . However , many of these estimators are parametric and require an intermediate density estimate . To avoid this non trivial task , a non parametric distance estimate between distributions is more desirable . Recently , Gretton et al . [ 5 ] introduced the Maximum Mean Discrepancy ( MMD ) for comparing distributions based on the Reproducing Kernel Hilbert Space ( RKHS ) distance . Let the kernel induced feature map be φ : R ff→ H , where H is the corresponding feature space . The MMD between the source domain DS and the target domain DT is defined as follows :
180 MMD[DS , DT ] = sup
( EQ[f ( x
.
) − EP [ f ( x)] ] ) fiffiH≤1 = .EQ[φ(x
.
) ] − EP [ φ(x)].H .
( 1 )
The empirical measure of the MMD in ( 1 ) is defined as :
‚‚‚‚‚ 1 n2
P x.∈DT
.
) − 1 n1
φ(x
P x∈DS
‚‚‚‚‚
φ(x )
.
H
( 2 )
MMD[DS , DT ] =
Therefore , the distance between two distributions of two samples is simply the distance between the two mean elements in the RKHS .
2.4 Kernel Mean Matching
Due to the change of distribution from different domains , training with samples from the source domain may degrade the generalization performance in the target domain . To reduce the mismatch between the two different domains , Huang et al . [ 6 ] proposed a two step approach Kernel Mean Matching ( KMM ) . The first step is to diminish the difference of means of samples in RKHS between the two domains by re weighting the samples φ(xi ) in the source domain as βiφ(xi ) , where βi is learned by using the MMD criterion in ( 2 ) . Then the second step is to learn a decision classifier f ( x ) = w φ(x ) + b that separates patterns of opposite classes using the loss function re weighted by βi in the objective function .
2.5 Maximum Mean Discrepancy Embedding However , the simple re weighting scheme may have a limited improvement in the target domain when the dimensionality of the data is high . In particular , some features may cause the data distribution between domains to be different , while others may not . Some features may preserve the structure of data for adaptation , while others may not . To address this problem , Pan et al . [ 11 ] proposed Maximum Mean Discrepancy Embedding ( MMDE ) for domain adaptation by embedding both the source and target domain data onto a shared low dimensional latent space . The key idea is to formulate this as a kernel learning problem using the kernel trick Kij = K(xi , xj ) = φ(xi ) φ(xj ) , and to learn the kernel matrix defined on all the data :
»
–
K =
KS,S KS,T KT,S KT,T
∈ R
( n1+n2)×(n1+n2 )
,
( 3 ) where KS,S , KT,T and KS,T are the Gram matrices defined on the source domain , target domain , and cross domain data , respectively . By minimizing the distance ( measured by MMD ) between the source and target domain data . The square of the MMD in ( 2 ) can be written as where trace(KD ) ,
8>>< >> :
1 2 n 1 1 2 n −1 2 n1n2
Dij = when xi , xj ∈ DS when xi , xj ∈ DT otherwise .
( 4 )
( 5 )
This leads to a Semi Definite Programming ( SDP ) problem . After that , the embedding of data can be extracted by performing eigen decomposition on the learned kernel matrix K in ( 3 ) , and can be used for training classifiers .
3 .
FEATURE EXTRACTION FOR MULTICLASS DOMAIN ADAPTATION
In previous domain adaptation methods [ 6 , 11 ] , the weights βi ’s in KMM or the kernel matrix K of samples in MMDE are learned separately using the MMD criterion in ( 2 ) defined on the input data only without considering any labels . While the use of labels in linear discriminant analysis usually helps extract more discriminative features , the label information from the source domains may be also useful to learn kernels or extract features for a better domain adaptation . In addition , there are two main limitations associated with MMDE . First , MMDE is transductive and cannot generalize on unseen patterns . Second , it requires to solve an expensive SDP problem , which takes O((n1 + n2)6.5 ) time to solve the optimization problem ( 4 ) . Although polynomial time solvers are available , current interior point methods are still too computationally intensive for large scale SDPs in real applications . Note that only the low dimensional embedding of the data is extracted from the learned kernel matrix K in MMDE , and is then used for the training of the decision classifiers . Therefore , not all components from the learned kernel matrix K are required to train the classifiers for domain adaptation . 3.1 Proposed Framework
Based on the above discussions , instead of using two step approaches as in [ 6 , 11 ] , we propose a unified domain adaptation learning framework to find the discriminative feature subspace Θ , and to learn decision classifiers fl(x ) ’s simultaneously . In particular , our proposed method minimizes the distribution difference between the samples of the source and target domains after the projection into the subspace Θ ( ie Θxi ) , as well as the structural risk functional of the n1 labeled data from the source domain DS . Moreover , we suppose that the learning problem is in multiclass setting , and there are m decision classifiers fl(x ) ’s . Let us denote the label indicator matrix as Y ∈ Rn1×m , and Yil = 1 if the i th sample belongs to the l th class , and 0 if it is labeled as others . Similar to other feature extraction methods , we also suppose Θ is orthogonal on rows so that ΘΘT = I . The optimization problem is then formulated as follows : mX n1X mX min fi(fl , Yil , xi ) + α l=1 i=1 l=1
Ω(fl ) + βdistΘ(DS , DT ) ,
( 6 ) subject to ΘΘT = I . Here , the first term is the empirical risk functional of the decision functions fl ’s on the labeled data from the source domain DS , and '(· ) is the empirical loss function . The regularizer Ω(· ) controls the complexity of fl , and the last term measures the distribution difference between the embedding of DS and DT . Two tradeoff parameters α > 0 and β > 0 are introduced to control the fitness of the decisions functions , and to balance the difference of distribution from the two domains and the structural risk functional for the labeled patterns , respectively . Hence , using ( 6 ) , the subspace Θ and the decision functions fl ’s can be learned at the same time .
311 Shared Subspace for Label Dependency
To capture the label dependency , we follow [ 7 ] to define the m decision functions : fl(x ) = w l x = u l x + v l Θx , l = 1,· ·· , m where wl ∈ Rd is the weight vector for the decision function , Θ ∈ Rr×d is the matrix of the shared subspace for the m decision functions , and vl ∈ Rr is the weight vector defined in
( 7 )
181 the projected subspace Θ , and ul ∈ Rd is the weight vector defined in the original input space . With the parametric form ( 7 ) of the m decision classifiers , the learned subspace Θ can capture the intrinsic structure of label dependency in multiclass problems [ 7 ] . The weight vector vl is the discriminative direction in the subspace Φ for each class , while the weight vector ul can be used to fit the residue wl − Θ vl for each class independently .
Though we learn a linear shared subspace in ( 7 ) , the linear subspace is usually more efficient and also achieves good generalization performance for high dimensional data such as text documents . Moreover , one can simply replace the input x by the feature mapped input φ(x ) in ( 7 ) and apply the Representer Theorem for wl , ul and Θ , which gives rise to the kernel variant of the proposed framework for the nonlinear generalization performance , which is beyond the scope of this paper . For simplicity , we use the notation x instead of φ(x ) in the sequel .
312 Loss Function and Regularization
For the empirical loss on the labeled data , we employ the square loss function : fi(fl , Yil , xi ) = ( fl(xi ) − Yil ) l xi − Yil )
= ( w l Θxi − Yil)2 . l xi + v
= ( u
2
2
Suppose XS = [ x1,·· · , xn1 ] ∈ Rd×n1 is the data matrix of the source domain , W = [ w1,··· , wm ] ∈ Rd×m , U = [ u1 , ··· , um ] ∈ Rd×m , and V = [ v1,··· , vm ] ∈ Rr×m , then the first term in ( 6 ) can be rewritten as : mX n1X fi(fl , Yil , xi ) = l=1 i=1
=
‚‚‚W ‚‚‚U
‚‚‚2 ΘXS − Y
XS − Y
XS + V
‚‚‚2
.
314 Final Formulation
Combining all the above , we arrive at the following mini mization problem :
‚‚‚W min Θ,W,V st ΘΘ
XS−Y
= I ,
‚‚‚2
‚‚‚W−Θ
+α
‚‚‚2
+βtrace(ΘXDX
V
)
Θ
( 9 ) which learns both the shared subspace Θ , and the parameters W and V in decision functions simultaneously .
3.2 Detailed Algorithm
In this section , we show that the optimization problem ( 9 ) can be solved efficiently by alternatively finding the optimal subspace matrix Θ , and the matrices V and W of the weight vectors .
321 Computing V
∗
First , we show that the optimal V
∗ in the optimization problem ( 9 ) can be expressed in terms of Θ and W .
Proposition 1 . For the fixed W and Θ , the optimal V that solves the optimization problem ( 9 ) is
∗
∗
( 10 ) Proof . Setting the derivative of the optimization prob
= ΘW .
V lem ( 9 ) wrt V to zeros , we have :
V ) = 0 or ΘΘ
V = ΘW .
Θ(W − Θ
Since ΘΘ
= I , this completes the proof .
322 Computing W
∗
Second , we show that the optimal W in the optimization problem ( 9 ) has a closed form solution in terms of Θ and V .
∗
Based on the parametric form ( 7 ) of the decision function fl , we introduce the following regularizer :
Ω(fl ) = ul2
=
‚‚‚wl − Θ vl
‚‚‚2
, which controls the complexity of each classifier independently . The second term in ( 6 ) can be rewritten as : mX
Ω(fl ) = U2
=
‚‚‚W − Θ
V
‚‚‚2
. l=1
313 Distribution Gap between Domains
Recall that the last term in ( 6 ) measures the mismatch between the embedding of the source and target domains . Here , we use the MMD criterion in ( 4 ) as the nonparametn2 ] ∈ . ric measure for the mismatch . Suppose XT = [ x Rd×n2 and X = [ XS , XT ] ∈ Rd×(n1+n2 ) , are the data matrices defined on the target domain and all input data , reΘX . spectively , and assume φ(x ) = Θx , and so K = X Then , the criterion ( 4 ) becomes
1,··· , x .
Θ
2 MMD
[ DS , DT ] = trace(X
Θ
= trace(ΘXDX
( 8 )
ΘXD )
Θ
) .
Proposition 2 . For a fixed Θ and V , the optimal W
∗ has a closed form solution :
−1
S )
V ) .
W = ( αI+XS X
( 11 ) Proof . As shown in the optimization problem ( 9 ) , the last term does not depend on W , so we can simplify the objective function as follows :
( XSY +αΘ
‚‚‚W
‚‚‚2 ‚‚‚W−Θ XS −Y
+α XS−Y XS−Y
)(W Y−2W
S +αV X ( Y
‚‚‚2 V +αtrace(W−Θ
)
Θ )
+W
= trace(W
= trace(Y
V)(W−Θ
S )W )
( αI+XSX
V )
( 12 )
Setting the derivatives of ( 12 ) wrt W to zeros , we have :
−(Y
S +αV
X
Θ )
+ ( αI+XSX
S )W = 0 .
This completes the proof .
−1 can be preSince the matrix inversion ( αI + XS X computed , and the data matrix XS is usually sparse for text documents , this inversion can be computed by performing Singular Value Decomposition ( SVD ) on the data matrix XS in O(dn1 min(d , n1 ) ) time . Using ( 11 ) and ( 10 ) , the update of W can be computed in O(d2m ) time .
S )
182 323 Computing Θ
∗
∗
Moreover , we can show that the optimal Θ in ( 9 ) can be solved efficiently by performing SVD on a matrix in term of W .
Proposition 3 . For a fixed W and V , the optimal Θ can be obtained by solving the following SVD problem :
∗ min
Θ(βXDX
−αWW T )Θ
= I ,
( 13 )
Θ st ΘΘ ∗ and the matrix Θ has the rank at most min(d , m + 1 ) .
Proof . As shown in the optimization problem ( 9 ) , the first term does not depend on Θ , and using ( 10 ) , we can rewrite the objective function as follows :
‚‚‚W−Θ
α
ΘW
‚‚‚2
“
”
Θ
.
+βtrace
ΘXDX
Moreover , using ΘΘ W −W
αtrace
W
“
= I , the objective is simplified as :
”
“
”
Θ
ΘW
+βtrace
ΘXDX
Θ
,
Note that D in ( 5 ) can be decomposed as D = ee so that we can arrive at the optimization problem ( 13 ) . , where e ∈ Rn1+n2 is a vector with the first n1 entries equal 1/n1 and the remaining entries equal −1/n2 , and so XDX is of rank one . Moreover , the matrix W W has rank at most min(d , m ) . Thus , the matrix βXDX has rank at most min(d , m + 1 ) .
−αWW
Combining all of the above , the optimization problem ( 9 ) can be solved by updating the matrices W , Θ , and V iteratively until convergence . The detailed algorithm of solving the optimization problem ( 9 ) is summarized in Algorithm 1 . Moreover , based on the Proposition 3 , one can perform SVD on the low rank matrix βXDX to obtain the efficiently . Assuming that the input dimension is optimal Θ very high , ie d m , the time complexity is O(d2m ) only . The update of V takes O(dm2 ) time . Therefore , the overall time complexity of Algorithm 1 is only O(d2m ) assuming the inverse of the matrix ( αI + XS X
S ) is pre computed .
−αWW
∗ i)}n2 . i=1 in DT , regularization parameters α and β .
Algorithm 1 : The Algorithm of Our Proposed Domain Adaptation Input : labeled patterns {(xi , yi)}n1 terns {(x Output : The optimal projection matrix Θ for feature subspace , the matrix V of weight vectors in the embedded space , and the matrix W of weight vectors of m decision classifiers . Initialize Θ← I , V ← 0 . repeat i=1 in DS , unlabeled pat
1 Update W using ( 11 ) .
2 Compute Θ by solving SVD in ( 13 ) .
3 Set V = ΘW . until convergence
3.3 Prediction
After extracting the shared subspace Θ , and the weight vectors wl and vl for each class , one can perform prediction using ( 7 ) . However , the weight vector wl is learned to minimize the empirical loss of the labeled data in the source domain DS , and may not be the discriminative direction for the testing data in the target domain DT .
Recall that the subspace Θ is learned to minimize the MMD criterion in ( 8 ) , and capture the intrinsic structure of data for domain adaptation . Moreover , the weight vector vl is the discriminative direction defined on the projected subspace Φ , so the prediction on the testing data in the target domain DT can be performed by a decision classifier fT l(x vl is the discriminative direction for the l th class in the target domain . 3.4 Discriminative Feature Propagation
) in ( 7 ) , and Θ instead of fl(x l Θx
) = v
.
.
.
One major problem in text mining is the sparsity of features in the high dimensional space . Specifically , some discriminative features occur frequently in the target domain DT but seldom appear or even are absent in the source domain DS . For example , for the task of extracting sentences corresponding to job requirements from job Web sites , some common terms may be “ qualified ” , “ year ” , “ experience ” and so on . However , some characteristic words are dependent of the job nature . For instance , “ CPA ” , “ CA ” , “ ACCA ” are discriminative terms for the domain of accounting whereas “ CCP ” , “ physiology ” are discriminative terms for the domain of health care . To address this issue , we develop the following feature propagation strategy . According to the discussion in Section 3.2 , we can extract a common feature set F from both domains for each specific task l by selecting the features with high weight in . Θ vl . Based on the co occurrence information in the target domain , we can compute the similarity between the common features in the set F and the remaining features ( noncommon features ) in another set ¯F . For each non common feature , we can sum up its similarity with all the common features . Finally we rank all the non common features by its similarity with the common feature set in descending order . By selecting the top K high similarity non common terms , and combining with all the existing common features , we can get a set of characteristic features Fc ⊂ F ∪ ¯F for the target domain .
Based on the assumption that similar features should have similar prediction power in the target domain , we can construct a feature similarity graph G . In G , each vertex v represents a feature , and edge weights are given by a symmetric matrix E ∈ Rd×d , whose entries Euv = πu , πvff ≥ 0 , where ·,·ff means that the inner product , πi represents the vector of normalized occurrence in the target domain . Deu∼v Euv , then we can fine the degree of vertex v as dv = define the normalized graph Laplacian matrix : if u = v and du ff= 0
P
Luv =
1 − Euv/du √ −Euv/ dudv if u and v are adjacent otherwise .
( 14 )
8>< > :
0
We also define a column vector ρ = [ ρ1 , . . . , ρd ]
∈ Rd representing the discriminative weight vector of characteristic features . Intuitively , similar features should have similar weights . Therefore , we introduce a manifold regularizer using the feature graph Laplacian matrix in ( 14 ) as :
ρT Lρ =
„
X
Euv u,v
ρu√ du
− ρv√ dv
«2
, which propagates the weight of the common features to other characteristic features via the manifold structure of the feature graph .
183 DT is bounded by
T ( h , gT ( x Proof . T ( h , gT ( x
. ) )
( 16 )
Moreover , we also require the discriminative weight vector ρ be close to the discriminative direction learned for each class in the target domain . Thus , we arrive at the following optimization problem :
‚‚ρl − Θ vl
‚‚2 min
+ γρT l
Llρl ,
( 15 ) where the first term minimizes the difference between ρl and vl , and the second term enforces that the assignment of Θ the weight of the characteristic features is propagated from the common features . In addition , the optimization problem ( 15 ) can be solved according to the following lemma :
Lemma 1 . Let vl be the classifier for the class l on the shared feature subspace Θ , therefore the corresponding optimal ρl has a closed form in term of Θ and vl .
Proof . We first rewrite the objective function as fol lows :
‚‚2
‚‚ρl − Θ Llρl vl + γρ Llρl l ρl − 2v l l vl + γρ l Θρl + v l ( I + γLl)ρl − 2v l l Θρl + v l vl
= ρ = ρ
Setting the derivation of ( 16 ) with respect to ρl to zeros , we have :
ρl = ( I + γLl )
−1Θ vl .
This completes the proof .
Therefore , the prediction on the testing patterns in the target domain can be performed by :
. fT l(x
) = v l Θ(I + γLl )
−1x
.
.
However , computing the matrix inversion ( I + γLl )
−1 is still computational intensive ( with complexity O(d3) ) . Note that when the predefined parameter γ satisfies 0 < γ < 1 , we have the following Taylor expansion : − γ
= I − γLl + γ
As Ll is usually very sparse , especially when γ is small , one can approximate ( I + γLl ) −1 as I − γLl and the revised discriminative direction is :
( I + γLl ) l +
2L2
3L3
−1 l
ρl = Θ vl − γLlΘ vl ,
Then the decision function on the testing patterns in the target domain becomes : .
. fT l(x
) = v l Θ(I − γLl)x
,
As a result , the computation of the prediction is much reduced .
As discussed above , Θ vl is the optimal discriminative direction of the l th class in ( 9 ) . From the propagation of the feature graph G , the discriminative information from other characteristic features Fc can be used to compute the weight vector −γLlΘ vl to correct the discriminative direction . 3.5 Error Analysis on Domain Adaptation
In this section , we study the error analysis of our proposed domain adaptation method in the target domain . First , we denote the labeling function in DT as follows : vT Θx if 0 ≤ vT Θx ≤ 1 , 1 0 if 1 < vT Θx , if vT Θx < 0 ,
8>< > : gT ( x ) = and h(x ) : X → {0 , 1} is the truth labeling function . Let σ(x ) be a continuous loss function defined as :
σ(x)=|h(x)−gT ( x)| .
( 17 )
The expected loss of gT in DT is defined as : T ( h , gT ( x
) ) = Ex.∼DT [ |h(x
)| ] = Ex.∼DT [ σ(x
.
.
.
.
) ] .
) − gT ( x x + v
Note that fS ( x ) = u Θx is the proposed decision function in ( 7 ) for the labeled data in the source domain , then we also define the expected loss of fS in DS as : Θx| ] .
S ( h , fS ( x ) ) = Ex∼DS [ |h(x ) − u x − v
For simplicity , we denote Ex∼DS = EP and Ex.∼DT = EQ . Based on the definition of σ(x ) in ( 17 ) , we know that 0 ≤ σ(x ) ≤ 1 . With a mild assumption that ffiσffiH is bounded by a finite number C , where H is a RKHS , we obtain the following theorem :
Theorem 1 . Suppose ffixffi = 1 , the expected loss of gT in
.
) ) ≤ S ( h , fS ( x ) ) + MMD[DS , DT ]C + u
( 18 )
))− S ( h , fS ( x ) ) . ))−EP [ |h(x)−u Θx| ] x−v
. Θx|]+ EP [ |u ))−EP [ |h(x)−v
.
= S ( h , fS ( x))+ T ( h , gT ( x = S ( h , fS ( x))+ T ( h , gT ( x ≤ S ( h , fS ( x))+ T ( h , gT ( x ≤ S ( h,fS ( x))+EQ[|h(x x| ]
)|]−EP [ |h(x)−gT(x)|]+EP [ |u x| ] .
. ( 19 ) The second last inequality holds because of the triangle inequality
)−gT(x .
|h(x ) − v
Θx| ≤ |h(x ) − u x − v
Θx| + |u x| , and the last inequality holds due to
|h(x ) − gT ( x)| ≤ |h(x ) − v
Θx| .
Moreover , using the Cauchy Schwarz inequality , we have :
EP [ |u x| ] ≤ EP [ ux ] = uEP [ x ]
Since ffixffi = 1 , so that
EP ( |u x| ) ≤ u
( 20 )
By the virtual of RKHS property , for any function σ(x ) in this RKHS , it can be expressed as σ(x ) = σ , φ(x)ffH . Then , we can obtain the following bound :
)|]−EP [ |h(x)−gT ( x)| ]
.
EQ[|h(x )−gT ( x . )]−EP [ σ(x ) ] . = EQ[σ(x = EQ[(φ(x ) , σ)H]−EP [ (φ(x ) , σ)H ] . = ( EQ[φ(x ) ] − EP [ φ(x) ] , σ)H . . ) ] − EP [ φ(x) ] , σ)H ≤ ‚‚EQ[φ(x
Assume ffiσffiH ≤ C , similar to ( 1 ) , we have : ( EQ[φ(x
.
.
) ] − EP [ φ(x ) ]
= MMD[DS , DT ]C .
‚‚
H C ( 21 )
Substitute ( 20 ) and ( 21 ) into ( 19 ) . This completes the proof .
Based on the expected error bound in ( 18 ) , we can conclude that minimizing the MMD in ( 8 ) , the empirical loss of labeled data in the source domain DS , and the regularizer ffiuffi simultaneously as in ( 9 ) can also minimize the expected loss in the target domain DT .
4 . EXPERIMENTS
We demonstrate the effectiveness of our proposed domain adaptation method by conducting experiments on various data sets covering two common text mining problems : document classification and information extraction .
184 4.1 Document Classification
411 Experiment Setup
We use the 20 Newsgroup corpus to conduct experiments on document classification . This corpus consists of 18,846 newsgroup articles harvested from 20 different Usenet newsgroups . It can be observed that the marginal distributions of the articles among different newsgroups are not identical . There exists distribution shift from one newsgroup to any other newsgroups . However , we observe that some newsgroups are related . For example , the newsgroups rec.autos and rec.motorcycles are related to car . The newsgroups compsysmachardware and compsysibmpchardware are related to hardware , etc . Table 1 depicts the detailed information of the data sets , derived from 20 Newsgroup , used in our experiments . There are four class labels , namely , car , ball game , hardware , and OS . For each class label , there are two related newsgroups , and we can select the articles in one newsgroup as labeled data in the source domain and the articles in the other newsgroup as unlabeled data in the target domain . The data sets NG1 2class , NG2 2class , and NG32class have only two class labels . For example , the NG12class data set has the class labels car and ball game . The source domain contains 400 random articles selected from the newsgroup rec.auto and rec.baseball for the class label car and ball game respectively . There are 800 articles in total for the source domain . The target domain contains 400 random articles selected from the newsgroup rec.motorcycle and rec.hockey for the corresponding class label car and ball game respectively . There are also 800 articles in the target domain . The datasets NG4 4class and NG5 4class both have 4 class labels , namely , car , ball game , hardware , and OS . The composition of articles in each label in the source and target domains is clearly shown in Table 1 . Each article is represented by the vector space model and normalized to unit length .
In order to verify the effectiveness of our method , we compare with three typical classification methods : SVM , Transductive SVM , and CDSC as presented in [ 10 ] . They represent supervised classification , semi supervised classification , and a recent domain adaptation method respectively . SVM and TSVM [ 9 ] are implemented by1 SVMlight and the parameters are all set as default in the package . The parameters setting in CDSC is the same as those reported in the paper . For those three comparison algorithms , since they can only handle binary classification , we transform the multiclass problems to the 1 VS rest problem setting for training . For each data set , we repeated all the algorithms 10 times by randomly sampling the articles in each run and calculate the average performance , so as to decrease the sampling bias .
412 Results and Discussion
We adopt the recall , precision , and F1 measure as the evaluation metrics . Recall is defined as the number of articles that are correctly classified , divided by the actual number of articles in each class . Precision is defined as the number of articles that are correctly classified , divided by the number of all the articles predicted as the same class . F1measure is defined as the harmonic mean of recall and precision . Results of all the methods on all data sets depicted in Table 1 are summarized in Table 2 with the best results
1http://svmlightjoachimsorg shown in bold font . It can be observed that the supervised method , namely , SVM , which trains only in the source domain and tests in the target domain always gets the worst performance among the four algorithms . Semi supervised learning method TSVM outperforms the supervised learning method SVM by taking advantages of the unlabeled data in the target domain . Because the articles in the source domain and target domain are related , then the unlabeled data in target domain will supply some distribution information for the training so as to improve the prediction in the target domain . CDSC has been reported for the good performance in two class cross domain adaptation . Those results are verified again in our experiments especially when the two classes in the target domain are well separated such as the data set NG3 2class . However , for multiclass problems especially when the multiple classes in the target domain are not very easy to separate such as the data set NG4 4class and NG5 4class , the performance of CDSC is not as good as that in two class problems . On the other hand , our domain adaptation method can get comparable results with CDSC for the well separated two class problems and achieve better performance for all the other data sets . 4.2 Information Extraction
421 Experiment Setup
We conducted a set of experiments in the area of information extraction . The objective of information extraction is to extract precise text fragments , which are basically chunks of consecutive tokens , for each field of interest from a semistructured text document . In our experiments , we aim at extracting the job related information from Web pages in some recruitment Web sites . The fields of interest are job title , company , location , salary , post date , education , experience , and duty . The online job advertisement documents were collected from different recruitment Web sites in 3 different domains ( or industries ) . Table 3 depicts the details of the collected data . The first , second , and third columns refer to the domain label , domain name , and the number of job advertisements collected in the domain respectively . For each online job advertisement collected , we automatically segment the document into a number of text fragments by considering the document object model ( DOM)2 and extract the text contained in the text nodes of the DOM structure . Long paragraphs contained in text nodes are further segmented into sentences by an automatic sentence segmentator for finer granularity . The fourth column of the table shows the number of text fragments in the domain after segmentation . Each text fragment should be labeled as one of the eight job fields mentioned above , or the “ not a field ” label . Two human accessors were invited to manually label all the text fragments in the three domains . If there was any disagreement on the judgment between the two accessors , it was resolved by a discussion among them . The manual label information is used as the ground truth in the experiments . In each domain , we have conducted different sets of experiments to demonstrate the performance and compare with existing methods . The first set of experiment is to use the labeled training example in the source domain and the unlabeled data in the target domain to learn the extraction model using our domain adaptation method . The learned 2The details of the document object model can be found in http://wwww3org/DOM
185 Data set
Domain
NG1 2class
NG2 2class
NG3 2class
NG4 4class
NG5 4class source target source target source target source target source target car rec.auto rec.motorcycle ball game rec.baseball rec.hockey
N/A N/A rec.auto rec.motorcycle rec.auto rec.motorcycle rec.motorcycle rec.auto
N/A N/A N/A N/A rec.baseball rec.hockey rec.hockey rec.baseball class label hardware
N/A N/A
OS N/A N/A compsysibmpchardware compwindowsx compsysmachardware composms windowsmisc compsysibmpchardware compsysmachardware compsysibmpchardware
N/A N/A compwindowsx compsysmachardware compsysmachardware composms windowsmisc composms windowsmisc compsysibmpchardware compwindowsx
.# doc .
800 800 800 800 800 800 1600 1600 1600 1600
Table 1 : The details of the data collected for the document classification experiments .
Data set class label
NG1 2class
NG2 2class
NG3 2class
NG4 4class
NG5 4class car ball game average hardware
OS average car hardware average car ball game hardware
OS average car ball game hardware
OS average
Average
P 0.788 0.949 0.869 0.652 0.707 0.680 0.876 0.874 0.880 0.710 0.818 0.637 0.623 0.696 0.743 0.832 0.552 0.507 0.658 0.757
SVM R 0.960 0.744 0.852 0.760 0.589 0.674 0.884 0.885 0.880 0.845 0.899 0.630 0.441 0.704 0.435 0.819 0.715 0.574 0.636 0.749
F1 0.867 0.833 0.850 0.702 0.643 0.672 0.880 0.879 0.880 0.771 0.854 0.634 0.517 0.694 0.549 0.825 0.623 0.538 0.634 0.746
P 0.812 0.957 0.884 0.743 0.762 0.753 0.934 0.916 0.925 0.803 0.873 0.669 0.666 0.753 0.745 0.856 0.550 0.579 0.683 0.800
TSVM R 0.966 0.778 0.869 0.641 0.782 0.713 0.912 0.937 0.925 0.854 0.905 0.633 0.633 0.756 0.628 0.760 0.697 0.577 0.665 0.786
F1 0.884 0.863 0.774 0.693 0.772 0.732 0.923 0.927 0.925 0.828 0.889 0.650 0.649 0.754 0.682 0.805 0.615 0.578 0.670 0.791
P 0.841 0.982 0.912 0.767 0.799 0.783 0.984 0.916 0.950 0.730 0.955 0.633 0.815 0.783 0.862 0.913 0.577 0.623 0.743 0.834
CDSC R 0.985 0.815 0.900 0.810 0.755 0.783 0.910 0.985 0.948 0.890 0.955 0.700 0.550 0.774 0.750 0.835 0.820 0.495 0.725 0.826
Our approach
P F1 0.912 0.907 0.984 0.891 0.948 0.899 0.855 0.788 0.840 0.776 0.847 0.782 0.984 0.945 0.949 0.914 0.947 0.949 0.773 0.802 0.955 0.920 0.819 0.665 0.796 0.657 0.827 0.770 0.750 0.802 0.891 0.872 0.678 0.763 0.734 0.552 0.785 0.726 0.825 0.871
R 0.985 0.905 0.945 0.835 0.859 0.847 0.907 0.985 0.946 0.903 0.917 0.792 0.692 0.826 0.916 0.843 0.779 0.596 0.783 0.870
F1 0.947 0.943 0.945 0.845 0.849 0.847 0.944 0.948 0.946 0.833 0.918 0.805 0.741 0.824 0.825 0.866 0.771 0.658 0.780 0.869
Table 2 : The classification performance of different sets of experiments . P , R , and F1 refer to the precision , recall , and F1 measure respectively .
Domain Domain Label D1 D2 D3
Name Accounting Logistic Health
# of Job
# of Text Advertisements Fragments
273 202 201
7462 5636 6402
Table 3 : The details of the data collected for the information extraction experiments . model is then applied to the testing data in the target domain and the performance is measured . For example , let D1 and D2 be the source and target domains respectively . We use the labeled training fragments in D1 and the unlabeled fragments in D2 to learn a model . Then the learned model is applied to predict the fields of the text fragments in the testing data . The other sets of experiments are designed in a similar manner as the first set . In the second set of experiments , we use transductive support vector machine for model training . As can be seen , in each training , the total number of text fragments in the source domain and target domain is larger than 10,000 . Since CDSC needs to compute and store the pairwise similarity for any two fragments , it cannot handle this information extraction data set . We cannot compare with it due to its enormous memory requirement . Note that each text fragment is represented by the vector space model and normalized to unit length .
422 Results and Discussion
We adopt the recall , precision , and F1 measure as the evaluation metrics . Recall is defined as the number of text fragments that are correctly labeled by our framework , divided by the actual number of text fragments . Precision is defined as the number of text fragments that are correctly labeled by our framework , divided by the number of predicted text fragments using our framework . F1 measure is defined as the harmonic mean of recall and precision .
In each set of experiments , we have conducted 6 runs using different combination of the source and target domains . Table 4 depicts the performance of the experiments . In each run , we measure the recall , precision , and F1 measure for each field . The figure in each cell of Table 4 is the average performance among the 8 fields of interest in the corresponding experiment . For example , our approach achieves an average precision , recall , and F1 measure of 0.814 , 0.845 , and 0.825 respectively in the target domain when the source and target domains are D1 and D2 respectively . Our approach achieves an average precision , recall , and F1 measure of 0.820 , 0.802 , and 0799 It outperforms TSVM which obtains a F1 measure of 0744
Figure 1 shows the detailed comparison between our method and TSVM . The x axis denotes the eight job fields and the y axis denotes the extraction performance measured by F1 measure . In each plot , we show the F1 measure on each job field when training is conducted in one domain and adapting to the other two domains . For example , “ TSVMD1 D2 ” and “ Our D1 D2 ” represent the result of TSVM and our method respectively on the data set in which D1 is the source domain and D2 is the target domain . It can be observed that our domain adaptation method can get better
186 1
0.9
0.8
0.7
0.6
0.5 e r u s a e m − F
0.4 comp loca title sala time
Field
Our−D1−D2 TSVM−D1−D2 Our−D1−D3 TSVM−D1−D3
1
0.9
0.8
0.7
0.6
0.5 e r u s a e m − F
Our−D2−D1 TSVM−D2−D1 Our−D2−D3 TSVM−D2−D3 educ expe duty
0.4 comp loca title sala time
Field
1
0.9
0.8
0.7
0.6
0.5
0.4 e r u s a e m − F educ expe duty comp loca title
Our−D3−D1 TSVM−D3−D1 Our−D3−D2 TSVM−D3−D2 educ expe duty sala time
Field
Figure 1 : Comparison of the extraction performance of each job field with different source domain and target domain . From left to right : the source domain is D1 , D2 , and D3 respectively . The fields in the x axis from left to right are company , location , job title , salary , post date , education , experience , and duty .
Experiment Setting
Source Domain
Target Domain
D1 D1 D2 D2 D3 D3
D2 D3 D1 D3 D1 D2
Average
TSVM
Our Approach
P 0.730 0.717 0.782 0.782 0.742 0.727 0.743
R 0.815 0.771 0.772 0.796 0.739 0.784 0.775
P F1 0.814 0.759 0.813 0.731 0.766 0.866 0.770 0.830 0.790 0.731 0.793 0.737 0.744 0.820
R 0.845 0.804 0.789 0.762 0.789 0.791 0.800
F1 0.825 0.800 0.807 0.765 0.779 0.786 0.799
Table 4 : The extraction performance of different sets of experiments . P , R , and F1 refer to the precision , recall , and F1 measure respectively . extraction performance than TSVM in almost all of the fields in each data set .
5 . CONCLUSIONS
In this paper , we present a domain adaptation method by extracting the shared concept space between the source domain with sufficient labeled data and the target domain with only unlabeled data . In our method , we parameterize the shared space by a linear transformation and finding the optimal solution by considering the combination of two criteria : the empirical loss on the source domain , and the embedded distribution gap between the source domain and the target domain . Theoretical analysis of the adaptation error bound in the target domain shows that it can be well controlled by the criteria in our objective function . Experimental results on document classification and information extraction demonstrate that our method can outperform other competitive methods in the domain adaptation setting .
In the future , we will extend our method to extract discriminative concepts in multiple source domain adaptation problems . Exploration of other domain knowledge for extracting more discriminative concepts is also one of major directions to our domain adaptation method .
6 . REFERENCES [ 1 ] M . Belkin , P . Niyogi , and V . Sindhwani . Manifold regularization : A geometric framework for learning from labeled and unlabeled examples . Journal of Machine Learning Research , 12:2399–2434 , 2006 .
[ 2 ] S . Bickel , C . Sawade , and T . Scheffer . Transfer learning by distribution matching for targeted advertising . In Advances in Neural Information Processing Systems 21 , pages 145–152 , 2009 .
[ 3 ] J . Blitzer , R . McDonald , and F . Pereira . Domain adaptation with structural correspondence learning . In Proceedings of the Conference on Empirical Methods in Natural Language Processing , pages 120–128 , 2006 .
[ 4 ] H . Daum´e III . Frustratingly easy domain adaptation . In
Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics , pages 256–263 , June 2007 .
[ 5 ] A . Gretton , K . Borgwardt , M . Rasch , B . Sch¨olkolpf , and
A . Smola . A kernel method for the two sample problem . In Advances in Neural Information Processing Systems 19 , pages 513–520 , 2007 .
[ 6 ] J . Huang , A . Smola , A . Gretton , K . M . Borgwardt , and
B . Sch¨olkopf . Correcting sample selection bias by unlabeled data . In Advances in Neural Information Processing Systems 19 , pages 601–608 , 2007 .
[ 7 ] S . Ji , L . Tang , S . Yu , and J . Ye . Extracting shared subspace for multi label classification . In Proceedings of the 14th ACM SIGKDD International Conference On Knowledge Discovery and Data Mining , pages 381–389 , 2008 .
[ 8 ] J . Jiang and C . Zhai . Instance weighting for domain adaptation in NLP . In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics , pages 264–271 , 2007 .
[ 9 ] T . Joachims . Learning to Classify Text Using Support Vector
Machines – Methods , Theory , and Algorithms . Kluwer/Springer , 2002 .
[ 10 ] X . Ling , W . Dai , G R Xue , Q . Yang , and Y . Yu . Spectral domain transfer learning . In Proceedings of the 14th ACM SIGKDD International Conference On Knowledge Discovery and Data Mining , pages 488–496 , 2008 .
[ 11 ] S . J . Pan , J . T . Kwok , and Q . Yang . Transfer learning via dimensionality reduction . In Proceedings of the 23rd AAAI conference on Artificial Intelligence , pages 677–682 , 2008 .
[ 12 ] R . Raina , A . Battle , H . Lee , B . Packer , and A . Y . Ng .
Self taught learning : transfer learning from unlabeled data . In Proceedings of the 24th Annual International Conference on Machine Learning , pages 759–766 , 2007 .
[ 13 ] S . Satpal and S . Sarawagi . Domain adaptation of conditional probability models via feature subsetting . In Proceedings of European Conference on Principles and Practice of Knowledge Discovery in Databases , pages 224–235 , 2007 .
[ 14 ] A . Storkey and M . Sugiyama . Mixture regression for covariate shift . In Advances in Neural Information Processing Systems 19 , pages 1337–1344 , 2007 .
[ 15 ] M . Sugiyama , S . Nakajima , H . Kashima , P . von Bunau , and
M . Kawanabe . Direct importance estimation with model selection and its application to covariate shift adaptation . In Advances in Neural Information Processing Systems 20 , pages 1433–1440 , 2008 .
[ 16 ] J . Yang , R . Yan , and A . G . Hauptmann . Cross domain video concept detection using adaptive svms . In ACM Multimedia , pages 188–197 , 2007 .
187
