Primal Sparse Max Margin Markov Networks jun zhu@mailsthueducn ; epxing@cscmuedu ; dcszb@tsinghuaeducn
Bo Zhang(cid:63 )
‡School of Computer Science Carnegie Mellon University
Pittsburgh , PA 15213
Jun Zhu(cid:63)‡
Eric P . Xing‡
( cid:63)Dept . Comp . Sci . & Tech .
TNList Lab , Tsinghua University
Beijing , 100084 China
ABSTRACT Max margin Markov networks ( M3N ) have shown great promise in structured prediction and relational learning . Due to the KKT conditions , the M3N enjoys dual sparsity . However , the existing M3N formulation does not enjoy primal sparsity , which is a desirable property for selecting significant features and reducing the risk of over fitting . In this paper , we present an 1 norm regularized max margin Markov network ( 1 M3N ) , which enjoys dual and primal sparsity simultaneously . To learn an 1 M3N , we present three methods including projected sub gradient , cutting plane , and a novel EM style algorithm , which is based on an equivalence between 1 M3N and an adaptive M3N . We perform extensive empirical studies on both synthetic and real data sets . Our experimental results show that : ( 1 ) 1 M3N can effectively select significant features ; ( 2 ) 1 M3N can perform as well as the pseudo primal sparse Laplace M3N in prediction accuracy , while consistently outperforms other competing methods that enjoy either primal or dual sparsity ; and ( 3 ) the EM algorithm is more robust than the other two in prediction accuracy and time efficiency .
Categories and Subject Descriptors I51 [ Pattern Recognition ] : Models Statistical General Terms Algorithms , Experimentation Keywords 1 norm Max margin Markov networks , Primal sparsity , Dual sparsity
1 .
INTRODUCTION
In recent years , discriminative learning has expanded its scope to model structured data based on composite features that explicitly exploit the structural dependencies among elements in high dimensional inputs ( eg , text sequences , image lattices ) and structured interpretational outputs ( eg , part of speech tagging , image segmentation ) . Three major approaches have been successfully explored to perform discriminative learning by using graphical models to capture sequential , spatial or relational structure : ( 1 ) maximum conditional likelihood [ 16 ] , ( 2 ) max margin [ 20 ] , and ( 3 ) maximum entropy discrimination [ 27 ] .
Discriminative models , such as conditional random fields ( CRFs ) [ 16 ] and max margin Markov networks ( M3N ) [ 20 ] , usually have a complex and high dimensional feature space , because in principle they can use arbitrary and overlapping features of inputs . Thus it is desirable to pursue a sparse representation of such models that leaves out irrelevant features . We say a model enjoys the primal sparsity if only a few input features in the original model have non zero weights ( see Section 3.1 for precise definitions ) . Primal sparsity is an important property for selecting significant features and reducing the risk of over fitting [ 12 ] . Feature selection is also useful to interpret complex data and to reduce memory cost and running time . To achieve primal sparsity , in likelihoodbased estimation , a commonly used strategy is to add an 1penalty to the likelihood function , which can be viewed as a maximum a posteriori ( MAP ) estimation under a Laplace prior . The sparsity of the 1 norm regularized maximum ( conditional ) likelihood estimation is due to a hard threshold introduced by the Laplace prior , and weights less than the threshold will be set to exact zeros [ 14 ] .
Another type of sparsity is the dual sparsity as enjoyed by large margin models , like the unstructured ( ie , the output is univariate ) SVM and the structured M3N [ 20 ] or structural SVM [ 23 , 13 ] . Dual sparsity refers to a phenomenon that only a few lagrange multipliers in the dual form of the original model are non zero . A dual sparse model has a robust decision boundary which depends only on a few support vectors . The dual sparsity also provides a theoretical motivation of the cutting plane algorithms [ 23 , 13 ] and the bundle methods [ 21 ] , which generally explore the fact that in max margin Markov models only a few ( eg , polynomial ) number of constraints are necessary to achieve a sufficiently accurate solution . Although both primal and dual sparsity can benefit structured prediction models , unfortunately , they do not co exist in a single existing structured prediction model . For example , the M3N is only dual sparse , while the 1 norm regularized CRF [ 2 ] is only primal sparse .
In this paper , we introduce the 1 norm regularized maxmargin Markov networks ( 1 M3N ) , which enjoy the primal and dual sparsity simultaneously . The 1 M3N is a generalization of the unstructured 1 norm SVM [ 4 , 26 ] to the much broader structured prediction . The primal sparsity of the 1 M3N makes it able to select significant features . To learn an 1 M3N , we present three methods : ( 1 ) projected sub gradient ; ( 2 ) cutting plane ; and ( 3 ) a rather novel EMstyle algorithm based on an equivalence between 1 M3N and a novel adaptive M3N . The EM algorithm is similar to the variational learning method of the Laplace max margin Markov network ( LapM3N ) [ 27 ] but essentially differs in updating scaling coefficients . Because of a smooth shrinkage effect , LapM3N is pseudo primal sparse ( ie , only a few input features have large weights ) and does not select significant features . Finally , we perform extensive studies on both synthetic and real data sets . Results show that 1 M3N can effectively select significant features and can perform as well as the closely related pseudo primal sparse LapM3N , while consistently outperforms competing models that enjoy either dual or primal sparsity ; and the EM algorithm is more robust than the other two methods .
This paper is structured as follows . The next section presents the preliminaries of max margin Markov networks . Section 3 presents the primal sparse 1 M3N . Section 4 presents the three learning algorithms for the 1 M3N . Section 5 presents our empirical studies , and Section 6 concludes this paper .
2 . PRELIMINARIES In structured prediction , our goal is to learn a predictive function h : X → Y from a structured input x ∈ X to a structured output y ∈ Y , where Y = Y1×···×Yl represents a space of multivariate and structured outputs . For example , in part of speech ( POS ) tagging , each input x is a word sequence , Yi consists of all the POS tags and each output ( label ) y = ( y1,··· , yl ) is a sequence of POS tags . We assume a finite feasible set of labels Y(x ) for any x .
Let F ( x , y ; w ) be a parametric discriminant function . In this paper , we concern ourselves with the special case of a linear model , where F is defined by a set of K feature functions fk : X×Y → R and their weights wk : F ( x , y ; w ) = wf ( x , y ) . The generalization to the non linear case can be done similarly as the feature selection in an SVM [ 11 ] .
By using different loss functions , the parameters w can be estimated by maximizing the conditional likelihood [ 16 ] or by maximizing the margin [ 1 , 20 , 23 ] . We focus on the max margin models and will provide empirical comparison with likelihood based methods . 2.1 Max Margin Markov networks
The max margin Markov networks ( M3N ) [ 20 ] approach the structured prediction problem by defining a predictive rule as an optimization problem : h0(x ; w ) = arg max y∈Y(x )
F ( x , y ; w ) ,
( 1 ) where the model parameter w is estimated by solving a constrained optimization problem , with a set of fully labeled training data D = {(xi , yi)}N i=1 :
P0 ( M3N ) : min w,ξ w2
2 + C
1 2 st ∀i , ∀y ∈ Y(xi ) : w∆fi(y ) ≥ ∆i(y ) − ξi , ξi ≥ 0 ,
N i=1
ξi
N i=1 l j=1 I(yj = yi j ) , where I(· ) is an indicator function that equals to one if the argument is true and zero otherwise .
The problem P0 can be efficiently solved or approximately solved with a cutting plane [ 23 ] , message passing [ 20 ] , or gradient decent [ 3 , 19 ] method , which generally explores the sparse dependencies among individual labels in y , as reflected in the specific design of the feature functions ( eg , based on pair wise labeling potentials ) . As described shortly , these algorithms can be directly employed as subroutines in solving our proposed model . The prediction problem ( 1 ) can be efficiently solved too by exploring these sparse dependencies in y , eg , using the Viterbi algorithm when the graphical model is a linear chain [ 16 ] .
3 . PRIMAL SPARSE MAX MARGIN MARKOV
NETWORKS
In this section , we introduce a primal sparse max margin Markov network . We begin with a brief overview of three types of sparsity . 3.1 Sparsity
Primal Sparsity : We say a model enjoys the primal sparsity , if only a few features in the original model have non zero weights . The term “ primal ” stems from a convention in the optimization literature , which generally refers to ( constrained ) problems pertaining to the original model . For example , P0 is the primal form of the max margin Markov networks . By primal sparsity , we mean that only a few elements of w are non zero .
As we have stated , primal sparsity is important for selecting significant features and reducing the risk of overfitting . To achieve the primal sparsity , 1 regularization has been extensively studied in different learning paradigms . For likelihood based estimation , recent work includes the structure learning of graphical models [ 17 , 24 ] . For max margin learning , the unstructured 1 norm SVM [ 26 , 29 ] has been proposed . Our work represents a generalization of the 1norm SVM to structured learning , as we shall see .
Dual Sparsity : Dual sparsity refers to a phenomenon that only a few lagrange multipliers in the dual form of the original model turn out to be non zero . Dual sparsity is an intrinsic property as enjoyed by max margin models . For instance , for the max margin Markov networks , the Lagrangian of P0 is as follows :
L(w , ξ , α , v)=
1 2 w2
2 + C
ξi
− i,y∈Y(xi )
αi(y)[w∆fi(y ) − ∆i(y ) + ξi ] − vξ , where αi(y ) , ∀i , ∀y ∈ Y(xi ) are nonnegative lagrange multipliers , one for each of the margin constraints in P0 , and vi are the lagrange multipliers for the constraints ξi ≥ 0 , ∀i . Since P0 is a convex program and satisfies the Slater ’s condition [ 6 ] , the saddle point of the Lagrangian L is the KKT point of P0 . From the stationary condition , we get the optimum solution of P0 : w =
αi(y)∆fi(y ) . i,y∈Y(xi ) where ξi represents a slack variable absorbing errors in training data , C is a positive constant , and ∆fi(y ) = f ( xi , yi ) − f ( xi , y ) . w∆fi(y ) is the “ margin ” favored by the true label yi over a prediction y , and ∆i(y ) is a loss penalized on y compared with yi , eg , hamming loss [ 20 ] : ∆i(y ) =
From the Complementary Slackness condition , we get : ∀i , ∀y ∈ Y(xi ) : αi(y)[w∆fi(y ) − ∆i(y ) + ξi ] = 0 ,
Thus , for those constraints that are inactive , ie , the equality does not hold , the corresponding lagrange multipliers
αi(y ) will be zero , and the optimum solution w does not depend on these inactive constraints .
When a model is dual sparse , its decision boundary depends on a few number of support vectors , which in principle leads to a robust decision boundary . Moreover , as we have stated , the dual sparsity provides a theoretical motivation of the cutting plane [ 23 , 13 ] and bundle [ 21 ] methods .
Pseudo primal Sparsity : Besides the primal and dual sparsity , there is another type of regularization which yields pseudo primal sparse estimates . We say a model is pseudoprimal sparse , if only a few elements of w have large values . In other words , w can have many non zero small elements . Thus , a pseudo primal sparse model does not explicitly discard features by setting their weights to exactly zeros . The recently proposed Laplace max margin Markov network ( LapM3N ) [ 27 ] is pseudo primal sparse because of a smooth shrinkage effect .
Unfortunately , although both the primal and dual sparsity can benefit structured prediction models , they usually do not co exist . For example , the powerful M3N is not primal sparse , because it employs an 2 norm penalty that cannot automatically select significant features . Below , we present a primal sparse max margin Markov network to achieve both primal and dual sparsity in one single model . 3.2 Primal Sparse M3N
To introduce the primal sparsity in max margin Markov networks , we propose to use the 1 norm instead of the 2norm of model parameters . Therefore , we formulate the 1 M3N as follows , which is a generalization of the unstructured 1 norm SVM [ 26 , 4 ] to the structured learning setting :
P1 ( 1 M3N ) : min w,ξ st ∀i , ∀y ∈ Y(xi ) : w where .1 is the 1 norm . w1 + C
1 2 ∆fi(y ) ≥ ∆i(y ) − ξi ; ξi ≥ 0 , i=1
ξi
Like P0 , the problem P1 is a convex program and satisfies the Slater ’s condition . Therefore , due to KKT conditions , the 1 M3N enjoys the dual sparsity . The difference between the 1 M3N and the standard M3N lies in the regularizer they use . Compared to the 2 norm in M3N , which is differentiable everywhere , the 1 norm in the 1 M3N is not differentiable at the origin . This singularity property will ensure that the 1 M3N is able to remove many noise features by estimating their weights to exactly zeros . However , the differentiability of 2 norm makes the M3N have all the input features for prediction . When the feature space is high dimensional and has many noise features , the M3N will suffer a poor generalization ability caused by these noise features . Thus , the 1 M3N would be a better choice when the underlying true model is sparse . More insights about the primal sparsity of the 1 M3N will be presented in the next section , along with the algorithm development . 4 . THREE LEARNING ALGORITHMS
In this section , we introduce three algorithms for learning an 1 M3N and will empirically compare them . 4.1 Cutting plane Method
The first method we propose is a cutting plane method
[ 15 ] , which has been used to learn an M3N [ 23 , 13 ] .
Basically , the cutting plane method aims to find a small sub set of the constraints in the problem P1 to get a suffi
N
Algorithm 1 Cutting plane Method
Input : data D = {(xi , yi)}N Output : w Initialize Si ← ∅,∀1 ≤ i ≤ N . repeat i=1 , constants C and for i = 1 to N do
Compute ˆy = arg maxy∈Y(xi ) Hi(y ; w ) . Compute ξi = max{0 , maxy∈Si Hi(y ; w)} . if Hi(ˆy ; w ) > ξi + then
Si ← Si ∪ {ˆy} . w ← optimize the LP problem over S = ∪iSi . end if end for until no change in S . ciently accurate solution . To construct such a sub set , the algorithm takes an iterative procedure to select informative ( eg , the mostly violated ) constraints under the current model . This procedure will terminate when no constraints are selected or the solution is accurate enough . As we have stated , due to the dual sparsity of the max margin models , only a few constraints are active for the optimum solution . Therefore , the cutting plane method is a valid strategy . For example , for the M3N , as shown in [ 23 ] , a polynomial number of constraints are sufficient to get a good solution . Let Hi(y ; w ) = ∆i(y ) − w∆fi(y ) . The cutting plane algorithm is outlined in Algorithm 1 , where is a precision parameter . Specifically , the algorithm maintains working sets Si for each training data to record the selected constraints . The constraints in the sub set S = ∪N i=1Si defines a relaxation of the problem P1 . The algorithm proceeds to find the most violated constraint , ie , the constraint associated with ˆy , for each data xi . If the margin violation of this constraint exceeds the current ξi by more than , the constraint is added to the working set Si . Once a new constraint has been added , the algorithm re computes the solution by solving an LP ( linear programming ) problem over the working set S . The LP formulation of the problem P1 over the working set S = ∪N Let w = µ − v , where µk , vk ≥ 0 , ∀k . For each component k , we can assume that at least one of µk and vk is zero ; otherwise , we can minus each of them by the smaller one , without changing wk . Therefore , w1 = µ + v , and the LP formulation of the problem P1 is as follows : i=1Si is as follows :
N i=1 st ∀i , ∀y ∈ Si : ∀k : min µ,v,ξ
1 2 ( µ − v )
µk , vk ≥ 0 ,
( µ + v ) + C
ξi
∆fi(y ) ≥ ∆i(y ) − ξi ; ξi ≥ 0 ,
This LP problem can be solved with a standard solver , such as MOSEK . In Alg . 1 , finding ˆy is a loss augmented prediction problem : ˆy = arg maxy∈Y(xi ) wf ( xi , y ) + ∆i(y ) , which can be efficiently done as the prediction problem ( 1 ) . Since the working set S is increased during the iteration , the relaxation of the problem P1 gets tighter and the solution gets more accurate as the algorithm proceeds . 4.2 Projected Sub gradient
The second learning algorithm we present is a projected sub gradient method based on an equivalent reformulation of the problem P1 .
421 Reformulation In the problem P1 , each ξi is associated with a set of constraints : ∀y ∈ Y(xi ) : w∆fi(y ) ≥ ∆i(y ) − ξi . This set of constraints can be equivalently written as one constraint :
ξi ≥ max y∈Y(xi )
( ∆i(y ) − w
∆fi(y) ) .
Since the above constraint ensures that ξi ≥ 0 , the prob lem P1 can be equivalently written as :
N
ξi min w,ξ
1 2 w1 + C st ∀i :
ξi ≥ max y∈Y(xi ) i=1
( ∆i(y ) − w
∆fi(y) ) , where the equality of the constraints holds when the convex program gets the optimum ; otherwise , we can find a new ξ that yields a smaller objective value while satisfying all the constraints . Putting all the above arguments together , we get the following equivalent formulation of the problem P1 : w1 + N CRhinge(w )
( 2 ) i maxy∈Y(xi)[∆i(y ) − w∆fi(y ) ] where Rhinge(w ) 1 is the structured hinge loss . It is piecewise linear and convex . The last step is to show that the formulation ( 2 ) is equiv min
1 2 w
N alent to the following constrained optimization problem :
P1
: min w
Rhinge(w ) , st : w1 ≤ λ
2 ≤ λ .
This is because for the optimum solution w(cid:63 ) of the problem ( 2 ) with a specific C , we can set λ = w(cid:63)1 in P1 . Then , w(cid:63 ) is the optimum solution of P1 , otherwise , we can find a new w that achieves a smaller objective value in the problem ( 2 ) . Conversely , we can inverse the mapping to find those values of C in ( 2 ) that give rise to the same solution as P1 . The problem P0 has a similar formulation as the P1 , but with the constraint replaced by w2 422 Projected Sub gradient Algorithm To solve the constrained convex optimization problem P1 , probably the most widely used algorithm is the projected sub gradient method for convex optimization [ 5 ] . Let W denote the convex set of w defined by the constraint w1 ≤ λ , and let denote a sub gradient of the structured hinge loss Rhinge(w ) , then projected sub gradient methods minimize the hinge loss Rhinge(w ) by generating the sequence {w(t)} via : w(t+1 ) = ΠW w(t ) − ηt ( t)(cid:162 )
( 3 ) where ( t ) is any sub gradient evaluated at w(t ) , ηt is a learning rate , and ΠW ( w ) = arg minµ∈W µ − w1 .
( cid:161 )
,
Definition 1
Here , by sub gradient , we mean a vector defined as follows : ( Sub gradient ) . Let h : W → R be a convex function . A sub gradient at w ∈ W is a vector g ∈ RK such that ∀w ∈ W , h(w ) ≥ h(w ) + g(w − w ) .
Finding one sub gradient of the hinge loss Rhinge(w ) is easy . Specifically , for each component : maxy∈Y(xi)[∆i(y)− w∆fi(y) ] , one sub gradient1 is : gi(w ) = f ( xi , y(cid:63))−f ( xi , yi ) , where y(cid:63 ) = arg maxy∈Y(xi ) wf ( xi , y ) + ∆i(y ) , which is the loss augmented prediction under the current model and is because maxy∈Y(xi)[∆i(y ) − w∆fi(y ) ] = 1This maxy∈Y(xi)[∆i(y ) + wf ( xi , y ) ] − wf ( xi , yi ) , where the last term is a constant . This is a piecewise linear maximization problem , whose sub gradient is determined by the component that achieves the maximum value [ 7 ] .
Algorithm 2 Projected Sub gradient Method
Input : data D = {(xi , yi)}N number T Output : wT for t = 1 to T − 1 do
N
( cid:161 )
Compute g(w(t ) ) = 1 N Update w(t+1 ) = ΠW i=1 gi(w(t) ) . w(t ) − ηtg(w(t ) )
( cid:162 )
. i=1 , constants λ , iteration end for
N
Figure 1 : Projection of a point to the 2 ball ( solid ) ; and 1 ball ( dashed ) in the two dimensional space . can be efficiently done as we stated in Section 41 Therefore , one sub gradient of Rhinge(w ) is : g(w ) = 1 i=1 gi(w ) , and can be efficiently computed .
N
With the sub gradients computed , we can develop a batch or an online algorithm to learn the 1 M3N , like the subgradient methods [ 19 ] for M3N . Algorithm 2 outlines the batch version , where the projection to an 1 ball can be performed with the efficient projection algorithms in [ 8 ] .
1 , and the projection to the 1 ball is p(cid:63 )
From the perspective of projection , we can also see the difference between the M3N and 1 M3N . As illustrated in Figure 1 , the first orthant ( the other orthants are similar ) in a two dimensional space is partitioned into three regions by the dashed lines . For the points inside the balls ( either 1 ball or 2 ball ) , no projection is needed . Thus , we only consider the points outside of the balls . For the point p1 , which is far from both axes , the projection to the 2 ball is the point p 1 . Both p 1 and p(cid:63 ) 1 do not have a zero coordinate . For the point p2 , whose second coordinate is small ( ie , close to the axis u ) , the projection to the 2 ball is p 2 , while the projection to the 1 ball is p(cid:63 ) 2 whose second coordinate is zero . Similarly , the projection of the point p3 to the 1 ball is p(cid:63 ) 3 , whose first coordinate is zero , while the projection to the 2 ball ( ie , the point p 3 ) does not have a zero coordinate . In general , for the 1 M3N , the projection of a small weight ( ie , close to one axis ) to an 1 ball tends to be zero . In contrast , the projection of a non zero weight to an 2 ball is always nonzero . Thus , the existing 2 norm M3N does not explicitly set small weights to zeros and cannot select significant features , while the 1 M3N can select significant features . 4.3 EM Style Algorithm
The third rather novel method we present to learn the 1M3N is an EM style algorithm , which is based on an equivalence between the 1 M3N and an adaptive M3N .
431 Equivalence Theorem In [ 10 ] , an equivalence between the adaptive regression and LASSO is presented . Here , we extend the result to the max margin Markov networks . Basically , we show that the 1 M3N is equivalent to an adaptive Max Margin Markov Network ( AdapM3N ) , defined as follows :
Algorithm 3 EM Style Algorithm
Input : data D = {(xi , yi)}N tion number T Initialize βk ← √ Output : wT for t = 1 to T − 1 do
λ ( ie , τk ← 1 ) , ∀1 ≤ k ≤ K . i=1 , constants λ and C , itera
P2 ( AdapM3N ) : min w,τ,ξ wΣ−1w + C
ξi ,
Compute ( γ , ξ ) by solving the QP problem ( 5 ) . Update β using Eq ( 6 ) .
N i=1
∀k :
τk =
λ
K|wk| k |wk|
( 4 )
In practice , to avoid divergent results ( ie , zero diagonal entries in Σ ) , re parametrization of the problem P2 can be performed . Specifically , we define new variables :
∀k : γk =
1 λτk wk , and βk =
λτk .
Then , the EM style algorithm alternatively updates ( γ , ξ ) and β , as outlined in the Algorithm 3 and detailed below :
Update ( γ , ξ ) : solve the problem : λγγ + C min γ,ξ
N i=1
ξi ,
( 5 ) st ∀i , ∀y ∈ Y(xi ) :
γdiag(β)∆fi(y ) ≥ ∆i(y ) − ξi ; ξi ≥ 0 st ∀i , ∀y ∈ Y(xi ) : w∆fi(y ) ≥ ∆i(y ) − ξi ; ξi ≥ 0
K k=1
∀k :
1 K
τk =
; τk ≥ 0 .
1 λ where Σ = diag(τ ) . Here , by adaptivity , we mean that the values of the scaling factors τ are automatically adapted during the estimation process .
The rationale behind the adaptive M3N is that : by adaptively penalizing different components , the coefficients of irrelevant features can be shrunk to zero , ie , the corresponding τk go to zero . Because of the constraint 1 k=1 τk = 1 λ , K each estimate is a balance among τk . The idea of using adaptive regularization to achieve sparse estimates has been extensively studied in Automatic Relevance Determination ( ARD ) [ 18 ] and sparse Bayesian learning [ 22 ] .
K
Now , we show that the adaptive M3N produces the same estimate as the 1 M3N and thus gives sparse estimates .
Theorem 2 . The AdapM3N yields the same estimate as the 1 M3N . Proof : See Appendix A .
432 An EM style Algorithm Based on Theorem 2 , we can develop an EM style algorithm to approximately learn an 1 M3N by solving the problem P2 . Specifically , the algorithm iteratively solves the following two steps until a fixed point ( ie , a local optimum ) is obtained :
Step 1 : keep τ fixed , optimize P2 over ( w , ξ ) : wΣ−1w + C min w,ξ
N i=1
ξi st ∀i , ∀y ∈ Y(xi ) : w∆fi(y ) ≥ ∆i(y ) − ξi ; ξi ≥ 0 , This is an M3N problem and can be efficiently solved with the sub gradient [ 19 ] or exponentiated gradient [ 3 ] methods . Step 2 : keep ( w , ξ ) fixed , optimize P2 over τ . The prob lem reduces to : wΣ−1w , min
τ st :
1 K
τk =
1 λ
, τk ≥ 0 , ∀k .
K k=1
By forming a Lagrangian and doing some algebra , it is easy to show that the solution is : end for
Again , this is an M3N problem and can be efficiently
Update β : solved with the methods [ 19 , 3 , 13 ] . K|γk| j=1 γ2 j
K
∀k : βk =
√
.
( 6 )
More details about the derivation are in the proof of Theorem 2 . Note that the EM algorithm is similar to the variational learning method of the LapM3N [ 27 ] , which also iterates between solving an M3N problem and updating adaptive coefficients . The essential difference lies in the second step . When τk = 0 in Eq ( 6 ) , the feature k will be discarded in the final sparse estimate . But , the update rule in the LapM3N ensures that adaptive coefficients are always positive and finite . Therefore , the LapM3N does not explicitly discard features . This observation ( approximately ) explains why the 1 M3N is primal sparse , while the LapM3N is pseudo primal sparse . See [ 28 ] for more theoretical analysis . In this algorithm , we usually keep C fixed , while tuning the parameter λ .
( 4 ) , or βk = 0 in Eq
5 . EXPERIMENTS
This section presents some empirical evaluation of the 1M3N on both synthetic and real data sets . We compare it with the un regularized conditional random fields ( CRFs ) [ 16 ] , the 2 norm regularized CRFs ( 2 CRF ) , the primal sparse 1 norm regularized CRFs ( 1 CRF ) , the dual sparse M3N , and the pseudo primal sparse LapM3N . We use the method [ 2 ] to learn an 1 CRF and [ 19 ] to learn an M3N . 5.1 Evaluation on Synthetic Data Sets
We follow the method as described in [ 27 ] to do the experiments . We generate sequence data sets , ie , each input x is a sequence ( x1,··· , xL ) , and each component xl is a d dimensional vector of input features . The synthetic data are generated from pre specified CRF models with either iid instantiations of the input features or correlated instantiations of the input features , from which samples of the structured output y , ie , a sequence ( y1,··· , yL ) , can be drawn from the conditional distribution p(y|x ) defined by the CRF based on a Gibbs sampler .
Due to space limitation , we only report the results on the data sets with correlated input features . We get the same conclusions in the iid case . Specifically , we set d = 100 and 30 input features are relevant to the output . The 30 relevant features are partitioned into 10 groups . For the features in each group , we first draw a real value from a standard normal distribution and then corrupt the feature with a random Gaussian noise ( zero mean and standard variance 0.05 ) to get 3 correlated features . Then , we generate 10 linear chain CRFs with 8 binary states ( ie , L = 8 and Yl = {0 , 1} ) . The feature functions include : 200 real valued state feature functions , of which each is over a one dimensional input feature
Figure 2 : Error rates of different models . and a class label ; and 4 ( 2 × 2 ) transition feature functions capturing pairwise label dependencies . Each CRF generates a data set that contains 1000 instances .
We do K fold cross validation on each data set and take the average ( both accuracy and the variance ) over the 10 data sets as the final results . In each run we choose one part to do training and test on the rest K−1 parts . K is changed from 20 , 10 , 7 , 5 , to 4 . In other words , we use 50 , 100 , about 150 , 200 , and 250 samples during the training . Figure 2 shows the average performance . We can see that the primal sparse models ( ie , 1 M3N and 1 CRFs ) outperform the M3N , which is only dual sparse . Because of a smooth shrinkage effect [ 27 ] , the LapM3N can shrink the weights of irrelevant features to be extremely small by choosing an appropriate regularization constant . In fact , the 1 M3N is an extreme case of the LapM3N when the LapM3N ’s regularization constant goes to infinity [ 28 ] . Thus , the LapM3N performs similarly to the primal sparse models . Obviously , the un regularized CRFs perform much worse than the other models which use regularization on these sparse data sets . The 2 CRFs perform comparably with the M3N , which also uses the 2 norm regularizer . This is reasonable because as noted in [ 9 ] , the 2 norm regularized maximum likelihood estimation of CRFs has a similar convex dual as that of the M3N , and the only difference is the loss they try to optimize , namely , 2 CRFs minimize the log loss while M3N minimizes the structured hinge loss .
Figure 3 shows the average weights of different models doing 10 fold CV on the first data set and the weights of the CRF model ( first plot ) that generates this data set . For CRFs , 2 CRFs , LapM3N and M3N , all the weights are nonzero , although the weights of LapM3N are generally much smaller because of the shrinkage effect [ 27 ] . For 1 M3N and 1 CRFs , the estimates are sparse . Both of them can discard all the noise features when choosing an appropriate regularization constant . As shown in [ 27 ] , 1 CRFs are sensitive to the regularization constant . As we shall see the 1 M3N with the EM style algorithm is very robust . Note that all the models have quite different average weights from the model that generates the data . This is because we use a stochastic procedure ( ie , Gibbs sampler ) to assign labels to the generated data samples . In fact , if we use the CRF model to predict on its generated data , the error rate is about 05 Thus , the learned models , which get higher accuracy , are different from the model that generates the data . 5.2 OCR Data Sets
The OCR data set is partitioned into 10 subsets for 10 fold CV as in [ 20 , 19 ] . We randomly select N samples from each fold and put them together to do 10 fold CV . We vary N from 100 , 150 , 200 , to 250 , and denote the selected data sets by OCR100 , OCR150 , OCR200 , and OCR250 , respectively .
Figure 3 : The average weights of different models .
Figure 4 : Evaluation results on OCR data sets with different numbers of selected data .
On these data sets and the web data as in Section 5.4 , our implementation of the cutting plane method for 1 M3N is extremely slow . The warm start simplex method of MOSEK does not help either . For example , if we stop the algorithm with 600 iterations on OCR100 , then it will take about 20 hours to finish the 10 fold CV . Even with more than 5 thousands of constraints in each training , the performance is still bad ( the error rate is about 045 ) The projected subgradient and the EM style algorithm both are efficient . The EM algorithm yields slightly better performance than the projected sub gradient , as we shall see .
The results are shown in Figure 4 , which are achieved by the EM style algorithm . We can see that as the number of training data increases , all the models get lower error rates and smaller variances . Generally , the 1 M3N performs comparably with the LapM3N , while consistently outperforms all the other models . M3N outperforms the standard , un regularized , CRFs and the 1 CRFs . Again , 2 CRFs perform comparably with M3N . This is reasonable due to the understanding of their only difference on the loss functions [ 9 ] as we have stated . 5.3 Comparison of the Algorithms
We have presented three algorithms to learn an 1 M3N , including the projected sub gradient , cutting plane , and the EM style algorithm . We empirically compare them on the synthetic and OCR data sets . We focus on : ( 1 ) effectiveness
0501001502002503000180202202402602803Size of training dataError Rate CRFsL2−CRFsL1−CRFsM3NL1−M3NLapM3N0204060801001201401601802000051W020406080100120140160180200−02002LapM3N020406080100120140160180200−02002M3N020406080100120140160180200−02002L1−M3N020406080100120140160180200−02002CRF020406080100120140160180200−02002L2−CRF020406080100120140160180200−02002State Feature FunctionsL1−CRF0250303504Error Rate on OCR100Error Rate CRFsL2−CRFsL1−CRFsM3NL1−M3NLapM3N0250303504Error Rate on OCR150Error Rate CRFsL2−CRFsL1−CRFsM3NL1−M3NLapM3N0250303504Error Rate on OCR200Error Rate CRFsL2−CRFsL1−CRFsM3NL1−M3NLapM3N0250303504Error Rate on OCR250Error Rate CRFsL2−CRFsL1−CRFsM3NL1−M3NLapM3N Figure 5 : The error rates and training time ( CPUseconds ) of different algorithms on the first synthetic data set against the regularization constants .
Figure 6 : The error rates and training time ( CPUseconds ) of different algorithms on the OCR100 data set against the regularization constants . in recovering sparse patterns ; ( 2 ) prediction accuracy ; and ( 3 ) time efficiency . For the EM algorithm , we keep C fixed .
531 Recovering Sparse Patterns Table 1 shows the numbers of non zero average weights learned by different algorithms with different regularization constants doing 10 fold CV on the first synthetic data set . The rows indicated by “ Total ” show the numbers of non zero weights for all the 200 state feature functions and the rows with “ Irrelevant ” show the numbers of non zero weights for the 140 state feature functions based on the 70 irrelevant input features . In the EM algorithm , we set τ ( or β ) to be zero if it is less than 10−4 . We can see that the EM algorithm has similar numbers ( in reverse orders because of different effects of their regularization constants ) of non zero weights as the cutting plane method . However , the sub gradient method keeps many features , whose weights are small but not exactly zero , and truncating the feature weights with the same threshold as in the EM algorithm doesn’t change the sparse pattern much . Perhaps tuning the learning rate could make this tail of very small features disappear . Note that for different algorithms , the regularization parameters are generally incomparable . In these experiments , we select a set of values around the best one we have tried for each algorithm .
532 Accuracy and Efficiency Figure 5 shows the error rates and training time of the three algorithms for the 1 M3N doing 10 fold CV on the first synthetic data set . We can see that both the subgradient and cutting plane methods are sensitive to their regularization constants ( see exact values in Table 1 ) . For the sub gradient method , the time depends largely on the 1 ball ’s radius . For larger balls , the projection will be easier . Consider the extreme case that the 1 ball is big enough to contain the model weights ( a point in the space RK ) , then the projection is not needed . So , the training time decreases when λ increases ( ie , the radius of 1 ball increases ) . For the cutting plane method , the time is mainly dependent on the LP solver , eg , MOSEK as we use . As C gets bigger , the training time increases very fast because more features have non zero weights ( see Table 1 ) . For the EM algorithm , both the error rate and training time are stable . We use 15 EM iterations in these experiments and each iteration takes about 3.7 cpu seconds , smaller than the time of the sub gradient method . Since the number of features ( 204 in total ) is small , the projection to an 1 ball can be efficiently done by using the algorithm [ 8 ] . Therefore , the overall training time of the projected sub gradient algorithm is smaller than those of the other two methods . The best performance of all the three algorithms on this data set is comparable .
Figure 6 shows the training time and error rates of the sub gradient and EM style algorithms on the OCR100 . The cutting plane method is not scalable on this data set , as we stated in Section 52 We can see that both algorithms are robust in the performance on this data set . The best performance of the EM algorithm is slightly better than that of the sub gradient method . This may be due to an improper learning rate in the sub gradient method . For the training time , the sub gradient method is again sensitive to its regularization constant , while the EM algorithm is very stable . As we have stated , the decrease in the training time of the sub gradient method is due to the fact that projection to a large 1 ball is easier than projection to a smaller ball . 5.4 Web Data Extraction
Web data extraction is a task to identify interested information from web pages , as extensively studied in [ 25 ] . Each sample is a data record or an entire web page which is represented as a set of HTML elements . One striking characteristic of web data extraction is that various types of structural dependencies between HTML elements exist , eg the HTML tag tree or the Document Object Model ( DOM ) structure is itself hierarchical . In [ 25 ] , hierarchical CRFs are shown to have great promise and achieve better performance than flat models like linear chain CRFs [ 16 ] . One method to construct a hierarchical model is to first use a parser to construct a so called vision tree . Then , based on the vision tree , a hierarchical model can be constructed accordingly to extract the interested attributes . See [ 25 ] for an example of the vision tree and the corresponding hierarchical model .
In these experiments , we identify four attributes—Name , Image , Price , and Description for each product item . We use the data set that is built with web pages generated by 37 different templates [ 25 ] . For each template , there are 5 pages for training and 10 for testing . We assume that data records are given , and compare different models on the accuracy of extracting attributes in the given records . There are 1585 data records in the 185 training pages and
05020250303504Proj−SubgradientError Rate05020250303504Cutting Plane0051020250303504EM Alg.0551015square root of lambdaTraining Time050200400600square root of C00.5150556065lambda / 1400012141618202203403603804Proj−SubgradientError Rate051003403603804EM Alg.12141618202250100150square root of lambdaTraining Time051050100150square root of lambda √
λ
Proj Subgradient
Irrelevant
√ Total
C
Cutting plane
Irrelevant
EM Alg .
Total
λ
14000
Irrelevant
Total
Table 1 : The number of non zero average weights by different algorithms on the first synthetic data set .
0.5 138 198
0.5 0 13
0.7 136 196
0.7 0 21
0.036 140 200
0.069 140 198
0.87 140 200
0.87
1
140 200
1.41 140 200
1.73 140 200
2
140 200
2
3
140 200
3
1 16 44 0.21 108 158
1.41 103 145 0.35 48 90
1.73 122 169 0.5 18 54
134 184 0.64
2 26 0.14 128 182 7 . ACKNOWLEDGMENTS
139 186 0.78
2 34
0 32
4
140 200
4
140 189 0.93
0 28
Figure 7 : Average F1 and block instance accuracy with different numbers of training data .
3391 data records in the 370 testing pages . We use the two comprehensive evaluation measures , ie average F1 and block instance accuracy [ 25 ] . Average F1 is the average value of the F1 scores of the four attributes , and block instance accuracy is the percent of data records whose Name , Image , and Price are all correctly identified .
We randomly select m = 5 , 10 , 15 , 20 , 30 , 40 , or , 50 percent of the training records as training data , and test on all the testing records . For each m , 10 independent experiments are conducted and the average performance is summarized in Figure 7 . We can see that : first , the models ( especially the max margin models , ie , M3N , 1 M3N , and LapM3N ) with regularization ( ie , 1 norm , 2 norm , or the entropic regularization of LapM3N ) can significantly outperform the un regularized CRFs . Second , the max margin models generally outperform the conditional likelihood based models . Third , the primal sparse 1 M3N performs comparably with the pseudo primal sparse LapM3N , and outperforms all other models , especially when the number of training data is small . Finally , as in the previous experiments on OCR data , the 1 M3N generally outperforms the 1 CRFs , which suggests the potential promise of the max margin based 1 M3N .
6 . CONCLUSIONS
We have presented the 1 norm max margin Markov network ( 1 M3N ) , which enjoys both the primal and dual sparsity , and introduced three methods to learn an 1 M3N , including a projected sub gradient , a cutting plane and a novel EM style algorithm that is based on an equivalence between the 1 M3N and an adaptive M3N . We conducted extensive empirical studies on both synthetic and real world OCR and web data . Our results show that : ( 1 ) the 1 M3N can effectively select significant features ; ( 2 ) the 1 M3N can perform as well as the closely related pseudo sparse LapM3N in prediction accuracy , while consistently outperforms other competing models that enjoy either primal or dual sparsity ; and ( 3 ) the EM style algorithm is more robust than the other two in both prediction accuracy and time efficiency .
This work was done while JZ was a visiting researcher at CMU under a support from NSF DBI 0546594 and DBI0640543 awarded to EX ; JZ and BZ are also supported by Chinese NSF Grant 60621062 and 60605003 ; National Key Foundation R&D Projects 2003CB317007 , 2004CB318108 and 2007CB311003 ; and Basic Research Foundation of Tsinghua National Lab for Info Sci & Tech .
8 . REFERENCES [ 1 ] Y . Altun , I . Tsochantaridis , and T . Hofmann . Hidden
Markov support vector machines . In ICML , 2003 .
[ 2 ] G . Andrew and J . Gao . Scalable training of
1 regularized log linear models . In ICML , 2007 .
[ 3 ] P . Bartlett , M . Collins , B . Taskar , and D . McAllester .
Exponentiated gradient algorithms for larg margin structured classification . In NIPS , 2004 .
[ 4 ] K . Bennett and O . Mangasarian . Robust linear programming discrimination of two linearly inseparable sets . Optim . Methods Softw , ( 1):23–34 , 1992 .
[ 5 ] D . P . Bertsekas . Nonlinear Programming . Athena
Scientific , 1999 .
[ 6 ] S . Boyd and L . Vandenberghe . Convex Optimization .
Cambridge University Press , 2004 .
[ 7 ] S . Boyd , L . Xiao , and A . Mutapcic . Subgradient methods . In Lecture Notes of EE392o , Stanford University , Autumn Quarter , 2003 2004 .
[ 8 ] J . Duchi , S . Shalev Shwartz , Y . Singer , and
T . Chandra . Efficient projection onto the l1 ball for learning in high dimensions . In ICML , 2008 .
[ 9 ] A . Globerson , T . Y . Koo , X . Carreras , and M . Collins .
Exponentiated gradient algorithms for log linear structured prediction . In ICML , 2007 .
[ 10 ] Y . Grandvalet . Least absolute shrinkage is equivalent to quadratic penalization . In ICANN , 1998 .
[ 11 ] Y . Grandvalet and S . Canu . Adaptive scaling for feature selection in svms . In NIPS , 2002 .
[ 12 ] T . Hastie , R . Tibshirani , and J . Friedman . The Elements of Statistical Learning : Data Mining , Inference and Prediction . Springer Verlag , NY , 2001 . [ 13 ] T . Joachims , T . Finley , and C N Yu . Cutting plane training of structural svms . Machine Learning , 2009 .
[ 14 ] A . Kaban . On Bayesian classification with laplace priors . Pattern Recognition Letters , 28(10):1271–1282 , 2007 .
[ 15 ] J . E . Kelley . The cutting plane method for solving convex programs . J . Sco . Indust . Appl . Math . , ( 4):703–712 .
0102030405008608708808909091092093094095Training RatioAverage F1 CRFsL2−CRFsL1−CRFsM3NL1−M3NLapM3N010203040500740760780808208408608809Training RatioBlock Instance Accuracy CRFsL2−CRFsL1−CRFsM3NL1−M3NLapM3N [ 16 ] J . Lafferty , A . McCallum , and F . Pereira . Conditional random fields : Probabilistic models for segmenting and labeling sequence data . In ICML , 2001 .
[ 17 ] S I Lee , V . Ganapathi , and D . Koller . Efficient structure learning of Markov networks using 1 regularization . In NIPS , 2006 .
[ 18 ] R . M . Neal . Bayesian learning of neural networks . In
Leture Notes in Statistics , 1996 .
[ 19 ] N . D . Ratliff , J . A . Bagnell , and M . A . Zinkevich .
( Online ) Subgradient methods for structured prediction . In AISTATS , 2007 .
[ 20 ] B . Taskar , C . Guestrin , and D . Koller . Max margin
Markov networks . In NIPS , 2003 .
[ 21 ] C . H . Teo , Q . Le , A . Smola , and S . Vishwanathan . A scalable modular convex solver for regularized risk minimization . In SIGKDD , 2007 .
[ 22 ] M . E . Tipping . Sparse bayesian learning and the relevance vector machine . JMLR , ( 1):211–244 , 2001 .
[ 23 ] I . Tsochantaridis , T . Hofmann , T . Joachims , and
Y . Altun . Support vector machine learning for interdependent and structured output spaces . In ICML , 2004 .
[ 24 ] M . J . Wainwright , P . Ravikumar , and J . Lafferty . High dimensional graphical model selection using 1 regularized logistic regression . In NIPS , 2006 .
[ 25 ] J . Zhu , Z . Nie , B . Zhang , and J R Wen . Dynamic hierarchical Markov random fields for integrated web data extraction . JMLR , ( 9):1583–1614 , 2008 .
[ 26 ] J . Zhu , S . Rosset , T . Hastie , and R . Tibshirani . 1 norm support vector machines . In NIPS , 2004 .
[ 27 ] J . Zhu , E . Xing , and B . Zhang . Lapalce maximum margin Markov networks . In ICML , 2008 .
[ 28 ] J . Zhu and E . P . Xing . On primal and dual sparsity of
Markov networks . In ICML , 2009 .
[ 29 ] H . Zou . An improved 1 norm svm for simultaneous classification and variable selection . In AISTATS , 2007 .
APPENDIX A . PROOF OF THEOREM 2 Proof : Our proof uses some techniques of the proof in [ 10 ] . wk , and We do re parametrization by defining γk = λτk , ∀k . Then , w = diag(γ)β = diag(β)γ g(γ , β ) ,
√
βk = and the problem P2 changes to :
λτk
1
P3 : min γ,β,ξ
λγγ + C
ξi ,
N i=1 st ∀i , ∀y ∈ Y(xi ) :
∀k :
K g(γ , β)∆fi(y ) ≥ ∆i(y ) − ξi ; ξi ≥ 0 k=1 k = K ; βk ≥ 0 . β2 N
The Lagrangian for P3 is as follows :
L(γ , β , ξ , α , µ,v , η ) = λγγ + C
ξi + µξ + v(
K k − K ) β2 i=1
−
αi(y)(g(γ , β)∆fi(y ) − ∆i(y ) + ξi ) + ηβ . Taking the derivation of L wrt γ and β , we get the nor i,y mal equations :
∂L
∂γ = 2λγ − ∂g(γ,β ) ∂β = 2vβ − ∂g(γ,β )
∂β
∂γ
∂L
(
( i,y αi(y)∆fi(y ) ) i,y αi(y)∆fi(y ) ) + η
( 7 ) i=1
With the definition of g(γ , β ) , we can get ∂g(γ,β )
∂γ = diag(β ) ∂β = diag(γ ) . Let ( ˆγ , ˆβ , ˆξ ) be the optimum solution and ∂g(γ,β ) of P3 , and let ( ˆα , ˆµ , ˆv , ˆη ) be the dual optimum . Using the optimality condition that ∂L ∂β = 0 and doing same algebra , we can get :
∂γ = 0 and ∂L
2λdiag(ˆγ)ˆγ = 2vdiag( ˆβ ) ˆβ + diag( ˆβ)ˆη .
( 8 ) By the Complementary slackness theorem , diag( ˆβ)ˆη = 0 .
Thus , we have :
The equality constraint ∀k , ˆβk =
∀k , ˆβ2
ˆγ2 k .
K
λ k = v k β2 √ k = K implies that : K|ˆγk| j=1 ˆγ2 j
.
( 9 )
( 10 )
( 11 )
To get the optimality conditions for the original problem , we let ˆw be the optimum solution of P2 . From the definition of γ and β , we get ∀k , | ˆwk| = ˆβk|ˆγk| . Thus ,
√ k
K ˆγ2 j ˆγ2 j
| ˆwk|K j=1 | ˆwj| = kK
ˆγ2 j=1 ˆγ2 j
⇒
∀k , | ˆwk| =
From the first equation in ( 7 ) and using the optimality condition of ∂L
∂γ = 0 , we can get :
∀k , 2λˆγk − ˆβk(
ˆαi(y)∆f k i ( y ) ) = 0 .
( 12 ) if ˆβk = 0 , then we have
−
ˆγk ˆβk
∀k , 2λ
ˆαi(y)∆f k
From Eq ( 10 ) and ( 11 ) , we get : ˆβ2
We consider two cases . First , ˆwk = ˆγk = 0 . Second , if ˆβk = 0 , then we get : i ( y ) = 0 . K K k = K| ˆwk| j=1 | ˆwj| . Thus , K j=1 | ˆwj| = 0
Therefore , the optimality conditions are : ∀k , K sign( ˆwk )
( cid:189 ) − i,y ˆαi(y)∆f k i ( y ) + 2 λ sign( ˆwk )
1 ˆβ2 k
ˆγk ˆβk
= ˆwk
∀k ,
= ˆγk
1 K
ˆβk j=1 i,y
| ˆwj| . ( 14 )
( 13 ) or ˆwk = 0 , which can easily be shown to have the same form as the optimality conditions of the following problem : i,y
K j=1 | ˆwj| K| ˆwk| =
( cid:161 ) K
|wk|(cid:162)2 + C
N
λ K i=1 k=1
ξi , st ∀i , ∀y ∈ Y(xi ) : w∆fi(y ) ≥ ∆i(y ) − ξi ; ξi ≥ 0
P4 : min w,ξ
K K
K k=1 | ˆwk| =
K The last step is to show that the optimum lagrange multipliers in P3 and P4 are the same . Eq ( 11 ) implies that : k=1 | ˆwk|)2 . Substituting this result into the Lagrangian L and using the equality constraint k = K and Complementary slackness theorem that ˆη ˆβ = 0 , we can get : | ˆwk|)2 + C j . Thus , ˆγˆγ = 1 K ( K N
L(ˆγ , ˆβ , ˆξ , ˆα , ˆµ , ˆv , ˆη ) =
ˆξi + ˆµ ˆξ i=1 β2 j=1 ˆγ2
√
K
(
λ K k=1 i=1
−
ˆαi(y)( ˆw∆fi(y ) − ∆i(y ) + ˆξi ) , i,y which is the Lagrangian of the problem P4 evaluated at the optimal solution . Therefore , the optimum dual variables ˆα are the same for both P3 and P4 , and the above optimality conditions are the optimality conditions of P4 .
Similar to the re formulation of the 1 M3N , the problem
P4 can be formulated as the problem :
K which is an 1 M3N problem as in P1 .
Rhinge(w ) , st : min k=1 w
|wk| ≤ λ ,
