Mining Discrete Patterns via Binary Matrix Factorization
Bao Hong Shen
Arizona State University Tempe , AZ 85287 , USA bhshen@ieee.org
Shuiwang Ji
Arizona State University Tempe , AZ 85287 , USA shuiwangji@asuedu
Jieping Ye
Arizona State University Tempe , AZ 85287 , USA jiepingye@asuedu
ABSTRACT Mining discrete patterns in binary data is important for subsampling , compression , and clustering . We consider rankone binary matrix approximations that identify the dominant patterns of the data , while preserving its discrete property . A best approximation on such data has a minimum set of inconsistent entries , ie , mismatches between the given binary data and the approximate matrix . Due to the hardness of the problem , previous accounts of such problems employ heuristics and the resulting approximation may be far away from the optimal one . In this paper , we show that the rank one binary matrix approximation can be reformulated as a 0 1 integer linear program ( ILP ) . However , the ILP formulation is computationally expensive even for small size matrices . We propose a linear program ( LP ) relaxation , which is shown to achieve a guaranteed approximation error bound . We further extend the proposed formulations using the regularization technique , which is commonly employed to address overfitting . The LP formulation is restricted to medium size matrices , due to the large number of variables involved for large matrices . Interestingly , we show that the proposed approximate formulation can be transformed into an instance of the minimum s t cut problem , which can be solved efficiently by finding maximum flows . Our empirical study shows the efficiency of the proposed algorithm based on the maximum flow . Results also confirm the established theoretical bounds .
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications Data Mining General Terms Algorithms Keywords Binary matrix factorization , rank one , integer linear program , regularization , minimum cut , maximum flow
1 .
INTRODUCTION
Many applications involve data with discrete attributes and high dimensionality , such as binary images and documentterm associations . For such applications , it is desirable to retain the discrete nature of the original data in the factorized components . PCA [ 8 ] is a commonly used method for reducing the dimension of continuous data , and it has been used widely in computer vision and many other applications [ 16 , 18 ] . PCA computes a set of orthogonal projection vectors by the Singular Value Decomposition ( SVD ) [ 4 ] , which capture the largest variations in the data . PCA and many other matrix based techniques [ 12 ] deal with data of continuous attributes . Analysis of discrete data sets , however , generally leads to NP complete/hard problems , especially when physically interpretable results in discrete spaces are desired . For discrete data sets , where all the entries are either 0 or 1 , it is desirable to find decomposition where all the entries involved are from {0 , 1} .
A technique , called PROXIMUS [ 13 ] was proposed to mine discrete patterns from binary data . PROXIMUS is based on the binary matrix factorization , which separates a data set based on the entries of a binary vector and on recursive partition in the direction of such vectors . The final product for a given binary matrix by PROXIMUS is a tree structure , representing various degrees of similarity among data points . Similar to SVD , the rank one approximation is at the core of PROXIMUS . A binary rank one approximation is given by the outer product of two binary vectors , one associated with data points and the other with their attributes . Starting from the root node of a tree , the entries in the data vector serves as indicators for bisection . That is , data points with 1 are made into one group while those with 0 are into the other , eg , two disjoint submatrices . By this , two child nodes of the root are constructed . The tree is completed by this way recursively for each submatrix until some termination criterion is met , eg , bisection is not possible for a data vector of all ones . PROXIMUS has been applied for mining high dimensional discrete attribute data [ 13 , 14 , 15 ] .
The problem of binary rank one approximations for a given binary matrix is to find two binary vectors x1 and x2 such that the outer product of x1 and x2 is at the minimum Hamming distance from the given binary matrix [ 13 ] . In particular , for a binary matrix A ∈ {0 , 1}I1×I2 , where I1 and I2 are some positive integers , x1 ∈ {0 , 1}I1 and x2 ∈ {0 , 1}I2 are computed by solving the following optimization problem :
( cid:176)(cid:176)(cid:176)A − x1xT
2
( cid:176)(cid:176)(cid:176)2
F
,
( 1 )
I1,I2 i,j=1 min x1,x2
|(A − x1xT
2 )i,j| ≡ min x1,x2 where || · ||F denotes the Frobenius norm [ 4 ] of a matrix . The problem in ( 1 ) is conjectured to be NP hard [ 14 ] , and an iterative heuristic for rank one approximation was proposed in PROXIMUS [ 13 ] , whose solutions are sensitive to the initialization . Eight heuristics were proposed to initialize the approximation vectors . However , there is no theoretical guarantee on the quality of the approximations [ 15 ] .
In this paper , we propose high quality low rank approximations of discrete data sets with guaranteed error bounds . Our main contributions include :
• We show that the rank one binary matrix approximation can be formulated as a maximum weight problem in Section 2 . We further show that this maximum weight problem can be solved exactly by a 0 1 integer linear program ( ILP ) . Optimal approximations of small scale data may be found in acceptable time , and their results may be required by error sensitive applications , eg , a matrix usually of small scale representing associations between patients and pathological symptoms for medical study .
• Based on the equivalent ILP formulation , we propose a linear program ( LP ) relaxation in Section 3 , which scales to medium size matrices . We show that the solution from the LP relaxation achieves an approximation error bounded by two fold of that from an optimal approximation . We further integrate the LP and an iterative procedure to improve the quality of the solution
• We extend the proposed exact and approximate formulations using the regularization technique , which is commonly employed to address overfitting . The regularization algorithms require minimal modifications to the standard ones and provide guaranteed solution quality . The error by the regularized approximation algorithm is a function of the regularization parameter λ > 0 , and it is shown to be upper bounded by 1+min{1,λ} in Section 4 .
2
• The algorithms based on the liner programs are still computationally expensive for large matrices , due to the large number of variables involved . In Section 5 , we show that the proposed approximate formulation can be transformed into an instance of minimum s t cut problem , which can be solved efficiently by finding maximum flows .
• We have conducted empirical studies using both synthetic and real data in Section 6 . Empirical results show the efficiency of the proposed algorithm based on the maximum flow . Results also confirm the consistency and tightness of the established theoretical bounds .
2 . REFORMULATION AS A MAXIMUM
WEIGHT PROBLEM
In this section , the problem of rank one approximation is transformed into a maximum weight problem ( definition follows ) . The simple form of the objective function in this problem permits elegant mathematical formulations for optimal solutions .
I1,I2
Wi,jzi,j ,
Maximize i,j=1 subject to −x1i − x2j + 2zi,j ≤ 0 x1i + x2j − zi,j ≤ 1 x1i , x2j , zi,j ∈{0 , 1} x1i , x2j , zi,j ∈ [ 0 , 1 ] for Wi,j = 1 , for Wi,j = −1 , ∀i , j for ILP , or ∀i , j for LP1 .
( 3 )
( 4 )
( 5 )
( 6 )
Figure 1 : The 0 1 integer linear program ( 0 1 ILP ) including constraints ( 3)(4)(5 ) for the maximum weight problem and its relaxation ( LP1 ) including constraints ( 3)(4)(6 ) .
Since every entry of the given matrix A is either 0 or 1 , the objective function of the binary rank one approximation can be simplified as follows :
I1,I2
( cid:175)(cid:175)(cid:175)(A − x1xT i,j=1
=
=
( cid:175)(cid:175)(cid:175 ) =
2 )i,j i,j=1
I1,I2 I1,I2 I1,I2 i,j=1
( Ai,j − x1ix2j)2 i,j − 2Ai,jx1ix2j + x2
1ix2
2j )
( A2
Ai,j − I1,I2 x1i(2Ai,j − 1)x2j i,j=1 i,j=1
= A2
F − xT
1 Wx2 ,
( 2 ) where W ∈ {−1 , 1}I1×I2 is the weight matrix defined for a given binary matrix A ∈ {0 , 1}I1×I2 as Wi,j = 1 , if Ai,j = 1 , and Wi,j = −1 , otherwise . Since A is fixed in equation ( 2 ) , an optimal solution can be found by solving maxx1,x2 xT 1 Wx2 . We call this the “ Maximum Weight Problem ” ( MWP ) as formally stated below :
Maximum Weight Problem ( MWP ) : Given a weight matrix W ∈ {−1 , 1}I1×I2 , find two vectors x1 ∈ {0 , 1}I1 and x2 ∈ {0 , 1}I2 such that i,j=1 Wi,jx1ix2j is maximized .
I1,I2
A 0 1 integer linear program ( ILP ) is given for the exact solution of MWP in Figure 1 . We will show that solving this 0 1 ILP is equivalent to solving MWP . The constraint ( 5 ) is employed in the 0 1 ILP instead of ( 6 ) , which will be used for the LP relaxation in Section 3 . There are three types of binary variables x1i , x2j , and zi,j in the ILP for 1 ≤ i ≤ I1 and 1 ≤ j ≤ I2 . Variables x1i and x2j represent the i th and j th entries of solution vectors x1 and x2 respectively . Variable zi,j is an auxiliary variable whose value is 1 , if and only if both the values of x1i and x2j are 1 . The objective function of MWP can therefore be rewritten as i,j=1 Wi,jzi,j as in Figure 1 . Constraints ( 3 ) and ( 4 ) are for the relation between zi,j and the pair of x1i and x2j . Notice that variable zi,j for each pair of i and j appears exactly twice in two places of the ILP : ( i ) the term Wi,jzi,j in the objective function and ( ii ) the one constraint for Wi,j . This implies that the value of zi,j is determined by the sign of Wi,j and the other two variables involved in the constraint for Wi,j .
I1,I2
( x1i + x2j ) − zi,j ,
Wi,j =−1 for Wi,j = −1 , ( 7 ) for ∀i , j , and ( 8 ) for Wi,j = −1 . ( 9 )
Maximize subject to
1 2
Wi,j =1 x1i + x2j − zi,j ≤ 1 x1i , x2j ∈ [ 0 , 1 ] zi,j ∈ [ 0 , 1 ]
Figure 2 : LP2 : A linear program for the relaxed problem .
For each positive Wi,j , constraint ( 3 ) ensures that zi,j equals zero if the value of x1i or x2j is zero . If x1i and x2j both equal to one , the value of zi,j is free to be either 0 or 1 ; however , the maximization forces the value of zi,j to be one since Wi,j is positive . For each negative Wi,j , constraint ( 4 ) makes the value of zi,j one if x1i and x2j both equal to one . If any of x1i and x2j equals zero , then the value of zi,j is free to be either 0 or 1 ; nevertheless , at optimality , zi,j must equal zero since Wi,j has negative contribution to the objective in this case . Consequently , the relation between zi,j and the pair x1i and x2j for some i and j at optimality is as follows : zi,j = 1 , if and only if x1i = 1 and x2j = 1 . The relation between the ILP and MWP is summarized below . Proposition 1 . For a given matrix A ∈ {0 , 1}I1×I2 , the i,j=1
0 1 ILP in Figure 1 defines an optimal solution S = {x2j}I2 j=1 , {zi,j}I1,I2 of binary values . That is , matrix x1xT 2 is an optimal rank one approximation for A , where x1 = [ x11 ··· x1I1 ]T , and x2 = [ x21 ··· x2I2 ]T . Furthermore , A2 i,j=1 Wi,jzi,j is the minimum value over all possible pairs of x1 and x2 for A .
F −I1,I2
( cid:162 )
( cid:161){x1i}I1 i=1 ,
3 . ERROR BOUNDED APPROXIMATIONS The ILP formulation in Figure 1 defines the optimal solutions for MWP . However , it is computationally expensive to solve even for small scale problems . In this section , we propose an efficient algorithm based on the linear programming relaxation for computing error bounded rank one approximations . The error by this algorithm is shown to be no more than twice of that by the optimal solution . 3.1 Relaxation for Maximum Weight Problem We relax the integral constraint ( 5 ) : x1i , x2i , zij ∈ {0 , 1} , to the constraint ( 6 ) : x1i , x2i , zi,j ∈ [ 0 , 1 ] , and obtain a linear program ( LP ) in Figure 1 . We call this LP as LP1 . Each entry of x1 and x2 in LP1 is allowed to have any nonnegative value no more than 1 .
Solving LP1 is less difficult than solving its original ILP in Figure 1 . Linear programs are polynomial time solvable [ 10 ] . Simplex Method [ 2 ] and its variants are widely used to solve LP ’s . Despite their exponential worst case complexity [ 11 ] , simplex algorithms have persistently shown to perform very well in practice [ 17 ] . A randomized polynomial time simplex algorithm is given by [ 9 ] . We use simplex algorithms to solve LP relaxations .
A compact LP , named LP2 , that defines the same set of optimal solutions for LP1 is given in Figure 2 . LP2 has fewer constraints and variables , ie , more efficient than LP1 in terms of running time . The main difference between the two formulations is that all the constraints at ( 3 ) in LP1 are incorporated into the objective function of LP2 . This
Algorithm 1 : LPIT — Algorithm for binary rank one approximation Data : matrix A ∈ {0 , 1}I1×I2 Result : a pair of vectors in {0 , 1}I1 and {0 , 1}I2 Phase One : Formulate an instance of LP2 for A ; Find an optimal solution x1 ∈ RI1 , x2 ∈ RI2 ; Phase Two : Construct a weight matrix W from A ; Apply the iterative procedure to improve x1 and x2 ; can be done because of the following facts : ( i ) LP1 is a maximization problem , ( ii ) the Wi,j for the zi,j in ( 3 ) is positive , ( iii ) each zi,j appears in exactly one constraint , ( iv ) a zi,j in ( 3 ) is bounded above by 1 2 ( x1i + x2j ) , and ( v ) every variable in LP1 can have a value in the interval [ 0 , 1 ] . Those facts imply that the equality at ( 3 ) must hold for LP1 at optimality . Suppose , as contradiction , that there exists a case that the equality at ( 3 ) dose not hold at optimality , ie , zi,j < 1 2 ( x1i + x2j ) for some i and j . Due to the facts that Wi,j = 1 and zi,j does not involve in any other constraint , increasing the value of zi,j gives a feasible solution with better objective value , a contradiction . On the other hand , the equality of each constraint at ( 4 ) , for Wi,j = −1 , may not hold at optimality , eg , zi,j = −1 for x1i = 0 and x2j = 0 if the equality would hold but zi,j ≥ 0 . Each variable zi,j for positive Wi,j in LP1 becomes redundant and is removed from LP2 . However , their values can be obtained from the solution for LP2 as zi,j =
1 2
( x1i + x2j ) for each Wi,j = 1 .
( 10 )
Since removing or relaxing a constraint of a maximization program does not reduce an optimal value , the relations among the optimal values lILP , lLP1 , and lLP2 of the ILP , LP1 , and LP2 , respectively , are follows : lILP ≤ lLP1 = lLP2 for the same given weight matrix .
Proposition 2 . For the same problem instance , the op timal value of MWP is no more than that of LP2 . 3.2 Approximation Algorithm
Our algorithm for binary rank one approximation consists of two phases : ( i ) find a solution by LP2 , and ( ii ) apply an iterative procedure to further improve the solution . The iterative procedure [ 13 ] can be done as follows : Since the objective value for MWP is xT 1 Wx2 , one of the vectors x1 and x2 can be quickly determined if the other is fixed . The procedure alternatively finds one of the vectors , while fixing the other , until a termination criterion is met . The method , named LPIT , is summarized in Algorithm 1 .
Two questions arise from the algorithm LPIT : ( 1 ) Is a solution found in Phase One of LPIT valid ? That is , does LP2 find a pair of binary vectors ? and ( 2 ) What is the quality of solutions found by LPIT ?
Every variable in LP2 is allowed to have any value within the interval between 0 and 1 . A matrix with fractional values cannot be a binary approximation . The solution by simplex algorithms for LP2 is shown in Section 3.3 to have binary property ; namely , every entry in x1 and x2 is either 0 or 1 . In addition to feasibility , the quality of solutions by approximation is also important . The second phase of the algorithm LPIT is a heuristic , whose improvement on solutions is less predictable . The assurance on error bounded approximations therefore depends on the initial vectors , ie , optimal solutions for LP2 . The discrepancy between objective values of MWP and LP2 is the focus of analysis on the error bound , which is covered in Section 34 3.3 Solutions with the Binary Property
The main result of this section is summarized in the the orem below .
Theorem 3 . The value of every variable found by the simplex algorithm for LP2 is either 0 or 1 .
The following definitions are relevant to the coefficient matrix of the constraints in LP2 . A square matrix is unimodular if its determinant is −1 or 1 . A matrix A is totally unimodular if every square submatrix B of A is unimodular or singular , ie , det B ∈ {−1 , 0 , 1} . A matrix A is totally unimodular if the condition below holds [ 5 ] .
Sufficient Condition 1
( Total Unimodularity ) . Suppose that every column of matrix A ∈ {−1 , 0 , 1}I1×I2 has two nonzero entries or fewer . There exists a pair of disjoint sets S1 and S2 , with S1 ∪ S2 = {1 , 2 , . . . , I1} , such that the following condition holds : For every column j with two nonzero entries Ai,j and Ak,j , where i ∈ Sp and k ∈ Sq , p , q ∈ {1 , 2} , ( i ) Ai,j = Ak,j for p = q , and ( ii ) Ai,j = Ak,j for q = p .
A standard unit vector e is a vector with exactly one entry of one and the others are of zero , eg , [ 0 , 0 , 1 , 0]T . The lemma below can be proved by using basic facts about determinants .
Lemma 4 . Suppose matrix A ∈ RI1×I2 is totally unimodular . The following matrices are totally unimodular : ( a ) its transpose AT , ( b ) the matrix [ A| − e ] ∈ RI1×(I2+1 ) , and ( c ) a matrix obtained from A by interchanging two rows or columns .
Proof . ( a ) is due to the fact that det BT = det B for any square matrix B . ( c ) is due to the fact that interchanging n two rows or columns changes only the sign of a determinant . For any matrix C ∈ Rn×n and any row i of C , det C = k=1 ( −1)i+kCi,k det C[i,k ] , where C[i,k ] denotes the square submatrix of C obtained by removing row i and column k from C . A square submatrix C of the matrix [ A| − e ] ∈ RI1×(I2+1 ) is either ( i ) a square submatrix of A , ( ii ) [ D|0 ] , or ( iii ) [ D| − e ] , where D ∈ Rn×(n−1 ) is a submatrix of A for some n , 1 ≤ n ≤ min{I1 , I2 + 1} . The matrix C in case ( i ) is unimodular due to the total unimodularity of A . Since matrix C in case ( ii ) has a column consisting entirely of zeros , det C = 0 , which means C is unimodular . For a row i corresponding to the nonzero entry of e in case ( iii ) , det C = ( −1)i+n+1 det D[i,∅ ] ∈ {−1 , 0 , 1} in this case since D[i,∅ ] is a square submatrix of A . Thus , [ A| − e ] is totally unimodular .
Now consider the coefficient matrix C of the constraints in LP2 . Rearranging constraints and variables in different way results in a distinct matrix for C . However , due to ( c ) in Lemma 4 , total unimodularity is invariant under such operations . Without loss of generality , the rows and columns of C are assumed to be properly ordered with respect to the format in Figure 2 . Let ei,n denote a standard unit vector of size n with i th entry being 1 . Suppose that a given I1 × I2 weight matrix W has negative entries Wi1,j1 , Wi2,j2 , . . . , and Wik,jk for some k , 1 ≤ k ≤ I1I2 . Thus , C can be partitioned into

C = eT i1,I1 eT i2,I1
eT j1,I2 eT j2,I2
eT ik,I1 eT jk,I2
1,k
2,k
−eT −eT −eT k,k
 = [ X1 | X2 | − Ik ] , where X1 ∈ {0 , 1}k×I1 , X2 ∈ {0 , 1}k×I2 . By convention , the constraints of variable bounds , eg , ( 8 ) and ( 9 ) in Figure 2 , are not included in a coefficient matrix .
Lemma 5 . The coefficient matrix of constraints in LP2 is totally unimodular .
Proof . Let X be the matrix [ X1|X2 ] , where X1 and X2 are submatrices defined for C . Its transpose XT is totally unimodular since it satisfies the total unimodularity condition in Sufficient Condition 1 . By Lemma 4 , X is totally unimodular ; so does the matrix [ X| − Ik ] = C .
To characterize an optimal solution by simplex algorithms , consider the following . A polyhedron is a solution set of some finite number of linear equalities and inequalities . A bounded polyhedron is called a polytope . Let S ⊆ {0 , 1}n be the feasible solution set for some problem instance of the ILP in Figure 1 . The convex hull of S is the smallest convex set containing S . Since S is a finite set , the convex hull is a polytope ; that is , the feasible region of LP1 is a polytope ; so does LP2 . A point x in a polytope P is an extreme point if and only if x is not a convex combination of any pair of distinct points in P . If an LP has optimal solutions , then one of the optimal solutions must be an extreme point due to linear property . Simplex algorithms are based on repeatedly searching for a better extreme point on a polyhedron . Suppose that a polytope P is defined by x ∈ RI2 | bl ≤ Ax ≤ bu and cl ≤ x ≤ cu
, where A ∈ RI1×I2 , bl ∈ RI1 , bu ∈ RI1 , cl ∈ RI2 , and cu ∈ RI2 . It has been shown that the coordinates of every extreme point of P are integral if the following holds [ 7 ] .
Sufficient Condition 2
( Integral Extreme Points ) .
The coefficient matrix A is totally unimodular , and the vectors bl , bu , cl , and cu are integral .
Proof of Theorem 3 . For a given matrix of all 1 ’s , LP2 finds vectors of all 1 ’s , due to empty set of constraints ( 7 ) in this case . For other cases , the coefficient matrix of LP2 is totally unimodular , by Lemma 5 . All the constants involved in the constraints of LP2 are integral . This also includes an implicit lower bound 0 for each constraint ( 7 ) of LP2 . Thus , the Sufficient Condition 2 is satisfied . It follows that every extreme point of a polytope for LP2 is integral . More specifically , each of those integers is either 0 or 1 due to the bounds on variables in LP2 . The feasible region of an instance for LP2 is a polytope , ie , a bounded region . A simplex algorithm always finds an extreme point in the optima if the LP is feasible and bounded .
We assume that an optimal solution for LP2 hereinafter has this binary property .
3.4 Analysis on Solution Quality
The main result of this section is summarized in the fol lowing theorem .
Theorem 6 . Let x1xT
2 be the binary rank one approximation by the algorithm LPIT for a binary matrix A . The Hamming distance for binary vectors p and q .
≤ 2×minp,q
F
F
2
( cid:176)(cid:176)A − pqT
( cid:176)(cid:176)2
( cid:176)(cid:176)A − x1xT
( cid:176)(cid:176)2
The corollary below follows directly from this theorem .
Corollary 7 . For a given rank one binary matrix A , the algorithm LPIT finds a pair of vectors x1 and x2 such that x1xT
2 = A .
Theorem 6 shows an error bound on solutions by LPIT . Several definitions , relevant to its proof , are defined as follows :
Definition 1 . Given an optimal solution
( cid:161){x11 , . . . , x1I1} ,{x21 , . . . , x2I2} ,{zi,j}Wi,j =−1
S =
( cid:162 ) for LP2 by a simplex algorithm with an input matrix A ∈ {0 , 1}I1×I2 , define the following : 2 |{Wi,j x1i + x2j = 1 , Wi,j = 1}| , 1 . W1 = 1 2 . W2 = |{Wi,j x1i + x2j = 2 , Wi,j = 1}| , 3 . W(− ) = −|{Wi,j x1i + x2j = 2 , Wi,j = −1}| , 4 . Wopt = The objective value of S for LP2 , and 5 . Wapp = The objective value of a solution for MWP by the algorithm LPIT for the matrix A .
Lemma 8 . Wopt = W1 + W2 + W(− ) . Proof . Note that variable zi,j = 1 for Wi,j = −1 if and only if x1i + x2j = 2 at optimality . By Theorem 3 , every variable in the solution equals 0 or 1 . Using the objective function of LP2 , Wopt can be expressed , by Definition 1 , as
Wi,j =1
Wopt =
=
1 2
1 2
Wi,j =−1 2 −
( x1i + x2j ) −
1 +
1 2
Wi,j =1 x1i+x2j =1
Wi,j =1 x1i+x2j =2
= W1 + W2 + W(− ) . zi,j
Wi,j =−1 x1i+x2j =2
1
Lemma 9 . Wapp ≥ W2 + W(− ) . Proof . Let x1 and x2 be the two vectors found in Phase One of the algorithm LPIT . The objective value is xT 1 Wx2 by equation ( 2 ) . After the completion of LPIT , we have Wapp ≥ xT 1 Wx2 due to the iterative procedure in Phase Two . Moreover , by the definition of MWP , the value can be expressed , by Definition 1 , as xT
1 Wx2 =
=
Wi,jx1ix2j =
Wi,j i,j : x1i+x2j =2
1 = W2 + W(− ) .
Wi,j =1 x1i+x2j =2
Wi,j =−1 x1i+x2j =2 i,j
( cid:162 )
Lemma 10 . W1 ≤ 1
F − W2 input matrix for the algorithm LPIT .
2
, where A is a binary
1 −
( cid:161)A2
( cid:162 )
( cid:161){x11 , . . . , x1I1} , {x21 , . . . , x2I2} ,
Proof . Suppose that
{zi,j}Wi,j =−1 is an optimal solution for an instance of LP2 with weight matrix W of A . Since every entry in A is binary and Ai,j = 1 if and only if Wi,j = 1 , by Definition 1 , it follows that A2
F =|{Wi,j Wi,j = 1}| ≥|{Wi,j x1i + x2j = 1 , Wi,j = 1}|
+ |{Wi,j x1i + x2j = 2 , Wi,j = 1}|
=2W1 + W2 .
Proof of Theorem 6 . The Hamming distance by an optimal approximation for A is min p,q
= min p,q ≥ A2 = A2 ≥ A2
F
( cid:180 )
( cid:176)(cid:176)(cid:176)2
F − pT Wq
( cid:176)(cid:176)(cid:176)A − pqT ( cid:179 ) A2 F − Wopt ( cid:161)A2 F − W2 − W(− ) − W1 ( cid:162 ) − 1 F − W2 − W(− ) − 1 2 ( cid:162 ) F − W2 − W(− ) F − W2 − W(− ) F − Wapp
( cid:161)A2 ( cid:161)A2 ( cid:161)A2 ( cid:176)(cid:176)(cid:176)A − x1xT
( cid:176)(cid:176)(cid:176)2
( cid:162 )
2
2
F
1 = 2 ≥ 1 2 ≥ 1 2 1 2
=
F − W2
W(− ) by ( 2 )
( cid:162 ) by Proposition 2 by Lemma 8 by Lemma 10 by W(− ) ≤ 0 by Lemma 9 by ( 2 ) .
4 . REGULARIZATION
Regularization is one of key techniques underlying many well known data mining and machine learning methods , including support vector machines ( SVM ) and ridge regression . In the regularization framework , the smoothness of solution functions is typically optimized along with the error term and a regularization parameter is used to control the tradeoff between them . This generally results in the following objective function : f = ferr + λfreg , where ferr , freg , and λ are the original error function , regularizer , and regularization parameter , respectively . Specifically , we propose to solve the following optimization problem :
( cid:182 )
+ λx12
2 x22
2
,
( 11 )
( cid:176)(cid:176)(cid:176)2
F
( cid:181)(cid:176)(cid:176)(cid:176)A − x1xT ( cid:176)(cid:176)2 ( cid:176)(cid:176)A − x1xT
2 min x1,x2
2 where A ∈ {0 , 1}I1×I2 , x1 ∈ {0 , 1}I1 , and x2 ∈ {0 , 1}I2 . in ( 11 ) is the total number of misThe value matches between the given matrix A and the solution matrix x1xT 2 . The nonnegative regularization parameter λ in ( 11 ) controls the balance between the approximation error and the solution complexity . The problem at ( 1 ) is a special case of ( 11 ) when λ = 0 .
F
It follows from ( 2 ) that an optimal solution to ( 11 ) can be found by solving the following maximization problem :
( cid:179 )
( cid:180 ) max x1,x2 xT
1 Ux2
≡ max x1,x2
1 Wx2 − λx12 xT
2 x22
2
,
( 12 )
( cid:179 )
( cid:180 ) where U ∈ {−(1 + λ ) , 1 − λ}I1×I2 is called the λ regularized weight ( λ RW ) matrix of A in this paper and is defined as Ui,j = 1 − λ , if Ai,j = 1 , and Ui,j = −(1 + λ ) , otherwise . The equivalent relation in ( 12 ) is due to the equality below , with 1 being a vector of all 1 ’s : 1 Wx2 − λx12 xT
1 11T x2
2 x22
( cid:179 )
2 = xT = xT 1
1 Wx2 − λxT W − λ11T
( cid:180 ) x2 = xT
1 Ux2 .
Next , we present the proposed methods for exact and error bounded solutions for regularized rank one approximation at ( 11 ) . Our methodology is similar to the one in Section 3 : the exact solutions can be found by integer linear programming ; a relaxed integer program , called ILP2 , is proposed for error bounded solutions . 4.1 Exact and Relaxed Formulations
A 0 1 integer linear program , called ILP1 , for exact solutions of the problem at ( 12 ) is given in Figure 3 . There are three types of binary variables x1i , x2j , and zi,j in the ILP1 for 1 ≤ i ≤ I1 and 1 ≤ j ≤ I2 . Variables x1i and x2j represent the i th and j th entries of solution vectors x1 and x2 respectively . Variable zi,j is an auxiliary variable whose value is 1 , if and only if both the values of x1i and x2j are 1 . The objective function at ( 12 ) can therefore be rewritten as
I1,I2 i,j=1 Ui,jzi,j as in Figure 3 .
I1,I2
Maximize subject to −x1i − x2j + 2zi,j ≤ 0 x1i + x2j − zi,j ≤ 1 x1i , x2j , zi,j ∈{0 , 1}
Ui,jzi,j , i,j=1 for Ai,j = 1 , for Ai,j = 0 , and ∀i , j .
( 13 )
( 14 )
( 15 )
Figure 3 : ILP1 : The exact formulation .
Constraints ( 13 ) and ( 14 ) enforce the relationship between zi,j and the pair of x1i and x2j for all the i and j . Specifically , for any pair of i and j , zi,j = 1 if and only if x1i = 1 and x2j = 1 at optimality .
1 − λ
Maximize
2 subject to
( x1i + x2j)− ( 1 + λ )
Ai,j =1 x1i + x2j − zi,j ≤ 1 x1i , x2j ∈ {0 , 1} zi,j ∈ {0 , 1}
Ai,j =0 for Ai,j = 0 , for ∀i , j , and for Ai,j = 0 . zi,j ,
( 16 )
( 17 )
( 18 )
Figure 4 : ILP2 : The relaxed formulation .
The corresponding variable zi,j for each positive Ai,j is bounded above by 1 2 ( x1i +x2j ) at ( 13 ) . Replacing every such zi,j by its upper bound 1 2 ( x1i + x2j ) results in an ILP whose optimal value is not less than that of the ILP1 . Figure 4 shows this resulting formulation ILP2 . The constraint for Ai,j = 1 becomes redundant and is removed from Figure 4 . Since removing or relaxing a constraint does not make an objective value of the same instance worse , the proposition below follows :
Proposition 11 . The optimal value at ( 12 ) is no more than that of ILP2 for the same problem instance .
The main result of the error bounded approximation is given as Theorem 12 ( the proof follows similar techniques in Section 3 and is omitted ) .
Theorem 12 . Let x1xT
( cid:176)(cid:176)A − x1xT
2 be the solution matrix for regularized rank one approximation found by ILP2 for a binary matrix A and a regularization parameter λ ∈ R+ . 2 is bounded above by The cost 1+min{1,λ} × minp,q for binary
( cid:176)(cid:176)2 ( cid:161)A− pqT2
2 x22 F + λp2
+ λx12
2 q2
( cid:162 )
2
2
F
2 vectors p and q .
Similar to ILP1 in Section 3 , we relax the integral constraint of ILP2 , ie , x1i , x2i , zi,j ∈ [ 0 , 1 ] , and solve its corresponding linear program ( LP ) . The solution found by LP relaxation of ILP2 is feasible for ILP2 mainly because the coefficient matrix of ILP2 is totally unimodular [ 5 ] . That is , we essentially get integral x1i , x2i , zi,j ∈ {0 , 1} for “ free ” by solving the LP relaxation .
The algorithm , called rLPIT for ( regularized ) binary rankone approximation is similar to LPIT and it consists of two phases : ( i ) find an error bounded solution , and ( ii ) apply the iterative procedure [ 13 ] to further improve the solution .
5 . EFFICIENT APPROXIMATION
In Sections 3 and 4 , we compute the binary matrix approximation by solving liner programs . However , solving the LP for a large matrix is still computationally expensive , eg , too many variables . In this section , we transform the problem into an instance of minimum s t cut problem , which can be solved efficiently by finding maximum flows . 5.1 Approximation by Minimum s t Cuts
Consider the Generalized Independent Set Problem ( GIS ) [ 6 ] : The input of GIS is ( i ) an undirected graph G = ( V , E ) , ( ii ) a nonnegative weight w(v ) for each vertex v ∈ V , and ( iii ) a nonnegative penalty p(e ) for each edge e ∈ E . The problem of GIS is to find a vertex subset S ⊆ V so as to maxv1,v2∈S , {v1,v2}∈E p(v1 , v2 ) . imize the gain : v∈S w(v ) −
Observation . ILP2 in Figure 4 defines an instance for
GIS , and the corresponding graph is bipartite :
1 . Every vertex x1i is in the same partite while a vertex x2j is in the other . There is an edge between x1i and x2j if Ai,j = 0 .
2 . The weight of a vertex x1i ( or x2j ) is 1−λ
2 j Ai,j ( or
1−λ 2 i Ai,j ) .
3 . By constraint ( 16 ) , zi,j = 1 if and only if x1i = 1 and x2j = 1 at optimality . Thus , the variable zi,j indicates whether both the endpoints of an edge {x1i , x2j} are in the solution set .
4 . The penalty of each edge {x1i , x2j} is 1 + λ .
5 . The values of entries in x1 and x2 of a solution for ILP2 indicate the solution set S ∈ V of vertices for GIS .
It follows that the objective function of this instance for GIS is the same as the one in Figure 4 . GIS is at least as difficult as maximum independent set problem ; consequently ,
GIS is NP hard . However , it has been shown that GIS for bipartite graphs can be solved exactly in polynomial time by transformation [ 6 ] to an instance of minimum st cut problem . Techniques in [ 6 ] implies the following transformation from problem at ( 11 ) to the maximum flow problem : Given A ∈ {0 , 1}I1×I2 and λ ∈ R+ , the input is reduced to a network G = ( V , E ) , a source s ∈ V , a destination t ∈ V , and a capacity c(e ) for each link e ∈ E as follows : Nodes : Create ( i ) a node set V1 of size I1 for rows in A , ( ii ) a node set V2 of size I2 for columns of A , and ( iii ) a node pair s and t . Let the node set V = V1∪V2∪{s , t} , where s and t are the source and destination nodes respectively .
Links : For each node pair vi ∈ V1 and vj ∈ V2 , create a link ( i , j ) ∈ E if Ai,j = 0 . Also create ( i ) a link ( s , vi ) ∈ E for each vi ∈ V1 and ( ii ) a link ( vj , t ) ∈ E for each vj ∈ V2 .
Capacities : Denote srow,i and scol,j as the sums of the row i and column j of A respectively . ( i ) For link ( s , vi ) , vi ∈ 2 × srow,i . ( ii ) For link ( vj , t ) , vj ∈ V1 , let c(s , vi ) = 1−λ 2 ×scol,j . ( iii ) For link ( vi , vj ) ∈ E , V2 , let c(vj , t ) = 1−λ where vi ∈ V1 and vj ∈ V2 , let c(vi , vj ) = 1 + λ .
For example , Figure 5(c ) depicts the network from the input matrix in Figure 5(a ) and λ = 1 2 .
Figure 5 : ( a ) A matrix consists of 4 data points and a regularization parameter 05 ( b ) The solution found by the minimum s t cut . ( c ) The network transformed from the input in ( a ) . ( d ) The residual network showing the cut and its assignment .
The network can be solved efficiently by any algorithm for finding maximum flows . The result partitions the node set V \ {s , t} into two sets S1 and S2 , where the destination t is reachable from every node v2 ∈ S2 in the final residual network but not for any node v1 ∈ S1 ( page 928 , Goldberg & Tarjan [ 3] ) . Figure 5(d ) shows the residual network as a result of applying a maximum flow algorithm on the network in Figure 5(c ) . The solution vectors x1 ∈ {0 , 1}I1 and x2 ∈ {0 , 1}I2 for
ILP2 is obtained from the two sets S1 and S2 as follows :
1 . The value of x1i is 1 if and only if its corresponding node vi ∈ V1 is in S1 .
2 . The value of x2j is 1 if and only if its corresponding node vj ∈ V2 is in S2 .
3 . All the other entries have values of zero .
Figure 5(b ) shows the corresponding assignments and the solution for the example .
6 . EXPERIMENTS
In this section , we present experimental results to evaluate the proposed algorithms . We present comparisons for the results by the minimum s t cut in Phase 1 ( P1 ) of the rLPIT algorithm , the improvement by the iterative process in Phase ( P2 ) of rLPIT , theoretical upper bounds , and running time of regularized rank one approximations by the minimum s t cut . All algorithms are implemented using C++ . 6.1 Experimental Setup
We use both the synthetic and real data in the experiments . We collected 3000 Drosophila gene expression pattern images of lateral view from three early developmental stage ranges , ie , 1 3 , 4 6 , 7 8 , from the FlyExpress database1 . The images from this database had gone through labor intensive segmentation , alignment , and enhancement . The collection contains 128× 320 black and white images of staining patterns and were subsampled down to 64 × 160 to reduce the computational cost for the purpose of this experiment . Each image is represented as a vector of length 10240 . We randomly generate binary matrices with the density of non zeros around 30 % , which is close to the average density of gene pattern expression images . We present the cases up to 1000×1000 in size , and 1000 cases for distinct size for nonregularization . For regularization , we varied the parameter λ from 0.0 up to 1.0 with an increment of 01 The optimal objective values for ILP2 were used to provide bounds for our test cases , since the optimal value of ( 12 ) is no more than that of ILP2 as summarized in Proposition 11 . For example , if the ratio by the minimum s t cut to ILP2 is 1.5 , then the exact ratio is 1.5 or less .
To figure out error ratios in acceptable time , ie , avoiding integer programming , the linear programming ( LP ) relaxation for ILP2 were solved , eg , x1i , x2j , zi,j ∈ [ 0 , 1 ] . Note that an optimal solution for the LP is also an optimal solution for ILP2 , mainly due to the total unimodularity of ILP2 ’s coefficient matrix , assuming a simplex method for the LP is applied . The implementation was coded in C ++ with the LP solvers lpsolve 2 , which has a simplex base algorithm . For large test cases , solving the corresponding LP ’s becomes a challenge . For example , there are one million variables or more for the size of 1000× 1000 . For such cases , the errors are compared with iterative heuristic , eg , with random initial vectors proposed in [ 14 ] . 6.2 Results on Rank one Approximation with out Regularization
Each chart of Figure 6 shows the histogram of an errorbound distribution for a given set of fixed size test cases . The bound is a ratio of the Hamming distance by solution from a minimum s t cut to the objective value of ILP2 . In a case that a distance by ILP2 is zero , the ratio is 1 because the distance by a minimum s t cut is also zero for all the test 1http://wwwflyexpressnet 2http://sourceforge.net/projects/lpsolve in S1in S2assign 1assign 01010X1=0111X2=1010000010101010X12X21X24X23X22X13X11X14X1X2 =T(b)(c)12StX13X11X14134343434X12X21X24X23X2212123232323232(d)tS11434141432321X14X21X24X23X22X13X1211X11(a)1110100111101011X12X21X24X23X22X13X11X14A=λ=12 , ( a ) 5 × 5 test cases
( b ) 20 × 20 test cases
( c ) 50 × 50 test cases
( d ) Mixed test cases
Figure 6 : Distributions of error bounds for the test cases without regularization . P1 and P2 correspond to the first and second phases of LPIT , respectively . P2 improves the result of P1 by the iterative procedure .
( a ) 5 × 5 test cases
( b ) 20 × 20 test cases
( c ) 50 × 50 test cases
( d ) 400 × 100 test cases
Figure 7 : Parameterized error bounds for the test cases with regularization . P1 and P2 correspond to the first and second phases of rLPIT , respectively . P2 improves the result of P1 by the iterative procedure . cases . All the figures indicate that none of the test cases has any ratio exceeding 2 . This is consistent with the result in Theorem 12 .
Figure 6(a ) depicts that the iterative procedure has negligible improvement on the solution quality in this case . Comparing this result with those of Figures 6(b ) and 6(c ) , we found that the role of iterative procedure is getting important as the input matrix gets complex . This is due to the lack of dominant patterns in randomly generated large matrices . In all those test cases , the mean of error bounds is steady in the range of 1.45 to 1.70 by the algorithm . The vector pair found by the minimum s t cut serves good starting points for the iterative procedure in those test cases . Figure 6(d ) shows the error ratios of the algorithm to the iterative heuristic given in [ 15 ] . 6.3 Results on Rank one Approximation with
Regularization
1+λ , 0 ≤ λ ≤ 1 . The bounds are tight for some
Figure 7 shows the solution quality with varying regularization parameters for the test cases . In all those figures , none of the test cases has any error ratio exceeding the theoretical bound 2 of the test cases . For highly complex matrices , the proposed algorithm obtains solutions with errors close to theoretical bounds , eg , the results in Figures 7(b ) and 7(c ) . The improvement by the iterative procedure appears less persistent for regularization . This phenomenon is likely due to shorter Hamming distances of optimal solutions as penalty gets very high for complex solutions . 6.4 Running Time Comparison
( a ) Change in features
( b ) Change in data points
Figure 8 : Results of running time tests . minimum s t cuts . We used the implementation of a maximum flow algorithm given in [ 1 ] , which is reported to have good average performance in the context of image applications . In our test , there were two scenarios , each we fixed one of the dimensions while incrementally changed the other . Any of the fixed dimensions has a size of 1000 . The regularization parameter was set to 05
Figure 8(a ) shows the running time for increasing the number of features . The running time is linear with respect to the change for our test cases . Figure 8(b ) shows the running time for increasing the number of data points . It is clear that solving a matrix with a higher dimension in data points is more costly by the algorithms , and such cost gets higher faster . However , we can always improve the performance by solving the transpose of such a matrix or simply switching the source and destination sides of the network . 6.5 Hierarchical Tree Visualization
We use the gene expression images for evaluating the running time of finding error bounded approximations using
A total of three trees , each for a stage range , were constructed . Each tree has 1000 leaf nodes , the number of input
1121416182050100150Error boundNumber of cases P1P2131415161718192020406080100120140160180Error boundNumber of cases P1P2155161651717518185191952050100150200250300350400450500Error boundNumber of cases P1P250x50100x100100x200200x100200x200400x400400x800800x400800x8001000x10000102030405060708091Size of a Test CaseError Ratio to the iterative hueristic001234567891011112131415161718192Regularization parameter ˇAverage error bound P1P2Theoretical upper bound001234567891011112131415161718192Regularization parameter ˇAverage error bound P1P2Theoretical upper bound001234567891011112131415161718192Regularization parameter ˇAverage error bound P1P2Theoretical upper bound001234567891011112131415161718192Regularization parameter ˇAverage error bound P1P2Theoretical upper bound010002000300040005000010203040506070Number of FeaturesRunning Time in Seconds050010001500200025003000010203040506070Number of Data PointsRunning Time in Seconds images for a stage range . Extracted patterns of a tree were represented as an image . To illustrate the typical traits of trees built by the proposed approximation algorithm , Figure 9 shows a result for a set of 40 input images from stage range 4 6 . We observe that the shape of a tree relies on the regularization parameter λ and the degree of variation of the input images . When λ = 0 , ie , without regularization as in [ 13 , 15 ] , both x1 and x2 tend to be either all zeros or all ones , which is undesirable for tree construction . In our experiments , we set λ = 04 We can observe from the figure that images from the same branch share similar patterns .
7 . CONCLUSION
In this paper , we study the problem of computing rankone binary matrix approximations , which have been previously used for subsampling , compression , and clustering . Specifically , we reformulate the rank one binary matrix approximation problem as a maximum weight problem , based on which we propose a linear program ( LP ) formulation , which is shown to achieve a guaranteed approximation error bound . We further extend the proposed formulations using the regularization technique . In addition , we show that the proposed approximate formulation can be transformed into an instance of minimum s t cut problem , which can be solved efficiently by finding maximum flows . Our empirical evaluation shows the efficiency of the proposed algorithm based on minimum s t cuts . Results also confirm the consistency and tightness of the established theoretical bounds .
As a sample application , we apply the proposed algorithm for the hierarchy construction and pattern discovery for Drosophila gene expression pattern images . We plan to examine the biological significance of the resulting hierarchy . We plan to apply the proposed algorithm to other applications involving binary data . We are currently investigating how the solutions of the proposed algorithm depend on the regularization parameter as well as its estimation .
Acknowledgments This work was supported by NSF IIS 0612069 , IIS 0812551 , CCF0811790 , NIH R01 HG002516 , and NGA HM1582 08 1 0016 .
8 . REFERENCES [ 1 ] Y . Boykov and V . Kolmogorov . An experimental comparison of min cut/max flow algorithms for energy minimization in vision . IEEE TPAMI , Sep 2004 .
[ 2 ] G . Dantzig . Maximization of a linear function of variables subject to linear inequalities . In Activity Analysis of Production and Allocation . Wiley , 1951 .
[ 3 ] A . V . Goldberg and R . E . Tarjan . A new approach to the maximum flow problem . J . ACM , 35(4):921–940 , 1988 .
[ 4 ] G . H . Golub and C . F . Van Loan . Matrix Computations .
The Johns Hopkins University Press , Baltimore , MD , USA , 3rd edition , 1996 .
[ 5 ] I . Heller and C . B . Tompkins . An extension of a theorem of
Dantzig ’s . Ann . of Math . Stud . , no . 38 , pages 247–254 . 1956 .
[ 6 ] D . S . Hochbaum and A . Pathria . Forest harvesting and minimum cuts : a new approach to handling spatial constraints . Forest Science , 43(4):544–554 , 1997 .
[ 7 ] A . J . Hoffman and J . B . Kruskal . Integral boundary points of convex polyhedra . Annals of Mathematics Studies , no . 38 , pages 223–246 . Princeton University Press , 1956 .
[ 8 ] I . T . Jolliffe . Principal Component Analysis .
Springer Verlag , New York , 1986 .
Figure 9 : The hierarchy and patterns for a set of 40 images from developmental stage range 4 6 .
[ 9 ] J . A . Kelner and D . A . Spielman . A randomized polynomial time simplex algorithm for linear programming . In ACM STOC 2006 , pages 51–60 , 2006 .
[ 10 ] L . G . Khachiyan . A polynomial algorithm in linear programming [ in russian ] . Doklady Akademii Nauk SSSR , 244:1093–1096 , 1979 . English translation : Soviet Mathematics Doklady 20 ( 1979 ) , 191 194 .
[ 11 ] V . Klee and G . J . Minty . How good is the simplex algorithm ? In Inequalities , III , pages 159–175 . Academic Press , New York , 1972 .
[ 12 ] T . G . Kolda and D . P . O’Leary . A semidiscrete matrix decomposition for latent semantic indexing information retrieval . ACM Trans . Inf . Syst . , 16(4):322–346 , 1998 .
[ 13 ] M . Koyut¨urk and A . Grama . PROXIMUS : a framework for analyzing very high dimensional discrete attributed datasets . In ACM SIGKDD , pages 147–156 , 2003 .
[ 14 ] M . Koyut¨urk , A . Grama , and N . Ramakrishnan .
Compression , clustering , and pattern discovery in very high dimensional discrete attribute data sets . IEEE TKDE , 17(4):447– 461 , April 2005 .
[ 15 ] M . Koyuturk , A . Grama , and N . Ramakrishnan .
Nonorthogonal decomposition of binary matrices for bounded error data compression and analysis . ACM Trans . Math . Softw . , 32(1):33–69 , 2006 .
[ 16 ] K . Nishino , Y . Sato , and K . Ikeuchi . Eigen texture method : appearance compression based on 3d model . In IEEE CVPR 1999 , pages 618–624 , 1999 .
[ 17 ] D . A . Spielman and S H Teng . Smoothed analysis of algorithms : Why the simplex algorithm usually takes polynomial time . Journal of ACM , 51(3):385–463 , 2004 .
[ 18 ] M . Turk and A . Pentland . Eigenfaces for recognition . Journal of Cognitive Neuroscience , 3(1):71–86 , 1991 . insitu21634_sjinsitu21764_sjinsitu21768_sjinsitu21635_sjbdgp00002076_sjbdgp00002077_sjbdgp00002081_sjinsitu13462_sjinsitu13466_sjinsitu13467_sjinsitu13468_sjinsitu13471_sjinsitu13461_sjinsitu22877_sjinsitu22876_sjinsitu13460_sjinsitu13463_sjinsitu22875_sjinsitu22878_sjinsitu22882_sjinsitu31752_sjinsitu31753_sjinsitu31736_sjinsitu31738_sjinsitu31734_sjinsitu31737_sjinsitu14896_sjinsitu13455_sjbdgp00003082_sjinsitu18539_sjinsitu18540_sjbdgp00003081_sjbdgp00003083_sjbdgp00003084_sjbdgp00003086_sjinsitu14891_sjinsitu14892_sjinsitu28403_sjbdgp00002200_sjinsitu28404_sjinsitu28405_sjbdgp00000042_sjbdgp00001457_sjbdgp00001458_sjbdgp00002201_sjHierarchical StructureImage SetImage ID ’s 10101010101010101010101010Images of the patterns
