Large Scale Sparse Logistic Regression
Jun Liu
Arizona State University
Tempe , AZ 85287 jliu@asuedu
Jianhui Chen
Arizona State University
Tempe , AZ 85287 jianhuichen@asuedu
Jieping Ye
Arizona State University
Tempe , AZ 85287 jiepingye@asuedu
ABSTRACT Logistic Regression is a well known classification method that has been used widely in many applications of data mining , machine learning , computer vision , and bioinformatics . Sparse logistic regression embeds feature selection in the classification framework using the .1 norm regularization , and is attractive in many applications involving high dimensional data . In this paper , we propose Lassplore for solving Large scale sparse logistic regression . Specifically , we formulate the problem as the .1 ball constrained smooth convex optimization , and propose to solve the problem using the Nesterov ’s method , an optimal first order black box method for smooth convex optimization . One of the critical issues in the use of the Nesterov ’s method is the estimation of the step size at each of the optimization iterations . Previous approaches either applies the constant step size which assumes that the Lipschitz gradient is known in advance , or requires a sequence of decreasing step size which leads to slow convergence in practice . In this paper , we propose an adaptive line search scheme which allows to tune the step size adaptively and meanwhile guarantees the optimal convergence rate . Empirical comparisons with several state of theart algorithms demonstrate the efficiency of the proposed Lassplore algorithm for large scale problems .
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications Data Mining General Terms Algorithms Keywords Logistic regression , sparse learning , .1 ball constraint , Nesterov ’s method , adaptive line search
1 .
INTRODUCTION
Logistic Regression ( LR ) [ 18 , 23 ] is a classical classification method that has been used widely in many applications includ ing document classification [ 4 , 5 ] , computer vision [ 9 ] , natural language processing [ 14 , 22 ] , and bioinformatics [ 2 , 20 , 29 , 34 , 35 ] . For applications with many features but limited training samples , LR is prone to overfitting . Regularization is commonly applied to reduce overfitting and obtain a robust classifier . The .2 norm regularization has been used extensively in the LR model , leading to a smooth ( differentiable ) unconstrained convex optimization problem . Standard optimization algorithms such as Newton method and conjugate gradient method [ 3 , 23 ] can be applied for solving such a formulation . Recently , there is a growing interest in applying the .1 norm regularization in the LR model . The use of the .1 norm regularization has long been recognized as a practical strategy to obtain a sparse model . The .1 norm regularized sparse LR model is attractive in many applications involving high dimensional data in that it performs feature selection and classification simultaneously [ 6 , 10 , 11 , 16 , 18 , 26 , 28 , 30 , 31 ] .
Solving the .1 regularized logistic regression is , however , more challenging than solving the .2 regularized counterpart , since the regularization term is non differentiable . Many algorithms have been proposed in the past for solving the .1 regularized logistic regression . The iteratively reweighted least squares least angle regression algorithm ( IRLS LARS ) used a quadratic approximation for the average logistic loss function , which was subsequently solved by the LARS method [ 7 , 18 ] . The Bayesian logistic regression ( BBR ) algorithm used a cyclic coordinate descent method for the Bayesian logistic regression [ 8 ] . Glmpath is a general solver for the .1 regularized generalized linear models using path following methods [ 27 ] ; it can solve the .1 regularized logistic regression . SMLR is another general solver for various sparse linear classifiers [ 17 ] ; and it can solve the sparse logistic regression . In [ 16 ] , an interior point method was proposed for solving the .1 regularized logistic regression . Recently , an algorithm based on the fixed point continuation algorithm [ 13 ] was proposed to solve the .1 regularized logistic regression [ 32 ] . An extensive comparison among twelve sparse logistic regression algorithms was given in [ 30 ] .
In this paper , we propose the Lassplore algorithm for solving large scale sparse logistic regression . Specifically , we formulate the problem as the .1 ball constrained logistical regression formulation , in which the objective function is continuously differentiable , and the problem domain set is closed and convex . We further propose to solve this problem using the Nesterov ’s method , which has the optimal convergence rate among the first order methods . One of the critical issues in the use of the Nesterov ’s method is the estimation of an appropriate step size in each of the optimization iterations . One simple approach is to apply a constant step sizes , which assumes that the Lipschitz gradient is given in advance . Another approach is to estimate the appropriate step size via the inexact line search scheme ; however , such a scheme generates a se quence of decreasing step size and may result in slow convergence in practice . We propose to make use of the Nesterov ’s estimate sequence for deriving a specialized adaptive line search scheme , in which the appropriate step size is tuned adaptively for each of the optimization iteration . The proposed line search scheme can lead to improved efficiency in practical implementations while preserving the optimal convergence rate . We have performed experimental studies using a collection of large scale data sets . Empirical comparisons with several state of the art algorithms demonstrate the efficiency of the proposed Lassplore algorithm for the large scale sparse logistic regression problems .
Organization : We introduce sparse logistic regression in Section 2 , review the Nesterov ’s method in Section 3 , derive an adaptive line search scheme in Section 4 , present the Lassplore algorithm in Section 5 , report empirical studies in Section 6 , and conclude this paper in Section 7 .
2 . SPARSE LOGISTIC REGRESSION
Let a ∈ Rn denote a sample , and b ∈ {−1 , +1} be the associ ated ( binary ) class label . Logistic regression model is given by :
1
Prob(b|a ) =
,
1 + exp(−b(wT a + c ) )
( 1 ) where Prob(b|a ) is the conditional probability of the label b , given the sample a , w ∈ Rn is the weight vector , and c ∈ R is the intercept . wT a + c = 0 defines a hyperplane in the feature space , on which Prob(b|a ) = 05 The conditional probability Prob(b|a ) is larger than 0.5 if wT a + c has the same sign as b , and less than 0.5 otherwise . Suppose that we are given a set of m training data {ai , bi}m i=1 , where ai ∈ Rn denotes the i th sample and bi ∈ {−1 , +1} denotes the corresponding class label . The likelihood function assoi=1 Prob(bi|ai ) . The ciated with these m samples is defined as negative of the log likelihood function is called the ( empirical ) logistic loss , and the average logistic loss is defined as :
Q m mY
Prob(bi|ai ) i=1 log(1 + exp(−bi(wT ai + c)) ) ,
( 2 ) f ( w , c ) = − 1 mX m log
=
1 m i=1 which is a smooth and convex function . We can determine w and c by minimizing the average logistic loss : f ( w , c ) ,
( 3 ) min w,c leading to a smooth convex optimization problem .
When m , the number of training samples is smaller than n , the dimension of the samples , directly solving the logistic regression formulation in ( 3 ) is ill posed and may lead to overfitting . A standard technique to avoid overfitting is regularization . 2.1 .1 Ball Constrained Logistic Regression
When adding an .1 norm regularization to f ( w , c ) , we obtain the .1 norm regularized logistic regression problem : f ( w , c ) + ||w||1 , min w,c
( 4 ) where > 0 is a regularization parameter . The solution to the .1norm regularized logistic regression can be interpreted in a Bayesian framework as the maximum a posteriori probability estimate of w and c , when w has a Laplacian prior distribution on Rn and covariance I , and c has the uniform prior on R .
Since the .1 regularization term is nonsmooth ( non differentiable ) , solving the .1 norm regularized logistic regression is much more challenging than solving the .2 norm case . Despite of its computational challenge , the .1 norm regularized logistic regression has recently received great interests , due to the sparseness of the resulting model and its empirical success [ 11 , 16 , 18 , 28 , 30 , 31 ] . There are several strategies for solving ( 4 ) . The first strategy is to treat ( 4 ) as a nonsmooth optimization problem , and then solve it by subgradient based algorithms [ 24 ] , eg , Gauss Seidel [ 31 ] , grafting [ 28 ] , shooting [ 11 ] , BBR [ 8 ] . The second strategy is to apply some smooth approximation to the .1 norm , so that ( 4 ) can be approximated by a smooth function , which can then be solved by smooth optimization methods , eg , the smoothl1 [ 30 ] . The third strategy is to introduce additional variables to reformulate ( 4 ) as a smooth optimization problem with smooth constraint functions , eg , l1 logreg [ 16 ] and ProjectionL1 [ 30 ] . The fourth strategy is to convert ( 4 ) to the equivlalent .1 ball constrained optimization problem with a smooth objective function , eg the iteratively reweighted least squares ( IRLS LARS ) [ 18 ] .
The Lassplore algorithm proposed in this paper belongs to the fourth category . Specifically , we convert the problem ( 4 ) to its equivalent counterpart , the .1 ball constrained logistic regression : min w,c f ( w , c )
( 5 ) for some value of z ≥ 0 , the radius of the 1 ball It is known that there is a one to one correspondence between ( 4 ) and ( 5 ) . subject to ||w||1 ≤ z ,
We can also employ two types ( .1 and .2 ) of regularization by solving the following optimization problem ρ f ( w , c ) + 2 subject to ||w||1 ≤ z . min w,c ffwff2
( 6 )
The problem ( 6 ) reduces to : ( i ) the logistic regression ( 3 ) , when setting ρ = 0 and z a sufficiently large value ; ( ii ) the .2 norm regularized logistic regression , when setting ρ > 0 and z a sufficiently large value ; and ( iii ) the .1 ball constrained logistic regression , when setting ρ = 0 . In the following discussion , we focus on solving the more general formulation ( 6 ) . 2.2 Function Value and Gradient Evaluation Our proposed Lassplore algorithm is a first order black box oracle method that evaluates at each iteration the function value and ffwff2 are easy to comgradient . In ( 6 ) , the value and gradient of ρ 2 pute , so we focus on f ( w , c ) in the following discussion . Let b = [ b1 , b2 , . . . , bm]T ∈ Rm , 1 ∈ Rn be the vector of all ones , A = [ b1a1 , b2a2 , . . . , bmam]T ∈ Rm×n , and c ∈ Rm be an ( w , c ) = m dimensional vector with all entries being c . Denote f [ ∇wf ( w , c)T ,∇cf ( w , c)]T . We have
.
( 7 )
( 8 )
∇cf ( w , c ) = − 1 m ∇wf ( w , c ) = − 1 m bT ( 1 − p ) , AT ( 1 − p ) , p = 1./(1 + exp(−Aw − b c) ) ,
( 9 ) where pi = Prob(bi|ai ) is the conditional probability of bi given ai , denotes the componentwise multiplication , and ./ denotes componentwise division . We can compute f ( w , c ) as f ( w , c ) = − 1 m
1T log(p ) .
( 10 )
From ( 7) (10 ) , we see that only the matrix vector multiplications involving A and AT are required for computing the objective and the gradient . Thus , when A is sparse , we can efficiently deal with sparse logistic regression problems with large m and n .
Since our proposed Lassplore algorithm is built on the Nesterov ’s method with an adaptive line search scheme , we first review the Nesterov ’s method in Section 3 , and derive an adaptive line search scheme for the Nesterov ’s method in Section 4 . Note that , the Nesterov ’s method can deal with constrained optimizations by the usage of gradient mapping [ 25 ] . For convenience of illustration , we first focus on the unconstrained smooth convex optimization in Sections 3 & 4 , and then discuss the extension to the constrained optimization in Section 5 . The discussions in Sections 3 & 4 are applicable to the general smooth convex optimization .
3 . THE NESTEROV’S METHOD
Let us consider the following unconstrained smooth convex min imization problem : x∈Rn g(x ) , min
( 11 ) where the function g(x ) belongs to the class of convex and differential family S 1,1
μ,L(Rn ) , with L and μ satisfying
L ≡ max x'=y μ ≡ min x'=y ffg g
.
.
( x ) − g . ffx − yff ( x ) − g .
( y)ff ( y ) , x − yff
< +∞ , ffx − yff2
≥ 0 .
( 12 )
( 13 )
.
( x ) fi LIn,∀x ,
L defined in ( 12 ) is called the Lipschitz gradient of the function g(x ) . μ defined in ( 13 ) is nonnegative , as the gradient g ( . ) is monotone when g(x ) is convex . Moreover , when μ > 0 , g(x ) is called a strongly convex function . When the function g(x ) is twice differentiable , we have μIn fi g
( 14 ) where In is an n× n identity matrix , and A fi B indicates that the matrix B − A is positive semidefinite . The inequality ( 14 ) implies ( x),∀x ; that , L is the upper bound of the largest eigenvalue of g and similarly μ is the lower bound of the smallest eigenvalue of ( x),∀x . It is clear that the relationship μ ≤ L always holds . g In the following discussion , we denote x∗ as an optimal solution and the optimal objective function value , respectively . 3.1 The Algorithm The Nesterov ’s method utilizes two sequences : {xk} and {sk} , where {xk} is the sequence of approximate solutions , and {sk} is the sequence of searching points . The searching point sk is the affine combination of xk−1 and xk as and g sk = xk + βk(xk − xk−1 ) ,
( 15 ) where βk is a tuning parameter . The approximate solution xk+1 can be computed as a gradient step of sk as
∗ xk+1 = sk − 1 Lk
. g
( sk ) ,
( 16 ) where 1/Lk is the step size . Fig 1 illustrates how the Nesterov ’s method works . Starting from an initial point x0 , we compute sk and xk+1 recursively according to ( 15 ) and ( 16 ) , and arrive at the optimal solution x∗ In the Nesterov ’s method , βk and Lk are two key parameters . When they are set properly , the sequence {xk} can converge to the optimal x∗ at a certain convergence rate . The Nesterov ’s constant scheme [ 25 ] and the Nemirovski ’s line search scheme [ 24 ] are two well known ones for setting βk and Lk .
.
1 , 1 , 0
2
2 k ( cid:533)k k k 1 k g k Lk k k+1
3
6
3
4
7
4
5
6
5
Figure 1 : Illustration of the Nesterov ’s method . We set x1 = x0 , and thus s1 = x1 . The search point sk is the affine combination of xk−1 and xk ( the dashed lines ) , and the next approximate solution is obtained by a gradient step of sk ( solid lines ) . We assume that x6 = x7 = x∗ is an optimal solution .
, and x∗
The Nesterov ’s constant scheme assumes that both L and μ are known in advance , and it sets Lk = L and βk according to L and μ . It has been shown that the resulting scheme can achieve the lower complexity bounds ( in the same order ) for the class S 1,1 μ,L(Rn ) by first order black box methods , and thus the Nesterov ’s method is an optimal first order black box method .
Although the Nesterov ’s constant scheme can achieve the lower complexity bounds , one major limitation is that both L and μ need to be known in advance , which is however not the case in many applications ( eg , sparse logistic regression ) . Moreover , the exact computation of L and μ might be much more challenging than solving the problem itself . 3.2 The Nemirovski ’s Line Search Scheme
To address the problem that L is usually unknown in advance , Nemirovski [ 24 ] proposed a line search scheme for determining Lk ( see Algorithm 1 ) . In this scheme , we first initialize Lk with Lk−1 ( see Step 2 ) , and then apply a line search process ( Steps 4 11 ) for finding the Lk that satisfies the condition in Step 6 . It is clear that , the sequence Lk is non decreasing with increasing k . Moreover , Lk is upper bounded by 2L since once Lk ≥ L , the condition in Step 6 always holds [ 24 , Chapter 10.2 , page 163 ] . For βk , it is computed based on the sequence {tk} . As the sequence {tk} is independent of L , μ and the function g(x ) , the sequence {βk} is identical for all the ( smooth convex ) optimization problems . and Lk = Lk−1
Algorithm 1 The Nemirovski ’s Line Search Scheme Input : L0 > 0 , x1 = x0 , N , t−1 = 0 , t0 = 1 Output : xN 1 : for k = 1 to N do Set βk = tk−2−1 2 : tk−1 Compute sk = xk + βk(xk − xk−1 ) 3 : for j = 1 to . . . do 4 : 5 : 6 : 7 : 8 : 9 : 10 : 11 : 12 : 13 : end for
Compute xk+1 = sk − 1 if g(xk+1 ) ≤ g(sk ) − 1 end if end for Set tk = ( 1 +
. ( sk ) g ffg ( sk)ff2 then . goto Step 12
Lk = 2Lk k−1)/2
1 + 4t2 q else
2Lk
Lk
THEOREM 1 . [ 24 , Chapter 10.2 , pages 163 165 ] Let Lg = max(2L , L0 ) and Rg be the distance from the starting point to the optimal solution set . Then for Algorithm 1 , we have g(xN+1 ) − g
∗ ≤ 2LgR2 g
( N + 1)2 .
( 17 )
Theorem 1 shows that the scheme presented in Algorithm 1 is optimal for the class S 1,1 0,L(Rn ) . However , there are two limitations : ( i ) it cannot achieve the Q linear rate [ 25 ] for the class S 1,1 μ,L(Rn ) even when μ > 0 is known ; and ( ii ) the step size 1/Lk is only allowed to be monotonically decreasing ( note , in proving the convergence rate in Theorem 1 , the relationship Lk ≤ Lk+1 is explicitly enforced [ 24 , Chapter 10.2 , page 165] ) . In the next section , we shall propose an adaptive line search scheme that can avoid these limitations .
4 . AN ADAPTIVE LINE SEARCH SCHEME In this section , we propose an adaptive line search scheme for the Nesterov ’s method . Our line search scheme is built upon the estimate sequence [ 25 , Chapter 2.2 ] , which will be reviewed in Section 41 In our line search scheme , we do not assume that L and μ are known in advance , but we assume that , ˜μ , the lower bound of μ is known in advance . This assumption is reasonable , since 0 is always a lower bounded of μ as shown in ( 13 ) . Moreover , for the sparse logistic regression formulation in ( 6 ) of Section 2.1 , we have μ ≥ ρ . The proofs follow similar arguments in [ 25 ] , and are given in the Appendix . 4.1 Estimate Sequence
Definition 1 . [ 25 , Chapter 2.2 ] A pair of sequences {φk(x)} and {λk ≥ 0} is called an estimate sequence of the function g(x ) if the following two conditions hold : k→∞ λk = 0 , lim φk(x ) ≤ ( 1 − λk)g(x ) + λkφ0(x),∀x ∈ Rn .
( 18 )
( 19 )
The following theorem provides a systematic way for construct ing the estimate sequence :
THEOREM 2 . [ 25 , Chapter 2.2 ] 1 Let us assume that :
1 . g(x ) is smooth and convex , with Lipschitz gradient L and strongly convexity parameter μ . Moreover , we know the value of ˜μ , which satisfies μ ≥ ˜μ ≥ 0 .
2 . φ0(x ) is an arbitrary function on Rn . 3 . {sk} is an arbitrary searching sequence on Rn . P∞ k=0 αk = ∞ . 4 . {αk} satisfies : αk ∈ ( 0 , 1 ) and 5 . λ0 = 1 .
Then {φk(x ) , λk} defined by the recursive rules :
λk+1 = ( 1 − αk)λk ,
φk+1(x ) = ( 1 − αk)φk(x ) + αk[g(sk ) + ffx − skff2
( sk ) , x − skff + g
.
˜μ 2 is an estimate sequence .
If we choose a simple quadratic function for φ0(x ) as φ0(x ) = ffx − v0ff2 , then we can specify the estimation sequence ∗ 0 + γ0 2
φ defined in Theorem 2 as [ 25 ] : ∗ k +
φk(x ) = φ ffx − vkff2
,
γk 2
( 22 )
1Compared to the theorem proposed in [ 25 ] , we employ ˜μ in ( 21 ) rather than μ , where ˜μ is a known lower bound of μ . We make such a substitution for cases where μ is unknown . then , the condition in ( 26 ) holds .
( 20 )
Next , we show in the following lemma that sk in ( 29 ) can be simplified as the combination of xk−1 and xk :
]
( 21 )
LEMMA 2 . The search point given in ( 29 ) can be computed by sk+1 = xk+1 + βk+1(xk+1 − xk ) , where
βk+1 =
γk+1(1 − αk )
αk(γk+1 + Lk+1αk+1 )
.
Based on the results in Lemmas 1 & 2 , we propose our adaptive line search scheme shown in Algorithm 2 . The while loop from Step 2 to Step 11 determines whether Lk should be increased , so that the condition ( 31 ) in Step 6 holds . Note that , like the Nemirovski ’s line search scheme , Lk is upper bounded by 2L , since where the sequences {γk} , {vk} and {φ
ˆ
} satisfy : ( 1 − αk)γkvk + ˜μαksk − αkg
∗ k
1
γk+1 vk+1 = γk+1 = ( 1 − αk)γk + αk ˜μ , k+1 = ( 1 − αk)φ ∗ „
φ
αk(1 − αk)γk k + αkg(sk ) − α2 ∗ 2γk+1 ffsk − vkff2 ffg + g k
.
+
( sk)ff2 ( sk ) , vk − skff .
«
.
˜μ 2
γk+1
˜
,
.
( sk )
( 23 )
( 24 )
The estimate sequence defined in Definition 1 has the following
( 25 ) important property :
THEOREM 3 . [ 25 , Chapter 2.2 ] Let {φk(x)} and {λk ≥ 0} be an estimate sequence . For any sequence {xk} , if x∈Rn φk(x ) , g(xk ) ≤ φ k ≡ min ∗ we have g(xk ) − g
∗ ≤ λk[φ0(x∗
) − g
∗
] → 0 .
4.2 Proposed Adaptive Line Search Scheme
Our proposed line search scheme is based on Theorem 3 . Specifically , we aim at looking for the approximate solution sequence {xk} ( generated with the adaptive step size 1 ) that satisfies the condition in ( 26 ) , so that the convergence rate of the solution sequence can be analyzed with the sequence {λk} , according to Theorem 3 .
Lk
We first show how to satisfy the condition ( 26 ) in the following
Lemma :
LEMMA 1 . Let v0 = x0 and φ
∗ 0 = g(x0 ) . If we compute the approximate solution xk+1 and searching point sk by xk+1 = sk − 1 Lk
. g
( sk ) , sk =
αkγkvk + γk+1xk
γk + αk ˜μ
,
( 28 )
( 29 ) where vk is updated according to ( 23 ) , and αk , γk , Lk , sk and xk+1 satisfy k = γk+1 = ( 1 − αk)γk + αk ˜μ , ( sk)ff2 ≥ g(xk+1 ) ,
2 Lkα g(sk ) − 1 2Lk ffg
.
( 26 )
( 27 )
( 30 )
( 31 )
( 32 )
( 33 )
( 31 ) always holds when Lk ≥ L . In Step 12 , we initialize Lk+1 as Lk+1 = Lk · h(τ ) . Here , τ = 2Lk ( g.(sk )(2 ≥ 1 , due to the g(sk )−g(xk+1 ) condition in Step 6 . Intuitively , when τ is large , the step size 1 Lk used in computing xk+1 as the gradient step of sk is small . In this paper , we employ the following simple piecewise linear function :
( h(τ ) =
1 , 1 ≤ τ ≤ 5 0.8 ,
τ > 5 .
( 34 )
Thus , Lk+1 is reduced to 0.8Lk when τ is large . Our experiments show that this particular choice of h( . ) works well . We set βk according to ( 33 ) . It is clear that βk is dependent on Lk . while 1 do
Algorithm 2 An Adaptive Line Search Scheme Input : ˜μ , α−1 = 0.5 , x−1 = x0 , L0 = L−1 , γ0 ≥ ˜μ , λ0 = 1 Output : xN 1 : for k = 0 to N do 2 : 3 :
Compute αk ∈ ( 0 , 1 ) as the root of Lkα2 k = ( 1−αk)γk+ αk ˜μ , γk+1 = ( 1 − αk)γk + αk ˜μ , βk = γk(1−αk−1 ) Compute sk = xk + βk(xk − xk−1 ) Set xk+1 = sk − 1 . ( sk ) if g(xk+1 ) ≤ g(sk ) − 1 ffg
( sk)ff2 then
αk−1(γk+Lk αk )
Lk g
.
2Lk else
4 : 5 : 6 : 7 : 8 : 9 : 10 : 11 : 12 : 13 : 14 : end for goto Step 12
Lk = 2Lk end if
Recall that the step size 1 Lk is only allowed to be monotonically decreasing , ie , Lk ≤ Lk+1 , in the Nemirovski ’s scheme . We show that although the step size is allowed to decrease , the proposed line search scheme preserves the convergence property , as summarized in the following theorem :
THEOREM 4 . For Algorithm 2 , we have
8< :ΠN
λN ≤ min r
) ,
˜μ Lk k=1(1 − h g(x0 ) − g
∗ ≤ λN and g(xN ) − g
P
1
N k=1 q
1 2
( 1 +
9= ; ,
γ0 Lk
)2 i
5 . THE PROPOSED APPROACH
We are ready to present the Lassplore algorithm for solving ( 6 ) . We first discuss the gradient mapping for dealing with the constrained optimization in Section 5.1 , and then present the Lassplore algorithm in Section 52
5.1 Gradient Mapping
The constrained optimization problem in ( 6 ) is a special case of the following constrained smooth convex optimization : g(x ) , min x∈G
( 37 ) where G is a convex set . In ( 6 ) , G is the 1 ball
To deal with the constrained optimization problem ( 37 ) , we construct the gradient mapping , which acts a similar role as the gradient in unconstrained optimization . Let Lx > 0 . We define ffy − xff2 gLx,x(y ) = g(x ) + g
( x ) , y − xff +
,
.
Lx 2 which is the tangent line of g( . ) at x , regularized by the square distance between y and x . Minimizing gLx,x(y ) in the domain G is the problem of Euclidean projections onto G : πG(x − 1 ffy − ( x − 1 1 Lx Lx 2 gLx,x(y ) .
( x ) ) ≡ arg min y∈G = arg min y∈G
( x))ff2
( 38 ) f f
.
. the “ gradient mapping" of g( . ) on G . From ( 39 ) , we have
πG(x − 1 Lx
.
( x ) ) = x − 1 Lx f p(Lx , x ) , which shows that , πG(x − 1 ( x ) ) can be viewed as the result Lx f of the “ gradient" step in the anti direction of the gradient mapping p(Lx , x ) with stepsize 1
.
With the gradient mapping , the discussions in Sections 3 & 4 can be extended to solve the constrained optimization problem ( 37 ) . Moreover , the constrained problem has the same convergence rate as the unconstrained one [ 25 , Chapter 223 ]
Lx . end while Set τ = 2Lk ( g.(sk )(2 Set λk+1 = ( 1 − αk)λk g(sk)−g(xk+1 )
, Lk+1 = h(τ )Lk
We call p(Lx , x ) = Lx(x − πG(x − 1 Lx
. f
( x) ) )
( 39 )
( 35 )
5.2 The Lassplore Algorithm
∗
+
γ0 2 ffx0 − x∗ff2
.
( 36 )
From ( 35 ) , we can observe that , the smaller Lk is , the smaller λN is . Therefore , by decreasing the value of Lk , we can accelerate the convergence .
Next , we compare our proposed line search scheme with existing ones . The proposed scheme is clearly different from Nesterov ’s constant scheme , as we assume that the Lipschitz gradient is not known in advance . It differs from the Nemirovski ’s scheme in the following aspects . First , Lk is allowed to decrease in our scheme , which is not the case in the Nemirovski ’s scheme . Second , in our scheme , βk is dependent on Lk , while βk in the Nemirovski ’s scheme is independent on Lk . Third , the Nemirovski ’s scheme can only achieve the convergence rate of O( 1 N2 ) , while our method can achieve the Q linear convergence rate [ 25 ] in the strongly convex case , if ˜μ ( >0 ) , the lower bound of μ is known .
2 ffwff2 . k]T . We also note that g(w , c ) = f ( w , c ) + ρ
In this subsection , we present the proposed Lassplore algorithm for solving ( 6 ) . For convenience of illustration , we denote the approximate solution xk = [ wT k , ck]T , and searching point sk = [ (sw k )T , sc Algorithm 3 is an application of Algorithm 2 to solve the sparse logistic regression problem . Next , we point out the main differences . First , due to the .1 ball constraint , in Step 5 , wk+1 is computed by the Euclidean projection πG( ) In our problem , G = {x ∈ Rn| ffxff1 ≤ z} is the .1 ball , and thus πG( . ) is the Euclidean projection onto the 1 ball We make use of method proposed in [ 21 ] for computing the projection . Second , the condition in Step 6 is replaced with g(xk+1 ) ≤ g(sk ) + g ( sk ) , xk+1 − skff + Lk ||xk+1 − sk||2 . When this condition holds , we say that Lk is “ appropriate" for sk [ 24 , Chapter 11 ] . Due to such a change , we also revise the computation of τ in Step 12 .
.
2
By the similar analysis , we can extend Algorithm 1 to solve the sparse logistic regression problem . while 1 do
Algorithm 3 Lassplore : Large Scale Sparse Logistic Regression Input : ˜μ = ρ , z > 0 , α−1 = 0.5 , L0 , γ0 ≥ ˜μ , x−1 = x0 Output : x 1 : for k = 0 to . . . do 2 : 3 :
Compute αk ∈ ( 0 , 1 ) as the root of Lkα2 k = ( 1−αk)γk+ αk ˜μ , γk+1 = ( 1 − αk)γk + αk ˜μ , βk = γk(1−αk−1 ) Compute sk = xk + βk(xk − xk−1 ) − 1 Compute wk+1 = πG(sw k ) ) k ∇cg(sw k , sc ck+1 = sc k ) k if g(xk+1 ) ≤ g(sk)+g ( sk ) , xk+1−skff+ Lk . sk||2 then
||xk+1−
∇wg(sw
αk−1(γk+Lk αk )
4 : 5 :
6 :
− 1 k , sc
Lk
Lk
2 goto Step 12
Lk = 2Lk end if else
7 : 8 : 9 : 10 : 11 : 12 : 13 : 14 : end if 15 : 16 : end for
Lk||xk+1−sk||2 end while Set τ = if convergence criterion is satisfied then x = xk+1 and terminate the algorithm
/2 g(xk+1)−g(sk )−)g.(sk ),xk+1−sk , Lk+1 = h(τ )Lk
Table 1 : Statistics of the test data sets . m denotes the sample size , and n denotes the data dimensionality .
Data set colon cancer leukemia duke breast cancer rcv1 real sim news20 m 62 38 44 20,242 72,309 19,996 n # nonzeros 124,000 270,902 313,676 1,498,952 3,709,083 9,097,916
2,000 7,129 7,129 47,236 20,958 1,355,191
6 . EMPIRICAL EVALUATIONS
We have performed experimental studies to evaluate the scalability of the proposed algorithm using the following six data sets : colon cancer ( colon ) [ 1 ] , leukemia ( leu ) [ 12 ] , duke breast cancer ( duke ) [ 33 ] , rcv1 [ 19 ] , real sim , and news20 [ 15 ] . The statistics of the test data sets are given in Table 1 ( for rcv1 , real sim , and news20 , we use a total of 2,000 samples in the following experiments ) . All experiments were carried out on an Intel ( R ) ( T2250 ) 1.73GHZ processor . The source codes are available online2 . 6.1 Convergence Comparison
In this experiment , we examine the convergence property of the proposed Lassplore algorithm . We conduct experiments on the three large data sets including real sim , rcv1 , and news20 . We set the .1 ball radius z = m , the .2 regularization parameter ρ = 0 and ˜μ = ρ . We run the algorithms for a total of 1,000 iterations , and report both Lk ( 1/Lk is the step size ) and the objective function value in ( 6 ) . The results are shown in Fig 2 , where the red curve corresponds to the proposed adaptive line search scheme , denoted as “ Adap" , and the blue curve corresponds to the Nemirovski ’s line search scheme , denoted as “ Nemi" .
We can observe from Fig 2 that ( i ) in the Nemirovski ’s scheme , the value of Lk is nondecreasing , and Lk becomes quite large after a few iterations ; ( ii ) in the proposed scheme , the value of Lk
2http://wwwpublicasuedu/~jye02/Software/lassplore/
100
10−1 k
L
10−2
10−3
10−4
0
200
100
10−1 k
L
10−2
10−3
10−4
0
200
100
10−1 k
L
10−2
10−3
10−4
0
200
400
600 iteration rcv1
400
600 iteration news20 e u l a v n o i t c n u f e v i t c e j b o e u l a v n o i t c n u f e v i t c e j b o e u l a v n o i t c n u f e v i t c e j b o
10−1
10−2
0
100
10−1
10−2
0
100
10−1
800
1000
Nemi Adap
800
1000
Nemi Adap
200
200
800
1000
10−2
0
200 real−sim
100
Nemi Adap real−sim
Nemi Adap
400
600 iteration rcv1
400
600 iteration news20
800
1000
Nemi Adap
800
1000
Nemi Adap
800
1000
400
600 iteration
400
600 iteration
Figure 2 : Comparison of the proposed adaptive line search scheme ( Adap ) and Nemirovski ’s line search scheme ( Nemi ) in terms of the value of Lk ( left column ) and the objective function value ( right column ) . For all the plots , the y axis is plotted in a logarithmic scale . varies during the iterations ; ( iii ) in most cases , the value of Lk in the proposed scheme is about 1/10 of the one in the Nemirovski ’s scheme . As a result , the proposed scheme converges much faster , which is clear from the plots in the right column ; and ( iv ) the proposed Lassplore algorithm converges rapidly in the first few iterations ( note that the y axis is plotted in a logarithmic scale ) , which is consistent with the result in Theorem 4 . 6.2 Pathwise Solutions
In this experiment , we evaluate the pathwise solutions . It is often the case in practical applications that the optimal .1 constraint parameter z is unknown . One common approach for solving this problem is to compute the solutions corresponding to a sequence of values of the parameter , eg , z1 < z2 < . . . < zs , from which the optimal one is chosen by evaluating certain criteria . This can be done by simply applying the Lassplore algorithm to solving the s independent problems ( called “ cold start" ) . However , a more efficient approach is through the so called “ warm start" , which uses the solution of the previous problem as the warm start of the latter . Indeed , the proposed Lassplore algorithm can benefit from the warm start technique , as the solution corresponding to zi is always within the .1 ball of radius zi+1 , and thus is feasible to the problem corresponding to zi+1 .
We conduct the experiments using the colon data set . We choose 100 values of z , uniformly distributed over [ 0.005m , 0.5m ] on a logarithmic scale . The results are presented in Fig 3 ( left plot ) . We can observe from the figure that the warm start approach requires a fewer number of iterations ( and thus less computation time ) than the cold start approach . We show the number of nonzeros of the solution w under different values of z in the right plot of Fig 3 . We can observe that the number of nonzeros usually increases when the radius z becomes larger . s n o i t a r e t i f o r e b m u n
160
140
120
100
80
60
40
20
0 warm start cold start
5
10
15 z
20
25
30 s o r e z n o n f o r e b m u n
45
40
35
30
25
20
15
10
5
0
5
10
15 z
20
25
30
Figure 3 : Comparison of cold start and warm start for computing the pathwise solutions using the colon data set in terms of the number of iterations required ( left plot ) . The right plot shows the number of nonzeros of the solution w . z is uniformly distributed over [ 0.005m , 0.5m ] on a logarithmic scale .
6.3 Time Efficiency
In this experiment , we compare the proposed Lassplore algorithm with two recent solvers : ProjectionL1 [ 30 ] and l1 logreg [ 16 ] in terms of the computational time for solving the sparse logistic regression . ProjectionL1 has been shown to be one of the fastest methods among the twelve methods studied in [ 30 ] , and l1 logreg is quite efficient for solving large scale sparse logistic regression . Both ProjectionL1 and the proposed algorithms are implemented in Matlab ; while l1 logreg is implemented in C , with various external supports such as BLAS , LAPACK and Intel MKL libraries . The results reported below should be interpreted with caution : “ It is very difficult , if not possible , to carry out a fair comparison of solutions methods , due to the issue of implementation ( which can have a great influence on the algorithm performance ) , the choice of algorithm parameters , and the different stopping criterion" [ 16 ] .
Both ProjectionL1 and l1 logreg solve the .1 norm regularized logistic regression , while our proposed algorithms solve the .1 ball constrained logistic regression . To make a fair comparison , we first run the competing algorithm to obtain the solution corresponding to a given .1 norm regularization parameter , from which we compute the corresponding radius z of the .1 ball , and finally run our proposed algorithms with the computed z . It is known that there exists a max [ 16 ] , at which the solution to the problem ( 4 ) is zero , and thus is usually set as a fractional ratio of max .
−3 max , and 10
Comparison with ProjectionL1 We use the colon data set in this experiment . We terminate the proposed algorithm , once it achieves the value of f ( w , c ) equal to or less than that obtained by ProjectionL1 . To examine the scalability of the algorithms with an increasing dimensionality ( under a fixed sample size ) , we conduct an experiment by sampling the first 100 , 200 , 300 , 400 , and 500 −2 max , dimensions . We try four settings for : 10 −4 max , and report the results in Fig 4 . We 10 can observe from the figure that ( i ) the computational time of ProjectionL1 grows much faster than the proposed algorithms when the data dimensionality increases ; and ( ii ) the proposed algorithm based on the adaptive line search scheme consumes much less time than the one based on the Nemirovski ’s scheme , which is consistent with our previous study . We observe a similar trend on other two small data sets including leukemia and duke breast cancer . We have not performed the comparison on the three large data sets , as ProjectionL1 does not scale to large data sets .
−1 max , 10
Comparison with l1 logreg We use all the six data sets in this experiment . l1 logreg employs the duality gap as the stopping crite−5 , rion , and we try the following three settings : 10 for exploring the time efficiency under different precisions . Meanwhile , we try the following five values for the .1 norm regular−4 max ization parameter : 10
−3 max , 10
−4 and 10
−1 max , 10
−2 max , 10
−3 , 10
102
101
100
10−1 e m i t l a n o i t a t u p m o c
10−2
100
103
102
101
100
10−1 e m i t l a n o i t a t u p m o c
10−2
100
ProjectionL1 Nemi Adap
150
200
ProjectionL1 Nemi Adap
150
200
ProjectionL1 Nemi Adap
103
102
101
100
10−1 e m i t l a n o i t a t u p m o c
250
300
350 dimension
250
300
350 dimension
400
450
500
10−2
100
103
102
101
100
10−1 e m i t l a n o i t a t u p m o c
400
450
500
10−2
100
150
200
ProjectionL1 Nemi Adap
150
200
250
300
350 dimension
250
300
350 dimension
400
450
500
400
450
500
Figure 4 : Comparison of the proposed algorithms and ProjectionL1 in terms of the computational time ( in seconds ) using the colon data set . The x axis denotes the data dimensionality and the y axis denotes the computational time . The four plots ( from left to right and from top to bottom ) correspond to the .1 norm −3 max and regularization parameter 10 10
−4 max , respectively .
−2 max , 10
−1 max , 10
−5 max . For a given and duality gap , we first run l1 logreg and 10 to compute the solution ( w1 , c1 ) ; we then compute the objective value f ( w1 , c1 ) ; and finally we run the proposed algorithm until the obtained objective function value is within the corresponding duality gap of f ( w1 , c1 ) .
The results are shown in Table 2 . We can observe from the table that ( i ) all three methods are quite efficient for solving sparse logistic regression problems of high dimensionality ( see Table 1 ) ; ( ii ) the proposed algorithm based on the adaptive line search scheme outperforms the one based on the Nemirovski ’s scheme by a large margin . In most cases , Adap is over three times faster than Nemi ; and ( iii ) Nemi is generally slower than l1 logreg , while Adap is very competitive with l1 logreg in most cases .
7 . CONCLUSION
In this paper , we propose the Lassplore algorithm for solving large scale sparse logistic regression . Specifically , we formulate the sparse logistic regression problem as the .1 ball constrained smooth optimization problem , and propose to solve the problem by the Nesterov ’s method , an optimal first order black box method for the smooth convex optimization . One of the critical issues in the use of the Nesterov ’s method is the estimation of the step size at each of the optimization iterations . The Nesterov ’s constant scheme and the Nemirovski ’s line search scheme are two wellknown approaches for setting the step size . The former scheme assumes that the Lipschitz gradient of the given function ( to be optimized ) is known in advance , which may not be the case in practice ; the latter scheme requires a decreasing sequence of the step sizes which leads to a slow convergence . In this paper , we propose an adaptive line search scheme which allows to adaptively tune the step size and meanwhile guarantees an optimal convergence rate . We have conducted an extensive empirical study by comparing the proposed algorithm with several state of the art algorithms . Our empirical results demonstrate the scalability of the Lassplore algorithm for solving large scale problems .
The efficiency of the proposed algorithm depends on the choice of the function h(τ ) in ( 34 ) . We plan to explore other choices of
−4 ; Table 2 : Comparison of the computational time ( in seconds ) . Upper part : duality gap =10 −5 ) correspond to different ratios of over Bottom part : duality gap =10 max ( is the regularization parameter employed in l1 logreg , and the solution is zero when = max ) . “ Nemi" denotes the algorithm based on the Nemirovski ’s line search scheme , and “ Adap" denotes the algorithm based on the proposed adaptive line search scheme .
−5 . The second to the sixth columns ( 10
−3 ; Median part : duality gap =10
−1 , 10
−2 , . . . , 10
10−2
10−3
/ max methoda colon leu duke real sim rcv1 news20 colon leu duke real sim rcv1 colon leu duke real sim rcv1
10−1
10−4
10−5 l1 log Nemi Adap 0.34 0.15 0.34 0.85 0.33 0.90 0.64 0.51 1.65 1.31 74.70 53.06 0.16 0.36 0.38 1.07 0.39 1.11 0.87 0.61 2.47 1.61 0.45 0.15 0.30 1.27 0.30 1.24 1.88 0.72 2.04 5.85
0.50 2.17 2.18 0.98 2.17 78.77 0.82 4.24 4.30 1.41 3.33 1.45 7.03 7.13 2.75 8.37 l1 log Nemi Adap 0.39 0.17 0.39 0.96 0.39 0.96 1.34 0.79 4.52 2.40 58.40 105.3 0.20 0.45 0.43 1.24 0.42 1.20 2.27 1.40 7.18 4.00 0.59 0.25 0.42 1.53 0.42 1.59 4.14 1.91 5.17 14.28
1.24 5.36 5.28 2.41 10.71 267.9 2.10 8.60 8.43 3.88 18.07 4.46 18.42 18.51 7.24 36.04 l1 log Nemi Adap 0.41 0.19 0.41 1.37 0.43 1.40 1.87 1.97 5.02 6.42 282.0 129.8 0.23 0.58 0.48 2.29 0.49 2.35 3.26 2.30 9.19 11.80 0.79 0.25 0.40 2.70 0.41 2.75 5.42 3.36 14.90 14.21
2.26 7.28 7.52 6.17 24.64 644.2 6.21 19.21 19.24 12.53 55.14 9.04 21.58 21.71 23.27 56.43 l1 log Nemi Adap 0.24 0.18 0.23 0.36 0.22 0.34 1.59 1.85 3.66 6.52 541.0 86.40 0.24 0.47 0.56 1.57 0.55 1.52 3.23 2.29 8.09 9.96 0.71 0.24 0.43 2.80 0.44 2.82 5.85 3.41 17.20 14.94
1.35 1.13 1.06 7.91 24.03 576.6 9.38 21.42 21.50 26.33 57.76 8.83 21.08 22.16 32.37 57.47 l1 log Nemi Adap 0.11 0.15 0.17 0.29 0.18 0.32 1.12 3.58 1.91 3.52 926.0 61.85 0.22 0.27 0.40 0.64 0.40 0.61 2.77 3.85 5.22 7.72 0.59 0.23 0.41 2.52 0.41 2.47 6.87 4.40 12.20 11.01
0.47 0.47 0.51 5.32 9.66 422.0 5.43 6.16 6.20 29.71 57.27 9.07 21.38 22.10 33.12 58.87
The results reported in this table should be interpreted with caution , as the issue of implementation can have a great influence on the algorithm performance . Note that , the algorithms Nemi and Adap are implemented in Matlab ; while l1 logreg is implemented in C , with various external supports such as BLAS , LAPACK and Intel MKL libraries . h(τ ) to further improve the algorithm . Many real world classification problems involve data from multiple classes . We plan to extend the Lassplore algorithm for solving large scale sparse multinomial logistic regression .
8 . ACKNOWLEDGMENTS
This work was supported by NSF IIS 0612069 , IIS 0812551 ,
CCF 0811790 , NIH R01 HG002516 , and NGA HM1582 08 1 0016 .
9 . REFERENCES [ 1 ] U . Alon , N . Barkai , D . A . Notterman , K . Gish , S.Ybarra , D.Mack , and A . J . Levine . Broad patterns of gene expression revealed by clustering analysis of tumor and normal colon tissues probed by oligonucleotide arrays . Cell Biology , 96:6745–6750 , 1999 .
[ 2 ] M . P . Asgary , S . Jahandideh , P . Abdolmaleki , and A . Kazemnejad .
Analysis and identification of β turn types using multinomial logistic regression and artificial neural network . Bioinformatics , 23(23):3125–3130 , 2007 .
[ 3 ] S . Boyd and L . Vandenberghe . Convex Optimization . Cambridge
University Press , 2004 .
[ 4 ] J . R . Brzezinski . Logistic regression modeling for context based classification . In DEXA ’99 : Proceedings of the 10th International Workshop on Database & Expert Systems Applications , pages 755–759 , 1999 .
[ 5 ] M . Chang , W . Yih , and C . Meek . A logistic regression model for detecting prominences . In ACM SIGKDD International conference on Knowledge Discovery and Data mining , 1996 .
[ 6 ] J . Duchi , S . Shalev Shwartz , Y . Singer , and C . Tushar . Efficient projection onto the '1 ball for learning in high dimensions . In International Conference on Machine Learning , 2008 .
[ 7 ] B . Efron , T . Hastie , I . Johnstone , and R . Tibshirani . Least angle regression . Annals of Statistics , 32(2):407–499 , 2004 .
[ 8 ] S . Eyheramendy , E . Genkin , W . Ju , DD Lewis , and D . Madigan .
Sparse bayesian classifiers for text categorization . Technical report .
[ 9 ] J . Friedman , T . Hastie , and R . Tibshirani . Additive logistic regression : a statistical view of boosting . Annals of Statistics , 28:337–407 , 2000 .
[ 10 ] J . Friedman , T . Hastie , and R . Tibshirani . Regularized paths for generalized linear models via coordinate descent . Technical report , Department of Statistics , Stanford University , 2008 .
[ 11 ] W . Fu . Penalized regressions : the bridge versus the lasso . Journal of
Computational and Graphical Statistics , 7:397–416 , 1998 .
[ 12 ] T . R . Golub , D . K . Slonim , P . Tamayo , C . Huard , M . Gaasenbeek ,
J . P . Mesirov , H . Coller , M . L . Loh , J . R . Downing , M . A . Caligiuri , C . D . Bloomfield , and E . S . Lander . Molecular classification of cancer : class discovery and class prediction by gene expression monitoring . Science , 286:531–537 , 1999 .
[ 13 ] ET Hale , W . Yin , and Y . Zhang . A fixed point continuation method for '1 regularized minimization with applications to compressed sensing . Technical report , CAAM TR07 07 , 2007 .
[ 14 ] D . Jurafsky and J . H . Martin . Speech and Language Processing : An
Introduction to Natural Language Processing , Computational Linguistics and Speech Recognition . Prentice Hall , 2000 .
[ 15 ] S . S . Keerthi , K . B . Duan , S . K . Shevade , and A . N . Poo . A fast dual algorithm for kernel logistic regression . Machine Learning , 61:151–165 , 2005 .
[ 16 ] K . Koh , S . Kim , and S . Boyd . An interior point method for large scale l1 regularized logistic regression . Journal of Machine Learning Research , 8:1519–1555 , 2007 .
[ 17 ] B . Krishnapuram and A . Hartemink . Sparse multinomial logistic regression : fast algorithms and generalization bounds . IEEE Trans . Pattern Analysis and Machine Intelligence , 27(6):957–968 , 2005 . [ 18 ] S . Lee , H . Lee , P . Abbeel , and A . Y . Ng . Efficient '1 regularized logistic regression . In The Twenty first National Conference on Artificial Intelligence , 2006 .
[ 19 ] D . D . Lewis , Y . Yang , T . G . Rose , and F . Li . Rcv1 : A new benchmark collection for text categorization research . Journal of Machine Learning Research , 5:361–397 , 2004 .
[ 20 ] J . G . Liao and K . Chin . Logistic regression for disease classification using microarray data . Bioinformatics , 23(15):1945–1951 , 2007 . [ 21 ] J . Liu and J . Ye . Efficient euclidean projections in linear time . In
International Conference on Machine Learning , 2009 .
[ 22 ] A . Maghbouleh . A logistic regression model for detecting prominences . In The Fourth International Conference on Spoken Language , 1996 .
[ 23 ] T . P . Minka . A comparison of numerical optimizers for logistic regression . Technical report , 2007 .
[ 24 ] A . Nemirovski . Efficient methods in convex programming . Lecture
Notes , 1994 .
[ 25 ] Y . Nesterov . Introductory Lectures on Convex Optimization : A Basic
Course . Kluwer Academic Publishers , 2003 .
[ 26 ] AY Ng . Feature selection , '1 vs . '2 regularization , and rotational invariance . In International Conference on Machine Learning , 2004 .
[ 27 ] MY Park and T . Hastie . '1 regularized path algorithm for generalized linear models . Journal of the Royal Statistical Society : Series B , 69:659–677 , 2007 .
[ 28 ] S . Perkins , K . Lacker , and J . Theiler . Grafting : fast , incremental feature selection by gradient descent in function space . Journal of Machine Learning Research , 3:1333–1356 , 2003 .
[ 29 ] M . A . Sartor , G . D . Leikauf , and M . Medvedovic . Lrpath : A logistic regression approach for identifying enriched biological groups in gene expression data . Bioinformatics , 25(2):211–217 , 2008 .
[ 30 ] M . Schmidt , G . Fung , and R . Rosales . Fast optimization methods for '1 regularization : A comparative study and two new approaches . In The Eighteenth European Conference on Machine Learning , pages 286–297 , 2007 .
[ 31 ] S . K . Shevade and S . S . Keerthi . A simple and efficient algorithm for gene selection using sparse logistic regression . Bioinformatics , 19(17):2246–2253 , 2003 .
[ 32 ] J . Shi , W . Yin , S . Osher , and P . Sajda . A fast algorithm for large scale
'1 regularized logistic regression . Technical report , CAAM TR08 07 , 2008 .
[ 33 ] M . West , C . Blanchette , H . Dressman , E . Huang , S . Ishida , R . Spang ,
H . Zuzan , J . A . Olso , J . R . Marks , and J . R . Nevins . Predicting the clinical status of human breast cancer by using gene expression profiles . National Academy of Sciences , 98(20):11462–11467 , 2001 . [ 34 ] J . Zhu and T . Hastie . Kernel logistic regression and the import vector machine . Journal of Computational and Graphical Statistics , 14(1):1081–1088 , 2001 .
[ 35 ] J . Zhu and T . Hastie . Classification of gene microarrays by penalized logistic regression . Biostatistics , 5(3):427–443 , 2004 .
APPENDIX Proof of Theorem 2 : Prove by induction . Considering λ0 = 1 , we have φ0(x ) ≤ ( 1 − λ0)g(x ) + λ0φ0(x ) ≡ φ0(x ) . As g(x ) belongs to the family class S 1,1 g(x ) ≥ g(sk ) + g
,∀x . Let ( 19 ) holds for some k ≥ 0 . Then from ( 21 ) , we have
( sk ) , x − skff +
μ 2
.
μ,L(Rn ) , we have [ 25 ] : ffx − skff2
φk+1(x ) ≤(1 − αk)φk(x ) + αkg(x )
=(1 − αk)(φk(x ) − ( 1 − λk)g(x ) )
+ ( 1 − ( 1 − αk)λk)g(x )
≤(1 − ( 1 − αk)λk)g(x ) + ( 1 − αk)λkφ0(x ) =(1 − λk+1)g(x ) + λk+1φ0(x ) ,
( 40 )
( 41 ) where the first inequality follows from ( 40 ) and μ ≥ ˜μ , and the last equality utilizes ( 20 ) . Therefore , ( 19 ) holds for k + 1 . Moreover , the condition 4 ensures that λk → 0 . 2 Proof of Lemma 1 : Prove by induction . It is easy to verify that 0 holds . Let g(xk ) ≤ φ ∗ ∗ g(x0 ) = φ k hold for some k . From ( 25 ) , we have
Proof of Lemma 2 : From ( 23 ) , ( 28 ) , ( 29 ) and ( 30 ) , we can write vk+1 as the combination of xk and xk+1 as : vk+1 =
{ 1 − αk 1 γk+1 + ˜μαksk − αkg
αk
[ (γk + αk ˜μ)sk − γk+1xk ] .
( sk)}
=xk +
=xk +
1 αk 1 αk
( sk − xk ) − 1 αkLk ( xk+1 − xk ) ,
. g
( sk )
( 43 ) where the first equality follows from ( 29 ) , the second equality follows from ( 30 ) , and the last equality follows from ( 28 ) . Hence , from ( 29 ) , we can write sk+1 as : sk+1 = xk+1 +
αk+1γk+1(vk+1 − xk+1 ) γk+1 + αk+1 ˜μ αk+1γk+1(1 − αk ) = xk+1 + αk(γk+1 + αk+1 ˜μ ) = xk+1 + βk+1(xk+1 − xk ) ,
( xk+1 − xk )
( 44 ) where the first equality follows from ( 30 ) , the second equality utilizes ( 43 ) , and the last equality follows from the definition of βk+1 in ( 33 ) . 2
Proof of Theorem 4 : Prove by induction . As required by the input of Algorithm 2 , γ0 ≥ ˜μ . If γk ≥ ˜μ , we have
( 45 ) Therefore , we conclude that γk ≥ ˜μ always holds . From ( 30 ) , we have γk+1 = Lkα2 . Since λk = i=1(1 − αk ) , we can get Πk
γk+1 = ( 1 − αk)γk + αk ˜μ ≥ ˜μ . q ≥ ˜μ , so that αk ≥ !
˜μ Lk k r
λN ≤ ΠN k=1
1 −
˜μ Lk
.
( 46 )
We have γ0 ≥ γ0λ0 , since λ0 = 1 . If γk ≥ γ0λk , we have
γk+1 = ( 1−αk)γk+αk ˜μ ≥ ( 1−αk)γk ≥ ( 1−αk)γ0λk = γ0λk+1 , where the first inequality utilizes ˜μ ≥ 0 , and the last equality follows from λk+1 = ( 1 − αk)λk . Therefore , Lkα2 k = γk+1 ≥ γ0λk+1 always holds . Since αk ∈ ( 0 , 1 ) and λk+1 = ( 1 − αk)λk , it is clear that λk is strictly decreasing . Denote ak = 1√
. We have ak+1 − ak =
λk p ≥ λk − λk+1 p λk − λk+1 λk + p
λkλk+1( p
√
=
2λk
λk+1
2
λk+1 ) ≥ 1 2
αk λk+1 r
γ0 Lk
.
We have a0 = 1√
λ0 = 1 , since λ0 = 1 . From ( 47 ) , we have r kX i=1 ak =
≥ 1 +
1√ λk
1 2
γ0 Lk
,
λN ≤
( 1 +
P
1
N k=1 q
1 2
.
γ0 Lk
)2
( 47 )
( 48 )
( 49 )
. ffg
( sk)ff2
φ
. k
αk(1 − αk)γk k+1 ≥ ( 1 − αk)g(xk ) + αkg(sk ) − α2 ∗ 2γk+1 g ( sk ) , vk − skff ( sk)ff2 ffg αkγk γk+1
+ γk+1 ≥ g(sk ) − α2 2γk+1 + ( 1 − αk)g . ≥ g(xk+1 ) ,
( sk ) , k
.
( 42 ) which leads to
( vk − sk ) + xk − skff ffsk − where the first inequality follows from φ vkff2 ≥ 0 , the second inequality utilizes the convexity of g(x ) , ie , ( sk ) , xk − skff , and the last inequality follows g(xk ) ≥ g(sk ) +g from ( 28 31 ) .
≥ g(xk ) and ˜μ
∗ k
2
.
2
Incorporating ( 46 ) and ( 49 ) , we obtain ( 35 ) . In Algorithm 2 , Steps 3 6 ensure that conditions ( 28 31 ) hold . According to Lemma 1 , the condition ( 26 ) holds . By using Theorem 3 , we obtain ( 36 ) . 2
