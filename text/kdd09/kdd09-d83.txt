Information Theoretic Regularization for Semi Supervised Boosting
Lei Zheng
Knoesis Center
Wright State University leizheng@wrightedu
Shaojun Wang Knoesis Center
Wright State University shaojunwang@wrightedu
Yan Liu
Wright State University yanliu@wrightedu
Chi Hoon Lee
Yahoo! Lab chihoon@yahoo inc.com
ABSTRACT We present novel semi supervised boosting algorithms that incrementally build linear combinations of weak classifiers through generic functional gradient descent using both labeled and unlabeled training data . Our approach is based on extending information regularization framework to boosting , bearing loss functions that combine log loss on labeled data with the information theoretic measures to encode unlabeled data . Even though the information theoretic regularization terms make the optimization non convex , we propose simple sequential gradient descent optimization algorithms , and obtain impressively improved results on synthetic , benchmark and real world tasks over supervised boosting algorithms which use the labeled data alone and a state of the art semisupervised boosting algorithm .
Categories and Subject Descriptors I.2 [ Artificial Intelligence ] : Learning
General Terms Algorithms , Experimentation , Performance
Keywords Ensemble method , semi supervised learning
1 .
INTRODUCTION
Boosting , as one of the most powerful learning ideas introduced in early 1990s ( Hastie et al . 2009 ) , is a supervised machine learning and data mining technique that incrementally builds linear combinations of \weak" models to generate a \strong" predicative model and is proved to be one of the most successful and practical methods in machine learning . Schapire ( 1990 ) developed the first provable polynomial time boosting algorithm , based on PAC learning , and showed how to improve a weak learner ’s performance by training two additional classifiers . Freund and Schapire ( 1997 ) later invented the popular AdaBoost algorithm using the idea of adaptively resampling the data : that is , AdaBoost starts with a weak classifier and seeks its improvements iteratively based on its performance on the training data . Since its inception , AdaBoost algorithm for classification has attracted much attention in the machine learning community as well as in related areas in statistics . Various variants of AdaBoost algorithm have proven to be very competitive in prediction accuracy in a variety of applications . Boosting methods were originally proposed as ensemble methods , which rely on the principle of generating multiple predictions and majority voting ( averaging ) among the individual classifiers .
Semi supervised learning ( Chapelle et al . 2006 ) is a machine learning technique that uses both labeled and unlabeled data for training | typically a small amount of labeled data with a large amount of unlabeled data . Semisupervised learning is touted as one of the most natural forms of training for prediction tasks , since unlabeled data is plentiful whereas labeled data is usually limited or expensive to obtain . Many approaches have been proposed for semi supervised learning ( Chapelle et al . 2006 ) , including : generative models ( Castelli and T . Cover 1996 , Cohen and Cozman 2006 , Nigam et al . 2000 ) , self learning ( Celeux and Govaert 1992 ) , co training ( Blum and Mitchell 1998 ) , information theoretic regularization ( Grandvalet and Bengio 2004 , Corduneanu and Jaakkola 2006 ) and graphbased transductive methods ( Zhou et al . 2004 and Zhu et al . 2003 ) .
Although highly desirable , semi supervised boosting has not been studied as widely as the other semi supervised settings mentioned above , with very few exceptions ( Benett et al . 2002 , Chen and Wang 2007 , d’Alche Buc et al . 2002 , Valizadegan et al . 2008 ) . These approaches are essentially a self learning algorithm where the class labels of unlabeled data are updated iteratively ; they essentially operate like self training where the class labels of unlabeled examples are updated iteratively : first a classifier is constructed using a small amount of labeled data , then it is used to predict the pseudo labels for unlabeled examples ; a new classifier is then constructed using both labeled and pseudo labeled examples ; the processes of constructing classifiers and predict ing pseudo labels alternate iteratively until certain stopping criterion is reached . The main drawback of this approach is that it relies solely on the pseudo labels predicted by the classifiers constructed so far when generating new classifiers . Since the pseudo labels predicted by the constructed classifiers could be inaccurate , especially at the first few steps , the resulting new classifiers might also be unreliable . The errors might propagate due to this ripple effect , thus finally hurt the performance .
We propose semi supervised boosting algorithms that use information theoretic measures such as entropy and/or mutual information ( Grandvalet and Bengio 2004 , Corduneanu and Jaakkola 2006 , Wang et al . 2009 ) as a vehicle for regularization on unlabeled data . The motivation is that minimizing conditional entropy or minimizing mutual information over unlabeled data encourages the algorithm to find putative labelings for the unlabeled data that are mutually reinforcing with the supervised labels ; that is , greater certainty on the putative labelings coincides with greater conditional likelihood on the supervised labels , and vice versa . For a single classification variable , minimizing entropy criterion has been shown to effectively partition unlabeled data into clusters ( Grandvalet and Bengio 2004 , Roberts et al . 2000 ) . Later on , this work was extended to semi supervised learning for structured prediction such as sequence labeling ( Jiao et al . 2006 ) and image segmentation ( Lee et al . 2006 ) , achieving very impressive improvements over using labeled data alone . Recently Wang et al . ( 2009 ) present a mutual information regularized semi supervised learning as a data compression scheme that is formulated into a rate distortion ( Cover and Thomas 1991 ) framework , and demonstrate encouraging results with two real world problems to show the effectiveness of the proposed approach : text categorization as a multi class classification problem , and hand written character recognition as a sequence labeling problem . In this paper , we demonstrate how to use similar ideas for semi supervised boosting . Different with existing self learning type semi supervised boosting algorithms , our approach is grounded on a firm information theoretic motivation . We propose simple sequential gradient descent optimization algorithms , and obtain impressively improved results on synthetic , benchmark and real world tasks over supervised boosting algorithms ( which use the labeled data alone ) and a state of the art semi supervised boosting algorithm , ASSEMBLE proposed in ( Benett et al . 2002 ) .
2 . BOOSTING AS AN OPTIMIZATION
METHOD
A fundamental theoretical issue for AdaBoost and its many variants is convergence , which is not addressed in the original AdaBoost paper ( Freund and Schapire 1997 ) . In fact , much work has been done to prove the convergence of boosting algorithm in terms of an optimization method . They can be categorized into two basic approaches : greedy function optimization and maximum entropy approach .
In the first approach , AdaBoost is viewed as a sequential gradient descent algorithm in function space , inspired by numerical optimization and statistical estimation . It was Breiman ( 1999 ) who made this path breaking observation . This insight opened new perspectives and was extended to a variety of related objective functions . Many variant of Adaboost , such as logistic regression and least square ( Fried man , et al , 2000 , Mason et al . 1999 ) have also been developed for contexts other than classification , such as regression and density estimation . In this approach , statistical models are typically additive expansions in a set of basis functions and are fitted by minimizing a loss function averaged over the training data . For many loss functions or basis functions , this requires computationally intensive numerical optimization techniques . The boosting approach is a forward stagewise additive modeling ( Friedman et al . 2000 ) that approximates the solution by sequentially adding new basis functions to the expansion without adjusting the parameters and coefficients of those that have already been added . At each iteration , one solves for the optimal basis function and corresponding coefficients to add to the current expansion . This produces new expansion and the process is repeated .
In the second approach ( Collins et al 2002 , Della Pietra et al . 1997 , Lebanon and Lafferty 2002 , Haffari et al . 2008 ) , the boosting algorithm is cast in terms of maximizing generalized entropy subject to certain linear feature constraints , enforcing their expectations meet the empirical expectations . AdaBoost can be described as a greedy feature induction algorithm that incrementally builds random fields to solve the maxent problem . The greediness of the algorithm arises in steps that select the most informative feature . In these steps each feature in a pool of candidate features is evaluated by estimating the reduction in the Kullback Leibler divergence that would result from adding that feature to the field . This reduction is approximated as a function of a single parameter and is equal to the exponential loss reduction . This approximation is one of the key elements making it practical to evaluate a large number of candidate features at each stage of the induction algorithm . By using an auxiliary function to bound the change in generalized K L divergence from below , the iterative scaling algorithm can be derived and thus convergence to the global optimal solution is proved .
The first of these two methods searches the weak learner myopically . Thus it only approximately finds the best one and then obtains this weak learner ’s optimal voting parameter . The second method , in contrast , looks into weak learners one by one , then chooses the weak learner that induces largest loss reduction . However , it is more computationally expensive since the optimal voting parameter for each weak learner has to be calculated . Moreover , for general loss functions , it is in general very hard to construct auxiliary functions with closed form solutions . In this paper , therefore , we adopt the first approach .
3 . GENERIC SEMI SUPERVISED BOOST
ING ALGORITHM
Let X be a random variable over data to be labeled , and Y be a random variable over corresponding labels ranging over a finite label alphabet Y . Assume we have a set of labeled examples , Dl = ( x1 ; y1 ) ; ; ( xN ; yN ) , and unlabeled examples , Du = xN +1 ; ; xM . We would like to construct a discriminant function of the form hT ( x ) = 1h(x ; 1 ) + + T h(x ; T )
( 1 ) such that the prediction error is small . Here h(x ; t ) : X ! Y denote weak learners , for example , decision stumps whose predictions are +1 and 1 , from a fixed class H , characterized by a set of parameters and t 2 < are the weak learner
J(ht ) =
N
Xi=1
Ll(yiht(xi ) ) + fl
M
Xi=N +1
Lu(ht(xi ) )
( 2 ) the training examples : weights . They also correspond to the features in random fields ( Della Pietra et al . 1997 ) and sufficient statistics in an exponential model ( Lebanon and Lafferty , 2002 ) . Our goal is to learn such a model from the combined set of labeled and unlabeled examples , Dl [ Du .
Just as in supervised learning case for boosting , the estimation for the combination is simply minimization of the following surrogate risk functional over 0/1 loss ; where the first term denotes the surrogate loss for labeled data , which is a monotonically decreasing and differentiable function of its argument yiht(xi ) , such that the more the discriminant function agrees with the label yi , the smaller the loss . The second term represents the surrogate loss for unlabeled data which behaves like a clustering criterion . fl is a trade off parameter that controls the influence of the unlabeled data .
To derive the boosting algorithm that can accommodate any loss function , suppose that we have already included t , 1 component classifiers ht,1(x ) = 1h(x ; ^1 ) + + th(x ; ^t,1 )
( 3 ) and we wish to add another h(x ; ) . The estimation criterion for the overall discriminant function , including the new component with votes , is given by
J( ; ) =
N
Xi=1
+fl
Ll ( yiht,1(xi ) + yih(xi ; ) )
( 4 )
M
Xi=N +1
Lu ( ht,1(xi ) + h(x ; ) )
Note that we explicate only how the objective depends on the choice of the last component and the corresponding votes , since the parameters of the t , 1 previous components along with their votes have already been set and won’t be modified further .
As in the case of supervised boosting , there are two parameters to optimize . We implement this optimization approximately in two steps . We first find the new component or parameters so as to maximize its potential in reducing the surrogate loss , \potential" in the sense that we can subsequently adjust the votes to actually reduce the surrogate loss . More precisely , we set so as to minimize the derivative
J( ; )j=0 d d N
=
=
Xi=1
+fl
N
Xi=1
+fl d d
Ll ( yiht,1(xi ) + yih(xi ; ) ) j=0
M
Xi=N +1 d d
Lu ( ht,1(xi ) + h(x ; ) ) j=0 dLl ( yiht,1(xi ) ) yih(xi ; )
M
Xi=N +1Xy dLu ( yht,1(xi ) ) yh(xi ; ) dz . Note this derivative d where dL(z ) = dL(z ) d J( ; )j=0 precisely captures the amount by which we would start to reduce the surrogate loss if we gradually increase the votes for the new component with parameters . Minimizing this reduction seems like a sensible estimation criterion for the new component or . This strategy permits us to set and subsequently optimize to actually minimize the surrogate loss .
Define the following weights and normalized weights w on w(t,1 ) i
= ,dLl ( yiht,1(xi ) ) for i = 1 ; ; N w(t,1 ) i
( y ) = ,dLl ( yht,1(xi ) ) for i = N + 1 ; ; M 8y
Note that for each piece of unlabeled data , since its label is unknown , we assign an individual weight for each possible label using the derivative of loss functions for labeled data . In fact , how to choose the weight on each labeled and unlabeled data is quite arbitrary , but the main purpose is to incorporate a scaling procedure for numerical consideration , since each time we add a weak classifier , the cost function becomes smaller and will exceed the precision range of essentially any machine ( even in double precision ) . Thus defining a weight is the only reasonable way to perform the computation .
Then the normalized weights w on the training examples are
~w(t,1 ) i
=
~w(t,1 ) i
( y ) = w(t,1 ) i i=1 w(t,1 ) PN for i = 1 ; ; N
+PM i=N +1Py w(t,1 ) i i w(t,1 ) i
( y ) i=1 w(t,1 ) PN for i = N + 1 ; ; M 8y i=N +1Py w(t,1 )
+PM i i
( y )
( y )
These weights are guaranteed to be non negative since the loss function for labeled data is a decreasing function of its argument ( its derivative has to be negative or zero ) . By ignoring the multiplicative constant ( constant at iteration t ) we will estimate by minimizing
N yih(xi ; )
( 5 )
,
+fl i
~w(t,1 )
Xi=1 Xi=N +1Xy
M dLu ( yht,1(xi ) )
~w(t,1 ) i
( y )
~w(t,1 ) i
( y)yh(xi ; )
After we find ^ , we solve the minimization problem for t over the following objective function ,
J( ; ^t ) =
N
Xi=1
Llyiht,1(xi ) + yih(xi ; ^t )
( 6 )
+fl
M
Xi=N +1
Luht,1(xi ) + h(xi ; ^t )
This can be done by one dimensional numerical line search 1 .
1The search is quite expensive since it is to be performed at each round t . In fact , we can compute from the data a finite interval to which we know that belongs . This gives us a formula for a worst case search of approximate ( Janodet et al . 2004 ) .
We are now ready to cast the steps of the semi supervised boosting algorithm as function gradient descent in a form similar to AdaBoost . the entropy of exponential models ; justification for general situations can be found in ( S . Boyd and L . Vandenberghe 2004 ) .
Generic function gradient descent semi supervised boosting algorithm
1 . Initialize the observation weights w 2 . For t = 1 to T ;
( a ) Compute the negative Gateaux derivative dJ( ) of the functional J( ) ,
,dJ(f )(x ) = ,
@ @
J(f + ffix)j=0 then fit a classifier ht(x ; ) to this gradient using weights w , Thus ht(x ; ) can be viewed as an approximation of the negative vector .
( b ) Set the votes t for the new component by mini mizing the overall surrogate loss ( 6 ) .
( c ) Update the weights on the training examples , la beled and unlabeled , based on the new base learner : w(t ) i
= ,ff dLl ( yiht(xi) ) ) for i = 1 ; ; N w(t ) i ( y ) = ,ff dLl ( yht(xi) ) ) for i = N + 1 ; ; M ; 8y where ht(xi ) = ht,1(xi)+th(xi ; ^t ) and ff is cho(y ) = 1 , this ensures that the new weights sum to one after the update . sen such thatPN i=N +1Py ~w(t,1 )
+PM i=1 ~w(t,1 ) i i
For ease of exposition , we first consider binary classification , that is , y 2 f,1 ; 1g . We then extend to classification with multiple classes . In both cases , we use normalized loglinear models p(yjx ) = e(,yh(x ) )
2 .
Py e(,yh(x ) ) 4.1 Binary Classification
Consider normalized log linear models p(yjx ) = e(,yh(x ) ) we use the logistic loss , that is , negative log probability , for labeled data ,
Py e(,yh(x ) ) ,
Ll(yiht(xi ) ) = , log p(yijxi ) = log(1 + e(,yiht ( xi) ) )
( 7 )
Note that there should exist a constant 2 inside the exponential but we omit it by scaling the weaker learners by half . log(1+e,z ) , thus
Let Ll(z ) = log(1 + e,z ) , then dLl(z ) = , e,z the weights are given by
^w(t,1 ) i
= ff
^w(t,1 ) i
( y ) = ff e,yiht,1 ( xi )
1 + e,yiht,1 ( xi ) e,yht,1(xi )
1 + e,yht,1(xi ) for i = 1 ; ; N for i = N + 1 ; ; M ; 8y e i=1
,yi ht,1 ( xi ) where ff = PN
,yht,1 ( xi ) ,yht,1 ( xi ) . For unlabeled data , again we have two options . We can minimize either the negative conditional Kullback Leibler divergence
,yi ht,1 ( xi ) +PM i=N +1 Py
1+e
1+e e
,D(p(yjx ) ; U(yjx ) ) = ,Xx2Du
~p(x)Xy p(yjx ) log p(yjx )
U(yjx ) ( 8 )
3 . Output final classifier hT ( x ) =
T
Xt=1 tht(x ; ^t ) where U(yjx ) is the uniform distribution and ~p(x ) denotes the empirical distribution of X , or the mutual information of unlabeled data
I(~p(x ) ; p(yjx ) ) = D(~p(x)p(yjx ) ; ( ~p(x)p(y ) )
The following result shows the convergence of the above algorithm to a local minimum or stop early at round T . Let Ll and Lu be any lower bounded Lipschitz differentiable cost functionals . Either the sequence of combined classifiers generated by the algorithm above halts on round T with its gradient being positive , or the combined loss function converges to a local minimum J fi , in which case limt!1 < rJ(ht ) ; ht >= 0 . A similar result has been shown in ( Mason et al . 1999 ) , where J( ) is a convex function to guarantee convergence to a global minimum , here we relax J( ) to be non convex , thus only local minimum can be reached . The proof technique is similar to that in ( Mason et al . 1999 ) with minor modifications ( Bertsekas 1999 ) .
4 .
INFORMATION THEORETIC REGULARIZATION APPROACH
We use information theoretic measures , entropy and mutual information , as regularization for the use of unlabeled data . The rationale of using these terms has been explained in Grandvalet and Bengio ( 2004 ) , Jiao et al . ( 2006 ) and Wang et al . ( 2009 ) . Unfortunately both measures on unlabeled data are not convex over , mainly because they are composition functions of convex/concave functions over the parameters . Jiao et al . ( 2006 ) explained one simple case ,
= Xx2Du p(y ) ( 9 ) where p(y ) = Px ~p(x)p(yjx ) . In the following , we illustrate the derivation for using both entropy and mutual information as regularization on unlabeled data . p(yjx ) log
~p(x)Xy p(yjx )
Entropy regularization Minimizing negative conditional Kullback Leibler divergence is equivalent to minimizing the sum of the conditional entropy of unlabeled data ,
Lu(yht(xi ) ) = H(p(yjxi ) ) p(yjxi ) log p(yjxi )
Lu(ht(xi ) ) = Xy = ,Xy = Xy
1
1 + e(,yht(xi ) ) log(1 + e(,yht(xi) ) )
Looking at the above formula , clearly entropy regularization is merely plain boosting with the unlabeled examples 2At the first sight , it seems that similar idea can be applied to unnormalized model once we use generalized conditional Kullback Leibler divergence ( Lebanon and Lafferty 2002 ) to define the entropy and mutual information for unnormalized model , however it doesn’t work for entropy case , the surrogate loss in Step 2(b ) in this case is lowerly unbounded . replaced by labeled examples with all classes , where each class is assigned a weight by the class conditional probability given unlabeled example .
M
+fl
Xi=N +1Xy dLu ( yht,1(xi ) ) yh(xi ; )
Let Lu(z ) = 1
1+e,z log(1 + e,z ) , then dLu(z ) = e,z
( 1+e,z )2
( ,1 + log(1 + e,z) ) .
The loss function for the weaker learner in step 2(a ) is
J( ; )j=0 d d N
=
Xi=1
+fl dLl ( yiht,1(xi ) ) yih(xi ; )
M
Xi=N +1Xy dLu ( yht,1(xi ) ) yh(xi ; )
( 10 )
= ,
N
Xi=1
~w(t,1 ) i yih(xi ; ) , fl
( ,1 + log(1 + e,yht,1(xi) ) )
( 1 + e(,yht,1(xi) ) )
M
Xi=N +1Xy ~w(t,1 ) i
( y)yh(xi ; )
We look over all of the weak learners and choose the one h( ; ^ ) which has the lowest value of this loss function .
The minimization over the surrogate loss in Step 2(b ) is used to determine the optimal value of .
Mutual information regularization Minimizing mutual information of unlabeled data is equivalent to minimizing the sum of the difference between the entropy of unlabeled data and the conditional entropy of unlabeled data ,
Lu(ht(xi ) ) = Xy
Lu(yht(xi ) )
= H(p(y ) ) , H(p(yjxi ) )
= ,Xy Xx
~p(x)p(yjxi ) logXx
~p(x)p(yjxi ) p(yjxi ) log p(yjxi )
+Xy
= , log(M , N )
M , N Xy
M
Xi=N +1
1
1 + e(,yht(xi ) )
+
1
M , N Xy
M
Xi=N +1
1
1 + e(,yht(xi ) ) log(
M
Xi=N +1
1
1 + e(,yht(xi ) ) )
,Xy
1
1 + e(,yht(xi ) ) log(1 + e(,yht(xi) ) )
Let Lu(z ) = , log(M ,N ) 1 1
M ,N PM 1+e(,z ) ) + dLu(z ) can be computed easily .
1+e(,z ) log(PM i=N +1 i=N +1
1
1+e(,z ) + 1
M ,N PM
1
1+e(,z ) log(1 + e(,z) ) , then i=N +1
The loss function for the weaker learners in step 2(a ) is
J( ; )j=0 d d N
=
Xi=1 dLl ( yiht,1(xi ) ) yih(xi ; )
( 11 )
N
= ,
Xi=1
~w(t,1 ) i yih(xi ; )
M
,fl log(M , N )
Xi=N +1Xy i
~w(t,1 ) ( y)yh(xi ; ) ( 1 + e(,yht,1(xi) ) )
M
+fl
Xi=N +1Xy
( ,1 + log(1 + e,yht,1(xi) ) )
( 1 + e(,yht,1(xi) ) )
~w(t,1 ) i
( y)yh(xi ; )
M
,flXy
Xi=N +1 log(PM
1
,yht,1 ( xi ) + 1 )
1+e i=N +1 ( 1 + e(,yht,1(xi) ) )
~w(t,1 ) i
( y)yh(xi ; )
Again we look over all of the weak learners and choose the one h( ; ^ ) which has the lowest value of this loss function . The minimization over the surrogate loss in Step 2(b ) is used to determine the optimal value of . 4.2 Multi class Classification
In the multi class classification setting , we use the approach proposed in ( Zhu et al . 2005 ) and recode the class label y 2 Y = f1 ; ; Kg with a K dimensional vector c , with all entries equal to , 1 K,1 except a 1 in position k if y = k , ie c = ( c1 ; ; cK )T , and ck = fl 1 ;
, 1
K,1 ; if y = k ; if y 6= k : and there is a one to one correspondence between y and c . We use c(y ) to denote the vector corresponding to class y . A generalization of the exponential loss function for the labeled data to the multi class case then naturally follows :
Ll(c(y ) ; h(x ) ) = e( , 1
K c(y)T h(x ) ) where h(x ) = ( h1(x ) ; ; hK ( x))T and hk(x ) correspond to class k and h(x ) satisfies symmetric constraint : h1(x ) + + hK ( x ) = 0
So when K = 2 , this multi class exponential loss function reduces to the binary exponential loss .
We require the weak learner h(x ; ) to satisfy the sym metric constraints : h1(x ; ) + + hK ( x ; ) = 0
( 12 )
Specifically , at a given x , h(x ) maps x onto C ; h : x ! C , where C is the set containing K K dimensional vectors :
( 1 ; , 1 ( , 1
K,1 ; ; , 1 K,1 ; 1 ; ; , 1
K,1 )T K,1 )T
( , 1
K,1 ; ; , 1
K,1 ; 1)T
C =
8>>>>>>< >>>>>> :
9>>>>>>= >>>>>> ;
It is easy to check that all the derivation for binary classification will remain the same for multi class classification with simple modular modification : substitute yh(x ) with 1 K c(y)T h(x ) .
It is natural to see that the normalized log linear model is p(yjx ) = e
( , 1 K ( , 1 K
Py e c(y)T h(x ) ) c(y)T h(x ) ) , the logistic loss , that is , negative log probability , for labeled data , is e( , 1
Ll(y ; h(x ) ) = log(1 +Xy0
K ( c(y),c(y0))T h(x) ) )
( 13 )
The entropy of unlabeled data is then
Lu(h(x ) ) = Xy = Xy
Lu(y ; h(x ) ) = ,Xy
1 c(y)T h(x ) ) p(yjx ) log p(yjx )
( 14 ) e( , 1
K ( c(y),c(y0))T h(x) ) )
K
1 + e( , 1 log(1 +Xy0
Again it is easy to check that all the derivation for binary classification will remain the same for multi class classification with simple modular modification : substitute e( , 1 e(,yht,1(x))e(,yh(x ; ^t) ) ) withPy0 e( , 1 Py0 e( , 1 K c(y)T h(x) ) .
1
K ( c(y),c(y0))T h(x ; ^t )) ) ; substitute e(,yht,1(x ) ) with
K ( c(y),c(y0))T ht,1 ( x) ) ) and substitute yh(x ) with
K ( c(y),c(y0))T ht,1 ( x) ) )
Similar derivations can be obtained for mutual informa the first class is ( 4 ; ; 4)T and the mean of the second is ( ,4 ; ; ,4)T . The covariance matrix is diagonal matrix with standard deviation being 20 . We set the number of labeled data N = 50 and we increase the number of unlabeled data from 50 to 1500 . We use development data to choose the best value of regularization parameter , ie fl in our proposed method and ff and fi in Assemble method . The size of development and test data is 450 . s r o r r e t s e T
0.39
0.38
0.37
0.36
0.35
0.34
0.33
0.32
0.31
0.3
0
AdaBoost
LogitBoost
Assemble.LogitBoost
Semi Mutual Information
Semi Entropy
5
10
15
20
25
30
Ratio of the size of unlabeled data and labeled data tion case .
The final classifier is given by hT ( x ) =
T
Xt=1 tht(x : ^t ) and for new sample data x , we assign its class label as y = arg max y0 c(y0)T hT ( x )
( 15 )
( 16 )
5 . EXPERIMENTAL EVALUATION
In this section , we report experimental results on synthetic , benchmark , and real world data . It is important to note that it is not our intention to show that the proposed semi supervised boosting algorithm always outperforms a variety of semi supervised learning algorithms . Instead , our objective is to demonstrate that the proposed semi supervised boosting algorithm is able to effectively improve the accuracy of the well known supervised boosting algorithms using the unlabeled examples , and it is more effective than the existing semi supervised boosting algorithms . Hence , the empirical study is focused on a comparison with the existing supervised and semi supervised boosting algorithms , rather than a wide range of semi supervised learning algorithms . To be more specific , we evaluate empirically supervised AdaBoost and LogitBoost , and a state of the art semi supervised boosting algorithm , Assemble ( Bennett et al . 2002 ) , ( Assemble.LogitBoost ) with our proposed semisupervised boosting methods . The weak learner we used in all of our experiments is the decision stump FindAttrTest as described in ( Freund and Schapire , 1996 ) . All of the experiments are repeated 10 times , the results are expressed by the mean and its standard deviation . 5.1 Synthetic Data
We first consider a 2 class problem with a 10 dimensional input space . Each class is generated by a normal distribution with equal mixing weight probability . The mean of
Figure 1 : Test errors on data generated by two mixtures of 10 dimensional Gaussian distribution when we increase the size of unlabeled data .
Figure 1 and Table 1 show the results of the classification error rate on test data using Assemble.LogitBoost and both entropy and mutual information on unlabeled data as regularization term when we increase the size of unlabeled data . Clearly this result shows that our semi supervised boosting algorithms overcomes both AdaBoost and LogitBoost as well as AssembleLogitBoost Especially , unlabeled data does help improve performance . With the increment of unlabeled data , the error rates of our algorithms tend to decrease .
In the experiment , we also find that , in terms of error rate on test data , both entropy based semi supervised boosting algorithm and mutual information based semi supervised boosting algorithm converge more quickly than AdaBoost and LogitBoost . Figure 2 shows how the error rate on test data changes at each iteration when the ratio of unlabeled data and labeled data is set to 5 . Clearly both entropy and mutual information regularization terms on unlabeled data behave as excellent data dependent regularizers , and entropy based approach has better regularization effect than mutual information based approach . We’ve also used l2 norm of the parameters suggested in ( Lebanon and Lafferty 2002 ) as a regularization term , but unfortunately it has almost no affect on the performance .
Since the minimization problem in Step 2(b ) is simply one dimensional one , we study the objective function being minimized when we use conditional entropy on unlabeled data as a regularization term . Figures 3 and Figure 4 illustrate the loss curves at the 5th iteration where we set fl = 0:2 and fl = 1 respectively , and the ratio of unlabeled data and labeled data is 5 . The loss function on labeled data is convex and the loss function on unlabeled data is non convex . When the regularization parameter fl is small ,
Ratio AdaBoost 3621(459 ) 1 2 3710(423 ) 3772(245 ) 3 3652(323 ) 4 3680(234 ) 5 3600(313 ) 6 7 3580(231 ) 3648(367 ) 8 3590(231 ) 9 3603(252 ) 10 3646(335 ) 15 30 3675(323 )
LogitBoost Assemble.LogitBoost 3617(331 ) 3541(434 ) 3787(380 ) 3624(627 ) 3518(512 ) 3573(431 ) 3612(501 ) 3692(427 ) 3608(433 ) 3547(464 ) 3589(489 ) 3523(550 )
3581(399 ) 3557(380 ) 3595(326 ) 3533(363 ) 3502(320 ) 3514(233 ) 3516(239 ) 3505(243 ) 3521(234 ) 3516(243 ) 3508(226 ) 3512(161 ) fl ( MI ) .0070 .0050 .0030 .0025 .0018 .0019 .0018 .0012 .0013 .0011 .0010 .0005
MI fl ( Entropy )
Entropy
3462(345 ) 3444(331 ) 3402(424 ) 3378(291 ) 3280(322 ) 3250(340 ) 3308(301 ) 3222(248 ) 3210(256 ) 3293(283 ) 3267(217 ) 3264(206 )
.0025 .0023 .0030 .0025 .0024 .0022 .0022 .0020 .0015 .0011 .0009 .0006
3410(252 ) 3444(268 ) 3470(477 ) 3411(282 ) 3400(297 ) 3289(227 ) 3222(244 ) 3175(234 ) 3200(278 ) 3160(195 ) 3105(183 ) 3067(190 )
Table 1 : Error rates ( % ) on Gaussian mixture test data and the corresponding values of regularization parameter for both entropy and mutual information ( MI ) based semi supervised boosting when varying the ratio of the size of unlabeled and labeled Gaussian mixture data .
AdaBoost LogitBoost Entropy Based Semi−Boosting Mutural Information SemiBoosting Assemble.LogitBoost the loss function on labeled data dominates to force the total loss function to be convex , this guarantees that we indeed find a global minimum solution using line search ; When the regularization parameter fl is large , the loss function on labeled data is not able to dominate , so the total loss function is non convex , as Figure 4 illustrates , there are one minimum and several saddle points exist . When the iteration gets larger , the loss functions look more like in Figure 3 and the total loss function becomes convex , this is true even for large fl . In all the experiments on synthetic , benchmark and real data , usually smaller regularization parameter fl gives better test error , thus non convexity of the objective function is not a big issue at all in practice . We have similar observation when we use mutual information on unlabeled data as a regularization term . e t a R r o r r
E
0.5
0.48
0.46
0.44
0.42
0.4
0.38
0.36
0.34
0.32
100
101
102
103
104
Number of Iterations
Figure 2 : Test errors vary at each iteration with maximum iteration being 2500 where the ratio of unlabeled data and labeled data is set to 5 .
600
500
400 s s o L
300
200
100
0
0
Loss on labeled data Loss on unlabeled data Total loss
1
2
3
4
5
6
7
8
9
10
Figure 3 : Loss curves to be minimized at the 5th iteration with regularization parameter fl = 0:2 .
2000
1800
1600
1400
1200 s s o L
1000
800
600
400
200
0
0
Loss on labeled data Loss on unlabeled data Total loss
2
4
6
8
10
12
14
16
18
20
Figure 4 : Loss curves to be minimized at the 5th iteration with regularization parameter fl = 1 .
Finally we apply our algorithms to 3 class synthetic data . Each class is still generated by a 10 dimensional normal distribution with equal mixing weight probability . The means of each class are ( ,8 ; ; ,8)T , ( 0 ; ; 0)T and ( 8 ; ; 8)T respectively . The covariance matrix is diagonal with standard deviation being 10 . The number of labeled data is N = 30 , and the number of unlabeled data is 210 . We also use unlabeled data as test data . In this experiment , the error rate of LogitBoost is 33.81 % ( 2.71 ) , the error rate of Assemble.LogitBoost is 32.86 % ( 4.49 ) , while error rate of l l the entropy based semi supervised boosting is 30.47 % ( 2.04 ) with fl = 0:05 , and error rate of mutual information semisupervised boosting is 29.50 % ( 2.82 ) with fl = 0:05 . 5.2 Benchmark Data
In this experiment , we use several benchmark datasets : Balance scale weight & distance , Pima , Wisconsin diagnostic breast cancer and BUPA liver disorders , in UCI Machine Learning Repository to test the performance of our proposed semi supervised boosting algorithms . We use 15 % as labeled data and 85 % as unlabeled data . These unlabeled data are used as the test data in the experiment . Table 2 shows the results of the classification error rates on test data .
Logit
Data 2743(152 ) Bala 2250(252 ) Pima Wins 514(074 ) BUPA 3724(559 )
Assemble 2576(147 ) 2087(347 ) 415(112 ) 3617(340 )
MI
2480(172 ) 2044(375 ) 292(077 ) 2984(379 )
Entropy
2410(202 ) 1987(303 ) 377(107 ) 3177(231 )
Table 2 : Error rates ( % ) on four benchmark UCI data sets where the corresponding values of regularization parameter fl for entropy based method are 0.01 , 0.001 , 0.10 and 0.01 and those for mutual information based method are 0.007 , 0.001 , 0.10 and 010
This experiment shows that our semi supervised learning algorithms can get helpful information from unlabeled data and significantly improve the classification results . 5.3 Human Mental Workload Data
We now report results on a real world problem : human mental workload classification task in modern aviation systems . Mental workload refers to the information processing demands imposed on the operator by the performance of cognitive tasks , accurate and reliable real time assessment of operators’ cognitive states , ie their mental abilities to carry out the jobs in time is the key for successful implementation of adaptive human aiding techniques in modern aviation systems , such as uninhabited air vehicles ( UAVs ) and uninhabited combat air vehicles ( UCAVs ) . One of the measures of mental workload is the Electroencephalogram ( EEG ) , a measurement of electrical activity produced by the brain as recorded from electrodes placed on the scalp , and it is used to predict human workload .
In this experiment , we use real EEG data to model human work load . The input variable X is a 105 dimensional vector and the class label Y has three states \Low" , \Medium" and \High" .
First we combine the \Low" class and \Medium" class together as one class and solve a binary classification problem . We set the number of labeled data to be N = 30 , we then increase the number of unlabeled data from 60 to 240 . We use development data to choose the best value of regularization parameter . The size of development data is 50 . We repeat this process 10 times .
Figure 5 and Table 3 show the results of the classification error rate on EEG test data using both entropy and mutual information on unlabeled EEG data as regularization terms when we increase the size of unlabeled EEG data .
Finally we consider 3 class cases of EEG data , in which we do not combine \Low" class and \Medium" class . In this experiment , the number of labeled data is 30 , and the number of unlabeled data is 70 . Unlabeled data are still the s r o r r e t s e T
0.19
0.18
0.17
0.16
0.15
0.14
0.13
0.12
0.11
2
LogitBoost
Assemble.LogitBoost
Semi Entropy
Semi Mutual Information
3
4
5
6
7
8
Ratio of the size of unlabeled data and labeled data
Figure 5 : Test errors for LogitBoost ( blue ) , Assemble.LogitBoost ( red ) , semi supervised entropy based boosting ( magenta ) and semi supervised mutual information based boosting ( yellow ) on EEG data when we increase the size of unlabeled data .
Ratio 2 4 6 8
Logit
1778(301 ) 1821(323 ) 1846(163 ) 1763(178 )
Assemble 1732(232 ) 1603(217 ) 1624(225 ) 1693(184 )
MI
1667(260 ) 1375(259 ) 1280(231 ) 1229(168 )
Entropy
1625(370 ) 1361(370 ) 1313(251 ) 1167(110 )
Error
Table 3 : rates on EEG test data for both entropy and mutual information based semisupervised boosting when varying the ratio of the size of unlabeled and labeled EEG data , where the corresponding values of regularization parameter fl for both entropy based method are 0.015 , 0.007 , 0.015 and 0.009 and mutual information based method are 0.0015 , 0.0009 , 0.0005 and 00005 test data . We repeat this process 10 times . The error rate of LogitBoost is 32.94 % ( 2.47 ) ; the error rate of Assemble.LogitBoost is 31.01 % ( 1.97 ) ; the error rate of our entropy semi supervised boosting algorithm is 29.43 % ( 2.44 ) when fl = 0:07 , and the error rate of mutual information based semi supervised boosting algorithm is 30.58 % ( 2.51 ) when fl = 0:05 .
6 . CONCLUSION
In this paper , we present semi supervised boosting learning where information theoretic terms , both entropy and mutual information , are used to encode the information provided by unlabeled data and behave as data dependent priors . The combined loss functions are non convex , we derive simple sequential gradient descent optimization algorithms and test these algorithms on synthetic , benchmark and real world tasks . Experimental results show that by exploiting the availability of auxiliary unlabeled data , our proposed semi supervised boosting algorithms can impressively improve the performances of both supervised boosting algorithms and a state of the art semi supervised boosting algorithm . We are working on a formal analysis to give some theoretical justifications on why these information measures can be used to improve classification performance .
7 . ACKNOWLEDGEMENT
The authors wish to thank researchers at the 711th HPW/ RHCP lab of the Wright Patterson Air Force Base for providing them the EEG human mental workload classification dataset .
8 . REFERENCES [ 1 ] K . Benett , A . Demiriz and R . Maclin . Exploiting unlabeled data in ensemble methods . The 8th International Conference on Knowledge Discovery and Data Mining , 289 296 , 2002 .
[ 2 ] D . Bertsekas . Nonlinear Programming , 2nd Edition ,
Athena Scientific , 1999 .
[ 3 ] A . Blum and T . Mitchell . Combining labeled and unlabeled data with co training . The Workshop on Computational Learning Theory , 92 100 , 1998 .
[ 4 ] S . Boyd and L . Vandenberghe . Convex Optimization ,
Cambridge University Press , 2004 .
[ 5 ] L . Breiman . Prediction games and arcing classifiers .
Neural Computation , 11:1493 1517 , 1999 .
[ 6 ] V . Castelli and T . Cover . The relative value of labeled and unlabeled samples in pattern recognition with an unknown mixing parameter . IEEE Trans . on Information Theory , 42(6):2102 2117 , 1996 .
[ 7 ] G . Celeux and G . Govaert . A classification EM algorithm for clustering and two stochastic versions . Computational Statistics and Data Analysis , 14:315 332 , 1992 .
[ 8 ] O . Chapelle , B . Scholk(cid:127)opf and A . Zien .
Semi Supervised Learning , MIT Press , 2006 . [ 9 ] K . Chen and S . Wang . Regularized boost for semi supervised learning . Advances in Neural Information Processing Systems 20 , 2007 .
[ 10 ] I . Cohen and F . Cozman . Risks of semi supervised learning . Semi Supervised Learning , O . Chapelle , B . Scholk(cid:127)opf and A . Zien , 55 70 , MIT Press , 2006 . [ 11 ] M . Collins , R . Schapire and Y . Singer . Logistic regression , AdaBoost and Bregman distances . Machine Learning , 48(1 3):253 285 , 2002 .
[ 12 ] A . Corduneanu and T . Jaakkola . Data dependent regularization . Semi Supervised Learning , O . Chapelle , B . Scholk(cid:127)opf and A . Zien , 163 182 , MIT Press , 2006 .
[ 13 ] T . Cover and J . Thomas . Elements of Information
Theory , John Wiley & Sons , 1991 .
[ 14 ] F . d’Alche Buc , Y . Grandvalet and C . Ambroise .
Semi supervised marginBoost . Advances in Neural Information Processing Systems 14 , 553 560 , 2002 .
[ 15 ] S . Della Pietra , V . Della Pietra and J . Lafferty .
Inducing features of random fields . IEEE Trans . on Pattern Analysis and Machine Intelligence , 19(4):380 393 , 1997 .
[ 16 ] Y . Freund and R . Schapire . Experiments with a new boosting algorithm . The 13th International Conference on Machine Learning , 148 156 , 1996 . [ 17 ] Y . Freund and R . Schapire . A decision theoretic generalization of on line learning and an application to boosting . Journal of Computer and System Sciences , 55(1):119 139 , 1997 .
[ 18 ] J . Friedman , T.Hastie and R . Tibshirani . Additive logistic regression : A statistical view of boosting . The Annals of Statistics , 28(2):337 407 , 2000 .
[ 19 ] Y . Grandvalet and Y . Bengio . Semi supervised learning by entropy minimization . Advances in Neural Information Processing Systems , 17:529 536 , 2004 .
[ 20 ] G . Haffari , Y . Wang , S . Wang , G . Mori and F . Jiao .
Boosting with incomplete information . The 25th International Conference on Machine Learning , 368 375 , 2008 .
[ 21 ] T . Hastie , R . Tibshirani , J . Friedman . The Elements of Statistical Learning : Data Mining , Inference , and Prediction , 2nd Edition , Springer , 2009 .
[ 22 ] J . Janodet , R . Nock , M . Sebban and H . Suchier . Boosting grammatical inference with confidence oracles . The 21st International Conference on Machine Learning , 54 61 , 2004 .
[ 23 ] F . Jiao , S . Wang , C . Lee , R . Greiner and D .
Schuurmans . Semi supervised conditional random fields for improved sequence segmentation and labeling . The Joint 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics , 209 216 , 2006 .
[ 24 ] G . Lebanon and J . Lafferty . Boosting and maximum likelihood for exponential models . Advances in Neural Information Processing Systems 14 , 447 454 , 2002 .
[ 25 ] C . Lee , S . Wang , F . Jiao , D . Schuurmans and R . Greiner . Learning to model spatial dependency : Semi supervised discriminative random fields . Advances in Neural Information Processing Systems 19 , 793 800 , 2007 .
[ 26 ] L . Mason , J . Baxter , P . Bartlett and M . Frean .
Functional gradient techniques for combining hypotheses . In Advances in Large Margin Classifiers , A . Smola , P . Bartlett , B . Scholk(cid:127)opf and D . Schuurmans , editors , 221 246 , MIT Press , 2000 .
[ 27 ] K . Nigam , A . McCallum , S . Thrun and T . Mitchell .
Text classification from labeled and unlabeled documents using EM . Machine Learning . 39(2/3):135 167 , 2000 .
[ 28 ] S . Roberts , R . Everson and I . Rezek . Maximum certainty data partitioning . Pattern Recognition , 33(5):833 839 , 2000 .
[ 29 ] R . Schapire . The strength of weak learnability .
Machine Learning , 5(2):197 227 , 1990 .
[ 30 ] H . Valizadegan , R . Jin and A . Jain . Semi supervised boosting for multi class classification . The European Conference on Machine Learning and Knowledge Discovery in Databases , 522 537 , 2008 .
[ 31 ] Y . Wang , G . Haffari , S . Wang and G . Mori . Rate distortion based semi supervised discriminative learning . Technical Report , 2009 .
[ 32 ] D . Zhou , O . Bousquet , T . Navin Lal , J . Weston and
B . Sch(cid:127)olkopf . Learning with local and global consistency . Advances in Neural Information Processing Systems , 16:321 328 , 2004 .
[ 33 ] J . Zhu , S . Rosset , H . Zhou and T . Hastie . Multiclass
AdaBoost . Technical Report , 2005 .
[ 34 ] X . Zhu , Z . Ghahramani and J . Lafferty .
Semi supervised learning using Gaussian fields and harmonic functions . The 20th International Conference on Machine Learning , 912 919 , 2003 .
