Characteristic Relational Patterns
Arne Koopman
Department of Computer Science
Universiteit Utrecht koopman@csuunl
Arno Siebes
Department of Computer Science
Universiteit Utrecht arno@csuunl
ABSTRACT Research in relational data mining has two major directions : finding global models of a relational database and the discovery of local relational patterns within a database . While relational patterns show how attribute values co occur in detail , their huge numbers hamper their usage in data analysis . Global models , on the other hand , only provide a summary of how different tables and their attributes relate to each other , lacking detail of what is going on at the local level .
In this paper we introduce a new approach that combines the positive properties of both directions : it provides a detailed description of the complete database using a small set of patterns . More in particular , we utilise a rich pattern language and show how a database can be encoded by such patterns . Then , based on the MDLprinciple , the novel RDB KRIMP algorithm selects the set of patterns that allows for the most succinct encoding of the database . This set , the code table , is a compact description of the database in terms of local relational patterns . We show that this resulting set is very small , both in terms of database size and in number of its local relational patterns : a reduction of up to 4 orders of magnitude is attained .
Categories and Subject Descriptors H24 [ Database Management ] : Systems—Relational databases ; H28 [ Database Management ] : Database Applications—Data Mining
General Terms Algorithms , Experimentation
Keywords Frequent Patterns , Relational Data Mining , MDL
1 .
INTRODUCTION
Relational data mining seeks to generalise traditional , singletable data analysis to the analysis of multiple inter related tables .
As relational models are more expressive than their single table counterparts , they allow for a more succinct description of the database than when one has to summarize each and every table and all their connections separately . Current research in relational data mining follows one of two directions . Either one aims to find a ‘global’ model of the complete database , or one seeks interesting ‘local’ patterns in the database .
Both directions have their specific merits and shortcomings . A ‘global’ database model , for example a Probabilistic Relational Model ( PRM ) [ 9 ] , is often small and interpretable , but it lacks detail as it only shows how the attributes of the tables interact . On the other hand , one can use an approach like WARMR to find frequent patterns [ 1 , 3 ] . These patterns can be regarded as ‘local’ models that describe a partial structure of the relational database . A well known drawback of this method is the exponential pattern set growth when mining for less frequent , but often interesting , patterns .
In this paper we present a new approach , RDB KRIMP , that combines the strengths of both approaches . It finds a global model that describes the complete database using only a small set of characteristic relational patterns . While collectively the patterns show the global structure , individually they reveal the local interactions in the database . A global model that describes the complete relational database must capture the behaviour of all tables and their interactions , and thus requires a focus on all tables . While frequent pattern mining techniques prune the pattern search space using a single ‘target’ table , all tables function as ‘targets’ in our global approach . Hence , our pattern language is essentially that of FARMER [ 13 ] except we do not restrict the patterns to have their ’roots’ in a single target table .
Given this pattern set , we use the Minimum Description Length ( MDL ) principle [ 7 ] to select a small set of characteristic patterns for the database . RDB KRIMP uses the MDL principle to find those patterns that together describe the database well . In contrast to KRIMP [ 14 ] , this enriched pattern language allows RDB KRIMP to find more complex patterns in the database . However , finding these richer models for relational databases requires a novel lossless encoding scheme that relies on its ordering . During the encoding , RDB KRIMP reorders the tuples based on the patterns of the model such that the encoded database can always be decoded in a lossless fashion . RDB KRIMP proves effective : with its lossless encoding , it can find models that stay compact and utilise our enhanced pattern language .
After presenting our theoretical framework and the RDB KRIMP algorithm , we validate our claims with experimental results obtained on publicly available KDDcup databases . We show that the global models stay compact , in contrast to the original candidate pattern sets that grow large for lower minimum supports .
Furthermore , we show that we attain good results as a result of the specific characteristics of our enhanced pattern language . We show that simpler pattern sets lead to worse results and that target table based approaches are much less effective . We conclude by discussing the readability of our models , and their related work .
2 . DATA AND PATTERNS
In this section we formally define the data type , relational patterns , how such patterns occur in the database , and how to calculate the support of a pattern . 2.1 Data
We assume that the data resides in a multi relational database in which the relations between the tuples in the various tables is coded , as usual , via foreign keys . We assume that any pair of two tables have at most one foreign key relationship between them . This is without loss of generality as databases can always be losslessly recoded such that this assumption holds . Moreover , we assume that all attributes of all tables have a categorical domain . To introduce our notation , we give a brief formal description . The database db consists of a set of tables , db = {T 1 , . . . , T n} , and we assume that all the table names ( the T i ) are unique . Each table T has a schema S(T ) . This schema consists of a key , 0 or more foreign keys , and 1 or more attributes , i.e , S(T i ) = ( K i,F i,Ai ) in which :
• K i is the key . Without loss of generality , we assume that the key name is unique . Its domain , D(K i ) , is a set of integers . • F i is a set of 0 or more foreign keys . We assume that the database schema is consistent , that is
– For each foreign key F i j of T i , there is a table , say , T l ∈ db for which it is the key , ie , F i j = K l . As noted above , we assume that there is at most one foreign key in T i that refers to K l
– The domain of F i j is the domain of that K l .
• Ai consists of 1 or more attributes . Each attribute Ai k has a categorical domain D(Ai k ) .
• Summing up , the domain of table T i denoted by D(T i ) is the cartesian product of all domains involved , ie ,
Y
Y
Figure 1 : An illustrative relational database : an excerpt from the Financial database .
• k ∈ D(K i ) , • the tuple has one entry for each F i each Ai j ∈ Ai . j ∈ F i and one entry for j refers , then kj ∈ D(K l ) ,
• let Kl be the key to which F i • for referential integrity on the database db , we have that for ( t ) = any tuple t ∈ T i , there is a tuple t ∈ T l such that πF i πKl ( t ) . l
• vl ∈ D(Ai l ) .
Again as usual , we will suppress the labels in tuples whenever possible . That is , we simply write ( 40 , 10 , Owner ) ∈ DISPOSITION for a tuple in our example database in Figure 1 . 2.2 Patterns
The prototypical example of patterns are item sets . In the case of a ( single ) table of categorical data , an item set generalises to a selection . Clearly such patterns should be included into our pattern language . However , “ true ” relational patterns should cross multiple tables . That is , they should describe related selections over multiple tables .
After formally introducing our pattern definition , we will illus trate it with an example .
DEFINITION 1
( PATTERN ) . Let db = {T 1 , . . . , T n} be a data base for which each table T i has schema S(T i ) = ( K i,F i,Ai ) . • Let {A1 , . . . , Al} ⊆ Ai and let vj ∈ D(Aj ) , then the ex pression p defined as
D(T i ) = D(K i ) ×
D(F i j ) × j ∈F i F i k∈Ai Ai
D(Ai k ) p = T i({A1 = v1 , . . . , Al = vl} ) is a pattern for T i ; this is denoted by p ∈ P i .
In our example database , shown bottom right in Figure 1 , the DISPOSITION table has as key : dispID ( depicted in bold ) and as foreign key accountID . Moreover , it has an attribute Type whose domain is : {Owner , Disponent} .
Next to a schema , each table has an extend , consisting of a set of tuples . As usual , we blur the distinction between the table and its extend and say that a tuple t is in table T i ; denoted by t ∈ T i . The database as whole should satisfy referential integrity . That is , foreign key values in a tuple refer to existing tuples in the table for which this foreign key is the key . More formally we have :
A tuple for table T i with S(T i ) = ( K i,F i,Ai ) is given by : t = ( K i = k,{F i j = kj},{Ai l = vl} ) in which :
• Let T i have key K i , moreover , let K i be a foreign key of T j ; l = K i . Let p0 ∈ P i that is , there is an F j and let {p1 , . . . pk} ⊆ P j . The expression p defined as l ∈ F j such that F j is a pattern for T i , ie , p ∈ P i . p = p0[p1 , . . . , pk ]
• Let T i have key K i , moreover , let K i be a foreign key of the q tables T j1 , . . . , T jq , such that if r = s , then T jr = T js . Let p0 ∈ P i and for l ∈ {1 , . . . , q} , let {pl kl} ⊆ P jl . The expression p defined as
1 , . . . pl p = p0[[p1
1 , . . . , p1 k1 ] , . . . , [ pq
1 , . . . pq kq
] ] is a pattern for T i , ie , p ∈ P i .
1006/200721103/200631208/200631303/200621405/2008106/200809/200808/200609/200605/200806/200809/200820101411000UVERNULLST21103592000SIPOSIPOSIPOQR22118508501000YZ2328324131000ST2000OP1330101024512A31101372224B32112731336B33122714712B34122719436D351820312C3613133028912B4010OWNEROWNEROWNERDISPONENTDISPONENT4111421143124412FrequencyT1 = ACCOUNTDateaccountIDT4 = DISPOSITIONaccountIDTypedispIDDateT2 = LOANaccountIDAmountDurationPaymentloanIDkkjv1v2Ai1Ai2TiFij KiBank ToT3 = ORDERaccountIDAmount ToAmountTypeordernID The third component is a generalisation of the second , which is included to simplify part of the following definitions .
In our pattern definition we define the alphabet patterns as those patterns that select one attribute and assign it to one value ( ie p = T i(Aj = vj) ) .
Also , we define the size of a pattern as the number of attributes within the pattern : size(p = T i({A1 = v1 , . . . , Aj = vj} ) ) = j size(p0[[p1 km ] ] ) = k1 ] , . . . , [ pm
1 , . . . , p1
Pm
Pki
1 , . . . pm j ) . j=1 size(pi size(p0 ) + i=1
Using our example database shown in Figure 1 , we can now illustrate our pattern definition . An example of the first component , a single table selection on the ACCOUNT table , would be ACCOUNT({Frequency =2} ) . This pattern can be seen in the database in the set of tuples with account ids 10 and 13 .
The second component allows a pattern to have multiple selections from a table . As an example , in our database we see the pattern that accounts of Frequency 2 have dispositions of type Owner ( seen at account id 10 ) . A more complex example is that there are accounts of Frequency 3 that have both dispositions of type Owner and dispositions of type Disponent . For this last pattern , one tuple in the ACCOUNT table is related to two distinct tuples in the DISPOSITION table ( account ids 11 and 12 ) . This last pattern is represented as follows : ACCOUNT({Frequency = 3} ) [ DISPOSITION({Type = Owner} ) , DISPOSITION({Type = Disponent}) ] .
The third component extends this last example by allowing the patterns to span more than two tables . Note that in our pattern language a pattern can ’s tart’ at one of the tables T ∈ {T 1 , . . . , T n} . We do not restrict the ’root’ of these patterns to be at one specific target table , because interesting patterns within the database can start from all possible tables . Furthermore , as one single tuple can be joined with multiple other tuples , the structure of our patterns matches the complexity of the database .
Having a richer pattern language is no virtue in its own right . Alternatively , we could have used pattern languages such as those used in WARMR [ 3 ] , FARMER [ 13 ] , or in our earlier work [ 10 ] . The experiments however show that our pattern set , consisting of FARMER pattern sets without a fixed target table , allows us to capture more important structure within the data . 2.3 Pattern Occurrences
Our the patterns can become rather complicated structures , containing lists of lists of lists of . . . of tuples . Hence , it is illustrative to consider what the domain of such patterns is . That is , what does an instance look like ? The definition of these domains follows the inductive structure of the patterns .
DEFINITION 2
( DOMAIN ) . Let db = {T 1 , . . . , T n} be a database for which each table T i has schema S(T i ) = ( K i,F i,Ai ) . Moreover , let p ∈ P i . The domain of p , denoted by D(p ) , is given by
• If p = T i({A1 = v1 , . . . , Aj = vj} ) , then D(p ) = D(T i ) . • If p = p0[p1 , . . . , pk ] , then
D(p ) = D(p0 ) × [ D(p1 ) , . . . , D(pk) ] .
• If p = p0[[p1
1 , . . . , p1 k1 ] , . . . , [ pq
1 , . . . pq kq
] ] , then D(p ) =
D(p0 ) × [ [D(p1
1 ) , . . . , D(p1 k1 ) ] , . . . , [ D(pq
1 ) , . . . D(pq kq
)] ] .
Algorithm 1 Generate Pθ
P i
Generate Pθ(db , θ ) 1 : for all T i ∈ db do 2 : 3 : end for 4 : Pθ = i P i 5 : return Pθ
S
θ
θ = F ARM ER(db , θ , target = T i )
Note that this domain definition is broad : it does not enforce that patterns describe related tuples . The reason for this liberal definition is that here we are only interested in the general structure of the domain . Referential integrity does play its role in the definition of an occurrence of a pattern . While these domains may have a rather complicated structure , there is some simplicity . For a pattern p ∈ P i , the domain is either D(T i ) or it is the cartesian product of D(T i ) with a complicated list domain . That is , D(p ) = D(T i ) × X , in which X denotes a list domain . This observation has a useful consequence . It means that if p is a pattern for T i ( ie , p ∈ P i ) , and t is an instance of p ( i.e , t ∈ D(p) ) , we can project t on the keys , the foreign keys and the attributes of T i . We will use these projections in the definition of an occurrence .
An occurrence of a pattern in the database is an instance of that pattern in the database . However , different from these instances , for occurrences we do require referential integrity . That is , the occurrences should consist of related tuples only . This makes the definition slightly more complex .
DEFINITION 3
( OCCURRENCE ) . Let db = {T 1 , . . . , T n} be a database for which each table T i has schema S(T i ) = ( K i,F i , Ai ) . Moreover , let p ∈ P i .
• If p = T i({A1 = v1 , . . . , Aj = vj} ) , occ(p ) is the set of those tuples t ∈ T i for which
∀k ∈ {1 , . . . , j} : πAk ( t ) = vk
• If p = p0[p1 , . . . , pk ] , we know that p0 ∈ P i and that there is a table T j such that firstly {p1 , . . . , pk} ⊆ P j and secondly that the key K i of T i is a foreign key , say F j l of T j . An instance t = ( t0 , [ t1 , . . . , tk ] ) of p is an occurrence of p , denoted by t ∈ occ(p ) , if :
– t0 ∈ occ(p0 ) and for m ∈ {1 , . . . , k} : tm ∈ occ(pm ) .
– for m , n ∈ {1 , . . . , k} : m = n → tm = tn – for m ∈ {1 , . . . , k} : πF j ( tm ) = πKi ( t0 ) k1 ] , . . . , [ pq
1 , . . . , p1
1 , . . . pq kq l
• If p = p0[[p1 t ∈ D(p ) given by
] ] , then an instance
1 , . . . , t1 t = ( t0 , [ [t1 k1 ] , . . . , [ tq is an occurrence of p ( ie , t ∈ occ(p) ) , if ∀l ∈ {1 , . . . , q} : ( t0 , [ tl
1 , . . . , tl
1 , . . . , tq kq
] ] ) kl ] ) ∈ occ(p0[pl
1 , . . . , pl kl ] ) .
As usual , the support of a pattern is the number of its occurrences . We say that a pattern is frequent if the support exceeds some userdefined threshold called the minimum support θ .
Note that we use lists in our occurrence definition . Each occurrence can span multiple tuples within one table T i that are stored in a list [ ti ki ] . We will preserve this order of the tuples as it
1 , . . . , ti
Algorithm 2 REORDERDB return false
REORDERDB ( reorder , [ t1 , . . . , ti ] ) 1 : if [ t1 , . . . , ti ] ∩ reorder ⊆ord reorder then reorder = reorder ∪ [ t1 , . . . , ti ] 2 : return true 3 : 4 : else 5 : 6 : end if A : [ a1 , . . . , an ] ⊆ord B : [ b1 , . . . , bm ] 1 : if A = ∅ then return true 2 : 3 : else if B = ∅ then 4 : 5 : else if a1 = b1 then 6 : 7 : else if a1 = b1 then 8 : 9 : end if return false return [ a2 , . . . , an ] ⊆ord [ b2 , . . . , bm ] return [ a1 , . . . , an ] ⊆ord [ b2 , . . . , bm ] a code table CT is a two column table . On the left hand side reside relational patterns as defined above , on the right hand side reside the codes . The codes are taken from a prefix code C . For each code , we calculate a Shannon entropy based length : more frequently used codes obtain smaller lengths . Further , the number of patterns in the code table is denoted by |CT| .
We encode the relational database using the patterns from a code table CT . Figure 2 shows an example on how this encoding comes about through a database cover . Here , we show two patterns that ( partially ) cover the database . Using our pattern notation , we write the first code table pattern as :
ACCOUNT({Frequency = 2})[
[ ORDER({Bank To = ST , Amount = 1000} ) , ORDER({Amount = 2000 , Type = SIPO} ) ] [ LOAN({Date = 06/2008 , Duration=12} ) , LOAN({Date = 09/2008 , Payment=B}) ] ]
We cover the database by replacing the related attribute values with the code of the pattern . As this pattern occurs at two yet uncovered locations in the database ( id=10 and 13 ) , it is used to describe this part of the database . Note that in this figure each code table element has its own distinct colour .
In order for the encoding to be lossless , we need to be able to decode every part of the occurrence . As an occurrence may span multiple tuples within the database , we need to write the code at each tuple covered by this occurrence . Furthermore , as each pattern has just one code the necessity of a database and pattern order becomes clear : we need to know which tuple is covered with which pattern from the list .
In the first pattern ( p1 in fig .
We match the order of the tuples within the database with the order of the pattern . ( 2) ) , LOAN : {Date = 06/2008 , Duration = 12} is ordered before LOAN : {Date = 09/2008 , P ayment = B} . Note the swap of tuples 35 and 36 to align the database order with the order of the pattern . Once covered , the complete database is encoded and looks like a mosaic , which can be decoded using the database order , and the code table ( see fig . 2 ) .
So , for unambiguous decoding , the order of the tuples in the database has to be aligned with the order in the code table patterns . Therefore , we allow a ( partial ) re ordering of the tuples in the database . However , a pair of tuples is only re ordered once ,
Figure 2 : The database is partially covered with the two first patterns of the code table using RDB KRIMP . The uncoloured part of the database is covered by alphabet patterns . Note that for a lossless decoding we incorporate the database order ( seen at the swap of LOAN:loan id=35 and 36 ) . is essential to encode the database in a lossless manner , as we will show below .
Given this pattern definition , in order to derive the set of frequent patterns Pθ one can resort to existing relational mining algorithms like FARMER [ 13 ] or attribute tree miners like FATminer [ 2 ] . Partial frequent pattern sets generated by either approach can be combined into Pθ ( see Algorithm 1 ) .
Finally , we define a canonical order on our patterns . We assume for each table T i , each attribute Aj , and each attribute value vk a unique ( string ) label . We denote by l(X ) the unique label assigned to X ( X ∈ {T i , Aj , vk} ) . Canonical forms are simply strings , hence we have the familiar lexicographic order , denote by <lex , on them . Using this order , we define : canonical(p = T i({A1 = v1 , . . . , Aj = vj} ) ) = l(T i ) : {l(A1 ) : l(v1 ) , . . . , l(Aj ) : l(vj)} canonical(p0[[p1 canonical(p0 ) :m
1 , . . . , p1 i=1:ki
1 , . . . pm k1 ] , . . . , [ pm j=1 canonical(pi j ) km ] ] ) =
This allows us to define a canonical order on our patterns : p0 <can p1 iff canonical(p0 ) <lex canonical(p1 ) .
3 . PROBLEM STATEMENT
Now we have defined our patterns and database , we can present our problem formally . In order to find a good global model for our relational database , we use the minimum description length ( MDL ) [ 7 ] principle , which is a practical application of Kolmogorov Complexity [ 11 ] . Given a set of models H , we want to find a model H that minimises L(H ) + L(D|H ) , in which
• L(H ) is the length , in bits , of the description of H , and • L(D|H ) is the length , in bits , of the description of the data
D when encoded with H .
The data that is encoded by our model resides within a relational database as defined in Section 2 , or more specifically within its attribute data . Similar to [ 10 , 14 ] our models are code tables . Such
1006/200721103/200631208/200631303/200621405/200811503/200631606/200706/200809/200808/200609/200605/200806/200809/2008220101411000UVERNULLST21103592000SIPOSIPOSIPOQR22118508501000YZ2328324131000ST2000OP1330101024512A31101372224B32112731336B33122714712B34122719436D351820312C3613133028912B4010OWNEROWNEROWNERDISPONENTDISPONENT4111421143124412FrequencyT1 = ACCOUNTDateREORDERDBaccountIDT4 = DISPOSITIONaccountIDTypedispIDDateT2 = LOANfrequency(P1 ) = 2 , count(P1 ) = 10 , size(P1 ) = 9P1 : ACCOUNT({ Frequency = 2 } ) [ [ ORDER({ Bank To=ST , Amount=1000 } ) , ORDER({ Amount=2000 , Type=SIPO } ) ] , [ [ LOAN({ Date=’06/2008’ , Duration=12 } ) , LOAN({ Date=’09/2008’ , Payment=B } ) ] ]P2 : ACCOUNT({ Frequency = 3 } ) [ [ DISPOSITION({ Type = Disponent } ) , DISPOSITION({ Type = Owner } ) ] ]frequency(P2 ) = 2 , count(P2 ) = 6 , size(P2)=3Partially Covered DatabaseaccountIDAmountDurationPaymentloanIDBank ToT3 = ORDERaccountIDAmount ToAmountTypeordernID33OWNEROWNERDISPONENTDISPONENT06/200806/200809/200806/200809/200809/200806/200806/200809/200806/200809/200809/20081212BB2STSTST1000200010001000200010002000SIPOSIPOSIPO Algorithm 3 COVER and COVERDB
Algorithm 4 RDB KRIMP if REORDERDB(reorder,{t} ) then end if
{At} = {At} \ p f requency++ if p ⊆ {At} then
COVER ( db , p , reorder ) 1 : f requency = 0 2 : for all {t} ∈ occ(p ) do 3 : 4 : 5 : 6 : 7 : end if 8 : 9 : end for 10 : return f requency COVERDB ( db , CT ) 1 : reorder = ∅ 2 : for all ci ∈ CT do 3 : 4 : 5 : end for f requency(ci ) = COVER(db , ci , reorder ) count(ci ) = f requency(ci ) × |coverspots(ci)| otherwise the unambiguous decoding property will be lost . Hence , we keep track of the order . Initially this ordered list , reorder , is empty . Whenever two ( or more tuples ) that are not yet in the list are re ordered their identifiers are appended .
We cover the database with patterns p until the database is completely covered . An occurrence of a pattern can only cover the database if the database order can be aligned to match the pattern . This is the case when the tuples related to the occurrence are not yet ordered , or if they are already ordered in the correct order ( eg the order of the tuples matches the order of the pattern ) . We update and check partial database order via the REORDERDB algorithm ( see Algorithm 2 ) . The algorithm takes as input the current ( partial ) database order reorder and a list of tuples of the current occurrence [ t1 , . . . , ti ] . Initially the database order reorder is an empty list as the database is unordered . To check whether [ t1 , . . . , ti ] ∩ reorder is an ordered subset of reorder we use the ⊆ord operator ( line 1 ) . If so , the order is updated by appending the current list of tuples that are not yet part of the database order ( 2 ) . Otherwise , this particular occurrence cannot be used to cover the database .
Now that we can adjust reorder to align with a pattern occurrence , we can partially cover a database given a single code table pattern ( see Algorithm 3 ) . For each pattern , we have a set of occurrences occ(p ) ( 1 ) . COVER iterates over all occurrences and evaluates whether it can be covered ( 3 ) . We only cover the current occurrence ( the list of tuples ) if all related attributes are still uncovered ( 4 ) , and if the current occurrence is an ordered subset of the current database order ( 5 ) . If so , the attributes of the occurrence are covered and the frequencies are updated ( 6,7 ) . We define the frequency of a pattern as the number of times we cover the database with it .
Using the code table CT , we cover the complete relational database using the COVERDB algorithm ( see Algorithm 3 ) . Considering the patterns in the code table , it covers the database using the COVER algorithm ( 2 3 ) . During the cover process , we obtain for each code table pattern its f requency ( line 3 ) .
To be able to decode the encoded database , we have to write its code at each involved tuple . We define coverspots as the set of tuples at which we need to write down the code for pattern p . More
RDB KRIMP ( db , Pθ ) 1 : CT = CTinit 2 : for all c ∈ Pθ do 3 : 4 : 5 : 6 : end if 7 : 8 : end for
CT = CTnew
CTnew = CT + c in order COVERDB(db , CTnew ) if L(CTnew , db ) < L(CT , db ) then formally , we define coverspots as : coverspots(p = T i({Ai = vi} ) ) = {t} coverspots(p0[[p1 coverspots(p0 ) ∪m
1 , . . . , p1 i=1 ∪ki k1 ] , . . . , [ pm 1 , . . . pm km ] ] ) = j=1coverspots(pi j )
We denote the number of coverspots by : |coverspots(c)| . Consider the first pattern , p1 , from Figure 2 . The number of coverspots is 5 , as each occurrence has five distinct tuples in the database associated with it ( one in ACCOUNT , two in ORDER , and two in LOAN ) . We denote the count as the total number of times we have to write the code given a code table pattern c : count(c ) = f requency(c ) × |coverspots(c)| ( line 4 ) .
Given the obtained counts , we can now determine the code length for each code table element ci . For optimal encoding , we use a Shannon code , ie ,
P
LCT ( ci ) = − log( count(ci ) cj∈CT count(cj )
) .
Each code table element c has a standard length , which is the decoded length using alphabet patterns only : Lst(c ) . The encoded length of the complete code table then is :
L(CT ) =
LCT ( ci ) + Lst(ci ) .
In covering the database , we write count(ci ) codes for each code table element . Given the code lengths , the encoded size of the database becomes
LCT ( db ) =
LCT ( ci ) × count(ci ) .
In our approach , we define the total encoded size as :
L(db , CT ) = L(CT ) + LCT ( db ) .
Now that we can derive the total encoded size of the database given a code table , we can formulate our problem as follows .
PROBLEM STATEMENT . Let db = {T 1 , . . . T n} be a relational database as defined in Section 2 . Find the code table CT that minimises L(CT , db ) .
4 . RDB KRIMP ALGORITHM
Finding the optimal code table , the one that compresses the database best , is a very hard problem . The search space is extremely large and there is no useful structure to prune it . Hence , we have to use heuristics and we therefore resort to an approach similar to the one described in [ 14 ] .
The algorithm we introduce to this end , RDB KRIMP , approximates the optimal code table for a given database . It does this by starting with the simplest possible code table and iteratively testing
X ci∈CT
X ci∈CT
Table 1 : Characteristics of the used databases . Shown are for each table , the number of tuples ( #t ) , the number of attributes ( #a ) , the number of keys ( K and F ) ( #k ) , and the average number of joins a single tuple can make ( join ) .
FINANCIAL
GENES
HEPATITIS table
ACCOUNT CLIENT LOAN CARD DISP ORDER
GENES1 GENES2 INT META1 META2
BIO IFN OLAB PATIENT
#t 682 827 682 36 827 1513 862 862 910 4151 4151 694 198 31039 771
#a 2 2 5 2 1 3 4 4 2 4 4 5 4 3 3
#k 3 3 2 3 4 3 1 1 3 2 2 2 2 2 1 join 5.43 2 6.70 1 1.04 1 6.86 6.86 1 1 1 1 1 1 42.4 each pattern in a candidate set . A candidate pattern is only kept in the code table if it improves the compression . The RDB KRIMP algorithm is shown in pseudo code in Algorithm 4 .
RDB KRIMP starts with a database db and a frequent pattern set Pθ as input . To ensure that the complete database can be covered always , the code table is initialized with CTinit ( line 1 ) , which contains all alphabet elements ( ie p = T i(Aj = vj) ) . One by one , it takes a candidate pattern from Pθ and tests whether it contributes to improve compression ( 2 8 ) . To do this , a new code table CTnew is constructed by adding the candidate pattern to the previous code table CT ( 3 ) . Using this code table we compute a cover of the database ( 4 ) and the compressed sizes of the old and new code table are compared ( 5 ) . If the addition of the new candidate pattern improves compression , it is kept in the code table ( 6 ) . Otherwise , it is permanently discarded .
Note that we need to define two orders : the first on the candidate pattern set Pθ and the second on the patterns in the code table . For Pθ we define an order for all pattern pairs ( p1 , p2 ) : if support(p1 ) > support(p2 ) → p1 > p2 else if size(p1 ) > size(p2 ) → p1 > p2 else if p1 >can p2 → p1 > p2 else p1 ≤ p2
For the code table , we define the following order on all pattern pairs ( p1 , p2 ) : if size(p1 ) > size(p2 ) → p1 > p2 else if p1 >can p2 → p1 > p2 else if support(p1 ) > support(p2 ) → p1 > p2 else p1 ≤ p2
We assume that both the frequent pattern sets and code tables are always ordered in this fashion . Note that we are not after the actual attained compression , but rather the patterns that contribute to it . We use lossless compression as a means , not as a goal , to find a good set of descriptive patterns .
Figure 3 : Results for different minimum support values θ : ( a ) The encoded length obtained for the database , ( b ) the number of frequent patterns , and ( c ) the number of code table patterns in CT .
5 . EXPERIMENTS
To experimentally validate our approach we run experiments on publicly available relational data sets from previous KDD cups ( see Table 1 ) . These databases are : the financial 1 , genes interaction 2 , and hepatitis 3 databases . We use a frequent attributed tree miner [ 2 ] to generate the frequent pattern sets , as our relational patterns can be represented as attributed trees . 5.1 Describing the Database
In order to find the optimal model of the database , RDB KRIMP would ideally evaluate all patterns . However , in order to be efficient , we evaluate all frequent patterns . To measure the effect of the minimum support value , we generate a frequent candidate set Pθ for various θ . Given a Pθ we compress the database using RDB KRIMP , which results in a code table CT and an encoded database size L(CT , db ) .
The effect of sweeping the minimum support is shown in Figure 3a . For all used databases , we see that increasingly lower encoded database sizes are obtained for lower minimum support values .
The smaller encoded database sizes relate to the larger available sets of candidate patterns ( see Figure 3b ) . In all cases we see that this candidate set growth is exponential . These larger candidate sets contain more patterns that can be inserted in the code table to contribute to the database description .
While we see that Pθ grows exponentially for lower values of θ , we do not see this trend in the size of the code table ( see Figures
1http://lispvsecz/challenge/ 2http://pagescswiscedu/ dpage/kddcup2001/ 3http://lispvsecz/challenge/
0 5%10%15%20 % 25%30%35%40%45%50%70%75%80%85%90%95%70%0 5%10%15%20%25%30%35%40%45%50%0100200300400500Database CompressionCode Table GrowthCandidate Set Growth0 5%10%15%20%25%30%35%40%45%50%0123456101010101010(a)(b)(c)financialgeneshepatitisfinancialgeneshepatitisfinancialgeneshepatitisL(db , CT)log PθCTminsup ( θ ) %minsup ( θ ) %minsup ( θ ) % Figure 4 : Choosing a particular table as a target decreases performance . For the lowest used minimum supports , the use of a target table leads to a worse encoding on all databases . Even for the star shaped Hepatitis database , which seems well suited for a target table based approach .
3b and 3c respectively ) . Our code table grows for lower values of θ , but still only a small set is necessary to model the data . With respect to the original candidate set , our code tables achieve up to 4 orders of magnitude reduction in terms of number of patterns . 5.2
Initial Database Order
As described in Section 3 , RDB KRIMP re orders the database in order to ensure a lossless encoding for the database . As the initial database order is not determined by our algorithm , it can potentially influence the resulting tuple order . Therefore , we need to evaluate the extend of its influence on the resulting database order and the resulting encoded database length . In order to evaluate the effect of this initial order , we randomly shuffle the tuples within the tables . The experiments indicate that the influence on the resulting compression is minimal . The number of patterns used to encode the database is very similar to the original versions and lead to a similar database compression . The deviation for the database encoding for the financial , genes , and hepatitis database is respectively 1.12 % , 0.35 % , and 001 % Hence , RDB KRIMP is robust with respect to the initial database order . 5.3 Fixing a Target Table
In our approach , we generalise the FARMER pattern language such that we do not rely on a specific target table . We expect a better description of the database by considering patterns starting from all possible tables . In order to evaluate this , we compare the results from alternative trials using a fixed target table as is usual in relational data mining [ 3 , 13 ] . In these experiments , we encode the complete database using solely patterns that originate from a specific target table .
For all possible target tables , we determine how well we can approximate the original result that is obtained for the lowest used minimum support . We have depicted the deviation from the original result in Figure 4 . We see that fixing the candidate set to a specific target table has a negative influence on the encoding of the database . The compression deteriorates up to 24 % compared to the best obtained encoded size . This shows that allowing patterns to start at any table leads to a better description of the database .
The hepatitis database shows some additional interesting results . This database is a prime example of a target table based database ,
Figure 5 : Our generalised relational patterns lead to better results . While the number of candidates grows large for low minimum supports ( b ) , we obtain good database encodings ( a ) using compact code tables ( c ) ( shown for the genes database ) . as all ( medical test ) data surrounds a main table : patient . Even in this case , we see that picking a single target table leads to an increase in encoding length ( a worse code table ) . Apparently , we can describe some data better when we do not pick the patient table as the single target table . 5.4 Comparison to Other Pattern Types
In Section 2 , we defined a rich pattern language to match the database complexity . To measure whether or not we can describe the database better using more intricate patterns , we here compress the database with an increasingly more general pattern definition . A better database description would lead to smaller encoded database sizes .
A single complex pattern can describe structure in the database that would otherwise require multiple simpler patterns , possibly leading to a better compression . We have used the following characteristic pattern definitions to evaluate : Single Table Patterns . Here we only allow p = T i({A1 , . . . , Aj} ) patterns in our Pθ . In other words , all candidate patterns only cover one table , and no joins are allowed . With this candidate set , we compress the database solely with single table patterns .
WARMR like Patterns . In this candidate set , each pattern covers one tuple per table at most . In our notation , we define these patterns as : p = p0[p1 , . . . , pk ] . These patterns are similar to those used in WARMR like approaches , which use an existential quantifier to select strictly one tuple from a table . Unlike WARMR applications , we do allow these patterns to start at any table in the database , in stead of a single target table [ 3 ] .
Our language . Here we consider patterns as defined in Section k1 ] , . . . , ] ] . Recall that this is similar to grouping all FARMER
2 . In this set , the patterns are defined as p = p0[[p1 [ pq
1 , . . . , p1
1 , . . . pq kq target tabledeviation encoded db length %Target Table based Approximationsgenes1genes2meta1meta2interactionaccountclientloanordercarddispbioifnolabpatient0 5%10%15%20%25%financialgeneshepatitisL(db , CT)log PθCTminsup ( θ ) %minsup ( θ ) %minsup ( θ ) %4%6%8%10%12%14%16%18%20%262728293x 105Database CompressionCandidate Set GrowthCode Table Growth4%6%8%10%12%14%16%18%20 % 1001021031041054%6%8%10%12%14%16%18%20%0100200300400Single Table PatternsWARMR PatternsALL PatternsSingle Table PatternsWARMR PatternsALL PatternsSingle Table PatternsWARMR PatternsALL Patterns(b)(a)(c ) Table 2 : On all databases more general patterns lead to smaller encoded sizes for the database . single table L % #CT 29 91 % 72 87 % 99 % 5 financial genes hepatitis
WARMR
L % 76 % 86 % 98 %
#CT 130 191 13
L % 76 % 83 % 97 % all
#CT 117 342 26 generated pattern sets for each table .
Note that we order all three candidate sets as defined in Section 4 . For all above scenarios , we evaluate the effect on the candidate set size |Pθ| , the number of code table elements |CT| , and the compressed encoded size of the database L(CT , db ) .
The depicted result of the genes database in Figure 5a shows a typical result in terms of compressing the database . We see that the single table patterns lead to the worst encoded length for the database . A better compression can be derived when we allow WARMR patterns in our candidate set , which consequently can be improved by allowing all patterns to cover the database . We outline the obtained results on all databases in Table 2 .
As before , we see that the number of candidate patterns grow very steeply for lower minimum support values . Also , we see that for more general pattern types we see a steeper candidate set growth ( see fig . 5b ) . Note that we use a log scale to depict the number of patterns .
We have seen that achieving a smaller database encoding relies on the ability to draw patterns from an enriched pattern set . The inclusion of these enriched patterns indicates that essential characteristic structure within the database can be best described with these type of patterns : simpler patterns are apparently not sufficient . Although RDB KRIMP steers to small code tables , it allows these additional patterns in the code table only if they aid in the description of essential structure that would otherwise be described in a much less efficient manner ( see fig . 5c ) .
5.5 Relational Code Tables
Compactness is not the only feat of interest .
In order for our model to provide insight in the database it should be interpretable . As our code table contains a collection of characteristic patterns , we can pick single code table patterns to examine .
To show an example , we pick a code table pattern that is intuitive without expert knowledge . Shown in Figure 6 is a pattern that reads as follows : ‘A gene localised in the nucleus having a transcription function that has two distinct physical interactions’ . As one would expect , a characteristic pattern for the GENES database is that transcription often involves physical interactions in order to copy information and that it occurs in the nucleus . Note that this pattern selects two distinct tuples from the INTERACTION table . The small encoded database lengths are obtained largely due to the availability of the more complex patterns in our pattern language . Patterns that select multiple tuples from one table are used more often in the database description , leaving simpler patterns to ’fill up’ the database .
In the code tables obtained for the lowest used minsup , patterns that select multiple tuples from one table make up for 16 % , 63 % , and 71 % of the content for the financial , genes , and hepatitis databases respectively .
Figure 6 : A partial description : a code table element from the description of the Genes database . Note that these patterns select multiple tuples from the same table .
6 . RELATED WORK
Our code tables are models for a complete database . Similarly , Probabilistic Relational Models ( PRM ) are graph based models that can be applied to model relational databases [ 5 , 9 ] . A PRM is one graph , in which attribute values are linked with their co occurrence probability . This contrasts to our code table , which is not a single graph , but a collection of local tree like patterns . Moreover , an attribute value can occur multiple times in different local models , if this aids in describing the database better . This means that we can regard an attribute value in different contexts , instead of one .
In the field of Relational Data Mining ( RDM ) , ILP based approaches , such as the WARMR algorithm , allow for the discovery of relational patterns [ 3 , 15 ] . As seen in our experiments , we obtain better database descriptions when we use our pattern language compared to WARMR type patterns ( see fig . 5 ) .
The main application for a WARMR like approach is when one is specifically interested in a target table , for example when trying to improve the classification scores on a target table given relational information [ 16 ] . This target table leads to an effective manner to prune the search space , and allows for aggregate functions to improve the efficiency [ 8 ] . An efficient implementation is FARMER [ 13 ] , which in addition allows for a more general pattern language similar to the patterns used in this work .
In the work of both [ 4 ] and [ 12 ] the goal to find a different type of pattern : multi valued dependencies ( MVD ) and functional dependencies ( FD ) . These are ‘global’ patterns : a dependency is a ‘higher order’ pattern similar to a constraint on the relation , in contrast to code table patterns , which are local patterns .
6.1
R KRIMP
In earlier work , we introduced R KRIMP which finds patterns of the form : [ p0 , . . . , pn ] . These patterns are less expressive than the patterns used in RDB KRIMP , and thus have less descriptive potential . R KRIMP patterns are similar to the work of [ 6 ] , who define these patterns as simple conjunctive queries .
In RDB KRIMP a single code table element can describe more structure in the database than with R KRIMP . As an example , consider a RDB KRIMP style pattern from our illustrative database : ’A account with frequency 3 having both a disponent type disposition and a owner type disposition’ ( see fig . 2 ) . This single patterns translates requires two individual R KRIMP style patterns : ’A account with frequency 3 having a disponent type disposition’ and ’A account with frequency 3 having a owner type disposition’ .
Note that R KRIMP in this case would cover ACCOUNT ORDER in an overlapping manner . As tuples can occur multiple times within a join , R KRIMP patterns allow for an overlapping cover . Thus , even in the initial case , when solely alphabet patterns are used , the covering of duplicate tuples will lead to a very different encoding .
P : GENE({ Localization = nucleus } ) [ [ INTERACTION({ Type = physical } ) , INTERACTION({ Type = physical } ) ] , [ META({ Function = transcription } ) ] ] 7 . DISCUSSION
In our experiments , we see that compared to the candidate set the code table growth shows a much slower pace .
Not only it stays small compared to the candidate set , it also stays compact compared to the original database . From the original set of frequent candidates only a few patterns contribute to the database description , leading up to 4 orders of magnitude reduction .
When we look at the influence of the initial database order , we see that this does not have much effect . We obtain only a slight deviation , up to around 1 % from the original encoded length .
Enforcing a single target table rather than treating all tables equal yields a much worse database description . The encoded database length shows an increase of up to 25 % compared to the original result .
In the hepatitis database all tables center around one single central table : the PATIENT table . While this database seems like a good case to pick as a target table , even here we obtain our best description when patterns are allowed to start at other tables .
Allowing a richer pattern language is a fruitful effort . We see that we generate more potentially interesting patterns in this manner , from which we can select those that describe the database best . Compared to WARMR like patterns , we see that we can describe the database better : we achieve shorter encoded lengths . To obtain these good database descriptions , the code tables rely for the large part on these enriched patterns , which are of the type that selects multiple tuples from a table .
Our code tables stay compact and do not show exponential set growth as frequent pattern sets do for lower minimum supports . RDB KRIMP shows to be successfully to select compact models . Even under its MDL selection pressure , larger enriched patterns are selected to describe the database , indicating that these patterns are very characteristic for the database .
8 . CONCLUSION
Currently , in order to obtain insight from a relational database , either one aims to find a ‘global’ model of the complete database , or one seeks a collection of ‘local’ patterns in the database . While both approaches have their merits , they have their shortcomings . Global models tend to blur out interesting local structure for the sake of the global structure , and pattern collections tend to drown the global picture in a sea of patterns . In this paper , we propose a method that combines the merits of both directions : it mines a compact but detailed description of the complete relational database .
For a database model to be descriptive it should be able to reflect the patterns that are present within it . Relational databases allow for tuples in one table to be connected to multiple tuples from other tables . Hence , if we want to find interesting patterns in a database , our pattern language should be rich enough to reflect such relations . In this paper , we introduce RDB KRIMP , an algorithm that describes the complete relational database using only a small collection of characteristic patterns : the code table . While the code table serves as a global model for the complete database , its patterns preserve the local details . Using the MDL principle , RDB KRIMP results in a compact set of patterns that together describe the database well . With respect to the original set of frequent candidates , we obtain up to 4 orders of magnitude reduction .
Our rich relational pattern language allows RDB KRIMP to draw from a larger pool of interesting frequent candidate patterns . The experiments reported on in this paper verify our claims both on the usefulness of our pattern language and on the ability of RDBKRIMP to select just a few highly descriptive patterns . Firstly , each code table heavily relies on the introduced pattern language : up to
70 % of the patterns select multiple tuples from one table . Secondly , experiments show that our pattern language leads to a far better compression . In other words , these patterns highlight characteristic structure in the database . The fact that these patterns are not only characteristic but also easily interpretable is shown by an example from the Genes database ( see fig . 6 ) . As part of the code table we find a pattern that describes gene interactions : ‘A gene localised in the nucleus having a transcription function that has two distinct physical interactions’ .
In contrast to current frequent pattern mining approaches , our approach does not rely on a target table . That is , our patterns can start from any table . In all cases this yields to improvements , of up to almost 25 % . Even in a typical ’target table’ style database , we see that we obtain a better description without the use of a single table as target .
9 . REFERENCES [ 1 ] R . Agrawal , T . Imielinski , and A . N . Swami . Mining association rules between sets of items in large databases . In P . Buneman and S . Jajodia , editors , Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data , Washington , DC , May 26 28 , 1993 , pages 207–216 . ACM Press , 1993 .
[ 2 ] J . De Knijf . Fat miner : mining frequent attribute trees . In SAC ’07 :
Proceedings of the 2007 ACM symposium on Applied computing , pages 417–422 , New York , NY , USA , 2007 . ACM .
[ 3 ] L . Dehaspe and H . Toivonen . Discovery of frequent datalog patterns .
Data Min . Knowl . Discov . , 3(1):7–36 , 1999 .
[ 4 ] P . A . Flach and I . Savnik . Database dependency discovery : a machine learning approach . AI Commun . , 12(3):139–160 , 1999 .
[ 5 ] L . Getoor , N . Friedman , D . Koller , A . Pfeffer , and B . Taskar .
Probabilistic relational models . In L . Getoor and B . Taskar , editors , An Introduction to Statistical Relational Learning . MIT Press , 2007 . [ 6 ] B . Goethals , W . Le Page , and H . Mannila . Mining association rules of simple conjunctive queries . In M . Zaki and K . Wang , editors , SDM , pages 96–107 . SIAM , 2008 .
[ 7 ] P . D . Grünwald . Minimum description length tutorial . In
P . Grünwald and I . Myung , editors , Advances in Minimum Description Length . MIT Press , 2005 .
[ 8 ] A . Knobbe . Multi Relational Data Mining . PhD thesis , Universiteit
Utrecht , Utrecht , the Netherlands , 2004 .
[ 9 ] D . Koller and A . Pfeffer . Probabilistic frame based systems . In
Proceedings of the 15th National Conference on Artificial Intelligence ( AAAI ) , pages 580–587 , 1998 .
[ 10 ] A . Koopman and A . Siebes . Discovering relational items sets efficiently . In M . Zaki and K . Wang , editors , SDM , pages 108–119 . SIAM , 2008 .
[ 11 ] M . Li and P . Vitányi . An Introduction to Kolmogorov Complexity and its Applications . Springer Verlag , 1993 .
[ 12 ] H . Mannila and K J Räihä . Algorithms for inferring functional dependencies from relations . Data Knowl . Eng . , 12(1):83–99 , 1994 .
[ 13 ] S . Nijssen and J . N . Kok . Efficient frequent query discovery in farmer . In In Proc . of the 7th PKDD , volume 2838 of LNCS , pages 350–362 . Springer , 2003 .
[ 14 ] A . Siebes , J . Vreeken , and M . van Leeuwen . Item sets that compress .
In J . Ghosh , D . Lambert , D . B . Skillicorn , J . Srivastava , J . Ghosh , D . Lambert , D . B . Skillicorn , and J . Srivastava , editors , SDM , pages 393–404 . SIAM , 2006 .
[ 15 ] M . S . Tsechansky , N . Pliskin , G . Rabinowitz , and A . Porath . Mining relational patterns from multiple relational tables . Decis . Support Syst . , 27(1 2):177–195 , 1999 .
[ 16 ] X . Yin , J . Han , J . Yang , J . Yang , and P . S . Yu . Crossmine : Efficient classification across multiple database relations . In ICDE ’04 : Proceedings of the 20th International Conference on Data Engineering , pages 399–411 , Washington , DC , USA , 2004 . IEEE Computer Society .
