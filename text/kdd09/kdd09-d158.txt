Toward Autonomic Grids : Analyzing the Job Flow with
Affinity Streaming
Xiangliang Zhang
INRIA,Université Paris Sud LRI Bat . 490 , F 91405 Orsay xlzhang@lri.fr
Cyril Furtlehner
INRIA
LRI Bat . 490 , F 91405 Orsay CyrilFurtlehner@lrifr
Julien Perez
Université Paris Sud
LRI Bat . 490 , F 91405 Orsay perez@lri.fr
Cecile Germain Renaud
Université Paris Sud
LRI Bat . 490 , F 91405 Orsay cecile@lri.fr
Michèle Sebag
CNRS
LRI Bat . 490 , F 91405 Orsay sebag@lri.fr
ABSTRACT The Affinity Propagation ( AP ) clustering algorithm proposed by Frey and Dueck ( 2007 ) provides an understandable , nearly optimal summary of a dataset , albeit with quadratic computational complexity . This paper , motivated by Autonomic Computing , extends AP to the data streaming framework . Firstly a hierarchical strategy is used to reduce the complexity to O(N 1+ε ) ; the distortion loss incurred is analyzed in relation with the dimension of the data items . Secondly , a coupling with a change detection test is used to cope with non stationary data distribution , and rebuild the model as needed . The presented approach Strap is applied to the stream of jobs submitted to the EGEE Grid , providing an understandable description of the job flow and enabling the system administrator to spot online some sources of failures .
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications— Data Mining ; I53 [ Pattern Recognition ] : Clustering— Algorithms , Similarity measures
General Terms Algorithms , Experimentation
Keywords Affinity Propagation , Autonomic Computing , Online Clustering
1 .
INTRODUCTION
The clustering of large scale dynamic datasets is a key issue for most application domains , at the crossroad of data bases , data mining and machine learning [ 3 ] . High performance computers and large size memory storage do not per se sustain scalable and accurate clustering . Typically , advances in large scale clustering ( see eg , [ 12 ] ) mainly proceed by distributing the dataset and processing the subsets in parallel ; when dealing with dynamic datasets , such Divideand Conquer approaches face some limitations in terms of latency and/or communication costs .
Furthermore , the choice of a clustering method must reflect the applicative needs . Our motivating application pertains to the strategic field of Autonomic Computing [ 19 ] , aimed at providing large computational systems with selfmodelling , self configuring , self healing and self optimizing facilities . More specifically , the applicative goal of the present paper is to enable the administrator of a large scale grid system , the EGEE Grid1 , to analyze the flow of jobs submitted to and processed by the grid . The input data thus is made of the Logging and Bookkeeping ( L&B ) files , automatically generated by the grid middleware . As noted by [ 7 ] , modern data mining is more and more concerned with automatically generated datasets ( “ computers are fueling each other ” ) ; building understandable summaries thereof is even more critical . For this reason , it is highly desirable that a job cluster be summarized by an actual job ( as opposed to an artefact , as done in K means ; more in Section 4 ) .
Affinity Propagation ( AP ) , a message passing based clustering algorithm proposed by Frey and Dueck [ 6 ] , does satisfy the above interpretability constraint . Akin K centers , AP maps each data item onto an actual data item , called exemplar , and all items mapped onto the same exemplar form one cluster . Contrasting with K centers , AP builds quasioptimal clusters in terms of distortion ( section 2.1 ) , thus enforcing the cluster stability [ 6 ] . The price to pay for these understandability and stability properties is AP quadratic computational complexity , severely hindering its usage on large scale datasets . In an earlier work [ 22 ] , a 2 level hierarchical approach was proposed to decrease AP complexity from O(N 2 ) to O(N 3/2 ) , where N denotes the number of items . Independently , some coupling with a change detection test was in
1The EGEE grid was established in the EU project Enabling Grid for E SciencE , http://wwweu egeeorg It involves 41,000 CPUs , 5 Petabytes storage and concurrently supports 20,000 jobs on 24/24 , 7/7 basis .
987 vestigated to extend AP to dynamic data distributions , enabling online clustering aka Data Streaming [ 22 ] .
This paper features three contributions along these same lines . Firstly , a straightforward generalization of the 2 level hierarchical approach is proposed , showing that a h level hierarchical approach would reduce the computational complexity to O(N h+2 h+1 ) up to poly logarithmic terms . Secondly , the price to pay for this complexity reduction , namely the distortion loss incurred along the hierarchical Divide andConquer , is analyzed ; the distortion loss is shown to be negligible except in the particular case of two dimensional datasets . Thirdly , an adaptive mechanism inspired from [ 21 ] is used to optimize the change detection test parameters .
The extended Strap algorithm ( Streaming Affinity Propagation ) is finally applied to a challenging real world problem , the online monitoring of the EGEE grid . The Strap specificity compared to prominent data streaming algorithms ( eg , [ 2 , 3 , 7 ] ) is twofold . On the one hand , Strap inherits AP understandability , modelling the data stream through exemplars ( actual data items ) as opposed to artefacts . On the other hand , this model is available at any time step contrasting with eg , [ 2 ] . Strap thus makes it feasible to provide the EGEE administrator with a real time dashboard of the job data flow , enabling the discovery of anomalies .
The paper is organized as follows . Section 2 first briefly describes AP for the sake of self containedness , before presenting Divide and Conquer AP . The computational complexity thereof is derived , and the distortion loss is analyzed . The Strap algorithm extending Hi AP to data streaming is presented in Section 3 , and the self adaptive change detection test is detailed . Section 4 describes a large scale real world application : the profiling of 5M+ jobs submitted to the EGEE grid . Strap is assessed in terms of algorithmic robustness and performance compared to a k centers baseline approach , and the added value for the grid administrator is discussed . The paper concludes with some perspectives for further research .
2 . SCALABLE CLUSTERING WITH AP
This section presents the Affinity Propagation algorithm , referring the reader to [ 6 ] for a comprehensive description . How to make AP scalable , and what is the price to pay for the complexity reduction , are described thereafter . 2.1 AP and Weighted AP
Let E = {e1 , . . . , eN} denote a set of N items , and let d(ei , ej ) denote the distance or dissimilarity between items ei and ej . Letting K denote a positive integer , the K center problem consists of finding K items ei1 , . . . , eiK in E , referred to as exemplars , minimizing the dataset distortion L :
L = j∈ 1N min k∈ 1K d2(ej , eik ) defined as the squared distance between ej and its closest exemplar , summed over all ej in E .
Affinity Propagation is a message passing algorithm tackling the above optimization problem as follows . Let c be defined as a mapping from E onto E , associating to each item ei its exemplar ci ( ci ∈ E ) .
( cid:161 )
( cid:162 )
∗ Find c
= argmax
E[c ]
, with
E[c ] = where
S(ei , ci ) =
N S(ei , ci ) − N −d2(ei , ci ) i=1 i=1
−s∗ otherwise log χ(p ) i
[ c ]
( 1 ) if ei = ci
Parameter s∗ , the penalty for having an additional exemplar , controls the clustering granularity : s∗ = 0 leads to the trivial solution where every item is an exemplar ; s∗ = ∞ leads to the single cluster solution . Contrasting with Kcenters , AP only indirectly controls the number of clusters through s∗ ( usually set to the median value of d2(ei , ej) ) . χ(p ) [ c ] is a set of constraints initially meant to enforce the fact that , if ei is chosen as exemplar , it must be its own exemplar ( clusters are ball shaped ) [ 6 ] . A relaxation of the χ constraints enabling tree structured clusters , was proposed by [ 13 ] . i
The Divide and Conquer approach presented next relies on the Weighted AP algorithm ( WAP ) [ 22 ] , extending AP to the case of multiply defined items , and/or dense subsets of items . Let F denote a subset of E , made of n items with small pair distance ( ∀ei , ej ∈ F , d2(ei , ej ) < ) . WAP proceeds by replacing all items in F with a single example ef . The clustering problem defined on E is made equivalent to the one defined on ( E\F )
{ef} , by setting :
S(ef , ej ) −→
S(ei , ej ) ,
∀ej ∈ E\F ei∈F ei∈F
S(ej , ef ) −→ 1 n
S(ej , ei ) ,
∀ej ∈ E\F
S(ef , ef ) −→ −s∗ + ( n − 1 )
The WAP algorithm will be used in the remainder of the paper to iteratively cluster exemplars produced in former clustering steps . 2.2 Complexity of Hierarchical AP
AP computational complexity2 is N 2log(N ) ; it involves the matrix S of pair distances , with quadratic complexity in the number N of items , severely hindering its use on largescale datasets . This AP limitation can be overcome through a Divideand Conquer heuristics inspired from [ 9 ] . Dataset E is randomly split into b data subsets ; AP is launched on every subset and outputs a set of exemplars ; the exemplar weight is set to the number of initial samples it represents ; finally , all weighted exemplars are gathered and clustered using WAP ( the complexity is O(N 3/2 ) [ 22] ) . This Divide and Conquer strategy − which could actually be combined with any other basic clustering algorithm − can be pursued hierarchically in a self similar way , as a branching process with b representing the branching coefficient of the procedure , defining the Hierarchical AP ( Hi AP ) algorithm .
Formally , let us define a tree of clustering operations , where the number h of successive random partitions of the data represents the height of the tree . At each level of the hierarchy , the penalty parameter s∗ is set such that the expected number of exemplars extracted along each clustering step is upper bounded by a constant K .
2Except if the similarity matrix is sparse , in which case the complexity reduces to N Klog(N ) with K the average connectivity of the S matrix [ 6 ] .
988 Proposition 21 Let us define the branching factor b as
( cid:161 ) N
( cid:162 ) 1 b = h+1 ,
K
Then the overall complexity C(h ) of Hi AP is given by
C(h ) ∝ K up to logarithmic terms . h h+1 N h+2 h+1
N ( cid:192 ) K , and
Proof . M = N/bh is the size of each subset to be clustered at level h ; at level h − 1 , each clustering problem thus involves bK = M exemplars with corresponding complexity
( cid:162 ) 2 h+1 .
C(0 ) = K 2(cid:161 ) N h
K
Ncp = bi = i=0 bh+1 − 1 b − 1
,
The total number Ncp of clustering procedures involved is
C(h ) = K 2(cid:161 ) N with overall computational complexity : ≈ N(cid:192)K
( cid:162 ) 1 K − 1 h+1 − 1
( cid:162 ) 2
( cid:161 ) N h+1
K
N
K 2(cid:161 ) N
K
( cid:162 ) h+2 h+1 .
K
It is seen that C(0 ) = N 2 , C(1 ) ∝ N 3/2 , . . . , and C(h ) ∝ N for h ( cid:192 ) 1 . 2.3 Distortion Regret of Hi AP
Let us examine the price to pay for this complexity reduction . As mentioned earlier on , the clustering quality is assessed from its distortion , the sum of the squared distance between every data item and its exemplar :
N
D(c ) = d2(ei , ci ) i=1
The distortion loss incurred by Hi AP wrt AP is examined in the simple case where the data samples follow a centered distribution in IRd . By construction , AP aims at finding the cluster exemplar rc nearest to the center of mass of the sample points noted rcm :
D(c ) = |rcm − rc|2 + Cst
The distortion loss incurred by Hi AP can be assessed from the relative entropy , or Kullback Leibler distance , between the distribution Pc of the cluster exemplar computed by AP , and the distribution Pc(h ) of the cluster exemplar computed by Hi AP with hierarchy depth h :
( cid:161 )
( cid:162 )
DKL
Pc||Pc(h )
=
Pc(h)(r ) log
Pc(h)(r ) Pc(r ) dr
( 2 )
In the simple case where points are sampled along a centered distribution in IRd , let ˜rc denote the relative position of exemplar rc with respect to the center of mass rcm :
˜rc = rc − rcm
The probability distribution of ˜rc conditionally to rcm is cylindrical ; the cylinder axis supports the segment ( 0 , rcm ) , where 0 is the origin of the d dimensional space . As a result , the probability distribution of rcm+ ˜rc is the convolution of a spherical with a cylindrical distribution .
Let us define the following notations . Subscripts sd refer to sample data , ex to the exemplar , and cm to center of mass . Let x(cid:166 ) denote the corresponding square distances to the origin , f(cid:166 ) the corresponding probability densities and F(cid:166 ) their cumulative distribution . Assuming
∞
σ def= E[xsd ] =
α def= − lim x→0 xfsd(x)dx ,
0 log(Fsd(x ) ) d 2 x
,
( cid:162 )
.
( cid:161 ) d ( cid:161 ) d
2 , 2x Γ
( cid:162 ) dσ
2 lim M→∞ Fcm(
Γ
) = x M exist and are finite , then the cumulative distribution of xcm of a sample of size M satisfies by virtue of the central limit theorem . In the meanwhile , x(cid:102)ex= xex − xcm has a universal extreme value distribution
( up to rescaling ) :
M→∞ F(cid:102)ex( lim
1
M 2/d x ) = exp
( cid:161)− ˜αx
( cid:162 ) d 2
. where ˜α = α stands for the fact that the extreme value parameter is possibly affected by the displacement of the center of mass . To see how the clustering error propagates along with the hierarchical process , one proceeds inductively . At hierarchical level h , M samples , spherically distributed with variance σ(h ) are considered ; the sample nearest to the center of mass is selected as exemplar . Accordingly , at hierarchical level h + 1 , the next sample data is distributed after the convolution of two spherical distributions , the exemplar and center of mass distributions at level h . The following scaling recurrence property ( proof in appendix ) holds :
Proposition 22 M→∞F ( h+1 ) M ( h+1)γ ) = lim Γ( d σ(h+1 ) ) 2 , Γ( d 2 ) x sd
( x
( cid:161)−α(h+1)x
( cid:162 ) d 2
 exp d > 2 , γ = exp(−β(h+1)x ) d = 2 , γ = 1 .
2 d d < 2 , γ = 1 with
σ(h+1 ) = σ(h ) , α(h+1 ) = α(h ) ,
β(h+1 ) =
β(h )
2
.
It follows that the distortion loss incurred by Hi AP does not depend on the hierarchy depth h except in dimension d = 2 . Fig 1 shows the distribution of the clustering distortion depending on the hierarchy depth h and the dimension d of the dataset . The distortion curve for h = 1 corresponds to the AP case , showing that the distortion loss due to the hierarchical approach is moderate to negligible in dimension d = 2 provided that the number of samples per cluster at each clustering level is “ sufficient ” ( say , M > 30 for the law of large numbers to hold ) . In dimension d > 2 , the distance of the center of mass to the origin is negligible with respect to its distance to the nearest exemplar ; the distortion behaviour thus is given by the Weibull distribution which is stable by definition ( with an increased sensitivity to small sample size M as d goes to 2 ) . In dimension d = 1 , the distribution is dominated by the variance of the center of mass , yielding the gamma law which is also stable with respect to
989 by relaxation . The model update is parameterized from a ( user supplied ) time length ∆ ; the idea is that clusters which have not received any additional item during ∆ consecutive time steps [ 22 ] should disappear . If data item et does not fit the model , it is considered to be an outlier and put in the reservoir . The reservoir gathers the last M outliers .
A change point detection test is used to monitor the stability of the data distribution . The so called Page Hinkley ( PH ) statistical test [ 16 , 11 ] is applied to the outlier rate ( section 32 ) Upon triggering the PH test , the stream model is rebuilt using WAP from the current model ( exemplars weighted by the current size of the associated cluster ) and the outliers in the reservoir .
Algorithm 1 Strap Algorithm
Data streams e1 , . . . et , . . . ; fit threshold ε Init
AP(e1 , . . . , eT ) → Strap Model Reservoir = {} for t > T do
Compute ei = nearest exemplar to et if d(et , ei ) < ε then else
Update Strap model Reservoir ← et end if if P Ht > λ then Rebuild Strap model Reservoir = {} end if end for
3.2 Self Adaptive Change Detection Test
Among the main change detection tests are Wald tests , also referred to as SPRT ( sequential probability ratio test [ 15] ) , and CUSUM test ( cumulative sum [ 16] ) ; kernelized versions of the CUSUM test have also been developed , see eg , [ 10 ] . The Page Hinkley test has been used within the Strap algorithm because it minimizes the time expectancy before detecting a change , conditionally to a given false alarm rate [ 16 , 11 ] .
Formally , the PH test monitors a scalar random variable pt . The test is parameterized after a threshold λ , classically governing the rate of false alarms ; additionally , a small constant tolerance parameter δ has been used to cope with slowly varying distributions : t =1 ( p − ¯p + δ ) ¯pt = 1 t Mt = max{|m| , = 1t} P Ht = ( Mt − |mt| ) > λ t
=1 p mt =
In its current state , the PH test is only triggered when pt tends to increase , as the monitored variable pt relates to the presence of outliers . When the rate or severity of outliers decreases , there is no need to rebuild the stream model . Several scalar indicators pt have been considered , among which the distance of the current data item to the nearest exemplar , possibly normalized by the associated distortion . Empirically , the best performing indicator is found to be pt =
1 + 1ot 1 + Ot
Figure 1 : Radial distribution plot of exemplars obtained by clustering of Gaussian distributions of N = 106 samples in IRd in one single cluster exemplar , with hierarchical level h ranging in 1,2,3,6 , for diverse values of d : d = 1 ( upper left ) , d = 2 ( upper right ) , d = 3 ( bottom left ) and d = 4 ( bottom right ) . Fitting functions are of the form f ( x ) = Cxd/2−1 exp(−αxd/2 ) . the hierarchical procedure . In dimension d = 2 however , the Weibull and gamma laws do mix at the same scale ; the overall effect is that the width of the distribution ( of the distortion ) increases like h2 , as shown in Fig 1 ( top right ) .
3 . MODELING DATA STREAMS WITH AP This section is concerned with adapting Hi AP to online clustering and dynamic data distributions , defining the Strap algorithm . Strap combines Hi AP with a change detection test : if the test is triggered , it is likely that the data distribution has changed and the stream model is rebuilt . Two extensions have been brought to Strap initial version [ 22 ] . An adaptive mechanism inspired from [ 21 ] is used to automatically optimize the parameters of the change detection test . Secondly , the exemplars built by Strap fuel an offline clustering process , enabling some multi scale description of the data stream . This section finally discusses the strengths and weaknesses of Strap with respect to the state of the art . 3.1 Exemplar based Clustering with Change
Detection
The early Strap algorithm is summarized for the sake of self containedness , referring the reader to [ 22 ] for more detail . The model of the stream is initialized by applying Hi AP to the first data items . Formally , the stream model is made of a set of clusters Ci = ( ei , ni , Σi , ti ) , where ei is the cluster exemplar , ni and Σi respectively stand for the cluster size and distortion , and ti is the last time stamp when a data item joined the cluster .
As the stream flows in , current data item et is checked against the model . If its distance to the nearest exemplar ei is less than a threshold computed in the initialization step , et joins the Ci cluster . The Ci time stamp is set to the current time step t , while Ci size and distortion are updated
00,0010,0020,0030,0040,005x050010001500f(x)h=1h=2h=3h=6N=10e6 d=100,0050,010,0150,020,025x0100200300400500600700f(x)h=3 alpha=115333h=2 alpha=227643h=6 alpha=18036h=1 alpha=460764N=10e6 d=200,010,020,030,040,050,060,07x020406080100f(x)h=1h=2h=3h=6N=10e6 d=300,050,10,150,2x010203040f(x)h=1 alpha = 269086h=2 alpha=251774h=3 alpha=214337h=6 alpha=70248N=10e6 d=4990 where 1ot is set to 1 if the current data item is an outlier and 0 otherwise , and Ot is the fraction of data items considered to be outliers since the model was last ( re)built .
Threshold λ is adjusted in order to optimize the model representativity , in the spirit of the Bayesian Information Criterion [ 20 ] . The optimization criterion is set to :
|C| ( cid:161 ) 1 i=1 ni ej∈Ci
Fλ = − 1 |C|
( cid:162 ) − ϕ d(ej , e
∗ i ) log N − ηOt
( 3 ) d 2 where |C| is the number of clusters , e∗ i and ni respectively the exemplar and size of the i th cluster , d the dimension of the data stream , N the number of data items recognized by the stream model since the last restart , ϕ and η are two constants to make the penalty term on the same scale as the distortion item .
The optimization of λ has been tackled in a discrete ( considering a finite set of values ) and a continuous ( considering a continuous domain ) setting , respectively using greedy optimization and a Gaussian Process based estimate of Fλ [ 21 ] . 3.3 Multi Scale Modeling of the Stream
While data streaming aims at providing an accurate description of the instant flow distribution , it might be desirable to also provide “ the big picture ” , depicting the evolution of this distribution on a larger time scale .
The fact that at each time step the stream model is based on exemplars makes it natural to apply Hi AP on the overall set of exemplars gathered along time3 , thus extracting “ super exemplars ” . These super exemplars capture the various trends of the data stream along time , enabling to characterize any period ( day , week or month ) after the representativity of each such super exemplar ( number of data items falling in each super cluster ) .
Figure 2 : Online ( 1st level ) and retrospective ( 2nd level ) representation of a data stream with Strap .
3.4 Discussion
Among the prominent challenges of Data Streaming ( see eg , [ 8 , 3 ] ) are the computational time and space resources needed , on the one hand , and the efficient modeling of nonstationary distributions on the other hand . There is little doubt that the computational requirements of a data streaming algorithm govern its usability ; typically when the system under examination generates the equivalent of 27 CDs per minute , linear or quasi linear computational complexity is the maximum one can afford to keep up with realtime processing . The second challenge , namely the pursuit
3The exemplars with low representativity are filtered out . of a moving target distribution , has been extensively considered in the Signal Processing and Data Analysis literature [ 1 ] . It however needs to be reconsidered in the Data Streaming context , subject to the above mentioned computational limitations . This challenge can be viewed as yet another instance of the Exploration vs Exploitation dilemma ; indeed , a competent data streaming algorithm should simultaneously be able to catch up with any true change in the data distribution ( exploration ) while discarding outliers in order not to spoil the model ( exploitation ) .
A third and equally important challenge is to provide the user with understandable results . As ill defined as understandability might be , it remains that providing understandable results is mandatory in order to keep the user in the loop [ 5 ] . The presented Strap algorithm aims at understandable , stable and computationally efficient Data Streaming , through the selection of the exemplars best representing the ( majority of ) data items at any time step . The Continuous Distributed clustering ( CDC ) algorithm presented by Cormode et al . [ 3 ] is most related to Strap , with two important differences . Firstly , CDC is interested in “ conquering the divide ” , ie building a model of a distributed stream , whereas Strap is interested in splitting the data stream to overcome the complexity barrier . Secondly , CDC is based on K centers and its goal is to minimize the radius ( maximal distance between a point and its exemplar ) or the diameter ( maximal distance between two points with same exemplars ) of the clustering , whereas the Strap goal is to minimize the distortion ; the difference between both criteria can be understood as the difference between L∞ and L1 or L2 norms . To our best knowledge , Strap is the only Data Streaming algorithm modeling a centralized data flow through a set of exemplars . This unique feature of Strap is both a strength and a weakness compared to the Data Streaming algorithms at the state of the art . On the weak side , Strap was shown to be slower by an order of magnitude than DenStream [ 2 ] on the KDD Intrusion Detection Dataset [ 22 ] . This lesser computational efficiency is blamed on two facts . Firstly , DenStream , extending the DBScan clustering algorithm [ 4 ] to the streaming context , constructs an artefactbased model , smoothly updating an implicit K means like model at any time step , whereas Strap explicitly rebuilds the model whenever some change in the underlying data distribution is detected . Secondly , the data model is available at any time step in Strap , whereas it is only computed upon request by DenStream .
In counterpart , to our best knowledge Strap is the only applicable Data Streaming algorithm when i ) the data representation makes it impossible to build artefacts ; ii ) some performance and stability/reproducibility guarantees are needed , barring the use of K centers . Domains such as molecular chemistry , image processing , or social networks fall in the first category ( eg , defining an “ average ” molecule still is an open problem ) . Safety critical domains fall in the second category .
4 . MONITORING THE JOB FLOW IN A
GRID SYSTEM
This section reports on the application of Strap to the Autonomic Grid context , specifically the monitoring of the jobs submitted to the EGEE grid . After briefly describing the application , the section describes the goal of experiments
991 and the experimental setting , before discussing the empirical results . 4.1 Grid Monitoring and Job Streaming
Grid Monitoring involves two main functionalities : acquisition and usage of the relevant information . The acquisition functionality includes sensors that instrument grid services or applications , and data collection services that filter , centralize and/or distribute the sensor data to the usage functionality . Usage , which is more specifically investigated in this paper , includes consumer services such as real time presentation and interpretation . It also includes middleware services as far as feedback loops are considered , typically in the Autonomic Computing framework . Many architectures and integration frameworks offer advanced presentation , user interaction and reporting facilities , such as the EGEE dashboard [ 23 ] and Real Time Monitor [ 24 ] . Data interpretation , aimed at revealing meaningful ( compound ) features which go beyond elementary statistics , is much less developed in the grid area .
The goal of the proposed Job Streaming facility , enabling the real time inspection of the jobs submitted to and processed by the grid , is to provide some interpretation of the grid running status . The job stream considered in the following is the log of 39 Resource Breakers ( RB ) of all gLiteoperated jobs in the whole EGEE grid from early January 2006 to end of May , including a total of 5,268,564 jobs . Through the Real Time Monitor system ( RTM ) [ 24 ] , the acquisition module provides a real time description of the jobs through XML records ( available at http://wwwgridobservatoryorg/ ) Each job is labeled after its final status , successfully finished ( good job ) or failed . Circa 45 error classes ( eg , “ Cancel requested by WorkloadManager ” , “ RB Cannot plan ” ) exist ; about 25 error classes are significantly represented ( with more than 1,500 occurrences ) in the job stream . These labels will not be accounted for in the clustering process , for the following reason . Following grid experts , error types do not necessarily relate to operational aspects and could blur the picture of the grid status . For instance , although cannot plan means that the Resource Broker was unable to find a matching resource , the real cause might be that the user ’s requests were truly unreachable ; or the Broker information is stalled and does not see that resources have been released . Therefore , the job labels will only be used a posteriori to assess the clustering performance .
Each job is described by 6 continuous and 6 boolean attributes4 . The first 6 attributes describe the time cost duration spent in different services along the job lifecycle :
1 . Submission Time : time for submission to Workload Management System ( WMS ) 2 . Waiting Time : time to find a matching resource 3 . Ready for Transfer Time : time acceptation and transfer to the found resource , reported by JobController ( JC ) 4 . Ready for CE accept Time : the same as Ready for Transfer Time , but reported by LogMonitor ( LM ) 5 . Scheduled Time : queuing delay in local cite 6 . Running Time : execution time .
4An additional categorical attribute , the name of the queue visited by the job will not be considered in this paper , although a proper handling of categorical attributes was the main motivation for using exemplar based clustering as opposed to K − means approaches .
In principle , attributes 3 and 4 are redundant ( JC is a standalone logging service , while the LM integrates various logs , and returns them in the L&B database ) ; as will be seen , the discrepancy between both attributes however provide useful clues about grid misbehaviors . All numerical attributes are centered and normalized ; the first data subset is used to estimate the average and standard deviation , which are thereafter updated using an additive relaxation scheme as the dataflow goes in .
In case the job does not reach a given service due to a failure in the job lifecycle , the durations of all subsequent services are set to 0 . Six additional boolean attributes are thus considered , indicating whether the job reaches the corresponding service . The job dissimilarity is the Euclidean distance on IR12 . 4.2 Experiment Goal and Setting
The goal of the experiments is to assess the Strap algorithm from an algorithmic and an applicative perspectives . On the one hand , Strap is assessed from its ability to provide useful hints on the grid state . On the other hand , the algorithmic performance of Strap is assessed with comparison to hierarchical k centers streaming5 , measured after four criteria : The Clustering Accuracy and Clustering Purity are classically measured with respect to the job labels : the accuracy is the percentage of jobs with same class label as their exemplar ; the purity is the fraction of the jobs in each cluster belonging to the majority class of the cluster , averaged over all clusters . The clustering purity is known to be more robust than the clustering accuracy in case of imbalanced clusters and/or classes . The Clustering Stability measures the clustering performance after [ 14 ] . Specifically , a set of super exemplars computed for a given setting s∗ of Strap induces a partition C(s∗ ) of the jobs ; the sensitivity of the algorithm is measured from the independence of the partitions obtained for different settings , defined as the sum taken over all clusters Ci ∈ C(s∗ ) , C j ∈ C(s∗ ) of the quantity
P ( ek ∈ Ci ∩ C j|ek ∈ Ci).P ( ek ∈ Ci ∩ C min(|C(v)|,|C(v)| ) j|ek ∈ C j )
The Streaming Stability measures the stability of the stream model wrt the change detection test , and parameter λ . The model dynamics is reflected by the restart schedule ( number of restarts per day ) ; its stability is assessed by computing the correlation of the restart schedules obtained for various values of λ ; the significance of the correlation is measured after a permutation test ( considering the correlation values obtained for 100 restart schedules with randomly ordered days ) .
In all experiments , the penalty parameter s∗ of AP is initially set to the median similarity value in the first bunch of the data stream , and updated by relaxation from the sequence of data considered .
The outlier threshold is set to ε = 025 The PH tolerance threshold is set to δ = 001 The PH threshold parameter λ is adjusted online , using discrete or continuous optimisation . In the discrete case , λ ranges in 40 , 50 , . . . 120 and a α greedy
5The baseline algorithm is defined by replacing AP with kcenters with multiple restarts ; the best performance out of 30 restarts is kept , ensuring that Strap and the baseline algorithm have same computational runtime .
992 optimization of the empirical average distortion ( section 3.2 ) is achieved ( α = 5% ) . In the continuous case , a Gaussian Process based estimate of the distortion ( Eq ( 3 ) ) is built , the the λ value with minimal estimated empirical distortion is selected and the model is updated [ 21 ] . 4.3 Algorithmic Assessment
The accuracy and purity of the Strap modelling are respectively displayed in Fig 3 and 4 . With respect to the 21 classes of jobs , the accuracy is consistently over 85 % , significantly outperforming the baseline algorithm . The adaptive adjustment of the λ parameter , based on discrete or continuous optimization , preserves Strap accuracy while decreasing the number of restarts ( omitted for space limitations ) . The clustering purity ( Fig 4 ) is over 90 % and confirms the quality of the clustering model . The relatively high number of clusters ( circa 200 ) must be understood in relation with the average number of jobs per day ( circa 15,000 ) . Unexpectedly , the clustering purity is higher than the accuracy , although the former indicator usually is a pessimistic one ( since all clusters , including those related to rare classes , have same weight ) ; experimentally , the difference in performance is explained as rare failure classes are associated to pure clusters .
Figure 4 : Strap : Clustering Purity and Number of Clusters , measured at each restart step ( discrete optimization of the change detection test parameter ) .
Table 1 : Stability of the Data Stream Clustering Model
Correlation Reference
0.7700 0.7362 0.7094 0.6609 0.6504
0.0430 0.0451 0.0398 0.0351 0.0353
|C1| 62 79 79 62 79
|C2| 103 124 103 69 69 s∗C1 4.82 3.21 3.21 4.82 3.21 s∗C2 4.42 2.95 4.42 8.84 8.84
Table 2 : Correlation between the restart schedules for different λ values ( Reference = maximal correlation out of 100 permutation tests )
λ1 40 40 40 50 50 75
λ2 50 75 100 75 100 100
Correlation Reference
0.88 0.77 0.70 0.79 0.74 0.87
0.17 0.24 0.19 0.26 0.18 0.20
4.4 Applicative Assessment
The typical summaries of the job flow provided by Strap to the EGEE system administrator are displayed in Fig 5 . The top snapshot corresponds to a standard situation , with a few outliers ( Reservoir ) , circa 10 % jobs stopping after registration ( exemplar ( 7 0 0 0 0 0) ) , circa 15 % stopping before arriving at the CE ( exemplar ( 10 47 54 129 0 0) ) , about 60 % successful short jobs and 10 % computationally heavy jobs . Two days later ( bottom snapshot ) , a new cluster appear including about 40 % of the jobs . The corresponding exemplar ( 10 18 29 20091 395 276 ) is immediately interpreted by the administrator as an alarm signal ; LM is getting clogged ( exemplar value 20091s , higher than the standard one by two orders of magnitude ) .
The load dynamics and trends can be assessed from the number of model restarts per day ( Fig 6 ) , comforted by the robustness analysis of this indicator presented in the previous section . This indicator however only provides a coarse feedback , for frequent restarts can be explained from several causes : i ) the load is abruptly increasing ; ii ) new job patterns appear ; iii ) job patterns oscillate , frequently appearing and disappearing .
Figure 3 : Strap : Online Clustering Accuracy
The stability of the data stream model is measured considering various values of the AP penalty parameter s∗ , ranging in 2.95 , 3.21 , 4.42 , 482 Table 1 displays the correlation of the partitions induced by the super exemplars ( clusters with representativity less than .5 % of the jobs have been filtered out ) : columns 3 and 4 respectively indicate the number of clusters obtained for s∗ values in columns 5 and 6 . The actual correlation ( column 1 ) is assessed from the reference value ( column 2 ) , computed as the best correlation of clustering Ci with 100 random perturbations of C2 .
Finally , the stability of the model dynamics is measured by varying λ in 40 , 50 , 75 and 100 , respectively inducing 699 , 558 , 371 and 284 restarts . The correlation between the restart schedules is significant up to the confidence level 99 % ( Table 2 ) :
012345x 1067580859095100time stepaccuracy( % ) lambda =40lambda =100lambda e−greedylambda gp streaming K−centers010020030040050080859095100Averaged purity of each cluster ( %)Restarts 050100150200250300350400450500050100150200250300Number of clustersNumber of clustersAveraged purity of each cluster993 Figure 7 : Visualization of the Stream Model along time ( x axis : time ; y axis : super exemplars ordered by attribute 4 ) results reported in this paper , made possible by the Grid Observatory initiative6 , have been considered to provide relevant and useful hints into the types and hidden causes of the grid traffic jams .
A current limitation of the approach remains its computational cost ; while it meets the real time constraint of the EGEE job stream , its batch performances still are about 8 hours by Matlab code and 2 hours by C/C++ ( on Intel 2.66GHz Dual Core PC with 2 GB memory ) for 5M+ jobs . Another limitation , deeply rooted in the AP frame , is that the number of exemplars is not easily controlled from the penalty parameter s∗ . How to address this limitation is our main perspective for further study . Independently , the Strap framework will be enhanced with visual mining facilities to support a flexible multi scale dashboard .
Acknowledgments This work has been partly supported by the Pascal 2 European Network of Excellence and the EGEE III Infrastructure Project .
6 . REFERENCES [ 1 ] F . Bergeaud and S . Mallat . Matching pursuit of images . In ICIP , pages 53–56 , 1995 .
[ 2 ] F . Cao , M . Ester , W . Qian , and A . Zhou .
Density based clustering over an evolving data stream with noise . In SIAM Conference on Data Mining ( SDM ) , pages 326–337 , 2006 .
[ 3 ] G . Cormode , S . Muthukrishnan , and W . Zhuang . Conquering the divide : Continuous clustering of distributed data streams . In ICDE , pages 1036–1045 , 2007 .
[ 4 ] M . Ester . A density based algorithm for discovering clusters in large spatial databases with noise : the uniqueness of a good optimum for k means . In SIGKDD , pages 226–231 , 1996 .
[ 5 ] U . Fayyad , G . Piatetsky Shapiro , and P . Smyth . From data mining to knowledge discovery : An overview . In Advances in Knowledge Discovery and Data Mining , pages 1–34 . MIT Press , 1996 .
6Deployed in the EGEE III European Infrastructure Project ( 2008 2013 ) http://wwwgrid observatoryorg/
Figure 5 : Online Snapshots of the Job Stream
Figure 6 : Strap : Number of restarts per day
A more detailed view of the load dynamics in a long timescale is based on using super exemplars ( section 33 ) The overall stream is visualized as a tapestry , each row corresponding to a given super exemplar , and each column corresponding to a day ( or a time period ; a zooming functionality allows the administrator to adjust the granularity of the visualization ) . The color of the super exemplar indicates the percentage ( or number ) of jobs associated to this superexemplar in the time period , enabling the administrator to spot the load regularities .
5 . DISCUSSION AND PERSPECTIVES
Resuming an earlier work devoted to Data Streaming with Affinity Propagation [ 22 ] , this paper shows that the computational complexity of the Strap algorithm can be reduced to a quasi linear complexity through a generalized Divideand Conquer approach − without incurring a significant distortion loss except in dimension 2 . Further , an adaptive procedure automatically adjusting the parameters of the change detection test has been proposed . Besides its theoretical analysis , the Strap algorithm has been validated on a challenging real world application , specifically the modelling of the EGEE Grid status from the job data flow . The first
12345020406080100Reservoir700000 10 47 54129 0 0 8 18 24 30595139 7 13 14 24 972819190ClustersPercentage of jobs assigned ( % ) exemplar shown as a job vector12345678020406080100Reservoir000000700000 10 47 54129 0 0 9 18 2520110 0 0 8 18 24 30595139 6 5 10 14 12710854 10 18 2920091 395 276ClustersPercentage of jobs assigned ( %)exemplar shown as a job vector0204060801001201401600246810121416daysnumber of restarts per day Sat & SunMonTueWedThuFrilineDaysSuper Clusters 204060801001201401020304050607080010%20%30%40%50%60%70%80%90%994 [ 6 ] B . Frey and D . Dueck . Clustering by passing messages between data points . Science , 315:972–976 , 2007 .
[ 7 ] J . Gama , R . Rocha , and P . Medas . Accurate decision trees for mining highspeed data streams . In SIGMOD , pages 523–528 , 2003 .
[ 8 ] J . Gama and P . P . Rodrigues . Stream based electricity load forecast . In PKDD , pages 446–453 , 2007 .
[ 9 ] S . Guha , A . Meyerson , N . Mishra , R . Motwani , and
L . O’Callaghan . Clustering data streams : Theory and practice . TKDE , 15:515–528 , 2003 .
[ 10 ] Z . Harchaoui , F . Bach , and E . Moulines . Kernel change point analysis . In NIPS , 2008 .
[ 11 ] D . Hinkley . Inference about the change point from cumulative sum tests . Biometrika , 58:509–523 , 1971 .
[ 12 ] D . Judd , P . K . McKinley , and A . K . Jain . Large scale parallel data clustering . IEEE Trans . Pattern Anal . Mach . Intell . , 20:871–876 , 1998 .
[ 13 ] M . Leone , Sumedha , and M . Weigt . Clustering by soft constraint affinity propagation : Applications to gene expression data . Bioinformatics , 23:2708 , 2007 .
[ 14 ] M . Meila . The uniqueness of a good optimum for k means . In ICML , pages 625–632 , 2006 .
[ 15 ] S . Muthukrishnan , E . v . d . Berg , and Y . Wu .
Sequential change detection on data streams . In ICDM Workshops , 2007 .
[ 16 ] E . Page . Continuous inspection schemes . Biometrika ,
41:100–115 , 1954 .
[ 17 ] N . Palatin , A . Leizarowitz , A . Schuster , and R . Wolff . Mining for misconfigured machines in grid systems . In SIGKDD , pages 687–692 , 2006 .
[ 18 ] C . E . Rasmussen and C . K . Williams . Gaussian
Processes for Machine Learning . MIT Press , 01 2006 .
[ 19 ] I . Rish , M . Brodie , and S . M . et al . Adaptive diagnosis in distributed systems . IEEE Trans . on Neural Networks , 16:1088–1109 , 2005 .
[ 20 ] G . Schwarz . Estimating the dimension of a model . The
Annals of Statistics , 6:461–464 , 1978 .
[ 21 ] J . Villemonteix , E . Vazquez , M . Sidorkiewicz and
E . Walter . Global optimization of expensive to evaluate functions : an empirical comparison of two sampling criteria . Journal of Global Optimization , vol 43 ( 2 3 ) , p.373 389 , 2009
[ 22 ] X . Zhang , C . Furtlehner , and M . Sebag . Data streaming with affinity propagation . In ECML/PKDD , pages 628–643 , 2008 .
[ 23 ] J . Andreeva , B . Gaidioz , J . Herrala , and et al .
Dashboard for the LHC experiments . Journal of Physics : Conference Series , vol . 119 , 2008 .
[ 24 ] Real Time Monitor : http://gridportalhepphicacuk/rtm/
[ 25 ] X . Zhang , C . Furtlehner , and M . Sebag . INRIA research report in progress .
APPENDIX A . SCHEMATIC PROOF OF PROPOSITION
2.2
For the sake of readability and to lighten the argument , the influence between the center of mass and extreme value statistics distribution is neglected , enabling us to use a spherical kernel instead of cylindrical kernel and making no dis tinction between ex and ˜ex , to write the recurrence ( see [ 25 ] for a complete discussion ) . Between level h and h + 1 , one has : f ( h+1 ) sd
( x ) =
K ( h,M )(x , y)f ( h,M ) ex
( y)dy
∞
0 with
M→∞ M lim
−1K ( h,M )( x M
, y M
) =
( 4 )
( 5 ) dy σ(h ) ) where K(x , y ) is the d dimensional radial diffusion kernel , d−2
2−d
1 2 x
K(x , y ) def= 2 −1 the modified Bessel function of index d/2 − 1 . with I d The selection mechanism of the exemplar yields at level h ,
4 I d−2
4 y xy
( cid:161 )
F ( h,M ) ex
( x ) =
F ( h ) sd ( x ) and with a by part integration , ( 4 ) rewrites as : f ( h+1 ) sd
( x ) = K ( h,M )(x , 0 ) + with lim M→∞ M
0
−1K ( h,M )(
∞
( cid:161 )
F ( h ) sd ( y ) x M
, 0 ) =
( cid:161 ) dx
2σ(h ) d 2Γ( d 2 )σ(h ) d σ(h ) K( dx σ(h ) ,
2
− x+y e 2 .
( cid:161)√ ( cid:162 ) ( cid:162)M , ( cid:162)M ∂K ( h,M ) ( cid:161)− dx ( cid:162 ) d
2 −1 exp
∂y
( x , y)dy ,
( cid:162 )
.
2σ(h )
At this point the recursive hierarchical clustering is described as a closed form equation . Proposition 2.2 is then based on ( 5 ) and on the following scaling behaviors ,
M→∞ F ( h,M ) lim ex so that
M→∞ F ( h+1 ) lim sd x M γ ) = lim
M→∞ M 1−γ
( cid:162 )
( cid:161 ) x
2 d
M
= exp
( cid:162 ) d 2
,
( cid:161)−α(h)x ∞ dy
0 duf ( h,M )
˜ex
( y
2 d
M
)K(M 1−γu ,
M 1− d 2 y σ(h )
) .
(
∞ x
σ(h )
Basic asymptotic properties Id/2−1 yield with a proper choice of γ , the non degenerate limits of proposition 22 In the particular case d = 2 , taking γ = 1 , it comes :
∞ dy x
σ(h )
∞
0 du dy
( cid:162 )
α(h )
1 + α(h)σ(h ) x
,
M→∞ F ( h+1 ) lim sd
= − x M
) =
∞ x
σ(h )
(
∞ ( cid:161)−
0 dy duf ( h )
˜ex ( σ(h)y)K(u , y ) de− ˜α(h)σ(h)x
√ I0(2
−(u+y ) uy)e with help of the identity dxxν e
−αxI2ν ( 2β
√ x ) =
( cid:162)2ν e
( cid:161 ) β
α
1 α
β α .
= exp
∞
0
Again in the particular case d = 2 , by virtue of the exponential law one further has α(h ) = 1/σ(h ) , finally yielding :
β(h+1 ) =
β(h ) .
1 2
( 6 )
995
