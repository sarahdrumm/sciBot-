Collective Annotation of Wikipedia Entities in Web Text
Sayali Kulkarni , Amit Singh , Ganesh Ramakrishnan , and Soumen Chakrabarti ganesh@cseiitbacin , soumen@cseiitbacin
IIT Bombay
ABSTRACT To take the first step beyond keyword based search toward entity based search , suitable token spans ( “ spots ” ) on documents must be identified as references to real world entities from an entity catalog . Several systems have been proposed to link spots on Web pages to entities in Wikipedia . They are largely based on local compatibility between the text around the spot and textual metadata associated with the entity . Two recent systems exploit inter label dependencies , but in limited ways . We propose a general collective disambiguation approach . Our premise is that coherent documents refer to entities from one or a few related topics or domains . We give formulations for the trade off between local spot to entity compatibility and measures of global coherence between entities . Optimizing the overall entity assignment is NP hard . We investigate practical solutions based on local hill climbing , rounding integer linear programs , and pre clustering entities followed by local optimization within clusters . In experiments involving over a hundred manuallyannotated Web pages and tens of thousands of spots , our approaches significantly outperform recently proposed algorithms . Categories and Subject Descriptors : H33 [ Information Search and Retrieval ] : Information Systems – Information Storage And Retrieval General Terms : Algorithms , Experimentation Keywords : Entity annotation/disambiguation , Wikipedia , collective inference 1 .
INTRODUCTION
A critical step in bridging between unstructured Web text and semistructured search and mining applications is to identify textual references ( called “ spots ” ) to named entities and annotate the spots with unambiguous entity IDs ( called “ labels ” ) from a catalog . These entity ID annotations enable powerful join operations that can combine information across pages and sites . Named entity recognition and tagging have seen widespread success [ 17 ] . Here we are concerned with the second step : entity disambiguation from a given catalog , such as Wikipedia . ( The availability of a catalog makes this a supervised setting , unlike unsupervised coreference resolution . ) 1.1 Entity catalogs
The success of semantic annotation is greatly determined by widespread adoption of the entity catalog . For common English words , WordNet [ 14 ] provides an authoritative lexical network designed by linguists , and widely used for disambiguation of common words [ 1 ] . CYC and OpenCYC [ 12 ] Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . To copy otherwise , to republish , to post on servers or to redistribute to lists , requires prior specific permission and/or a fee . KDD’09 , June 28–July 1 , 2009 , Paris , France . Copyright 2009 ACM 978 1 60558 495 9/09/06 $500 are partly commercial efforts to maintain entity catalogs , rules and reasoning engines . To understand and maintain TAP [ 8 ] , WordNet , or OpenCYC , substantial training is needed in knowledge representation and linguistics .
In contrast , the “ Web 2.0 ” trend is to throw open tagging and cataloging of knowledge to the masses . Wikipedia is a stunning example of the success of this strategy : it has over 0.34 million categories and over 2.6 million cataloged entities , and keeps up with world events on an hourly or daily basis . The flip side is that Wikipedia lacks the rigorous “ knowledge base ” quality of TAP or OpenCYC . There is little by way of schema , quality of authorship is diverse , and the category hierarchy is haphazard . The challenge of Web mining systems is to harness the chaotic “ wisdom ” of the crowds into relatively clean knowledge .
1.2 Prior work and limitations
Most existing systems annotate only salient entity references . In some prototypes , only entities of specific recognized types ( most often people and locations ) are disambiguated . The goal is to emulate Wikipedia ’s restrained , informative , editorial links on ordinary Web pages . SemTag . The first Web scale entity disambiguation system was SemTag [ 5 ] . SemTag annotated about 250 million Web pages with IDs from the Stanford TAP entity catalog [ 8 ] . The basic technique was to compare the surrounding context of a spot s with text metadata associated with candidate entity γ in TAP . SemTag preferred high precision over recall , proposing only about 450 million annotations , ie , fewer than two annotations per page on average . Wikify! . Wikify! [ 13 ] has two components . The first , keyword extraction , decides if a phrase should be linked to Wikipedia . This is based on how often a word or phrase is found to be in the anchor text of some link internal to Wikipedia . The second step is disambiguation . Wikify! , too , is conservative in flagging keywords , so much so that even random disambiguation results in an F1 score of 082 Suppose Wikify! is considering linking spot s to entity γ . Wikipedia ’s page describing γ is explicitly referred from other Wikipedia pages . The context of these known citations is compared with the context of s to decide on a compatibility score . This may be regarded as generalizing SemTag , where known references to γ form part of the metadata of γ . Bunescu and Pasca [ 3 ] further improved the compatibility function using SVMs with tree kernels . However , none of these systems attempt collective disambiguation across spots . M&W . A limited form of collective disambiguation proposed by Milne and Witten [ 15 ] yields considerable improvement beyond Wikify! . M&W propose a relatedness score ) between two entities . From the set of all spots S0 , r(γ , γ they identify the subset S! of so called context spots that can refer to exactly one entity each ( let this entity set be Γ! ) . They define a notion of coherence of a context spot
.
457 γ ∈ Γ! based on its relatedness to other context spots . For an ambiguous spot s fi∈ S! , the score of a candidate entity γ fi∈ Γ! is strongly influenced by its mention independent prior probability Pr0(γ|s ) , its relatedness to context entities on the page , their coherence , and a measure of overall quality of context entities . M&W also propose a link detector ( a function similar to keyword extraction in Wikify! ) that , like SemTag and Wikify! , sacrifices recall for high precision . For the spots picked by M&W for labeling , even random disambiguation achieves an F1 score of 053 Cucerzan ’s algorithm . To our knowledge , Cucerzan [ 4 ] was the first to recognize general interdependence between entity labels in the context of Wikipedia annotations . He represents each entity γ as a high dimensional feature vec ) as the inner product ( or tor g(γ ) , and expressed r(γ , γ cosine , if 'g(γ)' are normalized ) g(γ ) fi ) , also written as g(γ ) · g(γ ) . Let Γ0 be all possible entity disambiguations for all spots on a page . He precomputes the average vecγ∈Γ0 g(γ ) . The score of candidate γ for spot tor g(Γ0 ) = s depends on two factors . The first , like SemTag or Wikify! , is a local context compatibility score . The second is g(Γ0 \ {γ} ) , reminiscent of leave one out cross validag(γ ) tion . Cucerzan also annotates very sparingly : only about 4.5 % of all tokens are annotated .
P g(γ fi
.
.
.
A problem with this approach is that Γ0 is contaminated with all possible disambiguations of all spots , so this check for “ agreement with the majority ” may be misleading . Note that both M&W and Cucerzan avoid direct joint optimization of all spot labels , which is precisely what we undertake .
The above line of work has some similarity to identifying mentions of entities in databases ( eg product catalogs ) amidst unstructured text ( eg , blogs ) [ 2 ] , but , in such applications , the “ entity catalog ” is a clean relational database , and , to our knowledge , no collective labeling is employed . 1.3 Our goals and contributions
Our goal in this paper is aggressive open domain annotation of Web pages with entity IDs from an entity catalog such as Wikipedia . We contrast this with a more restricted disambiguation of entities which achieves high precision by sacrificing recall . The central purpose of our annotation is not direct human consumption , but downstream indexing , search and mining .
For example , we may gather from one page that m is a mathematician , and from another , that m plays the violin . Such data can be aggregated to explore whether scientists tend to play music significantly more or less often than other people mentioned on the Web .
Our guiding premise is that documents largely refer to topically coherent entities , and this “ coherence prior ” can be exploited for disambiguation . While Michael Jordan and Stuart Russell can refer to seven ( basketball player , footballer , actor , machine learning researcher , etc . ) and three ( politician , AI researcher , DJ ) persons respectively in Wikipedia ( as of early 2009 ) , a page where both Michael Jordan and Stuart Russell are mentioned is almost certainly about computer science , disambiguating them completely .
We propose a collective optimization problem that precisely models the combination of evidence from local spot toentity compatibility ( “ node potential ” ) and global page level topical coherence ( “ clique potential ” ) of the entities chosen to disambiguate all spots . Our optimization is equivalent to
S0 S ⊆ S0 s ∈ S Γs Γ0 Γ ⊆ Γ0 γ ∈ Γ na ρna
N0 ⊆ S0
A0 = S0 \ N0
Sγ SΓ
All candidate spots in a Web page Arbitrary set of spots One spot , including surrounding context S Candidate entity labels for spot s s∈S0 Γs , all candidate labels for page
An arbitrary set of entity labels An entity label value , here , a Wikipedia URN The “ no attachment ” decision The “ reward ” for setting ys = na Spots assigned na Spots assigned some γ '= na S Spots that can disambiguate to γ
γ∈Γ Sγ
Figure 1 : Notation . searching for the maximum probability annotation configuration ( inference ) in a probabilistic graphical model where each page is a clique . Inference is NP hard . We propose practical and effective heuristics based on local hill climbing and linear program relaxations . Our framework also applies to word sense disambiguation [ 1 ] , and therefore , may be of independent interest .
We describe our experiments with two data sets . Cucerzan ’s ground truth data [ 4 ] has annotations only for persons , places and organization and is limited to only 700 spots on nonWikipedia data . While SemTag was run on 264 million pages and produced 434 million annotations , human judgement was collected on only about 1300 spot labelings ( and this data is not publicly available ) . We built a browserbased annotation UI that six volunteers used to collect over 19,000 spot annotations on more than 100 pages ; largest . Experiments show that we can among known prior work significantly push the recall envelope without hurting precision . Our trained node potential alone can improve F1 accuracy considerably compared to all of Wikify! , Cucerzan and M&W ’s algorithms . Taking clique potentials into consideration using LP rounding or greedy hill climbing gives further accuracy gains . 2 . NOTATION AND PRELIMINARIES 2.1 Spots and entity labels
1
Because we process one page at a time , we will elide the page in our notation . A spot s is a ( short ) token ( sequence ) that is potentially a direct reference to an entity in Wikipedia . We do not consider indirect references like pronouns , but do aim to resolve imperfect matches such as Michael for Michael I . Jordan . The context of s is the text in a suitable window around s . An entity is expressed as a URN in Wikipedia , and denoted γ . na is a special label denoting “ no attachment ” , ie an algorithm can avoid labeling a spot to increase precision at the cost of recall . ρna ≥ 0 is a tuned parameter to guide this tradeoff . See Figure 1 . 2.2 Compatibility feature vector fs(ys ) ys is a variable denoting the entity label , taking a value from Γ0 ∪{na} . y is the vector of all spot labels . fs(γ ) ∈ R d is a feature vector whose elements express various measures of compatibility between s and γ . The context of a spot is a bag of words collected from a suitable window around the candidate entity reference .
1
The data is in the public domain , see http://soumencse iitbacin/~soumen/doc/CSAW/ or http://wwwcseiitb ac.in/~soumen/doc/CSAW/
458 to an entity γ is represented by four fields .
Wikipedia is preprocessed so that each page corresponding • Text from the first descriptive paragraph of γ . • Text from the whole page for γ . • Anchor text within Wikipedia for γ . • Anchor text and five tokens around it .
Each field is turned into a bag ( multiset ) of words . Three text match scores are computed between a field of γ and s :
• Dot product between word count vectors . • Cosine similarity in TFIDF vector space . • Jaccard similarity between word sets .
So in all , we get 4 × 3 = 12 features . Sense probability prior . Some γ ∈ Γs are very obscure and rare ; ie , Pr(γ|s ) is very low . Eg , Intel is ( also ) a fictional cartel in a 1961 BBC TV serial , but this sense is much rarer than the semiconductor giant . We can easily count from intra Wikipedia links the fraction of times a link with Intel in the anchor text points to every sense of Intel , and use this as a prior estimate of Pr0(γ|s ) . The last element of fs(γ ) is log Pr0(γ|s ) ( the log is explained below ) . This feature is somewhat different from local compatibility features , so we will often study its effect separately . 2.3 Compatibility score ( node potential ) d fi fs(γ ) where w ∈ R
The local compatibility score between s and γ is modeled is a model vector . For a locally as w optimal choice , we would pick arg maxγ∈Γs w fs(γ ) as the P If we had to normalize this to a probability , label for s . we would use a logistic model Pr(γ|s ) = exp(w fi fs(γ))/Zs ) ) , is the partition function where Zs = ( hence the log in the sense probability feature above ) . We fs(· ) ) the node potential of s , using graphical call exp(w model [ 9 ] terminology .
γ.∈Γs exp(w fs(γ fi fi fi
.
We train w using a max margin technique . Given ground s ∈ Γs , we want w ∗ ∗ s ) to be larger truth assignment γ fi fs(γ ) , with a margin ; this gives us the than any other w s fi= na only ) : ∗ usual SVM linear constraints ( for spots with γ fs(γ fi fi fi fs(γ s ) − w ∗ s ∈ Γs : w ∗
∀s,∀γ fi= γ fs(γ ) ≥ 1 − ξs P and we minimize over ξ ≥ ff0 and w the objective 'w'2 2 + s ξs where C is the usual balancing parameter . Our C training data had tens of thousands of spots , and the Γs has more than 10 elements , leading to ∼ 10 constraints . Rather 2 and a QP solver , we used 'w'1 which let us than use 'w'2 use a more scalable LP solver ( Mosek ) . More notation is summarized in Figure 2 . 2.4 Label embedding and relatedness
6
The last and most important piece in our model is an embedding of labels γ in a suitable ( usually high dimensional ) feature space : g : Γ0 → R . This embedding is used to define relatedness between two entities . c
.
241 Cucerzan ’s category based g(γ ) and r(γ , γ The Wikipedia page for γ lists a set of categories that γ belongs to . Eg , γ = Michael_jordan is a Sportspeople of multiple sports while γ = Michael_I._Jordan is a Machine learning researcher . If there are c categories in Wikipedia , the categories that γ belongs to can be represented by a c long bit vector , which is designated as g(γ ) . Cucerzan
) ys y ∗ y ∗ Γ d fs(ys ) ∈ R w ∈ R NPs(ys ) g(γ ) ∈ R c . ) r(γ , γ CP(y )
Variable denoting label assigned to spot s Vector of all label assignment for page Vector of ground truth labels Set of ground truth labels in y
∗ d Compatibility feature vector between s and ys
´
` w fi fs(ys )
Compatibility weight vector exp An embedding of γ in suitable space ( see text ) Topical relatedness between γ , γ Topical coherence among all labels for page
, spot to label compatibility
.
Figure 2 : More notation . defined the relatedness between two entities γ , γ p
.
) = r(γ , γ fi p
. g(γ ) fi g(γ ) g(γ
) . g(γ fi ) g(γ
,
.
) g(γ )
. as a standard cosine measure .
Wikipedia ’s categorization is organic and uncontrolled : γ = Michael_I._Jordan also belongs to categories called Living people and Year of birth missing , which are not topical . We tried to mitigate this with various weighting schemes , but Cucerzan ’s algorithm nevertheless performed worse than each of our algorithms , which used the relatedness definition described next . 242 M&W ’s inlink based g(γ ) and r(γ , γ Cocitation has been used to detect relatedness for a long time [ 11 ] . Milne and Witten [ 15 ] represented g(γ ) as the set of Wikipedia pages that link to γ , with size |g(γ)| . Let c be the total number of Wikipedia pages . M&W defined a relatedness measure ( larger value implies more related ) as :
)
.
.
) = r(γ , γ log |g(γ ) ∩ g(γ
.
)| − log max{|g(γ)|,|g(γ
.
)|} log c − log min{|g(γ)|,|g(γ
.
)|}
The numerator is a slight variation on Jaccard similarity , )|} . and the denominator is inversely related to min{|g(γ)|,|g(γ . Unless otherwise specified this is the measure we use . 2.5 Range compression
To robustly balance between local and global signals having diverse dynamic ranges , we apply a range compressor function R(· ) to all elements of vectors fs(· ) and g(· ) . Specifically ,
(
R(t ) = log(1 + t ) , − log(1 − t ) , t ≥ 0 t < 0 .
This limits the numeric output range without clipping it at . any value . Henceforth , when we write “ fs(ys ) ” or “ g(γ ) we will mean R(fs(ys ) ) or R(g(γ ) ) ) ) respectively . To keep notation uncluttered , we will hide R and this preprocessing step . Note that R(· ) is not applied to ρna , the reward for assigning label na to a spot . 3 . THE DOMINANT TOPIC MODEL
R(g(γ g(γ fi fi
.
) ” ,
We now describe our main model and inference approaches . The key is to define , over and above node potentials , a collective score based on pairwise topical coherence of all γs used for labeling . 3.1 Coherence score ( clique potential )
.
. are used as labels for s , s
For the moment , disallow ys = na . Consider Figure 3 . If , their agreement is defined ) . For the whole page , the overall agreement is
γ , γ as r(γ , γ
.
459 Candidate labels
γ
Γ s
γ′ g(γ ) g(γ′ )
Document s
Spots s’ l e b a l o t t o p S y t i l i b i t a p m o c
P
Γ s’ . ∈ Γs . have to be chosen Figure 3 : Labels γ ∈ Γs , γ . to maximize a combination of spot tofor spots s , s label compatibility scores NPs(γ ) , NPs . ( γ ) as well as topical similarity between γ and γ
Inter label topical coherence
, say , g(γ ) g(γ fi
) .
.
.
.
“ P
Q s=s.∈S0 r(ys , ys . ) . In keeping with standard aggregated as graphical models style [ 9 ] , we can turn this into a clique potential
”
CP(y ) = exp s=s.∈S0 r(ys , ys . )
,
( 1 )
Q
P and the overall probability of a label assignment y is writs∈S0 NPs(ys ) , where Z = ten as Pr(y ) = ( 1/Z ) CP(y ) . s ) is a scale factor that makes the y . CP(y s∈S0 NPs(y
)
. probabilities add up to 1 over all possible y .
Evaluating Z is difficult because an exponential number of terms need to be added up . For predicting the most likely label vector , finding Z is not needed ; we just need arg max y
Pr(y ) = arg max y
CP(y ) s
Y
NPs(ys )
X
= arg max y log CP(y ) + log NPs(ys ) s r(ys , ys . ) + fi w fs(ys ) .
X
= arg max y s=s.∈S0
X s∈S0
The two sums have different number of terms , which also vary from page to page . To be able to use a single consistent w across all pages , we need to scale the two parts to a compatible magnitude . So our objective , barring nas , is
´ X 1`|S0|
1|S0| 3.2 Recall precision balance r(ys , ys . ) + s=s.∈S0
2
X s∈S0 fi w fs(ys ) .
Almost a third of spots in our ground truth data are marked “ na ” by volunteers , meaning that no suitable entity was found in Wikipedia . This is a reality on the opendomain Web , and many systems [ 13 , 3 , 15 ] can back off from annotation ( indeed , back off aggressively ) . To implement a recall precision balance , we use one tuned parameter ρna ≥ 0 , the reward for not assigning a spot any label . Let N0 ⊆ S0 be the spots assigned na , and A0 = S0\N0 the remaining spots . We thus get our final objective :
! max y
1|S0|
X s∈N0
+
ρna +
X ´ X 1`|S0| s∈A0
2 s=s.∈A0 fi w fs(ys )
( NP ) r(ys , ys . )
( CP1 )
´ `|S0|
`|A0| ´
2
2 be replaced by in The reader may demand that ( CP1 ) . This creates difficulty for at least one of our inference approaches , because A0 depends on y and the resulting optimization can no longer be written as an integer linear program . One possible rationalization is that na has zero topical coherence with any other label , including another instance of na : r(na,· ) = r(· , na ) = r(na , na ) = 0 ;
( 2 ) therefore , the edge potential sum can be rewritten over s fi= . ∈ S0 , not s fi= s denominator is s acceptable .
. ∈ A0 , so that the
`|S0| ´
2 fi
A reasonable way to tune ρna would be to first compute fs(ys ) across all pages and spots in the typical value of w the training set , then sweep ρna between 0.1× to 10× of that typical value . We use this approach in our experiments . 3.3 Complexity of inference
Figure 3 , ( NP ) and ( CP1 ) get to the heart of the collective disambiguation problem , so it is of interest to understand the complexity of inference . Proposition 1 . Inference problem maxy ( NP ) + ( CP1 ) is NP hard , even when ρna = −∞ and therefore A0 = S0 .
The reduction is from the maximal clique problem [ 7 ] . We also note that other natural definitions of CP do not make the problem easier . Proposition 2 . The inference problem remains NP hard with the following alternative definitions of CP : 'g(yi ) − g(yj)'2
CP(y ) = exp
X
( 3 )
2
1 A «
CP(y ) = exp
'g(yi ) − g(yj)'∞
( 4 )
0 @− „ i=j − max i=j
Hardness using ( 3 ) is shown using a reduction from exact cover by 3 sets [ 7 ] . Hardness using ( 4 ) is shown using a reduction from 3SAT . Proofs are omitted to save space . 3.4 LP rounding approach Guided by approaches to Quadratic Assignment Problems ( QAPs ) [ 16 ] we can turn our optimization into a 0/1 integer linear program , and then relax it to an LP . First disallow ys = na . The ILP is designed with up to |Γ0|+|Γ0|2 variables zsγ = [ [spot s is assigned label γ ∈ Γs ] ] uγγ . = [ [both γ , γ assigned to spots ] ]
.
The node potential part is written as fi zsγw fs(γ )
. ( NP
) and the clique potential part is written as uγγ . r(γ , γ
.
)
. ( CP1
)
X
X s∈S0
γ∈Γs
X
1|S0| ´ X 1`|S0|
2 s=s.∈S0
γ∈Γs,γ∈Γs where we assume ( 2 ) . So the goal is to
. . max ) + ( CP1 {zsγ ,uγγ.}(NP ∀s , γ : zsγ ∈ {0 , 1} , P : uγγ . ≤ zsγ γ zsγ = 1 .
∀s , γ , γ . ∀s :
) st ∀γ , γ . and uγγ . ≤ zsγ .
: uγγ . ∈ {0 , 1}
( 5 )
( 6 ) ( 7 )
460 Constraints ( 6 ) enforce what we need , because , if zsγ = zsγ . = 1 , the objective will push uγγ . = 1 . The formulation generalizes readily to the na case using one more variable zsna per spot , changing constraint ( 7 ) to
X
∀s : zsna +
P zsγ = 1
γ
. s∈S0 ρnazsna to the objective . and adding a term 1|S0| 341 Integrality gap The relaxed LPs replace constraints ( 5 ) with 0 ≤ zsγ ≤ 1 and 0 ≤ uγγ . ≤ 1 . The optimal LP objective will be an upper bound on the optimal ILP objective . To understand how loose the upper bound can be in the worst case , consider the following “ butterfly graph ” example . ( Disallow na using ρna = −∞ . ) There are two spots s1 , s2 , with Γs1 = {γ1 , γ2} and Γs2 = {γ3 , γ4} . Assume all node potentials are zero , and ´ ) = 1 . The optimal integral solution can have at all r(γ , γ most one uγγ . = 1 , leading to an objective value of 1/ = 1 . The fractional solution will find it best to assign all zsγ = uγ,γ . = 1/2 , with an objective of 4×0.5 = 2 . The gap can be increased arbitrarily by increasing the bipartite clique size , ie , |Γs| . 342 Rounding policy In our experiments , we found about 70 % of pages to give completely integral ( hence , optimal ) solutions . The obvious rounding strategy for fractional solutions is arg maxγ∈Γs∪na zsγ . We found that this tended to label na as some γ fi= na . Insisting that zsγ > 1/2 was more reticent and gave slightly better F1 .
`
2 2
( 0 ) select a small spot set SΔ for each s ∈ SΔ do
1 : initialize some assignment y 2 : for k = 1 , 2 , . . . do 3 : 4 : 5 : 6 : 7 : 8 : return latest solution y to y find new γ that improves objective ( k ) s = γ greedily change y
( k−1 ) s if objective could not be improved then
( k )
Figure 4 : Dominant cluster hill climbing ( Hill1 )
3.5 Hill climbing approach
Another approach is to avoid math programming and deploy a direct greedy hill climbing approach . Hill climbing has the advantage that it can be easily stopped and interpreted at any time , and may achieve acceptable accuracy faster than solving and rounding an LP . The generic template is shown in Figure 4 . It remains to specify the initialization , and how to make label modifications . 351 Initialization Some initializations suggest themselves : • Initialize all ys = na • Initialize all ys fi= na as per node potential alone , ie , arg maxγ∈Γs w fi fs(γ ) ( we use this option )
In our experiments we did not find significant differences between accuracies obtained using the above initializations . 352 Label updates We tried perturbing sets SΔ of sizes 1 and 2 . The rationale for trying to perturb a pair of spots was that any single spot perturbation may appear unattractive while at a local optimum . However , |SΔ| = 2 was already too slow to improve upon LP speeds . So we concentrate on single moves . If the label of s is changed from γ1 to γ2 , node score ( NP ) j ff changes by − ff j
ρna fi w fs(γ2 )
( γ2 = na ) ( ow )
ρna fi w
( γ1 = na ) ( ow ) fs(γ1 )
+ |S0| and edge score ( CP1 ) changes by
´ X 1`|S0|
` s.=s
2
´ r(ys . , γ2 ) − r(ys . , γ1 )
4 . EXPERIMENTS 4.1 Testbed
411 Preprocessing Wikipedia We downloaded the August 2008 version of Wikipedia , and prepared a dictionary of entity IDs , their labels and mentions , as follows : • A set of 5.15 million entity IDs , including titles , redirections , disambiguations , and category names was first collected from the dump . • A subset of these entity IDs was filtered out . An entity ID was filtered out either if it was composed purely of verbs , adverbs , conjunctions or prepositions or if it conformed to certain lexical patterns ( eg , fewer than three characters ) . The former rules pruned about 15,000 entity IDs ; the latter pruned about 16,100 ( of a total of 2.6 million ) . • To enable efficient lookups of entity labels at runtime , a trie ( prefix tree ) of the filtered Wikipedia entity IDs 2 was constructed using the Webgraph • Spots are identified by first tokenizing the document ( based on punctuation and white space as delimiters ) and then identifying token sequences that maximally match an entity ID in the trie . Consequently , any candidate spot that happens to be a substring of another candidate spot will be subsumed into the latter . • For each identified spot indexed i , all entity IDs found to have the same surface forms are associated with the spot to yield a set Γi of its possible disambiguations . framework .
412 Preparing corpora and annotations Earlier work has used Wikipedia text itself as ground truth annotations . This is not suited to our aggressive recall target , so we looked for other data . SemTag collected only about 1300 manually labeled spots for quality checking , and these are not publicly available . Data used by Bunescu and Pasca [ 3 ] was not publicly available . Cucerzan ’s data [ 4 ] ( which we abbreviate to ‘CZ’ ) is available and we do use it , but annotations are sparse and limited to a few entity types . Several URN labels in CZ data no longer exist in Wikipedia . Moreover , there is no na annotation .
Therefore we undertook to build a ground truth collection ( which we call “ IITB ” ) using a browser based annotation system . Documents for manual annotation were collected from the links within homepages of popular sites belonging to a
2 http://webgraphdsiunimiit/
461 Figure 5 : Browser based annotation GUI . For each s , trainers choose γ
∗ s from a pulldown menu showing Γs . handful of domains that included sports , entertainment , science and technology , and health ( sources : http://news.go ogle.com/ and http://wwwespnstarcom/ ) Figure 6 summarizes some important corpus statistics . The annotations are available in the public domain . Both IITB and CZ data have high average ambiguity . CZ ’s is higher because the spots are limited to common person and place names . Obviously , random assignment would get very poor accuracy , unlike M&W . 413 Browser based annotation GUI CZ data came pre annotated , but for the IITB corpus , we built a browser based annotation tool . As illustrated in Figure 5 , candidate spots are highlighted to differentiate between pending and already annotated spots . Clicking on a spot drops down a list of possible disambiguations . Hovering on a specific Wikipedia label shows an excerpt from the definition paragraph of the corresponding entity .
In the IITB data , we collected a total of about 19,000 annotations by 6 volunteers . Unlike in previous work , volunteers were told to be as exhaustive as possible and tag all possible segments , even if to mark them as na . The number of distinct Wikipedia entities that were linked to was about 3,800 . About 40 % of the spots was labeled na , highlighting the importance of backoffs . However , this also says that 60 % of the spots were attached by volunteers , which by far exceeds the token rate of attachment in earlier work . While its absolute scale is impressive , SemTag produced only 434 million annotations from 264 million Web pages , or fewer than two per page . From Figure 6 , we see that the CZ data identifies only about 15 spots per page . We thus highlight that we are in a completely different recall regime .
The annotation module allows each document to be tagged by two volunteers . Figure 7 summarizes some statistics on inter annotator agreement . Clearly , a considerable number of disagreements are over na vs . “ not na ” . 414 Evaluation measures
Accuracy . A simple option would be to count the fraction ∗ s ) which of spots s ( that have manually associated labels γ ∗ ∗ get assigned ys = γ 0 and A 0 alike . However , typical applications will be asymmetric in how they react to these labels . Eg , an indexing engine that incorporates ob
∗ s , over N ject IDs will simply ignore na labels . Therefore , we need to ∗ 0 separately . also focus on A Recall , precision , F1 . Suppose , in ground truth , the set ∗ of spots marked na is N 0 is the set of spots marked some label other than na . We will be largely concerned about the precision , recall , and F1 scores of spots ∗ 0 . The fate of such a spot can be one of the following : in A
0 = S0 \ N ∗ ∗ 0 , and A
A → A : Correctly labeled A . A : Algorithm picks wrong label γ fi= na A . na : Algorithm picks γ = na |{A → A}|
|{A → A}| + |{A . A}| + |{A . na}| |{A → A}| precision = recall =
|A 0| ∗
Precision and recall are ( macro ) averaged across documents and overall F1 computed from average precision and recall . Note that the presence of na makes these definitions different from what Cucerzan and M&W measured as spot labeling accuracy after spot detection .
All parameters were tuned using 2 fold cross validation .
4.2 Local NP optimization
As a first step , we are interested in evaluating the effect of training w , isolated from the influence of clique potentials . For this , we ran a very simple system that we will call Local . Local used the trained w to choose a label for each spot independent of others , without any collective information :
Number of documents Total number of spots Spot per 100 tokens Average ambiguity per Spot
IITB 107 17,200 30 5.3 Figure 6 : Corpus statistics .
CZ 19 288 4.48 18
#Spots tagged by more than one person #na among these spots #Spots with disagreement #Spots with disagreement involving na
1390 524 278 218
Figure 7 : Inter annotator agreement .
462 fi fi fs(γ ) fs(γ0 ) > ρna then return γ0 else return na
1 : γ0 ← arg maxγ∈Γs w 2 : if w If fs(· ) does not include the sense probability prior , we call the above strategy Local , otherwise we call it Local+Prior . 4.3 Effect of learning node potentials
M&W use two important signals , relatedness and commonness , in their disambiguator . In Figure 8 we present ablation studies showing the relative effectiveness of various features , together with the benefits of using all features with a learnt model w .
75
70
65
60
55
50
%
, 1 F
Wiki full page , cosine
Anchor text , cosine
Anchor text context , cosine
Wiki full page , Jaccard
All features ( learn w )
Figure 8 : Training a model w is better than using any single spot label compatibility feature .
4.4 Comparison with earlier algorithms
Rather surprisingly , Local already produced significantly better F1 scores than the two state of the art annotations systems by M&W and Cucerzan .
The M&W algorithm can be directly executed on any page . The API includes a knob text using a Web service API to control the recall precision balance . We implemented Cucerzan ’s algorithm locally . Cucerzan ’s algorithm does not have a recall precision knob . In Local , we used ρna as the knob .
3
100
% i
, n o s c e r i
P
80
60
40
20
Local Local+Prior M&W Cucerzan
Recall , % 0
20
40
60
80
Figure 9 : Even a non collective Local approach that only uses trained node potential dominates both Cucerzan and M&W ’s algorithms wrt both recall and precision ( IITB data ) .
Figure 9 shows recall precision plots . Cucerzan ’s algorithm is shown by a single point . M&W ’s precision is very high , consistent with their claims . However , the R/P knob cannot increase recall beyond 20 % . Meanwhile , the ρna knob
3 http://wwwnzdlorg/pohutukawa/wikifier/indexjsp in Local can be used to push it to 70 % recall while remaining comparable to M&W precision . If we dial down our recall to levels comparable with M&W , our precision becomes visibly larger than M&W . Cucerzan ’s recall and precision are both dominated by Local , like M&W . Local+Prior is substantially better than Local , and is a formidable F1 level to beat .
Cucerzan did not learn the node potential but hardwired it . We gave Cucerzan ’s algorithm the benefit of our learned node potentials . The F1 score improved to 51.8 % , which was still short of Local and far short of Local+Prior . 4.5 Hill1 update trajectories
In Figure 10 we consider the trajectory of several documents ( one line per document ) as Hill1 optimizes their labels . Specifically , we plot the objective minus the ρna contribution on the x axis , and correspondingly , the F1 score for spots that are marked some non na label in ground truth on the y axis . Although there are occasional expected setbacks and oscillations , increasing the objective is generally good for F1 too . This lends credibility to our basic dominantcluster model . doc1 doc3 doc5 doc2 doc4 doc6
%
,
1 F
50
40
30
20
0.75
0.8
0.85
0.9
0.95
1
Objective ( normalized )
Figure 10 : As Hill1 improves our proposed objective , it usually improves F1 as well ( IITB data ) .
Hill1 LP1 rounded LP1 relaxed
1000 e v i t c e b O j l a t o T
900
800
700
1
2
3
4 rhoNA
5
6
7
8
Figure 11 : Hill1 can attain objectives comparable to relaxed LP1 ( IITB data ) .
4.6 Hill1 vs . LP1
For over 70 % of the documents , LP1 gives fully integral solutions , which are therefore optimal for our integer programs . Even otherwise , LP1 gives an efficiently computable , yet reliable upper bound to the objective that Hill1 is trying to attain . Figure 11 shows that in practice , the integrality gap is small , that Hill1 gets reasonably close to the upper
463 bound , and that rounding makes LP1 slightly worse than Hill1 .
%
,
1 F
65
55
45
35
25
15
Local Hill1 LP1
90
80
Local Hill1 LP1
Local+prior Hill1+prior LP1+prior
%
,
70 i n o s c e r i
60
P
50
40
Recall , %
40
50
60
70
80
1
2
3
4 5 rhoNA
6
7
8
Figure 14 : Recall/precision on IITB data .
Figure 12 : Hill1 attains almost the same F1 score as LP1 ; both are better than Local ( IITB data ) .
100
More directly useful is Figure 12 , which compares F1 scores of Hill1 and LP1 ( rounded ) . They are very close , but Hill1 is slightly better at high recall levels of our interest . Both Hill1 and LP1 are robust to ρna , whereas Local suffers if ρna is chosen poorly . Hill1 and LP1 scale mildly quadratically wrt |S0| , as shown in Figure 13 . For most documents , Hill1 takes about 2–3 seconds and LP1 takes around 4–6 seconds , much of which is fixed overhead .
10
8
6
4
2 s
, e m T i
Hill1
LP1
0 # of spots 0
50
100
150
200
250
Figure 13 : Scalability of Hill1 and LP1 , IITB data .
4.7 Comparing Local , Hill1 , LP1
Having established that Local alone can significantly improve upon prior work , we investigate whether collective inference gives additional accuracy gains compared to Local . In Figures 14 and 15 we plot precision against recall for the Local , Hill1 , and LP1 , for our two data sets .
In case of the IITB data set , we see that collective inference has distinct precision advantage ( almost 9% ) , especially as we push recall aggressively beyond 70–75 % . Summarized below are F1 scores obtained by 2 fold cross validation of ρna : no Prior +Prior
Local Hill1 63.45 % 64.87 % 67.02 % 68.75 % 67.46 % 69.69 %
LP1
From Figure 6 , we see that the CZ data is much smaller , sparse in ground truth annotations , but has more potential ambiguity . Here LP1 led to more fractional solutions and overall worse accuracy than Hill1 , which still beat M&W ’s F1 = 63 % with our score of 69 % , although M&W attained larger precision than us at lower recall .
F1=63 %
Local Hill1 LP1 Milne Cucerzan
F1=69 %
% i
, n o s c e r i
P
80
60
40
20
0 Recall , % 20
30
40
50
60
70
Figure 15 : Recall/precision on Cucerzan ’s data .
5 . MULTI CLUSTER MODELS
∗
S
Our clique potentials ( expressions 1 , 3 , 4 ) implicitly encourage a single cluster model , because the clique potentials are largest when all g(γ ) are close to each other . Let ∗ s be the entity labels used on the optimal assign= Γ in g(·) space will ment . show one giant component cluster ? s y Is it true that a clustering of Γ
∗
∗
Figure 16 shows a dendrogram formed by agglomeratively , the ground truth entities on a page . The clustering Γ “ single cluster hypothesis ” is only somewhat true . There is a cluster corresponding to the broad topic of the page , but this typically covers fewer than a third of the spots . The rest belong to smaller clusters , or are singletons , in which case they bear no collective information .
P
´ `|S0|
Might our implicitly single cluster model losing out on recall by not modeling and covering multiple clusters ? Here is how this could happen . Say Hill1 is considering changing ys from na to γ , where s is a member of a non dominant cluss.=s r(ys . , γ ) may be too small ter . Therefore , ( 1/ to overcome the ρna barrier . The large denominator could share the blame . The end result is that a small but tight cluster of such spots cannot “ secede ” from the dominant cluster .
`|S0| ´
)
2
2
Accordingly , we retained the node potential ( NP ) , but demanded that an inference algorithm produce not only a of the label vector y but also a partitioning C = Γ labels used . We modified the clique potential to
, . . . , Γ
K
1
Γk∈C s,s . : ys,ys.∈Γk r(ys , ys . ) .
( CPK )
X
1|C|
1`
Γk 2
´ X ´ `
Γk 2
`
´
S0 2
, this objective By using denominator rewards smaller coherent clusters as desired , but it is no longer possible to express a simple linear objective as in instead of
464 based on categories . This involves extending the training process from NP to the whole objective . We are also investigating why the multi cluster extensions of our model obtained no significant accuracy gains . Finally , we are considering collective decisions beyond page boundaries . 7 . REFERENCES [ 1 ] E . Agirre and G . Rigau . Word sense disambiguation using conceptual density . In Computational Linguistics , pages 16–22 . Association for Computational Linguistics , 1996 .
[ 2 ] S . Agrawal , K . Chakrabarti , S . Chaudhuri , V . Ganti ,
A . C . K¨onig , and D . Xin . Exploiting web search engines to search structured databases . In WWW Conference , pages 501–510 , 2009 .
[ 3 ] R . Bunescu and M . Pasca . Using encyclopedic knowledge for named entity disambiguation . In EACL , pages 9–16 , 2006 .
[ 4 ] S . Cucerzan . Large scale named entity disambiguation based on Wikipedia data . In EMNLP Conference , pages 708–716 , 2007 .
[ 5 ] S . Dill et al . SemTag and Seeker : Bootstrapping the semantic Web via automated semantic annotation . In WWW Conference , 2003 .
[ 6 ] U . Feige , D . Peleg , and G . Kortsaz . The dense k subgraph problem . Algorithmica , 29(3):410–421 , Dec . 2001 .
[ 7 ] M . Garey and D . Johnson . Computers and intractability : A guide to the theory of NP completeness , 1979 .
[ 8 ] R . V . Guha and R . McCool . TAP : A semantic web test bed . Journal of Web Semantics , 1(1):81–87 , 2003 .
[ 9 ] M . I . Jordan , editor . Learning in Graphical Models .
MIT Press , 1999 .
[ 10 ] S . Khot . Ruling out PTAS for graph min bisection , densest subgraph and bipartite clique . In FOCS Conference , pages 136–145 , 2004 .
[ 11 ] R . Larson . Bibliometrics of the world wide web : An exploratory analysis of the intellectual structure of cyberspace . In Annual Meeting of the American Society for Information Science , 1996 . Online at http : //sherlockberkeleyedu/asis96/asis96html
[ 12 ] D . B . Lenat . Cyc : A large scale investment in knowledge infrastructure . Communications of the ACM , 38(11):32–38 , nov 1995 . Also see http://wwwcyccom/ and http://wwwopencycorg/
[ 13 ] R . Mihalcea and A . Csomai . Wikify! : linking documents to encyclopedic knowledge . In CIKM , pages 233–242 , 2007 .
[ 14 ] G . Miller , R . Beckwith , C . FellBaum , D . Gross ,
K . Miller , and R . Tengi . Five papers on WordNet . Princeton University , Aug . 1993 .
[ 15 ] D . Milne and I . H . Witten . Learning to link with
Wikipedia . In CIKM , 2008 .
[ 16 ] V . Nagarajan and M . Sviridenko . On the maximum quadratic assignment problem . In SODA , pages 516–524 . Society for Industrial and Applied Mathematics , 2009 .
[ 17 ] S . Sarawagi . Information extraction . FnT Databases ,
1(3 ) , 2008 .
Figure 16 : Hierarchical clustering of Γ embedding shows more than one clusters . using the g
∗
. ( CP1
) , because |Γ k| themselves depend on y and C .
.
We extended the LP1 framework to optimize ( NP)+(CPK ) approximately . In experiments , ( CPK ) did not perform significantly better than LP1 , giving less than 0.5 % F1 boost . We conjecture that this is because of the extreme sparsity ) were of r(γ , γ used as edges in a graph , the graph is easy to partition , and LP1 finds it easy to make correct decisions within each partition , even if the LP1 objective tries to account for crosscluster edges . However , this may change with denser sources of relatedness information .
) , which had only 5 % fill . Basically , if r(γ , γ
.
6 . CONCLUSION AND OUTLOOK
We proposed new models and algorithms for a highly motivated problem : annotating unstructured ( Web ) text with entity IDs from an entity catalog ( Wikipedia ) . Unlike prior work that is biased toward specific entity types like persons and places , with low recall and high precision , our intention is aggressive , high recall open domain annotation for indexing and mining tasks downstream .
Our main contribution is a formulation that captures a tradeoff between local spot to label compatibility and a global , document level topical coherence between entity labels . Inference in this model is intractable in theory , but we show that LP relaxations often give optimal integral solutions or achieve close to the optimal objective . We also give a simple local hill climbing algorithm that is comparable in speed and quality to LP relaxation . Both these algorithms are significantly better than two recently proposed annotation algorithms .
In continuing work , we are trying to cast the annotation problem as special cases of quadratic assignment that can be approximated well [ 6 , 16 ] or show that even approximation is difficult [ 10 , 16 ] . We are trying to combine the generally ) based on low recall , high precision nature of M&W ’s r(γ , γ . ) inlinks with the converse properties of Cucerzan ’s r(γ , γ
.
465
