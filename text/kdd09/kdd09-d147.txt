Spatial temporal Causal Modeling for
Climate Change Attribution
A . Lozano , H . Li , A . Niculescu Mizil , Y . Liu , C . Perlich , J . Hosking , and N . Abe
{aclozano , liho , anicule , liuya , perlich , hosking , nabe}@usibmcom
IBM T . J . Watson Research Center
Yorktown Heights , NY 10598
ABSTRACT Attribution of climate change to causal factors has been based predominantly on simulations using physical climate models , which have inherent limitations in describing such a complex and chaotic system . We propose an alternative , data centric , approach that relies on actual measurements of climate observations and human and natural forcing factors . Specifically , we develop a novel method to infer causality from spatial temporal data , as well as a procedure to incorporate extreme value modeling into our method in order to address the attribution of extreme climate events , such as heatwaves . Our experimental results on a real world dataset indicate that changes in temperature are not solely accounted for by solar radiance , but attributed more significantly to CO2 and other greenhouse gases . Combined with extreme value modeling , we also show that there has been a significant increase in the intensity of extreme temperatures , and that such changes in extreme temperature are also attributable to greenhouse gases . These preliminary results suggest that our approach can offer a useful alternative to the simulation based approach to climate modeling and attribution , and provide valuable insights from a fresh perspective .
Categories and Subject Descriptors I26 [ Artificial Intelligence ] : Learning
General Terms Algorithms
Keywords Climate modeling , Climate change attribution , Spatio temporal modeling , Causal modeling Granger causality
1 .
INTRODUCTION
Climate change is one of the most critical socio technological issues mankind faces in the present century [ 1 ] . Though it is regarded primarily as an energy related problem , computing technology will play an important role in devising potential solutions in a variety of ways . One that particularly interests us is that of applying data modeling to the climate data in order to better understand and quantify the causal effects of various parameters involved . There is a clear need for an effective methodology of data modeling that will allow us to analyze the large amount of time series data on the climate and climate forcing agents and draw conclusions on how these factors affect each other and which parameters are to be controled for the best environmental results .
It is well recognized that climate is a chaotic system , and hence it is difficult to reliably model it as a whole . Nonetheless , there are reasons to believe we can meaningfully characterize causal or statistical relationships that exist among parameters of interest , and make assertions about the presence or absence of such relationships and quantify them . ( Recently , there have been a number of articles published in prominent scientific journals that carry out studies of this type . [ 11 , 15 , 5 ] ) Fundamentally , our goal is to focus on ‘climate change detection and attribution’ ( ie , identification , quantification and prioritization of the effects of controllable forcing factors on climate ) , rather than on ‘climate projection’ ( ie , prediction of the evolution of the global climate system in the next decades ) . The climate system comprises complex relationships between a large number of variables . Hence , the factors of interest involve many dimensions , including measurements of climate parameters , anthropogenic factors , and regional factors [ 2 ] . Fortunately , many of these data are publicly available in forms that are well suited for data modeling – eg Climate Research Unit ( CRU ) dataset , NOAA NESDIS data set , Carbon Dioxide Info analysis Center ( CDIAC ) . Considerable amount of scientific investigations have been carried out to date in the community of climate change study , to address these very questions [ 11 ] . The dominant existing approach in the community , however , is based on forward simulation with climate models built using fundamental physical laws . These models are used to estimate the expected space time pattern ( fingerprints ) of the response to individual anthropogenic or natural forcing factors on the observed climate . The task of detection and attribution is then performed by estimating the factors by which these modelsimulated patterns have to be scaled to be consistent with the observed change ( optimal fingerprinting ) , and by applying standard statistical significance tests for isolated hypotheses on the value of the estimated factors . As these existing approaches rely heavily on the employed climate models , they are subject to the models’ shortcomings ( eg models’ uncertainties , simplifications , and discrepancies from observed data ) .
Given the understanding of the existing approaches and their limitations , what we propose is an alternative approach based on data modeling , with special attention paid to address unique characteristics of climate modeling . First has to do with our emphasis on attribution , rather than forecasting , of climate change , motivating us to look to techniques that aim at modeling causality . Secondly , the climate data are spatio temporal in nature , where both the climate observations and forcings are associated with specific points in space and time , and these aspects will be critical for conducting informed analyses on climate change over time and across regions over the globe . Thirdly , there is a particular interest in modeling the extreme climate events , such as the frequency and severity of heatwaves and floods , beyond just the change in the mean climate behavior [ 15 , 5 ] .
To address the modeling challenge described above , we develop and employ methods of ‘spatial temporal causal modeling,’ which allow us to model causal relationships between time and spacepersistent features , given spatio temporal data . More specifically , we develop a spatio temporal version of the so called ‘graphical Granger modeling methods,’ which is an emerging collection of methods that combine the graphical modeling techniques with the notion of ’Granger causality’ to derive effective methods for causal modeling based on time series data . Here ‘Granger causality’ is an operational definition of causality from econometrics , which is based on the premise that ‘a cause necessarily precedes its effects,’ and its adoption to graphical modeling allows us , to model causal relationships between a large number of time series variables .
Specifically , we develop a novel method we call “ Group Elastic Net ” , which can address the spatio temporal aspect of climate modeling , and use it as our primary modeling methodology . This algorithm incorporates the spatio temporal structure in the data in the variable selection process of the regression procedure underlying graphical Granger modeling . That is , the lagged variables from different time steps for the same feature are grouped together and the penalty function used in variable selection is modified so as to enforce sparsity at the group level , rather than at the level of the individual lagged variables . Additionally , the spatial smoothness is enforced by an additional penalty term that encourages similarity between coefficients for spatial neighbors . This formulation leads to a grouped version of the so called “ elastic net ” problem , for which we devise an efficient solution .
One potential weakness of a data centric approach to climate modeling is the lack of sufficient past data on extreme events , which may pose difficulties in modeling and attributing such events . Here we develop a dynamic modeling method by applying the theory of extreme event and value modeling . Extreme value theory [ 3 ] provides a natural family of probability distributions for modeling the magnitude of the largest or smallest of a large number of events , and a canonical stochastic process model ( [7 ] , sec . 7.3 ) for the occurrence of rare events , those whose magnitude exceeds a very high ( or very low ) threshold . The stochastic process model involves three parameters , which specify the rate of occurrence of extreme events and the distribution of the magnitude of events that exceed a threshold . We treat these parameters as varying over space and time and we model their variation by means of a Bayesian hierarchical model in which the parameters are regarded as random variables . The outputs of the model are a posteriori estimates of the parameters at potentially all locations in space and time . From these outputs we can estimate the spatial and temporal variation of properties of the distribution of annual extremes . In particular we look for evidence of climate change in the temporal variation of our estimates of the “ N year event ” , the event magnitude that occurs on average once every N years .
The relationship between extreme event modeling and graphical Granger modeling has been underexplored in the literature to date . In the present work , we employ a relatively simple approach to combining the two : using our Bayesian hierarchical model we estimate the N year event magnitudes associated with the climate metrics of interest , and we incorporate these estimated variables as additional variables in causal modeling and attribution in the spatiotemporal modeling with the grouped elastic net algorithm described above . The choice of N year event magnitudes as a proxy of extreme temperature is , in part , motivated by the fact that they are typically approximated using normal distributions , which is consistent with our causal modeling method , using linear Gaussian models as component models of conditional distributions .
We evaluate our proposed approach with two sets of experiments : In the first set of experiments , we use simulated spatio temporal data to demonstrate the advantage of the proposed spatial temporal modeling method based on group elastic net , as compared to methods that do not take advantage of the spatial aspects of the data . In the second , and main , set of experiments , we use our developed methods to model real climate data , focusing on the data for the last couple of decades in the North American region . We collected and processed a wide range of climate related data for these space and time ranges , including the climatological observations , natural forcings ( eg solar radiance ) , as well as greenhouse gas measurements . The results we obtained to date include : 1 ) Spatio temporal causal modeling attributes the change in the temperature significantly to that of CO2 and other greenhouse gases , even in the presence of solar radiance ; 2 ) Extreme value modeling confirms that the intensity of extreme weather events , such as unseasonably hot summer days and warm winter days , have significantly increased between the years of 1982 and 2001 ; 3 ) The combination of the two approaches indicate that , for the N year return level of temperature as well , CO2 and other greenhouse gases are attributed even in the presence of , and with greater significance than , the solar radiance .
2 . METHODOLOGY 2.1 Spatio temporal Causal Modeling 211 Preliminaries : Graphical Granger Modeling We briefly review the notion of “ Granger Causality ” [ 9 ] , which was introduced by the Nobel prize winning economist , Clive Granger , and has proven useful as an operational notion of causality for time series analysis in econometrics . It is based on the idea that if a time series variable causally affects another , then the past values of the former should be helpful in predicting the future values of the latter , beyond what can be predicted based only on their own past values . More specifically , a time series x is said to “ Granger cause ” another time series y , if regressing for y in terms of past values of y and x is more accurate with statistical significance , as compared to regressing just with past values of y . Let {xt}T t=1 denote the time t=1 the same for y . The so called series variables for x and {yt}T Granger test first performs the following two regressions : yt ≈
L!l=1 aj · yt−l +
L!l=1 bj · xt−l
L!j=1
( 1 )
( 2 ) yt ≈ aj · yt−j where L is the maximum “ lag ” allowed in past observations , and then applies a statistical test to determine whether or not ( 1 ) is more accurate than ( 2 ) , with statistical significance .
The notion of Granger causality , as reviewed above , was defined for a pair of time series variables . Now in the context of climate modeling , we are actually interested in cases in which there are many variables present as opposed to a pair , and each one is a spatio temporal variable as opposed to a time series variable ; and we wish to determine the causal relationships between them . Hence , the notion of Granger causality needs to be appropriately extended to incorporate the spatial dimension . Let us , for any measurement or feature over time and space ( eg temperature , CO2 , etc ) , use variables ( eg x ) to refer to the entire spatio temporal series , and use indexed variables ( eg xt,s ) to denote the associated individual spatially and temporally lagged variables .
For convenience , we assume that the measurements are sampled along a regular spatial grid . Similarly to the notion of maximum temporal lag , one may consider a maximum “ spatial lag ” and suppose that each point is influenced by a finite neighborhood around it . Let N ( s ) denote the set of points in the neighborhood of s . We assume that the neighborhood structure is identical for each grid point , and thus consider neighborhoods of the form N ( s ) = s +Ω , where Ω= {ω1 , . . . , ωK} is a set of “ relative locations ” . of the following two regressions :
Now , the extended Granger causality notion is defined in terms
Spatial Temporal Causal Modeling
1 . Input : Measurement data {xt,s}t=1,,T , s∈S where each xt,s is a N dimensional vector of measurements taken at time t and location s . Input : A regression method with group variable selection , REG .
2 . Initialize the adjacency matrix for the N measurements , ie G =
!V , E" where V is the set of N measurements ( eg by all 0 ’s ) .
3 . For each measurement xi ∈ V , run REG on regressing for xi in terms of the past lagged variables , xj t−l,s+ω , j ∈ 1 , . . . , N , l ∈ 1 , . . . , L , ω ∈ Ω . For each measurement xj ∈ V place an edge xj → xi into E , if and only if xj was selected as a group by REG . t,s
Figure 1 : Generic Spatial Temporal Causal Modeling Method
212
Spatio temporal Granger Modeling via Group Elastic Net yt,s ≈ !ω∈Ω
L!l=1 al,ω · yt−l,s+ω +!ω∈Ω yt,s ≈ !ω∈Ω
L!l=1
L!l=1 bl,ω · xt−l,s+ω
( 3 ) al,ω · yt−j,s+ω
( 4 )
The above , simplified , scheme is symmetric with respect to time and space , but there is a difference between space and time that calls for a refinement of this formulation .
For applying Granger causality to many variables , or measurements , there is a collection of methods , known as graphical Granger modeling , which combines methods of graphical modeling with the notion of Granger causality . A particularly relevant approach is that of applying regression algorithms with variable selection to determine the causal links for each variable . Lasso [ 14 ] is a prime example , which trades off the minimization of the sum of squared errors and that of the sum of the absolute values of the regression coefficients in the penalty term . t−l,s+ω , . . . ,xN
Consider N measurements xi ( i = 1 , . . . , N ) ( eg temperature , pressure , etc ) For each such measurement xi , denote by xi t,s its sample at time t and location s . For any given measurement xi , one can view the variable selection process in the regression for t,s in terms of x1 t−l,s+ω l = ( 1 , . . . , L),ω ∈ Ω , as xi an application of the Granger test on xi against x1 , . . . , xN . By extending the pairwise Granger test to one involving an arbitrary number of spatial temporal series , it makes sense to say that xj Granger causes xi if xi t−l,s+ω is selected for any time and spatial lags l , ω in the above variable selection process . A critical aspect that is worth emphasizing , and is overlooked in most of the existing methods in the literature , is that the question we are interested in is whether an entire series {xj t−l,s+ω , l ∈ {1 , . . . , L},ω ∈ Ω} provides additional information for the prediction of xi t,s , and not whether for specific time and spatial lags l , ω xj t−l,s+ω provides additional information for predicting xi t,s . Therefore , a faithful instantiation of Granger causal modeling , in the context of spatial temporal modeling , should take into account the group structure imposed by the spatial temporal series into the fitting criterion that is used in the variable selection process .
The foregoing discussions naturally lead to the proposal of our novel method , “ group elastic net ” , which addresses both the issue of “ grouping ” the lagged variables for the same feature , and that of the smoothness desired for the spatial dimension .
The generic spatio temporal causal modeling method we described in the foregoing section is given in Figure 1 . We now describe the variable selection procedure which we propose to use as an instance of the REG procedure in Step 3 of our algorithm . We assume “ spatial stationarity ” , ie , that the same model applies to each point on the grid ( relaxing this assumption will be the object of future work ) . More precisely , we consider regression coefficients of the form βk l,ω where k is the measurement ( eg temperature ) , l is the time lag , ω is the relative location between the point considered and a point in its neighborhood .
Let S be the set of ( interior ) locations s such that for each ω ∈ Ω s + ω is a point of the grid ( and not outside the grid ) . Let , t = 1 , . . . , T be the time points considered .
For a given measurement xi , we propose to use the following penalized regression model to determine which spatial temporal series xj ( j = 1 , . . . , N ) Granger cause xi .
ˆβ = arg min
T!t=L+1
( xi t,s −
N!j=1
βj l,ωxj t−l,s+ω)2
+λ2
( βj l , : )T ˜∆jβj l , :
+λ1
,
( 5 )
: $∆j
SpatialPenalty
SparsityPenalty where βj l , : = vect(βj l,ω)ω∈Ω,β j
: = vect(βj
β !s∈S N!j=1 L!l=1 " ∆j =
#$
˜∆j 0
0 ˜∆j
L!l=1!ω∈Ω N!j=1 $βj " #$ 
0 ˜∆j
,
%
0 0 0
% l,ω)l=(1,,L),ω∈Ω , 0 0
0 0
0 0 and $y$∆j = ( yT ∆jy)1/2 . The role of the “ spatial penalty ” is to enforce spatial regularization . Specifically the matrix ˜∆j is meant to enforce spatial smoothness as well as some form of distance based coefficient decay . Namely the regression coefficients are penalized more as they correspond to increasingly distant neighborhood locations . For instance , ˜∆j could be a diagonal matrix such that the diagonal entry corresponding to βj
The “ sparsity penalty ” is a group Lasso penalty [ 16 ] , which imposes sparsity across measurements . More precisely , the regression coefficients corresponding to spatial temporal samples of the same l,ω equals $ω$ .
: $∆j ,$β2
: $∆j , . . . ,$βN
: $∆j imposes that measurement are penalized as a group , namely through $βj : $∆j . Then l1 norm of,$β1 the coefficients corresponding to a given measurement are either included as a group in the model or excluded . Note that the dependence in j of ˜∆j and ∆j is due to the fact that we may consider different regularization matrices for different measurements . Let Y be the vector of length ( T −L+1)|S| formed by xi t,s , t = ( L , . . . , T ) , s ∈ S . Consider the spatially and temporally lagged matrix X of dimension ( (T − L + 1)|S| ) × ( N L|Ω| ) such that the row corresponding to the pair ( t , s ) is the vector formed by xj t−l,s+ω , j = ( 1 , . . . , N ) , l = ( 1 , . . . , L),ω ∈ Ω . Let β be the corresponding vector of regression coefficients , ie β is of length length N L|Ω| formed by βj l,ω , j = ( 1 , . . . , N ) , l = ( 1 , . . . , L),ω ∈ Ω . Denote by βj the restriction of β to the elements corresponding to measurement xj , i.e βj is the vector formed by βj l,ω , l = ( 1 , . . . , L),ω ∈ Ω . Then Eq 5 can be rewritten as ˆβ = arg min β
L(λ1,λ 2,β )
N!j=1
N!j=1
∆j + λ1
$βj$2
= arg min
β $Y − Xβ$2 + λ2
$βj$∆j . Notice that the above formulation resemble a “ group version ” of the Elastic net problem [ 17 ] , hence we call it the Group Elastic Net . The following proposition states that the Group Elastic Net problem can be transformed into a group Lasso problem , and hence can be efficiently solved using existing algorithms .
PROPOSITION 21 Assume that ∆j ( j = 1 , . . . , N ) is positive definite , let ∆j = ST j Sj . Let Aj = ( ST j Sj)−1ST j , and
C =
A1 0 0 A2
0 0
0 0
0 0
0 0 0 0 AN
.

The Group Elastic Net problem solution
βGEN = arg min
β $Y − Xβ$2 + λ2
N!j=1
$βj$2
∆j + λ1 can be obtained by solving the Group Lasso problem
N!j=1
$βj$∆j ( 6 )
βGL = arg min
N!j=1 β $ ˆY − ˆXβ$2 + γ $βj$2 , 1√1+λ2 . XC√λ2I / 0(p ) / , ˆX = , ˆY = . Y where γ = λ1√1+λ2 and where p is the number of columns of X , and setting βj 1√1+λ2 j ST
∆−1 j βj
GL .
GEN =
PROOF . Let ˜X = XC and ˜βj = Sjβj . Then solving Eq 6 is equivalent to solving
N!j=1 $ ˜βj$2
N!j=1 min$Y − ˜X ˜β$2 + λ2
N!j=1
$ ˜βj$2
2 + λ1
$ ˜βj$2
= min$Y − ˜X ˜β$2 + λ2$ ˜β$2 + λ1
( 7 )
Set γ = λ1√1+λ2 equivalent to solving min$Y −
, and ˆβ = 0(1 + λ2 ) ˜β Then solving Eq 7 is $ ˆβj$2 ( 8 ) 0(1 + λ2 )
λ2 1 + λ2 $ ˆβ$2 + γ
˜X ˆβ$2 +
N!j=1
1
Let p = N L|Ω| , i.e , p is the number of columns of ˆX . Let q = ( T −L+1)|S| , i.e , q is the length of Y and also the number of rows ˜X√λ2I / . of ˜X . Let ˆY(q+p ) = . Y
0(p ) / , and ˆX(q+p)×p = 1√1+λ2 .
Then the problem is equivalent to solving which is the Group Lasso formulation . min$ ˆY − ˆX ˆβ$2 + γ1J j=1 $ ˆβj$2
Similar to [ 17 ] , the penalty parameters are tuned as follows . We consider a set of candidate parameters Λ2 for λ2 , ( for instance Λ2 = ( 0 , 0.01 , 0.1 , 1 , 10 , 100) ) . For each λ2 ∈ Λ2 we run the equivalent Group Lasso algorithm for γ ∈ Γ , where Γ is a set of candidate parameters for γ ( eg Γ = ( 0 , 0.01γmax , 0.1γmax,γ max ) , where γmax is a value which is so high that no group gets selected . ) Then we pick the pair ( λ∗2,γ ∗ ) = arg min BIC(λ2,γ ) , where BIC(λ , γ ) = $ ˆY − ˆXβGL(λ2,γ )$2
+ ( log(n)/n)dfGL(λ2,γ ) , ( 9 ) where df GL is the degrees of freedom estimate for Group Lasso as proposed by [ 16 ] , ie , nσ2 dfGL(λ , γ ) ≈
N!j=1
I($ ˆβj$2 > 0 ) +
N!j=1
$ ˆβj$ $ ˆβj OLS$
, where ˆβ = ˆβGL(λ , γ ) and ˆβOLS is the ordinary least squares solution when using all the variables . 2.2 Extreme Value Modeling 221 Preliminaries : Extreme Value Modeling We now give a brief review of extreme value theory [ 7 ] . We will show that a natural statistical model for the occurrence of extreme events is a Poisson point process that yields a generalized extreme value ( GEV ) distribution for the magnitude of the largest event in a fixed time period and a generalized Pareto distribution ( GPD ) for the amounts by which the magnitudes of extreme events exceed a specified threshold .
Let X1,··· , Xn be a sequence of independent and identically distributed random variables , and let Mn = max{X1,··· , Xn} . If there exist sequences of constants an > 0 and bn such that
Pr . Mn − bn an
/ → G(z ) as n → ∞ ,
( 10 ) for some nondegenerate distribution function G , then G is a generalized extreme value distribution , with distribution function
G(z ) = exp2−31 + ξ4 z − µ
+ 7 , σ 56−1/ξ
( 11 ) defined on {z : 1+ξ(z−µ)/σ > 0} , with −∞ < µ < ∞ , σ> 0 , and −∞ < ξ < ∞ . If the limiting distribution ( 11 ) exists , then , for a large threshold u , the exceedance Y = X − u , conditional on X > u , is well approximated by a generalized Pareto distribution H(y ) = Pr ( X > u + y|X > u ) =.1 + defined on {y : y > 0 and ( 1 + ξy/˜σ ) > 0} , where
˜σ/−1/ξ
( 12 )
ξy
,
˜σ = σ + ξ(u − µ ) .
The parameters of the generalized Pareto distribution of threshold excesses are uniquely determined by those of the associated GEV distribution of block maxima .
These results provide two approaches for statistical modeling of extreme values . The block maxima ( eg annual maxima of meteorological variables ) can be modeled as independent observations from a GEV distribution , or the excesses over a high threshold can be modeled by a GPD . Both approaches have weaknesses . The GEV approach uses only one observation per block , which may be wasteful if more data than just the block maxima are available . In the GPD approach , the probability of exceeding the threshold is not available . These weaknesses can be overcome by formulating the behaviour of extreme events using a Poisson point process . This encompasses the GEV and GPD models and the process is completely defined by the same parameters that describe GEV distribution of block maxima . The model leads directly to a likelihood that enables a natural formulation of nonstationarity in threshold excesses , for example by including spatio temporal correlation .
A point process on a set A is a stochastic rule that describes the occurrence and position of point events . For a set A ⊂A , we define the non negative integer valued random variable N ( A ) to be the number of points in the set A . In a Poisson process the occurrence of events at different points a ∈A is statistically independent and N ( A ) has a Poisson distribution ,
N ( A ) ∼ Poi(Λ(A ) ) ,
Λ(A ) =8A
λ(a ) da ,
( 13 )
( 14 ) with where the intensity function λ(a ) , a ∈A , indicates the relative frequency of occurrence of events at different locations in A . In extreme value modeling the set A has the form ( −∞ , +∞ ) × [ u,∞ ) , the two components respectively indicating time and event magnitude . The intensity function is
λ(t , x ) = σ−131 + ξ x − µ
σ 6−1/ξ−1
,
( 15 ) which yields distributions of block maxima and of excesses over threshold u that have the forms ( 11 ) and ( 12 ) respectively .
Spatio Temporal Point Process
222 Since the statistical characteristics of extreme climate data vary over space and time , the model specified by ( 13)–(15 ) cannot be used directly . We have therefore developed a more general version of the model , a spatio temporal point process in which the location parameter µ and scale parameter σ are permitted to vary over space and time , and the threshold u varies over space .
To incorporate spatial and temporal correlation among the data , we build a hierarchical Bayesian spatio temporal dynamic model [ 4 ] . This modeling strategy involves three stages . The first stage is the data model which models only observation process given a latent process . Stage 2 specifies the latent process ; in our case , this is a Poisson point process and incorporates spatio temporal dependence structures that are much more complicated than could be specified directly . In stage 3 we specify prior distributions for the parameters occurring in stage 2 ; here we can include external knowledge and expert opinion .
Let X i s,t be the ith exceedance over threshold us at location s in year t , where i = 1 , . . . , ns,t , s = 1 , . . . , S and t = 1 , . . . , T . In the observation process , the likelihood function of the Poisson point process can be written as
L(µs,t,σ s,t,ξ ; X 1 s,t , . . . , X ns,t s,t
, s = 1 , . . . , S , t = 1 , . . . , T )
σs,t /<−1/ξ= exp:−;1 + ξ . us − µs,t S9s=1 σ−1;1 + ξ . xi /<−1/ξ−1 s,t − µs,t σs,t
,
∝
×
T9t=1 N9i=1
( 16 ) where µs,t and σs,t are varied over space and time .
In the process model , we model the location parameter µs,t through s θµ
µt = Bµ a dynamic linear model and σs,t is modeled in the same procedure : ( 17 ) where µt = ( µ1,t , . . . , µS,t)& at time t . θµ t , a K × 1 vector , is s in ( 17 ) , is a S × K matrix which can recalled state vector . Bµ duce the spatial dimension from S to K ( K < S ) . We choose Bs as a Matern kernel with fixed smoothness parameter [ 12 ] . t is a random Gaussian process to include systematic error . t + µ t ,
( 18 ) where Γµ in the transition equation is specified through an AR(1 ) process . t−1 + ωt ,
θµ t =Γ µθµ t and ωµ
In stage 3 , we assign noninformative priors to all the parameters . Given the data model ( 16 ) , the process model ( 17 ) and ( 18 ) , and the prior process , we can derive the posterior distribution . Markov Chain Monte Carlo ( MCMC ) algorithm can be used to draw sample from the full conditional distributions . The full conditional distributions of the variance parameters which characterize the random process µ t are inverse gamma distributions and can be drawn through Gibbs sampler . Some full conditional distributions of the parameters , such as µt and the temporal correlation parameters in Γµ , are hard to sample directly , and hence MetropolisHasting algorithm is used . θt are jointly sampled by forward filtering backward sampling ( FFBS ) algorithm [ 6 ] . After obtaining the MCMC samples , we can make inferences for the parameters in the model . We drew 15,000 samples and discarded the first 5,000 . The chains were thinned by choosing every 10th samples to reduce the correlation . So 1,000 samples for each chain were left for analysis . Convergence was checked on trace plots of posterior samples .
It is usually more convenient to interpret extreme value models in terms of return levels , rather than individual parameter values . Let zm be the return level associated with the return period m years ; zm is the level exceeded by the annual maximum in any particular year with probability 1/m . Statistically , the return level is the 1/m upper quantile of generalized extreme distribution . Let n be the number of observations in a year , and zm satisfies the equation n log p = log(1 − 1/m ) , where p = 1 − n−1 [ 1 + ξ(zm − µs,t)/σs,t]−1/ξi , if [ 1 + ξ(zm − µs,t)/σs,t ] > 0 , otherwise p = 1 . Here µs,t,σ s,t , and ξ are the parameters of the point process for year t and location s . This equation can be solved for zm using standard methods .
3 . DATA
The mere amount of publicly available climate data is outright staggering . There are a large number of governmental and scientific institutions who publish measurements for a given geographical range on a multitude of relevant variables on the Web . This being said , it is nevertheless a major challenge to obtain consistent
270 274 269 265 259 253 246 242 248 252 261 268 270 274 269 265 259 253 246 242 248 252 261 268 270 274 269 265 259 253 246 242 248 252 261 268 270 274 269 265 259 253 246 242 248 252 261 268
Latitude LongitudeAltitude Value
CRU climate data
Latitude LongitudeAltitude Value
Local Spline Interpolation
0 0.4 0.4 2
1.9 999.9 44.6* 21.2* 4.1 999.9 50.0* 35.6* 1.9 999.9 48.2* 35.6* 16.9 50.0* 35.6*
10 24 12 24 Solar Irradiation
8
Latitude LongitudeAltitude Value
ALT 1985 06 10 18 20 387 85 P co2 349.220 X U3 1985 08 01 11 36 82.45 62.52 210.00 825 ALT 1985 06 10 18 20 388 85 P co2 348.920 X U3 1985 08 01 11 38 82.45 62.52 210.00 826 ALT 1985 06 17 19 27 389 85 P co2 350.590 U3 1985 08 01 11 08 82.45 62.52 210.00 827 ALT 1985 06 17 19 27 390 85 P co2 350.320 U3 1985 08 01 11 10 82.45 62.52 210.00 828
Joined Data
Latitude LongitudeAltitude Value
De Seasonalization
Latitude Longitude Altitude Date Var1 Var2
Aerosoles/Greenhouse Gases
Raw Data
Data in Normal Form
Spatial temporal features data
Figure 2 : Data collection and pre processing longitudinal records that cover with comparable temporal and spacial resolution all relevant variables . Another problem is the large variety of formats in which data are available . 3.1 Data Sources and Collection
We compiled a comprehensive set of relevant variables for climate modeling in North America . Aside from the primary climate variables that we eventually wish to explain , the literature distinguishes human and natural agents or forcings that are known to affect the climate . These include solar irradiance and volcanic activities , greenhouse gases and aerosols ( small particles dispersed in air ) . Figure 2 shows a schematic view of the data collection and preparation process . Table 1 lists the variables that we used in our analysis . We note that the “ temperature extreme ” variable is to be distinguished from all the others , in that they are estimated , using the extreme value modeling technique described in the previous section . We used for this study data from the following 5 sources : 1 ) CRU : Climate Research Unit provides monthly climatology data at http://wwwcruueaacuk/cru/data for 11 surface variables including precipitation , wet day frequency , mean , max , min temperature , vapor pressure , relative humidity , sunshine percent , cloud cover , frost frequency , wind speed from 1901 to present on a 0.5 degree latitude and longitude resolution . This grid data was interpolated from station data as a function of latitude , longitude , and elevation using thin plate splines by New et al.[13 ]
2 ) NOAA : The data center http://wwwcdcnoaagov/data/gridded/ of the National Oceanic and Atmospheric Administration is considered the ” World ’s largest archive of climate data ” . We downloaded the greenhouse data from 170 worldwide stations from http:// wwwesrlnoaagov/gmd/dv/ftpdatahtml
3 ) NASA : NASA uses satellite images to estimates of the ambient aerosol optical thickness based on the resulting ultra violet irradiation . We collected this data from http://iridlldeocolumbiaedu/ SOURCES/.NASA/ GSFC/TOMS/NIMBUS7/
4 ) NCDC : The National Climate Data Center was our source for the different solar radiation measurements in 997 different locations at http://rredcnrelgov/solar/old_data/nsrdb/
5 ) CDIAC : Daily temperature data are obtained from US historical climatology network ( http://cdiacornlgov/epubs/ndp/ushcn/ usahtml ) The data of daily maximum temperature were collected from year 1948 to 2005 at 351 stations in US We cleaned the data by removing invalid temperature observations . 3.2 Data Pre Processing
The preparation of the data for the modeling involved a number of steps :
1 ) Normalization : Initially we transformed each dataset into monthly observations in a standard format including longitude , latitude , altitude , date , variable , value , unit , and source .
Variables ( Variable group ) Methane ( CH4 ) Carbon Dioxide ( CO2 ) Hydrogen ( H2 ) Carbon Monoxide ( CO ) UV ( AER ) Temperature ( TMP ) Temp Range ( TMP ) Temp Min ( TMP ) Temp Max ( TMP ) Precipitation ( PRE ) Vapor ( VAP ) Cloud Cover ( CLD ) Wet Days ( WET ) Frost Days ( FRS ) Global Horizontal ( SOL ) Direct Normal ( SOL ) Global Extraterrestrial ( SOL ) Direct Extraterrestrial ( SOL ) 1 year return level for temperature extreme ( TMP.EXT )
Type Greenhouse Gases
Source NOAA
Aerosol Index NASA Climate
CRU
Solar Radiation
Climate
NCDC
Estimated using temp from CDIAC
Table 1 : Variables and data sources .
2 ) Interpolation and Smoothing : We interpolated the data from NOAA and NCDC into a common 25x25 degree grid for North America to allow us to join multiple data sources . For this process we used thin plate splines on the monthly data to be consistent with the interpolation method used for the CRU data . Since the data from NASA and CRU were provided for a finer resolution grid , we performed spatial averaging to get data on the common 25x25 degree grid .
3 ) De seasonalization : We performed de seasonalization by re moving seasonal averages .
4 . EXPERIMENTS
As we noted in Introduction , we conduct two sets of experiments , one involving generic spatio temporal data that are simulated from an artificial model , and the other involving the actual climate data we described in the previous section . The experiments involving real climate data consist of the following steps : 1 ) Using spatiotemporal extreme value modeling technique to estimate the 1 year return levels ( 1 year event magnitudes ) of temperature ; 2 ) Incorporating the estimated 1 year return levels as a proxy for extreme temperature in the spatio temporal causal modeling using Group Elastic Net .
In the subsequent subsections , we describe the details of these experimental procedures and their results .
4.1 Simulation Experiments
We performed two sets of experiments on synthetic data to evaluate the performance of “ Group Elastic Net ” ( which takes into account spatial interactions through spatial lagging and appropriate penalization in the regression ) , against that of a method that neglects such interactions and considers instead that a measurement at location s is only affected by variables at the same location . Specifically , the comparison method solves the following group Lasso problem for each measurement xi .
Difference of return levels
ˆβ = arg min t,s −
( xi
T!t=L+1 β !s∈S N!j=1> L!l=1 l )2?1/2
( βj
+λ
N!j=1
L!l=1 l xj βj t−l,s)2
.
( 19 )
We generated synthetic spatial temporal data using a spatial temporal vector autoregressive ( VAR ) model as generative model . More specifically , we considered N = 10 measurements x1 , . . . xN , taken on a 15 × 15 spatial grid . For each ( interior ) point s = ( s1 , s2 ) we consider the neighborhood structure Ω= {(ω1,ω 2 ) ∈ {−2,−1 , 0 , 1 , 2} × {−2,−1 , 0 , 1 , 2} . We set the maximum lag L = 3 . Let xt,s denote the vector formed by all the measurements t,s , i = 1 , . . . , N . We considered the following generative model . xi xt,s =
Al,ωxt−l,s+ω + η .
L!l=1!ω∈Ω
The matrices Al,ω where generated as follows . We first generated an N × N adjacency matrix A , where the entry A[i , j ] = 1 indicates that xi causes xj , and A[i , j ] = 0 otherwise . The value of each entry was chosen by sampling from a binomial distribution , where the probability that an entry equals to one was set to 02
For the first set of experiments , we use a setup we call “ random coefficient weighting . ” That is , for each pair ( l , ω ) and each i , j , we set Al,ω[i , j ] = cl,ω(i , j ) · A[i , j ] , where cl,ω(i , j ) ∼ Unif(−0.1 , 01 ) For the second set of experiments , we use a different setup we refer to as “ decaying coefficient weighting ” , which is meant to represent situations where the influence decays with the distance . Formally , we first focus on the central location ω0 = ( 0 , 0 ) , and for each l and each i , j , we set Al,ω0 [ i , j ] = cl,ω0 ( i , j)· A[i , j ] , where cl,ω0 ( i , j ) ∼ Unif(−0.1 , 01 ) Then for each pair ( l , ω ,= ω0 ) , for each i , j we set Al,ω[i , j]=4 cl,ω0 ( i,j ) + ˜η5A[i , j ] , where ˜η is some random noise ∼ Uniform(−0.01 , 001 ) For both sets of experiments the noise η was sampled according to a normal distribution N ( 0 , 001 ) For each setup , we generated 10 models , and for each model simulated data for 100 time points . For Group Elastic Net , we used ˜∆j = I for the first set of experiments , and ˜∆j = diag(exp($ω$/2),ω ∈ Ω ) for the second ( since in practice one may not know the exact type of distance based decay , e.g polynomial , exponential ) .
1+’ω’
We measure the accuracy of each method with respect to their ability to correctly identify the underlying adjacency matrix A . We report the average F1 score along with standard deviation . The F1 score is defined as F1 = 2 P R P +R , where P is the precision and R the recall . The results are reported in Table 2 . Under both settings , Group Elastic Net exhibits higher accuracy than the comparison method , and the difference in accuracy is greater for the “ decaying coefficient weighting ” . This illustrates the importance of taking spatial interactions into account in the modeling .
Method
Random coef Decaying coef
Grp Lasso 0.53 ± 0.01 0.49 ± 0.02
Grp Elastic Net 0.60 ± 0.01 0.67 ± 0.01
Table 2 : The accuracy ( F1 ) of two comparison methods : Group Lasso ( no spatial interaction ) and Group Elastic Net for spatial temporal VAR models with random and decaying coefficient weighting .
5 4
0 4
5 3
0 3
5
0
−5
−120
−110
−100
−90
−80
−70
Figure 3 : A comparison of average return levels from 1948 to 1980 and from 1981 to 2005 .
4.2 Modeling extreme temperature
We used the daily temperature data from CDIAC for modeling extreme temperature . To obtain the exceedances over a threshold required for the modeling , we calculated the 95th quantile of the temperature distribution over the 58 years at each location and chose the observations which exceed the location specific threshold . We removed the stations which have very few years with at least 2 exceedances . Thus the data used for our model are the daily maximum temperature exceedances for 58 years at 254 stations .
As discussed in Section 222 , we interpret the extreme value analysis through return levels . We obtained the return level for years from 1948 to 2005 and each of the 254 stations . To investigate the evidence of global warming , for each station , we calculated the average return levels from 1948 to 1980 and from 1981 to 2005 and compared if there is any increase in terms of return level for these two periods . Figure 3 gives the return level difference which indicates the return levels increase over the past 58 years in midwest , western and eastern coastal areas of the United States . We observe a clear trend that the difference is mostly positive , with some of the regions exhibiting as much as 5 degrees Fahrenheit increase during this period .
4.3 Spatio temporal modeling and attribution We applied our spatio temporal causal modeling method on two datasets : one monthly , the other yearly . Both contain data for 19902002 on a 25x25 degree grid for latitudes in ( 30.475 , 50.475 ) , and longitudes in ( −11975,−7975 ) The monthly dataset contains the first 19 variables listed in Table 1 . The annual dataset contains in addition the estimated return levels for the extreme temperature . Having two different time resolutions allows us to investigate short term and longer term influences . Note that since the return levels were estimated yearly , we did not incorporate them into the monthly dataset ( Estimating monthly return levels will be the object of future work . )
For the spatial temporal causal modeling , we used a 3x3 spatial neighborhood structure and a maximum time lag of 3 months for the monthly data , and 3 years for the yearly data . In our modeling , we considered the temperature variables as a group ( TMP ) , as well as the solar variables ( SOL ) , in addition to the natural grouping structure by spatial temporal series .
Figure 4 shows the results of attributing the changes in return level for extreme temperatures using the yearly dataset , while Figures 5 and 6 show the results on attributing the changes in temperature , using respectively the yearly and the monthly dataset . In assessing the strength of causal links identified in our outputs , we use two separate metrics . One is the l2 norm of the regression coefficients corresponding to the variable group in question , which coincides with its contribution to the spatio temporal penalty term in the Group Elastic Net modeling . The other is the point at which the causal link in question appears in the output graph , as the parameter dictating how much emphasis is placed on the model complexity penalty in BIC is varied . This is done by multiplying the estimated noise variance in the penalty term ( σ2 in Equation 9 ) by a varying constant , which determines the trade off between the model fit and model complexity . ( The noise variance estimate is known to add a certain degree of arbitrariness to BIC with finite samples . ) Each of the figures exhibits several causal graphs corresponding to different values of this parameter , with models becoming denser going from left to right and top to bottom . Also in the figures the edge thickness represents the l2 norm of the regression coefficient . It is apparent that the two measures coincide for the most part ( order of appearance and edge thickness ) , and in particular , CO2 and other greenhouse gases are judged to have greater causal strength than solar radiance , according to both of these measures .
EXT_TEMP
AER
CO2
EXT_TEMP
AER
CO2
SOL
TMP
FRS
CH4
SOL
CO
H2
TMP
FRS
CH4
CO
H2
PRE
WET
PRE
VAP
CLD
WET
VAP
CLD
EXT_TEMP
AER
CO2
EXT_TEMP
CO2
AER
SOL
TMP
FRS
CH4
SOL
CO
TMP
H2
FRS
CH4
CO
H2
PRE
VAP
CLD
WET
PRE
WET
VAP
CLD
EXT_TEMP
AER
CO2
SOL
TMP
FRS
CH4
CO
H2
PRE
WET
VAP
CLD
Figure 4 : Attributing the change in 1 year return level for temperature extremes using annual data . Output causal structures for decreasing degrees of sparsity . Edge thickness represents the causality strength .
5 . CONCLUDING REMARKS
In the present paper we initiated a data centric approach to climate change attribution . The results to date are preliminary but encouraging , and in the future we plan to refine them , validate them with the domain experts , and explore ways in which they can provide assistance to the dominant , simulation based , approach to climate modeling . Acknowledgments We would like to thank the following people for their contributions to this work in a variety of ways : Huijing Jiang , Elena Novakovskaia , Cezar Pendus , Saharon Rosset and Lloyd Treinish . 6 . REFERENCES [ 1 ] Climate change 2007 the physical science basis IPCC
Fourth Assessment Report on scientific aspects of climate change for researchers , students , and policymakers .
[ 2 ] Barnett , TP , Pierce,DW and Schnur , R . ( 2001 ) . Detection of anthropogenic climate change in the world ’s oceans . Science , 292 .
[ 3 ] Beirlant , J . , Goegebeur , Y . , Segers , J . , and Teugels , J . ( 2004 ) . Statistics of Extremes : Theory and Applications . New York : Wiley .
[ 4 ] Banerjee , S . , Carlin , B . , and Gelfand , A . ( 2004 ) .
Hierarchical Modeling and Analysis for Spatial Data . Boca Ration , Florida : Chapman & Hall .
[ 5 ] Christidis , N . , Peter , SA , Brown , S . , Office , M . and Hegerl ,
J CGC ( 2005 ) . Detection of changes in temperature extremes during the second half of the 20th century . Geophys . Res . Lett . , 32(L20716 ) , 2005 .
[ 6 ] Carter , C . K . and Kohn , R . ( 2001 ) . On Gibbs sampling for state space models . Biometrica , 81 , 541 553 .
[ 7 ] Coles , S . ( 2001 ) . An Introduction to Statistical Modeling of
Extreme Values . Berlin : Springer .
[ 8 ] Gillett , NP , Zwiers,FW , Weaver,AJ and Stott , PA ( 2003 ) .
Detection of human influence on sea level pressure . Nature , 422(b ) .
[ 9 ] Granger , C . ( 1980 ) . Testing for causlity : A personal viewpoint . Journal of Economic Dynamics and Control 2 , 329 352 .
[ 10 ] Karoly , D . J . , Braganza , K . , Stott , P . A . , Arblaster , JM
Meehl , Anthony , GA , Broccoli , J . and Dixon , KW ( 2003 ) Detection of a human influence on north american climate . Science , 302 .
[ 11 ] Luo , L . Wahba , G . and Johnson , DR ( 1998 )
Spatial temporal analysis of temperature using smoothing spline anova . J . Climate , 11 .
[ 12 ] Matern , B . ( 1960 ) . Spatial Variation . New York : Springer . [ 13 ] New , M . , Hulme , M . and Jones , PD ( 1999 ) Representing twentieth century space time climate variability . Part 1 : development of a 1961 90 mean monthly terrestrial climatology . Journal of Climate 12 , 829 856
[ 14 ] Tibshirani , R . ( 1996 ) . Regression shrinkage and selection via the lasso . J . Royal . Statist . Soc B . , Vol . 58 ( 1 ) , 267 288 . [ 15 ] PA Stott , DA Stone , and MR Allen . ( 2004 ) Human contribution to the european heatwave of 2003 . Nature , 432 . [ 16 ] Yuan , M . and Lin , Y . ( 2006 ) Model selection and estimation in regression with grouped variables . J . R . Stat . B 68 , 49 67 .
[ 17 ] Zou , H . , Hastie T . ( 2005 ) Regularization and variable selection via the Elastic Net . J . R . Statist . Soc . B 67(2 ) 301 320 .
EXT_TEMP CO2
AER
EXT_TEMP
AER
CO2
EXT_TEMP
CO2
AER
SOL
TMP
FRS
CH4
CO
H2
SOL
TMP
FRS
CH4
CO
H2
SOL
TMP
FRS
CH4
CO
H2
PRE
WET
VAP
CLD
PRE
WET
PRE
WET
VAP
CLD
VAP
CLD
EXT_TEMP
AER
CO2
EXT_TEMP
CO2
AER
EXT_TEMP
AER
CO2
SOL
TMP
FRS
CH4
CO
H2
SOL
TMP
FRS
CH4
CO
H2
SOL
TMP
FRS
CH4
CO
H2
PRE
VAP
CLD
WET
PRE
WET
VAP
CLD
PRE
WET
VAP
CLD
Figure 5 : Attributing change in temperature using annual data . Output causal structures for decreasing degrees of sparsity . Edge thickness represents the causality strength .
CO2
AER
CH4
CO2
AER
CH4
AER
CO2
CH4
SOL
TMP
FRS
SOL
TMP
FRS
PRE
AER
VAP
CO2
CLD
CH4
CO
H2
WET
CO
H2
WET
SOL
TMP
FRS
SOL
TMP
FRS
PRE
AER
VAP
CO2
CLD
CH4
CO
H2
WET
CO
H2
WET
SOL
TMP
FRS
SOL
TMP
FRS
PRE
AER
VAP
CO2
CLD
CH4
CO
H2
WET
CO
H2
WET
PRE
CLD
VAP
PRE
VAP
CLD
PRE
CLD
VAP
Figure 6 : Attributing change in temperature using monthly data . Output causal structures for decreasing degrees of sparsity . Edge thickness represents the causality strength .
