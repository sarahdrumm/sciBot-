CoCo : Coding Cost For Parameter Free Outlier Detection
Christian Böhm University of Munich Munich , Germany boehm@dbsifilmude
Nikola S Müller
Max Planck Institute of
Biochemistry
Martinsried , Germany nimuell@biochemmpgde
Katrin Haegler University of Munich Munich , Germany katrinhaegler@meduni muenchen.de Claudia Plant
Technische Universität
München
Munich , Germany plant@lrztumde
ABSTRACT How can we automatically spot all outstanding observations in a data set ? This question arises in a large variety of applications , eg in economy , biology and medicine . Existing approaches to outlier detection suffer from one or more of the following drawbacks : The results of many methods strongly depend on suitable parameter settings being very difficult to estimate without background knowledge on the data , eg the minimum cluster size or the number of desired outliers . Many methods implicitly assume Gaussian or uniformly distributed data , and/or their result is difficult to interpret . To cope with these problems , we propose CoCo , a technique for parameter free outlier detection . The basic idea of our technique relates outlier detection to data compression : Outliers are objects which can not be effectively compressed given the data set . To avoid the assumption of a certain data distribution , CoCo relies on a very general data model combining the Exponential Power Distribution with Independent Components . We define an intuitive outlier factor based on the principle of the Minimum Description Length together with an novel algorithm for outlier detection . An extensive experimental evaluation on synthetic and real world data demonstrates the benefits of our technique . Availability : The source code of CoCo and the data sets used in the experiments are available at : http://wwwdbsifilmude/Forschung/KDD/Boehm/CoCo
Categories and Subject Descriptors H28 [ Database applications ] : Data mining
General Terms Algorithms , Design , Reliability
Keywords Outlier Detection , Coding Costs , Minimum Description Length , Data Compression
1 .
INTRODUCTION
Automatic outlier detection in large data sets is often equally or even more important than the detection of regularities . In various application fields like economy , biology , or medicine , the detection of extraordinary observations is of great interest . For example , the identification of criminal activities , such as credit card fraud , is crucial in electronic commerce applications [ 9 ] . In biology , an automatic detection of outstanding measurements or noise is critical for high throughput data generated with eg mass spectrometry or gene expression analysis . The wide range of application fields also includes entertainment , sports , eg performance analysis of athletes , and many more .
Today , many data mining publications are in the field of clustering or outlier detection . The first field searches for regularities in a data set whereby the second identifies irregular data . Closer inspection of both fields reveals a strong relationship , whereby one goes barely without the other : On one hand , most clustering algorithms are confronted with outliers which deteriorate the cluster quality and/or destabilize the algorithm . Thus , the outliers need to be removed beforehand . On the other hand , outlier detection algorithms require a definition of the underlying cluster structure although clusters are not explicitly identified . Only if the cluster structure ( of the regular data ) is known , outliers can be identified without any doubt . Following the definition of Hawkins [ 5 ] :
An outlier is an observation that deviates so much from other observations as to arouse suspicion that it was generated by a different mechanism .
To formalize this definition , the ordinary and potentially clustered points as well as the outliers need to be differentiated with respect to a well defined distinction criterion . In existing outlier detection approaches , the distinction criterion is quantified by a metric distance function and parameter settings . The results are only meaningful if the distance function is well characterized with respect to the object similarity and suitable parameter settings . However ,
149 Compared to the transfer of 8 bits for object c , the increase in coding costs can deposit c as an outlier . Unlike the character strings in the simple example , the object P is a point , ie a d dimensional vector of continuous values . Inferred from the idea of Huffman coding , we can apply the data compression idea by assigning few bits to frequent values and many bits to rare values of the coordinates of P . Frequent and rare values can be clearly distinguished using the above mentioned EPD . This principle is generally called the Minimum Description Length ( MDL ) .
CoCo effectively applies the MDL principle to parameterfree outlier detection . No a priori information about the data set is required , like the number of clusters and outliers , the cluster size , a distance metric , or the cluster density . Furthermore , we define a CoCo outlier factor with the concept of coding costs of an object , given the entire data set . With the outlier factor we can clearly separate the cluster points from the outliers .
The paper is organized as follows : In the next section , we briefly survey the related work . In Section 3 we introduce CoCo by elaborating a flexible model for continuous data relying on two major building blocks : The Independent Component Analysis ( ICA ) and the EPD . Furthermore , we define our CoCo outlier factor . Section 4 provides an extensive experimental evaluation and Section 5 concludes the paper .
2 . RELATED WORK
The most established approaches to outlier detection in databases can be classified into the two categories of distanceand density based approaches . Additionally , a brief survey of the application of the information theoretic MDL principle in data mining is given . For an extended survey on anomaly detection please refer to [ 4 ] . 2.1 Distance based Outlier Detection
Distance based outlier detection is among the earliest approaches and has been proposed and further elaborated by EM Knorr and RT Ng [ 10 , 11 , 12 ] . An object o of a database DB is a distance based outlier if at least a fraction β of the objects in DB have a distance greater than a previously specified distance d . This basic approach provides binary flagging of points as outliers or non outliers . An extension [ 12 ] proposes algorithms to support semantic interpretation of distance based outliers . However , without knowledge of the data distribution , it is difficult to specify suitable values for the parameters β and d . In addition , a fixed distance threshold d identifies only global outliers . 2.2 Density based Outlier Detection
Density based outlier detection introduces an outlier notion derived from density based clustering and , therefore , detects not only global but also local outliers . A point is flagged as an outlier if it does not fit well into the objects neighborhood density .
The local outlier factor LOF [ 3 ] formalizes this idea by considering the M inP ts nearest neighbors of an object as its neighborhood . The LOF of an object is defined by the ratio of its M inP ts nearest neighbor distance and the mean M inP ts nearest neighbor distance in its neighborhood . However , the global parameter M inP ts strongly affects the outlier detection result : Arbitrary high or low values of M inP ts
Figure 1 : Data compression : The principle of MDL is to detect regularities in the data and compress accordingly . these premises assume a prior characterization of the data set .
To cope with the problems of defining a distinct criterion and parametrization , we present CoCo , a parameterfree outlier detection method based on the ideas of data compression and coding costs . CoCo is able to identify the outliers in a data set based on a flexible definition of the regular data . The regular data is flexibly defined by a very general Probability Density Function ( PDF ) , in our case a mixture model of the Exponential Power Distribution ( EPD ) . The EPD is a family of distribution functions which contains the Gaussian distribution , the uniform distribution , the Laplacian distribution , and a great variety of other distribution functions . Compared to previous outlier detection approaches , the EPD is not restricted to either uniform or Gaussian distribution functions . We demonstrate with our experiments ( cf . Section 4 ) that the EPD is powerful enough to model the regular data in a variety of applications .
CoCo considers a point P as outlier , unless it fits nicely in any of the distribution functions to be estimated of the points in the neighborhood of P , independent of the neighborhood size . To measure the quality of the fit of P we adopt the idea of data compression : If a point fits well into a distribution function , it can be compressed efficiently . To connect the data compression efficiency of P with the degree of P being an outlier , Figure 1 illustrates an intuitive example : Suppose , we want to transfer data via a communication channel . The sender wants to transfer the string ambmc to the receiver . A naive way would be to transfer each single character requiring in total 16,008 bits for m = 1 , 000 and 8 bits per character . To minimize the communication costs , a smart sender exploits regularities in the data . A little program could generate the first part of the string by printing 1,000 times the character a followed by 1,000 times b . An efficient coding in an arbitrary language requires eg 344 bits . The sender additionally transfers c as single character ( 8 bits ) instead of adding a print statement to the little program ( which would require 64 bits ) . Thus , 352 bits are required to transfer the string in total . This clever compression reduces the communication cost to 215 % In the example , object c is an outlier generated by a different mechanism than the other objects . The regular objects can be strongly compressed by formulating the underlying mechanism with a model ( here , the little program ) and require only a transfer of 344/2 , 000 = 0.17 bits , each . ambmcaaaaaabbbbbbbbcfor i=0 to 2m do if i < m print(a ) else print(b ) end c16008 bits352 bits150 either regard small cluster points as outliers or do not detect outliers , respectively . approaches are especially designed for time series and their goal is to reconstruct the signal as accurate as possible .
LOCI [ 14 ] is a density based multi granularity outlier factor . Similar to LOF , points are regarded as outliers if the object density in their local neighborhood significantly deviates from the average object density in the local neighborhood . The local neighborhood is specified by two parameters , which are called counting and sampling neighborhood . The counting neighborhood specifies some volume of the feature space which is used to estimate the local object density . The sampling neighborhood is larger than the counting neighborhood and contains all points which are used to compute the average object density in the neighborhood . LOCI differs from LOF by this decoupling of counting and sampling neighborhoods . It can be demonstrated that without this decoupling , density estimation leads to incorrect results in some specific cases . In addition , the decoupling allows for efficient algorithms for approximate computation of LOCI . However , the decoupling requires the specification of additional parameters . Together with the outlier factor , the LOCI approach proposes a visualization , the so called LOCI plot which displays the LOCI of a point wrt increasing sizes of the local neighborhood and , thereby , allows eg to identify micro clusters . However , LOCI as well as LOF apply the Euclidean distance as a global metric distance function . In addition , the LOCI approach proposes to flag points as outliers which deviate in their local object density more than three times of the standard deviation of the overall object density of the sampling neighborhood . This flagging assumes a Gaussian distribution of the object densities . 2.3 Minimum Description Length in Data
Mining
Information theoretic concepts , especially the MDL principle and related ideas have been recently successfully applied to clustering [ 1 , 2 , 15 ] , and are also established in the areas of regression [ 17 ] , rule mining [ 19 ] , classification [ 8 ] , and anomaly detection [ 7 ] . The MDL principle relates learning and data compression , as already illustrated in Figure 1 . Learning regularities from data allows to compress the data more efficiently . For model selection in clustering and classification , MDL allows to compare different candidate models achieving a natural balance between goodness of fit and model complexity . To the best of our knowledge , the MDL principle has not been applied to the problem of outlier detection so far .
Regarding the problem specification , clustering is most related to outlier detection . However , outliers are regarded as a problem for clustering , since they can severely affect the result of most algorithms . A parameter free extension of K Means clustering is X Means [ 15 ] . However , the XMeans algorithm is restricted to spherical Gaussian clusters and very sensitive to outliers . RIC [ 1 ] has been designed as a post processing step to improve an initial clustering of an arbitrary conventional clustering algorithm . After filtering the initial clusters from noise , for each cluster a model is determined . This model comprises a rotation matrix determined by PCA and a PDF assigned to each coordinate selected from a set of predefined PDFs . The recently proposed algorithm OCI [ 2 ] introduces a very general clustering notion based on the EPD and ICA . Also related are approaches to MDL based de noising of signals [ 16 , 18 ] . However , these
3 . COCO CODING COST OUTLIER DE
TECTION
With CoCo , we introduce an entirely parameter free outlier detection method based on coding costs . Following Hawkins [ 5 ] , we adapt the outlier definition to the MDL principle for data compression . A data point is considered as outlier , if its compression rate is unusually high . As reference to define a high compression rate , we consult a compression rate of a cluster point . This approach nicely avoids the definition of a distance metric which would require thresholding of an undefined and unknown neighborhood .
Data sets may be rotated or distorted with respect to the Cartesian coordinate system . The ICA enables us to process data sets which are not aligned to the orthogonal axes . However , the idea of an ordinary point needs to be clearly defined . In contrast to currently available outlier detection methods , we expect real life data to underly not only Gaussian distributions . Besides , we want to include several other distributions . A generalization of the Gaussian PDF is the EPD . The EPD includes , among others , the uniform , and the Laplacian PDF . By utilizing an EPD , any a priori information on the type of distribution is required . Therefore , we do not create a bias towards Gaussian data models . Combining ICA with EPD as the description of a regular subset of the data set , we cover many real world data sets without taking explicit care of cluster density , shape , and orientation .
Entirely automatic , CoCo detects outliers having high coding costs with respect to the ordinary points which can be effectively compressed . We implemented a bottom up approach to identify all irregular data points while choosing the best compression model of ordinary points .
For each data point o , we initiate a set of nearest neighbors . Without prior knowledge of the underlying cluster shape , we extract a substantial number of nearest neighbors nno based on their Euclidean distance to o . We reliably center and whiten the set of nearest neighbors with ICA , icanno , and fit an EPD , epdnno . Iteratively , we expand the nearest neighbor set with those remaining data points to be best compressed based on the current epdnno . After each update of the set of nearest neighbors nno , we simply adjust the icanno and epdnno since it is an expensive operation to estimate it anew . For each epdnno estimate , we can calculate the coding costs costo as compression rate of the object o under the given cluster description epdnno . If the data is fully explored for each object o , we extract the most suitable EPD cluster model by selecting the minimum compression rate of any object included in cost minnno . The outlier factor for the data object costo(j ) is determined by its corresponding compression excess to cost minnno ( j ) .
The following defines the principles of ICA , EPD , data compression , and their link to the parameter free outlier detection with CoCo . 3.1 Independent Component Analysis
It was observed that mixtures of signals get best de mixed when searching for non Gaussianity . A mixture of several signals originated from any distribution type is always more Gaussian than the originals . The entropy of a Gaussian dis
151 Algorithm 1 CoCo Input : Database D OF := {} // Outlier Factors for data object o ∈ D do costo := {} cost minnno := {} nno := initial set of Nearest Neighbors not nno := D\nno icanno := ICA(nno ) nno,ica := transform(nno ; icanno ) epdnno := estimate EPD(nno,ica ) while not nno = {} do costo := costo∪ coding cost(oica ; epdnno ) cost minnno := cost minnno ∪ min(coding cost(nno,ica ; epdnno ) ) not nno,ica := transform(not nno ; icanno ) costnot nno,ica := coding cost(not nno,ica ; epdnno ) nno := nno∪{not nno,ica with lowest costnot nno,ica} not nno := D\nno update icanno nno,ica := transform(nno ; icanno ) update epdnno end while j := min(cost minnno )// index best compressed cluster OF := OF ∪ ( costo(j ) − cost minnno ( j ) ) end for
XMeans(OF ) to obtain outlier & cluster points
PCA transform of x is determined by
√ y :=
−1 × V T × c . Λ diag(p1/λ1 , . . . ,p1/λd ) are both diagonal matrices . that Λ = diag(λ1 , . . . , λd ) and
Note ,
√
−1
Λ
=
For optimal projection of the data we need to determine the directions of minimal entropy ( generated with ICA ) rather than the one of maximal variance ( created by PCA ) . After transforming the data to white space , the FastICA algorithm [ 6 ] determines a weighting matrix W containing the independent components . Regarding the original space , the independent components are not orthonormal in contrast to the principal components . The iterative optimization of W expects the input data to be whitened . The fix point iteration optimizes W = ( w1 , . . . , wd ) , whereby the weight vectors are updated with the following rule : wi := E{y × g( wi
T × y)} − E{g
T × y)} × wi
( wi ds
We use tanh(s ) for the non linear contrast function g(s ) . is the derivative of g(s ) and E{ . . .} Note that g(s ) = dg(s ) is the expected value . W is updated until convergence and then orthonormalized . The overall projection of the original data into the white space of independent components is achieved by the de mixing matrix M−1 . With M = V × √
Λ × W we denote
M
−1 = W T 1√ Λ
V T . is simply det(M−1 ) =Q
1≤i≤d p1/λi . Recall that the rota
W and V are orthonormal matrices , thus the determinant tion performed in the white space is expressed by W , and whitening is achieved by multiplying the coordinate vector by the scaled Eigenvector matrix . tribution is maximal , whereby , all other distributions have a lower entropy . However , the coding costs , measured by the entropy , need to be minimized in order to guarantee a maximal compression efficiency . Thus , we apply the ICA to maximize non Gaussianity as a measure of statistical independence . Its algorithm favors the directions in the data which are not similar to the Gaussian distribution .
We assume that most data sets in experimental data usually do not follow equally dense distributions . They are rather distorted data sets with respect to the Cartesian coordinate system . The ICA first transforms the data into a so called white space . Whitening involves de correlation and normalization of the data to unit variance which enables us to implicitly handle unequally dense clusters .
After the independent components are determined , we can simply project the data x into the independent component space with z = M
−1 × ( x − m ) .
3.2 Exponential Power Distribution
The EPD is a generalization of the Gaussian distribution in such a way , that it also includes the Laplacian and the uniform distribution , depending on the parameter setting . Its PDF has three different parameters . Beside the location parameter µ , and the scale parameter σ , a shape parameter p is introduced [ 13 ] . For a random variable X , the EPD is
The Principal Component Analysis ( PCA ) identifies the directions of maximal variance y given a set of coordinates x ∈ C in a d dimensional space . First , the data get centered c = x − m around the empirical mean
X x∈C x m =
1 |C| of the data set C . Second , the centered data c need to be normalized to unit variance in all directions . The eigenvalue decomposition of the covariance matrix Σ is Σ := V × Λ × V T , where V and Λ are orthogonal matrices containing the eigenvectors and eigenvalues of Σ , respectively . Finally , the
Figure 2 : Different shapes of the Exponential Power Distribution for different choices of parameter p .
4 2024000204EPD with different shapesp= 1p= 2p ∞152 tion with respect to σi of the EPDP P z∈C fEP D(zi ; µi , σi , pi ) : z∈C |zi − µi|pi dfEP D(C ; µi , σi , pi )
= 0 . dσi
⇒ σi =
+
σi
= −|C|
X
1 |C| z∈C
σpi+1 i
!
|zi − µi| 1 pi
The parameters µi and pi need to be optimized explicitly . We use a nested bisection search as optimization technique to find pi and µi in their parameter space . The direction to browse through the space is determined by the derivatives of the log likelihood function with respect to µi dfEP D(C ; µi , σi , pi ) dµi
= − 1 σp i and pi dfEP D(C ; µi , σi , pi ) dpi z∈C spi i + p log σi
P
+ p2 i
= −|C| P z∈C spi i σpi p2 i
|zi − µi|1pi−1sign(zi − µi ) z∈C
X „ i − pP log pi + Ψ(1 + z∈C ( spi
« ) − 1 i − log si )
1 pi
, ds with si = |zi − µi| . Ψ(s ) = d ln Γ(s ) is the digamma function being the logarithmic derivative of the gamma function . The EPD is estimated by this maximum likelihood approach until convergence of pi . 3.5 Coding Cost with MDL
After we estimated an exact representation fEP D(x ; M−1 , m , µ , σ , p ) of the data x with ICA and EPD , we need a reliable approach to judge the accuracy of the fit . We create the link of the concept of PDFs to the principle of data compression with the help of the MDL . Based on the Huffman coding , a number of bits are assigned to each object with the inverse logarithm of the probability of the object . This negative log likelihood represents the coding costs cP DF of an object x , given any PDF , and is defined as :
„
« cP DF ( x ) = log2
1 fP DF ( x )
= − log2(fP DF ( x) ) .
In order to represent the coding cost in the number of bits , the logarithm is typically used to a basis of 2 . With CoCo , we underly an EPD as PDF . Thus , the relative coding cost of a data point x under a given EPD after ICA is :
`|det(M
−1)|´− X
1≤i≤d cEP D(x ) = log2 log2 ( fEP D(z ; µi , σi , pi ) ) .
We can neglect to determine the absolute coding costs depending on different PDFs and the coding of the PDF parameters . It is absolute crucial to determine statistically independent major directions with ICA to guarantee optimal data compression . Figure 4 clearly demonstrates that ICA transforms the data in such a way that it removes redundancy in the data with respect to the axes for best compression . 3.6 CoCo Outlier Factor and Detection
Putting everything together , for each set of coordinates x from the nearest neighbors nno generated with CoCo , we determine the rotation and the cluster description with EPD epdnno . For each estimate epdnno , the data compression
Figure 3 : Data set approximated with an EPD and a Gaussian distribution . defined as : exp(− |x−µ|p pσp ) 1 p Γ(1 + 1 p )
2σp fEP D(x ; µ , σ , p ) =
Note that Γ(s ) =R ∞ ts−1 exp(−t)dt is the gamma function as an extension of the factorial operator for real numbers .
0
The shape parameter p determines kurtosis , or the sharpness of the distribution . For p > 2 , the EPD is platykurtic , with p → ∞ mimicking a uniform distribution . For p = 2 , the EPD corresponds to a Gaussian distribution . If 2 < p < 0 , the EPD is leptokurtic , including a Laplacian distribution for p = 1 ( Fig 2 ) . 3.3 EPD after ICA
After projection of the coordinates into the white space and ICA , the data z is de correlated and independent . This allows us to describe each coordinate independently by an own EPD . Typically , a multi dimensional data space contains d different PDF representations fEP D(zi ; µi , σi , pi ) with 1 ≤ i ≤ d . All d distributions are combined in a mixing matrix M , where the data points x correspond to x = M × z + m , with m being the shifting vector and M determined by PCA , as described above . M allows the independent components vectors to be not orthogonal . The EPD in a d dimensional space ( after ICA ) is defined for a point x as fEP D(x ; M
−1 , m , µ , σ , p ) =
1≤i≤d fEP D(zi ; µi , σi , pi )
| det(M−1)|
Figure 3 illustrates the effect of the approximation of a data set with an EPD after ICA . While the approximation of the same data with a Gaussian distribution is rather inappropriate . 3.4 EPD Approximation
The estimation of the three parameters is a non trivial problem . Although , µi = 0 and σi = 1 are defined for p=2 ( Gaussian distribution ) after ICA , µi and σi are no longer identical to the empirical mean and standard deviation , respectively . All three parameters µi , σi and pi can be optimized by estimating the maximum likelihood , given a data set C . Only a simultaneous approximation of all parameters ensures that the derivatives of the likelihood of the EPD vanish with respect to µi , σi and pi .
Assuming µi and pi to be given , the parameter σi can be simply calculated with the derivative of the likelihood func
Q
EPDGaussian153 Figure 4 : ICA creates redundancy in the data by centering and whitening . rate is calculated with cEP D(o ) , o being the whitened coordinates of object o . We determine the efficiency to compress the data points nno , with an epdnno estimate , with any object p ∈ nno having minimal coding cost : We gather information of compression rates for each set of nno with increasing size . Ideally we need to know the optimal neighborhood cluster size of o to determine the perfect compression of o regarding C . Practically , we only have information for each epdnno estimate throughout the data set . With it comes the information of any object ( p ) exhibiting the minimal coding cost in nno . The best compression rate ( min(cost minnno ) , throughout all generated nno sets ) represents the best epdnno estimate for any nno . In order to obtain the factor of o being an outlier , the CoCo outlier factor is the absolute compression rate increase with respect to a minimal p .
The structure of a data set is usually unknown . We screen C coming from o iteratively by adding a set of neighbors ; its size growing exponentially with respect to the size of C . To guarantee a stable estimate of EPD we initiate nno with a set of 20 neighbors . This screening approach of CoCo is however quadratic in the number of points n . In addition , the runtime is cubic in the dimensionality d due to PCA and EPD estimation .
After all CoCo outlier factors are obtained , we expect all outliers to exhibit unusually high costs in comparison to the ordinary , perhaps clustered points . The cluster points can be compressed very effectively and show outlier factors around 0 . Flagging of outliers is difficult , since it involves to define a suitable threshold , which is a non trivial task for an unknown data set . Instead , we simply apply an X Means algorithm to determine the set of clustering points being the cluster closest to 0 . Theoretically , we can establish an outlier order by simply organize the other CoCo outlier factor groups in ascending order . In practice , X Means usually finds two clusters , one containing the clustering points , the other determining all outliers .
CoCo combines ICA with EPD as cluster description to determine outliers entirely parameter free with the principle of data compression . No a priori knowledge of the number of outliers or the underlying cluster shape or density is required .
4 . EVALUATION
In the following we evaluate our outlier factor CoCo in comparison to LOF [ 3 ] and LOCI [ 14 ] using one synthetic data set as well as NBA data . We implemented CoCo and LOF in Java and obtained the implementation of LOCI from the authors . The synthetic data set was created to exemplify the strength of CoCo . 4.1 Synthetic Data
We detected the outliers of a synthetic data set with our novel algorithm CoCo and compared them with outliers detected by LOF and LOCI . Figure 5 provides the results of CoCo , LOF , and LOCI for the synthetic data set . The synthetic data set consists of four clusters C1 4 containing 184 ( C1 ) , 154 ( C2 ) , 52 ( C3 ) , and 50 ( C4 ) data points . Each
Figure 5 : Outlier detection results from CoCo ( left ) , LOF ( middle , M inP ts = 50 selecting only the top 26 outliers ) , and LOCI ( right , α = 0.5 and rmin = 10 ) for a synthetic data set consisting of four clusters ( C1 4 ) and 26 outliers . Detected outliers are highlighted with red crosses .
ICA20402008060604010080402060804020060800C1C2C3C412321405060708020806040100154 cluster has different cluster properties and a non orthogonal major orientation . Cluster C3 underlies a Gaussian PDF . All together 26 noise points were added to the data set .
CoCo correctly detects all 26 outlier points highlighted with red crosses ( Fig 5 , left ) . All belong to one group of outliers , beside the group of cluster points shown in black . Note , that CoCo requires no input parameter in order to identify all noise points . It can handle different types of cluster shapes and orientations without expecting an explicit description of their distributions .
LOF was applied to identify the outliers based on a M inP ts neighborhood of 50 determined by the size of the smallest cluster in the set ( Fig 5 , middle ) . We obtain the top 26 outliers ( highlighted with red crosses ) since we know how many outliers are present in the data set . There are 24 out of the 26 noise points assigned correctly . Two noise points next to cluster C2 ( circled in blue as No . 3 ) are not detected , leading to two falsely identified cluster points as outlier ( circled in blue as No1&2 ) Note , that we collected the top 26 data points ranked by the LOF score . Setting the parameter M inP ts to a value smaller or equal than 10 , LOF identifies more cluster points as outliers while leaving many true outliers undetected ( data not shown ) . A M inP ts value of 20 to 50 leads to the result shown in Figure 5 . If we have no a priori information about the number of outliers , it is only possible to determine an arbitrary number of outliers . In addition , an approximate cluster size needs to be known in advance to set M inP ts , in order to get a meaningful output . These assumptions make it difficult to apply LOF to real world data .
LOCI was applied to our synthetic data set with α = 0.5 and rmin = 10 ( Fig 5 , right ) and could identify 43 outlier points based on the suggested outlier flagging criteria . All together 17 true outliers were missed , while two points from within cluster C3 and 27 points from cluster C4 were labeled as outliers . Different parameter settings of rmin may detect more true outliers , but at the same time label more cluster points as outliers . Obviously , LOCI is not able to deal with clusters showing low density , like C4 . In Figure 7 , we have a closer look at the LOCI plot of an outlier point ( circled in blue as No . 1 in Figure 5 , right ) and a cluster point ( No .
Figure 7 : LOCI plot for two points detected as outliers . ( 1 ) True outlier . ( 2 ) Falsely labeled cluster point .
2 ) . The LOCI plots look very similar even though they are supposed to emphasize the difference between a cluster point and an outlier . We have to note , that although we applied the algorithm with the suggested parameter settings , the result was difficult to interpret even after correspondence with the authors . 4.2 CoCo Outlier Factor Visualization
To emphasize the difference and strength of the CoCo outlier factor in comparison to the LOF score , we introduce a visualization of the “ outlierness ” ( Fig 6 ) . A scatter plot of the data in x y directions is combined with a bar representation of the outlier factors in the z dimension . We can clearly show that the utilization of data compression is able to separate the outliers from the cluster points in comparison to the outlier factor of LOF . For the majority of the cluster points the CoCo coding costs are close to 0.0 which can be seen by the short , dark blue bars . Outliers are either light blue or even red indicating their extraordinariness , ranging from 6.4 up to 242 Due to the large range between cluster points and outliers it is possible to clearly differentiate them using CoCo . In contrast , LOF produces values ranging from 0.8 up to 2.3 which makes it almost impossible to clearly differentiate cluster points from outliers explicitly .
The visualization of the outlier factors of LOF demonstrates , that the cluster structure is based on Euclidean dis
Figure 6 : Visualization of the CoCo Outlier Factor and the LOF Score for the synthetic data set .
510152025303540450100200300400500600510152025303540450100200300400500600LOCI PlotOutlier Point #1LOCI PlotCluster Point #2LOF ScoreCoCo Outlier FactorLOFColormap0 max155 Figure 8 : Outlier identification with CoCo for the NBA data set . Top 10 outlier are highlighted with red crosses and marked with player names . tances : the outlier factors continuously increases circular from the cluster centers to the cluster margins . In contrast to LOF , the CoCo outlier factors are equally low throughout the entire cluster except for the cluster edge points . It is based on the flexible cluster structure description using ICA and EPD . 4.3 Experimental Data
After extensive evaluation of CoCo on synthetic data sets , we want to apply our novel parameter free outlier detection method to experimental data . We used the NBA data available at the NBA website http://wwwnbacom In the Season 2007/08 , 450 players are described with four attributes : the number of games played ( GP ) , the number of points ( PPG ) , the rebounds ( RPG ) , and assists ( APG ) per game . CoCo was applied to this NBA data detecting 105 outliers . Figure 8 displays scatter plots of the data . For simplicity reasons , we highlight only the top 10 outliers in red as listed in Table 1 . Obviously , the data distribution is non Gaussian . The top 10 outliers identified by CoCo , include outstanding players like Stephon Marbury with a coding cost of 19.6 being 12 times higher than the average coding costs . Marbury is an outstanding player with respect to all attributes . He played only 24 games out of 82 and was still able to achieve 13.9 points and additionally assisted in 4.7 points , resulting in being involved in 18.6 points per game . Jamaal Tinsley , has played 39 games in this season but was still able to assist in 8.4 game points . He was involved in 20.3 points and played more games than Marbury . Gilbert Are nas exhibits a rare combination of playing 13 games while achieving 19.4 points per game . Jason Kidd is outstanding in the number of rebounds having played in 80 out of 82 games . Elton Brand has played only few games but still was able to achieve an outstanding number of points . As evident from Figure 8 , outstanding players such as Kidd or Brand are best characterized with the most general model with only one component .
To put the CoCo outlier detection method into a context , we applied LOF and LOCI to the NBA data set , as well . Table 2 displays the top 10 outliers identified by LOF . Highlighted in bold are all players that were identified as top 10 outlier of CoCo , like Marbury , Arenas , or Brand . Except for one , all players from the top 10 outlier of CoCo are at least under the top 20 of LOF . However , the outstanding player Kidd was missed by LOF ranked at the 50th position with a LOF score of 116 In addition , as observed for synthetic data , the result of LOF strongly depends on its parameterization . Only seven players are reproducibly detected as top 10 for a M inP ts = 40 ( players are marked with an asterix ) . All five players which were found to be under the top 10 of CoCo were also included in the intersect of M inP ts = 40 and M inP ts = 50 which strikes that they are strongly outstanding . The top 10 outliers found by LOCI are shown in Table 3 . The intersect between LOCI and CoCo is again highlighted in bold .
Games0204060800102030BrandMurayKiddSessionsFoye020406080051015BrandMurayKiddSessionsFoye020406080051015BrandMurayKiddSessionsFoye0102030020406080MarburyTinsleyArenasBynumKamanPoints0102030051015BrandMurayKiddSessionsFoye0102030051015BrandMurayKiddSessionsFoye051015020406080MarburyTinsleyArenasBynumKaman0510150102030MarburyTinsleyArenasBynumKamanRebounds051015051015BrandMurayKiddSessionsFoye051015020406080MarburyTinsleyArenasBynumKaman0510150102030MarburyTinsleyArenasBynumKaman051015051015MarburyTinsleyArenasBynumKamanAssists156 CoCo of Name 19.6 17.9 16.1 15.4 13.6 12.9 12.8 12.5 12.3 12.0
Stephon Marbury Jamaal Tinsley Gilbert Arenas Andrew Bynum Elton Brand Ronald Muray Jason Kidd Chris Kaman Ramon Sessions Randy Foye
GP PPG RPG APG 24 39 13 35 8 73 80 56 17 39
4.7 8.4 5.1 1.7 2 1.3 10.1 1.9 7.5 4.2
13.9 11.9 19.4 13.1 17.6 9.1 10.8 15.7 8.1 13.1
2.5 3.6 3.9 10.2 8 4.5 7.5 12.7 3.4 3.3
Table 1 : Top 10 outliers identified with CoCo on NBA data . ( of = outlier factor ) .
LOF Name 1.43 Elton Brand* 1.32 Steve Francis 1.31 Kasib Powell 1.28 Gilbert Arenas* 1.28 1.27 1.26 Dwyane Wade* 1.25 1.24 Andrew Bynum* 1.24 Chris Kaman*
Chris Webber* Stephon Marbury*
LeBron James
GP PPG RPG APG 8 10 11 13 9 24 51 75 35 56
17.6 5.5 7.6 19.4 3.9 13.9 24.6 30 13.1 15.7
8 2.3 4 3.9 3.6 2.5 4.2 7.9 10.2 12.7
2 3 1.6 5.1 2 4.7 6.9 7.2 1.7 1.9
Table 2 : Top 10 outliers identified by LOF with M inP ts = 50 on NBA data sorted by outlier factor . Players also among the top 10 of CoCo are marked in bold . The asterix indicates players which are also among the top 10 using M inP ts = 40 . Note that all players found to be under the top 10 of CoCo and LOF M inP ts = 50 are also found using M inP ts = 40 .
5 . CONCLUSION
In this paper , we proposed CoCo , a parameter free outlier detection . The perspective of data compression in outlier detection allows to define a notion of outliers , which is intuitive to interpret and requires no parameter settings . Our experiments demonstrate that CoCo is not restricted to Gaussian data but applicable to a wide range of data distributions .
In future work , we will further elaborate techniques to facilitate the interpretation of cost based outliers . In addition , we will focus on online algorithms for cost based outlier detection in data streams , since online monitoring is essential in many applications involving outlier detection .
Name GP PPG RPG APG LeBron James 75 Kobe Bryant 82 Dwyane Wade 51 Chris Kaman 56 Elton Brand 8 Andrew Bynum 35 Jamaal Tinsley 39 48 Mike Bibby 42 Jermaine O’Neal Udonis Haslem 49
30 28.3 24.6 15.7 17.6 13.1 11.9 13.9 13.6 12
7.9 6.3 4.2 12.7 8 10.2 3.6 3.3 6.7 9
7.2 5.4 6.9 1.9 2 1.7 8.4 6 2.2 1.4
Table 3 : Top 10 outliers identified by LOCI on NBA data . Players also among the top 10 of CoCo are marked in bold .
6 . REFERENCES [ 1 ] C . B¨ohm , C . Faloutsos , J Y Pan , and C . Plant . Robust information theoretic clustering . In KDD Conference , pages 65–75 , 2006 .
[ 2 ] C . B¨ohm , C . Faloutsos , and C . Plant . Outlier robust clustering using independent components . In SIGMOD Conference , pages 185–198 , 2008 .
[ 3 ] M . M . Breunig , H P Kriegel , R . T . Ng , and
J . Sander . Lof : Identifying density based local outliers . In SIGMOD Conference , pages 93–104 , 2000 .
[ 4 ] V . Chandola , A . Banerjee , and V . Kumar . Anomaly detection : A survey . ACM Computing Surveys , 2009 . [ 5 ] D . Hawkins . Identification of Outliers . Chapman and
Hall , London , 1980 .
[ 6 ] A . Hyv¨arinen , J . Karhunen , and E . Oja . Independent
Component Analysis . 2001 .
[ 7 ] E . Keogh , S . Lonardi , and C . A . Ratanamahatana . Towards parameter free data mining . In KDD ’04 : Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining , pages 206–215 , New York , NY , USA , 2004 . ACM .
[ 8 ] S . Kim and I S Kweon . Simultaneous classification and visualword selection using entropy based minimum description length . In ICPR ( 1 ) , pages 650–653 , 2006 .
[ 9 ] E . M . Knorr . On digital money and card technologies .
Technical Report Technical Report 97 02 , University of British Columbia , 1997 .
[ 10 ] E . M . Knorr and R . T . Ng . A unified notion of outliers : Properties and computation . In KDD , pages 219–222 , 1997 .
[ 11 ] E . M . Knorr and R . T . Ng . Algorithms for mining distance based outliers in large datasets . In VLDB , pages 392–403 , 1998 .
[ 12 ] E . M . Knorr and R . T . Ng . Finding intensional knowledge of distance based outliers . In VLDB , pages 211–222 , 1999 .
[ 13 ] A . Mineo and M . Ruggieri . A software tool for the exponential power distribution : The normalp package . Journal of Statistical Software , 12(4 ) , 1 2005 .
[ 14 ] S . Papadimitriou , H . Kitagawa , P . B . Gibbons , and C . Faloutsos . Loci : Fast outlier detection using the local correlation integral . In ICDE , pages 315– , 2003 .
[ 15 ] D . Pelleg and A . Moore . X means : Extending
K means with efficient estimation of the number of clusters . In ICML Conference , pages 727–734 , 2000 .
[ 16 ] J . Rissanen . Mdl denoising . IEEE Transactions on
Information Theory , 46(7):2537–2543 , 2000 .
[ 17 ] M . Robnik Sikonja and I . Kononenko . Pruning regression trees with mdl . In ECAI , pages 455–459 , 1998 .
[ 18 ] J . Xie , D . Zhang , and W . Xu . Spatially adaptive wavelet denoising using the minimum description length principle . IEEE Transactions on Image Processing , 13(2):179–187 , 2004 .
[ 19 ] T . Yoshida , H . Motoda , and T . Washio . Adaptive ripple down rules method based on minimum description length principle . In ICDM , pages 530–537 , 2002 .
157
