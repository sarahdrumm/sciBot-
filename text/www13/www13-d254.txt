A Biterm Topic Model for Short Texts
Xiaohui Yan , Jiafeng Guo , Yanyan Lan , Xueqi Cheng yanxiaohui@softwareictaccn , {guojiafeng , lanyanyan , cxq}@ictaccn
Institute of Computing Technology , CAS
Beijing , China 100190
ABSTRACT Uncovering the topics within short texts , such as tweets and instant messages , has become an important task for many content analysis applications . However , directly applying conventional topic models ( eg LDA and PLSA ) on such short texts may not work well . The fundamental reason lies in that conventional topic models implicitly capture the document level word co occurrence patterns to reveal topics , and thus suffer from the severe data sparsity in short documents . In this paper , we propose a novel way for modeling topics in short texts , referred as biterm topic model ( BTM ) . Specifically , in BTM we learn the topics by directly modeling the generation of word co occurrence patterns ( ie biterms ) in the whole corpus . The major advantages of BTM are that 1 ) BTM explicitly models the word co occurrence patterns to enhance the topic learning ; and 2 ) BTM uses the aggregated patterns in the whole corpus for learning topics to solve the problem of sparse word co occurrence patterns at document level . We carry out extensive experiments on real world short text collections . The results demonstrate that our approach can discover more prominent and coherent topics , and significantly outperform baseline methods on several evaluation metrics . Furthermore , we find that BTM can outperform LDA even on normal texts , showing the potential generality and wider usage of the new topic model .
Categories and Subject Descriptors H33 [ Information Search and Retrieval ] : Information Search and Retrieval ; I53 [ Pattern Recognition ] : Clustering
Keywords Short Text , Topic Model , Biterm , Content Analysis , document clustering
1 .
INTRODUCTION
Short texts are prevalent on the Web , no matter in traditional Web sites , eg Web page titles , text advertisements and image captions , or in emerging social media , eg tweets , status messages , and questions in Q&A websites . Uncovering the topics of such short texts is crucial for a wide range of content analysis tasks , such as content characterizing [ 26 ,
Copyright is held by the International World Wide Web Conference Committee ( IW3C2 ) . IW3C2 reserves the right to provide a hyperlink to the author ’s site if the Material is used in electronic media . WWW 2013 , May 13–17 , 2013 , Rio de Janeiro , Brazil . ACM 978 1 4503 2035 1/13/05 .
35 , 14 ] , user interest profiling [ 32 ] , emerging topic detecting [ 20 ] and so on . However , unlike the traditional normal documents ( eg news articles and academic papers ) , the lack of rich context in short texts makes the topic modeling a challenging problem .
Conventional topic models , like PLSA [ 16 ] and LDA [ 3 ] , are widely used for uncovering the hidden topics from text corpus . In general , documents are modeled as mixtures of topics , where a topic is a probability distribution over words . Statistical techniques are then utilized to learn the topic components and mixture coefficients of each document . In essence , the conventional topic models reveal the latent topics within the text corpus by implicitly capturing the document level word co occurrence patterns [ 5 , 30 ] . Therefore , directly applying these models on short texts will suffer from the severe data sparsity problem ( ie the sparse word co occurrence patterns in each short document ) [ 17 ] . More specifically , 1 ) the occurrences of words in short document play less discriminative role compared to lengthy documents where the model has enough word counts to know how words are related [ 17 ] ; 2 ) The limited contexts make it more difficult for topic models to identify the senses of ambiguous words in short documents .
One simple but popular way to alleviate the sparsity problem is to aggregate short texts into lengthy pseudo documents before training a standard topic model . For example , Weng et al . [ 32 ] aggregated the tweets published by individual user into one document before training LDA . Besides the user based aggregation , Hong et al . [ 17 ] also aggregated the tweets containing the same word , and shown that topic models trained on these aggregated messages work better than the regular LDA . However , such heuristic data aggregation methods are highly data dependent . For example , the user information is not always available in some datasets , like the collection of Web page titles or advertisements . Even if the user information is available , eg in tweets data , most users only have few tweets which makes the aggregation less effective .
Another way to deal with the problem is to make stronger assumptions on the data . A typical way is to assume that a short document only covers a single topic . For example , Zhao et al . [ 35 ] modeled each tweet in the way of mixture of unigrams [ 23 ] . Similar approach can be found in [ 12 ] , where words in each sentence are assumed to be drawn from the same topic . Compared to LDA and PLSA , the simplified data generation process may help alleviate the sparsity problem in short texts . However , it loses the flexibility to capture different topic ingredients in one document , and suf
1445 fers from overfitting issues due to the peaked posteriors of topics P(z|d ) [ 3 ] .
Unlike these approaches , in this paper , we propose a novel topic model for short texts to tackle the sparsity problem . The main idea comes from the answers of the following two questions . 1 ) Since topics are basically groups of correlated words and the correlation is revealed by word co occurrence patterns in documents , why not explicitly model the word co occurrence for topic learning ? 2 ) Since topic models on short texts suffer from the problem of severe sparse patterns in short documents , why not use the rich global word cooccurrence patterns for better revealing topics ?
Specifically , we propose a generative biterm topic model ( BTM ) , which learns topics over short texts by directly modeling the generation of biterms in the whole corpus . Here , a biterm is an unordered word pair co occurred in a short context . The data generation process under BTM is that the corpus consist of a mixture of topics , and each biterm is drawn from a specific topic . Compared with conventional topic models , the major differences and advantages of BTM lie in that 1 ) BTM explicitly models the word co occurrence patterns ( ie biterms ) , rather than documents , to enhance the topic learning ; and 2 ) BTM uses the aggregated patterns in the whole corpus for learning topics to solve the problem of sparse patterns at document level . By learning BTM , we can obtain the topic components and a global topic distribution of the corpus , except the topic distribution of each individual document as it does not model the document generation process . However , we show that the topic distribution of each document can be naturally derived based on the learned model .
We conduct extensive experiments on two real world short text collections , ie the datasets from Twitter and a Q&A website . Experimental results show that BTM can discover more prominent and coherent topics than the baseline methods . Quantitative evaluations confirm the superiority of BTM on several evaluation metrics . Additionally , we also test our approach on a normal text collection , ie 20Newsgroup . It is surprising for us to find that BTM can outperform LDA even on normal texts , showing the potential generality and wider usage of the new topic model .
The rest of the paper is organized as follows : in Section 2 , we give a brief review of related works . Section 3 introduces our model for short text topic modeling , and discuss its implementation in Section 4 . Experimental results are presented in Section 5 . Finally , conclusions are made in the last section .
2 . RELATED WORKS
In this section , we briefly summarize the related work from the following two perspectives : topic models on normal texts , and that on short ones . 2.1 Topic Models on Normal texts
Topic models have been proposed to uncover the latent semantic structure from text corpus . The effort of mining semantic structure in a text collection can be dated from latent semantic analysis ( LSA ) [ 9 ] , which utilizes the singular value decomposition of the document term matrix to reveal the major associative words patterns . Probabilistic latent semantic analysis ( PLSA ) [ 16 ] improves LSA with a sounder probabilistic model based on a mixture decomposition derived from a latent class model . In PLSA , a docu ment is presented as a mixture of topics , while a topic is a probability distribution over words . Extending PLSA , Latent Dirichlet Allocation ( LDA ) [ 3 ] adds Dirichlet priors on topic distributions , resulting in a more complete generative model . Due to its nice generalization ability and extensibility , LDA achieves huge success in text mining domain .
In the last decade , topic models have been extensively studied . Many more complicated variants and extensions of LDA and PLSA have been proposed , such as the authortopic model [ 27 ] , Bayesian nonparametric topic model [ 29 ] , and supervised topic model [ 2 ] . Among them two works close to us are the recently proposed regularized topic model [ 22 ] and the generalized P´olya model [ 21 ] , which also employ word co occurrence statistics to enhance topic learning . However , both of them utilize word co occurrences as structure priors for topic word distribution , rather than directly modeling their generation process . Above all , almost all the models mentioned above deal with normal text without considering the specificity of short texts . 2.2 Topic Models on Short Texts
Early studies mainly focused on exploiting external knowledge to enrich the representation of short texts . For example , Sahami et al.[28 ] suggested a search snippet based similarity measure for short texts . Phan et al.[24 ] learned hidden topics from large external resources to enrich the representation of short texts . Jin et al.[19 ] learned topics on short texts via transfer learning from auxiliary long text data . These ways may be helpful in some specific domains , but not general since favorable external dataset might not be always available . Additionally , these approaches and ours are complementary rather than competitive .
With the emergence of social media in recent years , topic models have been utilized for social media content analysis in various tasks , such as content characterizing [ 26 , 35 ] , event tracking [ 20 ] , content recommendation [ 25 , 8 ] , and influential users prediction [ 32 ] . However , due to the lack of specific topic models for short texts , some researchers directly applied conventional ( or slightly modified ) topic models for analysis [ 26 , 31 ] . Some others tried to aggregate short texts into lengthy pseudo documents based on some additional information , and then train conventional topic models [ 32 , 35 ] . Hong et al . [ 17 ] made a comprehensive empirical study of topic modeling in Twitter , and suggested that new topic models for short texts are in demand .
In our previous works , we developed methods based on non negative matrix factorization for short text clustering [ 34 ] and topic learning [ 33 ] by exploiting global word co occurrence information . This work extends them by proposing a more principle approach to model topics over short texts . To the best of our knowledge , the proposed topic model is the first one focusing on general domain short texts , which does not exploit any external knowledge .
3 . OUR APPROACH
Conventional topic models learn topics based on documentlevel word co occurrence patterns , whose effectiveness will be highly influenced in short text scenario where the word co occurrence patterns become very sparse in each document . To tackle this problem , here we propose a novel biterm topic model , which learns topics over short texts by directly modeling the generation of all the biterms ( ie word co occurrence patterns ) in the whole corpus .
1446 Figure 1 : Graphical representation of ( a ) LDA , ( b ) mixture of unigrams , and ( c ) BTM . Different from LDA and mixture of unigrams , BTM models the generation procedure of biterms in a collection , rather than documents . For clarity , the fixed hyperparameters α , β are not presented .
3.1 Biterm Extraction
Without loss of generality , topics are represented as groups of correlated words in topic models , while the correlation is revealed by word co occurrence patterns in documents . For example , if the words “ apple ” , “ iphone ” , “ ipad ” and “ app ” frequently co occur with each other in the same contexts , we can identify that they belong to a same topic ( ie apple company and its products ) . Conventional topic models implicitly capture such word co occurrence patterns by modeling word generation from the document level . Different from those approaches , our BTM directly models the word cooccurrence patterns based on biterms . A biterm denotes an unordered word pair co occurring in a short context ( ie an instance of word co occurrence pattern ) . Here the short context refers to a proper text window containing meaningful word co occurrences . In short texts , since documents are usually short and specific , we just take each document as an individual context unit . We extract any two distinct words in a short text document as a biterm . For example , in the short text document “ I visit apple store . ” , if we ignoring the stop word “ I ” , there are three biterms , ie “ visit apple ” , “ visit store ” , “ apple store ” . The biterms extracted from all the documents in the collection compose the training data of BTM . 3.2 Biterm Topic Model
The key idea of BTM is to learn topics over short texts based on the aggregated biterms in the whole corpus to tackle the sparsity problem in single document . Specifically , we consider that the whole corpus as a mixture of topics , where each biterm is drawn from a specific topic independently1 . The probability that a biterm drawn from a specific topic is further captured by the chances that both words in the biterm are drawn from the topic . Suppose α and β are the Dirichlet priors . The specific generative process of the corpus in BTM can be described as follows :
1 . For each topic z
( a ) draw a topic specific word distribution φz ∼ Dir(β ) 2 . Draw a topic distribution θ ∼ Dir(α ) for the whole collection
1Strictly speaking , two biterms in a document sharing the same word occurrence are not independent . This simplified assumption facilitate the computation by considering BTM as a model built upon a biterm set .
3 . For each biterm b in the biterm set B
( a ) draw a topic assignment z ∼ Multi(θ ) ( b ) draw two words : wi , wj ∼ Mulit(φz )
Following the above procedure , the joint probability of a biterm b = ( wi , wj ) can be written as :
P ( b ) =
=
P ( z)P ( wi|z)P ( wj|z ) .
θzφi|zφj|z
.
. z z
( 1 )
( 2 )
Thus the likelihood of the whole corpus is : fi
.
P ( B ) =
θzφi|zφj|z
( i,j ) z
We can see that , here we directly model the word cooccurrence pattern , rather than a single word , as an unit conveying semantics of topics . No doubt the co occurrence of a pair of words can much better reveal the topics than the occurrence of a single word , and then enhance the learning of topics . Moreover , all the biterms from the whole corpus , rather than from a single document , are aggregated together for the topic learning . Therefore , we can fully leverage the rich global word co occurrence patterns to better reveal the latent topics .
For better understanding the uniqueness of BTM from conventional topic models , here we make a comparison between BTM and two typical models for topic learning , ie LDA and mixture of unigrams . Figure 1 illustrates the graphical representation of the three models . We can see , in LDA each document is generated by first drawing a documentlevel topic distribution θd , and then iteratively sampling a topic assignment z for each word w in the document . LDA implicitly captures the document level word co occurrence patterns since the topic assignment variable z of each word depends on other words in the same document through sharing the same document level topic distribution θd . Hence , when documents are short , LDA will suffer from the sparsity problem due to its excessive reliance on local observations for the inference of word topic assignment z , which in turn hurts the learning of topics φ .
Different from LDA , mixture of unigrams draws the topic assignment z for each document from a corpus level topic distribution θ . Leveraging the information of the whole corpus , it alleviates the sparsity problem in topic inference ,
1447 which in turn helps the learning the topic components φ . However , mixture of unigrams assumes that all the words in a document are sampled from the same topic . This assumption is so strong that it prevents the model from modeling fine topics in documents . As we can see , even in short texts , there might be multiple topics in one document .
BTM , shown in Figure 1(c ) , overcomes the data sparsity problem of LDA by drawing topic assignment z from the corpus level topic distribution θ as mixture of unigrams does . Meanwhile , it also surmounts the disadvantage of mixture of unigrams by breaking documents into biterms . In this way , BTM not only can keep the correlation between words , but also can capture multiple topic gradients in a document , since the topic assignments of different biterms in a document are independent . 3.3 Inferring Topics in a Document
A major difference between BTM and conventional topic models is that BTM does not model the document generation process . Therefore , we cannot directly obtain the topic proportions of documents during the topic learning process . To infer the topics in a document , we assume that the topic proportions of a document equals to the expectation of the topic proportions of biterms generated from the document :
P ( z|d ) =
P ( z|b)P ( b|d ) .
( 3 ) In Eq ( 3 ) , P ( z|b ) can be calculated via Bayes’ formula based on the parameters estimated in BTM : b
.
P ( z|b ) =
P ( z)P ( wi|z)P ( wj|z ) ' z P ( z)P ( wi|z)P ( wj|z )
, where P ( z ) = θz , and P ( wi|z ) = φi|z . Then the remaining problem is how to obtain P ( b|d ) . Here we simply take the empirical distribution of biterms in the document as the estimation
P ( b|d ) = nd(b)' b nd(b )
, where nd(b ) is the frequency of the biterm b in the document d . In short texts , P ( b|d ) is nearly an uniform distribution over all biterms in the document d . Despite of its simplicity , we find this estimation always obtains good results in practice . More sophisticated ways may be studied in the future work .
4 . PARAMETERS INFERENCE In this section , we describe the algorithm to infer the parameters {φ , θ} in BTM , and compare its complexity with LDA . 4.1 Inference by Gibbs Sampling
Similar as LDA , inference cannot be done exactly in BTM . Hence , we adopt Gibbs sampling to perform approximate inference . Gibbs sampling is a simple and widely applicable Markov chain Monte Carlo algorithm . Compared to other inference methods for latent variable models , like variational inference and maximum posterior estimation , Gibbs sampling has two advantages . First , it is in principal more accurate since it asymptotically approaches the correct distribution . Second , it is more memory efficient since it only requires to maintain the counters and state variables , mak
Algorithm 1 : Gibbs sampling algorithm for BTM
Input : the number of topics K , hyperparameters α , β , biterm set B
Output : multinomial parameter φ and θ initialize topic assignments randomly for all the biterms for iter = 1 to Niter do for b ∈ B do draw zb from P ( z|z−b , B , α , β ) update nz , nwi|z , and nwj|z compute the parameters φ in Eq ( 5 ) and θ in Eq ( 6 ) ing it preferred for large scale dataset . More detailed comparison of these methods can be found in [ 1 ] .
The basic idea of Gibbs sampling is to estimate the parameters alternatively , by replacing the value of one variable by a value drawn from the distribution of that variable conditioned on the values of the remaining variables . In BTM , we need to sample all the three types of latent variables z , φ and θ . However , with the technique of collapsed Gibbs sampling [ 10 ] , φ and θ can be integrated out due to the conjugate priors α and β . Consequently , we only have to sample the topic assignment for each biterm from its conditional distribution given the remaining variables .
To perform Gibbs sampling , we first choose initial states for the Markov chain randomly . Then we calculate the conditional distribution P ( z|z−b , B , α , β ) for each biterm b = ( wi , wj ) , where z−b denotes the topic assignments for all biterms except b , B is the global biterm set . By applying the chain rule on the joint probability of the whole data , we can obtain the conditional probability conveniently : ( nwi|z + β)(nwj|z + β )
P ( z|z−b , B , α , β ) ∝ ( nz + α )
'
( 4 )
,
( w nw|z + M β)2 where nz is the number of times of the biterm b assigned to the topic z , and nw|z is the number of times of the word w assigned to the topic z . Following the conventions of LDA , here we use symmetric Dirichlet priors α and β . Note that once a biterm b is assigned to the topic z , the two words wi and wj in it will be assigned to the topic simultaneously .
Finally , with the counters of the topic assignments of biterm and word occurrences , we can easily estimate the topic word distributions φ and global topic distribution θ as :
φw|z =
θz =
' nw|z + β w nw|z + M β nz + α |B| + Kα
, where |B| is the total number of biterms .
,
( 5 )
( 6 )
An overview of the Gibbs sampling procedure we use is shown in Algorithm 1 . Due to space limitation , we omit the detailed derivation of it . 4.2 Complexity Analysis
The major time consuming part in the Gibbs sampling procedure of BTM is evaluating the conditional probability in Eq ( 4 ) for all the biterms , with time complexity O(K|B| ) . During the entire process , we need to keep the counters nz , nw|z , and the topic assignment z for each biterm , in total of ( K + M K +|B| ) variables in memory . Note that in LDA , we
1448 Table 1 : Time complexity and the number of variables need to be maintained in Gibbs sampling implementation of LDA , mixture of unigrams , and BTM method time complexity
O(K|D|¯l ) O(K|B| )
#variables
|D|K + M K + |D|l K + M K + |B|
LDA BTM
Table 2 : Time cost ( seconds ) per iteration of BTM and LDA on Tweets2011 collection .
K
50
100
150
200
250
LDA 38.07s BTM 128.64s
74.38s 250.07s
108.13s 362.27s
143.47s 476.19 s
178.66s 591.24s
2
' i li/|D| need to draw topic assignment for every word occurrence in documents , which costs time O(K|D|¯l ) , where ¯l = is the average length of documents in the collection . For memory cost , LDA has to maintain the counters nz|b , nw|z , and the topic assignment z for each word occurrences[15 ] , in total of ( |D|K + M K + |D|l ) variables . Table 1 lists the time complexity and variables required to be maintained in the Gibbs sampling procedure of LDA , and BTM . LDA , we approximately rewrite |B| as2 : |B| ≈ |D|¯l(¯l − 1 )
To compare the time and memory cost between BTM and
.
We can see the time complexity of BTM is about ( ¯l − 1)/2 times of LDA . In short texts , the average length of documents are very small , eg ¯l = 5.21 in the Tweets2011 collection , thus the run time of BTM is still comparable with LDA . However , for very large dataset and a large topic number K , LDA is susceptible to memory problems owing to a huge value of |D|K .
Table 2 shows the average run time ( per iteration ) of BTM and LDA in our experiments on the Tweets2011 collection . We find the run time of BTM is always about 3 times of LDA for different topic number K . Table 3 shows the overall memory cost of BTM and LDA in the same collection . We find that memory required by LDA rapidly increases as the topic number K grows , which costs more than 10 times of memory than BTM when K is larger than 200 . As opposed to LDA , memory required by BTM grows very slowly . With further investigation , we find the major part of memory in BTM is used to store the biterms in training dataset . Therefore , BTM is a better choice for large dataset and a large topic number K , when the memory cost is a bottleneck .
5 . EXPERIMENTS
In this section , we conduct experiments on real world short text collections to demonstrate the effectiveness of our proposed approach . We take two typical topic models as our baseline methods , namely LDA and mixture of unigrams .
All the experiments were carried on a Linux server with Intel Xeon 2.33 GHz CPU and 16G memory . Both BTM 2For a document with length l , we generate l(l − 1)/2 biterms . Here we simply take all the documents as with the same length , since the variance of the length of short documents is not large .
Table 3 : Memory cost ( m ) per iteration of BTM and LDA on Tweets2011 collection .
K
50
100
150
200
250
LDA 3177m 5524m 7890m 10218m 12561m BTM 927m 946m 964m 1002m
984m and mixture of unigrams were implemented via C++ code3 . For LDA , we used the open source implementation GibbsLDA++4 . Parameters were tuned via grid search : for LDA , α = 0.05 and on short text collections , and α = 50/K on the normal text collection , β = 0.01 ; for BTM and mixture of unigrams , α = 50/K and β = 001 In all the methods , Gibbs sampling was run for 1,000 iterations . The results reported are the average over 10 runs .
One typical way for topic model evaluation is to compare the perplexity or marginal likelihood on a held out test set [ 3 , 11 , 12 ] . However , since BTM not models the generation process of documents , these measures are not available for us . Moreover , these measures do not reflect the topic quality rightly [ 6 ] . Therefore , we evaluate the performance of BTM on topic modeling on some other task dependent metrics . 5.1 Evaluation on Tweets2011 Collection
To verify the effectiveness of BTM on short texts , we carried experiments on a standard short text collection , namely Tweets20115 . It was published in TREC 2011 microblog track , which provides approximately 16 million tweets sampled between January 23rd and February 8th , 2011 . Besides the complete content of tweets , it also includes an user id , and a timestamp for each tweet . To reduce lowquality tweets , we processed the raw content via the following normalization steps : ( a ) removing non Latin characters and stop words ; ( b ) converting letters into lower case ; ( c ) removing words with document frequency less than 10 ; ( d ) filtering out tweets with length less than 2 ; ( e ) removing duplicate tweets . At last , we left 4,230,578 valid tweets , 98,857 distinct words , and 2,039,877 users . The average document length is 521
We compared BTM with three topic modeling methods on this short texts collection : ( a ) the standard LDA , which takes each tweet as a document ; ( b ) LDA U , which aggregates all the tweets from a user to a big psudo document before training LDA ; ( c ) mixture of unigrams ( denoted as Mix ) , which assumes each tweet only exhibits a single topic . In this collection , we set the number of topics K = 50 for all the methods .
511 Quality of Topics
To investigate the quality of topics discovered by all the test methods , we first sample some topics for visualization . Following [ 7 ] , we randomly drew two topics shared by the topic sets discovered by the four methods . The selection process is described as follows . Firstly , we collected the top 5 words in each topic into a topical word set for each method individually . Then we randomly chose two terms ( ie , job and snow ) from the intersection of the four topical word sets . For each topic , besides the top 20 words , which are
3Code of BTM : http://codegooglecom/p/btm/ 4http://gibbsldasourceforgenet/ 5http://trecnistgov/data/tweets/
1449 most representative for a topic , we also listed 20 non top words ( ie ranked from 1001 to 1020 ) ordered by P ( w|z ) . Ideally , a high quality topic should be coherent as much as possible . Hence , it is expected that the non top words should be relevant to the top words in the same topic .
Table 4 presents the top words ( first row ) and non top words ( second row ) of the topic selected by the word “ job ” . We find the two words “ job ” and “ jobs ” are ranked highest by all the four methods . However , in LDA , some other words , like “ web ” , “ website ” , and “ google ” , are more related to a topic about website , rather than job . The results in LDAU and mixture of unigrams seem a little better than LDA , but still include a few of less relevant words like “ website ” and “ www ” . While in BTM , the top 20 words are more prominent and precise about “ job ” . In the non top words , we find LDA includes the least words about “ job ” , which is hard to connect them to the top words . On the contrary , BTM includes more relevant words about “ job ” than others , suggesting this topic discovered by BTM is more coherent . Table 5 presents the top words ( first row ) and non top words ( second row ) of the topic selected by another word “ snow ” . In the first row , again we can see that the top words in LDA are mixed with words about two different subjects “ weather ” and “ car ” . The results in LDA U is similar to LDA , but more about “ weather ” . In contrast , the top words in mixture of unigrams and BTM clearly describe weather . In the second row , both LDA and LDA U list words almost have no connection to “ snow ” , while some of them are related to “ car ” . For mixture of unigrams , it is hard to explain the topic based on these non top words . In BTM , there are still many words about “ weather ” , like “ temperature ” and “ cyclone ” . Besides the two topics presented here , we also find similar phenomenon in remaining topics , which suggests that the topics discovered by BTM are is more prominent and coherent than the three baselines .
In order to perform more comprehensive analysis , we utilize an automated metric , namely coherence score , proposed by Mimno et al [ 21 ] for topic quality evaluation . Given a topic z and its top T words V ( z ) = ( v(z ) T ) ordered by P ( w|z ) , the coherence score is defined as :
1 , , v(z )
C(z ; V ( z )
) =
T . t . t=2 l=1 log
D(v(z ) l m , v(z ) D(v(z ) l
)
) + 1
, where D(v ) is the document frequency of word v , D(v , v . ) is the number of documents words v and v . co occurred . The coherence score is based on the idea that words belonging to a single concept will tend to co occur within the same documents . It is empirically demonstrated that the coherence score is highly correlated with human judged topic coherence . It must be stressed that the coherence score only is appropriate for measuring frequent words in a topic . Because the frequency of rare words is less reliable .
To evaluate the overall quality of a topic set , we calculated k C(zk ; V ( zk) ) , for the average coherence score , namely 1 K each method . The result is listed in Table 6 , where the number of top words T ranges from 5 to 20 . We find the result is in agreement with previous qualitative analysis . BTM receives the highest coherence score in all the settings , and the superiority is statistically significant ( P value < 0.01 by T test ) . Both LDA U and mixture of unigrams outperform LDA slightly , but the differences are not significant .
'
Table 6 : Average coherence score on the top T words ( ordered by P ( w|z ) ) in topics discovered by LDA , LDA U , mixture of unigrams , and BTM . A larger coherence score means the topics are more coherent . It suggests that BTM outperforms others significantly ( P value < 0.01 by t test ) .
T
5
−236.4 ± 2.0 −1015.7 ± 5.9 −55.0 ± 0.4 LDA LDA U −54.2 ± 0.8 −234.8 ± 1.1 −1009.4 ± 4.4 −233.0 ± 1.4 −1007.6 ± 6.7 −53.8 ± 0.1 Mix BTM −52.4 ± 0.1 −227.8 ± 0.3 −990.2 ± 3.8
20
10
Table 7 : Hashtags used for evaluation , not including the prefix ’#’ . jan25 superbowl sotu wheniwaslittle mobsterworld jobs agoodboyfriend bieberfact glee lfc rhoa itunes thegame celebrity tcyasi americanidol cancer socialmedia jerseyshore photography jp6foot7remix factsaboutboys meatschool libra android sagittarius thissummer tnfisherman sagawards ausopen bears weather jaejoongday skins bfgw fashion pandora realestate teamautism travel nba football marketing design oscars food dating kindle snow obama
512 Quality of Topical Representation of Documents In the Tweets2011 collection , there is no category information for tweets . Manual labeling might be difficult due to the incomplete and informal content of tweets . Fortunately , some tweets are labeled by their authors with hashtags in the form of “ #keyword ” . By investigating the data , we find there are mainly three types of usage of hashtags : ( a ) marking events or topics ; ( b ) defining the types of content , like “ #ijustsayin ” , “ #quote ” ; ( c ) realizing some specified functions , like “ #fb ” means importing the tweet to Facebook in the meanwhile . In our case , only the first type of hashtags are useful . Therefore , we manually chose 50 frequent hashtags in type ( a ) , listed in Table 7 .
Since each hashtag in Table 7 denotes a specific topic labeled by its author , we organized documents with the same hashtag into a cluster . The following evaluation is based on the fact that these clusters should have low intra cluster distances and high inter cluster distances .
Considering topic models as a type of dimension reduction methods , each document can be represented by a vector of posterior distribution of topics : di = [ p(z1|di ) , , p(zk|di) ] .
( 7 )
Then we can measure the distance of two documents by the Jensen–Shannon divergence : dis(di , dj ) =
DKL(di||m ) +
DKL(dj||m ) ,
1 2
1 2
'
2 ( di + dj ) , and DKL(p||q ) = where m = 1 is the Kullback–Leibler divergence . Given a set of clusters C = {C1 , , CK} , we introduce two distance scores i pi ln pi qi
Average Intra Cluster Distance :
IntraDis(C ) =
K . k=1
1 K
⎡ ⎢⎢⎣
. di,dj∈Ck i'=j
⎤ ⎥⎥⎦
2dis(di , dj ) |Ck||Ck − 1|
1450 Table 4 : Topics selected by the word “ job ” on the Tweets collection . The first row lists the top 20 words , while the second row lists non top words ranked from 1001 to 1020 based on P ( w|z ) .
LDA job jobs business web website google design online marketing site blog project manager search www company service sales services post nonprofit gallery announced presence published converting select reps requirement mgr territory recruiters power involved announce poster larry dynamics feeds bristol
LDA U job jobs design manager project web website site business service company hiring www support sales services london blog senior engineer expertise unemployed med iii host educational fort tags apps assignments labor introduction leads github assurance avon manchester starting automotive table
Mixture of unigrams jobs job business marketing social media online web design website manager blog project seo internet sales tips company site hiring understand rep industrial sustainability rankings scholarships stay single campus extra cheap 101 vp relationships beginners colorado compliance face winning mechanical
BTM jobs job manager business sales hiring service services project company senior engineer management marketing nurse office assistant center customer development springfield mlm recruit oil req unemployment processing overview awards recruiters ict finish entrepreneur comp assist 1000 alliance locations patent auditor
Table 5 : Topics selected by the word “ snow ” on the Tweets collection . The first row lists the top 20 words , while the second row lists non top words ranked from 1001 to 1020 based on P ( w|z ) .
LDA snow car weather cold drive storm winter ice road bus driving rain ride traffic cars safe closed due warm train western dmv covering a4 push pulling milwaukee remains pace idiots 95 commuter buick owner cta transmission cyclist flurries camping tyre
LDA U snow weather cold winter ice storm rain stay warm due car closed coming spring drive traffic safe sun blizzard city locations sunset drizzle mississippi interstate residents portland students fireplace letting yuck ton counties signal counting blankets pushed 3pm springfield venture
Mixture of unigrams snow weather cold storm winter ice rain warm degrees stay sun spring safe blizzard coming wind cyclone chicago freezing inches australian thankful station stops groundhogday possibly cleveland traveling sidewalk covering predicting ten grass meant double affect zoo schedule blew causing
BTM snow cold weather early stay ready ice winter storm hour hours weekend warm late coming spring rain tired sun hot temperature cyclone warmth issued colder mood couch snows pre traveling polar outages umbrella filled yawn outage flurries online gloves speed
Average Inter Cluster Distance : ⎡ ⎣ . di∈Ck
K(K − 1 )
Ck,Ck.∈C
InterDis(C ) =
1
. k'=k .
⎤ ⎦
. dj∈Ck . dis(di , dj ) |Ck||Ck.|
The intuition is that if the average inter cluster distance is small compared to the average intra cluster distance , the topical representation of documents agrees well with human labeled clusters ( via hashtag ) . Therefore , we calculate the following ratio to evaluate the quality of one topical representation of documents as [ 4 , 13 ] :
H =
IntraDis(C ) InterDis(C )
.
Given a set of different topical representations of documents , the best one is which minimizes the H score .
Table 8 shows the H score for all the test methods . From the results , we can see that BTM preforms significantly better than other three methods ( P value < 0001 ) LDA U outperforms LDA slightly , implying that aggregating tweets for individual users brings moderate benefit . Although LDA dominates mixture of unigrams on normal texts , it is somehow surprising that the performance of mixture of unigrams outperforms LDA and LDA U substantially in this short text collection . It suggests that the data sparsity problem seriously affects LDA and LDA U , while less influences mixture of unigrams and BTM . However , the H score of mixture of ungirams is still much worse than BTM . With some further analysis , we find the average intra cluster distance of mixture of unigrams is extremely large , owing to its peaked posterior distribution of P ( z|d ) . In other words , mixture of
Table 8 : H score for different methods on the Tweets2011 collection , smaller value is better . The significant levels(P value by t test ) are denoted as 0.1* , 0.01** , 0001*** Method 0.576 ± 0.007 LDA LDA U 0.564 ± 0.011 0.503 ± 0.008 Mix BTM 0.474 ± 0.005 >Mix***>LDA U***>LDA***
>LDA U**>LDA***
Significant differences
H score
>LDA* unigrams fails to recognize the resemblance of many documents .
From the above results , we find the improvement of LDAU over LDA is not so much as shown in [ 17 ] . An explanation for this difference is that there are less tweets posted by an user in average in our dataset than theirs . Figure 2 shows the proportions of users who posted certain number of tweets in the Tweets2011 collection , we find 63.3 % of users posted one tweet , and only 2.1 % of users posted more than 9 tweets . Thus it is not strange that aggregating tweets for individual users has limited affects .
5.2 Evaluation on Question Collection
In order to demonstrate the effectiveness of our approach is domain independent , we evaluated it on another short text collection , called Question collection . This collection includes 648,514 questions crawled from a popular Chinese Q&A website6 . Each question has a category label assigned by its questioner , making it convenient for automatic evalu
6http://zhidaobaiducom
1451 s r e s u f o n o i t r o p o r P
0.8
0.6
0.4
0.2
0
1
2
4
5
6
3 Number of tweets posted
7
8
9 >9
Figure 2 : Proportions of users who posted certain number of tweets in the Tweets2011 collection .
Figure 3 : Classification performance of BTM , mixture of unigrams , and LDA on the Question collection . ation . For pre process , we removed stop words and low frequency words ( ie document frequency is less than 3 ) . The final collection contains 189,080 documents , 26,565 distinct words , and 35 categories . The average length of documents is 394 Note that in this collection , our baselines do not include LDA U , since there is few users whole submitted more than one question .
We performed the evaluation based on document classification . Considering topic model as a way for dimensionality reduction , which reduces a document to a fixed set of topical features P ( z|d ) , we would like to see how accurate and discriminative of the topical representation of documents for classification . We randomly split documents into training and test subsets with the ratio 4 : 1 , and classified them by the linear SVM classifier LIBLINEAR7 . We reported the accuracy on 5 fold cross validation in Figure 3 .
From the results , we can see that BTM always dominates the two baselines . Moreover , the advantage of BTM becomes more notable as the topic number K grows . That is because when the number of topics is small , topics discovered are usually very general . In such case , a short document is more likely to belong to a single topic , thus the performance of BTM is close to mixture of unigrams . In contrast , with the increase of the topic number K , we will learn more specific topics . However , mixture of unigrams is unable to capture the multiple topics exhibited in a document . Thus the difference between BTM and mixture of unigrams becomes larger . At the same time , a large topic number will aggravate the data sparsity problem of LDA by introducing more parameters , thus the gap between BTM and LDA also increases . Another important finding is that mixture of unigrams outperforms LDA all the time . It suggests that LDA is not a good choice for short texts due to the data sparsity problem .
One may wonder the impact of training data size on these methods . We randomly sampled different proportion of documents , from 0.2 to 1 , to train and test these methods separately . The results are shown in Figure 4 . We can see when the size of the training data grows , all the methods work better . However , both BTM and mixture of unigrams achieve more improvement than LDA . LDA only get close to mixture of unigrams on small training data . It suggests that increasing the training data will not overcome the data sparsity problem in LDA , since the documents are still short .
Figure 4 : Classification performance comparison with different data proportions on the Questions collection ( K=40 ) .
Comparing mixture of unigrams with BTM , we find BTM has stable superiority over mixture of unigrams no matter of the size of the training data . 5.3 Evaluation on Normal Texts
In previous experiments , we have demonstrated the effectiveness of BTM on short texts . Although we propose BTM for the short text scenario , there is no limitation for our model to be applied on normal text collections . Therefore , it is also interesting to see how effective is BTM on normal text . For this purpose , we compared BTM with LDA , one of most popular topic models , on a normal text collection . The experiments were carried out on the 20Newsgroup collection8 , a standard corpora including 18,828 messages harvested from 20 different Usenet newsgroups . Each newsgroup corresponding to a different topic . Table 9 lists the names of these newsgroups . For pre process , we removed stop words and words with frequency less than 3 , but without stemming . Finally , 42697 words are left .
We directly trained LDA on the original documents without any other processing . Note that in BTM , we need to extract biterms from the collection . This process is a little different from that in short texts . Recall that a biterm is defined as a word pair co occurred in a short context . It is not appropriate to view a lengthy document as a single short context , since it may involve a wide range of topics . In or
7http://wwwcsientuedutw/˜cjlin/liblinear/
8http://qwone.com/˜jason/20Newsgroups/
1452 Figure 5 : Clustering performance of BTM with different context range thresholds and LDA on the 20 Newsgroups collection ( K = 20 ) .
Table 9 : The newsgroup names in the 20 Newsgroups collection
No . Newsgroup Name
1 2 3 4 5 6 7 8 9 10 alt.atheism comp.graphics composms windowsmisc compsysibmpchardware compsysmachardware compwindowsx misc.forsale rec.autos rec.motorcycles recsportbaseball
No . Newsgroup Name 11 12 13 14 15 16 17 18 19 20 recsporthockey sci.crypt sci.electronics sci.med sci.space socreligionchristian talkpoliticsguns talkpoliticsmideast talkpoliticsmisc talkreligionmisc der to reduce meaningless and noise biterms , the biterm set is constructed by extracting any two words co occur within a context window with range no larger than a predefined threshold r in each document .
531 Quantitative Evaluation
For quantitative evaluation , we compare the clustering performance of BTM and LDA . Document clustering evaluation is a direct way to measure the effectiveness of a topic model without depending on any extrinsic methods . For document clustering , we take each topic as a cluster , and assign each document d to the topic z with highest value of conditional probability P ( z|d ) . Note that we do not know the optimal context range threshold r ahead , therefore , we tested different values of it , and report their results together . We adopt three standard metrics in clustering evaluation as follows . Let Ω = {ω1,··· , ωK} be the set of output clusters , and C = {c1,··· , cP} be P labeled classes of the documents .
• Purity . Suppose documents in each cluster should take the dominant class in the cluster . Purity is the accuracy of this assignment measured by counting the number of correctly assigned documents and divides by the total number of test documents . Formally : purity(Ω , C ) =
1 n
K . max j i=1
|ωi ∩ cj| .
Note that when all the documents in each cluster are with the same class , purity is highest with value of 1 . Conversely , it is close to 0 for bad clustering .
• Normalized Mutual Information(NMI ) . Let I(Ω ; C ) denotes the mutual information between the two partitions Ω and C , NMI penalized I(Ω ; C ) by their entropy H(Ω ) and H(C ) to avoid the value biasing to large number of clusters . Formally : I(Ω ; C ) '
[ H(Ω ) + H(C)]/2 |ωi∩cj|
NMI(Ω , C ) =
'
=
( i,j
|ωi| n log i n |ωi| n +
|ωi||cj| n|ωi∩cj| |cj| n log log ' j
|cj| n )/2
Note that NMI is 1 for perfect match between Ω and C , while 0 if the clustering is random with respect to class membership .
• Adjusted Rand Index(ARI)[18 ] . Consider documents clustering as a series of pair wise decisions . If two documents both in the same class and the same cluster , or both in different classes and different clusters , the decision is considered to be correct , else false . Rand index measures the percentage of decisions that are correct . Adjusted Rand index is the corrected for chance version of Rand index , whose expected value is 0 , while the maximum value is also 1 for exactly match . ff|cj| fi ff fi ]/ fi ff|cj| fi' ff|ωi∩cj| fi ' fi' ff|ωi|
' ff|ωi|
ARI =
' n 2 i,j fi − [ ' fi ff|cj| ] − [ ff|ωi| '
2 i
2 i i
2 j
2 j
2
1 2 [
2
+ j
2 ff fi n 2
]/
The results are shown in Figure 5 . On the whole , it is clear that BTM outperforms LDA significantly when the context range threshold r is between 30 and 60 , suggesting that BTM also performs very well on normal texts . In particular , we find when r = 10 , LDA works better than BTM , implying that the context information utilized by BTM is not enough . As the context range threshold r increases , more word co occurrence patterns are included , which improves the performance of BTM substantially . However , the improvement slows down when the context range threshold r increases from 30 to 60 . An explanation for this behavior is that when the distance between two words increasing , they might be less relevant . At this point , the assumption that the two words in a biterm have the same topic will be less credible . Moreover , a larger context range threshold r will generate much more biterms , which increases the training cost . Therefore , for both effectiveness and efficiency consideration , the context range threshold r should not be too small or too large for normal texts in practice .
1453 Table 10 : Topics discovered from the 20 Newsgroup collection by BTM and LDA ( K=20 ) . “ sim ” in the last column denotes the cosine similarity of the two topics in a row .
BTM ax max g9v b8f a86 1d9 pl 145 3t giz god jesus christ church bible people lord christian key encryption chip clipper keys government system window server display widget set application xterm file space earth launch mission orbit shuttle system solar writes article don ca david uk wrote cs org ax 0d cx 145 ah 34u w7 mv scx uw people don fbi fire children koresh gun batf people don god writes make good point question people government president don make time american disease medical people patients don time writes good drive scsi mac bit card apple system monitor problem image jpeg file graphics images files color data format
1 2 3 4 5 6 7 8 9 10 11 12 13 14 mail university information fax internet list email 15 16 17 18 windows dos file system files run don os pc program 19 20 car don writes cars good ve engine time 00 year team 10 game 55 play players games 20 1993 health men number 10 hiv april study homosexual armenian armenians people war muslims turkish file entry output program build line printf char info
LDA ax max b8f g9v a86 145 1d9 pl 0t 3t god jesus bible christian church christ christians paul key encryption chip clipper government keys public window server set application sun display problem manager space earth nasa gov time system mission launch writes article university uk ca cs michael mail brian 0d cx ah w7 mv sp 17 uw scx air people writes gun fbi fire children article koresh people writes true don religion evidence question god president government people state states rights american medical health disease drug study drugs men cancer windows drive dos card mac system apple scsi disk file image program files bit jpeg color output line graphics ftp software data mail pub computer car cars armenian armenians engine muslims turkish 000 writes year play game good ca insurance scott team games 10 1993 20 15 00 12 93 11 30 don people ve time good ll make things thing doesn information group list book post questions read subject writes price buy sale problem cost power good interested sim 0.99 0.95 0.95 0.93 0.91 0.90 0.86 0.83 0.82 0.80 0.79 0.75 0.74 0.62 0.62 0.61 0.54 0.25 0.15 0.03
532 Qualitative Evaluation
Here we study the quality of topics discovered by the two topic models . In practice , a topic model which finds topics with good readability and accurately reflecting the topical structure of data is preferred . Table 10 presents all the topics learned by BTM and LDA , when the number of topics is set to 20 . These topics from the two methods are matched based cosine similarity using greedy algorithm . For each topic we list its top words ordered by P ( w|z ) . We can see that the topics 1 16 in BTM and LDA are very similar . Comparison Table 9 and Table 10 , we find it is easy to identify the corresponding newsgroup of a topic in topics 1 16 , except topic 1 and topic 7 . For example , topic 2 is with respect to the newsgroup “ socreligionchristian ” . It suggests that both BTM and LDA uncover the inherent topical structure of the collection closely .
We also note that topics 17 20 in Table 10 are very different in BTM and LDA . In BTM , we can still identify that topics 17 20 relate to the newsgroups “ sci.med ” , “ composmswindowsmisc ” , “ talkpoliticsmideast ” , “ composms windowsmisc ” respectively . But in LDA , topic 17 is about numeral , topic 18 is a set of common words , while topics 19 and 20 are with poor interpretability . In our view , the differences between the results of the two models are caused by the following reasons . BTM explicitly model the word cooccurrences in local context , it well captures the short range dependencies between words . Conversely , LDA captures the long range dependencies in documents [ 11 ] , which are less specific than short range ones , resulting in the last four topics more common but less readable .
6 . CONCLUSION & FUTURE WORKS
Topic modeling for short texts is an increasingly important task due to the prevalence of short texts on the Web . Compared to normal documents , short texts lack of word frequency and context information , causing severe sparsity problems for conventional topic models . In this paper , we propose a novel probabilistic topic model for short texts , namely biterm topic model ( BTM ) . BTM can well capture the topics within short texts as it explicitly models the word co occurrence patterns and uses the aggregated patterns in the whole corpus . We carried on experiments on two realworld short text collections and one normal text collection . The results demonstrated that BTM not only can learn higher quality topics , but also more accurately capture the topics of documents than previous methods . Besides , BTM is simple and easy to implement , and also scales up well . All these benefits makes BTM a practicable choice for content analysis on short texts in a wide range of applications .
To the best of our knowledge , we are the first to propose a topic model for general short texts . However , there is still room to improve our work in the future . For example , we would like to find more sophisticated way to estimate the distribution P ( b|d ) , which is uniform in the current work for simplicity . Moreover , it is also interesting to explore the usage of our model in various real world applications , like content recommendation , event tracking , and short texts retrieval , etc .
7 . ACKNOWLEDGEMENTS
This work is funded by the National Natural Science Foundation of China under Grant No . 61202213 , 61203298 , No . 60933005 , No . 61173008 , No . 61003166 , and 973 Program of China under Grants No . 2012CB316303 . We would like to thank the anonymous reviewers for their helpful comments .
8 . REFERENCES [ 1 ] A . Asuncion , M . Welling , P . Smyth , and Y . Teh . On smoothing and inference for topic models . In In Proceedings of the 25th Conference on UAI , 2009 .
[ 2 ] D . Blei and J . McAuliffe . Supervised topic models . In J . Platt , D . Koller , Y . Singer , and S . Roweis , editors , Advances in Neural Information Processing Systems 20 , pages 121–128 . MIT Press , Cambridge , MA , 2008 .
[ 3 ] D . Blei , A . Ng , and M . Jordan . Latent dirichlet allocation . The Journal of Machine Learning Research , 3:993–1022 , 2003 .
[ 4 ] I . Bordino , C . Castillo , D . Donato , and A . Gionis .
Query similarity by projecting the query flow graph . In SIGIR , pages 515–522 . ACM , 2010 .
1454 [ 5 ] J . Boyd Graber and D . M . Blei . Syntactic topic
[ 22 ] D . Newman , E . V . Bonilla , and W . Buntine . models . Technical Report arXiv:1002.4665 , Feb 2010 . [ 6 ] J . Boyd Graber , J . Chang , S . Gerrish , C . Wang , and
D . Blei . Reading tea leaves : How humans interpret topic models . In NIPS , 2009 .
[ 7 ] D . Cai , Q . Mei , J . Han , and C . Zhai . Modeling hidden topics on document manifold . In Proceedings of the 17th ACM conference on Information and knowledge management , pages 911–920 . ACM , 2008 .
[ 8 ] J . Chen , R . Nairn , L . Nelson , M . Bernstein , and
E . Chi . Short and tweet : experiments on recommending content from information streams . In Proceedings of the 28th international conference on Human factors in computing systems , pages 1185–1194 . ACM , 2010 .
[ 9 ] S . Deerwester , S . Dumais , G . Furnas , T . Landauer , and R . Harshman . Indexing by latent semantic analysis . Journal of the American society for information science , 41(6):391–407 , 1990 .
[ 10 ] T . Griffiths and M . Steyvers . Finding scientific topics .
Proceedings of the National Academy of Sciences of the United States of America , 101(Suppl 1):5228–5235 , 2004 .
[ 11 ] T . Griffiths , M . Steyvers , D . Blei , and J . Tenenbaum . Integrating topics and syntax . NIPS , 17:537–544 , 2005 . [ 12 ] A . Gruber , M . Rosen Zvi , and Y . Weiss . Hidden topic markov models . Artificial Intelligence and Statistics ( AISTATS ) , 2007 .
[ 13 ] J . Guo , X . Cheng , G . Xu , and X . Zhu . Intent aware query similarity . In Proceedings of the 20th ACM international conference on Information and knowledge management , pages 259–268 . ACM , 2011 . [ 14 ] J . Guo , G . Xu , X . Cheng , and H . Li . Named entity recognition in query . In SIGIR , pages 267–274 . ACM , 2009 .
[ 15 ] G . Heinrich . Parameter estimation for text analysis .
Technical report , 2005 .
[ 16 ] T . Hofmann . Probabilistic latent semantic indexing . In
SIGIR , pages 50–57 . ACM , 1999 .
[ 17 ] L . Hong and B . Davison . Empirical study of topic modeling in twitter . In Proceedings of the First Workshop on Social Media Analytics , pages 80–88 . ACM , 2010 .
[ 18 ] L . Hubert and P . Arabie . Comparing partitions .
Journal of classification , 2(1):193–218 , 1985 .
[ 19 ] O . Jin , N . Liu , K . Zhao , Y . Yu , and Q . Yang .
Transferring topical knowledge from auxiliary long texts for short text clustering . In Proceedings of the 20th ACM international conference on Information and knowledge management , pages 775–784 . ACM , 2011 .
[ 20 ] C . X . Lin , B . Zhao , Q . Mei , and J . Han . Pet : a statistical model for popular events tracking in social communities . In Proceedings of the 16th ACM SIGKDD , pages 929–938 . ACM , 2010 .
[ 21 ] D . Mimno , H . Wallach , E . Talley , M . Leenders , and
A . McCallum . Optimizing semantic coherence in topic models . In Proceedings of the Conference on Empirical Methods in Natural Language Processing , pages 262–272 . Association for Computational Linguistics , 2011 .
Improving topic coherence with regularized topic models . In Advances in Neural Information Processing Systems 24 , pages 496–504 . 2011 .
[ 23 ] K . Nigam , A . McCallum , S . Thrun , and T . Mitchell .
Text classification from labeled and unlabeled documents using em . Machine learning , 39(2):103–134 , 2000 .
[ 24 ] X . Phan , L . Nguyen , and S . Horiguchi . Learning to classify short and sparse text & web with hidden topics from large scale data collections . In Proceedings of the 17th international conference on World Wide Web , pages 91–100 . ACM , 2008 .
[ 25 ] O . Phelan , K . McCarthy , and B . Smyth . Using twitter to recommend real time topical news . In Proceedings of the third ACM conference on Recommender systems , pages 385–388 , New York , NY , USA , 2009 . ACM .
[ 26 ] D . Ramage , S . Dumais , and D . Liebling .
Characterizing microblogs with topic models . In International AAAI Conference on Weblogs and Social Media , volume 5 , pages 130–137 , 2010 .
[ 27 ] M . Rosen Zvi , T . Griffiths , M . Steyvers , and
P . Smyth . The author topic model for authors and documents . In UAI , 2004 .
[ 28 ] M . Sahami and T . Heilman . A web based kernel function for measuring the similarity of short text snippets . In Proceedings of the 15th international conference on World Wide Web , pages 377–386 . ACM , 2006 .
[ 29 ] Y . W . Teh , M . I . Jordan , M . J . Beal , and D . M . Blei .
Hierarchical dirichlet processes . Journal of the American Statistical Association , 101 , 2004 .
[ 30 ] X . Wang and A . McCallum . Topics over time : a non markov continuous time model of topical trends . In Proceedings of the 12th ACM SIGKDD , pages 424–433 , New York , NY , USA , 2006 . ACM .
[ 31 ] Y . Wang , E . Agichtein , and M . Benzi . Tm lda : efficient online modeling of latent topic transitions in social media . In Proceedings of the 18th ACM SIGKDD , pages 123–131 , New York , NY , USA , 2012 . ACM .
[ 32 ] J . Weng , E . Lim , J . Jiang , and Q . He . Twitterrank : finding topic sensitive influential twitterers . In Proceedings of the third ACM international conference on Web search and data mining , pages 261–270 . ACM , 2010 .
[ 33 ] X . Yan , J . Guo , S . Liu , X . Cheng , and Y . Wang .
Learning topics in short texts by non negative matrix factorization on term correlation matrix . In Proceedings of the SIAM International Conference on Data Mining . SIAM , 2013 .
[ 34 ] X . Yan , J . Guo , S . Liu , X q Cheng , and Y . Wang .
Clustering short text using ncut weighted non negative matrix factorization . In Proceedings of the 20th ACM international conference on Information and knowledge management , pages 2259–2262 , New York , NY , USA , 2012 . ACM .
[ 35 ] W . Zhao , J . Jiang , J . Weng , J . He , E . Lim , H . Yan , and X . Li . Comparing twitter and traditional media using topic models . Advances in Information Retrieval , pages 338–349 , 2011 .
1455
