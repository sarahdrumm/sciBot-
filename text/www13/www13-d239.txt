Attributing Authorship of Revisioned Content
Luca de Alfaro
∗
Computer Science Dept . University of California
Santa Cruz , CA 95064 , USA luca@ucsc.edu
ABSTRACT A considerable portion of web content , from wikis to collaboratively edited documents , to code posted online , is revisioned . We consider the problem of attributing authorship to such revisioned content , and we develop scalable attribution algorithms that can be applied to very large bodies of revisioned content , such as the English Wikipedia .
Since content can be deleted , only to be later re inserted , we introduce a notion of authorship that requires comparing each new revision with the entire set of past revisions . For each portion of content in the newest revision , we search the entire history for content matches that are statistically unlikely to occur spontaneously , thus denoting common origin . We use these matches to compute the earliest possible attribution of each word ( or each token ) of the new content . We show that this “ earliest plausible attribution ” can be computed efficiently via compact summaries of the past revision history . This leads to an algorithm that runs in time proportional to the sum of the size of the most recent revision , and the total amount of change ( edit work ) in the revision history . This amount of change is typically much smaller than the total size of all past revisions . The resulting algorithm can scale to very large repositories of revisioned content , as we show via experimental data over the English Wikipedia .
Categories and Subject Descriptors H53 [ Information Interfaces and Presentation ] : Group and Organization Interfaces—Computer supported cooperative work ; I71 [ Document and Text Processing ] : Document and Text Editing—Version control
Keywords Authorship ; Wikipedia ; revisioned content
1 .
INTRODUCTION
Versioned content is abundant on the Web . Wikipedia , and wikis , constitute the most prominent example , and they account for a large portion of total page views . Blogs with multiple authors , and pages served by content management systems , are another example in which the versioning is
∗The authors are listed in alphabetical order .
Copyright is held by the International World Wide Web Conference Committee ( IW3C2 ) . IW3C2 reserves the right to provide a hyperlink to the author ’s site if the Material is used in electronic media . WWW 2013 , May 13–17 , 2013 , Rio de Janeiro , Brazil . ACM 978 1 4503 2035 1/13/05 .
Michael Shavlovsky Computer Science Dept . University of California mshavlov@ucsc.edu
Santa Cruz , CA 95064 , USA present , but not directly exposed to the viewer . Code is another prominent example of revisioned content , and one that is becoming common on the web , thanks to the success of sites like GitHub , where users can share their code repositories .
We study in this paper the problem of attributing revisioned content to its author , and more generally , to the revision where it was originally introduced . This problem is insteresting for several reasons . The Wikipedia Reuse Policy1 requires people reusing Wikipedia material to either provide a link to the original article and revision history , or to cite the most prominent authors of the content . Furthermore , in the Wikipedia community it is felt that proper content attribution is an important way to acknowledge and reward contributors , and to foster participation and contributions from communities where authorship has been traditionally recognized and rewarded , such as the academic community [ 7 , 12 ] . Tracking the authorship of Wikipedia content is also an important tool in assisting editors , and viewers , in determining the origin of assertions , and analyzing page evolution . In code , as in wikis , authorship tracking is useful to properly reward contributors . Furthermore , authorship tracking can be useful in determining the reason behind implementation choices . Several revisioning systems implement “ blame ” methods , which attribute every line to an author/revision , but this attribution is extremely crude and imprecise , as it cannot cope with blocks of code that are transposed from one location to another , or from one file to another — changes that are common when code is polished or refactored .
At first glance , the attribution problem for revisioned content seems trivial : surely we can simply compare each revision with the previous one , detect any new content , and attribute it to the revision ’s author . Unfortunately , things are not quite so simple . Content in revisioned systems is often deleted , only to be later introduced , and it is important to be able to trace the authorship to the first original introduction . In Wikipedia , the content of pages is frequently removed by vandals , and re instated in subsequent revisions : this is illustrated in Figure 9 , where the periodic dips in page size correspond to content deletions . One way to guard against such attacks is to check whether the most recent revision happens to coincide with one of the previous revisions , in which case , authorship is carried over from the previous revision . However , this ad hoc remedy cannot cope with broader attacks . For instance , attackers could first use
1http://enwikipediaorg/wiki/Wikipedia:Reusing_ Wikipedia_content
343 a fake identity to remove the page contents , then use their main identity to restore the page to its previous contents , except for some small , imperceptible changes that foil the revision equality check : the whole page content would then be attributed to them . The goal of this work is to present algorithms that can be used on Wikipedia , with the resulting authorship information available to visitors . Once authorship information is prominently displayed , attacks that aim at inflating the size of one ’s authorship are likely , prompting our quest for general , robust algorithms . A more general solution also benefits code attribution , since blocks of code are commonly moved from one branch to another , or deleted and later re inserted .
We propose to attribute authorship of revisioned content by comparing the content of the most recent revision , with the entire content of all previous revisions . For every symbol ( word , or character , or token ) in the most recent revision , we compute all statistically significant matches with previous content : these are the matches whose sequence of symbols is rare enough that the match is likely to be due to a shared origin , rather than serendipitous re invention . The symbol is then assigned the earliest possible origin that is compatible with all the matches . We call this approach the earliest plausible attribution approach . We show that earliest plausible attribution yields a more natural content attribution than other approaches , including approaches based on longest matches with previous content , or approaches inspired by the edit analysis work of Tichy [ 14 ] . By comparing new revisions with the full set of previous revisions , the earliest plausible attribution approach achieves resistance to page deletions and vandalism . As Figure 8 ( A0 vs . A1 ) later in the paper indicates , the resulting attribution differs by over 75 % from the attribution computed via comparisons to the most recent revision only , the difference being due chiefly to deletion reinsertion attacks and other vandalism . We introduce efficient algorithms for earliest plausible attribution . If fed all revisions at once , the algorithms can compute the content origin in time proportional to the size of the revision history , which is clearly optimal . More commonly , though , revisions are created and must be analyzed one and a time . A practical implementation must maintain a summary of all past revisions , and process a new revision on the basis of such a summary . We show that the algorithm we propose uses a summary of size proportional to all the past change in the previous revisions — and this change size is typically much smaller than the total revision history size , since a new revision is usually identical to the preceding one except for a few small changes . The algorithm runs in time proportional to the sum of the size of the previous summary , and the size of the most recent revision . Again , since both summary and most recent revision must be read , this is optimal .
Related work . The WikiTrust tool computes a value of reputation for Wikipedia authors and content , as well as the revision where each word was inserted [ 1 , 2 , 3 ] . The attribution algorithm achieves resistance to vandalism by comparing the most recent revision not only with the preceding one , but also with a set of “ reference ” revisions , consisting of recent revisions that either have high content reputation , or that were created by a high reputation author . The approach is fairly effective in practice , but the attribution depends on the reputation computation : there is no independent characterization of the attribution that is computed , and the process is computationally involved . Furthermore , it is not clear how to extend the approach beyond Wikipedia . In [ 3 ] , several text matching algorithms are evaluated for their ability to explain the editing process in Wikipedia . Tichy inspired algorithms [ 14 ] were found to be highly efficient , and as precise as any alternative , for the problem of comparing two revisions . In contrast , in this work we show that for the problem of comparing a revision with all the preceding ones , the earliest plausible attribution yields more efficient algorithms , and arguably more natural results . The problem of attributing Wikipedia content has also been studied in [ 6 ] , where an algorithm based on hierarchical content matching is proposed . When a revision is compared with the preceding revision , matches of large sections of text ( sections , paragraphs , sentences ) are evaluated first , and a finer grained algorithm based on the Python difflib library is used to attribute any remaining content . The resulting attribution is reported to compare favorably with the one computed by WikiTrust .
String matching is a very well studied problem ; see eg [ 9 ] for an in depth overview . The algorithms presented in this paper make use of several results on string matching , including tries and suffix trees . Sophisticated string matching algorithms developed for genetic applications involve a two step process : a coarse alignment is computed between the strings , followed by a finer grained analysis of string differences ( see [ 9 ] again ) . This “ genetic ” approach is resistant to the transcription errors that occur in gene sequencing . The algorithms developed in this paper are based instead on exact matching of short sequences . It is an interesting open question whether the algorithms for attribution of revisioned content could benefit from the genetic approach .
The attribution problem considered in this paper is a special case of information provenance problem . For an overview of information provenance , see eg [ 4 , 5 ] for provenance in databases , and [ 13 , 11 , 8 ] for an overview in a broader context .
Paper organization . After introducing some notation and concepts , we compare in Section 3 conceptual methods of defining attribution , providing justifications for our choice of earliest plausible attribution . In 4 we describe an efficient algorithm for earliest plausible attribution , and we prove that the algorithm is optimal . We present empirical results obtained in the analysis of the English Wikipedia in Section 5 , and we conclude with some discussion of the results and possible future work in Section 6 . All the code and data for the algorithms can be found at https://sitesgooglecom/a/ucscedu/ luca/the wikipedia authorship project .
2 . DEFINITIONS
Revisions . We model revisioned content as a sequence of revisions ρ = ρ0 , ρ1 , ρ2 , . . Each revision ρ consists in a sequence of tokens t0 , t1 , . . . , tm−1 , taken from a set T of tokens , where len(ρ ) = m is the length of ρ . We assume that len(ρ0 ) = 0 , so that ρ0 represents the initial , empty revision that exists before any subsequent revision is created . For ρ = t0 , t1 , . . . , tm−1 , we indicate with ρ[i ] the token ti , and we write ρ[i : j ] for ti , ti+1 , . . . , tj−1 . Depending on the application , the tokens can be individual unicode characters , or they can be words in a text , tokens of a programming language , and so forth . Given a sequence ρ of revisions , a
344 global position is a pair ( n , k ) with n ≥ 0 and 0 ≤ k < len(ρk ) . Matches . A match M = ( n , i , j , n , i , j ) between positions [ ij − 1 ] of revision ρn and positions [ ij − 1 ] of revision ρn , denoted informally ( and more intuitively ) as M = ( ρn[i : j ] = ρn [ i : j] ) , consists of two revisions ρn , ρn , and indices 0 ≤ i < j ≤ len(ρn ) , 0 ≤ i < j ≤ len(ρn ) , such that :
• j − i = j − i > 0 , so that the matched portions have equal length and are non empty ;
• for all 0 ≤ k < j − i , we have ρn[i + k ] = ρn [ i + k ] , so that tokens at corresponding positions of ρn and ρn match .
Given a match M = ( ρn[i : j ] = ρn [ i : j] ) , we denote by len(M ) = j−i its length . We say that a position k is matched by M if i ≤ k < j . For a position k matched by M , we let M ( n , k ) = ( n , k−i+i ) : thus , we think of matches as partial functions between global positions that relate positions filled by equal tokens . We denote by M(ρn , ρm ) the set of all matches between revisions ρn and ρm . We say that a match M = ( ρn[i : j ] = ρn [ i : j ] ) is a sub match of M = ( ρn[ˆß : ˆj ] = ρn [ ˆß : ˆj ] ) if ˆß ≥ i , ˆj ≤ j , and ˆß − i = ˆß − i ; we say that the sub match is proper if at least one of the two inequalities is strict .
Interesting matches . Our interest in matches is due to the fact that a match between a later revision and earlier one may indicate that the content of the later revision originated in the earlier one . Not all matches correspond to a common origin of the content , however . For instance , in English , the two word sequence “ such that ” is very common , and it would be unreasonable to assume that they have been copied from an earlier revision whenever they appear in a later one . In order to use matches to study authorship , we need to distinguish fortuitous matches from those that indicate shared origin . An in depth approach would likely require a probabilistic model of content structure , and of how content propagates from one revision to the next . Such a model could then be used to compute , for each revision token , a probability distribution over the places where the content might have originated .
We follow a simpler , discrete approach , where content is attributed deterministically to a revision of origin . Deterministic attribution leads to efficient algorithms that can scale to very large bodies of content , such as Wikipedia . We also remark that users of authorship information generally expect a deterministic attribution : Wikipedia visitors and editors want to know who wrote what , and copyright is based on deterministic , not probabilistic attribution . Probabilistic attribution algorithms , and the question of their efficient implementation at scale and possible accuracy advantages , remain a topic for future work . Consider a match M = ( ρn[i : j ] = ρk[i : j ] ) between two revisions ρn and ρk , with k < n . To decide whether to attribute the sequence σ = ρ[i ] , ρ[i + 1 ] , . . . , ρ[j − 1 ] to ρn or ρk , we use a rarity function γ : T ∗ → IR+ : intuitively , the larger γ(σ ) is , the more likely it is that the sequence σ in ρn and ρk shares the same origin . We require that a rarity function γ satisfies the following two conditions :
• γ(∅ ) = 0 : the rarity of the empty sequence is zero . • For all σ ∈ T ∗ and all t ∈ T , we have γ(σ ) < γ(σt ) : longer sequence are strictly rarer than shorter ones . pt of each token t , we can take γ(t0 , t1 , . . . , tm ) =m
A simple choice is γ(σ ) = len(σ ) : the rarity of a sequence is equal to its length . More sophisticated rarity functions can be used : for instance , if we know the occurrence probability . Rarity functions based on the occurrence frequency of multitoken sequences could also be used . Given a match M = ( ρn[i : j ] = ρk[i : j] ) , we define its interest γ(M ) = γ(ρn[i ] , ρn[i + 1 ] , . . . , ρn[j − 1 ] ) to be equal to the rarity of the matched sequence of tokens . We define the interesting matches between revisions ρn and ρm , according to the rarity function γ and threshold ∆ , as the set of matches of rarity at least ∆ : i=0
1 pti
M(ρn , ρm | γ ≥ ∆ ) = {M ∈ M(ρn , ρm ) | γ(M ) ≥ ∆} .
We note that if we choose γ = len , the set M(ρn , ρm | len ≥ l ) will consist of all matches between ρn and ρm that have length at least l . Given a position 0 ≤ k < len(ρn ) of revision ρn , we denote by M[k](ρn , ρm | γ ≥ ∆ ) the interesting matches between ρn and ρm that have interest at least ∆ as measured by γ , and that match position k of ρn .
Origin labeling . An origin labeling associates with each token the revision where the token originated . Precisely , an origin labeling Θ for a ( finite or infinite ) sequence ρ = ρ0 , ρ1 , ρ2 , . . . of revisions is a labeling that associates with each global position ( n , k ) of ρ its origin Θ(n , k ) ∈ IN , with Θ(n , k ) ≤ n . If Θ(n , k ) = n , we say that the token ρn[k ] is new in ρn .
3 . CONCEPTUAL ALGORITHMS
In some instances of revisioned content , such as Google Docs , full information about the edit actions by each individual user are available . In this case , the authorship can be computed by observing directly the typing , cutting , pasting , etc , performed by each editor . In many other instances , however , we can observe only the outcome of the editing process , namely , the sequence of revisions produced by the various users . This is the case for Wikipedia , and for code repositories , since the environments where users edit the code are independent from the repositories . In these cases , we must infer authorship after the fact , by comparing the result of the editing with previous content . There is no a priori correct way to infer authorship , as we cannot reconstruct the mental process of the editors to tell whether they are copying or reinventing . One of the contributions of this paper is to introduce the notion of earliest plausible attribution for revisioned content , showing that it leads to plausible attribution in practice . We remark that , even when the actions of users are observable during editing , as in Google Docs , we can never be sure whether editors are retyping a passage , copying it from paper , or reinventing it anew : earliest plausible attribution can thus be a useful notion even when edit actions are observable in detail . In this section we define earliest plausible attribution and we compare it with other attribution methods . The question of efficient implementation will be the subject of the next section . 3.1 Comparison with preceding revision
Algorithm A0 computes the origin of tokens in a revision ρn by comparing the revision with the previous one in the sequence . Given a sequence ρ = ρ0 , ρ1 , ρ2 , . . . of revisions ,
345 with ρ0 = ∅ as the initial empty revision , algorithm A0 computes an origin labeling Θ for ρ proceeding inductively on the revisions . The first revision ρ0 , being empty , has a null labeling . For each subsequent revision ρn , n > 0 , algorithm A0 computes all interesting matches with the preceding revision ρn−1 . Every unmatched token in ρn is assigned an origin label of n . Each matched token is assigned the origin label of the matching position in the previous revision ; if the token had multiple matches to different positions , the token is assigned the minimum of the origin labels of the corresponding positions .
Algorithm A0 Matches with previous revision . Input : A sequence ρ = ρ0 , ρ1 , ρ2 , . . . , ρm of revisions , with ρ0 = ∅ , along with a rarity function γ and a threshold ∆ . Output : An origin labeling Θ for ρ . for all positions 0 ≤ k < len(ρn ) of ρn do
Let M := M[k](ρn , ρn−1 | γ ≥ ∆ ) . if M = ∅ then Θ(n , k ) := minM∈M Θ(M ( n , k) ) .
1 : for revisions n = 1 , 2 , 3 , . . . do 2 : 3 : 4 : 5 : 6 : 7 : 8 : 9 : 10 : end for end if end for
Θ(n , k ) := n else
One may conceive a variant algorithm , termed Algorithm A0M , where only the most interesting match(es ) for each token are considered : the idea being that the longer the match , the more likely it is to correspond to origin . Algorithms A0 and A0M may yield different labelings , as illustrated in Figure 1 . In the figure , we use sequence length as the rarity function , together with a threshold of 3 , so that matches that are 3 or more tokens are considered interesting . In labeling symbols b c in ρ3 , Algorithm A0 considers two interesting matches : ( ρ3[0 : 3 ] = ρ2[0 : 43 ) , involving a b c , and ( ρ3[1 : 6 ] = ρ2[4 : 9] ) , involving b c z z z . The first match yields origin 1 1 for b c , the second 2 2 . The origin assigned by A0 is the least of these two , namely , 1 1 . Algorithm A0M , on the other hand , considers only the second match , as it is longer , and assigns to b c origin 2 2 . This example highlights why we prefer to consider all interesting matches , rather than just the longest ones : even though a b c in ρ3 matches a b c in ρ1 , it is assigned origin 1 2 2 according to A0M . We take the point of view that a match that is interesting ( with a matched sequence of tokens that is sufficiently rare ) denotes a common origin of the content . If there is more than one interesting match for a token position , we look at all such interesting matches as possible explanations for the origin of the content , and we err on the side of the oldest possible attribution , yielding the min in line 7 of Algorithm A0 . 3.2 Earliest plausible attribution
Algorithm A0 ( and A0M ) relies on comparisons with the immediately preceding revision only . In many relevant examples of versioned content , content can be deleted from one revision only to reappear several revisions later . For instance , the content of Wikipedia pages is frequently deleted by vandals . If authorship is determined via a comparison with the immediately preceding revision only , then an edi
Algorithm A0M Origin via most interesting matches with previous revision . Input : A sequence ρ = ρ0 , ρ1 , ρ2 , . . . , ρm of revisions , with ρ0 = ∅ , along with a rarity function γ and a threshold ∆ . Output : An origin labeling Θ for ρ . for all positions 0 ≤ k < len(ρn ) of ρn do
Θ(n , k ) = n
Let M := M[k](ρn , ρn−1 | γ ≥ ∆ ) . if M := ∅ then Let ( cid:102)M = arg maxM∈M γ(M ) . Θ(n , k ) := minM∈(cid:102)M Θ(M ( n , k) ) .
1 : for revisions n = 1 , 2 , 3 , . . . do 2 : 3 : 4 : 5 : 6 : 7 : 8 : 9 : 10 : 11 : end for end if end for else
ρ3 :
ρ2 :
ρ1 :
ρ3 :
ρ2 :
ρ1 : a1 b1 c1 z2 z2 z2 a1 b1 c1 x2 b2 c2 z2 z2 z2 a1 b1 c1
( a ) A0 a1 b2 c2 z2 z2 z2 a1 b1 c1 x2 b2 c2 z2 z2 z2 a1 b1 c1
( b ) A0M
Figure 1 : A sequence of revisions , with origin labeled according to algorithms A0 and A0M . We represent each revision by its list of tokens , using letters to denote tokens . The origin labels are computed for a rarity function equal to sequence length , and threshold of 3 . We write above every token the origin that the algorithm assigns to it . tor who restores the contents of a Wikipedia page after it is deleted would be attributed the authorship of all the restored content . As these periodic acts of vandalism that destroy most of a page ’s content are common on Wikipedia , authorship algorithms that are based only on comparisons with the immediately preceding revision will grossly misattribute content , as we will show experimentally in Section 5 .
Our preferred algorithm for attribution of revisioned content , Algorithm A1 , compares the latest revision with all the previous revisions , looking for matches with any prior content , rather than just content in the immediately preceding revision . We call this process earliest plausible attribution , since the attribution it produces is the earliest that is compatible with an explanation by interesting matches . Figures 2 and 3 provides a comparison of algorithm A0 and A1 in presence of a delete and restore attack , as common on Wikipedia , and of a more complex attack involving content that is deleted , then gradually re instated . 3.3 Tichy based matching
One of the better known algorithms for generating edit differences between revisions is due to Tichy [ 14 ] . Since the Tichy algorithm performs well in explaining the edit his
346 Algorithm A1 Origin via interesting matches with all preceding revisions . Input : A sequence ρ = ρ0 , ρ1 , ρ2 , . . . , ρm of revisions , with ρ0 = ∅ , along with a rarity function γ and a threshold ∆ . Output : An origin labeling Θ for ρ . for all positions 0 ≤ k < len(ρn ) of ρn do 0<m<n M[k](ρn , ρm | γ ≥ ∆ ) .
Let M := if M = ∅ then Θ(n , k ) := minM∈M Θ(M ( n , k) ) .
1 : for revisions n = 1 , 2 , 3 , . . . do 2 : 3 : 4 : 5 : 6 : 7 : 8 : 9 : 10 : end for end if end for
Θ(n , k ) := n else
ρ4 : a4 b4 c4 x4 f4 g4 h4 ρ3 : p3 q3 ρ2 : a1 b1 c1 x2 f1 g1 h1 ρ1 : a1 b1 c1 f1 g1 h1
( a ) A0
ρ4 : a1 b1 c1 x2 f1 g1 h1 ρ3 : p3 q3 ρ2 : a1 b1 c1 x2 f1 g1 h1 ρ1 : a1 b1 c1 f1 g1 h1
( b ) A1
Figure 2 : A sequence of revisions , with origin labeled according to algorithms A0 and A1 , with rarity equal to length and threshold 3 . This sequence illustrates a delete and restore event , common on Wikipedia . tory of Wikipedia [ 3 ] , it is of interest to adapt it to origin computation and compare it to A1 . Given a revision ρn = t0 , t1 , . . . , tm−1 , the Tichy based Algorithm A2 searches revisions ρ0 , . . . , ρn−1 for the longest prefix of t0 , t1 , . . . , tm−1 . If this longest prefix is , say , t0 , t1 , . . . , tk , with k ≤ m − 1 and γ(t0 , t1 , . . . , tk ) > ∆ for the chosen rarity function γ and threshold ∆ , then the algorithm fixes the origin of t0 , t1 , . . . , tk in ρn according to the origin of the matching tokens ( taking the minimum , in case the longest prefix appears multiple times ) . The algorithm then proceeds searching for the longest prefix of the remaining unlabeled portion tk+1 , tk+2 , . . . , tm−1 . If no longest prefix can be found , or if the longest prefix from t0 has rarity below the threshold , then t0 is labeled as having origin n , or Θ(n , 0 ) := n , and the search continues from the remaining unlabeled portion t1 , t2 , . . . , tm−1 . The process continues until the whole of ρn has been labeled according to its origin .
Figure 4 compares the origin labelings computed by Algorithms A1 and A2 . We see that Algorithm A2 attributes to the tokens c d a m in ρ4 origins 2 2 4 4 , even though these tokens constituted the first revision ρ1 . The attribution 1 1 1 1 computed by A1 seems more appropriate . 3.4 Properties Given a sequence of revisions ρ0 , ρ1 , ρ2 , . . . and two origin labelings Θ , Θ , we write Θ ≤ Θ if Θ(n , k ) ≤ Θ(n , k ) at all positions n , k of the sequence ; we write Θ < Θ if Θ ≤ Θ , and if there is at least a position ( n , k ) where Θ(n , k ) < Θ(n , k ) . The following property establishes that , among A0 , A0M , and A1 , Algorithm A1 computes the earliest attribution and A0M the latest .
ρ6 :
ρ5 :
ρ4 :
ρ3 :
ρ2 :
ρ1 :
ρ6 :
ρ5 :
ρ4 :
ρ3 :
ρ2 :
ρ1 : a4 b4 c4 d4 e6 x6 w6 g6 h6 l6 q3 r3 a4 b4 c4 d4 f5 g5 p3 q3 r3 a4 b4 c4 d4 p3 q3 r3 a1 b1 c1 d1 e1 x2 f1 g1 h1 l1 a1 b1 c1 d1 e1 f1 g1 h1 l1 m1
( a ) A0 a1 b1 c1 d1 e1 x2 w6 g1 h1 l1 q3 r3 a1 b1 c1 d1 f5 g5 p3 q3 r3 a1 b1 c1 d1 p3 q3 r3 a1 b1 c1 d1 e1 x2 f1 g1 h1 l1 a1 b1 c1 d1 e1 f1 g1 h1 l1 m1
( b ) A1
Figure 3 : A sequence of revisions , with origin labeled according to algorithms A0 and A1 , with rarity equal to length and threshold 3 . In this sequence , content is first deleted and replaced with spam , then almost entirely restored .
ρ4 :
ρ3 :
ρ2 :
ρ1 : a3 b3 c1 d1 a1 m1 a3 b3 c2 d2 g2 h2 c2 d2 g2 h2 c1 d1 a1 m1
( a ) A1
ρ4 :
ρ3 :
ρ2 :
ρ1 : a3 b3 c2 d2 a4 m4 a3 b3 c2 d2 g2 h2 c2 d2 g2 h2 c1 d1 a1 m1
( b ) A2
Figure 4 : A sequence of revisions , as labeled by Algorithms A1 and A2 with rarity equal to length and threshold 3 .
Property 1 . Let ΘA0 , ΘA0M , and ΘA1 be origin labelings computed by Algorithms A0 , A0M , and A1 respectively for a sequence of revisions . Then , ΘA1 ≤ ΘA0 ≤ ΘA0M . Moreover , there are sequences of revisions for which each of two above inequalities is strict .
Proof . The weak inequalities follow from the fact that , in deriving the label of a token , the matches considered by A0M are a subset of those considered by A0 , which are in turn a subset of those considered by A1 . The fact that the inequalities can be strict is witnessed by Figure 1 and 2 .
If a portion of content with rarity above the threshold occurs twice in the revision history , Algorithm A1 will assign to the later occurrence an origin that is no later than that assigned to the earliest occurrence , and that can in fact be strictly smaller . a
Property 2 . sequence
Consider revisions ρ0 , . . . , ρi , . . . , ρk , and a match M = ( ρi[l : m ] = ρk[l : m ] ) that is sufficiently interesting . Let Θ be the origin labeling computed by A1 . Then , Θ(k , m + j ) ≤ Θ(i , m + j ) for all 0 ≤ j < m − l , and there are cases where the inequality can be strict . of
347 Algorithm A2 Origin via Tichy matching with all preceding revisions . Input : A sequence ρ = ρ0 , ρ1 , ρ2 , . . . , ρm of revisions , with ρ0 = ∅ , along with a rarity function γ and a threshold ∆ . Output : An origin labeling Θ for ρ . k := 0
1 : for revisions n = 1 , 2 , 3 , . . . do 2 : 3 : while k < len(ρn ) do 4 : tk , tk+1 , . . . , tlen(ρn)−1 .
Search in ρ0 , . . . , ρn for the longest matching prefixes of Let tk , . . . , tm be the longest matched prefix , and let A = {(n1 , k1 ) , . . . , ( np , kp)} be the ( possibly empty ) set of pairs where the longest matches occur . if A = ∅ ∧ γ(tk , . . . , tm ) ≥ ∆ then for i ∈ {0 , 1 , . . . , m − k} do
Θ(n , k + i ) := min1≤j≤p Θ(nj , kj + i )
5 : 6 : 7 : 8 : 9 : 10 : 11 : 12 : 13 : 14 : 15 : end for else end if end while end for k := m + 1
Θ(n , k ) := n k := k + 1
ρ5 :
ρ4 :
ρ3 :
ρ2 :
ρ1 : a3 b3 c1 d1 g2 h2 a3 b3 c1 d1 a1 m1 a3 b3 c2 d2 g2 h2 c2 d2 g2 h2 c1 d1 a1 m1
Figure 5 : A sequence of revisions , as labeled by Algorithm A1 with rarity equal to length and threshold 3 . Note that ρ5 = ρ3 , yet the origin labels for some tokens in ρ5 are smaller than the corresponding ones in ρ3 .
Proof . The result follows from the fact that the matches for ρk[l : m ] include ρi[l : m ] . The fact that the inequality can be strict is illustrated in Figure 5 .
This property formalizes the robustness to attacks of Algorithm A1 . Consider a good revision ρi of a page . If vandals produce revisions ρi+1 , ρi+2 , . . . , ρk−1 , and the page is then restored to its good state ρk = ρi , the property ensures that no content in ρi = ρk becomes attributed to an author of ρi+1 , ρi+2 , . . . , ρk−1 , ρk.2
4 . EFFICIENT ALGORITHMS
In the previous section , we presented various conceptual algorithms for attributing origin to versioned content . The algorithms presented there are extremely inefficient , and have conceptual value only . In this section , we examine the question of efficient implementation for these conceptual algorithms .
2This is true provided that the content of ρk forms an interesting match with the content of ρi . This can be ensured by adding start and end markers to the text of revisions , and by considering interesting any match that contains such markers . algoirthms is |ρ| =n
Input size and change size . Given a sequence of revisions ρ = ρ0 , ρ1 , . . . , ρn , the input size for our attribution i=0 len(ρi ) ( assuming that tokens can be represented in constant space ) . In revisioned content , it is often the case that only a small portion of the content is modified at each revision , so that consecutive revisions differ only in a few tokens . It is thus insightful to study the performance of the algorithms not only as a function of the size of the input , but also as a function of the size of the change that occurred . To this end , given two consecutive i=1 |γi| , where β1 , . . . , βk and γ1 , . . . , γm are the shortest sequences so that we can write : revisions ρ , ρ , we define ∆(ρ , ρ ) = m i=1 |βi| +m
ρ = α0β1α1β2α2 ··· βnαn = α0γ1α1γ2α2 ··· γnαn
ρ
In other words , we write ρ and ρ as composed of maximal sequences of unchanged portions of text α0 , . . . , αm , and of portions β1 , . . . , βm in ρ that will be replaced by sequences γ1 , . . . , γm in ρ . We then define the change size change(ρ ) of ρ0 , ρ1 , . . . , ρn as change(ρ ) =n−1 i=0 ∆(ρi , ρi+1 ) .
Summary size and one revision update . Revisioned content is produced , as the name implies , one revision at a time . When computing the origin of the tokens in the newest revision ρn , it would be impractical to read and re process all previous revisions ρ0 , . . . , ρn−1 . Practical algorithms rely on a summary Sn−1 of ρ0 , . . . , ρn−1 , containing all the information that the algorithm needs to know about the preceding revisions to attribute later revisions . The algorithms compute the origin labeling for ρn on the basis of Sn−1 and ρn , producing as output both Sn and the origin labeling for ρn . We refer to this computation as the one revision update . We will thus study how the summary size , and the running time for the one revision update depend on the input size and change size . 4.1 Algorithm A3
Consider a fixed a rarity function γ and a rarity threshold ∆ . We say that a sequence of tokens t1 , t2 , . . . , tn is minimally interesting if γ(t1 , t2 , . . . , tn ) ≥ ∆ , and at least one of γ(t2 , . . . , tn ) < ∆ or γ(t1 , . . . , tn−1 ) < ∆ holds . When the rarity function is simply the number of tokens , and the rarity threshold ∆ is an integer , then the minimally interesting sequences are the sequences consisting of ∆ tokens . We say that a match M = ( ρn[i : j ] = ρn [ i : j ] ) is minimally interesting if ρn[i ] , ρn[i+1 ] , . . . , ρn[j−1 ] is minimally interesting . To obtain an efficient implementation of Algorithm A1 , we start from the observation that in Step 3 of Algorithm A1 , we need to consider only minimally interesting matches .
If in Step 3 of Algorithm A1 the set M is
Lemma 1 . limited only to minimally interesting matches , the labeling computed by the algorithm is unchanged .
Proof . For a token tk of ρn , let M be a match realizing the minimum in Step 7 of Algorithm A1 , and let ti , . . . , tj be the matched sequence , with i ≤ k ≤ j . If M is minimally interesting , the result holds . If M is not minimally interesting , then both sub matches for ti , . . . , tj−1 and ti+1 , . . . , tj are interesting , and tk belongs to one of them . Continuing in this fashion , we can find a submatch M of M that contains tk and that is minimally interesting . Since tk would be assigned the same origin under M or M , the result holds .
348 Algorithm A3 Implementation of A1 in terms of tries . Input : A sequence ρ = ρ0 , ρ1 , ρ2 , . . . , ρm of revisions , with ρ0 = ∅ , along with a rarity function γ and a threshold ∆ . Output : An origin labeling Θ for ρ . 1 : T := ⊥ 2 : for revisions n = 1 , 2 , 3 , . . . do 3 : 4 : 5 : 6 : end for for all minimally interesting sequences tk , . . . , tm of ρn do for all positions 0 ≤ k < len(ρn ) of ρn do
Θ(n , k ) := n if tk , . . . , tm ∈ T then
7 : 8 : 9 : 10 : 11 : 12 : 13 : 14 : else
15 : 16 : 17 : 18 : 19 : 20 : 21 : 22 : end for end if end for ik , . . . , im := ( tk , . . . , tm ) for all j ∈ [ k , . . . , m ] do
Θ(n , j ) := min{Θ(n , j ) , ij} end for end if end for for all minimally interesting sequences tk , . . . , tm of ρn do if tk , . . . , tm ∈ T then
( tk , . . . , tm ) := Θ(n , k ) , . . . , Θ(n , m ) T := Ins(T ; tk , . . . , tm ) ( tk , . . . , tm ) := Θ(n , k ) , . . . , Θ(n , m ) minimally interesting match for which the minimum in Line 7 is realized ( this exists , due to Lemma 1 ) . By induction hypothesis , the trie T will contain the sequence ρn [ i : j ] with its minimal labeling , in which the token ρn[k ] is labeled with origin p . Thus , Algorithm A3 in Steps 3–13 will assign to ρn[k ] an origin no larger than p .
• Conversely , assume that Algorithm A3 assigns origin p to token ρn[k ] . Then , T must have contained a minimally interesting sequence ρn[j : l ] = tj , . . . , tl−1 , for j ≤ k < l , where tk is labeled by p . By inducton hypothesis , p is the minimal label of tk in all occurrences of tj , . . . , tl−1 in ρ0 , . . . , ρn−1 , indicating that Algorithm A1 also labels ρn[k ] with label no greater than p .
Second , we show that once ρn is processed by A3 , the induction hypothesis holds also for ρ0 , . . . , ρn . Consider a minimally interesting sequence σ occurring in ρn ( the situation of minimally interesting sequences not occurring in ρn is unchanged ) . The arguments in the first part of this proof ensure that once Steps 3–13 have terminated , the sequence σ in ρn is labeled according to its minimal labeling . Steps 14– 21 ensure then that the sequence σ is present in the trie T , and is labeled in it according to its minimal labeling . This completes the induction step .
The following theorem characterizes the time and space requirements for Algorithm A3 .
Theorem 2 .
If there is an integer M such that all token sequences of length at least M are interesting , then given a
Figure 6 : Trie resulting after processing revisions ρ1 , ρ2 , ρ3 as in Figure 4 .
This result suggests implementing Algorithm A1 in terms of a trie . A trie T is a tree whose edges are labeled with tokens , and such that the edges outgoing from a node are labeled by distinct tokens . We say that a sequence of tokens t1 , t2 , . . . , tm belongs to the trie T , written t1 , t2 , . . . , tm ∈ T , if there is a path from the root labeled with the sequence , and we use the sequence to refer to the node where the path ends . If the sequence t1 , t2 , . . . , tm is minimally interesting , we say that the corresponding node is minimally interesting . If γ = len and ∆ is an integer , the minimally interesting nodes are those at depth ∆ in the trie . We denote by ⊥ the empty trie consisting only of a root node , and we denote by Ins(T ; t1 , . . . , tm ) the result of creating a path labeled by t1 , . . . , tm in T in the trie . In the implementation of A1 , we use tries to represent all the minimally interesting sequences of tokens that have occurred in past revisions . Each minimally interesting node t1 , . . . , tm of the trie is labeled with the origin k1 , . . . , km = ( t1 , . . . , tm ) of the sequence of tokens t1 , . . . , tm . This yields Algorithm A3 .
Figure 6 illustrates the trie resulting after processing revisions ρ1 , ρ2 , ρ3 as in Figure 4 , for a rarity function equal to length , and threshold 3 . The leaf nodes are the minimally interesting nodes . To save space in the trie , we omit the noninteresting nodes that have a single child , concatenating the labels of the edges leading into and out of such nodes .
The following theorem shows that Algorithms A3 and A1 compute the same origin labels .
Theorem 1 . Algorithm A3 computes the same origin labels as Algorithm A1 .
To state the proof of this theorem , consider a sequence σ = t1 , . . . , tk occurring at least once in a set of revisions ρ0 , . . . , ρm that has been labeled according to its origin by Algorithm A1 . For 1 ≤ j ≤ k , let pj be the minimum label that token tj is assigned in any of these occurrences . We say that p1 , . . . , pk is the minimal labeling of σ in ρ0 , . . . , ρm .
Proof . The proof proceeds by induction , using the after processing revisons inductive hypothesys ρ0 , . . . , ρn , the trie T contains exactly all the minimally interesting sequences occurring in ρ0 , . . . , ρn , each labeled with its minimal labeling in ρ0 , . . . , ρn . that ,
Assume that Algorithm A3 has processed ρ0 , . . . , ρn−1 al ready , and is processing ρn .
First , we show that this inductive hypothesis implies that algorithms A1 and A3 produce the same labeling . There are two directions to the argument .
• Assume that Algorithm A1 assigns origin label p to token ρn[k ] . Let M = ( ρn[i : j ] = ρn [ i : j ] ) be the rootc d d 3 3 2a b c 3 2 2b c d 1 1 11 1 1a 2 2 2 g a m 2 2 2 g h 349 sequence ρ0 , ρ1 , ρ2 , . . . of revisions , Algorithm A3 can perform a one revision update for revision ρn using a summary of size O(change(ρ0 , . . . , ρn−1) ) , and in time O(len(ρn) ) .
Proof . Algorithm A3 uses as summary for ρ0 , . . . , ρn−1 the trie Tn−1 resulting from the processing of these revisions . To prove the space requirement , we can prove by induction over n that |Tn| ≤ K · change(ρ0 , . . . , ρn ) , for some fixed K ≥ 0 . Note that M is a bound for the length of any minimally interesting sequence : in fact , any interesting sequence σ longer than M has its leftmost M tokens , and rightmost M tokens , also form interesting sequences , contradicting the minimality of σ . Let K = M ( M + 1)/2 be the maximum number of sequences of length at most M that contain a given position . Note that a single insertion or deletion going from ρn−1 to ρn affects at most K minimally interesting sequences in ρn . Therefore , at most K · ∆(ρn−1 , ρn ) new minimally interesting sequences are going to be inserted in Tn−1 in order to obtain Tn . This leads to the space bound for the summary .
To prove the time bound for the processing of ρn , it suffices to note that there are at most len(ρn ) minimally interesting matches involving ρn , and that processing each one of them ( including accessing the trie for retrieving the minimal labeling of any match ) takes constant time ( the trie has depth at most K ) .
Note that the theorem implies that the processing of a sequence ρ0 , . . . , ρn of revisions can be done in time O(|(|ρ0 , . . . , ρn) ) .
If the rarity of a sequence of tokens is taken to be its length , then trivially all sequences longer than the rarity threshold are rare . Another case when the length of minimally interesting sequences of tokens is bounded is when the rarity of a sequence of tokens t1 , . . . , tk is computed for some token probabilities 0 ≤ pki ≤ 1 , and if there is an upper bound c < 1 for the probability of any token . as γ(t1 , . . . , tk ) = k i=1
1 pki
These results suggest that Algorithm A3 is optimal : it is not possible to label a sequence of revisions in time less than the input size , and it is not possible to label a new revision storing less information about the past than all change that has occurred ( except if compression techniques are used ; such techniques can also be applied to the representation of our trie summaries ) .
In large scale implementations of origin analysis , the summary of a revision sequence cannot be stored permanently in memory : rather , it must be read from persistent storage ( such as a database ) before the algorithm analyzes a new revision , and written back to persistent storage once the analysis is done . If the time to read and write the summary is included , then the time required for analyzing revision ρn of sequence ρ0 , ρ1 , ρ2 , . . . is in O(len(ρn ) + change(ρ0 , . . . , ρn) ) . 4.2 Tichy matching
The Tichy based Algorithm A2 is defined in terms of longest common matches . We can obtain an efficient implementation in terms of suffix trees , which provide the most time efficient implementation of the longest common substring problem [ 9 ] . A suffix tree is a tree like data structure that can represent all the suffixes ai , ai+1 , ai+2 , . . . , am , 0 ≤ i < m , of a given string a0 , a1 , . . . , am ; they can be constructed in time linear in the length of the string [ 16 , 10 , 15 ] . The construction of suffix trees can be adapted so that Sn−1 is a suffix tree representing all the suffixes of ρ0 , ρ1 , . . . , ρn−1 ; see [ 9 ] for similar adaptations . The origin information can be associated with the suffix tree in similar fashion to what was done for the trie ; we omit the details to conserve space . The drawback of this algorithm , compared to A3 , is that the size required by the summary is proportional to the size of all previous revisions , rather than to the change size . This because a change involving a token in the middle of a revision of length m gives rise to m/2 new suffixes on average , each of which corresponds to at least one new suffix tree node .
Theorem 3 .
Let M = |ρ0 , . . . , ρn| and D = change(ρ0 , . . . , ρn ) . The suffix tree based implementation of Algorithm A2 produces the origin labels for revision ρn in time O(M ) ; the time for labeling the complete sequence ρ0 , . . . , ρn is O(M 2 ) . There are some examples of input for which the running time for ρn exceeds K · D , for any K ≥ 0 , so that the running time is not O(D ) . The size of Sn is O(M ) , and is not in O(D ) .
Proof . The space and time results are a consequence of the results on the construction of suffix trees [ 16 , 10 , 15 , 9 ] . The existence of sequences in which the summary size is proportional to the entire input size , rather than to the change size , follows from the fact that changing a single token in a revision of length m leads to the creation of a number of new suffixes that is proportional to m ( on average , equal to m/2 ) . These new suffixes must be represented in the suffix tree , so that Sn is in O(|ρ0 , . . . , ρn| ) but not in O(change(ρ0 , . . . , ρn) ) .
5 . EXPERIMENTAL RESULTS
We have produced a robust , scalable implementation of Algorithm A3 that can be applied to very large wikis , including the English Wikipedia . Each revision is parsed in a sequence of tokens , which consists of white space separated sequences of non whitespace characters : tokens thus loosely correspond to words . This tokenization step could be improved by considering the structure of the MediaWiki markup language . We do not use individual ( unicode ) characters as our unit of tokenization , for two reasons . First , using words as attribution units tends to produce more natural results , since contributors typically create or rearrange content in word units ; word level attribution is also easier to display via coloring or other visual cues . Second , using individual characters as tokens would lead to a larger size for the trie summary , as the trie would grow deeper .
The algorithm uses as rarity function the length of a token sequence , and a configurable threshold . The algorithm does not use a rarity function that depends on token ( word ) frequency , chiefly to save space by avoiding the need to store the frequency of a large number of words ; we may revisit this decision at a later time . For each wiki page P , the algorithm stores in persistent storage the pair ( n,Tn ) , consisting of the index n of the last revision of P that has been processed , along with the labeled trie Tn representing the summary . When a new revision ρm for P is produced , with m > n , the algorithm processes all revisions ρn+1 , . . . , ρm : there can be multiple revision to analyze , since the algorithm may have been inactive at times ( due to system maintenance ) , or indeed , it may not have run yet on the page . Each of ρn+1 , . . . , ρm is fed to the algorithm ; the algorithm
350 ∆N = 200 , ∆T = 180 days ∆N = 100 , ∆T = 90 days attribution difference
1.3 % 2.1 % trie size
70 % 55 %
Figure 7 : Attribution difference , and trie size , for various aging thresholds , as compared to no content aging . computes and stores the origin of these revisions , and finally stores ( m,Tm ) associated with page P . of this
The code , and a demo implementation is available at https://sitesgooglecom/a/ucscedu/ luca/the wikipedia authorship project , along with all the data used for the experiments reported here . We provide experimental data computed on two revison datasets : • Dataset A : articles with more than 200 revisions in files wiki 00000066xmlgz and wiki 00000193xmlgz The dataset consists of 78k revisions in 75 articles .
• Dataset B : articles with at revisions occurring in files wiki 00000066xmlgz , wiki00000193xmlgz , wiki 00000134xmlgz and wiki00000384xmlgz The dataset consists in 50 revisions . least
1000
Figure 8 : Difference between attribution by A0 , A1 and A2 for length rarity function with various threshold .
The above *xmlgz files were chosen at random among the first 1000 files obtained by splitting in 100 page portions a 2010 dump of the English Wikipedia . Unless otherwise noted , we provide results for a rarity function equal to length , and threshold 4 .
Content aging . In the editing of Wikipedia revisions , it occasionally happens that vandals introduce vast amounts of spurious content . This content is almost immediately removed by editors or non vandal users . Yet , since our algorithms store a representation of the entire history of each page , that spurious content would persist indefinitely in our trie summary . This would offer an avenue to vandals for severely impacting our performance . To limit this effects of vandalism , our implementation discards content that has not appeared in any recent revision : this is acceptable in practice , since content that has been long removed from a page is unlikely to be re inserted . To this end , we label every node of the trie T used in Algorithm A3 with the node age , consisting of a pair ( N , T ) . The integer N and the timestamp T represents , respectively , the most recent revision index and the most recent time when the node was traversed by the algorithm . Once Algorithm A3 has processed a revision n produced at time Tn , and before writing back the trie to persistent storage , we prune from the trie all the nodes that have both n − N > ∆N and Tn − T > ∆T , where the thresholds ∆N and ∆T are configurable . Table 7 compares the difference in attribution and size arising from different aging thresholds . The table was obtained from dataset A .
Attribution comparison among A0 , A1 , A2 . Figure 8 plots the difference in the attributions computed by Algorithms A0 , A1 , and A2 , for a rarity function equal to token sequence length , and various values of rarity threshold . These comparisons have been done without using any age driven pruning of trie nodes in Algorithm A1 , to make the comparison fair across algorithms . The figure gives the tokens with different attribution , as percentage of the total tokens , for dataset A . As we can see , Algorithm A1 computes an attribution that is over 75 % different from the one
Figure 9 : Length of revisions ( in number of characters ) of the article “ Dance Dance Revolution ” compare to length of json string with the trie summary . Dips in the revision size indicate content deletions due to vandalism . computed by Algorithm A0 . This is due to the fact that Algorithm A0 considers new any content that is re inserted after a deletion . As an example , Figure 9 plots the size of the revisions , and summary trie , for the Wikipedia article on “ Dance Dance revolution ” ; the frequent dips in revision size correspond to content deletions by vandals . From Figure 8 we see also that the attribution difference between algorithms A1 and A2 is of only a few percentage points , when the length of minimally interesting matches is 3 or more . The advantage of Algorithm A1 over A2 lies in its efficient implementation .
Size of trie and suffix tree summaries . Figure 10 plots the ratio between the size of the trie serialized in Json , and the average size of the last 10 revisions , for aging values ∆N = 100 and ∆T = 90 days and dataset B . We use the average size of the last 10 revisions , rather than the size of the last revision , to avoid very large spikes in the ratio when the content of a revision is deleted by vandals . The average ratio is approximately 10 ; the ratio can be reduced to about 3 by compressing the trie serializations with gzip . This is a very practical amount of storage , which is dwarfed in the English Wikipedia by the amount of storage required to store all revisions of every page .
23456778798081828384858687PercentofmismatchesA0vsA1234567Lengthraritythreshold01234567PercentofmismatchesA1vsA2050010001500200025003000Revisionnumber101102103104105106Stringlength(n.ofsymbols)Article ” DanceDanceRevolution ” summarytriejsonrevisionofthearticle351 Figure 10 : Ratio between the trie summary size and the average size of the last 10 revisions .
Figure 11 : Size comparison between trie summaries for A3 and suffix tree summaries for A2 .
In Figure 11 we compare the size of the trie summaries used by Algorithm A3 , with the size of the suffix tree summaries used in implementing Algorithm A2 . Dataset B was used , and no content aging was applied , to make the comparison fair . The trie sizes are tied to the change between revisions , and since we discard text that has been dead for long , they tend to be a constant multiple of the revision size . On the other hand , the suffix tree sizes are proportional to the total size of past revisions .
Time performance . Figure 12 summarizes the time performance of Algorithm A3 , as a function of revision size . In the figure , the algorithm time is the time required by steps 3–21 , as well as content aging , of A3 ; the serialization time is the time required for serializing and deserializing the trie into json . As we see , these two times are of the same order of magnitude , indicating that there is limited scope for improvement by optimizing the implementation of steps 3–21 . The figure was obtained using dataset A .
6 . DISCUSSION
We have considered so far revisioned content that consists in a single revisioned entity . Most revisioned content , however , consists of multiple entities : a national Wikipedia consists of a set of pages , each of which is versioned , and a code repository similarly consists of multiple files , each revisioned . Furthermore , in modern revisioning systems such as git ( http://git scm.com ) , the various revisions are organized in branches . Since code is commonly copied across files , and to a lesser extent , content is moved across Wikipedia pages , an origin analysis that spans a whole repository is often desirable .
We can perform such repository wide analysis with the algorithms we discussed in this paper , by considering the stream of all revisions ρ0 , ρ1 , ρ2 , . . . in the order they are created , regardless of the entity ( page , or file , or branch ) to which they belong . The content of each new revision will be compared with all previous content , assigning origin via
Figure 12 : Time performance of algorithm A3 . Each point in the plot represents an article , with the average revision size on the X axis . The times required by attribution computation , and trie serialization and deserialization , are reported on the Y axis . matching with corresponding occurrences . The algorithms could be improved by considering as more likely the matches that relate different versions of the same entity , as compared to matches that relate different entities . For software repositories , which are of moderate size , and where revisions are typically generated at low speed ( even large industrial code bases have intervals between revisions of several seconds ) , such a global origin analysis would be feasible . In the English Wikipedia , however , several revisions per second may be created . From our experimental data , the size of a global summary would about ten times the cumulative size of the most recent revisions of all pages , leading to a size of approximately one terabyte . This size exceeds the RAM memory easily available in a single , low cost host . The design of a system capable of comparing , in real time , every revision of Wikipedia with the whole of its past history would be challenging , and the result expensive to operate . For this reason , in our implementation we have opted to compare new revisions only with the previous content of the page to which the revisions belong . If required , we will address content moved across pages via specialized tools .
Compared to the algorithm of [ 6 ] , the one presented here offers a simple mathematical definition of authorship , is applicable to any revisioned content , comes with compexity bounds and robustness characterization , and is well suited to an implementation in which the authorship information needs to be computed on line , as revisions are made . Unfortunately , we became aware of the work of [ 6 ] too late to be able to present here a quantitative comparison of how well the two algorithms match a human perception of authorship on the Wikipedia . Acknowledgements This work was supported in part by the NIH award 1R01GM089820 01A1 , and by a gift from Google , Inc . The content is solely the responsibility of the authors and does not necessarily represent the official views of the NIH .
7 . REFERENCES [ 1 ] B . Adler and L . de Alfaro . A content driven reputation system for the Wikipedia . In WWW 2007 , Proc . of the 16th Intl . World Wide Web Conference . ACM Press , 2007 .
[ 2 ] B . Adler , L . de Alfaro , I . Pye , and V . Raman .
Measuring author contributions to the Wikipedia . In WikiSym : International Symposium on Wikis , 2008 .
02004006008001000Revisionnumber100101102Ratio0100200300400500600Revisionnumber102103104105106107NumberofnodessuffixtreeforA2trieforA3010002000300040005000600070008000Averagerevisionsize(inwords)−010001020304050607Secondsalgorithmtimeserializationtime352 [ 3 ] B . T . Adler . WikiTrust : Content Driven Reputation for the Wikipedia . PhD thesis , UC Santa Cruz , 2012 . [ 4 ] P . Buneman , S . Khanna , and T . Wang Chiew . Data provenance : Some basic issues . In FST TCS 2000 , Lect . Notes in Comp . Sci . , pages 87–93 . Springer Verlag , 2000 .
[ 5 ] P . Buneman , S . Khanna , and T . Wang Chiew . Why and where : A characterization of data provenance . In ICDT 2001 : Intl . Conf . on Database Theory , volume 1973 of Lect . Notes in Comp . Sci . , pages 316–330 . Springer Verlag , 2001 .
[ 6 ] F . Fl¨ock and A . Rodchenko . Whose article is it anyway ? — Detecting authorship distribution in wikipedia articles over time with WIKIGINI . In Proceedings of the Wikipedia Academy . Online Publication , 2012 .
[ 7 ] A . Forte and A . Bruckman . Why do people write for the Wikipedia ? Incentives to contribute to open content publishing . In SIGGROUP 2005 Workshop : Sustaining Community , 2005 .
[ 8 ] J . Freire , D . Koop , E . Santos , and C . Silva .
Provenance for computational tasks : A survey . Computing in Science and Engineering , 10(3 ) , 2008 .
[ 9 ] D . Gusfield . Algorithms on Strings , Trees , and Sequences . Cambridge University Press , 1997 . [ 10 ] E . McCreight . A space economical suffix tree construction algorithm . J . ACM , 23:262–272 , 1976 . [ 11 ] L . Moreau , P . Groth , S . Miles , J . Vazquez Salceda ,
J . Ibbotson , S . Jiang , S . Munroe , O . Rana , A . Schreiber , V . Tan , and L . Varga . The provenance of electronic data . Communications of the ACM , 51(4 ) , 2008 .
[ 12 ] O . Nov . What motivates wikipedians ? Comm . ACM ,
50(11):60–64 , 2007 .
[ 13 ] Y . Simmhan , B . Plale , and D . Gannon . A survey of data provenance in e Science . ACM SIGMOD Record , 34(3 ) , 2005 .
[ 14 ] W . Tichy . The string to string correction problem with block move . ACM Trans . on Computer Systems , 2(4 ) , 1984 .
[ 15 ] E . Ukkonen . On line construction of suffix trees .
Algorithmica , 14:249–260 , 1995 .
[ 16 ] P . Weiner . Linear pattern matching algorithms . In
Proc . of the 14th IEEE Symp . on Switching and Automata Theory , pages 1–11 , 1973 .
353
