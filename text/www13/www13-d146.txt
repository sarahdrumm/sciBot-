One class Collaborative Filtering with Random Graphs
Ulrich Paquet
Microsoft Reseach Cambridge ulripa@microsoft.com
Noam Koenigstein Microsoft R&D , Israel noamko@microsoft.com
ABSTRACT The bane of one class collaborative filtering is interpreting and modelling the latent signal from the missing class . In this paper we present a novel Bayesian generative model for implicit collaborative filtering . It forms a core component of the Xbox Live architecture , and unlike previous approaches , delineates the odds of a user disliking an item from simply being unaware of it . The latent signal is treated as an unobserved random graph connecting users with items they might have encountered . We demonstrate how large scale distributed learning can be achieved through a combination of stochastic gradient descent and mean field variational inference over random graph samples . A fine grained comparison is done against a state of the art baseline on real world data .
Categories and Subject Descriptors G.3 [ Mathematics of computing ] : Probability and statistics
Keywords One class collaborative filtering , random graph , variational inference
1 .
INTRODUCTION
This paper highlights a solution to a very specific problem , the prediction of a “ like ” or “ association ” signal from oneclass data . One class or “ implicit ” data surfaces in many of Xbox ’s verticals , for example when users watch movies through Xbox Live . In this vertical , we recommend media items to users , drawing on the correlations of their viewing patterns with those of other users . We assume that users don’t watch movies that they dislike ; therefore the negative class is absent . The problem is equivalent to predicting new connections in a network : given a disjoint user and an item vertex , what is the chance that they should be linked ?
We introduce a Bayesian generative process for connecting users and items . It models the “ like ” probability by interpreting the missing signal as a two stage process : firstly , by modelling the odds of a user considering an item , and secondly , by eliciting a probability that that item will be viewed or liked . This forms a core component of the Xbox Live architecture , serving recommendations to more than
Copyright is held by the International World Wide Web Conference Committee ( IW3C2 ) . IW3C2 reserves the right to provide a hyperlink to the author ’s site if the Material is used in electronic media . WWW 2013 , May 13–17 , 2013 , Rio de Janeiro , Brazil . ACM 978 1 4503 2035 1/13/05 .
50 million users worldwide , and replaces an earlier version of our system [ 10 ] . The two stage delineation of popularity and personalization allows systems like ours to trade them off in optimizing user facing utility functions . The model is simple to interpret , allows us to estimate parameter uncertainty , and most importantly , easily lends itself to large scale inference procedures .
Interaction patterns on live systems typically follow a power law distribution , where some users or items are exponentially more active or popular than others . We base our inference on a simple assumption , that the missing signal should have the same power law degree distribution as the observed user item graph . Under this assumption , we learn latent parametric descriptions for users and items by computing statistical averages over all plausible “ negative graphs ” .
The challenge for one class collaborative filtering is to treat the absent signal without incurring a prohibitive algorithmic cost . Unlike its fully binary cousin , which observes “ dislike ” signals for a selection of user item pairs , each unobserved user item pair or edge has a possible negative explanation . For M users and N items , this means that inference algorithms have to consider O(M N ) possible negative observations . In problems considered by Xbox , this amounts to modelling O(1012 ) latent explanations . The magnitude of real world problems therefore casts a shadow on models that treat each absent observation individually [ 17 ] .
Thus far , solutions to large scale one class problems have been based on one of two main lines of thought . One line formulates the problem as an objective function over all observed and missing data , in which the contribution by the “ missing data ” drops out in the optimization scheme [ 7 ] . It relies on the careful assignment of confidence weights to all edges , but there is no methodical procedure for choosing these confidence weights except an expensive exhaustive search via cross validation . If a parametric definition of confidence weights is given , a low rank approximation of the weighting scheme can also be included in an objective function [ 18 ] . The work presented here differs from these approaches by formulating a probabilistic model rather than an optimization problem , and quantifies our uncertainty about the parameters and predictions .
A second approach is to randomly synthesize negative examples . Our work falls in this camp , for which there already exists a small body of work . The foremost of these is arguably Bayesian Personalized Ranking ( BPR ) , which converts the one class problem into a ranking problem [ 21 ] . In it , it is assumed that the user likes everything that she has seen more than the items that she hasn’t seen . This
999 assumption implies a constrained ordering of many unobserved variables , one arising from each item . This user wise ranking of items facilitates the inference of latent features for each user and item vertex . By design , there is no distinction between missing items in BPR ; however , popularity sampling of the unobserved items was employed to give more significance to popular missing items [ 5 ] . This approach was effectively utilized by many of the leading solutions in the KDD Cup’11 competition [ 4 ] . An alternative , more expensive approach is to construct an ensemble of solutions , each of which is learned using a different sample of synthesized “ negative ” edges [ 19 ] .
We motivate our approach by discussing properties of typical bipartite real world graphs in Section 2 . A generative model for collaborative filtering when such graphs are observed is given in Section 3 . A component of the model is the hidden graph of edges—items that a user considered , but didn’t like . Section 4 addresses the hidden graph as a random graph . Section 5 combines variational inference and stochastic gradient descent to present methods for large scale parallel inference for this probabilistic model . In Section 6 , we show state of the art results on two practical problems , a sample of movies viewed by a few million users on Xbox consoles , and a binarized version of the Netflix competition data set .
2 . TYPICAL REAL WORLD DATA
The frequency of real world interactions typically follows some form of power law . In Xbox Live , we observe a bipartite graph G of M users and N items , with two ( possibly vastly ) different degree distributions for the two kinds of vertices . Figure 1 ( top ) illustrates the degree distribution of a sample of M = 6.2 × 106 users that watched N = 1.2 × 104 different movies on their Xbox consoles , where an edge appears if a user viewed a movie . Throughout the paper the edges in the observed graph G will be denoted with the binary variable gmn ∈ {0 , 1} for vertices m and n , with a zero value indicating the absence of an edge . We denote the observed degree distributions as puser(d ) and pitem(d ) . If a user viewed on average μ items , and an item was viewed on average ν times , then Epuser [ d ] = μ and Epitem [ d ] = ν , and the constraint
( 1 )
μ N
=
ν M should hold [ 15 ] . In Figure 1 ( top ) , the empirical distributions satisfy μ = 7.1 and ν = 3780 , validating the mean constraint μ/N = ν/M = 00006 We overlay a power law degree distribution to items pitem(d ) ∝ d −077 The user distribution exhibits an marked exponential cut off , with puser(d ) ∝ d −d/70 , and shares its form with many scientific collaboration networks [ 16 ] . The degree distribution of the publicly available Netflix data set is shown in FigIn it , we have M = 4.8 × 105 users and ure 1 ( bottom ) . N = 1.8 × 104 items . We took a positive edge to be present if a user rated an item with four or five stars .
−1.4 e
Given puser(d ) and pitem(d ) , one can sample iid graphs with the given degree distribution . Firstly , generate vertex degrees for each user and item at random , and calculate their sum . If the sums are unequal , randomly choose one user and item , discard their degrees , and replace them with new degrees of the relevant distributions . This process is repeated until the total user and item degrees are equal , after which vertex pairs are randomly joined up [ 15 ] . y c n e u q e r f y c n e u q e r f
106
105
104
103
102
101
100
103
102
101
100
Xbox movies items users
100
101
102
103 degree
104
105
Netflix ( 4 and 5 stars ) items users
100
101
102
103 degree
104
105
Figure 1 : Degree distributions for two bipartite graphs between users and movies : a sample of 4.4 × 107 edges for movies viewed on Xbox ( top ) and the 5.6×107 four and five starred edges in the Netflix prize data set ( bottom ) .
3 . COLLABORATIVE FILTERING
Our collaborative filtering model rests on a basic assumption , that if an edge gmn = 1 appears G , user m liked item n . However , a user must have considered additional items that she didn’t like , even though the dislike or “ negative ” signals are not observed . This hidden graph with edges hmn ∈ {0 , 1} is denoted by H . We say that a user considered an item if and only if hmn = 1 , and the rule gmn = 1 ⇒ hmn = 1 holds ; namely , a user must have considered all the items that she “ liked ” in G . The latent signal is necessary in order to avoid trivial solutions , where the interpretation inferred from data tells us that everyone likes everything or that every edge should be present . It strongly depends on our prior beliefs about H , like its degree distribution or power law characteristics . G is observed as a subgraph of H , while the rest of the edges of the hidden graph H form the unobserved “ negative ” signals .
3.1 The likelihood and its properties
On knowing the hidden graph , we define a bilinear or “ matrix factorization ” collaborative filtering model . We asso
1000 α , β
τbu
τu algorithm
τv
τbv hmn um gmn vn bm
M bn
N
Figure 2 : The graphical model for observing graph G connecting M user with N item vertices . The prior on the hidden graph H is algorithmically determined to resemble the type of the observed graph . ciate a latent feature um ∈ RK with each user vertex m , and vn ∈ RK with each item vertex n . Additionally , we add biases bm ∈ R and bn ∈ R to each user and item vertex . The odds of a user liking or disliking an item under consideration ( h = 1 ) is modelled with p(g | u , v , b , h = 1 ) =σ
' 1−σ uT v +b uT v +b fiff
, ( 2 )
1−g
. fi
. g def with the logistic or sigmoid function being σ(a ) = 1/(1 + −a ) , with a = uT v + b . Subscripts m and n are dropped e in ( 2 ) as they are clear from the context ; b denotes the sum of the biases bm + bn . The likelihood of g for any h is given by the expression p(g | a , h ) =
( 3 ) As g = 1 ⇒ h = 1 by construction , the last factor can be ignored in ( 3 ) . If the binary “ considered ” variable h is marginalized out in ( 3 ) , we find that
σ(a)g(1 − σ(a))1−g h · ( 1 − g)1−h . p(g = 1 | a ) = p(h = 1 ) σ(a ) , p(g = 0 | a ) = p(h = 1)(1 − σ(a ) ) + ( 1 − p(h = 1 ) ) .
( 4 )
In other words , the odds of encountering an edge in G is the product of two probabilities , separating popularity from personalization : p(h = 1 ) , the user considering an item , and σ(a ) , the user then liking that item .
3.2 The full model
( def
M m=1
N ( um ; 0 , τ
The probability of G depends on the prior distributions of the vertices’ hidden features . We choose them to be Gaus−1 sian : p(U ) = u I ) for the users , where = {um}M U m=1 , with similar Gaussian priors on the parameters governing the item vertices . These are shown in the graphical model in Figure 2 . To infer the various scale parameters τ , we place a conjugate Gamma hyperprior on
M )
N ) m=1 n=1
··· ffmn fffi mvn + bmn)gmn · ·· 1 − σ(uT
σ(uT mvn + bmn )
N ( um ; 0 , τ u I ) N ( bm ; 0 , τ −1 bm ) · G(τu ; α , β ) −1 p(G , θ ) =
· M ) · N ) m=1 each , for example
G(τu ; α , β ) = βα/Γ(α ) · τ α−1
−βτu . e u
The only prior beliefs in Figure 2 that do not take an explicit form is that of H . It could be parameterized with a particular degree distribution , be it Poisson , exponential , or a power law with an exponential cut off . However , we would like this to ( approximately ) be in the same family as the observed data , and determine an algorithm which can generate such graphs . Section 4 elaborates on this , including the closure of the graph family under random sampling of subnetworks .
We collectively denote our parameters by θ def
= {H , U , V , b , τ} , def with bmn = bm+bn as shorthand notation . The joint density of all the random variables , given the hyperprior parameters α and β , is fl hmn(1−gmn )
N ( vm ; 0 , τ v I ) N ( bn ; 0 , τ −1 bn ) · G(τv ; α , β ) −1 n=1
· G(τbm ; α , β ) · G(τbn ; α , β ) · p(H ) .
( 5 )
The sigmoid product is denoted with mn , and will later appear in a variational bound in ( 11 ) . Obtaining a posterior approximation to ( 5 ) would follow known literature [ 20 , 25 ] , were it not for the unknown occurrence of edges hmn in H . Sections 4 and 5 are devoted to treating H .
One might also consider placing Normal Wishart hyperprior on the means and variances of um and vn [ 20 , 23 ] . In practice , we benefit from additionally using meta data features in the hyperpriors . They allow us to learn how shared features connect the prior distributions of various items , but is beyond the scope of this paper . 3.3 Factorized approximation
It is analytically intractable to compute the Bayesian averages necessary for marginalization in ( 5 ) . This hurdle is commonly addressed in one of two ways : Samples from the posterior can be drawn by simulating a Markov chain with the posterior as its stationary distribution , and these samples used for prediction [ 14 ] . Alternatively , one might substitute the integration problems required for Bayesian marginalization with an optimization problem , that of finding the best deterministic approximation to the posterior density [ 9 ] .
We approximate the posterior from ( 5 ) , rather than sample from it , as it allows a compact representation to be serialized to disk . The posterior from ( 5 ) is approximated with the fully factorized distribution q , p(θ|G ) ≈ q(θ ) q(umk ) · N )
M )
K )
K ) q(bm ) q(vnk ) def = q(bn ) · q(τu ) q(τv ) q(τbu ) q(τbv ) q(H ) . m=1 n=1 k=1 k=1
( 6 )
The factors approximating each of the vertex features in U , V , and b are chosen to be a Gaussian , for example
1001 q(umk ) = N ( umk ; ηmk , ω −1 mk ) . Similarly , the τ ’s are approximated by Gamma factors in the conjugate exponential family , for example q(τu ) = G(τu ; φu , ϕu ) .
The remaining the question is , what to do with p(H ) , and the posterior marginal approximation q(H ) ?
4 . RANDOM GRAPHS
Although an observation gmn = 1 implies that q(hmn = 1 ) = 1 , we cannot estimate every one of M N q(hmn ) ’s , as there are typically O(1012 ) or more of them . As a recourse , we shall specify q as an algorithm that stochastically generates connections hmn = 1 , so that p(H ) produces ( roughly ) the same type of graphs as is observed in G .
The graphical model in Figure 2 specifies that every “ considered ” edge ( m , n ) in H contains a “ like ” probability σmn . For each edge in H , a coin is flipped , and revealed with probability σmn to give G . If we assume that the coin is on average unbiased , half the edges will be revealed , and |H| ≈ 2|G| . Alternatively , G is a subnet of H , containing half ( or some rate of ) its connections . Working back , we sample graphs H at this rate , and the family of graphs H that can be generated this way constitutes our prior . This places no guarantee that the two graphs will always be of the same type , as not all graph types are closed under random sampling . For example , random subnets drawn from exact scale free networks are not themselves scale free [ 26 ] . However , the practical benefits of this algorithmic simplification outweigh the cost of more exact procedures .
4.1 Sampling q(H )
The factor q(H ) is defined stochastically , with the criteria that it should not be too expensive to draw random samples H . One approach would be to generate samples , similar to Section 2 , by specifying a degree distribution conditioned on the number of degrees d that each user and item vertex in G has . If the mean of each is 2d , one can show that a version of ( 1 ) will also hold for H . At the cost of many redraws , one can sample half edges , as in Section 2 , and connect them until all half edges are paired . We propose a simpler scheme here , which samples H from G in O(|G| log N ) time . The scheme has the flavour of “ sampling by popularity ” [ 5 ] . We define a multinomial histogram M(π ) on the N items , where πn ≥ 0 for n = 1 , . . . , N . This mimics a pseudo degree distribution for missing degrees . Let user m have degree dm , or have viewed dm items . For user m , the subset of dm edges in H that corresponds to gmn = 1 is marked . We then sample dm random “ negative ” edges from M(π ) without replacement—this fills in the remaining values for row m in H , ie hmn for n = 1 , . . . , N . For user m the sample without replacement can be drawn in O(log N ) time by doing bookkeeping with a weighed binary tree on the items .
There are many ways to define histogram π , one of which is to simply let πn = dn , the number of degrees ( or views ) of item n . This is effectively a uniform prior : each item should have the same rate of negatives . If we believe that there is some quality bar that drives popular items to be more generally liked , the histogram can be adjusted with
πn = d γ n
( 7 ) so that it obeys a version of the observed power law . A free rate parameter r is introduced , so that the most popular
2
1.5
1
0.5 o i t a r e v i t a g e n
/ e v i t i s o p
0
100
Xbox movies
101 Item degree ( number of users per item )
103
104
102 r = 1 r = 0.5 105
Figure 3 : The ratio of positive to negative edges per item , from a single sample from q(H ) . ( The ratio is skewed at the head : sampled edges to more popular items have higher odds to already exist in G . Discarding and resampling them leaves popular items underrepresented in the “ negative ” set . This can be overcome with another adjustment of π in M(π) . ) item with degree dmax = max{dn} has histogram weight
πmax = rdmax .
( 8 )
As an example , r = 1 edges to H for that item . A substitution gives a power
2 will add half as many unobserved
γ = 1 − log dmax/ log r
( 9 ) with which the histogram is adjusted in ( 7 ) .
Figure 3 shows two samples of the edges of H for two settings of r . For each item , it shows the ratio of “ positive ” to “ negative ” edges . A side effect is that at the head , the most popular items are underrepresented in the remainder of H . This is because the items ( or edges ) sampled from M(π ) might already exist in G , and are discarded and another edge sampled .
5 . VARIATIONAL INFERENCE ffi q(θ ) log p(G , θ ) dθ + H[q(θ ) ] .
The approximation q(θ ) in ( 6 ) is found by maximizing a variational lower bound on the partition function of ( 5 ) , with log p(G ) ≥ L[q ] = ( 10 ) Here H[q ] is the ( continuous ) entropy of our choice of q . The expression in ( 10 ) is not analytically tractable due to the sigmoids in mn , which appear in p(G , θ ) in ( 5 ) , as they are not conjugate with respect to the q(umk ) ’s or any of the other vertex factors . We additionally lower bound mn with the logistic or Jaakkola Jordan bound [ 8 ] , introducing an additional variational parameter ξmn on each edge . The logistic bound is ≥ eg(uT v+b )
2 ( uT v+b+ξ)−λ(ξ)((uT v+b ) g+h(1−g )
σ(ξ ) e
2−ξ
' ff
2
)
− 1
( 11 ) where subscripts m and n that are clear from the context are suppressed . The bound depends on a deterministic function
,
1002 def
= 1
2ξ [ σ(ξ ) − 1
λ(ξ ) 2 ] . The substitution of the lower bound in ( 11 ) to mn creates a pξ(G , θ ) that leaves the bounded likelihood conjugate with respect to its prior . The bound Lξ ,
L[q ] ≥ Lξ[q ] = q(θ ) log pξ(G , θ ) dθ + H[q ] ,
( 12 ) ffi is therefore explicitly maximized over both the ( variational ) distribution q and the additional variational parameters ξ = {ξmn} . 5.1 Variational updates
The variational updates for the user factors q(umk ) are presented in this section . As the model is bilinear , the gradients of Lξ with respect to the item factors can be set to zero following a similar pattern . To minimize Lξ with respect to q(umk ) , one might take functional derivatives ∂Lξ/∂q(umk ) with respect to each q(umk ) , and sequentially equate them to zero . This is slow , as each update will require a loop for the user , K loops over all over all the vertex ’s edges : the items will be required . The vertex factor can alternatively be updated in bulk , by first equating the gradients of Lξ with respect to a full Gaussian ( not factorized ) approximation ˜q(um ) to zero . The fully factorized q(umk ) can then be recovered from the intermediate approximation ˜q(um ) as those that minimize the Kullback Leibler divergence DKL( means of q(umk ) match that of ˜q(um ) , while their precisions match the diagonal precision of ˜q(um ) .
( k=1 q(umk)˜q(um) ) : this is achieved when the
K
How do we find ˜q(um ) ? The functional derivative ∂Lξ/∂ ˜q(um ) is zero where ˜q(um ) has as natural parameters a precision matrix of
Nffl
· 2λ(ξmn ) · Eq vnvT n
+ Eq[τu]I
( 13 )
Pm =
Eq hmn n=1 and mean times precision vector μmPm , which will be stated in ( 15 ) . Apart from having to average hmn over q(H ) , which we cannot do analytically , the update in ( 13 ) suffers from having a summation over all N item vertices .
The burden of having to determine a sum over a full item catalogue in ( 13 ) can be removed with a clever rearrangement of expectations . As hmn is binary ,
Nffl
Eq hmn f ( vn ) = n=1 q(H ) hmn f ( vn ) ffl ffl
H
Nffl ffl n=1
= q(H ) f ( vn ) .
( 14 )
H n:hmn=1
The sum over H in ( 14 ) runs over all 2M N possible instantiations of H . A rearrangement of ( 13 ) therefore allows the updates to appear as a stochastic average , ffl ffl n:hmn=1 vnvT n
+ Eq[τu]I
2λ(ξmn ) · Eq . gmn − 1
2
· ·· fi n:hmn=1
·· · − 2λ(ξmn ) · Eq bm + bn
Eq vn
.
( 15 )
Pm = Eq(H )
μmPm = Eq(H )
Inside the expectation over q(H ) , the mean field update in ( 15 ) is a quantity specified on the hidden graph H only , and not all N plausible edges for the user . We are able to sample graphs from q(H ) according to Section 4 . Retrospectively , this choice now bears fruit , as the update exists as an average amenable to stochastic gradient descent . We remark , too , that the natural parameters in ( 15 ) define the natural gradients of the variational objective function [ 1 , 24 ] . The full natural gradient is periodic in the number of vertices and the updates are component wise , and convergence with such updates can also be achieved using a stochastic gradient algorithm [ 12 ] .
There are additional variational parameters at play in ( 15 ) . For the required edges hmn = 1 that connect user m with items n , the values ξmn that maximize Lξ or Eq[log pξ(G , θ ) ] are each given by
ξ2 mn = Eq
( uT mvn + bm + bn)2
,
( 16 ) and they are computed and discarded when needed . We take the positive root as ξmn , and refer the reader to Bishop [ 2 ] for a deeper discussion .
Given Pm and μmPm from ( 15 ) , we have sufficient statistics for ˜q(um ) , and hence for updating each of the K q(umk ) ’s in bulk . Deriving sufficient statistics for q(vnk ) , q(bm ) and q(bn ) is similar to that presented in ( 15 ) , and the derivation will not be repeated . Given these , optimization proceeds as follows : At time t , we sample a hidden graph H , over which the user and item vertex factors are updated . Focussing on user m , let P(t−1 ) be the ( diagonal ) precision K k=1 q(umk ) . We then matrix of the factorized distribution find Pm in ( 15 ) , and now the precision matrix of ˜q(um ) m = tPm + ( 1 − t)P(t−1 ) will be P(t ) m , where t ∈ [ 0 , 1 ] . The factors q(umk ) are then recovered from ∞ the bulk computation of ˜q(um ) . The mean times precision ∞ vector of ˜q(um ) is given through a similar stochastic update . The series { t}∞ t=1 t = ∞ and t < ∞ , guarding against premature convergence and m , found through P(t ) t=1 should satisfy
( m t=1 2 infinite oscillation around the minimum [ 22 ] .
Finally , the marginal approximations for the hyperparameters are updated by setting the functional derivatives , say ∂Lξ/∂q(τu ) , to zero . For instance for q(τu ) = G(τu ; φu , ϕu ) the shape φu and rate ϕu are
φu = α + KM/2
ϕu = β +
1 2 fflM m=1
Eq uT mum
.
( 17 )
As q(umk ) is dependent on H , the rate is also stochastically updated as described above . 5.2 Large scale inference
The use of a bipartite graph ensures that variational updates are parallelizable . For instance , by keeping all q(vnk ) , q(bn ) and q(bm ) fixed for the item and user vertices , the gradients ∂Lξ/∂ ˜q(um ) , and hence the stochastic updates resulting from ( 15 ) , have no mutual dependence . Consequently , the loop over user vertex updates m = 1 . . . M is embarrassingly parallel ; the same is true for other updates . This will not hold for more general graphs like those of social networks , though , where more involved logic will be required . Due to the fact that a variational lower bound is optimized for , optimization can also be distributed across multiple machines , as long as the bound holds . For example , one might distribute the graph according to item vertices in blocks Bb , and iteratively optimize one block at a time , or optimize
1003 blocks concurrently ( with embarrassingly parallel optimization inside the blocks , as discussed earlier ) . In this example the sparse user item graph ( matrix ) G is distributed such that all observations for a set Ba of items are co located on the same machine . The natural gradients for the users then distribute across machines , and can be written so that the dependence on the data blocks on various machines separates . When optimizing using the item wise data block Ba on one machine , we write Pm in ( 15 ) as
⎡ ⎢⎢⎣ ffl ffl n∈Ba n:hmn=1
2λ(ξmn ) · Eq fffi
2λ(ξmn ) · Eq vnvT n vnvT n
··· ⎤ ⎥⎥⎥⎦ + Eq[τu]I . fl
Pm = Eq(H ) ffl b'=a
+ n:hmn=1 n∈Bb block b ’s natural gradient X(b ) m ; fixed
( 18 )
Update ( 18 ) defines a thin message interface between various machines , where each block has to communicate only its natural gradients X(b ) m —and similar mean times precision gradients—to other blocks.1 In block Ba we might iterate between updates ( 18 ) and full item updates for all n ∈ Ba , whilst keeping the incoming messages X(b ) m from other machines fixed . After a few loops over users and items , one can move to the next block . Similarly , different machines can optimize on all the blocks {Bb} in parallel , as long as the natural gradient messages are periodically communicated to other machines . The scheme presented here generalizes to a further subdivision of user vertices into blocks .
6 . RESULTS
Given G , a pivotal task of collaborative filtering is that of accurately predicting the future presence of an edge . This allows online systems to personalize towards a user ’s taste by recommending items that the user might like .
The collaborative filtering model in Section 3 explicitly separated the probability of a user considering an item from σ , the probability for the user liking the item . The odds of liking an item depends on our inferred certainty of the user and item parameters,2 ffi p(g = 1 | h = 1 ) ≈ ≈
σ(uT v + b ) q(u ) q(v ) q(b ) du dv db
σ(a)N ( a ; μa , σ2 a ) da ≈ σ ffiffiffi fl
. ( 19 )
1 + πσ2 a/8
μa
= uT v + b , with The random variable a was defined as a its density approximated with its first two moments under = Eq[(uT v + b− μa)2 ] . The q , ie μa final approximation of a logistic Gaussian integral follows from MacKay [ 13 ] .
= Eq[uT v + b ] and σ2 a def def def
1The division of data to machines will be dictated by the size of M and N ; for N ) M a user wise division gives a smaller message interface , as only natural gradients for the items’ updates will be required . 2We suppress subscripts m and n for clarity , and write q(u ) for the diagonal Gaussian
(
K k=1 q(umk ) . r o r r e n o i t a c i f i s s a c l
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
Classification error on like probability
Xbox movies Netflix ( 4 and 5 stars )
10
20
30
User degree ( number of items per user )
40
Figure 4 : The classification error on Gtest , given h = 1 ( the ground truth is g = 1 ) . The full histograms of probabilities p(g = 1|h = 1 ) are presented in Figure 5 .
6.1 Evaluation
We evaluated our model by removing a test set from the Xbox movies and Netflix ( 4 and 5 stars ) data sets . The degree distributions for these data sets are presented in Figure 1 . The training data Gtrain was created by randomly removing one edge ( or item ) for each user from G ; the removed edges formed the test set .
A core challenge of real world collaborative filtering algorithms is to find a balance between popular recommendations and personalized content in a structured form . Based on our experience , a criteria of a good recommender is the ability to suggest non trivial items that the user will like , and surface less popular items in the tail of the item catalogue . In the evaluations we highlight this by grouping results according to item popularity in Figure 6 , for example .
Two evaluations are discussed below . Firstly , given that an item is presented to a user with hmn = 1 , we are interested in the classifying gmn → {0 , 1} . This is one of the key contributions that our model brings to the table . As far as we are aware , there are no other algorithms that isolate p(like ) in this way . To be able to draw a comparison with a known state of the art algorithm , we consider various forms of a rank based metric in a second evaluation .
In the tests below , K = 20 latent dimensions were used . The user biases were clamped at zero , as q(H ) was defined to give balanced samples for each user . The rate and shape parameters of the hyperprior were set to α = β = 0.01 , giving a hyperprior on the τ ’s with mean 1 and a flexible variance of 100 . The means of the hyperparameter posterior estimates were E[τbv ] = 0.4 , E[τv ] = 3.5 , and E[τu ] = 20 When rounded to one decimal place , these values were similar on both the Netflix ( 4 and 5 stars ) dataset and the Xbox Movies dataset .
611 The “ like ” probability
The classification error on the held out data converges to a stable value as users view between ten and twenty items . Its plot is presented in Figure 4 , and has a natural interpre
1004 Xbox movies d user d user
= 1 = 40
6
5
4
3
2
1 y t i s n e d l a c i r i p m e
0
0
0.2
0.4 0.6 p(g = 1 | h = 1 )
.
Netflix ( 4 and 5 stars )
0.8 d user d user
= 1 = 40
4
3.5
3
2.5
2
1.5
1
0.5 y t i s n e d l a c i r i p m e
0
0
0.2
0.4 0.6 p(g = 1 | h = 1 )
0.8
1
Figure 5 : The distribution of p(gmn = 1|hmn = 1 ) on the held out items in the evaluation , sliced incrementally according to users connected to duser = 1 to 40 items . The ground truth is gmn = 1 . tation . Conventional wisdom dictates that the error rates for explicit ratings based recommendation systems are typically in the order of 20 % of the ratings range . For Netflix ’s five star ratings datasets , this error is around one star [ 11 ] , while an error of around 20 points in the 0 100 scale of the Yahoo! Music dataset is usual [ 3 ] . The 16 19 % classification error in Figure 4 is therefore in line with the signal to noise ratio in well known explicit ratings datasets . When users viewed only one item , the bulk of the predictive probability mass p(g = 1|h = 1 ) is centered around 50 % , slightly skewed to being less certain . This is illustrated in Figure 5 . As users view more items , the bulk of the predictive probability skews towards being more certain3 . The probability p(g = 1|h = 1 ) is useful in presenting a user with interesting recommendations , as it is agnostic
3A property of a good probabilistic classification system is that it produces an exact callibration plot . For example , we expect 10 % of edges to be misclassified for the slice of edges that are predicted with p(g = 1|h = 1 ) = 10 % . The callibration plot requires a ground truth negative class g = 0 , which is latent in our case . Figures 4 and 5 aim to present an equivalent to a callibration plot .
1 to each item ’s popularity . It is therefore possible to define a utility function that trades this quantity off with an item ’s popularity , effectively giving a knob to emphasize exploration or exploitation . Such a utility can be optimized through A/B tests in a flighting framework , but is beyond the scope of this paper .
612 Average rank
We turn to a ranking task to draw a comparison against known work , as we are unaware of other algorithms that isolate p(like ) . On seeing Gtrain , the absent edges ( where gmn = 0 ) are ranked for each user m . The ranking is based on various scores smn : like the odds of a user liking an item , namely smn = p(gmn =
1| hmn = 1 ) as approximated in ( 19 ) ; popularity smn = πn ; popularity×like the odds of a user considering and liking an item , namely smn = πn p(gmn = 1| hmn = 1 ) . ff Our metric is computed as follows : If item n
We evaluated models for the two settings of r in ( 9 ) ; a sample from H for each was shown in Figure 3 . ff the rank score counts the position of n diction list was removed , in an ordered pre ff* ffl ffl
' ff Srank(m , n
) def =
I smn . > smn
1 . ( 20 ) n:gmn=0 n:gmn=0
Random guessing would give S = 0.5 , while S = 1 places the held out item at the head of the list .
As a benchmark , we use the Bayesian Personalized Ranking ( BPR ) model of Rendle et al . [ 6 , 21 ] . It has shown state of the art performance on ranking metrics against methods ranging from singular value decompositions and nearest neighbours to weighed regularized matrix factorization [ 19 ] . BPR was also used as a key component in many of the leading solutions for the second track of the KDD Cup’11 competition [ 4 ] . The competition was designed to capture the ability of models to personalize recommendations that “ fit ” specific users regardless of an item ’s popularity . In that setting , BPR was trained with missing items sampled with probabilities proportional to their popularity as described in [ 5 ] . We therefore implemented and trained two BPR models : BPR uniform with missing items sampled uniformly ;
BPR popularity with missing items sampled proportional to their popularity .
These two models capture two different aspects of recommender systems . BPR uniform is optimized to learn a userwise ranking of items , where the objective function specifies that items that are liked ( ie gmn = 1 ) should be ranked above missing items ( ie gmn = 0 ) .
The metric in ( 20 ) follows [ 21 ] . Because BPR uniform directly optimizes this metric , it should come as no surprise that it will perform better than methods that do not optimize it directly ( see Figure 6 ) . However , meaningful insights can still be gleaned from the comparison . BPR popularity is aimed at ranking observed “ liked ” items above other popular items that are missing from the user ’s history . While two BPR models are required to capture these two different aspects of recommendations , our generative model captures both of these aspects in a structured manner .
1005 0.95 k n a r e g a r e v a
0.9
0.85
0.8
100
0.95
0.9
0.85 k n a r e g a r e v a
0.8
100
Xbox movies popularity like ( r = 1 ) like ( r = 0.5 ) pop * like ( r = 1 ) pop * like ( r = 0.5 ) BPR−uniform BPR−pop
101
102
User degree ( number of items per user )
Netflix ( 4 and 5 stars )
103
. popularity like ( r = 1 ) like ( r = 0.5 ) pop * like ( r = 1 ) pop * like ( r = 0.5 ) BPR−uniform BPR−pop 103 User degree ( number of items per user )
101
102
1
0.8
0.6
0.4
0.2 k n a r e g a r e v a
0
100
1
0.8
0.6
0.4
0.2 k n a r e g a r e v a
0
101
Xbox movies popularity like ( r = 1 ) like ( r = 0.5 ) pop * like ( r = 1 ) pop * like ( r = 0.5 ) BPR−uniform BPR−pop 104
105
101
102
103
Item degree ( number of users per item )
Netflix ( 4 and 5 stars ) popularity like ( r = 1 ) like ( r = 0.5 ) pop * like ( r = 1 ) pop * like ( r = 0.5 ) BPR−uniform BPR−pop 104
105
102
103
Item degree ( number of users per item )
Figure 6 : The rank Srank(m , n ) in ( 20 ) , averaged over users ( left ) and items ( right ) , grouped logarithmically by their degrees . The top evaluation is on the Xbox movies sample , while the bottom evaluations are on the Netflix set , as given in Figure 1 .
Figure 6 illustrates the mean rank scores , grouped logarithmically by user and item degrees . In the plots that are grouped by user degrees , we see improved results for algorithms that prefer popularity , ie popularity×like and BPR uniform . This is explained by the dominance of popularity biases in both datasets . As expected , BPR uniform show best results as it is optimizes the task at hand directly . The estimates for users with an order of 103 to 104 degrees are noisy as the data is very sparse ( see Figure 1 ) . However , when looking at the per item breakdown , we learn that BPRuniform and the popularity×like models perform poorly on less popular items and their superior results are based on recommendations from the short head of the popular items . When it comes to recommending from the long tail of the less familiar items , the like models show best results , with BPR popularity just behind . These trends are consistent on both datasets .
The distribution of the ranks over all users ( and items ) is heavy tailed , and whilst the average is often reported , the median is much higher than the average reported in Figure 6 . Figure 7 shows the error bars using the percentiles of the rank scores for tests like and popularity×like for r = 1 2 . The rank variance decreases as users view a few movies , but increases for heavy users which are harder to model . When popularity is included in the ranking , the error bars get tighter for heavy users , which implies that these users’ lists are mostly governed by popularity patterns .
7 . CONCLUSIONS
Random graphs can be leveraged to predict the presence of edges in a collaborative filtering model . In this paper we showed how to incorporate such graphs in an inference procedure by rewriting a variational Bayes algorithm in terms of random graph samples . As a result , we were able to explicitly extract a “ like ” probability that is largely agnostic to the popularity of items . The use of a bipartite graph , central to this exposition , is not a hindrance , as user user interactions in a general network can be similarity modelled with mum . ) . While scalable parallel inference is not immeσ(uT diately obvious , we believe this to be a worthwhile pursuit . By employing the same machinery on general graphs , one should be able to model connections in social or other similar networks .
The use of a Bayesian graphical model makes it easy to adapt the model to incorporate richer feedback signals . Similarly , both structured and unstructured meta data can be
1006 1
0.9
0.8
0.7
0.6 k n a r
0.5
100
1
0.9
0.8
0.7
0.6 k n a r
0.5
100
Xbox movies : like ( r = 0.5 )
90th percentile 80th " 70th " 60th " 50th " 40th " 30th " 20th " 10th "
101
102
User degree ( number of items per user )
Xbox movies : pop * like ( r = 0.5 )
.
90th percentile 80th " 70th " 60th " 50th " 40th " 30th " 20th " 10th "
101
102
User degree ( number of items per user )
103
103
Figure 7 : Error bars on the rank tests . The median is much higher than the average rank reported in Figure 6 . plugged into the graphical model . The hidden graph H may also be partly observed , for example from system logs . In that case some true negatives exist . Alternatively , we may know a priori when a user could never have considered an item , fixing some h at zero . In both these scenarios the process of drawing random hidden graphs H can be adjusted accordingly . For the sake of clarity , none of these enhancements were included in this paper .
8 . ACKNOWLEDGMENTS
The authors are indebted to Nir Nice , Shahar Keren , and Shimon Shlevich for their invaluable input , management , and stellar engineering skills .
9 . REFERENCES [ 1 ] S . Amari . Natural gradient works efficiently in learning . Neural Computation , 10(2):251–276 , 1998 .
[ 2 ] C . M . Bishop . Pattern Recognition and Machine
Learning . Springer , 2006 .
[ 3 ] G . Dror , N . Koenigstein , and Y . Koren . Yahoo! music recommendations : Modeling music ratings with temporal dynamics and item taxonomy . In Proc . 5th ACM Conference on Recommender Systems , 2011 .
[ 4 ] G . Dror , N . Koenigstein , Y . Koren , and M . Weimer .
The Yahoo! music dataset and KDD Cup’11 . Journal Of Machine Learning Research , 18:3–18 , 2012 .
[ 5 ] Z . Gantner , L . Drumond , C . Freudenthaler , and
L . Schmidt Thieme . Personalized ranking for non uniformly sampled items . Journal of Machine Learning Research , 18:231–247 , 2011 .
[ 6 ] Z . Gantner , S . Rendle , C . Freudenthaler , and
L . Schmidt Thieme . MyMediaLite : A free recommender system library . In 5th ACM International Conference on Recommender Systems , 2011 .
[ 7 ] Y . F . Hu , Y . Koren , and C . Volinsky . Collaborative filtering for implicit feedback datasets . In IEEE International Conference on Data Mining , 2008 .
[ 8 ] T . Jaakkola and M . Jordan . A variational approach to
Bayesian logistic regression problems and their extensions . In Artificial Intelligence and Statistics , 1996 .
[ 9 ] M . Jordan , Z . Ghahramani , T . Jaakkola , and L . Saul . An introduction to variational methods for graphical models . Machine Learning , 37:183–233 , 1999 .
[ 10 ] N . Koenigstein , N . Nice , U . Paquet , and N . Schleyen .
The Xbox recommender system . In Proc . 6th ACM Conference on Recommender Systems , 2012 .
[ 11 ] Y . Koren . The BellKor solution to the Netflix Grand
Prize . 2009 .
[ 12 ] H . J . Kushner and G . G . Yin . Stochastic
Approximation and Recursive Algorithms and Applications . Springer , 2003 .
[ 13 ] D . J . C . MacKay . The evidence framework applied to classification networks . Neural Computation , 4(5):698–714 , 1992 .
[ 14 ] R . M . Neal . Probabilistic inference using Markov chain Monte Carlo methods . Technical Report CRG TR 93 1 , Dept . of Computer Science , University of Toronto , 1993 .
[ 15 ] M . E . J . Newman , S . H . Strogatz , and D . J . Watts . Random graphs with arbitrary degree distributions and their applications . Phys . Rev . E , 64:026118 , 2001 .
[ 16 ] M . E . J . Newman , D . J . Watts , and S . H . Strogatz .
Random graph models of social networks . Proc . Natl . Acad . Sci . USA , 99:2566–ˆa ˘A¸S2572 , 2002 .
[ 17 ] K . Palla , D . A . Knowles , and Z . Ghahramani . An infinite latent attribute model for network data . In 29th International Conference on Machine Learning , 2012 .
[ 18 ] R . Pan and M . Scholz . Mind the gaps : Weighting the unknown in large scale one class collaborative filtering . In KDD , pages 667–675 , 2009 .
[ 19 ] R . Pan , Y . Zhou , B . Cao , N . Liu , R . Lukose ,
M . Scholz , and Q . Yang . One class collaborative filtering . In IEEE International Conference on Data Mining , pages 502–511 , 2008 .
[ 20 ] U . Paquet , B . Thomson , and O . Winther . A hierarchical model for ordinal matrix factorization . Statistics and Computing , 22(4):945–957 , 2012 .
[ 21 ] S . Rendle , C . Freudenthaler , Z . Gantner , and
L . Schmidt Thieme . BPR : Bayesian personalized ranking from implicit feedback . In Uncertainty in Artificial Intelligence , pages 452–461 , 2009 .
1007 [ 22 ] H . Robbins and S . Monro . A stochastic approximation
[ 24 ] M . Sato . Online model selection based on the method . The Annals of Mathematical Statistics , 22(3):400–407 , 1951 . variational Bayes . Neural Computation , 13(7):1649–1681 , 2001 .
[ 23 ] R . Salakhutdinov and A . Mnih . Bayesian probabilistic matrix factorization using Markov chain Monte Carlo . In Proceedings of the 25th International Conference on Machine Learning , pages 880–887 , 2008 .
[ 25 ] D . Stern , R . Herbrich , and T . Graepel . Matchbox :
Large scale Bayesian recommendations . In International World Wide Web Conference , 2009 .
[ 26 ] M . P . H . Stumpf and C . Wiuf . Sampling properties of random graphs : The degree distribution . Phys . Rev . E , 72:036118 , 2005 .
1008
