Know Your Personalization : Learning Topic level
Personalization in Online Services
Anirban Majumder
Bell Labs Research , Bangalore , India anirbanmajumder@alcatel lucentcom
Nisheeth Shrivastava
Bell Labs Research , Bangalore , India nisheethshrivastava@alcatel lucentcom
2 1 0 2 c e D 4 1
]
G L . s c [
1 v 0 9 3 3
.
2 1 2 1 : v i X r a
ABSTRACT Online service platforms ( OSPs ) , such as search engines , news websites , ad providers , etc . , serve highly personalized content to the user , based on the profile extracted from his history with the OSP . Although personalization ( generally ) leads to a better user experience , it also raises privacy concerns for the user—he does not know what is present in his profile and more importantly , what is being used to personalize content for him . In this paper , we capture OSP ’s personalization for an user in a new data structure called the personalization vector ( η ) , which is a weighted vector over a set of topics , and present techniques to compute it for users of an OSP .
Our approach treats OSPs as black boxes , and extracts η by mining only their output , specifically , the personalized ( for an user ) and vanilla ( without any user information ) contents served , and the differences in these content . We believe that such treatment of OSPs is a unique aspect of our work , not just enabling access to ( so far hidden ) profiles in OSPs , but also providing a novel and practical approach for retrieving information from OSPs by mining differences in their outputs .
We formulate a new model called Latent Topic Personalization ( LTP ) that captures the personalization vector into a learning framework and present efficient inference algorithms for it . We do extensive experiments for search result personalization using both data from real Google users and synthetic datasets . Our results show high accuracy ( R pre = 84 % ) of LTP in finding personalized topics . For Google data , our qualitative results show how LTP can also identifies evidences—queries for results on a topic with high η value were re ranked . Finally , we show how our approach can be used to build a new Privacy evaluation framework focused at end user privacy on commercial OSPs .
1 .
INTRODUCTION
Personalization is being used by most online service platforms ( OSPs ) such as search , advertising , shopping , etc . The goal is to lure users by offering a better service experience customized to their individual interests . A popular trend is to employ profile based personalization , where OSPs build extensive profile for the user ( based his past interactions search queries , browsing history , links shared , etc . ) and personalize the content based on this profile . Several popular search1 , movie services employ such personalization , eg recommendations2 , etc .
While OSPs definitely track rich user histories , they can infer a great deal more by mining this raw data . Informally speaking , OSPs can determine users interests and biases on different categories , which can then be used ( along with his history ) for personalization . For example ( see [ 19 ] for details ) , Google is shown to have inferred users political affiliations ( republican or democratic ) , and use it to re ranked results .
For an user , this raises a significant privacy concern—he does not know what was tracked in his history , what has been inferred , and more importantly , is currently being used to personalize content . Moreover , as both the personalization techniques and the data they operate on are the key differentiators of these OSPs ( their secret sauce ) , they do not reveal either of them , making it even harder for an user to understand how personalization is done for him .
In this paper , we aim at extracting an user ’s profile from the OSP . We model an user ’s profile as a weighted personalization vector over topics , where the weight on a topic indicates his interest in it ( higher means more interested ) . Informally , a topic is any concept or phenomenon that the user could be interested in , eg a specific sport , a preference over cuisine , favorite author , movie genre , etc.3
Our goal is not to reverse engineer the OSPs inference algorithms . In fact , we treat the OSP as a black box . We assume that we only have access to their output , which is basically the ( personalized ) content served by them to the user on different urls4 . The key idea is to get both personalized content ( served for the user ) and vanilla content ( served for a new/not logged in user ) for the same url from the OSP , and determine the topics of personalization based these two content and the differences among them .
1http://privacymicrosoftcom/en us/Bingmspx ; http://supportgooglecom/websearch/bin/answerpy?answer=1710607
2Netflix : http://wwwnetflixprizecom 3More specifically , we define it as a distribution over bag of words as common in the topic modeling literature ( see Section 3 for details ) .
4The url could point to a static page , eg reviews and other information on a movie , or dynamically generated , eg search results .
The profiles in OSPs have remained opaque so far , with little knowledge of user profiles hidden in them . Our paper provides a novel approach to crack this problem , giving insights into the user profiles without the knowledge of the inference techniques or the history of the user . We believe that this aspect of comparing the differences in output to extract the hidden personalized topics is unique to our paper and opens a new directions in privacy research that can be aimed at commercial OSPs .
Let us consider the case of a search engine . For any query , we can get the personalized and vanilla results by making the query from a browser with and without logging in , respectively . These results are basically two ranked lists with some urls in the latter moved up or down in the former , based on the user profile . We study these movements over multiple queries and determine the most likely topics of interest for the user that can best explain them .
For the remaining of this paper we will talk only about search result personalization . However , our techniques can be easily extended to any service where a ) we can observe both vanilla and personalized content and b ) we can get a ranked ordering of the content . For example , we can apply it to movie recommendation ( in say Netflix ) based on the personalized ( and vanilla ) ranked list of related movies presented when on a web page of a particular movie . 1.1 Search Personalization and Re ranking
Although the exact details of personalization for many popular services of today ’s web remain a mystery , recent works in the web search community have thrown some light into the intricacies of search engine personalization [ 8 , 22 , 17 ] . These techniques vary considerably in terms of their description and complexity but the common underlying theme for them is to first populate the vanilla result using the semantics of the query string and then personalize it by rearranging the items in this list , using the profile information . Therefore , conceptually , the vanilla and personalized responses are re ordering of the same set of items . We take advantage of this re ranking of results to determine the topics present in the user ’s profile with the OSP .
The restriction of re ranking over the same urls is useful for exposition of our solution approach , but can be easily lifted by simply adding the extra urls in one list to the end of the other list5 . The important point is that personalization , by definition , will affect ranks of results shown , which is what we use in this paper .
Note that these topics may not be explicitly maintained at the OSP ; in fact they could be using something completely unrelated to our definition of topics to model the user profile . Our paper hinges on the intuition that an user ’s interests with most OSPs can be captured by a set of topics that he is interested in . And any OSP that personalizes results based on his interests must give higher preference to results matching these topics . Thus our approach of finding topic level personalization is fairly generic—working on OSPs who do not necessarily have topic based profiles of users and without the knowledge of the profiling algorithms they use .
A alternate competitive approach to recreate the user pro file could be by mining the input to the OSP ( ie user ’s history)[8 , 22 , 21 ] . However , this approach has several shortcomings compared to us . One , it is very hard to catch up to the commercial techniques used by OSPs that are usually more advanced and rapidly evolving . Two , due to proprietary nature of OSPs , it is not clear what algorithm or even what part of the history is being used by them ( eg most personalization is done using only recent history , but it is not clear as to how recent for each user ) . In other words , with any profiling tool , there is no certainty that it can infer all that the OSP has . Finally , in many cases the history information may not be available publicly ( ie while a Google user ’s search history is available , past ads served are not ) , limiting the effectiveness of these approaches . In contrast , our approach is agnostic to OSP ’s personalization scheme and can work even when the history is not public . 1.2 A new privacy preserving framework
The topics of personalization for an user can be utilized in building a novel privacy evaluation and prevention toolkit , that we describe next . The toolkit presents user with the topics his profile is personalized on and ask him to determine , based on his personal judgment , whether some of these topics are sensitive 6 . Now , a topic which is both sensitive and has high personalization score can be deemed a privacy leak , as the OSP is using an user ’s data in a way he does not agree with . These leaks for a user can now be detected and monitored over time , and in several cases can also be plugged , eg by undoing the re ranking on sensitive topics or simply served the vanilla content . These ideas are currently being developed into a privacy preserving toolkit in which the techniques developed in this paper are a key component . 1.3 Our Contributions
The main contributions of the paper are as follows . • We propose a new direction in privacy research that enables users in getting a glimpse of their profile information being used by commercial OSPs to serve personalized content . We formally capture this information as a topic level personalization vector that provides a concise and accurate summary of the user profile .
• We propose a novel way to compute this topic level personalization based on the personalized and vanilla content served by OSPs . This formulation treats the services as a black box and hence can work with a variety of online services . We believe that this is a unique aspect of our work and can open a new direction for privacy research by enabling access to ( so far hidden ) profile information in OSPs .
• We present a probabilistic model ( named Latent Topic Personalization , or LTP ) that captures the intuition behind our approach . LTP is both expressive and leads to computationally efficient inference algorithms ( LTP INF and LTP EM ) that find the personalization vector on real datasets .
5In our experiments with Google , only 15 % of personalized results contain any extra result compared to vanilla , and even these contain on average only 14 % extra urls ( or , 1.4 urls for an avg . result size of 10 ) .
6The definition of sensitive , informally stated as any topic he find uncomfortable getting personalization on , can vary across users and could include health conditions , financial , sexual preferences , etc .
• Our experiments with synthetic dataset using stateof the art personalization engine show that LTP can learn the personalization parameters very accurately , getting on average 84 % precision in learning personalized topics .
• We perform experiments on a novel real life dataset containing the personalized and vanilla query results collected from 10 Google users . We also demonstrate how our techniques can be used to find the evidence of personalization which can be very helpful in user facing tools ( see Section 12 )
2 . RELATED WORK Search personalization : A large body of work exists on personalizing search results using user profiles [ 8 , 22 , 17 ] , that collectively give overwhelming evidence of its benefits . More recently , researchers have also explored creating profiles using topic models [ 21 ] and other textual information [ 23 ] . These works are not competitors of our paper , but rather serve as a motivation for us , as they highlight existence and importance of profiles in the state of the art in personalization .
Another body of work explores short term and session based personalization [ 1 , 8 ] , that personalize based on user ’s current intention , based on his recent history or session . While such approach is not aligned with our idea , there are two important points to note—a ) they do no imply profilebased personalization does not happen , rather , they are typically used in conjunction with each other [ 1 , 13 ] , and b ) since they are applicable only during a session , it is easy to remove their affect by making sure no coherent session is tracked during our data collection ( by doing queries randomly and multiple times while matching results ) .
Researchers have also found that personalization is not always beneficial and have proposed various approaches , such as click entropy [ 24 , 26 ] , dynamic user interests [ 13 ] and query difficulty [ 30 ] , to filter queries that should not be personalized ( irrespective of user ’s profile ) . Such filtering is very hard replicate in our approach since the output may not contain any information to model them . We therefore allow for existence of this hidden process in our model via a latent variable deciding ( randomly ) if personalization happens on a query ( see Section 4.1 for details ) . Topics Models : Although topic models are clearly a popular tool for processing textual information and have been also used in personalization , there is no work to our knowledge that models the differences in two documents ( or two ranked set of documents ) as us . A recent work by Bischof et . al.[2 ] comes close—they find exclusive topics ( that are sufficiently different from each other ) so that the documents can be classified into non overlapping hierarchy . While this also involves finding topics which are present in some documents and not in others , it is still very different from our approach of finding a consistent ( may not be exclusive ) set of personalized topics that can differentiate personalized and vanilla content . Privacy : Finally , our problem stems from the general area of user privacy . Various studies have highlighted problems of privacy in information leaks from OSPs[11 , 16 , 10 ] . Korolova et . al.[10 ] showed how targeted ads can pin point individual users in Facebook , Mao et . al.[16 ] analyzed tweets to find vacation plans , medical conditions etc . for real users . How ever , these studies are focused on finding instances of privacy leaks from the entire OSP network and do not help users understand leaks in their own account . Other approaches of privacy preserving personalization aim at building a system from the scratch that ensures certain norms are preserved in the personalized output , eg grouping user profiles [ 28 , 29 ] to preserve k anonymity or making a differentially private recommender system[14 ] . Recently , Chen et . al.[6 ] presented a more user centric approach that gives user control over fine grained categories ( represented as a fixed hierarchical taxonomy ) which they want personalization on . These techniques however require users switch to these new systems from their existing OSPs , which is not practical , while we aim at finding personalization in existing OSPs .
3 . PROBLEM FORMULATION
In this section , we introduce our notations and define the technical problem that we consider in this paper . 3.1 Notation
Let I = {i1 , i2,···} be the universe of all the items being present at the personalization server , where , an item might represent a url ( for search engines like Google , Bing etc. ) , a product web page ( for e commerce sites like Amazon , NetFlix etc . ) or an advertisement ( for ad servers ) . For a query q , let πq and σq denote the personalized and vanilla lists of content . In the following discussion , we will often drop the subscript q , when the query is understood from the context . As mentioned earlier , both π and σ are treated as permutations over a set of items7 I ⊂ I . Technically , a ranking/permutation8 is a bijection from a set to itself . For any permutation π , π(i ) denotes the item assigned to rank i , hence π = ( π(1 ) , π(2),··· ) . π−1(d ) denotes the rank i of an item d ∈ I in π such that π(i ) = d . For any two permutations π and σ , we use the notation σ−1(π(i ) ) to denote the rank of the item π(i ) in σ . Observe that π−1(π(i ) ) = i . We use Sn to denote the set of all permutation of n items . We assume that there are T topics {β1 , β2,··· , βT} in our system where each topic βk is defined as a multino mial distribution over a fixed vocabulary V . For each word w ∈ V , we have a parameter βk,w = Pr(w | βk ) such that w∈V βk,w = 1 . Each item9 i ∈ I is represented by its topic map θi which is a multinomial distribution over the set of topics . By inspecting each component of θi , one can infer how related the item is to a particular topic . We now describe our representation of topic level user profile information . For each user u and topic βk ∈ β , we associate a variable ηu,k ∈ R . It captures the importance of βk ( more relevant topics have higher values ) for serving personalized content to u . The complete profile information ( we name it as latent personalization vector ) is denoted by ηu = ( ηu,1 , ηu,2,··· , ηu,T ) . We often drop the subscript u and refer to it simply as η whenever the user is understood from the context .
7Strictly speaking , they may not contain exactly same set of items , but it is normally the case . Eg in our experiments with Google , personalized results are identical to vanilla for 85 % of queries and contain only avg . 14 % extra items on the remaining . These extra items can be handled easily by adding them to the end of personalized ( or vanilla ) list .
8We often use them interchangeably . 9Specifically , the textual content or meta data of the item .
3.2 Problem
Our strategy to learn the personalization vector η is to repeatedly frame queries to the server and observe the difference between its vanilla and personalized responses . For a given user u , we first sign in to her account and submit a query to the server . This gives the server an opportunity to personalize the result by using u ’s profile information and through this process , we obtain the personalized response π . Next , we submit the same query in an anonymized form , by removing all cookies from the http request , thus removing all account details ( but keeping all other parameters same such as IP address , User Agent , etc ) This time the server sends back the vanilla response σ . We expect that as this process is repeated many times , the cumulative difference between these two responses will become statistically significant and contain substantial evidence of η . In this paper , we study the following problem : Given pairs of query results ( σ1 , π1 ) , ( σ2 , π2)··· ( σm , πm ) , how do we learn the latent personalization vector η , for a given user ?
Non profile factors Although personalization normally yields its benefits by presenting more relevant results to the users , it is also known to be less effective and even detrimental in many cases . For example , while personalizing results are known to work well for short and ambiguous queries [ 25 ] where user searching same query may be looking for completely different things , for common and specific queries two users with very different profiles are normally looking for the same information and are satisfied with the same ( ordering of ) results . In such cases , even though user ’s profile implies re ranking , the server may decide not to personalize . This creates a problem for our approach as a search engine ’s decision whether to personalize the result of a search query or not , is influenced not only by the topical content of the query result , but also through other filtering processes that are hidden from us .
We take care of this in our model by introducing a latent parameter that , during training phase , filters out such inexplicable events and reduces the noise in the personalization vector . In our experiments with the Google dataset , we found several instances of queries with results at higher ranks having higher “ scores ” ( see Section 4 for definition of scores ) the ones at lower ranks , that were not personalized , while another query with similar scores was personalized . Without this latent parameter , these instances would have reduced the effectiveness of learning η .
4 . LTP MODEL
The goal of topic based personalization learning is to capture the following information : topics on which personalization takes place and a weight vector corresponding to the degree of personalization on these topics . In addition , the approach has to scale with large number of queries . To meet these objectives , we first propose Latent Topical Personalization model ( LTP ) to study the problem from a bayesian perspective . Following that , we develop efficient variational inference and estimation techniques for learning the parameters of this model . 4.1 Model Description
We now formally describe the proposed LTP model . LTP models ( Figure 1 ) both topics and personalization . It involves a topic block to model the topical content creation of
Figure 1 : Graphical model representation of LTP . the items and a personalization block to model the personalized responses ( ie π1 , π2,··· , πm ) .
Topic Block The topic block follows the description of standard topic models ( cf LDA [ 3 ] ) and we present it here for the sake of completeness . The generative process for the topic block is as follows
• For each topic βk , k = 1 , 2··· , T 1 . Sample βk ∼ Dirichlet(ν ) .
• For each item i ∈ I
1 . Sample its topic map θi ∼ Gaussian(0 , diag(α2) ) . 2 . For each word position j = 1··· ni for item i
( a ) Sample topic Ki,j with Pr(Ki,j = k ) ∝ eθi,k . ( b ) Sample word Wi,j ∼ Multinomial(βKi,j ) .
The joint distribution for the topic block can be written as i∈I p(θi | α ) · T k=1 p(βk | ν ) p(θ , K , W , β | α , ν ) = ni p(Ki,j | θi ) · p(Wi,j | Ki,j , β1···T )
( 1 ) i∈I j=1
Personalization Block Our design of the personalization block is little more involved . The main difficulty stems from the non profile based factors , which may lead to no re ranking of results even when the user profile ( ie η ) indicates personalization should happen . In LTP , we achieve it by introducing a latent switch variable z ( refer to Figure 1 ) . Independently , for each query , we sample z , governed by a prior parameter τ and based on its value decide whether to allow topical personalization or not . The parameter τ is user specific and controls the rate at which topical personalization takes place ( for that user ) .
Based on the value of z , we pick a probability distribution over permutations and sample π from it . Probabilistic models on permutations have recently been applied to solve various problems related to ranking [ 20 ] . Probability distributions defined over permutations can be broadly categorized into two types—distance based and score based . In a distance based model [ 15 ] , the probability of a permutation is defined according to its distance from a central permutation . They have rich expressive power as they can incorporate a wide variety of distance functions over permutations but are , in general , computationally inefficient . and at each stage uses both the central permutation σ and a set of scores , to determine π . Each item d ∈ I is assigned a score ηT θd . In the ith stage , g selects the item π(i ) with probability exp(ληT θπ(i ) + ( 1 − λ)(i − σ−1π(i) ) ) j≥i exp(ληT θπ(j ) + ( 1 − λ)(i − σ−1π(j) ) )
The working principle for g is similar to f , except that it now allows for deviations from σ only if it is explained by the scores . Parameter λ is tuned to adjust the relative importance of the scores and the central permutation σ . For example , if λ = 0 then the scores are ignored and if λ = 1 then the central permutation does not play any role . We treat 0 ≤ λ ≤ 1 as a free parameter whose value needs to be learned from the data . The overall probability of sampling π is given by exp(ληT θπ(i ) + ( 1 − λ)(i − σ−1π(i) ) ) j≥i exp(ληT θπ(j ) + ( 1 − λ)(i − σ−1π(j) ) ) It can be verified that g is also a valid probability distribution . g(π | η ; σ , λ , θ ) = i
The generative process for the personalization block can be described as
• For each user u
1 . Sample τ ∼ Beta(δ , δ ) . 2 . Sample η ∼ Gaussian(0 , diag(γ2 ) )
• For each query qi , i = 1 , 2,··· , m
1 . Sample zi ∼ Bernoulli(τ ) to decide whether to allow topical personalization .
2 . If zi = 1 , sample πi ∼ g(· | σi , λ , θ , η ) . 3 . Else , sample πi ∼ f ( · | σi , µ ) .
The joint distribution for the personalization block can be written as m p(π,z , τ , η | θ ; γ , δ , µ , λ , σ ) = p(η | γ ) · p(τ | δ ) p(zi | τ ) · g(πi | σi , λ , θ , η)zi f ( πi | σi , µ)1−zi
( 3 ) i=1
Finally , the full joint distribution for LTP can be obtained by multiplying Equations 1 and 3 . We treat the parameters ν , α , δ , γ as constant and do not consider learning them . However , the parameters µ and λ that controls the permutation models need to be learned . We have assumed a Gaussian prior on η . The role of this prior is to set η to zero when we do not observe any significant difference between π and σ i.e πi ≈ σi .
We first assume that λ and µ are predefined constants and describe the inference ( LTP INF ) of the personalization vector η based on these values in Section 42 We will then use LTP INF to also estimate these parameters in Section 43 4.2
Inference of Personalization Vector
The key inferential problem that we study in this work is to obtain the posterior distribution on the latent variables ie to determine p(θ , K , β , z , τ , η | σ ; λ , µ ) . As with simpler topic models , the exact inference is intractable and therefore , we resort to approximate inference techniques . Given
Figure 2 : An example illustrating the steps of f . We have assumed µ = 1 . At each stage , the actual outcome is marked in blue and the most likely outcome is marked in red .
Score based models [ 12 ] , on the other hand , are very efficient as they divide permutation construction into stages and assign scores on each stage such that the final probability is a combination ( multiplication ) of stage wise scores . However , being defined as a specific function over scores , they have limited expressive power eg they can not take into account any central permutation in the generative process . For LTP , we have a central permutation ( vanilla list σ ) and want to model π as being generated from it . Further , as explained later , we define scores on items as a function η . Therefore , we need a model which combines the notion of distance with scores and is computationally efficient .
The probability distribution f ( Figure 1 ) is a process for generating the personalized response π , and is decomposed into sequential stages . Observed that ( see Figure 1 ) this process is activated only if z = 0 , thereby , implying no topical personalization should happen . In the first stage , we pick exp(µ(1−σ−1π(1) ) ) j≥1 exp(µ(1−σ−1π(j) ) ) . Note the item π(1 ) with probability that this probability is maximum when the two permutations agree with the first position ie π(1 ) = σ(1 ) . However , if we happen to pick some other item ie π(1 ) = σ(1 ) , then for the second stage , the most likely outcome is to bring back the item σ(1 ) and put it at the second position of π ie π(2 ) = σ(1 ) .
In general , in the kth stage , the probability of selecting exp(µ(k−σ−1π(k) ) ) π(k ) is j≥k exp(µ(k−σ−1π(k) ) ) . Intuitively , at each stage k , the model determines the items among σ(1 ) , σ(2),··· , σ(k− 1 ) which are not yet sampled by f and assigns higher probability on picking them . In Figure 2 gives an example of this sampling process .
Considering all the stages , we obtain the overall probability of sampling π which is given by the following expression f ( π | σ , µ ) =
 exp(µ(i − σ−1π(i) ) ) exp(µ(i − σ
−1π(j) ) )
 i
( 2 ) j≥i f ( π | σ , µ ) ≥ 0 for all π ∈ Sn and
It can be shown that f is a valid probability distribution ie π f ( π | σ , µ ) = 1 . The parameter µ controls the spread of the distribution ie if µ → 0 then f converges to the uniform distribution over Sn ; otherwise , for µ > 0 the distribution is concentrated around σ . We assume µ ≥ 1 .
We now describe our next permutation model g that captures the topic level personalization which is invoked only if z = 1 . Model g is also decomposed into sequential stages
Algorithm 1 LTP INF : Variational Inference Algorithm for LTP 1 : Input Training data set 2 : Output Values ( φ 1···m , κ 3 : Initialization Randomly initialize to ( φ(0 ) 1 , κ(0 )
( π , σ)1,2,··· ,m ; values 2 , ˜η ) that maximize Λ ;
1···m , κ(0 ) 2 > 0 ;
1 , κ(0 ) 2 ,
λ , γ , δ , µ ;
1 , κ for
1 , κ(0 )
2 , ˜η(0) ) ;
1 ← δ +m 2 ← δ +m
1m > 0 and κ(0 ) ˜η(0 ) ) such that 1 > φ(0 ) 4 : i ← 0 ; ∆(0 ) ← Λ(φ(0 ) 1···m , κ(0 ) 5 : while ∆ has not converged do 6 : j=1(1 − φ(i−1 ) 7 : j=1 φ(i−1 ) 2 ) − Ψ(κ(i ) i ← i + 1 ; κ(i ) κ(i ) for j = 1m do µj ← Ψ(κ(i ) 1 ) + ln f ( πj Er[ln g(πj | η ; σj , θ , λ) ] ; j ← 1/(1 + eµj ) ; /* Update φj */ φ(i ) j ;
) ; j
8 : 9 : 10 :
11 : 12 : 13 :
| σj , µ ) − end for ˜η(i ) ← argmax
˜η
Λ(φ(i )
1···m , κ(i )
1 , κ(i )
2 , ˜η ) ; /* Use conju gate gradient to optimize this block */
1···m , κ(i )
1 , κ(i )
2 , ˜η(i) ) ;
14 : ∆(i ) ← Λ(φ(i ) 15 : end while 16 : return ( φ(i )
1···m , κ(i )
1 , κ(i )
2 , ˜η(i ) ) function ,
Λ(φ , κ1 , κ2 , ˜η ) = Er [ ln p ] + H(r )
( 4 ) where H(r ) is the entropy and Er denotes expectation wrt the distribution r .
We use block coordinate wise ascent to maximize the expression in Equation 4 . Intuitively , we perform fixed point iterations by updating one block of parameters at a time , keeping all other parameters fixed to their most recent value . The update rule for parameters φ1,2,··· ,m , κ1 , κ2 are obtained by setting the partial derivatives of Λ to zero . Due to our choice of r , the update rules for φ , κ1 , κ2 are particularly simple and have closed form expressions .
To maximize Λ with respect to ˜η , we use the conjugate gradient algorithm11 . The objective function for ˜η can be written as
L(˜η ) = − 1
· ˜η +
2γ2 ˜η
( 1 − φi ) · Er [ ln g(πi | σi , θ , λ ) ] i
It can be proved that L is concave ( with respect to ˜η ) and therefore , using simple optimizers like conjugate gradient , we will be able to obtain the global maximum [ 4 ] . Algorithm 1 summarizes the inference procedure . See Section 421 for the derivations .
421 Derivations We now outline the key steps in deriving the update equations . Our first goal is to obtain the expressions for the entropy and expectation terms ( Equation 4 ) . entropy expression given by −m
Entropy of z The multinomial variate z has a simple i=1(φi ln φi + ( 1− φi ) ln(1−
φi) ) .
Entropy of τ The entropy expression for the beta variate
11http://enwikipediaorg
/wiki/Nonlinear conjugate gradient method
Figure 3 : Variational distribution used for inferring personalization in LTP . the non conjugacy of π and θ , sampling based techniques In this paper , we propose a are unlikely to be efficient . variational approximation scheme . In a variational inference , one defines a family of simpler distribution over the latent variables to approximate the true posterior distribution . This family of distribution is indexed by additional parameters ( called variational parameters ) which are tuned so as to minimize the KL divergence with the true posterior . We first simplify the inference by breaking it into two parts . For the first part , we ignore the dependency between the topic and the personalization block . Therefore , our strategy is to first infer the topics and use the inferred topics and the topic maps of the items to carry out inference for the personalization block . This will simplify the exposition greatly and the ideas that we develop here will carry over naturally to the general case of inferring the blocks jointly . We revisit the inference for the complete model in Section 44 Inference for the topic block follows standard techniques ( see eg [ 3 ] ) and therefore , we omit the details here . For the rest of this sub section we assume that the topics have been inferred and develop an inference scheme for the personalization block . For the personalization block , the key inferential problem is to obtain the posterior distribution p(z , τ , η | σ ; λ , µ ) . This posterior is approximated with the help of a variational distribution r . Figure 3 illustrates its graphical model representation . The personalization vector η is assumed to be Gaussian with the following density − 1 2γ2 ( η − ˜η ) r(η | ˜η ) = ( 2πγ2 )
· ( η − ˜η )
2 exp
− T
Here , the variational parameter ˜η represents the mean of the gaussian and its variance is γ2I . For query qi we assume that zi is sampled from a Bernoulli distribution with parameter φi ∈ ( 0 , 1 ) . Finally , for user u , we assume that τ is sampled from a beta distribution having the following density function r(τ | κ1 , κ2 ) =
Γ(κ1 + κ2 ) Γ(κ1)Γ(κ2 )
τ κ1−1(1 − τ )κ2−1 where the parameters κ1 , κ2 > 0 and Γ(x ) is the Gamma function . We use the notation Ψ(x ) for the digamma function which is defined as d dx ln Γ(x ) .
The next step in our variational analysis is to learn the particular value of the parameters ( φ , κ1 , κ2 , ˜η ) that minimizes the KL divergence between r and the true posterior It can be shown10 that minimizing the KL divergence p . has the same effect as maximizing the following objective
10Refer to [ 3 ] for the proof .
τ is well known 12 and given by the following expression , ln Γ(κ1 ) + ln Γ(κ2 ) − ln Γ(κ1 + κ2 )
−(κ1 − 1)Ψ(κ1 ) − ( κ2 − 1)Ψ(κ2 ) + ( κ1 + κ2 − 2)Ψ(κ1 + κ2 ) Entropy of η The entropy of a gaussian is a function of the covariance matrix only , which is γ2I and therefore a constant . Deriving Er[ln p(zi | τ ) ] This expression requires us to determine Er[ln τ ] which can be obtained using the technique outlined in Blei etal [ 3 ] . Finally , the expression is given by φi(Ψ(κ1 ) − Ψ(κ1 + κ2 ) ) + ( 1 − φi)(Ψ(κ2 ) − Ψ(κ1 + κ2 ) ) Deriving Er[ln p(τ | δ ) ] This derivation is similar to the last one and is given by
( δ − 1)(Ψ(κ1 ) + Ψ(κ2 ) − Ψ(κ1 + κ2 ) )
2γ2 ˜η · ˜η .
Deriving Er[ln p(η | γ ) ] This can be derived using standard gaussian identities 13 . The expression is simply given by − 1 Deriving Er[ln(πi | σi , η ) ] This derivation is more subtle . First observe that the expression is of the form ln(eθ 1η + eθ 2η + ··· ) . This expression is in general unwieldy as the exponential terms appear inside the logarithm . We use a standard trick of simplifying this form in the following way , ln(eθ
1η + eθ
2η + ··· ) ≤ 1 ζ
( eθ
1η + eθ
2η + ··· ) + ln ζ − 1 where ζ is an additional variational parameter . Observe that the inequality holds for every ζ > 0 and equality is attained only for
ζ = eθ
1η + eθ
2η + ···
We now have to deal with the expectation term Er[eθη ] . Observe that in the variational model , we have assumed ηi to be independent ( conditioned on γ ) and therefore , this expression is equivalent to
Er[eθη ] = Er[ eθk·ηk ] k
= k
Er[eθk·ηk ] k · ˜η2 k ) .
The expectation term can be derived using the Moment Generating Function of gaussian distribution and evaluates to
2 θ2 k exp(θk ˜ηk + 1 The expression for ELBO can be obtained by summing up all the entropy and expectation terms . Finally the update equations are derived by setting the partial derivatives of ELBO to zero for each block . 4.3 Parameter Estimation
We now focus our attention at learning λ and µ . We use Maximum Likelihood Estimators ( MLE ) for this , where one finds the value of the parameters that maximizes the ( log ) likelihood of the observed data ie the following expression ln p(π | λ , µ ; σ ) = ln p(πi | λ , µ ; σi )
( 5 )
12see http://enwikipediaorg/wiki/Beta distribution 13See wwwcsnyuedu/ roweis/notes/gaussid.pdf i=1 m
Algorithm 2 LTP EM : Variational EM Algorithm for LTP 1 : Input Training data set ( π , σ)1,2,··· ,m 2 : Output Values ( λ , µ ) that maximize Equation 5 3 : Initialization Randomly initialize ( λ(0 ) , µ(0 ) ) st 0 ≤
λ(0 ) ≤ 1 and µ(0 ) > 0 .
4 : while ( λ , µ ) have not converged do 5 : E step ( φ 1 , κ Λ(i)(λ , µ ) ←
1···m , κ
• •
E r(φ,κ 1,κ
2,˜η )
[ ln p ] ;
/* The variational inference step */ 2 , ˜η ) ← LTP INF(σ , π , λ(i ) , µ(i) ) ;
6 : M step /* Learn new estimates of the parameters */
•
( λ(i+1 ) , µ(i+1 ) ) ← argmax µ>0 1≥λ≥0
Λ(i)(λ , µ ) i ← i + 1 7 : 8 : end while 9 : return ( λ(i ) , µ(i ) )
However , to calculate the likelihood function , we have to marginalize over the latent variables which is difficult in our model for both real variables ( η , τ ) , as it leads to integrals that are analytically intractable , and discrete variables ( z1···m ) , it involves computationally expensive sum over exponential ( ie 2m ) number of terms .
We use the variational Expectation Maximization ( EM ) algorithm to circumvent this difficulty . In the E step , Algorithm 1 approximates the true posterior distribution over the latent variables , using the current estimates of the parameters . The variational parameters learned in this step are used in the subsequent M step to maximize the likelihood function ( over the true parameters λ and µ ) .
Algorithm 2 summarizes the steps of the variational EM . It can be shown ( see Section 421 ) that the constraint maximization problem in step 6 is a concave program and therefore , can be solved optimally and efficiently [ 4 ] . 4.4 Learning Topic Distributions
For inference in the topic block ( Figure 1 ) , we augment our variational distribution with additional parameters in the following way . Topic distribution βk is sampled from a Dirichlet prior with parameters { ˜βk,w | w ∈ V } . The topic assignments Ki,j are sampled from a multinomial distribution with parameters ωi,j,1···T and θi is sampled from a normal distribution with mean ˜θi and variance α2I . Using the same recipe as in Section 4.2 ( cf Equation 4 ) , we arrive at the following simple update rule for learning the topic distributions
βk,w = ν +
ωi,j,k i,j
Wi,j =w
The topic assignments ωi,j also has a closed form update rule as given by ωi,j,k ∝ exp(Er[ln θi ] + Er[ln βk,wi,j ] )
Learning of topic maps of the urls ( ie θi ’s ) is more subtle . The main difficulty stems from the coupling between the personalization and the topic blocks through θ . While determining Er[ln g(π | η , θ ; σ , λ ) ] ( step 8 of Algorithm 1 ) , we now have to take expectation over θ , in addition to η . Specifically , we have to compute an expectation of the form
2 θ · θ ) ] which is however tracktable due
Er[exp(λη · θ + λ2γ2 to our assumption of independence and gaussian priors on θ and η . We use gradient descent on θ to solve it . The rest of the calculation remains unchanged .
5 . EXPERIMENTS
In this section , we describe a comprehensive set of experiments designed to evaluate the accuracy and effectiveness of our techniques . 5.1 Datasets
The input to our algorithm consists of a set of queries and the personalized and vanilla results ( ie π , σ pairs ) for them , returned by a search engine . During the training phase , we present these queries to LTP and let it learn the personalization vector η . Once η is learned , the next step is to validate it , by measuring how well it corresponds to the ground truth . However , in practice , such validation schemes are often difficult to design as the search engines do not reveal the actual user profile14 . We therefore perform our experiments on both real world dataset comprised of Google search history of a few users , and a large scale synthetic dataset .
511 Google Search Personalization We collected search result and history data15 from 10 real Google users . This data collection was done as part of a larger survey to understand the topic level personalization and privacy concerns of users , and is part of an ongoing initiative to build a privacy evaluation toolkit ( see Section 12 ) Of this larger group , due to privacy concern , only 10 participants volunteered to share their search history .
For these 10 users , we fetched their entire history of search queries . The average number of ( distinct ) search queries was 872 . We issued each query to Google both by using their login credentials and without it to retrieve the search results . We used the Mallet [ 18 ] toolkit to extract topics from the entire collection of urls 16 returned for all queries , for each user .
We found ample evidence of profile based personalization on Google . Even when the personalized and vanilla queries were performed with identical parameters , such as location and IP address ( same machine ) , user agent , other http connection , etc . , roughly 30 % queries received personalized results . We also found that the personalization is much more subtle compared to the impression we get from search personalization literature ( and our experiments with AlterEgo server)—most queries ( ≈ 70 % ) were not personalized and while there were some queries with fair amount of personalization , on an average , we observed very little difference between the results17 .
512 AlterEgo
14Google , however , publishes the categories of topics used to serve personalized ads . Unfortunately , this data is not quite helpful as the categories are very high level and do not convey rich enough information .
15http://historygooglecom/history 16We used the snippets that Google returns along with the search results to obtain text for the urls .
17The avg . EMD ( earth mover ’s distance ) over queries the EMD of moving a with personalization was 5.9 ( eg single url at rank 5 to rank 1 is 4 )
We use an open source search personalization engine called AlterEgo [ 17 ] to generate the synthetic dataset . AlterEgo contains implementation of various popular profiling and personalization techniques ; we used their “ unique matching ” technique for our experiments18 . In our simulation , we used AlterEgo as a surrogate personalization engine ie we obtain the vanilla result from Google and use AlterEgo to personalize it . The benefit of this approach is that we can train AlterEgo on topics of our choice and use this information to validate the model output η . The work flow and details of the data generation steps are presented below . Generating Topics We extracted a set of 500 topics by running Mallet on approximately 420k urls obtained from the Delicious dataset[27 ] . We manually select 50 topics and label them into 10 categories ( examples are health , cooking , science , finance , etc. ) ; these topics serve as a ground truth for us . The selection of these topic categories and urls ( used in the next step ) is intended to simulate a typical user behavior , where , a user in interested in ≈ 10 categories of topics . Training AlterEgo For each topic , we inspect the topicmaps of the urls and identify the ones which have significant ( > 0.2 ) weight ( on this topic ) . These urls are used to train AlterEgo profile . We generated 10 profiles trained on a subset of 1 to 10 topics ( ie 10 profile for 1 topic , 10 profile on 2 randomly selected topics , and so on ) , generating a total of 50 profiles . Queries We generated 500 queries for each topic by randomly combining the top 10 relevant words from them . This gives us a total of 5k queries ( over 10 categories ) . For each query , we retrieved the vanilla results from Google . Note that , if a query is related to a topic used for training the profile , only then AlterEgo will be able to personalize it . Otherwise , the vanilla and personalized results will be more or less identical . 5.2
Implementation Details
We implemented Algorithms 1 and 2 in the Java programming language . For solving the convex program in Algorithm 2 ( step 6 ) , we use JOptimizer [ 9 ] a java based open source optimization package . All our experiments are carried out on a Intel Pentium IV machine with 3.0GHz processor and 4GB of RAM .
We use the following values of the hyperparameters : δ = 2.0 , γ = 10 For computational efficiency , we used Mallet for inference in the topic block ( see Figure 1 ) and do not use the inference process described in Section 44 5.3 Results with the AlterEgo data set
In this section , we summarize the result of our experi ments with the AlterEgo data set . 531 Precision Recall Our first set of experiments are designed to evaluate the accuracy of LTP in correctly learning the personalized topics . On each AlterEgo profile , we train LTP and learn the personalization vector η . Next we compare it with the actual list of topics that were used to train this profile ( by AlterEgo ) . Let Tact be the true set of personalized topics and Tinf be the one inferred by LTP . For this experiment ,
18We also did experiments with their “ matching ” technique , and got very similar results which are omitted due to lack of space . we measure the precision and recall values , where precision is defined as ie the fraction of reported topics
|Tact∩Tinf |
|Tinf | that are actually personalized and recall by ie the fraction of the original personalized topics that we are able to identify .
|Tact|
|Tact∩Tinf |
P@1 97.80
P@3 84.02
P@5 70.60
R pre P@+1 P@+3 MAP 97.60 84.66
54.44
70.69
Table 1 : Performance ( in % ) of LTP in finding personalized topics .
We re order the topics based on the ( decreasing ) value of η computed by LTP . For each k , we declare the top k topics ( with maximum η values ) as personalized and calculate the precision and recall value for this decision . Table 1 summarizes the precision scores obtained by LTP . Specifically , we evaluate its performance in terms of Precision@1(P@1 ) , P@3 , P@5 , R precision ( R pre ) and mean average precision ( MAP ) [ 5 , 7 ] . Note that the size of actual topics was quite different for different runs ( varies from 1 10 ) . Hence , along with the top k topics , we also study the precision at |Tact+k| ( denoted as P@+k ) .
Figure 4 : Precision Recall results for LTP in retrieving the personalized topics .
In Figure 4 , we illustrate the recall performance of our algorithm . At the expense of low precision ( < 0.4 ) , LTP is able to retrieve all the personalized topics ( recall ≥ 0.93 ) and its recall performance is relatively insensitive to precision ; however , if we require high precision ( > 0.8 ) , the recall drops to ≈ 05 As evident from the figure , a typical operating characteristic of LTP is precision ≈ 0.7 and recall ≈ 0.7 , which is achieved when we return top 3 topics . 532 Classification Tests In this section , we develop two classification tests to evaluate LTP ’s predictive power . For both these experiments , we randomly split the π , σ list into data sets D1 ( 80% ) , used for training LTP , and D2 ( 20% ) , used for testing . We repeat this split with 10 random seeds and report the average number in all the data presented below .
Query Disambiguation In this experiment , while testing on D2 , we hide which result is personalized and which one is vanilla and the task of the model is to determine the correct labels . We proceed with the classification task in the following way . Let η be the parameter learned by LTP during the
#topics
1 2 3 4 5 6 7 8 9 10
Time ( secs )
Accuracy ( µ ± σ ) LTP EM LTP INF LTP EM LTP INF .74 ± .09 .72 ± .09 .70 ± .09 .72 ± .06 .68 ± .06 .70 ± .05 .69 ± .04 .67 ± .05 .69 ± .05 .67 ± .05 .67 ± .04 .65 ± .05 .65 ± .05 .65 ± .04 .63 ± .04 .63 ± .04 .63 ± .05 .62 ± .05 .62 ± .02 .62 ± .02
80.7 154.3 221.6 272.2 336.1 333.2 342.5 348.2 354.4 359.2
22.7 31.5 42.4 53.7 69.8 70.7 71.1 73.6 76.4 79.5
Table 2 : Summary of results with the AlterEgo dataset training . For input lists l1 and l2 , LTP calculates the likelihood values p(l1 | l2 , η ) and p(l2 | l1 , η ) and whichever likelihood is higher is assigned to the personalized result ie if p(l1 | l2 , η ) > p(l2 | l1 , η ) then l1 is declared to be the personalized result and vice versa . We name this test as P V disambiguation for a given profile . Over all the test points in D2 , the fraction of queries that were labeled correctly is referred to as disambiguation accuracy .
Table 2 summarizes the result of this experiment . In summary , we achieve disambiguation accuracy in the range of 62 74 % . For each profile , we collect the accuracy values for the 10 different runs and report its mean and standard deviation ( µ ± σ ) . Observe that our accuracy decreases slightly as the AlterEgo profile is trained with more and more topics . Table 2 also reports the training time of LTP EM . For profiles trained with many topics , LTP EM takes more time to converge . We repeat the experiment with LTP INF with the parameter values fixed to λ = 0.9 and µ = 100 As the results show , LTP INF is up to 5 times faster to train but achieves slightly lower accuracy . The accuracy however , improves slightly ( < 3 % ) if we increase the amount of training data ( D1 ) from 80 % to 90 % ( not shown in the table ) .
User Classification For this experiment , we consider groups of users ( ie profiles ) and develop a classification test within the group members . We vary the size of the group from 2 to 10 and for each group size , randomly pick 10 groups . For each group G , we present a ( π , σ ) pair to LTP but do not reveal the user it belongs to . The task of the model is to correctly predict the user . We again use the likelihood test for this task . Specifically , for each user u ∈ G and input ( π , σ ) , we calculate p(π | σ , η u ) ( η u learned during training ) and output the user for which the likelihood attains its maximum value .
In Figure 5 , we summarize the result of this experiment . There are two parameters in this experiment the size of the group and the number of topics used to train AlterEgo for each profile in the group . For simplicity , we present here results for the homogenous case , where we combine profile which are trained on the same number of topics 19 . Observe that the accuracy reported by LTP is significantly higher than a random guess ( which is 1/g , g being the group size ) . The accuracy decreases slightly if profiles are trained with many topics . We believe this reduction in accuracy is also an
19We also performed experiments on the general case ( eg by grouping profiles trained on 3 topics with 5 topics ) . The results are similar and not repeated here .
Google Category
Topic in LTP
Comics & Animation online read manga
Anime & Manga Autos & Vehicles Vehicle Shopping
Computers
Software Utilities World Localities kyojin shingeki chapter car india chrysler price jaguar sport bmw class import common org public implement seoul citi hotel
South Asia location shop mall coex
η
0.60
0.42
0.15
0.13
Table 3 : Correlation between personalized topics in LTP and Google categories .
User Id
1 2 3 4 5 6 7 8 9
15 Topics
74±5 68±5 67±13 54±8 54±11 85±7 73±4 66±3 52±4
20 Topics
70±5 70±4 72±14 51±6 47±9 78±5 70±5 62±3 52±3
50 Topics
70±6 70±4 67±13 55±6 49±11 84±4 71±6 61±4 50±4
100 Topics
73±4 65±3 73±11 59±7 43±9 81±7 73±6 64±3 54±4
Table 4 : Accuracy of LTP over 9 Google users . with a user20 . We try to match topics with high η ( top k such topics ) with the broad categories in Google . Table 3 shows the result of such matching for 3 users . Take for example , the “ Anime and Manga ” category , that was also assigned a very high η = .6 ( compared to an average value of .004 ) by LTP .
Such anecdotes show that our techniques have , in fact , learned the personalization vector correctly . 542 Quantitative Experiments Query Disambiguation Table 4 summarizes the result of query disambiguation on the Google dataset . We first study the effects of number of topics ( T ) chosen for the user . We notice that only a few topics 15 50 are enough to get good accuracy for any user . Our accuracy results differ significantly for different users , varying from as low as 54 % to 85 % . We believe this is because the amount of personalization is different for various users , and this affects the learning accuracy of our techniques .
User Classification Table 5 show that even with 3 users , we are able to get an accuracy of up to 60 % . For this experiments , we extracted η values over a common set of topics for each user . These η values learned were also very different for different users ( data not shown ) . This shows that η is in fact learned tailored to the personalization of each user . 6 . CONCLUSIONS
In this paper we have presented a novel approach to extract user profile information in the form of personalization
20Shown in Google ads preference manager https://wwwgooglecom/settings/ads/onweb/
Group
Size
2 3
10
.59 ± .06 .48 ± .04
20
Number of Topics .65 ± .04 .61 ± .06 .60 ± .05 .53 ± .05
50
100
.58 ± .05 .50 ± .06
Table 5 : User classification accuracy on Google data .
Figure 5 : Performance of LTP in user classification . artifact of our data generation—profiles trained on multiple topics can ( and do ) have topics in common , that will make it hard to distinguish personalized response on two profile trained on the same topic .
In summary , these results , together with the precisionrecall values from last section highlight that our model fits the data well and learns the correct set of personalized topics on synthetic data .
5.4 Results with the Google dataset
In this section we describe the results with the Google dataset . Note that since we do not know the actual personalization on different topics ( ground truth ) for a real Google user , we cannot perform the precision recall experiments as with AlterEgo dataset , and resort to only query disambiguation and user classification test described above . However , we also perform some qualitative tests that give ample indication that we have found a good personalization vector .
541 Qualitative Evidences for Correctness of η We now present our analysis on finding qualitative correctness of η using evidences of personalization . An evidence is an instance of π , σ where results were re ranked such that the ones with η were moved up . Note that while such evidence have no statistical significance , they are much more helpful for a user ’s understanding of his profile compared to the personalization vector . Such evidences are a core feature of the privacy toolkit we are building ( see Section 12 )
Figure 6 shows an example evidence of personalization happening on a user ’s account . The result for query Q ( “ how to decide mixing of markov chain ” ) and theta values for two relevant topics T1 ( about “ Algorithms ” defined by words algorithm , design , complexity ) and T2 ( about “ Probability ” defined by words probability , distribution ) are shown . For this user , η value for T1 is very high compared to T2 . Observe that the wiki link U1 ( in the box ) , although less relevant to the query , is placed higher in the personalized results . As our analysis shows , U2 is has a high weight on topic T1 compared to U2 , which leads to this personalization . The user can therefore see not just his inferred interests ( more in “ Algorithms ” compared to “ Probability ” ) , but also how it affects his results .
We next move to another qualitative analysis of η by comparing it directly with the categories Google itself associates
Topic
T1 T2 .40 η .90 .30 Q .01 U1 .15 .10 .25 .01 U2
Figure 6 : An example to illustrate the difference between personalized ( left ) and vanilla ( right ) search results ( for a real user ) returned by Google . vector over topics from commercial OSPs ( such as Google search ) . Our approach treats OSPs as black boxes , ie assumes no knowledge of the personalization algorithms and history of users maintained by them , and works by comparing the personalized and vanilla content served by them .
To the best of our knowledge , this is the first work that tries to extract information based solely on mining the output of OSPs . This aspect of our work make it unique and is beneficial in not just enabling access to ( so far hidden ) profiles in OSPs , but also in providing a novel and practical approach for retrieving information from OSPs by mining differences in their outputs .
Our approach also has direct benefits for end users , as it for the first time , enables them to access their ( so far hidden ) profile information tracked by an OSP . While being an informational tool by itself , this has wider implications to the outlook of user privacy research—it can be used to infer the personalization happening on sensitive topics ( eg financial , medical history , etc. ) , which a user may not be comfortable with . We believe that this can be used to build an enduser privacy perserving tool and are currently working on a prototype for the same .
7 . REFERENCES [ 1 ] Bennett et . al . Modeling the impact of short and long term behavior on search personalization . In SIGIR , page 185–194 , 2012 .
[ 2 ] J . Bischof and E . Airoldi . Summarizing topical content with word frequency and exclusivity . In ICML , 2012 .
[ 3 ] Blei et . al . Latent dirichlet allocation . J . Mach . Learn .
Res . , 2003 .
[ 4 ] S . Boyd and L . Vandenberghe . Convex Optimization .
2004 .
[ 5 ] C . Buckley and E . M . Voorhees . Retrieval evaluation with incomplete information . In SIGIR , 2004 .
[ 6 ] Chen et . al . Ups : efficient privacy protection in personalized web search . In SIGIR , 2011 .
[ 7 ] Craswell et . al . Overview of the trec 2005 enterprise track . In TREC , 2005 .
[ 8 ] Z . Dou , R . Song , and J . R . Wen . A large scale evaluation and analysis of personalized search strategies . In WWW , 2007 .
[ 9 ] JOptimizer . http://wwwjoptimizercom/
[ 10 ] A . Korolova . Privacy violations using microtargeted ads : A case study . In ICDMW , 2010 .
[ 11 ] Lindamood et . al . Inferring private information using social network data . In WWW , 2009 .
[ 12 ] R . Luce . Individual Choice Behaviour . WIley , 1959 . [ 13 ] J . Luxenburger , S . Elbassuoni , and G . Weikum .
Matching task profiles and user needs in personalized web search . In CIKM , 2008 .
[ 14 ] A . Machanavajjhala , A . Korolova , and A . D . Sarma .
Personalized social recommendations : accurate or private . VLDB , 2011 .
[ 15 ] C . Mallows . Non null ranking models . Biometrika ,
1957 .
[ 16 ] H . Mao , X . Shuai , and A . Kapadia . Loose tweets : an analysis of privacy leaks on twitter . In WPES , 2011 .
[ 17 ] N . Matthijs and F . Radlinski . Personalizing web search using long term browsing history . In WSDM , 2011 .
[ 18 ] A . K . McCallum . Mallet : A machine learning for language toolkit . http://malletcsumassedu , 2002 . [ 19 ] E . Pariser . The Filter Bubble : What the Internet Is
Hiding from You . Penguin Press , 2011 .
[ 20 ] T . Qin , X . Geng , and T . Y . Liu . A new probabilistic model for rank aggregation . In NIPS , 2010 .
[ 21 ] Sontag et . al . Probabilistic models for personalizing web search . In CIKM , 2012 .
[ 22 ] B . Tan , X . Shen , and C . X . Zhai . Mining long term search history to improve search accuracy . In KDD , 2006 .
[ 23 ] J . Teevan , S . T . Dumais , and E . Horvitz .
Personalizing search via automated analysis of interests and activities . In SIGIR , 2005 .
[ 24 ] J . Teevan , S . T . Dumais , and E . Horvitz . Potential for personalization . HCI , 2010 .
[ 25 ] J . Teevan and other . History repeats itself : repeat queries in yahoo ’s logs . In SIGIR , 2006 .
[ 26 ] Teevan et . al . To personalize or not to personalize : modeling queries with variation in user intent . In SIGIR , 2008 .
[ 27 ] Wetzker et . al . Analyzing social bookmarking systems : A delicious endeavour . International Journal of Data Warehousing and Mining , 6 , 2010 .
[ 28 ] Xu et . al . Privacy enhancing personalized web search .
In WWW , 2007 .
[ 29 ] Y . Zhu , L . Xiong , and C . Verdery . Anonymizing user profiles for personalized web search . In WWW , 2010 . [ 30 ] Zhu et . al . To divide and conquer search ranking by learning query difficulty . In CIKM , 2009 .
