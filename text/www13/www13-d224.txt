Inferring Dependency Constraints on Parameters for Web Services
Qian Wu1 , Ling Wu1 , Guangtai Liang1 , Qianxiang Wang1 , Tao Xie2 , Hong Mei1
1Institute of Software , School of Electronics Engineering and Computer Science
Key Laboratory of High Confidence Software Technologies ( Peking University ) , Ministry of Education
Peking University , Beijing , 100871 , China
{wuqian08 , wuling07 , lianggt08 , wqx , meih}@seipkueducn
2Department of Computer Science , North Carolina State University , Raleigh , NC 27695 , USA xie@cscncsuedu
ABSTRACT Recently many popular websites such as Twitter and Flickr expose their data through web service APIs , enabling third party organizations to develop client applications that provide functionalities beyond what the original websites offer . These client applications should follow certain constraints in order to correctly interact with the web services . One common type of such constraints is Dependency Constraints on Parameters . Given a web service operation O and its parameters Pi , Pj,… , these constraints describe the requirement on one parameter Pi that is dependent on the conditions of some other parameter(s ) Pj . For example , when requesting the Twitter operation “ GET statuses/user_timeline ” , a user_id parameter must be provided if a screen_name parameter is not provided . Violations of such constraints can cause fatal errors or incorrect results in the client applications . However , these constraints are often not formally specified and thus not available for automatic verification of client applications . To address this issue , we propose a novel approach , called INDICATOR , to automatically infer dependency constraints on parameters for web services , via a hybrid analysis of heterogeneous web service artifacts , including the service documentation , the service SDKs , and the web services themselves . To evaluate our approach , we applied INDICA‐ TOR to infer dependency constraints for four popular web services . The results showed that INDICATOR effectively infers constraints with an average precision of 94.4 % and recall of 955 % Categories and Subject Descriptors D24 [ Software Engineering ] : Software/Program Verification— Programming by contract ; D25 [ Software Engineering ] : Testing and Debugging ; H35 [ Information Storage and Retrieval ] : Online Information Services—Web based services Keywords Web service ; Constraints ; Parameters ; Testing ; Service SDK ; Documentation Analysis 1 . INTRODUCTION In recent years , many popular websites expose their data through web service APIs , enabling third party organizations to develop client applications that provide functionalities beyond what the original websites offer . For example , by making requests to Twitter web services , third party applications allow users to share movie tastes with their friends , check the comments on a particular restaurant and so on .
Copyright is held by the International World Wide Web Conference Committee ( IW3C2 ) . IW3C2 reserves the right to provide a hyperlink to the author's site if the Material is used in electronic media . WWW 2013 , May 13–17 , 2013 , Rio de Janiero , Brazil . ACM 978 1 4503 2035 1 /13/05 .
These third party client applications should follow certain constraints in order to correctly interact with the web services . For example , when requesting the Twitter operation 1 “ GET  status‐ es/user_timeline ” , client applications are required to specify either a user_id or a screen_name parameter . Violations of such constraints can cause fatal errors or incorrect results in the client applications . These constraints are mainly expressed in natural language in the service documentation . A widely adopted strategy [ 7 ] by developers to build correct client applications is to first read through the service documentation , trying to memorize the constraints , and then develop client applications accordingly . However , conformance to constraints cannot be assured . In fact , a recent study [ 14 ] showed that developers may still make mistakes even when they have been rather familiar with the documentation . Therefore , it is desirable that client applications are formally verified against these constraints , and violations of each constraint are detected as bugs . However , these verification techniques [ 8 , 9 , 14 ] require formally specified constraints , which are often not readily available , due to the large amount of effort needed to manually specify them . For example , it took one of the authors more than 10 hours to only browse the documentation of the Ebay2 web service operation “ AddFixedPriceItem ” , let alone the time needed to extract and formalize the constraints . To address the issue , in this paper , we propose a novel approach to automatically infer formal usage constraints for web services . In particular , we focus on one type of constraints that commonly exists in web services , and we call these constraints Dependency Constraints on Parameters . We refer to such constraints as dependency constraints in short in the rest of this paper . Given a web service operation O and its parameters Pi , Pj,… , these constraints describe the dependency relationships between parameters : the requirement on the occurrence or the valid value of one parameter Pi depends on the occurrence or the current value of some other parameter(s ) Pj . For example , the aforementioned constraint “ either a user_id or a screen_name must be specified ” can be interpreted as “ when user_id is not specified , screen_name must be specified , and vice versa ” . These constraints are beyond type definitions ( ie , requirements on the structure and format of the request message , which are specified in WSDL files for SOAPbased web services ) , and are currently expressed in only natural language in service documentation . We manually investigated the documentation of four popular web services ( Twitter , Flickr ,
1 Twitter Web Service : available at https://devtwittercom/docs/api/1 2 Ebay Services : available retailing service , at an online https://wwwxcom/ developers/ebay/products/trading api .
1421 Lastfm3 , and Amazon Product Advertising API ( APAA)4 ) and found that an average of 21.9 % of their service operations have dependency constraints on their parameters . Most existing approaches infer usage constraints for web services by testing these web services [ 2 , 5 ] . These approaches first use the type definitions of the service operations to generate test cases , then execute the test cases by submitting web service requests , and finally infer constraints by observing the responses . A constraint is produced if and only if its satisfying test cases pass and its violating test cases fail . However , two challenges remain unaddressed by these existing approaches . First , relying on information from only the type definitions would cause an explosion in the number of generated test cases , while very few of them would lead to discovery of real constraints . For example , to find the constraint “ either a user_id or a screen_name must be specified ” for the Twitter operation “ GET  status‐ es/user_timeline ” with totally ten parameters , all combinations of every two parameters must be tested , while 44/45 ( 97.8 % ) of the test cases contribute to no discovery of constraints . In addition , to save bandwidth and CPU time on the server side , service providers typically limit the rate of clients’ requests , making testing web services expensive in either monetary or time cost . For example , the Flickr5 method “ flickractivityuserComments ” can be invoked only once an hour by each authenticated user . Therefore , running a large number of generated test cases would not be feasible . Second , test results may be affected by multiple constraints for one operation , leading to false negatives and false positives . In particular , for a real constraint P , its satisfying test cases could fail due to violations of another constraint Q , which has not been inferred and is unknown to the approach , thus hindering P from being discovered . For example , a Flickr operation “ flickrplaces placesForContacts ” has a constraint that “ either woe_id or place_id must be provided ” , and the test cases would not pass unless they conform to not only this constraint but also all the other constraints , such as “ either place_type or place_type_id must be provided ” . Failing to fulfill these latter constraints would prevent the discovery of the former one . If we modify the criterion of producing a constraint to consider only whether its violating test cases fail , a false positive problem occurs . For a false constraint P , its violating test cases may fail , but due to only violating constraint Q , thus making P a false positive . Taking the same Flickr operation as an example : many false constraints concerning the other optional parameters would be produced , such as “ either threshold or contact must be specified ” , because all the satisfying and violating test cases fail due to violating the aforementioned two constraints . To address the preceding two challenges , our approach , called INDICATOR ( INference of Dependency ConstrAinTs On parameteRs ) , infers dependency constraints using a hybrid analysis of heterogeneous web service artifacts , including the service documentation , the service SDKs , and the web services themselves . INDICATOR consists of two stages : constraint candidate generation and constraint candidate validation . In the candidate generation stage , INDICATOR analyzes the service documentation and service SDKs to generate constraint candidates . In the candidatevalidation stage , INDICATOR validates the candidates through
3 Lastfm : available at http://wwwlastfm/api 4 Amazon Product Advertising API : available at https://affiliate program . amazoncom/gp/advertising/api/detail/mainhtml API Version : 201108 01 .
5 Flickr Service : available at http://wwwflickrcom/services/api/ testing : INDICATOR invokes the web services with requests satisfying/violating a constraint candidate , and observes the results to determine whether the candidate is a real constraint . Thanks to the hybrid analysis of heterogeneous artifacts , INDICA‐ TOR offers two main advantages . First , the candidate generation stage benefits the candidate validation stage by not only narrowing down the search space for real constraints , but also reducing the false positives and false negatives of candidate validation . The candidate generation stage produces multiple constraint candidates for each operation . With these constraint candidates , the candidate validation stage generates test cases each of which is guaranteed to violate no more than one constraint candidate . Such guided test case generation enables our approach to validate a constraint candidate without being influenced by any other constraint candidate for the same operation . Second , the candidatevalidation stage benefits the candidate generation stage by refining the generated constraint candidates to reduce false positives . To evaluate our approach , we applied INDICATOR to infer dependency constraints for four popular web services ( Twitter , Flickr , Lastfm , and APAA ) . The results show that INDICATOR infers constraints with an average precision of 94.4 % and recall of 955 % Compared with existing approaches based on only web services themselves , INDICATOR improves the precision by 39.4 % and recall by 10.3 % , while saving 84.7 % of the efforts . In summary , this paper makes the following main contributions : An empirical investigation of web service documentation to show the non trivial presence of dependency constraints on parameters in web services , along with their further classification . We also discuss possible reasons leading to the nontrivial presence of such constraints in web services .
An effective and efficient approach to inferring dependency constraints of using web services via a hybrid analysis of heterogeneous artifacts , including the service documentation , the service SDKs , and the web services themselves .
The rest of this paper is organized as follows . Section 2 presents the results of the empirical investigation of the distribution and classification of dependency constraints in popular web services . Section 3 gives an overview of our approach . Section 4 describes our approach in detail . Section 5 presents the evaluation results . Section 6 discusses the related work and Section 7 concludes . 2 . DEPENDENCY CONSTRAINTS IN WEB SERVICES In this section , we first show the results of our empirical investigation of dependency constraints in popular web services , and next discuss reasons leading to the non trivial presence of such constraints in web services . 2.1 Empirical Investigation We manually investigated the service documentation of four popular web services , and the results are listed in Table 1 ( more details of the investigation can be found at http://saseforgeorg/ indicator/ ) . In particular , column “ Mashups ” lists the number of applications known to be built on each web service ( according to the statistics from http://wwwprogrammablewebcom ) ; column “ OP ” lists the number of operations provided by each web service ; column “ DC ” lists the percentage of operations with dependency constraints . From the statistics , we could observe that a non trivial percentage ( an average of 21.9 % ) of web service operations have dependency constraints . In addition , hundreds of client applications would benefit from the constraints inferred for a web service .
1422 W We further divid in n Figure 1 . We te ers in one oper v values . The first F For example , the o of A and B ) shou e egory . The seco s should ( not ) be in c categories restric g gory requires tha th he same request o only the most b c constraints could c connections . ded these constra use “ A ” and “ B ” ation , and use “ t three categorie e first category re uld be specified ” ond category re ncluded in the sa ct parameter valu at “ when B is spe t ” . Note that the basic constraint d be composed b aints into six cat ” to denote two “ p ” and “ q ” to es restrict param equires “ either A ” , which is the m equires “ when ame request ” . Th ues . For exampl ecified , A must ( categories show categories , from y using conjunc tegories , as show wn medifferent param denote paramet ter es . meter occurrence A or B ( at least on ne atmost prevalent ca B A is included , he fourth and fif fth tele , the fourth cat ( not ) be set to p in are wn in Figure 1 a ex m which comple tive or disjunctiv ve ults of empirica Table 1 . Resu co onstraints in pop D Description cro blogging mi ph hoto sharing o online radio on line retailing
Subject Twitter Flickr Lastfm APAA Total al investigation of dependency pular web servi ices . #O #Mashups OP %DC 38.1 1 105 10.2 1 186 23.1 130 1 9 55.6 21.9 4 430
748 615 225 416
( 5)A=p p )→q),  ( or A≠p)B=q ( or B≠q 8.3 %   ( 4)B→A=p ( or A≠p),  14.8 % ( 3)A A=p ≠p)8.3 % ( or A →B, 8
( 6)7 )Others,  7.5 %
( 1)A38.3 A(cid:1513)B,  3 %
( 2)A→B ( or  A→¬B),  22.8 % ch We
Figure 1 . Cat tegories and dis s straints in popu 2.2 Reasons s of Depend 2 rically that the d W We found empir n web services , m more common in n next discuss two main reasons fo values in web se F First , parameter v ormally , in loca f flexible way . No rameters for a m o order of the par services , param c contrast , in web th heir parameter n names , allowing d in any order . te ers being passed would look like s service request w json?screen_nam e es/user_timeline.j f flexible way of p passing paramet t whether a para s straints to restrict eb services supp S Second , most we oriented local AP ty ypes . In object o s based on their s sulated in objects e er design could h have enforced th order to maintain In n contrast , in o es ( especially R m most web servic o only of primitive e types ; in other pendency constribution of dep es . ular web service traints dency Const nstraints are muc dependency con API libraries . W , than in local A or such phenome enon . ed in a much mo ervices are passe , the number an al API libraries , ctly stipulated . method are stric e indexed by on meter values are values of any n number of param a typical RESTf For example , a “ https://api.twit tter.com/1/statu ount=2 ” . Such me=twitterapi&co ters thus demand ds additional co e present . ameter should be only of primitiv ve port parameters apameters are enca PI libraries , para opnships , and a pro internal relation tween parameter he constraints bet rs . tly erability , current n good interope ers upport paramete RESTful ones ) su r words , all the p parameters hidde en ore nd In nly meful us‐ a on in obje for web mation operatio while , objects ple par tion “ P and lon shows GeoLoc that thi applica calling 1.publi 2.   p 3.   p 4.   pu 5 .      6 .      7.   } 8.   pu 9.   pu 10.}    Figure the p ects for API libr b services . As a as their counte ons require a m the internal rel s for API librarie rameters for web POST statuses/up ngitude should a likely design cation object in J is constraint no ations are alway the constructor ic class GeoLo rivate double  rivate double  ublic GeoLocat  this.latitud  this.longitu raries are flatten a result , to carry erparts in local much larger num lationships betw es have turned in b services . For e pdate ” requires be paired . The of encapsulating Java local API l longer exists in ys enforced to p in Line 4 to crea ocation{  longitude;  latitude;  ion(double lat e = latitude;  de = longitude e;  ned to primitive y the same amou API libraries , mber of parame ween parameter nto constraints a example , the Tw that the parame e code snippet g these two para ibraries . It can this snippet , be pass both param ate a GeoLocatio e parameters unt of inforweb service eters . Means hidden in across multiwitter operaters latitude in Figure 2 ameters in a be observed ecause client meters when n object . titude,double l longitude){ ublic double g ublic double g getLatitude(){r getLongitude()  return latitude {return longit e;}  tude;}  e 2 . Example co parameters latit ode snippet sho tude and longitu wing the encap ude in OO API sulation of libraries . web services specific to n hiding via e , additional e the correct rvices . ral approach nteroperability , w eristics that are h as information uages . Therefore n order to ensure and the web ser EW ew of our gener approach infers brid analysis of ation , the servic TOR starts with a formation from stages , eg , type f parameters . Th In the candidate e documentation didates . In the tes the candida nt candidates tha dependency three inforce SDK , and preparatory the service e definitions he main ape generation n and service candidateates through at have been achieve good in w away characte languages , such ct oriented langu s are entailed in ent applications H OVERVIE ovide an overvie mples . ur INDICATOR a rvices via a hyb rvice documenta selves . INDICAT cts necessary inf bsequent main s descriptions of in two stages . I lyzes the service constraint cand ICATOR validat nly the constrain act constraint ca ATOR includes , to find a poten umentation and arameter A or B d constraint cand s . While leading to the word ch ds , false negativ alent constraint tes . To alleviate trategy , which g r of distinct para ches with the n andidates from two strategies : ntial matching b predefined con B is required ” . didates describe to relatively pre hoices of a con es would be pro is described in e this limitation , generates a const ameters appearin number of para the service the “ rigid ” between denstraint temThe “ rigid ” d by similar ecise results , straint ’s deoduced when n words not INDICATOR traint canding in its deameters in a
In gene are des certain encapsu depend interact 3 . AP In this through As Fig constra mation the web analysi docume of serv proach stage , I SDKs validati testing , validate In part docume and “ lo scriptio plates , strategy words a this str scriptio a sema covered include date wh scribing eral , in order to signed to throw programming l ulation in objec dency constraints tion between cli PPROACH section , we pro h a series of exam gure 3 shows , ou aints for web ser sources : the ser b services thems is , which collec entation for sub vice operations , then proceeds INDICATOR anal to generate c ion stage , INDI , and outputs on ed . ticular , to extra entation , INDICA oose ” strategies , ons in the docu eg , “ either pa y intends to find as the templates rategy is fragile on . In other word antically equiva d by the templat es the “ loose ” st hen the number g sentence mat
1423 Service
Documentation
Preparatory
Analysis
Type
Definitions , etc .
Candidate Extraction from Doc
Service SDK
Candidate Inference from SDK
Stage 1
Figure 3 . Architecture of INDICATOR .
Formal
Constraints
Constraint Candidates
Candidate Validation via Testing Stage 2
Web Service template . Our insight here is that rather than trying very hard to extract the semantics out of sentences , we use only simple and reliable information from the sentences and then rely on testing ( to be conducted in the subsequent stage ) to figure out the constraint , reflecting a benefit from integrating heterogeneous information sources . In addition to service documentation , INDICATOR also exploits information from service SDKs . For most popular web services , there are SDKs available in various programming languages . Such SDKs wrap interactions with web services , allowing clientapplication developers to invoke web services as if invoking methods from local API libraries . INDICATOR includes a novel technique to infer constraint candidates from service SDKs , based on our insight that SDKs demonstrate legal ways of invoking web services . For example , the code snippet in Figure 4 shows the API method in the Java SDK of the Lastfm web service . Such API method wraps the invocation of the “ artist.getInfo ” operation . The statements in Lines 2 9 collect the parameters , and the statement in Lines 11 12 makes the remote call . As shown in the code snippet , it is ensured that “ either an mbid or an artist is specified ” , which is actually the constraint for this operation . Our main idea is to identify all possible combinations of parameters included by the SDK to invoke each service operation , and then apply a simple statistical analysis to learn constraints . For the method in Figure 4 , there are four distinct paths , making four possible combinations of parameters : {mbid , lang} , {artist , lang} , {mbid} , and {artist} . Then by analyzing the combinations , we notice that either mbid or artist is present in the set of included parameters , thus leading us to infer the constraint . Finally , INDICATOR validates each constraint candidate through testing . For each candidate , INDICATOR generates both satisfying and violating test cases , all of which are ensured to conform to all the other constraint candidates in the same operation . INDICATOR considers a constraint candidate to be real , if ( 1 ) all its violating test cases fail and at least one of its satisfying test cases passes , or ( 2 ) all test cases fail , and the error messages of the violating test cases look alike to the candidate ’s description . Using these criteria , INDICATOR intends to find constraints that are required by correct interactions with the web services . INDICATOR reduces false negatives by accommodating occasions when test cases conforming to the constraint candidate fail due to violating other constraints . Meanwhile , INDICATOR reduces false positives by exploiting information from the error messages of violating test cases , so that INDICATOR confirms that the failure was indeed caused by violating the constraint candidate under validation , but not some other constraints .
  params.put("mbid", artistOrMbid);}  
  params.put("artist", artistOrMbid);} 
1. public Artist getInfo(String artistOrMbid, Locale loc) { 2.    Map<String, String> params = new HashMap<String,  3. String>();  4    if(StringUtilitiesisMbid(artistOrMbid)){  5.  6.    else{  7.  8.    if(loc!=null)  9         paramsput("lang",locgetLanguage());  10     11   Result result = CallergetInstance()call(  12 "artistgetInfo", params);  13   return ResponseBuilderbuildItem(result,   14 Artistclass);  15.}  Figure 4 . Code snippet of the API method deumasslastfm Art ist.getInfo from the Java SDK of Lastfm web service .
4 . APPROACH DETAIL We next describe the details of our INDICATOR approach . In particular , we present the preparatory analysis in Section 41 We next present the candidate generation from documentation and SDK in Section 4.2 and Section 4.3 , respectively . We finally present the candidate validation through testing in Section 44 4.1 Preparatory Analysis In preparatory analysis , INDICATOR collects various types of information needed by subsequent steps , including ( 1 ) the service type definitions , eg , the available operations names , the list of mandatory and optional parameter names for each operation , and available values for parameters of enumeration types , ( 2 ) descriptions for operations and parameters , which might contain usage constraints , and ( 3 ) other technical details required to enable automatic invocation of the service operations , eg , the URL address of the service , the HTTP method to make the request . Unlike SOAP based web services ( most of whose preceding information is available in formal WSDL files ) , for most currently popular RESTful web services , all the preceding information could be extracted from only the service documents in the form of HTML files . Fortunately , all the information is easy to locate , given user defined XPath6 expressions to specify the paths of their corresponding nodes in the HTML files . Only very little manual effort is needed to write the XPath expressions , since documents of different operations in a web service share the same style and structure .
6 XPath : http://wwww3org/TR/xpath20/
1424 4.2 Candidate Extraction from Service Documentation The basic idea to extract constraint candidates from service documentation is to find potential matchings between the descriptions in documentation and the predefined constraint templates . Table 2 shows some typical template sentences , in which “ A ” and “ B ” denote two different parameter names , and “ p ” and “ q ” denote parameter values . In particular , the first column presents their correspondence with the constraint categories shown in Figure 1 . According to our manual investigation , parameter values incorporated in such constraints always belong to enumeration types , and note that all parameter names and available values are already extracted in advance by the preparatory analysis . INDICATOR includes two strategies to locate the candidatedescribing sentences in documentation . The “ loose ” strategy marks a sentence as describing a constraint candidate when the number of distinct parameters and values appearing in the sentence matches with that in a template . This strategy could identify candidate describing sentences , even those using completely different words from the templates . However , the number of generated sentences might grow quickly with only a small portion describing real constraints , causing a potential waste of effort in subsequent steps .
Table 2 . Example templates of dependency constraints .
Category ID
Template Sentences
( 1 ) ( 1 ) ( 2 ) ( 3 ) ( 5) 
Either A or B must be specified .
One of A or B is required .
If providing A , B is also required . A is required , when B is set to p .
When A is set to p , B cannot be set to q .
The “ rigid ” strategy marks a sentence as describing a constraint candidate when the sentence uses similar words as the template sentences . This strategy computes a Jaccard Similarity7 between the bag of words used in the candidate describing sentence and a template sentence . Some standard Natural Language Processing ( NLP ) procedures should be conducted in advance , such as stopword removing , word stemming , and synonym replacement . In addition , parameter names are also excluded in the similarity computation . Compared with the “ loose ” strategy , this strategy produces more precise results , but it is sensitive to the word choices of a candidate ’s description : false negatives would be produced when a real constraint is described in words not covered by the templates . For example , using the first sentence in Table 2 without using the second one would cause the approach to miss constraints described in the second sentence in Table 2 , which use different words but share the same semantics . Our approach provides a way to flexibly choose between the two strategies . It first adopts the “ loose ” strategy to generate candidate describing sentences . If the number of the generated sentences is overwhelmingly too many ( eg , beyond a user defined threshold ) , it then adopts the “ rigid ” strategy to further refine the sentences . In this way , INDICATOR could be adaptive to any incoming web services , while maintaining a balance between recall and efficiency .
|(cid:3002)(cid:1515)(cid:3003)| , where in our approach , A and B repre
7 ( cid:1836)(cid:1853)(cid:1855)(cid:1855)(cid:1853)(cid:1856) Similarity(cid:4666)A,B(cid:4667)|(cid:3002)(cid:1514)(cid:3003)| sent two sets of words .
Finally , for each candidate describing sentence , INDICATOR generates the constraint candidate by filling the matched template with the relevant parameter names and values . For templates in which the order of the parameters matters ( eg , the last three templates in Table 2 ) , if the templates contain parameter values , IN‐ DICATOR then determines the order of parameters by first identifying the parameter to which the value belongs ; otherwise , it simply fills the templates with all possible parameter sequences . Thanks to integrating information from heterogeneous artifacts , our approach could use only simple information from the documentation , rather than adopting sophisticated NLP techniques : all the constraint candidates are to be ( in)validated by means of testing web services in the subsequent stage . 4.3 Candidate Inference from SDK INDICATOR infers constraint candidates from SDKs based on the observation that the code of the SDK demonstrates legal ways of invoking web services . The main idea of this technique is to identify all combinations of parameters and their values ( only values of enumeration types are considered ) included by the SDK to invoke each service operation , and then apply a simple statistical analysis to learn the constraints . The technique takes as input the code of the service SDK and the service type definitions ( including the operation names and the list of parameter names and values for each operation , which are extracted in advance by the preparatory analysis presented in Section 4.1 ) , and produces as output the inferred constraint candidates . This technique proceeds in four steps . ( 1 ) As a preparatory step , INDICATOR applies a constant propagation [ 1 ] to the code of the SDK to replace each String variable with the actual String literal , so that INDICATOR could identify parameter names and values without tracking into the content of their holding variables . In addition , INDICATOR performs an inter procedural analysis to build the call graph for the whole program . ( 2 ) For each operation , INDICATOR searches in the SDK for all the public methods that wrap its invocation , by searching for methods that directly or indirectly access its operation name . For example , for the Lastfm operation “ artist.getInfo ” , INDICATOR first locates the method “ getInfo(String , Locale ) ” , which accesses the operation name in Line 12 as shown in Figure 4 , and then searches for all callers of this method based on the call graph . ( 3 ) For each SDK method found by Step 2 , INDICATOR applies a forward Data Flow Analysis Algorithm [ 1 ] to compute all the combinations of parameter names and values included in the method . The formal algorithm of this step is shown in Figure 5 , which is explained later . ( 4 ) Finally , INDICATOR gathers all the available sets of parameter names and values for each operation , and applies a statistical analysis to learn the constraints . We next use the SDK method “ artist.getInfo ” as an example to illustrate the algorithm shown in Figure 5 . For simplicity , the shown algorithm deals with only parameter names , but the same algorithm could be easily adapted to compute all the encountered parameter value combinations in each method . Intuitively , this algorithm computes the sets of already encountered parameters by propagating such information from the method entry point along each path of the Control Flow Graph ( CFG ) to the method exit point . There are three key variables in the propagation process . For each node n ( a block of consecutive statements ) of the CFG , IN[n ] and OUT[n ] denote the set of data at the program point before and after the execution of statements in this node , respectively . In particular , each element of IN[n ] and OUT[n ] is a set of parameter names already encountered along some path leading from the entry point to their corresponding program point . The execu
1425 10 . 11 . IN[n ] =Ø ; 12 . foreach predecessor node p of n do
14 .
16 . OUT[n ] = Ø ; 17 . foreach element eleIn in IN[n ] do path} Begin 1 . Build a control flow graph CFG for m ; 2 . foreach node n in CFG do tion of the statements in the node changes IN[n ] to OUT[n ] by including parameter names , which are all recorded in GEN[n ] . Figure 6 depicts the CFG for the method “ artist.getInfo ” , tagged with the computed information at each program point . Algorithm computeParamCombinations Input m the public SDK method wrapping the invocation of OP ; NOP the list of parameter names for OP ;
4 . OUT[Entry ] = IN[Entry ] = Ø ; 5 . Changed = {All nodes in CFG} – Entry ; 6 . 7 . while Changed ≠ Ø do 8 . choose a node n from Changed ;
Output NS {N | N is a set of parameter names included on each 3 . OUT[n ] = Ø ; 9 . Changed = Changed \ {n} ; 13 . IN[n ] = IN[n ] , OUT[p ] ; 15 . GEN[n ] = {param | param , NOP , param visited in n} 18 . OUT[n ] = OUT[n ] , {GEN[n ] , eleIn} ; Changed = Changed , {s} ;
19 . 20 . if ( OUT[n ] changed ) then 21 . foreach successor node s of n do 22 . 23 . 24 . return OUT[Exit ] ; End Figure 5 . The algorithm in Step 3 for extracting all the combi nations of parameter names for an SDK method .
As Figure 5 shows , INDICATOR starts the algorithm by building a CFG for the given method ( Line 1 in Figure 5 ) . In the process , all paths leading to exception throwing statements are pruned ; these paths demonstrate illegal ways of invoking the method or the wrapped operation . After properly initializing the variables ( Lines 2 5 ) , INDICATOR iteratively computes the concerned data for each program point until all data converge ( Lines 7 22 ) . Once the data OUT for a node n has been updated , all the IN ( and hence OUT ) data for all n ’s successor nodes must also be recomputed ( Lines 20 22 ) . INDICATOR records all the nodes requiring re computation in the variable Changed . In each iteration , INDICATOR randomly selects a node that requires re computation and computes its IN and OUT data ( Lines 11 22 ) . INDICATOR computes the IN data of each node by doing a union of the OUT data of all its predecessors ( Lines 11 13 ) , so as to gather all possible combinations of parameters encountered so far along each path . For example , for Node 6 in Figure 6 , by doing a union of OUT[4 ] and OUT[5 ] , the IN[6 ] is computed as {{mbid} , {artist} , {mbid , lang} , {artist , lang}} , while each element corresponds to the paths ( 1 2 4 6 ) , ( 1 3 4 6 ) , ( 1 2 4 5 6 ) , and ( 1 3 4 5 6 ) , respectively . INDICATOR next computes the OUT data of each node n by adding the set of parameter names visited by statements in n ( Line 15 ) to each element of the IN data ( Lines 16 18 ) . Note that the size of OUT[n ] is equal to that of IN[n ] , which represents the number of feasible paths leading from the entry to the corresponding point . For example , for Node 5 , by adding the parameter name “ lang ” to each element of IN[5 ] , we arrive at OUT[5 ] , which is {{mbid , lang} , {artist , lang}} , each element corresponding to the paths ( 1 2 4 5 ) and ( 1 3 4 5 ) , respectively . Finally , INDICATOR returns the OUT data of the exit point as the final sets of all combinations of parameter names included by the method ( Line 24 ) . Due to space limit , we omit from Figure 5 the details of dealing with method invocation statements . In fact , for each such statement , the technique would apply an inter procedural analysis to track the sets of parameters encountered by statements inside the invoked method , and then add all the tracked data into that of the caller method .
Figure 6 . CFG for method “ artist.getInfo ” in Figure 4 along with the OUT[n ] and IN[n ] data for each node n .
Finally , for each operation OP , the technique gathers the computed sets of parameter names and values from all its public wrapping methods in the SDK , and applies a simple statistical analysis to derive the constraints . For example , for the operation “ art‐ ist.getInfo ” , we found two other public wrapping methods in the SDK , contributing the additional sets of encountered parameter names as follows : {{mbid , username} , {artist , username} , {mbid , lang , username} , {artist , lang , username}} . From the total of the eight sets of parameter names , we could learn that each set includes either “ mbid ” or “ artist ” , thus leading us to infer the constraint : “ either mbid or artist is required ” . 4.4 Constraint Candidate Validation through Testing The candidate validation stage determines whether a constraint candidate is real or not via testing . We consider a constraint as real , if it is required to ensure correct interactions with web services . While we cannot prove a constraint to be real , we could observe whether a constraint candidate causes the same consequences on the execution of test cases as a real constraint does . A real constraint typically demonstrates certain observable characteristics : ( a ) all test cases that violate the constraint would fail ( determining test case failing or passing is
1426 elaborated in the end of this section ) ; meanwhile , ( b ) all test cases that conform to the constraint as well as all the other constraints would pass . Thus the task of candidate validation becomes to first generate test cases according to the requirements in ( a ) and ( b ) , and to then determine the candidate ’s likelihood of being real based on the execution outcomes of the test cases . Unfortunately , requirements in ( b ) may not be fulfilled due to the lack of complete constraints ( their presence would obviate the need to infer them in the first place ) . As a result , false positives/negatives would be produced when test cases violate constraints other than the one under validation , as discussed earlier in Section 1 . Our approach addresses the preceding issue in two ways , attempting to validate a constraint candidate appropriately without the knowledge of complete constraints . First , INDICATOR improves the quality of generated test cases to reduce the possibility of violating other constraints . In the context of web services , the quality of test cases depends on mostly the quality of parameter values . INDICATOR collects parameter values from four sources . ( 1 ) INDICATOR extracts parameter values of enumeration types from the service documentation , as was done by the preparatory analysis . ( 2 ) INDICATOR caches the responses of executed test cases , in order to provide values for parameters whose values are returned by other operations . For example , in APAA , the only valid value of parameter CartID in operation Car‐ tAdd is returned by operation CartCreate . INDICATOR identifies such producer consumer relationships through name and type matching between input and output parameters of different operations . Then INDICATOR prioritizes the test cases of operations , so that a parameter is always produced before it is consumed . In this way , errors such as “ the cart specified does not belong to you ” or “ no data found ” could be avoided . ( 3 ) INDICATOR extracts the available example values from service documentation , and these values typically adhere to the syntax requirements and are within the valid range . ( 4 ) INDICATOR solicits parameter values from users for the remaining parameters if it is feasible to do so .
Table 3 . Sample test cases that conform to or violate a given constraint .
Constraint : Either A or B must be specified .
Conformance
Given A , no B Given B , no A Given A and B
Violation no A no B
In addition , with the knowledge of constraint candidates provided by previous steps , our INDICATOR approach ensures that each generated test case for one candidate must also conform to all the other constraint candidates of the same operation in a nonconflicting way . The conformance of constraint A conflicts with constraint B , when the conformance of A influences the conformance or violation of B . Table 3 shows sample test cases satisfying or violating the constraint “ either A or B must be specified ” . Note that there is only one way ( ie , including parameter A and not B ) to conform to this constraint without conflicting with another constraint “ either B or C must be specified ” , because including parameter B would coincidentally cause a conformance with the latter constraint . Second , we adjust the criteria of determining real constraints . INDICATOR considers a constraint candidate as real , if a ) all the violating test cases fail and at least one of the satisfying test cases passes ; or b ) all the test cases fail , and the error messages of the violating test cases look alike to the description of the constraint candidate .
Therefore , INDICATOR first invalidates a candidate , if some of its violating test cases pass . INDICATOR next attempts to validate a candidate by checking whether the execution outcomes of the test cases conform to either of the preceding criteria . Based on the criteria , on one hand , INDICATOR reduces false negatives by accommodating occasions when a satisfying test case fails due to violating other constraints . On the other hand , INDICATOR reduces false positives by comparing the error messages of the violating test cases with the candidate ’s description , ensuring to some extent that the failure was indeed caused by violating the candidate under validation , rather than some other constraints . In our evaluation , of the final constraints produced by INDICATOR , 86.5 % were validated using criterion a , while 13.5 % were validated using criterion b . In particular , INDICATOR determines whether the error messages and the candidate ’s description are alike , by computing the highest similarity between each pair of the error message and the candidate ’s description . The same similarity computation technique presented in Section 4.2 is applicable here . Specifically , for constraint candidates that do not have descriptions ( ie , those generated from SDKs or by the loose strategy from documents ) , INDI‐ CATOR uses the template sentences of the relevant constraint category as the candidates’ possible descriptions . In our implementation , we consider a test case to pass or fail based on whether the response is legal or illegal . Such classification is straightforward , since an illegal response would normally contain indications such as an error code or an error message . We did not further examine whether the data contained in a legal response matches with a predefined golden oracle , which would be difficult to specify in the context of web services [ 10 ] . The response data for an operation might be changing from time to time . For example , the Twitter operation “ GET  statuses/user_timeline ” queries for the recent statuses of a user , for which the response data would change once the user posts new statuses . 4.5 Limitations We next discuss limitations of our approach in terms of potential false negatives and false positives . False negatives are produced when a real constraint cannot be generated from either the documentation or the SDK , or its generated candidate cannot be validated by testing . In the step of candidate extraction from documentation , INDICATOR would miss real constraints described in words not covered by the given templates , when the rigid strategy is adopted . We have sought to alleviate this issue by introducing the loose strategy . However , the approach is still subject to the quality of the documentation : constraints might be actually missing in the documentation . INDICA‐ TOR mitigates these issues by including the service SDK as a complement . In the step of candidate inference from SDK , real candidates might not be inferred due to noises in un pruned infeasible paths . Finally , in the step of candidate validation , INDICA‐ TOR might not be able to validate a real candidate , when all the test cases fail due to violating some other constraints , and the error messages do not convey similar information as the candidate ’s description . INDICATOR alleviates this issue by ensuring that test cases of a candidate conform to all the other candidates in the same operation .
1427 Subject
#Real
Final Output
Table 4 . Evaluation results of INDICATOR . Constraint Candidates
Twitter Flickr Lastfm APAA Ttl/Avg
40 12 34 2 88
#Ttl %Pre %Rec 92.5 91.7 100.0 100.0 95.5
97.4 100.0 91.9 66.7 94.4
38 11 37 3 89
#Doc %DP %DR 77.5 83.3 88.2 100.0 82.9
27.4 9.9 23.4 22.2 20.8
113 101 128 9 351
#SDK %SP %SR %Flt 68.1 89.2 71.1 66.7 75.1
100.0 100.0 100.0 100.0 100.0
30.0 8.33 67.7 0.0 40.9
12 1 23 0 36
#Exced
TC
% Svd
TC
221 174 147 12 554
79.2 87.3 56.0 98.2 84.7
False positives are produced when a false candidate is mistakenly validated through testing . Such case happens when the conformance or violation of a candidate causes unintended side effect , which coincidentally leads to the conformance or violation of some other constraints . INDICATOR mitigates this issue by making sure that test cases for one candidate are generated without conflicting with the other candidates in the same operation . Concrete examples of false negatives and false positives observed from our evaluations are further discussed in Section 5 . 5 . EVALUATION To evaluate the effectiveness of our INDICATOR approach , we applied INDICATOR to infer dependency constraints for four web services , ie , Twitter , Flickr , Lastfm , and APAA . Specifically , this section shows the evaluation results of inferring the most prevalent category of dependency constraints among various categories , ie , “ either parameter A or parameter B must be specified ” ( we refer to constraints of this category as ( A , B)either‐or in this section ) . We applied INDICATOR to infer constraint candidates from Java SDKs , namely twitter4j8 for Twitter , flickrj9 for Flickr , and lastfm API10 for Lastfm ; there is no Java SDK available for the APAA web service . We solicited parameter values from one researcher in the Institute of Software at Peking University , for only 28 ( 1.7 % ) of the involved parameters , while values for all the remaining parameters were collected automatically by INDICATOR . We spent two weeks to prepare a golden standard for the web services . We first ran test cases concerning all combinations involving every two parameters for each operation , and then invited two researchers from the institute to manually inspect the results . The golden standard and details of our evaluation results are available at http://saseforgeorg /indicator/ . Our evaluation addresses the following research questions : • RQ1 : How effectively and efficiently can INDICATOR infer constraints ?
• RQ2 : How well can information in documentation and SDKs complement each other ?
• RQ3 : How much can the candidate validation stage benefit the candidate generation stage ?
• RQ4 : How much can the candidate generation stage benefit the candidate validation stage ?
The first research question concerns the overall effectiveness and performance of our approach , while the next three ones concern the benefits of integrating information from heterogeneous artifacts . We answer the first three questions in Section 51 To an
8 twitter4j 225 : http://twitter4jorg/ 9 flickrj 1.2 : http://flickrjsourceforgenet/ 10 lastfm API : http://wwwu massde/lastfm swer the last question , we conducted an additional evaluation and present the results in Section 52 5.1 Effectiveness and Efficiency of Constraint Inference We measure the effectiveness of INDICATOR using both precision and recall metrics . We measure the efficiency of INDICATOR using the number of executed test cases as the metric , rather than the exact time that INDICATOR spent on the web services . The reason is that most ( over 95 % ) of the time was spent on running test cases , and we must control the rate of making requests to avoid exceeding the rate limits imposed by the service providers , eg , INDICATOR slept for 10 seconds after making each authenticated request to Twitter . The evaluation results are listed in Table 4 . Column “ Real ” lists the number of real constraints used as the golden standard for each web service . For statistics concerning the final output of INDICATOR , column “ Ttl ” lists the total number of constraints produced by INDICATOR ; and columns “ Pre ” and “ Rec ” list the precision and recall , respectively . For statistics concerning the constraint candidates , column “ Doc ” lists the number of candidates generated from documentation using the loose strategy ( we also applied the rigid strategy of extracting candidates from documentation , but the detailed results are omitted due to space limit , instead we will compare their results briefly in the end of this section ) . Columns “ DP ” and “ DR ” list the precision and recall of the candidates generated from documentation , respectively . Column “ SDK ” lists the number of candidates generated from SDK . Columns “ SP ” and “ SR ” list the precision and recall of the candidates generated from SDK , respectively . Column “ Flt ” lists the percentage of candidates that are filtered ( ie , have not been validated ) by testing , and are considered as false candidates by INDI‐ CATOR . Column “ Exced TC ” lists the number of distinct test cases executed by INDICATOR . Column “ Svd TC ” lists the percentage of test cases that are exempted from being executed by INDICATOR , compared with a brute force approach of finding the concerned constraints ; such brute force approach tests all combinations involving every two parameters in each operation . From the results in Table 4 , we have the following observations . First , INDICATOR achieved high precisions and recalls on these web services , with an average precision of 94.4 % and recall of 955 % We will later show examples of the false positives and negatives , and analyze the reasons for producing them . Second , compared with documentation , INDICATOR inferred much fewer constraint candidates from SDKs , but with much higher precisions . We further depict the percentages of the real constraints covered by each source in Figure 7 . In general , 28.4 % of the constraints are covered by both sources , while 54.5 % come from only documentation , and 12.5 % come from only SDKs , indicating that documentation and SDKs complement each other . Third , most ( 75.1 % ) of the constraint candidates are filtered in the candidatevalidation stage through testing , indicating that testing plays an
1428 im mportant role i s straints . Finally , a and SDKs , INDI n needed to be exe p proach in a bru m manner by exist o only type definiti in improving th by integrating ICATOR greatly ecuted , compare ute force manner ting approaches ions . he quality of th information fro ( 84.7 % ) reduc ed with a constr r , which is actu to generate tes onhe produced co on om documentatio ced the test cas ses apraint inference a ed ually the adopte st cases based o on
Doc‐Only BothSDK‐Only Neither
Twitter LastfmFlickr APAA Average
0 %
20 % 40 % 60 %
% 80 % 100 %
Percentag ge
Figure 7 . Perce entages of the e different infor e reasons for pro ve false positives violations and sa other than the “ for Twitter oper which was cause and long must b documentation ) . dates for this op . To test the form ct of the latter o INDICATOR final ccording to crite failed due to pro test cases pass
W We next analyze ti ives . All the fiv b by coincidental v in ng to categories a a false positive f racy , lat)either‐or , w r s straint that “ lat a p plicitly in the d c constraint candid ( lat , long)either‐or . ( la ation and confli in n all test cases . I r eal constraint ac la ating test case f o of the satisfying ong . lo A Another false po z zon operation “ c constraint “ at lea p provided ” . INDIC th he only candidat a any two of the th he candidate as to o providing non te est cases passed title . t tives could be re T These false posit s . In theory , they s straint categories ge of what cons la ack of knowledg way to alleviate v vice . Another w riteria of valida a apply stricter cr or messages of t r equiring the erro candidates’ de s sistent with the sult in a high pre c criteria would res might use diffe e error messages m scribe the constr d description to de error message for r violating “ lat a e . li id Coordinates ” . e negatives of IN A All the four false u umentation does not even mentio inconsistencies te ence , indicating v vice implementa ation : either th sitive example i ItemSearch ” , ca ast one of its tw CATOR first gen te for the operat other parameter real , because all ne of the search d due to providi either or constra aints covered by y . mation sources gaoducing false po ositives and neg ervices are cause s for the four se ed ngonstraints belon atisfactions of c le , gory . For exampl Either‐Or ” categ /search ” is ( acc ration “ GET geo/ cu‐ satisfying the co ed by violating/s one paired ” ( whic h is described e exwo rst generated tw INDICATOR fir acy , lat)either‐or an peration , ( accura nd ioATOR avoided vi mer one , INDICA g a long paramet one by including ter lly considered th he former one as s a ioon 4.4 : all the vi erion a in Sectio me t no lat , and som oviding long but ed due to provi iding both lat an nd s ( keywords , titl aused by violat wenty search pa nerated ( keyword tion , because no rs . INDICATOR f l the violating te parameters , and ing at least one male)either‐or for Am he ting/satisfying th be arameters must b as ds , title)either‐or sentences includ de ed finally considere ue est case failed du ng d all the satisfyin e of keywords an nd educed by consi y are still unavo straint categories the false positi ating constraint the violating tes scriptions . How ecision in the pr rent words from raint ’s violation and long must be onidering more co oidable due to th he ers exist in one se to ive problem is lly s by additional onst cases to be co ter wever , the strict all : rice of a low reca m the constraint t ’s . For example , th he e paired ” is “ Inv va
NDICATOR are du on the two param between docum he documents m ue to that the do ocmeters in one se enermentation and se missed describin ng ceding results w ct constraint can strategy , INDICA rom the other thr e web service . Co oduced results w 98.7 % ) but a lo e rigid strategy g g only 22.9 % of e lower recall wa use words not co nstraints in the d y , INDICATOR di e)either‐or for the y the sentence “ name parameter ibing sentence , a strategy based a d strategy are om Candidate V eration tion stage from validation stage earch space for r t cases from bein only type definit false positives ng guidance fo t case violates n constraints , or th he service imple e not necessary . nstraints stated e ition , seven con by testing , indic ere invalidated b For example , the mplementation . F esForUser ” states lickrplacesplace e_id or a woe_id d ” , whereas test rs passed . hat all the prec trategy to extrac opt the rigid s bing sentences fr entences for one , INDICATOR pro an average of 9 ) . In addition , th ch , by producing se strategy . The s’ descriptions u ue to missing con he loose strategy d , screen_name ) l ” , mentioned by er_id , or screen_n constraint descri ve for our rigid s sults of the rigid Benefit to C didate Gene andidate generat s the candidate v ows down the se 84.7 % of the test ches based on o d , it reduces the ion , by providin ng that each test ate . luate the latter b t cases without same operation ATOR , as shown are omitted , fo with those of IND pproach resulted the average of 9 95.5 % to 85.2 % false positives in ATOR ) were due r real constraint r the Twitter op proach produced gn , id)either‐or , ( om e real constraint s were produced e all violating te and some of the benefit , we modi considering the n , and compared n in Figure 8 . In for which the m DICATOR . It can d in a non trivia 94.4 % to 55.1 % ) % ) . ntroduced by the to coincidental v candidate in the peration “ GET st d three false con mit_script , id)eith candidate , ( url , d according to est cases failed , e satisfying test c some c that are In addi tion we vice im tion “ fl a place rameter Note th loose st To ado describ plate se results , sion ( a 84.1 % ) approac the loo straints also du using th ( user_id lists/all the use not a c negativ tion res 5.2 B Cand The ca benefit ly narro saved 8 approac Second validati ensurin candida To eval ate test in the INDICA APAA same w fied ap ( from t age of 9 Most fa INDICA another ple , for fied app or , ( alig ing the straints because nor id , ing id . All fals INDICA candida troduced by the e to violation o me operation , wh se negatives intr ATOR ) were due ates in the sam emented some r equirements explicitly in the cating potential b e document for F s that “ you must t cases without documentabugs in serFlickr operat pass either neither pa were obtained a didates from doc ATOR used the ree web services ompared with th with a slightly h ower recall ( an greatly saved the f the candidates p as not only due overed by the tem documentation . F iscovered the rea e Twitter oper “ The user is spe rs ” . This sentenc and thus resulte approach . Detail mitted due to spac Validation fr adopting the cumentation . constraints as the temhe preceding higher preciaverage of e cost of the produced by to that conmplates , but For example , al constraint ration “ GET ecified using ce is clearly ed in a false ls of evaluace limit . rom documentation in two ways . Fi real constraints . ng executed , com tions to generate and negatives fo or test case gene no more than on n and SDKs irst , it greatINDICATOR mpared with e test cases . or candidate eration , and ne constraint ified our approa other constrain d the results wi n particular , the modified results n be observed tha al degradation i ) and recall ( fro ach to genernt candidates ith those of e results for remain the at the modiin precision om the aver e modified appro violation and sa e same operation tatuses/oembed ” nstraints , ( maxwi er‐or , due to viola id)either‐or . Thes criterion a in S due to providing cases passed , du oach ( not by atisfaction of n . For exam” , the modiidth , id)either‐ ating/satisfyse false conSection 4.4 , g neither url ue to provid modified appro of the other rea hich contains m oach ( not by al constraint multiple real
1429 constraint candidates . For example , for the Flickr operation “ flickrplacesplacesForContacts ” , a false negative ( place_type , pla‐ ce_type_id)either‐or was produced , because all the test cases failed due to violating the other real candidate ( place_id , woe_id)either‐or , with only vague error messages “ missing required parameters ” .
0%20%40%60%80%100 % 0%20%40%60%80%100 %
Twitter
Precision
Recall
Lastfm
Precision
Recall
0%20%40%60%80%100 %
100 % 80 % 60 % 40 % 20 % 0 %
Flickr
Precision
Recall
Average
Precision
Recall
Indicator Modified
Figure 8 . Precisions and recalls of INDICATOR and the modi fied approach .
6 . RELATED WORK To the best of our knowledge , there are only two existing approaches to automatically inferring formal constraints of interacting with web services . Bertolino et al . [ 2 ] proposed an approach to synthesize temporal constraints , such as “ a CartCreate operation should be invoked before a CartAdd operation ” . Their approach first derives these constraints from service type definitions based on data type analysis , and then checks the conformance between the derived constraints and the service implementation by means of testing . Fisher et al . [ 5 ] presented an approach also based on testing to discover simple constraints involving single parameters , such as whether a parameter is required . They further applied the discovered constraints to detect imprecision errors in WSDL files , such as declaring a required parameter to be optional . Compared with these two approaches , INDICATOR infers a new and important type of constraints , Dependency Constraints on Parameters . In addition to testing web services , INDICATOR also integrates information from natural language service documentation and service SDKs to infer constraints effectively and efficiently , addressing the two challenges faced by these approaches , as discussed earlier in Section 1 . All these constraints of interacting with web services could be formally described using service modeling languages such as WSML [ 3 ] or OWL S [ 11 ] . To facilitate the automation of service discovering , composing , and invoking , researchers and developers proposed such languages to conceptually model web services . However , according to our investigation , such conceptual descriptions for most popular web services are not readily available . IN‐ DICATOR could automatically discover these constraints , and might help to build the conceptual models for web services . Our work is also related to program verification approaches [ 7 9 , 14 ] that use formally described constraints to detect violations of constraints as bugs in client applications . In particular , Rubinger and Bultan [ 14 ] presented their experience on applying the Microsoft Code Contract system to the Facebook API . They provided the system with formal contracts ( which are called constraints in this paper ) that were manually created according to the Facebook API documentation . The system verified API client applications for contract violations . Their experience indicates that program verification based on contracts enables to build more robust client applications with less effort spent on debugging . Similarly , Hallé et al . [ 7 ] conducted a case study on APAA of verifying client applications at runtime against formally described con straints . Both these pieces of work demonstrate the importance of our approach : INDICATOR automatically infers formal constraints , thus making these constraint based verification approaches practical and usable . We finally present some technically related approaches concerning the constraint inference for local API libraries . According to their inference data sources , these approaches fall into three categories . The first category of approaches [ 4 , 15 ] analyzes the source code of API client applications , and infers the frequent API usage patterns as constraints . Although there are also plenty of open source client applications for web services , inferring constraints from these client applications is unlikely to achieve desirable results . The main issue is the low coverage of web service operations : our manual investigation shows that only the several most popular operations are invoked in the available client applications . However , we plan to explore in future work to include client applications as a complementary information source . The second category of approaches [ 12 , 16 18 ] extracts constraints from API library documentation . In particular , Zhong et al . [ 18 ] proposed an approach to infer resource manipulation constraints from Javadocs . Pandita et al . [ 12 ] proposed an approach to infer pre conditions and post conditions for invocations of API methods from their method descriptions . Both the two approaches infer constraints by combining sophisticated NLP and machine learning techniques . In contrast , thanks to integrating heterogeneous information sources , INDICATOR uses only simple and reliable information from documentation , and then relies on testing to refine the results . The third category of approaches [ 6 , 13 ] infers constraints by testing . In particular , Gabel and Su [ 6 ] described a framework to automatically validate temporal constraints inferred from client applications by testing . Their framework validates a constraint if all its violating test cases fail . As we earlier discussed in Section 1 , using only this criterion would lead to many false positives , when the test cases failed in consequence of violating some other constraints rather than the one under validation . INDI‐ CATOR avoids these false positives by additionally requiring either that some of the satisfying test cases pass , or that the error messages of the violating test cases are consistent with the constraint ’s description . 7 . CONCLUSION In this paper , we have proposed a novel approach called INDICA‐ TOR to automatically inferring Dependency Constraints on Param‐ eters for web services . INDICATOR infers dependency constraints effectively and efficiently via a hybrid analysis of heterogeneous web service artifacts , including the service documentation , the service SDKs , and the web services themselves . To evaluate our approach , we applied INDICATOR to infer dependency constraints for four popular web services . The results show that INDICATOR infers constraints with an average precision of 94.4 % and recall of 955 % Compared with existing approaches based on only web services themselves , INDICATOR improves the precision by 39.4 % and recall by 10.3 % , while saving 84.7 % of the efforts . 8 . ACKNOWLEDGMENTS The authors from Peking University are sponsored by the National Basic Research Program of China ( Grant No . 2009CB320703 ) , the National Natural Science Foundation of China ( Grant No . 61121063，61033006 ) , and the High Tech Research and Development Program of China ( Grant No . 2012AA011202 ) . Tao Xie's work is supported in part by NSF grants CCF 0845272 , CCF0915400 , CNS 0958235 , CNS 1160603 , an NSA Science of Security Lablet grant , and a NIST grant .
1430 9 . REFERENCES [ 1 ] Aho , A . V . , Lam , M . S . , Sethi , R . and Ullman , J . D . 1986 .
Compilers : Principles , Techniques , and Tools . Addison Wesley .
[ 2 ] Bertolino , A . , Inverardi , P . , Pelliccione , P . and Tivoli , M .
2009 . Automatic synthesis of behavior protocols for composable web services . In Proceedings of the 7th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering ( Amsterdam , The Netherlands , August 24 28 , 2009 ) . ESEC/FSE '09 . ACM , New York , NY , 141 150 .
[ 3 ] Bruijn , J . D . , Fensel , D . , Keller , U . , Kifer , M . , Lausen , H . , Krummenacher , R . , Polleres , A . and Predoiu , L . 2005 . Web Service Modeling Language ( WSML ) . Available at http://wwww3org/Submission/WSML/
[ 4 ] Engler , D . , Chen , D . Y . , Hallem , S . , Chou , A . and Chelf , B . 2001 . Bugs as deviant behavior : a general approach to inferring errors in systems code . In Proceedings of the 18th ACM symposium on Operating systems principles ( Chateau Lake Louise , Banff , Canada , October 21 24 , 2001 ) . SOSP '01 . ACM , New York , NY , 57 72 .
[ 5 ] Fisher , M . , Elbaum , S . and Rothermel , G . 2007 . Automated Refinement and Augmentation of Web Service Description Files . Technical Report . University of Nebraska Lincoln . [ 6 ] Gabel , M . and Su , Z . D . 2010 . Testing mined specifications . In Proceedings of the 20th International Symposium on the Foundations of Software Engineering ( Cary , North Carolina , November 11 16 , 2012 ) . FSE '12 . ACM , New York , NY .
[ 7 ] Hallé , S . , Bultan , T . , Hughes , G . , Alkhalaf , M . , Villemaire , R . 2010 . Runtime verification of web service interface contracts . Computer . 43 , 3 ( March 2010 ) , 59 66 .
[ 8 ] Havelund , K . and Pressburger , T . 1999 . Java PathFinder , a translator from Java to Promela . In Proceedings of the 5th and 6th International SPIN Workshops on Theoretical and Practical Aspects of SPIN Model Checking ( Trento , Italy , July 5 , 1999 , Toulouse , France , September 21 and 24 , 1999 ) . Springer Verlag London , UK , 152 .
[ 9 ] Hovemeyer , D . and Pugh , W . 2004 . Finding bugs is easy .
ACM SIGPLAN Notices . 39 , 12 ( December 2004 ) , 92 106 .
[ 10 ] Martin , E . , Basu , S . and Xie , T . 2007 . Automated testing and response analysis of web services . In Proceedings of the IEEE International Conference on Web Services , Application Services and Industry Track ( Salt Lake City , Utah , USA , July 9 13 , 2007 ) . ICWS '07 . 647 654 .
[ 11 ] Martin , D . , Burstein , M . , Hobbs , J . , Lassila , O . , McDermott ,
D . , McIlraith , S . , Narayanan , S . , Paolucci , M . , Parsia , B . , Payne , T . , Sirin , E . , Srinivasan , N . and Sycara , K . 2004 . OWL S : Semantic Markup for Web Services . Available at http://wwww3org/Submission/OWL S/
[ 12 ] Pandita , R . , Xiao , X . S . , Zhong , H . , Xie , T . , Oney , S . and
Paradkar , A . 2012 . Inferring method specifications from natural language API descriptions . In Proceedings of the 34th International Conference on Software Engineering ( Zurich , Switzerland , June 2 9 , 2012 ) . ICSE '12 . IEEE Press Piscataway , NJ , USA , 815 825 .
[ 13 ] Pradel , M . and Gross , T . R . 2012 . Leveraging test generation and specification mining for automated bug detection without false positives . In Proceedings of the 34th 2012 International Conference on Software Engineering ( Zurich , Switzerland , June 2 9 , 2012 ) . ICSE '12 . IEEE Press Piscataway , NJ , USA , 288 298 .
[ 14 ] Rubinger , B . and Bultan , T . 2010 . Contracting the Facebook API . In Proceedings Fourth International Workshop on Testing , Analysis and Verification of Web Software ( Antwerp , Belgium , September 20 24 , 2010 ) . TAV WEB '10 . 63 74 .
[ 15 ] Weimer , W . and Necula , G . C . 2005 . Mining temporal speci fications for error detection . In Proceedings of the 11th International Conference on Tools and Algorithms for the Construction and Analysis of Systems ( Edinburgh , UK , April 48 , 2005 ) . TACAS '05 . Springer Verlag , Edinburgh , UK , 461476 .
[ 16 ] Wu , Q . , Liang , G . T . , Wang , Q . X . and Mei , H . 2011 . Mining effective temporal specifications from heterogeneous API data . Journal of Computer Science and Technology . 26 , 6 ( November 2011 ) , 1061 1075 .
[ 17 ] Wu , Q . , Liang , G . T . , Wang , Q . X . , Xie , T . and Mei , H .
2011 . Iterative mining of resource releasing specifications . In Proceedings of the 26th IEEE/ACM International Conference on Automated Software Engineering ( Lawrence , Kansas , November 6 12 , 2011 ) . ASE '11 . IEEE Computer Society Washington , DC , USA , 233 242 .
[ 18 ] Zhong , H . , Zhang , L . , Xie , T . and Mei , H . 2009 . Inferring resource specifications from natural language API documentation . In Proceedings of the 24th IEEE/ACM International Conference on Automated Software Engineering ( Auckland , New Zealand , November 16 20 , 2009 ) . ASE '09 . IEEE Computer Society Washington , DC , USA , 307 318 .
1431
