Can we use Linked Data Semantic Annotators for the
Extraction of Domain Relevant Expressions ?
Michel Gagnon
Ecole Poytechnique de
Montréal michelgagnon@polymtlca
Amal Zouaq
Royal Military College of amalzouaq@rmcca
Canada
Ludovic Jean Louis Ecole Poytechnique de
Montréal ludovic.jean louis@polymtl.ca
ABSTRACT Semantic annotation is the process of identifying expressions in texts and linking them to some semantic structure . In particular , Linked data based Semantic Annotators are now becoming the new Holy Grail for meaning extraction from unstructured documents . This paper presents an evaluation of the main linked data based annotators available with a focus on domain topics and named entities . In particular , we compare the ability of each tool to annotate relevant domain expressions in text . The paper also proposes a combination of annotators through voting methods and machine learning . Our results show that some linked data annotators , especially Alchemy , can be considered as a useful resource for topic extraction . They also show that a substantial increase in recall can be achieved by combining the annotators with a weighted voting scheme . Finally , an interesting result is that by removing Alchemy from the combination , or by combining only the more precise annotators , we get a significant increase in precision , at the cost of a lower recall .
Categories and Subject Descriptors [ Natural Language Processing ] : mation extraction ]
[ Text analysis , Infor
Keywords Semantic annotation , topic extraction , evaluation
1 .
INTRODUCTION
Semantic annotation is the process of identifying expressions in texts and linking them to some entities in a knowledge base . The importance of semantic annotation for the Semantic Web is not anymore to be demonstrated . The Semantic Web realization depends on the availability of metadata , defined through some formal semantic structures , and describing web content . Thus , the acquisition of metadata through the development of automatic annotation tools is a major challenge for the Semantic Web . There have been many semantic annotators developed based on ontologies [ 1 , 2 ] during the past decade , and some comparative studies have been performed on these annotators [ 3 , 4 ] . However , a recent trend has emerged with the development of semantic annotators that are based on the Linked Open Data cloud
Copyright is held by the International World Wide Web Conference Committee ( IW3C2 ) . IW3C2 reserves the right to provide a hyperlink to the author ’s site if the Material is used in electronic media . WWW 2013 Companion , May 13–17 , 2013 , Rio de Janeiro , Brazil . ACM 978 1 4503 2038 2/13/05 .
( LOD ) [ 5 ] . Typically , they detect named entities and concepts and link them to some semantic entities in a given LOD dataset . The most prominent and exploited dataset is DBpedia [ 6 ] , which is considered as a hub on the LOD .
Due to the recent emergence of linked data annotators , very few comparative studies have been published on their performance [ 7 , 8 ] . The available studies generally focused on traditional named entities ( eg person , organization , etc . ) in their evaluation . These studies were mainly interested in evaluating the capability of the annotators to correctly link expressions to semantic entities . Recently , the Knowledge Base Population track has proposed a specific task named entity linking dedicated to the evaluation of such annotators . In that context , annotators have to link a named entity mention in a document with its corresponding entry in Wikipedia . [ 9 , 10 ] give an overview of the methods used by the track participants . Previous studies did not draw any conclusion about the relevance of the detected entities , which is a crucial point . We know that semantic annotators have not necessarily been developed with the objective of finding only expressions that are relevant to the domain . Nevertheless , one might be interested in an annotator that does not only correctly disambiguate entities , but also produce a relevant set of annotations . Evaluating the existing annotators in this respect will help us determine how far we are from a state where they could be efficiently used for such a task .
In this paper , we address directly this issue . Our main research questions can be articulated around the following points : i ) whether linked data based semantic annotators can be used to extract relevant expressions in a specific domain and ii ) whether the performance of individual semantic annotators can be improved using voting methods and machine learning . Answering these two research questions is crucial for the further development and use of semantic annotators and is of major importance to several communities , including the text mining and the Semantic Web community . Noting that there are two kinds of annotated expressions , that is , named entities and other expressions that are linked to conceptual entities ( which we will call topics in this paper ) , we also evaluated the relevance of detected expressions separately for these two kinds of annotations .
To answer these research questions , we evaluate the performance of seven state of the art linked data annotators ,
1239 namely DBPedia Spotlight1[11 ] , Wikimeta2 , OpenCalais 3 , Alchemy 4 , Lupedia 5 , Yahoo Content Analysis API 6 and Zemanta 7 . Section 2 details each annotator . Then section 3 describes the research methodology used to perform the annotations , create a domain dependent Gold Standard , and compare the results . In section 4 , we show the performances for each individual annotator . In section 5 we describe the voting methods and machine learning algorithms used in our experiments , followed by the presentation of their performances , in section 6 .
2 . SEMANTIC ANNOTATORS
As previously said , semantic annotation , also called entity linking , consists in identifying a unique and precise link between a word sequence and a semantic resource descriptor , among those available in repositories such as the Linked Open Data ( LOD ) datasets . Semantic annotation is based on various techniques in natural language processing ( NLP ) and machine learning for entity extraction and relies upon Semantic Web knowledge representations such as ontologies and open linked data for entity linking ( see , for example , [ 11 , 12 , 13 , 14] ) .
Semantic annotation can be seen as encompassing the named entity recognition task , which limits itself to the identification of expressions belonging to a closed set of class labels , such as person , product , organization , etc . For example , in the sentence Barack Obama is the President of the United States , the NER task would identify Barack Obama as a person and United States as a geographical location . In contrast , semantic annotation would associate these two expressions to a unique entity in a knowledge base such as DBpedia ( http://dbpedia.org/resource/Barack_Obama and http : //dbpedia.org/resource/United_States , respectively ) .
The following sections give a short description of the most prominent semantic annotators used in this evaluation . In some cases , the description is very generic due to the lack of published descriptions of the services .
AlchemyAPI employs sophisticated deep linguistic parsing and statistical language processing for performing the annotations . It offers various APIs , among which two are relevant for our experiments : named entity extraction and keyword extraction . The named entity extractor is able to disambiguate the detected entities and resolve co references . Entities are linked to various datasets on the LOD . Keyword extraction focuses on topics , but cannot be considered as an “ annotation task ” per se , as the service produces a list of keywords and does not indicate the portions of texts that refer to these keywords . By default , the keyword extractor returns a maximum of 50 keywords . Contrarily to keywords , the position of named entities in texts is returned by the named entity extractor .
1https://github.com/dbpedia spotlight/ dbpedia spotlight/ 2http://wikimeta.org/ 3http://http://wwwopencalaiscom/ 4http://wwwalchemyapicom/ 5http://lupediaontotextcom/ 6http://developeryahoocom/contentanalysis/ 7http://developerzemantacom/
DBpedia Spotlight achieves semantic annotation using a three step approach [ 11 ] . The first step , the spotting phase , is the identification of candidate word sequences that could be linked to an entity in the DBpedia dataset . DBpedia Spotlight offers various methods ( spotters ) for this phase . Then for each candidate word sequence in text , DBpedia Spotlight pre ranks DBpedia entities for which there is an associated label that corresponds to the sequence surface form . Finally , in the disambiguation phase , DBpedia Spotlight uses a similarity score to determine which candidate entity is the most relevant . The similarity score takes into account the context of the expression ( a window of words around the expression ) and the context of each candidate entities . Since each DBpedia entity is represented as a URI that mirrors a corresponding entry in Wikipedia , the union of the set of words around the Wikipedia hyperlinks that point to the corresponding URI is used as a context for these candidate entities .
Wikimeta performs named entity detection as a first step using a statistical model . It then tries to link each detected named entity to some entity in DBpedia based on a disambiguation process that is described in [ 12 ] . Similarly to DBpedia Spotlight , a word context around the expression is compared to the contexts of candidate resources in DBpedia . Essentially , Wikimeta differs from DBpedia Spotlight in the way expressions are detected in texts and in the lexical resource structure used to find candidate semantic entities .
Yahoo! Content Analysis API detects several types of expressions including entities , concepts , categories , and relationships within texts . Entities are ranked by their overall relevance , and some of these entities are then mapped to Wikipedia pages when possible . Unfortunately , very few details regarding the techniques used for the implementation of the service are available .
Open Calais semantic annotations include entities , facts , events and categories . Expressions in texts are linked to entities described in Open Calais ontology rather than on the LOD . However , some entity types , such as cities , countries or companies can be further described with a link to DBpedia . Open Calais also relies on natural language processing ( NLP ) , machine learning and other methods to create its annotations .
Lupedia enrichment service is mainly a named entity recognition system . It uses a gazetteer , which is essentially a list of surface forms that are associated to a subset of entities in DBpedia ( events , organizations , persons , places and works ( eg musical or artistic ) ) and LinkedMDB ( a dataset that contains movies descriptions : films , directors , actors , etc ) The default configuration takes the longest sequence of consecutives words that corresponds to some entry in the gazetteer and annotates it with the corresponding entity in the knowledge base . The annotation does not take into consideration the context of the expression to disambiguate between multiple potential candidates , contrary to Wikimeta or DBPedia Spotlight .
Zemanta : By opposition to most of the other semantic annotators , Zemanta does not annotate the position of
1240 each detected expression in texts . Rather , it provides a set of expressions , which are not necessarily found in texts and it identifies topics that can represent the content as a whole . For each expression , Zemanta specifies a list of links that point to corresponding entries in some knowledge resources , such as Wikipedia , IMDB , MusicBrainz and Amazon book listings .
Table 1 summarizes the main characteristics of these se mantic annotators .
Annotator Alchemy Spotlight Wikimeta Lupedia Open Calais NE/Top Yahoo NE/Top Top Zemanta
Pos . in text Detects Only for NE NE/Top NE/Top yes Mainly NE yes yes NE yes yes no
Table 1 : Characteristics of semantic annotators ( NE = named entites , Top = Topics )
3 . RESEARCH METHODOLOGY
To understand to what extent semantic annotators are adequate for the extraction of relevant domain expressions , we made two experiments . First , we evaluated the performances of the semantic annotators separately on a domain corpus . As we will see , most of the annotators are not sufficiently efficient in terms of precision and recall to identify domain relevant expressions . In our second experiment , various combination approaches have been tested . In all experiments , an expression is considered relevant if it is directly related to the content of the document where it appears . Thus , by taking the union of the sets of relevant expressions extracted from each document , we obtain a set of expressions that may be considered as relevant for the domain .
In our experiments , we analyzed the performance in three different situations , depending on the type of expressions that are considered : all expressions , only named entities and only topics ( that is , every relevant expression that is not a named entity ) . To perform our experiments , we relied on a corpus of 8 texts taken from Wikipedia , all related to the artificial intelligence domain . Together , these texts represent 10570 words . By applying all annotators to these texts , we obtained 2023 expression occurrences among which 1151 were distinct .
We asked a human evaluator to analyze each expression and make the following decisions :
• Does the annotated expression represent an understandable named entity or topic ? To be understandable , an expression must be a complete and well formed expression ( not part of another expression ) and it must be semantically significant . Verbs , adjectives , adverbs and generic nouns ( like person , system , theory ) are not considered as understandable annotations .
• Is the expression a relevant keyword according to doc ument content ?
• Is the expression a relevant named entity according to
The last question is not simple , because the notion of named entity has evolved with the latest advancements of semantic annotators . Traditional named entity definition involves entities with common names such as persons , locations , organizations , products , events and dates . However , we have witnessed a tendency to refine and extend possible categories of named entities through various taxonomies defined for semantic annotators . The NERD evaluation platform [ 8 ] , in an attempt to unify these taxonomies , describes 85 possible categories , including sport events , operating systems , political events and websites , all of which cannot be considered as “ traditional ” named entities . Similarly , in specialized domains , such as the biomedical domain , a specific fine grained categorization of named entities is used and includes genes , proteins , diseases , drugs , or organisms [ 15 ] . In the computer science domain , some questions regarding the possible categorizations emerge as well , as we must decide if some expressions such as XML and RDF are named entities or topics . For example , XML and RDF somehow refer to unique instances of entities ( here metadata languages ) , but their use in sentences such as An RDF based data model is naturally suited to certain kinds of knowledge representation does not reflect the usual definition of a named entity . In our evaluation , we adopt the “ classical interpretation ” of named entities and consider , in this case , XML and RDF as topics . The answers of the human annotator allowed us to build a Gold Standard whose expression distribution is given in Table 2 . Note that only 639 out of 1151 detected expressions are understandable ( 56% ) . Thus a substantial number of detected expressions represents noise . However , a high ratio of understandable expressions is considered relevant ( 79% ) . It is also interesting to note that , while 11 % of detected expressions are named entities , few of them are relevant . In fact , only 6 % of relevant expressions are named entities ( 30 out of 507 ) .
Type of expression Total detected Total understandable Topics Named entities
Quantity Nb . Relevant 1151 639 516 123
507 477 30
Table 2 : Distribution of expressions in our Gold Standard . We consider only distinct expressions .
Table 3 shows all expressions detected at least 6 times in our corpus . For each expression , we also indicate whether it has been tagged as understandable and relevant .
4 . PERFORMANCES OF SEMANTIC AN
NOTATORS
Using the obtained Gold Standard , we evaluated the output of each semantic annotator using standard information retrieval measures : precision , recall and F measure . Precision , recall and F score are defined in the following way : Let
SA = ND(ai ) = Set of distinct expressions detected by the ai , . . . an where ai is an annotator . annotator ai . document content ?
NO =
NR(ai ) = Set of distinct relevant expressions de tected by the annotator ai . ai∈SA
NR(ai ) .
1241 Expression artificial intelligence intelligence AI intelligent agent unknown machine learning Arthur C . Clarke John McCarthy data mining computer science Ray Kurzweil knowledge intelligent agents Thomas Nagel system Russell neural network natural language processing Marvin Minsky Hubert Dreyfus computational linguistics world Web Ontology Language theory systems SPSS software engineering semantic web science Roger Penrose learning Hubble space telescope Herbert Simon FIPA algorithms
# occ . Und . Rel . yes 38 yes 18 15 yes yes 13 no 12 yes 12 no 12 11 yes yes 11 yes 11 yes 10 yes 10 yes 9 8 no no 8 no 8 yes 8 yes 8 7 yes no 7 yes 7 no 6 yes 6 no 6 6 no yes 6 yes 6 yes 6 yes 6 no 6 6 yes no 6 yes 6 yes 6 6 yes yes yes yes yes no yes yes yes yes yes yes yes yes yes no no yes yes yes yes yes no yes no no yes yes yes yes yes yes yes yes yes yes
Table 3 : List of expressions detected at least 6 times . For each expression , we indicate if it has been tagged as understandable and relevant .
Prec(ai ) =
Rec(ai ) =
|NR(ai)| |ND ( ai)| |NR(ai)|
|NO|
F Score(ai ) = 2×Prec(ai)×Rec(ai ) Prec(ai)+Rec(ai )
Note that we adopted an unusual way of computing recall , since our Gold Standard has been produced by considering only the expressions detected by at least one annotator , instead of considering independently all expressions that are found in our corpus . To obtain values that are comparable to the ones obtained using combination methods ( see section 5 ) , precision and recall values are not computed on the whole extracted expressions , but rather averaged over five balanced partitions of the expressions8 .
Table 4 shows the results obtained for each annotator taken individually . We can notice that F score is low for almost all annotators . This is not really surprising , considering that semantic annotators usually do not have as objective the extraction of relevant expressions . Low precision of Wikimeta is explained by the fact that it detected some
8Exactly the same partitions used for evaluating combination methods .
Method Alchemy Spotlight Wikimeta Lupedia Open Calais Yahoo Zemanta
Alchemy Spotlight Wikimeta Lupedia Open Calais Yahoo Zemanta
Alchemy Spotlight Wikimeta Lupedia Open Calais Yahoo Zemanta
All expressions
Det Top NE Und . Rel 347 619 356 108 96 243 12 42 91 147 74 105 77 61
564 338 149 21 107 93 69
55 18 94 21 40 12 8
428 149 130 28 127 85 69
Only named entities
55 18 94 21 40 12 8
564 338 149 21 107 93 69
0 0 0 0 0 0 0
55 18 94 21 40 12 8
50 17 51 19 39 11 8
Only topics 378 132 79 9 88 74 61
0 0 0 0 0 0 0
564 338 149 21 107 93 69
18 8 24 6 12 5 5
329 100 72 6 79 69 56
P/R/F 056/069/062 03/021/025 039/019/025 027/0024/0043 062/018/028 07/015/024 077/012/021
037/062/044 044/027/032 025/078/038 031/022/024 033/039/035 028/015/02 047/017/025
059/069/063 03/021/025 048/015/023 03/0012/0023 075/017/027 074/015/024 081/012/021
Table 4 : Results obtained for each annotator taken individually : number of detected expressions(Det ) , topics ( Top ) , named entities ( NE ) , understandable ( Und. ) , relevant ( Rel ) , precision ( P ) , recall ( R ) and F score ( F ) . named entities , like times and dates , that are not usually relevant as keyphrases . DBpedia Spotlight annotated many expressions that are not considered understandable . Note that if we consider only understandable expressions , precision values for Alchemy , Spotlight and Wikimeta would be 0.81 , 0.73 and 0.74 , respectively , Lupedia ’s precision would still be low ( 0.43 ) and , as expected , precision values for Open Calais , Yahoo and Zemanta would remain very high ( 0.72 , 0.87 and 0.88 , respectively ) .
One semantic annotator that clearly distinguishes itself is Alchemy , which obtains a very high value for recall when all entities or only topics are considered . This means that a great proportion of relevant expressions detected by other annotators are also detected by Alchemy . But the precision of Alchemy ’s results is not very high , compared to the results of Open Calais , Yahoo and Zemanta , which exhibit the highest precision . However , recall values for these three annotators are very low . Interestingly , peformances are very low when only named entities are considered : none of the annotators was good at precisely detecting relevant named entities . Note the high recall value obtained with Wikimeta . Table 5 describes the frequency of detected expressions . As can be noticed , most of the expressions have been detected by only one ( 76 % ) or two ( 16 % ) annotators . From these results , we can conclude that semantic annotators are complementary . Thus it is reasonable to expect that a combination of their decisions would provide better performances .
5 . COMBINATION METHODS
To test the above mentioned hypothesis , we implemented a number of voting measures . An evident combination method is a vote among the annotators . In its simplest implementation , an expression is considered relevant if a minimum of m among n annotators detect it . Another strategy is to produce a weighted vote by using the precision of each indi
1242 N 1 2 3 4 5 6 7 Total
Number of expressions 872 183 57 24 8 5 2 1151
Table 5 : Frequency of detected expressions . For each value N , we provide the number of distinct expressions that have been detected by exactly N annotators . vidual annotator on a training corpus . Given the set of annotators used in the experiment SA = {a1 . . . an} , ann(i , e ) is equal to 1 if expression e is detected by ai and 0 otherwise . More formally , the voting methods may be described in the following way :
Simple vote : For each detected expression e , return e as a
Weighted vote : For each detected expression e , return e relevant expression if old , such that 0 ≤ th ≤ n . ai∈S ann(i , e ) ≥ th , where th is a predefined threshas a relevant expression if ai∈S wi × ann(i , e ) ≥ th , th is a predefined threshold , such that 0 ≤ th ≤ 1 , prec(ai ) denotes the precision for annotator ai , where : and weight wi is defined as prec(ai ) ai∈S prec(ai ) . training , we realized 5 experiments for each of the following situations : all entities , only named entities and only topics . For each combination method ( weighted vote , K nearest neighbors , Naive Bayes , decision tree and rule induction ) , a relevant expression classifier is obtained based on the training set . Then , the test set is used to compute precision , recall and F measure . Precision for combination methods is the ratio of expressions correctly classified as relevant expressions , whereas recall is the ratio of the entire set of relevant expressions ( according to the Gold Standard ) that has been recognized by the classifier . As we said earlier , we adopted an unusual way of computing recall , since our Gold Standard is produced by considering only the expressions detected by annotators , instead of considering independently all possible relevant expressions in the corpus ( some relevant expressions might have been missed by all annotators ) . More formally , let D be the set of detected expressions in our corpus . Sg is the set {e | e ∈ D , ai ∈ SA,∃ai such that ann(i , e ) = 1 , rel(e ) = 1} . Put simply , Sg is the set of relevant expressions that have been detected by at least one annotator . Let Dec(e ) be the decision made by our classifier for expression e ( returns 1 if it classifies e as a relevant expression , and 0 otherwise ) . Se = {e | e ∈ D , Dec(e ) = 1} is the set of expressions considered as relevant by our classifier and Sr ⊆ Se is the subset of these expressions that are relevant expressions according to Gold Standard , that is , Sr = {e | e ∈ Se , rel(e ) = 1} . Precision , recall and Fscore are defined in the following way :
Precision : P =
Recall :
R =
|Sr| |Se| |Sr| |Sg|
F score :
F = 2×P×R
P +R
As can be noticed , the precision of an annotator is obtained by computing the ratio of relevant expressions among the ones detected by each annotator . Relevant expressions are those defined in the Gold Standard .
Another voting scheme is the K nearest neighbours classifier . For each detected expression e in a training dataset , we define a vector v(e ) composed of the n decisions of annotators a1 to an for this expression . Given D , the set of detected expressions in the training dataset , we obtain a set T = {v(e ) , rel(e ) | e ∈ D} , where rel(e ) = 1 if e is a relevant expression . To classify a new expression e , we simply find the K closest vectors TK ⊆ T and consider e as relevant if it is relevant for a majority of vectors in TK .
Finally , we also experimented with various classical machine learning methods . For all these methods , we used Weka ’s implementations with default configurations : Naive Bayes classifier , decision tree and rule induction . For decision tree , we selected the C4.5 algorithm , and for rule induction , the PART algorithm [ 16 ] , where rules are extracted from partial decision trees .
6 . EVALUATION OF COMBINATION METH
ODS
To evaluate the combination methods , we partitioned the set of detected expressions into 5 balanced partitions . By selecting one partition for testing and the remaining ones for
For all the voting methods ( simple vote and weighted vote ) , a threshold must be determined . Table 6 provides the performance values under various thresholds for the simple vote method . The highest F score is obtained with threshold = 1 , but it is due to the fact that all expressions are classified as relevant expressions , since all of them are detected by at least one annotator . Therefore , it is not surprising that none of the actual relevant expressions has been missed . However , precision is too low . In fact , we may consider this situation as our baseline . The optimal value for threshold is 2 , but still , this value does not outperform any of the best annotators taken individually .
Table 7 reports the performance values obtained with the weighted vote method . We can observe that a threshold of 0.10 gives the best F score value , for cases where all entities or all topics are considered . For named entities , the threshold must be slightly higher to obtain good performances . If we consider that precision is more important than recall , a threshold of 0.15 would be a better choice . We also see that the F score value is higher than the value obtained by annotators taken separately ( for Alchemy the increase may be not significant ) .
If precision is very important , Table 7 shows that the weighted vote approach may be a good solution if we use a threshold of 0.25 , in which case the precision is 069 Recall is low , but we still obtain 45 detected expressions , on average . In this case , the list of extracted expressions for one
1243 Threshold P
F All expressions considered
R
1 2 3 4 5 6 7
1 2 3 4 5 6 7
1 2 3 4 5 6 7
0.44 0.65 0.67 0.6 0.55 0.4 0.1
1.0 0.36 0.13 0.047 0.018 0.0098 0.0019
0.61 0.46 0.21 0.086 0.034 0.019 0.0036
Only named entities considered 0.39 0.47 0.46 0.29 0.14 0.15 0.05
1.0 0.62 0.5 0.26 0.095 0.095 0.029
0.24 0.38 0.45 0.34 0.3 0.4 0.2
Only topics considered
0.46 0.72 0.83 0.9 0.8 0.2 0.0
1.0 0.34 0.1 0.036 0.012 0.0043 0.0
0.63 0.46 0.18 0.069 0.024 0.0083 0.0
Table 6 : Precision ( P ) , recall ( R ) and F score ( F ) values obtained for simple vote method . partition would be the following one9 : RDF Schema , ( Hubert Dreyfus ) , Computational learning theory , ( Roger penrose ) , ( lengthy research ) , science , ( AGIRI ) , Semantic Web , Neural network , Stock market analysis , bioinformatics , continuous planning algorithm , ( Japan ) , DAG , graphical model , research , ( computer hardware ) , ( human traits ) , ( John ) , pattern recognition , semantic networks , ( chess ) , ( costly search processes ) , NLP , XML , NLP algorithms , weak AI hypothesis , Dijkstra , artificial neural network , ILP , ( data mining efforts ) , ( Oxford University Press ) , ( scientist ) , singularity institute for artificial intelligence , ( human ) , Norvig , machine learning algorithms , ( William Clocksin ) , hierarchical task networks , SVM .
Tables 8 compares the results of the machine learning approaches and the best voting method , which is weighted vote . On the average , weighted vote displays the best precision among combination methods if only topics are considered . In this case , we see that precision obtained with weighted vote is significantly higher than the one obtained with Alchemy ( 0.66 vs 059 ) Machine learning methods achieve better recall and F score on the average , especially if decision tree or rule induction ( PART ) is used . Finally , we see that considering only named entities , the combination methods do not improve the performances .
Finally , Table 9 compared the performances obtained with weighted vote using all annotators with a combination that excludes Alchemy . We notice a slight increase in average precision , with a maximum value of 078 This suggests that Alchemy has the effect of finding many relevant expressions that are not discovered by other annotators ( recall of 0.78
9Irrelevant expressions , according to our Gold Standard , are enclosed within parenthesis .
Threshold P
F All expressions considered
R
0.0 0.05 0.1 0.15 0.2 0.25 0.3 0.35
0.0 0.05 0.1 0.15 0.2 0.25 0.3 0.35
0.44 0.44 0.52 0.57 0.67 0.69 0.7 0.69
1.0 1.0 0.92 0.78 0.39 0.31 0.24 0.16
0.61 0.61 0.66 0.64 0.49 0.42 0.36 0.26 Only named entities considered 0.39 0.39 0.4 0.48 0.48 0.5 0.49 0.36
1.0 1.0 0.85 0.7 0.65 0.57 0.54 0.35
0.24 0.24 0.26 0.36 0.38 0.47 0.46 0.39
0.0 0.05 0.1 0.15 0.2 0.25 0.3 0.35
Only topics considered
0.46 0.46 0.56 0.66 0.7 0.78 0.8 0.88
1.0 1.0 0.91 0.53 0.37 0.29 0.23 0.12
0.63 0.63 0.69 0.56 0.48 0.42 0.35 0.21
Table 7 : Average values for precision ( P ) , recall ( R ) and F score ( F ) with weighted vote method . vs 0.37 ) , but at the cost of some noise . Also , the negative effect on precision due to the presence of Alchemy may be explained by the fact that it detects substantially more expressions than any other annotator . Thus , the probability of adding its vote to an expression is high , with the effect of pushing the score over the threshold for many irrelevant expressions that would otherwise bien overlooked . More detailed analysis must be achieved to obtain a clear explanation of this phenomenon .
We also produced one additional combination , by taking into account only the most precise ( on an individual basis ) annotators OpenCalais , Yahoo and Zemanta . As expected , this had the effect of increasing the precision ( 0.64 ) while also improving the recall of each of these annotators separately .
7 . CONCLUSIONS
In this paper , we presented a first attempt to evaluate the capability of linked data semantic annotators to detect expressions that are relevant to a domain . We are aware that these annotators were not necessarily developed with this objective ( especially DBpedia Spotlight and Wikimeta ) , but this does not undermine the pertinence of this kind of evaluation . Our results show clearly that if relevancy to the domain is important , the extraction of key expressions achieved by these tools would be only a first step that should be followed by some filtering or refinement . Even if we consider only named entities , for which these annotators are
1244 Method
Average P/R/F
Min P/R/F
Max P/R/F
Weighted vote ( th = 0.15 ) KNN ( N=3 ) Bayes Dec . tree PART
Weighted vote ( th = 0.15 ) KNN ( N=3 ) Bayes Dec . tree PART
Weighted vote ( th = 0.15 ) KNN ( N=3 ) Bayes Dec . tree PART
All entities considered
057/078/064
053/043/049
059/09/071
054/039/048 058/071/062 052/068/062 057/072/063 054/078/066 058/082/067 057/082/068 054/078/066 Only named entities considered 02/043/027 036/07/048
00/00/00 048/012/017 035/015/019 00/00/00 004/005/0044 00/00/00 047/0095/015 00/00/00
Only topics considered
062/086/068 058/077/065 06/087/07 059/087/07
054/10/07
10/029/033 10/033/05 02/025/022 10/017/029
066/053/056
053/039/051
073/089/066
059/073/064 059/077/067 059/083/069 059/083/069
052/043/052 053/073/063 054/073/066 054/072/066
066/084/07 066/082/073 064/09/075 064/09/075
Table 8 : Average , min and max values obtained for precision ( P ) , recall ( R ) and F score ( F ) .
Method
All annotators Without Alchemy Only OpenCalais , Yahoo and Zemanta
Average P/R/F 057/078/064 063/037/047
Min P/R/F
Max P/R/F
053/043/049 054/029/038
059/09/071 078/046/058
064/035/045
054/024/033
079/044/057
Table 9 : Peformance , using weighted vote with threshold = 0.15 usually good , performance would not be sufficient : the detected named entities are relevant . few of
Another observation we made is that semantic annotators are complementary : most of the expressions have been detected by at most two annotators . This led us to experiment combination methods , which revealed a clear improvement on recall , meaning that more relevant expressions are detected . But precision remains unsatisfactory , at about 057 Our results show that if we could automatically identify named entities and exclude them from the set of detected expressions , precision would increase significantly up to 0.66 if we used weighted vote , but at the cost of a lower recall ( 053 )
It is important to note that in our combination experiments , the only feature we use is whether the annotator has detected an expression or not . But there are more features that could be used : some annotators try to identify the category of the entity and some provide a link to Wikipedia , DBpedia or some other resource . We expect that using these features would help increase the precision .
The kind of evaluation we tried to realize is made difficult by the absence of evaluation corpora . We had to create our own evaluation corpus , which is costly . We are aware that our expriment should be repeated with a larger dataset , previously annotated such that real recall values could be computed , but nevertheless our results already help us to make a useful assessment on the capability of semantic annotators to detect relevant entities . We are planning to work on the creation of such a large corpus , which will be made available to the research community .
In future works , apart from taking into consideration more features in combination methods , we will explore other annotators . We will also compare the results of linked data semantic annotators with keyphrase extractors .
8 . REFERENCES
[ 1 ] Dill , S . , Eiron , N . , Gibson , D . , Gruhl , D . , Guha , R . ,
Jhingran , A . , Kanungo , T . , Rajagopalan , S . , Tomkins , A . , Tomlin , J . , Zien , J . : Semtag and seeker : Bootstrapping the semantic web via automated semantic annotation . In : Proceedings of the 12th International Conference on World Wide Web , Budapest , Hungary ( 2003 ) 178–186
[ 2 ] Popov , B . , Kiryakov , A . , Kirilov , A . , Manov , D . ,
Ognyanoff , D . , Goranov , M . : KIM semantic annotation platform . In : 2nd International Semantic Web Conference ( ISWC 20013 ) , Sanibel Island , Florida ( 2004 ) 834–849
[ 3 ] Reeve , L . , Han , H . : Survey of semantic annotation platforms . In : Proceedings of the 2005 ACM symposium on Applied computing . SAC ’05 , New York , NY , USA , ACM ( 2005 ) 1634–1638
[ 4 ] Rajput , Q . , Haider , S . : A comparison of two ontology based semantic annotation frameworks . In : AIAI . ( 2010 ) 187–194
[ 5 ] Heath , T . , Bizer , C . : Linked data : Evolving the web into a global data space . Synthesis Lectures on the Semantic Web : Theory and Technology 1 ( 2011 ) 1–136
[ 6 ] Auer , S . , Bizer , C . , Kobilarov , G . , Lehmann , J . ,
Cyganiak , C . , Ives , Z . : Dbpedia : A nucleus for a web of open data . In : Proceedings of the 6th International Semantic Web Conference ( ISWC2007 ) , Busan , Korea ( 2007 )
[ 7 ] Hachey , B . , Radford , W . , Nothman , J . , Honnibal , M . ,
Curran , JR : Evaluating entity linking with wikipedia . Artificial Intelligence 194 ( 2013 ) 130–150 [ 8 ] Rizzo , G . , Troncy , R . , Hellmann , S . , Bruemmer , M . : NERD meets NIF : Lifting NLP extraction results to the linked data cloud . In : Linked Data on the Web ( LDOW2012 ) . ( 2012 )
[ 9 ] Ji , H . , Grishman , R . , Dang , HT : Overview of the
TAC 2011 knowledge base population track . In : Proceedings of the Text Analysis Conference . ( 2011 )
[ 10 ] Ji , H . , Grishman , R . : Knowledge base population :
Successful approaches and challenges . In : Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics : Human Language Technologies , Association for Computational Linguistics ( 2011 )
[ 11 ] Mendes , PN , Jakob , M . , Garc´ıa Silva , A . , Bizer , C . :
Dbpedia spotlight : shedding light on the web of documents . In : Proceedings of the 7th International Conference on Semantic Systems . I Semantics ’11 , New York , NY , USA , ACM ( 2011 ) 1–8
[ 12 ] Charton , E . , Gagnon , M . , Ozell , B . : Automatic semantic web annotation of named entities . In : Canadian Conference on AI . ( 2011 ) 74–85
1245 [ 13 ] Oren , E . , Hinnerk M¨oller , K . , Scerri , S . , Handschuh ,
[ 15 ] Leaman , R . , Gonzalez , G . : BANNER : An executable
S . , Sintek , M . : What are semantic annotations ? Technical report , DERI ( 2006 )
[ 14 ] Grassi , M . , Morbidoni , C . , Nucci , M . , Fonda , S . ,
Ledda , G . : Pundit : Semantically structured annotations for web contents and digital libraries . In : Proceedings of the Second International Workshop on Semantic Digital Archives ( SDA 2012 ) , Paphos , Cyprus ( 2012 ) survey of advances in biomedical named entity recognition . In : Pacific Symposium on Biocomputing . ( 2008 ) 652–663
[ 16 ] Holmes , G . , Mark , H . , Eibe , F . : Generating rule sets form model trees . In : Australian Joint Conference on Artificial Intelligence . ( 1999 ) 1–12
1246
