Localized Matrix Factorization for Recommendation based on Matrix Block Diagonal Forms
Yongfeng Zhang , Min Zhang , Yiqun Liu , Shaoping Ma , Shi Feng
State Key Laboratory of Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Science & Technology , Tsinghua University , Beijing , 100084 , China zhangyf07@gmail.com {z m,yiqunliu,msp}@tsinghuaeducn fredfsh@gmail.com
ABSTRACT Matrix factorization on user item rating matrices has achieved significant success in collaborative filtering based recommendation tasks . However , it also encounters the problems of data sparsity and scalability when applied in real world recommender systems . In this paper , we present the Localized Matrix Factorization ( LMF ) framework , which attempts to meet the challenges of sparsity and scalability by factorizing Block Diagonal Form ( BDF ) matrices . In the LMF framework , a large sparse matrix is first transformed into Recursive Bordered Block Diagonal Form ( RBBDF ) , which is an intuitionally interpretable structure for user item rating matrices . Smaller and denser submatrices are then extracted from this RBBDF matrix to construct a BDF matrix for more effective collaborative prediction . We show formally that the LMF framework is suitable for matrix factorization and that any decomposable matrix factorization algorithm can be integrated into this framework . It has the potential to improve prediction accuracy by factorizing smaller and denser submatrices independently , which is also suitable for parallelization and contributes to system scalability at the same time . Experimental results based on a number of realworld public access benchmarks show the effectiveness and efficiency of the proposed LMF framework .
Categories and Subject Descriptors H33 [ Information Storage and Retrieval ] : Information Filtering ; H35 [ Online Information Services ] : Webbased services ; G16 [ Mathematics of Computing ] : Optimization
Keywords Matrix Factorization ; Collaborative Filtering ; Block Diagonal Form ; Graph Partitioning
1 .
INTRODUCTION
Latent factor model has been one of the most powerful approaches for collaborative filtering . Some of the most successful realizations of latent factor models are based on Matrix Factorization ( MF ) techniques [ 17 ] . The fundamental idea of these approaches is that user preferences can be determined by a relatively small number of latent factors . A variety of matrix factorization methods have been proposed
Copyright is held by the International World Wide Web Conference Committee ( IW3C2 ) . IW3C2 reserves the right to provide a hyperlink to the author ’s site if the Material is used in electronic media . WWW 2013 , May 13–17 , 2013 , Rio de Janeiro , Brazil . ACM 978 1 4503 2035 1/13/05 . and applied to various collaborative filtering tasks successfully , such as Singular Value Decomposition ( SVD ) [ 17 , 33 ] , Non negative Matrix Factorization ( NMF ) [ 18 , 19 ] , MaxMargin Matrix Factorization ( MMMF ) [ 34 , 23 ] and Probabilistic Matrix Factorization ( PMF ) [ 26 , 25 ] .
However , MF approaches have also encountered a number of problems in real world recommender systems , such as data sparsity , frequent model retraining and system scalability . As the number of ratings given by most users is relatively small compared with the total number of items in a typical system , data sparsity usually decreases prediction accuracy and may even lead to over fitting problems . In addition , new ratings are usually made by users continuously in real world recommender systems , leading to the need for refactoring rating matrices periodically , which is time consuming for systems with millions or even billions of ratings , and further restricts the scalability of MF approaches .
In this study , we propose a novel MF framework named Localized Matrix Factorization ( LMF ) , which is general and intrinsically compatible with many widely adopted MF algorithms . Before problem formalization , we would like to use an intuitional example to briefly introduce the matrix structures used in LMF . Figure 1(a ) is a sparse matrix where each row/column/cross represents a user/item/rating . By permuting Row4 , Row9 and Column7 to ‘borders’ , the remaining part is partitioned into two ‘diagonal blocks’ , which results in a Bordered Block Diagonal Form ( BBDF ) [ 4 ] matrix in Figure 1(b ) . By ‘recursively’ permuting the first diagonal block , we obtain a Recursive Bordered Block Diagonal Form ( RBBDF ) matrix in Figure 1(c ) . BBDF and RBBDF structures are generalizations of Block Diagonal Form ( BDF ) structure which has no ‘border’ .
RBBDF structure is intuitionally interpretable in collaborative filtering tasks . Consider movie recommendation as an example . Different users may have different preferences on movie genres , which form different communities , corresponding to the diagonal blocks in the BBDF structure . However , there does exist ‘super users’ whose interests are relatively broad and thus fall into different communities . This type of user is represented by row borders in the BBDF structure . There are also some classical or hot movies widely known and enjoyed by users from different communities , which are ‘super items’ making up column borders . The structure may recurse at multiple finer grained levels in a community , resulting in the generation of RBBDF structures . As different communities may have different rating patterns , it would be better to factorize them independently .
The LMF framework transforms a sparse matrix into RBBDF structure and further extracts denser submatrices to construct a BDF matrix . Factorization of the BDF matrix is used to approximate the original sparse matrix . The framework brings several attractive benefits to recommender systems : 1 ) Factorizing extracted dense submatrices instead of the whole sparse matrix improves the prediction accuracy of matrix factorization algorithms . 2 ) The locality property of LMF makes it possible to refactorize only the recentlyupdated submatrices rather than the whole matrix . 3 ) The framework is suitable for parallelization , which further contributes to the scalability of recommender systems .
In summary , the main contributions of this work are : • The RBBDF structure of rating matrices is investigated , which is intuitionally interpretable in CF tasks . • A density based algorithm is designed to transform a sparse matrix into RBBDF structure .
• The LMF framework is proposed and its rationality is shown through theoretical analyses .
• Through a comprehensive experimental study on four benchmark datasets , both the efficiency and effectiveness of the LMF framework is verified .
The remainder of this paper will be organized as follows : Section 2 reviews some related work , and Section 3 presents some preliminaries . In Section 4 , the LMF framework is introduced and investigated . Experimental results are shown in Section 5 . Some discussions will be made in Section 6 , and the work is concluded in Section 7 .
2 . RELATED WORK
Collaborative Filtering ( CF ) [ 35 ] techniques have been known to have several attractive advantages over other recommendation strategies , such as Content based Filtering [ 22 ] in Personalized Recommender Systems [ 21 ] . Early CF algorithms mainly focus on memory based approaches such as User based [ 24 ] and Item based [ 29 ] methods , which calculate the similarities of users or items to make rating predictions [ 21 ] . To gain better prediction accuracies and to overcome the shortcomings of memory based algorithms , modelbased approaches have been investigated extensively , which estimate or learn a model on user item rating matrices to make rating predictions [ 35 , 21 ] .
Latent Factor Models ( LFM ) based on Matrix Factorization ( MF ) [ 36 ] techniques have been an important research direction in model based CF methods . Recently , MF approaches have gained great popularity as they usually outperform traditional methods [ 35 , 12 ] and have achieved state of the art performance , especially on large scale recommendation tasks [ 17 ] . A variety of MF algorithms have been proposed and investigated in different CF settings , such as Principle Component Analysis ( PCA ) [ 1 ] , Singular Value Decomposition ( SVD ) [ 16 , 17 , 33 ] , Non negative Matrix Factorization ( NMF ) [ 18 , 19 ] , Max Margin Matrix Factorization ( MMMF ) [ 34 , 23 ] , and Probabilistic Matrix Factorization ( PMF ) [ 26 , 25 ] . They aim at learning latent factors from a matrix , with which to make rating predictions .
According to the unified view of MF in [ 32 ] , MF algorithms are optimization problems over given loss functions and regularization terms . Different choices of loss functions and regularization terms lead to different MF methods .
However , MF approaches also suffer from a number of problems in real world recommender systems , such as data
( a ) Original matrix ( b ) BBDF matrix ( c ) RBBDF matrix
Figure 1 : An example of ( R)BBDF structure sparsity , frequent model retraining and system scalability . To overcome the problem of data sparsity , earlier systems rely on imputation to fill in missing ratings and to make the rating matrix dense [ 28 ] . However , imputation can be very expensive as it significantly increases the amount of ratings , and inaccurate imputation may distort the data considerably [ 17 ] . The problem of frequent model retraining and scalability results from the fact that the total number of users and items is usually very large in practical systems , and new ratings are usually made by users continuously .
Most efforts to improve system scalability focus on matrix clustering techniques [ 38 , 31 , 10 , 9 , 37 ] or designing incremental and distributed versions of existing MF algorithms [ 30 , 20 , 11 ] . Usually , they can achieve only approximated results compared with factorizing the whole matrix directly , and many of them restrict themselves to one specific MF algorithm . In contrast with these approaches , we demonstrate that the LMF framework on a BDF matrix is theoretically equal to factorizing the whole matrix directly , and that it is compatible with any existing decomposable MF algorithm . Another related research field is graph partitioning , as permuting a sparse matrix into BBDF structure is equivalent to conducting Graph Partitioning by Vertex Separator ( GPVS ) on its corresponding bipartite graph [ 4 ] . Graph partitioning is known to be NP hard [ 7 ] , but this problem has been investigated extensively , and many efficient and highquality heuristic based methods have been proposed [ 15 ] , such as multilevel methods [ 14 , 6 ] , spectral partitioning [ 3 ] and kernel based methods [ 2 ] . It is verified both theoretically and experimentally that multilevel approaches can give both fast execution time and very high quality partitions [ 27 , 4 , 14 , 6 , 15 ] , which guides us to choosing the multilevel graph partitioning approach in this work .
3 . PRELIMINARIES 3.1 Matrix Factorization
We take the unified view of MF proposed in [ 32 ] , which is sufficient to include most of the existing MF algorithms . Let X ∈ Rm×n be a sparse matrix , and let U ∈ Rm×r , V ∈ Rn×r be its factorization . An MF algorithm P = ( f,DW ,C,R ) can be defined by the following choices : 1 . Prediction link f : Rm×n → Rm×n . 2 . Optional data weights W ∈ Rm×n
, which if used must
+ be an argument of the loss function .
3 . Loss function DW ( X , f ( U V T ) ) ≥ 0 , which is a measure of the error when approximating X with f ( U V T ) .
4 . Hard constraints on factors : ( U , V ) ∈ C . 5 . Regularization penalty : R(U , V ) ≥ 0 . DW ( X , f ( U V T ) ) + R(U , V )
For an MF model X ≈ f ( U V T ) X∗ , we solve :
. argmin ( U,V )∈C
( 1 )
!"""#"""$"""%""""&"""’"""("""")"""*""!+""!! !"""""#""$""%""&""’"("")" !"""#""""$"""%""&&"""’"""("""&""&)"""*"""+ !"""""#""$""%""&"’""(")" !"""#"""$"""%"""&&""’"""("""&"""&)"""*"""+ !"""""#""$""%""&"’""(")" The loss D(·,· ) is typically convex in its second argument , and often decomposes into a ( weighted ) sum over elements of X [ 32 ] . For example , the loss function of WSVD [ 33 ] is :
DW ( X , f ( U V T ) ) = W fi ( X − U V T )2
F ro where fi denotes the element wise product of matrices . In this paper , we refer to X = U V T as an Accurate Matrix Factorization of X , and refer to X ≈ f ( U V T ) X∗ as an Approximate Matrix Factorization of X . 3.2 BDF , BBDF and RBBDF
( 2 )
We consider permuting the rows and/or columns of a sparse matrix to reform its structure . X is a Block Diagonal Form ( BDF ) matrix if :

X =
D1
D2
. . .
Dk
 diag(Di )
It is not always the case that a sparse matrix can be permuted into BDF , but usually it can be permuted into Bordered Block Diagonal Form ( BBDF ) [ 4 ] shown in ( 4 ) . Each Di(1 ≤ i ≤ k ) is a ‘diagonal block’ . Rb [ R1 ··· RkB ] and Cb [ C T k BT ]T are row and column ‘borders’ :
1 ··· C T
X =

D C R B
=

D1
R1
. . .
C1 Dk Ck · · · Rk B
( 3 )
( 4 )
Any of the k diagonal blocks Di in ( 4 ) may be permuted into BBDF structure recursively , resulting in Recursive Bordered Block Diagonal Form ( RBBDF ) . To avoid notational clutter , we present the following example , where I∗ and J∗ denote the row and column index sets :
D1
J1 J2 JB C1 D2 C2 R1 R2 B
 I1I2IB

=
X =
C11 D12 C12 R11 R12 B1
J11 J12 JB1 J2 JB C1 D11 1 C2 1 C3 1 D2 C2 1 R2 B
1 R2
1 R3
R1

I11I12IB1I2IB
In ( 5 ) , D1 is permuted into BBDF recursively . Note that permuting rows and columns related to D1 affects R1 and C1 , but it only changes the order of the non zeros therein . Diagonal blocks D11 , D12 and D2 may be further permuted depending on certain stopping rules . This will be introduced in our algorithm for constructing RBBDF structures . 3.3 Graph Partitioning by Vertex Separator A sparse rating matrix can be equally represented by a bipartite graph . Consider Figure 1(a ) and Figure 2(a ) as examples . Each row or column of the matrix corresponds to an R node or a C node in the bipartite graph .
GPVS partitions a graph into disconnected components by removing a set of vertices ( vertex separator ) and their incident edges . As demonstrated by [ 4 ] , permuting a sparse matrix into BBDF structure is equivalent to conducting GPVS on its bipartite graph . For example , removing nodes R4 , R9 and C7 in Figure 2(b ) corresponds to permuting Row4 , Row9 and Column7 to borders in Figure 1(b ) , and the two resulting disconnected components correspond to two diagonal blocks . GPVS is conducted recursively on the left component , and the RBBDF matrix in Figure 1(c ) is constructed .
( a ) Bipartite graph
( b ) GPVS on the bipartite graph
Figure 2 : The bipartite graph for a sparse matrix and graph partitioning by vertex separator on it . 4 . LMF 4.1 Definitions and Theorems
We present some definitions , propositions and theorems in this section , which will be the basis of the LMF framework . 411 Accurate Matrix Factorization A matrix in BDF or ( R)BBDF structure has some impor tant properties in terms of accurate matrix factorization .
Proposition 1 . For a BDF matrix X = diag(Di ) in ( 3 ) , for each diagonal block Di ; we then i ) as a factorization for X . fi if we have Di = UiV T have X = diag(Ui ) · diag(V T i
This proposition shows the independence of diagonal blocks from each other in a BDF matrix in terms of accurate matrix factorization . As stated above , it is not guaranteed that a sparse matrix can be permuted into BDF structure . However , we have the following proposition for a BBDF matrix .
Proposition 2 . For a BBDF matrix X in ( 4 ) , let :
Di Ci
Ri B
˜Xi
Ui1
Ui2
. V T i1
= UiV T i =
V T i2
( 7 )
( 5 ) be a factorization of ˜Xi ; thus , we have :
Di = Ui1V T i1 Ri = Ui2V T i1 Ci = Ui1V T i2 B = Ui2V T i2 and let :
U =
U21
. . .
 V =
 V11
V21
. . .
Uk1 U12 U22 · · · Uk2
Vk1 V12 V22 · · · Vk2 fi

 U11 
We then have :
U11V T 11
U V T =
U21V T 21
. . .
U12V T
11 U22V T k1 Uk1V T k2
Uk1V T 21 · · · Uk2V T k1
Ui2V T i2
 =
D1
. . .
C1 . . . Dk Ck R1 · · · Rk kB

The only difference between U V T and X in ( 4 ) is that the border cross B in matrix X is multiplied by the number of diagonal blocks k . fi
Proposition 2 is in fact factorizing a block diagonal form ( 1 ≤ i ≤ k ) , as matrix ˜X = diag( ˜Xi ) = diag denoted in ( 7 ) . According to Proposition 1 , if ˜Xi = UiV T i , then we have ˜X = diag(Ui ) · diag(V T i ) . By averaging the
Ri B
U11V T 12 U21V T 22
. . . k i=1
Di Ci
!" !# !$ !% !& !’ !( !) !* +" +# +$ +% +& +’ +( +) +* +", +"" !" !# !$ !% !& !’ !( !) !* +" +# +$ +% +& +’ +( +) +* +", +"" 
X1
X2
X =
 ≈ f,U V T = f


 . V T
1
U1 U2 Uk
. . .
Xk
 = f fi


V T 2
· · · V T k
 
( 6 ) k
· · · U1V T · · · U2V T . . . · · · UkV T
. k k
U1V T U2V T
1 U1V T 2 1 U2V T 2
.
.
UkV T
1 UkV T 2 duplicated submatrices , for example , submatrix B in ( 4 ) , the original matrix X is reconstructed with the factorizations of ˜Xi = UiV T i , where 1 ≤ i ≤ k .
This property can be generalized to an RBBDF matrix . To avoid notational clutter , the example in ( 5 ) is again used here . To transform X into BDF , diagonal block D1 is transformed into BDF first , resulting in an intermediate matrix :
˜Xint . =

J11 JB1 J12 JB1 J2 JB C1 D11 C11 1 C3 R11 B1 1 C2 1 C3 1 D2 C2 1 R2 B
D12 C12 R12 B1
1 R3
1 R3 1
R1
R2
I11IB1I12IB1I2IB
( 8 )
 
This is a BBDF matrix with 3 diagonal blocks . By conducting the same procedure on ˜Xint . , it is transformed into a BDF matrix ( ˜Xij = 0 for i = j ) :

J11 JB1 JB J12 JB1 JB J2 JB D11 C11 C1 1 R11 B1 C3 1 R1 1 B
1 R3
˜X13
˜X12
˜X21
˜X31
D12 C12 C2 1 R12 B1 C3 1 R2 1 B
1 R3 ˜X32
˜X23
D2 C2 R2 B
˜X =
I11IB1IBI12IB1IBI2IB
( 9 ) diag( ˜X1 , ˜X2 , ˜X3 )
Similarly , ˜X1 , ˜X2 and ˜X3 can be factorized independently , and duplicated submatrices are averaged to reconstruct the original matrix X in ( 5 ) .
In fact , ( 9 ) can be derived from ( 5 ) directly without constructing intermediate matrices . Each diagonal block ˜Xi in ˜X corresponds to a diagonal block Di in X . By piecing together Di with the parts of borders on the right side , down side and right bottom side , ˜Xi can be constructed directly . Additionally , permuting any Di into BBDF structure recursively in ( 5 ) would not affect other block diagonals in ˜X . 412 Approximate Matrix Factorization In practical applications , approximate matrix factorization algorithms formalized by ( 1 ) are used . Consider the BDF matrix X = diag(Xi)(1 ≤ i ≤ k ) in terms of the approximate matrix factorization denoted by ( 6 ) . For notational clarity , the superscript ‘tilde’ of ˜X is removed in this section . Decomposable properties will be investigated in different aspects in detail in this section , as they are of core importance with respect to what types of matrix factorization algorithms the framework can handle . In the following definitions and theorems , Xij =
0 ( i=j ) is used to denote submatrices of X in ( 6 ) , and Wij denotes the weight matrix of Xij . f ( U V T )ij denotes the submatrix in f ( U V T ) that approximates Xij , namely , Xij ≈ f ( U V T )ij . Specifically , f ( U V T )i is used for f ( U V T )ii , and Wi is used for Wii when i = j .
Xi ( i=j )
Definition 1 . Decomposable prediction link . A pre diction link f : Rm×n → Rm×n is decomposable if : f ( U V T )ij = f ( UiV T j ) ( 1 ≤ i , j ≤ k )
( 10 )
A large class of MF algorithms use element wise prediction links for each pair of element in Y = f ( X ) , namely , yij = f ( xij ) . For example , the prediction link is f ( x ) = x in SVD , and in NMF , f ( x ) = log(x ) . Element wise link functions lead to the decomposable property above naturally .
Definition 2 . Decomposable loss function . A loss function DW ( X , f ( U V T ) ) is decomposable if :
DW ( X , f ( U V T ) ) =
DWi ( Xi , f ( U V T )i )
( 11 ) k i=1
This property can be viewed in two aspects here . First , a substantial number of MF algorithms restrict D to be expressed as the sum of losses over elements , eg , SVD[17 , 33 ] , NMF[19 ] , MMMF[34 ] and PMF[26 ] . Various decomposable regular Bregman divergences are the most commonly used loss functions that satisfy this property [ 5 ] . The per element effect gives the following property :
DW ( X , f ( U V T ) ) =
DWij ( Xij , f ( U V T )ij )
( 11.1 ) i,j
Second , rating matrices in CF tasks are usually incomplete and very sparse in practical recommender systems . A ‘zero’ means only that the user did not make a rating on the corresponding item , rather than rating it zero . As a result , many MF algorithms optimize loss functions on observed ratings . Specifically , Wij = 0 ( i = j ) in a BDF matrix , and : ( 11.2 )
DWij ( Xij , f ( U V T )ij ) = 0 ( i = j )
( 11.1 ) and ( 11.2 ) gives the decomposable property of loss functions in Definition 2 .
Definition 3 . Decomposable hard constraint . A hard constraint C is decomposable if :
( U , V ) ∈ C iff . ( Ui , Vi ) ∈ C ( 1 ≤ i ≤ k )
( 12 )
Many MF algorithms do not apply hard constraints to target factorizations , but there are MF methods that require ( U , V ) to meet some special requirements .
Some commonly used hard constraints are non negativity ( the elements of U ,V are non negative ) , orthogonality ( the columns of U ,V are orthogonal ) , stochasticity ( each row of U ,V sums to one ) , sparsity ( the row vectors of U ,V meet a desired sparseness constraint ) and cardinality ( the number of non zeros in each row of U ,V satisfies a given constraint ) . In this sense , non negativity , stochasticity , sparsity and cardinality constraints are decomposable hard constraints . For example , each row of ( U , V ) sums to one if and only if the same property holds for any ( Ui , Vi ) ( 1 ≤ i ≤ k ) . However , orthogonality is not decomposable : the orthogonality in ( U , V ) does not ensure the orthogonality in each ( Ui , Vi ) . Our primary focus is on decomposable hard constraints . k
Definition 4 . Decomposable regularization penalty .
A regularization penalty R(U , V ) is decomposable if :
R(U , V ) =
R(Ui , Vi )
( 13 )
The most commonly used regularization penalty is the i=1 p norm regularizer , which is decomposable :
R(U , V ) = λUUp p + λV V p k
,λUUip
= p k
= p + λV Vip p
R(Ui , Vi ) i=1 i=1
The Frobenius norm is p norm where p = 2 . The basic MMMF algorithm takes the trace norm XΣ ( the sum of singular values of X ) [ 34 ] , which is unfortunately not a decomposable regularizer . However , a fast MMMF algorithm F + V 2 based on the equivalence XΣ = min F ) is proposed in [ 23 ] , which also takes p norm regularizers .
2 ( U2
X=U V T
1
Definition 5 . Decomposable matrix factorization . A matrix factoirzation algorithm P = ( f,DW ,C,R ) is decomposable if f,DW ,C,R are decomposable . Namely , properties ( 10)∼(13 ) hold . ( U , V ) = P(X , r ) denotes the factorization of X by P using r factors .
It is necessary to point out that many commonly used MF algorithms are decomposable , including some of the stateof the art techniques , although they are required to satisfy all these four decomposable properties , which seems to be somewhat too strict . Some typical examples are SVD , NMF , PMF , MMMF and their variations , which will be primarily considered and investigated in this work .
Theorem 1 . Suppose X is a BDF matrix in ( 6 ) , and P = ( f,DW ,C,R ) is decomposable . Let ( U , V ) = P(X , r ) and ( Ui , Vi ) = P(Xi , r)(1 ≤ i ≤ k ) . We have : 2 ··· V T i . U = [ U T ii . Xij ≈ f ( UiV T Proof . i . Consider the optimization problem defined in ( 1 ) with decomposable properties of prediction link f , loss function DW , hard constraint C , and regularizer R ; we have : k ]T , V = [ V T j ) ( 1 ≤ i , j ≤ k )
2 ··· U T
1 V T
1 U T k ]T
( U , V ) = P(X , r ) i=1
DW ( X , f ( U V T ) ) + R(U , V )
DWi ( Xi , f ( U V T )i ) + R(Ui , Vi ) k DWi ( Xi , f ( UiV T k i ) ) + R(Ui , Vi ) DWi ( Xi , f ( UiV T i ) ) + R(Ui , Vi ) k(cid:94 )
( Ui , Vi ) i=1
= argmin ( U,V )∈C
= argmin ( U,V )∈C
= argmin ( U,V )∈C k(cid:94 ) k(cid:94 ) i=1 i=1 i=1 argmin ( Ui,Vi)∈C
P(Xi , r )
2 ··· U T
1 U T
=
=
= thus , U = [ U T k ]T and V = [ V T
1 V T
2 ··· V T k ]T . ii . This can be derived directly from the decomposable property of prediction link f in ( 10 ) :
Xij ≈ f ( U V T )ij = f ( UiV T j ) and it holds for any 1 ≤ i , j ≤ k , including zero submatri ces where i = j .
According to Theorem 1 , each diagonal block can be factorized independently , and the results can be used directly to approximate not only the non zero diagonal blocks but also the zero off diagonal blocks . 4.2 LMF for Collaborative Prediction
Consider predicting the missing values of an incomplete sparse rating matrix through the LMF framework . A sparse rating matrix is permuted into an RBBDF matrix ( 5 ) first and further transformed into a BDF matrix ( 9 ) . LMF is then performed on the resulting BDF matrix to make rating predictions for the original matrix . Suppose an RBBDF matrix X is transformed into a BDF matrix ˜X = diag( ˜Xi)(1 ≤ i ≤ k ) . XI∗∼J∗ and ˜XI∗∼J∗ are used to denote the submatrices in X and ˜X correspondingly . For example , R12 = XIB1 ∼J12 in ( 5 ) , and it is duplicated twice by ˜XIB1 ∼J12 in ( 9 ) . The LMF framework approximates the original matrix X through the approximations of ˜X with three steps : i . For a decomposable matrix factorization algorithm P = ( f,DW ,C,R ) , obtain the factorization ( Ui , Vi ) = P( ˜Xi , r ) of each diagonal block ˜Xi . Then : i ) ˜X
˜Xi ≈ f ( UiV T
∗ i ˜X
( 14 )
∗ ii where ˜X∗ i denotes the approximation of ˜Xi . ii . Predict zero blocks ˜Xij(i = j ) in ˜X using factorizations of ˜Xi and ˜Xj :
Now ˜X∗ ( ˜X∗
˜Xij ≈ f ( UiV T ij|1 ≤ i , j ≤ k ) approximates ˜X . j ) ˜X
∗ ij
( 15 ) iii . Average duplicated submatrices in ˜X∗ to approximate the corresponding submatrix in X . Suppose that XI∗∼J∗ is duplicated k times in ˜X , and the tth duplication is in block ˜Xitjt , whose approxima∗(itjt ) tion is ˜X I∗∼J∗ . Then the approximation of XI∗∼J∗ is : k t=1
∗ I∗∼J∗ =
X
1 k
˜X
∗(itjt ) I∗∼J∗
( 16 )
To make it easier to understand , take R12 = XIB1
∼J12 in ∼J12 is duplicated twice in ( 9 ) : one
( 5 ) as an example . XIB1 in ˜X12 and the other in ˜X22 . As a result :
∗ IB1
X
∼J12 = Approximation X∗
( ˜X
∗(12 ) IB1
∼J12
1 2
+ ˜X
∗(22 ) IB1
∼J12
)
I∗∼J∗ is constructed for each submatrix XI∗∼J∗ in X . By piecing them together , approximation X∗ = {X∗ 4.3 Algorithm for RBBDF Permutation
I∗∼J∗} is finally achieved for X .
As shown in Section 3.3 , permuting a matrix into ( R)BBDF structure is equivalent to performing GPVS ( recursively ) on its bipartite graph . In this work , both the performance and efficiency of graph partitioning algorithms are concerned , as the datasets to experiment on are huge1 . As a result , multilevel graph partitioning approach is chosen . Perhaps the
1One of the four datasets used is Yahoo! Music from KDDCUP 2011 , containing approximately 1m users and 0.6m items , which is the largest in present day datasets . most widely known and used package for graph partitioning is Metis [ 13 ] by Karypis , which is based on multilevel approach . The core routine for GPVS in Metis is Node based Bisection , which partitions a graph into two disconnected components by a vertex separator .
A density based algorithm to permute sparse matrices into RBBDF structure is designed , as dense submatrices or subgraphs are usually interpreted as communities , which is widely used in community detection and graph clustering problems [ 8 ] . The density of a matrix X is defined as ρ(X ) = n(X ) s(X ) , where n(X ) is the number of non zeros in X , and s(X ) is the area of X . The average density of k matrices X1X2 ··· Xk is defined as ¯ρ(X1X2 ··· Xk ) = . Note that the density of a matrix is equal to the density of its corresponding bipartite graph [ 8 ] .
For an RBBDF matrix X with k diagonal blocks D1D2 ··· Dk i=1 n(Xi ) i=1 s(Xi ) k k
( eg , the matrix in ( 5 ) has 3 diagonal blocks : D11D12 and D2 , and the original rating matrix is viewed as a single diagonal block ) , ˜X = diag( ˜X1 ˜X2 ··· ˜Xk ) is used to denote its corresponding BDF matrix ( eg , the matrix in ( 9) ) . Algorithm 1 shows the procedure , followed by more detailed explanations and analyses . RBBDF(X , ˆρ , 1 ) is called to start the procedure .
Algorithm 1 RBBDF(X , ˆρ , k ) Require :
User item rating matrix : X Average density requirement : ˆρ Current number of diagonal blocks in X : k
Ensure :
Matrix X be permuted into RBBDF structure BDF matrix ˜X which is constructed from X
1 : ρ ← ¯ρ( ˜X1 ˜X2 ··· ˜Xk ) 2 : if ρ ≥ ˆρ then 3 : 4 : else 5 : return ˜X Density requirement has been reached
[ Ds1 Ds2 ··· Dsk ] ← Sort,[D1D2 ··· Dk] Sort diag onal blocks by size in decreasing order for i ← 1 to k do si D2 si ] ← MetisNodeBisection(Dsi ) Partition [ D1 Dsi into 2 diagonals using core routine of Metis if ¯ρ( ˜Xs1 ··· ˜Xsi−1 ˜Xsi+1 ··· ˜Xsk ) > ρ then
X ← Permute Dsi into [ D1 si D2 RBBDF(X , ˆρ , k + 1 ) Recurse break No need to check the next diagonal si ] in X
˜X 2 si
˜X 1 si end if end for return ˜X No diagonal improves average density
6 : 7 :
8 : 9 : 10 : 11 : 12 : 13 : 14 : 15 : end if
Algorithm 1 requires a ‘density requirement’ ˆρ as input , which is the expected average density of submatrices ˜X1 ··· ˜Xk in the final BDF matrix ˜X . In each recursion , the algorithm checks each diagonal block Di of X in decreasing order of matrix areas . If the average density of extracted submatrices can be improved by partitioning a diagonal block , then the algorithm takes the partitioning and recurses , until ˆρ is reached or none of the diagonal blocks can improve the average density any more .
Note that , to gain a high efficiency , a fundamental heuristic is used in this algorithm , which is the area of diagonal blocks , and we explain its rationality here .
( a ) ˜Xi for Di
( b ) ˜X 1 i and ˜X 2 i for D1 i and D2 i
Figure 3 : Partition a diagonal block , rearrange its corresponding borders , and extract new submatrices . The shaded areas represent non zero blocks . i and D2 i and ˜X 2
Figure 3(a ) shows a diagonal block Di along with its corresponding borders . Note that this figure is , in fact , the submatrix ˜Xi in ˜X corresponding to Di . Two new submatrices ˜X 1 i are constructed when Di is partitioned into D1 i , which are boxed by dotted lines in Figure 3(b ) . One can see that the area boxed by solid lines in Figure 3(b ) constitutes the original submatrix ˜Xi . As a result , transforming ˜Xi to ˜X 1 is essentially removing the two zero blocks and replacing them with some duplicated non zero blocks . t=1 s( ˜Xt ) and n = k t=1 n( ˜Xt ) ; let ∆s1 be the total area of removed zero blocks , ∆s2 be the total area of duplicated non zero blocks , and ∆n be the number of nonzeros in ∆s2 . The increment of average density after partitioning Di is : − ρ =
Let s = k i and ˜X 2 n + ∆n
∆ρ = ρ
( 17 )
= i s − ∆s1 + ∆s2
− n s s∆n + n∆s s(s − ∆s ) where ρ and ρ are the average densities of diagonal blocks in ˜X before and after partitioning Di , and ∆s ∆s1 − ∆s2 .
Because s − ∆s > 0 , we have the following : ∆ρ > 0 ↔ s∆n + n∆s = s∆n + n(∆s1 − ∆s2 ) > 0 If ∆s > 0 , then ( 18 ) holds naturally . Otherwise , the fol
( 18 ) lowing is required : n s
<
∆n
∆s2 − ∆s1
( 19 )
Although not guaranteed , ( 19 ) can usually be satisfied as the following property usually holds : n s
<
∆n ∆s2
<
∆n
∆s2 − ∆s1
( 20 )
Intuitionally , ( 20 ) means that the density of duplicated non zero blocks after partitioning is usually greater than the original average density of k submatrices ˜X1 ˜X2 ··· ˜Xk , as the latter contains many zero blocks . Additionally , a large ∆s tends to yield a large density increment ∆ρ according to ( 17 ) , which leads to adopting areas of diagonal blocks as heuristics . It will be verified experimentally that this heuristic improves the average density at the first attempt nearly all the time .
The time complexity of Node based bisection in Metis is O(n ) , where n is the number of non zeros in a matrix [ 14 ] . Suppose that matrix X is permuted into an RBBDF structure which has k diagonal blocks ( k n ) in the end , and the algorithm chooses the largest diagonal block for partitioning in each recursion ; then , the height of the recursion tree will be O(lg k ) and the total computational cost in each level of the tree is O(n ) . As a result , the time complexity of Algorithm 1 is O(n lg k ) .
Di Di D 2 i 1 !"# $!"% Di Di !&’ 1 2 5 . EXPERIMENTS 5.1 Datasets Description
A series of experiments were conducted on four real world datasets : MovieLens 100K , MovieLens 1M , DianPing and Yahoo!Music . The MovieLens dataset is from GroupLens Research . We also collected a year ’s data from a famous restaurant rating web site DianPing 2 from Jan . 1st to Dec . 31st 2011 and selected those users with 20 or more ratings . The ratings also range from 0 to 5 . The Yahoo!Music dataset is from KDD Cup 2011 , and its ratings range from 0 to 100 . Statistics of these datasets are presented in Table 1 .
Table 1 : Statistics of four datasets density of ˜X1 ˜X2 ··· ˜Xk is ¯ρk , and the average density goes to ¯ρk+1 after partitioning a diagonal block . Then , the relative density increment is ∆ρ/ρ1 = ( ¯ρk+1 − ¯ρk)/¯ρ1 , where the constant ¯ρ1 is the density of the whole original matrix . Experimental results show that the relative density increment becomes lower and lower as the number of diagonal blocks increases . As a result , it is relatively easy to improve the average density at the beginning , as partitioning a large diagonal block Di gains a large density increment ∆ρ . However , this process tends to be more and more difficult when diagonal blocks become small . The experimental result is in accordance with the analysis in ( 17)∼(20 ) . These results partially verify the heuristic used in Algorithm 1 .
943 1,682
6,040 3,952 100,000 1,000,209 165.598 106.045 253.089 59.453 0.0630 0.0419
11,857 22,365 510,551 43.059 22.828 0.00193
ML 100K ML 1M DianPing Yahoo!Music 1,000,990 624,961 256,804,235 256.550 410.912 0.000411
#users #items #ratings #ratings/user #ratings/item average density These datasets are chosen as they have different sizes and densities . Besides , two of them have more users than items , and the others are the opposite . We expect to verify how our framework works on datasets of different sizes and densities . 5.2 Algorithms and Evaluation Metrics
Four popular and state of the art matrix factorization al gorithms are the subject of experimentation in this work :
SVD : The Alternating Least Squares ( ALS ) algorithm in
[ 17 ] is used for SVD learning .
NMF : The NMF algorithm based on divergence cost in [ 19 ] is used . We also use F norm regularizer , similar to SVD . PMF : The Bayesian PMF by Markov Chain Monte Carlo method in [ 25 ] is used .
MMMF : The fast version of MMMF in [ 23 ] is used . For easier comparison with previous proposed methods in the literature , we use Root Mean Square Error ( RMSE ) to measure the prediction accuracy in this work .
For N rating prediction pairs ri , ˆri :
N
RMSE = i=1(ri − ˆri)2
N
Five fold cross validation is conducted to calculate the average RMSE for MovieLens and DianPing . The Yahoo! Music dataset itself is partitioned into training and validation sets , which are used for training and evaluation , respectively . 5.3 Analyses of RBBDF Algorithm 531 Number of diagonal blocks The only parameter to be tuned in the RBBDF algorithm is the average density requirement ˆρ . Intuitionally , a low density requirement gives less and larger diagonal blocks in ˜X , and vice versa . The relationship between the number of diagonal blocks k and the density requirement ˆρ on four datasets is shown by red solid lines in Figure 4 .
We see that the number of diagonal blocks increases faster and faster with an increasing density requirement . To investigate the underlying reason , a more straightforward view is given by the relative density increment in Figure 5 . Suppose that the current number of diagonal blocks is k ; the average
2http://wwwdianpingcom
( a ) ML 100K & ML 1M ( b ) DianPing & Yahoo!Music
Figure 5 : Relationship between the Relative Density Increment ∆ρ/ρ1 and the Current number of diagonal blocks k on four datasets . 532 Verification of heuristic The First Choice Hit Rate ( FCHR ) is used to verify the heuristic used in the RBBDF algorithm , as we expect the average density to be improved by partitioning the first diagonal block Ds1 in the sorted list [ Ds1 Ds2 ··· Dsk ] , in which case there is no need to check the remaining diagonal blocks .
FCHR =
# recursions where Ds1 is chosen
# recursions in total
One can see that FCHR = 1 means that average density can always be improved by partitioning the largest diagonal block directly . The relationships between FCHR and density requirement on four datasets are shown by blue dotted lines in Figure 4 . On all of the four datasets , FCHR = 1 at the beginning and begins to drop when density requirement is high enough , which is also in accordance with the analysis in Section 43 As a result , by taking the areas of diagonal blocks as a heuristic , only one diagonal block is partitioned in each recursion , and there is no redundant computation when an appropriate density requirement is given .
When the density requirement is high , we have FCHR < 1 , and redundant computation will be introduced : we might partition a diagonal block without improving the average density . However , we would like to note that it does not matter very much in practice . First , when considering the O(n ) complexity of the Node based Bisection , it will be faster and faster to partition diagonal blocks as they become smaller . Second , there is no need to split a matrix into hundreds or even thousands of diagonal blocks in practice . According to our experiments in the following sections , it is sufficient to gain both high prediction accuracy and computational efficiency by partitioning a matrix into a small number of diagonal blocks , in which case we have FCHR = 1 . 533 Computational Time of RBBDF Algorithm Experiments were conducted on an 8 core 3.1GHz linux server with 64G RAM . We tuned the density requirement ˆρ
248163264128256512102400050101502# Diagonal BlocksRelative Density IncrementML−100KML−1M2481632641282565121024002040608112# Diagonal BlocksRelative Density IncrementDianPingYahoo! Music ( a ) ML 100K
( b ) ML 1M
( c ) DianPing
( d ) Yahoo! Music
Figure 4 : Number of Diagonal Blocks ( #DB , solid lines ) and First Choice Hit Rate ( FCHR , dotted lines ) under different density requirements ˆρ . The tuning steps of ˆρ are 0.01 , 0.01 , 0.0008 and 0.0004 , respectively . to achieve the expected number of diagonal blocks k . The run time of the RBBDF algorithm is shown in Table 2 .
In the experiments , we see that the run time increases along with the number of diagonal blocks , and it takes less time to partition a submatrix as they become smaller . Moreover , the time used by the RBBDF algorithm is much less than that used for training an MF model on matrix X . We will show the results on model training time in Section 55
Table 2 : Computational time of the RBBDF algorithm with different numbers of diagonal blocks k . k ML 100K / ms ML 1M / s DianPing / s Yahoo! / min
200 100 10 5 340 493 160 180 4.45 5.61 9.51 10.25 10.74 6.01 9.69 11.61 12.84 14.64 15.06 16.18 16.95 8.03 9.54 10.95 12.08 17.67 21.83 23.35 24.73
50 224 8.31
20 208 6.76
15 196 6.25
150 422
5.4 Prediction Accuracy 541 Number of latent factors The number of latent factors r plays an important part in MF algorithms . It would be insufficient for approximating a matrix if r is too low , and would be computationally expensive if r is too high . As the diagonal blocks in ˜X and the original matrix X are of different sizes , it ’s important to investigate how to choose a proper r in practical applications . We use MovieLens 1M to test the impact of r in the LMF framework . The density requirement is ˆρ = 0.055 , and matrix X is permuted into an RBBDF structure with 4 diagonal blocks ; then , ˜X = diag( ˜X1 , ˜X2 , ˜X3 , ˜X4 ) is constructed . Some statistical information about ˜X is shown in Table 3 .
Table 3 : Statistics of the four diagonal blocks
˜X1 1,507 2,491
˜X4 1,150 #users #items 3,304 #ratings 118,479 259,665 462,586 192,267 density 0.0506
˜X3 1,743 3,616
˜X2 1,683 3,108
0.0734
0.0316
0.0496
We tuned r from 5 to 100 , with a tuning step of 5 . It ’s necessary to note that r is required to be comparable with min(m , n ) in MMMF , where m and n are the numbers of users and items in X . However , it would be time consuming to train a model using thousands or even millions of factors . Fortunately , according to [ 23 ] , it ’s sufficient to use much smaller values of r to achieve satisfactory performance in practice ( r = 100 is used in [ 23 ] for ML 1M ) . As a result , the tuning range of 5 ∼ 100 is also used for MMMF .
For each of the four MF algorithms , two sets of experiments were conducted . First , we approximate the original matrix X using r factors directly , and record the RMSE . Second , predictions are made by the LMF framework in Section
4.2 using the four diagonal blocks in ˜X , each with r factors . Cross validation is performed on X to find the best regularization parameters for each MF method . In SVD and NMF , λ is set to 0.065 ; in PMF , λU and λV are both 0.002 ; and in MMMF , the regularization constant C is set to 15 RMSE vs the number of latent factors r is shown in Figure 7 .
Experimental results show that better performance in terms of RMSE can be achieved in the LMF framework . Furthermore , the improvement tends to be more obvious when the number of latent factors r is relatively small . This result could arise because , in such cases , r is not sufficient to approximate the original matrix X , while it is sufficient to approximate relatively small submatrices in ˜X . We view this as an advantage of the LMF framework , as better performance can be achieved with fewer factors , which benefits the model complexity and training time . 542 Different density requirements The final number of diagonal blocks k in ˜X is different under different density requirements ˆρ . We experimented RMSE with different density requirements . The number of latent factors r is set to 60 , as we find it sufficient to smooth the performance improvement on the datasets . The regularization coefficients are the same : λ = 0.065 for SVD and NMF , λU = λV = 0.002 for PMF , and C = 1.5 for MMMF . The RMSE versus different choices of ˆρ on all of the four datasets are plotted in Figure 6 . In each subfigure , the four curves correspond to the four MF methods used , which are SVD , NMF , PMF and MMMF . The density requirement on the first point of each curve is the average density of the corresponding dataset ; as a result , RMSE on this point is the baseline performance for the matrix factorization algorithm . Thus , points below the beginning point of a curve indicate an improvement on prediction accuracy , and vice versa .
Experimental results show that our LMF framework helps decomposable MF algorithms to gain better prediction accuracies if appropriate density requirements are given , but might bring negative effects if ˆρ is not appropriately set .
( a ) SVD & NMF
( b ) PMF & MMMF
Figure 7 : RMSE vs different numbers of latent factors . Solid/dotted lines are results of approximating X directly/through the LMF framework .
0060080101201401601801002003004005006007008009001000123581424501221862513145891008Density RequirementNumber of Diagonal Blocks0060080101201401601800204060811111111104550297022017600940055First Choice Hit RateMovieLens−100K#DBFCHR0040060080101201401601802022010002000300040005000139182940567694129159200253342460664109620955346Density RequirementNumber of Diagonal Blocks004006008010120140160180202200204060811111111111111111106570257First Choice Hit RateMovieLens−1M#DBFCHR23456789101112x 10−3010002000300040005000600070001223482467124210270351479648977234846106908Density RequirementNumber of Diagonal Blocks23456789101112x 10−300204060811111111111093307370540399026401100560037First Choice Hit RateDianPing#DBFCHR0412228364452668768492x 10−3050100150200250300350123444689111316212636506989112146189259338Density RequirementNumber of Diagonal Blocks0412228364452668768492x 10−3002040608111111111111111111110923075606070502First Choice Hit RateYahoo! Music#DBFCHR0204060801000835084084508508550860865087#FactorsRMSESVDSVD−LMFNMFNMF−LMF020406080100086508708750880885089089509#FactorsRMSEPMFPMF−LMFMMMFMMMF−LMF ( a ) MovieLens 100K
( b ) MovieLens 1M
( c ) DianPing
( d ) Yahoo! Music
Figure 6 : RMSE on four datasets using the LMF framework under different density requirements . Dotted lines in each subfigure represent baseline performance of the corresponding matrix factorization algorithm .
Here , by ‘appropriate’ , we mean that ˆρ is not too high . According to the experimental results on four datasets , better prediction accuracies are achieved along with an increasing ˆρ ( and also the number of diagonal blocks k ) at the beginning , but the performance tends to drop when ˆρ is set too high . In our view , this is not surprising because many small scattered diagonal blocks are extracted when a high density requirement is set , which would bring negative effects to MF algorithms . Table 4 presents the average number of users and items in the diagonal blocks of ˜X under different ˆρ on MovieLens 1M . We see that the average number of users goes to only a hundred or less when ˆρ ≥ 01
Table 4 : Average number of users and items in diagonal blocks of ˜X under different ˆρ on MovieLens 1M . 0.045 0.052 0.060 0.069 0.081 0.102 0.129 0.160 ˆρ 256 k 61 Avg #users Avg #items 3030
8 779 3055
64 128 3007
4 1520 3129
128 82 3015
2 3020 3170
16 409 3064
32 220 3015
However , better performance is achieved given appropriate density requirements . By combining this observation with the experimental results in Section 5.3 , it is neither reasonable nor necessary to use high density requirements that result in hundreds or even thousands of diagonal blocks .
Table 5 shows the best RMSE achieved on each dataset for each MF method . To calculate the average RMSE on each dataset , five fold cross validation was conducted on MovieLens and DianPing , and experiments were conducted five times on Yahoo! Music . The standard deviations were ≤ 0.002 on MovieLens and DianPing , and were ≤ 0.01 on Yahoo! Music . We see that , in the best cases , MF algorithms benefit from the LMF framework in terms of RMSE on all of the four datasets . Specifically , the sparser a matrix is , the higher RMSE increment LMF tends to gain . 5.5 Speedup by Parallelization
An important advantage of LMF is that , once the BDF matrix ˜X = diag( ˜Xi ) is constructed , diagonal blocks ˜Xi can be trained in parallel . According to the decomposable property in Theorem 1 , sub problems of learning different ( Ui , Vi ) are not coupled ; as a result , there is no need to implement rather complicated parallel computing algorithms . In fact , simple multi threading technique is adequate for the task , which contributes to the scalability of recommender systems while , at the same time , keeps system simplicity .
The experiment comprises three stages . In the first stage , X is permuted into an RBBDF structure , and a BDF matrix ˜X = diag( ˜Xi)(1 ≤ i ≤ k ) is constructed . As we have 8 cores , the density requirement is tuned on each dataset to construct ˜X with 8 diagonal blocks . In the second stage , each diagonal block is factorized independently with a thread , and ( Ui , Vi ) is achieved . In the last stage , ( Ui , Vi ) from all of the diagonal blocks are used to approximate the original matrix X by LMF . The computational time consumed in each stage is recorded ( in the second stage , the time recorded is the longest among all of the diagonal blocks ) . Finally , the total time of the three stages is adopted to evaluate the overall efficiency . The number of factors and the regularization coefficients are the same as those in Section 542 The results are shown in Table 6 , where ‘Base’ represents the computational time of factorizing X directly , ‘LMF’ is the time used by the LMF framework , and ‘Speedup’ is ‘Base/LMF’ .
Table 6 : Computational time and speedup by multithreading with 8 diagonal blocks .
MovieLens 100K
MovieLens 1M
Method
Base 23.9s 8.7s 43.8s
SVD NMF PMF MMMF 19.6min 4.71min
LMF 7.7s 3.9s 11.6s
Speedup Base 184.9s 86.6s 265.1s 1.73h 21.5min
LMF 43.4s 22.1s 60.1s
3.10 2.23 3.78 4.16
Speedup
4.26 3.92 4.41 4.83
DianPing
Yahoo!Music
Method
Base 143.7s 64.4s 190.5s
SVD NMF PMF MMMF 48.5min 10.2min
LMF 35.7 16.6s 44.1s
Speedup Base 6.22h 4.87h 7.91h 38.8h
4.03 3.88 4.32 4.75
LMF 1.21h 1.05h 1.48h 6.22h
Speedup
5.14 4.64 5.34 6.24
Experimental results show that the LMF framework helps to save a substantial amount of model training time through very simple multithreading parallelization techniques . This is especially helpful when learning large magnitude datasets , which is important in real world recommender systems .
6 . DISCUSSIONS
Unlike benchmark datasets , rating matrices in real world recommender systems usually change dynamically as new ratings are made by users continuously . A typical way to settle this problem in practice is to retrain MF models periodically or when a predefined prediction accuracy threshold ( say RMSE ) is reached . However , it would be time consuming to refactorize large rating matrices as a whole and to do so frequently . In the LMF framework , however , it is possible to only refactorize those submatrices whose prediction accuracies have reached a predefined threshold , rather than refactorize the whole matrix , which further benefits system scalability . This potential advantage that LMF might bring about will be investigated both by simulation and in practical real world recommender systems in future work .
7 . CONCLUSIONS
In this paper , we explored the BDF , BBDF and RBBDF structures of sparse matrices and their properties in terms of matrix factorization . The LMF framework is proposed , and to explicitly indicate the scope of matrix factorizations
00600801012014088090920940960981Density RequirementRMSEMovieLens−100KSVDNMFPMFMMMF0040060080101208208308408508608708808909Density RequirementRMSEMovieLens−1MSVDNMFPMFMMMF2345678x 10−309092094096098Density RequirementRMSEDianPingSVDNMFPMFMMMF02468x 10−3215222252323524Density RequirementRMSEYahoo!MusicSVDNMFPMFMMMF Table 5 : Best performance achieved in LMF with corresponding density requirement ˆρ and number of diagonal blocks k . Bold numbers indicate improvements that are ≥ 0.01 on MovieLens and DianPing or ≥ 0.2 on Yahoo!Music . The standard deviations are ≤ 0.002 on MovieLens and DianPing and ≤ 0.01 on Yahoo!Music .
Method
SVD NMF PMF
MMMF
MovieLens 100K
MovieLens 1M
DianPing
Yahoo!Music
ˆρ k RMSE baseline baseline 0.9249 0.08 3 0.9165 0.8487 0.05 3 0.8423 0.9138 0.08 3 0.9102 0.8461 0.05 3 0.8388 0.9598 0.08 3 0.9534 0.8741 0.05 3 0.8664 0.9807 0.08 3 0.9703 0.8810 0.06 9 0.8740
ˆρ k RMSE baseline
ˆρ k RMSE baseline k RMSE 0.9244 0.0036 3 0.9145 22.713 0.0044 13 22.519 0.9376 0.0044 4 0.9267 23.538 0.0052 21 23.335 6 0.9636 0.0044 4 0.9575 22.121 22.312 0.0028 9 23.007 0.9457 0.0036 3 0.9352 23.218 0.0036
ˆρ that the framework can handle , decomposable properties of matrix factorization algorithms were investigated in detail . Based on graph partitioning theories , we designed a densitybased algorithm to permute sparse matrices into RBBDF structures , and studied its algorithmic properties both formally and experimentally . Experimental results show that LMF helps the matrix factorization algorithms we studied to gain better performance and , at the same time , contributes to system scalability by simple parallelization techniques . 8 . ACKNOWLEDGEMENT
The authors thank Jun Zhu for the fruitful discussions and the reviewers for their constructive suggestions . This work was supported by Natural Science Foundation ( 60903107 , 61073071 ) and National High Technology Research and Development ( 863 ) Program ( 2011AA01A205 ) of China . 9 . REFERENCES [ 1 ] H . Abdi and L . J . Williams . Principal component analysis . WIREs Comp Stat , 2:433–459 , 2010 .
[ 2 ] C . Alzate and J . A . Suykens . Multiway spectral clustering with out of sample extensions through weighted kernel pca . PAMI , 32:335–347 , 2010 .
[ 3 ] B . Arsic et al . Graph spectral techniques in computer sciences . Appl . Anal . Discrete Math , 6:1–30 , 2012 .
[ 4 ] C . Aykanat et al . Permuting sparse rectangular matrices into block diagonal form . SISC , 2004 .
[ 5 ] A . Banerjee , S . Merugu , I . S . Dhillon , and J . Ghosh .
Clustering with bregman divergences . JMLR , 2005 .
[ 6 ] E . Boman and M . Wolf . A nested dissection approach to sparse matrix partitioning for parallel computations . AMM , 2007 .
[ 7 ] T . N . Bui and C . Jones . Finding good approximate vertex and edge partitions is np hard . IPL , 1992 .
[ 8 ] J . Chen and Y . Saad . Dense subgraph extraction with application to community detection . TKDE , 2012 .
[ 9 ] I . S . Dhilon et al . Concept decompositions for large sparse text data using clustering . JMLR , 2001 .
[ 10 ] E . Gallopoulos and D . Zeimpekis . Clsi : A flexible approximation scheme from clustered term document matrices . SDM , 2005 .
[ 11 ] R . Gemulla et al . Large scale matrix factorization with distributed stochastic gradient descent . KDD , 2011 .
[ 12 ] J . Herlocker et al . An algorithmic framework for performing collaborative filtering . SIGIR , 1999 .
[ 13 ] G . Karypis . Metis a software package for partitioning unstructured graphs , meshes , and computing fill reducing orderings of sparse matrices v50 2011 .
[ 14 ] G . Karypis et al . A fast and high quality multilevel scheme for partitioning irregular graphs . SISC , 1999 . [ 15 ] J . Kim , I . Hwang , Y . Kim , et al . Genetic approaches for graph partitioning : A survey . CECCO , 2011 .
[ 16 ] Y . Koren . Factorization meets the neighborhood : a multifaceted collaborative filtering model . KDD , 2008 .
[ 17 ] Y . Koren , R . Bell , et al . Matrix factorization techniques for recommender systems . Computer , 2009 .
[ 18 ] D . Lee and H . Seung . Learning the parts of objects with nonnegative matrix factorization . Nature , 1999 .
[ 19 ] D . Lee and H . Seung . Algorithms for non negative matrix factorization . NIPS , 2001 .
[ 20 ] C . Liu , H . Yang , J . Fan , L . He , et al . Distributed nonnegative matrix factorization for web scale dyadic data analysis on mapreduce . WWW , 2010 .
[ 21 ] J . Liu , M . Chen , J . Chen , et al . Recent advances in personal recommender systems . JISS , 2009 . [ 22 ] M . J . Pazzani and D . Billsus . Content based recommendation systems . Adaptive Web LNCS , 2007 .
[ 23 ] J . Rennie et al . Fast maximum margin matrix factorization for collaborative prediction . ICML , 2005 . [ 24 ] P . Resnick et al . Grouplens : An open architecture for collaborative filtering of netnews . CSCW , 1994 .
[ 25 ] R . Salakhutdinov and A . Mnih . Bayesian probabilistic matrix factorization using markov chain monte carlo . ICML , 2008 .
[ 26 ] R . Salakhutdinov and A . Mnih . Probabilistic matrix factorization . NIPS , 2008 .
[ 27 ] P . Sanders and C . Schulz . Engineering multilevel graph partitioning algorithms . ESA , 2011 .
[ 28 ] B . Sarwar et al . Application of dimension reduction in recommender systems a case study . WebKDD , 2000 . [ 29 ] B . Sarwar , G . Karypis , et al . Item based collaborative filtering recommendation algorithms . WWW , 2001 .
[ 30 ] B . Sarwar , G . Karypis , et al . Incremental singular value decomposition algorithms for highly scalable recommender systems . ICCIT , 2002 .
[ 31 ] B . Savas et al . Clustered low rank approximation of graphs in information science applications . SDM , 2011 . [ 32 ] A . P . Singh and G . J . Gordon . Relational learning via collective matrix factorization . KDD , 2008 .
[ 33 ] N . Srebro and T . Jaakkola . Weighted low rank approximations . ICML , 2003 .
[ 34 ] N . Srebro , J . Rennie , and T . S . Jaakkola .
Maximum margin matrix factorization . NIPS , 2005 .
[ 35 ] X . Su and T . Khoshgoftaar . A survey of collaborative filtering techniques . Advances in AI . , 2009 .
[ 36 ] G . Takacs , I . Pilaszy , B . Nemeth , and D . Tikk .
Investigation of various matrix factorization methods for large recommender systems . ICDM , 2008 . [ 37 ] B . Xu , J . Bu , and C . Chen . An exploration of improving collaborative recommender systems via user item subgroups . WWW , 2012 .
[ 38 ] G . Xue , C . Lin , Q . Yang , et al . Scalable collaborative filtering using cluster based smoothing . SIGIR , 2005 .
