DIGTOBI : A Recommendation System for Digg Articles using Probabilistic Modeling
Younghoon Kim
Yoonjae Park
Kyuseok Shim †
Seoul National University
Seoul National University
Seoul National University
Seoul , Korea yhkim@kddsnuackr
Seoul , Korea yjpark@kddsnuackr
Seoul , Korea shim@eesnuackr
ABSTRACT Digg is a social news website that lets people submit articles to share their favorite web pages ( eg blog postings or news articles ) and vote the articles posted by others . Digg service currently lists the articles in the front page by popularity without considering each user ’s preference to the topics in the articles . Helping users to find the most interesting Digg articles tailored to each user ’s own interests will be very useful , but it is not an easy task to classify the articles according to their topics in order to recommend the articles differently to each user .
In this paper , we propose DIGTOBI , a personalized recommendation system for Digg articles using a novel probabilistic modeling . Our model considers the relevant articles with low Digg scores important as well . We show that our model can handle both warm start and cold start scenarios seamlessly through a single model . We next propose an EM algorithm to learn the parameters of our probabilistic model . Our performance study with Digg data confirms the effectiveness of DIGTOBI compared to the traditional recommendations algorithms .
Categories and Subject Descriptors H28 [ Database Applications ] : Data mining ; H35 [ Online Information Service ] : Web based services
General Terms Algorithm
Keywords Digg article recommendation ; Topic modeling ; Collaborative filtering ; Expectation Maximization ; Probabilistic latent semantic indexing
1 .
INTRODUCTION
Digg [ 10 ] is a social news website that allows people to submit articles for sharing their favorite web pages ( eg blogs or news articles ) and to vote the articles posted by others . When a Digg user finds an interesting web page with which he wants to share , he can submit an article to Digg so that
† Corresponding author .
Copyright is held by the International World Wide Web Conference Committee ( IW3C2 ) . IW3C2 reserves the right to provide a hyperlink to the author ’s site if the Material is used in electronic media . WWW 2013 , May 13–17 , 2013 , Rio de Janeiro , Brazil . ACM 978 1 4503 2035 1/13/05 .
Digg score
Submission title & link to the web page
291
The physics behind your favorite science fiction theme thunes io9.com When you think about science fiction them tunes , chances are there are a few that are especially stiming and heroic . Star Wars , 2001 : A Space Ody 7 hr 55 min ago via mikiopez share save
Digging button
Burying button
User id
Brief description
Figure 1 : Components in a Digg submission other users can read his article and vote either thumbs up ( also called digging ) or thumbs down ( also called burying ) for the article . Each article in Digg consists of a user id , its submission title , a brief description and the link to the interesting web page as shown in Figure 1 .
Digg service displays not only the submitted articles with a lot of “ diggings ” but also new submissions with their Digg scores where the Digg score of a submission is defined as the number of diggings subtracted by the number of buryings . Digg service currently lists the Digg articles in its front page without considering each user ’s topic preference . Helping users to find the most interesting Digg articles tailored to each user ’s own interests is very useful , but it is not an easy task to classify the submissions according to their topics so that the submitted articles are recommended differently to each user based on his own topic preference . Another challenge in Digg service is the problem of cold start recommendations [ 21 , 25 ] which occurs when the voting histories and submitted articles of users are not sufficient .
Other popular websites such as Facebook [ 11 ] and CiteULike [ 6 ] not only allow users to vote for the messages and the papers posted by other users respectively , but also post the voting scores which can be utilized to infer the topic preferences of users . Thus , our work in this paper can also be used to enhance the quality of recommendation systems for Facebook and CiteULike as well .
In our paper , we propose DIGTOBI which is a personalized recommendation system for DIGg arTicles using prObaBIlistic modeling . Our probabilistic model is a generalization of the probabilistic latent semantic indexing ( PLSI ) in [ 12 ] which assumes that the description in each submission has its own topic relevance model and each word in the description is selected by following the word distributions of its related topics . Since recommendation algorithms can be naturally derived in a principled manner by utilizing probabilistic modeling , it is another reason why we decide to apply the probabilistic modeling approach to the Digg article recommendation .
We develop a generative model from a unifying viewpoint such that the description in each submission is produced by
691 a topic mixture model and each user votes thumbs up for each submission based on his topic preference . Since Digg service provides us only the dug articles and does not provide the buried articles by users , we could not utilize the votes of thumbs down in our model . If we blindly apply the PLSI model to Digg , we cannot fully utilize each user ’s digging history and submitted articles together . Thus , we introduce not only a topic mixture model of generating the descriptions in the submitted articles , as the PLSI model does , but also another topic model of producing the histories of digging and writing by each user , and apply both topic models into our probabilistic model . In our probabilistic model , we assume that every user writes the descriptions in his submissions according to his topic preference . Furthermore , we assume that when a user votes for submissions , the submissions with the similar topic relevances to his own topic preference have high chances of being voted for .
The traditional recommendation algorithms tend to offer users the articles with high scores which are probably preferred by most of users . However , users are interested in the relevant articles with low scores to their topic preferences as well . Thus , our model also considers the relevant articles with low Digg scores to be important . It is possible that the articles with high Digg scores may be dug by many users simply because they have more chances to be displayed in Digg service , and the others have no chance of being voted for at all . In our model , when an article with a small Digg score is dug by a user , we assume that the article has more similar topic relevance to the user ’s topic preference . This assumption enables our model to capture the topic preference of each user precisely and discover the interesting articles to each different user , even if the articles have low Digg scores .
The contributions of this paper are as follows :
• We develop the personalized recommendation system called DIGTOBI which utilizes our novel probabilistic generative model . The model is suitable for representing the activities in Digg service as well as the other websites which allow users to vote the contents posted by other users .
• We improve the quality of recommendations by reducing the bias of the collaborative filtering which mainly recommends the popular articles with high Digg scores .
• We present the EM algorithm which estimates the model parameters maximizing the likelihood of our probabilistic model used by DIGTOBI .
• We also introduce the recommendation algorithms for both warm start and cold start naturally derived from our probabilistic model of DIGTOBI in a principled manner .
• Our performance study with Digg data confirms the effectiveness of DIGTOBI compared to the traditional recommendation algorithms .
The rest of this paper is organized as follows . After discussing related work in Section 2 , we provide the definitions to define our problem and present the problem formulation in Section 3 . We next propose a generative probabilistic model and its EM algorithm in Section 4 . Then , we present our recommendation algorithms in Section 5 . Finally , the performance study is provided in Section 6 and we summarize our paper in Section 7 .
2 . RELATED WORK
Model based algorithms build their models based on the behaviors of users and utilize the models to predict the users’ preferences on unseen items . In [ 22 ] , it was shown that recommendations using probabilistic modeling outperform other approaches in terms of precision . The probabilistic matrix factorization technique in [ 23 ] was also developed for movie recommendations in Netflix [ 20 ] to predict user ratings on movies . However , these algorithms utilize only the history of ratings or votings for the items generated by users and do not consider additional hints such as the textual content of the items for recommendations .
More complex probabilistic models were later proposed in various recommendation applications . In [ 13 , 17 , 26 ] , recommendations utilizing the PLSI in [ 12 ] were investigated . The LDA model , introduced in [ 4 ] , is generalized to model both latent topics and hidden communities between users in [ 1 , 2 ] . However , these algorithms make recommendations by utilizing either the user ’s past ratings on items or the textual content of items , but not both .
In [ 28 ] , a system called CTR was proposed to recommend scientific papers to each user in CiteULike [ 6 ] . CiteULike is a specialized search engine for searching the technical papers and allows people to share their favorite papers by voting thumbs up for the papers . To make recommendations by utilizing both of user ’s votes and the content of papers , they combine the matrix factorization [ 23 ] and the LDA model [ 4 ] together , and show that CTR outperforms the recommendations based on either matrix factorization or the LDA model only [ 28 ] .
While both CiteULike and Digg service allow the users to post their votes for each document , CiteULike does not allow the users to post the descriptions of why they like . Digg service not only allows the users to submit the descriptions of web documents , but also posts the Digg scores . Thus , our probabilistic model is more general in that we consider both Digg scores and user descriptions , while CTR in [ 28 ] does not considered both of them . If we want to use the CTR model for recommending Digg articles , we can extend it by ignoring the Digg scores and the descriptions posted by users .
In [ 5 ] , recommending URLs in Twitter messages was studied by exploiting the social network of users and the popularity of the URLs in Twitter . They simply represent Twitter messages of a user as a bag of word for content based recommendations without considering any sophisticate user modeling , while our work utilizes probabilistic modeling .
To recommend the articles with hot topics in Digg service , HotDigg which is a probabilistic model based recommendation system was proposed recently in [ 14 ] . However , HotDigg aimed to recommend the articles on recent hot topics to general users using the Digg scores and the descriptions of submissions only , while we aims to provide personalized recommendations in this paper .
3 . PRELIMINARIES
We first provide the definitions used for defining our recommendation problem and next present the problem formulation .
3.1 Problem Formulation
Let D = {d1 , , dn} be a collection of the Digg article ids submitted to Digg service . Let U = {u1 , , um} be a set of
692 Sherlock , Holmes , 221B , Baker iPhone , Apple , Samsung iPhone , AppStore , Samsung
Samsung , iPhone , AppStore , Baker
Sherlock , 221B , Apple , Baker
D(u )
W ( di ) d1 d2 d3 d4 d5 d6 d7 ( a ) Warm start data set d3,d4 d1,d2,d3
Apple , iPhone , Sherlock Sherlock , Holmes , iPhone si 3
246 324
2 3 2 2
User u1
L(u ) d5 u2 u3 u4
User u5 u6
L(u )
D(u ) d2,d3,d4
W ( di ) si
U ( di ) iPhone , AppStore , Samsung d8 d9 Baker , Apple ( b ) Cold start data set
1 1 u2,u3
Figure 2 : An example of users and articles
Digg user ids . A user can submit an article which consists of the user id as well as the title , a brief description and the link to the interesting web page of the submitted article . Let D(ua ) denote the set of Digg article ids which are submitted by the user ua ∈ U and let W ( di ) denote the bag of words appearing in either the title or the description of the article di ∈ D . The number of words in W ( di ) is denoted by Ni and we use wij ( 1≤j≤Ni ) to represent the j th word appearing in the article di ∈ D . For each article di ∈ D , we generate the bag of words W ( di ) by deleting the stop words from its original title and description . Let W = {w1 , , wℓ} be the vocabulary which is the set of distinct words occurring in at least an article di ∈ D . We define n(di , w ) to denote the number of occurrences of the word w ∈ W in W ( di ) .
Digg users can vote thumbs up or thumbs down , called digging or burying respectively , for each Digg article . Every article di ∈ D has a Digg score si of an integer which is the number of diggings subtracted by the number of buryings voted for di . Let L(ua ) denote the set of article ids in D for which the user ua ∈ U votes thumbs up . For burying , we cannot obtain the list of articles for which users vote thumbs down and thus we utilize the history of diggings only . Let U ( di ) denote the set of users in U who dug the article di ∈ D . Finally , we refer to a user ua ∈ U to whom Digg articles are recommended as an active user .
Problem definition : Assume that we are given a collection D of Digg articles with Digg scores and a set of users U with the history of diggings for each user in U . The top K Digg article recommendation problem is defined as follows :
Problem 1 . For an active user ua ∈ D and a set of candidate articles C which are not submitted nor dug by the user ua yet , the problem is to find the top K Digg articles which the user ua would like the most among the candidate articles in C .
Example 31 : Consider a set of users U ={u1,,u4} and a set of Digg articles D={d1,,d7} in Figure 2(a ) . The articles dug by each user are presented in the column of L(u ) . We can see that there exist two topics of ‘smartphone’ and ‘Sherlock Holmes’ . The topic of ‘smartphone’ is much more popular than that of ‘Sherlock Holmes’ in the Digg articles . Thus , the articles on ‘smartphone’ usually obtain higher scores and have more chances to be listed in the front page of Digg service .
Suppose that we recommend the top 1 article for the user u3 who posted the article d6 and voted thumbs up for d3 and d4 both of which are submitted by u2 . Considering the articles he dug and submitted , we can see that he obviously wants
Seen article Unseen article
Seen user
Warm start
Cold start type 2
Unseen user
Cold start type 1
Cold start type 3
Figure 3 : Four types of recommendations to read the articles about ‘smartphone’ . If we simply consider only the other articles submitted ( or voted ) by the user u2 , who submitted the articles that u3 mainly dug , we would recommend the article d5 which is posted by u2 . However , the user u3 prefers the topic of ‘smartphone’ and it is better to recommend the article d2 on ‘smartphone’ rather than the article d5 which is about the topic of ‘Shelock Holmes’ .
Now , suppose that we want to recommend the top 1 article for the user u4 who submitted an article d7 which contains more words referring to ‘Sherlock Holmes’ than ‘smartphone’ . From the history of his diggings , we may think that he is interested in the articles on ‘smartphone’ since he dug d2 and d3 . Due to their high scores , the articles on ‘smartphone’ generally have more chances to be displayed in the front page of Digg service and thus the reason why u4 dug them is probably because he has actually seen the articles d2 and d3 in the front page . However , considering the contents of d7 submitted by the user u4 , recommending the article d5 , which is related to the topic ‘Sherlock Holmes’ , is also important to the user u4 .
3.2 Warm and Cold start Recommendations The top K recommendation problem is classified into four categories based on whether the active users , the candidate articles to be recommended , or both of them are included in the training data , which is used to learn the parameters of models , as illustrated in Figure 3 .
• Warm start recommendation : This is the case when both of the active users and the candidate articles are seen ( ie , the active users and candidate articles were included in the training data used for learning the model ) .
• Cold start recommendation of type 1 : It is the recommendation of seen candidate articles to an unseen active user ( ie , the active user was not included in the training data ) .
• Cold start recommendation of type 2 : This refers to the recommendation of unseen candidate articles ( ie , the candidate articles were not included in the training data ) to a seen active user .
• Cold start recommendation of type 3 : It is the case of recommending unseen candidate articles to an unseen active user .
4 . OUR GENERATIVE MODEL FOR DIGG
4.1 Our Generative Model
Previous works have shown the effectiveness of topic mixture models in clustering text collections by representing hidden topics with conditional probability distributions[4 , 12 , 15 , 18 ] . However , the traditional topic mixture models do not consider the ratings of text documents provided by users in order to get a clue for the topic preferences of users . To model both writing and digging behaviors of users , we use a model structure with the mixtures of conditional probability distributions by generalizing the PLSI model in [ 12 ] to consider the digging behaviors of users in Digg service .
693 z|u ~ γ : Multinomial d z w
N
Du w|z ~ φ : Multinomial
Figure 4 : A graphical model
Observed data : We assume that our generative model produces the following observed data : ( 1 ) the bag of words W ( di ) for every Digg article di∈D , ( 2 ) the list D(ua ) of Digg article ids submitted by every user ua∈U , ( 3 ) the list L(ua ) of Digg article ids dug by every user ua∈U . Hidden topics : We assume that there exist t number of major topics denoted by the integers from Z={1 , 2 , , t} in a collection of Digg articles D . We let wij be the j th word appearing in the article di ∈ D and let a random variable zij be the hidden variable to represent one of the t number of topics according to which the word wij is chosen . Note that we do not know in advance the actual label with which we can identify the topic zij . Conditional probability distribution functions ( PDFs ) : We introduce the following three conditional PDFs to represent the topic relevance models of articles , topic preference models of users as well as word selection models of topics :
( 1 ) The topic relevance models of articles : We introduce θ(z|d=di ) which is the conditional PDF with which we select the topic z ∈ Z for a given Digg article di ∈ D . It is a multinomial distribution over the topics in Z satisfying
θ(z=zk|d = di ) = 1 for every di ∈ D ,
( 1 )
X zk∈Z
( 2 ) The topic preference models of users : We define γ(z|u=ua ) that is the conditional PDF with which a user ua ∈ U chooses the topic z ∈ Z . It follows a multinomial distribution over the topics in Z and thus we have
γ(z=zk|u = ua ) = 1 for every ua ∈ U ,
( 2 )
X zk∈Z
( 3 ) The word selection models in topics : We use φ(w|z=zk ) to represent the conditional PDF from which the word w ∈ W is drawn , given the topic zk ∈ Z . It is also a multinomial distribution over the words in W which satisfies
X wj ∈W
φ(w=wj |z = zk ) = 1 for every zk ∈ Z
( 3 )
As long as it is clear from the context , we denote θ(z|d=di ) , γ(z|u=ua ) and φ(w|z=zk ) by θ(z|di ) , γ(z|ua ) and φ(w|zk ) respectively for the sake of simple representation .
Figure 4 illustrates the graphical representation of our proposed mixture model . Note that pa and pv , both of which will be dicussed in Section 4.2 , denote the probabilities of submitting an article and digging an article by a user respectively . Our generative model includes two independent generative processes . The first process considers the behavior of writing Digg articles by users . The second one represents how Digg users dig the articles written by other users . Both processes are illustrated below :
Writing Digg articles : Each Digg article di ∈ D(ua ) is produced by the user ua ∈ U with repeating the following steps while choosing the words stochastically :
• The user ua decides to write the Digg article di ∈ D(ua ) by following the Bernoulli distribution with the success probability computed by the function EXP KL(θ(z|di ) ; α , γ(z|ua ) ) where α is a constant in our model . As the multinomial distributions θ(z|di ) and γ(z|ua ) become similar , the EXP KL function generates high probability values . Similarly , as they become dissimilar , it produces low values closer to 0 . We will discuss the EXP KL function in the next section .
• Each word in W ( di ) is selected by repeating the following two steps Ni times .
1 . To select the j th word wij in di , we first choose a topic zij∈Z by following the conditional PDF θ(z|di ) . 2 . With the topic zij selected in the above , the word wij is chosen according to the conditional PDF φ(w|zij ) .
Digging Digg articles : A user ua determines whether he digs each Digg article di∈D(ua ) or not by following the Bernoulli distribution with the success probability of EXPKL( θ(z|di ) ; β/si , γ(z|ua ) ) where si is the Digg score of di and β is a constant in our model . Since the Digg articles with high scores have more chances to be shown to each user , if the article di has a high Digg score si , the user ua tends to dig di with a higher probability than the Digg articles with small Digg scores in the formulation of EXPKL(θ(z|di ) ; β/si , γ(z|ua) ) .
We will next introduce the EXP KL function used in our model and discuss its property .
4.2 The Exponential KL Function
Given two multinomial distributions , we propose to utilize the exponential KL divergence ( EXP KL ) function in our probabilistic model in order to obtain higher success probabilities for Bernoulli distributions [ 30 ] as the two multinomial distributions become similar to each other . Given a scalar value λ and a t dimensional multinomial distribution µ , the exponential KL divergence function EXP KL(θ ; λ , µ ) over the t dimensional multinomial distribution θ is defined as
EXP KL(θ ; λ , µ ) = λe−λ·KL[µ||θ ] ,
( 4 ) where KL[µ||θ ] is the Kullback Leibler ( KL ) divergence between µ and θ , which is presented in [ 7 ] as
KL[µ||θ ] = t
X i=1
µi · log(µi/θi ) ,
( 5 ) and λ is the parameter which determines the steepness of the EXP KL function .
The EXP KL function generates the probability density of λ when θ=µ . The probability densities calculated by using EXP KL decrease with increasing the distance between θ and µ . Furthermore , with a large λ , the probability densities decrease faster when the distance between θ and µ grows . We next show how the EXP KL function is utilized in our model in details .
694 The probability of submitting an article by a user : A user ua∈U , whose topic preference is γ(z|uz ) , submits the article di∈D , of which topic relevance is θ(z|di ) , with the probability of
EXP KL(θ(z|di ) ; α , γ(z|ua ) ) = αe−α·KL[γ(z|ua)||θ(z|di) ] ,
( 6 ) where α is a constant in our model . With a small α , it becomes more probable that an article with a quite different topic relevance of θ(z|di ) from its author ’s topic preference is submitted . Thus , by setting a small value to α , we can allow Digg users to submit the articles with diverse topic relevances .
The probability of digging an article by a user : A user ua∈U , whose topic preference is γ(z|ua ) , digs the article di∈D with Digg score si , whose topic relevance is θ(z|di ) , with the probability of
EXP KL(θ(z|di ) ; β/si , γ(z|ua ) )
=
β si e−(β/si)·KL[γ(z|ua)||θ(z|di) ] ,
( 7 ) where β is a constant such that with a small value of β , it becomes more probable that a user ua with the topic preference of γ(z|ua ) votes thumbs up for the article di with a quite different topic relevance of θ(z|di ) from his topic preference γ(z|ua ) . Furthermore , since the articles with high Digg scores not only are interesting to many users but also have more chances to be displayed in the front page of Digg , such articles will be dug by many users with higher probabilities . For the article di with the Digg score si , by using the steepness parameter ( β/si ) in the EXP KL function , the users tend to dig the articles with larger Digg scores in our model .
4.3 The Likelihood of Digg Data
Let D and U be a set of Digg articles and a set of Digg users respectively . For each Digg article di ∈ D , we have a bag of words W ( di ) and a Digg score si . For each Digg user ua ∈ U , let D(ua ) and L(ua ) be the set of articles that the user ua submitted and the set of articles that ua dug respectively . Let ppost(di|ua ) and pdigg(di|ua ) represent the probability that ua summits the Digg article di and the probability that ua votes thumbs up for di respectively . Since each user ua ∈ U posts the Digg articles in D(ua ) and digs those in L(ua ) independently in our model , the likelihood L of the Digg data becomes
L = Y ua∈U
 
  Y di∈Du ppost(di|ua) 
·   Y di∈Lu pdigg(di|ua) 
.
 
Since each word in W ( di ) is sampled independently after the user ua decides to submit the article di , we have ppost(di|ua ) = EXP KL(θ(z|di ) ; α , γ(z|ua ) ) · Y wij ∈W ( di ) p(wij |di ) .
Furthermore , if we marginalize p(wij|di ) with the random variable zij , p(wij|di ) becomes Pzk∈Z φ(w=wij|zk ) θ(z=zk|di ) . Finally , since each article in L(ua ) is dug by ua independently by following the distribution EXP KL(θ(z|di ) ; β/si , γ(z|ua) ) , we obtain the likelihood as follows
L = Y ua∈U
Y di∈D(ua )
αe−α·KL[γ(z|ua)||θ(z|di ) ]
Y di∈D(ua )
  X zk∈Z
φ(wj |zk)θ(zk|di) 
Y wj ∈W n(di,wj )
Y di∈L(ua )
( β/si)e−(β/si)·KL[γ(z|ua)||θ(z|di) ] ,
( 8 )
· Y ua∈U
· Y ua∈U where n(di , wj ) denotes the number of appearances of the word wj ∈ W in the article di .
4.4 The Maximum Likelihood Estimate
Assume that the observed data is generated from our generative model . Let Θ denote our initially unknown model parameters , which consist of the distributions φ(w|zk ) for every zk ∈ Z , θ(z|di ) for every di ∈ D and γ(z|ua ) for every ua ∈ U . We wish to find the model parameters Θ such that the likelihood L in Equation ( 8 ) is maximized . This is known as the Maximum Likelihood ( ML ) estimation [ 19 ] for computing Θ . In order to estimate Θ , we generally introduce the log likelihood function defined as log L =
X ua∈U
X di∈D(ua )
X wj ∈W n(di , wj ) log X zk∈Z
φ(wj |zk)θ(zk|di )
− α X ua∈U
− β X ua∈U
X di∈D(ua )
KL[γ(z|ua)||θ(z|di ) ]
X di∈L(ua )
( 1/si ) · KL[γ(z|ua)||θ(z|di ) ]
+ X ua∈U
X di∈D(ua ) log α + X ua∈U
X di∈L(ua ) log(β/si ) .
( 9 )
The likelihood function is considered to be a function of the parameters Θ for the Digg data . Since log L is a strictly increasing function , the parameters of Θ which maximize log likelihood of log L also maximize the likelihood L [ 31 ] . Note that the parameters θ(z|d ) , γ(z|u ) and φ(w|z ) are probability values and thus we have the constraints of Equations ( 1)–(3 ) . Using these constraints , we calculate the model parameters Θ with maximizing the log likelihood log L in Equation ( 9 ) .
The effect of α and β : Note that KL EXP(γ(z|uz)||θ(z|di ) ) in the log likelihood of Equation ( 9 ) can be represented as −H[γ(z|ua ) ] − Pz log θ(z|di ) where H[γ(z|ua ) ] is the entropy of γ(z|ua ) . It is known that the entropy of multinomial distribution is maximized when the distribution is uniform [ 7 ] . Since KL EXP(γ(z|uz)||θ(z|di ) ) is multiplied by −α and −β in Equation ( 9 ) , with large α and β , maximizing the entropy terms increases the log likelihood of Equation ( 9 ) more than maximizing the other terms in Equation ( 9 ) does . Thus , when α and β are large , the distribution of γ(z|ua ) becomes closer to the uniform distribution . Furthermore , when both of α and β are zeros , our model becomes the original PLSI model in [ 12 ] which is a special case of our model .
4.5 Estimation of Model Parameters
Without any prior knowledge to the model parameters , we can apply the maximum likelihood estimator to compute all the parameters by applying the EM algorithm [ 8 ] . An
695 E step : p(k+1)(zij =zk|di , wij ) =
φ(k)(w=wj |zk)θ(k)(z=zk|di )
Pz′ ∈Z φ(k)(w=wj |z′)θ(k)(z=z′|di )
M step :
φ(k+1)(w=wj |zk ) =
Pdi ∈D n(di , wj ) · p(k+1)(z=zk|di , wj )
Pw′ ∈W Pdi ∈D n(di , w′ ) · p(k+1)(z=zk|di , w′ )
θ(k+1)(z=zk|di ) =
γ(k+1)(z=zk|ua ) =
Pwj ∈W n(di , wj ) · p(k+1)(z=zk|di , wj ) + αγ(k)(z=zk|ua ) + ( β/si ) Pu′ ∈U ( di ) γ(k)(z=zk|u′ )
Pz′ ∈Z {Pwj ∈W n(di , wj ) · p(k+1)(z=z′|di , wj ) + αγ(k)(z=z′|ua ) + ( β/si ) Pu′ ∈U ( di ) γ(k)(z=z′|u′)} hQdi ∈D(ua ) θ(k+1)(z=zk|di)α Qdj ∈L(ua ) θ(k+1)(z=zk|dj )β/sj i1/(α|Du|+Pdj ∈L(ua ) β/sj )
Pz′ ∈Z hQdi ∈D(ua ) θ(k+1)(z=z′|di)α Qdj ∈L(ua ) θ(k+1)(z=z′|dj )β/sj i1/(α|Du|+Pdj ∈L(ua ) β/sj )
Figure 5 : The formulas for E step and M step
( 10 )
( 11 )
( 12 )
( 13 )
EM algorithm performs the iterations with the two steps of an expectation step ( E step ) and a maximization step ( Mstep ) . In the E step , the probability distributions of the hidden variables are computed by using the current estimate of parameters , and in the M step , the parameters maximizing the log likelihood are calculated by utilizing the expectation computed in the E step . The parameters estimated in the M step are then used in the E step of the next iteration .
The E step : This step calculates the expectation of the hidden variables . Each hidden variable is the topic zij which is chosen for selecting the word wij ( ie , the j th word occurring at di ) . Let p(zij=zk|di , wij ) be the probability that a word wij is generated from the topic zk in the Digg article di . The formula to compute p(k+1)(zij=zk|di , wij ) in the Estep of the ( k+1) th iteration using the model parameters Θ(k ) computed in the k th iteration is presented in Figure 5 . The M step : In order to find the parameters Θ(k+1 ) maximizing Equation ( 9 ) , we apply the method of Lagrange multipliers [ 3 ] . The obtained formulas for the M Step to update the model parameters Θ at the ( k+1) th step are listed in Figure 5 . Note that the topic preference γ(z=zk|ua ) of the user ua is calculated in the form of geometric mean , which is commonly used to compute the average of ratio values [ 29 ] , with the topic relevances θ(z=zk|dj ) of the Digg articles written and dug by the user ua .
We iterate the E Step and M Step until we obtain the convergence of the log likelihood in Equation ( 9 ) . Since our EM algorithm only guarantees to find a local maximum of the likelihood , we perform multiple trials and choose the best one among the local optima found .
5 . RECOMMENDATIONS USING MODEL
PARAMETERS
Once the parameters Θ in our model are estimated , recommendations can be made by utilizing the model parameters .
Top K warm start recommendation : To recommend the top K Digg articles , for a user ua and an article di , we predict the preference of the user ua for an article di using the KL divergence between γ(z|ua ) and θ(z|di ) as
Score(ua , di ) = 1/KL[γ(z|ua)||θ(z|di ) ]
( 14 ) and recommend the top K scored articles among the candidates . It is because the KL divergence between γ(z|ua ) and θ(z|di ) increases as both of γ(z|ua ) and θ(z|di ) become more different . Note that since we should recommend the articles with similar topic preferences to the user ’s preference regardless of the articles’ Digg scores , we simply utilize KL[γ(z|ua)||θ(z|di ) ] only for warm start recommendations . Top K cold start recommendations : We make coldstart recommendations of the three types mentioned in Section 3.2 as follows .
• For the type 1 , we estimate the topic preference ˆγ(z|ua ) of the active user ua unseen in the training data and compute Score(ua , di ) in Equation ( 14 ) by using θ(z|di ) of the candidate article di seen in the training data and the estimated ˆγ(z|ua ) for top k recommendation .
• For the type 2 , if an article di in the candidate set is unseen in the training data , we first estimate ˆθ(z|di ) , and compute Score(ua , di ) using the estimated ˆθ(z|di ) and the topic preference γ(z|ua ) of the active user ua seen in the training data .
• For the type 3 , we estimate both of ˆγ(z|ua ) and ˆθ(z|di ) to compute Score(ua , di ) in Equation ( 14 ) .
We next present how to estimate γ(z|ua ) for an unseen user ua and θ(z|di ) for an unseen Digg article di .
Computing ˆγ(z|ua ) for an unseen user ua:When an unseen user ua is not included in the training data but the articles submitted or dug by ua participate in the data , given the model parameters θ(z|di ) of those articles di , the probability that ua posts the articles in D(ua ) and diggs those in L(ua ) for the unknown distribution γ(z|ui ) can be computed by using Equation ( 8 ) as
Y di∈D(ua ) e−αKD[γ(z|ua)||θ(z|di ) ]
· Y di∈L(ua ) e−(β/si)KD[γ(z|ua)||θ(z|di) ] ,
( 15 ) where θ(z|di ) is the topic preference of the article di which exists in the data . By Lagrangian method , we can derive the optimal distribution of ˆγ(z|ua ) , which maximizes the probability in Equation ( 15 ) , as follows
ˆγ(z|ua ) =
1
Z(ua )
· Y di∈L(ua )
θ(z|di)α di∈D(ua )
  Y θ(z|di)β/si 
α|D(ua )|+Pdi ∈L(ua ) β/si
1
,
( 16 )
696 Topic User z1 z2 u1 0.47 0.52 u2 0.87 0.13 u3 0.81 0.19 u4 0.25 0.75 u5 0.94 0.06
Topic Topic Doc z1 z2 Doc z1 z2 d1 0.11 0.89 d5 0.43 0.57 d2 0.87 0.13 d6 0.70 0.30 d3 0.96 0.04 d7 0.31 0.69 d4 0.93 0.06 d8 0.99 0.01 d9 0.65 0.35
( a ) γ(z|u ) ( b ) θ(z|d )
Topic Topic Topic Word z1 z2 Word z1 z2 Word z1 z2 Sherlock 0 0.41 Baker 0.08 0.18 Samsung 0.21 0 Holmes 0 0.20 iPhone 0.35 0 AppStore 0.14 0 221B 0 0.20 Apple 0.21 0
( c ) Φ(w|z )
Seen Unseen user user u5 Doc u3 0.56 d1 0.76 d2 90.7 d3 d4 1.68 d5 3.17 5.91 d6 1.14 d7 1.86 u4 0.66 15.1 2.31 d8 2.60 0.34 d9 2.98
15.2 4.38
( d ) scores
Alg .
Training
Warm start
Recommendation Cold start Type 1 Type 2 Type 3 DIGTOBI DIGTOBI EM DIGTOBI W DIGTOBI C1 DIGTOBI C2 DIGTOBI C3 CTR CTR TR CTR W CTR C1 CTR C2 CTR C3 HotDigg HOTDIGG EM HOTDIGG W MEM MEM W COS COS W COS C1 COS C2 COS C3 Baseline BASE W BASE C1 BASE C2 BASE C3
Figure 7 : The implemented algorithms k=1 ˆγ(z=k|ua )
Figure 6 : The resulting model parameters and scores where Z(ua ) is the normalization factor to make Pt = 1 . Since the articles written by ua are not generally contained in the training data when the author ua is not in the training data , the set D(ua ) of the articles submitted by ua is usually empty . Computing ˆθ(z|di ) for an unseen article di : For an unseen Digg article di in the training data , which was submitted by a seen user uj and dug by seen users U ( di ) , we can formulate , by using Equation ( 8 ) , the probability that di is generated to include the words in W ( di ) , posted by uj and dug by the users in U ( di ) as follows :
αe−αKD[γ(z|ua)||θ(z|di ) ] · Y
X
φ(w|z′)θ(z′|di ) w∈W ( di ) z′∈Z
· Y u∈U ( di )
( β/si)e−(β/si)KD[γ(z|u)||θ(z|di) ] .
( 17 )
To compute ˆθ(z|di ) maximizing the above probability , we should develop another EM algorithm requiring heavy computation . Thus , we approximate ˆθ(z|di ) by performing only the first iteration of the EM algorithm with initializing ˆθ(z|di ) uniformly and calculate it as
1
Z(di )
  X w∈W ( di )
φ(w|z )
Pz′∈Z φ(w|z′ )
+ αγ(z|uj ) +
β si X u∈U ( di )
γ(z|u)  ( 18 )
, where Z(di ) is the normalization factor to make Pt = 1 . Our experiments show that executing only a single iteration of the EM algorithm estimates good ˆθ(z|di)s enough for cold start recommendations of both type 1 and type 3 . k=1
ˆθ(z=k|di )
Example 51 : Consider the Digg articles and the users shown in Figure 2 . We assume that the number of topics t is 2 and α = β = 1 . Figure 6(a ) presents the model parameters γ(z|u ) , θ(z|d ) and φ(w|z ) after our EM algorithm finishes . Note that the hidden topics z1 and z2 represent the topics of ‘smartphone’ and ‘Sherlock Holmes’ respectively .
Warm start recommendations : Let us find the top 1 article to recommend to the user u3 among the candidate set C={d1,d2 , d5,d7} which are not submitted nor dug by u3 . The scores of the articles in C according to Equation ( 14 ) are provided in Figure 6(d ) . As we discussed in Example 3.1 , the user u3 showed his interest in the topic of ‘smartphone’ . Since the article d2 is on the topic ‘smartphone’ and obtained the highest score among the articles in C , we recommend d2 to u3 as the top 1 article .
Cold start recommendations of type 1 : The values of ˆγ(z|u5 ) for an unseen user u5 according to Equation ( 16 ) are presented in Figure 6(a ) . Since u5 dug the articles d2,d3 and d4 on the topic ‘smartphone’ that is represented by z1 , ˆγ(z1|u5 ) has the highest score as expected . Thus , among the candidate articles C={d1,d2,d5,d7} which are seen and not dug by u5 , d6 has the highest score and is thus recommended as the top 1 article to u5 .
Cold start recommendations of type 2 : Let C={d8,d9} be the candidate articles which are unseen in the training data . The values of ˆθ(z|d8 ) and ˆθ(z|d9 ) are shown in Figure 6(b ) . As expected from the content of d8 , ˆθ(z1|d8 ) has the highest probability representing that d8 is related the most to the topic ‘smartphone’ . Thus , the article d8 obtains the highest score among the articles in C for the user u3 and u5 . Furthermore , since d9 is mainly dug by the users interested in the topic of ‘smartphone’ which is represented by z1 , we can see that ˆθ(z1|d9 ) > ˆθ(z2|d9 ) .
Cold start recommendations of type 3 : The recommendation scores Score(ua , di ) between the unseen user u5 and the unseen articles {d8 , d9} are shown in Figure 6(d ) . Among the unseen articles , d8 on the topic ‘smartphone’ is recommended to u5 as the top 1 article since u5 is interested in the same topic but d9 is related to another topic .
6 . EXPERIMENTS
We empirically evaluated the performance of our proposed algorithms . All experiments reported in this section were performed on the machine with Intel(R ) Core(TM)2 Duo 2.66GHz and 2GB of main memory running Linux . The following algorithms were implemented using GCC Compiler of version 413
• DIGTOBI : We implemented our proposed EM algorithm in Section 4.5 and the top K recommendation algorithms in Section 5 .
• CTR : Since CTR [ 28 ] combines the matrix factorization and the LDA model , and is shown to outperform the recommendations based on either matrix factorization or the LDA model only [ 28 ] , we selected CTR as the state of the art for this application . We downloaded the implementation of CTR in C language available at http://wwwcsprincetonedu/∼chongw/citeulike/ In [ 28 ] , they proposed the recommendation algorithms for warmstart and cold start of type 2 only . To deal with cold start of type 1 and type 3 , we estimated the latent vector ui of the i th user to maximize the likelihood of the CTR model ( Equation ( 7 ) in [ 28 ] ) as follows : ui = ( λ · I + B)−1 A
( 19 ) where B is a matrix such that Bxy = Pj cijvjxvjy and A is a vector such that Ax = Pj cijrijvjx . Note that vj is the latent vector of the j th item estimated by the CTR model and the other parameters ( ie , cij , rij and λu ) are the given values .
697 • COS : We implemented the recommendation algorithm in [ 5 ] as well . It was proposed to recommend Twitter messages using only the text messages and cosine similarity without any domain specific knowledge . They do not apply any user modeling method but simply represent Twitter messages of a user as a bag of word for content based recommendations . Thus , this recommendation algorithm does not need the training phase .
• HOTDIGG : It is the implementation of HotDigg recommendation algorithm in [ 14 ] . It recommends Digg articles without considering each user ’s preference .
• MEM : We also implemented the simple memory based recommendation algorithm in [ 24 ] which works for warmstart recommendations only .
• BASE : This is the implementation of the baseline algorithm which is capable of both warm and cold start recommendations by selecting K articles randomly among the candidate articles .
All algorithms for warm and cold start recommendations are categorized in Figure 7 where each empty entry denotes that its corresponding algorithm cannot handle the case represented by its column . In our experiments , the train phases of all algorithms were performs 10 times for finding local maxima with the termination condition used by CTR in [ 28 ] and we chose the best one as the model parameter values .
6.1 Data Sets
We downloaded 120,896 Digg articles submitted or dug by 239,847 users in two months from Dec . 2011 to Jan . 2012 using Digg API available at http://developersdiggcom/ The number of diggings , which are represented by the pairs between those downloaded users and articles , is 680,971 . We removed the stop words appearing in more than 80 % of all articles . Furthermore , we also deleted the words occurring in less than 3 articles since such words do not provide any clue for topical clustering . All words in Digg articles were stemmed by using the stemmer library in Lucene [ 16 ] . We refer to this data as ORG DATA . We selected a subset of diggings from ORG DATA to generate the test data sets and the rest of the other data is used as the training data to estimate the model parameters . We generated the test data sets as follows .
• TEST W1 : For warm start recommendations , we selected 10,000 diggings of 100 users from ORG DATA by the following steps : We first chose a seed user from ORGDATA randomly and selected another 99 test users from ORG DATA by choosing each user , who has the least common diggings with the users selected already , one by one greedily . Then , we extract 100 diggings of each selected user randomly . We found 7,479 number of distinct dug articles in the extracted diggings for all test users and used them as the candidate articles for recommendations .
• TEST W2 : For warm start recommendations , we selected another set of 10,000 diggings randomly . We first choose 100 users randomly from ORG DATA and next extracted 100 diggings for each selected user randomly . We also used distinct dug articles in the extracted diggings as the candidate articles for recommendations . The number of candidate articles was 4,887 .
• TEST C1 : For cold start recommendation of type 1 , we chose 40 new users who are not included in ORG DATA but have dug at least 10 articles in ORG DATA . We used the articles dug by the 40 new users as the candidate articles for recommendations . Note that we could not select more test data for cold start recommendations because there are a small number of new users who have dug enough articles .
• TEST C2 : For cold start recommendations of type 2 , we chose 40 users from ORG DATA randomly and for each user , we downloaded 10 more Digg articles which are dug by the user but not included in ORG DATA . Then , the newly downloaded 400 articles were used as the candidate articles for recommendations .
• TEST C3 : For cold start recommendations of type 3 , we downloaded 40 new users with 10 diggings for each user such that both of the users and their dug articles are not included in ORG DATA . The downloaded 400 new articles are used as the candidate articles for recommendations .
6.2 Quality of Recommendations
We conducted our experiments with varying the number of recommendations K , the number of hidden topics t , the parameters α and β which are the constants used in the exponential KL divergence distributions in Section 4 . The default values of these parameters are : K=30 , t=100 , α=106 and β=106 . Quality measures : We computed three basic quality measures called recall at K , precision at K [ 27 ] and average hitrank [ 9 ] . ( The recall at K is also known as hit rate [ 9] . ) With the top K recommendations for a test user u , let h be the number of correctly matched articles among the top K recommended articles and nT ( u ) be the number of articles dug by the user u in the test data . Assume that d1 , , dh are the h number of correctly matched articles by recommendations . Let si and pi denote the Digg score of di and the rank of di among the top K recommended articles respectively . Then , the recall at K and precision at K for u are h/nT ( u ) and h/K respectively . The average hit rank is calculated as 1/nT ( u ) · Ph i=1 1/pi which measures the effectiveness of ranking for each test user . As the dug articles by a test user appear with higher ranks in the top K recommended articles , this measure becomes larger . Thus , the high values of average hit rank are more desirable .
Digg articles with high scores tend to be exposed in the front pages of Digg service and thus get more chances of being dug . However , it is also desirable to recommend the relevant articles with low Digg scores to user ’s preference . Thus , we revised the above three goodness measures and also calculated those new measures to show that DIGTOBI does not simply select popular articles but finds each user ’s favorite articles among the low scored articles as well . To have better scores for the correctly recommended articles with small Digg scores , when si denotes the Digg score of the correctly matched article di , the weighted recall atK is calculated as 1/nT ( u ) · Ph i=1 1/si which is the sum of the inverse of the Digg score for every correctly recommended article . Similarly , the weighted precision at K and weighted average hit rank are calculated as 1/K · Ph i=1 1/si and 1/nT ( u ) · Ph i=1 1/(pi·si ) respectively .
698 K t i a n o s c e r P i
K t i a n o s c e r p i d e t i h g e W
K t i a n o s c e r P i
1
0.8
0.6
0.4
0.2
0
DIGTOBI W CTR W COS W MEM W HOTDIGG W BASE W k n a r t i h e t a r e v A
0.05
0.04
0.03
0.02
0.01
0
DIGTOBI W CTR W COS W MEM W HOTDIGG W BASE W
K t a l l a c e R
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
DIGTOBI W CTR W COS W MEM W HOTDIGG W BASE W
1 10 20
40
60
K
80
100
120
1 10 20
40
60
K
80
100
120
1 10 20
40
60
K
80
100
120
( a ) Precision at K
( b ) Average hit rank
( c ) Recall at K
1
0.1
0.01
0.001
0.0001
DIGTOBI W CTR W COS W MEM W HOTDIGG W BASE W k n a r t i h e t a r e v a d e t i h g e W
1
0.1
0.01
0.001
0.0001
1e 005
DIGTOBI W CTR W COS W MEM W HOTDIGG W BASE W
K t a l l a c e r d e t i h g e W
1
0.1
0.01
0.001
0.0001
DIGTOBI W CTR W COS W MEM W HOTDIGG W BASE W
1 10 20
40
60
K
80
100
120
1 10 20
40
60
K
80
100
120
1 10 20
40
80
100
120
60
K
( d ) Weighted precision at K
( e ) Weighted average hit rank
( f ) Weighted recall at K
Figure 8 : Top K Digg article recommendations with varying K ( TEST W1 )
0.25
0.2
0.15
0.1
0.05
0
DIGTOBI W CTR W COS W MEM W HOTDIGG W BASE W k n a r t i h e t a r e v A
0.014
0.012
0.01
0.008
0.006
0.004
0.002
0
DIGTOBI W CTR W COS W MEM W HOTDIGG W BASE W
K t a l l a c e R
0.2
0.15
0.1
0.05
0
DIGTOBI W CTR W COS W MEM W HOTDIGG W BASE W
1 10 20
40
60
K
80
100
120
1 10 20
40
80
100
120
1 10 20
40
60
K
60
K
80
100
120
( a ) Precision at K
( b ) Average hit rank
( c ) Recall at K
Figure 9 : Top K Digg article recommendations with varying K ( TEST W2 )
Warm start recommendations : We first evaluate the quality of warm start recommendations using TEST W1 . Figures 8(a)–(c ) show precision at K , average hit rank and recall at K of the six warm start recommendation algorithms listed in Figure 7 respectively with varying K from 1 to 120 . The graphs illustrate that DIGTOBI W for every K outperforms the other recommendation algorithms in terms of every quality measure . As expected , HOTDIGG and BASE are the worst performers since both of them are not personalized recommendation algorithms .
As we increase K , since we have more chances to answer the right articles correctly , both recall at K and average hitrank of all algorithms grow gradually . The precision at K of DIGTOBI W is the largest when K=1 and decreases gradually as K is increased . However , the precision at K of the other algorithms increases with growing K . This is because DIGTOBI W has good characteristics of making the correct answers to have higher ranks while the other algorithms do not . For the same reason , the performance improvement of our algorithm DIGTOBI W to the other algorithms becomes better with respect to the average hit rank rather than the recall at K for a fixed K . For instance , when K=10 , DIGTOBI W shows better performance than CTRW by 2.32 times with the recall at K , but with the average hit rank , DIGTOBI W outperforms CTR W by 3.07 times . With the same experiments , we also plotted the three new weighed quality measures in Figure 8(d)–(f ) . The log scale was used in the y axis . The graphs show that the recommendations by DIGTOBI W show better qualities than those by the other algorithms for every K . COS W and CTR W are the second and the third best performers respectively .
For example , when K=10 , the modified precision at K of DIGTOBI W is 1.92 times higher than that of COS W and the modified average hit rank of DIGTOBI W is 1.89 times better than that of COS W . Furthermore , when K=10 , the modified precision at K of DIGTOBI W is 23.9 times higher than that of CTR W and the modified average hit rank of DIGTOBI W is 40.5 times better than that of CTR W . Thus , we conclude that DIGTOBI W recommends the relevant articles to each user ’s preference effectively even though they have low Digg scores .
We also tested with TEST W2 and plotted the precisionat K , average hit rank and recall at K in Figure 9(a)–(c ) . The quality of recommendations shows similar trends with the results of using TEST W1 . Since the graphs for the weighted quality measures also show similar trends , we do not present the graphs with TEST W2 here . In general , the values of all quality measures with TEST W2 are smaller than those with TEST W1 . The reason is as follows : For TEST W1 , since we selected the users having a small number of common diggings as possible , it is more likely that the candidate articles not dug by a test user ua is the one in which ua is actually not interested . However , for TESTW2 , since we selected the test data randomly , the candidate articles probably include the articles which are not dug by a test user ua but relevant to the preference of ua . Even though such articles are recommended to the user ua , we have to regard them as incorrect answers . Thus , the values of all quality measures with TEST W2 become smaller than those with TEST W1 .
Cold start recommendations : Using TEST C1 , we show the recall at K and precision at K of DIGTOBI C1 , CTR
699 K t a l l a c e R
K i t a n o s c e r P i
DIGTOBI C1 CTR C1 COS C1 BASE C1
1
5
10
1 0.8 0.6 0.4 0.2
1 0.8 0.6 0.4 0.2
1
5
10
15
K
15
K
20
30
K t a l l a c e R
K i t a n o s c e r P i
DIGTOBI C2 COS C2 CTR C2 BASE C2
1
5
10
0.6
0.4
0.2
0.6
0.4
0.2
20
30
1
5
10
15
K
15
K
20
30
K t a l l a c e R
K i t a n o s c e r P i
DIGTOBI C3 COS C3 CTR C3 BASE C3
1
5
10
0.6
0.4
0.2
0.6
0.4
0.2
20
30
1
5
10
20
30
20
30
15
K
15
K
( a ) Type 1
( b ) Type 2
( c ) Type 3
Figure 10 : Top K cold start recommendations with varying K
2000
1800
1600
1400
1200
1000
800
600
400
200
0
10
CTR TR DIGTOBI EM
K t a l l a c e R
0.25
0.2
0.15
0.1
0.05
0
50
100
150
200
1
5
10
The number of topics t=200 t=100 t=40 t=10 t=5 t=2
K t a l l a c e R
0.2
0.15
0.1
0.05
0
α=106/β=106 α=106/β=104 α=106/β=108 α=103/β=103 α=1/β=1
15
K
20
30
1
5
10
20
30
15
K
) n m i
( e m i t n o i t u c e x E
Figure 11 : Execution time varying t
Figure 12 : Recall at K varying t
Figure 13 : Recall at K varying ( α,β )
C1 , COS C1 and BASE C1 with increasing K from 1 to 30 in Figure 10(a ) . Note that MEM cannot handle any type of cold start recommendation . We can see that DIGTOBIC1 makes the best recommendations to the test users in TEST C1 , even though the test users are not considered in the phase of learning model parameters . For every value of K , CTR C1 shows the second best performance in terms of both recall at K and precision at K .
With TEST C2 , we evaluate the qualities of cold start recommendations of type 2 by DIGTOBI C2 , CTR C2 , COSC1 and BASE C2 , and plot both quality measures in Figure 10(b ) . Remember that the type 2 recommends the Digg articles , which are not seen in the training data , to the seen users included in the training data . The graph shows that DIGTOBI C2 also outperforms the other algorithms for cold start recommendations of type 2 .
Figure 10(c ) shows the recall at K and precision at K of
DIGTOBI C3 , CTR C3 , COS C3 and BASE C3 using TESTC3 for cold start recommendation of type 3 . Here , we recommend Digg articles to the test users in TEST C3 where both articles and test users are not included in the training data . The graph shows that COS C3 is better than DIGTOBI C3 with K≤5 but our algorithm DIGTOBI C3 shows better performance with the other values of K .
Execution time for estimating model parameter : In Figure 11 , with varying the number of topics t from 2 to 200 , we plotted the running times of the inference algorithms DIGTOBI EM and CTR TR . The graph shows that as the number of topics t is increased , the execution time of our inference algorithm DIGTOBI EM grows linearly with t . However , the speed of CTR TR slows down very fast as t becomes larger . We conclude that DIGTOBI EM outperforms the variational EM algorithm of the LDA model used in CTR TR in terms of speed for estimating model parameters .
Varying t : With varying K from 1 to 30 and the number of topics t from 2 to 200 together , we measured the quality of recommendations by our recommendation algorithm DIGTOBI W in terms of the recall at K with the test data
TEST W1 as shown in Figure 12 . The graph shows that DIGTOBI W obtains the best quality of recommendations when t≥100 and the quality of recommendations does not improve any more with t>100 . Thus , we use t=100 for the default value in our experiments .
Varying α and β : To find the best values for the constants α and β in our probabilistic model presented in Section 4.2 , we varied α and β together and plotted the recall at K with our recommendation algorithm DIGTOBI W in Figure 13 . The quality of recommendations by our algorithm was the best when α=106 and β=106 . Thus , we set the default values of both α and β to 106 in our experiments .
7 . CONCLUSION
We presented DIGTOBI , a personalized recommendation system for Digg articles using a novel probabilistic modeling . Utilizing the observations that Digg users submit their Digg articles and vote thumbs up for the Digg articles submitted by others depending on their topic preferences , we designed our generative model to describe the probabilistic processes of submitting and digging articles by each user . Our model improves the quality of recommendations by enabling the relevant articles with low Digg scores as well as high Digg scores to be considered important . We developed the EM algorithm for learning the best model parameters in our probabilistic model . We also provided effective warm start and cold start recommendation algorithms utilizing our model parameters . By performance study , we confirmed the effectiveness of DIGTOBI by comparing the performance with the traditional recommendation algorithms .
Acknowledgment This work was supported by the National Research Foundation of Korea(NRF ) grant funded by the Korea government(MEST ) ( No . 2012 0000111 ) . This was also supported by Next Generation Information Computing Development Program through the National Research Foundation of Korea(NRF ) funded by the Ministry of Education , Science and Technology ( No . 2012 033342 ) .
700 8 . REFERENCES [ 1 ] D . Agarwal and B C Chen . Regression based latent factor models . In KDD , 2009 .
[ 2 ] D . Agarwal and B C Chen . fLDA : matrix factorization through latent dirichlet allocation . In WSDM , pages 91–100 , 2010 .
[ 3 ] D . P . Bertsekas . Nonlinear Programming ( Second ed )
Cambridge , 1999 .
[ 4 ] D . M . Blei , A . Y . Ng , and M . I . Jordan . Latent dirichlet allocation . Journal of Machine Learning Research , 3:993–1022 , 2003 .
[ 5 ] J . Chen , R . Nairn , L . Nelson , M . S . Bernstein , and
E . H . Chi . Short and tweet : experiments on recommending content from information streams . In CHI , 2010 .
[ 6 ] CiteULike . http://wwwciteulikeorg [ 7 ] T . M . Cover and J . A . Thomas . Elements of information theory . Wiley Interscience , 1991 .
[ 16 ] Lucene . http://luceneapacheorg [ 17 ] B . Mehta , T . Hofmann , and W . Nejdl . Robust collaborative filtering . In RecSys , 2007 .
[ 18 ] Q . Mei , X . Ling , M . Wondra , H . Su , and C . Zhai .
Topic sentiment mixture : modeling facets and opinions in weblogs . In WWW , 2007 .
[ 19 ] T . M . Mitchell . Machine Learning . McGraw Hill , 1997 . [ 20 ] Netflix . http://wwwnetflixcom [ 21 ] S T Park and W . Chu . Pairwise preference regression for cold start recommendation . In RecSys , 2009 .
[ 22 ] M . J . Pazzani and D . Billsus . Learning and revising user profiles : The identification of interesting web sites . Machine Learning , 27(3):313–331 , 1997 .
[ 23 ] R . Salakhutdinov and A . Mnih . Probabilistic matrix factorization . In NIPS , 2007 .
[ 24 ] B . M . Sarwar , G . Karypis , J . A . Konstan , and
J . Riedl . Item based collaborative filtering recommendation algorithms . In WWW , 2001 .
[ 8 ] A . P . Dempster , N . M . Laird , and D . B . Rubin .
[ 25 ] A . I . Schein , A . Popescul , L . H . Ungar , and D . M .
Maximum likelihood from incomplete data via the em algorithm . Journal of Royal Statist . Soc . , 39:1–38 , 1977 .
[ 9 ] M . Deshpande and G . Karypis . Item based top n recommendation algorithms . ACM Trans . Inf . Syst . , 22(1):143–177 , 2004 .
[ 10 ] Digg . http://digg.com , 2011 . [ 11 ] Facebook . http://wwwfacebookcom [ 12 ] T . Hofmann . Probabilistic latent semantic indexing . In
SIGIR , 1999 .
[ 13 ] T . Hofmann . Latent semantic models for collaborative
Pennock . Methods and metrics for cold start recommendations . In SIGIR , 2002 .
[ 26 ] L . Si and R . Jin . Flexible mixture model for collaborative filtering . In ICML , pages 704–711 , 2003 . [ 27 ] B Q Vuong , E P Lim , A . Sun , M T Le , and H . W . Lauw . On ranking controversies in wikipedia : models and evaluation . In WSDM , 2008 .
[ 28 ] C . Wang and D . M . Blei . Collaborative topic modeling for recommending scientific articles . In KDD , 2011 .
[ 29 ] Wikipedia . Geometric mean . http://enwikipediaorg/wiki/Geometric mean . filtering . ACM Trans . Inf . Syst . , 22(1 ) , 2004 .
[ 30 ] Wikipedia . Bernoulli distribution .
[ 14 ] Y . Kim and K . Shim . HotDigg : Finding recent hot topics from digg . In DASFAA , 2012 .
[ 15 ] W . Li and A . McCallum . Pachinko allocation :
Dag structured mixture models of topic correlations . In ICML , pages 577–584 , 2006 . http://enwikipediaorg/wiki/Bernoulli distribution .
[ 31 ] C . F . J . Wu . On the convergence properties of the em algorithm . The Annals of Statistics , 11(1):95–103 , 1983 .
701
