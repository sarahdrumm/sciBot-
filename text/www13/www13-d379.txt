Scalable K Nearest Neighbor Graph Construction Based on Greedy Filtering
Youngki Park , Sungchan Park , Sang goo Lee
School of Computer Science and Engineering
Seoul National University
{ypark , baksalchan , sglee}@europasnuackr
Woosung Jung
Department of Computer Engineering
Chungbuk National University wsjung@cbnuackr
ABSTRACT K Nearest Neighbor Graph ( K NNG ) construction is a primitive operation in the field of Information Retrieval and Recommender Systems . However , existing approaches to KNNG construction do not perform well as the number of nodes or dimensions scales up . In this paper , we present greedy filtering , an efficient and scalable algorithm for selecting the candidates for nearest neighbors by matching only the dimensions of large values . The experimental results show that our K NNG construction scheme , based on greedy filtering , guarantees a high recall while also being 5 to 6 times faster than state of the art algorithms for large , high dimensional data .
Categories and Subject Descriptors H33 [ Information Search and Retrieval ] : Information Filtering
Keywords K nearest neighbor graphs ; greedy filtering ; similarity join
1 .
INTRODUCTION
K NNG construction is a primitive operation in the field of Information Retrieval and Recommender Systems . For example , assuming that we constructed a K NNG graph whose nodes represent users , we can quickly recommend items to user u by looking at the purchase lists of u ’s nearest neighbors . As another example , if we implement an enterprise search system , we can easily provide an additional feature that finds K documents most similar to recently viewed documents .
Let us define the K NNG construction formally . We assume that the nodes of the K NNG are represented by normalized vectors , as shown in Table 1 . Let the normalized vectors be V and the dimensions in V be D . Each vector v ∈ V consists of di , rj pairs where di ∈ D and 0 ≤ rj ∈ R ≤ 1 . Given cosine similarity sim(vi ∈ V , vj ∈ V ) = ( vi · vj)/(vivj ) as a similarity measure , the KNNG construction returns the top k similar vectors for each vector .
As far as we know , NN Descent [ 1 ] is the most efficient approach for constructing K NN graphs . While brute force algorithms require too many similarity calculations that cost
Copyright is held by the author/owner(s ) . WWW 2013 Companion , May 13–17 , 2013 , Rio de Janeiro , Brazil . ACM 978 1 4503 2038 2/13/05 .
Table 1 : An example of greedy filtering v1 v2 v3 v4 v5 d1 0.5 d1 0.73 d2 0.4 d5 0.8 d3 0.48 d3 0.37 d2 0.55 d7 0.29 d4 0.35 d5 0.37 d8 0.33 d5 0.37 d6 0.27 d3 0.3 d7 0.34 d4 0.31 d3 0.1 d1 0.25 d2 0.27 d4 0.32 d9 0.23 d8 0.05 d10 0.1 d1 0.25 d10 0.2
O(|V |2|D| ) , NN Descent dramatically reduces the number of comparisons . However , it does not perform well as the number of nodes or dimensions scales up . In this paper , we present a novel , scalable algorithm and compare its performance with those of existing approaches .
2 . CONSTRUCTING K NN GRAPHS
Assuming that we sorted the elements of each vector into a descending order according to their values and that the prefix of each vector is determined beforehand ( See Table 1 , for example , where the prefixes are colored ) , according to length filtering [ 2 ] , we can guarantee that sim(suffix(vi ) , suffix(vj ) ) ≤ τvi ,∀vj based only on the values of prefix(vi ) . This raises the question of whether we can guarantee that sim(vi , vj ) ≤ τvi if the dimensions of prefix(vi ) and prefix(vj ) do not match at all . If sim(vi , vj ) ≤ τvi , then there ’s a high probability that the vectors vi , vj are not similar , because we sorted the elements so that τvi is a small value . In Table 1 , there is a counterexample in which sim(v2 , v4 ) > τv2 = 0.58 despite the fact that the dimensions of the prefixes do not match . However , we observe that the counterexamples are few and far between .
If we generalize this observation , we can assert that in most cases , two vectors are not similar if their prefixes do not overlap at all . If two vectors are not similar , there would be no edge in K NNG between them . That is to say , we would obtain an approximate K NNG by calculating similarities between sorted vectors that have at least one common dimension in their prefixes . In Table 1 as an example , we calculate the similarities of v1 , v3 ,v1 , v4 and we filter out v3 , v5 . Because this approach initially checks whether the dimensions of large values match , we will call it greedy filtering .
227 Algorithm 1 K NNG Construction Based on GF Data : sorted vectors V , parameters ρ , K Result : K NN queues Q begin L[di ] ←− φ,∀di ∈ D for vi ∈ V do L[dim(e1 P [ vi ] ←− 2,∀vi ∈ D vi ) ] = L[dim(e1 vi ) ] ∪ vi end repeat for di ∈ D do for vj ∈ V do vj ←− the jth vector in L[di ] SC[vj ] ←− SC[vj ] + |L[di]| end end for vi ∈ V do if SC[vi ] < ρK and |vi| ≥ P [ vi ] then
L[dim(eP [ vi ] P [ vi ] ←− P [ vi ] + 1 vi
) ] ←− L[dim(eP [ vi ]
) ] ∪ vi vi end end until L is not changed in this iteration Q[vi ] ←− φ,∀vi ∈ V for di ∈ D do compare all pairs vx , vy in L[di ] update the priority queues , Q[vx ] and Q[vy ] end return Q end
Algorithm 1 describes how a K NNG is constructed based on greedy filtering . ej vi denotes the jth element of the vector vi , dim(e ) denotes the dimension number of the element e , and SC[vi ] denotes the number of similarity calculations for the vector vi . For vectors whose SC < ρK , we incrementally increase their prefix sizes in a round robin fashion . Table 1 shows the prefixes determined by ρ = 2 . By default , we use ρ = 2 , which means that we expect there are K nearest neighbors for each vector among approximate ρK nearest neighbors .
3 . EXPERIMENTS
We considered four types of algorithms for comparison : Inverted Index Join as a baseline algorithm , which calculates all similarities with inverted indices [ 2 ] ; NN Descent [ 1 ] , which was developed for the purpose of constructing KNN graphs ; similarity join algorithms [ 2][3 ] , which return every pair vi , vj whose similarity is above ; and the topk similarity join algorithm [ 4 ] , which returns k pairs with the highest similarity values . The third and fourth types of algorithms should be repetitively executed while varying or k until K NN graphs are found . Unfortunately , because they did not perform better than Inverted Index Join , we only report the results of Inverted Index and NN Descent for comparison in this paper . We implemented all algorithms in Java and executed them on a single workstation with 14GB RAM and a 3GHz CPU .
Figure 1 shows the execution time for the DBLP dataset as the data scales ( ρ = 2 and K = 10 ) . Not shown is the data preprocessing time , which takes only a minor portion .
Greedy Filtering
NN Descent
Inverted Index Join
) s d n o c e s n i ( e m i t n o i t u c e x E
10,000
5,000
0
50K
100K
150K
200K
250K
Node Size
Figure 1 : Execution time
Table 2 : Recall and scan rate
Nodes
Dim .
50,000 100,000 150,000 200,000 250,000
55,010 87,759 115,703 140,913 163,841
NN Descent scan . recall 0.469 0.016 0.008 0.339 0.006 0.282 0.004 0.247 0.211 0.004
Ours recall 0.898 0.916 0.914 0.901 0.906 scan . 0.061 0.045 0.032 0.024 0.019
Our approach performs best among all algorithms ; comparatively much better when the data scales . Table 2 shows the recall and scan rate with the same parameters . Recall is the number of correct k nearest neighbors in K NNG divided by the number of edges in K NNG ; the scan rate is the total number of similarity calculations of the algorithm divided by the total number of similarity calculations of the bruteforce search . The results show that our approach maintains a stable recall value above 0.9 as the data scales , whereas the recall of NN Descent decreases gradually . An interesting finding is that the scan rate of NN Descent is lower than that of our approach . However , because our approach does not require significant overhead , such as overhead for maintaining hash tables , it is faster than NN Descent .
4 . ACKNOWLEDGMENTS
This work was supported by the National Research Foundation of Korea(NRF ) grant funded by the Korea government(MEST ) ( No . 20120005695 ) . This work was supported by Seoul R&BD Program(WR080951C0209725 ) .
5 . REFERENCES [ 1 ] W . Dong , C . Moses , and K . Li , “ Efficient k nearest neighbor graph construction for generic similarity measures , ” in WWW ’11 , pp . 577–586 , 2011 .
[ 2 ] D . Lee , J . Park , J . Shim , and S g Lee , “ An efficient similarity join algorithm with cosine similarity predicate , ” in DEXA ’10 , pp . 422–436 , 2010 .
[ 3 ] R . J . Bayardo , Y . Ma , and R . Srikant , “ Scaling up all pairs similarity search , ” in WWW ’07 , pp . 131–140 , 2007 .
[ 4 ] C . Xiao , W . Wang , X . Lin , and H . Shang , “ Top k set similarity joins , ” in ICDE ’09 , pp . 916–927 , 2009 .
228
