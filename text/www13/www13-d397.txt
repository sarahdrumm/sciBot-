Aggregating Crowdsourced Binary Ratings
Nilesh Dalvi Facebook , Inc . Menlo Park , CA nileshdalvi@gmail.com
Anirban Dasgupta
Yahoo! Labs Sunnyvale , CA anirbandasgupta@gmailcom
Ravi Kumar
Google
Mountain View , CA ravik53@gmailcom
Vibhor Rastogi
Google
Mountain View , CA vibhorrastogi@gmailcom
ABSTRACT In this paper we analyze a crowdsourcing system consisting of a set of users and a set of binary choice questions . Each user has an unknown , fixed , reliability that determines the user ’s error rate in answering questions . The problem is to determine the truth values of the questions solely based on the user answers . Although this problem has been studied extensively , theoretical error bounds have been shown only for restricted settings : when the graph between users and questions is either random or complete . In this paper we consider a general setting of the problem where the user–question graph can be arbitrary . We obtain bounds on the error rate of our algorithm and show it is governed by the expansion of the graph . We demonstrate , using several synthetic and real datasets , that our algorithm outperforms the state of the art .
Categories and Subject Descriptors H4m [ Information Systems ] : Miscellaneous
Keywords Crowdsourcing , mechanical turk , spectral methods
1 .
INTRODUCTION
Ever since Amazon launched its Mechanical Turk in 2005 , crowdsourcing and human computing have become part and parcel of the World Wide Web experience ( enwikipediaorg/wiki/ Crowdsourcing ) . The topic frequently hits popular media , ranging from plaudits1 to all round skepticism2 . Crowdsourcing has also attracted the attention of the research community at large , as evinced by the number of workshops and tutorials in many recent conferences dedicated to this topic : WWW3 , WSDM4 , SIGIR5 , CHI6 , KDD/AAAI7 .
1sfgate.com/business/prweb/article/ Crowdsourced mobile fraud intervention solution 4009930.php 2wwwtechnologyreviewcom/view/416966/ how mechanical turk is broken/ 3crowdsearchcomopolimiit/ 4irischoolutexasedu/csdm2011/ 5irischoolutexasedu/cse2010/ 6crowdresearch.org/chi2011 workshop/ 7wwwhumancomputationcom
Copyright is held by the International World Wide Web Conference Committee ( IW3C2 ) . IW3C2 reserves the right to provide a hyperlink to the author ’s site if the Material is used in electronic media . WWW 2013 , May 13–17 , 2013 , Rio de Janeiro , Brazil . ACM 978 1 4503 2035 1/13/05 .
As its name suggests , crowdsourcing taps into the wisdom of crowds . In its most basic version , it involves posing a presumably hard question to a set of users and aggregating their individual responses in order to deduce the answer to the question . This simple paradigm is useful in two scenarios where human labeling offers some version of the ground truth . First , it can be used to generate large quantities of labeled examples for algorithms that are based on machine learning . Second , it can be used for large scale human evaluation and comparison of different algorithms for a problem .
Even this simplest version of crowdsourcing already poses an interesting research challenge : how to aggregate the responses of the users in order to obtain the true answer to each question ? Meticulous users can be more accurate than the others in answering the questions , whereas unreliable/lazy ( or spammy ) users can provide random ( or even adversarial ) answers . To further complicate the problem , in many such systems , the reliability of a user may not be known a priori ; indeed , a large fraction of the users may even be new recruits . These issues entail a holistic approach to the problem : rather than aggregate the answers for each question in isolation , it becomes necessary to look at the global matrix of user provided answers to all the questions in order to simultaneously elicit both the user reliabilities and the true answers .
There have been several approaches [ 5 , 10 , 19 , 2 , 14 , 3 , 15 , 11 ] to formalizing this problem . These approaches posit a set of items with binary qualities , and a set of users indicating the qualities of items . Not all users necessarily rate all items . A bipartite graph G between items and users captures the set of items rated by each user . Typically , a simple model is assumed for users : each user is associated with a reliability measure , which is used to independently “ corrupt ” her perception of the true quality of the item . Given a set of user ratings , the problem is to collectively determine the reliability of each user and the true quality of each item . These approaches fall into two broad categories : machine learning based and linearalgebraic based . The machine learning approaches are based on variants of EM , which work on any graph G , but offer no guarantees as to how well they perform ( see Section 2 ) .
Algebraic approaches , on the other hand , can provide theoretical guarantees on the error in estimating item qualities , but so far have been limited to either complete assignment graphs ( when each user rates all items ) or to random graphs ( when the assignment of users to items is random ) . One of the first algebraic approaches was proposed by Ghosh et al . [ 5 ] , who present an algorithm with the following guarantee : for a random user–item assignment graph with n users , where in expectation each user rates D items and each item receives ∆ ratings , the fraction of incorrectly estimated items D3 ) . This bound is vacuous for sparse graphs where each user rates o(n1/3 ) items . Karger et al . [ 10 ] show that bounded by O( n
285 for random graphs , in the limit when number of items is going to infinity , the error in item qualities can be asymptotically bounded by e−O(∆ ) , where ∆ is again the expected number of users rating an item . Thus their bound is stronger than [ 5 ] and holds for sparse random graphs as well , but only asymptotically .
Our work is motivated by the fact that the user–item graph G is in practice neither random nor regular . Often , users determine both the number as well as the set of items they want to rate . The former is a function of their motivation level while the latter is determined by their expertise and familiarity with the items . Under such circumstances , it is not obvious how the techniques developed in [ 5 ] and [ 10 ] generalize—eg , [ 5 ] depends crucially on the fact that the “ expected ” item–item agreement matrix is low rank and hence recoverable under random perturbations , which the assumed generative mechanism posits as the model for user mistakes . Similarly , the performance of [ 10 ] depends crucially on whether belief propagation converges in arbitrary graphs . Thus it remains an open question to develop a strategy for aggregating user ratings when we do not have too much control in deciding which users choose what set of items to rate—whether there are characteristics of the user–item rating graph that make it amenable to good aggregation . Main results . Our main contribution is an eigenvector based technique to estimate both the user reliabilities and the item qualities that works for arbitrary user–item assignment graphs G . We bound the error rate as a function of the expansion gap , ie , the gap between the first and second eigenvalues of the graph GtG . The essence of our technique is to look at the user–user agreement matrices—measuring agreement between pairs of users—that are normalized by the number of items they decided to co rate , and to then extract its topmost eigenvector . A key element of our approach is to show a concentration result for structured random matrices , using the matrix version of McDiarmid ’s inequality . We then present two algorithms that are based on matrix completion ; for each of the algorithms , we prove that the estimated user reliabilities are close to the truth if the graph GtG has some expansion properties . If the assignment graph is random , our estimate for user reliabilities translates into an approximation for the item qualities as well .
In particular , for a ( D , ∆) regular graph with a large eigengap , our bounds translate into a user reliability estimation error of ˜O( 1√ ∆ ) . On the other hand , even if we knew the true answer to each 1 of the D questions that a user responds to , the estimated user reliability would still have a variance of 1/D , resulting in an estimation error of Ω(1/D ) in the user reliabilities . Our error bound of O(D−1/2 ) is not too far off from this lower bound . For ( D , ∆)random assignment graph our algorithm makes mistakes on only e−O(∆ ) fraction of the items . Our bound generalizes both the results of [ 5 ] and [ 10 ] , since this result holds for sparse graphs ( unlike [ 5 ] ) without requiring the asymptotic argument of the number of items going to infinity ( unlike [ 10] ) .
D
Finally we also demonstrate our algorithms on real world datasets and show how they improve upon the state of the art in terms of accuracy of estimates in both item qualities and user reliabilities .
2 . RELATED WORK
Crowdsourcing , using the global marketplace to perform microtasks in a scalable way , is a topic that has generated much excitement [ 8 , 12]—labeling and rating items consists of a large fraction of such tasks . A key problem in here is to decide how to aggregate the labels from multiple labelers of varying reliabilities such that the effect of the underlying noise is mitigated . The extensive empirical work by Sheng et al . [ 15 ] shows that getting more noisy labels per item and then aggregating them is more accurate than getting more expensive , and hence allegedly more “ accurate" , labels ; their work uses only majority voting to aggregate labels from multiple users , and is primarily concerned with identifying the items that will benefit from more labels . Dekel et al . [ 4 ] show that such aggregation can be improved if the bad raters are pruned .
A more general analysis of the user reliabilities was done by Dawid et al . [ 3 ] , who are the first to model the obfuscation of labels by judges , and use the EM algorithm in order to derive the true labels . Unfortunately , the EM technique suffers from lack of theoretical guarantees and has issues regarding convergence and initialization . Since then , there has been a host of followup work modifying this approach using a Bayesian technique [ 2 , 11 ] , studying it in the context of learning a specific classifier [ 14 ] , and modifying it by finding out spammers , ie , labelers deliberately giving incorrect responses [ 13 ] . Other related results in applying machine learning techniques to cleaning user labels include [ 19 , 16 , 14 , 18 ] .
Much of the above work does not come with theoretical guarantees on the inferred user reliabilities or the item labels . Both Ghosh et al . [ 5 ] and Karger et al . [ 10 ] study this problem independently in the same generative setting , where each user rates a random set of items , and has an inherent probability of identifying the correct label , or flipping it . Our model is essentially a generalization of their setting to arbitrary user–item assignment graphs . Ghosh et al . [ 5 ] present a spectral algorithm that provably learns the true item qualities , with bounded error . However , as pointed out , these bounds are useful only when each user performs a large number of ratings . Karger et al . [ 10 ] uses belief propagation8 to derive both a set of user reliabilities and an estimate for item qualities for a sparse random graph . Their convergence analysis uses techniques from density evolution and hence critically depends on the fact the graph is both sparse and random . Liu et al . [ 11 ] extend the BP algorithm of [ 10 ] via a Bayesian approach by choosing a suitable prior for item qualities and user reliabilities , and uses clever techniques to make the message passing more efficient .
An orthogonal question to ours , and one that has received much attention , is how to design incentives such that each user performs to the best of his abilities and provides truthful ratings [ 6 , 9 ] .
+
3 . PROBLEM DESCRIPTION Let m be the number of items and n the number of users . Let qi ∈ {−1 , 1} denote the quality of the ith item . Let q denote the column vector of length m with qi as the ith entry . Each user rates a subset of items . Let G ∈ {0 , 1}m×n denote the item–user assignment matrix , ie , Gij = 1 if item i is rated by user j . 3.1 Rating generation model
The ratings given by n users on m items is represented by a stochastic matrix U generated by the following random process ( similar to [ 5] ) . Each user j is associated with a probability pj ∈ [ 0 , 1 ] that captures how correct is her rating . Independently , for each item i she rates ( as dictated by G ) , she tosses a coin with bias pj : with probability pj , she rates item i ( correctly ) as qi and with probability 1 − pj , she rates item i ( incorrectly ) as −qi . Thus , the random matrix U ∈ {−1 , 0 , 1}m×n can be described as
qi
−qi 0
Uij = if Gij = 1 , wp pj , if Gij = 1 , wp 1 − pj , if Gij = 0 .
( 1 )
8Belief Propagation ( BP ) operates on the user–item bipartite graph , and like any standard BP algorithm , excludes the message from the node when computing the outgoing message to that node—if this message is included , then the algorithm reduces to that of [ 5 ] .
286 We call this random process as rating generation . Let wj = 2pj − 1 ; we call wj the reliability of user j . Thus , the user reliabilities are in the range [ −1 , 1 ] , where a reliability of 1 indicates a user who always answers correctly , a reliability of −1 indicates one who always answers incorrectly , and a reliability of 0 indicates one who answers uniformly at random . Let w ∈ n denote the vector of user reliabilities . 3.2 Problem definition
The algorithm is given as input a realization of the stochastic rating matrix U , assumed to be generated from the set of latent parameters q and w , which are unknown . The aim is to estimate both the user reliabilities and the item qualities simultaneously , ie , an estimate ˆw ∈ [ −1 , 1]n for the user reliabilities and an estimate ˆq ∈ {−1 , 1}m for the item qualities . The performance of the algorithm is measured by the distance to the underlying reliability vector and the quality vector . The errors for the estimates ˆw and ˆq provided by the algorithm will thus be defined 2 ) and by the following quantities : error( ˆw ) = 1 m 1[ˆqi = qi ] . E(||ˆq − q||2 n error(ˆq ) = 1 m 4 . TECHNIQUES
E(|| ˆw − w||2
2 ) = 4
We review some definitions from linear algebra before present ing our algorithms . 4.1 Background
Throughout the paper , we represent ( column ) vectors using lowercase letters ( a , b , w , . . . ) and matrices by uppercase letters ( M , N , . . ) Let x · y denote the innerproduct of x and y and let xt denote the transpose of x . For a matrix M , the spectral and Frobenius norms are denoted by M = M2 = maxx2=1 M x2 and ij)1/2 respectively . For two matrices M and N of matching dimensions , we define the following Hadamard products :
MF = ( ij M 2
( M ⊗ N )ij = MijNij ; ( M ff N )ij =
Mij/Nij 0 if Nij = 0 , otherwise .
For any matrix M ∈ m×n , we define the indicator matrix I(M ) ∈ {0 , 1}m×n such that I(M )ij = 1[Mij = 0 ] , ie , I(M )ij is 1 if and only if the corresponding entry of M is nonzero . We will also denote the ( scaled ) top eigenvector of a matrix M as v1(M ) = arg minx M − xxt2 , x(1 ) ≥ 0 .
We use the convention that indices i always denote items and indices j , k , . . . denote users . Let δi denote the number of ratings that item i gets , ie , the number of non zero entries in row i in G . Similarly , let dj denote the number of ratings that user j supplies . Define D = maxj dj and ∆ = maxi δi . 4.2 Algorithms
Recall that we are only presented with a realization of the ratings matrix U ( and hence its indicator , the assignment matrix G as well ) and we need to estimate both the item qualities and the user reliabilities . Before describing the algorithms , we present the intuition behind them .
The main idea is to work with the two user–user matrices U tU and GtG . The entry ( GtG)jk is the number of common items rated by users j and k . The entry ( U tU )jk is the difference between the number of agreements and disagreements of the users j and k . Let E denote the matrix which contains itemwise expected values of the random matrix U , ie , Eij = E[Uij ] = ( pjqi + ( 1 − pj)(−qi))Gij = qiGijwj .
1 : Input : U ∈ {−1 , 0 , 1}m×n , G ∈ {0 , 1}m×n . 2 : Output : ˜w ∈ n , ˆq ∈ m . 3 : ˆw = v1(U tU ) ff v1(GtG ) 4 : Define ˜wj = sgn( ˜wj ) max(| ˆwj| , 1 )
5 : Define ˆqi = sgn( j Uij ˜wj ) .
6 : Output ˆw , ˆq .
Algorithm 1 : Ratio of eigenvectors .
It is easy to see that
EtE = ( GtG ) ⊗ ( wwt ) , EtE ff GtG = I(GtG ) ⊗ wwt .
( 2 )
Suppose we knew the expected matrix E . We could then estimate the user reliabilities by solving the following problem A − B ⊗ ( wwt)F ,
F ( A , B ) = arg min w st ∀j , w2 j ≤ 1 .
( 3 ) with ( A , B ) as either ( EtE , GtG ) or ( EtE ff GtG , I(GtG) ) . It is easy to see why this approach works if the graph G is complete : the expected matrix E = qwt and solving ( 3 ) would give us back the user reliabilities w exactly . This approach , however , has a few problems when the graph G is arbitrary . First , the above program is computationally intractable for arbitrary G ( eg , [ 7] ) . But more importantly , we show that for arbitrary assignment graphs the matrix EtE might not be informative , as shown in the following example . Suppose there were two disjoint user groups A and B , and a user x ∈ A ∪ B . All users in A have reliability 1 , those in B have reliability −1 , and user x has reliability 0 . The items have two disjoint groups S and T of size m/2 . All users in A rate all items in S , all users in B rate all in T , and user x rates all items in S ∪ T . It is clear that by looking only at the matrix EtE , it is not possible to distinguish the highly reliable users from the non reliable ones . It is easy to extend this construction to k + 2 user partitions such that we cannot distinguish the high and low proficiency users even if we are explicitly given , in addition to EtE , the names of k users who answer all the questions . Thus , we want to characterize the class of graphs G that allows us to recover w with small errors 9 .
One of our main contributions is to identify the expansion of the graph G as a sufficient property that enables us to estimate w both efficiently and with low errors— the resulting algorithms are presented in Algorithm 1 and 2 . Since the matrix E is not observable , we instead work with the matrix U . Algorithm 1 is inspired by the observation that when GtG has rank one , ( 3 ) has an exact solution ˆw where ˆw ⊗ v1(GtG ) = v1(EtE ) and hence ˆw = v1(EtE ) ff v1(GtG ) . We will show that when the graph G has sufficiently high expansion , this solution , even when using U tU in place of EtE , is a reasonable approximation . Algorithm 2 is inspired by ( 2 ) and uses the same intuition that ( 3 ) is approximable when I(GtG ) is close to a rank one matrix . Hence , in this case , we first compute the rank one approximation v1(I(GtG ) ) and then use it to compute the final estimate ˆw .
We next show an error bound on the estimate ˆw for user reliability obtained from Algorithm 1 . ( Similar bounds can be shown for Algorithm 2 , which we defer to the full version . ) Our error bound holds for arbitrary graphs having expansion properties . However ,
9Previous approaches have looked at the matrix U U t ( as a proxy for EEt ) [ 5 ] ; by augmenting the above construction it is possible to show that such approaches will also incur a constant fraction error for arbitrary assignment graphs .
287 1 : Input : U ∈ {−1 , 0 , 1}m×n , G ∈ {0 , 1}m×n . 2 : Output : ˜w ∈ n , ˆq ∈ m . 3 : Define ˆw = v1(U tU ff GtG ff v1(I(GtG ) ) 4 : Define ˜wj = sgn( ˆwj ) max(| ˆwj| , 1 )
5 : Define ˆqi = sgn( j Uij ˜wj ) .
6 : Output ˆw , ˆq .
Algorithm 2 : Eigenvectors of ratio . in order to illustrate our bounds , we state the results for ( D , ∆)regular graphs . The more general result is stated and proved in denote the averSection 5 ( See Theorem 512 ) Let ¯w = age reliability of users . i w2 i n
1
THEOREM 4.1
( USER ERROR BOUND ) . Let , δ < 1 be a fixed positive constants . If G is a ( D , ∆) regular graph such that ∆ > 8 ¯w2 , D > 256 log(n/δ ) and the second eigenvalue of GtG , denoted by µ2 , satisfies the condition µ2 < ¯w2D∆ , then with probability 1 − δ , Algorithm 1 returns an estimate ˆw , such that
2 ¯w2
16
1
∆
.
+
1√ D error( ˆw ) = ˜O
When the item–user assignment graph is random , this error bound translates into a bound for error in item estimates . The question of whether such a bound holds for fixed graphs , under some assumptions , remains open .
THEOREM 42 Let G be a random ( D , ∆) regular graph . With high probability , Algorithm 1 returns estimates ˆq , such that error(ˆq ) ≤ exp
−O
∆
¯w2 − 1 ∆
− 1√ D
2
.
When the average reliability ¯w is some constant bounded away from 0 ( ie , users are good on average ) , then error(ˆq ) scales as exp(−O(∆) ) . This matches the bound in [ 10 ] . However , the bound in [ 10 ] requires that the limit of number of items goes to infinity , an assumption we no longer require . 4.3 Alternate projections
So far we have considered the case when G has a large expansion gap , ie , when the second eigenvalue is much smaller than the first . We propose a heuristic , without any theoretical guarantees , that improves the performance of both Algorithms 1 and 2 for low expansion graphs . This heuristic is based on the standard alternating projections technique [ 1 ] for solving the weighted low rank approximation problem . Recall that we are trying to find a user reliability vector w as a solution to the problem F ( A , B ) = arg minw A−B⊗(wwt)F . When B and I(B ) satisfy expansion properties , Algorithm 1 and 2 both give good approximations to this problem . Consider a slight generalization of this problem that instead finds two vectors u and v to minimize arg minu,v A − B ⊗ ( uvt)F . When one of the vectors , say u , is known , the other can be computed by solving a simple least squares problem . Thus , this gives an EM style alternating projections algorithm to iteratively compute u and v . On convergence , we are guaranteed to achieve a local optimum , which for symmetric matrices A and B implies that u = v . This common converged value can thus be used instead of w .
One problem with this approach is that since the original problem is not convex , the convergence can happen at a local minima . Thus , both the rate of convergence and the quality of converged solution depends on the initialization for u and v . In practice we observed that when u = v = ˆw , where ˆw is the estimate obtained by either Algorithm 1 or 2 , then both rate of convergence and quality of converged solution is good . Intuitively , this is because Algorithm 1 and 2 already try to minimize the objective function ( at least in the case of graphs with good expansions ) and hence provide a very good seed for the alternating projections heuristic .
5 . ANALYSIS
In this section we prove guarantees on the performance of our algorithms both in terms of the error incurred in estimating user reliabilities as well as for item qualities . The underlying intuition behind the proof is as follows . First we show that the response matrix U tU is close to the expectation matrix EtE . In order to prove this concentration bound , we need to use machinery aimed towards giving Chernoff like tail bounds for sums of random matrices . We then use the expansion ( and corresponding eigenvalue gap ) of the user–user co rating graph GtG to show that the gap between the first and second eigenvalues of GtG translates to a corresponding gap between the first and second eigenvalue of EtE as well . Using this , we then characterize the top eigenvector of EtE in terms of the top eigenvector of GtG and the reliability vector w ; the error in this characterization depends , among other quantities , on the ratio between the top two eigenvalues of the graph GtG . This enables us to use the eigenvalues of GtG and U tU to create ˆw , an estimate of w . After creating an estimate ˆw of the user reliabilities , we can then use it to create an estimate of the item qualities ˆq—the error in ˆq will depend on the error in ˆw . 5.1 Matrix tail bounds
We start with a statement of the matrix McDiarmid inequality that we will use as a tool . The underlying intuition behind this concentration result from [ 17 ] is that a random matrix is close to its expectation in terms of the spectral norm , if it can be expressed as the output of a function having bounded sensitivity over its input variables . Note that A B denotes the usual semidefinite ordering , ie , B − A is semidefinite .
THEOREM 5.1
( MATRIX BOUNDED DIFFERENCE [ 17] ) . Let {Zk}n k=1 be an independent family of random variables , and let H be a function that maps n variables to a self adjoint matrix of dimension d . Consider a sequence {Ak} of fixed self adjoint matrices that satisfies
( H(z1 , . . . , zk , . . . , zn ) − H(z1 , . . . , z i . Compute the variance parameter σ = where zi and z random vector z = ( Z1 , . . . , Zn ) . Then , for all t ≥ 0 , −t2/8σ2 i range over all possible values of Zi for each index 22 . Denote the k , . . . , zn))2 Ak
Pr[H(z ) − E[H(z ) ] > t ] ≤ d · e k Ak
2 ,
. i=1 Gijδ2 a user j , denote ρj = m
We will use Theorem 5.1 to show that the user–user agreement matrix U tU is close to its expectation EtE in the following sense . For i , ie , ρj is the sum of squared degrees of items that j responds to , and denote ρ = maxn j=1 ρj . We first define the function H(· ) . Lemma 5.2 then characterizes the structure of the difference matrices when any of the random variables is perturbed . Using this structural characterization Lemma 5.3 shows that function H(· ) satisfies the sensitivity conditions of Theorem 5.1 , and Lemma 5.4 shows the final bound that we get using the sensitivity condition derived in Lemma 53
We abuse notation and define the sequence of random variables U = {U11 , . . . , U1n , U21 , . . . , U2n , . . . , Um1 , . . . , Umn} .
288 diagonal matrix with kth diagonal entry as8GikGij(δi − 1 ) for
The function H(· ) is then defined as H(U ) = U tU , which is a selfadjoint matrix in n×n . We also define the sequence of self adjoint matrices {Aij ∈ n×n , i ∈ [ m ] , j ∈ [ n]} where each Aij is a all k ∈ [ n ] . Lastly , we define column vectors ej and rij of length n as following : ej is the unit vector with 1 as the jth element , and rij[k ] = −2UijUik if k = j , and 0 otherwise .
The following Lemma shows the structure of the sensitivity ma trices .
LEMMA 52 For any response matrix U , denote ∆ij = H(U )− H(U ) , where U is the response matrix identical to U in all entries ij = −Uij and U except with the ( i , j)th entry switched , ie , U kl = ij + 4(δi − 1)Gijejet Ukl for ( k , l ) = ( i , j ) . Then ∆2 j . ij = rijrt
PROOF . Recall that H(U ) = U tU is an n × n matrix with the ( j , j)th diagonal entry dj , where dj is the number of items rated by user j . Also H(U )kl = akl − bkl where ( bkl ) akl denotes the number of ( dis ) agreements between users k and l in rating the items that they have in common . Now since ∆ij = H(U ) − H(U ) , where U differs from U only in the ( i , j)th entry , ∆ij is again an n × n matrix with all but the jth row and column as 0 . To see why , consider ( k , l)th entry of ∆ij such that k = j and l = j . Both users k and l have same responses in both U and U . Thus the number of agreements and disagreements between k and l is same in U and U . Hence the ( k , l)th entry of ∆ij is zero . Since H(U ) = U tU and H(U ) = UtU are symmetric matrices , so is their difference ∆ij . Thus , the jth row and column for ∆ij are identical . We will show that the column is precisely the ij ) . Consider the kth element of this vector rij ( and hence row is rt row . If user k has rated item i , and k and j agree according to U , then they will disagree according to U . Similarly , if they disagree according to U , then they will agree according to U . Thus , kth element of rij , which is the difference in agreements and disagreements of users k and j will change by either 2 or −2 . These cases can be summarized succinctly as −2UijUik = rij[k ] , by definition . Only exception is rij[j ] , which is always 0 , since no user disagrees with himself on the same item i . Thus the jth column of ∆ij is precisely rij .
The fact that ∆ij is the matrix with jth row and column equal to rij and rest elements as zero can be written as
∆ij = rijet j + ejrt ij , which yields that
∆2 ij = ( rijet +(ejrt = rij(et +ej(rt j)(rijet ij)(rijet jrij)et ijrij)et = 0 + ( 1)rijrt j ) + ( rijet j ) + ( ejrt j)(ejrt ij ) ij)(ejrt ij ) jej)rt ij ijej)rt ij j + rij(et j + ej(rt ij + 0 + 4(δi − 1)Gijejet j . jej is 1 , and rt ijrij is 4Gij(δi − 1 ) ( since
Here , the last equation follows from using the following values of the four innerproducts highlighted in penultimate equation : rt ijej = jrij is 0 ( since ej has only jth entry as non zero , which is zero et in rij ) , et k rij[k]2 = k=j 4GijGik = 4(δi − 1)Gij ) . The k=j(−2UikUij)2 = entry equals8GikGij(δi − 1 ) . Using the above lemma , we can
Let Aij ∈ n×n be defined as a diagonal matrix where the kth proof follows . show that ∆2 ij is bounded by the matrix Aij .
LEMMA 53 ∆2 ij A2 ij .
PROOF . From Lemma 5.2 , ∆2 ij A2 ij + 4(δi − 1)Gijejet j . ij = rijrt Now if we show that rijrt ij/2 , then the proof of lemma is complete , since trivially , 4(δi − 1)Gijejet j A2 ij A2 ij . If k , l = j , then we have ij/2 , consider the ( k , l)th element , denoted
To show rijrt by Rkl , of rijrt ij/2 .
Rkl = ( −2UijUik)(−2UijUil ) = 4GijUikUil , and hence
|Rkl| = 4Gij|Uik||Uij| = 4GijGikGil .
If either k = j or l = j , then the ( k , l)th element is 0 . Hence for the kth row , the sum of the absolute values of ( k , l)th entries is
|Rkl| =
4GijGikGil = 4GijGik(δi − 1 ) , l l m since each user l = j who rated item i contributes exactly 1 to the sum . Thus the diagonal matrix with 4GijGik(δi − 1 ) as the kth diagij/2 by definition ij A2 onal entry , diagonally dominates rijrt is precisely such a diagonal matrix . Hence rijrt ij . Now A2 ij/2 .
The next statement shows that U tU is close to the expectation matrix EtE . Recall that ρ = maxj i . i=1 Gijδ2
LEMMA 54 Suppose the matrix U is generated by the rating generation process described above . Then , for every δ ∈ ( 0 , 1 ) ,
U tU − E[U tU ]2 ≤ 8ρ log ( n/δ )
≥ 1 − δ .
Pr
PROOF . Using the statement of Lemma 5.2 , we get that the sensitivity of H(· ) with respect to each variable Uij is bounded by A2 ij . Thus , from the statement of Theorem 5.1 , the variance parameter σ is given by flflflflfl m i=1 n j=1
σ2 =
A2 ij flflflflfl . m n n m m i=1 ij is diagonal , so is this sum . The kth diagonal entry ij is 8GikGij(δi − 1 ) and hence the kth diagonal entry of the
Since each A2 of A2 sum is given by
8GikGij(δi − 1 ) =
8(δi − 1)Gik
Gij j=1
8(δi − 1)Gikδi ≤ 8 i=1 j=1
Gikδ2 i = 8ρk .
= m i=1 i=1
Hence the spectral norm , which is the largest diagonal entry for a diagonal matrix , is simply 8 maxk ρk = 8ρ and hence σ2 = 8ρ . Using this value for σ , setting d = n , and t2 = 8σ2 log(n/δ ) = 64ρ log(n/δ ) in Theorem 5.1 completes the proof .
Finally , this implies the following result .
LEMMA 55 For a matrix U generated by the random rating generation process , with probability 1−δ , and E = E[U ] , U tU−
EtE ≤ 8ρ log ( n/δ ) + D , where D is the maximum number of ratings done by a person .
PROOF . Assuming the result of Lemma 5.4 holds , we only need to bound the norm of EtE − E[U tU ] . This is a diagonal matrix , with the jth diagonal entry to be dj(1 − w2 j ) . Hence , EtE − E[U tU ] ≤ maxj d2 j = D .
289 5.2 Analysis of estimators
In this section we show that the estimators for user reliabilities and item qualities have a small error . For notational simplicity , we assume that the event in Lemma 5.4 holds , ie , the matrix U tU is close to its expectation . 521 Algorithm 1 : Ratio of eigenvectors We first show the proof for Algorithm 1 , which takes the ratio of the top eigenvectors of U tU and GtG . The proof strategy is to first show that under a suitable set of assumptions for G , the matrix EtE has a large gap between the first and second eigenvalues , and hence can be represented accurately using only the topmost eigenvector— this will ensure that the eigenvector based Algorithm 1 has small error .
Let the first and second eigenvalues of GtG be denoted by µ1 and µ2 respectively , and the top two eigenvalues of EtE be denoted by λ1 and λ2 . Let g denote the first eigenvector of GtG , and e be that of EtE . Let gmin denote the minimum entry of g ; by Perron– j . Define Frobenius theorem , gmin > 0 . Recall ¯w2 = 1 n κ = U tU − EtE2 and W ∈ n × n to be the diagonal matrix with wj for the jth diagonal entry . j w2
LEMMA 56 λ1 ≥ µ1W g2 − µ2 . PROOF . Recall that EtE = ( GtG ) ⊗ ( wwt ) . Since µ1 and g are the first eigenvalue and vector of GtG , we have that GtG = µ1ggt +A , where A is the matrix defined as the difference between GtG and µ1ggt . Thus , A = µ2 .
EtE = ( GtG ) ⊗ ( wwt ) = ( µ1ggt + A ) ⊗ ( wwt )
= µ1(W g)(W g)t + A ⊗ ( wwt ) . Hence we can write using the triangle inequality :
( 4 )
EtE ≥ µ1(W g)(W g)t − A ⊗ ( wwt )
≥ µ1W g2 − A = µ1W g2 − µ2 , where we use A ⊗ ( wwt ) = W AW ≤ W2A ≤ A . This completes the proof .
LEMMA 57 ( etW g)2 ≥ W g2 − 2µ2 PROOF . From ( 4 ) , we know that EtE = µ1(W g)(W g)t + A⊗
( wwt ) , where A ≤ µ2 . Also eEtEet = λ1 and
µ1
. etEtEe = et(µ1(W g)t(W g ) + A ⊗ ( wwt))e
= µ1(etW g)2 + et(A ⊗ ( wwt))e ≤ µ1(etW g)2 + µ2 , where the last inequality again follows from A ⊗ ( wwt ) ≤ AW2 ≤ µ2 maxj w2 j ≤ µ2 . Thus , we have λ1 = etEtEe ≤ µ1(etW g)2 + µ2 .
LEMMA 58 λ2 ≤ 3µ2 . PROOF . Let x be the second eigenvector of EtE . Then xEtExt =
λ2 . Also x is perpendicular to the largest eigenvector e of EtE . So , we know ( etW g)2 + ( xtW g)2 ≤ W g2 . From Lemma 5.7 , we know ( etW g)2 ≥ W g2 − 2µ2/µ1 . Hence , ( xtW g)2 ≤ 2µ2/µ1 . Thus , we can write
λ2 = xtEtEx = xtµ1(W g)(W g)t + A ⊗ ( wwt )
= µ1(xtW g)2 + x(A ⊗ ( wwt))x ≤ µ1 · 2µ2 µ1
+ µ2 = 3µ2 .
LEMMA 59 Let κ = U tU − EtE2 and τ = W g . If
λ2+3κ
λ1
< 1 and 2µ2
µ1τ 2 < 1 , then flflflflu − W g
τ flflflfl ≤
2
λ2 + 3κ
λ1
+
4µ2
µ1τ 2 .
PROOF . Since u and e are the top eigenvector of U tU and EtE respectively , and κ = U tU − EtE , by applying a standard matrix perturbation bound [ 5 , Lemma 3.2 ] ,
( e · u)2 ≥ 1 − λ2 + 3κ
.
λ1
√
τ )2 ≥ We write the bound derived in Lemma 5.7 as follows : ( et W g µ1τ 2 . From the condition stated in the Lemma , since 2µ2 ≤ 1 − 2µ2 τ ≥ 1 − x ≥ 1 − x for 0 < x < 1 , we have et W g µ1τ 2 , and 1 − 2µ2 τ ≤ 4µ2 µ1τ 2 ≥ 1− 2µ2 and thus e− u2 ≤
Similarly , etu ≥
µ1τ 2 . Hence e− W g 1 − λ2+3κ
τ 2 = 2−2 etW g
≥ 1− λ2+3κ
µ1τ 2
λ1
λ1
2 λ2+3κ
λ1
. The proof follows from the triangle inequality .
LEMMA 510 Denote τ = W g and let ˆw be the vector with the ith element τ ui/gi . If λ2+3κ
< 1 and 2µ2
µ1τ 2 < 1 , then λ2 + 3κ 4µ2 µ1τ 2
λ1
+
2
≤ τ 2 ng2 min
λ1
ˆw − w2 n flflflflu − W g
τ flflflfl ≤
2
λ2 + 3κ
λ1
+
4µ2
µ1τ 2 .
PROOF . From Lemma 5.9 , we know that error( ˆw ) =
Hence
ˆw − w2 = ( τ u − W g ) ff g2 ≤ τ 2u − W g/τ2 √ Since (
√ y)2 ≤ 2(x + y ) , we have g2 min x + ˆw − w2 ≤ τ 2 g2 min
2
λ2 + 3κ
λ1
+
4µ2 µ1τ 2
, and hence the proof .
.
.
τ ≥ ¯w/r .
LEMMA 511 Let ¯w = users ; let τ = W g and r = gmax/gmin . Then , i w2 i n be the average reliability of
PROOF . This follows from considering the weighted graph cor responding to GtG . Then i ≥ n ¯w2g2 i g2 w2 min ≥ ¯w2n(g2 max/r2 ) ≥ ¯w2/r2 , i which completes the proof .
Combining the above lemmas , we get the final theorem about the error bounds .
THEOREM 512 For a fixed assignment graph G and a rating matrix U that is generated by the random rating generating process , if the graph G satisfies
µ1 ¯w2
µ2 <
( 5 ) then with probability 1 − δ , Algorithm 1 returns estimates ˆw , such that
4r
− 6ρ log ( n/δ ) − D
µ2 + D + 5ρ log ( n/δ )
. error( ˆw ) <
10 µ1ng2 min
290 PROOF . From Lemma 5.5 , with probability 1 − δ ,
U tU − EtE2 ≤ 8ρ log ( n/δ ) + D .
Assume that the above event holds . Also , for Lemma 5.10 , we need the following bounds :
λ2 + 3κ
λ1
< 1 ,
2µ2 µ1τ 2 < 1 .
( 6 )
Using the bounds on λ1 , λ2 and κ , the above bounds are satisfied if
− 6ρ log ( n/δ ) − D − 6ρ log ( n/δ ) − D .
µ2 <
<
µ1τ 2
4
µ1 ¯w2
4r
( 7 )
,
Conditioned on this and Lemma 5.10 , we have that error( ˆw ) =
ˆw − w2 n
≤ τ 2 ng2 min
2
λ2 + 3κ
λ1
+
4µ2 µ1τ 2 min where κ = U tU −EtE2 . Plugging in this value , and the bounds on λ2 and λ1 from Lemma 5.6 and Lemma 5.8 , we have that error( ˆw ) ≤ τ 2 ng2
3µ2 + 3D + 24ρ log ( n/δ )
6µ2 + 6D + 48ρ log ( n/δ )
µ2 + D + 5ρ log ( n/δ )
We simplify this by using µ1τ 2 − µ2 ≥ µ1τ 2/2 to give error( ˆw ) ≤ τ 2 ng2 ≤ 10τ 2 ng2
µ1τ 2 − µ2
4µ2 µ1τ 2
4µ2 µ1τ 2
µ1τ 2
µ1τ 2 min
+
+
1
.
. min
In order to illustrate our bounds better , we also state a corollary for ( D , ∆) regular graphs . This is also a restatement of Theorem 4.1 and thus completes its proof .
THEOREM 5.13 graph such that ∆ > 1 eigenvalue µ2 satisfies the condition
( THEOREM 41 ) If G is a ( D , ∆) regular and the second
8 ¯w2 , D > 256 log(n/δ )
2 ¯w2
The proof of this theorem is based on the following lemma .
LEMMA 515 Denote α = ∆ n ( w · ˆw ) . Then ( i ) ∆ ≥ α ≥ ∆( ¯w2 − )/2 and ( ii ) if ¯w2 > , the probability that the ith item is wrong is at most e−α2/16∆ ≤ e−∆( ¯w2− )2/64 .
PROOF . For ( i ) , note that = error(w , ˆw ) = w − ˆw2/n = . Thus w · ˆw/n = ( |w|2 + | ˆw|2 − n )/2n ≥
|w|2+| ˆw|2−2w· ˆw ( ¯w2 − )/2 , which yields the result .
For ( ii ) , define zi = n j Uij ˆwj . Then
E[zi ] = qiE[Gij]wj ˆwj = qi(∆/n)w · ˆw = qiα . j
Then from ( i ) and assuming ¯w2 > , we get α > 0 . Thus , sgn(E[zi ] ) is same as qi . Thus sgn(zi ) = qi implies that |zi − E[zi]| > E[zi ] . Thus the probability that sgn(zi ) = qi is at most Pr[|zi − E[zi]| > E[zi] ] . ity . Define yij = Uij ˆwj . Then zi =
For computing this probability , we will use Bernstein ’s inequalj yij . Also E[yij ] = qi(∆/n)wj ˆwj . Denote xij = yij − E[yij ] . Now we will apply Bernstein ’s inequality over xij for a fixed i but j from 1 to n . Note that −1 − |E[yij]| ≤ xij ≤ 1 + E[yij ] . Thus , it is safe to say that −2 ≤ xij ≤ 2 . Also ij ] = E[y2 ij ] − E[yij]2 = ( ∆/n ) ˆw2 j ( 1 − ( ∆/n)w2 ij ] ≤ ( ∆/n ) ˆw2 j − ( ∆/n)2w2 j ) ≤ ∆/n . Applying
E[x2 Thus , E[x2 j ˆw2 j .
Pr xij
≤ e
Bernstein ’s inequality for t = α/2 , we get fififififi fififififi ≥ α/2 j xij = Now j yij−E[ Thus | j yij| ≥ |qiα|−| j yij ] =
≤ e qiα j j e−α2/16∆ , which yields the result .
−α2 /8 ]+2(α/2)(1/3 )
E[x2 ij
−α2/8 ∆+α/3 ≤ e
−α2/16∆ . j yij−E[zi ] = j yij− j xij| ≥ α− α/2 with probability
µ2 <
¯w2D∆
,
16 then with probability 1 − δ , Algorithm 1 returns estimate ˆw , such that log(n/δ )
√
D error( ˆw ) = O
µ2 D∆
+
1 ∆
+
= O( ) .
6 . EXPERIMENTS
Analysis for Algorithm 2 . The proof for Algorithm 2 follows a similar route . We first show a similar matrix concentration inequality and then use it to follow the the proof outline in Section 5 . We postpone the details to the final version .
The proof is straightforward , after noting that gmin = 1√ nD = m∆ , and using a bound on µ1 ≥ m∆2 gree in GtG . n and n , the average de
Asymptotically , this gives error( ˆw ) = ˜O( 1
) . Finally , we show that estimating the set of user reliabilities accurately enables us to estimate the quality of each item with small error . We show that for a random ( D , ∆) regular graph the total error in estimating item quality falls exponentially with the maximum item degree , as well as with the average reliability . This is also a restatement of Theorem 4.2 and thus completes its proof .
∆ + 1√
D
THEOREM 5.14
( THEOREM 42 ) Let G be a random ( D , ∆ ) . Let ˆw be an estimate with regular graph . Let ¯w = error(w , ˆw ) ≤ . Then , error(ˆq ) ≤ e−∆( ¯w2− )2/64 . In particular , for ˆq obtained by Algorithm 1 , error(ˆq ) ≤ e
−O(∆( ¯w2− 1
∆ − 1√ i w2 i n
D
)2 ) .
In this section we experimentally analyze the accuracy of the proposed algorithms in estimating both item ratings and user reliabilities . We implemented both Algorithm 1 and 2 , which we denote by ALGORITHM 1 and ALGORITHM 2 respectively . We compare them with the following algorithms : the simple majority voting algorithm denoted by MAJORITY , the iterative EM algorithm denoted by EM , the spectral algorithm from Ghosh et al . [ 5 ] denoted by GKM , and the belief propagation algorithm from Karger et al . [ 10 ] denoted by KOS . We also implement LOWERBOUND which uses ground truth to compute the user reliabilities , and then uses the reliabilities to infer item ratings . Since it uses ground truth , it is not a true algorithm , but provides a benchmark to compare the performance of other algorithms .
Our implementation of ALGORITHM 1 and ALGORITHM 2 include the alternating projections heuristic described in Section 43 Datasets . To illustrate the properties of our algorithms we use both synthetic and real datasets as described below .
291 ( a ) TREC Items
( b ) NLP Items
( c ) TREC Users
( d ) NLP Users
Figure 1 : Error analysis on real datasets : ( a ) and ( b ) measure error in item ratings estimates as % of incorrect items , and ( c ) and ( d ) measure error in user reliabilities using correlation coefficient . Lower % means better item estimates , while higher correlation coefficient means better user estimates . In all cases , ALGORITHM 2 is either best or second best . In terms of aggregate error , ALGORITHM 2 is best in both the item rating estimates and one user reliability estimate .
Name
TREC.stage2 TREC.task1 TREC.task2
NLP.rte NLP.temp
NLP.emotions m 3568 3297 19033 800 462 600 labels 3568 3297 2275 800 800 600 n 181 120 762 164 76 228 responses 10,751 12000 88385 8000 4620 6000
Table 1 : Statistics for the real datasets used in our experiments .
( 1 ) TREC10 : this dataset is a collection of topic document pairs labeled as relevant or non relevant by mechanical turks . Several of the labels have ground truth assigned as well . There are three distinct datasets corresponding to different competitions of the workshop : namely , TREC.stage2 , TREC.task1 , and TRECtask2 The number of items , labeled items , users , and user responses for these datasets have been summarized in Table 1 .
( 2 ) NLP : this dataset [ 16 ] is a collection of three human judged
( 3 ) Synth : datasets , all having ground truth labels , as summarized in Table 1 . this is a synthetically generated dataset to help us analyze various algorithms in a controlled setting as a function of the numbers of responses by users and user reliabilities . 6.1 Real datasets
We compare the different algorithms over the TREC and NLP datasets . We evaluate both item rating estimates and user reliability estimates . Error in item ratings is measured in terms of % of incorrect item rating . Thus lower the value , better is the estimate . Figure 1(a ) shows the error for the three TREC datasets . We also show the overall aggregate error , which is the % of total items incorrectly predicted over the three datasets . For the first two datasets , the best algorithms are ALGORITHM 2 and EM , with MAJORITY much worst than the rest . This is perhaps because as we will see in synthetic datasets , MAJORITY is very sensitive to presence of spammers . In the third dataset , MAJORITY is in fact the best , along with ALGORITHM 2 . Thus overall , ALGORITHM 2 is the most robust algorithm and has lowest aggregate error for the TREC dataset.11
10sitesgooglecom/site/treccrowd/home
11Surprisingly , LOWERBOUND for TREC.task1 is worse than some
0 5 10 15 20 25 30 35 40 45 50stage2task1task2aggregateIncorrect Items ( %)DatasetMajorityEMGKMKOSAlgorithm 1Algorithm 2Lowerbound 5 10 15 20 25 30 35 40 45 50rtetempemotionsaggregateIncorrect Items ( %)DatasetMajorityEMGKMKOSAlgorithm 1Algorithm 2Lowerbound 0 0.05 0.1 0.15 0.2 0.25 0.3stage2task1task2aggregateCorrelation CoefficientDatasetMajorityEMGKMKOSAlgorithm 1Algorithm 2 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1rtetempemotionsaggregateCorrelation CoefficientDatasetMajorityEMGKMKOSAlgorithm 1Algorithm 2292 ( a ) Equal Spammers , Positive Correlation , Low Max Degree
( b ) Equal Spammers , Positive Correlation , High Max Degree
( c ) Equal Spammers , Negative Correlation , Low Max Degree
( d ) Equal Spammers , Negative Correlation , High Max Degree
( e ) No Spammers , Positive Correlation , Low Max Degree
( f ) No Spammers , Positive Correlation , High Max Degree
( g ) No Spammers , Negative Correlation , Low Max Degree
( h ) No Spammers , Negative Correlation , High Max Degree
Figure 2 : Item errors on synthetic data . User degrees are drawn according to a power law . First row considers equal number of spammers , hammers , and random users , while second considers only hammers and random users . First two columns consider user reliabilities positively correlated with their degrees , while third and fourth considers negative correlation . We break each scenario into two graphs to better visualize the differences . In all graphs as max degree increases , so does the degree skew , and then ALGORITHM 2 performs consistently better than GKM and KOS . In presence of spammers ( first row ) , MAJORITY and EM deteriorate . ALGORITHM 2 performs well across the spectrum .
Figure 1(b ) shows the item errors for the three NLP datasets . Again we see a similar story here , ALGORITHM 2 is best in two out of the three datasets . For the third , MAJORITY is best , with ALGORITHM 2 not far behind . Overall , ALGORITHM 2 has the lowest aggregate error in NLP .
Next we analyze the error in user reliability estimates . Since some of the algorithms like KOS give user reliabilities only up to a constant normalization factor , we cannot directly measure user reliability estimates by comparing them to the ground truth ( as they could be off by a constant factor ) . Thus we use Pearson ’s correlation coefficient to measure the accuracy of user reliability estimates , which is always a number between −1 and 1 , and measures the correlation between two vector quantities . A value of 1 means complete positive correlation ( up to some affine transformation ) , 0 means the two quantities are independent of each other , and −1 means they are negatively correlated . Larger the value , more the positive correlation , and therefore lower the error .
Figure 1(c ) and 1(d ) show that ALGORITHM 2 is either the best or close to the best in estimating user reliabilities for all the datasets , while other algorithms significantly underperform in at least one of the datasets . 6.2 Synthetic data
To better understand the performance of the algorithms with respect to the different parameters , we perform experiments over synthetic datasets . We generate synthetic datasets using the following steps . The number of items and the number of users is fixed to 1000 and 100 respectively . For the items , their binary ratings are generated as iid Bernoulli variables with p = 1/2 . algorithms . This is because many of the users are close to random , as evident in high % errors for this dataset . Thus having a true estimate for these random users is not helpful , and LOWERBOUND is in fact worse than some of the other algorithms .
For generating the bipartite graph between the items and users , we use powerlaw sequences for user degrees , where the number of items rated by users follow a powerlaw distribution with an exponent of 25 In each case , we generate a random graph satisfying the given degree sequence . We study the accuracy of different algorithms as a function of the maximum degree . We define three types of users : hammers , which have reliability 0.8 , spammers , who have reliability −0.8 , and random , who have reliability of 0 . We study the performance of algorithms as a function of the fraction of spammers , hammers and random users in the dataset . We consider two configurations : equal spammers , consisting of equal number of hammers , spammers and random users , and no spammers , consisting of equal number of hammers and random users .
To model real life scenarios we consider cases when the user reliabilities are correlated with degrees . For eg , reliable users could be more expensive , and hence offer less number of labels . Thus we consider the case of negative correlation where reliabilities are negatively correlated to the user degrees . For the sake of completeness , we also consider the case of positive correlation where reliabilities are positively correlated to user degrees .
This gives us four combinations : equal vs . no spammers and positive vs . negative correlations . Figure 2 shows the performance of all the algorithms for the four combinations . We explain the results below .
Figures 2(a ) and 2(b ) contains the results of the dataset with equal spammers and positive correlations . We break the graph into two parts to focus on the low and high degree parts separately . Because of a large number of spammers , MAJORITY has an error rate close to 50 % , which is so large that it does not even appear in the plot . EM also has a very large error for low max degree , but becomes competitive for high max degree . As the maximum user degrees become larger , the skew in degrees also becomes larger , and
10 15 20 25 30 35 40 45 50103050Incorrect Items ( %)Maximum DegreeLowerboundMajorityEMGKMKOSAlgorithm 1Algorithm 2 0 1 2 3 4 5 6 7 87090110130150170Incorrect Items ( %)Maximum DegreeLowerboundMajorityEMGKMKOSAlgorithm 1Algorithm 2 10 15 20 25 30 35 40 45 50103050Incorrect Items ( %)Maximum DegreeLowerboundMajorityEMGKMKOSAlgorithm 1Algorithm 2 0 1 2 3 4 5 6 7 87090110130150170Incorrect Items ( %)Maximum DegreeLowerboundMajorityEMGKMKOSAlgorithm 1Algorithm 2 10 15 20 25 30 35 40 45 50103050Incorrect Items ( %)Maximum DegreeLowerboundMajorityEMGKMKOSAlgorithm 1Algorithm 2 0 5 10 15 207090110130150170Incorrect Items ( %)Maximum DegreeLowerboundMajorityEMGKMKOSAlgorithm 1Algorithm 2 10 15 20 25 30 35 40 45 50103050Incorrect Items ( %)Maximum DegreeLowerboundMajorityEMGKMKOSAlgorithm 1Algorithm 2 0 5 10 15 207090110130150170Incorrect Items ( %)Maximum DegreeLowerboundMajorityEMGKMKOSAlgorithm 1Algorithm 2293 we notice that ALGORITHM 2 performs consistently better than the spectral methods of GKM and KOS for high maximum degree . This difference , although slight in synthetic data , manifests as a large one in real datasets , where the degree sequences are even more non uniform . We see a very similar trend in Figures 2(c ) and 2(d ) . For Figures 2(e ) and 2(f ) , which have no spammers and positive correlation , MAJORITY and EM do better than before . In fact , EM does slightly better than the spectral algorithms . Among the spectral algorithms , ALGORITHM 2 outperforms everyone else because of the non uniform degree sequence . Figures 2(g ) and 2(h ) show as a similar trend for negative correlations as in the case of positive correlation , but the effect is less pronounced with all the algorithms bunched together more closely .
In summary , KOS and GKM perform well when the degrees are uniform ( maximum degree is small and close to the minimum ) , but deteriorate when there is a skew in the degrees . EM performs well when there are no spammers , but deteriorates with the introduction of spammers . ALGORITHM 2 works well across the spectrum , and is robust to spammers and non uniform degree sequences . This helps ALGORITHM 2 perform well on most synthetic and real datasets .
7 . CONCLUSIONS
We studied the problem of aggregating user ratings when the user–item rating graph is arbitrary . We formulated a matrix completion problem and presented two eigenvector based algorithms that have guaranteed error bounds when the resulting user–user corating graph satisfies expansion properties . It would be interesting to see if one can say anything directly about the alternate projection based technique under a similar set of assumptions . In practice not all items need similar effort to rate ; incorporating this difficulty is also an interesting open direction .
8 . REFERENCES [ 1 ] S . Boyd and J . Dattorro . Alternating projections , 2003 . www . stanfordedu/class/ee392o/altprojpdf
[ 2 ] B . Carpenter . A hierarchical Bayesian model of crowdsourced relevance coding . In Prof . 12th TREC , 2011 . [ 3 ] A . Dawid and A . Skene . Maximum likelihood estimation of observer error rates using the EM algorithm . Applied Statistics , pages 20–28 , 1979 .
[ 4 ] O . Dekel and O . Shamir . Vox populi : Collecting high quality labels from a crowd . In Proc . 22nd COLT , 2009 .
[ 5 ] A . Ghosh , S . Kale , and R . P . McAfee . Who moderates the moderators ? : Crowdsourcing abuse detection in user generated content . In Proc . 12th EC , pages 167–176 , 2011 .
[ 6 ] A . Ghosh and R . P . McAfee . Crowdsourcing with endogenous entry . In Proc . 21st WWW , pages 999–1008 , 2012 .
[ 7 ] N . Gillis and F . Glineur . Low rank matrix approximation with weights or missing data is NP hard . SIAM J . Matrix Analysis Applications , 32(4):1149–1165 , 2011 .
[ 8 ] P . G . Ipeirotis and P . K . Paritosh . Managing crowdsourced human computation : a tutorial . In Proc . 20th WWW ( Companion Volume ) , pages 287–288 , 2011 .
[ 9 ] E . Kamar and E . Horvitz . Incentives for truthful reporting in crowdsourcing . In AAMAS , pages 1329–1330 , 2012 .
[ 10 ] D . Karger , S . Oh , and D . Shah . Iterative learning for reliable crowdsourcing systems . In Proc . 25th NIPS , pages 1953–1961 , 2011 .
[ 11 ] Q . Liu , J . Peng , and A . Ihler . Variational inference for crowdsourcing . In Proc . 26th NIPS , 2012 .
[ 12 ] P . K . Paritosh , P . Ipeirotis , M . Cooper , and S . Suri . The computer is the new sewing machine : benefits and perils of crowdsourcing . In Proc . 20th WWW ( Companion Volume ) , pages 325–326 , 2011 .
[ 13 ] V . C . Raykar and S . Yu . Eliminating spammers and ranking annotators for crowdsourced labeling tasks . JMLR , 13:491–518 , 2012 .
[ 14 ] V . C . Raykar , S . Yu , L . H . Zhao , G . H . Valadez , C . Florin ,
L . Bogoni , and L . Moy . Learning from crowds . JMLR , 11:1297–1322 , 2010 .
[ 15 ] V . Sheng , F . Provost , and P . Ipeirotis . Get another label ?
Improving data quality and data mining using multiple , noisy labelers . In Proc . 14th KDD , pages 614–622 , 2008 .
[ 16 ] R . Snow , B . O’Connor , D . Jurafsky , and A . Y . Ng . Cheap and fast–but is it good ? Evaluating non expert annotations for natural language tasks . In EMNLP , pages 254–263 , 2008 . [ 17 ] J . A . Tropp . User friendly tail bounds for sums of random matrices . Found . Comput . Math . , 12:389–434 , 2012 .
[ 18 ] P . Welinder , S . Branson , S . Belongie , and P . Perona . The multidimensional wisdom of crowds . In Proc . 24th NIPS , pages 2424–2432 , 2010 .
[ 19 ] D . Zhou , J . Platt , S . Basu , and Y . Mao . Learning from the wisdom of crowds by minimax entropy . In Proc . 26th NIPS , pages 2204–2212 , 2012 .
294
